Mach Learn 103:141–183
DOI 10.1007/s10994-016-5555-y
Online Passive-Aggressive Active learning
Jing Lu1 · Peilin Zhao2 · Steven C. H. Hoi1
Received: 24 March 2015 / Accepted: 23 February 2016 / Published online: 22 March 2016
© The Author(s) 2016
Abstract We investigate online active learning techniques for online classiﬁcation tasks.
Unlike traditional supervised learning approaches, either batch or online learning, which
often require to request class labels of each incoming instance, online active learning queries
only a subset of informative incoming instances to update the classiﬁcation model, aiming to maximize classiﬁcation performance with minimal human labelling effort during the
entire online learning task. In this paper, we present a new family of online active learning algorithms called Passive-Aggressive Active (PAA) learning algorithms by adapting
the Passive-Aggressive algorithms in online active learning settings. Unlike conventional
Perceptron-based approaches that employ only the misclassiﬁed instances for updating the
model, the proposed PAA learning algorithms not only use the misclassiﬁed instances to
update the classiﬁer, but also exploit correctly classiﬁed examples with low prediction con-
ﬁdence. Speciﬁcally, we propose several variants of PAA algorithms to tackle three types
of online learning tasks: binary classiﬁcation, multi-class classiﬁcation, and cost-sensitive
classiﬁcation. We give the mistake bounds of the proposed algorithms in theory, and conduct extensive experiments to evaluate the empirical performance of our techniques on both
standard and large-scale datasets, in which the encouraging results validate the empirical
effectiveness of the proposed algorithms.
Editors: Hang Li, Dinh Phung, Tru Cao, Tu-Bao Ho, and Zhi-Hua Zhou.
B Steven C. H. Hoi
 
 
Peilin Zhao
 
School of Information Systems, Singapore Management University, Singapore 178902, Singapore
Institute for Infocomm Research, A*STAR, Singapore 138632, Singapore
Mach Learn 103:141–183
Online learning · Active learning · Passive-aggressive · Cost-sensitive
classiﬁcation · Multi-class classiﬁcation
1 Introduction
Both online learning and active learning have been extensively studied in machine learning
and data mining . In a traditional online learning task (e.g., online classiﬁcation), a
learner is trained in a sequential manner to predict the class labels of a sequence of instances
as accurately as possible. Speciﬁcally, at each round of a typical online learning task, the
learner ﬁrst receives an incoming instance, and then makes a prediction of its class label. After
that, it is assumed to always receive the true class label from an oracle, which can be used to
measure the loss incurred by the learner’s prediction so as to update the learner if necessary.
In many real-world applications especially for mining real-life sequential arriving data (e.g.,
spam email ﬁltering), acquiring the true class labels from an oracle is often time-consuming
and costly due to the unavoidable interaction between the learner and the environment. This
has motivated the recent study of Online Active Learning , which explores active learning
strategy in an online learning setting to avoid requiring to request class labels of all incoming
instances.
A pioneering and state-of-the-art technique to online active learning is known as
Label Efﬁcient Perceptron or Selective Sampling Perceptron , or called Perceptron-based Active
Learning . In particular, consider an online classiﬁcation task, when a
learner receives an incoming instance xt, the learner ﬁrst makes a prediction ˆyt = sign( f (xt))
where f (xt) = wt ·xt, and then uses a stochastic approach to decide whether it should query
the class label or not, where the query probability is inversely proportional to the prediction
conﬁdence (e.g., the magnitude of the margin, i.e., δ/(δ + | f (xt)|) where δ is a positive
smoothing constant). If no class label is queried, the learner makes no update; otherwise, it
acquires the true label yt from the environment and follows the regular Perceptron algorithm
to make update (i.e., the learner will update the model if and only if the instance is misclassiﬁed according to the true class label). We summarize the common Framework of Online
Active learning algorithms in Algorithm 1.
In the above Perceptron-based active learning, if an incoming instance is predicted with
low conﬁdence by the current model, the learner very likely would query its class label.
However, if the instance is correctly classiﬁed according to the acquired true label, this
training instance will be discarded and never be used to update the learner according to
the principle of the Perceptron algorithm. Clearly this is a critical limitation of wasting the
effort of requesting class labels. To overcome this limitation, we present a new scheme for
online active learning, i.e., the Passive-Aggressive Active (PAA) learning, which explores
the principle of passive-aggressive learning . It not only decides when
the learner should make a query appropriately, but also attempts to fully exploit the potential
of every queried instance for updating the classiﬁcation model. To tackle different kinds of
machine learning tasks, we propose several variants of the PAA algorithms, i.e. the PAA
algorithm for online binary classiﬁcation tasks, the Multi-class Passive-Aggressive Active
(MPAA) learning algorithm for online multi-class classiﬁcation tasks and the Cost-Sensitive
Mach Learn 103:141–183
Algorithm 1 The Framework of Online Active learning
INITIALIZE : classiﬁer f0.
for t = 1, . . . , T do
Observe: xt ∈Rd, make prediction ˆyt based on ft(xt);
Make a decision whether to query the label (Zt = 1) or not (Zt = 0);
if Zt = 1 then
Query label yt, and suffer loss ℓt( ft);
if ℓt( ft) ̸= 0 then
Update: calculate classiﬁer ft+1;
Passive-Aggressive Active learning (CSPAA) algorithm for online binary classiﬁcation with
extremely unbalanced data. We theoretically analyse the mistake bounds of the proposed
algorithms and conduct extensive experiments to examine their empirical performance, in
which encouraging results show clear advantages of our algorithms over the baselines.
The rest of this paper is organized as follows. Section 2 reviews background and related
work. Section 3 presents the proposed framework of Passive-Aggressive Active-learning
(PAA) algorithms for online binary classiﬁcation tasks and analyzes the mistake bounds
of the proposed algorithms. Section 4 extends the proposed framework for online multiclass classiﬁcation by presenting a family of multi-class PAA algorithms (MPAA). Section 5
extends the proposed learning framework to tackle cost-sensitive classiﬁcation by presenting
cost-sensitive PAA (CSPAA) algorithms. Section 6 discusses our empirical studies and the
applications of our technique to large-scale real-world online learning tasks, and ﬁnally
Sect. 7 concludes this work.
2 Related work
Our work is closely related to three major categories of machine learning studies: online
learning, online active learning and cost-sensitive classiﬁcation. Below we brieﬂy review
some representative related work in each category.
2.1 Online learning
Online learning represents a family of efﬁcient and scalable machine learning algorithms . Unlike conventional batch learning methods
that assume all training instances are available prior to the learning task, online learning
repeatedly updates the predictive models sequentially, which is more appropriate for largescaleapplicationswheretrainingdataoftenarrivesequentially.Inliterature,avarietyofonline
learning methods have been proposed in machine learning. A classical online learning method
is the Perceptron algorithm , which updates the
model by adding a new example with some constant weight into the current set of support
vectors when the example is misclassiﬁed. Recently a lot of new online learning algorithms
Mach Learn 103:141–183
have been developed based on the criterion of maximum margin . One notable
technique in this category is the online Passive-Aggressive (PA) learning method , which updates the classiﬁcation function when a new example is misclassiﬁed
or its classiﬁcation score does not exceed some predeﬁned margin. PA algorithms have been
proved as a very successful and popular online learning technique for solving many realworld applications. Finally, we note that there are also a number of emerging online learning
algorithms proposed recently, such as second-order online learning , which make more accurate predictions and often converge faster
than ﬁrst-order algorithms.
Most of the above existing online learning methods generally belong to supervised and
passive learning. One major weakness of such supervised passive online learning methodology is the unrealistic assumption in that it assumes class labels of every
incoming instance will be already requested or made available at the end of every iteration,
which limits the application of online learning techniques for many real-world online learning tasks where class labels may not be always available or may be expensive to collect or
2.2 Online active learning
Online Active Learning algorithms emerge to address the main problem of conventional
supervised online learning approach, i.e. the strong dependence on labeled data. The basic
process of online active learning works in iterations. At each iteration, one unlabelled instance
is presented to the learner, and the learner needs to decide whether to query its label. If the
label is queried, then the learner can use the labelled instance to update the model, otherwise
the model is kept unchanged.
Speciﬁcally, there are two kinds of settings for online active learning, selective sampling
setting and label efﬁcient learning setting. We summarize their differences in several
aspects.Firstly,intheselectivesamplingsettingtheinstancesaredrawnrandomlyfromaﬁxed
distribution, while in the label efﬁcient setting the instances can be generated adversarially.
Secondly, the label efﬁcient model must make predictions on those instance where the label is
notrequested,whiletheselectivesamplingmodelsareconcernedwiththegeneralizationerror
rather than the performance of the algorithm on the sequence of instances. Our work belongs
to the second category. One of the most representative existing work in label efﬁcient learning
setting is the Label Efﬁcient Perceptron algorithm, where the probability of querying the label
is decided by the classiﬁcation conﬁdence. Following the similar setting, many variants of
this algorithm were proposed including Adaptive Label Efﬁcient Perceptron, Label Efﬁcient
Second-Order Perceptron , Adaptive Label Efﬁcient Second-Order
Perceptron and Label Efﬁcient Winnow . Although extensively
studied, the existing active learning algorithms still suffer from a serious limitation: the
effort of querying a correctly classiﬁed instance is wasted due to the adoption of Perceptron
update strategy.
In this work, we apply the popular PA algorithm to solve the online active learning task.
Our work enjoys two advantages. On one hand, different from the regular PA setting which
assumes every class label will be revealed, our approach queries the class labels of only a
limited amount of incoming instances through active learning. On the other hand, our effective
updating strategy fully exploits the potential of every queried instance and thus achieves a
superior performance compared to existing active learning algorithms.
Mach Learn 103:141–183
In addition, it is important to note that our work in online active learning is highly related to
but different from active learning in data streams .
Both of them attempt to achieve the highest prediction accuracy while querying the fewest
instance labels in the situation of sequentially arriving data. However, online active learning
algorithms focus on the updating strategy when one new instance arrives and may discard
past instances for efﬁcient learning. While active learning in data streams must preserve the
data distribution to detect the potential concept drift .
2.3 Cost-sensitive classiﬁcation
Cost-sensitive classiﬁcation has been extensively studied in data mining and machine learning
 . To address this problem,
researchers have proposed a variety of cost-sensitive metrics. The well-known examples
include the weighted sum of sensitivity and speciﬁcity , and the weighted misclassiﬁcation cost that takes cost into consideration when
measuring classiﬁcation performance . As a special case,
when the weights are both equal to 0.5, the weighted sum of sensitivity and speciﬁcity is
reduced to the well-known balanced accuracy . Although lots of
algorithms have been proposed, most of them are in batch setting.
Recently a few existing works have attempted to address online cost-sensitive classiﬁcation problems. Perceptron Algorithms with Uneven Margin (PAUM) is
an extension of the Perceptron with Margin (PAM) algorithm 
where the classiﬁer is updated whenever the classiﬁcation margin is smaller than a prede-
ﬁned threshold. The PAUM algorithm achieves a cost-sensitive update by setting different
thresholds for different class labels. Cost-Sensitive Passive-Aggressive (CPA) is proposed as a variant of PA algorithms, where the loss function is the
sum of a margin based term and a constant depending on the mistake type. Although
both of the above algorithms can achieve cost-sensitive updating, one main drawback is
that they are not designed to optimize a cost-sensitive measurement directly. Recently
some algorithms such as Cost-Sensitive Online Learning and Online
AUC maximization are proposed to address this drawback by directly
solving an optimization problem that optimizes the target cost-sensitive measurements.
Our work follows the idea of cost-sensitive learning but extends it to the active learning
setting, which enjoys a great advantage in saving the labor of labeling huge amount of
instances. Finally, we note that this journal article is based the extensions of our previous
conference papers .
3 Passive-Aggressive Active learning
3.1 Problem formulation and background review
We ﬁrst introduce the problem setting of a regular online binary classiﬁcation task. Let
{(xt, yt)| t = 1, . . . , T } be a sequence of input patterns for online learning, where each
instance xt ∈Rd received at the tth trial is a vector of d dimension and yt ∈{−1, +1}
is its true class label. The goal of online binary classiﬁcation is to learn a linear classiﬁer
ˆyt = sign(wt · xt) where wt ∈Rd is the weight vector at the tth round.
For the Perceptron algorithm , a learner
ﬁrst receives an incoming instance xt at tth round; it then makes a prediction based on the
Mach Learn 103:141–183
current classiﬁer wt; ﬁnally the true class label yt is disclosed. If the prediction is correct,
i.e., ˆyt = yt, no update is applied to the learner; otherwise, Perceptron updates the model
with the misclassiﬁed instance (xt, yt):
wt+1 ←wt + ytxt
Unlike Perceptron that updates the model only when a misclassiﬁcation occurs, the
Passive-Aggressive (PA) algorithms make update whenever the loss
function ℓ(wt; (xt, yt)) is nonzero, e.g., one can choose the hinge loss ℓ(wt; (xt, yt)) =
max(0, 1 −ytwt · xt). In particular, PA algorithms update the model wt+1 by solving three
variants of the optimization task:
2∥w −wt∥2 s.t. ℓt(w; (xt, yt)) = 0,
2∥w −wt∥2 + Cℓt(w; (xt, yt)),
2∥w −wt∥2 + Cℓt(w; (xt, yt))2,
where C > 0 is a penalty cost parameter. The closed-form solutions can be derived for the
above optimizations, i.e., wt+1 ←wt +τt ytxt, where the stepsize τt is computed respectively
as follows:
ℓt(wt; (xt, yt))/∥xt∥2,
min(C, ℓt(wt; (xt, yt))/∥xt∥2),
ℓt(wt; (xt, yt))/(∥xt∥2 + 1/(2C)).
Thus, PA algorithms are more aggressive in updating models than Perceptron.
3.2 Passive-Aggressive Active learning algorithms
In this section, we aim to develop new algorithms for online active learning. Unlike conventional online learning and pool-based active learning , the key challenges to an online active learning task are
two-fold: (i) when a learner should query the class label of an incoming instance, and (ii)
when the class label is queried and disclosed, how to exploit the labeled instance to update
the learner in an effective way. We propose Passive-Aggressive Active (PAA) learning to
tackle the above challenges. In particular, the PAA algorithms adopt a simple yet effective
randomized rule to decide whether the label of an incoming instance should be queried, and
employ state-of-the-art PA algorithms to exploit the labeled instance for updating the online
In particular, for an incoming instance xt at the tth round, the PAA algorithm ﬁrst computes
its prediction margin, i.e.,
pt = wt · xt,
by the current classiﬁer, and then decides if the class label should be queried according to a
Bernoulli random variable Zt ∈{0, 1} with probability equal to
δ/(δ + |pt|),
where δ ≥1 is a smoothing parameter. Such an approach is similar to the idea of marginbased active learning and has been adopted in
other previous work . If the outcome Zt = 0,
the class label will not be queried and the learner is not updated; otherwise, the class label is
queried and the outcome yt is disclosed. Whenever the class label of an incoming instance is
Mach Learn 103:141–183
queried, the PAA algorithms will try the best effort to exploit the potential of this example for
updatingthelearner.Speciﬁcally,itadoptsthePAalgorithmstoupdatethelinearclassiﬁcation
model wt+1 according to Eq. (1). Based on the different updating strategies, we name the hard
margin algorithm as PAA and the two soft margin algorithms as PAA-I and PAA-II. Clearly
this is able to overcome the limitation of the Perceptron-based active learning algorithm that
only updates the misclassiﬁed instances and wastes a large amount of correctly classiﬁed
instances with low prediction conﬁdence which can be potentially beneﬁcial to improving
the classiﬁer. Finally, we summarize the detailed steps of the proposed PAA algorithms in
Algorithm 2.
Algorithm 2 Passive-Aggressive Active learning algorithms (PAA)
INPUT : penalty parameter C > 0 and smoothing parameter δ ≥1.
INITIALIZATION : w1 = (0, . . . , 0)⊤.
for t = 1, . . . , T do
observe: xt ∈Rd, set pt = wt · xt, and predict ˆyt = sign(pt);
draw a Bernoulli random variable Zt ∈{0, 1} of parameter δ/(δ + |pt|);
if Zt = 1 then
query label yt ∈{−1, +1}, and suffer loss ℓt(wt) = max(0, 1 −ytwt · xt);
compute τt according to Eq. (1), and wt+1 = wt + τt ytxt;
wt+1 = wt;
3.3 Analysis of mistake bounds for the PAA algorithms
In this section, we aim to theoretically analyze the mistake bounds of the proposed PAA
algorithms. Before presenting the mistake bounds, we begin by presenting a technical lemma
which would facilitate the proofs in this section. With this lemma, we could then derive
the loss and mistake bounds for the three variants of PAA algorithm. For convenience, we
introduce the following notation: M = {t|t ∈[T ], ˆyt ̸= yt}, and L = {t|t ∈[T ], ˆyt =
yt, ℓt(wt; (xt, yt)) > 0}, where [T ] denotes {1, 2, . . . , T }.
Lemma 1 Let (x1, y1), . . . , (xT , yT ) be a sequence of input instances, where xt ∈Rd and
yt ∈{−1, +1} for all t. Let τt be the stepsize parameter for either of the three PAA variants
as given in Eq. (1). Then, the following bound holds for any w ∈Rd and any α > 0
Lt(α −|pt|) + Mt(α + |pt|)
2ατtℓt(w),
where Mt = I(t∈M), Lt = I(t∈L), I is the indicator function.
The detailed proof of Lemma 1 can be found in “Appendix 1”.
Based on Lemma 1, we ﬁrst derive the expected mistake bound for the PAA algorithm in
the separable case. We assume there exists some w such that yt(w · xt) ≥1, ∀t ∈[T ].
Theorem 1 Let (x1, y1), . . . , (xT , yT ) be a sequence of input instances, where xt ∈Rd and
yt ∈{−1, +1} and ∥xt∥≤R for all t. For any vector w that satisﬁes ℓt(w) = 0 for all
t, assuming δ ≥1, the expected number of mistakes made by the PAA algorithm on this
sequence of examples is bounded by
Mach Learn 103:141–183
By setting δ = 1, we can obtain the best upper bound as follows:
Proof Since ℓt(w) = 0, ∀t ∈[T ], according to Lemma 1, we have
Lt(α −|pt|) + Mt(α + |pt|)
Further, the above inequality can be reformulated as:
Zt2τt [Lt (α −|pt|) + Mt (α + |pt|)] −
α −|pt| −τt
α + |pt| −τt
α −|pt| −ℓt(wt)
α + |pt| −ℓt(wt)
α −|pt| −1 −yt pt
α + |pt| −1 −yt pt
α −|pt| −1 −|pt|
α + |pt| −1 + |pt|
α −1 + |pt|
α −1 −|pt|
Plugging α = δ+1
2 , δ ≥1 into the above inequality results in
Mt Ztτt(δ + |pt|),
since when Lt = 1, |pt| ∈[0, 1), (α −1+|pt|
) = δ−|pt|
> 0, and (α −1−|pt|
) = δ+|pt|
In addition, combining the fact τt = ℓt(wt)/∥xt∥2 ≥ℓt(wt)/R2 with the above inequality
concludes:
Mt Ztℓt(wt)(δ + |pt|).
Mach Learn 103:141–183
Taking expectation with the above inequality results in
Mtℓt(wt)(δ + |pt|)EZt
Mtℓt(wt)Zt(δ + |pt|)
Remark Since, the above theorem holds for any w such that ∀t, ℓt(w) = 0, we obtain the
tightest bound when w = w∗, where
w∗= arg min
t∈[T ] ytw⊤xt ≥1.
We can draw two observations as follows. When the norm of instances diverse greatly, i.e.
mint ||xt|| ≪R, to guarantee the zero loss of the smallest xt, the optimal ||w∗||2
2 should be
extremely large, which indicates a loose theoretical bound. Thus a proper data pre-processing
scheme, for example scaling all instance vectors to the same norm, will help improve performance. However, adopting two different data scaling scheme, for example, scaling ||xt|| = R
and ||xt|| = cR, c ∈R+, will not affect the performance. That is because when changing xt
to cxt (R to cR), w∗is changed to w∗/c, which makes no change to the theorem.
The above mistake bound indicates that the expected number of mistakes is proportional
to the upper bound of the instances norm R and inversely proportional to the margin 1/∥w∥2,
which is consistent with existing research . One disadvantage of the
above theorem is the linear separable assumption, since real world datasets are usually not
separable. To solve this problem, we present the expected mistake bound for the PAA-I
algorithm, which is suitable for the non-separable problem.
Theorem 2 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt ∈Rd and yt ∈
{−1, +1} and ∥xt∥≤R for all t. Assuming δ ≥1, for any vector w ∈Rd , the expected
number of prediction mistakes made by PAA-I on this sequence of examples is bounded from
∥w∥2 + (δ + 1)C
where β = 1
C , R2} and C is the aggressiveness parameter for PAA-I. Setting δ = 1
leads to the following bound
Mach Learn 103:141–183
Setting δ =
leads to the following bound
∥w∥2 + 4C
Proof According to Lemma 1, we have
α −|pt| −τt
α + |pt| −τt
α −1 + |pt|
α −1 −|pt|
Similar to Theorem 1, plugging α = δ+1
2 , δ ≥1 into the above inequality will result in
(δ + 1)τtℓt(w) ≥
Mt Ztτt(δ + |pt|).
Since τt ≥min{C, 1
R2 } holds when Mt = 1, the above inequality implies:
(δ + 1)τtℓt(w) ≥min
Mt Zt(δ + |pt|).
Taking expectation with the above equality and re-arranging the result conclude the theorem.
This theorem shows that the number of expected mistakes is bounded by a weighted sum
of the model complexity ∥w∥2 and the cumulative loss T
t=1 ℓt(w) suffered by it. Finally,
we give the mistake bound of the PAA-II algorithm in the following theorem.
Theorem 3 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt ∈Rd and yt ∈
{−1, +1} and ∥xt∥≤R for all t. Then, for any vector w ∈Rd , assuming δ ≥1, the expected
number of prediction mistakes made by PAA-II on this sequence of examples is bounded from
where C is the aggressiveness parameter for PAA-II. By setting δ = 1, we can further have
 103:141–183
Proof Deﬁne
O = α2∥w∥2 +
2ατtℓt(w),
Q = α2∥w∥2 +
2Cα2ℓt(w)2,
then it is easy to verify that O ≤O + P = Q.
Combining O ≤Q with Lemma 1, we have the following
(Lt Zt2τt(α −|pt|) + Mt Zt2τt(α + |pt|) ≤Q.
Furthermore, the above formulation can be reformulated as:
2Cα2ℓt(w)2
Lt(α −|pt|) + Mt(α + |pt|)
α −1 + |pt|
α −1 −|pt|
Similar to Theorem 1, plugging α = δ+1
2 , δ ≥1 into the above inequality results in
Mt Ztτt(δ + |pt|).
Taking expectation with the above inequality and using τt ≥1/{R2 + 1
2C }, will conclude the
Remark As proven in previous work , the expected mistake bounds
for active learning perceptron, which in our notation, could be expressed as:
≤(2δ + R2)2
By setting δ = 1, they further have
≤(2 + R2)2
We could ﬁnd that generally speaking, the bounds are similar and it depends on the parameters
to determine which is better. This is similar to the comparison between the PA bound and Perceptron bound 103:141–183
1999). However, the bound for Percetron Based Active learning has a R4∥w∥2 order term,
which may make it inferior to ours.
One problem in the above theorem is that the value of δ must be larger than 1, which
may result in too many requests. To ﬁx this issue, we propose the following theorem that can
resolve this limitation.
Theorem 4 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt ∈Rd and yt ∈
{−1, +1} and ∥xt∥≤R for all t. Assume there exists a vector w such that ℓt(w) = 0
for all t. For the PAA algorithm, if change the parameter for the Bernoulli distribution to
δ/(δ + 1 + |pt|) and δ ≥0, then its expected number of prediction mistakes on this sequence
is bounded by
When setting δ = 2, we get the best upper bound
Proof As proven in Theorem 1,
2ατtℓt(w) ≥
α −1 + |pt|
α −1 −|pt|
Plugging α = δ
2 + 1, δ ≥0 into the above inequality results in
Mt Ztτt(δ + 1 + |pt|),
since, when Lt = 1, |pt| ∈[0, 1), (α−1+|pt|
) = δ+1−|pt|
> 0, and (α−1−|pt|
) = δ+1+|pt|
Taking expectation with the above inequality and using τt ≥ℓt(wt)/R2 will conclude the
Remark Note this theorem demonstrates that for a new sampling probability
δ+1+|pt|, the
expected number of mistakes of the PAA algorithm is bounded. Similar property also holds
for PAA-I and PAA-II. It’s easy to get the corresponding bounds using the facts that τt ≥
R2 } for PAA-I and τt ≥1/{R2+ 1
2C } for PAA-II when Mt = 1. We omit the theorems
since it is similar to Theorem 4.
4 Extension to multi-class online classiﬁcation
In this section, we willgeneralize the PAA algorithms to solve online multi-class classiﬁcation
Mach Learn 103:141–183
4.1 Problem formulation and background review
We ﬁrst introduce the problem setting of the multi-class classiﬁcation problem. Let
{(xt, yt)| t = 1, . . . , T } be a sequence of input patterns for online learning, where each
instance xt ∈Rd received at the tth trial is a vector of d dimension and yt ∈Y = {1, . . . , k}
is its true class label. We adopt the multi-prototype model in Crammer et al. . The
classiﬁer is made up of k weight vectors wr ∈Rd,r ∈Y, where each vector corresponds to
one class label. During the prediction period of the tth iteration, the classiﬁer ﬁrst generates
a sequence of k prediction scores for all the class labels:
t · xt, . . . , wr
t · xt, . . . , wk
Then, by comparing the above scores, it picks the class label with the largest score as the
prediction,
ˆyt = arg max
We further deﬁne st as the irrelevant class with the highest prediction score:
st = arg max
The margin with respect to the hypothesis in the tth iteration is deﬁned to be the gap between
the prediction score of class yt and st:
t · xt −wst
Obviously, in a correct prediction, the margin γt > 0. In the max-score multi-class Perceptron
algorithm , the classiﬁer is only updated when a prediction
mistake occurs, i.e. ˆyt ̸= yt; otherwise, Perceptron updates the model with the misclassiﬁed
instance (xt, yt):
Unlike Perceptron that updates the model only when a misclassiﬁcation occurs, the Multiclass Passive-Aggressive (MPA) algorithms will also updates the
classiﬁer when the prediction is correct while the margin is not large enough. Speciﬁcally,
MPA algorithms will update the model when the hinge loss is nonzero, where the hinge loss
is deﬁned as,
ℓt(wt) = max(0, 1 −γt),
in which wt denotes the set of all k weight vectors in the classiﬁer.
If the hinge loss is positive, then multi-class PA algorithms will update the model wt+1
by solving three variants of the following optimization objectives:
r=1 ∥wr −wr
t ∥2 s.t. ℓt(w; (xt, yt)) = 0,
r=1 ∥wr −wr
t ∥2 + Cℓt(w; (xt, yt)),
r=1 ∥wr −wr
t ∥2 + Cℓt(w; (xt, yt))2,
where C > 0 is a penalty cost parameter. Luckily, the above optimizations enjoy closed-form
solutions as follows,
Mach Learn 103:141–183
where the stepsize τt is computed respectively as follows:
ℓt(wt; (xt, yt))/(2∥xt∥2),
min(C, ℓt(wt; (xt, yt))/(2∥xt∥2)),
ℓt(wt; (xt, yt))/(2∥xt∥2 + 1/(2C)).
These update rules generally implies that larger losses will result in larger learning rates.
4.2 Multi-class Passive-Aggressive Active learning algorithms (MPAA)
In this section, we aim to develop a group of new algorithms for online active multi-class
classiﬁcation tasks, termed as the Multi-class Passive-Aggressive Active learning (MPAA)
algorithms. Firstly, a similar but different stochastic rule is adopted in deciding whether to
query the label of a certain instance. This rule will be introduced in the later paragraphs. If
a label is queried, the update rules of MPAA algorithms simply follow those of the MPA
algorithms introduced in the previous section.
Now we will introduce the randomized rule for querying labels. As stated in the PAA
algorithms, the probability of querying a label, Pr(Zt = 1) should be inversely proportional
to the margin of the model on the current instance (with a smoothing parameter δ), which is
considered as a kind of conﬁdence of the model. However, in multi-class setting, the margin
of the model on the current example is not available, since the label is not disclosed when
the probability is computed.
To solve this problem, we introduce the label with second largest prediction score:
˜yt = arg max
r∈Y,r̸= ˆyt
and propose a different conﬁdence score
pt = w ˆyt
t · xt −w ˜yt
which is the gap between the prediction scores of predicted label ˆyt and the second label ˜yt.
Note that pt ≥0 holds for all cases. The relation between the conﬁdence value pt and the
margin γt has two cases: 1) if the prediction is correct, i.e., ˆyt = yt and st = ˜yt, then pt = γt;
2) if the prediction is incorrect, then it is easy to check pt ≤|γt|. Given this conﬁdence value,
the probability of querying a label is set as
Pr(Zt = 1) =
where δ > 0 is a smooth parameter. This probability is larger than
δ+|γt|, which will facilitate
the theoretical analysis later.
Finally,wesummarizethedetailedstepsoftheproposedMPAAalgorithmsinAlgorithm3.
4.3 Analysis of mistake bounds for the MPAA algorithms
In this section, we aim to theoretically analyze the mistake bounds of the proposed MPAA
algorithms.
Theorem 5 Let (x1, y1), . . . , (xT , yT ) be a sequence of input instances, where xt ∈Rd and
yt ∈Y and ∥xt∥≤R for all t. For any classiﬁer w such that ℓt(w) = 0 for all t, assuming
δ ≤1 the expected number of mistakes made by the MPAA algorithm on this sequence of
Mach Learn 103:141–183
Algorithm 3 Multi-class Passive-Aggressive Active learning algorithms (MPAA)
INPUT : penalty parameter C > 0 and smoothing parameter δ ≥1.
INITIALIZATION : wr
1 = (0, . . . , 0)⊤, for each r ∈Y.
for t = 1, . . . , T do
observe: xt ∈Rd, predict ˆyt as Equation(2) and set pt as Equation (5).
draw a Bernoulli random variable Zt ∈{0, 1} of parameter δ/(δ + pt);
if Zt = 1 then
query label yt ∈{1, . . . , k}, and suffer loss ℓt(wt) = max(0, 1 −γt);
compute τt according to equation (4), and update the model as Equation (3)
wt+1 = wt;
examples is bounded by
By setting δ = 1, we can obtain the best upper bound as follows:
The proof can be found in “Appendices 2 and 3”. Similarly, we can get the mistake bounds
for the other variants of MPAA algorithms. Because it is easy, we skip it for conciseness.
5 Extension to cost-sensitive online classiﬁcation
In this section, we further extend the PAA algorithms to deal with highly imbalanced binary
classiﬁcation tasks, where instead of simply maximizing the accuracy, we should further
consider some cost-sensitive evaluation metrics.
5.1 Problem formulation and cost-sensitive classiﬁcation review
For binary classiﬁcation, the result of each prediction for an instance can be classiﬁed into
four cases: (1) TruePositive (TP) if ˆyt = yt = +1; (2) FalsePositive (FP) if ˆyt = +1 and
yt = −1; (3) TrueNegative (TN) if ˆyt = yt = −1; and (4) FalseNegative (FN) if ˆyt =
−1 and yt = +1. We now consider a sequence of training examples (x1, y1), . . . , (xT , yT )
for online learning. Then, we denote by M to be the set of indexes that correspond to the
trial of misclassiﬁcation:
M = {t|yt ̸= sign(wt · xt), ∀t ∈[T ]}.
Similarly, we denote by Mp = {t|t ∈M and yt = +1} the set of indexes for false negatives,
and Mn = {t|t ∈M and yt = −1} the set of indexes for false positives. Further introduce
notation M = |M| to denote the number of mistakes, Mp = |Mp| to denote the number of
false negative and Mn = |Mn| to denote the number of false positives. Let Tp denote the
number of positive instances and Tn denote the number of negative instances, we have the
following performance metrics: sensitivity is deﬁned as the ratio between the number of true
Mach Learn 103:141–183
positives Tp −Mp and the number of positive examples; and speciﬁcity is deﬁned as the ratio
between Tn −Mn and the number of negative examples. These can be summarized as:
sensitivity = Tp −Mp
speci f icity = Tn −Mn
Without loss of generality, we assume “positive” is the rare class, i.e., Tp ≪Tn.
For traditional online learning, the performance is measured by the prediction accuracy (or
mistakerateequivalently)overthesequenceofexamples.Thisisinappropriateforimbalanced
data because a trivial learner that simply classiﬁes any example as negative could achieve a
quite high accuracy for a highly imbalanced dataset. Thus, we propose to study new online
learning algorithms, which can optimize a more appropriate performance metric, such as the
sum of weighted sensitivity and speciﬁcity, i.e.,
sum = ηp × sensitivity + ηn × speci f icity,
where 0 ≤ηp, ηn ≤1 and ηp + ηn = 1. When ηp = ηn = 1/2, sum is the wellknown balanced accuracy, which is adopted as a metric in the existing studies for anomaly
detection(LiandTsang2011).Ingeneral,thehigherthesumvalue,thebettertheperformance.
Besides, another suitable metric is the total cost suffered by the algorithm, which is deﬁned
cost = cp × Mp + cn × Mn,
where Mp and Mn are the number of false negatives and false positives respectively, 0 ≤
cp, cn ≤1 are the cost parameters for positive and negative classes, respectively, and we
assume cp + cn = 1. The lower the cost value, the better the classiﬁcation performance.
5.2 Cost-sensitive Passive-Aggressive Active learning algorithm (CSPAA)
We now propose the CSPAA framework for cost-sensitive online binary classiﬁcation task
by optimizing the previous two cost-sensitive measures. Before presenting our algorithms,
we prove an important proposition below to motivate our solution. For simplicity, we assume
∥xt∥= 1 for the rest.
Proposition 1 Consider a cost-sensitive classiﬁcation problem, the goal of maximizing the
weighted sum in (6) or minimizing the weighted cost in (7) is equivalent to minimizing the
following objective:
ρI(ytw·xt<0) +
I(ytw·xt<0),
where ρ = ηpTn
ηnTp for the maximization of the weighted sum, Tp and Tn are the number of
positive examples and negative examples respectively, ρ = cp
cn for the minimization of the
weighted misclassiﬁcation cost, and Iπ is the indicator function that outputs 1 if the statement
π holds and 0 otherwise.
The proof can be found in the “Appendix 4”.
Proposition 1 gives the explicit objective function for optimization, but the indicator
function is non-convex. To tackle this issue, we replace the indicator function by its convex
surrogate, i.e., a modiﬁed hinge loss function:
ℓ(w; (x, y)) = max(0, ρ ∗I(y=1) + I(y=−1) −y(w · x)).
Mach Learn 103:141–183
As a result, we can formulate the primal objective function as follows:
where the regularization parameter C > 0, the loss function ℓt(w) = max(0, ρt −yt(w ·xt))
and ρt = ρ ∗I(yt=1) +I(yt=−1). The idea of this formulation is somewhat similar to the biased
formulation of batch SVM for learning with imbalanced datasets .
To online optimize the above objective (10), following the passive-aggressive learning
method , we have a similar online optimization objective:
wt+1 = arg min
2∥w −wt∥2 + Cℓt(w),
which enjoys the following closed-form solution:
wt+1 ←wt + τt ytxt,
where τt = min(C, ℓt(wt)).
At the t th round, the CSPAA algorithm decides if the class label should be queried
according to the same margin based Bernoulli random variable Zt ∈{0, 1} as that used in
the PAA algorithm. Finally, Algorithm 4 summarizes the details of the proposed CSPAA
algorithms.
Algorithm 4 Cost-Sensitive Passive-Aggressive Active learning algorithm (CSPAA).
INPUT: penalty parameter C, bias parameter ρ and smooth parameter δ.
INITIALIZATION : w1 = 0.
for t = 1, . . . , T do
receive an incoming instance xt ∈Rd;
predict label ˆyt = sign(pt), where pt = wt · xt;
draw a Bernoulli random variable Zt ∈{0, 1} of parameter δ/(δ + |pt|);
if Zt = 1 then
query label yt ∈{−1, +1};
suffer loss ℓt(wt) = ℓ(wt; (xt, yt));
wt+1 = wt + τt ytxt, where τt = min{C, ℓt(wt)};
wt+1 = wt + τt ytxt, where τt = 0;
Remark It is interesting to analyze the impact of the sampling factor parameter δ. In general,
the larger the value of δ, the larger the resulting number of queries issued by the online active
learner. In particular, when setting δ →∞, it is reduced to the extreme case of querying
class label of every instance in the online learning process. In general, one can simply ﬁx
δ to some constant to trade off a proper ratio of queries. Besides, an even better approach
is to adaptively change the value of δ during the online learning process. In particular, we
expect to query more examples at the beginning of the online learning task in order to build
a good classiﬁer, and gradually reduce the ratio of queries when the classiﬁer becomes
more and more accurate during the online learning process. To this purpose, we suggest
a simple yet effective scheme to adaptively update the parameter δ at the t th learning
step as: δt ←δt−1 ∗
t+1. We will examine the impact of the sampling factor δ in our
experiments.
Mach Learn 103:141–183
5.3 Theoretical bound analysis for the CSPAA algorithms
Below gives theoretical analysis of its performance on online binary cost-sensitive active
learning tasks in terms of two types of performance metrics. The proofs can be found in the
“Appendices 5, 6 and 7”.
Theorem 6 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt ∈Rd and yt ∈
{−1, +1} and ∥xt∥= 1 for all t. Then, for any vector w ∈Rd , the expected weighted number
of prediction mistakes made by CSPAA on this sequence of examples is bounded as:
(1 + δ)Cℓt(w)
where C ≥ρ is the aggressiveness parameter for CSPAA.
Now our goal is to analyze the performance of the proposed algorithm in terms of the two
metrics, sum and cost. We ﬁrst consider the weighted sum of sensitivity and speciﬁcity, i.e.,
sum = ηp × sensitivity + ηn × speci f icity,
where ηp + ηn = 1 and ηp ≥ηn > 0. The following theorem gives the bound on the sum
by the proposed CSPAA algorithm.
Theorem 7 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples, where xt ∈Rd, yt ∈
{−1, +1} and ∥xt∥= 1 for all t. By setting ρ = ηpTn
ηnTp , and assuming C ≥ρ, for any w ∈Rd,
we have the following bound for the proposed CSPAA algorithm:
E[sum] ≥1 −ηn
(1 + δ)Cℓt(w)
Furthermore, when ηp = ηn = 1/2, the balanced accuracy (BA) is bounded from below by
E[B A] ≥1 −
(1 + δ)Cℓt(w)
Remark In the above, setting δ = 1 leads to the following bound
E[sum] ≥1 −ηn
Setting δ =
t=1 ℓt(wt)
leads to the following bound
E[sum] ≥1 −ηn
∥w∥2 + 4C
In the above approach, the bias parameter ρ is set to ηpTn
ηnTp , in which the ratio Tn
not be available in advance. To alleviate this issue, we consider another approach using the
cost based performance metric. Speciﬁcally, we propose to set ρ = cp
cn , where cp and cn are
the predeﬁned cost parameters of false negative and false positive, respectively. We assume
Mach Learn 103:141–183
cp + cn = 1 and 0 ≤cn ≤cp since we would prefer to improve the accuracy of predicting
the rare positive examples. By this setting, the following theorem gives us the cumulative
cost bound of the proposed CSPAA algorithm.
Theorem 8 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples, where xt ∈Rd, yt ∈
{−1, +1} and ∥xt∥= 1 for all t. By setting ρ = cp
cn , and assuming C ≥ρ, for any w ∈Rd,
the overall cost made by the proposed CSPAA algorithm over this sequence of examples is
bounded as follows:
E[cost] ≤cn
(1 + δ)Cℓt(w)
Remark Setting δ = 1 for the above theorem leads to the following bound:
E[cost] ≤cn
Setting δ =
t=1 ℓt(wt)
leads to the following bound:
E[cost] ≤cn
∥w∥2 + 4C
6 Experimental results
In this section, we evaluate the empirical performance of the proposed family of Passive-
Aggressive Active-learning algorithms for three types of online active learning tasks: binary
classiﬁcation, multi-class classiﬁcation, and cost-sensitive classiﬁcation tasks.
6.1 Evaluation of PAA algorithms for binary classiﬁcation tasks
This section will evaluate the empirical performance of the proposed PAA algorithms on
online binary classiﬁcation tasks.
6.1.1 Compared algorithms and experimental testbed
We compare the proposed PAA algorithms with the Perceptron-based Active learning, and
their random variants, which are listed as follows:
– “RPE”: the Random Perceptron algorithm ;
– “RPA”: the Random Passive-Aggressive algorithms, including RPA, RPA-I, RPA-II.
These algorithms adopt the same updating strategy as the proposed PAA algorithms,
while the querying strategy is different. Instead of actively querying the class label, these
algorithms utilize a uniform random sampling approach. The comparison between the
RPA algorithms and the proposed PAA algorithms will validate the effectiveness of the
active querying strategy.
– “PEA”: the Perceptron-based Active learning algorithm ; This
algorithm adopts the same active querying strategy as the proposed PAA algorithm, while
Mach Learn 103:141–183
Table 1 Summary of datasets
used in binary online
classiﬁcation experiments
#Instances
theupdatingrulefollowsthePerceptronalgorithm.Thecomparisonbetweentheproposed
algorithms and the Perceptron-based algorithms will support our main motivation, to fully
exploit the potential of every queried instance.
– “SEL-ada”: the Selective Sampling Perceptron with Adaptive Parameter ; Unlike the PEA algorithm where the smoothing parameter δ is a constant
for all iterations, the δ in the SEL-ada algorithm is set in an online fashion, i.e. δt ∝
√1 + ErrCountt−1.
– “SEL-2nd”: the Selective Sampling Second-Order Perceptron algorithm . This algorithm adopts the same active querying strategy as the proposed PAA
algorithm, but the update strategy follows the Second-order Perceptron algorithm . The SEL-2nd achieved the best performance among all compared
algorithms in .
– “PAA”: the Passive-Aggressive Active learning algorithms, including PAA, PAA-I, PAA-
To examine the performance, we conduct extensive experiments on a variety of benchmark
datasets from web machine learning repositories. Table 1 shows the details of twelve binaryclass datasets used in our experiments. All of these datasets can be downloaded from LIBSVM
website1 and UCI machine learning repository.2 These datasets are chosen fairly randomly
in order to cover various sizes of datasets.
All the compared algorithms learn a linear classiﬁer for the binary classiﬁcation tasks. The
penalty parameter C is searched from 2[−5:5] through cross validation for all the algorithms
and datasets. The smoothing parameterδ is setas 2[−10:10] in order to examine varied sampling
situations. All the experiments were conducted over 20 runs of different random permutations
for each dataset. All the results were reported by averaging over these 20 runs. For performance metrics, we select F-measure, which is deﬁned as F-measure = 2 Precision∗Recall
Precision+Recall .
6.1.2 Evaluation on ﬁxed ratio of queries
In the ﬁrst experiment, we evaluate the performance of our proposed PAA algorithms on
the online binary classiﬁcation task with ﬁxed query rates. We adjust δ to make the percentage of queried instances near 10 and 20% and compare all the algorithms on a fair
platform. The results are shown in Tables 2 and 3. Several observations can be drawn from the
1 
2 
Mach Learn 103:141–183
Table 2 Evaluation of the PAA algorithms against the other baselines (time in seconds)
Request 10% labels
Request 20% labels
0.837 ± 0.006
9.77 ± 0.82
0.832 ± 0.003
20.13 ± 0.76
0.836 ± 0.005
9.83 ± 1.34
0.831 ± 0.005
19.83 ± 1.83
0.838 ± 0.009
9.91 ± 2.62
0.834 ± 0.007
20.99 ± 5.98
0.800 ± 0.008
9.72 ± 0.31
0.796 ± 0.006
20.21 ± 0.47
0.793 ± 0.009
9.82 ± 0.29
0.796 ± 0.008
19.82 ± 0.51
0.859 ± 0.003
9.67 ± 0.39
0.860 ± 0.002
19.95 ± 0.39
0.858 ± 0.004
9.69 ± 0.35
0.858 ± 0.002
19.85 ± 0.56
0.830 ± 0.006
9.90 ± 0.62
0.827 ± 0.005
19.86 ± 0.91
0.864 ± 0.002
9.71 ± 0.29
0.864 ± 0.002
20.00 ± 0.49
(p < 0.0001)
(p < 0.0001)
0.863 ± 0.001
9.73 ± 0.37
0.862 ± 0.002
19.93 ± 0.60
(p < 0.0001)
(p = 0.0031)
0.991 ± 0.001
9.85 ± 0.46
0.994 ± 0.001
20.76 ± 0.93
0.990 ± 0.002
9.870 ± 0.63
0.992 ± 0.001
19.87 ± 0.82
0.993 ± 0.001
10.36 ± 0.77
0.995 ± 0.001
19.82 ± 1.20
0.971 ± 0.006
9.82 ± 0.28
0.984 ± 0.003
20.95 ± 0.37
0.987 ± 0.002
10.21 ± 0.19
0.992 ± 0.002
20.26 ± 0.42
0.986 ± 0.003
10.15 ± 0.22
0.992 ± 0.001
20.29 ± 0.36
0.986 ± 0.002
9.86 ± 0.34
0.992 ± 0.002
20.52 ± 0.35
0.996 ± 0.000
10.09 ± 0.33
0.997 ± 0.000
20.24 ± 0.58
0.996 ± 0.001
10.14 ± 0.35
0.997 ± 0.000
20.33 ± 0.37
(p < 0.0001)
(p < 0.0001)
0.996 ± 0.001
9.73 ± 0.44
0.997 ± 0.000
20.43 ± 0.51
(p < 0.0001)
(p < 0.0001)
0.572 ± 0.008
10.29 ± 0.28
0.567 ± 0.007
19.81 ± 0.70
0.568 ± 0.006
10.36 ± 0.58
0.565 ± 0.007
19.83 ± 1.06
0.569 ± 0.007
9.81 ± 0.67
0.574 ± 0.006
20.36 ± 1.25
0.510 ± 0.005
10.29 ± 0.15
0.514 ± 0.005
19.87 ± 0.21
0.512 ± 0.006
9.77 ± 0.11
0.516 ± 0.006
20.22 ± 0.23
0.606 ± 0.003
9.97 ± 0.16
0.608 ± 0.003
19.91 ± 0.20
0.603 ± 0.003
9.71 ± 0.13
0.606 ± 0.002
19.93 ± 0.18
0.571 ± 0.006
9.72 ± 0.41
0.566 ± 0.005
20.20 ± 0.47
0.621 ± 0.003
9.90 ± 0.83
0.626 ± 0.003
19.90 ± 0.39
(p < 0.0001)
(p < 0.0001)
0.623 ± 0.004
9.73 ± 0.39
0.628 ± 0.004
19.90 ± 0.37
(p < 0.0001)
(p < 0.0001)
Obviously, the two proposed soft margin algorithms, PAA-I and PAA-II are always the two winners (as
highlighted). To test the signiﬁcance of our experiment, we compare the F-measures of PAA-I and PAA-II
with that of the best one among all the 7 compared algorithms and report the p value of the t test
Mach Learn 103:141–183
Table 3 Evaluation of the PAA algorithms against the other baselines (time in seconds)
Request 10% labels
Request 20% labels
0.846 ± 0.006
10.06 ± 0.46
0.854 ± 0.005
20.51 ± 0.67
0.835 ± 0.012
10.45 ± 0.79
0.849 ± 0.004
20.22 ± 1.08
0.843 ± 0.012
9.85 ± 0.57
0.859 ± 0.006
20.36 ± 0.77
0.800 ± 0.012
10.04 ± 0.44
0.819 ± 0.007
20.51 ± 0.89
0.827 ± 0.011
9.94 ± 0.47
0.838 ± 0.009
20.33 ± 0.60
0.858 ± 0.007
9.87 ± 0.50
0.875 ± 0.003
20.10 ± 0.63
0.860 ± 0.005
10.01 ± 0.47
0.875 ± 0.005
19.98 ± 0.64
0.865 ± 0.006
9.83 ± 0.56
0.867 ± 0.006
20.35 ± 0.84
0.881 ± 0.004
9.72 ± 0.41
0.888 ± 0.002
20.06 ± 0.44
(p < 0.0001)
(p < 0.0001)
0.884 ± 0.003
9.91 ± 0.40
0.889 ± 0.003
19.71 ± 0.50
(p < 0.0001)
(p < 0.0001)
0.747 ± 0.014
9.813 ± 0.48
0.770 ± 0.007
19.73 ± 0.74
0.740 ± 0.012
9.95 ± 0.49
0.759 ± 0.007
20.01 ± 0.93
0.752 ± 0.012
9.71 ± 0.49
0.773 ± 0.008
20.54 ± 1.06
0.718 ± 0.015
9.80 ± 0.59
0.746 ± 0.012
19.99 ± 0.55
0.741 ± 0.015
10.12 ± 0.42
0.763 ± 0.008
20.47 ± 0.72
0.768 ± 0.011
9.62 ± 0.57
0.793 ± 0.006
19.94 ± 0.85
0.771 ± 0.010
9.96 ± 0.47
0.795 ± 0.008
20.02 ± 0.49
0.780 ± 0.008
9.96 ± 0.39
0.790 ± 0.006
20.23 ± 0.67
0.794 ± 0.008
9.70 ± 0.44
0.813 ± 0.005
19.87 ± 0.69
(p < 0.0001)
(p < 0.0001)
0.790 ± 0.008
9.72 ± 0.39
0.812 ± 0.005
20.17 ± 0.63
(p < 0.0001)
(p < 0.0001)
0.263 ± 0.015
10.04 ± 0.32
0.342 ± 0.013
20.05 ± 0.26
0.236 ± 0.018
9.98 ± 0.57
0.304 ± 0.016
20.06 ± 0.39
0.363 ± 0.019
9.72 ± 0.53
0.428 ± 0.016
20.48 ± 0.83
0.188 ± 0.007
10.01 ± 0.09
0.225 ± 0.007
20.07 ± 0.15
0.198 ± 0.013
9.78 ± 0.14
0.249 ± 0.010
20.03 ± 0.15
0.280 ± 0.009
9.83 ± 0.11
0.370 ± 0.008
19.88 ± 0.12
0.227 ± 0.011
10.06 ± 0.12
0.290 ± 0.011
19.93 ± 0.12
0.385 ± 0.015
9.78 ± 0.32
0.441 ± 0.021
20.03 ± 0.66
0.391 ± 0.015
9.81 ± 0.16
0.461 ± 0.009
19.92 ± 0.22
(p < 0.0001)
(p < 0.0001)
0.386 ± 0.015
9.98 ± 0.26
0.458 ± 0.012
19.89 ± 0.37
(p < 0.0001)
(p < 0.0001)
Obviously, the two proposed soft margin algorithms, PAA-I and PAA-II are always the two winners (as
highlighted). To test the signiﬁcance of our experiment, we compare the F-measures of PAA-I and PAA-II
with that of the best one among all the 7 compared algorithms and report the p value of the t test
Mach Learn 103:141–183
First of all, we observe that all the active learning algorithms outperform their corresponding random versions in terms of F-measure results, which validates the efﬁcacy and
advantage of the active learning strategies.
Second, we ﬁnd that the two soft-margin PAA algorithms (i.e., PAA-I and PAA-II) achieve
similar F-measure performance on most of the datasets, while the hard-margin PAA usually
performs slightly worse. This may possibly be caused by overﬁtting on noisy training data,
since PAA conducts a more aggressive update and is thus more sensitive to noise.
Third, under the same fraction of queried labels, the two soft PAA algorithms always
achieve signiﬁcantly higher F-measure than all compared algorithms. This promising result
indicates that PAA can effectively exploit those requested labeled data, especially for those
that are correctly classiﬁed but with low conﬁdence. And this observation again supports
our main motivation that the PAA algorithms are designed to address the key limitation of
PEA who wastes the efforts of querying labels but may never uses them for effective update.
Furthermore, the running time cost of PAA and PEA algorithms are similar, as well as in the
same order of magnitude with randomized query algorithms, which validates the efﬁciency
of the proposed methods.
Finally, we would like to discuss the performance of the SEL-ada and SEL-2nd algorithms,
which were proposed as improved variants of the PEA algorithm. Surprisingly, SEL-ada
performs slightly worse than PEA in all the datasets. This observation consists with earlier
results while no explanation was provided by Cesa-Bianchi et al. . We think that
the inferior performance of SEL-ada may be caused by the ineffective setting the smoothing
parameter δ. When aiming at maximizing the accumulated accuracy along the whole learning
process, querying labels earlier is more likely to result in better performance. However, the
increasing δ with time t is actually encouraging later query.
When analyzing the performance of SEL-2nd, we ﬁnd that this algorithm usually outperforms PEA in terms of F-measure, which can be explained by the utilization of second order
information. However, it still suffers from the same drawback with PEA, i.e. wasting the
queried labels when the prediction is correct but with low conﬁdence. Thus the F-measure
is still signiﬁcantly lower than PAA-I and PAA-II. Besides, the O(d2) time and space complexity of SEL-2nd limits its application in high dimensional applications, which is indicated
by its time cost on the “w8a” dataset with 300 feature dimension.
6.1.3 Evaluation on varied ratio of queries
This experiment is to evaluate the performance of the proposed algorithms by varying the
query rate of different online learning algorithms, shown in Fig. 1. From the experimental
results, several observations can be drawn.
Similar to the previous experiments under ﬁxed query rate, we ﬁnd that the two soft
margin PAA algorithms always achieve higher F-measure than all the other active learning
algorithms, which validates the successful update strategy of our proposed algorithm. And
all the active learning algorithms outperform their corresponding random versions, which
demonstrates the advantage of the active querying rule.
In addition, we observe that the F-measure usually increases as the fraction of queried
labels increases at the beginning, but saturates quickly after the fraction of queried labels
exceeds some value. This result indicates the proposed online active learning strategy can
effectively explore those most informative instances for updating the classiﬁers in a rather
effective and efﬁcient way.
Finally, it is interesting to see that on some datasets (e.g., a8a, magic04, svmguide1,
etc.), the F-measures achieved by PAA,PEA, SEL-ada and SEL-2nd could decrease when
Mach Learn 103:141–183
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fig. 1 Evaluation of F-measure against the fraction of queried labels on all binary datasets. The plotted curves
are averaged over 20 random permutations
Mach Learn 103:141–183
increasing the fraction of queried labels. This seems a little bit surprising as we usually
expect the more the labeled data queried, the better the predictive performance. Note that this
phenomenon only appears in the hard margin algorithms (PAA, PEA, SEL-ada and SEL-2nd),
which are not designed for noisy data. While the other two soft-margin algorithms (PAA-I
and PAA-II), which are robust to noisy, tend to be able to avoid such situations. Consequently,
we suspect this was mainly caused due to the overﬁt issue on the noisy training data.
6.1.4 Application to online text classiﬁcation
In this section, we apply our proposed Passive-Aggressive Active learning algorithms to
online text classiﬁcation. Our experimental testbed consists of: (i) a subset of the Reuters
Corpus Volume 1 (RCV1)3 which contains 4086 documents with 29,992 distinct words; (ii)
20 Newsgroups datasets,4 we extract the “comp” versus “rec” and “rec” versus “sci” to form
two binary classiﬁcation tasks, which have a total of 8870 and 8928 documents, respectively.
Each document is represented by a feature vector of 26,214 distinct words.
As discussed earlier, the SEL-ada algorithm mostly performs worse that PEA because
of the ineffective setting of the δ value. In addition, the SEL-2nd is not applicable to high
dimensional applications due to its O(d2) space and time complexity while its performance
is always worse than PAA-I and PAA-II. We remove the two compared algorithms for the
conciseness of our later experiments.
The text classiﬁcation results are shown in Fig. 2. We could see that Passive-Aggressive
based algorithms usually outperform the Perceptron based algorithms, which empirically
shows the advantages of large margin approaches for active learning. Among all methods,
PAA algorithms consistently perform better than random querying methods and perceptron based active learning methods, which further validates the efﬁcacy of our proposed
approaches.
6.1.5 Application to web data classiﬁcation
To further evaluate the PAA algorithms, we apply them to web data classiﬁcation tasks,
which are (i) URL classiﬁcation which contains 1,782,206 URLs with
3,231,961 features; (ii) webspam classiﬁcation , which have a total of
350,000 instance with 254 features, respectively. These two datasets can be downloaded
from the LIBSVM website.5 Similar phenomenon could be observed from the results, as
shown in Fig. 3.
6.2 Evaluation of the MPAA algorithm in multi-class classiﬁcation tasks
This section will evaluate the empirical performance of the proposed MPAA algorithms on
online multi-class classiﬁcation tasks.
6.2.1 Compared algorithms and experimental testbed
We compare the proposed MPAA algorithms with the Multi-class Perceptron Active learning
algorithm, which adopts the same querying strategy as MPAA algorithms do but updates in
3 
4 
5 
Mach Learn 103:141–183
Fig. 2 Evaluation of F-measure
against the fraction of queried
labels for text classiﬁcation
applications
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
20Newsgroup“comp”vs“rec”
20Newsgroup“rec”vs“sci”
Mach Learn 103:141–183
Fraction of queried labels
Fraction of queried labels
Fig. 3 Evaluation of F-measure against the fraction of queried labels for web applications
Table 4 Details of multi-class
classiﬁcation datasets
#Instances
the Perceptron rule. To demonstrate the advantage of our querying strategy, we also compare
the active learning algorithms with their random variants. Note that for all Perceptron based
algorithms, we choose the max-score variant since it is similar to the PA based algorithms
in only updating two weight vectors during each iteration, which is a fair comparison. The
compared algorithms are listed as follows:
– “MRPE”: the Multi-class Random Perceptron algorithm, an extension of RPE to multi-class setting;
– “MRPA”: the Multi-class Random Passive-Aggressive algorithms, including MRPA,
MRPA-I, MRPA-II, which will uniformly randomly query labels;
– “MPEA”: the Multi-class Perceptron-based Active learning algorithm, an extension of
PEA in multi-class setting;
– “MPAA”: the Multi-class Passive-Aggressive Active learning algorithms, including
MPAA, MPAA-I, MPAA-II.
To examine the performance, we conduct extensive experiments on a variety of benchmark
datasets from web machine learning repositories. Table 4 shows the details of 9 multi-class
datasets used in our experiments. All of these datasets can be downloaded from LIBSVM
website.6 These datasets are chosen fairly randomly in order to cover various sizes of datasets.
6 
Mach Learn 103:141–183
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fraction of queried labels
Fig. 4 Evaluation of accuracy against the fraction of queried labels on all multi-class datasets. The plotted
curves are averaged over 20 random permutations
The parameter settings mostly follow those in the previous binary classiﬁcation experiments excepts that we select online accuracy for performance metrics.
6.2.2 Performance evaluation
Next we evaluate the performance of all the algorithms on online multi-class active learning
tasks. Figure 4 summarizes the average performance of the eight different algorithms for
online active learning on the 9 datasets. Similar phenomenon could be observed from the
results in Fig. 4 as that in binary setting, which further demonstrates that our proposed
algorithms are effective in dealing with online multi-class active learning tasks.
6.2.3 Application to online text classiﬁcation
In this section, we apply our proposed Multi-class Passive-Aggressive Active learning algorithms to online text classiﬁcation. Our experimental testbed consists of: (i) 20 Newsgroups
datasets,7 we use the 20-class dataset, which have a total of 15,935 documents. Each document is represented by a feature vector of 62,061 distinct words. (ii) Reuters Corpus Volume
7 
Mach Learn 103:141–183
Fraction of queried labels
Fraction of queried labels
(a) news20
Fig. 5 Evaluation of accuracy against the fraction of queried labels for multi-class online text classiﬁcation
applications
1 (RCV1)8 which is a 53-class datasets and contains 15,564 documents with 47,236 distinct words; The text classiﬁcation results are shown in Fig. 5. Similar phenomenon could
be observed from the results: Passive-Aggressive based algorithms usually outperform the
Perceptron based algorithms and PAA algorithms consistently perform better than random
querying methods and perceptron based active learning methods, which further validates the
efﬁcacy of our proposed approaches.
6.3 Evaluation of CSPAA algorithms for cost-sensitive classiﬁcation tasks
This section will evaluate the empirical performance of the proposed CSPAA algorithm.
Speciﬁcally, we will evaluate all the algorithms on online malicious URL detection , which is a large-scale online learning task. Our experiments are designed to
answer several open questions: (i) how does the class imbalance issue affect the predictive
performance of online active learning? (ii) if the proposed online active learning approach is
effective to reducing the amount of labeled data signiﬁcantly in order to maintain comparable
performance? (iii) how is the efﬁciency and scalability of the proposed learning algorithms
for a web-scale application?
6.3.1 Experimental testbed
To examine the performance of the proposed CSPAA algorithms, we test them on a largescale benchmark dataset for malicious URL detection tasks , which can be
downloaded from The original data set was created in
purpose to make it somehow class-balanced. In our experiment, we create two subsets by
sampling from the original data set to make it close to a more realistic distribution scenario
where the number of normal URLs is signiﬁcantly larger than the number of malicious
URLs. Table 5 shows the data sets used in our experiment for online malicious detection,
where Tp/Tn denotes the ratio between the number of positive (malicious) instances and the
number of negative (normal) instances. A variety of features were extracted to represent the
content of a URL, including both lexical features 103:141–183
Table 5 The data set of
malicious URL detection
#Training examples
tokens, etc) and host-based features (such as WHOIS info, IP preﬁx, AS number, Geographic,
etc.). More details can be found in .
6.3.2 Compared algorithms and setup
We compare the proposed CSPAA algorithms against a variety of state-of-the-art algorithms
as follows:
– “PE”: the classical PErceptron algorithm , which queries label of every
instance; this is impractical as it requires huge amount of labeled data, which is used as
a yardstick to evaluate the efﬁcacy of our algorithm;
– “PA”: the regular Passive-Aggressive algorithm , which also queries
class label of every instance; similarly, this is another yardstick for comparison;
– “CW-diag”: the Conﬁdence Weighted (CW) algorithm , which also
queries label of every instance, and exploits the second-order info. We adopt the CW-diag
version to make it feasible for high-dimensional data.
– “PAUM”: this is the cost-sensitive Perceptron Algorithm with Uneven Margin , which also queries label of every instance;
– “CPA”: the Cost-sensitive Passive-Aggressive algorithm based on prediction which also queries all labels;
– “LEPE”: the Label Efﬁcient Perceptron algorithm , which
actively queries label for informative instances;
– “CSRND”: a variant of the proposed CSPAA algorithm, but randomly queries label of
incoming instances;
– “CSPAA”: the proposed Cost-Sensitive Passive-Aggressive Active learning algorithm as
shown in Algorithm 4.
To make a fair comparison, all the algorithms adopt the same setup. All the compared
algorithms learn a linear classiﬁer for the malicious URL detection task. In particular, for all
the compared algorithms, we set the penalty parameter C = ρ = Tn/Tp. For the proposed
CSPAAsum algorithm, we set ηp = ηn = 1/2 for all cases, while for the CSPAAcos, we set
cp = Tn/T and cn = Tp/T . The smoothing parameter δ for LEPE and CSPAA is set as
2[−10:2:10] in order to examine varied query ratios.
All the experiments were conducted over 5 random permutations of the dataset. The
results were reported by averaging over these 5 runs. We evaluate the online classiﬁcation
performance by two key metrics: the weighted sum of sensitivity and speciﬁcity, and the
weighted cost. We denote by CSPAAsum the algorithm aiming to improve the weighted sum
of sensitivity and speciﬁcity, and CSPAAcos the algorithm aiming to improve the overall cost.
All experiments were run on a machine of 2.3GHz CPU.
6.3.3 Evaluation on ﬁxed ratio of queries
The ﬁrst experiment is to evaluate the performance by ﬁxing the ratio of queries issued by the
(active learning) algorithms. Table 6 shows the results of the sum performance under a ﬁxed
Mach Learn 103:141–183
Table 6 “Sum” evaluation of cost-sensitive online classiﬁcation for malicious URL detection
We compare the Sum of our proposed algorithms with the best one among all compared algorithms with the
same query rate and report the p value of the t test
a CSPAA versus CSRND p < 0.0001; CSPAA(a) versus CSRND p < 0.0001
b CSPAA versus CSRND p < 0.0001
Mach Learn 103:141–183
Table 7 “Cost” evaluation of cost-sensitive online classiﬁcation for malicious URL detection
11,471.580
(±124.696)
(±125.053)
(±126.476)
We compare the Cost of our proposed algorithms with the best one among all compared algorithms with the
same query rate and report the p value of the t test
a CSPAA versus CSRND p < 0.0001; CSPAA(a) versus CSRND p < 0.0001
b CSPAA versus CSRND p < 0.0001
Mach Learn 103:141–183
ratio of queries to about 2% for URL1 and 10% for URL2 dataset, and Table 7 summarizes
the cost performance under the similar query ratio.
Several observations can be drawn from the results. First of all, according to the classi-
ﬁcation accuracy (a misleading metric for cost-sensitive classiﬁcation), we found that both
PE and PA algorithms signiﬁcantly outperform the other algorithms, while, in terms of both
sum and cost measures, they are considerably worse than their cost-sensitive variants (i.e.,
PAUM and CPA). This indicates the importance of taking the class imbalance issue into
consideration for online malicious detection tasks. Second, when querying the same ratio of
labeled data, in terms of both sum and cost performances, CSPAA signiﬁcantly outperforms
the LEPE algorithm, which validates the effectiveness of the proposed cost-sensitive online
updating strategy. Third, when querying the same ratio of labels, CSPAA signiﬁcantly outperforms CSRND, which implies the proposed querying strategy is able to actively select those
fairly informative instances for querying labels, which are considerably better than just randomly querying. Moreover, among all the approaches, the proposed CSPAA algorithm and
the PAUM algorithm achieve the highest sum performance. However, the proposed CSPAA
only queried a extremely small subset of labels, while the PAUM algorithm requires to query
the labels of all the incoming instances, which is very expensive to label 1-million training
instances in a real-world application. We thus believe the proposed CSPAA algorithm is more
practically attractive and suitable for a web-scale application.
Finally, we notice that the proposed CSPAA algorithm not only achieves the best sensitivity
performance, but also achieves fairly good speciﬁcity performance which is generally quite
comparable to the other algorithms. This implies that the proposed CSPAA algorithm not only
signiﬁcantly improves the prediction accuracy on the rare class, but also does not sacriﬁce
much the prediction accuracy on the other majority classes. This promising observation again
validates the effectiveness of the proposed CSPAA algorithm.
6.3.4 Evaluation on varied query ratios
This experiment is to evaluate the performance of the proposed algorithms by varying the
ratios of queries for comparing different online active learning algorithms. Figure 6 shows
the online average sum performance and the online average cost performance under varied
query ratios, respectively. From the experimental results, several observations can be drawn
as follows.
First of all, among all four fully supervised online learning algorithms (PE, PA, PAUM,
and CPA), the cost-sensitive algorithms (PAUM and CPA) generally outperform the costinsensitive versions. This result validates the importance of studying the proposed costsensitive online learning methodology class imbalanced tasks.
Second, compared with the CSRND algorithm that randomly queries the labels, CSPAA
consistently achieves much higher sum and much lower cost performance over all the ratios
of queried labels, especially when the query ratio is relatively small. This promising result
indicates that the querying strategy of the proposed CSPAA technique is able to effectively
query the informative labeled data from the sequentially arriving of unlabeled data instances.
Third, compared with LEPE, CSPAA achieves higher sum over all the ratios of queried
labels, which implies that the proposed online updating strategy is able to effectively exploits
the labeled data for improving the classiﬁer. In addition, compared with PA, CSPAA with
query ratio equals to 1 (equivalent to querying label of every instance) achieves a signiﬁcantly
higher sum performance, which shows the biased penalty function does effectively optimize
the objective metric of the weighted sum of sensitivity and speciﬁcity.
Mach Learn 103:141–183
Finally, we notice that when the query ratio increases, we generally observe an improvement of the cost-sensitive classiﬁcation performance by the proposed CSPAA algorithm.
However, While the query ratio reaches about 1%, the improvement tends to become saturated, which is very close to the same algorithm that queries the label of every unlabeled
data. This interesting observation indicates that the proposed learning strategy is able to attain
potentially the best possible predictive performance using a small amount of label data (only
1% or even less) over the entire training data set, which can thus save a signiﬁcant amount
of labeling cost in a practical real-world application.
6.3.5 Evaluation on adaptive sampling factor
In the above experiments, the sampling factor δ was simply ﬁxed to a constant. This experiment aims to examine if it is possible to further improve the proposed CSPAA approach
using the adaptive sampling factor, denoted as “CSPAA(a)” for short (as discussed in the
“remark” before Sect. 5.3). In this experiment, the initial value of δ is set to an extremely
large value, i.e., δ0 = 214, and is updated adaptively using the proposed strategy in Sect. 5.3.
To enable a fair comparison, we set appropriate parameters of the other algorithms (LEPE,
CSRND and CSPAA) to make them sample the similar ratio of labeled data. Table 8 shows the
experimental results for URL2 dataset, where “CSPAA” adopts the constant sampling factor.
In addition, we also show the results on URL1 dataset of varied ratio of queries in Fig. 6.
Fraction of queried labels
Fraction of queried labels
Ratio of Queried Labels
Ratio of Queried Labels
online cumulative average sum for URL1
online cumulative average sum for URL2
online cumulative average cost for URL1
online cumulative average cost for URL2
Fig. 6 Evaluation of the performance with respect to varied query ratios
Mach Learn 103:141–183
Table 8 Evaluation of malicious URL detection performance on “URL2”
(±152.639)
(±466.228)
(±299.126)
(±117.269)
We compare the Cost and Sum of our proposed algorithms with the best one among all compared algorithms
with the same query rate and report the p value of the t test
a CSPAA versus CSRND p < 0.0001; CSPAA(a) versus CSRND p < 0.0001
b CSPAA versus CSRND p = 0.0006; CSPAA(a) versus CSRND p < 0.0001
Some observations can be drawn from the results. First, the CSPAA(a) algorithm using
the adaptive sampling factor signiﬁcantly outperforms both CSRND using the random query
strategy and CSPAA using a constant sampling factor under the same query ratio. Second,
we found that by querying only 0.5% out of the entire 1-million instances, the proposed
CSPAA(a) algorithm is able to achieve the best performance, which is almost the same
(statistically no difference according to student t test) to the state-of-the-art cost-sensitive
algorithm CPA which has to query labels for all the 1-million instances. This promising result
shows that the proposed CSPAA technique is able to save a signiﬁcant amount of labeling
cost while maintaining the state-of-the-art performance.
6.3.6 Evaluation on efﬁciency and scalability
Finally, we examine the time efﬁciency of the proposed algorithms. The “time” columns of
Tables 6, 7 and 8 show the average time costs of the proposed CSPAA algorithms on the
Mach Learn 103:141–183
Fig. 7 Evaluation of online
cumulative time cost on the
“URL2” dataset
Number of Instances
Cumulative Time (seconds)
ﬁxed query ratios. In addition to these tables, we also evaluate the scalability of the proposed
algorithms, as shown in Fig. 7, which measures the online cumulative time cost of different
algorithms over the number of received instances in the online malicious URL detection
From the results, we can see that all the proposed online learning algorithms are fairly
efﬁcient and scalable, which typically took about 20–30s to run on the data set with 1million instances on a single regular machine. Moreover, by examining the efﬁciency and
scalability of the proposed CSPAA algorithms, we found that CSPAA is among the most
efﬁcient and scalable algorithms, which is at least as efﬁcient as the other algorithms and
even slightly better than some of the other algorithms. These encouraging results again
validate the practical value of the proposed CSPAA algorithm for web-scale real-world
applications.
7 Conclusions
This paper investigated online active learning techniques for resolving the open challenges of
learning sequentially arriving data in varied settings. The proposed novel online active learning technique not only overcomes the drawback of conventional supervised passive online
learning algorithms that have to query (or wait) class labels of every incoming instances,
but also improves the limitation of the existing perceptron-based active learning algorithm
that often wastes a lot of queried/received labeled instances that are barely classiﬁed correctly but with low prediction conﬁdence. Speciﬁcally, we have proposed a family of passive
aggressive active (PAA) learning algorithms to tackle three different kinds of online predictive tasks, including online binary classiﬁcation, online multi-class classiﬁcation, and
cost-sensitive online classiﬁcation tasks. We theoretically analyzed the mistake bounds for
the proposed PAA algorithms in the three different settings, in which the bounds generally
enjoy the similar bounds as those regular fully supervised Passive-Aggressive online learning algorithms when requesting class labels of every incoming instance. We conducted an
extensive set of empirical studies, in which our encouraging results showed that the proposed PAA algorithms signiﬁcantly outperform the baseline approaches. For future work,
we plan to address more other challenges of online learning tasks, such as concept drifting
issues .
Mach Learn 103:141–183
Appendix 1: Proof Lemma 1
Proof First of all, we need to prove the following inequality holds for every t
(Lt Zt2τt(α −|pt|) + Mt Zt2τt(α + |pt|)
≤(∥wt −αw∥2 −∥wt+1 −αw∥2) + τ 2
t ∥xt∥2 + 2ατtℓt(w).
To prove that, we enumerate all the possible cases for discussions as follows:
Case 1: “Zt = 0” It is clear that the inequality holds with equality since wt = wt+1 and
Case 2: “Zt = 1 and Mt = 0” The label is requested, but no mistake occurs. Sub-case
2.1: “Lt = 0” Since ℓt(wt) = 0, τt = 0 and wt+1 = wt. Thus, the inequality holds.
Sub-case 2.2: “Lt = 1” Since ℓt(wt) > 0, we have
∥wt −αw∥2 −∥wt+1 −αw∥2 = −2τt ytwt · xt + 2τtαytw · xt −τ 2
Since ℓt(w) = max(0, 1 −ytw · xt) ≥1 −ytw · xt, we have
∥wt −αw∥2 −∥wt+1 −αw∥2 + τ 2
t ∥xt∥2 + 2ατtℓt(w) ≥2τt(α −ytwt · xt).
Also Mt = 0 and ℓt(wt) > 0 implies 0 ≤ytwt · xt < 1. Thus, we have the inequality
||wt −αw∥2 −∥wt+1 −αw∥2 + τ 2
t ∥xt∥2 + 2ατtℓt(w) ≥2τt(α −|pt|).
Case 3: “Zt = 1 and Mt = 1” It means the label is requested and a mistake occurs, but
Lt = 0. Similarly, we have
∥wt −αw∥2 −∥wt+1 −αw∥2 + τ 2
t ∥xt∥2 + 2ατtℓt(w) ≥2τt(α −ytwt · xt).
Since Mt = 1 implies ytwt · xt ≤0 and −ytwt · xt = |pt|, we have
∥wt −αw∥2 −∥wt+1 −αw∥2 + τ 2
t ∥xt∥2 + 2ατtℓt(w) ≥2τt(α + |pt|).
Combining the above cases for all t = 1, . . . , T , we have
(Lt Zt2τt(α −|pt|) + Mt Zt2τt(α + |pt|)
(∥wt −αw∥2 −∥wt+1 −αw∥2) + τ 2
t ∥xt∥2 + 2ατtℓt(w)
2ατtℓt(w) .
Appendix 2: Lemma 2
Similarly to the binary section, before presenting the mistake bounds for multi-class classiﬁcation, we begin by presenting a technical lemma which would facilitate the proof of
Theorem 5.
Mach Learn 103:141–183
Lemma 2 Let (x1, y1), . . . , (xT , yT ) be a sequence of input instances, where xt ∈Rd and
yt ∈Y = {1, . . . , k} for all t. Let τt be the stepsize parameter for either of the three MPAA
variants as given in Eq. (4). For any α > 0, the following bound holds for any classiﬁer w
made up of k vectors wr ∈Rd,r ∈Y.
Lt(α −pt) + Mt(α + pt)
2ατtℓt(w),
where Mt = I(t∈M), Lt = I(t∈L), I is an indicator function.
Proof First of all, we need to prove the following inequality holds for every t
(Lt Zt2τt(α −pt) + Mt Zt2τt(α + pt)
t −αwr∥2 −∥wr
t+1 −αwr∥2) + 2τ 2
t ∥xt∥2 + 2ατtℓt(w).
To prove that, we should enumerate all the possible cases for discussions. For conciseness,
we only prove the two cases: Mt = 1 and Lt = 1 and omit the others since they are similar
to that in Lemma 1.
First, when Lt = 1, Since ℓt(wt) > 0, we have
t −αwr∥2 −∥wr
t+1 −αwr∥2)
t −αwyt ∥2 −∥wyt
t −αwyt + τtxt∥2
t −αwst ∥2 −∥wst
t −αwst −τtxt∥2
t ||xt||2 + 2τt(xt · wst
t −xt · wyt
t ) + 2ατt(xt · wyt −xt · wst ).
Note that st is the highest ranked irrelevant label with regard to the classiﬁer wt, not w.
In a correct prediction, xt · wst
t −xt · wyt
t = −pt and xt · wyt −xt · wst ≥γt,w, where γt,w
is used to denote the margin of instance xt with regard to the classiﬁer w (since we used γt
to denote the margin of xt with regards to wt).
According to the deﬁnition of hinge loss, we have ℓt(w) ≥1 −γt,w and thus γt,w ≥
1 −ℓt(w). Combining the above facts with (13) yields to
t −αwr∥2 −∥wr
t+1 −αwr∥2) ≥−2τ 2
t ||xt||2 −2τt pt + 2ατt(1 −ℓt(w))
t ||xt||2 + 2τt(α −pt) −2ατtℓ(w).
We write the above formula as:
2τt(α −pt) ≤
t −αwr∥2 −∥wr
t+1 −αwr∥2) + 2τ 2
t ||xt||2 + 2ατtℓt(w).
Second, when Mt = 1, i.e. incorrect prediction, st = ˆyt and thus
t −xt · wyt
Mach Learn 103:141–183
Combining this fact and (13), we get
2τt(α + pt) ≤
t −αwr∥2 −∥wr
t+1 −αwr∥2) + 2τ 2
t ||xt||2 + 2ατtℓt(w).
Considering all cases, we can ﬁnally prove (12) is correct. This proof is ﬁnished by summing
(12) over all iterations t = 1, . . . , T .
Appendix 3: Proof of Theorem 5
Proof Since ℓt(w) = 0, ∀t ∈[T ], according to Lemma 2, we have
Lt(α −pt) + Mt(α + pt)
Lt(α −pt −τt∥xt∥2) + Mt(α + pt −τt∥xt∥2)
Lt(α −pt −ℓt(wt)
) + Mt(α + pt −ℓt(wt)
Plugging α = δ+1
2 , δ ≥1 into the above inequality results in
Mt Ztτt(δ + pt),
In addition, combining the fact τt = ℓt(wt)/2∥xt∥2 ≥ℓt(wt)/2R2 with the above inequality
concludes:
Mt Ztℓt(wt)(δ + pt).
Taking expectation with the above inequality results in
Mtℓt(wt)Zt(δ + pt)
Mach Learn 103:141–183
Appendix 4: Proof of Proposition 1
Proof First of all, by analyzing the weighted sum in (6), we can derive:
I(ytw·xt<0) +
I(ytw·xt<0)
Thus, maximizing sum is equivalent to minimizing
I(ytw·xt<0) +
I(ytw·xt<0).
Second, by analyzing the weighted cost in (7), we can also derive:
cost = cpMp + cn Mn = cn
I(ytw·xt<0) +
I(ytw·xt<0)
Thus, minimizing cost is equivalent to minimizing
I(ytw·xt<0) +
I(ytw·xt<0).
Thus, the proposition holds by setting ρ = ηpTn
ηnTp for sum, and ρ = cp
cn for cost.
Appendix 5: Proof of Theorem 6
Proof As proven in Theorem 2,
α −1 + |pt|
α −1 −|pt|
Plugging α = 1+δ
2 , δ ≥1 into the above inequality results in
(1 + δ)τtℓt(w) ≥
Mt Ztτt(δ + |pt|),
since when Lt = 1, |pt| ∈[0, 1), (α −1+|pt|
) = δ−|pt|
> 0, and (α −1−|pt|
) = δ+|pt|
Furthermore, because when Mt = 1, τt = min{C, ℓt(wt)
∥xt∥2 } ≥min{C, ρt} = ρt, and τt ≤C,
the above inequality implies:
(1 + δ)Cℓt(w) ≥
ρt Mt Zt(δ + |pt|),
Mach Learn 103:141–183
Taking expectation with the above equality and re-arranging the result conclude the theorem,
ρt Mt Zt(δ + |pt|) = E
ρt Mt(δ + |pt|)Et Zt = δE
Appendix 6: Proof of Theorem 7
Proof Following the condition that ρ = ηpTn
ηnTp ≥1 and the result of Theorem 6, we have
(1 + δ)Cℓt(w)
ρEMp + EMn
ηp(1 −Esen) + ηn(1 −Espe)
(1 −E[sum]).
Rearranging the above inequality leads to the conclusion:
E[sum] ≥1 −ηn
(1 + δ)Cℓt(w)
Appendix 7: Proof of Theorem 8
Proof Following the result of Theorem 6, we have
(1 + δ)Cℓt(w)
≥(EMp(ρ) + EMn)
Rearranging the above inequality concludes the theorem.