Wasserstein Distance Guided
Representation Learning for Domain Adaptation
Jian Shen, Yanru Qu, Weinan Zhang, Yong Yu
Shanghai Jiao Tong University
{rockyshen, kevinqu, wnzhang, yyu}@apex.sjtu.edu.cn
Domain adaptation aims at generalizing a high-performance
learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation
is to learn domain invariant feature representations while
the learned representations should also be discriminative in
prediction. To learn such representations, domain adaptation
frameworks usually include a domain invariant representation
learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classiﬁcation. Inspired by
Wasserstein GAN, in this paper we propose a novel approach
to learn domain invariant feature representations, namely
Wasserstein Distance Guided Representation Learning (WD-
GRL). WDGRL utilizes a neural network, denoted by the
domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature
extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of
Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical
studies on common sentiment and image classiﬁcation adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation
learning approaches.
Introduction
Domain adaptation deﬁnes the problem when the target domain labeled data is insufﬁcient, while the source domain
has much more labeled data. Even though the source and
target domains have different marginal distributions , domain adaptation
aims at utilizing the knowledge distilled from the source
domain to help target domain learning. In practice, unsupervised domain adaptation is concerned and studied more
commonly since manual annotation is often expensive or
time-consuming. Faced with the covariate shift and the lack
of annotations, conventional machine learning methods may
fail to learn a high-performance model.
To effectively transfer a classiﬁer across different domains, different methods have been proposed, including
instance reweighting . All rights reserved.
2009), subsampling ,
feature mapping and weight regularization . Among these
methods feature mapping has shown great success recently,
which projects the data from different domains to a common latent space where the feature representations are domain invariant. Recently, deep neural networks, as a great
tool to automatically learn effective data representations,
have been leveraged in learning knowledge-transferable feature representations for domain adaptation .
On the other hand, generative adversarial nets (GANs)
 are heavily studied during recent
years, which play a minimax game between two adversarial networks: the discriminator is trained to distinguish real
data from the generated data, while the generator learns to
generate high-quality data to fool the discriminator. It is intuitive to employ this minimax game for domain adaptation
to make the source and target feature representations indistinguishable. These adversarial adaptation methods have
become a popular solution to reduce domain discrepancy
through an adversarial objective with respect to a domain
classiﬁer . However,
when the domain classiﬁer network can perfectly distinguish
target representations from source ones, there will be a gradient vanishing problem. A more reasonable solution would
be to replace the domain discrepancy measure with Wasserstein distance, which provides more stable gradients even if
two distributions are distant (Arjovsky, Chintala, and Bottou
In this paper, we propose a domain invariant representation learning approach to reduce domain discrepancy for domain adaptation, namely Wasserstein Distance Guided Representation Learning (WDGRL), inspired by recently proposed Wasserstein GAN . WDGRL trains a domain critic network to estimate
the empirical Wasserstein distance between the source and
target feature representations. The feature extractor network
will then be optimized to minimize the estimated Wasserstein distance in an adversarial manner. By iterative adversarial training, we ﬁnally learn feature representations invariant to the covariate shift between domains. Additionally,
WDGRL can be easily adopted in existing domain adap-
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
tation frameworks 
by replacing the representation learning approaches. Empirical studies on common domain adaptation benchmarks
demonstrate that WDGRL outperforms the state-of-the-art
representation learning approaches for domain adaptation.
Furthermore, the visualization of learned representations
clearly shows that WDGRL successfully uniﬁes two domain
distributions, as well as maintains obvious label discrimination.
Related Works
Domain adaptation is a popular subject in transfer learning . It concerns covariate shift between two data distributions, usually labeled source data
and unlabeled target data. Solutions to domain adaptation problems can be mainly categorized into three types:
i). Instance-based methods, which reweight/subsample the
source samples to match the distribution of the target domain, thus training on the reweighted source samples guarantees classiﬁers with transferability . ii). Parameter-based methods, which transfer knowledge through shared or regularized parameters of
source and target domain learners, or by combining multiple reweighted source learners to form an improved target learner . iii). The last but the most popular and effective methods are feature-based, which can be
further categorized into two groups . Asymmetric feature-based methods transform the features of one domain to more closely match
another domain while symmetric feature-based methods
map different domains to a common latent space where the
feature distributions are close.
Recently, deep learning has been regarded as a powerful
way to learn feature representations for domain adaptation.
Symmetric feature-based methods are more widely studied
since it can be easily incorporated into deep neural networks
 .
Among symmetric feature-based methods, minimizing the
maximum mean discrepancy (MMD) 
metric is effective to minimize the divergence of two distributions. MMD is a nonparametric metric that measures
the distribution divergence between the mean embeddings
of two distributions in reproducing kernel Hilbert space
(RKHS). The deep domain confusion (DDC) method utilized MMD metric in the last fully connected
layer in addition to the regular classiﬁcation loss to learn representations that are both domain invariant and discriminative. Deep adaptation network (DAN) was
proposed to enhance the feature transferability by minimizing multi-kernel MMD in several task-speciﬁc layers. On the
other hand, correlation alignment (CORAL) method was proposed to align the secondorder statistics of the source and target distributions with a
linear transformation and extended
CORAL and proposed Deep CORAL to learn a nonlinear
transformation that aligns correlations of layer activations in
deep neural networks.
Another class of symmetric feature-based methods uses
an adversarial objective to reduce domain discrepancy. Motivated by theory in suggesting that a good cross-domain representation contains no discriminative information about the origin (i.e. domain) of the
input, domain adversarial neural network (DANN) was proposed to learn domain invariant features by a minimax game between the domain classiﬁer and the feature extractor. In order to backpropagate the gradients computed from the domain classi-
ﬁer, DANN employs a gradient reversal layer (GRL). On the
other hand, proposed a general framework for adversarial adaptation by choosing adversarial loss
type with respect to the domain classiﬁer and the weight
sharing strategy. Our proposed WDGRL can also be viewed
as an adversarial adaptation method since it evaluates and
minimizes the empirical Wasserstein distance in an adversarial manner. Our WDGRL differs from previous adversarial
methods: i). WDGRL adopts an iterative adversarial training strategy, ii). WDGRL adopts Wasserstein distance as the
adversarial loss which has gradient superiority.
Another related work for domain adaptation is optimal
transport , which is equivalent to Wasserstein distance. And
 gave a theoretical analysis that Wasserstein distance can guarantee generalization
for domain adaptation. Though these works utilized Wasserstein distance in domain adaptation, there are distinct differences between WDGRL and the previous ones: these works
are asymmetric feature-based methods which design a transformation from source representations to target ones based
on optimal transport while WDGRL is a symmetric method
that projects both domains to a common latent space to learn
domain invariant features. And WDGRL can be integrated
into other symmetric feature-based adaptation frameworks.
Besides learning shared representations, domain separation network (DSN) was proposed
to explicitly separate private representations for each domain
and shared ones between the source and target domains. The
private representations were learned by deﬁning a difference
loss via a soft orthogonality constraint between the shared
and private representations while the shared representations
were learned by DANN or MMD mentioned above. With the
help of reconstruction through private and shared representations together, the classiﬁer trained on the shared representations can better generalize across domains. Since our work
focuses on learning the shared representations, it can also be
integrated into DSN easily.
Wasserstein Metric
Before we introduce our domain invariant feature representation learning approach, we ﬁrst give a brief introduction of
the Wasserstein metric. The Wasserstein metric is a distance
measure between probability distributions on a given metric
space (M, ρ), where ρ(x, y) is a distance function for two
instances x and y in the set M. The p-th Wasserstein distance between two Borel probability measures P and Q is
Wp(P, Q) =
ρ(x, y)pdμ(x, y)
where P, Q ∈{P :
ρ(x, y)pdP(x) < ∞, ∀y ∈M} are
two probability measures on M with ﬁnite p-th moment and
Γ(P, Q) is the set of all measures on M ×M with marginals
P and Q. Wasserstein metric arises in the problem of optimal
transport: μ(x, y) can be viewed as a randomized policy for
transporting a unit quantity of some material from a random
location x to another location y while satisfying the marginal
constraint x ∼P and y ∼Q. If the cost of transporting a unit
of material from x ∈P to y ∈Q is given by ρ(x, y)p, then
Wp(P, Q) is the minimum expected transport cost.
The Kantorovich-Rubinstein theorem shows that when M
is separable, the dual representation of the ﬁrst Wasserstein
distance (Earth-Mover distance) can be written as a form of
integral probability metric 
W1(P, Q) =
Ex∼P[f(x)] −Ex∼Q[f(x)],
where the Lipschitz semi-norm is deﬁned as ∥f∥L
sup |f(x) −f(y)|/ρ(x, y). In this paper, for simplicity,
Wasserstein distance represents the ﬁrst Wasserstein distance.
Wasserstein Distance Guided
Reprensentation Learning
Problem Deﬁnition
In unsupervised domain adaptation problem, we have a labeled source dataset Xs = {(xs
i=1 of ns samples
from the source domain Ds which is assumed sufﬁcient to
train an accurate classiﬁer, and an unlabeled target dataset
j=1 of nt samples from the target domain Dt.
It is assumed that the two domains share the same feature
space but follow different marginal data distributions, Pxs
and Pxt respectively. The goal is to learn a transferable classiﬁer η(x) to minimize target risk ϵt = Pr(x,y)∼Dt[η(x) ̸=
y] using all the given data.
Domain Invariant Representation Learning
The challenge of unsupervised domain adaptation mainly
lies in the fact that two domains have different data distributions. Thus the model trained with source domain data may
be highly biased in the target domain. To solve this problem,
we propose a new approach to learn feature representations
invariant to the change of domains by minimizing empirical
Wasserstein distance between the source and target representations through adversarial training.
In our adversarial representation learning approach, there
is a feature extractor which can be implemented by a neural
network. The feature extractor is supposed to learn the domain invariant feature representations from both domains.
Given an instance x ∈Rm from either domain, the feature
extractor learns a function fg : Rm →Rd that maps the
instance to a d-dimensional representation with corresponding network parameter θg. And then in order to reduce the
discrepancy between the source and target domains, we use
the domain critic, as suggested in , whose goal is to estimate the Wasserstein distance between the source and target representation distributions. Given a feature representation h = fg(x) computed
by the feature extractor, the domain critic learns a function
fw : Rd →R that maps the feature representation to a real
number with parameter θw. Then the Wasserstein distance
between two representation distributions Phs and Pht, where
hs = fg(xs) and ht = fg(xt), can be computed according
to Eq. (2)
W1(Phs, Pht) =
EPhs [fw(h)] −EPht [fw(h)]
EPxs [fw(fg(x))] −EPxt [fw(fg(x))].
If the parameterized family of domain critic functions {fw}
are all 1-Lipschitz, then we can approximate the empirical
Wasserstein distance by maximizing the domain critic loss
Lwd with respect to parameter θw
Lwd(xs, xt)= 1
fw(fg(xs))−1
fw(fg(xt)).
Here comes the question of enforcing the Lipschitz constraint. proposed to
clip the weights of domain critic within a compact space
[−c, c] after each gradient update. However pointed out that weight clipping will cause capacity
underuse and gradient vanishing or exploding problems. As
suggested in , a more reasonable way
is to enforce gradient penalty Lgrad for the domain critic
parameter θw
Lgrad(ˆh) = (∥∇ˆhfw(ˆh)∥2 −1)2,
where the feature representations ˆh at which to penalize the
gradients are deﬁned not only at the source and target representations but also at the random points along the straight
line between source and target representation pairs. So we
can ﬁnally estimate the empirical Wasserstein distance by
solving the problem
θw {Lwd −γLgrad}
where γ is the balancing coefﬁcient.
Since the Wasserstein distance is continuous and differentiable almost everywhere, we can ﬁrst train the domain
critic to optimality. Then by ﬁxing the optimal parameter of
domain critic and minimizing the estimator of Wasserstein
distance, the feature extractor network can learn feature representations with domain discrepancy reduced. Up to now
the representation learning can be achieved by solving the
minimax problem
θw {Lwd −γLgrad}
where γ should be set 0 when optimizing the minimum operation since the gradient penalty should not guide the representation learning process. By iteratively learning feature
Figure 1: WDGRL Combining with Discriminator.
representations with lower Wasserstein distance, the adversarial objective can ﬁnally learn domain invariant feature
representations.
Combining with Discriminator
As mentioned above, our ﬁnal goal is to learn a highperformance classiﬁer for the target domain. However, the
process of WDGRL is in an unsupervised setting, which
may result in that the learned domain invariant representations are not discriminative enough. Hence it is necessary
to incorporate the supervision signals of source domain data
into the representation learning process as in DANN . Next we further introduce the combination of
the representation learning approaches and a discriminator,
of which the overview framework is given by Figure 1. A detailed algorithm of the combination is given in Algorithm 1.
We further add several layers as the discriminator after the
feature extractor network. Since WDGRL guarantees transferability of the learned representations, the shared discriminator can be directly applied to target domain prediction
when training ﬁnished. The objective of the discriminator
fc : Rd →Rl is to compute the softmax prediction with
parameter θc where l is the number of classes. The discriminator loss function is deﬁned as the cross-entropy between
the predicted probabilistic distribution and the one-hot encoding of the class labels given the labeled source data:
Lc(xs, ys) = −1
i = k) · log fc(fg(xs
where 1(ys
i = k) is the indicator function and fc(fg(xs
corresponds to the k-th dimension value of the distribution
i)). By combining the discriminator loss, we attain
our ﬁnal objective function
Lc + λ max
Lwd −γLgrad
where λ is the coefﬁcient that controls the balance between
discriminative and transferable feature learning and γ should
be set 0 when optimizing the minimum operator.
Note that this algorithm can be trained by the standard
back-propagation with two iterative steps. In a mini-batch
Algorithm 1 Wasserstein Distance Guided Representation
Learning Combining with Discriminator
Require: source data Xs; target data Xt; minibatch size m; critic
training step n; coefﬁcient γ, λ; learning rate for domain critic
α1; learning rate for classiﬁcation and feature learning α2
1: Initialize feature extractor, domain critic, discriminator with
random weights θg, θw, θc
Sample minibatch {xs
i=1 from Xs and Xt
for t = 1, ..., n do
hs ←fg(xs), ht ←fg(xt)
Sample h as the random points along straight lines between hs and ht pairs
ˆh ←{hs, ht, h}
θw ←θw + α1∇θw[Lwd(xs, xt) −γLgrad(ˆh)]
θc ←θc −α2∇θcLc(xs, ys)
θg ←θg −α2∇θg[Lc(xs, ys) + Lwd(xs, xt)]
12: until θg, θw, θc converge
containing labeled source data and unlabeled target data, we
ﬁrst train the domain critic network to optimality by optimizing the max operator via gradient ascent and then update the
feature extractor by minimizing the classiﬁcation loss computed by labeled source data and the estimated Wasserstein
distance simultaneously. The learned representations can be
domain invariant and target discriminative since the parameter θg receives the gradients from both the domain critic and
the discriminator loss.
Theoretical Analysis
In this section, we give some theoretical analysis about the
advantages of using Wasserstein distance for domain adaptation.
Gradient Superiority
In domain adaptation, to minimize
the divergence between the data distributions Pxs and Pxt,
the symmetric feature-based methods learn a transformation function to map the data from the original space to a
common latent space with a distance measure. There are
two situations after the mapping: i). The two mapped feature distributions have supports that lie on low dimensional
manifolds in the latent space.
In such situation, there will be a gradient vanishing problem if adopting the domain classiﬁer to make data indistinguishable while Wasserstein distance could provide reliable gradients . ii).
The feature representations may ﬁll in the whole space since
the feature mapping usually reduces dimensionality. However, if a data point lies in the regions where the probability of one distribution could be ignored compared with
the other distribution, it makes no contributions to the gradients with traditional cross-entropy loss since the gradient computed by this data point is almost 0. If we adopt
Wasserstein distance as the distance measure, stable gradients can be provided wherever. The detailed analysis is
provided in the supplementary material1. So theoretically
1 
in either situation, WDGRL can perform better than previous adversarial adaptation methods .
Genralization Bound
 proved that the target error can be bounded by the
Wasserstein distance for empirical measures. However, the
generalization bound exists when assuming the hypothesis
class is a unit ball in RKHS and the transport cost function
is RKHS distance. In this paper we prove the generalization
bound in terms of the Kantorovich-Rubinstein dual formulation under a different assumption.
We ﬁrst formalize some notations that will be used in the
following statements. Let X be an instance set and {0, 1}
be the label set for binary classiﬁcation. We denote by μs
the distribution of source instances on X and use μt for the
target domain. We denote that two domains have the same
labeling function f : X → which is always assumed
to hold in domain adaptation problem. A hypothesis class H
is a set of predictor functions, ∀h ∈H, h : X → . The
probability according to the distribution μs that a hypothesis
h disagrees with the labeling function f (which can also be
a hypothesis) is deﬁned as ϵs(h, f) = Ex∈μs[|h(x)−f(x)|].
We use the shorthand ϵs(h) = ϵs(h, f) and ϵt(h) is de-
ﬁned the same. We now present the Lemma that introduces
Wasserstein distance to relate the source and target errors.
Lemma 1. Let μs, μt ∈P(X) be two probability measures.
Assume the hypotheses h ∈H are all K-Lipschitz continuous for some K. Then the following holds
ϵt(h, h′) ≤ϵs(h, h′) + 2KW1(μs, μt)
for every hypothesis h, h′ ∈H.
Proof. We ﬁrst prove that for every K-Lipschitz continuous
hypotheses h, h′ ∈H, |h −h′| is 2K-Lipschitz continuous.
Using the triangle inequality, we have
|h(x)−h′(x)|≤|h(x)−h(y)|+|h(y)−h′(x)|
≤|h(x)−h(y)|+|h(y)−h′(y)|+|h′(x)−h′(y)|
and thus for every x, y ∈X,
|h(x)−h′(x)|−|h(y)−h′(y)|
≤|h(x)−h(y)|+|h′(x)−h′(y)|
Then for every hypothesis h, h′, we have
ϵt(h, h′)−ϵs(h, h′)=Eμt[|h(x)−h′(x)|]−Eμs[|h(x)−h′(x)|]
Eμt[f(x)]−Eμs[f(x)]
=2KW1(μs, μt)
Theorem 1. Under the assumption of Lemma 1, for every
h ∈H the following holds
ϵt(h) ≤ϵs(h) + 2KW1(μs, μt) + λ
where λ is the combined error of the ideal hypothesis h∗that
minimizes the combined error ϵs(h) + ϵt(h).
ϵt(h) ≤ϵt(h∗) + ϵt(h∗, h)
= ϵt(h∗) + ϵs(h, h∗) + ϵt(h∗, h) −ϵs(h, h∗)
≤ϵt(h∗) + ϵs(h, h∗) + 2KW1(μs, μt)
≤ϵt(h∗) + ϵs(h) + ϵs(h∗) + 2KW1(μs, μt)
= ϵs(h) + 2KW1(μs, μt) + λ
Thus the generalization bound of applying Wasserstein
distance between domain distributions has been proved,
while the proof of using empirical measures on the source
and target domain samples can be further proved according
to Theorem 2.1 in as the
same way in and this
proof is provided in the supplementary material.
The assumption made here is to specify the hypothesis
class is K-Lipschitz continuous for some K. While it may
seem too restrictive, in fact the hypotheses are always implemented by neural networks where the basic linear mapping
functions and the activation functions such as sigmoid and
relu are all Lipschitz continuous, so the assumption is not
that strong and can be fulﬁlled. And the weights in neural
networks are always regularized to avoid overﬁtting which
means the constant K will not be too large. Compared with
the proof in the assumptions are different and can be used for different cases.
Application to Adaptation Frameworks
WDGRL can be integrated into existing feature-based domain adaptation frameworks . These frameworks are all symmetric feature-based
and aim to learn domain invariant feature representations
for adaptation using divergence measures such as MMD
and DANN. We provide a promising alternative WDGRL
to learn domain invariant representations, which can replace
the MMD or DANN. We should point out that although WD-
GRL has gradient advantage over DANN, it takes more time
to estimate the Wasserstein distance. Although we only apply WDGRL on one hidden layer, it can also be applied on
multilayer structures as implemented in .
Experiments
In this section, we evaluate the efﬁcacy of our approach
on sentiment and image classiﬁcation adaptation datasets.
Compared with other domain invariant representation learning approaches, WDGRL achieves better performance on
average. More experimental results including synthetic experiment are provided in the supplementary material.
Amazon review benchmark dataset. The Amazon review
dataset2 is one of the most widely used
benchmarks for domain adaptation and sentiment analysis.
It is collected from product reviews from Amazon.com and
2 
contains four types (domains), namely books (B), DVDs
(D), electronics (E) and kitchen appliances (K). For each
domain, there are 2,000 labeled reviews and approximately
4,000 unlabeled reviews (varying slightly across domains)
and the classes are balanced. In our experiments, for easy
computation, we follow to use the 5,000
most frequent terms of unigrams and bigrams as the input
and totally A2
4 = 12 adaptation tasks are constructed.
Ofﬁce-Caltech object recognition dataset. The Ofﬁce-
Caltech dataset3 released by is comprised of 10 common categories shared by the Ofﬁce-31 and
Caltech-256 datasets. In our experiments, we construct 12
tasks across 4 domains: Amazon (A), Webcam (W), DSLR
(D) and Caltech (C), with 958, 295, 157 and 1,123 image samples respectively. In our experiments, Decaf features
are used as the input. Decaf features 
are the 4096-dimensional FC7-layer hidden activations extracted by the deep convolutional neural network AlexNet.
Compared Approaches
We mainly compare our proposed approach with domain
adversarial neural network (DANN) ,
maximum mean discrepancy metric (MMD) and deep correlation alignment (CORAL) since these approaches and our proposed WD-
GRL all aim at learning the domain invariant feature representations, which are crucial to reduce the domain discrepancy. Other domain adaptation frameworks are not included in the comparison, because these frameworks focus on adaptation architecture design and all compared approaches can be easily integrated
into these frameworks.
S-only: As an empirical lower bound, we train a model
using the labeled source data only, and test it on the target
test data directly.
MMD: The MMD metric is a measurement of the divergence between two probability distributions from their
samples by computing the distance of mean embeddings in
DANN: DANN is an adversarial representation learning
approach that a domain classiﬁer aims at distinguishing the
learned source/target features while the feature extractor
tries to confuse the domain classiﬁer.
CORAL: Deep correlation alignment minimizes domain
discrepancy by aligning the second-order statistics of the
source and target distributions and can be applied to the layer
activations in neural networks.
Implementation Details
We implement all our experiments4 using TensorFlow and
the models are all trained with Adam optimizer. We follow
the evaluation protocol in and evaluate all
compared approaches through grid search on the hyperparameter space, and report the best results of each approach.
For each approach we use a batch size of 64 samples in total
3 
4Experiment code: 
with 32 samples from each domain, and a ﬁxed learning rate
10−4. All compared approaches are combined with a discriminator to learn both domain invariant and discriminative
representations and to conduct the classiﬁcation task.
We use standard multi-layer perceptron (MLP) as the basic network architecture. MLP is sufﬁcient to handle all the
problems in our experiments. For Amazon review dataset
the network is designed with one hidden layer of 500 nodes,
relu activation function and softmax output function, while
the network for Ofﬁce-Caltech dataset has two hidden layers
of 500 and 100 nodes. For each dataset the same network
architecture is used for all compared approaches and these
approaches are all applied on the last hidden layer.
For the MMD experiments we follow the suggestions of
 and use a linear combination of 19
RBF kernels with the standard deviation parameters ranging
from 10−6 to 106. As for DANN implementation, we add a
gradient reversal layer (GRL) and then a domain classiﬁer
with one hidden layer of 100 nodes. And the CORAL approach computes a distance between the second-order statistics (covariances) of the source and target features and the
distance is deﬁned as the squared Frobenius norm. For each
approach, the corresponding loss term is added to the classiﬁcation loss with a coefﬁcient for the trade-off. And the
coefﬁcients are tuned different to achieve the best results for
each approach.
Our approach is easy to implement according to Algorithm 1. In our experiments, the domain critic network is designed with a hidden layer of 100 nodes. The training steps
n is 5 which is chosen for fast computation and sufﬁcient optimization guarantee for the domain critic, and the learning
rate for the domain critic is 10−4. We penalize the gradients
not only at source/target representations but also at the random points along the straight line between the source and
target pairs and the coefﬁcient γ is set to 10 as suggested in
 .
Results and Discussion
Amazon review benchmark dataset. The challenge of
cross domain sentiment analysis lies in the distribution shift
as different words are used in different domains. Table 1
Table 1: Performance (accuracy %) on Amazon review
shows the detailed comparison results of these approaches
in 12 transfer tasks. As we can see, our proposed WDGRL
outperforms all other compared approaches in 10 out of 12
domain adaptation tasks, and it achieves the second highest scores in the remaining 2 tasks. We ﬁnd that as adversarial adaptation approaches, WDGRL outperforms DANN,
which is consistent with our theoretical analysis that WD-
GRL has more reliable gradients. MMD and CORAL are
both non-parametric and have lower computational cost than
WDGRL, while their classiﬁcation performances are also
lower than WDGRL.
Ofﬁce-Caltech object recognition dataset. Table 2
shows the results of our experiments on Ofﬁce-Caltech
dataset. We observe that our approach achieves better performance than other compared approaches on most tasks.
Ofﬁce-Caltech dataset is small since there are only hundreds
of images in one domain and it is a 10-class classiﬁcation
problem. Thus we can draw a conclusion that the empirical Wasserstein distance can also be applied to small-scale
datasets adaptation effectively. We note that CORAL performs better than MMD in Amazon review dataset while it
performs worse than MMD in Ofﬁce-Caltech dataset. A possible reason is that the reasonable covariance alignment approach requires large samples. On the other hand, we can see
that these different approaches have different performances
on different adaptation tasks.
Feature Visualization
We randomly choose the D→E domain adaptation task of
Amazon review dataset and plot in Figure 2 the t-SNE visualization following 
to visualize the learned feature representations. In these ﬁgures, red and blue points represent positive and negative
samples of the source domain, purple and green points represent positive and negative samples of the target domain. A
transferable feature mapping should cluster red (blue) and
purple (green) points together, and meanwhile classiﬁcation
can be easily conducted between purple and green points.
We can see that almost all approaches learn discriminative
and domain invariant feature representations to some extent.
And representations learned by WDGRL are more transfer-
Table 2: Performance (accuracy %) on Ofﬁce-Caltech
dataset with Decaf features.
(a) t-SNE of DANN features
(b) t-SNE of MMD features
(c) t-SNE of CORAL features
(d) t-SNE of WDGRL features
Figure 2: Feature visualization of the D→E task in Amazon
review dataset.
able since the classes between the source and target domains
align better and the region where purple and green points
mix together is smaller.
Conclusions
In this paper, we propose a new adversarial approach WD-
GRL to learn domain invariant feature representations for
domain adaptation. WDGRL can effectively reduce the domain discrepancy taking advantage of the gradient property
of Wasserstein distance and the transferability is guaranteed by the generalization bound. Our proposed approach
could be further integrated into other domain adaptation
frameworks to attain better transferability. Empirical results on sentiment and image
classiﬁcation domain adaptation datasets demonstrate that
WDGRL outperforms the state-of-the-art domain invariant
feature learning approaches. In future work, we will investigate more sophisticated architectures for tasks on image
data as well as integrate WDGRL into existing adaptation
frameworks.
Acknowledgement
This work is ﬁnancially supported by NSFC (61702327) and
Shanghai Sailing Program (17YF1428200).