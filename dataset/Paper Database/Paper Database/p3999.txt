Machine Learning, 24, 173–202 
c⃝1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
Error Reduction through Learning Multiple
Descriptions
KAMAL M. ALI
 
MICHAEL J. PAZZANI
 
Department of Information and Computer Science, University of California, Irvine, CA 92717.
Editor: Lorenza Saitta
Abstract. Learning multiple descriptions for each class in the data has been shown to reduce generalization error
but the amount of error reduction varies greatly from domain to domain. This paper presents a novel empirical
analysis that helps to understand this variation. Our hypothesis is that the amount of error reduction is linked to
the “degree to which the descriptions for a class make errors in a correlated manner.” We present a precise and
novel deﬁnition for this notion and use twenty-nine data sets to show that the amount of observed error reduction is
negatively correlated with the degree to which the descriptions make errors in a correlated manner. We empirically
show that it is possible to learn descriptions that make less correlated errors in domains in which many ties in
the search evaluation measure (e.g. information gain) are experienced during learning. The paper also presents
results that help to understand when and why multiple descriptions are a help (irrelevant attributes) and when they
are not as much help (large amounts of class noise).
Keywords: Multiple models, Combining classiﬁers
Introduction
Learning multiple models of the data has been shown to improve classiﬁcation error rate
as compared to the error rate obtained by learning a single model of the data . Although much work has been done
in learning multiple models not many domains were used for such studies. There has also
been little attempt to understand the variation in error reduction (the error rate of multiple
models compared to error rate of the single model learned on the same data) from domain
to domain. Three of the data sets used in our study for which this approach provides the
greatest reduction in error (Tic-tac-toe, DNA, Wine) have not been used in previous studies.
For these data sets, the multiple models approach is able to reduce classiﬁcation error on
a test set of examples by a factor of up to seven! This paper uses a precise deﬁnition of
“correlated errors” to provide an understanding of the variation in error reduction. We also
present the idea of “gain ties” to understand why the multiple models approach is effective
- especially why it is more effective for domains with more irrelevant attributes.
Figure 1 shows an example of multiple learned models of the form used in this paper. In
the multiple models approach, several models of one training set are learned. Each model
consists of a description for each class. Each description is a set of rules for that class (i.e.
K. ALI AND M. PAZZANI
1st concept description for class a
class-a(X,Y) :- b(X),c(Y).
class-a(X,Y) :- d(X,Z),e(Z,Y).
1st model of the data
1st concept description for class b
class-b(X,Y) :- e(X,Y),f(X,X).
class-b(X,Y) :- g(X),class-b(Y,X).
2nd model of the data
2nd concept description for class a
class-a(X,Y) :- b(X),c(Y).
class-a(X,Y) :- d(X,Z),h(Z,Y).
2nd concept description for class b
class-b(X,Y) :- e(X,Y),k(X,X).
class-b(X,Y) :- g(X),class-b(Y,X).
Figure 1. An example of learning multiple models - each model consists of a set of class descriptions.
each class description is a set of ﬁrst-order Horn clauses1 for that class). The set of learned
models is called an ensemble .
Previous work in learning multiple models has mainly been concerned with demonstrating
that the multiple models approach reduces error as opposed to the goal of this paper which
is to explain the variation in error reduction from domain to domain. Previous work has
compared different search strategies compared different
searchevaluationmeasures , evaluatedtheeffectsof
pruning and compared different ways of generating
models (nearly all authors). Except for the work of Buntine, all the other comparisons
have been made on a few domains so we still do not have a clear picture of how domain
characteristics affect the efﬁcacy of using multiple models. It is important to analyze these
experimental data because the amount of error reduction obtained by using multiple models
varies a great deal. On the wine data set, for example, the error obtained by uniformly
weighted voting between eleven, stochastically-generated descriptions is only one seventh
that of the error obtained by using a single description. On the other hand, on the primarytumor data set, the error obtained by the identical multiple models procedure is the same as
that obtained by using a single description.
Much of the work on learning multiple models is motivated by Bayesian learning theory which dictates that to maximize predictive accuracy,
instead of making classiﬁcations based on a single learned model, one should ideally use
all hypotheses (models) in the hypothesis space. The vote of each hypothesis should be
weighted by the posterior probability of that hypothesis given the training data. Since the
theory requires voting from all hypotheses or models in the hypothesis space, all tractable
implementations of this theory have to be approximations. This raises the following experimental question: what model-generation/evidence-combination method yields the lowest
error rates in practice? Or, how can one characterize the domains in which a particular
method works best and why does it work best on such domains?
The main hypothesis examined in this paper is whether error is most reduced for domains
for which the errors made by models in the ensemble are made in an uncorrelated manner.
In order to test this hypothesis, we ﬁrst need to deﬁne error reduction more precisely. Two
obvious measures comparing the error of the ensemble (Ee) to the error of the single model
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
(Es)areerrordifference(Es−Ee)anderrorratio(Er = Ee/Es). Weuseerrorratiobecause
it reﬂects the fact that it becomes increasingly difﬁcult to obtain reductions in error as the
error of the single model approaches zero. Error ratios less than 1 indicate that the multiple
models approach was able to obtain a lower error rate than the single model approach. The
lower the error ratio, the greater the error reduction. A precise deﬁnition of the notion of
“correlated errors” is presented in Section 5.2. Brieﬂy, our metric (φe, “fraction of same
(correlated) errors”) measures the proportion of the test examples on which members of an
ensemble make the same kinds of misclassiﬁcation errors. Two models are said to make
a “correlated error” when they both classify an example of class i as belonging to class
j, j ̸= i.
The paper presents results on why it is possible to learn models with more uncorrelated
errors for some domains than for others. We also explore the effect of varying two domain
characteristics (level of class noise and number of irrelevant attributes) on error ratio.
Finally, we examine the effect of syntactic diversity on ensemble error. This follows the
work of Kwok & Carter which postulates that learning more syntactically diverse
decision trees leads to lower ensemble error.
The remainder of the paper is organized as follows. After an examination of the main
issues in learning multiple models, we present our core learning algorithm HYDRA which we modify in various ways to learn multiple models.
Next, we present results of experiments designed to answer the following questions:
What effect does the multiple models approach have on classiﬁcation error as compared
to the error produced by the single model learned from the same training data?
What is the relationship between the amount of observed error reduction (Er) and the
tendency of the learned models to make correlated errors?
Can the amount of error reduction observed for a domain be predicted from the number
of ties in gain experienced by the learning algorithm on that domain?
How does increasing the amount of class noise affect the amount of error reduction?
How does increasing the number of irrelevant attributes affect the amount of error
reduction?
Does increasing the diversity of the models necessarily lead to greater reduction in
Background
Previous empirical work in using multiple models has mainly focused on demonstrating error reduction through using multiple models and exploration of novel methods of generating models and combining their
classiﬁcations. The work can be characterized along three dimensions: the kind of model
being learned (tree, rule etc.), the method of generating multiple models, and the method
of combining classiﬁcations of the models to produce an overall classiﬁcation. The work
K. ALI AND M. PAZZANI
of Kwok & Carter also serves as foundation for our work on the effect of syntactic
diversity on error rate. They showed that ensembles with decision trees that were more
syntactically diverse obtained better accuracies than ensembles with trees that were less
Previous theoretical work in learning multiple models includes Buntine’s formulation of
general Bayesian learning theory, Schapire’s Boosting algorithm and the results from
Hansen & Salamon and Drobnic & Gams . Schapire’s work proceeds on
the basis that models that make errors in a completely
independent manner will produce lower ensemble error. His Boosting algorithm is the
only learning algorithm which incorporates the goal of minimizing correlated errors during
learning. However, the number of training examples needed by that algorithm increases as
a function of the accuracy of the learned models. Schapire’s method could not be used to
learn many models on the modest training set sizes used in this paper.
Other theoretical results on the effects of using multiple models come from Hansen &
Salamon who prove that if all models have the same probability of making an error,
and this probability is less than 0.5 and if they all make errors completely independently
then the overall error must decrease monotonically as a function of the number of models.
Theoretical analysis of using multiple regression models has also been done by Breiman (in
press). However, this research does not say anything about the amount of error reduction
and Hansen and Salamon’s research does not say anything when errors are not completely
independent.
With the exception of Buntine , most of the empirical work has been done on a
small number of domains ; three: Kononenko & Kovacic
 ; three: Smyth et al. ). The small number of domains used reduces the chance
of accurately characterizing the conditions under which the method works. Furthermore,
although Buntine used many data sets, he did not try to explain the variation in error
reduction. By using twenty-nine data sets from twenty-one domains we are better able to
study what domain characteristics are factors in error reduction (a data set being different
from a domain in that it also involves specifying parameters such as number of training
examples, noise levels and irrelevant attributes).
Methods for learning multiple class descriptions
Weconsidertwomethodsforgeneratingmultipleclassdescriptions: stochastichill-climbing
 and deterministic learning from a k-fold partition of the training data . Although these methods are not new, our goal is
to show that our results pertaining to error reduction, correlatedness of errors and gain ties
apply to more than one method of generating multiple models.
We use HYDRA to learn a single model consisting of a description
for each class. HYDRA is based on extensions to FOIL2 proposed in Ali
& Pazzani and Pazzani et al. . HYDRA is then further modiﬁed to learn
several models.
The pseudo-code for FOIL is presented in Table 1. FOIL learns one clause (rule) at a time,
removing positive training examples covered by that clause in order to learn subsequent
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
FOIL(POS-EGS,NEG-EGS,Positive-class-name,Arity):
Let LearnedDescription be the empty set
Until POS-EGS is empty do
Separate: (begin a new clause)
Let head of NewClause be Positive-class-name(V_1,...,V_Arity)
Let body of NewClause be empty, NEG be NEG-EGS, POS be POS-EGS
Until NEG is empty do:
Conquer: (build a clause body)
Conjoin to body of NewClause the literal that yields highest gain
Remove from POS and NEG examples that do not satisfy NewClause
Add NewClause to LearnedDescription
Remove from POS-EGS all positive examples that satisfy NewClause.
Return LearnedDescription
Table 1. Pseudo-code for FOIL.
clauses. This is referred to as the “separate and conquer” or “covering”
 strategy. The basic FOIL procedure learns as follows. A clause
for a given class such as class-a is learned by a greedy search strategy. It starts with an empty
clause body which covers all remaining positive and negative examples. Next, the strategy
considers all literals that it can add to the clause body and ranks each by the information
gained if that literal were to be added to the current clause body. Brieﬂy, the
information gain measure favors the literal whose addition to the clause body would result
in a clause that would cover many positive examples and exclude many negative examples.
The literal that yields the highest information gain is then added to the clause body. The
strategy keeps adding literals until either the clause covers no negative examples or there is
no candidate literal with positive information gain. Positive examples covered by the clause
are removed from the training set and the process continues to learn subsequent clauses on
the remaining examples, terminating when no more positive examples are left.
FOIL only learns in data sets consisting of two-classes, one of which must be identiﬁed
as the “positive” class. FOIL learns a class description only for the class identiﬁed as the
“positive” class. Thus, FOIL learns a single model consisting of a single class description.
FOIL uses the closed-world assumption for classiﬁcation: if the test example
matches the body of any clause learned for class “positive” then the example is assigned to
class “positive.” If it fails to match any clause, FOIL uses the closed-world assumption and
assigns the example to class “negative.”
The way we extend FOIL to learn a rule set for each class is by treating examples of all
other classes as negative. This is the algorithm used in HYDRA .
We prefer this way of learning for multi-class data rather than learning a set of rules of the
class(V1...Vn, X)
..., X = class-a
class(V1...Vn, X)
..., X = class-b
because of a technical limitation with FOIL - there is no guarantee in FOIL that the variable
corresponding to the class (X) will appear in the body of the learned clause.
K. ALI AND M. PAZZANI
Now we discuss two methods of learning several descriptions for each class in the training
data. These methods involve executing the HYDRA procedure once for each model to be
Stochastic Hill-climbing- Stochastic hill-climbing only involves modifying HYDRA’s
procedure for selecting which literal to add to the clause currently being learned. Instead
of picking the best literal (ranked according to some measure such as information gain)
stochastic hill-climbing stores all literals that are within some margin, β, of the best
and then picks non-deterministically from among that set. The probability of a literal
being picked is proportional to its gain. The set of literals whose gain exceeds β times
that of the best literal is called the “bucket.”
k-fold partition learning- This procedure generates k models by partitioning the
training data into k equal-sized sets and in turn, training on all but the i-th set. HYDRA
is called k times and each time it learns a class description for each class in the data set.
k-fold partition learning was ﬁrst used by Gams whose system learns ten models
using 10-fold partition learning and then combines them into a single model. By doing
so, however, he is not able to exploit the advantages of evidence combination from
different descriptions. Our version of this algorithm differs from Gams in retaining all
rule sets and using evidence combination to form overall classiﬁcations.
Methods for combining evidence
Our experiments compare four evidence combination methods: Uniform Voting, weighted
combination according to Bayesian probability theory , weighted combination according to Distribution Summation and Likelihood
Combination . Results using all four evidence combination methods and
both learning methods are given in the ﬁrst appendix. Our goal is to empirically demonstrate
that our hypotheses about error reduction apply for a wide variety of evidence combination
Figure 2 shows a situation which will be used to explain the evidence combination methods. Assume for the moment that only the ﬁrst model has been learned. The rules in bold
typeface indicate the rules that have been satisﬁed for the current test example. The ﬁgure
indicates that the preconditions of two rules for class a were satisﬁed by the test example.
The ﬁrst of these rules covers four training examples of class a. The ﬁgure also indicates
that the second rule of the ﬁrst description of class b covers one training example of class
a and two of class b.
Evidence combination within one description
Before describing evidence combination between descriptions of a given class, we explain
how classiﬁcation occurs when only one model has been learned. Each evidence combination method uses its own kind of reliability measure. When only one model is being used,
and more than one rule in a class description has been satisﬁed, three of the four methods
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
1st model of data (posterior prob. = 0.02)
1st description of class a
a(X,Y) :- j(X), g(Y). Covers (7,0) LS = 5.0, Accu. = 0.89
a(X,Y) :- c(X,Z), i(Y). Covers (4,1), LS = 1.6, Accu. = 0.71
a(X,Y) :- k(Y,X), d(X). Covers (3,1), LS = 1.3, Accu. = 0.67
1st description of class b
2nd model of data (posterior prob. = 0.015)
2nd description of class a
a(X,Y) :- j(X), d(Y). Covers (8,0). LS=5.6, Accu. = 0.90
a(X,Y) :- k(Y,X), i(Y). Covers (6,0), LS=4.4, Accu. = 0.88
2nd description of class b
b(X,Y) :- h(X,Y), f(Y). Covers (0,6), LS=11.2, Accu. = 0.88
b(X,Y) :- n(X,Y), f(Y). Covers (1,2), LS=2.4, Accu. = 0.60
b(X,Y) :- n(X,Y), i(Y). Covers (0,2), LS=4.8, Accu. = 0.75
b(X,Y) :- m(X,Y), f(Y). Covers (1,6), LS=5.2, Accu. = 0.78
Figure 2. Comparison of evidence combination methods. “Covers (0,6)” for a rule for class ’b’ indicates that the
body of that rule is true for 0 training examples of class ’a’ and for 6 training examples of other class(es). The
given accuracy is a Laplace estimate of the accuracy of the rule as estimated from the training data. LS is the
degree of logical sufﬁciency of that rule (explained under “Likelihood Combination” in Section 4).
Table 2. Four evidence combination methods. The composite evidence for a class is obtained by summing the
degrees of belief for that class over descriptions of that class.
Evidence comb.
Uniform Voting
Bayesian Comb.
0.02 * 0.71
0.015 * 0.90
0.02 * 0.88
Distribution Sum.
(4,1) + (3,1)
Likelihood Comb.
1.6 × 5.6 × 1.75
11.2 × 1 × 0.57
described here use only the most reliable of those satisﬁed rules. We will refer to this as
the “single, most reliable rule” bias. See Torgo for empirical support for using this
bias within each rule set.3 Only the Distribution Summation method takes all satisﬁed rules
within a class description into account. For each method, if the example does not satisfy
any rule of any class, each method predicts the class that was most frequent in the training
Uniform Voting - Uniform Voting assigns a uniform reliability of 1 to each rule. It
assigns a score of 1 to a class if any rule in that class was satisﬁed by the test example.
Otherwise the score is 0. So this means that for Figure 2, both classes in model 1
get a score of 1. Uniform Voting then randomly chooses between the classes with the
K. ALI AND M. PAZZANI
highest score. Uniform Voting is not competitive with the other methods when using
just a single model but it is competitive once several models are used.
Bayesian Combination - In Bayesian Combination, there are weights
associated with models (the posterior probability of the model) and weights associated
with rules (the accuracy of the rule). When only 1 model is being used, only the ruleweights are relevant. The accuracy of a rule r of class i with respect to a set S of
examples is the ratio of the number of examples of class i in set S which satisfy the
rule divided by the total number of examples (of any class) in S which satisfy the rule.
The accuracy is denoted as p(Classi|r).
A word about estimation of rule accuracy: we use the training set as set S. This will
typically over-estimate the accuracy of the rule so as a correction we use the Laplace
estimate . The Laplace estimate of the probability of the event
X = v where X is a variable and v is a value which has been observed to occur f times
in T consecutive trials is (f + 1)/(T + k) where k denotes the number of possible
values that X can take. In the context of rule accuracy, using the Laplace estimate
means that if N denotes the total number of examples that satisfy rule r and ni denotes
the number of examples of class i that satisfy r, ni+1
N+2 is used as an estimate of the rule
accuracy. We use “2” instead of the real number of classes since with respect to rules
of class i, all other classes are grouped together as the “negative class.”
In Figure 2, the accuracies of the satisﬁed rules of class a are 0.71 and 0.67 so the more
reliable (0.71) is used as the score for class a. Class b only has one satisﬁed rule so its
accuracy (0.88) is used. Bayesian Combination predicts the class with the higher score
- class b in this situation.
DistributionSummation -Thismethodassociatesak-component
vector (the distribution) with each rule. k denotes the number of classes. The vector
consists of the numbers of training examples from all k classes covered by that rule. A
component-wise sum is formed over all satisﬁed rules (of all classes) that match a test
example to produce a combined vector. So in Figure 2, the distributions of the satisﬁed
rules are added to yield the summed vector: (4, 1) + (3, 1) + (0, 6) = (7, 8). Since the
highest number in the summed vector corresponds to class b, this method will predict
Likelihood Combination - This method associates the “degree of
logical sufﬁciency of the rule” (LS) with each rule. In the context
of classiﬁcation, the LS of a rule of Classi is deﬁned as the ratio of the following
probabilities:
p(rule(τ) = true | τ ∈Classi)
p(rule(τ) = true | τ ̸∈Classi)
where τ is a random example. Each probability is estimated using the Laplace method.
LS is a generalization of the notion that the body of a rule is completely sufﬁcient to
conclude the head of the rule. This method uses the odds form of Bayes rule which can be restated for our purposes as:
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
O(Classi|Mi1) = O(Classi) × O(Mi1|Classi)
∝O(Classi) × O(Mi1|Classi)
where Mi1 denotes the description for class i in the ﬁrst (and in this section, only) model.
The odds of a proposition with probability p are deﬁned to be p/(1 −p). In order to
calculate O(Mi1|Classi), let {R1, . . . , Rn} denote the set of rules in description Mi1
that were satisﬁed by the example. If these rules are conditionally independent given
class i, we can write:
O(Mi1|Classi) =
where LSRj is the LS of rule Rj. However, since the rules were learned by a separate
and conquer strategy, rather than taking a product of the LS’s of the satisﬁed rules as
suggested by Equation 2, it is conceptually (and empirically) better to use only the LS
of the most reliable rule.
In Figure 2, class a had 14 of the 22 training examples for a “prior” probability of 0.63
and prior odds of 1.75. Class b had 8 of the 22 examples for a “prior” probability of
0.36 and prior odds of 0.57. So, in Figure 2, for class a, we multiply the prior odds of
class a with the LS of the more reliable of the two satisﬁed rules (1.6) to yield a score
of 2.8. This score represents the posterior odds of class a. Class b has prior odds of
0.57 and the LS of the most reliable satisﬁed rule of class b is 11.2, so its score is 6.384.
Therefore, Likelihood Combination predicts class b in this situation.
We chose Uniform Voting as a “straw man” method which the other methods should
be able to beat in terms of accuracy. We chose Bayesian Combination because it is an
approximation to the optimal Bayes approach. Distribution Summation was chosen because
rules that cover more examples are given higher weight in this method. As Muggleton et
al. have noticed, training coverage of a rule is more closely correlated with its
test-set accuracy than is its training accuracy. Finally, we chose Likelihood Combination
because the logical sufﬁciency measure used by that method has the ﬂavor of measuring both
coverage and accuracy. Most of the rules learned by HYDRA cover no negative training
examples. Under these conditions, the Laplace estimate of training set accuracy ranks rules
in order of the number of positive examples covered whereas training set LS ranks rules in
order of the fraction of positive space covered. Accordingly, we ﬁnd that rules of minor
classes are given relatively higher weights under the LS scheme.
Evidence combination between descriptions
Now we describe how to combine evidence when more than one model has been learned.
When more than one model has been learned, classiﬁcation proceeds by combining evidence
for each class from all its descriptions and then ﬁnally comparing that degree of evidence
to those of the other classes.
K. ALI AND M. PAZZANI
Uniform Voting - In the context of multiple models (Table 2) this method simply counts,
for each class, the number of descriptions of that class that have at least one satisﬁed
rule. So, in Figure 2, class a gets a score of 2 and class b gets a score of 1 so this method
predicts class a.
Bayesian Combination - In the general form of Bayesian Combination, the test example,
x, should be assigned to the class, c with the highest expected posterior probability:
ET (p(c|x, ⃗x,⃗c)) =
p(c|x, T)p(T|⃗x,⃗c)
where the expectation is taken with respect to T , the model (hypothesis) space of all
possible models, ⃗x denotes the training examples and⃗c denotes the class labels of those
training examples. x denotes the current test example. p(c|x, T) is the probability
of class c given a test example x and a particular model T. p(c|x, T) can be thought
of as the degree to which T endorses class c for example x. In this paper, since we
are using a “single, most reliable rule” bias, the Laplace accuracy of the most reliable
satisﬁed rule is used for p(c|x, T). p(T|⃗x,⃗c) denotes the posterior probability of the
model. Brieﬂy, models whose class descriptions are syntactically-compact and are well
able to separate the training examples of different classes end up with higher posterior
probabilities. Appendix 2 and detail how Buntine’s form for
the posterior probability of a decision tree is adapted for the kinds of
models described in this paper.
The general Bayesian method is used in our “single, most reliable rule” framework as
follows. As Figure 2 indicates, the ﬁrst model has posterior probability 0.02 and the
satisﬁed rule of class a with highest accuracy has accuracy 0.71. The second model has
posterior probability 0.015 and the accuracy of the matching rule is 0.90. This yields
an expected posterior probability for class a of 0.0277 (0.02 ∗0.71 + 0.015 ∗0.90).
Doing the same for class b yields a degree of belief of 0.0117 for class b. Hence, the
test example is assigned to class a.
Distribution Summation - This method is simply extended to multiple models by doing
a vector summation of the distributions of all satisﬁed rules across all models. So, in
Figure 2, this produces an summed vector of (4, 1) + (3, 1) + (0, 6) + (8, 0) = (15, 8)
and consequently the example is assigned to class a.
Likelihood Combination - In extending this method to multiple models, the prior odds
of the class only appear once: let Mi denote the set of class descriptions for class i and
Mij denote one such class description. Then the posterior odds of Classi are given by:
O(Classi|Mi) ∝O(Classi) ×
O(Classi|Mij)
For the term O(Classi|Mij) we use the LS of the most reliable satisﬁed rule in Mij.
As Table 2 shows, the posterior odds of class a are obtained by multiplying the prior
odds (1.75) by the LS of the most reliable matching rule in the ﬁrst description of class
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
a with the LS of the most reliable matching rule in the second description of class a.
This yields posterior odds of 15.68 for class a and posterior odds of 6.384 for class b.
Therefore, this evidence combination method will assign the example to class a.
Empirical analyses
For our experiments we chose domains from the UCI repository of machine learning
databases ensuring that at least one domain from each of the major
groups (molecular biology, medical diagnosis ...) was chosen. These include molecularbiology domains (2), medical diagnosis domains (7), relational4 domains (6 variants of the
King-Rook-King (KRK) domain, Muggleton et al., 1989), a chess domain with a “small
disjuncts problem” , and attribute-value domains (4 LED variants
and the tic-tac-toe problem).
For most of the domains tested here, we used thirty independent trials, each time training
on two-thirds of the data and testing on the remaining one-third. The exceptions to this
are the DNA promoters domain for which leave-one-out testing has traditionally been used
and we follow this tradition to allow comparability with other work. Other exceptions are
trials involving the King-Rook-King domain. For this domain, the training and test sets
are independently drawn (rather than being mutually exclusive) from the set of all 86 board
conﬁgurations. There is little chance of overlap between training and test sets at the sample
sizes we use. Whenever possible we tried to test learned models on noise-free examples
(including noisy variants of the KRK and LED domains) but for the natural domains we
tested on possibly noisy examples. The large variant of the Soybean data set was used and
the 5-class variant of the Heart data set was used.
Does using multiple rule sets lead to lower error?
In this section we present results of an experiment designed to answer the ﬁrst of the
questions listed in Section 1:
What effect does using multiple descriptions per class have on classiﬁcation error
as compared to the error produced by using a single description per class?
For this experiment, the Stochastic and Partition methods were used to learn eleven models
(we chose an odd number to prevent ties from occurring for the Uniform Voting combination
method for two-class domains). Although most of the results in the following sections are
given for eleven models, we also performed experiments using one, two and ﬁve models.
Figure 3 shows the effect of varying the number of models on classiﬁcation accuracy. Eleven
models were used since preliminary experiments indicated that for most data sets, using
more than eleven models yields little gains but costs a lot in terms of computation time.
Unfortunately, we do not have a method that will indicate the optimal number of models to
learn in a given data set.
For the Stochastic method, all literals that had gain at least 0.8 ( β = 0.8) as large as
that of the best literal were retained (see Section 5.6 for results on the effect of varying the
K. ALI AND M. PAZZANI
Tic tac toe
KRK 160egs, 5% attr. noise
Figure 3. The ﬁgures above illustrate the effect of varying number of stochastically-learned models (combined with
Uniform-Voting). Open circles represent accuracies obtained by HYDRA, closed circles represent the multiple
models method. At least one data set from each of the major types of data sets used in this paper is represented.
bucket size). For the Partition method, k was set to the same number of models (eleven)
as used during stochastic hill-climbing. For each description generation method, we tested
all four evidence combination schemes.
The results of using Likelihood Combination
on stochastically-generated descriptions are presented in Table 3. Results using all four
evidence combination methods and both learning methods are presented in the appendix.
Table 3 compares the accuracies obtained by using a single deterministically learned
description to the accuracies obtained by using eleven descriptions.
The ﬁrst column
indicates the domain name. Trailing sufﬁxes indicate number of irrelevant attributes (i),
number of training examples (e), percentage of attribute noise (a) or percentage of class
noise5 (c). The second column indicates the accuracy that would be attained by guessing the
most frequent class. An asterisk signiﬁes that the accuracy of the single description method
wasnotsigniﬁcantlybetterthanguessingthemostfrequentclass. Thethirdcolumnindicates
the accuracy obtained by using HYDRA (using information gain) to deterministically learn a
single description. The next two columns indicate error ratios for stochastic and k-partition
learning respectively. A ’+’ indicates a signiﬁcant (using the paired 2-tailed t-test at the
95% conﬁdence level) reduction in error, a ’–’ indicates a signiﬁcant increase. For the DNA
domain, the t-test is not applicable because we used leave-one-out testing. For this domain,
we used a sign-test .
The data sets are grouped as follows: the ﬁrst group contains noise-free training data
from artiﬁcial concepts (for which we know the true class descriptions), the second group
contains noisy data from artiﬁcial concepts the third contains data sets from molecular
biology domains and the ﬁnal group contains probably noisy data from medical diagnosis
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Table 3. Comparison of errors produced by a single description versus two methods (stochastic
hill-climbing and k-fold partition learning) of learning multiple descriptions. Eleven models were
used and the Likelihood Combination method was used for evidence combination. A ’+’ indicates
that the accuracy of multiple models was signiﬁcantly higher than that of the single model. A ’-’
indicates the accuracy was signiﬁcant lower. An asterisk indicates that the accuracy of the single
model version was not signiﬁcantly better than guessing the most frequent class.
11 Stochastic
11 Stochastic
11 Partition
Description
Descriptions
Descriptions
Descriptions
Error Ratio
Error Ratio
Tic-tac-toe
Krk 160e 5a
Krk 320e 5a
Krk 160e 20c
Krk 320e 20c
Hypothyroid
BC-Wisconsin
Horse-colic
Primary-tumor
and other “real world” domains. The domains in the last group are sorted so that those with
the highest single model accuracies appear ﬁrst.
Table 3 shows that stochastic search using Likelihood Combination is able to statistically signiﬁcantly (95% conﬁdence) reduce or maintain error on all domains except the
(Ljubljana) breast-cancer domain. On that breast cancer data set few learning methods
have been able to get an accuracy signiﬁcantly higher than that obtained by guessing the
most frequent class suggesting it lacks the attributes relevant for discriminating the classes.
The table shows that for approximately half the data sets, error is reduced by a statistically
signiﬁcant margin when using models learned by stochastic search and combined with Likelihood Combination. The appendix shows that the other evidence combination methods
and learning methods also lead to statistically signiﬁcant error reductions for many data
K. ALI AND M. PAZZANI
sets. There is no signiﬁcant change in error for most of the other data sets - on very few
occasions does the multiple models approach lead to a signiﬁcant increase in error.
Another striking aspect of the results presented in Table 3 is that the error is reduced
by a factor of 6 for the wine data set (representing an increase in accuracy from 93.3%
to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tac-toe. The molecular
biology data sets also experienced signiﬁcant reduction with the error being halved (for
DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction
is least for the noisy KRK and LED data sets and for the presumably noisy medical diagnosis
data sets. Eighty percent of the data sets which scored unimpressive error ratios (above 0.8)
were noisy data sets. This ﬁnding is further explored in Section 5.4 in which we explore the
effect of class noise on error ratios. The fact that the best error ratios were obtained on the
noise-free and molecular biology data sets holds for all four of the evidence combinations
schemes we used and both description generation methods (see appendix 1).
The LED domain, in particular, gives us some insight into the effect of irrelevant attributes
and class noise on error ratios. As the table shows, learning multiple descriptions helps a
lot in reducing errors of the LED data sets with irrelevant attributes. For eight irrelevant
attributes, the error is reduced from 12.8% to just 3.6%. This suggests that when irrelevant
attributes are present, using multiple descriptions provides a substantial beneﬁt. Backing
up this hypothesis are also the DNA and Splice domains for which the error is reduced by
a large factor. These domains have many (57 for DNA, 60 for Splice) attributes some of
which are probably irrelevant. These observations led us to more carefully investigate the
effect of irrelevant attributes on error ratio. The results of those investigations are presented
in Section 5.5.
Although error ratios for the noisy data sets represent a statistically signiﬁcant reduction
in error, the ratios are not as impressive as they are for noise-free domains containing
irrelevant attributes. Again, the LED data sets provide some insight. The LED variants
presented in the table differ by two dimensions: the variants with irrelevant attributes have
no noise and the noisy variants have no irrelevant attributes. The LED results suggest that
the error ratios obtained through the use of multiple descriptions become less beneﬁcial as
the amount of noise increases. This issue is explored in detail in Section 5.4.
In summary, the answer to the question for this section (“What effect does the use of
multiple descriptions have on classiﬁcation error?”) is that the use of multiple descriptions
leads to signiﬁcant reductions in classiﬁcation error for about half of the data sets tested
here. For most of the other data sets, the error does not change signiﬁcantly. Therefore, most
of the time, the multiple descriptions approach helps signiﬁcantly or does not hurt. This
is true for both description generation methods and all four evidence combination methods
tried here. The table in the appendix presents results for the other generation methods and
evidence combination methods.
Link between error reduction and correlated errors
In this section we explore the following question:
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
What is the relationship between the amount of observed error reduction (as measured by error ratio) and the tendency of the learned models to make correlated
Hansen & Salamon ﬁrst introduced the hypothesis that the ensemble of models
is most useful when its member models make errors totally independently with respect to
every other model in the ensemble. They proved that when all the models have the same
error and that error is less than 0.5 and they make errors completely independently that the
expected ensemble error must decrease monotonically with the number of models. The
question we explore here is more general ﬁrstly because it does not assume that the errors
are made completely independently and secondly because it attempts to explain the amount
of error reduction in terms of the fraction of correlated errors (φe).
Now we present a precise instantiation of the concept: “the degree to which the errors
made by models of the ensemble are correlated.” In our approach, we will compute a
correlation for each pair of models in the ensemble F = { ˆf1... ˆfT } and φe will be the
average of all those pairwise correlations. Let φij denote the correlation between the i-th
and j-th models. Let ˆfi(x) = y denote the event that model i has classiﬁed example x to
class y. Let f(x) denote the true class of x. Then φij has the following deﬁnition:
φij = p( ˆfi(x) = ˆfj(x), ˆfi(x) ̸= f(x))
and φe(F), the degree to which the errors in F are correlated, has the following deﬁnition:
p( ˆfi(x) = ˆfj(x), ˆfi(x) ̸= f(x))
Figure 3 plots error ratio as a function of percentage of correlated errors (100 × φe) for
all domains for which there was a statistically signiﬁcant reduction in error.6 The linear
correlation coefﬁcient (r) between fraction of correlated errors (φe) and error ratio (Er)
can be used to measure how well φe models error reduction as measured by Er. Of the 29
data sets used in this study, signiﬁcant error reduction was obtained (when using stochastic
learning and Uniform Voting) on 15 data sets. Error did not increase signiﬁcantly for any of
the remaining 14 data sets. The r2 of 0.56 in the Figure shows that 56% of the variance in
error ratio can be explained by the tendency of members of the ensemble to make correlated
For the other evidence combination methods, the values were 56% (Bayesian
Combination), 43% (Distribution Summation) and 41% (Likelihood Combination). When
k-fold partition learning was used, the values were 60% (Uniform Voting), 40% (Bayesian
Combination), 35% (Distribution Summation) and 41% (Likelihood Combination). This is
quite encouraging given that the data sets vary widely in type of class description, optimal
Bayes error level, numbers of training examples and numbers of attributes. Another point
to note is that φe is a pairwise measure, whereas what the error rate under Uniform Voting
counts is the proportion of the test examples on which at least half of the members in the
ensemble make an error.
How stable are these estimates of r2? In particular, is it possible that we are able to get
such a high r2 simply because of one point luckily appearing near the line of best ﬁt? In
K. ALI AND M. PAZZANI
Figure 4. Plot of error ratio as a function of 100 × φe. One point represents one data set. Learning method:
stochastic hill-climbing, evidence combination method: Uniform Voting.
order to measure the stability of these estimates of r2, for each of the eight combinations
of learning method and evidence combination method we calculated twenty-nine r2 values
- each time calculating what the r2 would be if one of the 29 data sets were left out. This
analysis (Table 4) shows that the r2 values presented above do not depend critically on
any single data set. We also performed signiﬁcance tests to compute the likelihood of the
observed results under the null hypothesis (that the population correlation, ρ, equals 0).
The tests showed that the likelihood of our data given H0 was less than 0.01 for each of
the eight combinations of learning method and evidence combination method. Therefore,
we can conclude that there is a signiﬁcant linear correlation between error ratio and the
tendency to make correlated errors for all the learning methods and evidence combination
methods used in this study. When φe is small, multiple models have a substantial impact
on reducing error. In Sections 5.4 and 5.5 we investigate how class noise and irrelevant
attributes affect φe and consequently the amount of error reduction achieved by multiple
In order to gain insight into why φe explains so much of the variance in error ratio consider
the simpler problem of modeling variation in error within a given data set (this removes
possibly confounding variables such as optimal Bayes error rate that vary from one data
set to another). Assume that N trials have been conducted to yield N ensemble error
values. Assume that the simplest evidence combination method (Uniform Voting) is used
and that the data set contains two classes and that the ensemble contains just two models.
In this situation, an ensemble error occurs if both the models make an error or if the models
disagree and the tie is broken so as to cause an error. Assume that a tie will occur for a
negligible proportion of the test examples. Under these assumptions, φe is an exact measure
of ensemble error (Ee).
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Table 4. Ranges of leave-one-out estimates of r2 between error ratio and φe (the
fraction of correlated errors).
Distribution
Likelihood
Combination
Combination
Stochastic
Hill-climbing
[0.54,0.61]
[0.47,0.71]
[0.37, 0.52]
[0.38,0.48]
k-fold Partition
[0.55,0.66]
[0.27,0.56]
[0.28,0.46]
[0.29,0.56]
As φe is a pairwise measure, how well it models within-dataset ensemble error depends
on the size of the ensemble. It is a better model of ensemble errors for ensembles of smaller
size. The evidence combination method also affects the ability to model ensemble error
using φe. φe is a better model of ensemble error obtained by Uniform Voting than it is
for evidence combination methods in which different models are given different “voting”
Gain ties and error reduction
φe provides a post-hoc way of understanding why the multiple models approach reduces
error more for some domains than for other domains. In this section, we explore whether we
can approximately predict the amount of error reduction due to the use of multiple models.
We explore the following question:
Can the amount of error reduction observed for a data set be predicted from the
number of ties in gain experienced by the learning algorithm on that data set?
The motivation for postulating this hypothesis is the observation that each time the stochastic
generation method is run, it uses the same training data. However, it is able to generate
different descriptions because it randomly picks from the literals whose gain is within some
factor β (β ∈ ) of the gain of the highest literal. If there are many such literals
then the possibility for syntactic variation from description to description is greater. The
greater syntactic diversity may leads to less correlation of errors as measured by φe which
in turn may lead to lower (i.e. better) error ratios. As a ﬁrst approximation measure of the
amount of syntactic variety in a data set as experienced by a learning algorithm, consider
the number of literals that tie for the highest information gain. If n literals tie for gain, that
event is recorded as representing n −1 ties in gain. The total number of ties experienced
during learning a model is then divided by the number of literals in the model to produce
the quantity g, the “average number of gain ties” for that data set. A large number of such
ties are a problem for a hill-climbing deterministic learner but represent an opportunity for
the multiple model learner. Figure 5 plots error ratio as a function of average gain ties (each
point represents results for one data set from Table 3). The ﬁgure shows that some of the
largest reductions in error are obtained for data sets for which such ties are frequent (on
average, there were 5.1 gain ties on the wine data set, 6.6 for the DNA promoters data set
K. ALI AND M. PAZZANI
Average Gain Ties
Error Ratio
Average Gain Ties
Error Ratio
Figure 5. Error ratio as a function of average gain ties for decision trees (left) and rule sets (right). The ensembles
of decision trees contained eleven decision trees stochastically learned with respect to the entropy gain function.
The ensembles of eleven rule sets were learned using stochastic hill-climbing and combined using Likelihood
Combination. Similar plots are obtained for other evidence combination methods and the other learning method.
and 2.5 for the Splice data set). However, the ﬁgure also shows that a high average value
for ties in gain is not a necessary condition for signiﬁcant reduction of error. For example,
multiple models are able to achieve low error ratios on the Tic-Tac-Toe and the noise-free
LED variants (bottom left of ﬁgure) even though there are not many ties in gain for those
data sets.
In summary, the answer to the question posed in this section is that if the number of gain
ties experienced on average for a data set is large (say 2 or more) then that data set will
beneﬁt quite a lot (i.e. have its error reduced by at least 40%) from the use of multiple
models. In our experiments, we have seen no exceptions to this trend. However, if the
number of gain ties is small, the amount of error reduction cannot be predicted. As Figure 5
shows, these gain-ties results are not just true for HYDRA - they are also true for ID3
 - the canonical decision tree learning algorithm.
Effect of class noise
The results of Section 5.1 showed that the majority (80%) of data sets for which unimpressive error ratios (above 0.8) were recorded were data sets with signiﬁcant amounts of
noise. Furthermore, experiments on the LED domain provided preliminary evidence that
the addition of attribute noise increases (worsens) error ratios. In this section we follow up
on that hypothesis by asking:
How does increasing the amount of class noise affect the amount of error reduction?
We choose to study the effect of class noise rather than attribute noise because attributes
in some domains have more values than attributes in other domains and an attribute with
fewer values is more likely by chance to have large information gain. Therefore it would
not be easy to compare levels of attribute noise across domains.
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Table 5. Effect of increasing class noise on error ratios (using 11 stochastically-learned models and Uniform
Voting for evidence combination). Similar plots are obtained for other evidence combination methods and
the other learning method.
Err. ratio
Err. ratio
Err. ratio
Err. ratio
Table6. Distributionofensembleerrorsasafunctionofthenumberofmodelscorrectlyclassifyinga
test example. Learning method: stochastic hill-climbing; evidence combination method: Uniform
Voting. Eleven models were combined using Uniform Voting so an ensemble error occurs if six
or more of the models made an error. Values at 20% and 30% noise lie in between the values
presented in the table.
Number of models
got test eg. correct
% of ensemble
Cumulative % of
% of ensemble
Cumulative % of
ensemble errors
ensemble errors
Table 5 shows the effect of adding class noise to four very different kinds of data sets.
Noise was only added to the training data. We chose the wine and tic-tac-toe data sets
because the multiple models approach was able to reduce error by a large amount (error
ratios of 0.16 and 0.22 respectively) for these data sets. We wanted to see if this advantage
would be eroded by the addition of noise. The table shows that for each of the four chosen
data sets the advantage yielded by the multiple models approach lessens as class noise is
increased.
More careful examination of the patterns of errors of models in the ensemble shows that
at 40% noise, a relatively larger proportion of the test examples on which the ensemble
made an error were incorrectly classiﬁed by all the models in the ensemble. That is, as
noise increases, some of the examples become “hard” for all the models.
In a follow-up experiment (Table 6), we studied the distribution of the ensemble errors.
We wanted to know what proportion of the ensemble errors were caused by all the models
making an error and what proportion were caused by a narrow majority of models making
an error. The ﬁrst column indicates the number of models that correctly classiﬁed the test
example. The remaining columns are arranged in two groups. Columns two and three
K. ALI AND M. PAZZANI
Error ratio (0% class noise added)
Error ratio (40% class noise added)
Figure 6. Comparison of error ratios at 0% added class noise and 40% added noise. Learning method: stochastic
hill-climbing; evidence combination method: Uniform Voting.
present results for 10% class noise, the last two columns present results for 40% noise. The
i-th row corresponds to test examples that were correctly classiﬁed by i (out of 11) models.
The ﬁrst column in each set indicates the number of test examples characterized by that
situation. Let a m/n split indicate the situation for a test example where m models make
a correct classiﬁcation and n make a mistake (m + n = 11). Therefore, the table indicates
that a 0/11 split occurred on 15.4% of the test examples after learning with 10% class
noise and it occurred on 27.6% of the test examples after learning with 40% class noise.
Therefore, the table indicates that that as noise level increases, all the models misclassify
a test example on a greater proportion of the test examples for which an ensemble error
is made. This indicates that as noise level increases, some test examples become more
difﬁcult for all the models.
Figure 6 compares the error ratio (11 models, stochastic learning, Uniform Voting) with
0% added noise to that with 40% added noise. In each case, the addition of noise causes the
error ratio to go towards 1 indicating the erosion of the advantage of the multiple models
In summary, the answer to the question of this section (“How does increasing the amount
of class noise affect the amount of error reduction?”) is that increasing class noise causes
the multiple models approach to produce poorer error ratios. Extrapolation of these results
suggests that at 100% noise the error ratios for all data sets would be 1.0. This makes sense
because the training data contains no discrimination information so there is no reason to
expect the multiple models approach to do better than the single models approach.
Effect of irrelevant attributes
The experiments presented in Section 5.1 provide preliminary evidence that the beneﬁt
of using the multiple models approach increases with increasing numbers of irrelevant
attributes. In this section we describe further experiments to explore this question:
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Table 7. Error ratio as a function of number of added Boolean irrelevant attributes (using
Uniform Voting of eleven stochastically generated models). The number below each data set
identiﬁer indicates the number of training examples. “5a” indicates 5% attribute noise. Similar
results are obtained for other evidence combination methods and the other learning method.
irrelevant
attributes
How does increasing the number of irrelevant attributes affect the amount of error
reduction?
To study this question, we added varying number of Boolean irrelevant attributes to a
representative sample of data sets. We chose Boolean attributes rather than constructing
irrelevant attributes whose values were domain speciﬁc because the attributes in some data
sets can take on many more values than attributes in other data sets leading to comparison
difﬁculties.
Table 7 corroborates the hypothesis that the multiple models approach is able to attain
especiallyimpressiveerrorreductionswhenmanyirrelevantattributesarepresentinthedata.
The table shows that error ratio decreases as a function of increasing numbers of irrelevant
attributes. To understand this, consider the Uniform Voting evidence combination scheme.
For the multiple models approach to make an error due to irrelevant attributes, at least half
of the learned models need to involve an irrelevant attribute that leads to a classiﬁcation
error. If the number of irrelevant attributes is not too large, it is unlikely that at least half
of the models will be affected in this manner. Therefore, the multiple models approach
will not make an error in this situation. But the single model approach need only make
a mistake due to learning a rule involving an irrelevant attribute early in its separate and
conquer strategy for most of the subsequent rules to go off track. Hence the single model
approach is much more likely to suffer due to irrelevant attributes. Figure 7 extends the
irrelevant attributes experiment to all 29 data sets. It plots the error ratio obtained after the
addition of 50 irrelevant binary attributes against the error ratio before the addition of any
irrelevant attributes. The fact that most of the plotted points lie below the diagonal indicate
that for most of the data sets adding irrelevant attributes leads to smaller (better) error ratios.
Table 7 also shows that the average number of gain ties experienced increases as the
number of irrelevant attributes increases. This conﬁrms the results (Section 5.3, Figure 5)
that better (lower) error ratios are obtainable for data sets where the learning algorithm
experiences more gain ties.
Consider, however, what would happen if an arbitrarily large number of irrelevant attributes were to be added to a data set. By adding enough irrelevant attributes, one could
force all the learned models to go astray. In this situation, one would expect that the error
ratio should go to 1 as both the deterministic and multiple models approaches would perform
at chance level. Hence, we predict that for large enough numbers of irrelevant attributes the
error ratio would increase with increasing numbers of irrelevant attributes. This hypothesis
K. ALI AND M. PAZZANI
Error ratio with 0 added irrelevant attributes
Error ratio with 50 added irrelevant attributes
Figure 7. Comparison of error ratios with 0 added irrelevant binary attributes and 50 added irrelevant binary
attributes. Learning method: stochastic hill-climbing; evidence combination method: Uniform Voting.
Table 8. Error ratio as a function of number of added Boolean irrelevant attributes
for small sample sizes (using ensembles of eleven stochastically-learned models;
combined with Uniform Voting).
The number below each data set identiﬁer
indicates the number of training examples.
Number of irrel.
KRK 5% attr.
BC. Wisconsin
attributes
is difﬁcult to test with data sets of reasonable size because such data sets tend to have literals
with high information gain and one needs exponentially many irrelevant attributes for an
irrelevant attribute to have higher information gain purely by chance for reasonably-sized
data sets. So, to test this hypothesis, we performed 100 trials with training sets of size 20.
In particular, we were interested to see if the exceptionally low error ratio obtained on the
wine data set could be made to increase with increasing numbers of irrelevant attributes.
Table 8 shows that for very small training set sizes, adding irrelevant attributes makes no
signiﬁcant difference to error ratios in 4 domains and increases the error for the wine data
set thus validating our hypothesis.
In summary, the answer to the question posed in this section (“How does increasing the
number of irrelevant attributes affect the amount of error reduction?”) is that error ratios
initially decrease as irrelevant attributes are added thus providing an opportunity for the
multiple models approach. However, beyond some point, adding irrelevant attributes will
begin to hurt the multiple models approach and error ratios will begin increasing towards
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Table 9. Effect of varying diversity on the tendency to make correlated errors (φe) and accuracy
(learning method: stochastic hill-climbing; evidence combination method: Uniform Voting).
Accuracies
1. In the limit, neither the single model approach or the multiple models approach will be
much use, and the error ratio will be 1.
Effect of diversity
In this section we explore the following question:
Does increasing the diversity of the models necessarily lead to greater reduction in
This question is motivated by the conclusions in Kwok & Carter in which they show
(on two domains) that ensembles consisting of syntactically more diverse decision trees are
able to achieve lower error rates than ensembles consisting of less diverse decision trees.
In this experiment, we modiﬁed the stochastic hill-climbing algorithm slightly by allowing
the user to specify a ﬁxed bucket size. Larger bucket sizes lead to ensembles whose members
are more syntactically diverse. We chose a variety of domains for this study: LED-8 and
KRK 100 are noise-free, Diabetes and Iris may contain class and attribute noise and the
Splice domain may contain classes which can be succintly described with “m of n” rules
Spackman, 1988).
Table 9 shows the accuracies obtained by combining eleven
stochastically generated models using the Uniform Voting evidence combination method.
Our hope is that increasing the bucket size will lead to an increase in ensemble accuracy.
However, as Table 9 shows, increasing the bucket size does not always lead to an increase
in ensemble accuracy. To achieve higher accuracy, the models should be diverse and each
model must be quite accurate. In fact, it is easy to produce uncorrelated errors by learning
less accurate models.
A more detailed examination of the results shows that many equally accurate models were
learned for the Iris, Diabetes and Splice domains by increasing the bucket size. But for the
noise-free, artiﬁcial concept data sets (Led-8 and “KRK 100”) increasing the bucket size
led to a few accurate models and many less accurate models. For LED and KRK, we know
the target deﬁnitions so we know that all the relevant attributes are presented to the learning
K. ALI AND M. PAZZANI
algorithm. Maybe for these data sets, all the very accurate models that can be learned
are syntactically similar so increasing syntactic diversity is not a good idea in this kind of
data set. This experiment suggests that although theory prescribes evidence combination
from all models in the model or hypothesis space , in practice only a small
number of models are learned and so it may be necessary to screen out less accurate models
in order to maximize overall accuracy.
To summarize, our experiments indicate that in order to minimize ensemble error, it is
necessary to balance increased diversity with competence - ensuring the diverse members
of the ensemble are all competent (accurate). The “hold-back” approach would seem to
be an obvious approach. However, for some of the small data sets presented here, using a
hold-back set may decrease accuracy since there would not be enough examples to learn
good models.
Previous work
Breiman provides a characterization of learning algorithms which are
amenable to the multiple models approach. He puts forward the notion of an “unstable”
algorithm - an algorithm for which small perturbations in the training set will lead to
signiﬁcant differences in predicted classiﬁcations on an independent (test) set of examples.
Breiman shows that decision-tree induction algorithms and neural-network algorithms are
unstable whereas the basic nearest-neighbor algorithm is not. This work differs from ours in
that we provide a characterization of domains for which the multiple models approach will
be beneﬁcial (many irrelevant attributes, low noise levels) whereas Breiman characterizes
the learning algorithm.
Schapire’s Boosting algorithm is the only learning algorithm which
explicitly attempts to learn models that make errors statistically independently. Boosting
learns from an on-line “stream” of examples. Subsequent models are constructed on training
sets that amplify the number of examples misclassiﬁed by earlier models. The idea is to
concentrate on the difﬁcult examples. However, Schapire’s method could not be used to
learn many models on the modest training set sizes used in this paper because the number of
training examples required rapidly increases as a function of the accuracy of earlier models.
Modiﬁed-boosting designed to work with small data sets has
not been proved empirically and may end up concentrating noisy examples in subsequent
training sets.
The only previous work involving learning relational multiple models has been done by Kovacic . Kovacic shows that learning
multiple models by running mFOIL several times using simulated annealing yields signiﬁcantly lower error rates than mFOIL on the KRK and Finite-element mesh
data sets.
Previous work related to the effect of noise and multiple models includes that of Kovacic
 and Gams . Our observation that error ratios asymptote to 1 as (class) noise
is added is consistent with results tabulated in and although
those authors did not explore the issue in detail as they did not attempt to explain the
variation in error reduction from one domain to another.
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Previous work on diversity and multiple models has been done by Kwok & Carter 
in which they showed that allowing the root of a decision tree to vary from model to model
produces more diverse and more accurate ensembles than if variation is only allowed further
down the tree. Our work builds on this by showing that in some situations one is forced to
trade-off diversity for accuracy - in such situations many syntactically-diverse and accurate
models may not exist. Buntine also presents results in which option trees are able
to achieve better error rates than ensembles of trees obtained by different way of pruning a
single initial tree. He postulates that this is because different prunings do not lead to trees
that are as diverse as those captured by the option-tree representation.
Conclusions
Our experiments conﬁrmed previous work that using multiple descriptions lowers the generalization error. Because our experiments used a large sample of data sets from the UCI
repository we were able to ﬁnd three data sets (not previously used in multiple models work)
for which the multiple models approach offers striking error ratios: 1/7 for wine, 1/5 for
tic-tac-toe and 1/2.5 for DNA.
However, multiple models work in ways different to those we had anticipated. In particular, they were better at reducing error on tasks which were already fairly accurate (reduced
error for Tic-tac-toe from 1% to 0.2%) than they were at reducing error on noisy domains.
Such noisy data sets may be called “data-limiting.” However, when the limiting factor is not
the noise or difﬁculty of the data, the multiple models approach provides an excellent way
of achieving large reductions in error. One situation in which this occurs is for data sets with
many irrelevant attributes. The information necessary to differentiate the classes is present
in the data but the deterministic hill-climbing learning algorithm may have difﬁculty ﬁnding
it. On such (“search-limiting”) data sets, the multiple models approach does increasingly
better than the single model as the number of irrelevant attributes is increased. We also
ﬁnd that the average number of gain ties experienced increases as the number of irrelevant
attributes increases. This conﬁrms our earlier results that the multiple models approach
does especially well when there are many gain ties. Beyond some point, however, adding
irrelevant attributes begins to hurt the multiple models approach. In the limit, neither the
single model approach or the multiple models approach will be much use, and the error
ratio will be 1.
We have shown that there is a substantial (linear) correlation between the amount of error
reduction due to the use of multiple models and the degree to which the errors made by
individual models are correlated. Therefore, we conclude that a major factor in explaining
the variance in error reduction is the tendency of the learned models to make correlated
errors. But why is it possible to learn models that do not make correlated errors for some
domains and not for others? Part of the answer is that it is possible to learn models that make
different kinds of errors for domains for which there are many ties in gain. To follow up on
this, we tried to increase the number of gain ties for each data set by adding 50 irrelevant
binary attributes to each data set. This increased the number of gain ties experienced
and also produced greater reduction in error suggesting that an abundance of gain ties is
K. ALI AND M. PAZZANI
a problem for the single model hill-climbing learning method but an opportunity for the
multiple models approach.
Acknowledgments
This work was supported by NSF grant #IRI-9310413. We also acknowledge three anonymous reviewers, Wray Buntine and Padhraic Smyth for helpful suggestions and Pedro
Domingos, Dennis Kibler and Dan Frost for reading preliminary versions of the paper.
Appendix 1
The appendix contains a table (Table 1.1) of accuracies for all four evidence combination
methods crossed with the two multiple model learning methods and the single model,
deterministic hill-climbing method.
Appendix 2
The posterior probability of a model, p(T|⃗x,⃗c), is computed as follows : Using Bayes’ rule, we can write:
p(T|⃗x,⃗c) ∝p(⃗x,⃗c|T) × p(T)
p(T) is the prior probability of the model T. By further assuming that the training examples
are independent given the model, we can write:
p(⃗x,⃗c|T) =
p(xi, ci|T)
where N denotes the size of the training set.
Following Buntine, we assume that we
can divide up the training set into subsets which correspond to different types of training
examples (these can be different disjuncts or in Buntine’s case, different leaves of a decision
tree). Let there be V such subsets and let nj,k denote the number of training examples of
class j in the k-th subset. Then we can write
p(⃗x,⃗c|T) =
where φj,k represents the probability of generating a single example of class j in the k-th
subset and C denotes the number of classes. One can then show that the
contribution to the posterior from the k-th subset can be modeled by:
BC(n1,k + α, . . . , nC,k + α)
BC(α, . . . , α)
ERROR REDUCTION THROUGH LEARNING MULTIPLE DESCRIPTIONS
Table 1.1. The table below presents a comparison of methods of generating models and of evidence combination
methods. Each model generation method is represented by four columns corresponding to the evidence combination methods. They are, in order: Uniform Voting (U), Bayesian Combination (B), Distribution Summation (D)
and Likelihood Combination (L). ‘+’ indicates a signiﬁcant (95% conﬁdence) increase in accuracy as compared
to the single model method; ‘-’ indicates a signiﬁcant decline.
Deterministic, single model
Stochastic
k-fold partition
Hill-climbing
Hill-climbing
KRK 160e 5a
KRK 320e 5a
KRK 160e 20c 88.6
KRK 320e 20c 91.7
hypothyroid
where BC is the C-dimensional beta function and α is a parameter which denotes the
“weight” (in number of examples) that should be associated with the prior estimate (1/C)
of φj,k: Putting equations 2.3 and 2.4 together, we get:
p(⃗x,⃗c|T) =
BC(n1,k + α, . . . , nC,k + α)
BC(α, . . . , α)
Since, p(⃗x,⃗c|T) can be computed, then using Equation 2.1, the posterior probability,
p(T|⃗x,⃗c) can be calculated, so (using Equation 3) the ﬁnal quantity of interest, the expected posterior probability, can be calculated.
The foregoing discussion is enough to calculate posterior probabilities of models that are
decision trees. It depends on the observation that the training examples can be partitioned
into V disjoint subsets. We adapt it for the types of models considered in this paper in which
K. ALI AND M. PAZZANI
a separate description is learned for each class by observing that such a model partitions
the training examples C (number of classes) times. This is because each class description
partitions all the training examples since each description contains a “default rule” - one
whose body is the literal true. Then in order to compute the posterior probability of such a
model, we simply take the geometric average of the posterior probabilities of all the class
descriptions:
p(T|⃗x,⃗c) ∝p(T) ×
B(n1,ij + α, n2,ij + α)
Ri denotes the i-th class description in model T and ij indexes individual rules. Since,
within the class description for the i-th class, classes are grouped into two pseudo-classes
(class i is called the “positive” class, all the other classes are combined into the “negative”
class), we can use k = 2 in Equation 2.4 to obtain the Beta function terms in Equation 2.6.
1. Actually, we use a form of clause that is an extension of a Horn clause since we allow negated literals in the
body of clauses and Horn clauses do not.
2. Although FOIL is an algorithm that learns class descriptions consisting of relational (ﬁrst-order) clauses, in
this paper we are not concerned with issues pertaining to relational learning or inductive logic programming
 . We present results on the interaction of inductive logic programming and
learning multiple models in .
3. Ali & Pazzani presents details on how to deal with recursive concepts in the “single, most reliable
rule” framework.
4. King-Rook-King is a fully determinate domain so it can be converted into attribute-value form as is done by
Lavrac & Dzeroski . However, in this paper, that knowledge is not utilized by the learning programs
so the domain has to be treated as a relational domain. FOIL and HYDRA can also run on non-determinate
5. x% class noise means that the class assignments of x% of the examples were randomly reassigned - for a two
class problem, this means x
2 % of the examples will bear incorrect class labels.
6. The r2’s between Er and φe without the signiﬁcant error reduction restriction are: 50.7% (Uniform), 33.7%
(Bayes), 6.8% (Distribution) and 31.6% (Likelihood). The Mushroom data set causes a problem for the
Distribution combination strategy because both the ensemble error and multiple models error are close to 0 so
the ratio cannot be reliably estimated. The r2 for Distribution increases to 21.1% without Mushroom.