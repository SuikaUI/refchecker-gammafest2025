Temporal Information Extraction
Xiao Ling and Daniel S. Weld
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350, U.S.A.
{xiaoling, weld}@cs.washington.edu
Research on information extraction (IE) seeks to distill relational tuples from natural language text, such as the contents of the WWW. Most IE work has focussed on identifying static facts, encoding them as binary relations. This
is unfortunate, because the vast majority of facts are ﬂuents,
only holding true during an interval of time. It is less helpful
to extract PresidentOf(Bill-Clinton, USA) without the temporal scope 1/20/93 - 1/20/01. This paper
presents TIE, a novel, information-extraction system, which
distills facts from text while inducing as much temporal information as possible. In addition to recognizing temporal
relations between times and events, TIE performs global inference, enforcing transitivity to bound the start and ending
times for each event. We introduce the notion of temporal entropy as a way to evaluate the performance of temporal IE systems and present experiments showing that TIE outperforms
three alternative approaches.
Introduction
Information extraction (IE), the problem of extracting relational data from unstructured text, is gaining increased
attention by researchers who seek to distill knowledge
for the vast corpus of natural-language content on the
WWW. Bootstrapped pattern learners , supervised learning , human-engineered
rules ,
selfsupervised probabilistic sequential models , and numerous other approaches have been used to extract
facts from text. On reﬂection, however, almost all research
on information extraction has focused on the acquistion of
static (time invariant) facts.
In the cases where ﬂuents1
were extracted, e.g., employed-by(mark-craven,
carnegie-mellon-univ), temporal arguments were
neglected.
The sparsity of research on temporal extraction is surprising, since so many statements are temporally
qualiﬁed. In particular, sources such as newswire text or
Wikipedia are predominantly temporal.
Copyright c⃝2010, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
1Fluents are dynamic relations whose truth is a function of
time .
We don’t mean to suggest that scientists have neglected
the challenge of understanding temporal expressions in natural language; indeed, the literature is vast. However, most
research on the topic has focused on subproblems rather than
the complete task of temporal IE. While we discuss related
work more fully in the next section, we now highlight the
2007 TempEval challenge , which
asked researchers to identify event-time and event-event relations using a restricted set of Allen-style interval comparitors.
While TempEval has greatly spurred
research on temporal NLP, progress on its challenge does
not necessarily lead directly to gains on the broader task of
temporal IE due to several of TempEval’s simplifying assumptions. First, the TempEval corpus provides researchers
with gold-standard annotations of all relevant events, temporal expressions and document creation times. Secondly,
the three temporal relations used in TempEval (BEFORE,
AFTER and OVERLAP) are insufﬁcient to precisely bound
the start and ending points of the events (i.e. OVERLAP is
ambiguous) — as would be required, for example, to create
a comprehensive timeline of events.
Temporal Information Extraction
In contrast, we focus on the following problem. Given a corpus of natural
language text, T, output a set of temporal elements E and
temporal constraints C subject to the following conditions.
Every element in E should denote an event or a time. We
use the notion of temporal element to unify the notion of
times and events — indeed, one may view a temporal reference in the same way as an event, denoting an
interval of time.2 For every element, e ∈E, we refer to its
beginning and ending time points, e and e, respectively.
The constraints in C are linear inequalities between time
points.3 For example, the sentence “Steve Jobs revealed the
iPhone in 2007.” might produce the following constraints:
Year-2007 ≤Reveal(Jobs, iPhone)
Reveal(Jobs, iPhone) ≤Year-2007
At present, we restrict our attention to reasoning within one
sentence at a time. Our objective is to output a maximal set
2As has been previously observed, differing textual contexts
lead to varying temporal granularity. In one context an event may
appear atomic, while others discuss the event’s substructure.
3The next section explains why we use a metric, point-based
representation rather than Allen’s interval algebra.
Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)
of events and the tightest set of temporal constraints which
are directly implied by the text. As explained in the penultimate section (Experimental Results) we evaluate temporal
extraction systems on both precision (correctness of temporal constraints) and recall (number of said events with reasonable bounds on their starting and ending points).
The TIE System:
After an ofﬂine learning process,
which uses TimeBank data to
train TIE’s probabilistic model to recognize temporal relations, TIE processes each natural language sentence in two
sequential phases. First, TIE extracts events and identiﬁes
temporal expressions, applies a syntactic parser and a semantic role labeler to create a set of features. Secondly,
TIE uses inference over its probabilistic model to locate inequality relations between the endpoints of the extracted elements. In summary, this paper makes the following contributions.
• We describe TIE, a novel temporal information extractor, which recogizes a wide range of temporal expressions
and runs probabilistic inference to extract point-wise constraints on the endpoints of event-intervals by making full
use of transitivity.
• We introduce the notion of temporal entropy as a means
for visualizing the recall of a temporal extractor along
with the tightness of the induced temporal bounds.
• We present experiments comparing TIE to three other temporal extractors, demonstrating that TIE has slightly increased precision and substantially improved recall.
Previous Work
In this section, we discuss the previous work on representations of time, methods for identifying temporal relations and
existing systems for temporal information extraction.
Temporal Representation
Since Allen ﬁrst proposed the interval-based algebra for representing time in natural language , it has become the standard representation. However, Allen’s only
argument against a real-valued point-based representation
is that having two intervals seamlessly meet requires using an “unintuitive” half-open/half-closed interval representation. On the other hand, we observe that a point approach
has several beneﬁts: 1) Reals are much simpler, only requiring standard inequalities rather than Allen’s 11 interval
relations, 2) In cases where quantitative interval durations
are known (e.g., “the war started 2 days after the incident”)
arithmetic ﬁts naturally into the framework, and 3) Reasoning in Allen’s framework is intractable whereas fast linear-programming solvers (e.g., Simplex phase 1) may be used with linear inequalties.
TimeML is a notable markup language, based roughly on Allen’s relations, which is capable of annotating text to mark events, times and the links
between them; an example is shown in box 2 of Figure 1.
TARSQI is useful in this
regard, automatically annotating events and times.4 Time-
Bank 1.2 , a popular corpus of
hand-annotated news articles, uses TimeML and is popular
in recent research . The 2007 TempEval challenge used training and test
sets encoded in a subset of TimeML, which was restricted
to three interval relations; entrants competed on three subtasks, each identifying a subset of temporal relations: A)
between a speciﬁc event/time pair in the same sentence, B)
between a speciﬁed event and the Document Creation Time,
C) between the main events (provided) in two adjacent sentences. While the TempEval challenge successfully identiﬁed the strengths and weaknesses of alternative approachs
to the tasks and spurred new research ideas, its three-relation
representation is too simple for full temporal extraction. For
example, given “A OVERLAP B” one can conclude only
that A ∩B ̸= ∅, but nothing about the relation between A
and B or A and B.
Identifying Temporal Relations
Regardless of speciﬁc details of the target temporal relations, most researchers have approached the problem of
temporal identiﬁcation (event/event or event/time) by using supervised learning of a classiﬁer , but
this sacriﬁes recall by ignoring interactions due to transitivity.
Yoshikawa et al. are an
exception; they use probabilistic inference to jointly solve
all three TempEval tasks. Our work differs in two important ways.
First, Yoshikawa’s model requires Document
Creation Times to enable transitivity reasoning and DCTs
are unavailable for many sources on the Web, including the
Wikipedia articles which are our main focus.
there is a subtle question about how much probabilistic inference a temporal extraction algorithm should do. Suppose
someone asked you if the Ananji Revolution happened before 2000 and you had never heard of the event. Answering
“Yes” would likely give you a good score on a TempEvalstyle task, because, statistically, most revolutions in history
did happen before 2000. However, we seek to build an extraction system which uncovers all the relations most likely
directly implied by the text not to predict the most probable
relations.
Full Temporal Information Extraction
Many previous temporal information extraction systems
associated
stamps ; this is a simple and practical approach, but
sacriﬁes recall, because many events don’t have an associated timestamp. By additionally considering event/event relations and applying transitivity one may derive a great many
4TARSQI is a set of tools whose functionalities include annotating events (Evita), grounding temporal expressions (GUTime),
generating temporal linkings (Blinker, S2T), etc.When we use
the name “TARSQI”, we mean the temporal linking subparts of
Figure 1: Diagram showing the architecture of TIE
more constraints on the times of events. In addition to transitivity reasoning, TIE extracts a wide range of events, speciﬁed using both verb phrases and dated noun phrases . Semantic Role Labeling systems often extract some temporal information (e.g.
via the AM-TMP argument), although they don’t typically
reason about transitivity; we compare with Koomen’s system in our experiments. Schockaert
et al. describe a system that takes a set of named target events as input, retrieves
thousands of matching Web pages, and uses simple patterns
to extract possible dates; fuzzy temporal reasoning is used to
reconcile these extractions. Temporal reasoning is also required when summarizing a set of news stories, e.g. , but typically it is sentences that are ordered, not events. Bramsen et al. describe an algorithm for globally ordering
“temporal segments” in medical case summaries.
Temporal Information Extraction
Before describing the TIE algorithm, we state the problem
at hand. Given a sequence of natural language sentences, T,
we seek to output a set of temporal elements E and temporal
constraints C. Every element e ∈E should denote an event
or a temporal reference . The constraints in C are linear inequalities of the
form p1 ≤p2 + d, where d is a duration, often zero, and
p1 and p2 denotes either a beginning (e) or ending time
point (e) of a temporal element, e. Our objective is to output a maximal set of events and the tightest set of temporal
constraints which are directly implied by the text.
System Overview
Figure 1 summarizes the overall operation of TIE. The current TIE implementation considers each sentence Si ∈T in
isolation, performing the following pipeline of operations.
1. Preprocessing: Parse Si using a syntactic parser ). Detect semantic roles for verbs in the Si
by SRL . Use Evita and GUTime
from TARSQI to ﬁnd all temporal
events and times {e1, . . . , en} in the sentence. Generate
descriptional and syntactic features for each element ei as
well as between elements.
2. Classiﬁcation: Using a pretrained probabilistic model
(learned from TimeBank data) combined with transitivity rules, classify each pair of points (pi, pj) of elements
the point-wise relation.
Preprocessing & Feature Assignment
We start by using Evita and GUTime 
to identify the event and the time expressions in each sentence, as shown in Figure 1 box 2. We use the Stanford
parser to
create a dependency parse of the sentence (box 3). We also
use a semantic role labeling system to
locate the temporal arguments of verbs (box 4). At the end
of preprocessing, we have generated the following features:
• Event and Time Attributes: In TimeML , recognized events and times are associated
with the attributes showing their important aspects such
as tense for verb events, grounded time values etc.
• Dependency Features: Besides the features about each
individual temporal element, it is also crucial to have good
features for element pairs, (xi, xj). We observe from experience that most syntactic dependencies strongly indicate temporal relations. For example, in the sentence
Australia has been independent since 1901.
the parser would output
prep since .
The dependency prep since (one of about 80 tokens produced by De Marneffe et al.’s parser) indicates that independence happens at some point in 1901 and continues to
be true afterwards. TIE parses the text of each sentence to
get the syntactic parse tree and the sentence’s dependencies (Figure 1 box 3). Each dependency dep(w1, w2) is
considered in turn. If w1 and w2 are parts of the textual
expressions xi and xj, respectively, then TIE constructs a
feature dep(xi, xj) to capture the relation between xi and
xj. Statistically, these features are useful when predicting
the temporal ordering between xi and xj.
If an event e has no dependencies to other elements, TIE
creates a feature called proximity(e, x) where x is the
nearest element in the parse tree. This feature avoids the
situation where e cannot be linked to any element at all.
• SRL Features: TIE considers the AM-TMP argument (if
any) by SRL for each verb identi-
ﬁed as an event. A set of MLN rules interpret the argument by recognizing the initial preposition. For example,
the argument starting with “before” suggests that the verb
happens before the time in the AM-TMP argument.
Identifying Temporal Relations
To identify the ordering relation for each pair of elements,
we use Markov Logic Networks (MLN) to make predictions. Intuitively, Markov
Logic is a probabilistic extension of ﬁrst-order logic. Formally, an MLN is a set of weighted ﬁrst-order formulae.
Given a set of constants, an MLN can be instantiated into a
ground Markov network where each node is an atom. Each
formula represents a feature in the grounded Markov network with the corresponding weight. The probability of an
assignment x is P(x) =
i wini(x)) where Z is
the normalization constant, wi is the weight of the ith formula and ni(x) is the number of satisﬁed groundings for the
ith formula. MLN is a ﬂexible way to incorporate human
knowledge, since they allow using different combinations
of features in a straight-forward manner by setting different
formula templates and then learning the weights from the
training data. We use Alchemy as the implementation of
MLN . Due to the space limit, readers interested in details may refer to (Richardson and Domingos
Figure 1 box 0 illustrates how TIE’s MLN formulae ﬁt in
the overall architecture. The box has a dashed outline to signify that the formula weights are learned in an ofﬂine manner (as we describe below). We use the following formula
templates for relation classiﬁcation:
dep(x, y) →after(point(x), point(y))
srl after(p, q) →after(p, q)
after(p, q) ∧after(q, r) →after(p, q)
where x, y are elements and p, q, r are points The ﬁrst formula models the inﬂuence of each syntactic dependency on
the temporal relation between the arguments; this formula is
actually second-order, because dep stands for proximity or
any of the ∼80 Stanford dependencies; point denotes either
the starting or ending point of the element. The second formula integrates temporal information provided by the SRL
system. The last MLN rule decreases the probability of interpretations which are inconsistent with transitivity.
The weights for these formulae are learned
from a selected portion of the TimeBank data set.5 Each
Allen-style relation is translated into temporal constraints in
terms of its beginning and ending points (e.g. Figure 1 box
5). We ﬁlter the data by selecting the relations whose arguments reside in the same sentence, since TimeBank also has
labels for inter-sentence pairs of events.
For each sentence, we run inference using
MC-SAT with default param-
5In practice, we manually gave positive weights to the SRL and
transitivity rules to save training time. We also impose a negative
prior to the after predicate to suppress predictions unless there is
substantial evidence.
eters and return marginal probabilities for the relations of all
possible pairs. All temporal relations whose probability is
over threshold are returned as TIE’s predications. Varying
the threshold moves TIE along the precision/recall curve.
Experimentation
This section addresses following questions: 1) How accurately can TIE perform relation classiﬁcation? 2) How well
can it bound the times of events? and 3) What factors are
important to TIE’s performance?
Data Set The data for our experiments6 were collected
from Wikipedia articles7. We selected the four representative categories of articles (the number of articles is in
the brackets): Warfare(63), Universities(59), Celebrities(99)
and Riots(262).
Summing over all articles yields a total
40640 sentences. We randomly sampled 45 sentences for
hand-labeling prior to testing. Within this test set, there are
151 events, 56 times identiﬁed by GUTime and Evita and
therefore 644 potential constraints between one point of an
event and the other point of a time for testing.
To get ground truth, we asked people to label the temporal relations between all pairs of elements {(xi, xj)} by
comparing their start and ending points. Each pair was labeled by at least two people. If the assigned labels did not
agree, a third person resolved the disagreement.
Methods We present two evalution metrics. One is the familiar precision and recall. With the gold constraints, these
scores are computed by comparing with the constraints each
system outputs. Precision is computed as P = P
where ci is the number of correctly predicted constraints for
ith sentence, pi is the number of predictions and the summation is over all test data.
Note, however, that recall is an inadquate metric for temporal IE, since it records only whether an ordering relation
has been extracted, but doesn’t measure how tightly an event
is bounded in time. To compare the degree to which different
systems constrain the time of events, we introduce a novel
“recall-like” measure, Temporal Entropy (TE), for evaluating the number and “tightness” of induced temporal bounds.
When a human comprehends a piece of text, she can give
time bounds to the starting and end points of the events in
the text. We take the logarithm (base 10) of the length of
the tightest bound on a time point as its TE. Similarly, we
may compute and compare the logarithm of system-derived
bounds for the endpoints of each event. If a system fails to
recognize an event as such, we assign it a maximum entropy
equal to the log of the maximal temporal duration plus two.8
We visualize TE on the y axis of a graph where the x axis
represents different time points, sorted so that those with the
least entropy have the lowest index.
Extraction Algorithms Compared
We implemented
three additional extractors for comparison with TIE. The
6available at 
7Dump as of 2009/07/07.
8The addition of “two” is arbitrary; we choose this value for the
plots in order to graphically separate the points where a system ﬁnd
no bounds (the gray area in the upper part of Figure 3 & 4) from
events where the system bounded weakly.
ﬁrst (PASCA) is a pattern-based extraction system for question answering, which we reimplemented following . It exploits four lexico-syntactic patterns to ﬁnd the
temporal expressions, t, in each sentence, such as sentences
that start or end with a simple adverbial phrase containing a
date. Associated events, e, were assigned to the major verb.
By inspecting the patterns, we arrived at the appropriate constraints: t < e < e < t.
The second baseline is the SRL system described in the previous section. We use TARSQI
as the third system with point-based constraints translated
from the Allen-style relations.
For fairness, we performed
constraint propagation for all three
systems to get the ﬁnal results.
Figure 2: Precision-Recall curve.
Figure 3: Temporal Entropy. The curve of TIE is the closest
to the gold curve (HUMAN).
Results Figure 2 depicts the precision-recall curve of TIE
by enumerating its probability threshold from 0.50 to 0.99.
As seen from the curve, TIE is capable of very accurate extractions (the upper left corner of the plot). Compared to
PASCA, TIE extracts more correct constraints at comparable precision. However, we note that Pasca’s system runs
much more quickly, taking only few milliseconds per sentence, while TIE takes a few seconds depending on the number of temporal elements in the sentence. TIE is comparable
with SRL at 0.95 precision. Also, without sacriﬁcing much
precision, TIE is able to triple recall.
In Figure 39, the temporal entropies are ordered increasingly. A lower curve means reduced entropy and better per-
9We chose 0.5 as the threshold for displaying the temporal entropy plot throughout the experiments.
formance. TIE (the dashed line) is the closest to a human’s
Figure 4: Temporal Entropy. Ablation Test.
Ablation Test To understand what factors are crucial to
TIE’s performance, we removed some features from the system. The results are shown in Figure 2 and Figure 4. First,
in order to see the importance of SRL features in a Temporal IE system. We removed the SRL features in TIE-srl.
Secondly, we are also interested in investigating how much
the transitivity helps bound the events. To achieve that goal,
we removed the transitivity rule from our model (TIE-trans).
TIE-trans-srl omits both features. Let us take a look at the
temporal entropy plot (Figure 4) in two orthogonal directions. Horizontally, we can see how many points the system
is able to bound, i.e. recall. Vertically, the area between the
curves tells the difference in tightness of temporal bounds;
in other words, the lower the curve is, the tighter bounds
the system gives. In the experiments we observed that SRL
features improve precision, and complementarily, the transitivity rule helps increase the recall.
TempEval Task A Although TIE was not designed to perform the TempEval tasks, does not use information about
Document Creation Times (DCTs), and produces more
granular results, we report on it’s performance for completeness. TIE’s point-wise constraints from TIE’s output are converted back to Allen-style relations. The data set for testing is further restricted to {BEFORE, AFTER, OVERLAP}
(from 169 to 148). We see that TIE’s accuracy is 0.695,
which is comparable to 0.716 by the state-of-the-art algorithm 10.
Conclusion
In this paper, we describe TIE, a novel temporal information
extractor, that uses probabilistic inference to extract pointwise constraints on the endpoints of event-intervals by taking advantage of transitivity. Secondly, we introduce the notion of temporal entropy as a means for visualizing the recall of a temporal extractor along with the tightness of the
induced temporal bounds. Third, we present experiments
comparing TIE to three other temporal extractors, demonstrating that TIE outperforms other systems. In the future
we hope to extend TIE in several ways, including incorporation of inter-sentence event coreference and point-wise constraints with more than two arguments.
10The accuracy is adjusted to the smaller test set.
Acknowledgements We thank Oren Etzioni, Pedro Domingos,
Mausam, Raphael Hoffman, Hoifung Poon, Jesse Davis, Chloe
Kiddon, Fei Wu, Katsumasa Yoshikawa and all members of the
KnowItAll group for helpful discussions. We also thank the anonymous reviewers for valuable comments. This work is supported
the WRF / TJ Cable Professorship, a gift from Google and by
the Air Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, ﬁndings, and conclusion
or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the view of the AFRL.