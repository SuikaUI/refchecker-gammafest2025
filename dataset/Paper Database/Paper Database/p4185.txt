HAL Id: hal-00788440
 
Submitted on 14 Feb 2013
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Active Learning of Inverse Models with Intrinsically
Motivated Goal Exploration in Robots
Adrien Baranes, Pierre-Yves Oudeyer
To cite this version:
Adrien Baranes, Pierre-Yves Oudeyer.
Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots.
Robotics and Autonomous Systems, 2013, 61 (1), pp.69-73.
￿10.1016/j.robot.2012.05.008￿. ￿hal-00788440￿
Baranes, A., Oudeyer, P-Y. Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in
Robots, Robotics and Autonomous Systems.
Highlights:
1) SAGG-RIAC is an architecture for active learning of
inverse models in high-dimensional redundant spaces
2) This allows a robot to learn eﬃciently distributions of
parameterized motor policies that solve a corresponding
distribution of parameterized tasks
3) Active sampling of parameterized tasks, called active
goal exploration, can be signiﬁcantly faster than direct active
sampling of parameterized policies
4) Active developmental exploration, based on competence
progress, autonomously drives the system to progressively
explore tasks of increasing learning complexity.
Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in
Adrien Baranes and Pierre-Yves Oudeyer
INRIA and Ensta-ParisTech, France
We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots.
This allows a robot to eﬃciently and actively learn distributions of parameterized motor skills/policies that solve a corresponding
distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task
space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer
the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences
between policy and task parameters.
We present experiments with high-dimensional continuous sensorimotor spaces in three diﬀerent robotic setups: 1) learning the
inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped
robot, 3) an arm learning to control a ﬁshing rod with a ﬂexible wire. We show that 1) exploration in the task space can be a
lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing
competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity
and is statistically signiﬁcantly more eﬃcient than selecting tasks randomly, as well as more eﬃcient than diﬀerent standard active
motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach
and which part it cannot.
Active Learning, Competence Based Intrinsic Motivation, Curiosity-Driven Task Space Exploration, Inverse Models, Goal
Babbling, Autonomous Motor Learning, Developmental Robotics, Motor Development.
1. Motor Learning and Exploration of Forward and Inverse Models
To operate robustly and adaptively in the real world, robots
need to know how to predict the consequences of their actions
(called here forward models, mapping typically X = (S, πθ),
where S is the state of a robot and πθ : S →A is a parameterized action policy, to the space of eﬀect, or task space, Y).
Reversely, they need to be able to compute the action policies that can generate given eﬀects (called here inverse models,
(S, Y) →πθ). These models can be quite varied, for example
mapping joint angles to hand position in the visual ﬁeld, oscillation of the legs to body translation, movement of the hand in
the visual ﬁeld to movement of the end point of a tool, or properties of a hand tap an object to the sound it produces. Some of
these models can be analytically elaborated by an engineer and
provided to a robot (e.g. forward and inverse kinematics of a
rigid body robot). But in many cases, this is impossible either
because the physical properties of the body itself cannot be easily modeled (e.g. compliant bodies with soft materials), or because it is impossible to anticipate all possible objects the robot
might interact with, and thus the properties of objects. More
generally, it is impossible to model a priori all the possible effects a robot can produce on its environment, especially when
robots are targeted to interact with in everyday human environments, such as in assistive robotics. As a consequence, learning these models through experience becomes necessary. Yet,
this poses highly diﬃcult technical challenges, due in particular
to the combination of the following facts: 1) these models are
often high-dimensional, continuous and highly non-stationary
spatially, and sometimes temporally; 2) learning examples have
to be collected autonomously and incrementally by robots; 3)
learning, as we will detail below, can happen either through
self-experimentation or observation, and both of these takes signiﬁcant physical time in the real world. Thus, the number of
training examples that can be collected in a life-time is strongly
limited with regards to the size and complexity of the spaces.
Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently,
such as . Yet, these regression mechanisms are eﬃcient
only if the quality and quantity of data is high enough, which
is not the case when using unconstrained exploration such as
random exploration. Fundamental complementary mechanisms
for guiding and constraining autonomous exploration and data
 
June 20, 2012
collection for learning are needed.
In this article, we present a particular approach to address constrained exploration and learning of inverse models in
robots, based on an active learning process inspired by mechanisms of intrinsically motivated learning and exploration in humans. As we will explain, the approach studies the combination
of two principles for learning eﬃciently inverse models in highdimensional redundant continuous spaces:
• Active goal/task exploration in a parameterized task
space: The architecture makes the robot sample actively
novel parameterized tasks in the task space, each of which
triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. This allows to leverage the redundancies of the sensorimotor mapping, leading the system to explore densely only subregions of the
space of action policies that are enough to achieve all possible eﬀects. Thus, it does not need to learn a complete
forward model and contrasts with approaches that directly
sample action policy parameters and observe their eﬀects
in the task space. The system also leverages regression
techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task,
and based on the previously learnt correspondences between policy and task parameters.
• Interestingness as empirically evaluated competence
The measure of interestingness for a given
goal/task is based on competence progress empirically
evaluated, i.e. how previous attempts of low-level optimization directed at similar goals allowed to improve the
capability of the robot to reach these goals.
In the rest of the section, we review various related approaches to constraining exploration for motor learning.
1.1. Constraining the Exploration
A common way to carry out exploration is to use a set of
constraints on guiding mechanisms and maximally reduce the
size and/or dimensionality of explored spaces.
Social guidance is an important source of such constraints, widely studied
in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process . Typically, a robot teacher manually
interacts with the robot by showing it a few behaviors corresponding to a desired movement or goal that it will then have
to reproduce. This strategy prevents the robot from performing
any autonomous exploration of its space and requires an attentive demonstrator. Some other techniques allow more freedom
to the human teacher and the robot by allowing the robot to
explore. This is typically what happens in the reinforcement
learning (RL) framework where no demonstration is originally
required and only a goal has to be ﬁxed (as a reward) by the
engineer who conceives the system . Nevertheless,
when the robot evolves in high-dimensional and large spaces,
the exploration still has to be constrained. For instance, studies
presented in combine RL with the framework of learning
by demonstration. In their experiments, an engineer has to ﬁrst
deﬁne a speciﬁc goal in a task space as a handcrafted reward
function, then, a human demonstrator provides a few examples of successful motor policies to reach that goal, which is
then used to initialize an optimization procedure. The Shifting
Setpoint Algorithm (SSA) introduced by Schaal and Atkeson
 proposes another way to constrain the exploration process.
Once a goal ﬁxed in an handcrafted manner, a progressive exploration process is proposed: the system explores the world
gradually from the start position and toward the goal by creating a local model around the current position and shifting in
direction of the goal once this model is reliable enough, and so
on. These kinds of techniques therefore restrain the exploration
to narrow tubes of data targeted at learning speciﬁc tasks/goals
that have to be deﬁned by a human, either the programmer or a
non-engineer demonstrator.
These methods are eﬃcient and useful in many cases. Nevertheless, in a framework where one would like a robot to learn a
variety of tasks inside unprepared spaces like in developmental
robotics , or more simply full inverse models
(i.e. having a robot learn to generate in a controlled manner
many eﬀects rather than only a single goal), it is not conceivable that a human being interacts with a robot at each instant or
that an engineer designs and tunes a speciﬁc reward function for
each novel task to be learned. For this reason, it is necessary to
introduce mechanisms driving the learning and exploration of
robots in an autonomous manner.
1.2. Driving Autonomous Exploration
Active learning algorithms can be considered as organized
and constrained self-exploration processes .
In the regression setting, they are used to learn a regression
mapping between an input space X and an output space Y
while minimizing the sample complexity, i.e.
with a minimal number of examples necessary to reach a given performance level. These methods, typically beginning by random
and sparse exploration, build meta-models of performances of
the motor learning mechanisms and concurrently drive the exploration in various sub-spaces for which a notion of interest is
deﬁned, often consisting in variants of expected informational
gain. A large diversity of criteria can be used to evaluate the
utility of given sampling candidates, such as the maximization
of prediction errors , the local density of already queried
points , the maximization of the decrease of global model
variance , expected improvement , or maximal uncertainty of the model among others. There have been activeextensions to most of the existent learning methods, e.g. logistic regression , support vector machines , gaussian processes . Only very recently have these approaches
been applied to robotic problems, and even more recently if we
consider examples with real robots. Nevertheless examples that
consider robotic problems already exist for a large variety of
problems: building environment maps , reinforcement
learning , body schema learning , imitation ,
exploration of objects and body properties , manipulation
 , among many others.
Another approach to exploration came from an initially different problem, that of understanding how robots could achieve
cumulative and open-ended learning autonomously. This raised
the question of the task-independent mechanisms that may allow a robot to get interested in practicing skills and learn new
tasks that were not speciﬁed at design time.
Two communities of researchers, the ﬁrst one in reinforcement learning
 , the second one in developmental robotics
 , formalized, implemented and experimented
several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into
theories of motivation, spontaneous exploration, free play and
development in humans as well as in recent ﬁndings
in the neuroscience of motivation .
As argumented in , architectures based on
intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for
the self-organized formation of behavioral and developmental
complexity, can also also allow an agent to eﬃciently learn a
model of the world by parsimoniously designing its own experiments/queries. Yet, in spite of these similarities between work
in active learning and intrinsic motivation, these two strands
of approaches often diﬀer in their underlying assumptions and
constraints, leading to sometimes very diﬀerent active learning algorithms. In many active learning models, one often assumes that it is possible to learn a model of the complete world
within lifetime, and/or that the world is learnable everywhere,
and/or where noise is homogeneous everywhere. Given those
assumptions, heuristics based on the exploration of parts of the
space where the learned model has maximal uncertainties or
where its prediction are maximally wrong are often very ef-
ﬁcient. Yet, these assumptions typically do not hold in real
world robots in an unconstrained environment: the sensorimotor spaces, including the body dynamics and its interactions
with the external world, are simply much too large to be learned
entirely in a life time; there are typically subspaces which are
unlearnable due to inadequate learning biases or unobservable
variables; noise can be strongly homogeneous. Thus, diﬀerent authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become ineﬃcient in situations that are common in open-ended robotic
environments . This is the reason why new
active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept
of intrinsic motivations which relate to mechanisms that drive a learning agent to perform diﬀerent activities for their own sake, without requiring any external reward
 . Diﬀerent criteria were elaborated, such as the search for maximal reduction
in empirically evaluated prediction error, maximal compression
progress, or maximal competence progress . For instance, the architecture called Robust-Intelligent Adaptive Curiosity (RIAC) , which is a reﬁnement of the IAC architecture which was elaborated for open-ended learning of aﬀordances and skills in real robots , deﬁnes the interestingness
of a sensorimotor subspace by the velocity of the decrease of
the errors made by the robot when predicting the consequences
of its actions, given a context, within this subspace. As shown
in , it biases the system to explore subspaces of progressively increasing complexity.
Nevertheless, RIAC and similar ”knowledge based” approaches (see ) have some limitations: ﬁrst, while they can
deal with the spatial or temporal non-stationarity of the model
to be learned, they face the curse-of-dimensionality and can
only be eﬃcient when considering a moderate number of control dimensions (e.g. up to 9/10). Indeed, as many other active
learning methods, RIAC needs a certain level of sampling density in order to extract and compare the interest of diﬀerent areas of the space. Also, because performing these measure costs
time, this approach becomes more and more ineﬃcient as the
dimensionality of the control space grows . Second, they
focus on the active choice of motor commands and measures of
their consequences, which allows learning forward models that
can be re-used as a side eﬀect for achieving goals/tasks through
online inversion: this approach is sub-optimal in many cases
since it explores in the high-dimensional space of motor commands and consider the achievement of tasks only indirectly.
A more eﬃcient approach consists in directly actively exploring task spaces, which are also often much lower-dimensional,
by actively self-generating goals within those task spaces, and
then learn associated local coupled forward/inverse models that
are useful to achieve those goals. Yet, as we will see, the process is not as straightforward as learning the forward model,
since because of the space redundancy it is not possible to learn
directly the inverse model (and this is the reason why learning
the forward model and then only inversing it has often been
achieved). In fact, exploring the task space will be used to learn
a sub-part of the forward model that is enough for reaching
most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on
previously learnt correspondences, such as in .
1.3. Driving the Exploration at a Higher Level
In a framework where a system should be able to learn to
perform a maximal amount of diﬀerent tasks (here this means
achieving many goals/tasks in a parameterized task space) before focusing on diﬀerent ways to perform the same tasks (here
this means ﬁnding several alternative actions to achieve the
same goal), knowledge-based exploration techniques like RIAC
cannot be eﬃcient in robots with redundant forward models.
Indeed, they typically direct a robotic system to spend copious amounts of time exploring variations of action policies that
produce the same eﬀect, at the disadvantage of exploring other
actions that might produce diﬀerent outcomes, useful to achieve
more tasks. An example of this is learning 10 ways to push a
ball forward instead of learning to push a ball in 10 diﬀerent
directions. One way to address this issue is to take inspiration
infant’s motor exploration/babbling behavior, which has been
argued to be teleological via introducing goals explicitly inside
a task space and driving exploration at the level of these goals
 . Once a goal/task is chosen, the system would
then try to reach it with a lower-level goal-reaching architecture typically based on coupled inverse and forward models,
which might include a lower-level goal-directed active exploration mechanism.
Two other developmental constraints, playing an important
role in infant motor development, and presented in the experimentations of this paper, can also play an important role when
considering such a task-level exploration process. First, we use
motor synergies which have been shown as simplifying motor learning by reducing the number of dimensions for control (nevertheless, even with motor synergies, the dimensionality of the control space can easily go over several dozens, and
exploration still needs to be organized). These motor synergies are often encoded using Central Pattern Generators (CPG)
 or as more traditional innate low-level control loops which are part of the innate structure allowing a robot
to bootstrap the learning of new skills, as for example in 
where it is combined with intrinsically motivated learning. Second, we will use a heuristic inspired by observations of infants
who sometimes prepare their reaching movements by starting
from a same rest position , by resetting the robot to such
a rest position, which allows reducing the set of starting states
used to perform a task.
In this paper, we propose an approach which allows us to
transpose some of the basic ideas of IAC and RIAC architectures, combined with ideas from the SSA algorithm, into
a multi-level active learning architecture called Self-Adaptive
Goal Generation RIAC algorithm (SAGG-RIAC) (an outline and initial evaluation of this architecture was presented in
 ). Unlike RIAC which was made for active learning of forward models mapping action policy parametes to eﬀects in a
task space, we show that this new algorithm allows for eﬃcient
learning of inverse models mapping parameters of tasks to parameters of action policies that allow to achieve these tasks in
redundant robots. This is achieved through active sampling of
novel parameterized tasks in the task space, based on a measure
of competence progress, each of which triggers low-level goaldirected learning of the motor policy parameters that allow to
solve it. This takes advantage of both the typical redundancy of
the mapping and of the fact that very often the dimensionality of
the task space considered is much smaller than the dimensionality of motor primitives/action parameter space. Such an architecture also leverages both techniques for optimizing action
policy parameters for a single predeﬁned tasks (e.g. ),
as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized
task, and based on the previously learnt correspondences between policy and task parameters (e.g. ). While
approaches such as or do not consider the
problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work
as they could be used as the low-level techniques for low-level
learning of action parameter policies for self-generated tasks in
the SAGG-RIAC architecture.
SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning and is in line with concepts of mastery motivation, Flow, Optimal Level theories
and zone of Proximal Development introduced in psychology
 . In a competence based active exploration
mechanism, according to the deﬁnition , the robot is pushed
to perform an active exploration in the goal/operational space as
opposed to motor babbling in the actuator space.
Several strands of previous research have began exploration
various aspects of this family of mechanisms. First, algorithms
achieving competence based exploration and allowing general
computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective
 . While the high expressivity of these formalisms
allows in principle to tackle a wide diversity of problems, they
were not designed nor experimented for the particular family
of problems of learning high-dimensional continuous models
in robotics. While SAGG-RIAC also actively and adaptively
self-generates goals, this is achieved with a formalism based on
applied mathematics and dedicated to the problem of learning
inverse models in continuous redundant spaces.
Measures of interestingness based on a measure of competence to perform a skill were studied in , as well as in 
where a selector chooses to perform diﬀerent skills depending
on the temporal diﬀerence error to reach each skill. The study
proposed in is based on the competence progress, which
they use to select goals in a pre-speciﬁed set of skills considered
in a discrete world. As we will show, SAGG-RIAC also uses
competence progress, but targets learning in high-dimensional
continuous robot spaces.
A mechanism for passive exploration in the task space
for learning inverse models in high-dimensional continuous
robotics spaces was presented in , where a robot has to
learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-speciﬁed grid informing the robot
about the limits of its reachable space. In SAGG-RIAC exploration is actively driven in the task space, allowing the learning
process to minimize its sample complexity, and as we will show,
to reach a high-level of performances in generalization and to
discover automatically its own limits of reachability.
In the following sections we introduce the global architecture and formalization of the Self-Adaptive Goal-Generation
SAGG-RIAC architecture. Then, we study experimentally its
capabilities to allow a robot eﬃciently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals, and in the
context of three experimental setups: 1) learning the inverse
kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped
robot, 3) an arm learning to control a ﬁshing rod with a ﬂexible
wire. More precisely, we focus on the following aspects and
contributions of the architecture:
• SAGG-RIAC creates developmental trajectories driving
the robot to progressively focus on tasks of increasing
complexity of learnability;
• Drives the learning of a high variety of parameterized tasks
(i.e. capability to reach various regions of the goal/task
space) instead of numerous ways to perform the same task;
• Allows learning ﬁelds of tasks in high-dimensional highvolume control spaces as long as the task space is lowdimensional (it can be high-volume);
• Allows learning in task-spaces where only small and initially unknown subparts are reachable;
• Drives the learning of inverse models of highly-redundant
robots with diﬀerent body schemas;
• Guides the self-discovery of the limits of what the robot
can achieve in its task space;
• Allows improving signiﬁcantly the quality of learned inverse models in terms of speed of learning and generalization performance to reach goals in the task space, compared to diﬀerent methods proposed in the literature;
2. Competence Based Intrinsic Motivation:
Adaptive Goal Generation RIAC Architecture
2.1. Global Architecture
Let us consider the deﬁnition of competence based models
outlined in , and extract from it two diﬀerent levels for active learning deﬁned at diﬀerent time scales (Fig. 1):
1. The higher level of active learning (higher time scale)
takes care of the active self-generation and self-selection
of goals/tasks in a parameterized task space, depending
on a measure of interest based on the level of competences to reach previously generated goals (e.g. competence progress);
2. The lower level of active learning (lower time scale) considers the goal-directed active choice and active exploration of lower-level actions to be taken to reach the goals
selected at the higher level, and depending on local measures of interest related to the evolution of the quality of
learned inverse and/or forward models;
Interest Computation
Self-Generation
Goal-Directed Low-
Level Actions Interest
Computation
Goal Directed
Exploration and
Lower Level of Active Learning
Higher level of Active Learning
Figure 1: Global Architecture of the SAGG-RIAC architecture. The structure
is comprised of two parts deﬁning two levels of active learning: a higher level
which considers the active self-generation and self-selection of goals, and a
lower level, which considers the goal-directed active choice and active exploration of low-level actions, in order to reach the goals selected at the higher
2.2. Model Formalization
Let us consider a robotic system described in both a
state/context space S , and a task space Y which is a ﬁeld of
parameterized tasks/goals that can be viewed as deﬁning a ﬁeld
of parameterized reinforcement learning problems. For a given
context s ∈S , a sequence of actions a = {a1, a2, ..., an} ∈A,
potentially generated by a parameterized motor synergy πθ :
S →A (alternatively called an “option” and including a selftermination mechanism), allows a transition toward the new
states y ∈Y such that (s, a) →y, also written (s, πθ) →y.
For instance, in the ﬁrst experiment introduced in the following sections where we use a robotic manipulator, S represents
its actuator/joint space, Y the operational space corresponding
to the cartesian position of its end-eﬀector, and A relates to velocity commands in the joints. Also, in the second experiment
involving a quadruped where we use motor synergies, the context s is always reset to a same state and has thus no inﬂuence
on the learning, A relates to the 24 dimensional parameters of a
motor synergy which considers the frequency and amplitude of
sinusoids controlling the position of each joints over time, and
Y relates to the position and orientation of the robot after the
execution of the synergy during a ﬁxed amount of time.
SAGG-RIAC drives the exploration and learning of how to
reach goals given starting contexts/states. Starting states are
formalized as conﬁgurations s ∈S and goals as a desired
yg ∈Y. All states are considered to be potential starting states;
therefore, once a goal has been generated, the low-level goal
directed exploration and learning mechanism always tries to
reach it by starting from the current state of the system as formalized and explained below.
When the initiation position sstart, the goal yg and constraints
ρ (e.g. linked with the spent energy) are chosen, it generates a
motor policy πθ(Data)(sstart, yg, ρ) parameterized by sstart, yg and
ρ as well as parameters θ of internal forward and inverse models already learned with previously acquired data Data. Also,
it is important to notice that πθ(Data)(sstart, yg, ρ) can be computed on the ﬂy, as in the experiments below, with regression
techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on
the previously learnt correspondences between policy and task
parameters, such as in .
We can make an analogy of this formalization with the Semi-
Markov Option framework introduced by Sutton . In the
case of SAGG-RIAC, when considering an option ⟨I, π, β⟩, we
can ﬁrst deﬁne the initiation set I : S →[0; 1], where I is true
everywhere, because, as presented before, every state can here
be considered as a starting state. Also, goals are related to the
terminal condition β and β = 1 when the goal is reached, and the
policy π encodes the skill learned through the process induced
by the lower-level of active learning and shall be indexed by the
goal yg, i.e. πyg. More formally, as induced by the use of semimarkov options, we deﬁne policies and termination conditions
as dependent on all events between the initiation of the option,
and the current instant. This means that the policy π, and β are
depending on the history htτ = {st, at, st+1, at+1..., sτ} where t is
the initiation time of the option, and τ, the time of the latest
event. Denoting the set of all histories by Ω, the policy and
termination condition become deﬁned by π : Ω× A →[0; 1]
and β : Ω→[0; 1].
Moreover, because we have to consider cases where goals
are not reachable (either because of physical impossibility or
because the robot is not capable of doing it at that point of its
development), we need to deﬁne a timeout tmax which can stop
a goal reaching attempt once a maximal number of actions has
been executed. htτ is thus needed to stop π, (i.e. the low-level
active learning process), if τ > tmax.
Eventually, using the framework of options, we can deﬁne
the process of goal self-generation, as the self-generation and
self-selection of parameterized options, and a goal reaching
attempt corresponding to the learning of a particular option.
Therefore, the global SAGG-RIAC process can be described
as exploring and learning ﬁelds of options.
2.3. Lower Time Scale:
Active Goal Directed Exploration and Learning
In SAGG-RIAC, once a goal has been actively chosen at the
high-level, the goal directed exploration and learning mechanism at the lower can be carried out in numerous ways: the architecture makes only little assumptions about them, and thus is
compatible with many methods such as those described below
(this is the reason why SAGG-RIAC is an architecture deﬁning
a family of algorithms). Its main idea is to guide the system
toward the goal by executing low-level actions which allow a
progressive exploration of the world toward this speciﬁc goal
and that updates at the same time the local corresponding forward and inverse models, leveraging previously learnt correspondences with regression. The main assumptions about the
methods that can be used for this lower level are:
• Incremental learning and generalization: based on the
data collected incrementally, the method must be able to
build incrementally local forward and inverse models that
can be reused later on, in particular when considering other
goals, such as the task-space regression techniques presented in ;
• Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods or stochastic optimization ;
A optional feature, which is a variant of the second assumption
above, is:
• Active optimization: goal-directed optimization of the
parameters of the action policy for reaching a selfgenerated goal. A learning feedback mechanism has to
be added such that the exploration is active, and the selection of new actions depends on local measures about the
quality of the learned model.
In the following experiments that will be introduced, we will
use two diﬀerent methods: one mechanism where optimization
is inspired by the SSA algorithm , coupled with memorybased local forward and inverse regression models using local
Moore-Penrose pseudo-inverses, and a more generic optimization algorithm mixing stochastic optimization with memorybased regression models using pseudo-inverse.
Other kinds
of techniques could be used. For the optimization part, algorithms such as natural actor-critic architectures in model based
reinforcement learning , algorithms of convex optimization , algorithms of stochastic optimization like CMA (e.g.
 ), or path-integral methods (e.g. ).
For the regression part, we are here using a memory-based
approach, which if combined with eﬃcient data storage and
access structures , scales well from a computational
point of view. Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression
algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such
as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered , such as in
 .
2.4. Higher Time Scale:
Goal Self-Generation and Self-Selection
The Goal Self-Generation and Self-Selection process relies
on a feedback deﬁned using the concept of competence, and
more precisely on the competence improvement in given regions (or subspaces) of the task space where goals are chosen.
The measure of competence can be computed at diﬀerent instants of the learning process. First, it can be estimated once
a reaching attempt in direction of a goal has been declared as
terminated. Second, for robotic setups which are compatible
with this option, competence can be computed during low-level
reaching attempts. In the following sections, we detail these
two diﬀerent cases:
2.4.1. Measure of Competence for a Terminated Reaching Attempt
A reaching attempt for a goal is considered terminated according to two conditions:
• A timeout related to a maximum number of iterations allowed by the low-level of active learning has been exceeded.
• The goal has eﬀectively been reached.
We introduce a measure of competence for a given goal reaching attempt as dependent on two metrics: the similarity between
the point in the task space y f attained when the reaching attempt has terminated, and the actual goal yg; and the respect
of constraints ρ. These conditions are represented by a cost,
or competence, function C deﬁned in [−∞; 0], such that higher
C(yg, yf , ρ) will be, the more a reaching attempt will be considered as eﬃcient. From this deﬁnition, we set a measure of
competence Γyg directly linked with the value of C(yg, yf , ρ):
( C(yg, yf , ρ)
if C(yg, y f , ρ) ≤εsim < 0
where εsim is a tolerance factor such that C(yg, yf , ρ) > εsim
corresponds to a goal reached.
We note that a high value
of Γyg (i.e.
close to 0) represents a system that is competent to reach the goal yg while respecting constraints ρ.
typical instantiation of C, without constraints ρ, is deﬁned as
C(yg, y f , ∅) = −∥yg −y f ∥2, and is the direct transposition of prediction error in RIAC to the task space in SAGG-RIAC.
Yet, this competence measure might take some other forms in
the SAGG-RIAC architecture, such as the variants explored in
the experiments below.
2.4.2. Measure of Competence During a Reaching Attempt or
During Goal-Directed Optimization
When the system exploits its previously learnt models to
reach a goal yg, using a computed πθ through adequate local regression, or when it is using the low-level goal-directed
optimization to optimize the best current πθ to reach a selfgenerated goal yg, it does not only collect data allowing to
measure its competence to reach yg, but since the computed πθ
might lead to a diﬀerent eﬀect ye , yg, it also allows to collect
new data for improving the inverse model and the measure of
competence to reach other goals in the locality of ye. This allows to use all experiments of the robot to update the model of
competences over the space of paremeterized goals.
2.4.3. Deﬁnition of Local Competence Progress
The active goal self-generation and self-selection relies on
a feedback linked with the notion of competence introduced
above, and more precisely on the monitoring of the progress
of local competences. We ﬁrst need to deﬁne this notion of
local competence. Let us consider a subspace called a region
R ⊂Y. Then, let us consider diﬀerent measures of competence
Γyi computed for diﬀerent attempted goals yi ∈R, in a time window consisting of the ζ last attempted goals. For the region R,
we can compute a measure of competence Γ that we call a local
measure such that:
y j∈R(Γy j)
with |R|, cardinal of R.
Let us now consider diﬀerent regions Ri of Y such that Ri ⊂
i Ri = Y (initially, there is only one region which is then
progressively and recursively split; see below and see Fig. 2).
Each Ri contains attempted goals {yi1,t1, yi2,t2, ..., yik,tk}Ri and corresponding competences obtained {Γyi1,t1 , Γyi2,t2, ..., Γyik,tk }Ri, indexed by their relative time order of experimentation t1 < t2 <
... < tk|tn+1 = tn +1 inside this precise subspace Ri (ti are not the
absolute time, but integer indexes of relative order in the given
An estimation of interest is computed for each region Ri. The
interest interesti of a region Ri is described as the absolute value
of the derivative of local competences inside Ri, hence the amplitude of local competence progress, over a sliding time window of the ζ more recent goals attempted inside Ri (equation
Figure 2: Task space and example of regions and subregions split during the
learning process according to the competence level. Each region displays its
competence level over time, measure which is used for the computation of the
interest according to equation 2.
interesti =

−


By using a derivative, the interest considers the variation of
competences, and by using an absolute value, it considers cases
of increasing and decreasing competences. In SAGG-RIAC,
we will use the term competence progress with its general
meaning to denote this increase and decrease of competences.
An increasing competence signiﬁes that the expected competence gain in Ri is important. Therefore, potentially, selecting
new goals in regions of high competence progress could bring
both a high information gain for the learned model, and also
drive the reaching of not previously achieved goals.
Depending on the starting position and potential evolution
of the environment or of the body (e.g. breaking of a body
part), a decrease of competences inside already well-reached regions can arise. In this case, the system should be able to focus
again in these regions in order to at least verify the possibility
to re-establish a high level of competence inside. This explains
the usefulness to consider the absolute value of the competence
progress as shown in equation 2.
Using a sliding window in order to compute the value of interest prevents the system from keeping each measure of competence in its memory, and thus limits the storage resource
needed by the core of the SAGG-RIAC architecture.
2.4.4. Goal Self-Generation Using the Measure of Interest
Using the previous description of interest, the goal selfgeneration and self-selection mechanism carries out two diﬀerent processes:
1. Splitting of the space Y where goals are chosen, into subspaces, according to heuristics that allows to maximally
discriminate areas according to their levels of interest.
2. Selecting the next goal to perform.
Such a mechanism has been described in the RIAC algorithm
introduced in , but was previously applied to the actuator space S rather than to the goal/task space Y as is done in
SAGG-RIAC. Here, we use the same kind of methods such as
a recursive split of the space, each split being triggered once a
predeﬁned maximum number of goals gmax has been attempted
inside. Each split is performed such that it maximizes the difference of the interest measure described above in the two resulting subspaces. This allows the easy separation of areas of
diﬀering interest and therefore of diﬀering reaching diﬃculty.
More precisely, here the split of a region Rn into Rn+1 and Rn+2
is done by selecting among m randomly generated splits, a split
dimension j ∈|Y| and then a position vj such that:
• All the yi of Rn+1 have a jth component smaller than vj;
• All the yi of Rn+2 have a jth component higher than v j;
Qual(j, v j)
card(Rn+1).card(Rn+2).∥interestRn+1
interestRn+2∥
Finally, as soon as at least two regions exist after an initial
random exploration of the whole space, goals are chosen according to the following heuristics, selected according to probabilistic distributions:
1. mode(1): in p1% percent (typically p1 = 70%) of goal
selections, a random goal is chosen along a uniform distribution
inside a region which is selected with a probability proportional
to its interest value:
interestn −min(interesti)
i=1 interesti −min(interesti)
Where Pn is the selection probability of the region Rn, and
interesti corresponds to the current interest of the region Ri.
2. mode(2): in p2% (typically p2 = 20% of cases), a random
goal is chosen inside the whole space Y.
3. mode(3): in p3% (typically p3 = 10%), a region is ﬁrst selected according to the interest value (like in mode(1)) and then
a new goal is generated close to the already experimented one
which received the lowest competence estimation.
2.4.5. Reduction of the Initiation Set
In order to improve the quality of the learned inverse model,
we add a heuristic inspired by two observations on infant motor
exploration and learning. The ﬁrst one, proposed by Berthier
et al. is that infant’s reaching attempts are often preceded
by movements that either elevate their hand or move their hand
back to their side. And the second one, noticed in , is that
infants do not try to reach for objects forever but sometimes relax their muscles and rest. Practically, these two characteristics
allow them to reduce the number of initiation positions that they
use to reach an object, which simpliﬁes the reaching problem by
letting them learn a reduced number of reaching movements.
Such mechanism can be transposed in robotics to motor
learning of arm reaching tasks as well as other kind of skills
such as locomotion or ﬁshing as shown in experiments below. In such a framework, it directly allows a highly-redundant
robotic system to reduce the space of initiation states used to
learn to reach goals, and also typically prevent it from experimenting with too complex actuator conﬁgurations. We add
such a process in SAGG-RIAC, by specifying a rest position
(srest, yrest) reachable without any need of planning from the
system, that is set for each r subsequent reaching attempts (we
call r the reset value, with r > 0).
2.5. New Challenges of Unknown Limits of the Task Space
In traditional active learning methods and
especially
knowledge-based intrinsically motivated exploration , the system is typically designed to select actions
to perform inside a set of values inside an already known interval (for instance, the range of angles that can be taken by a
motor, or the phases and amplitudes of CPGs which can be easily identiﬁed). In these cases, the challenge is to select which
areas would potentially give the most information to the system, to improve its knowledge, inside this ﬁxed range of possibilities. As argued earlier, a limit of these approaches is that
they become less and less eﬃcient as the dimensionality of the
control space increases. Competence based approaches allow
to address this issue when a low-dimensional task space can
be identiﬁed. Nevertheless, in that case, a new problem arises
when considering unbounded learning: the space where goals
are reachable can be extremely large and it is generally very
diﬃcult to predict its limits and undesirable to ask the engineer
to identify them. Therefore, when carried out in large spaces
where the reachable area is only a small part of it, the algorithm
could necessitate numerous random goal self-generations to be
able to estimate interests of diﬀerent subregions. In order to reduce this number, and help the system to converge easily toward
regions where competence can be improved, we emphasize two
diﬀerent mechanisms that can be used in SAGG-RIAC, during
a reaching attempt:
1. Conservation of every point reached inside the task space
even if they do not correspond to the attempted goal (see
section 2.4.2): when the robot performs a reaching attempt
toward a goal y, and, instead of reaching it, terminates at
another state y′, we consider y′ as a goal reached with a
value of competence depending on constraints ρ. In cases
where no constraints are studied, we can consider the y′ as
another goal reached with the highest level of competence.
2. Addition of subgoals: in robotic setups where the process
of goal reaching can be subdivided and described using
subgoals which could be ﬁxed on the pathway toward the
goal, we artiﬁcially add states y1, y2, ..., yn that have to be
reached before y while also respecting the constraints ρ,
and estimate a competence measure for each one.
The consideration of these two heuristics has important advantages: ﬁrst, they can signiﬁcantly increase the number of estimations of competence, and thus the quantity of feedback returned to the goal self-generation mechanism.
This reduces
the number of goals that have to be self-generated to bootstrap
the system, and thus the number of low-level iteration required
to extract ﬁrst interesting subspaces. Also, by creating areas
of diﬀerent competence values around already reached states,
they inﬂuence the discovery of reachable areas. Finally, they result in an interesting emergent phenomena: they create a growing area of increasing competence around the ﬁrst discovered
reachable areas. Indeed, by obtaining values of competences
inside reachable areas, the algorithm is able to split the space
ﬁrst in these regions, and compute values of interest. These
values of interest will typically be high in already reached areas and inﬂuence the goal self-generation process to create new
goals in its proximity. Once the level of competence becomes
important and stabilized in eﬃciently reached areas, the interest becomes null, then, new areas of interest close to these ones
will be discovered, and so on.
2.6. PseudoCode
Pseudo-code 1 and algorithm 2 present the ﬂow of operations
in the SAGG-RIAC architecture. Algorithms 3 and 4 are simple alternative examples of low-level goal-directed optimization
algorithms that are used in the experimental section, but they
could be replaced by other algorithms like PI2 −CMA ,
CMA , or those presented in . The function Ineﬃcient
can also be built in numerous manners and will not be described
in details in the pseudo-code (examples will be described then
for each experimentation). Its function is to judge if the current
model has been eﬃcient enough to reach or come closer to the
decided goal, or if the model has to be improved in order to
In the following sections, we will present two diﬀerent kinds
of experiments. The ﬁrst one is a reaching experiment where
a robotic arm has to learn its inverse kinematics to reach selfgenerated end-eﬀector positions. It uses an evolving context
s ∈S , also called setpoint in SSA, representing its current joint
conﬁguration. Therefore, it can be described by the relationship
(s, a) →y where s, a and y can evolve. It is thus possible to use
a goal-directed optimization algorithm very similar to SSA in
this experiment, like the one in algorithm 3.
In the two other experiments, in contrast, we control the
robots using parameterized motor synergies and consider a
ﬁxed context (a rest position) s ∈S where the robot is reset
before each action: we will ﬁrst consider a quadruped learning
omnidirectional locomotion, and then an arm controlling a ﬂexible ﬁshing rod learning to put the ﬂoat in precise self-generated
positions on top of the water. Thus, these systems can be described by the relationship (s, πθ) →y, where s will here be
ﬁxed and θ will be the parameters of the motor synergy used
to control the robots. Thus, a variation of setpoint being prevented here, a variant of SSA will be proposed for such experiments (similar to a more traditional optimization algorithm),
where the context will not evolve and always be reset, like in
algorithm 4.
3. Experimental Setup 1:
Learning Inverse Kinematics
with a Redundant Arm
In this section, we propose an experiment carried out with
a robotic arm which has to explore and learn its forward and
inverse kinematics. Also, before discussing the details of our
active exploration approach in this ﬁrst experimentation case,
we ﬁrstly deﬁne the representations of the models and control paradigms involved in this experiment. Here, we focus on
robotic systems whose actuators are settable by positions and
velocities, and restrict our analysis to discrete time models.
Allowing robots to be self-adaptive to environmental conditions and changes in their own geometry is an important challenge of machine learning. These changes in the robot geome-
Algorithm 1 The SAGG-RIAC Architecture
S : State/Context space
Π: Space of paremeterized action policies πθ
Y: Space of parameterized tasks yi
M: regression model of the forward mapping (S, Π) →Y
M−1: regression model of the inverse mapping (S, Y) →Π
R: set of regions Ri ⊂Y and corresponding measures
interesti;
input: thresholds εC; εmax; timeout
input: rest position srest ∈S ; reset value: r
input: starting position sstart ∈S
input: number of explorative movements q ∈N
input: starting time: t
input: q budget of physical experiments for goal-directed
optimization
Reset the system in the resting state (sstart = srest) every r
iteration of the loop;
Active Goal Self-Generation (high-level):
Self-generate a goal yg ∈Y using the mode(m ∈[1; 2; 3])
with probability pm (see Section 2.4.4.)
Active Goal-Directed Exploration and Learning (low-level):
Let st represent the current context of the system
if Made possible by the sensorimotor space then
Compute a set of subgoals {y1, y2, ..., yn} ∈Y on the
pathway toward yg; (e.g. with a planning algorithm that
takes s, M, M−1 and yg into account);
{y1, y2, ..., yn} = ∅;
for each y j in {y1, y2, ..., yn} ∪yg do
while Γy j ≤εC & timeout not exceeded do
Compute and execute an action/synergy πθj ∈Π using M−1 such that it targets yj, e.g. using techniques
such as in ;
Get the resulting actually performed ey j and update M
and M−1 with new data (st, θj, ey j)
Compute the competence Γeyj (see section 2.4.1.)
UpdateRegions(R, ey j, Γey j);
if experiment with evolving context then
Goal-directed optimization of θ j to reach yj, with
SSA like algorithm such as Algorithm 3, and given
a budget of q allowed physical experiments;
Goal-directed optimization of θ j to reach yj such
as algorithm 4, or alternatively algorithms such as
 , and given a budget of q allowed physical experiments;
Compute the competence Γy j (see section 2.4.1.)
UpdateRegions(R, yj, Γyj);
Algorithm 2 Pseudo-Code of UpdateRegions
input: R: : set of regions Ri ⊂Y and corresponding measures interesti;
input: yt: current goal
input: Γyt: competence measure for yt
Let gMax be the maximal number of elements inside a region
Let ζ be a time window used to compute the interest
Find the region Rn in R such that yt ∈Rn;
Let k = card(Rn)
Add Γyt,k in Rn, where k is an indice indicating the ordinal
order in which Γyt was added in the region as compared to
other measures of competences in Rn ;
Compute the new value of interestn of Rn according to each
Γyi,l ∈Rn such that:
interestn =




if card(Rn) > gmax then
Split Rn; (see text, section 2.4.4)
Algorithm 3 Example of Pseudo-Code for the Low-Level
Goal-Directed Exploration with Evolving Context (used in the
experimentation introduced section 3.3)
input: q is the budget of physical experiment allowed to the
robot for local optimization;
Update the current context st = s j; {where sj is the context
after having performed πθ j}
if Ineﬃcient(M−1, eyj, y j) then
Local Exploration Phase:
for i = 1 to q do
Perform action policy πθi with θi drawn randomly in the
vicinity of θ j;
Measure the resulting yi and si;
Update M and M−1 with (st, θi, yi);
Update the context st = si;
Compute the competence Γyi;
UpdateRegions(R, yi, Γyi);
try directly have an impact on its Inverse Kinematics IK, relating workspace coordinates (where tasks are usually speciﬁed),
to actuators coordinates (like joint position, velocity, or torque
used to command the robot). Learning inverse kinematics is
useful in numerous machine learning cases, such as when no
accurate kinematic model of a robot is available or when an
online calibration is needed due to sensor or motor imprecision. Moreover, in developmental robotics studies, the a priori
knowledge of a precise model of the body is often avoided, because of its implausibility from the biological point of view. In
the following experiment, we assume that the inverse kinematics of our system is totally unknown, and we are interested in
Algorithm 4 Example of Pseudo-Code for the Low-Level
Goal-Directed Exploration with a Fixed or Resettable Context
(used in the experiments introduced sections 4 and 5)
input: q is the budget of physical experiment allowed to the
robot for local optimization;
Reset the current context: st = srest;
if Ineﬃcient(M−1, eyj, y j) then
Local Exploration Phase:
Initialize θk = θj and yk = y j and Γyk = Γy j
for i = 1 to q do
Perform πθi where θi is drawn randomly in the vicinity
Observe the resulting yi;
Update M and M−1 with the resulting (st, θi, yi);
Reset the current context: st = srest;
Compute the competence Γyi;
UpdateRegions(R, yi, Γyi);
if Γyi > Γyk then
studying how SAGG-RIAC can eﬃciently guide the discovery
and learning of its inverse kinematics.
3.1. Control Paradigms for Learning Inverse Kinematics
Let us mathematically formulate forward and inverse
kinematics relations.
We deﬁne the intrinsic coordinates
(joint/actuator positions) of a manipulator as the n-dimensional
vector S = α ∈Rn, and the position and orientation of the manipulator’s end-eﬀector as the m-dimensional vector y ∈Rm.
Relative to this formalization, actions of the robot corresponds
to speed commands parameterized by a vector θ = ˙α ∈Rn
which controls the instantaneous speed of each of the n joints
of the arm. The forward kinematic function of this system is
generally written as y = f(α), and inverse kinematics relationship is deﬁned as α = f −1(y).
When a redundant manipulator is considered (n > m), or
when m = n, solutions to the inverse relationship are generally
non-unique . The problem posed to inverse learning algorithms is thus to determine particular solutions to α = f −1(y),
when multiple solutions exists. A typical approach used for
solving this problem considers local methods, which learn relationships linking small changes ∆α and ∆y :
where J(α) is the Jacobian matrix.
Then, using the Jacobian matrix and inverting it to get a single solution ˙α corresponding to a desired ˙y raises the problem of
the non-convexity property of this last equation. A solution to
this non-convex problem has then been proposed by Bullock in
 who converted it into a convex problem, by only considering the learning task within the spatial vicinity b˙α of a particular
D(xstart, xg)
Figure 3: Values used to compute the competence Γyg, considering a manipulator of 7 degrees-of-freedom, in a 2 dimensions operational/task space. Here,
the arm is set in a position called rest position which is not straight and slightly
bent. (αrest, yrest).
3.2. Representation of Forward and Inverse Models to be
We use here non-parametric models which typically determine local models in the vicinity of a current datapoint. By
computing a model using parameterized functions on datapoints restrained to a locality, they have been proposed as useful
for real time queries, and incremental learning. Learning inverse kinematics typically deals with these kind of constraints,
and these local methods have thus been proposed as an eﬃcient
approach to IK learning . In the following study, we use
an incremental version of the Approximate Nearest Neighbors
algorithm (ANN) , based on a tree split using the k-means
process, to determine the vicinity of the current α. Also, in the
environments that we use to introduce our contribution, we do
not need highly robust, and computationally very complex regression methods. Using the pseudo-inverse of Moore-Penrose
 to compute the pseudo-inverse J+(α) of the Jacobian J(α)
in a vicinity b˙α is thus suﬃcient. Possible problems happening
due to singularities being bypassed by adding
noise in the joint conﬁgurations (see for a study about this
Also, in the following equation, we use this method to deduce
the change ∆α corresponding to a ∆x, for a given joint position
3.3. Robotic Setup
In the following experiments, we consider a n-dimensional
manipulator controlled in position and speed (as many of today’s robots), updated at discrete time values.
The vector
α ∈Rn which represents joint angles corresponds to the context/state space S and the vector y ∈Rm which is the position
of the manipulator’s end-eﬀector in m dimensions in the Euclidian space Rm corresponds to the task space Y (see Fig. 3 where
n = 7 and m = 2). We evaluate how the SAGG-RIAC architecture can be used by a robot to learn how to reach all reachable points in the environment Y with this arm’s end-eﬀector.
Learning the inverse kinematics is here an online process that
arises each time a micro-action θ = ∆α ∈A is executed by
the manipulator: by doing each micro-action, the robot stores
measures (α, ∆α, ∆x) in its memory and creates a database Data
which contains elements (αi, ∆αi, ∆yi) representing the discovered change ∆yi corresponding to a given ∆αi in the conﬁguration αi (this learning entity can be called a schema according to
the terminology of Drescher ). These measures are then
reused online to compute the Jacobian J(α) = ∆y/∆α locally
to move the end-eﬀector in a desired direction ∆ydesired ﬁxed
toward the self-generated goal. Therefore, we consider a learning problem of 2n dimensions, the relationship that the system
has to learn being (α, ∆α) →∆y. Also, in this experiment,
where we suppose Y Euclidian, and do not consider obstacles,
the direction to a goal can be deﬁned as following a straight line
between the current end-eﬀector’s position and the goal.
3.4. Evaluation of Competence
In this experiment, in order to clearly illustrate the main contribution of our algorithm, we do not consider constraints ρ and
only focus on the reaching of goal positions yg. It is nevertheless important to notice that a constraint ρ has a direct inﬂuence
on the low-level of active learning of SAGG-RIAC, and thus an
indirect inﬂuence on the higher level. As using a constraint can
require a more complex exploration process guided at the lowlevel, a more important number of iterations at this level can
be required to reach a goal, which could have an inﬂuence on
the global evolution of the performances of the learning process
used by the higher-level of SAGG-RIAC.
We deﬁne here the competence function C with the Euclidian distance D(yg, yf ), between the goal position and the ﬁnal
reached position y f , which is normalized by the starting distance D(ystart, yg), where ystart is the end-eﬀector’s starting position. This allows, for instance, to give a same competence
level when considering a goal at 1cm from the origin position,
which the robot approaches at 0.5cm and a goal at 1mm, which
the robot approaches at 0.5mm.
C(yg, yf , ystart)
−D(yg, yf )
D(ystart, yg)
where C(yg, y f , ystart) = 0 if D(ystart, yg) < εC (the goal is
too close from the start position) and C(yg, yf , ystart) = −1 if
D(yg, y f ) > D(ystart, yg) (the end-eﬀector moved away from the
3.5. Addition of subgoals
Computing local competence progress in subspaces/regions
typically requires the reaching of numerous goals.
reaching a goal can necessitate several micro-actions, and thus
time, obtaining competence measures can be long. Also, without biasing the learning process and as already explained in section 2.5, we improve this mechanism by taking advantage of the
Euclidian nature of Y: we increase the number of goals artiﬁcially, by adding subgoals on the pathway between the starting position and the goal, where competences are computed.
Therefore, considering a starting state ystart in Y, and a selfgenerated goal yg, we deﬁne the set of l subgoals {y1, y2, ..., yl}
where yi = (i/l) × (yg −ystart), that have to be reached before
attempting to reach the terminal goal yg.
We also consider another way to increase the number of competence measures which is to take into consideration each experimented position of the end-eﬀector as a goal reached with
a maximal competence value. This will typically help the system to distinguish which regions are eﬃciently covered, and to
discover new regions of interest.
3.6. Active Goal Directed Exploration and Learning
Here we propose a method inspired by the SSA algorithm to
guide the system to learn on the pathway toward the selected
goal position yg. This instantiation of the SAGG-RIAC architecture uses algorithm 3 and considers evolving contexts, as explained below.
3.6.1. Reaching Phase
The reaching phase deals with creating a pathway to the
current goal position yg. This phase consists of determining,
from the current position yc, an optimal micro-action which
would guide the end-eﬀector toward yg.
For this purpose,
the system computes the needed end-eﬀector’s displacement
∆ynext = v. yc−yg
∥yc−yg∥(where v is the velocity bounded by vmax and
∥yc−yg∥a normalized vector in direction of the goal), and performs the action ∆αnext = J+.∆ynext, with J+, pseudo-inverse of
the Jacobian estimated in the close vicinity of α and given the
data collected by the robot so far. After each action ∆ynext, we
compute the error ε = ∥f
∆ynext −∆ynext∥, and trigger the exploration phase in cases of a too high value ε > εmax > 0. εmax is
thus a parameter which has to be set depending on the range of
error ε that can be experienced, and will be set depending on
a tolerance that can be conceded to allow reaching goal positions with the current learned data. While a too high value of
εmax will prevent exploring and learning new data (the system
spending potentially too important amounts of time exploring
around a same conﬁguration and get trapped in local minima),
too low values of εmax will prevent an eﬃcient local optimization.
3.6.2. Exploration Phase
This phase consists in performing q ∈N small random explorative actions ∆αi, around the current position α, where the
variations can be derandomized such as in . This allows the
learning system to improve its regression model of the relationship (α, ∆α) →∆y, in the close vicinity of α, which is needed to
compute the inverse kinematics model around α. During both
phases, a counter is incremented for each micro-action and reset for each new goal. The timeout used to deﬁne a goal as
unreached and to stop a reaching attempt uses this counter. A
maximal quantity of micro-actions is ﬁxed for each goal as directly proportional to the number of micro-action it requires to
be reached. In the next experiments, the system is allowed to
perform up to 1.5 times the distance between ystart and yg before
stopping the reaching attempt.
3.7. Qualitative Results for a 15 DOF Simulated Arm
In the simulated experiment introduced in this section, we
consider the robotic arm presented Fig. 3 with 15 DOF, each
limb of the robot having the same length (considering a 15 DOF
arm corresponds to a problem of 32 continuous dimensions,
with 30 dimensions in the actuator/state space and 2 dimensions
in the goal/task space). We set the dimensions of the task space
Y as bounded in intervals yg ∈[0; 150] × [−150; 150], where 50
units is the total length of the arm, which means that the arm
covers less than 1/9 of the space Y where goals can be chosen
(i.e. the majority of areas in the operational/task space are not
reachable, which has to be self-discovered by the robot). We
ﬁx the number of subgoal per goal to 5, and the maximal number of elements inside a region before a split to gmax = 50. We
also set the desired velocity v = 2 units/micro-action, and the
number of explorative actions q = 20. Moreover, we reset the
arm to the rest position (αrest, yrest) (position displayed in Fig.
3) every r = 1 reaching attempts. This allows reducing the initiation set and prevent the system from experimenting with too
complex joint positions where the arm is folded, and where the
Jacobian is more diﬃcult to compute. Using a low value of r
is an important characteristic for the beginning of the learning
process. A too high value of r prevents learning rapidly how to
achieve a maximal amount of goal position, due to the diﬃculty
to reuse the previously learned data when the arm is folded in
unknown positions.
The bent character of the rest position is also useful to avoid
to begin a micro-action close to a singularity like when the arm
is totally unfolded. Also, in this experiment, we consider each
experimented position of the end-eﬀector as if it was a goal
reached with the maximal competence level (these numerous
positions are not displayed in the following ﬁgures in order to
not overload the illustrations).
Figure 4: Competence values corresponding to the entire set of self-generated
goals collected over an experiment of 30000 micro-actions on a 15 DOF arm.
The heterogeneous set of competence values situated inside the reachable space
illustrates the typical measures of competence that can be measured in this region over a whole experiment. For a visualization of the evolution of these
competence values, see ﬁgure 5
Figure 5: Evolution of competence values corresponding to self-generated goals collected during an experiment of 30000 micro-actions on a 15 DOF arm. Time is
indexed by the number of self-generated goals. Higher values (dark red) corresponds to position that has been reached using learned data. (For interpretation of the
references to color in this ﬁgure legend, the reader is referred to the web version of this article)
1-42 Goals
42-83 Goals
84-125 Goals
125-166 Goals
167-208 Goals
208-249 Goals
Figure 6: Evolution of the distribution of self-generated goals displayed over time windows indexed by the number of performed goals, for an experiment of
30000 micro-actions on a 15 DOF arm measuring 50 units. The black half-circle represents the contour of the area reachable by the arm. Higher values (dark red)
corresponds to higher density of self-generated goals. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of
this article)
3.7.1. Evolution of Competences over Time
4 represents the whole distribution of self-generated
goals and sub-goals selected by the higher-level of active learning module, and their corresponding competences after the execution of 30000 micro-actions. The global shape of the distribu-
Figure 7: Evolution of the splitting of the task/goal space and creation of subregions indexed by the number of goals self-generated (without counting subgoals), for
the experiment presented in Fig. 6.
167-208 Goals
208-249 Goals
125-166 Goals
84-125 Goals
Figure 8: Details of the evolution of the distribution of self-generated goals inside the reachable area for the experiment presented in Fig. 6. Gray points represent
the end-eﬀector rest position yrest.
tion of points allows observing the large values of competence
levels inside the reachable space and its close vicinity, and the
global low competence inside the remaining space.
The progressive increase of competences is displayed on Fig.
5 where we evaluate over time (indexed here by the number of
goals self-generated) the global competence of the system to
reach positions situated on a grid which covers the entire task
space. From these estimations of competence, we can extract
two interesting phenomena: ﬁrst of all, the two ﬁrst subﬁgures,
estimated after the self-generation of 42 and 83 goals, show that
the system is, at the beginning of the exploration and learning
process, competent to only attain areas situated close to the limits of the reachable space. Then, the 4 other subﬁgures show
the progressive increase of competences inside the reachable
space following an increasing radius whose the origin is situated around the end-eﬀector rest position.
The ﬁrst observation is due to the reaching mechanism in
itself, which, when possessing a few data acquired, does not allow the robot to experiment complex joint movements, but only
simple ones which typically leads to the limits of the arm. The
second phenomenon is due to the coupling of the lower-level of
active learning inspired by SSA with the heuristic of returning
to yrest every subsequent goals. Indeed, the necessity to be con-
ﬁdent in the local model of the arm to shift toward new positions
makes the system progressively explore the space, and resetting
it to its rest position makes it progressively explore the space by
beginning close to yrest. Finally, goal positions that are physically reachable but far from this radius typically present a low
competence to be reached initially, before the radius spreads
enough to reach them.
3.7.2. Global Exploration over Time
Fig. 6 shows histograms of goal positions self-generated during the execution of the 30000 micro-actions (only goals, not
subgoals for an easy reading of the ﬁgure). Each subﬁgure corresponds to a speciﬁed time window indexed by the number
of generated goals: the ﬁrst one (upper-left) shows that, at the
onset of learning, the system already focuses in a small area
around the end-eﬀector’s rest position, and thus discriminates
diﬀerences between a subpart of the reachable area and the remaining space (the whole reachable zone being represented by
the black half-circle on each subﬁgure of Fig. 6). In the second subﬁgure, the system is, inversely, focusing almost only on
regions of the space which are not reachable by the arm. This
is due to the imprecise split of the space at this level of the
exploration, which left very small reachable areas (which have
already been reached with a high competence), at the edge inside each large unreachable regions. This typically gives a high
mean competence to each of these region when they are created.
Then, due to the very large part of unreachable areas, in comparison to reachable ones, the mean competence decreases over
time. This brings interest to the region, thanks to the mathematical deﬁnition of the interest level, which, by using an absolute
value, pushes the robot toward areas where the competence is
decreasing. This complex process which allows driving the exploration in these kind of heterogeneous regions then allows dividing eﬃciently the task space into reachable and unreachable
Then, considering a global observation of subﬁgures 3 to 6,
we can conclude that the system eﬀectively autonomously discovers its own limits by focusing the goal self-generation inside reachable areas during the largest part of the exploration
period. The system is indeed discovering that only a subpart
is reachable due to the interest value becoming null in totally
unreachable areas where the competence value is low.
3.7.3. Exploration over Time inside Reachable Areas
A more precise observation of subﬁgures 3 to 6 is presented
in Fig. 8 where we can speciﬁcally observe the self-generated
goals inside the reachable area.
First, we can perceive that
the system is originally focusing in an area around the endeﬀector’s rest position yrest (shown by gray points in Fig. 8).
Then, it increases the radius of its exploration around yrest
and focuses on areas further aﬁeld to the end-eﬀector’s rest position. Subﬁgures 2 and 3 shows that the system explores new
reachable parts corresponding to the right part close to its basis
(subﬁgure 2), and then, the left part close to its basis (subﬁgures
Also, comparing the two ﬁrst subﬁgures, and the two last
ones, we observe a shift of the maximum exploration peak toward the arm basis. This is ﬁrst linked with the loss of interest
of self-generating goals around the end-eﬀector’s rest position.
Indeed, because the system becomes highly eﬃcient inside this
region, the competence level becomes high and stationary over
time, which leads to low interest values. At the same time, this
phenomenon is also linked with the increase of competences in
new reachable positions far from the end-eﬀector rest position
yrest, closer to its basis, which creates new regions of interest
(see the four last subﬁgures of Fig. 5).
3.7.4. Emergent Process
The addition of subgoals and the consideration of each endeﬀector’s position as a goal reached with the highest competence level have important inﬂuences on the learning process.
If we look at traditional active learning algorithms
which cannot deal with open-ended learning ,
as well as RIAC-like algorithms diﬀerent from SAGG-RIAC
 , we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable
or extremely complex areas, the learning process still has to
begin by a period of random exploration of the whole space,
to distinguish and extract which subparts are the most interesting according to the used deﬁnition of interest. Thanks to
the addition of sub-goals and/or the consideration of every endeﬀector’s position in SAGG-RIAC, in addition to exploring in
the task space, we reduce the number of needed random global
exploration, and improve the capability of the system to deal
with large (i.e. when the volume of reachable space is small as
compared to the volume of the whole space) task spaces. Using subgoals indeed creates a concentration of goals around the
current end-eﬀector’s position, which progressively grows according to new experimented positions.
Furthermore, the consideration of each end-eﬀector’s position for the estimation of competence allows discovering progressively which positions are reachable with a high competence level, and gives a fast indication of ﬁrst subregions where
these high competences are situated. This increases the number
of subregions close to the reachable areas and allows computing
the interest values in the growing vicinity of the end-eﬀector’s
experimented positions (see Fig. 7 where the progressive split
of subregions in reachable areas is displayed).
Therefore, these additions of competence measures allow the
system to discover and focus on areas where the competence is
high in a very low number of goal self-generation, and tackle
the typical problem of fast estimation and distinction of interesting areas. Nevertheless, this emergent process only helps
to increase the number of feedbacks required by the goal selfgeneration mechanism to split the space, and do not inﬂuence
the low-level active learning. Then, the timeout which deﬁnes
a goal as unreached during a single reaching attempt becomes
crucial when considering high-volume task spaces with large
unreachable parts as introduced in the following section.
3.7.5. Robustness in High-Volume Task Spaces
in the previous experiment, the timeout which describes a
goal as not reached and stops a reaching attempt is deﬁned as
directly proportional to the number of micro-actions required
to reach each goal. Practically, as introduced section 3.6.2, we
allowed the system to perform 1.5 times the distance between
ystart and yg before declaring a goal as not reached (including
explorative movements).
This timeout is eﬃcient enough to learn eﬃciently by discriminating regions of diﬀerent complexities in the middle-size
space S ′ = [0; 150] × [−150; 150] considered in this experiment. Nevertheless, it can have an important inﬂuence on the
SAGG learning process when considering extremely large task
spaces with small underlying reachable areas. For instance, if
1 to 336 Goals
337 to 672 Goals
673 to 1008 Goals
1009 to 1334 Goals
1345 to 1680 Goals
1681 to 2015 Goals
Figure 9: Histograms of self-generated goals displayed over time windows indexed by the number of performed goals, for an experiment of 30000 micro-actions
on a 15 DOF arm, for a high-volume task space S = [−500; 500] × [0; 500], according to the reachable space contained in [−50; 50] × [0; 50] (the black half-circle
represents the contour of the area reachable by the arm according to its length of 50 units).
we consider a task space Y = [−500; 500] × [−0; 500] where
only [−50; 50] × [0; 50] is reachable, the low-level of active
learning will spend an extremely large number of iterations trying to reach each unreachable goal if this kind of timeout is
Therefore, when considering such high-volume spaces, the
deﬁnition of a new timeout becomes crucial. In Fig. 9, we
demonstrate the high discriminating factor of SAGG-RIAC in
such a task space (Y = [−500; 500] × [−0; 500]) when using a
timeout which is not only based on the distance to the goal. This
one has also been designed to stop a reaching attempt according to the following blocking criteria: let us consider a selfgenerated goal yg that the low-level exploration and reaching
mechanisms try to reach. Then, if the system is not coming
closer to the goal even after some low-level explorations, the
exploration toward this precise goal stops. In a practical way,
when w consecutive low-level explorations are triggered (typically w ≥2) and thus no progress to the goal was made, we
declare a goal as unreached, and compute the corresponding
competence level. Using such a deﬁnition, the rapidity of discovering blocking situations will depend on both values of w
and number of explorative actions q. Minimal values of these
two parameters allows the fastest discoveries, but decrease the
quality of the low-level exploration mechanism when exploring
reachable spaces (in the experiment presented in Fig. 9 we use
q = 5 and w = 3).
3.7.6. Conclusion of Qualitative Results
When considering low-level mechanisms allowing an eﬃcient progressive learning, the SAGG-RIAC algorithm is capable to discriminate very eﬃciently reachable areas in such
high-volume spaces. Then, it is also able to drive a progressive self-generation of goals through reachable subspaces of
progressively growing complexities of reachability.
In this experiment, the reachable region in the task space was
convex and with no obstacles. Yet, as we will see in the ﬁshing
experiment below, SAGG-RIAC is capable of identifying correctly its zones of reachability, given a low-level optimization
algorithm, even if there are “holes” or obstacles: goals initially
generated in unreachable positions or in positions for which obstacles prevent their reaching provide a low level of competence
progress, and thus the system stops trying to reach them. It is
also possible to imagine that some given self-generated goals
might be reachable only by an action policy going around an
obstacle. Such a capability is not a property of the SAGG-RIAC
architecture by itself, but a property of the optimization algorithm, and action representation, that is used at the low-level
goal-directed mechanism. In the present experiment, low-level
optimization was a simple one only considering action policies going in a straight line to the goal.
Yet, if one would
have used more complex optimization leveraging continuous
domain planning techniques (e.g. ), the zones of reachability would be increased if obstacles are introduced since the
low-level system could learn to go around them.
3.8. Quantitative Results for Experiments
with Task Spaces of Diﬀerent Sizes
In the following evaluation, we consider the same robotic
system than previously described (15DOF arm of 50 units) and
design diﬀerent experiments. For each one, we estimate the
eﬃciency of the inverse model learned by testing how it allows in average the robot to reach positions selected inside a
test database of 100 reachable positions (uniformly distributed
in the reachable area and independent from the exploration of
the robot). We will also compare SAGG-RIAC to three other
types of exploration techniques:
1. SAGG-RANDOM, where goals are chosen randomly
(higher-level of active learning (RIAC) disabled)
2. ACTUATOR-RANDOM, where small random microactions ∆α are executed. This method corresponds to classical random motor babbling.
3. ACTUATOR-RIAC, which corresponds to the original
RIAC algorithm that uses the decrease in prediction errors
(α, ∆α) →∆x to compute an interest value and split the
space (α, ∆α).
Also, to be comparable to SAGG-RIAC, each ACTUATOR
technique will have the position of the arm reset to the rest
position every max micro-actions, max being the number of
micro-actions needed to reach the more distant reachable position.
max is proportional to the desired velocity which is
here of v = 2 units/micro-action as well as the size of the task
space (this will explain the diﬀerent results of each ACTUA-
TOR methods when used with task spaces of diﬀerent sizes). In
every graph, we present statistical results obtained after launching the same experiment with diﬀerent random seeds 15 times.
3.8.1. Exploration in the Reachable Space
The ﬁrst quantitative experiment is designed to compare
the quality of inverse models learned using babbling in the
task/operational space (i.e. using goals), instead of more traditional motor babbling heuristics executed in the conﬁguration/actuator space. We still consider a n=15 DOF arm of 50
units, also, to be suited for the ﬁrst study, dimensions of Y will
be bounded in intervals yg ∈[0; 50] × [−50; 50] which means
that the arm can reach almost all the space Y where goals can
be chosen (the limits of reachability are thus almost given to the
robot). In this experiment, we ﬁx q = 20 for the SAGG methods and use a timeout only relative to the distance to the current
goal (a end-eﬀector movement of 1.5 times the one needed is
Fig. 10 shows the evolution of the capability of the system to
reach the 100 test goals using the inverse model learned by each
technique, starting from the rest position. This capability is
computed using the mean Euclidian distance between the goal
and the ﬁnal state of a reaching attempt.
Globally, these results show that in order to learn inverse
kinematics of this highly-redundant arm, exploration in the
goal/operational space is signiﬁcantly more eﬃcient than exploration in the actuator space using either random exploration
or RIAC-like active learning. Moreover, better performances of
ACTUATOR-RANDOM compared to ACTUATOR-RIAC emphasizes that the original version of RIAC has not been designed for the eﬃcient learning of inverse models of highlyredundant systems (high-dimension in the actuator space).
Focusing on the evaluation of the two mechanisms which
use SAGG, we can also make the important observation that
Reaching Error
Number of Actions (time steps)
SAGG-RANDOM
ACTUATOR-RANDOM
ACTUATOR-RIAC
Figure 10: Evolution of mean distances between the goal and the end eﬀector
after reaching attempts over an independently randomly generated set of test
goals. Here SAGG-RIAC and SAGG-RANDOM are only allowed to choose
goals within Y = [0; 50] × [−50; 50] (i.e. most eligible goals are physically
reachable). Standard deviations are computed over 15 experiments at the same
instants for each curve, and shifted in graphs for an easy reading.
SAGG-RIAC is here more eﬃcient than SAGG-RANDOM
when considering a system which already knows its own limits of reachability. More precisely, we observe both increase in
learning speed and ﬁnal generalization performances (this results resonates with results from more classic active learning,
see ). These improvement signiﬁes that SAGG-RIAC is
eﬃciently able to progressively discriminate and focus on areas
which bring the highest informational amount (i.e. areas which
have not been visited enough). It brings to the learning system
more useful data to create an eﬃcient inverse model, contrarily
to the SAGG-RANDOM approach which continues to select
goals in already eﬃciently reached areas.
3.8.2. Robustness in Large Task Spaces
in the following experiment, we would like to test the capability of SAGG-RIAC to focus on reachable areas when facing
high volume task spaces (will call this phenomenon the discrimination capability). Therefore, we will here consider a task
space Y = [0; 500] × [−500; 500]. Fig. 11 shows the learning eﬃciency of SAGG-RIAC using the timeout with blocking criteria as described in the section 3.7.5. This allows to
test the quantitative aspect of the discrimination capability of
SAGG-RIAC and its comparison with the three other techniques when facing high volume task spaces where only small
subparts are reachable. As Fig. 11 shows, SAGG-RIAC is here
the only method able to drive an eﬃcient learning in such a
space. SAGG-RANDOM actually spends the majority of the
time trying to reach unreachable positions. Also, the size of
the task space has an inﬂuence on the two ACTUATOR algorithms if we compare results in Y = [0; 50] × [−50; 50] introduced Fig. 10 and in Y = [0; 500] × [−500; 500] introduced
Fig. 11. This is due to the value max of micro-actions performed by ACTUATOR methods which is proportional to the
size of the task space as explained section 3.8. Results consid-
Reaching Error
Number of Actions (time steps)
SAGG-RANDOM
ACTUATOR-RANDOM
ACTUATOR-RIAC
Figure 11: Evolution of mean distances between the goal and end eﬀector after
reaching attempts over an independently randomly generated set of test goals,
averaged over 15 experiments. Here SAGG-RIAC and SAGG-RANDOM are
allowed to choose goals within a large space corresponding to the one in Fig.
9, deﬁne as Y = [0; 500] × [−500; 500] (i.e. most eligible goals are physically
unreachable).
ering the space Y = [0; 500]×[−500; 500] seems more eﬃcient
for these methods, where the value of max is higher than in
Y = [0; 50] × [−50; 50]. An increase of max thus allows these
methods to explore more eﬃciently the reachable space whose
exploration is limited when considering a too low value of max.
3.8.3. Robustness in Very Large Task Spaces
Finally, we test the robustness of SAGG-RIAC in task spaces
larger than in the previous section.
12 shows the behavior of SAGG-RIAC when used with task spaces of different sizes, from 1 to 900 times the size of the reachable
space, and compare these results with a random exploration
in the actuator space when the value of max is ﬁxed as when
Y = [0; 500] × [−500; 500].
We can notice here that, although the high discriminative capacity of SAGG-RIAC in
large spaces such as Y = [0; 500] × [−500; 500], as shown
previously, the performances of this technique decrease when
the size of the considered task space increases.
Therefore,
we can observe that SAGG-RIAC obtains better results than
ACTUATOR-RANDOM since 5000 micro-actions when considering spaces smaller than Y = [0; 500] × [−500; 500]. Then,
this method shows better results than ACTUATOR-RANDOM
only after 10000 micro-actions when considering the space Y =
[0; 500] × [−500; 500]. And ﬁnally, this one becomes less eﬃcient than ACTUATOR-RANDOM when the considered space
increases in comparison to the reachable space, as shown by results when considering spaces Y = [0; 1000] × [−1000; 1000]
and Y = [0; 1500] × [−1500; 1500]. These results clearly show
that SAGG-RIAC is robust in spaces up to 100 times larger than
the reachable space, but has some diﬃculties to explore even
larger spaces. Therefore, despite the fact that SAGG-RIAC is
very eﬃcient in large spaces, it seems that the challenge of autonomous exploration in un-prepared spaces can not be totally
resolved by this algorithm, a human supervisor being still necessary to deﬁne a set of (even very approximate) limits for the
task space. As it will be emphasized in the perspective of this
work, some complementary techniques should be used in order
to bring robustness to such spaces, such as mechanisms inspired
by the notion of maturational constraints which are able to ﬁx
limits on the task space since the beginning of the exploration
S’ = [0;50]x[-50;50]
S’ = [-0;150]x[150;150]
S’ = [-0;500]x[500;500]
S’ = [-0;1000]x[1000;1000]
S’ = [-0;1500]x[1500;1500]
ACTUATOR-RANDOM
Number of Actions (time steps)
Reaching Error
Figure 12: Quantitative results of SAGG-RIAC when used with task spaces of
diﬀerent sizes and comparison with ACTUATOR-RANDOM.
3.9. Quantitative Results for Experiments
with Arm of Diﬀerent Number of DOF and Geometries
In every experiment, we set the dimensions of Y as bounded
by the intervals yg ∈[0; 150] × [−150; 150], where 50 units is
the total length of the arm, which means that the arm covers
less than 1/9 of the space Y where goals can be chosen (i.e. the
majority of areas in the operational/task space are not reachable,
which has to be discovered by the robot).
For each experiment, we set the desired velocity v = 0.5
units/micro-action, and the number of explorative actions q =
20. Moreover, we reset the arm to the rest position (αrest, yrest)
every r = 2 reaching attempts, which increases the complexity
of the reaching process.
We present a series of experiments aiming to test the robustness of SAGG-RIAC in arm setups with diﬀerent shapes
and numbers of degrees-of-freedom. Performed tests used 7,
15, and 30 DOF arms whose each limb has either the same
length or a decreasing length depending on its distance from
the arm’s base (we use the golden number to specify the relative size of each part, taking inspiration from the architecture of
human limbs). These experiments permit testing the eﬃciency
of the algorithm for highly redundant systems (considering a
30 DOF arm corresponds to a problem of 62 continuous dimensions, with 60 dimensions in the actuator/state space and 2
dimensions in the goal/task space), and diﬀerent morphologies.
Also, to stress the capability of the system to make the robot
self-discover its own limits, we remove the consideration of
each end-eﬀector position experimented as a goal reached with
the highest level of competence (see 3.7.4). In these experiments, the competence level is therefore evaluated only for
goals and subgoals. We ﬁx q = 100, and compute tests of inverse models over 200000 micro-actions.
Distance To Reach
30 Dim Decreasing Size
7 Dim Decreasing Size
15 Dim Decreasing Size
SAGG-Random
ACTUATOR-Random
ACTUATOR-RIAC
7 Dim Equal Size
15 Dim Equal Size
30 Dim Equal Size
SAGG-Random
ACTUATOR-Random
ACTUATOR-RIAC
Number of Actions (time steps)
Reaching Errors
Number of Actions (time steps)
Figure 13: Evolution of mean distances between the goal and end eﬀector after reaching attempts over an independently randomly generated set of test goals,
averaged over 15 experimentations. Here SAGG-RIAC and SAGG-random are only allowed to choose goals within Y = [0; 150] × [−150; 150] (i.e. the set of
reachable goals is only a small subset of eligible goals).
3.9.1. Quantitative Results
Fig. 13 illustrates the performances of the learned inverse
models when used to reach goals from an independent test
database and evolving along with the number of experimented
micro-actions. First, we can globally observe the slower decreasing velocity (over the number of micro-actions) of SAGG-
RANDOM and SAGG-RIAC, compared to the previous experiment, which is due to the higher value of q and the removed
consideration of every end-eﬀector position.
Graphs on the
ﬁrst line of Fig. 13 present the reaching errors of 7, 15 and 30
DOF arms with decreasing lengths. The ﬁrst subﬁgure shows
that when considering 7 DOF, which is a relatively low number of degrees of freedom, SAGG-RANDOM is not the second
more eﬃcient algorithm. Indeed, the ACTUATOR-RANDOM
method is here more eﬃcient than SAGG-RANDOM after
25000 micro-actions and is then stabilized, while SAGG-
RANDOM is progressively decreasing, reaching the same level
as ACTUATOR-RANDOM at the end of the experiment. This
is due to the high focalization of SAGG-RANDOM outside the
reachable area, which leads to numerous explorations toward
unreachable positions. As shown also in this subﬁgure, adding
the RIAC active component to SAGG eﬃciently improves the
learning capabilities of the system; SAGG-RIAC reaching errors were indeed the lowest for this 7 DOF system.
Experiments with 15 DOF and 30 DOF shows that both
SAGG methods are here more eﬃcient than actuator methods,
SAGG-RIAC showing a signiﬁcant improvement compared to
every other algorithm (for 15DOF, the level of signiﬁcance
is p = 0.002 at the end of the experiment (200000 microactions)).
Experiments presented with 7, 15 and 30 DOF arms where
each limb has the same length show the same kind of results.
The 7 DOF experiment shows that ACTUATOR-RANDOM
can be more eﬃcient than SAGG-RANDOM, and that the addition of RIAC allows obtaining a signiﬁcant improvement in
this case, but also when considering 15 and 30 DOF.
3.9.2. Conclusion of Quantitative Results
Globally, quantitative results presented here emphasize the
high eﬃciency and robustness of SAGG-RIAC when carried
out with highly redundant robotic setups of diﬀerent morphologies, compared to more traditional approaches which explore in
the actuator (input) space. They also showed that random exploration in the goal (output) space can be very eﬃcient when
used in high-dimensional systems, even when considering a
task space more than 9 times larger than the reachable subspace. These results therefore indicate the high potential of
competence based motor learning for IK learning in highlyredundant robots.
3.10. Qualitative Results for a Real 8 DOF Arm
In this section, we test the robustness of the algorithm in a
qualitative point of view when considering a real robotic setup
(not simulated) which corresponds to the simulation presented
above: we use a 8 DOF arm controlled in position. Also, helping to test the robustness of our method, we use low quality
motors whose averaged noise is 20% for each movement. The
ﬁxed task space corresponds to the whole surface observable
by a camera ﬁxed on top of the robot, which is more than three
times larger than the reachable space (see the left part of Fig.
14). In order to allow the camera to distinguish the end-eﬀector
of the arm and to create a visual referent framework on the
2D surface, we used visual tags and the software ARToolKit
Tracker .
Fig. 14 (right part) shows histograms of self-generated goals
displayed over sliding time windows indexed by the number
of performed goals (without counting subgoals) for an experiment of 10000 micro-actions. We can observe that the algorithm manages to discover the limits of the reachable area and
drives the exploration inside after the goal 57. Then, the system
continues to focus on the reachable space until the end of the experimentation, alternating between diﬀerent areas inside. More
precisely, we can notice while comparing the bottom-left sub-
ﬁgure to the two positioned on the second line, that the system
seems to concentrate only after some time on the areas situated
close to its basis, and therefore more diﬃcult to reach. The progressive increase of the complexity of positions explored which
appeared in simulation therefore also happens here. Finally, the
last subﬁgure shows that the system continues its exploration
toward an area more central of the reachable part. This is due
to the high level of noise of the motor control: while the system is originally not very robust in this part of the space, an
improvement of the generalization capacity of the learning algorithm allows obtaining an increase of competences in already
visited areas of the task space.
This experiment shows the eﬃciency of the SAGG-RIAC architecture to drive the learning process in real noisy robotic
setups with only a few iterations, as well as its capacity to
still control the complexity of the exploration when considering highly-redundant systems.
4. Experimental
Omnidirectional
Quadruped Locomotion with Motor Synergies
Sometimes stemming from pre-wired neuronal structures
(e.g. central pattern generators ), motor synergies are deﬁned as the coherent activations (in space or time)
of a group of muscles. They have been proposed as building
blocks simplifying the scaﬀolding of motor behaviors because
allowing the reduction of the number of parameters needed to
represent complex movements .
as crucial for the development of motor abilities, they can be
seen as encoding an unconscious continuous control of muscles
which simpliﬁes the complexity of the learning process: learning complex tasks using parameterized motor synergies (such
as walking, or swimming) indeed corresponds to the tuning of
relatively low-dimensional (but yet which can have a few dozen
dimensions) high-level control parameters, compared to the important number of degrees of freedom which have to be controlled (thousand in the human body, see ).
Figure 15: 12 degrees-of-freedom quadruped controlled using motor synergies
parameterized with 24 values : 12 for the amplitudes and 12 others for the
phases of a sinusoid tracked by each motor. Experiments consider a task space
u, v, α which corresponds to the 2D position and orientation of the quadruped.
4.1. Formalization
In the two following experiments, we simplify the learning
process by using such parameterized motor synergies controlling amplitude, phase, and velocity of Central Pattern Generators (CPGs). Mathematically, using motor synergies simpliﬁes the description of the considered robotic system.
the framework introduced above (section 2.2) we deﬁned our
system as being represented by the relationship (s, a) →y,
where for a given conﬁguration s ∈S , a sequence of actions
a = {a1, a2, ..., an} ∈A allows a transition toward y ∈Y.
1 to 29 Goals
29 to 57 Goals
57 to 85 Goals
85 to 113 Goals
113 to 141 Goals
141 to 169 Goals
Observation of the Camera (Goal Space)
Figure 14: Histograms of self-generated goals displayed over time windows indexed by the number of performed goals, for an experiment of 10000 micro-actions
on a real 8 DOF arm. Each histogram represents the surface covered by the camera, which here deﬁnes the task space.
In the current framework we consider the sequence of actions
as being generated directly by parameterized motor synergies
πθ, which means that the sequence of actions is directly encoded and controlled (using feedbacks internal to the synergy)
by setting parameters θ speciﬁed at the beginning of an action.
For instance, in the experiment described in this section, we
deﬁne a synergy as a set of parameterized sinusoids (one on
each joint) that a motor joint has to track with a low-level preprogrammed PID-like controller. Eventually, motor synergies
can be seen as a way to encapsulate the low-level generation
of sequences of micro-actions, allowing the system to directly
focus on the learning of models (s, πθ) →y, with s ∈S ﬁxed
(the rest position of the robot) and θ a set of parameters controlling the synergy (we will remove the ﬁxed context s in the next
notations for a easier reading and only write πθ →y).
4.2. Robotic Setup
In the following experiment, we consider a quadruped robot
simulated using the Breve simulator (physics simulation
is based on ODE). Each of its leg is composed of 2 joints, the
ﬁrst (closest to the robot’s body) is controlled by two rotational
DOF, and the second, one rotation (1 DOF). Each leg therefore
consists of 3 DOF, the robot having in its totality 12 DOF (See
This robot is controlled using motor synergies piθ whose parameters θ ∈Rn directly specify the phase and amplitude of
each sinusoid which controls the precise rotational value of
each DOF over time. These synergies are parameterized using a set of 24 continuous values, 12 representing the phase
ph of each joint, and the 12 others, the amplitude am; θ =
{ph1,2,..,12; am1,2,..,12}, where each joint i receives the command
am× sin(ωt+ ph), with ω a ﬁxed frequency. Each experimentation consists of launching a motor synergy πθ for a ﬁxed amount
of time, starting from a ﬁxed position. After this time period,
the resulting position yf of the robot is extracted into 3 dimensions: its position (u, v), and its rotation φ. The correspondence
θ →(u, v, φ) is then kept in memory as a learning exemplar.
The three dimensions u, v, φ are used to deﬁne the task space
of the robot. Also, it is important to notice that precise areas
reachable by the quadruped using these motor synergies cannot
be estimated beforehand. In the following, we set the original
dimensions of the task space to [−45; 45]×[−45; 45]×[−2π; 2π]
on axis (u, v, φ), which was a priori larger than the reachable
space. Then, after having carried out numerous experimentations, it appeared that this task space was actually more than
25 times the size of the area accessible by the robot (see red
contours in Fig. 16).
The implementation of our algorithm in such a robotic setup
aims to test if the SAGG-RIAC driving method allows the robot
to learn eﬃciently and accurately to attain a maximal amount of
reachable positions, avoiding the selection of many goals inside
regions which are unreachable, or that have previously been visited.
4.3. Measure of competence
In this experiment, we do not consider constraints ρ and only
focus on reaching of the goal positions yg = (ug, vg, φg). In
every iteration the robot is reset to a same conﬁguration called
the origin position (see Fig. 17). We deﬁne the competence
function C using the Euclidian distance goal/robot’s position
D(yg, y f ) after a reaching attempt, which is normalized by the
original distance between the origin position yorigin, and the goal
D(yorigin, yg) (See Fig. 17).
In this measure of competence, we compute the Euclidian
distance using (u, v, φ) where dimensions are rescaled in [0; 1].
Each dimension therefore has the same weight in the estimation
of competence (an angle error of φ =
2π is as important as an
Figure 16: Positions explored by the quadruped inside the task space u, v, φ after 10000 experiments (running a motor synergy during a ﬁxed amount of time), using
diﬀerent exploration mechanisms. Red lines represents estimated limits of reachability. (For interpretation of the references to color in this ﬁgure legend, the reader
is referred to the web version of this article)
Goal Position
Reached Position
(ug, vg, θg)
(uf, vf, θf)
Figure 17: Example of experimentation of the quadruped and illustration of
beginning position, goal position (ug, vg, φg), and a corresponding reached position (uf , vf , φf ) whose value are used to compute the measure of competence.
C(yg, yf , ystart)
−D(yg, y f )
D(ystart, yg)
where C(yg, yf , ystart) = 0 if D(ystart, yg) = 0.
4.4. Active Goal Directed Exploration and Learning
Reaching a goal yg necessitates the estimation of a motor synergy πθi leading to this chosen state yg. Considering a single
starting conﬁguration for each experimentation, and motor synergies πθ, the forward model which deﬁnes this system can be
written as the following:
θ →(u, v, φ)
Here, we have a direct relationship which only considers the 24
dimensional parameter vector θ = {ph1,2,..,12; am1,2,..,12} of the
synergy as inputs of the system, and a position in (u, v, φ) as
output. We thus have a ﬁxed context and use here an instantiation of the SAGG-RIAC architecture with local optimization
algorithm Alg. 4, detailed below.
4.4.1. Reaching Phase
The reaching phase deals with reusing the data already acquired and use local regression to compute an inverse model
((u, v, φ) →θ)L in the locality L of the intended goal yg =
(ug, vg, φg). In order to create such a local inverse model (numerous other solutions exist, such as ), we
extract the potentially more reliable data using the following
We ﬁrst extract from the learned data the set L of the l nearest
neighbors of (ug, vg, φg) and then retrieve their corresponding
motor synergies using an ANN method :
{{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}l}
Then, we consider the set M which contains l sets of m elements:

M1 : {{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}m}1
M2 : {{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}m}2
Ml : {{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}m}l

where each set {{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}m}i corresponds to the m nearest neighbors of each θi, i ∈L, and their
corresponding resulting position (u, v, φ).
For each set {{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}m}i, we estimate the standard deviation σ of the parameters of their motor
synergies θ :
θ j ∈{{u, v, φ, θ}1,...,m}
{{u, v, φ, θ}1, {u, v, φ, θ}2, ..., {u, v, φ, θ}m} inside M such that
it minimizes the standard deviation of its synergies:
Mk = argmini σ(Mi)
we estimate a local linear inverse model
((u, v, φ) →θ) by using a pseudo-inverse as introduced in the
reaching experiment, and use it to estimate the motor synergy
parameters θg which correspond to the desired goal (ug, vg, φg).
4.4.2. Exploration Phase
The system here continuously estimates the distance between
the goal yg and already reached position yc which is the closest
from the goal. If the reaching phase does not manage to make
the system come closer to yg, i.e. D(yg, yt) > D(yg, yc), with
yt as last eﬀectively reached point in an attempt toward yg, the
exploration phase is triggered.
In this phase the system ﬁrst considers the nearest neighbor
yc = (uc, vc, φc) of the goal (ug, vg, φg) and gets the corresponding known synergy θc. Then, it adds a random noise rand(24) to
the 24 parameters {ph1,2,..,12, am1,2,..,12}c of this synergy θc which
is proportional to the Euclidian distance D(yg, yc). The next
synergy θt+1 = {ph1,2,..,12, am1,2,..,12}t+1 to experiment can thus
be described using the following equation:
{ph1,2,..,12, am1,2,..,12}c
+ λ.rand(24).D(yg, yc)
where rand(i) returns a vector of i random values in [−1; 1],
λ > 0 and {ph1,2,..,12, am1,2,..,12}c the motor synergy which corresponds to yc.
4.5. Qualitative Results
Fig. 16 presents the positions explored by the quadruped
inside the task space u, v, φ after 10000 experimentations
(running of motor synergies during the same ﬁxed amount
of time) using the exploration mechanisms introduced previously.
ACTUATOR-RANDOM and ACTUATOR-RIAC
select parameters of motor synergies in this experiment,
whereas SAGG-RANDOM and SAGG-RIAC self-generate
goals (u, v, φ).
Reaching Error
Number of Actions (time steps)
SAGG-Random
ACTUATOR-Random
ACTUATOR-RIAC
Figure 18: Quantitative results for the quadruped measured using the reaching
error over the number of experimentations.
exploration
mechanisms
(ACTUATOR-RANDOM
ACTUATOR-RIAC)
cannot distinguish any notable diﬀerence, the space explored
appears similar and the extent of explored space on the (u, v)
axis is comprised in the interval [−5; 5] for u and [−2.5; 2.5]
for v on both graphs. Moreover, we notice that the diﬀerence
between u and v scales is due to the inherent structure of the
robot, which simpliﬁes the way to go forward and backward
rather than shifting left or right.
Considering SAGG methods, it is important to note the difference between the reachable area and the task space. In Fig.
16, red lines correspond to the estimated reachable area which
is comprised of [−10; 10]×[−10; 10]×[−π; π], whereas the task
space is much larger: [−45; 45] × [−45; 45] × [−2π; 2π]. We are
also able to notice the asymmetric aspect of its repartition according to the v axis, which is due to the decentered weight of
the robot’s head.
First, the SAGG-RANDOM method seems to slightly increase the space covered on the u and v axis compared to AC-
TUATOR methods, as shown by the higher concentration of
positions explored in the interval [−5; −3] ∪[3; 5] of u. However, this change does not seem very important when comparing
SAGG-RANDOM to these two algorithm.
Second, SAGG-RIAC, contrary to SAGG-RANDOM, shows
a large exploration range: the surface in u has almost twice as
much coverage than using previous algorithms, and in v, up to
three times; there is a maximum of 7.5 in v where the previous algorithms were at 2.5. These last results emphasize the
capability of SAGG-RIAC to drive the learning process inside
reachable areas which are not easily accessible (hardly discovered by chance).
4.6. Quantitative Results
In this section, we aim to test the eﬃciency of the learned
forward/inverse models to guide the quadruped to reach a set of
goal positions from an independently generated test database.
Here we consider a test database of 100 goals, generated independantly and covering approximately uniformly the reachable
part of the task space, and compute the distance between each
goal attempted, and the reached position. Fig. 18 shows performances of the 4 methods introduced previously. First of all, we
can observe the higher eﬃciency of SAGG-RIAC compared to
the other three methods which can be observed after only 1000
iterations. The high decreasing velocity of the reaching error (in
the number of experimentations) is due to the consideration of
regions limited to a small number of elements (30 in this experiment). It allows creating a very high number of regions within
a small interval of time, which helps the system to discover and
focus on reachable regions and its surrounding area.
ACTUATOR-RIAC shows slightly more eﬃcient performances than ACTUATOR-RANDOM. Also, even if SAGG-
RANDOM is less eﬃcient than SAGG-RIAC, we can observe
its highly decreasing reaching errors compared to ACTUATOR
methods, which allows it to be signiﬁcantly more eﬃcient than
these method when considered at 10000 iterations. Again, as in
the previous experiment, we can also observe that SAGG-RIAC
does not only allow to learn faster how to master the sensorimotor space, but that the asymptotic performances also seem to be
better .
4.7. Conclusion of Results for the Quadruped Experiment
These experiments ﬁrst emphasize the high eﬃciency of
methods which drives the exploration of motor synergies in
terms of their eﬀects in the task space. As illustrated by qualitative results, SAGG methods, and especially SAGG-RIAC, allows driving the exploration in order to explore large spaces
containing areas hardly discovered by chance, when limits of
reachability are very diﬃcult to predict. Then, quantitative results showed the capability of SAGG-RANDOM and SAGG-
RIAC methods to learn inverse models eﬃciently when considering highly-redundant robotic systems controlled with motor
synergies.
5. Experimental Setup 3: Learning to Control a Fishing
Rod with Motor Synergies
5.1. Robotic Setup
This experiments consists of having a robot learning to control a ﬁshing rod (with a ﬂexible wire) in order to attain certain
positions of the ﬂoat when it touches the water. This setup is
simulated using the Breve simulator, such as in the previous
experiment. The rod is ﬁxed on a 4 DOF arm controlled with
motor synergies which aﬀect the velocity of each joint, and are
parameterized by the values θ = (v1, v2, v3, v4), vi ∈[0; 1]. More
precisely, for each experimentation of the robot we use a lowlevel pre-programmed PID controller which tracks the desired
velocity vi of each joint i during a ﬁxed short amount of time
(2 seconds), starting from a ﬁxed rest position, until suddenly
Figure 19: 4 degrees-of-freedom arm with a ﬁxed ﬁshing rod at its extremity. The arm is controlled using motor synergies which aﬀect the velocity of
each joint, and are parameterized by 4 values. Experiments consider a twodimensional task space x, y which corresponds to the position of the ﬂoat when
touching the water after performing a movement.
stopping the movement. During the movement, as well as a few
second after, we monitor the 3D position of the ﬂoat in order
to detect a potential contact with the water (a ﬂat plane corresponding to the water level). If the water is touched, we extract
the 2D coordinates (x, y) of the ﬂoat on the plane (if not, we do
not consider this trial). These coordinates, as well as the parameters of the synergies will be used to describe the forward model
of the system as (v1, v2, v3, v4) →(x, y). Learning will thus be
performed while recording each set {(v1, v2, v3, v4), (x, y)}i as a
learning exemplar. In such a sensorimotor space, studying the
behavior of SAGG-RIAC is relevant according to the ﬂexible
aspect of the line, which makes this system very diﬃcult to
model analytically, because it is highly redundant and highly
sensitive to small variations of inputs. In the following experiment, the task space will consist of a limited area of the water
surface. We will consider the basis of the arm as ﬁxed on the
coordinates (0, 0), the limits of the task space will be ﬁxed to
[−3; 3] × [−3; 3] while the reachable region corresponds to a
disk whose radius is 1, and can be contained in [−1; 1]×[−1; 1]
(see Fig. 19).
5.2. Qualitative Results
20 shows histograms of the repartition of positions
reached by the ﬂoat on the water surface computed after 10000
”water touched” trials (a ”water touched” trial corresponds
to a reaching attempt where the ﬂoat eﬀectively touches the
surface), after running ACTUATOR-RANDOM and SAGG-
RIAC exploration processes. The point situated at the center
corresponds to the base to which the arm handling the ﬁshing
rod is situated (see Fig. 19). While observing the two ﬁgures,
we can note a repartition of positions situated inside a disk,
which radius delimits position reached when the line is maximally slack. Yet, the distribution of reached (and reachable) positions within this disk is both asymetrical among and between
the two exploration processes. The asymetries on each ﬁgure
are in fact reﬂecting the asymetries of the robot setup (see Fig.
19): the geometry of the robot is not symmetric and its starting/rest conﬁguration is also not symmetric. Coupled with the
structure of motor primitives, this makes that the structure of
the reachable positions is complex and asymetric, and this can
be observed especially in the ACTUATOR-RANDOM sub-
ﬁgure, since it shows the asymetric distribution of ﬂoat position
reached when the parameters of the action primitives are sampled uniformly (and thus symmetrically). Comparing the two
histograms, we note that SAGG-RIAC drives the exploration
toward positions of the ﬂoat not explored by ACTUATOR-
RANDOM, such as the large part situated at the bottom of
the reachable area.
Thus, SAGG-RIAC drives here the exploration toward more diverse regions of the space. SAGG-
RIAC is therefore able to avoid spending large amounts of
time exclusively guiding the exploration toward the same areas, as ACTUATOR-RANDOM does. Extended experimentation with this setup showed that the distribution of reached
points with SAGG-RIAC (right sub-ﬁgure) corresponds closely
to the actual whole reachable space. Eventually, these qualitative results emphasize that SAGG-RIAC is able to drive the
exploration process eﬃciently when carried out with highly redundant and complex robots with compliant/soft parts.
ACTUATOR-Random
Figure 20: Histograms of positions reached by the ﬂoat when entering in contact with the water in the ﬁshing experiment, after 10000 contact ﬂoat/water,
using ACTUATOR-RANDOM and SAGG-RIAC exploration methods.
5.3. Quantitative Results
21 shows the mean reaching errors obtained using
ACTUATOR-RANDOM and SAGG-RIAC, statistically computed after 10 experiments with diﬀerent random seeds. Here,
the comparison of these two methods shows that SAGG-RIAC
led to signiﬁcantly more eﬃcient results after 1000 successful
trials. Also, after 6000 trials, we can observe a small increase
in reaching errors of SAGG-RIAC. This phenomenon is due
to the discovery of new motor synergies which led to already
mastered goal positions. This discovered redundancy reduces
the generalization capability for computing the inverse model
for a small amount of time until these new parameters of motor
synergy have been explored enough to disambiguate the invert
model (i.e. two distinct local inverse models are well encoded
and do not interfere).
6. Conclusion and Future Work
This paper introduced the Self-Adaptive Goal Generation architecture, SAGG-RIAC, for active learning of inverse models in robotics through intrinsically motivated goal exploration.
Reaching Error
Number of Actions (time steps)
ACTUATOR-Random
Figure 21: Quantitative results for the ﬁshing experiment measured using the
reaching error over the number of experimentations.
First, we demonstrated the high eﬃciency of learning inverse
models by performing an exploration driven by the active selfgeneration of high-level goals in the parameterized task space
instead of traditional motor babbling speciﬁed inside a lowlevel control space. Active exploration in the task space leverages the redundancy often characterizing sensorimotor robotic
spaces: this strategy drives robots to learn a maximal amount
of tasks (i.e. learn to generate in a controlled manner a maximal number of eﬀects in the task space), instead of numerous
ways to perform the same tasks (i.e. learn many action policies to achieve the same eﬀect in the task space). Coupling
goal babbling and sophisticated intrinsically motivated active
learning also allows a robot to perform eﬃcient autonomous
learning of its limits of reachability, and of inverse models with
unknown high-dimensional body schemas of diﬀerent architectures. Intrinsically motivated active learning was here driven
by the active stochastic search of areas in the task space where
competence progress is maximal. This also allowed emerging
developmental trajectories by driving the robot to progressively
focus and learn tasks of increasing complexities, while discovering its own limits of reachability, avoiding to spend much exploration time trying to perform impossible tasks.
While we showed that such an approach could allow eﬃcient learning when the action space was continuous and highdimensional, the experiments performed here were assuming
that a low-dimensional task space was initially provided. It is
frequent to have such low-dimensional task spaces for useful
engineering problems in robotics, where one can assume that
an engineer helps the robot learner by designing by hand the
task space (including the choice of the variables and parameters specifying the task space). On the other hand, if one would
like to use an architecture like SAGG-RIAC in a developmental framework, where one would not assume low-dimensional
task spaces pre-speciﬁed to the robot, some additional mechanisms should be added to equip the robot with the following
two related capabilities:
• Find autonomously low-dimensional task spaces. Indeed,
a too high dimension of a task space would make the evaluation of “competence progress” suﬀer from the curse of
dimensionality;
• Explore actively multiple task spaces (potentially an openended number of task space), thus opening the possibility
to learn ﬁelds of skills which may be of diﬀerent kinds;
There are several potential approaches that could be used to
address these issues that include:
• Mechanisms for higher-level stochastic generation of
task spaces, and their active selection through global measures of competence progress, forming an architecture
with three levels of active learning (active choice of a task
space inside a space of tasks spaces, active choice of goals
inside the chosen task space, and active choice of actions
to learn to reach the chosen goal) would be a natural extension of the work presented in this article.
• Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive
the attention of a robot toward particular task spaces,
through physical guidance or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment , may be introduced. Inverse reinforcement learning mechanisms, which
are able to extract reward functions thanks to examples of
action policies could also be seen as a mean to infer interesting task spaces from human demonstrations .
Social guidance may also be used as a mechanism to bootstrap the evaluation of competence progress, and the identiﬁcation of zones of reachability, in very large or highdimensional spaces such as shown in , which presents
an approach to combine intrinsically motivated learning
like SAGG-RIAC with techniques for learning by demonstration.
• Maturational
constraints:
highly simpliﬁes the learning process by using goal babbling and drives it eﬃciently thanks to intrinsic motivations, learning still have to begin by a period of random
exploration in order to discriminate unreachable areas as
well as areas of diﬀering interests. This becomes a problem when the volume of reachable areas in the task space
is a lot smaller than the task space itself or when the task
space becomes itself high-dimensional. An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their
learning and development by numerous physiological and
cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities
of their brain . For instance, infants
have a reduced visual acuity which prevents them from accessing high visual frequencies as well as distinguishing
distant objects. This acuity then progressively grows as
the maturation process evolves. Using such constraints in
synergy with goal babbling and intrinsic motivation, such
as explored in , would potentially allow to constrain
and simplify further learning since the ﬁrst actions of the
robot , and could be crucial when considering
life-long learning in unbounded task spaces.
Acknowledgment
We thank everyone who gave us their feedback on the paper.
This research was partially funded by ERC Grant EXPLOR-
ERS 240007