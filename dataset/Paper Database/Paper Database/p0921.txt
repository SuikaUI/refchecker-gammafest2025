Biometrika Trust
The Probable Error of a Mean
Author(s): Student
Reviewed work(s):
Source: Biometrika, Vol. 6, No. 1 , pp. 1-25
Published by: Biometrika Trust
Stable URL: .
Accessed: 23/10/2012 11:13
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
 
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact .
Biometrika Trust is collaborating with JSTOR to digitize, preserve and extend access to Biometrika.
 
MARCH, 1908
BIOMETRIKA.
THE PROBABLE ERROR OF A MEAN.
By STUDENT.
Inttroduction.
ANY experiment may be regarded as forming an individual of a " population"
of experiments which might be performed under the same conditions. A series
of experiments is a sample drawn from this population.
Now any series of experiments is only of value in so far as it enables us to form
a judgment as to the statistical constants of the population to which the experi-
ments belong. In a great ntimber of cases the question finally turns on the value
of a mean, either directly, or as the mean difference
between the two quantities.
If the number of experiments be very large, we may have precise information
as to the value of the mean, but if our sample be small, we have two sources of
uncertainty:-(1) owing to the "error of random sampling" the mean of our series
of experiments deviates more or less widely from the mean of the population, and
(2) the sanmple
is not sufficiently
large to determine what is the law of distribution
of individuals. It is usual, however, to assume a normal distribution,
because, in
a very large number of cases, this gives an approximation so close that a small
sample will give no real information
as to the manner in which the population
deviates from normality: since some law of distribution mliust be assumed it is
better to work with a curve whose area and ordinates are tabled, and whose
properties are well known. This assumption is accordingly made in the present
paper, so that its conclusionis
are not strictly applicable to populations known not
to be normally distributed; yet it appears probable that the deviation from
normality must be very extreme to lead to serious error. We are concerned here
solely with the first of these two sources of uncertainty.
The usual method of determining the probability that the mean of the popula-
tion lies within a given distance of the mean of the sample, is to assume a normal
distribution about the mean of the sample with a standard deviation equal to
s/a/n, where s is the standard deviation of the sample, and to use the tables of
the probability integral.
Biometrika vi
The Probable Error of a Mean
But, as we decrease the number of experiments,
the value of the standard
the sample of experiments
itself subject to an increas-
ing error,
until judgments
reached in this way may become altogether
misleading.
In routine work there are two ways of dealing with this difficulty:
experiment
may be repeated many times, until suich a long series is obtained that
the standard deviation is determined
once and for all with sufficient
This value can then be used for subsequent
series of similar experiments.
(2) Where experiments
are done in duplicate in the natural course of the work,
the mean square of the difference
between corresponding
pairs is equal to the
standard deviation of the population multiplied
by v/2. We can thus combine
several series of experiments
for the purpose of determining
the standard
deviation. Owing however
to secular change, the value obtained is nearly always
too low, successive
experitnents
being positively
correlated.
There are other experiments,
which cannot easily be repeated very
often; in such cases it is sometimes
to judge of the certainty
results from a very small sample, which itself affords
the only indication
variability. Some chemical, many biological, and most agricultural
scale experiments
belong to this class, which has hitherto
been almost outside the
range of statistical
Again, although it is well known that the method of using the normal curve
is only trustworthy
when the sample is "large," no one has yet told us very
clearly where the limit between "l large " and " small " samples is to b-e drawn.
The aim of the present paper is to determine
the point at which we may use
the tables of the probability
in judging of the sig,nificance
of the mean of
a series of experiments,
and to furnish
alternative
tables for utse when the number
of experiments
is too few.
The paper is divided into the following
nine sections:
I. The equation is determined
of the curve which represents
the frequency
distribution
of standard
deviations
of samples drawn from
population.
IJ. There is shown to be no kind of correlation
between the mean and the
standard deviation of such a sample.
III. The equation is determined
of the curve representing
the frequency
distribution
of a quantity
z, which is obtained by dividing the distance between
the mean of a sample and the mean of the population
by the standard deviation
of the sample.
IV. The curve found in I. is discussed.
V. The curve found in III. is discussed.
VI. The two curves are compared with some actual distributions.
VII. Tables of the curves found in III. are given for samples of different
VIII and IX. The tables are explained
and some instances
are given of their
X. Conclusions.
BY STUDENT
SECTION I.
Samples of n individuals are drawh out of a population distributed normally,
to find an equation which shall represenit
the frequency of the standard deviations
of these samiiples.
If s be the standard deviation found from a sample x1 x2....x, (all these being
measured from the mean of the population), then
= S (X12) _ (X12) 2i (x1x2)
Summing for all samples and dividing by the number of samples we get the
mean value of S2 which we will write s2,
where , is the second moment coefficient
in the original normal distribution
since x1, x'2, etc., are not correlated and the distribution is normal, products in-
volving odd powers of x, vanish on summing, so that 2S(xx1a'2)
is equal to 0.
If MI; represent the Rth moment coefficient
of the distribution of S2 about the
end of the range where se = 0,
M =2 (n-1)
(S ($1))2}
= (iS (a2)j2
- 2S (X,2) (S (XI)y2 + (S (aXI))4
S (X'4) + 2S (x,2x22) 2S (X14)
4S (x,2x22) +S
68_(X,2a'2
14 .- ?+other terms involving odd powers of x, etc.,
whioh will vanish on summation.
Now S(X14) has n terms but S(x,2x22) has jn(n-1),
hence summing for all
samples and dividing by the number of samiiples
2. (n - 1)
(i - 1) {ni -2n + 3}.
Now since the distribution of a is normal, ,. = 3,u22, hence
+ 2 - 2n + 31.=A
?2 (n - 1) (n +41-
The Probable Error of a Mean
In a similar tedious way I find:
-1)(n+1)(n+3)
1) (n+ 1) (n +3) (n +5)
The law of formnation
of these moment coefficients
appears to be a simple one,
but I have not seen my way to a general proof.
If now MR be the Rth moment coefficient
of e2 about its mean, we have
+1)-(n-1)}=
(l(n+l)(n+
{n2+ 4n+3-6t+
6-n2+2n-1}-83(-
2p2'(n -1)(n+3)
A4 = 3(2i +3)
18,M2'it-I
2/32-383,-6
(n + 3)-24-6
Consequently a curve of Professor Pearson's type III. may be expected to fit
the distribution of s2.
The equation referred
to an origin at the zero end of the curve will be
y = GxPe-X,
M3 8n2,u23(n-
Consequently the equation becomes
CX 2 e 2t,,
which will give the distribution of 82.
The area of this curve is C| x
BY STUDENT
The first moment coefficient
about the end of the range will therefore
Cf x2e2b2dx
The first part vanishes at each limit and the second is equal to
and we see that the higher moment coefficients
will be formed by multiplying
successively by n + 1 /A22
J etc., just as appeared to be the law of formation
of M2', Ms', M4', etc.
Hence it is probable that the curve found represents the theoretical distribu-
tion of 82; so that although we have no actual proof we shall assume it to do so in
what follows.
The distribution
of s may be found from this, since the frequency of s is equal
to thiat of 82 and all that we must do is to compress the base line suitably.
- q (82) be the frequency
curve of S2
(82) = y2ds,
y2ds = 2y,sds,
y =2Cs(S2)2
is the distribution
This reduces to
2Csn-2 e 2A2 .
Hence y = AXn-2e
will give the frequiency
distribution of standard devia-
tions of samples of n, taken out of a population distributed normally with standard
deviation a. The constant A nmay
be found by equating the area of the curve as
Area = A fx2e
(Let 4p represent xPe
X I P- d (-e To) dox
n,nX2 . = 00
=- L(p- 1) 1p2,
since the first part vanishes at both limits.
The Probable Error of a Mean
By continuing
this process we find
2 (n -3) (n -5) ***3. 1 Io
2 (n-3)('n-5).
as n is even or odd.
Hence if n be even,
(n- 3)(n-5)... 3. 1 I
and if it be odd
(n -3) (n -) ...4 2(_
Hence the equation may be written
Y X-)n5~.31Tf2
Z"2-?(even)
2 (f~l )f2 l
where N as usual represents
the total frequency.
SECTION II.
To show that there is no correlation
between (a) the distance of the mean of
a sample from the mean of the population and (b) the standard deviation
sample with normal distribution.
(1) Clearly positive and negative positions
of the mean of the sample are
equally likely,
and hence there cannot be correlation
between the absolute value
of the distance of the mean from the mean of the population
and the standard
deviation,
but (2) there might be correlation
between the square of the distance
and the square of the standard
deviation.
U2 = (S())
and 32 = S__'32)
Then if mnl',
M1' be the mean valuies of u2 and 82, we have by the preceding
part M1'=p
BY STUDENT
- S (X12) (S (XI))2
(S (X12))2
+ 2 S (X1IX2). S (X12) S(X14)
_ -other terms of odd order which will vanish on summation.
for all values and dividing
by the number
of cases we get
BRu82satoagA + nzM,
2(n -1) _s4
where R,,2, is the correlation
u2 and e2.
at62 qp + 8122
1) /= 2 (n-i)
Hence Rp8aouq,82 = 0 or there is no correlation between u2 and s8.
SECTION III.
To find the equation representing
the frequency
distribution
of the mneans
of samples of n drawn from a normal population,
the Inean being expressed in
terms of the standard
of the sample.
We have y =
as the equation representing
the distribution
the standard deviation of a sample of n, when the samples are drawn from a
population
with standard
Now the means of these samples of n are distributed
to the equation
=/XNe~ 2as
and we have shown that there is no correlation
between x, the distance of the
mean of the sample,
and s, the standard
of the sample.
Now let us suppose x measured in terms of 8, i.e. let us find the distribution
If we have y, = + (x) and y2 = f (z) as the equations
representing
the frequency
of x and of z respectively,
yldX=y2dz=Y2
.a. y2 = sY1.
* Airy, Theory of Errors of Observations,
Part II. ? 6.
The Probable Error of a Meant
is the equiation representing the distribution of z for samples of ' with standard
deviation s.
Now the chance that s lies between s and s + ds is:
which represents the N in the above equation.
Hence the distribution
of z (lue to values of s which lie between s and s + ds is
and summing for all values of s we have as an equation giving the distribution
ns2 (1 +z2)
ff 2 8(sn-1e
By what we have already proved this reduces to
3(1 + Z2)2 if n be odd,
Since this equation is independent of a- it will give thje distribuition
distance of the mean of a sample from the mean of the population expressed in
terms of the standard deviation of the sample for any normal population.
SECTION IV.
Some Properties
of the Standard Deviation Frequency
By a similar method to that adopted for finding the constant we may find the
mean and moments: thus the mean is at 119-1,
which is equal to
(1% -2) (n-4)
(n - 3) (n - 5)
BY STUDENT
The second moment about the end- of the range is
The third momnent
about the end of the range is equal to
In+_ In+1 I n-i
= a2 x the mean.
The fourth momeint
about the end of the range is equal to
(n - 1)(n1 + 1)
If we write the distanice of the mean from the end of the range Do- and the
noments about the end of the range VI, v2, etc.
From this we get the moments
about the mean
{nD-3 (z - 1) D + 2D3}=
{2D2 - 2n + 3},
1- 4D2n + 6 (n -1) D2 - 3D4} =
1- D2 (3D2 - 2n + 6)}.
It is of interest to firnd
out what these become when it is large.
In order to do this we nmust
find out what is the value of D.
Now Wallis's expression for 7r derived from the infinite product value of sin x is
32 as52.. (2n _-1)2
If we assume a quantity 0 = ao + a' + etc.) which we may add to the 2n + 1
in order to make the expression approximate more rapidly to the truth, it is easy
to show that 6--+
-I--etc. atid we get
_ 2'.* 42*6 ... (2)2
=12 3%2..(
2 16n. i. 3 -. aa ... (2n -1)
From this we find that whether n be even or odd D2 approximates to n 2 +
when n is large.
* This expression
will be found
to give a much closer approximation
to wr than Wallis's.
Biometrika vi
Thte Prcobable
Error of a Mean
Substituting this value of D we get
Consequently the value of the standard deviation of a standard deviation which
we have found (
the same as that found
for the normal
curve by Professor Pearson (o-/V/2n)
when n is large enough to neglect the 1/4n in
comparison with 1.
Neglecting terms of lower order than 1 we find
n (471 - )
-2n (1+*2n)
Consequently as n increases 2 very soon approaches the value 3 of the normal
curve, but ,8 vanishes more slowly, so that the curve remains slightly skew.
DIGRAM I. Frequency curve giving the distribution
of Standard Deviations of samples of 10 taken
from .a normal population.
Equation y
Diagram I shows the theoretical distribution of the S.D. found from samples
3/=7.5.3Vtq<9e
SECTION V.
Some properties
of the cu.rve
Writing z= tan 9 the equation becomes y
... etc. x cosiB 9, which
an easy way of drawinlg
the curve. Also dz = dO/cos2
BY STUDENT
He-nce to find the area of the curve between any limits we must find
etc x fcos'-29d9
n -3 n -sa
'cosn-4 OdO +[cs'si
n - .5 n - .7.. etc. Cosn- Ode +
... etc. [coso3 9 sit) 9],
and by continuing the process the integral may be evaluated.
For example, if we wish to find the area between 0 and 9 for n = 8 we have
c- - s-dOS
=-8+!.Cos9sin4
cos39sii9+i
.-cos9sin9i
and it will be noticed that for n =10 we shall merely have to add to this same
expression the term A - - - cos7 9 sin 9.
The tables at the end of the paper give the area between - oo and z
(or =-- and
=tan-l z).
This is the same as *5 + the area between 9=0, and 9= tan- z, and as the
whole area of the curve is equial to 1, the tables give the probability that the
mean of the sample does not differ by more than z times the standard deviation
of the sample from the mean of the population.
The whole area of the curve is equal to
... etc. x |
cos-2 OdO,
2 - 3'n n- .5
and since all the parts between the limits vanish at both limits this reduces to 1.
Similarly the second moment coefficient
is equal to
= -n82 . n_
... etc. x
The Probable Error of a Mean
Hence the standard deviation of the curve is 1//n -3.
The fourth mionment
coefficielnt
is equial to
2 9 tan4 OdO
..... etc. x |
(cosqO- 2 cosql4O +
nt- 2 n -4
(n-3)(n-5)
The odd moments are of coturse
zero as the curve is symmetrical,
18 = 3 (n - 3)= 3 +
Hence as n increases the cutrve approaches the normal curve whose standald
deviation is 1/Vn-- 3.
,82 however is always greater than 3, indicating that large deviations are more
common than in the normal curve.
DIAGBAM Il.
Solid curve Y = Sx - . -, , 2 cos0? 0,
x/s'= tan 0.
Broken line curve Y=
N e is, the normal curve wvith
the same S.D.
3~~~~~~~~~~
N~~~~~~~~~~~~~~~~~
Distance of mean fionm
mean of population
I have tabled the area for the normal curve with standard deviation 1/V7 so as
to compare with my curve for n =10*.
It will be seen that odds laid according
to either table would not seriously differ till we reach z = -8, where the odds are
about 50 to 1 that the mean is within that limit: beyond that the normal curve
gives a. false feeling of secturity,
for example, according to the normal curve it is
99,986 to 14 (say 7000 to 1) that the mean of the population lies between -o
and + 1 3s whereas the real odds are only 99,819 to 181 (about 5.50 to 1).
* See p. 19.
BY STUDENT
Now 50 to 1 corresponds
to three times the probable error in the normal curve
and for most purposes would be considered significant; for this reason I have only
tabled my curves for values of n not greater tharn 10, but have given the n = 9
and n = 10 tables to one further
place of decimals. They can be use(d as foundations
for finding values for larger samples*.
The table for n =2 can be readily constructed by looking out 9 = tan-' z in
Chambers' Tables and then *5 + 9/7r gives the corresponding
Similarly i sin 9 +*5 gives the values when n = 3.
There are two points of interest in the n = 2 curve. Here s is equal to half
the distance between the two observations. tan-' 5
8so that between + s and
-s lies 2 x
.7r x 1 or half the probability,
i.e. if two observations have been made
and we have no other informationi,
it is an even chance that the mean of the
(normal) population will lie between thema. On the other hand the second moment
coefficient
or the stan(dard deviation is infinite while the probable error is finite.
SECTION VI. Practical Test of the foregoing Equations.
Before I had succeeded in solving my problem analyticallv, I had endeavoured
to do so empirically. The material used was a correlation table containing the
height and left middle finger measurements of 3000 criminals, fromn
a paper by
W. R. Macdonell (Biometrika, Vol. i. p. 219). The measurements were written
out on 3000 pieces of cardboard, which were then very thoroughly shuffled and
drawn at random. As each card was drawn its numbers were written down in a
book which thus contains the measurements
of 3000 criminals in a randomn
Finally each consecutive set of 4 was taken as a sample-750 in all-and the
mean, standard deviation, and correlation
t of each sample determined. The
difference
between the mean of each sample and the mean of the population was
then divided by the standard deviation of the sample, giving us the z of Section III.
This provides us with two sets of 750 standard deviations and two sets of
750 z's on which to test the theoretical resuilts arrived at. The height and left
middle finger correlation table was chosen because the distribution of both was
approximately
normal and the correlation was fairly high. Both frequency
deviate slightly from normality,
the constants being for height 8,1 = '0026,
192=3-175,
and for left middle finger lengths 19,=0030, ,/2=3-140,
and in consequence
there is a tendency for a certain number of larger standard deviations to occur
than if the distributions
were normal. This, however, appears to make very little
difference
to the distribution
* E.g. if n = 11, to the correspondilng
value for it = 9, we add X x x x i x
cos8 O sill
: if it = 13
we add as well 9 X x
X cosi0 sin 0 and so on.
t I hope to publislh the results of the correlation work slhortly.
The Probable Error of a Mean
Another thing which interferes
with the comparison is the comnparatively
groups in which the observations occur. The heights are arranged in 1 inch groups,
the standard deviation being only 2-54 inches: while the finger lengths were
originally grouped in millimetres,
but unfortunately
I did not at the time see the
importance of having a smaller unit, and condensed them into two millimetre
groups, in terms of which the standard deviation is 2-74.
Several curious results follow from takiiig samples of 4 fiom material disposed
in such wide groups. The following points may be noticed:
(1) The means only occuir as multiples of *25.
(2) The standard deviations occur as the square roots of the following
of numbers n, n + -19, n + *25, A + 50, n + *69, 2n + *75.
(3) A standard deviation belonging to one of these groups can only be
associated with a meau of a particular kind; thus a standard deviation of /2 can
only occur if the mean differs by a whole number from the group we take as
origin, while V1A69 will only occur when the mean is at n + *25.
(4) All the four individuals of the sample will occasionally come from the
same group, giving a zero value for the standard deviation. Now this leads to an
infinite value of z and is clearly due to too wide a grouping, for although two men
may have the same height when measured by inches, yet the finer the measure-
ments the more seldom will they be identical, till finally the chance that four men
will have exactly the same height is infinitely
small. If we had smaller grouping
the zero values of the standard deviation might be expected to increase, and a
similar consideration
will show that the smaller values of the standard deviation
would also be likely to itncrease,
such as -436, when 3 fall in one group and 1
in an adjacent group, or -50 when 2 fall in two adjacent groups. On the other
hand when the individuals of the sample lie far apart, the argument of Sheppard's
correction
will apply, the real value of the standard deviation being more likely to
be smaller than that found owing to the frequency in any group being greater on
the side nearer the mode.
These two effects
of grouping will tend to neutralise each other in their effect
on the mean value of the standard deviation, but both will increase the variability.
Accordingly we find that the mean value of the standard deviation is quite
close to that calculated, while in each case the variability is sensibly greater. The
fit of the curve is not good, both for this reason and because the frequency
evenly distributed owing to effects (2) and (3) of grouping. On the other hand
the fit of the curve giving the frequency of z is very good and as that is the only
practical point the comparison may be considered satisfactory.
The following
are the figures for height -
Mean value of standard deviations; calculated 2-027 + *021
observed 2-026
Difference= -001
BY STUDENT
Standard deviation of standard deviations:
Calculated *8556 + 015
Observed *9066
Difference
Comparison of Fit. Theoretical
Equation: y= 16X 75
Sckleinterms! '^
of standard
deviationof I
population
Calculated
frequenicy 1
frequency 3
141 241 137 107
77 77i 64 52* 49| 35 28 12A 9
Differenice
+1j +4 -2|
-8 +421 -Ili
-14 -11 -4 -7 -51+4j|
P= 000,06 (about).
In tabling the observed frequency, values between *0125 and *0875 were
included in one group, while between *0875 and *0125 they were divided over the
two groups. As an instance of the irregularity
due to grouping I may mention
that there were 31 cases of standard deviations 1 30 (in terms of the grouping)
which is *5117 in terms of the standard deviation of the population, and they were
divided over the groups *4 to *5 and *5 to *6. Had they all been counted
in groups *5 to *6 x2 would have fallen to 29-85 and P would have risen to 03.
test presupposes random sampling from a frequency following the given
law, but this we have not got owing to the interference
of the grouping.
When, however, we test the z's where the grouping has not had so much effect
we find a close correspondence
between the theory and the actual resuilt.
There were three cases of infinite valuies of z which, for the reasons given
above, were given the next largest values which occurred, namely + 6 or - 6.
The rest were divided into groups of 1; 04, 05 and *06, being divided between
the two groups on either side.
The calculated value for the standard deviation of the frequency curve was
1 (? 017) while the observed was-1P039. The value of the standard deviation is
really infinite,
as the fourth moment coefficient
is infinite, but as we have arbi-
trarily limited the infinite cases we may take as an approximation
which the value of the probable error given above is obtained. The fit of the
curve is as follows:
The Probable Error of a Meant
Comparison of Fit. Theoretical Equiationt: y = 2 cos4 6, z =tan 0.
Scaleofza ~~~~~ j~~
Scale of z
Ij~~~~~~~~
Calculated
frequen3cy 5
9i 13i 34j 44* 78 1119
119 78i| 441 34.1 13.1 9i- 5
frequency 9 14j 1lj 33
119i 151k 122
26i 16 110
Difference +4
+ 3 -11 + 4
whence X2=12a44, P=-56.
This is very satisfactory,
especially when we consider that as a rule observa-
tions are tested against curves fitted from the mean and one or more other
moments of the observations, so that considerable correspondence is only to be
expected; while this curve is exposed to the full errors of random sampling, its
constants having been calculated quite apart from the observations.
III. Comparison
of Calculated Standard Deviation Frequency
Curve with 750 actual
Standard Deviations.
3 *4 *5 *f
*9 i-n 1.1 1.2 1-3 1-4 1 r 1-6 1,7 1-8 1-9 2-0 2-1 2-2
2,3 2-4 2-5
Scale of Standard Deviation of the Population
The left middle finger samples show much the same features as those of the
height, but as the grouping is not so large compared to the variability the curves
fit the observations more closely. Diagrams III.* and IV. give the standard devia-
tions and the z's for this set of samples. The results are as follows:
* There are three small mistakes
in plotting
the observed
values in Diagram III., which
make the fit
appear worse than it really is.
BY STUDENT
@3~~~~~~~~~~c
0 @3~~~~~~~~~~c
04~~~~~~~~~
II~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Biometrika VI
The Probable Error of a Mean
Mean value of standard deviations; calculated 2 186 + 023
observed 2-179
Difference= - *007
Standard deviation of standard deviations:
Calculated 9224 + 016
Difference
Comparison
of Fit. Theoretical
Equation: Y= V2
Scaleinterms |||
of standard
deviation of
populatton |
Calculated
frequeney 1j
45j 64i 78i
frequency 2
94k 68k 65j
73. 48j 40j 42k 20 22i
Difference
+k +3k + + 5 -
+ 12| + 7 -19i -16 + 2 -9k -4k + 9k 3 + 7k +2| -| +k
Calculated value of standard
deviation 1 (+ 017)
Difference =-018
Comparison
of Fit. Theoretical
Equation: y=2 cos",
I Scaleofz I aI
'~~~~~~~x~
I~~~~~~~~~~+
ICalculatedl
5 9k 13 3k44 78k 119 141 119 1
|frequency? 4 15ki
18 13k 144 75 1122 138 120k| 71 |46k| 36, 11 |9 |6|
Differenlce
+6i +4k -1 |-k| -3k ? 3 | 3I +1k-7k +2Klk+1i -2k| -
whence x2= 7 39, P=92.
A very close fit.
We see then that if the distribution
is approximately
normal our theory
us a satisfactory
measure of the certainty
to be derived from
a small sample in
both the cases we have tested; but we have an indication
that a fine grouping
BY STUDENT
of advantage. If the distribution is not normal, the mean atid the standard
deviation of a sample will be positively correlated, so that although both will have
greater variability,
yet they will tend to counteract each other, a mean deviating
largely from the general mean tending to be divided by a larger standard deviation
Consequently I believe that the tables at the end of the present paper may be
used in estimating the degree of certainty arrived at by the mean of a few
experiments,
in the case of most laboratory or biological work where the distribu-
tions are as a rule of a ' cocked hat' type and so sufficiently
nearly normal.
SECTION VII.
Tables of n-2 n
1 .- n even
for values of n from 4 to 10 inclusive.
Together with
e 2dx for comparison when n = 10.
For comparison
70705 *71846
*7733 *78961 *80423
'85465 '86970
'90251 '91609
'93600 '94732
97328 '98007
'98279 '98780
*9887 *99280 99539
'99528 '99713
'99688 '99819
'99791 '99885
'99859 '99926
'99903 '99951
'99933 '99968
-99953 '99978
9992 '99967 '99985
'9967 '9986
9994 '99976 99990
*9989 '9996 '99983 '99993
9997 '99987 99995
'9998 '99991 '99996
-9957 '9985
9998 -99993 99997
9998 '99995 99998
'9999 '99996 *99999
.9991 -9997 9999 .99997 99999
'99998 '99999
'9999 '99998 '99999
The Probable Error of a Mean
SECTION VIII.
Explanation of Tables.
The tables give the probability that the value of the mean, measured from the
mean of the population, in terms of the standard deviation of the sample, will lie
between - oo and z. Thus, to take the table for samnples
of six, the probability
of the mean of the population lying between - oo and once the standard
deviation of the sample is *9622 or the odds are about 24 to 1 that the mean of
the population lies between these limits.
The probability is therefore
*0378 that it is greater than once the standard
deviation and *0756 that it lies outside + 10 times the standard deviation.
SECTION IX. Illustrations
of Method.
Illustration
I. As an inistance of the kind of use which may be made of the
tables, I take the following
figures from a table by A. R. Cushny and A. R. Peebles
in the Journal of Physiology
for 1904, showing the different
effects of the optical
isomers of hyoseyamine
hydrobromide
in producing sleep. The sleep of 10 patients
was measured without hypnotic and after treatment (1) With D. hyoseyarnine
hydrobromide,
(2) with L. hyoseyamine hydrobromide. The average number of
hours' sleep gained by the use of the drug is tabuilated below.
The conclusion arrived at was that in the usual dose 2 was, but 1 was not, of
value as a soporific.
Additional
hours' sleep gained by the use of hyoseyamine
hydrobromide.
1 (Dextro-)
2 (Laevo-)
Difference (2-1)
Mean + *75
Mean + 2-33
Mean + 1 58
First let lls see what is the probability that 1 will on the average give increase
of sleep; i.e. what is the chance that the mean of the population of which these
experiments are a sample is positive. + .75 = 44 and looking out z = 44 in the
BY STUDENT
table for ten experiment we find by interpolating
between *8697 and *9161 that *44
corresponds
to '8873, or the odds are *887 to '113 that the mean is positive.
That is about 8 to 1 and would correspond in the normal curve to about
18 times the probable error. It is then very likely that 1 gives an increase of
sleep, but would occasion no surprise if the results were reversed by further
experiments.
If now we consider the chance that 2 is actually a soporific
we have the mean
increase of sleep = 1.9O or 1'23 times the S.D.
From the table the probability
corresponding
to this is -9974, i.e. the odds are nearly 400 to 1 that such is the
case. This corresponds to about 4-15 times the probable error in the normal
curve. But I take it the real point of the authors was that 2 is better than 1.
This we must test by making a new series, subtracting 1 from 2. The nmean
value of this series is + 1C58 while the S.D. iS 1-17, the mean value being + 1-35
From the table the probability is -9985 or the odds are about 666
to 1 that 2 is the better soporific. The low value of the S.D. is probably due to
the different
drugs reacting similarly on the same patient, so that there is corre-
lation between the results.
Of course odds of tlhis kind make it alrnost certain that 2 is the better soporific,
and in practical life such a high probability is in most matters considered as
a certainty.
Illustration
Cases where the tables will be useful are not uncommon in
agricultural work, and they would be more numerous if the advantages of being
able to apply statistical reasoning were borne in mind when planning the experi-
ments. I take the following instances from the accounts of the Woburn farming
experiments published yearly by Dr Voelcker in the Journal of the Agricultural
A short series of pot culture experiments were conducted in order to deter-
mine the causes which lead to the production of Hard (gltutinous)
wheat or Soft
(starchy) wheat. In three suiccessive
years a bulk of seed corn of one variety vas
picked over by hand and two samples were selected, one consisting of "hard"
grains and the other of " soft." Some of each of these were planted in both heavy
and light soil and the resulting crops were weighed and examined for hard and
soft corn.
The conclusion drawn was that the effect of selecting the seed was negligible
compared with the influence of the soil.
This conclusion was thoroughly
justified, the heavy soil producing in each case
nearly 100 per cent. of hard corn, but still the effect of selecting the seed could
just be traced in each year.
But a curious point, to which Dr Voelcker draws attention in the 2nd year's
report, is that the soft seeds produced the higher yield of both corn and straw. In
The Probable Error of a Mean
view of the well-known
fact that the varieties
which have a high yield tend to
produce soft corn, it is interesting
to see how much evidence the experiments
as to the correlation
between softness
and fertility
in the same variety.
Mr Hooker* has shown that the yield of wheat in one year is largely
determined
by the weather durinig
the preceding
harvest. Dr Voelcker's
may afford
a clue as to the way in which the seed is affected,
and would almost
the selection
of particular
soils for growing
seed wheatt.
The figures
are as follows,
the yields being expressed
in grammes
Light Heavy. Light Heavy Light Heavy
Yield of corn from soft seed
8-89 14-81 13,55
7-48 15-39 11'328
8,32 13-81 13,36
7V97 13-13 10,643
Difference
+*57 +P100 +*19
- 49 + 2-26 + *685
Yield of straw from soft seed 12-81 12,87 22-22 2021
13-97 22'57
10-71 12A48 21P64 20,26 11,71 18-96 15-927
Difference . ...
+ 39 + *78
-*05 +2'66 +3-61 +515
If we wish to find the odds that soft seed will give a better yield of corn on the
we divide the average difference
by the standard
deviation,
Looking this up in the table for n =6 we find p= 9465 or the odds are
535, about 18: 1.
for straw s = 1-20, p = '9782, and the odds about 45:1.
In order to see whether
such odds are sufficient
for a practical
man to draw a
conclusion,
I take another set of experiments
in which Dr Voelcker
pares the effects
of different
artificial
manures used with potatoes on the large
The figures
the difference
between the crops grown with the use of
sulphate of potash and kainit respectively
in both 1904 and 1905.
awt. qr. lb.
ton owt. qr. lb.
+ 10 3 20: + 1
10 1 26 (two experiments in each year).
1905 + 6 0
* Journal of Royal Statistical Society, 1907.
t And perhaps a few experiments
to see whether there is a correlation between yield and ' mellow-
ness' in barley.
BY STUDENT
The average gain by the use of sulphate of potash was 1525 cwt. and the
S.D. 9 cwt., whence, if we want the odds that the conclusion given below is right,
z= 117 corresponding,
when n = 4, to p = *9698 or odds of 32: 1; this is midway
between the odds in the former
example. Dr Voelcker says ' It nmay
now fairly be
concluded that for the potato crop on light land 1 cwt. per acre of sulphate of
potash is a better dressing than kainit.
As an example of how the tables should be used with caution, I take the
followving
pot culture experiments to test whether it made any difference
large or small seeds were sown.
Illustration III.
In 1899 and in 1903 " head corn " and " tail corn " were taken
from the same bulks of barley and sown in pots. The yields in grarnmes were
as follows:
Large seed .....
Small seed .....
The average gain is thus *55 and the S.D. '05, giving z = 11. Now the table
for n = 2 is not given, but if we look up the angle whose tangent is 11 in
Chambers' tables,
tan-' 11 + 5 = 840 47 +5 = 97 1
so that the odds are about 33: 1 that small corn gives a better yield than large.
These odds are those which would be laid, and laid rightly,
by a man whose only
knowledge of the matter was contained in the two experitnents. Anyone cbn-
versant with pot culture would however know that the difference
between the two
results would generally be greater and would correspondingly mnoderate
certainty of his conclusion. In point of fact a large scale experiment confirmed
the result, the small corn yielding about 15 per cent. more than the large.
I will conclude with an example which comes beyond the range of the tables,
there being eleven experiments.
To test whether it is of advantage to kiln-dry barley seed before sowing, seven
varieties of barley were sown (both kiln-dried and not kiln-dried) in 1899 and four
in 1900; the results are given in the table.
It will be noticed that the kiln-dried seed gave on an average the larger yield
of corn and straw, but that the quality was almost always inferior. At first sight
this might be supposed to be due to superior germinating power in the kiln-dried
seed, but my farming friends tell me that the effect of this would be that the
kiln-dried seed would produce the better quality barley. Dr Voelcker draws the
conclusion "In such seasons as 1899 and 1900 there is no particular advantage in
kiln-drying
before sowing." Our examination completely jtustifies
this and adds
The Probable E1rror qf a Mean
"and the quality of the resulting barley is inferior though the yield may be
lbs. head corn per acre
Price of head corn in
Value of crop per acre
shillings per quarter
cwts. straw per acre
in shillings
Diff. N. K. D.
Diff. N. K. D. K. D.
Diff. N. K. D. K. D.
2463 1 - 33
184165 18752
14468 +114
Deviation*
* Straw being valtied at 15s. per ton.
In this case I propose to use the approximation given by the normal curve
with standard deviation V(
and therefore use Sheppard's tables, looking up
the difference
divided by s8
The probability in the case of yield of corui per
acre is given by looking up 3237 = 1 51 in Sheppard's tables. This gives p = 934,
or the odds are about 14 :1 that kiln-dried corn gives the higher yield.
Similarly -9 = 325, corresponding
to p =9994,* so that the odds are very
great that kiln-dried seed gives barley of a worse quality than seed which has not
been kiln-dried.
Similarly it is about 11 to 1 that kiln-dried seed gives more straw and about
2 :1 that the total value of the crop is less with kiln-dried seed.
* As pointed out in Section V. the normal curve gives too large a value for p when the probability
is large. I find the true value in this case to be p = 9976. It matters little however to a conclusion of
this kind whether the odds in its favour are 1,660: 1 or merely 416: 1.
BY STUDENT
SECTION X.
Conolutsion8.
I. A curve has been found representing
the frequency
distribution
of standard
deviations of samples drawn from a normal population.
II. A curve has been found representing
the frequency
distribution of values
of the means of such samples, when these values are measured from the mean of
the population in terms of the standard deviation of the sample.
It has been shown that this curve represents the facts fairly well even
when the distribution
of the population is not strictly normal.
IV. Tables are given by which it can be judged whether a series of experiments,
however short, have given a result which conforms to any required standard of
accuracy or whether it is necessary to continue the investigation.
Finally I should like to express iny thanks to Professor Karl Pearson, without
whose constant advice and criticism this paper could not have been written.
Biometrika vi