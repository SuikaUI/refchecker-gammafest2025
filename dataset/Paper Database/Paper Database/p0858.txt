EagleEye: Fast Sub-net Evaluation for Eï¬ƒcient
Neural Network Pruning
Bailin Li1, Bowen Wu2, Jiang Su1, Guangrun Wang2, and Liang Lin1,2
1 Dark Matter AI Inc.
2 Sun Yat-sen University
 , {wubw6,wanggrun}@mail2.sysu.edu.cn, ,
 
Abstract. Finding out the computational redundant part of a trained
Deep Neural Network (DNN) is the key question that pruning algorithms
target on. Many algorithms try to predict model performance of the
pruned sub-nets by introducing various evaluation methods. But they
are either inaccurate or very complicated for general application. In this
work, we present a pruning method called EagleEye, in which a simple
yet eï¬ƒcient evaluation component based on adaptive batch normalization is applied to unveil a strong correlation between diï¬€erent pruned
DNN structures and their ï¬nal settled accuracy. This strong correlation
allows us to fast spot the pruned candidates with highest potential accuracy without actually ï¬ne-tuning them. This module is also general to
plug-in and improve some existing pruning algorithms. EagleEye achieves
better pruning performance than all of the studied pruning algorithms
in our experiments. Concretely, to prune MobileNet V1 and ResNet-50,
EagleEye outperforms all compared methods by up to 3.8%. Even in
the more challenging experiments of pruning the compact model of MobileNet V1, EagleEye achieves the highest accuracy of 70.9% with an
overall 50% operations (FLOPs) pruned. All accuracy results are Top-1
ImageNet classiï¬cation accuracy. Source code and models are accessible
to open-source community.3
Keywords: Model Compression; Neural Network Pruning;
Introduction
Deep Neural Network (DNN) pruning aims to reduce computational redundancy
from a full model with an allowed accuracy range. Pruned models usually result
in a smaller energy or hardware resource budget and, therefore, are especially
meaningful to the deployment to power-eï¬ƒcient front-end systems. However,
how to trim oï¬€the parts of a network that make little contribution to the model
accuracy is no trivial question.
3 
 
DNN pruning can be considered as a searching problem. The searching space
consists of all legitimate pruned networks, which are referred as sub-nets or pruning candidates. In such space, how to obtain the sub-net with highest accuracy
with reasonably small searching eï¬€orts is the core of a pruning task.
Full-size Network
Pruned Network
Adaptive BN
Sensitivity Analysis
Meta Network
Short-term Fine-tuning
Evaluation Process
Fine-tuning
(Optional)
Fig. 1. A generalized pipeline for pruning tasks. The evaluation process unveils the
potential of diï¬€erent pruning strategies and picks the one that most likely to deliver
high accuracy after convergence.
Particularly, an evaluation process can be commonly found in existing pruning pipelines. Such process aims to unveil the potential of sub-nets so that best
pruning candidate can be selected to deliver the ï¬nal pruning strategy. A visual
illustration of this generalization is shown in Figure 1. More details about the
existing evaluation methods will be discussed throughout this work. An advantage of using an evaluation module is fast decision-making because training all
sub-nets, in a large searching space, to convergence for comparison can be very
time-consuming and hence impractical.
However, we found that the evaluation methods in existing works are suboptimal. Concretely, they are either inaccurate or complicated.
By saying inaccurate, it means the winner sub-nets from the evaluation process do not necessarily deliver high accuracy when they converge . This
will be quantitatively proved in Section 4.1 as a correlation problem measured
by several commonly used correlation coeï¬ƒcients. To our knowledge, we are the
ï¬rst to introduce correlation-based analysis for sub-net selection in pruning task.
Moreover, we demonstrate that the reason such evaluation is inaccurate is the
use of sub-optimal statistical values for Batch Normalization (BN) layers .
In this work, we use a so-called adaptive BN technique to ï¬x the issue and
eï¬€ectively reach a higher correlation for our proposed evaluation process.
By saying complicated, it points to the fact that the evaluation process in
some works rely on tricky or computationally intensive components such as a
reinforcement learning agent , auxiliary network training
 , knowledge
distillation , and so on. These methods require careful hyper-parameter tuning
or extra training eï¬€orts on the auxiliary models. These requirements make it
potentially diï¬ƒcult to repeat the results and these pruning methods can be
time-consuming due to their high algorithmic complexity.
Above-mentioned issues in current works motivate us to propose a better
pruning algorithm that equips with a faster and more accurate evaluation process, which eventually helps to provide the state-of-the-art pruning performance.
The main novelty of the proposed EagleEye pruning algorithm is described as
â€“ We point out the reason that a so-called vanilla evaluation step (explained
in Section 3.1) widely found in many existing pruning methods leads to poor
pruning results. To quantitatively demonstrate the issue, we are the ï¬rst to
introduce a correlation analysis to the domain of pruning algorithm.
â€“ We adopt the technique of adaptive batch normalization for pruning purposes in this work to address the issue in the vanilla evaluation step. It is
one of the modules in our proposed pruning algorithm called EagleEye. Our
proposed algorithm can eï¬€ectively estimate the converged accuracy for any
pruned model in the time of only a few iterations of inference. It is also general enough to plug-in and improve some existing methods for performance
improvement.
â€“ Our experiments show that although EagleEye is simple, it achieves the
state-of-the-art pruning performance in comparisons with many more complex approaches. In the ResNet-50 experiments, EagleEye delivers 1.3% to
3.8% higher accuracy than compared algorithms. Even in the challenging
task of pruning the compact model of MobileNet V1, EagleEye achieves the
highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned.
The results here are ImageNet top-1 classiï¬cation accuracy.
Related work
Pruning was mainly handled by hand-crafted heuristics in early time . So
a pruned candidate network is obtained by human expertise and evaluated by
training it to the converged accuracy, which can be very time consuming considering the large number of plausible sub-nets. In later chapters, we will show
that the pruning candidate selection is problematic and selected pruned networks cannot necessarily deliver the highest accuracy after ï¬ne-tuning. Greedy
strategy were introduced to save manual eï¬€orts in more recent time. But
it is easy for such strategy to fall into the local optimal caused by the greedy
nature. For example, NetAdapt supposes the layer lt with the least accuracy
drop, noted as dt, is greedily pruned at step t. However, there may exist a better
pruning strategy where dâ€²
t > dt, but dâ€²
t+1 < dt + dt+1. Our method searches
the pruning ratios for all layers together in one single step and therefore avoids
this issue.
Some other works induce sparsity to weights in training phase for pruning
purposes. For example, introduces group-LASSO to introduce sparsity of
the kernels and
 regularizes the parameter in batch normalization layer.
 ranks the importance of ï¬lters based on Taylor expansion and trimmed oï¬€
the low-ranked ones. The selection standards proposed in these methods are
orthogonal to our proposed algorithm. More recently, versatile techniques were
proposed to achieve automated and eï¬ƒcient pruning strategies such as reinforcement learning , generative adversarial learning mechanism and so on. But
the introduced hyper-parameters add diï¬ƒculty to repeat the experiments and
the trail-and-error to get the auxiliary models work well can be time consuming.
The technique of adjusting BN was used to serve for non-pruning purposes
in existing works.
 adapts the BN statistics for target domain in domain
adaptation tasks. The common point with our work is that we both notice the
batch normalization requires an adjustment to adapt models in a new setting
where either model or domain changes. But this useful technique has not been
particularly used for model pruning purposes.
Methodology
Evaluation
Fine-tuning
Candidates
Pruned Model
Fig. 2. A typical pipeline for neural network training and pruning
A typical neural network training and pruning pipeline is generalized and
visualized in Figure 2. Pruning is normally applied to a trained full-size network
for redundancy removal purposes. An ï¬ne-tuning process is then followed up
to gain accuracy back from losing parameters in the trimmed ï¬lters. In this
work, we focus on structured ï¬lter pruning approaches, which can be generally
formulated as
(r1, r2, ..., rL)âˆ—= arg min
r1,r2,...,rL
L(A(r1, r2, ..., rL; w)),
s.t. C < constraints,
where L is the loss function and A is the neural network model. rl is the pruning
ratio applied to the lth layer. Given some constraints C such as targeted amount
of parameters, operations, or execution latency, a combination of pruning ratios
(r1, r2, ..., rL), which is referred as pruning strategy, is applied to the full-size
model. All possible combinations of the pruning ratios form a searching space. To
obtain a compact model with the highest accuracy, one should search through the
search space by applying diï¬€erent pruning strategies to the model, ï¬ne-tuning
each of the pruned model to converged and pick the best one. We consider the
pruning task as ï¬nding the optimal pruning strategy, denoted as (r1, r2, ..., rL)âˆ—,
that results in the highest converged accuracy of the pruned model.
Apart from handcraft designing, diï¬€erent searching methods have been applied in previous work to ï¬nd the optimal pruning strategy, such as greedy algorithm , RL , and evlolutionary algorithm . All of the these methods
are guided by the evaluation results of the pruning strategies.
Motivation
In many published approaches in this domain, pruning candidates directly compare with each other in terms of evaluation accuracy. The sub-nets
with higher evaluation accuracy are selected and expected to also deliver high accuracy after ï¬ne-tuning. However, such intention can not be necessarily achieved
as we notice the sub-nets perform poorly if directly used to do inference. The inference results normally fall into a very low-range accuracy, which is illustrated
in Figure 3 left. An early attempt is to randomly generate pruning rates for
MobileNet V1 and apply L1-norm based pruning for 50 times. The dark
red bars form the histogram of accuracy collected from directly doing inference
with the pruned candidates in the same way that do before ï¬ne-tuning.
Because our pruning rates are randomly generated in this early attempt, so the
accuracy is very low and only for observation. The gray bars in Figure 4 shows
the situation after ï¬ne-tuning these 50 pruned networks. We notice a huge difference in accuracy distribution between these two results. Therefore, there are
two questions came up to our mind given above observation. The ï¬rst question
is why removal to ï¬lters, especially considered as unimportant ï¬lters, can cause
such noticeable accuracy degradation although the pruning rates are random?
The natural question to ask next is how strongly the low-range accuracy is positively correlated to the ï¬nal converged accuracy. These two questions triggered
our investigation into this commonly used evaluation process, which is called
vanilla evaluation in this work.
Fig. 3. Left:Histogram for accuracy collected from directly pruning MobileNet V1 and
ï¬ne-tuning 15 epoches. Right:Evolution of the weight distribution of a pruned MobileNetV1 during ï¬ne-tuning on ImageNet . Where X axis presents the magnitude
of the L1-norm of kernel, Y axis presents the quantity, Z axis presents the ï¬ne-tuning
Some initial investigations are done to tentatively address the above two
questions. Figure 3 right shows that it might not be the weights that mess up
the accuracy at the evaluation stage as only a gentle shift in weight distribution is observed during ï¬ne-tuning, but the delivered inference accuracy is very
diï¬€erent. On the other side, Figure 4 left shows that the low-range accuracy
indeed presents poor correlation with the ï¬ne-tuned accuracy, which means that
it can be misleading to use evaluated accuracy to guide the pruning candidates
selection.
Interestingly, we found that it is the batch normalization layer that largely
aï¬€ects the evaluation. Without ï¬ne-tuning, pruning candidates have parameters
that are a subset of those in the full-size model. So the layer-wise feature map
data are also aï¬€ected by the changed model dimensions. However, vanilla evaluation still uses Batch Normalization (BN) inherited from the full-size model.
The outdated statistical values of BN layers eventually drag down the evaluation accuracy to a surprisingly low range and, more importantly, break the
correlation between evaluation accuracy and the ï¬nal converged accuracy of the
pruning candidates in the strategy searching space. A brief training, also called
ï¬ne-tuning, all pruning candidates and then compare them is a more accurate
way to carry out the evaluation . However, it is very time-consuming to do
the training-based evaluation for even single-epoch ï¬ne-tuning due to the large
scale of the searching space. We give quantitative analysis later in this section
to demonstrate this point.
Firstly, to quantitatively demonstrate the idea of vanilla evaluation and the
problems that come with it, we symbolize the original BN as below:
y = Î³ x âˆ’Âµ
Ïƒ2 + Ïµ + Î²,
Where Î² and Î³ are trainable scale and bias terms. Ïµ is a term with small value
to avoid zero division. For a mini-batch with size N, the statistical values of Âµ
and Ïƒ2 are calculated as below:
ÂµB = E[xB] = 1
B = V ar[xB] =
(xi âˆ’ÂµB)2.
During training, Âµ and Ïƒ2 are calculated with the moving mean and variance:
Âµt = mÂµtâˆ’1 + (1 âˆ’m)ÂµB,
tâˆ’1 + (1 âˆ’m)Ïƒ2
where m is the momentum coeï¬ƒcient and subscript t refers to the number of
training iterations. In a typical training pipeline, if the total number of training
iteration is T, ÂµT and Ïƒ2
T are used in testing phase. These two items are called
global BN statistics, where â€globalâ€ refers to the full-size model.
Adaptive Batch Normalization
As brieï¬‚y mentioned before, vanilla evaluation used in apply global BN
statistics to pruned networks to fast evaluate their accuracy potential, which we
think leads to the low-range accuracy results and unfair candidate selection. If
the global BN statistics are out-dated to the sub-nets, we should re-calculate ÂµT
T with adaptive values by conducting a few iterations of inference on part of
the training set, which essentially adapts the BN statistical values to the pruned
network connections. Concretely, we freeze all the network parameters while
resetting the moving average statistics. Then, we update the moving statistics by
a few iterations of forward-propagation, using Equation 4, but without backward
propagation. We note the adaptive BN statistics as Ë†
Fig. 4. Correlation between ï¬ne-tuning accuracy and inference accuracy gained from
vanilla evaluation (left), adaptive-BN-based evaluation (right) based on MobileNet V1
experiments on ImageNet Top-1 classiï¬cation results.
Figure 4 right illustrates that applying adaptive BN delivers evaluation accuracy that has a stronger correlation, compared to the vanilla evaluation Figure 4
As another evidence, we compare the distance of BN statistical values between true statistics. We consider Âµ and Ïƒ2 sampled from the validation data as
the true statistics, noted as Âµval and Ïƒ2
val , because they are the real statistical
values in the testing phase. Specially, we are not obtaining insights from the validation data, which we think is unfair, but simply showing that our evaluation
results are closer to the ground truth compared to the vanilla method. Concretely, we expect Ë†ÂµT and Ë†Ïƒ2
T to be as close as possible to the true BN statistics
values,Âµval and Ïƒ2
val, so they could deliver close computational results. So we
visualize the distance of BN statistical values gained from diï¬€erent evaluation
methods (see Figure 5). Each pixel in the heatmaps represents a distance for a
type of BN statistics, either Âµval or Ïƒ2
val, between post-evaluation results and the
true statistics sampled via one ï¬lter in MobileNet V1 . The visual observation
shows that adaptive BN provides closer statistical values to the true values while
global BN is way further. A possible explanation is that the global BN statistics
are out-dated and not adapted to the pruned network connections. So they mess
up the inference accuracy during evaluation for the pruned networks.
Noticeably, ï¬ne-tuning also relieves such problem of mismatched BN statistics because the training process itself re-calculates the BN statistical values in
the forward pass and hence ï¬xes the mismatch. However, BN statistics are not
trainable values but sampling parameters only calculated in inference time. Our
Fig. 5. Visualization of distances of BN statistics in terms of the moving mean and
variance. Each pixel refers to the distance of one BN statistics of a channel in MobileNetV1. (a) âˆ¥ÂµT âˆ’Âµvalâˆ¥2, distance of moving mean between global BN and the
true values. (b) distance of moving mean between adaptive-BN and the true values
ÂµT âˆ’Âµvalâˆ¥2. (c)
2, distance of moving variance between global BN and
the true values. (d) distance of moving variance between adaptive-BN and the true
adaptive BN targets on this issue by conducting re-sampling in exactly the inference step, which achieves the same goal but with way less computational cost
compared to ï¬ne-tuning. This is the main reason that we claim the application
of adaptive BN in pruning evaluation is more eï¬ƒcient than the ï¬ne-tuning-based
Correlation Measurement
As mentioned before, a good evaluation process in the pruning pipeline should
present a strong positive correlation between the evaluated pruning candidates
and their corresponding converged accuracy. Here, we compare two diï¬€erent
evaluation methods, adaptive-BN-based and vanilla evaluation, and study their
correlation with the ï¬ne-tuned accuracy. So we symbolize a vector of accuracy
for all pruning candidates in the searching space (Figure 6) separately using the
above two evaluation methods as X1 and X2 correspondingly while ï¬ne-tuned
accuracy is noted as Y . We ï¬rstly use Pearson Correlation Coeï¬ƒcient (PCC)
ÏX,Y , which is used to measure the linear correlation between two variables X
and Y , to measure the correlation between ÏX1,Y and ÏX2,Y .
Since we particularly care about high-accuracy sub-nets in the ordered accuracy vectors, Spearman Correlation Coeï¬ƒcient (SCC) Ï†X,Y and Kendall rank
Correlation Coeï¬ƒcient (KRCC) Ï„X,Y are adopted to measure the monotonic
correlation. We compare the correlation between (X1, Y ) and (X2, Y ) in above
three metrics with diï¬€erent pruning rates. All cases present a stronger correlation for the adaptive-BN-based evaluation than the vanilla strategy. See richer
details about quantitative analysis in Section 4.1.
EagleEye pruning algorithm
Based on the discussion about the accurate evaluation process in pruning, we
now present the overall workow of EagleEye in Figure 6. Our pruning pipeline
[0.1, 0.5, ..., 0.5]
[0.5, 0.2, ..., 0.1]
[0.3, 0.2, ..., 0.7]
Pruning Strategy
Generation
ðœ”ð‘‡, à·œðœ‡ð‘‡, à·œðœŽð‘‡
Adaptive BN
Adaptive BN
Adaptive BN
Finetuning
ðœ”ð‘‡+Î”ð‘‡, à·œðœ‡ð‘‡+Î”ð‘‡, à·œðœŽð‘‡+Î”ð‘‡
Pruned Model
Winner Candidates
Sub-nets From
Searching Space
Strategy Generation
Filter Pruning
Adaptive-BN-based Candidate Evaluation
Fig. 6. Workï¬‚ow of the EagleEye Pruning Algorithm
contains three parts, pruning strategy generation, ï¬lter pruning, and adaptive-
BN-based evaluation.
Strategy generation outputs pruning strategies in the form of layer-wise
pruning rate vectors like (r1, r2, ..., rL) for a L-layer model. The generation process follows pre-deï¬ned constraints such as inference latency, a global reduction
of operations (FLOPs) or parameters and so on. Concretely, it randomly samples
L real numbers from a given range [0, R] to form a pruning strategy, where rl denotes the pruning ratio for the lth layer. R is the largest pruning ratio applied to
a layer. This is essentially a Monte Carlo sampling process with a uniform distribution for all legitimate layer-wise pruning rates, i.e. removed number of ï¬lters
over the number of total ï¬lters. Noticeably, other strategy generation methods
can be used here, such as the evolutionary algorithm, reinforcement learning etc.,
we found that a simple random sampling is good enough for the entire pipeline
to quickly yield pruning candidates with state-of-the-art accuracy. A possible
reason for this can be that the adjustment to the BN statistics leads to a much
more accurate prediction to the sub-netsâ€™ potential, so the eï¬€orts of generating
candidates are allowed to be massively simpliï¬ed. The low computation cost
of this simple component also adds the advantage of fast speed to the entire
algorithm.
Filter pruning process prunes the full-size trained model according to the
generated pruning strategy from the previous module. Similar to a normal ï¬lter
pruning method, the ï¬lters are ï¬rstly ranked according to their L1-norm and
the rl of the least important ï¬lters are trimmed oï¬€permanently. The sampled
pruning candidates from the searching space are ready to be delivered to the
next evaluation stage after this process.
The adaptive-BN-based candidate evaluation module provides a BN
statistics adaptation and fast evaluation to the pruned candidates handed over
from the previous module. Given a pruned network, it freezes all learnable parameters and traverses through a small amount of data in the training set to
calculate the adaptive BN statistics Ë†Âµ and Ë†Ïƒ2. In practice, we sampled 1/30 of
the total training set for 100 iterations in our ImageNet experiments, which takes
only 10-ish seconds in a single Nvidia 2080 Ti GPU. Next, this module evaluates
the performance of the candidate networks on a small part of training set data,
called sub-validation set, and picks the top ones in the accuracy ranking as winner candidates. The correlation analysis presented in Section 4.1 guarantees the
eï¬€ectiveness of this process. After a ï¬ne-tuning process, the winner candidates
are ï¬nally delivered as outputs.
Experiments
Quantitative analysis of correlation
We use three commonly used correlation coeï¬ƒcient(Ï,Ïƒ and Ï„) to quantitatively
measure the relation between X1, X2 and Y , which are deï¬ned in Section 3.3.
Fig. 7. Vanilla vs. adaptive-BN evaluation: Correlation between evaluation and ï¬netuning accuracy with diï¬€erent pruning ratios (MobileNet V1 on ImageNet classiï¬cation Top-1 results)
Firstly, as mentioned in Section 3.1 the poor correlation, presented by Figure 4 sub-ï¬gure, is basically 10 times smaller than adaptive-BN-based results
shown in Figure 4 right sub-ï¬gure. This matches with the visual observation
that the adaptive-BN-based samples are more trendy while the vanilla strategy
tends to give randomly distributed samples on the ï¬gure. This means the vanilla
evaluation hardly present accurate prediction to the pruned networks about their
ï¬ne-tuned accuracy.
Based on the above initial exploration, we extend the quantitative study to
a larger scale applying three correlation coeï¬ƒcients to diï¬€erent pruning ratios
as shown in Table 1. Firstly, the adaptive-BN-based evaluation delivers stronger
correlation measured in all three coeï¬ƒcients compared to the vanilla evaluation.
In average, Ï is 0.67 higher, Ï† is 0.79 higher and Ï„ is 0.46 higher. Noticeably, the
correlation high in Ï† and Ï„ means that the winner pruning candidates selected
Table 1. Correlation analysis quantiï¬ed by Pearson Correlation Coeï¬ƒcient ÏX,Y ,
Spearman Correlation Coeï¬ƒcient Ï†X,Y , and Kendall rank Correlation Coeï¬ƒcient Ï„X,Y .
FLOPs constraints ÏX1,Y ÏX2,Y
Ï†X1,Y Ï†X2,Y Ï„X1,Y Ï„X2,Y
0.793 0.079
0.850 0.025 0.679 0.063
0.819 -0.038 0.829 -0.030 0.656 -0.003
62.5% FLOPs
0.683 0.250
0.644 0.395 0.458 0.267
0.813 0.105
0.803 0.127 0.639 0.122
from the adaptive-based evaluation module are more likely to rank high in the
ï¬ne-tuned accuracy ranking as Ï† emphasizes the monotonic correlation.
Especially, the third to ï¬fth rows of Table 1 shows the correlation metrics
with diï¬€erent pruning rates (for instance, 75% FLOPs also means 25% pruning
rate to operations). The corresponding results are also visualized in Figure 7. The
second row in Table 1 means the pruning rate follows a layer-wise Monte Carlo
sampling with a uniform distribution among the legitimate pruning rate options.
All the above tables and ï¬gures prove that the adaptive-BN-based evaluation
shows stronger correlation, and hence a more robust prediction, between the
evaluated and ï¬ne-tuned accuracy for the pruning candidates.
Generality of the adaptive-BN-based evaluation method
The proposed adaptive-BN-based evaluation method is general enough to plugin and improves some existing methods. As an example, we apply it to AMC ,
which is an automatic method based on Reinforcement Learning mechanism.
AMC trains an RL-agent to decide the pruning ratio for each layer. At
each training step, the agent tries applying diï¬€erent pruning ratios (pruning
strategy) to the full-size model as an action. Then it directly evaluates the accuracy without ï¬ne-tuning, which is noted as vanilla evaluation in our paper, and
takes this validation accuracy as the reward. As the RL-agent is trained with the
reward based on the vanilla evaluation, which is proved to have a poor correlation
to the converged accuracy of pruned networks. So we replace the vanilla evaluation process with our proposed adaptive-BN-based evaluation. Concretely, after
pruning out ï¬lters at each step, we freeze all learnable parameters and do inference on the training set to ï¬x the BN statistics and evaluate the accuracy of the
model on the sub-validation set. We feed this accuracy as a reward to train the
RL-agent in place of the accuracy of vanilla evaluation. The experiment about
MobileNetV1 on ImageNet classiï¬cation accuracy is improved from 70.5%
(reported in AMC ) to 70.7%. It shows that the RL-agent can ï¬nd a better
pruning strategy with the help of our adaptive-BN-based evaluation module.
Another example is the short-term ï¬ne-tune block in , which also can be
handily replaced by our adaptiveBN-based module for a faster pruning strategy
selection. On the other side, our pipeline can also be upgraded by existing methods such as the evolutionary algorithm used in to improve the basic Monte
Carlo sampling strategy. The above experiments and discussion demonstrate the
generality of our adaptive-BN-based evaluation module, but can not be analyzed
in more detail due to the limited length of this paper.
Eï¬ƒciency of our proposed method
Table 2. Comparison of computation costs of various pruning methods in the task
where all pruning methods are executed to ï¬nd the best pruning strategy from 1000
potential strategies (candidates).
Evaluation Method
Candidate Selection
ThiNet 
1000Ã—10 ï¬netune epochs
NetAdapt 
104 training iterations
Filter Pruning 
1000Ã—25 ï¬netune epochs
Training an RL agent
Meta-Pruning 
PruningNet
Training an auxiliary network
adaptive-BN
<1000Ã—100 inference iterations
Our proposed pruning evaluation based on adaptive BN turn the prediction
of sub-net accuracy into a very fast and reliable process, so EagleEye is much
less time-consuming to complete the entire pruning pipeline than other heavy
evaluation based algorithms. In this part, we compare the execution cost for
various state-of-the-art algorithms to demonstrate the eï¬ƒciency of our method.
Table 2 compares the computational costs of picking the best pruning strategy among 1000 potential pruning candidates. As ThiNet and Filter Pruning require manually assigning layer-wise pruning ratio, The ï¬nal GPU hours
are the estimation of completing the pruning pipeline for 1000 random strategies.
In practice, the real computation cost highly depends on the expertâ€™s heuristic
practice of trial-and-error. The computation time for AMC and Meta-pruning
can be long because training either an RL network or an auxiliary network itself is time-consuming and tricky. Among all compared methods, EagleEye is
the most eï¬ƒcient method as each evaluation takes no more than 100 iterations,
which takes 10 to 20 seconds in a single Nvidia 2080 Ti GPU. So the total candidate selection is simply an evaluation comparison process, which also can be
done in negligible time.
Eï¬€ectiveness of our proposed method
To demonstrate the eï¬€ectiveness of EagleEye, we compare it with several stateof-the-art pruning methods on MobileNetV1 and ResNet-50 models tested on
the small dataset of CIFAR-10 and the large dataset of ImageNet.
ResNet Table 3 left shows EagleEye outperforms all compared methods in
terms of Top-1 accuracy on CIFAR-10 dataset. To further prove the robustness
of our method, we compare the top-1 accuracy of ResNet-50 on ImageNet under
Table 3. Pruning results of ResNet-56 (left) and MobileNetV1 (right) on CIFAR-10
FLOPs Top1-Acc
ResNet-56 125.49M
HRank 88.72M
EagleEye 62.23M
FLOPs Top1-Acc
0.75 Ã— MobileNetV1
FP(our-implement) 
0.5 Ã— MobileNetV1
FP(our-implement) 
0.25 Ã— MobileNetV1
FP(our-implement) 
diï¬€erent FLOPs constraints. For each FLOPs constraint (3G, 2G, and 1G),
1000 pruning strategies are generated. Then the adaptive-BN-based evaluation
method is applied to each candidate. We just ï¬ne-tune the top-2 candidates and
return the best as delivered pruned model. It is shown that EagleEye achieves
the best results among the compared approaches listed in Table 4.
ThiNet prunes the channels uniformly for each layer other than ï¬nding
an optimal pruning strategy, which hurts the performance signiï¬cantly. Meta-
Pruning trains an auxiliary network called â€œPruningNetâ€ to predict the
weights of the pruned model. But the adopted vanilla evaluation may mislead the
searching of the pruning strategies. As shown in Table 4, our proposed algorithm
outperform all compared methods given diï¬€erent pruned network targets.
MobileNet We conduct experiments of the compact model of MobileNetV1
and compare the pruning results with Filter Pruning and the directly-scaled
models. Please refer to supplementary material for more details about FP implementation and training methods to get the accuracy for the directly-scaled
models. Table 3 right shows that EagleEye gets the best results in all cases.
Pruning MobileNetV1 for ImageNet is more challenging as it is already a very
compact model. We compare the top-1 ImageNet classiï¬cation accuracy under
the same FLOPs constraint (about 280M FLOPs) and the results are shown in
Table 5. 1500 pruning strategies are generated with this FLOPs constraint. Then
adaptive-BN-based evaluation is applied to each candidate. After ï¬ne-tuning the
top-2 candidates, the pruning candidate that returns the highest accuracy is
selected as the ï¬nal output.
AMC trains their pruning strategy decision agent based on the pruned
model without ï¬ne-tuning, which may lead to a problematic selection on the
candidates. NetAdapt searches for the pruning strategy based on a greedy
algorithm, which may drop into a local optimum as analysed in Section 2. It is
shown that EagleEye achieves the best performance among all studied methods
again in this task (see Table 5).
Table 4. Comparisions of ResNet-50 and other pruning methods on ImageNet
FLOPs after pruning
FLOPs Top1-Acc Top5-Acc
ThiNet-70 
AutoSlim 
Meta-Pruning 
0.75 Ã— ResNet-50 
Thinet-50 
AutoSlim 
Meta-Pruning 
0.5 Ã— ResNet-50 
ThiNet-30 
AutoSlim 
Meta-Pruning 
Table 5. Comparisions of MobileNetV1 and other pruning methods on ImageNet
FLOPs Top1-Acc Top5-Acc
0.75 Ã— MobileNetV1 325M
NetAdapt 
Meta-Pruning 
Discussion and Conclusions
We presented EagleEye pruning algorithm, in which a fast and accurate evaluation process based on adaptive batch normalization is proposed. Our experiments show the eï¬ƒciency and eï¬€ectiveness of our proposed method by delivering
higher accuracy than the studied methods in the pruning experiments on ImageNet dataset. An interesting work is to further explore the generality of the
adaptive-BN-based module by integrating it into many other existing methods
and observe the potential improvement. Another experiment that is worth a try
is to replace the random generation of pruning strategy with more advanced
methods such as evolutionary algorithms and so on.
Acknowledgements
Jiang Su is the corresponding author of this work. This work was supported in
part by the National Natural Science Foundation of China (NSFC) under Grant
No.U1811463.