EagleEye: Fast Sub-net Evaluation for Eﬃcient
Neural Network Pruning
Bailin Li1, Bowen Wu2, Jiang Su1, Guangrun Wang2, and Liang Lin1,2
1 Dark Matter AI Inc.
2 Sun Yat-sen University
 , {wubw6,wanggrun}@mail2.sysu.edu.cn, ,
 
Abstract. Finding out the computational redundant part of a trained
Deep Neural Network (DNN) is the key question that pruning algorithms
target on. Many algorithms try to predict model performance of the
pruned sub-nets by introducing various evaluation methods. But they
are either inaccurate or very complicated for general application. In this
work, we present a pruning method called EagleEye, in which a simple
yet eﬃcient evaluation component based on adaptive batch normalization is applied to unveil a strong correlation between diﬀerent pruned
DNN structures and their ﬁnal settled accuracy. This strong correlation
allows us to fast spot the pruned candidates with highest potential accuracy without actually ﬁne-tuning them. This module is also general to
plug-in and improve some existing pruning algorithms. EagleEye achieves
better pruning performance than all of the studied pruning algorithms
in our experiments. Concretely, to prune MobileNet V1 and ResNet-50,
EagleEye outperforms all compared methods by up to 3.8%. Even in
the more challenging experiments of pruning the compact model of MobileNet V1, EagleEye achieves the highest accuracy of 70.9% with an
overall 50% operations (FLOPs) pruned. All accuracy results are Top-1
ImageNet classiﬁcation accuracy. Source code and models are accessible
to open-source community.3
Keywords: Model Compression; Neural Network Pruning;
Introduction
Deep Neural Network (DNN) pruning aims to reduce computational redundancy
from a full model with an allowed accuracy range. Pruned models usually result
in a smaller energy or hardware resource budget and, therefore, are especially
meaningful to the deployment to power-eﬃcient front-end systems. However,
how to trim oﬀthe parts of a network that make little contribution to the model
accuracy is no trivial question.
3 
 
DNN pruning can be considered as a searching problem. The searching space
consists of all legitimate pruned networks, which are referred as sub-nets or pruning candidates. In such space, how to obtain the sub-net with highest accuracy
with reasonably small searching eﬀorts is the core of a pruning task.
Full-size Network
Pruned Network
Adaptive BN
Sensitivity Analysis
Meta Network
Short-term Fine-tuning
Evaluation Process
Fine-tuning
(Optional)
Fig. 1. A generalized pipeline for pruning tasks. The evaluation process unveils the
potential of diﬀerent pruning strategies and picks the one that most likely to deliver
high accuracy after convergence.
Particularly, an evaluation process can be commonly found in existing pruning pipelines. Such process aims to unveil the potential of sub-nets so that best
pruning candidate can be selected to deliver the ﬁnal pruning strategy. A visual
illustration of this generalization is shown in Figure 1. More details about the
existing evaluation methods will be discussed throughout this work. An advantage of using an evaluation module is fast decision-making because training all
sub-nets, in a large searching space, to convergence for comparison can be very
time-consuming and hence impractical.
However, we found that the evaluation methods in existing works are suboptimal. Concretely, they are either inaccurate or complicated.
By saying inaccurate, it means the winner sub-nets from the evaluation process do not necessarily deliver high accuracy when they converge . This
will be quantitatively proved in Section 4.1 as a correlation problem measured
by several commonly used correlation coeﬃcients. To our knowledge, we are the
ﬁrst to introduce correlation-based analysis for sub-net selection in pruning task.
Moreover, we demonstrate that the reason such evaluation is inaccurate is the
use of sub-optimal statistical values for Batch Normalization (BN) layers .
In this work, we use a so-called adaptive BN technique to ﬁx the issue and
eﬀectively reach a higher correlation for our proposed evaluation process.
By saying complicated, it points to the fact that the evaluation process in
some works rely on tricky or computationally intensive components such as a
reinforcement learning agent , auxiliary network training
 , knowledge
distillation , and so on. These methods require careful hyper-parameter tuning
or extra training eﬀorts on the auxiliary models. These requirements make it
potentially diﬃcult to repeat the results and these pruning methods can be
time-consuming due to their high algorithmic complexity.
Above-mentioned issues in current works motivate us to propose a better
pruning algorithm that equips with a faster and more accurate evaluation process, which eventually helps to provide the state-of-the-art pruning performance.
The main novelty of the proposed EagleEye pruning algorithm is described as
– We point out the reason that a so-called vanilla evaluation step (explained
in Section 3.1) widely found in many existing pruning methods leads to poor
pruning results. To quantitatively demonstrate the issue, we are the ﬁrst to
introduce a correlation analysis to the domain of pruning algorithm.
– We adopt the technique of adaptive batch normalization for pruning purposes in this work to address the issue in the vanilla evaluation step. It is
one of the modules in our proposed pruning algorithm called EagleEye. Our
proposed algorithm can eﬀectively estimate the converged accuracy for any
pruned model in the time of only a few iterations of inference. It is also general enough to plug-in and improve some existing methods for performance
improvement.
– Our experiments show that although EagleEye is simple, it achieves the
state-of-the-art pruning performance in comparisons with many more complex approaches. In the ResNet-50 experiments, EagleEye delivers 1.3% to
3.8% higher accuracy than compared algorithms. Even in the challenging
task of pruning the compact model of MobileNet V1, EagleEye achieves the
highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned.
The results here are ImageNet top-1 classiﬁcation accuracy.
Related work
Pruning was mainly handled by hand-crafted heuristics in early time . So
a pruned candidate network is obtained by human expertise and evaluated by
training it to the converged accuracy, which can be very time consuming considering the large number of plausible sub-nets. In later chapters, we will show
that the pruning candidate selection is problematic and selected pruned networks cannot necessarily deliver the highest accuracy after ﬁne-tuning. Greedy
strategy were introduced to save manual eﬀorts in more recent time. But
it is easy for such strategy to fall into the local optimal caused by the greedy
nature. For example, NetAdapt supposes the layer lt with the least accuracy
drop, noted as dt, is greedily pruned at step t. However, there may exist a better
pruning strategy where d′
t > dt, but d′
t+1 < dt + dt+1. Our method searches
the pruning ratios for all layers together in one single step and therefore avoids
this issue.
Some other works induce sparsity to weights in training phase for pruning
purposes. For example, introduces group-LASSO to introduce sparsity of
the kernels and
 regularizes the parameter in batch normalization layer.
 ranks the importance of ﬁlters based on Taylor expansion and trimmed oﬀ
the low-ranked ones. The selection standards proposed in these methods are
orthogonal to our proposed algorithm. More recently, versatile techniques were
proposed to achieve automated and eﬃcient pruning strategies such as reinforcement learning , generative adversarial learning mechanism and so on. But
the introduced hyper-parameters add diﬃculty to repeat the experiments and
the trail-and-error to get the auxiliary models work well can be time consuming.
The technique of adjusting BN was used to serve for non-pruning purposes
in existing works.
 adapts the BN statistics for target domain in domain
adaptation tasks. The common point with our work is that we both notice the
batch normalization requires an adjustment to adapt models in a new setting
where either model or domain changes. But this useful technique has not been
particularly used for model pruning purposes.
Methodology
Evaluation
Fine-tuning
Candidates
Pruned Model
Fig. 2. A typical pipeline for neural network training and pruning
A typical neural network training and pruning pipeline is generalized and
visualized in Figure 2. Pruning is normally applied to a trained full-size network
for redundancy removal purposes. An ﬁne-tuning process is then followed up
to gain accuracy back from losing parameters in the trimmed ﬁlters. In this
work, we focus on structured ﬁlter pruning approaches, which can be generally
formulated as
(r1, r2, ..., rL)∗= arg min
r1,r2,...,rL
L(A(r1, r2, ..., rL; w)),
s.t. C < constraints,
where L is the loss function and A is the neural network model. rl is the pruning
ratio applied to the lth layer. Given some constraints C such as targeted amount
of parameters, operations, or execution latency, a combination of pruning ratios
(r1, r2, ..., rL), which is referred as pruning strategy, is applied to the full-size
model. All possible combinations of the pruning ratios form a searching space. To
obtain a compact model with the highest accuracy, one should search through the
search space by applying diﬀerent pruning strategies to the model, ﬁne-tuning
each of the pruned model to converged and pick the best one. We consider the
pruning task as ﬁnding the optimal pruning strategy, denoted as (r1, r2, ..., rL)∗,
that results in the highest converged accuracy of the pruned model.
Apart from handcraft designing, diﬀerent searching methods have been applied in previous work to ﬁnd the optimal pruning strategy, such as greedy algorithm , RL , and evlolutionary algorithm . All of the these methods
are guided by the evaluation results of the pruning strategies.
Motivation
In many published approaches in this domain, pruning candidates directly compare with each other in terms of evaluation accuracy. The sub-nets
with higher evaluation accuracy are selected and expected to also deliver high accuracy after ﬁne-tuning. However, such intention can not be necessarily achieved
as we notice the sub-nets perform poorly if directly used to do inference. The inference results normally fall into a very low-range accuracy, which is illustrated
in Figure 3 left. An early attempt is to randomly generate pruning rates for
MobileNet V1 and apply L1-norm based pruning for 50 times. The dark
red bars form the histogram of accuracy collected from directly doing inference
with the pruned candidates in the same way that do before ﬁne-tuning.
Because our pruning rates are randomly generated in this early attempt, so the
accuracy is very low and only for observation. The gray bars in Figure 4 shows
the situation after ﬁne-tuning these 50 pruned networks. We notice a huge difference in accuracy distribution between these two results. Therefore, there are
two questions came up to our mind given above observation. The ﬁrst question
is why removal to ﬁlters, especially considered as unimportant ﬁlters, can cause
such noticeable accuracy degradation although the pruning rates are random?
The natural question to ask next is how strongly the low-range accuracy is positively correlated to the ﬁnal converged accuracy. These two questions triggered
our investigation into this commonly used evaluation process, which is called
vanilla evaluation in this work.
Fig. 3. Left:Histogram for accuracy collected from directly pruning MobileNet V1 and
ﬁne-tuning 15 epoches. Right:Evolution of the weight distribution of a pruned MobileNetV1 during ﬁne-tuning on ImageNet . Where X axis presents the magnitude
of the L1-norm of kernel, Y axis presents the quantity, Z axis presents the ﬁne-tuning
Some initial investigations are done to tentatively address the above two
questions. Figure 3 right shows that it might not be the weights that mess up
the accuracy at the evaluation stage as only a gentle shift in weight distribution is observed during ﬁne-tuning, but the delivered inference accuracy is very
diﬀerent. On the other side, Figure 4 left shows that the low-range accuracy
indeed presents poor correlation with the ﬁne-tuned accuracy, which means that
it can be misleading to use evaluated accuracy to guide the pruning candidates
selection.
Interestingly, we found that it is the batch normalization layer that largely
aﬀects the evaluation. Without ﬁne-tuning, pruning candidates have parameters
that are a subset of those in the full-size model. So the layer-wise feature map
data are also aﬀected by the changed model dimensions. However, vanilla evaluation still uses Batch Normalization (BN) inherited from the full-size model.
The outdated statistical values of BN layers eventually drag down the evaluation accuracy to a surprisingly low range and, more importantly, break the
correlation between evaluation accuracy and the ﬁnal converged accuracy of the
pruning candidates in the strategy searching space. A brief training, also called
ﬁne-tuning, all pruning candidates and then compare them is a more accurate
way to carry out the evaluation . However, it is very time-consuming to do
the training-based evaluation for even single-epoch ﬁne-tuning due to the large
scale of the searching space. We give quantitative analysis later in this section
to demonstrate this point.
Firstly, to quantitatively demonstrate the idea of vanilla evaluation and the
problems that come with it, we symbolize the original BN as below:
y = γ x −µ
σ2 + ϵ + β,
Where β and γ are trainable scale and bias terms. ϵ is a term with small value
to avoid zero division. For a mini-batch with size N, the statistical values of µ
and σ2 are calculated as below:
µB = E[xB] = 1
B = V ar[xB] =
(xi −µB)2.
During training, µ and σ2 are calculated with the moving mean and variance:
µt = mµt−1 + (1 −m)µB,
t−1 + (1 −m)σ2
where m is the momentum coeﬃcient and subscript t refers to the number of
training iterations. In a typical training pipeline, if the total number of training
iteration is T, µT and σ2
T are used in testing phase. These two items are called
global BN statistics, where ”global” refers to the full-size model.
Adaptive Batch Normalization
As brieﬂy mentioned before, vanilla evaluation used in apply global BN
statistics to pruned networks to fast evaluate their accuracy potential, which we
think leads to the low-range accuracy results and unfair candidate selection. If
the global BN statistics are out-dated to the sub-nets, we should re-calculate µT
T with adaptive values by conducting a few iterations of inference on part of
the training set, which essentially adapts the BN statistical values to the pruned
network connections. Concretely, we freeze all the network parameters while
resetting the moving average statistics. Then, we update the moving statistics by
a few iterations of forward-propagation, using Equation 4, but without backward
propagation. We note the adaptive BN statistics as ˆ
Fig. 4. Correlation between ﬁne-tuning accuracy and inference accuracy gained from
vanilla evaluation (left), adaptive-BN-based evaluation (right) based on MobileNet V1
experiments on ImageNet Top-1 classiﬁcation results.
Figure 4 right illustrates that applying adaptive BN delivers evaluation accuracy that has a stronger correlation, compared to the vanilla evaluation Figure 4
As another evidence, we compare the distance of BN statistical values between true statistics. We consider µ and σ2 sampled from the validation data as
the true statistics, noted as µval and σ2
val , because they are the real statistical
values in the testing phase. Specially, we are not obtaining insights from the validation data, which we think is unfair, but simply showing that our evaluation
results are closer to the ground truth compared to the vanilla method. Concretely, we expect ˆµT and ˆσ2
T to be as close as possible to the true BN statistics
values,µval and σ2
val, so they could deliver close computational results. So we
visualize the distance of BN statistical values gained from diﬀerent evaluation
methods (see Figure 5). Each pixel in the heatmaps represents a distance for a
type of BN statistics, either µval or σ2
val, between post-evaluation results and the
true statistics sampled via one ﬁlter in MobileNet V1 . The visual observation
shows that adaptive BN provides closer statistical values to the true values while
global BN is way further. A possible explanation is that the global BN statistics
are out-dated and not adapted to the pruned network connections. So they mess
up the inference accuracy during evaluation for the pruned networks.
Noticeably, ﬁne-tuning also relieves such problem of mismatched BN statistics because the training process itself re-calculates the BN statistical values in
the forward pass and hence ﬁxes the mismatch. However, BN statistics are not
trainable values but sampling parameters only calculated in inference time. Our
Fig. 5. Visualization of distances of BN statistics in terms of the moving mean and
variance. Each pixel refers to the distance of one BN statistics of a channel in MobileNetV1. (a) ∥µT −µval∥2, distance of moving mean between global BN and the
true values. (b) distance of moving mean between adaptive-BN and the true values
µT −µval∥2. (c)
2, distance of moving variance between global BN and
the true values. (d) distance of moving variance between adaptive-BN and the true
adaptive BN targets on this issue by conducting re-sampling in exactly the inference step, which achieves the same goal but with way less computational cost
compared to ﬁne-tuning. This is the main reason that we claim the application
of adaptive BN in pruning evaluation is more eﬃcient than the ﬁne-tuning-based
Correlation Measurement
As mentioned before, a good evaluation process in the pruning pipeline should
present a strong positive correlation between the evaluated pruning candidates
and their corresponding converged accuracy. Here, we compare two diﬀerent
evaluation methods, adaptive-BN-based and vanilla evaluation, and study their
correlation with the ﬁne-tuned accuracy. So we symbolize a vector of accuracy
for all pruning candidates in the searching space (Figure 6) separately using the
above two evaluation methods as X1 and X2 correspondingly while ﬁne-tuned
accuracy is noted as Y . We ﬁrstly use Pearson Correlation Coeﬃcient (PCC)
ρX,Y , which is used to measure the linear correlation between two variables X
and Y , to measure the correlation between ρX1,Y and ρX2,Y .
Since we particularly care about high-accuracy sub-nets in the ordered accuracy vectors, Spearman Correlation Coeﬃcient (SCC) φX,Y and Kendall rank
Correlation Coeﬃcient (KRCC) τX,Y are adopted to measure the monotonic
correlation. We compare the correlation between (X1, Y ) and (X2, Y ) in above
three metrics with diﬀerent pruning rates. All cases present a stronger correlation for the adaptive-BN-based evaluation than the vanilla strategy. See richer
details about quantitative analysis in Section 4.1.
EagleEye pruning algorithm
Based on the discussion about the accurate evaluation process in pruning, we
now present the overall workow of EagleEye in Figure 6. Our pruning pipeline
[0.1, 0.5, ..., 0.5]
[0.5, 0.2, ..., 0.1]
[0.3, 0.2, ..., 0.7]
Pruning Strategy
Generation
𝜔𝑇, ො𝜇𝑇, ො𝜎𝑇
Adaptive BN
Adaptive BN
Adaptive BN
Finetuning
𝜔𝑇+Δ𝑇, ො𝜇𝑇+Δ𝑇, ො𝜎𝑇+Δ𝑇
Pruned Model
Winner Candidates
Sub-nets From
Searching Space
Strategy Generation
Filter Pruning
Adaptive-BN-based Candidate Evaluation
Fig. 6. Workﬂow of the EagleEye Pruning Algorithm
contains three parts, pruning strategy generation, ﬁlter pruning, and adaptive-
BN-based evaluation.
Strategy generation outputs pruning strategies in the form of layer-wise
pruning rate vectors like (r1, r2, ..., rL) for a L-layer model. The generation process follows pre-deﬁned constraints such as inference latency, a global reduction
of operations (FLOPs) or parameters and so on. Concretely, it randomly samples
L real numbers from a given range [0, R] to form a pruning strategy, where rl denotes the pruning ratio for the lth layer. R is the largest pruning ratio applied to
a layer. This is essentially a Monte Carlo sampling process with a uniform distribution for all legitimate layer-wise pruning rates, i.e. removed number of ﬁlters
over the number of total ﬁlters. Noticeably, other strategy generation methods
can be used here, such as the evolutionary algorithm, reinforcement learning etc.,
we found that a simple random sampling is good enough for the entire pipeline
to quickly yield pruning candidates with state-of-the-art accuracy. A possible
reason for this can be that the adjustment to the BN statistics leads to a much
more accurate prediction to the sub-nets’ potential, so the eﬀorts of generating
candidates are allowed to be massively simpliﬁed. The low computation cost
of this simple component also adds the advantage of fast speed to the entire
algorithm.
Filter pruning process prunes the full-size trained model according to the
generated pruning strategy from the previous module. Similar to a normal ﬁlter
pruning method, the ﬁlters are ﬁrstly ranked according to their L1-norm and
the rl of the least important ﬁlters are trimmed oﬀpermanently. The sampled
pruning candidates from the searching space are ready to be delivered to the
next evaluation stage after this process.
The adaptive-BN-based candidate evaluation module provides a BN
statistics adaptation and fast evaluation to the pruned candidates handed over
from the previous module. Given a pruned network, it freezes all learnable parameters and traverses through a small amount of data in the training set to
calculate the adaptive BN statistics ˆµ and ˆσ2. In practice, we sampled 1/30 of
the total training set for 100 iterations in our ImageNet experiments, which takes
only 10-ish seconds in a single Nvidia 2080 Ti GPU. Next, this module evaluates
the performance of the candidate networks on a small part of training set data,
called sub-validation set, and picks the top ones in the accuracy ranking as winner candidates. The correlation analysis presented in Section 4.1 guarantees the
eﬀectiveness of this process. After a ﬁne-tuning process, the winner candidates
are ﬁnally delivered as outputs.
Experiments
Quantitative analysis of correlation
We use three commonly used correlation coeﬃcient(ρ,σ and τ) to quantitatively
measure the relation between X1, X2 and Y , which are deﬁned in Section 3.3.
Fig. 7. Vanilla vs. adaptive-BN evaluation: Correlation between evaluation and ﬁnetuning accuracy with diﬀerent pruning ratios (MobileNet V1 on ImageNet classiﬁcation Top-1 results)
Firstly, as mentioned in Section 3.1 the poor correlation, presented by Figure 4 sub-ﬁgure, is basically 10 times smaller than adaptive-BN-based results
shown in Figure 4 right sub-ﬁgure. This matches with the visual observation
that the adaptive-BN-based samples are more trendy while the vanilla strategy
tends to give randomly distributed samples on the ﬁgure. This means the vanilla
evaluation hardly present accurate prediction to the pruned networks about their
ﬁne-tuned accuracy.
Based on the above initial exploration, we extend the quantitative study to
a larger scale applying three correlation coeﬃcients to diﬀerent pruning ratios
as shown in Table 1. Firstly, the adaptive-BN-based evaluation delivers stronger
correlation measured in all three coeﬃcients compared to the vanilla evaluation.
In average, ρ is 0.67 higher, φ is 0.79 higher and τ is 0.46 higher. Noticeably, the
correlation high in φ and τ means that the winner pruning candidates selected
Table 1. Correlation analysis quantiﬁed by Pearson Correlation Coeﬃcient ρX,Y ,
Spearman Correlation Coeﬃcient φX,Y , and Kendall rank Correlation Coeﬃcient τX,Y .
FLOPs constraints ρX1,Y ρX2,Y
φX1,Y φX2,Y τX1,Y τX2,Y
0.793 0.079
0.850 0.025 0.679 0.063
0.819 -0.038 0.829 -0.030 0.656 -0.003
62.5% FLOPs
0.683 0.250
0.644 0.395 0.458 0.267
0.813 0.105
0.803 0.127 0.639 0.122
from the adaptive-based evaluation module are more likely to rank high in the
ﬁne-tuned accuracy ranking as φ emphasizes the monotonic correlation.
Especially, the third to ﬁfth rows of Table 1 shows the correlation metrics
with diﬀerent pruning rates (for instance, 75% FLOPs also means 25% pruning
rate to operations). The corresponding results are also visualized in Figure 7. The
second row in Table 1 means the pruning rate follows a layer-wise Monte Carlo
sampling with a uniform distribution among the legitimate pruning rate options.
All the above tables and ﬁgures prove that the adaptive-BN-based evaluation
shows stronger correlation, and hence a more robust prediction, between the
evaluated and ﬁne-tuned accuracy for the pruning candidates.
Generality of the adaptive-BN-based evaluation method
The proposed adaptive-BN-based evaluation method is general enough to plugin and improves some existing methods. As an example, we apply it to AMC ,
which is an automatic method based on Reinforcement Learning mechanism.
AMC trains an RL-agent to decide the pruning ratio for each layer. At
each training step, the agent tries applying diﬀerent pruning ratios (pruning
strategy) to the full-size model as an action. Then it directly evaluates the accuracy without ﬁne-tuning, which is noted as vanilla evaluation in our paper, and
takes this validation accuracy as the reward. As the RL-agent is trained with the
reward based on the vanilla evaluation, which is proved to have a poor correlation
to the converged accuracy of pruned networks. So we replace the vanilla evaluation process with our proposed adaptive-BN-based evaluation. Concretely, after
pruning out ﬁlters at each step, we freeze all learnable parameters and do inference on the training set to ﬁx the BN statistics and evaluate the accuracy of the
model on the sub-validation set. We feed this accuracy as a reward to train the
RL-agent in place of the accuracy of vanilla evaluation. The experiment about
MobileNetV1 on ImageNet classiﬁcation accuracy is improved from 70.5%
(reported in AMC ) to 70.7%. It shows that the RL-agent can ﬁnd a better
pruning strategy with the help of our adaptive-BN-based evaluation module.
Another example is the short-term ﬁne-tune block in , which also can be
handily replaced by our adaptiveBN-based module for a faster pruning strategy
selection. On the other side, our pipeline can also be upgraded by existing methods such as the evolutionary algorithm used in to improve the basic Monte
Carlo sampling strategy. The above experiments and discussion demonstrate the
generality of our adaptive-BN-based evaluation module, but can not be analyzed
in more detail due to the limited length of this paper.
Eﬃciency of our proposed method
Table 2. Comparison of computation costs of various pruning methods in the task
where all pruning methods are executed to ﬁnd the best pruning strategy from 1000
potential strategies (candidates).
Evaluation Method
Candidate Selection
ThiNet 
1000×10 ﬁnetune epochs
NetAdapt 
104 training iterations
Filter Pruning 
1000×25 ﬁnetune epochs
Training an RL agent
Meta-Pruning 
PruningNet
Training an auxiliary network
adaptive-BN
<1000×100 inference iterations
Our proposed pruning evaluation based on adaptive BN turn the prediction
of sub-net accuracy into a very fast and reliable process, so EagleEye is much
less time-consuming to complete the entire pruning pipeline than other heavy
evaluation based algorithms. In this part, we compare the execution cost for
various state-of-the-art algorithms to demonstrate the eﬃciency of our method.
Table 2 compares the computational costs of picking the best pruning strategy among 1000 potential pruning candidates. As ThiNet and Filter Pruning require manually assigning layer-wise pruning ratio, The ﬁnal GPU hours
are the estimation of completing the pruning pipeline for 1000 random strategies.
In practice, the real computation cost highly depends on the expert’s heuristic
practice of trial-and-error. The computation time for AMC and Meta-pruning
can be long because training either an RL network or an auxiliary network itself is time-consuming and tricky. Among all compared methods, EagleEye is
the most eﬃcient method as each evaluation takes no more than 100 iterations,
which takes 10 to 20 seconds in a single Nvidia 2080 Ti GPU. So the total candidate selection is simply an evaluation comparison process, which also can be
done in negligible time.
Eﬀectiveness of our proposed method
To demonstrate the eﬀectiveness of EagleEye, we compare it with several stateof-the-art pruning methods on MobileNetV1 and ResNet-50 models tested on
the small dataset of CIFAR-10 and the large dataset of ImageNet.
ResNet Table 3 left shows EagleEye outperforms all compared methods in
terms of Top-1 accuracy on CIFAR-10 dataset. To further prove the robustness
of our method, we compare the top-1 accuracy of ResNet-50 on ImageNet under
Table 3. Pruning results of ResNet-56 (left) and MobileNetV1 (right) on CIFAR-10
FLOPs Top1-Acc
ResNet-56 125.49M
HRank 88.72M
EagleEye 62.23M
FLOPs Top1-Acc
0.75 × MobileNetV1
FP(our-implement) 
0.5 × MobileNetV1
FP(our-implement) 
0.25 × MobileNetV1
FP(our-implement) 
diﬀerent FLOPs constraints. For each FLOPs constraint (3G, 2G, and 1G),
1000 pruning strategies are generated. Then the adaptive-BN-based evaluation
method is applied to each candidate. We just ﬁne-tune the top-2 candidates and
return the best as delivered pruned model. It is shown that EagleEye achieves
the best results among the compared approaches listed in Table 4.
ThiNet prunes the channels uniformly for each layer other than ﬁnding
an optimal pruning strategy, which hurts the performance signiﬁcantly. Meta-
Pruning trains an auxiliary network called “PruningNet” to predict the
weights of the pruned model. But the adopted vanilla evaluation may mislead the
searching of the pruning strategies. As shown in Table 4, our proposed algorithm
outperform all compared methods given diﬀerent pruned network targets.
MobileNet We conduct experiments of the compact model of MobileNetV1
and compare the pruning results with Filter Pruning and the directly-scaled
models. Please refer to supplementary material for more details about FP implementation and training methods to get the accuracy for the directly-scaled
models. Table 3 right shows that EagleEye gets the best results in all cases.
Pruning MobileNetV1 for ImageNet is more challenging as it is already a very
compact model. We compare the top-1 ImageNet classiﬁcation accuracy under
the same FLOPs constraint (about 280M FLOPs) and the results are shown in
Table 5. 1500 pruning strategies are generated with this FLOPs constraint. Then
adaptive-BN-based evaluation is applied to each candidate. After ﬁne-tuning the
top-2 candidates, the pruning candidate that returns the highest accuracy is
selected as the ﬁnal output.
AMC trains their pruning strategy decision agent based on the pruned
model without ﬁne-tuning, which may lead to a problematic selection on the
candidates. NetAdapt searches for the pruning strategy based on a greedy
algorithm, which may drop into a local optimum as analysed in Section 2. It is
shown that EagleEye achieves the best performance among all studied methods
again in this task (see Table 5).
Table 4. Comparisions of ResNet-50 and other pruning methods on ImageNet
FLOPs after pruning
FLOPs Top1-Acc Top5-Acc
ThiNet-70 
AutoSlim 
Meta-Pruning 
0.75 × ResNet-50 
Thinet-50 
AutoSlim 
Meta-Pruning 
0.5 × ResNet-50 
ThiNet-30 
AutoSlim 
Meta-Pruning 
Table 5. Comparisions of MobileNetV1 and other pruning methods on ImageNet
FLOPs Top1-Acc Top5-Acc
0.75 × MobileNetV1 325M
NetAdapt 
Meta-Pruning 
Discussion and Conclusions
We presented EagleEye pruning algorithm, in which a fast and accurate evaluation process based on adaptive batch normalization is proposed. Our experiments show the eﬃciency and eﬀectiveness of our proposed method by delivering
higher accuracy than the studied methods in the pruning experiments on ImageNet dataset. An interesting work is to further explore the generality of the
adaptive-BN-based module by integrating it into many other existing methods
and observe the potential improvement. Another experiment that is worth a try
is to replace the random generation of pruning strategy with more advanced
methods such as evolutionary algorithms and so on.
Acknowledgements
Jiang Su is the corresponding author of this work. This work was supported in
part by the National Natural Science Foundation of China (NSFC) under Grant
No.U1811463.