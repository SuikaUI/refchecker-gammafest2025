HAL Id: hal-01253912
 
Submitted on 12 Jan 2016
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Detection and Classification of Acoustic Scenes and
Dan Stowell, Dimitrios Giannoulis, Emmanouil Benetos, Mathieu Lagrange,
Mark D. Plumbley
To cite this version:
Dan Stowell, Dimitrios Giannoulis, Emmanouil Benetos, Mathieu Lagrange, Mark D. Plumbley. Detection and Classification of Acoustic Scenes and Events. IEEE Transactions on Multimedia, 2015, 17
(10), ￿10.1109/TMM.2015.2428998￿. ￿hal-01253912￿
IEEE TRANS MULTIMEDIA
Detection and Classiﬁcation of Acoustic
Scenes and Events
Dan Stowell*, Dimitrios Giannoulis, Emmanouil Benetos, Mathieu Lagrange and Mark D.
For intelligent systems to make best use of the audio modality, it is important that they can recognise
not just speech and music, which have been researched as speciﬁc tasks, but also general sounds in
everyday environments. To stimulate research in this ﬁeld we conducted a public research challenge:
the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and
Classiﬁcation of Acoustic Scenes and Events (DCASE). In this paper we report on the state of the
art in automatically classifying audio scenes, and automatically detecting and classifying audio events.
We survey prior work as well as the state of the art represented by the submissions to the challenge
from various research groups. We also provide detail on the organisation of the challenge, so that our
experience as challenge hosts may be useful to those organising challenges in similar domains. We created
new audio datasets and baseline systems for the challenge: these, as well as some submitted systems,
are publicly available under open licenses, to serve as benchmark for further research in general-purpose
machine listening.
EDICS: 5-CONT
I. INTRODUCTION
Ever since advances in automatic speech recognition (ASR) were consolidated into working industrial
systems , the prospect of algorithms that can describe, catalogue and interpret all manner of sounds has
seemed close at hand. Within ASR, researchers continue to advance recognition quality, in challenging
This work is licensed under a Creative Commons Attribution 3.0 Unported License. DS, DG and EB are with the Centre
for Digital Music, Queen Mary University of London, UK. ML is with IRCCYN, Nantes, France. MDP is with the Centre for
Vision, Speech and Signal Processing, University of Surrey, Guildford, Surrey, UK. Much of this work was carried out while EB
was with the Department of Computer Science, City University London, UK and MDP was with the Centre for Digital Music.
*Corresponding author: 
January 12, 2016
IEEE TRANS MULTIMEDIA
audio conditions such as distant speech against noisy backgrounds . Elsewhere, advances in Music
Information Retrieval (MIR) have brought us systems that can transcribe the notes and chords in music
 , or identify the track title and artist from a low-quality sound snippet . However, speech and music
are just two of the many types of sound that can be heard in a typical indoor or outdoor environment.
Increasingly, machines deployed in diverse environments can hear—whether they be mobile phones,
hearing aids or autonomous robots—but can they make sense of what they hear?
Sound is often a useful complement to modalities such as video, carrying information not otherwise
present such as information from speech and birdsong. Sound can also be more convenient to collect,
e.g. on a mobile phone. Information gathered from a semantic audio analysis can be useful for further
processing such as robot navigation, user alerts, or analysing and predicting patterns of events . Beyond
listening devices, the same technologies have applications in cataloguing/searching audio archives, whose
digital collections have grown enormously in recent decades . Audio archives often contain a rich
diversity of speech, music, animal sound, urban soundscapes, ethnographic recordings and more, yet
their accessibility currently lags behind that of text archives.
In order to stimulate research in machine listening for general audio environments, in 2012–2013 we
organised a research challenge under the auspices of the IEEE Audio and Acoustic Signal Processing Technical Committee: the challenge on Detection and Classiﬁcation of Acoustic Scenes and Events (DCASE).
This challenge focused on two concrete but relatively general types of task that a general machine listening
system would carry out: recognising the general environment type (the acoustic “scene”), and detecting
and classifying events occurring within a scene.
These tasks which we describe as “machine listening” tasks can also be considered to come under
the umbrella of computational auditory scene analysis (CASA) . This nomenclature refers back to
Bregman’s inﬂuential work on human “auditory scene analysis” capabilities , and thus CASA is often
taken to imply an approach which aims either to parallel the stages of processing in human audition,
and/or to mimic the observed phenomena of human audition (which may include illusions such as the
“missing fundamental”) [7, Chapter 1]. These human-centric aims do not directly reﬂect our goal here,
which is to develop systems that can extract semantic information about the environment around them
from audio data.
The purpose of this paper is to give a complete description of the challenge, for two purposes: ﬁrstly
to acquaint the reader with the state of the art in machine listening, and secondly to provide guidance
and lessons learnt for the beneﬁt of people running research challenges in future. In the following, we
ﬁrst give some research background in the topic, and previous challenges that have been conducted in
January 12, 2016
IEEE TRANS MULTIMEDIA
neighbouring areas. Then we give detail on the experimental design of the tasks we designed, the approach
to evaluation, and the data which we collected for the tasks. We also consider some practicalities in the
conduct of the challenge. In Section V we give the results of each task in the challenge, results which
were ﬁrst presented at the IEEE WASPAA 2013 conference . We discuss issues emerging from the
results such as the level of task difﬁculty, and in particular we compare the “live” and “synthetic” variants
of our event detection challenge. Finally we consider the outlook for machine listening in light of the
challenge: the state of the art, future directions indicated, and the contribution that this challenge has
made. We also reﬂect on the organisational structure of this and other challenges, in relation to issues
such as reproducibility and sustainability.
II. BACKGROUND
In this section we will brieﬂy overview the tasks of acoustic scene classiﬁcation and detection of
sound events within a scene, both of which have been studied in recent literature. We discuss their
relation to other machine listening tasks, and outline standard approaches taken. We will then discuss
recent evaluation campaigns in machine listening, which set the context for our own campaign.
Acoustic scene classiﬁcation aims to characterize the acoustic environment of an audio stream by
selecting a semantic label for it . It can be considered as a machine-learning task within the widespread
single-label classiﬁcation paradigm, in which a set of class labels is provided and the system must select
exactly one for any given input [11, Chapter 1]. It therefore has parallels with audio classiﬁcation tasks
such as music genre recognition or speaker recognition , and with classiﬁcation tasks in other
time-based media such as video. When classifying time-based media, a key issue is how to analyse
temporally-structured data to produce a single label representing the media object overall. There are two
main strategies found in the literature. One is to use a set of low-level features under a “bag-of-frames”
approach, which treats the scene as a single object and aims at representing it as the long-term statistical
distribution of some set of local spectral features. Prevailing among different features for the approach is
the Mel-frequency Cepstral Coefﬁcients (MFCCs) that have been found to perform quite well . Foote
 is an early example, comparing MFCC distributions via vector quantisation. Since then, the standard
approach to compare distributions is by constructing a Gaussian Mixture Model (GMM) for each instance
or for each class . The other strategy is to use an intermediate representation prior to classiﬁcation
that models the scene using a set of higher level features that are usually captured by a vocabulary or
dictionary of “acoustic atoms”. These atoms usually represent acoustic events or streams within the scene
which are not necessarily known a priori and therefore are learned in an unsupervised manner from the
January 12, 2016
IEEE TRANS MULTIMEDIA
data. Sparsity or other constraints can be adopted to lead to more discriminative representations that
subsequently ease the classiﬁcation process. An example is the use of non-negative matrix factorization
(NMF) to extract bases that are subsequently converted into MFCCs for compactness and used to classify
a dataset of train station scenes . Building upon this approach, the authors in used shift-invariant
probabilistic latent component analysis (SIPLCA) with temporal constrains via hidden Markov models
(HMMs) that led to improvement in performance. In a system is proposed that uses the matching
pursuit algorithm to obtain an effective time-frequency feature selection that are afterwards used as
supplement to MFCCs to perform environmental sound classiﬁcation.
The goal of acoustic event detection is to label temporal regions within an audio recording, resulting
in a symbolic description such that each annotation gives the start time, end time and label for a single
instance of a speciﬁc event type. It is related in spirit to automatic music transcription , and also
to speaker diarisation, which similarly recovers a structured annotation of time segments but focusses
on speech “turns” rather than individual events . The majority of work in event detection treats the
sound signal as monophonic, with only one event detectable at a time , . In general audio scenes,
events may well co-occur, and so polyphonic event detection (allowing for overlapping event regions) is
desirable. However, salient events may occur relatively sparsely and there is value even in monophonic
detection. There has been some work on extending systems to polyphonic detection . Event detection
is perhaps a more demanding task than scene classiﬁcation, but at the same time heavily intertwined.
For example, information from scene classiﬁcation can provide supplementary contextual information for
event detection . Many proposed approaches can be found in the literature among which spectrogram
factorization techniques tend to be a regular choice. In a probabilistic latent semantic analysis (PLSA)
system, a closely related approach to NMF, was proposed to detect overlapping sound events. In a
convolutive NMF algorithm applied on a Mel-frequency spectrum was tested on detecting non-overlapping
sound events. Finally, a number of proposed systems focus on the detection and classiﬁcation of speciﬁc
sound events from environmental audio scenes such as speech , birdsong , musical instrument
and other harmonic sounds , pornographic sounds or hazardous events .
The issue of polyphony is pertinent to both of the above tasks, since audio scenes are polyphonic (multisource) in general. As with music, it is possible to perform some analysis on the audio signal as a whole
without considering polyphony, though it is likely that some beneﬁt can be obtained from considering the
component sources that make up the signal. Such a component-wise analysis is analogous to the auditory
streaming that occurs in Bregman’s model of human audition . In speech recognition applications it
can often be assumed that there is one dominant source that should be the focus for analysis , but
January 12, 2016
IEEE TRANS MULTIMEDIA
this is not the case for general audio scenes. One strategy to handle polyphonic signals is to perform
audio source separation, and then to analyse the resulting signals individually , . However, note
that the computational equivalent of auditory streaming does not necessarily require a reconstruction
of the individual audio signals—Bregman does not claim that human listeners do this—but could work
with some mid-level representation such as a multisource probabilistic model . Source-separation for
general-purpose audio is still a long way from being a solved problem . For example, the evaluation
used in recent challenges for “speech recognition in multisource environments” did not require submitted
algorithms to perform audio source-separation: evaluation was performed on speech transcription output.
Submitted algorithms generally did not involve a source-separation step, many used spatial or spectral
noise suppression in order to focus on one source rather than separating all sources .
In machine listening, public evaluation and benchmarking of systems serves a valuable role. It enables
objective comparison among various proposed systems, and can also be used for studying performance
improvements throughout the years. Many such challenges have been centred on speech. For example,
the DARPA EARS Rich Transcription evaluations focussed on speaker-diarisation tasks,
applied to broadcast news as well as recordings of meetings . The MIREX challenges 
evaluated MIR systems for their performance on speciﬁc musical tasks such as melody transcription
or rhythm tracking . The SiSEC challenges focussed on audio source separation
algorithms, both for speech mixtures and for music . The CHiME challenges focussed on
speech recognition in noisy multi-source sound environments . None of the aforementioned challenges
directly relates to the general-purpose machine listening tasks we consider here. Some of them use broadly
similar task outlines (e.g. classiﬁcation, diarisation), but often use domain-speciﬁc evaluation measures
(e.g. speech transcription accuracy, audio separation quality). They also attract contributions specialised
to the particular audio domain.
For the present purposes, the most closely-related challenge took place in 2006 and 2007, as part of
the CLEAR evaluations conducted during the CHIL project . Several tasks on audio-only, videoonly or multimodal tracking and event detection were proposed, among them an evaluation on “Acoustic
Event Detection and Classiﬁcation”. The datasets were recorded during several interactive seminars and
contain events related to seminars (speech, applause, chair moving, etc). From the datasets created for the
evaluations, the “FBK-Irst database of isolated meeting-room acoustic events” has widely been used in
the event detection literature; however, the aforementioned dataset contains only non-overlapping events.
The CLEAR evaluations, although promising and innovative at the time, were discontinued with the end
of the CHIL project.
January 12, 2016
IEEE TRANS MULTIMEDIA
One further related challenge in audiovisual research is TRECVID Multimedia Event Detection, where
the focus is on audiovisual, multi-modal event detection in video recordings . Some researchers have
used the audio extracted from the audiovisual TRECVID data in order to evaluate their systems; however
a dataset explicitly developed for audio challenges would offer a much better evaluation framework since
it would be much more varied with respect to audio.
III. THE CHALLENGE
In the present section we describe the evaluation design for our challenge tasks. Before this, we
describe the requirements gathering process that we conducted, and the considerations that fed into our
ﬁnal designs.
A. Requirements gathering
As described above, the tasks considered in this challenge relate to those explored in previous experimental studies, and to some degree to those explored in previous evaluation campaigns. There is
therefore a body of literature from which to draw potential task designs. Importantly, however, the task
designs were developed through a period of community discussion, primarily via a public email list.
This was crucial to ensure that the designs had broad relevance to current research, and did not unfairly
penalise potential participants. An example of the latter is in the choice of evaluation measures for event
detection: there was a debate about which evaluation measures were most appropriate, as well as issues
such as the appropriate level of granularity in framewise evaluation. It was this discussion that led to
the decision to report three different evaluation measures for event detection (see Section III-C3). Other
issues discussed included annotation data formats, the nature of synthetic sequences, and the use of other
existing datasets.
Our motivation was to design challenge tasks to reﬂect useful general-purpose inferences that could be
made in an everyday audio environment, pertinent to a broad range of machine listening applications. Our
focus was on everyday sounds beyond speech and music, since the latter are already well-studied. We
also wished to design tasks for which performance could be improved without necessarily being overly
reliant on other processing components such as high-quality source separation or ASR. We decided to
design challenge tasks separately for scene classiﬁcation and for event detection and classiﬁcation, using
data relating to urban and ofﬁce environments.
Many applications of machine listening relate to processing embodied in a ﬁxed hardware setup, such
as a mobile phone or a robot. This differs from applications such as audio archive analysis, for which
January 12, 2016
IEEE TRANS MULTIMEDIA
a system must be robust to signal modiﬁcations induced by variation of microphones and preprocessing
across the dataset . For embodied machine listening, aspects such as the microphone frequency
response will be constant factors rather than random factors. We chose to design our tasks each with a
ﬁxed conﬁguration of recording equipment.
One pertinent question was whether existing data could be used for our evaluation, or whether it would
be important to create new datasets. Previous studies have used relatively small datasets; further, some of
these are not publicly available. Alternatively, online archives such as Freesound hold a large amount of
soundscape data.1 However, these vary widely in recording conditions, recording quality and ﬁle format
 , , and so were unsuitable for our experimental aim to evaluate systems run with consistent audio
front-end. Thus it was important to make new recordings. This gave us various advantages: as well as
allowing us to control conditions such as the balance of sound types, it also meant that we were able to
create private testing data unseen by all participants, to ensure that there was no inadvertent overﬁtting
to the particulars of the task data. Conversely, it meant we could release the public data under a liberal
open-content license, as a resource for the research community even beyond our immediate focus.
Given that everyday sound environments are polyphonic—multiple sound events can occur at the same
time—with varying degrees of density, and given that general audio source separation is still a difﬁcult
problem, it was important to design event detection task(s) so that we could explore the effect of polyphony
on event detection systems. Such systems might be designed with a simplifying monophonic assumption;
with source separation used to feed multiple monophonic analyses; or with full polyphonic inference.
There is little data available to suggest how these different strategies perform as the event density varies.
In order to have experimental control over the event density, we chose two parallel approaches to creating
event detection audio data. In one, we made live recordings of scripted monophonic event sequences in
controlled environments. In the other, we made live recordings of individual events, and synthetically
combined these (along with ambient background recordings) into synthetic mixtures with parametrically
controlled polyphony. We describe these approaches further in Section III-C.
In December 2012 we conducted a survey of potential participants to characterise their preferred
software platforms. This indicated that most participants wished to use Matlab, Python, R or C/C++ to
create their submissions. However, all of these frameworks come in multiple versions across multiple
operating systems, and it can be difﬁcult to ensure that code running on one system will run correctly
on another. To minimise the risk of such issues, we created and published a Linux virtual machine
1 
January 12, 2016
IEEE TRANS MULTIMEDIA
which participants could use during development, and which would also be the environment used to run
the submission evaluations. For this we used VirtualBox software which runs on all common operating
systems, together with a disk image based on Xubuntu 12.10 Linux.2 The disk image was augmented by
adding the public datasets into the home folder, and also by installing Python, R and C/C++, as well as
some common audio-processing toolboxes for each environment. The resulting disk image is available
online from our research repository.3 Due to software licensing constraints we could not include Matlab
in the disk image, and so we handled Matlab-based submissions separately from the virtual machine.
We next describe the ﬁnalised design and data collection for the scene classiﬁcation task, and for the
event detection tasks.
B. Scene classiﬁcation task (SC)
Acoustic scene classiﬁcation can be considered as a single-label classiﬁcation task (see Section II).
Alternative designs are possible, such as classiﬁcation with hierarchical labels , unsupervised clustering of audio scenes, or multi-label “auto-tagging” . However, single-label classiﬁcation is the design
most commonly seen in prior literature in acoustic scene recognition , , , , , and also
lends itself to clear evaluation measures. We therefore designed the SC task as a train/test classiﬁcation
task, of similar design to previous audio classiﬁcation evaluations .
We created datasets across a pre-selected list of scene types, representing an equal balance of indoor/outdoor scenes in the London area: bus, busystreet, ofﬁce, openairmarket, park, quietstreet, restaurant, supermarket, tube, tubestation. The limitation to the London area was a pragmatic choice, known
to participants. We made sure to sample across a wide range of central and outer London locations, in
order to maximise generalisability given practical constraints. To enable participants to further explore
whether machine recognition could beneﬁt from the stereo ﬁeld information available to human listeners
[7, Chapter 5], we recorded in binaural stereo format using Soundman OKM II in-ear microphones.
For each scene type, three different recordists (DG, DS, EB) visited a wide variety of locations in
Greater London over a period of months , and in each scene recorded a few
minutes of audio. We ensured that no systematic variations in the recordings covaried with scene type:
all recordings were made in moderate weather conditions, and varying times of day, week and year, and
each recordist recorded each scene type.
2 
3 
January 12, 2016
IEEE TRANS MULTIMEDIA
We then reviewed the recordings to select 30-second segments that were free of issues such as mobile
phone interference or microphone handling noise (totalling around 50% of the recorded duration), and
collated these segments into two separate datasets: one for public release, and one private set for evaluating
submissions. The duration of 30 seconds is comparable with that of other datasets in this topic, and was
judged to be long enough to contain sufﬁcient information in principle to distinguish the classes. The
segments are stored as 30-second WAV ﬁles (16 bit, stereo, 44.1 kHz), with scene labels given in the
ﬁlenames. Each dataset contains 10 examples each from 10 scene types, totalling 50 minutes of audio
per dataset. The public dataset is published online under a Creative Commons CC-BY licence.4
For the SC task, systems were evaluated with 5-fold stratiﬁed cross validation. Our datasets were
constructed to contain a balance of class labels, and so classiﬁcation accuracy was an appropriate
evaluation measure . The raw classiﬁcation (identiﬁcation) accuracy and standard deviation were
computed for each algorithm, as well as a confusion matrix so that algorithm performance could be
inspected in more detail.
1) Baseline system for scene classiﬁcation: The “bag-of-frames” MFCC+GMM approach to audio
classiﬁcation (see Section II) is relatively simple, and has been criticised for the assumptions it incurs .
However, it is quite widely applicable in a variety of audio classiﬁcation tasks. Aucouturier and Pachet
 speciﬁcally claim that the MFCC+GMM approach is sufﬁcient for recognising urban soundscapes but
not for polyphonic music (due to the importance of temporal structure in music). It has been widely used
for scene classiﬁcation among other recognition tasks, and has served as a basis for further modiﬁcations
 . The model is therefore an ideal baseline for the Scene Classiﬁcation task.
Code for the bag-of-frames model has previously been made available for Matlab.5 However, for
maximum reproducibility we wished to provide simple and readable code in a widely-used programming
language. The Python language is very widely used, freely available on all common platforms, and is
notable for its emphasis on producing code that is readable by others. Hence we created a Python script
embodying the MFCC+GMM classiﬁcation workﬂow, publicly available under an open-source licence,6
and designed for simplicity and ease of adaptation .
4 
5 
6 
January 12, 2016
IEEE TRANS MULTIMEDIA
C. Event detection tasks (OL, OS)
For the Event Detection tasks, we addressed the problem of detecting acoustic scenes in an ofﬁce
environment, making use of existing ofﬁce infrastructure within Queen Mary University of London, and
also providing a continuation of the CLEAR evaluations , which also addressed the task of event
detection in an ofﬁce environment. In order to encourage wide participation, and also to explore the
challenge of polyphonic audio scenes, we designed two subtasks: event detection of non-overlapping
sounds (Event Detection - Ofﬁce Live) and event detection of overlapping sounds (Event Detection -
Ofﬁce Synthetic). In both cases, systems are required to detect predominant events in the presence of
background noise.
1) Recorded Dataset (OL): After a consultation period with members of the acoustic signal processing
community, for the Event Detection - Ofﬁce Live (OL) task we created recordings of ofﬁce scenes,
consisting of the following 16 classes: door knock, door slam, speech, laughter, clearing throat, coughing,
drawer, printer, keyboard click, mouse click, object (pen, pencil, marker) on table surface, switch, keys
(put on table), phone ringing, short alert (beep) sound, page turning.
Recordings were made in a number of ofﬁce environments at Queen Mary University of London, using
rooms of different size and with varying noise level or number of people in the room. We created three
datasets: a training, a development, and a test dataset. Training recordings consisted of instantiations
of individual events for every class. The development (validation) and test datasets consist of roughly
1min long recordings of scripted every-day audio events. Scripts were created by random ordering of
event types; we recruited a variety of participants to perform the scripts. For each script, multiple takes
were used, and we selected the best take as the one having the least amount of unscripted background
interference. Overall, the OL training dataset includes 24 recordings of individual sounds per class; the
development dataset includes 3 recordings of scripted sequences; and the test set consists of 11 scripted
recordings (the recording environments in the development and test datasets are non- overlapping).
Regarding equipment, recordings were made using a Soundﬁeld microphone system, model SPS422B,
able to capture 4-channel sound in B-format. The 4-channel recordings were converted to stereo (using the
common “Blumlein pair” conﬁguration). B-format recordings were stored along with the stereo recordings,
with scope for future challenges to be extended to full B-format and take into account spatial information.
Given the inherent ambiguity in the annotation process (especially for annotating offsets), we created
two sets of annotations. Annotators were trained to use Sonic Visualiser7 to use a combination of listening
7 
January 12, 2016
IEEE TRANS MULTIMEDIA
and inspecting waveforms/spectrograms to reﬁne the onsets and offsets of each sound event. We then
examined the two annotations per recording for consistency, and performed evaluations using an average
of both annotations. The OL training dataset8 and the development dataset9, both consisting of B-format
recordings, stereo recordings, and annotations, were released under a Creative Commons license.
2) Synthetic Dataset (OS): We also decided that this challenge presented a good opportunity to study
the relevance of considering artiﬁcial scenes built from a set of isolated events different from those of the
training corpus. Though we admit that it is important to evaluate machine listening systems using real
audio recordings, the potential gains from using artiﬁcial scenes as part of evaluation are numerous: ease
of annotation, ability to generate many scenes with similar properties in order to gain better statistical
signiﬁcance, control of the complexity in terms of events overlap, strength of the background, etc. This
will potentially help the designers of machine listening systems to better understand the behavior of those
As in other domains, using synthetic data may lead to biased conclusions. It is for example well
known that Independent Component Analysis (ICA) approaches in microphone arrays perform really
well in separating the different sources within an additive non-convolutive mixtures because the input
signal follows directly the mixture model assumed by those approaches. Special care was therefore taken
in order to minimize the amount of artiﬁcial regularity induced by the generating system that could
provide unrealistic beneﬁts to some evaluated machine listening systems.
The scene synthesizer we considered here is able to create a large set of acoustic scenes from many
recorded instances of individual events. The synthetic scenes are generated by randomly selecting, for
each occurrence of each event we wish to include, one representative excerpt from the natural scenes, then
mixing all those samples over a natural texture-like background with no distinctive sound events. The
distribution of events in the scene is also random, following high-level directives that specify the desired
density of events. The average Signal to Noise Ratio (SNR) of events over the background texture is also
speciﬁed and is the same for all event types, unlike in the OL scenes. This is a deliberate decision taken
to avoid issues with the annotation of potentially non perceptible events drowned in the background. In
order to avoid issues with artiﬁcial spatialization, the recordings of individual events were mixed down
to mono as an initial step.
The resulting development and testing datasets consist of 12 synthetic mono sequences with varying
8 
9 
January 12, 2016
IEEE TRANS MULTIMEDIA
durations, with accompanying ground-truth annotations. Three subsets were generated with increasing
levels of complexity in terms of event density: 4 recordings have a ‘low’ event density10 of 1.11, 4
recordings have a ‘medium’ event density of 1.27, and 4 recordings have a ‘high’ event density of 1.81.
Three SNR levels of events over the background texture were used: -6dB, 0dB, and 6dB.
3) Metrics: Following consultation with acoustic signal processing researchers, three types of evaluations were used for the OL and OS event detection tasks, namely frame-based, event-based, and classwise event-based evaluations. Frame-based evaluation was performed using a 10ms step and metrics were
averaged over the duration of the recording. The main metric used for the frame-based evaluation was
the acoustic event error rate (AEER) used in the CLEAR evaluations :
AEER = D + I + S
where N is the number of events to detect for that speciﬁc frame, D is the number of deletions (missing
events), I is the number of insertions (extra events), and S is the number of event substitutions, deﬁned as
S = min{D, I}. Additional metrics include the Precision, Recall, and F-measure (P-R-F). By denoting
as r, e, and c the number of ground truth, estimated and correct events for a given 10ms frame, the
aforementioned metrics are deﬁned as:
For the event-based metrics, two types of evaluations took place, an onset-only and an onset-offsetbased evaluation. For the onset-only evaluation, each event was considered to be correctly detected if the
onset was within a 100ms tolerance. This tolerance value was agreed during the community discussion
via the challenge mailing list. It was argued that having a tolerance smaller than 100ms would lead
to poor results particularly in the case of ill-deﬁned onsets and offsets for non-percussive events For
the onset-offset evaluation, each event was correctly detected if its onset was within a 100ms tolerance
and its offset was within 50% range of the ground truth event’s offset w.r.t. the duration of the event.
Duplicate events were counted as false alarms. The AEER and P-R-F metrics for both the onset-only
and the onset-offset cases were utilised.
Finally, in order to ensure that repetitive events did not dominate the evaluation of an algorithm, classwise event-based evaluations were also performed. Compared with the event-based evaluation, the AEER
10The average event density is calculated using 10ms steps, using only time frames where events are present. For the OL set,
the event density for each recording is by deﬁnition 1, because by design events did not overlap.
January 12, 2016
IEEE TRANS MULTIMEDIA
and P-R-F metrics are computed for each class separately within a recording and then averaged across
classes. For example, the class-wise F-measure is deﬁned as:
where Fk is the F-measure for events of class k. Matlab code for the metrics can be found online.11
4) Baseline System: We created a baseline system for both event detection tasks based on the nonnegative matrix factorization (NMF) framework. NMF has been shown to be useful for modelling the
underlying spectral characteristics of sources hidden in an acoustic scene , and can also support
overlapping events, making it suitable for both the OL and OS tasks. We chose to design a supervised
method for event detection, using a pre-trained dictionary of acoustic events .
The baseline method is based on NMF using the Kullback-Leibler divergence as a cost function .
As a time-frequency representation, we used the constant-Q transform with a log-frequency resolution
of 60 bins per octave . The training data is normalized to unity variance and NMF is used to learn
a set of N bases for each class. The numbers of bases tested is 5, 8, 10, 12, 15, 20 and 20i, the latter
corresponding to learning individually one basis per training sample, for all 20 samples. Putting together
the sets for all classes, we built a ﬁxed dictionary of bases used subsequently to factorize the normalized
input test data.
Formally, if we denote as ˆV ∈RΩ×T the constant-Q spectrogram of a test recording (Ω: number of
log-frequency bins; T: number of time frames), W ∈RΩ×N the pre-extracted dictionary and H ∈RN×T ,
the NMF model attempts to approximate V as a product of W and H. In the supervised case (when
W is known and kept ﬁxed), this involves simply estimating H iteratively until convergence using the
following multiplicative update, ensuring a non-increasing divergence between ˆV and WH :
H ←H ⊗W T ((WH)−1 ⊗ˆV )
In order to detect sound events, we sum together the (non-binary) activations per class obtained from
H. Finally, a threshold θ is chosen to binarise the real-valued activations per class (i.e. for each row
of H), in order to yield a sequence of estimated overlapped events. The value of θ is the same for all
event classes; the optimal N and θ values were chosen empirically by maximizing the F-measure for
the two annotations on the development set. Smoothing on the activations was also tested with no clear
11 
January 12, 2016
IEEE TRANS MULTIMEDIA
improvements. The baseline event detection system was made available to challenge participants under
an open-source license.12
D. Challenge organisation
The full timeline for the challenge organisation is given in Table I. Some of the items included of the
timeline will be obvious to an outside observer. However there are some aspects of the timeline and the
workload which we believe merit emphasis:
• There were two periods which required the most time commitment from the organising team: creating
the datasets, and running the code submissions. In particular, as has been remarked by organisers
of related challenges , no matter how many precautions are taken to ensure people submit
code that will run on the organisers’ hardware (formal speciﬁcations, published virtual machine),
it often requires many person-hours of attention before submitted code will run properly. This will
be discussed further below. Recording the datasets also took signiﬁcant time: this was not just the
audio recording itself, but also the supervision of annotators, and the listening sessions and manual
inspection to ensure data quality.
• We found it extremely useful to ask people to let us know of their intentions, in order to help us
plan. In December 2012 we surveyed the community for indicative data about the level of interest in
task participation, as well as the preferences for programming languages and operating systems. This
information fed directly into our design of a Linux virtual machine for people to test their code. Then
in March 2013 we asked participants to email us announcing their intentions to take part (with no
commitment implied). This enabled us to plan resources, and to follow up on expected submissions
that went astray. We received 20 notiﬁcations of intention to submit, some corresponding to multiple
task submissions; of these only three did not eventually submit.
• One aspect of the timeline that could have been improved was the long wait between collating the
results and releasing them publicly. It meant that participants could not compare and contrast results
while their systems were “fresh in their minds”. However, this was due to our decision to co-ordinate
with WASPAA 2013, which was an ideal forum for discussion of the challenge outcomes.
Regarding the execution of code submissions, our publication of a virtual machine as a standard
platform certainly reduced the number of compatibility issues we had to deal with. However, there
remained various software issues we encountered when running the code submissions:
12 
January 12, 2016
IEEE TRANS MULTIMEDIA
TIMELINE OF DCASE CHALLENGE ORGANISATION. THE TIMELINE IS DIVIDED INTO MAIN PHASES, AND MILESTONES ARE
HIGHLIGHTED.
April 2012:
Challenge proposed to IEEE AASP
Challenge accepted by IEEE AASP
Initial recordings made, to test equipment and to produce examples for discussion
Organisers of IEEE WASPAA 2013 agree to host a challenge results special session
Jun–Dec 2012:
Roaming recordings for SC task
⇒Call for participation published to various mailing lists; website established
Challenge publicised in IEEE SPS Newletter
Aug–Sep 2012:
Community discussion on dedicated challenge mailing list.
⇒Task design completed
Sep–Oct 2012:
Ofﬁce recordings for OL and OS tasks
Oct–Nov 2012:
Ground-truth annotation of OL recordings (external annotators: 2 x 35 hours)
Nov–Dec 2012:
Listening sessions to all recordings, to ensure data quality; error-checking and correction of manual annotations
⇒Public release of training/development datasets (audio and annotations)
Online survey of potential participants (preferred tasks, programming languages, operating systems)
Create the Linux virtual machine disk image
Jan–Feb 2013:
Write paper introducing the tasks and baseline systems
Jan–Feb 2013:
Generating synthetic testing dataset for OS task
⇒Publication of ﬁnalised task speciﬁcations (output formats, eval metrics etc.) as well as virtual machine
Publication of the scripts used to calculate the evaluation metrics
Publication of synthetic development dataset for OS task
Feb–Mar 2013:
Further community discussion; ofﬁcial conﬁrmation that for OL/OS three separate evaluation metrics will be applied
Request for participants to email to conﬁrm participation
⇒Deadline for participants to submit code
Apr–May 2013:
Running all code submissions – team liaises with authors about software issues etc, compiles result statistics
Deadline for participants to submit extended abstracts for WASPAA 2013
Results released privately to each participating team
Write paper giving results of the challenge
⇒Results released publicly at WASPAA 2013
• A frequent issue in Matlab submissions was opening the training annotation text ﬁles (for reading)
using mode ‘r+’ (which is for reading and writing). This fails when ﬁles are read-only. We had set
the test data as read-only, and in the speciﬁcation we had stated that submissions must not write
data in the test folder.
• One submission had been developed using Matlab on Windows; when we ran it using the same
January 12, 2016
IEEE TRANS MULTIMEDIA
version of Matlab, but on Linux, it got rather poor results, which we initially attributed to overﬁtting.
It later emerged that the poor performance was because a Matlab toolbox exhibited a bug only when
running on Linux.
• On the virtual machine, there were occasional problems with version mismatch between Dynamic
Link Libraries (DLLs). Such issues were reduced but not completely eliminated with the use of the
virtual machine, often because participants did not fully test using the virtual machine, or occasionally
added late modiﬁcations after testing.
• One submission output space-separated results rather than tab-separated. This was contrary to the
published speciﬁcations but easy to miss in manual checking.
• Some submissions contained subtle bugs in data parsing. One submission accidentally ignored the
last line of every text ﬁle it read, meaning that it output 19 decisions for each testing fold rather
than 20. This wasn’t detected early on (because the output was correctly-formatted and could be
scored), but only at the point where an overall confusion matrix was compiled. A different submission
involved a script which parsed the text output from an executable. When the script failed to parse
the text, it always decided on the last class in the list – failure was only detected in the large number
of “tubestation” outputs.
Some of these issues (e.g. the data format issues) could have been prevented by providing unit tests
which participants must pass before submitting.
Earlier in the process, we also encountered a data issue: after we published the development datasets,
a community member on the mailing list alerted us to a formatting error in some of the annotations (the
text label doorknock was used in some places rather than the ofﬁcial label knock). Such issues occur
despite the multiple steps of checking we performed before release. We updated the dataset to correct
the issue, re-released it and conﬁrmed this to the participants.
We required each submitted system to be accompanied by an extended abstract describing the system.
We experienced no issues in publishing these abstracts; however in future evaluations we would consider
explicit open-access licensing of the abstracts for greater clarity.
IV. SUBMITTED SYSTEMS
Overall, 11 systems were submitted to the scene classiﬁcation (SC) task, 7 systems were submitted to
the ofﬁce live (OL) event detection task, and 3 systems to the ofﬁce synthetic (OS) event detection task.
Variants for each system were allowed, which increased the total number of systems somewhat.
January 12, 2016
IEEE TRANS MULTIMEDIA
SUMMARY OF SUBMITTED SCENE CLASSIFICATION SYSTEMS.
Participants
Various features at 2 frame sizes, classiﬁed either: (a) per-frame SVM +
majority voting; (b) HMM
Elizalde 
Concatenation of 4 different mono mixdowns; “i-vector” analysis of
MFCCs, classiﬁed by pLDA
Diverse features, classiﬁed within 4-second windows using SVM, then
majority voting
“Cochleogram” representation, analysed for tonelikeness in each t-f bin,
classiﬁed by SVM
Li et al. 
Wavelets, MFCCs and others, classiﬁed in 5-second windows by treebagger,
majority voting
Nam et al. 
Feature learning by sparse RBM, then event detection and max-pooling,
classiﬁed by SVM
Nogueira et al.
MFCCs + MFCC temporal modulations + event density estimation +
binaural modelling features, feature selection, classiﬁed by SVM
Olivetti 
Normalised compression distance (Vorbis), Euclidean embedding, classiﬁed
by Random Forest
Elhilali 
Auditory representation analysed for spectrotemporal modulations, classi-
ﬁed within one-second windows using SVM, then weighted combination of
decision probabilities
Rakotomamonjy
and Gasso 
Computer vision features (histogram of oriented gradient) applied to
constant-Q spectrogram, classiﬁed by SVM
Recurrence Quantiﬁcation Analysis applied to MFCC time-series, classiﬁed
MFCCs, classiﬁed with a bag-of-frames approach
Majority Vote
Majority voting of all submissions
The systems submitted for the scene classiﬁcation task are listed in Table II, along with a short
description of each system. Citations are to the extended abstracts giving further technical details about
each submission. The methods for scene classiﬁcation are discussed further in a tutorial article ,
while in Section V-A we will expand on some aspects of scene classiﬁcation methods when considering
which approaches led to strong performance.
The systems submitted for the event detection tasks are listed in Table III, along with a short de-
January 12, 2016
IEEE TRANS MULTIMEDIA
SUMMARY OF SUBMITTED EVENT DETECTION SYSTEMS.
Participants
Chauhan et al.
Feature extraction - Segmentation - Likelihood ratio test classiﬁcation
MFCCs (features) - HMMs (detection)
Gemmeke et al.
NMF (detection) - HMMs (postprocessing)
Niessen et al.
Hierarchical HMMs + Random Forests (classiﬁcation) - Meta-classiﬁcation
Nogueira et al.
MFCCs (features) - SVMs (classiﬁcation)
Schr¨oder et al.
Gabor ﬁlterbank features - HMMs (classiﬁcation)
MFCCs (features) - GMMs (detection)
NMF with pre-extracted bases (detection)
Fig. 1. Schematic of event detection systems (nodes with a * are not systematically used). Below, state-of-the-art design choices
are given as examples.
Pre-processing*
Classiﬁcation
Post-processing*
scription of each system. Citations are to the extended abstracts giving further technical details about
each submission. Figure 1 shows the processing chain adopted by the submitted algorithms. The main
processing nodes are the feature computation and the classiﬁcation for which a variety of implementations
are considered. Optionally, the audio data can be pre-processed for example to reduce the inﬂuence
of background noise, and the decisions given by the classiﬁers can be smoothed to reduce unrealistic
transitions between events.
The system designs for each submission are now described:
• CPS: The CPS submission follows a scheme that combines segmentation, feature extraction, and
classiﬁcation. Firstly, various frequency-based and time-based features are extracted. The audio
stream is subsequently segmented using a speech segmenter that uses energy-based features. Each
January 12, 2016
IEEE TRANS MULTIMEDIA
segment is then assigned to a class using a generalised likelihood ratio test classiﬁer.
• DHV: The DHV submission was created for both the OL and OS tasks. It follows a generative
classiﬁcation scheme using HMMs with multiple Viterbi passes. Firstly, MFCCs are extracted as
features, and used as input to continuous-density HMMs (each state corresponds to an event class,
including background noise). Polyphonic detection is achieved by performing consecutive passes of
the Viterbi algorithm.
• GVV: The GVV submission uses a dictionary-based model using NMF. Firstly, a dictionary is
created using samples from training set (called exemplars), using mel-magnitude spectrograms as
time-frequency representations. The input spectrogram is projected onto the dictionary using NMF
using the Kullback-Leibler divergence. The resulting event probability estimates are post-processed
using an HMM containing a single state per event.
• NVM: The NVM submission follows a two-step classiﬁcation scheme. At the ﬁrst step, a large
variety of audio features that capture temporal, spectral or auto-correlation properties of the signal
are fed to two classiﬁers: a two-layer HMM and a random forest classiﬁer. Another HMM is then
used to combine the predictions.
• NR2: The NR2 submission follows a discriminative classiﬁcation scheme implemented with support
vector machines (SVMs). The classiﬁer is fed with MFCCs that are computed using either the
original signal or a noise-reduced one. The decisions coming from the classiﬁed versions are then
combined and smoothed to reduce short transitions.
• SCS: The SCS submission follows a generative classiﬁcation scheme with a 2-layer HMM decoding.
The classiﬁer is fed with 2 dimensional Gabor features (Time / Frequency) that allows percussive
events to be nicely modelled. Before feature computation, the audio signal is enhanced using a noise
suppression scheme that estimate the noise power spectral density and remove it in the spectral
• VVK: The VVK submission follows a generative classiﬁcation scheme with a GMM decoding.
GMM models for each class of events and the background are ﬁrst trained with MFCCs. The event
models are next re-estimated in order to reduce the impact of background frames on the model
likelihoods. At decoding the likelihoods are smoothed using a moving average ﬁlter and thresholded
to produce the prediction.
• Baseline: A detailed description of the Baseline system is given in Section III-C.
January 12, 2016
IEEE TRANS MULTIMEDIA
Chance level
Accuracy (%)
Fig. 2. Mean values and conﬁdence intervals of the accuracy of methods for SC evaluated on the DCASE private dataset using
stratiﬁed 5-fold cross-validation. The boxes enclose methods that cannot be judged to perform differently with a signiﬁcance
level of 95% using a sign test . For example, GSR > baseline, but we cannot conﬁrm that CHR > baseline. Figure is
adapted from .
V. RESULTS
A. SC results
Figure 2 shows the overall performance of submitted systems for the scene classiﬁcation task. The
baseline system achieved an accuracy of 55%; most systems were able to improve on this, although our
signiﬁcance tests were able to demonstrate a signiﬁcant improvement over baseline only for the strongest
four systems. The results indicate that level of difﬁculty for the task was appropriate: the leading systems
were able to improve signiﬁcantly upon the baseline, yet the task was far from trivial for any of the
submitted systems. Also, the sizes of the error bars indicate that performance across the ﬁve folds was
broadly consistent, indicating that the dataset was not overly heterogeneous. However, the statistical tests
did not demonstrate signiﬁcant differences between various systems (depicted by the large overlap of
boxes in Figure 2), which implies that a larger dataset may have enabled a more ﬁne-grained ranking of
systems. The results for this SC task are further analysed in a tutorial article . For that reason, here
we discuss brieﬂy the state of the art reﬂected in the SC task outcomes, allowing us to expand further
on the OL/OS task outcomes in the next section.
The majority of the submitted systems used discriminative training, with many of the strong performers
January 12, 2016
IEEE TRANS MULTIMEDIA
using an SVM as the ﬁnal classiﬁer. Further, most of the leading results were obtained by those who
captured medium-range temporal information in the features used for classiﬁcation. Four of the ﬁve
highest-scoring systems did this: Roma et al. captured temporal repetition and similarity using
“recurrence quantiﬁcation analysis”; Rakotomamonjy and Gasso used gradient features from imageprocessing; Geiger et al. extracted features from linear regression over time; Chum et al. trained
a HMM. Each of these is a generic statistical model for temporal evolution, whose ﬁtted parameters can
then be used as features for classiﬁcation.
From the perspective of CASA, it is notable that none of the submitted systems used any kind of
decomposition of each audio scene into auditory streams. We suggest that this is not due to any inherent
difﬁculty in decomposing audio scenes, since automatic classiﬁcation does not require “listening-quality”
outputs from such preprocessing. Instead it seems likely that it is more difﬁcult to design a classiﬁcation
workﬂow that makes use of structured scene analysis outputs, whose data may for example be sets of
labelled intervals rather than time-series statistics. Two submissions made use of event detection as part
of preprocessing, which does yield a structured parse of the audio scene , . Those authors then
used summary statistics from the density/strength of event detections as features. We propose that further
reﬁnement and development of this strategy may be a fruitful area for future work, perhaps via more
sophisticated temporal summary statistics such as those noted above.
Also notable is that the submissions with the more perceptually-motivated features—auditory spectrogram and cochleogram —did not lead to the strongest results. Nor did the unsupervised feature
learning of . The various ways to approach audio feature design—perceptual, acoustic, statistical—
each have their merits. Based on the present evaluation, we note only that the more sophisticated audio
features did not yield a decisive advantage over simpler features.
We tested a simple majority-vote classiﬁer from the pool of SC submissions, constructed by assigning
to an audio recording the label that was most commonly returned by other methods. This attained a
strong result, indicated as “MV” in the ﬁgure: 77% accuracy, slightly better than the leading individual
submission. The strong performance of this meta-classiﬁer is particularly notable given its simplicity—
all systems are combined with equal weights. It suggests that for around 77% of soundscapes some
algorithms make a correct decision, and the algorithms that make an incorrect classiﬁcation do not all
agree on one particular incorrect label. This allows to combine the decisions into a relatively robust
meta-classiﬁer. (Note that we did not test for signiﬁcance of the comparison between MV and the other
results, because the MV output is not independent of the output of the individual submissions.) More
sophisticated meta-classiﬁers could perhaps extend this performance further.
January 12, 2016
IEEE TRANS MULTIMEDIA
AGGREGATE CONFUSION MATRIX FOR SCENE CLASSIFICATION ACROSS ALL SUBMISSIONS. ROWS ARE GROUND TRUTH,
COLUMNS THE INFERRED LABELS. VALUES ARE EXPRESSED AS PERCENTAGES ROUNDED TO THE NEAREST INTEGER.
busystreet
openairmarket
quietstreet
restaurant
supermarket
tubestation
busystreet
openairmarket
quietstreet
restaurant
supermarket
tubestation
Table IV shows a confusion matrix for the scene labels as round percentages of the sum of all confusion
matrices for all submissions. Confusions were mostly concentrated over classes that share some acoustical
properties such as park/quietstreet and tube/tubestation. Our labels contained ﬁve indoor and ﬁve outdoor
locations, and both types showed a similar level of difﬁculty for the algorithms.
B. OL/OS results
Results for the event detection OL and OS tasks are summarized in Tables V and VI, respectively.13
The baseline was outperformed by most systems for both tasks. Results for the OL task indicate the high
level of difﬁculty in recognising sound events (from many possible classes, with great variability) from
noisy acoustic scenes. The best performance for the OL task using most types of metrics is achieved by
the SCS submission, which used a Gabor ﬁlterbank feature extraction step with by 2-layer hidden Markov
models (HMMs) for classifying events, followed by the NVM submission, which used a meta-classiﬁer
13The original OS results published at the time of the challenge differ from the results published here due to a systematic fault
affecting a subset of the labels in the original OS development and test datasets. This was found and ﬁxed, and the three teams
who submitted systems to the OS task were contacted and invited to revise their systems. The DHV system was re-trained on
the corrected OS development data; the conﬁgurations of the other systems (GVV, VVK and baseline) were not affected, and
were left unchanged. All three systems were re-evaluated on the corrected test datasets to obtain the results here. The corrections
to the data generally improved system performance, which is to be expected since they improved the correspondence between
training and test sets.
January 12, 2016
IEEE TRANS MULTIMEDIA
combining hierarchical HMMs and random forests. Results for each event class separately are visualised
in Figure 3, where it can be seen that most systems had solid detection rates for clearing throat, coughing,
door knocks and speech, but had weak results for drawers, printers, keyboards and switches.
Many of the OL/OS methods employed a decomposition step, either expliticly (e.g. GVV) or implicitly
(e.g. DHV), which is of interest from the perspective of CASA (see Section I). It should be noted that
MFCCs were not proven as useful for the event detection tasks as with the scene classiﬁcation tasks,
with more rich and auditory model-based representations proving to be more useful (such as Gabor
ﬁlterbanks and Mel-magnitude spectrograms). Again, contrary to the SVM-dominated scene classiﬁcation
task, variants of HMMs were proven to be both the most popular as well as reliable tools for event
detection, due to their ability to model timeseries data. Of particular interest are submissions that were
also submitted to the polyphonic OS task, where two systems experimented with multiple Viterbi passes
(DHV, GVV) in order to handle overlapping events.
Regarding statistical signiﬁcance tests in event detection systems, to the authors’ knowledge no such
tests have been attempted so far in the literature. As has been argued in for the multi-pitch detection
problem (which is structurally similar to sound event detection), indicators of statistical signiﬁcance are
not highly pertinent for multi-class detection problems: in practice, even a small performance difference
can often yield statistical signiﬁcance. Detailed system descriptions and detailed results per system can
be found on the challenge website.14 However, for the OL task there was a large enough number of
participants that we were able to examine statistically whether the different metrics tended to rank systems
in the same order. We applied the Kruskal-Wallis test, a nonparametric test for comparing whether multiple
groups of data (here, the evaluation ranks for each system) exhibit differences in distribution [?, Section
6.2.2]. This found a signiﬁcant pattern of agreement among the OL rankings judged across all evaluation
measures (H = 88.4, p = 3.3 × 10−14). The only systematic deviation from consensus among the
evaluation metrics was for submission GVV, which was ranked low (9th or 10th) on all F-measures but
3rd or better on all AEER measures.
For the OS task, the best performance across most of our evaluation metrics is achieved by the GVV
system, which used NMF decomposition followed by HMM postprocessing. Overall rates for each system
are broadly comparable with the OL task. It should also be noted that submitted systems performed better
for signals with lower polyphony, with the exception of the DHV system, which had better performance
with higher polyphony levels (5.57% frame-based F for low polyphony and 30.08% for high polyphony).
14 
January 12, 2016
IEEE TRANS MULTIMEDIA
RESULTS FOR THE PARTICIPATING SYSTEMS FOR THE OFFICE LIVE EVENT DETECTION TASK. THE STRONGEST
PERFORMANCE ACCORDING TO EACH METRIC IS HIGHLIGHTED IN BOLD.
Evaluation Method
Event-Based
Class-Wise Event-Based
Frame-Based
Foﬀset (%)
Foﬀset (%)
As expected, the onset-offset evaluation produced worse results compared to onset-only evaluation for
both tasks, although the performance difference is rather small (this may be explained by the percussive
nature of most events).
It is also instructive to look at the correlation between the ranking of systems that were both submitted
to the Ofﬁce Live and Ofﬁce Synthetic challenges. It allows us to study the consistency of performance
of the evaluated systems using natural and artiﬁcial data. The OL and OS tests are not independent, since
they partly use the same audio source material, but comparing their outcomes gives us an indication of
whether the synthesis procedure for OS had a strong impact on the eventual rankings. Let us consider
results achieved using the Class-Wise Event-Based metrics as they are more resilient to discrepancies
between datasets in terms of density of events within the scene. Apart from a slight permutation of GVV
and VVK systems, the strong level of correlation (average of 90% over the 4 metrics in terms of the
Spearman’s rank correlation coefﬁcient) indicates that considering artiﬁcially synthesised sound scenes
may have some meaning for this kind of task.
VI. REFLECTIONS AND RECOMMENDATIONS
Before concluding, we wish to draw some reﬂections out from the above results and from our experience
of managing the DCASE challenge, and to offer some recommendations for future evaluation challenges.
January 12, 2016
IEEE TRANS MULTIMEDIA
RESULTS FOR THE PARTICIPATING SYSTEMS FOR THE OFFICE SYNTHETIC EVENT DETECTION TASK. THE STRONGEST
PERFORMANCE ACCORDING TO EACH METRIC IS HIGHLIGHTED IN BOLD. SEE FOOTNOTE 13.
Event-Based
Class-Wise Event-Based
Frame-Based
Foﬀset (%)
Foﬀset (%)
Our challenge comes in the context of a series of challenges coordinated by the IEEE AASP, such as
challenges relating to distant and reverberant speech.15
Our design of the challenge involved participants submitting code, for the organisers to execute against
private datasets. This design, in common with MIREX music audio challenges , incurs resource costs
as the hosts must dedicate time to running the submissions. It also requires holding back some private
data, which cannot immediately beneﬁt the community as open data. However it has advantages such
as ensuring participants do not overﬁt to the test data, and ensuring that results are reproducible in the
sense of empirically verifying that the submitted software can be run by a third party.
An interesting point of comparison is provided by similar challenges run via the Kaggle website
such as the 2013 SABIOD machine listening challenges.16 These challenges centered around automatic
classiﬁcation of animal sounds. The mode of interaction in that case was not to submit code, but to
submit system output. Further, participants could iteratively modify their code and submit updated output,
getting feedback in the form of results on a validation dataset. This does carry some risk of overﬁtting
to the speciﬁcs of the challenge, and less direct reproducibility, although the winning submission was
required to be made open-source and conﬁrmed by the hosts. Relative to DCASE, the SABIOD challenges
appeared to encourage a greater amount of ad-hoc participation from independent machine-learning
professionals, perhaps due to the immediate feedback loop made possible by the online system. The
workﬂows represented by the DCASE and the SABIOD challenges each have their own strengths and
weaknesses, and we look forward to further reﬁnements in public evaluation methodology.
We have enumerated the steps involved in running the DCASE challenge, in particular to highlight
15 
16 
January 12, 2016
IEEE TRANS MULTIMEDIA
clearthroat
Fig. 3. Event detection (OL) results in class-wise F (%) for each event class separately.
the resource implications for hosting such challenges. Dataset collection and annotation was the main
requirement on staff time. This challenge was not funded explicitly by any project, and so would not have
been possible without the resources made available by a large research group (see Acknowledgments).
This includes staff and PhD students as core organisers, data annotators, programmers assisting with
issues such as code and the virtual machine, and infrastructure such as code- and data-hosting facilities.
In Section III-D we described various steps we took to ensure that the challenge would run smoothly,
such as publishing formal task speciﬁcations, baseline code and a virtual machine. This reduced but by
no means eliminated the time required to run and troubleshoot the code submissions received. A clear
recommendation that emerges from this experience is that a formal test for the submitted code to be run
January 12, 2016
IEEE TRANS MULTIMEDIA
at submission time would help greatly. This could be applied in the form of automated unit testing, or
more simply by the challenge organisers running the submissions using public data and conﬁrming that
the results obtained match the results that the submitters obtained on their own system.
Community involvement was crucial to the successful conduct of this challenge, in particular for
discussing the task speciﬁcations, but also for negotiating logistics of submission and discussing the ﬁnal
results. The support of the IEEE AASP Technical Committee and the IEEE WASPAA 2013 Conference
Committee helped us to form this community.
VII. CONCLUSIONS
With the DCASE challenge we aimed to frame a set of general-purpose machine listening tasks for
everyday audio, in order to benchmark the state of the art, stimulate further work, and grow the research
community in machine listening beyond the domains of speech and music. The challenge results illustrate
tasks we designed for this had the right level of difﬁculty for this: none of the tasks was trivial for
any submitted system, and a range of scores was achieved enabling comparison of the advantages and
disadvantages of systems. The strong level of participation from a diverse set of researchers indicates
that the tasks were pertinent to current research.
For the scene classiﬁcation (SC) task, the leading systems attained results signiﬁcantly above baseline
and comparable to average results from human listeners. A strategy used by many of the strongest systems
was to use feature representations which capture medium-scale temporal information about the sound
scene. However there is still room for improvement beyond the highest-scoring system; we demonstrated
this was possible with a simple majority-vote metaclassiﬁer aggregating the submitted systems, illustrating
that there is information yet present in the audio that can drive stronger performance in future. The best
way to improve the SC task in future rounds would be through larger dataset sizes in order to draw
stronger conclusions about the signiﬁcance of differences between system performances.
For the event detection (OL/OS) tasks, the leading systems achieved relatively strong performance,
although with substantial scope for improvement. This was particularly evident in the polyphonic OS
task, indicating that polyphony in audio scenes remains a key difﬁculty for machine listening systems
and more development is needed in this area. However, the class-wise analysis of results also indicates
that some event types proved harder to detect than others, even in the monophonic OL task, indicating
that the ability for one system to detect a wide range of sound types is also a key challenge. Future event
detection challenges could be improved with further community attention to evaluation metrics and their
relation to practical requirements. It may also be of value to evaluate systems explicitly regarding the
January 12, 2016
IEEE TRANS MULTIMEDIA
correlation between their performance and the level of polyphony in a scene.
Regarding the community formed around this research topic, we were very encouraged by the strong
level of participation, and by the decisions of various groups to publish their submitted systems as opensource code. These, alongside the resources which we published (open-source baseline systems; open
datasets; virtual machine disk image) provide a rich resource for others who may wish to work in this
area.17 The community has set a benchmark, establishing that leading techniques are able to extract
substantial levels of semantic detail from everyday sound scenes, but with clear room for improvement
in future.
ACKNOWLEDGMENTS
We would like to thank the IEEE AASP Technical Committee for endorsing and supporting this work,
as well as all challenge participants—not only for their submissions but their community participation
in shaping the challenges, and their presentations at IEEE WASPAA 2013. We would also like to thank
the IEEE WASPAA 2013 Conference Committee for their support in organising the special session.
This work was supported by EPSRC Leadership Fellowship EP/G007144/1, EPSRC Research Grant
EP/H043101/1, ANR Houle under reference ANR-11-JS03-005-01, and a City University London Research Fellowship.