Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2533–2538,
Lisbon, Portugal, 17-21 September 2015. c⃝2015 Association for Computational Linguistics.
A Multi-lingual Annotated Dataset
for Aspect-Oriented Opinion Mining
Salud Mar´ıa Jim´enez Zafra1, Giacomo Berardi2, Andrea Esuli2,
Diego Marcheggiani2, Mar´ıa Teresa Mart´ın-Valdivia1, Alejandro Moreo Fern´andez2
1Departamento de Inform´atica, Escuela Polit´ecnica Superior de Ja´en
Universidad de Ja´en, E-23071 - Ja´en, Spain
{sjzafra, maite}@ujaen.es
2Istituto di Scienza e Tecnologie dell’Informazione “A. Faedo”
Consiglio Nazionale delle Ricerche, I-56124 - Pisa, Italy
{firstname.lastname}@isti.cnr.it
We present the Trip-MAML dataset, a
Multi-Lingual dataset of hotel reviews
that have been manually annotated at the
sentence-level with Multi-Aspect sentiment labels. This dataset has been built
as an extension of an existent English-only
dataset, adding documents written in Italian and Spanish.
We detail the dataset
construction process, covering the data
gathering, selection, and annotation. We
present inter-annotator agreement ﬁgures
and baseline experimental results, comparing the three languages. Trip-MAML is
a multi-lingual dataset for aspect-oriented
opinion mining that enables researchers (i)
to face the problem on languages other
than English and (ii) to the experiment the
application of cross-lingual learning methods to the task.
Introduction
Reviews of products and services that are spontaneously produced by customers represent a source
of unquestionable value not only for marketing
strategies of private companies and organizations,
but also for other users since their purchasing decisions are likely inﬂuenced by other customers’
opinions .
Overall ratings (e.g., in terms of a ﬁve stars rating scale), and also aspect-speciﬁc ratings (e.g.,
the Cleanliness or Location of a hotel), are the typical additional information expressed by customers
in their reviews. Those ratings help to derive a
number of global scores to facilitate a ﬁrst screening of the product or service at hand. Notwithstanding, users who pay more attention to a particular aspect (e.g., the Rooms of a hotel) remain
constrained to manually inspect the entire text
of reviews in order to ﬁnd out the reasons other
users argued in that respect.
Methods for automatic analysis of the aspect-oriented sentiment
expressed in reviews would enable highlighting
aspect-relevant parts of the document, so as to allow users to perform a faster and focused inspection of them.
Previous work on opinion mining has already faced the overall sentiment
prediction , multiple aspectoriented analysis , and ﬁnegrained phrase-level analysis .
Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one
for which more NLP tools and resources are available. worked on the summarization of reviews by means of weakly supervised
feature mining. 
used a ﬁner-grained dataset in which global polarity annotation is applied also to each sentence
composing the document. Similarly did with the Stanford Sentiment Treebank, which annotates each syntactically plausible phrase in thousands of sentences using annotators from Amazon’s Mechanical Turk, annotating the polarity of phrases on a ﬁve-level scale.
 performed a single-label
polarity annotation of elementary discourse units
of TripAdvisor reviews, adopting ten aspect labels.
 did a similar annotation work, using sentences as the annotation elements and adopting a multi-label polarity annotation, i.e., each sentence can be assigned to zero,
one, or more than one aspect.
Cross-lingual sentiment classiﬁcation explores the
scenario in which training data are available for
a language that is different from the language of
the test documents. Cross-lingual learning methods have important practical applications, since
they allow to build classiﬁers for many languages
reusing the training data produced for a single language (typically English), probably giving up a bit
of accuracy, but compensating it with a large save
in terms of human annotation costs.
Multi-lingual datasets are beneﬁcial to the research community both as a benchmark to explore cross-lingual learning and also as resources
on which to develop and test new NLP tools for
languages other than English.
Prettenhofer and
Stein used a multi-lingual dataset focused
on full-document classiﬁcation at the global polarity level. Denecke used a dataset of 200
Amazon reviews in German to test cross-lingual
document polarity classiﬁcation using an English
training set.
Klinger and Cimiano produced a bi-lingual dataset (English and German),
named USAGE, in which aspect expressions and
subjective expressions are annotated in Amazon
product reviews. In 
aspect expressions can be any piece of text that
mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and are not categorical label, as in our dataset. The USAGE dataset is
thus more oriented at information extraction rather
than at text classiﬁcation applications. Banea et al.
 used machine translation to create a multilingual version of the information-extraction oriented MPQA dataset on six
languages (English, Arabic, French, German, Romanian and Spanish).
In this paper we present Trip-MAML, which
extends the Trip-MA1 dataset of Marcheggiani et
al. with Italian and Spanish annotated reviews. We describe Trip-MAML and report experiments aimed at deﬁning a ﬁrst baseline. Both
the dataset and the software used in experiments
are publicly available at 
cnr.it/trip-maml/.
Annotation Process
We recall the annotation process adopted by
Marcheggiani et al. for Trip-MA and the
procedure we employed to extend it into Trip-
MAML. We will use the national codes EN, ES,
1Marcheggiani et al. gave no name to their dataset,
here we name it Trip-MA to identify its source and its multiaspect nature.
and IT, to denote the English, Spanish, and Italian parts of the Trip-MAML dataset, respectively.
Note that EN coincides with Trip-MA.
English Reviews
The Trip-MA dataset was created by Marcheggiani et al. by annotating a set of 442
reviews, written in English, randomly sampled
from the publicly available TripAdvisor dataset of
Wang et al. , composed by 235,793 reviews.
Each review comes with an overall rating on a discrete ordinal scale from 1 to 5 “stars”. The dataset
was annotated according to 9 recurrent aspects frequently involved in hotel reviews: Rooms, Cleanliness, Value, Service, Location, Check-in, Business,
Food, and Building. The last two are not ofﬁcially
rated by TripAdvisor but were added because they
are frequently commented in reviews. Two “catchall” aspects, Other and NotRelated, were also added,
for a total of 11 aspect.
Aspect Other denotes
opinions that are pertinent to the hotel being reviewed, but not relevant to any of the former nine
aspects (e.g., generic evaluations like Pulitzer exceeded our expectations). Aspect NotRelated denotes opinions that are not related to the hotel
(e.g., Tour Eiffel is amazing).
If a sentence is relevant to an aspect, the possible sentiment label values are three: Positive, Negative, and Neutral/Mixed2.
Neutral/Mixed annotates
subjective evaluations that are not clearly polarized (e.g., The hotel was ﬁne with some exceptions).
Annotation protocol
Marcheggiani et al. relied on three human
annotators to annotate each sentence of the 442 reviews with respect to polarities of opinions that are
relevant to any of the 11 aspects. 73 reviews, out
of 442, were independently annotated by all the
annotators in order to measure the inter-annotator
agreement, while the remaining 369 reviews were
partitioned into 3 equally-sized sets, one for each
annotator. Bias in the estimation of inter-annotator
agreement was minimized by sorting the list of reviews of each annotator so that every eighth review was common to all annotators; this ensured
that each annotator had the same amount of coding
experience when labeling the same shared review.
2Marcheggiani et al. initially distinguished between implicit and explicit opinions but the human agreement
was so low they removed this distinction from the schema.
# Sentences
# Opinion-laden sentences
Table 1: Number of reviews, sentences, and sentences with at least one opinion annotation.
Spanish and Italian Reviews
For the creation of ES and IT parts of the Trip-
MAML dataset we followed the same annotation
protocol of Marcheggiani et al. , employing teams of three native speakers as annotators
for each language. We crawled the Spanish and
Italian reviews from TripAdvisor by accessing its
websites with the ‘.es’ and ‘.it’ domains, which
mostly contains reviews in the national language.
From that domains we downloaded the reviews
for the 10 most visited cities in Spain and Italy,
respectively. We downloaded 10 reviews for every hotel of each city, obtaining a total of 17,020
reviews for Spanish and 33,325 for Italian. For
each dataset, 500 reviews were selected by randomly sampling 50 reviews for each city. We thus
obtained 139 unique reviews for each annotator,
plus 83 reviews which all three annotators independently annotated.
We decided to annotate the aspects that were
ratable on TripAdvisor at the time of our crawl
 . Differently from the aspect schema in EN, we included the new aspect
Sleep Quality, and we did not consider the missing aspects Check-in and Businnes, which are, in
any case, the least frequent aspects in the Trip-MA
dataset (see Table 2). We kept the additional aspects Food, Building, Other, and NotRelated, as they
still appear frequently in the reviews. We adopted
the same 3-values sentiment label schema of EN,
i.e., Positive, Negative, or Neutral/Mixed.
Following the same procedure adopted by
Marcheggiani et al. , the Spanish and Italian annotator teams performed a preliminary annotation session on reviews not included in the ﬁnal dataset. This preliminary activity was aimed at
aligning the annotators’ understanding about the
labeling process for the different aspects, by sharing and solving any doubt that might arise during
the annotation of some examples.
Statistics
Table 1 shows that English reviews have, on average, about double the number of sentences of
Spanish and Italian reviews. This can be in part
motivated by observing that the sentences in EN
are, on average, 25% shorter than in ES and IT.
Also, after a manual inspection of the data, we
found that the EN part contains some reviews related to long vacations in resorts, thus describing in longer details the experience, while IT and
ES reviews are mainly related to relatively short
visits to classic hotels. However, the portion of
opinionated sentences is similar across the three
parts, indicating homogeneity in content, which is
conﬁrmed by the detailed aspect-level statistics reported in Table 2.
Both aspect and sentiment labels show imbalanced distributions that follow similar distributions across the three parts.
The most frequent
aspect in all collections is Other, followed by
Rooms, Service, and Location.
Building and Value
are among the least frequent ones. The average
value of the Pearson correlation between the lists
of the shared aspects ranked by their relative frequency, measured pairwise among the three parts,
is 0.795, which indicates a good uniformity of
content among the parts. In all the three parts, Positive is the most frequent sentiment label, followed
by Negative. Location is always the aspect with the
highest frequency of positive labels.
Inter-annotator Agreement
We measured the inter-annotator agreement in two
steps. The F1 score measures the agreement on
aspect identiﬁcation, regardless of the sentiment
label assigned. Then symmetric Macro-averaged
Mean Absolute Error (sMAEM) measures the agreement on sentiment
labels on the annotations for which the annotators
agreed at the aspect level. Aspect NotRelated is not
included in agreement evaluation, nor in the experiments of Section 4. sMAEM is computed between each of the three possible pairs of annotators and then averaged to determine the agreement
values reported in Table 3.
Agreement on aspect detection is higher for ES
and IT than for EN. This difference is in part motivated by the fact that the two aspects that are missing in ES and IT have low agreement on EN, and
the novel Sleep Quality aspect has instead a high
agreement.
However, also on the other aspects
Other Service Rooms Clean. Food Loc. Check-in Sleep-q. Value Build. Busin. NotRelated Total
Table 2: Number of opinion expressions at the sentence level of the datasets.
Service Rooms Clean.
Check-in Sleep-q.
Value Build. Busin.
Table 3: Inter-annotator agreement. F1 on sentence-level aspect identiﬁcation (higher is better). sMAEM
on sentence-level sentiment agreement (only on matching aspects, lower is better).
there is, in general, a higher or equal agreement in
ES and IT with respect to EN, indicating that the
formers two were annotated in a more consistent
way. The agreement on assignment of sentiment
label is rather similar across the whole dataset.
Experiments
The experiments we present here are aimed at
deﬁning a shared baseline for future experiments.
For this reason we chose a relatively simple setup
that uses a simple learning model and minimal linguistic resources. We used a sentence-level Linear
Chain (LC) Conditional Random Field as described by Marcheggiani et al.
 . With respect to the features extracted from
text, we used three simple features types: word
unigrams, bigrams, and SentiWordNet-based features, which consist of a Positive and a Negative
feature extracted every time the review contains
a word that is marked as such in SentiWordNet
 .
To use SentiWord-
Net on ES and IT, we used Multilingual Central Repository and
MultiWordNet to map sentiment labels to Spanish and to Italian, respectively.
Experiments were run separately on the EN, ES,
and IT parts, leaving cross-lingual experiments to
future work. On each part we built ﬁve 70%/30%
train/test splits, randomly generated by sampling
the reviews annotated by single reviewers (we left
out reviews annotated by all the reviewers, as we
consider that part of the dataset more useful as
a validation set for the optimization of methods
tested in future experiments). We then run the ﬁve
experiments and averaged their results.
Evaluation Measures
As for the agreement evaluation (Section 3), we
split the evaluation of experiments into two parts,
aspect detection and sentiment labeling. For the
sentiment labeling part we used simple Macroaveraged Mean Absolute Error (MAEM, not the
symmetric version) as the true dataset labels are
the reference ones in this case, while in the annotator agreement case the two sets of labels have
equal importance.
Experiments on ES and IT obtain better F1 values
than on EN, indicating that the observed higher
human agreement can be also explained by a lower
hardness of the task when working with Spanish
Other Service Rooms Clean. Food Loc. Check-in Sleep-q. Value Build. Busin. Avg
.516 1.002
Table 4: Linear Chain CRFs experiments. F1 on sentence-level aspect identiﬁcation (higher is better).
MAEM on sentence-level sentiment assignment (only on correctly identiﬁed aspects, lower is better).
and Italian.
MAEM values are all similar across languages,
again conﬁrming what has been observed on
agreement. However, MAEM values on experiments are sensibly worse than those measured on
agreement, possibly due to the fact that we used
very basic features, with limited use of sentimentrelated information.
Conclusion
We have presented Trip-MAML a multi-lingual
extension of Trip-MA, originally presented in
 . The extension process involved crawling and selecting the reviews
for the two new languages, Spanish and Italian,
and their annotation by a total of six native language speakers.
We measured dataset statistics
and inter-annotator agreement, which show that
the new ES and IT parts we produced are consistent with the original EN part. We also presented
experiments on the dataset, based on a linear chain
CRFs model for the automatic detection of aspects
and their sentiment labels, establishing a baseline
for future research. Trip-MAML enables the exploration of cross-lingual approaches to the problem of multi-aspect sentiment classiﬁcation.
Acknowledgments
This work has been partially supported by ATTOS
project (TIN2012-38536-C03-0) from the Spanish
Government.