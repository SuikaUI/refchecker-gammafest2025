Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1258–1268
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Improved Zero-shot Neural Machine Translation
via Ignoring Spurious Correlations
Jiatao Gu*
♦, Yong Wang*
†, Kyunghyun Cho
♦‡ and Victor O.K. Li
♦Facebook AI Research
†The University of Hong Kong
‡New York University, CIFAR Azrieli Global Scholar
♦{jgu, kyunghyuncho}@fb.com
†{wangyong, vli}@eee.hku.hk
Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is
an emergent property when training the system
in multilingual settings. However, na¨ıve training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot.
In this work, we address the degeneracy problem due to capturing spurious correlations by
quantitatively analyzing the mutual information between language IDs of the source and
decoded sentences. Inspired by this analysis,
we propose to use two simple but effective approaches: (1) decoder pre-training; (2) backtranslation. These methods show signiﬁcant
improvement (4 ∼22 BLEU points) over the
vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar
or better results than the pivot-based approach.
Introduction
Despite the recent domination of neural networkbased models in the ﬁeld of
machine translation, which have fewer pipelined
components and signiﬁcantly outperform phrasebased systems , Neural Machine Translation (NMT) still works poorly when
the available number of training examples is limited. Research on low-resource languages is drawing increasing attention, and it has been found
promising to train a multilingual NMT model for high- and row-resource languages to deal with low-resource translation . As an extreme in terms of the number of supervised examples, prior works dug into
* Equal contribution.
translation with zero-resource where
the language pairs in interest do not have any parallel corpora between them. In particular, Johnson et al. observed an emergent property of
zero-shot translation where a trained multilingual
NMT model is able to automatically do translation
on unseen language pairs; we refer to this setting
as zero-shot NMT from here on.
In this work, we start with a typical degeneracy issue of zero-shot NMT, reported in several recent works , that zero-shot NMT is sensitive to training conditions, and the translation
quality usually lags behind the pivot-based methods which use a shared language as a bridge for
translation . We ﬁrst quantitatively show that this degeneracy issue of zeroshot NMT is a consequence of capturing spurious correlation in the data. Then, two approaches
are proposed to help the model ignore such correlation: language model pre-training and backtranslation. We extensively evaluate the effectiveness of the proposed strategies on four languages
from Europarl, ﬁve languages from IWSLT and
four languages from MultiUN. Our experiments
demonstrate that the proposed approaches significantly improve the baseline zero-shot NMT performance and outperforms the pivot-based translation in some language pairs by 2∼3 BLEU points.
Background
Given a source sentence x = {x1, ..., xT ′}, a neural machine translation model factorizes the distribution over output sentences y = {y1, ..., yT } into
a product of conditional probabilities:
p(y|x; θ) =
p(yt|y0:t−1, x1:T ′; θ),
where special tokens y0 (⟨bos⟩) and yT+1 (⟨eos⟩)
are used to represent the beginning and the end of
a target sentence. The conditional probability is
parameterized using a neural network, typically,
an encoder-decoder architecture based on either
RNNs , CNNs or the Transformers .
Multilingual NMT
We start with a many-tomany multilingual system similar to Johnson et al.
 which leverages the knowledge from translation between multiple languages. It has an identical model architecture as the single pair translation model, but translates between multiple languages. For a different notation, we use (xi, yj)
where i, j ∈{0, ..., K} to represent a pair of sentences translating from a source language i to a
target language j. K +1 languages are considered
in total. A multilingual model is usually trained by
maximizing the likelihood over training sets Di,j
of all available language pairs S. That is:
|S| · |Di,j|
(xi,yj)∈Di,j,(i,j)∈S
θ(xi, yj), (2)
where we denote Lj
θ(xi, yj) = log p(yj|xi, j; θ).
Speciﬁcally, the target language ID j is given to
the model so that it knows to which language it
translates, and this can be readily implemented by
setting the initial token y0 = j for the target sentence to start with.1 The multilingual NMT model
shares a single representation space across multiple languages, which has been found to facilitate translating low-resource language pairs .
Pivot-based NMT
In practise, it is almost impossible for the training set to contain all K ×
(K+1) combinations of translation pairs to learn a
multilingual model. Often only one (e.g. English)
or a few out of the K + 1 languages have parallel sentence pairs with the remaining languages.
For instance, we may only have parallel pairs between English & French, and Spanish & English,
but not between French & Spanish. What happens
if we evaluate on an unseen direction e.g. Spanish to French? A simple but commonly used solution is pivoting: we ﬁrst translate from Spanish to English, and then from English to French
1 Based on prior works , both
options work similarly. Without loss of generality, we use the
target language ID as the initial token y0 of the decoder.
with two separately trained single-pair models or
a single multilingual model. However, it comes
with two drawbacks: (1) at least 2× higher latency than that of a comparable direct translation
model; (2) the models used in pivot-based translation are not trained taking into account the new
language pair, making it difﬁcult, especially for
the second model, to cope with errors created by
the ﬁrst model.
Zero-shot NMT
Johnson et al. showed
that a trained multilingual NMT system could automatically translate between unseen pairs without any direct supervision, as long as both source
and target languages were included in training. In
other words, a model trained for instance on English & French and Spanish & English is able to
directly translate from Spanish to French. Such
an emergent property of a multilingual system is
called zero-shot translation. It is conjectured that
zero-shot NMT is possible because the optimization encourages different languages to be encoded
into a shared space so that the decoder is detached
from the source languages. As an evidence, Arivazhagan et al. measured the “cosine distance” between the encoder’s pooled outputs of
each sentence pair, and found that the distance decreased during the multilingual training.
Degeneracy Issue of Zero-shot NMT
Despite the nice property of the emergent zeroshot NMT compared to other approaches such as
pivot-based methods, prior works ,
however, have shown that the quality of zero-shot
NMT signiﬁcantly lags behind pivot-based translation. In this section, we investigate an underlying cause behind this particular degeneracy issue.
Zero-shot NMT is Sensitive to Training
Conditions
Preliminary Experiments
Before drawing any
conclusions, we ﬁrst experimented with a variety
of hyper-parameters to train multilingual systems
and evaluated them on zero-shot situations, which
refer to language pairs without parallel resource.
We performed the preliminary experiments on
Europarl2 with the following languages: English
(En), French (Fr), Spanish (Es) and German (De)
with no parallel sentences between any two of Fr,
2 
x1000 steps
Es-Fr (zero-shot)
x1000 steps
Fr-De (zero-shot)
x1000 steps
En-De (parallel)
x1000 steps
Es-En (parallel)
Figure 1: Partial results on zero-shot and parallel directions on Europarl dataset with variant multilingual training conditions (blue: default, red: large-bs, orange: pytorch-init, green: attn-drop, purple:
layerwise-attn). The dashed lines are the pivot-based or direct translation results from baseline models.
Es and De. We used newstest20103 as the validation set which contains all six directions. The
corpus was preprocessed with 40, 000 BPE operations across all the languages. We chose Transformer – the state-of-the-art
NMT architecture on a variety of languages – with
the parameters of dmodel = 512, dhidden = 2048,
nheads = 8, nlayers = 6. Multiple copies of this network were trained on data with all parallel directions for {De,Es,Fr} & En, while we varied other
hyper-parameters. As the baseline, six single-pair
models were trained to produce the pivot results.
The partial results are shown in Fig. 1
including ﬁve out of many conditions on which we
have tested. The default uses the exact Transformer architecture with xavier uniform initialization for all layers, and
is trained with lrmax = 0.005, twarmup = 4000,
dropout = 0.1, nbatch = 2400 tokens/direction.
For the other variants compared to the default
setting, large-bs uses a bigger batch-size of
9,600; attn-drop has an additional dropout
(0.1) on each attention head ;
we use the Pytorch’s default method4 to initialize all the weights for pytorch-init; we also
try to change the conventional architecture with
a layer-wise attention between
the encoder and decoder, and it is denoted as
layerwise-attn. All results are evaluated on
the validation set using greedy decoding.
From Fig. 1, we can observe that the translation
quality of zero-shot NMT is highly sensitive to the
hyper-parameters (e.g. layerwise-attn completely fails on zero-shot pairs) while almost all
the models achieve the same level as the baseline
3 
4We use modules/torch/
nn/modules/linear.html#Linear
does on parallel directions. Also, even with the
stable setting (default), the translation quality
of zero-shot NMT is still far below that of pivotbased translation on some pairs such as Fr-De.
Performance Degeneracy is Due to
Capturing Spurious Correlation
We look into this problem with some quantitative
analysis by re-thinking the multilingual training in
Eq. (4). For convenience, we model the decoder’s
output yj as a combination of two factors: the output language ID z ∈{0, . . . , K}, and languageinvariant semantics s (see Fig. 2 for a graphical
illustration.). In this work, both z and s are unobserved variables before the yj was generated. Note
that z is not necessarily equal to the language id j.
The best practise for zero-shot NMT is to make
z and s conditionally independent given the source
sentence. That is to say, z is controlled by j and
s is controlled by xi. This allows us to change
the target language by setting j to a desired language, and is equivalent to ignoring the correlation between xi and z.
That is, the mutual information between the source language ID i and
the output language ID z – I(i; z) – is 0. However, the conventional multilingual training on an
imbalanced dataset makes zero-shot NMT problematic because the MLE objective will try to capture all possible correlations in the data including
the spurious dependency between i and z.
instance, consider training a multilingual NMT
model for Es as input only with En as the target language. Although it is undesirable for the
model to capture the dependency between i (Es)
and z (En), MLE does not have a mechanism to
prevent it (i.e., I(i; z) > 0) from happening. In
other words, we cannot explicitly control the trade
off between I(i; z) and I(j; z) with MLE training.
When I(i; z) increases as opposed to I(j; z), the
Figure 2: A conceptual illustration of decoupling the
output translation (yj) into two latent factors (language
type and the semantics) where the undesired spurious
correlation (in red) will be wrongly captured if i is always translated to j during training.
decoder ignores j, which makes it impossible for
the trained model to perform zero-shot NMT, as
the decoder cannot output a translation in a language that was not trained before.
Quantitative Analysis
We performed the quantitative analysis on the estimated mutual information I(i; z) as well as the translation quality of
zero-shot translation on the validation set. As an
example, we show the results of large-bs setting in Fig. 3 where the I(i; z) is estimated by:
˜p(z) · ˜p(i)
where the summation is over all possible language
pairs, and ˜p(·) represents frequency. The latent
language identity z = φ(yj) is estimated by an
external language identiﬁcation tool given the actual output . In Fig. 3, the
trend of zero-shot performance is inversely proportional to I(i; z), which indicates that the degeneracy is from the spurious correlation.
The analysis of the mutual information also explains the sensitivity issue of zero-shot NMT during training. As a side effect of learning translation, I(i; z) tends to increase more when the training conditions make MT training easier (e.g. large
batch-size). The performance of zero-shot NMT
becomes more unstable and fails to produce translation in the desired language (j).
Approaches
In this section, we present two existing, however, not investigated in the scenario of zero-shot
NMT approaches – decoder pre-training and backtranslation – to address this degeneracy issue.
x 1000 steps
Zero-shot Average BLEU
Figure 3: The learning curves of the mutual information between input and output language IDs as well as
the averaged BLEU scores of all zero-shot directions
on the validation sets for the large-bs setting.
Language Model Pre-training
The ﬁrst approach strengthens the decoder language model (LM) prior to MT training. Learning the decoder language model increases I(j; z)
which facilitates zero-shot translation. Once the
model captures the correct dependency that guides
the model to output the desired language, it is
more likely for the model to ignore the spurious
correlation during standard NMT training. That
is, we pre-train the decoder as a multilingual language model. Similar to Eq. (4):
|S| · |Di,j|
(xi,yj)∈Di,j,(i,j)∈S
θ(yj) = log p(yj|0, j; θ), which represents that pre-training can be implemented by
simply replacing all the source representations by
zero vectors during standard NMT training . In Transformer, it is equivalent
to ignoring the attention modules between the encoder and decoder.
The proposed LM pre-training can be seen as
a rough approximation of marginalizing all possible source sentences, while empirically we found
it worked well. After a few gradient descent steps,
the pre-trained model continues with MT training.
In this work, we only consider using the same parallel data for pre-training. We summarize the pros
and cons as follows:
Efﬁcient (a few LM training steps + NMT
training); no additional data needed;
The LM pre-training objective does not
necessarily align with the NMT objective.
Back-Translation
In order to apply language model training along
with the NMT objective, we have to take the encoder into account. We use back-translation , but in particular for multilingual training. Unlike the original purpose of
using BT for semi-supervised learning, we utilize
BT to generate synthetic parallel sentences for all
zero-shot directions , and train
the multilingual model from scratch on the merged
datasets of both real and synthetic sentences. By
doing so, every language is forced to translate to
all the other languages.
Thus, I(i; z) is effectively close to 0 from the beginning, preventing
the model from capturing the spurious correlation
between i and z.
Generating the synthetic corpus requires at least
a reasonable starting point that translates on zeroshot pairs which can be chosen either through
a pivot language (denoted as BTTP) or the current zero-shot NMT trained without BT (denoted
BTZS). For instance, in previous examples, to
generate synthetic pairs for Es-Fr given the training set of En-Fr, BTTP translates every En sentence to Es with a pre-trained En-Es model (used
in pivot-based MT), while BTZS uses the pretrained zero-shot NMT to directly translate all Fr
sentences to Es. Next, we pair the generated sentences in the reverse direction Es-Fr and merge
them to the training set. The same multilingual
training is applied after creating synthetic corpus
for all translation pairs. Similar methods have also
been explored by Firat et al. ; Zheng et al.
 ; Sestorain et al. , but have not been
studied or used in the context of zero-shot NMT.
BT explicitly avoids the spurious correlation. Also, BTZS potentially improves further
by utilizing the zero-shot NMT model augmented
with LM pre-training.
BT is computationally more expensive as
we need to create synthetic parallel corpora for all
language pairs (up to O(K2)) to train a multilingual model for K languages; both the performance
of BTTP and BTZS might be affected by the quality of the pre-trained models.
Experiments
Experimental Settings
We extensively evaluate the proposed
approaches (LM, BTTP, BTZS) on three mul-
parallel pairs
Es-En, De-En, Fr-En
Europarl-b
Es-En, Fr-De
De-En, It-En, Nl-En, Ro-En
De-En, En-It, It-Ro, Ro-Nl
Ar-En, Ru-En, Zh-En
Table 1: Overall dataset statistics where each pair has
a similar number of examples shown in the rightmost
column (we sub-sampled 2M sentences per language
pair for MultiUN). All the remaining directions are
used to evaluate the performance of zero-shot NMT.
tilingual datasets across a variety of languages:
Europarl, IWSLT5 and MultiUN.6 The detailed
statistics of the training set are in Table 1, where
we simulate the zero-shot settings by only allowing parallel sentences from/to English. With
IWSLT, we also simulate the scenario of having
a chain of pivot languages (IWSLT-b). Also, another additional dataset (Europarl-b) is included
where the zero-shot pairs have neither direct nor
pivot parallel sentences (similar to unsupervised
translation). In such cases, we expect pivot-based
methods (including the proposed BTTP) are not
applicable. We use the standard validation and test
sets to report the zero-shot performance. Besides,
we preprocess all the datasets following the protocol used in the preliminary experiments.
Training Conditions
For all non-IWSLT experiments, we use the same architecture as the preliminary experiments with the training conditions
of default, which is the most stable setting for
zero-shot NMT in Sec. 3.1. Since IWSLT is much
smaller compared to the other two datasets, we
ﬁnd that the same hyper-parameters except with
twarmup = 8000, dropout = 0.2 works better.
As the baseline, two pivot-based translation are considered:
• PIV-S (through two single-pair NMT models
trained on each pair;)
• PIV-M (through a single multilingual NMT
model trained on all available directions;)
Moreover, we directly use the multilingual system
that produce PIV-M results for the vanilla zeroshot NMT baseline.
5 
6 
As described in Sec. 4, both the LM pre-training
and BT use the same datasets as that in MT training. By default, we take the checkpoint of 20, 000
steps LM pre-training to initialize the NMT model
as our preliminary exploration implied that further increasing the pre-training steps would not be
helpful for zero-shot NMT. For BTTP, we choose
either PIV-S or PIV-M to generate the synthetic
corpus based on the average BLEU scores on parallel data. On the other hand, we always select
the best zero-shot model with LM pre-training for
BTZS by assuming that pre-training consistently
improves the translation quality of zero-shot NMT.
Model Selection for Zero-shot NMT
In principle, zero-shot translation assumes we cannot access any parallel resource for the zero-shot
pairs during training, including cross-validation
for selecting the best model. However, according
to Fig. 1, the performance of zero-shot NMT tends
to drop while the parallel directions are still improving which indicates that simply selecting the
best model based on the validation set of parallel
directions is sub-optimal for zero-shot pairs. In
this work, we propose to select the best model by
maximizing the likelihood over all available validation set ˆDi,j of parallel directions together with
a language model score from a fully trained language model θ′ (Eq. (4)). That is,
(xi,yj)∈ˆDi,j
θ(xi, yj) +
where ˆyk is the greedy decoding output generated
from the current model p(·|xi, k; θ) by forcing it to
translate xi to language k that has no parallel data
with i during training. The ﬁrst term measures
the learning progress of machine translation, and
the second term shows the level of degeneracy in
zero-shot NMT. Therefore, when the spurious correlation between the input and decoded languages
is wrongly captured by the model, the desired language model score will decrease accordingly.
Results and Analysis
Overall Performance Comparison
the translation quality of zero-shot NMT on the
three datasets in Table 2.
All the results (including pivot-based approaches) are generated using beam-search with beam size = 4 and length
x1000 steps
Figure 4: Learning curves of the two proposed approaches (LM, BTZS) and the vanilla ZS on Europarl
Fr→De with two conditions (default, large-bs).
The red dashed line is the pivot-based baseline.
penalty α = 0.6 . Experimental results in Table 2 demonstrate that both
our proposed approaches achieve signiﬁcant improvement in zero-shot translation for both directions in all the language pairs. Only with LM pretraining, the zero-shot NMT has already closed
the gap between the performance and that of the
strong pivot-based baseline for datasets. For pairs
which are lexically more similar compared to the
pivot language (e.g.
Es-Fr v.s.
En), ZS+LM
achieved much better performance than its pivotbased counterpart. Depending on which languages
we consider, zero-shot NMT with the help of
BTTP & BTZS can achieve a signiﬁcant improvement around 4 ∼22 BLEU points compared to the
na¨ıve approach. For a fair comparison, we also
re-implement the alignment method proposed by
Arivazhagan et al. based on cosine distance
and the results are shown as ZS+Align in Table. 2,
which is on average 1.5 BLEU points lower than
our proposed ZS+LM approach indicating that our
approaches might ﬁx the degeneracy issue better.
As a reference of upper bound, we also include
the results with a fully supervised setting, where
all the language pairs are provided for training.
Table 2 shows that the proposed BTTP & BTZS
are competitive and even very close to this upper bound, and BTZS is often slightly better than
BTTP across different languages.
We conduct experiments on the setting without available pivot languages. Shown in
Table 2(b), our training sets only contain Es-En
and De-Fr.
Then if we want to translate from
De to Fr, pivot-based methods will not work.
However, we can still perform zero-shot NMT
by simply training a multilingual model on the
(a) De, Es, Fr ↔En
(b) Es ↔En, Fr ↔De
– not applicable –
– not applicable –
ZS+Align 
– not applicable –
(c) De, It, Nl, Ro ↔En
(d) De ↔En ↔It ↔Ro ↔Nl
(e) Ar, Ru, Zh ↔En
Table 2: Overall BLEU scores including parallel and zero-shot directions on the test sets of three multilingual
datasets. In (a) (c) (e), En is used as the pivot-language; no language is available as the pivot for (b); we also
present partial results in (d) where a chain of pivot languages are used. For all columns, the highest two scores are
marked in bold for all models except for the fully-supervised “upper bound”.
merged dataset. As shown in Table 2(a) and (b),
although the setting of no pivot pairs performs
slightly worse than that with pivot languages, both
our approaches (LM, BTZS) substantially improve
the vanilla model and achieve competitive performance compared to the fully supervised setting.
A Chain of Pivots
We analyze the case where
two languages are connected by a chain of pivot
languages. As shown in Table 1(IWSLT-b), we
used IWSLT which contains pairs for De-En, En-
It, It-Ro, Ro-Nl. If we translate from De to Nl
with pivot-based translation, pivoting from a chain
of languages (De-En-It-Ro-Nl) is required, which
suffers from computational inefﬁciency and error
accumulation. In such cases, however, zero-shot
NMT is able to directly translate between any two
languages. Table 2(d) shows that the performance
of pivot-based methods dramatically degrades as
the length of the chain increases, while ZS does
not have this degradation and still achieves large
gains compared to the pivot-based translation.
Robustness Analysis
From Fig. 4, we show the
learning curves of zero-shot NMT with and without our proposed methods. Both the models with
LM pre-training and BTZS show robustness in two
conditions and achieve competitive and even better results than the pivot-based translation, while
the vanilla model is unstable and completely fails
менее значительные изменения в инде@@ кс@@ е развития человеческого потенциала еще больше сни@@ з@@ или
довер@@ ие к нему и веду@@ щую роль , которую могли играть доклады о развитии человеческого потенциала в
оценке уровня развития человека .
Ոᔄ ݎ઀ ೰හ ጱ ੜ@@ ଏ ץᦈ ᬰӞྍ ഖਸ਼ ԧ Ոᔄ ݎ઀ ಸޞ ੒ ᤍᰁ Ոᔄ ݎ઀ ጱ מ@@ ᥿@@ ଶ ޾ ᶾ੕@@ ێ ̶
ńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńńń
ࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁጱጱጱጱ҅ࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁࣁጱጱጱጱጱ̶
ࣁᬯොᶎ҅౯ժ୩᧣ԧࣁݎ઀ᓉᩒᳯ᷌ࢵᴬտᦓӤݐ஑ጱᬰ઀҅ଚ୩᧣ԧࣁݎ઀ᓉᩒᳯ᷌ࢵᴬտᦓӤݐ஑ጱ
ᬰ઀҅۱ೡࣁݎ઀ᓉᩒᳯ᷌Ӥݐ஑ԧᬰ઀̶
ࣁՈᔄݎ઀೰හӾ҅ๅ᯿ᥝጱݎ઀ԞๅےӸ᯿҅Ԟๅےᚈ୧҅Ԟฎࣁᦧ֌Ոᔄݎ઀ጱఘ٭ӥ҅ݢݎഀ᯿ᥝ
ࣁݎ઀ӾጱՈᔄݎ઀ොᶎ҅ݢզݎഀๅय़ጱ֢አ҅ଚֵٌᚆड़ࣁٌಸޞӾݎഀๅय़ጱ֢አ҅ଚֵٌᚆड़ࣁ
ݎ઀Ӿጱᚆێୌᦡ̶
Ոᔄݎ઀LQGH[ጱ᯿य़ݒ۸ԞᬰӞྍᴳ֗ԧ੒Ոᔄݎ઀ጱᚆێ҅ଚݎഀԧ᯿ᥝ֢አ҅ݢࣁՈᔄݎ઀ᦧ֌ොᶎ൉
׀KXPDQGHYHORSPHQWಸޞ̶
Ոᔄݎ઀೰හጱፘ੒᫾ੜጱݒ۸ᬰӞྍٺ੝ԧਙጱמஞ҅ଚֵਙᚆड़ݎഀՈᔄݎ઀ಸޞጱԆ##੕##֢አ̶
OHVVVLJQLƉFDQWFKDQJHVLQWKHKXPDQGHYHORSPHQWLQGH[KDYHIXUWKHUUHGXFHGLWVFUHGLELOLW\DQGOHDGHUVKLSUROHLQKXPDQ
GHYHORSPHQWDVVHVVPHQWUHSRUWV
Ոᔄݎ઀೰හጱ᫾ੜݒ۸ᬰӞྍٺ੝ԧՈժ੒Ոᔄݎ઀ጱמձ҅ଚݎഀԧ੒Ոᔄݎ઀࿜ଘጱᦧհ֢አ̶
step 15000
step 31000
step 62000
Figure 5: Zero-shot translation performance on Ru →Zh from MultiUN dataset. (↑) An example randomly
selected from the validation set, is translated by both the vanilla zero-shot NMT and that with LM pre-training
at four checkpoints. Translation in an incorrect language (English) is marked in pink color. (←) We showed the
two learning curves for the averaged zero-shot BLEU scores on validation set of Multi-UN with the corresponded
checkpoints marked.
after a small number of iterations on large-bs.
Case Study
We also show a randomly selected
example for Ru →Zh from the validation set of
MultiUN dataset in Fig. 5. We can see that at the
beginning, the output sentence of ZS+LM is ﬂuent while ZS learns translation faster than ZS+LM.
Then, En tokens starts to appear in the output sentence of ZS, and it totally shifts to En eventually.
Related Works
Zero-shot Neural Machine Translation
Zeroshot NMT has received increasingly more interest
in recent years. Platanios et al. introduced
the contextual parameter generator, which generated the parameters of the system and performed
zero-shot translation. Arivazhagan et al. 
conjectured the solution towards the degeneracy in
zero-shot NMT was to guide an NMT encoder to
learn language agnostic representations. Sestorain
et al. combined dual learning to improve
zero-shot NMT. However, unlike our work, none
of these prior works performed quantitative investigation of the underlying cause.
Zero Resource Translation
This work is also
closely related to zero-resource translation which
is a general task to translate between languages
without parallel resources. Possible solutions include pivot-based translation, multilingual or unsupervised NMT. For instance, there have been attempts to train a single-pair model with a pivotlanguage or
a pivot-image .
Unsupervised Translation
Unlike the focus of
this work, unsupervised translation usually refers
to a zero-resource problem where many monolingual corpora are available. Lample et al. ;
Artetxe et al. proposed to enforce a shared
latent space to improve unsupervised translation
quality which was shown not necessary by Lample
et al. in which a more effective initialization method for related languages was proposed.
Neural Machine Translation Pre-training
a standard transfer learning approach, pre-training
signiﬁcantly improves the translation quality of
low resource languages by ﬁne-tuning the parameters trained on high-resource languages . Our proposed LM pre-training can
also be included in the same scope while following a different motivation.
Conclusion
In this paper, we analyzed the issue of zero-shot
translation quantitatively and successfully close
the gap of the performance of between zero-shot
translation and pivot-based zero-resource translation. We proposed two simple and effective strategies for zero-shot translation. Experiments on the
Europarl, IWSLT and MultiUN corpora show that
our proposed methods signiﬁcantly improve the
vanilla zero-shot NMT and consistently outperform the pivot-based methods.
Acknowledgement
This research was supported in part by the Facebook Low Resource Neural Machine Translation
Award. This work was also partly supported by
Samsung Advanced Institute of Technology (Next
Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving
Deep Learning using Latent Structure). KC thanks
support by eBay, TenCent, NVIDIA and CIFAR.