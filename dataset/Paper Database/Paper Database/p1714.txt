On the determination of probability density
functions by using Neural Networks
Llu´ıs Garrido1,2, Aurelio Juste2
1) Dept. d’Estructura i Constituents de la Mat`eria,
Facultat de F´ısica, Universitat de Barcelona,
Diagonal 647, E-08028 Barcelona, Spain.
Phone: +34 93 402 11 91
Fax: +34 93 402 11 98
e-mail: 
2) Institut de F´ısica d’Altes Energies,
Universitat Aut`onoma de Barcelona,
E-08193 Bellaterra (Barcelona), Spain.
Phone: +34 93 581 28 34
Fax: +34 93 581 19 38
e-mail: 
It is well known that the output of a Neural Network trained to disentangle between two classes has a probabilistic interpretation in terms of the
a-posteriori Bayesian probability, provided that a unary representation is
taken for the output patterns. This fact is used to make Neural Networks
approximate probability density functions from examples in an unbinned
way, giving a better performace than “standard binned procedures”. In
addition, the mapped p.d.f. has an analytical expression.
PACS’96: 02.50.Ph, 07.05.Kf, 07.05.Mh
(Submitted to Comput. Phys. Commun.)
Introduction
Estimating a probability density function (p.d.f.) in a n-dimensional space is
a necessity which one may easily encounter in Physics and other ﬁelds. The
standard procedure is to bin the space and approximate the p.d.f. by the ratio
between the number of events falling inside each bin over the total and normalised to the bin volume.
The fact of binning not only leads to a loss of
information (which might be important unless the function is smoothly varying
inside each bin) but is intrinsically arbitrary: no strong arguments for a deﬁned
binning strategy, e.g. constant bin size versus constant density per bin, exists.
More sophisticated approaches imply for instance the deﬁnition of an “intelligent” binning, with smaller bins in the regions of rapid function variation.
However, the main drawback still remains: even for a low number of bins per
dimension, large amounts of data are necessary since the number of data points
needed to ﬁll the bins with enough statistical signiﬁcance grows exponentially
with the number of variables.
As it will be shown, Neural Networks (NN)
turn out to be useful tools for building up analytical n-dimensional probability
density functions in an unbinned way from examples.
This manuscript is organised as follows: in Sect. 2 the proposed method to
construct unbinned p.d.f.s from examples is described. After a brief introduction to the statistical interpretation of the output of a Neural Network applied
to pattern recognition in the case of only two classes, an expression for the
mapped p.d.f. is obtained. Then, a method to quantify the goodness of the
mapped p.d.f. is described. In order to illustrate the concept, an artiﬁcial example is discussed in Sect. 3, whereas Sect. 4 is devoted to the discussion of
an example of practical application in High Energy Physics. Finally, in Sect.
5, the conclusions are given.
Let us assume that we have a sample of N events distributed among 2 diﬀerent
classes of patterns (C1 and C2), each event e being characterised by a set of
n variables x(e). Each class of patterns has a proportion αi and is generated
by the normalised probability density function Pi(x), i = 1, 2 (in probability
terms, Pi(x) = P(x | Ci) and αi = P(Ci)).
By minimising over this sample the quadratic output-error E:
o(x(e)) −d(x(e))
with respect to the unconstrained function o(x), where d(x) takes the value
1 for the events belonging to class C1 and 0 for the events belonging to class
C2, it can be shown that the minimum is achieved when o(x) is the
a-posteriori Bayesian probability to belong to class C1:
o(min)(x) = P(C1 | x).
The above procedure is usually done by using layered feed-forward Neural
Networks (see e.g. for an introduction). In this paper we have considered
Neural Networks with topologies Ni × Nh1 × Nh2 × No, where Ni (No = 1) are
the number of input (ouput) neurons and Nh1, Nh2 are the number of neurons
in two hidden layers.
The input of neuron i in layer ℓis given by,
ℓ= 2, 3, 4
where x(e)
is the set of n variables describing a physical event e, the sum is
extended over the neurons of the preceding layer (ℓ−1), Sℓ−1
is the state of
neuron j at layer (ℓ−1) and Bℓ
i is a bias input to neuron i at layer ℓ. The
state of a neuron is a function of its input Sℓ
j), where F is the neuron
response function. In general the “sigmoid function”, F(Iℓ
j) = 1/(1 + e−Iℓ
chosen since it oﬀers a more sensitive modeling of real data than a linear one,
being able to handle existing non-linear correlations. However, depending on
the particular problem faced, a diﬀerent neuron response function may be more
convenient. For instance, in the artiﬁcial example described below, a sinusoidal
neuron response function, F(Iℓ
j) = (1 + sin(Iℓ
j))/2, has been adopted.
Back-propagation is used as the learning algorithm. Its main objective is to minimise the above quadratic output-error E by adjusting the wij
and Bi parameters.
Let us now consider the situation we are concerned in this paper: we have
a large amount of events (“data”) distributed according to the p.d.f. Pdata(x),
whose analytical expression is unknown and which we want precisely to approximate. If a Neural Network is trained to disentangle between those events and
other ones generated according to any kwown p.d.f., Pref(x) (not vanishing in
a region where Pdata(x) is non-zero), the Neural Network output will approximate, after training, the conditional probability for a given event to be of the
“data” type:
o(min)(x) ≃P(data | x) ≡
αdataPdata(x)
αdataPdata(x) + αrefPref(x),
where αdata and αref are the proportions of each class of events used for training,
satisfying αdata + αref = 1.
From the above expression it is straightforward to extract the NN approximation to Pdata(x) as given by:
data (x) = Pref(x) αref
1 −o(min)(x).
As a result, the desired p.d.f.
is determined in an unbinned way from
examples. In addition, P(NN)
data (x) has an analytical expression since we indeed
have it for Pref(x) and o(min)(x) is known once we have determined the network
parameters (weights and bias inputs).
For what the reference p.d.f. is concerned, a good choice would be a p.d.f.
built from the product of normalised good approximations to each 1-dimensional
projection of the data p.d.f., thus making easier the learning of the existing
correlations in the n-dimensional space. Since Pref(x) is a normalised p.d.f.
by construction, the normalisation of P(NN)
data (x) will depend on the goodness
of the Neural Network approximation to the conditional probability, so that
in general it must be normalised a-posteriori. In the artiﬁcial (High Energy
Physics) example shown below, the normalisation of the obtained p.d.f.s was
consistent with 1 at the 1% (3%) level.
On the other hand, one would like to test the goodness of the approximation
of the mapped p.d.f. to the true one. Given a data sample containing Ndata
events, it is possible to perform a test of the hypothesis of the data sample under
consideration being consistent with coming from the mapped p.d.f. For that,
one can compute the distribution of some test statistics like the log-likelihood
function of Eq.(2.6), which can be obtained by generating Monte Carlo samples
containing Ndata events generated using the mapped p.d.f.
L = log(L) =
data (x(e)))
Being Ldata the value of the log-likelihood for the original data sample, the
conﬁdence level (CL) associated to the hypothesis of the data sample coming
from the mapped p.d.f. is given by:
which in practice can be obtained as the fraction of generated Monte Carlo
samples of the data size having a value of the log-likelihood equal or below the
one for the data sample. If the mapped p.d.f. is a good approximation to Pdata,
the expected distribution for CL evaluated for diﬀerent data samples should
have a ﬂat distribution as it corresponds to a cumulative distribution.
Artiﬁcial example
In this section we propose a purely artiﬁcial example in order to illustrate how a
Neural Network can perform a mapping of a 5-dimensional p.d.f. in an unbinned
way from examples.
In this example our ”data” will consist in a sample of 100000 events generated in the cube [0, π]5 ∈R5 according to the following p.d.f.:
Pdata(x) = 1
C (sin(x1 + x2 + x3) + 1)
which we want to estimate from the generated events. In the above expression,
C is a normalisation factor such that Pdata(x) has unit integral. The above
p.d.f. has a rather intrincate structure of maxima and minima in both, the
3-dimensional space of the ﬁrst three variables and the 2-dimensional space of
the two last variables.
In order to map the above p.d.f., we need to train a Neural Network to disentangle between events generated according to Pdata(x) and events generated
according to any Pref(x) non-vanishing in any region where Pdata(x) is diﬀerent
from zero. In order to make easier the learning of the existing correlations in
the 5-dimensional space, as explained before, Pref(x) is chosen as the product
of good approximations to the 1-dimensional projections of Pdata(x), properly
normalised to have unit integral.
In the case of data p.d.f., it turns out that the 1-dimensional projections of
the three ﬁrst variables are equal and essentially ﬂat, whereas the 1-dimensional
projections for the two last variables can be parametrised as a 4th degree polinomial (P4). Therefore, we choose as reference p.d.f.:
Pref(x) = 1
C′ P4(x4) · P4(x5)
and generate a number of 100000 events according to it. As before, C′ is a
normalisation factor so that Pref(x) has unit integral.
After the training and normalisation, the p.d.f. given by Eq.(2.5) constitutes
a reasonably good approximation to Pdata(x), as it is indeed observed in Fig. 1,
where both are compared for diﬀerent slices in the 5-dimensional space with
respect to the variable x1. For comparison, it is also shown the reference p.d.f.
which, as expected, is unable to reproduce the complicated structure of maxima
and minima in the 5-dimensional space.
As explained in previous section, it is posible to perform a test of the goodness of the mapped p.d.f. For that, a number of 10000 Monte Carlo samples
have been generated with the mapped p.d.f., each one containing 100000 events,
which is the same number of events of the ”data” sample. The log-likelihood
is computed for each MC sample and its distribution is shown in Fig. 2a), in
which the arrow indicates the value of the log-likelihood for the original data
sample (Ldata). From this distribution and the value of Ldata we have found a
conﬁdence level of 5.5% associated to the hypothesis of the data sample coming
from the mapped p.d.f. This seems a low CL and needs further comments,
but as we know the true p.d.f given by Eq.(2.5), we can do much better than
performing a single measurement for CL and is to ﬁnd out its distribution.
Very often in High Energy Physics and other ﬁelds the problem consist
on estimating a p.d.f. from a sample of simulated Monte Carlo events which
is much larger (typically a factor 100 times larger) than the experimental data
sample over which we should use this p.d.f (see the High Energy Physics example
of Sect. 4). For this reason we have obtained the CL distribution in three
diﬀerent scenarios: when the number of experimental data events (Nexp ) has
the same number of events as the data sample used to obtain the mapped p.d.f.
(Ndata = 100000), and two with smaller statistics, one with Nexp = 10000 and
another with Nexp = 1000.
A number of 10000 Monte Carlo samples have been generated with the
mapped p.d.f., each containing Nexp events, for the three diﬀerent values of
Nexp and the log-likelihood is computed for each sample in all three scenarios.
On the other hand, a number of 1000 data samples are generated with the true
p.d.f. in the three scenarios and the conﬁdence level is computed according
to Eq.(2.7).
The distribution of CL is shown in Fig. 2b) for Nexp = 1000
(dotted line), 10000 (dashed line) and 100000 (solid line). It can be observed
that for Nexp = 1000 the distribution of CL is to a good approximation a
ﬂat distribution whereas for Nexp = 10000 it starts deviating from being ﬂat,
which indicates that the statistics of the data sample is high enough to start
“detecting” systematic deviations in the mapped p.d.f. with respect to the true
In the case of Nexp = 1000 which, as mentioned above illustrates a common
situation in High Energy Physics, the mapped p.d.f. turns out to be a good
enough approximation when used for the smaller experimental data sample. In
the other extreme, Nexp = 100000, which illustrates the situation in which there
is a unique data sample from which one wants to estimate the underlying p.d.f.,
it can be observed in Fig. 2b) (solid line) the existence of enough resolution to
detect systematic deviations in the mapped p.d.f. with respect to the true one.
It should be stressed the very complicated structure of the true p.d.f., which
makes extremely diﬃcult its accurate mapping and nevertheless the diﬀerence
between both distributions are the ones observed in Fig. 1 between the solid
and the dashed lines. In such situations we can not use the mapped p.d.f. for
ﬁne probability studies but it is clear that it is still very useful for other kind
of studies like classiﬁcation or discrimination.
High Energy Physics example
In order to illustrate the practical interest of p.d.f. mapping, the following High
Energy Physics example is considered.
One of the major goals of LEP200 is the precise measurement of the mass
of the W boson. At energies above the WW production threshold (√s > 161
GeV) W bosons are produced in pairs and with suﬃcient boost to allow a
competitive measurement of the W mass by direct reconstruction of its product
decays. Almost half of the times (45.6%) both W bosons decay hadronically,
so that four jets of particles are observed in the ﬁnal state.
Most of the information about the W mass is contained in the reconstructed
di-jet invariant mass distribution, so that MW can be estimated by performing a likelihood ﬁt to this 2-dimensional distribution. Therefore, the W mass
estimator, ˆ
MW , is obtained by maximising the log-likelihood function:
log P(s′(e)
with respect to MW , where P(s′(e)
| MW) represents the probability of
event e, characterised by the two measured invariant masses (s′(e)
MW which, accounting for the existing background, can be expressed as:
2 | MW ) = ρwwPww(s′
2 | MW) + (1 −ρww)Pbckg(s′
In the above expression ρww is the expected signal purity in the sample and
Pww and Pbckg are respectively the p.d.f. for signal (W-pair production) and
background in terms of the reconstructed di-jet invariant masses. For a typical
selection procedure above threshold at LEP200, signal eﬃciencies in excess of
80% with a purity at the level 80% can be obtained in the fully hadronic decay
Therefore, in order to determine MW , we need to obtain both p.d.f.s, for
signal and background, in terms of the reconstructed di-jet invariant masses.
At √s = 172 GeV and after selection, most of the background comes from
QCD. To map the p.d.f. for the background, a 2-5-2-1 Neural Network was
trained with ∼6000 selected q¯q Monte Carlo events generated with full detector
simulation (“data”) and the same number of “reference” Monte Carlo events
generated according to the 1-dimensional projections of the “data” sample.
As far as the signal p.d.f. is concerned, it depends on the parameter we
want to estimate: MW.
It can be obtained by a folding procedure of the
theoretical prediction for the 3-fold diﬀerential cross-section in terms of the 2
di-quark invariant masses (s1 and s2) and x (the fraction of energy radiated in
the form of initial state photons), with a transfer function T, which accounts
for distortions in the kinematics of the signal events due to fragmentation,
detector resolution eﬀects and biases in the reconstruction procedure.
transfer function represents the conditional probability of the reconstructed
invariant masses given some invariant masses at the parton level and initial
state radiation (ISR). The ISR is most of the times lost along the beam pipe
and therefore unknown, reason for which it must be integrated over.
conditional probability is given by:
2 | s1, s2, x) = f(s′
2, s1, s2, x)
g(s1, s2, x)
i stands for each reconstructed invariant mass and g(s1, s2, x) is theoretically known and has a compact expression, reason for which there is no need
to map it.
Then, the goal is to map the 5-dimensional p.d.f. f(s′
2, s1, s2, x). To do it,
a 5-11-5-1 Neural Network was trained with 40000 hadronic WW Monte Carlo
events generated with full detector simulation (“data”) and the same number
of “reference” events generated according to the 1-dimensional projections of
the “data” sample.
In order to test that the event-by-event p.d.f. is meaningful, the predicted
1-dimensional projection of the average invariant mass distribution is compared
to Monte Carlo in Figs. 3a) and b) for both signal and background by using the
obtained Pww and Pbckg, respectively. Note the overall good agreement between
the distributions.
The unbiasedness of the obtained estimator is checked by computing the
calibration curve with respect the true parameter by performing a large number
of ﬁts to Monte Carlo samples generated with diﬀerent values of MW .
The performance of the NN in mapping a n-dimensional p.d.f. has been
compared to the “box method” , a standard procedure to build up binned
p.d.f.s. In the case of the background p.d.f., which is only 2-dimensional, the
“box method” yielded reasonable results as shown in Fig. 3b), while in the
case of the 5-dimensional p.d.f. it showed strong limitations which made impossible its application. The main reason is the time required to compute the
ﬁnal p.d.f which needs an integration on top of the adjustement of the “box
method” parameters (initial box size, minimum number of MC points inside
each box, etc) in a space of high dimensionality and limited statistics. Is in this
environment where the mapping of p.d.f.s by means of NNs may be superior to
“standard binned procedures” in terms of accuracy (the p.d.f. is determined in
an unbinned way from examples) and speed (the resulting p.d.f. is an analytic
function).
Conclusions
We have shown that Neural Networks are useful tools for building up n-dimensional p.d.f.s from examples in an unbinned way. The method takes advantage
of the interpretation of the Neural Network output, after training, in terms
of a-posteriori Bayesian probability when a unary representation is taken for
the output patterns. A purely artiﬁcial example and an example from High
Energy Physics, in which the mapped p.d.f.s are used to determine a parameter
through a maximum likelihood ﬁt, have also been discussed. In a situation of
high dimensionality of the space to be mapped and limited available statistics,
the method is superior to “standard binned procedures”.
Acknowledgements
This research has been partly supported by CICYT under contract number
AEN97-1697.