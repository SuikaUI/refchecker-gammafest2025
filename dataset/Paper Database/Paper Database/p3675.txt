IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
Incremental Local Outlier Detection for Data Streams
Dragoljub Pokrajac
CIS Dept. and AMRC
Delaware State University
Dover DE 19901
Aleksandar Lazarevic
United Tech. Research Center
411 Silver Lane, MS 129-15
East Hartford, CT 06108, USA
Longin Jan Latecki
CIS Department.
Temple University
Philadelphia, PA 19122
Abstract. Outlier detection has recently become an important
problem in many industrial and financial applications. This
problem is further complicated by the fact that in many cases,
outliers have to be detected from data streams that arrive at an
enormous pace. In this paper, an incremental LOF (Local Outlier
Factor) algorithm, appropriate for detecting outliers in data
streams, is proposed. The proposed incremental LOF algorithm
provides equivalent detection performance as the iterated static
LOF algorithm (applied after insertion of each data record),
while requiring significantly less computational time. In addition,
the incremental LOF algorithm also dynamically updates the
profiles of data points. This is a very important property, since
data profiles may change over time. The paper provides
theoretical evidence that insertion of a new data point as well as
deletion of an old data point influence only limited number of
their closest neighbors and thus the number of updates per such
insertion/deletion does not depend on the total number of points
N in the data set. Our experiments performed on several
simulated and real life data sets have demonstrated that the
proposed incremental LOF algorithm is computationally
efficient, while at the same time very successful in detecting
outliers and changes of distributional behavior in various data
stream applications.
I. INTRODUCTION
Despite the enormous amount of data being collected in
many scientific and commercial applications, particular events
of interests are still quite rare. These rare events, very often
called outliers or anomalies, are defined as events that occur
very infrequently (their frequency ranges from 5% to less than
0.01% depending on the application). Detection of outliers
(rare events) has recently gained a lot of attention in many
domains, ranging from video surveillance and intrusion
detection to fraudulent transactions and direct marketing. For
example, in video surveillance applications, video trajectories
that represent suspicious and/or unlawful activities (e.g.
identification of traffic violators on the road, detection of
suspicious activities in the vicinity of objects) represent only a
small portion of all video trajectories. Similarly, in the
network intrusion detection domain, the number of cyber
attacks on the network is typically a very small fraction of the
total network traffic. Although outliers (rare events) are by
definition infrequent, in each of these examples, their
importance is quite high compared to other events, making
their detection extremely important.
Data mining techniques developed for this problem are
based on both supervised and unsupervised learning.
Supervised learning methods typically build a prediction
model for rare events based on labeled data (the training set),
and use it to classify each event . The major drawbacks
of supervised data mining techniques include: (1) necessity to
have labeled data, which can be extremely time consuming for
real life applications, and (2) inability to detect new types of
rare events. In contrast, unsupervised learning methods
typically do not require labeled data and detect outliers as data
points that are very different from the normal (majority) data
based on some measure . These methods are typically
called outlier/anomaly detection techniques, and their success
depends on the choice of similarity measures, feature selection
and weighting, etc. They have the advantage of detecting new
types of rare events as deviations from normal behavior, but
on the other hand they suffer from a possible high rate of false
positives, primarily since previously unseen (yet normal) data
can be also recognized as outliers/anomalies.
Very often, data in many rare events applications (e.g.
network traffic monitoring, video surveillance, web usage
logs) arrives continuously at an enormous pace thus posing a
significant challenge to analyze it . In such cases, it is
important to make decisions quickly and accurately. If there is
a sudden or unexpected change in the existing behavior, it is
essential to detect this change as soon as possible. Assume, for
example, there is a computer in the local area network that
uses only limited number of services (e.g., Web traffic, telnet,
ftp) through corresponding ports. All these services
correspond to certain types of behavior in network traffic data.
If the computer suddenly starts to utilize a new service (e.g.,
ssh), this will certainly look like a new type of behavior in
network traffic data. Hence, it will be desirable to detect such
behavior as soon as it appears especially since it may very
often correspond to illegal or intrusive events. Even in the case
when this specific change in behavior is not necessary
intrusive or suspicious, it is very important for a security
analyst to understand the network traffic and to update the
notion of the normal behavior. Further, on-line detection of
unusual behavior and events also plays a significant role in
video and image analysis . Automated identification of
suspicious behavior and objects (e.g., people crossing the
perimeter around protected areas, leaving unattended luggage
at the airport installations, cars driving unusually slow or
unusually fast or with unusual trajectories) based on
information extracted from video streams is currently an
active research area. Other potential applications include
traffic control and surveillance of commercial and residential
buildings. These tasks are characterized by the need for realtime processing (such that any suspicious activity can be
identified prior to making harm to people, facilities and
installations) and by dynamic, non-stationary and often noisy
environment. Hence, there is necessity for incremental outlier
detection that can adapt to novel behavior and provide timely
identification of unusual events.
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
Recently, LOF (Local Outlier Factor) algorithm has been
successfully applied in many domains for outlier detection in a
batch mode . In this paper, we propose a novel
incremental LOF algorithm that is appropriate for detecting
outliers in data streams. The proposed incremental LOF
algorithm is the first incremental outlier detection algorithm to
the best of our knowledge. It provides equivalent detection
performance as the static LOF algorithm, and has O(NlogN)
time complexity, where N is the total number of data points.
The paper shows that insertion of new data points as well as
deletion of obsolete points influence only limited number of
their nearest neighbors and thus insertion/deletion time
complexity per data point does not depend on the total number
of points N. Our experiments performed on several simulated
and real life data sets have demonstrated that the proposed
incremental LOF algorithm can be very successful in detecting
outliers in various data streaming applications.
II. BACKGROUND
Outlier detection techniques can be categorized into
four groups: (1) statistical approaches; (2) distance based
methods; (3) profiling methods; and (4) model-based approaches. In statistical techniques , the data points are
typically modeled using a stochastic distribution, and points
are labeled as outliers depending on their relationship with this
model. Distance based approaches detect outliers by
computing distances among points. Several recently proposed
distance based outlier detection algorithms are based on (1)
computing the full dimensional distances among points using
all the available features or only feature projections ;
and (2) on computing the densities of local neighborhoods . In addition, clustering-based techniques have also been
used to detect outliers either as side products of the clustering
algorithms (points that do not belong to clusters) or as
clusters that are significantly smaller than others . In profiling methods, profiles of normal behavior are built using
techniques
heuristic-based
approaches, and deviations from them are considered as
outliers (e.g., network intrusions). Finally, model-based
approaches usually first characterize the normal behavior
using some predictive models (e.g. replicator neural networks
 or unsupervised support vector machines ), and
then detect outliers as deviations from the learned model.
Initially proposed outlier detection algorithms determine
outliers once all the data records (samples) are present in the
dataset. We refer to these algorithms as static outlier detection
algorithms. In contrast, incremental outlier detection techniques identify outliers as soon as new data record
appears in the dataset. Incremental outlier detection was also
used within more general framework of activity monitoring
 . In addition, Domingos and Hulten proposed broad
requirements that incremental algorithms need to meet, while
Yamanishi and Takeuchi used on-line discounting distributional learning of Gaussian mixture model and scoring
based on the estimated probability density function.
In this study, we use propose an incremental outlier
detection algorithm based on computing the densities of local
neighborhoods. In our previous work , we have
experimented with numerous outlier detection algorithms for
network intrusion identification, and we have concluded that
the local density based outlier detection approach (e.g. LOF)
typically achieved the best prediction performance.
The main idea of the LOF algorithm is to assign to each
data record a degree of being outlier. This degree is called the
local outlier factor (LOF) of a data record. Data records
(points) with high LOF have local densities smaller than their
neighborhood and typically represent stronger outliers, unlike
data points belonging to uniform clusters that usually tend to
have lower LOF values. The algorithm for computing the
LOFs for all data records has the following steps:
1. For each data record q compute k-distance(q) as distance to
the k-th nearest neighbor of q (for definitions, see Section III).
2. Compute reachability distance for each data record q with
respect to data record p as follows:
reach-distk(q,p)= max(d(q,p), k-distance(p)) (1)
where d(q,p) is Euclidean distance from q to p.
3. Compute local reachability density (lrd) of data record q
as inverse of the average reachability distance based on the k
nearest neighbors of the data record q (In original LOF
publication , parameter k was named MinPts).
4. Compute LOF of data record q as ratio of average local
reachability density of q’s k nearest neighbors and local
reachability density of the data record q.
The main advantages of LOF approach over other outlier
detection techniques include:
- It detects outliers with respect to density of their neighboring
data records; not to the global model.
- It is able to detect outliers regardless the data distribution of
normal behavior, since it does not make any assumptions
about the distributions of data records.
In order to fully justify the need for incremental outlier
detection techniques, it is important to understand that
applying static LOF outlier detection algorithms to data streams
would be extremely computationally inefficient and/or very
often may lead to incorrect prediction results. Namely, static
LOF algorithm may be applied to data streams in three ways:
1. “Periodic” LOF. Apply LOF algorithm on the entire data set
periodically (e.g., after every data block of 1000 data
records is inserted, similar to the strategy discussed in )
or after all the data records are inserted. The major problem
of this approach is inability to detect outliers related to the
beginning of new behavior that initially appear within the
inserted block. Fig. 1 illustrates this scenario. Assume that a
new data point dn (red asterisk in Fig. 1a) is added to the
original data distribution (blue dots in Fig. 1a). Initially,
point dn is an outlier since it is distinct from all other data
records. However, when additional data records (red
asterisks in Fig. 1b) start to group around the initial data
record dn, these new points are no longer outliers since they
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
form their own cluster. Ideally, at the time of its insertion,
data record dn should be identified as an outlier .
However, in this “periodic” scenario, the LOF algorithm is
applied when all data records are added to the original data
set and thus already formed the new distribution. Hence, all
the points from the new cluster, including dn, will be
identified as normal behavior! On the other hand, if an
incremental LOF algorithm is applied after every new data
instance is inserted into the data set, not only it is possible to
detect points like dn as outliers but also to detect the moment
in time when this change of behavior occurred.
Fig. 1. Inability of static LOF algorithms to identify change of behavior: a)
Point dn can be correctly identified as outlier by “supervised” LOF algorithm
but not by “periodic” LOF; b) Points belonging to the new distribution are
incorrectly identified as outliers by “supervised” LOF.
2. “Supervised” LOF. Given a training data set D0 at time
interval t0, LOF algorithm is first applied to compute the kdistances, lrd and LOF values for all data records from the
training data set D0. For every time interval, t > t0, when a
new data record dn is inserted into the data set, k-distance,
reachability distances and lrd values are computed for the
new record dn. However, when computing LOF(dn) using
Eq. (3), lrd(dn) is used along with pre-computed lrd values
for the original data set D0. It is apparent that this approach
will result in several problems: (i) Estimated value LOF(dn)
will not be accurate, since it uses pre-computed k-distance,
reach-dist and lrd values; (ii) a new behavior (shown as red
asterisks in Fig. 1b) will always be detected as outlier since
this approach does not update the normal behavior profile;
(iii) masquerading (attempt of hiding within existing
distribution) cannot be identified, since all inserted data
points will always be considered as normal as they belong to
normal distribution (Fig. 2). Namely, assume that a new data
point dn (red square in Fig. 2a) is inserted within existing
data distribution and all new data points start to group
around the point dn (red squares in Fig. 2b) but with much
higher density than the original data distribution.
Apparently, these newly added points will form a cluster of
very high density which is substantially different than the
cluster of the original distribution. “Supervised” LOF
approach considers these points to belong to the original
data distribution, since it is not aware of new data points
forming the dense cluster. On the other hand, incremental
LOF algorithm, after insertion of each new data point,
would identify this phenomenon, since it can take into
account the newly added points, when updating lrd and LOF
values of existing points (that are already in the database).
Fig. 2. Detecting masqueraders (hiding within existing distribution)
3. “Iterated” LOF. Re-apply the static LOF algorithm every
time a new data record dn is inserted into the data set. This
static LOF algorithm does not suffer from aforementioned
problems, but is extremely computationally expensive, since
every time a new point is inserted, the algorithm recomputes
LOF values for all the data points from the data set.
Knowing that time complexity of LOF algorithm is O(n·log
n) , where n is the current number of data records in the
data set, total time complexity for this “iterated” approach,
after insertion of N points, is :
Our proposed incremental LOF algorithm is designed to
provably provide the same prediction performance (detection
rate and false alarm rate) as the “iterated” LOF. It is achieved
by consistently maintaining for all existing points in the
database the same LOF values as the “iterated” LOF
algorithm.
incremental
efficiently addresses the problems mentioned in Fig. 1 and 2,
but has time complexity O(N⋅logN) thus clearly outperforming
the static “iterated” LOF approach. After all N data records are
inserted into the data set, the final result of the incremental
LOF algorithm on N data points is independent of the order of
insertion and equivalent to the “periodic” LOF executed after
all the data records are inserted.
III. METHODOLOGY
When designing incremental LOF algorithm, we have been
motivated by two goals. First, the result of the incremental
algorithm must be equivalent to the result of the “static”
algorithm every time t a new point is inserted into a data set.
Thus, there would not be a difference between applying
incremental LOF and the “periodic” static LOF when all data
records up to time instant t are available. Second, asymptotic
time complexity of incremental LOF algorithm has to be
comparable to the static LOF algorithm. In order to have
feasible incremental algorithm, it is essential that, at any time
moment t, insertion/deletion of the data record results in
limited (preferably small) number of updates of algorithm
parameters. Specifically, the number of updates per each
insertion/deletion must not be dependent on the current
number of records in the dataset; otherwise, the performance
of the incremental LOF algorithm would be Ω(N2) where N is
the final size of the dataset. In this section, we demonstrate
efficient insertion and deletion of records in the incremental
LOF algorithm and provide its exact time complexity analysis.
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
A. Incremental LOF algorithm
The proposed incremental LOF algorithm computes LOF
value for each data record inserted into the data set and instantly
determines whether inserted data record is outlier. In addition,
LOF values for existing data records are updated if needed.
i. Insertion. In the insertion part, the algorithm performs two
steps: a) insertion of new record, when it computes reach-dist,
lrd and LOF values of a new point; b) maintenance, when it
updates k-distances, reach-dist, lrd and LOF values for
affected existing points. Let us first illustrate these steps
through the example of inserting a new data point n into a data
set shown on Fig. 3a. If we assume k = 2, we first need to
compute reachability distances to two nearest neighbors of the
new data point n (data points 4, 6 in Fig. 3a), so that its lrd
value can be computed. As it is shown further in the paper
(Theorem 1), insertion of the point n may decrease the kdistance of certain neighboring points, and it can happen only
to those points that have the new point n in their kneighborhood. Hence, we need to determine all such affected
points, (points 1, 3, 4, 6 have point n in their 2-neighborhood,
see Fig. 3a). According to Eq. (1), when k-distance(p) changes
for a point p, reach-dist(q,p) will be affected only for points q
that are in k-neighborhood of the point p. In our example,
previous 2-neighbors of data point 3 are the data points 2, and
11, so reach-dist(11,3), and reach-dist(2,3) will be updated
(Fig. 3b). According to Eq. (2), lrd value of a point q is
affected if: a) the k-neighborhood of the point q changes or b)
reach-dist from point q to one of its k-neighbors changes. The
2-neighborhood of a point will change only if the new point n
becomes one of its 2-neighbors. Hence, we need to update lrd
on all points to which the point n is now one of their 2neighbors (points 1, 3, 4, 6 in Fig. 3b) and on all points q
where reach-dist(q,p) is updated and p is among 2-nearest
neighbors of q (points 2,5,7 in Fig. 3c). According to Eq. (3),
LOF values of an existing point q should be updated if lrd(q)
is updated (points 1,2,3,4,5,6,7 in Fig. 3d) or lrd(p) of one of
its 2-neighbors p changes (points 8,9,10 In Fig 3d). Note that
LOF value of point 11 is not updated since point 3 (where lrd
is updated) is not among its 2 nearest neighbors.
The general framework for the incremental LOF method is
shown in Fig. 4. As in the static LOF algorithm , we define
k-th nearest neighbor of a record p as a record q from the
dataset S such that for at least k records o’∈S \ {p} it holds
that d(p,o’) ≤ d(p,q), and for at most k-1 records o’∈S \ {p}
holds that d(p,o’) < d(p,q), where d(p,q) denotes Euclidean
distance between data records p and q. We refer d(p,q) as kdistance(p). k nearest neighbors (referred to as kNN(p))
include all points r∈S \ {p} such that d(p,r) ≤ d(p,q). We also
define k reverse nearest neighbors of p ( referred to as kRNN
(p)) as all points q for which p is among their k nearest
neighbors. For a given data record p, kNN(p) and kRNN(p)
can be respectively retrieved by executing nearest-neighbor
and reverse (a.k.a. inverse) nearest neighbor queries on a dataset S. The correctness of the insertion
algorithm is based on the following Theorems 1-4.
Theorem 1. The insertion of point pc affects the k-distance at
points pj that have point pc in their k-nearest neighborhood,
i.e., where pj∈ kRNN(pc). New k-distances of the affected
points pj are updated as follows:
otherwise.
Proof. (sketch). In insertion, k-distance of an existing point pj
changes when a new point enters the k-th nearest
neighborhood of pj, since in this case the k-neighborhood of pj
changes. If a new point pc is the new k-th nearest neighbor of
pj, its distance from pj becomes the new k-distance(pj).
Otherwise, old k-1th neighbor of pj becomes the new k-th
nearest neighbor of pj (see Fig. 5).
Corollary 1. During insertion, k-distance cannot increase,
Theorem 2. Change of k-distance(pj) may affect reach-distk
(pi,pj) for points pi that are k-neighbors of pj. (see Fig 5b).
Proof (sketch). Using (1), ∀ pi d(pi, pj) > k-distance(old)(pj),
⇒ reach-distk
(old) (pi,pj)= d(pi, pj). According to Corollary 1, kdistance(pj) cannot increase, hence if d(pi, pj) > kdistance(old)(pj), reach-distk
(new) (pi,pj) = reach-distk
(old) (pi,pj).  
Theorem 3. lrd value needs to be updated for every record
(denoted with pm in Fig. 4) for which its k-neighborhood
changes or for which reachability distance to one of its kNN
changes. Hence, after each update of reach-distk (pi,pj) we
have to update lrd(pi) if pj is among kNN(pi). Also, lrd is
updated for all points pj whose k-distance was updated.
Proof (sketch). Change of k-neighborhood of pm affects the
scope of the sum in Eq. (2) computed for all k-neighbors of pm.
Change of the reachability distance between pm and some of
its k-nearest neighbors affects corresponding term in the
denominator of Eq. (2).  
Theorem 4. LOF value needs to be updated for all data
records pm which lrd has been updated (since lrd(pm) is a
denominator in Eq. (3)) and for those records that have records
pm in their kNNs. Hence, the set of data records where LOF
needs to be updated (according to (3)) corresponds to union of
records pm and their kRNN.
Proof (sketch). Similar to the proof of Theorem 3, using (3). 
ii. Deletion. In data stream applications it is sometimes
necessary to delete certain data records (e.g., due to their
obsoleteness). Very often, not only a single data record is
deleted from the data set, but the entire data block that may
correspond to particular outdated behavior. Similarly like in an
insertion, upon deleting the block of data records Sdelete, there
is a need to update parameters of the incremental LOF
algorithm.
The general framework for deleting the block of data records
Sdelete from the dataset S is given in Fig. 6. The deletion of each
record pc ∈ Sdelete from dataset S influences the k-distances of
its kRNN. k-neighborhood increases for each data record pj
that is in reverse k-nearest neighborhood of pc. For such
records, k-distance(pj) becomes equal to the distance from pj to
its new k-th nearest neighbor. The reachability distances from
pj’s (k-1) nearest neighbors pi to pj need to be updated.
Observe that the reachability distance from the k-th neighbor
of pj to record pj is already equal to their Euclidean distance
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
Fig. 3. The illustration of the proposed incremental LOF algorithm. a) Insertion of a new point n (red) results in computation of the reachability distance to its two
nearest neighbors 4, 6 (cyan) and to update of 2-distance to reverse nearest neighbors of n (1,3,4,6, yellow). b) Reachability distance reach-dist(q,p) is updated to
all reverse k-neighbors of the point n from their 2-neighbors (blue arrows from q to p). c) lrd is updated for all points where 2-distance is updated and for points
which reachability distance to their 2-neighbor changes (green). d) LOF is updated for points where lrd is updated and for points where lrd of one of their 2neighbors is updated (pink).
Fig. 4. The general framework for insertion of data record and computing its
LOF value in incremental LOF algorithm.
d(pi, pj) and does not need to be updated (Fig. 7). Analog to
insertion, lrd value needs to be updated for all points pj where
k-distance is updated. In addition, lrd value needs to be
updated for points pi such that pi is in kNN of pj and pj is in
kNN of pi. Finally, LOF value is updated for all points pm on
which lrd value is updated as well as on their kRNN. The
correctness of the deletion algorithm can be proven analog to
the correctness of the insertion algorithm.
Incremental LOF_insertion(Dataset S)
• Given: Set S {p1, … ,pN} pi ∈R
D, where D corresponds
to the dimensionality of data records.
• For each data point pc in data set S
 insert(pc)
 Compute kNN(pc)
3-distance(new)(pj)
3-distance(old)(pj )
< 3-distance(old)(pj )
3-distance(old)(pj )
d(pj, pc)≥ 3-distance(old)( pj )
3-distance(new)(pj)
 (∀pj∈ kNN(pc))
compute reach-distk(pc,pj) using Eq. (1);
//Update_neighbors of pc
 Supdate_k_distance =kRNN(pc);
 (∀pj ∈ Supdate_k_distance)
update k-distance(pj) using Eq.(5);
 Supdate_lrd = Supdate_k_distance;
 (∀pj ∈ Supdate_k_distance), (∀pi∈kNN(pj)\{pc})
reach-distk(pi,pj) =k-distance(pj);
if pj ∈ kNN(pi)
Supdate_lrd = Supdate_lrd ∪{pi};
 Supdate_LOF = Supdate_lrd;
Fig. 5. Update of k-nearest neighbor distance upon insertion of a new record
(k=3). a) New record pc is not among 3-nearest neighbors of record pj ⇒ 3distance(pj) does not change; b) New record pc is among 3-nearest neighbors
of pj ⇒ 3-distance(pj) decreases. Cyan dashed lines denote updates of
reachability distances between point pj and two old points.
 (∀pm ∈ Supdate_lrd)
update lrd(pm) using Eq. (2);
Supdate_LOF = Supdate_LOF ∪ kRNN(pm);
 (∀pl ∈ Supdate_LOF)
update LOF(pl) using Eq.(3);
 compute lrd(pc) using Eq.(2);
B. Computational efficiency of the incremental LOF algorithm
 compute LOF(pc) using Eq.(3);
• End //for
To determine time complexity of the proposed incremental
LOF algorithm, it is essential to demonstrate that the number
of affected data records (updates of k-distance, reachability
distances, lrd and LOF values) does not depend on the current
number n of records in the dataset, as stated by Theorems 5-8.
Subsequently, Corollaries 3-5 provide asymptotic time
complexity for the proposed algorithm.
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
Fig. 6. The framework for deletion of data record in incremental LOF method.
Fig. 7. Update of k-nearest neighbor distance upon deletion of record pc (k=3).
a) Prior to deletion, data record pc is among 3-nearest neighbors of record pj;
b) After deletion of pc, 3-distance(pj) increases and reachability distances from
two nearest neighbors of pj (denoted by cyan dashed lines) are updated to 3distance(pj).
Theorem 5. Maximal number F of k reverse nearest
neighbors of a record p is proportional to k, exponentially
proportional to the dimensionality D, and does not depend on
To prove Theorem 5, we will first establish a few definitions
 and prove Lemmas 1, 2 necessary to establish Ddimensional geometry of the space where the considered data
records reside.
Definition 1. The cone C in D-dimensional space with vertex
v and axis l is locus of points x such that the Euclidean
distance of the point x to the vertex d(x,v) is proportional to
the distance d(x’,v) of point’s projection x’ onto l to the vertex
v. A line containing a point x on cone and the origin is called
generatix. When the vertex is at the origin of the coordinate
system (v=O) we refer a cone as centered cone.
Fig. 8. Illustration of lemma 2 in three-dimensional space
Incremental LOF_deletion(Dataset S,Sdelete)
♦ Supdate_k_distance=∅;
♦ (∀pc ∈ Sdelete)
Supdate_k_distance = Supdate_k_distance∪kRNN(pc);
delete(pc); //we can delete pc after finding
// all reverse neighbors
♦ Supdate_k_distance= Supdate_k_distance\Sdelete; //points from Sdelete
may still be present when computing reverse knearest neighbors
♦ (∀pj ∈ Supdate_k_distance)
update k-distance(pj);
= Supdate_k_distance;
update_lrd
♦ (∀pj ∈ Supdate_k_distance) (∀pi∈(k-1)NN(pj))
reach-distk(pi,pj)=k-distance(pj);
if pj ∈ kNN(pi)
Supdate_lrd = Supdate_lrd ∪{pi};
= Supdate_lrd;
update_LOF
♦ (∀pm ∈ Supdate_lrd)
update lrd(pm) using Eq. (2);
Supdate_LOF = Supdate_LOF ∪ kRNN(pm);
♦ (∀pl ∈ Supdate_LOF)
update LOF(pl) using Eq.(3);
Definition 2. A half-axis angle of a cone is an angle
α/2=∠xvx’. It is angle between the axis and any generatrix.
Lemma 1. Coordinates X1, X2,…, XD of a point x on the
centered cone C, where the axis l of the cone is parallel to the
x1 axis of the coordinate system, satisfy:
3-distanceold(pj)
where a is a pre-specified parameter. Half-axis angle of C
satisfies the following condition:
3-distancenew(pj )
Proof. Follows directly from definitions 1 and 2 and the fact
that the length of x’ is X1 when l is parallel to x1.  
Definition 3. All points which coordinates satisfy relation
2 are inside the cone C and comprise
set inside(C).
Definition 4. The ball B(c,R) in D-dimensional space is
locus of points x such that the distance of the point x from a
prespecified point c is smaller or equal R>0.
Lemma 2. Consider cone C with vertex p and half-axis angle
α/2≤300. Consider ball B(p, R), and volume V=B∩C.
(∀p’∈C) d(p’,p)=R ⇒V ⊂ B’(p’,R) (See Fig. 8 for treedimensional illustration).
Proof. (sketch). Without loss of generality, we may assume
that the cone is centered, i.e., that p is at the origin. Let
coordinates of point p’ be X’1, X’2,…, X’D. Consider a point
p’’ symmetric to p’ with respect to the x1 axis. The point p’’
has coordinates X’1, -X’2,…, -X’D. It is easy to observe that
p’’∈C and d(p,p’’)=R. The distance between these two points
. Since points p’, p’’ are
on the sphere with radius R, from Eq. (7) we can obtain
. Therefore, the ball B’(p’,
R) contains the point p’’ antipodal to p’. It can be shown that
B’ also contains any other boundary point of V.  
Definition 5. A frame of cones Ci, i = 1,…,h is defined as a
set of h cones with the common vertex p and common angle
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
α. The frame of cones completely cover the D-dimensional
Euclidean space ED if:
(see Fig. 9).
Lemma 3. The lower bound1 for the number of α = 600
D-dimensional
Proof (sketch). The lower bound of the number of cones in a
frame is equal to the ratio of the area of the hypersphere and
the area of the hyperspherical cap (part of the hypersphere
inside the cone) with angle α. Details are presented in .
Note that hmin depends only on geometry of D-dimensional
space and is independent of the number or the placement of Ddimensional data records.  
The following Definition 6 and Corollary 2 link geometry of
D-dimensional cones to proximity notion in D-dimensional
Definition 6. Let S’ be set of the D-dimensional data records
inside a cone C with vertex p. k-nearest neighbor of point p in
the cone C is a record q from the dataset S’ such that for at
least k records o’∈S’\{p} it holds that d(p,o’) ≤ d(p,q), and for
at most k-1 records o’∈S’\{p} holds that d(p,o’)<d(p,q). We
also define k-distanceC(p) as a distance from record p to its kth nearest neighbor q in the cone. k-nearest neighborhood of p
in cone C, (referred to as kNNc(p)) includes all records r∈S’ \
{p} such that d(p,r) ≤ d(p,q).
Corollary 2. Consider cone C centered at data point p with
α ≤ 600. Let p’ be a data point in cone C .
k-distanceC(p)).
According to the Definition 6, the volume V=B∩C contains
exactly k points other than p. Consider data point p’ such that
d(p,p’) > k-distanceC(p). Consider now ball B’(p’, kdistanceC(p)). According to Lemma 2, this ball has as a subset
the whole volume V that contains total of k + 1 data points
(including p). Hence, k-distanceC(p’)< k-distanceC(p).  
Proof of Theorem 5. Due to Corollary 2 and Definitions 5, 6,
i, i=1,…,h is frame of cones
with vertex p. Hence, due to Lemma 3 and Definition 6,
1 Suboptimal values for h can be obtained by techniques that
construct spherical codes , followed by covering verification
(to ensure that the space around p is completely covered), e.g.,
based on examination of convex hulls facets . Analog to the
problem of optimal spherical codes, the problem of finding the
smallest possible h for arbitrary D is unresolved and is related (but
not equivalent) to sphere covering problem . Using the
aforementioned suboptimal construction, in the upper bound
on h is shown for several dimensions: more precisely, h is
demonstrated to have upper bound of 22 (see Fig. 8b), 85, 305 in
R3, R4, R5, correspondingly
.Since neither
nor k depend on n, the number of reverse nearest
neighbors does not depend on the current number of points in
the dataset.  
spherical cap
Fig. 9. (a) Two 60-degree 3D cones (b) Spherical caps of the frame consisting
of 22 cones that completely cover 3D space.
The following theorems provide the upper bound for the
number of points where k-distance, lrd and LOF are updated.
Theorem 6. The number of data records where k-distance
needs to be updated is |Supdate_k_distance| ≤ F for insertion, and
|Supdate_k_distance| ≤ F*|Sdelete|, for deletion block of size |Sdelete|.
Proof (sketch). For insertion/deletion of one data record s, kdistance needs to be updated on all data records that have the
inserted/deleted data record in its neighborhood, i.e., on all
reverse nearest neighbors of s. Theorem 5 bounds the number
of reverse neighbors with F.  
Theorem 7. Number of data records where lrd is updated is
|Supdate_lrd| ≤ k⋅|Supdate_k_distance|.
Proof (sketch). lrd values are updated on points from
|Supdate_k_distance| and may be updated on their k-nearest
neighbors. 
Theorem 8. Number of data records where LOF is updated is
|Supdate_LOF|≤ (1+F)⋅|Supdate_lrd|.
Proof (sketch). LOF value is updated on data records from
|Supdate_lrd| and their reverse nearest neighbors, thus giving the
bound stated in the Theorem 8.  
corollaries
asymptotic
complexity for the proposed algorithm.
Corollary 3. The asymptotic time complexity for insertion
and deletion in incremental LOF is2:
TincrLOF_ins= O(k⋅F⋅TkNN + k⋅F⋅TkRNN + F2⋅k + Tinsert), (8)
TincrLOF_del=O(|Sdelete|⋅ (k⋅F⋅TkNN + k⋅F⋅TkRNN + F2⋅k + Tdelete)).
Here, TkNN and TkRNN are time complexities of kNN and
kRNN algorithms respectively, while Tinsert and Tdelete
correspond to time needed for the insertion and deletion of a
data record into/from the database (including index updating).
2 By maintaining list of kRNN(p) for each record p, the time
complexities can be further reduced to:
TincrLOF_ins = O(k⋅F⋅TkNN + TkRNN + F2⋅k + Tinsert);
TincrLOF_del = O(|Sdelete| ⋅ (k⋅F⋅TkNN + F2⋅k + Tdelete))
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
Proof (sketch). Follows from the algorithms for insertion
and deletion in incremental LOF, given in Fig. 4 and Fig. 6
respectively, and Theorems 6-8.  
Corollary 4. When efficient algorithms for kNN [e.g. 24],
kRNN [e.g., 17-20,42], as well as efficient indexing structures
for inserting/deleting the records are used (where TkNN
= TkRNN = Tinsert = Tdelete = O(log n), the time complexities of
TincrLOFins and TincrLOFdel are logarithmic in the current size n of
the database, e.g.,:
TincrLOFins = O(k⋅F⋅ log n + F2⋅k).  
Corollary 5. Time complexity of the incremental LOF
algorithm after all updates to the dataset of size N are applied
is O(N⋅logN).
Proof. Directly follows from Corollary 4.  
Note that according to Theorem 5, the time complexity of
the incremental LOF may exponentially increase with the
dimensionality D. However, this is well-known problem of
static LOF as well as other density based algorithms and
not a particular issue with incremental LOF.
IV. EXPERIMENTAL RESULTS
Our experiments were performed on several synthetic data
and real life data sets. In all our experiments, we have
assumed that we have information about the outliers in the
data set, so we could evaluate the detection performance. In
the following subsectios we evaluate time complexity
(subsection A) and outlier detection accuracy (subsections B,
C) with respect to ground truth outlier information.
A. Time Complexity Analysis
Our time complexity analysis was performed on synthetic
data sets, since we could better control the total number of
data records N in the data set as well as the number of
dimensions D. Reported experimental results provide evidence
about (i) relation between the number of updates for LOF
values and the total number of data points N; (ii) the
dependence of the number of updates for LOF values on LOF
parameter k; and (iii) the dependence of the number of updates
for LOF values on the dimension D.
Our synthetic data sets had different number of data records
(N∈{100,200,…,5000}), as well as different number of
dimensions, (D∈{2,3,4,5,10}). For each pair (D, N), we have
created 100 data sets with N random records generated from
D-variate distribution. We experimented with both uniform
and standard (zero mean, unit covariance matrix) Gaussian
distribution. For each of 100 data sets generated for the pair
(D, N), we varied the values of the parameter k (5, 10, 15, 20)
of the algorithm and then measured the number of updates for
k-distance, reach-dist, lrd and LOF values in the incremental
LOF algorithm for insertion of a new data record into the
dataset. Here, for each pair (D, N) we report average number
of LOF updates for all 100 data sets generated using the
standard Gaussian distributions. Results obtained for the data
sets generated using uniform distribution are analog and not
reported here due to lack of space.
Fig. 10 shows how the number of updates of LOF values
depends on the total number of data records N (x-axis in Fig.
10) for different number of dimensions D (different lines in
graphs in Fig. 10), where each graph corresponds to distinct
value of parameter k.
Number of points
# LOF updates
Number of points
# LOF updates
Number of points
# LOF updates
Number of points
# LOF updates
Fig. 10. The dependence of number of LOF updates on the database size N for
different number of dimensions D and different values of parameter k. The
results are averaged over 100 data sets generated from standardized Gaussian
distribution.
Analyzing Fig. 10, it can be observed that the number of
updates of LOF value stabilizes for sufficiently large N, which
is in accordance with our theoretical analysis from section
III.B. showing O(1) updates with respect to N. It is interesting
to note that for larger k, the number of data records, necessary
to show stabilization of number of LOF updates, is generally
larger. However, for typically used values of k (5-20) the
number of LOF updates becomes constant for N>5000.
Fig. 11 shows the average number of LOF updates vs.
parameter k (each curve corresponds to a particular value of
D) on database of N = 2000 points. The left graph contains
abscise in linear scale, while the abscise in the right graph is
quadratic (proportional to k2). Fig. 11 shows that the actual
number of updates seems to change not faster than k2.
Therefore, the worst-case upper bound O(F2⋅k) =O(k3)
obtained in Section III.B. seems to be rather pessimistic. This
phenomenon is due to the fact that in reality, not all updates of
reach-dist values result in update of lrd value. Also, a data
record may belong to reverse nearest neighbors of multiple
records on which lrd has been updated. Hence, such data
record, although output from several kRNN queries, will result
only in one update of LOF value.
Fig. 11 also provides an insight on the dependence of the
number of LOF updates on the number of dimensions D.
While undoubtedly the number of LOF updates increases with
D, it was difficult to confirm (or reject) theoretical upper
boundary of the exponential dependence (see Section III.B).
However, it is evident that the growth of LOF updates with
respect to dimensionality D is not explosive (the average
number of updates stay bellow 1000 even for D=10, k=20).
One of the reasons is that considered upper bound for the
number of reverse neighbors is the worst case and is reached
infrequently.
anticipate
dimensionality of the data will not become the bottleneck of
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
the incremental LOF algorithm due to number of LOF
updates, but rather due to inability of indexing structures to
resist curse of dimensionality .
#LOF updates
#LOF updates
Fig. 11. The dependence of number of LOF updates on the value of parameter
k for different number of dimensions D. The results are averaged over 100
datasets with 2000 records generated from standardized normal distribution. a)
Linear abscise; b) Abscise proportional to k2.
B. Learning New Behavior
Our first synthetic data set used to analyze ability of
incremental LOF to learn new behavior in a non-stationary
data corresponded to 1000 data records generated from 2modal mixture of 2-dimensional Gaussian distributions with
different means. The data set consisted of 500 records
generated from Gaussian distribution N(μ1,Σ1) and 500
distribution
Fig.12 shows current LOF value after inserting data records
501, 505, 510 and 1000. At n=501, we identified inserted
record from a new distribution as an outlier as soon as it
appeared. However, when the “switch” to the new distribution
was complete, the incremental LOF learned the new
distribution as a part of regular behavior (starting from n=510)
so the new records were correctly labeled as normal. As
discussed in Section II, if we used “supervised” static LOF
(trained at n=500), the data records from the new distribution
will always be marked as outliers, since they did not appear in
original data on which the “supervised” static LOF was trained
(see Fig. 1). After all records were inserted, the result (LOF
value for n=1000) was identical to LOF values obtained using
the “periodic” static LOF algorithm. However, the “periodic”
LOF algorithm will identify all records belonging to the new
distribution as normal (although some of them were outliers at
the time of insertion).
The second synthetic data set consisted of 1000 data records
belonging to 2-modal 2-dimensional Gaussian mixture with
same mean but different variances. This data set was created
to illustrate the attempt of masquerading (hiding within
existing distribution, see Fig. 2). The data set had 500 data
records of Gaussian distribution N(μ1,Σ1) and 500 data
records with Gaussian distribution N(μ2,Σ2), where
Fig.13 shows current LOF values obtained using the incremental LOF algorithm, after insertion of records 501, 515, 600
and 1000. Initially (at n=501), the computed LOF values do
not reveal any change of behavior. However, while additional
data records continue to appear within a new distribution, their
lrd values will start to increase (larger local density of the new
distribution). According to Eq. (3) this fact will cause increase
of LOF values for points from the old distribution that are on
the borders of the new distribution (e.g., for n=600, the
maximal LOF value becomes larger than 10). Since
incremental LOF algorithm keeps track of updates of LOF
values for existing data records (already in the database), this
phenomenon is easy to identify. Therefore, it is apparent that
the incremental LOF algorithm is capable of identifying the
masquerading attempt as well as an approximate time when
the attempt begins! As already discussed in Section II,
“supervised” static LOF (trained at n=500 on records from the
first distribution) is not capable of identifying this behavior,
since new data records (corresponding to new, much denser
distribution) are not considered when computing lrd.
No points 501
No points 505
No points 510
No points 1000
Fig. 12. The result of incremental LOF algorithm (k=10) on dataset consisting
of 500 records of Normal distribution N(μ1,Σ) followed by 500 records with
Normal distribution N(μ2,Σ).
a) n=501; b) n=505; c) n=510; d) n=1000 points inserted. New data records
(inserted at time instant n) are marked red.
In contrast, “periodic” LOF method will be able to identify
the masquerading, but this identification will be delayed for
the period of LOF update.
C. Experiments on real life data sets
To illustrate the ability of incremental LOF algorithm to
identify outliers in dynamic environment, we first selected two
real life data sets containing video sequences. The first data set
is composed of 100 video frames (data is available at:
www.cis.temple.edu/~latecki/TestData/SimTest.zip).
features from video frames are extracted using the procedure
described in . Our goal was to identify sudden changes in
selected video frames, which is important problem in video
analysis , especially in analysis of streaming videos (e.g.,
video-surveillance). Analyzing Fig. 14, it can be observed that
the proposed incremental LOF correctly detected all the
sudden changes in video frames, while not producing any false
alarm. These changes were caused by the appearance of a new
object (frame 21), zooming objects to camera (frame 31) and
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
novel video content (frames 41, 61, 71, 91). On the other
hand, static “periodic” LOF algorithm computed after all 100
frames did not detect any of these frames as outliers, while
“supervised” LOF algorithm had very large false alarm rate
due to data non-stationarity.
Dynamic LOF
Identified transitions
Static LOF
video frames
No points 501
No points 515
No points 600
No points 1000
Fig. 13. The result of incremental LOF algorithm (k=10) on dataset consisting
of 500 recordsaussian distribution N(μ,Σ1) and 500 records with Gaussian
distribution N(μ,Σ2). μ=[0 0], Σ1=diag(1,1), Σ2=diag(10-4, 10-4), after a) 501;
b) 515; c) 600; d) 1000 data records inserted.
Finding unusual trajectories in surveillance videos may lead
to early identification of illegal behavior and hence the outliers
need to be detected as soon as they appear. In such
applications, due to data non-stationarity, static LOF algorithm
is not suitable. To identify outliers in this environment, we
have performed experiments on the second real life data set.
The dataset corresponds to 239 video motion trajectories,
where only 2 trajectories (225, 237) are visually identified as
unusual behavior (person walking right and then back left and
person walking very slowly). The trajectories were extracted
from IR surveillance videos using our motion detection and
tracking algorithms . Each trajectory was represented by
five equidistant points in [x,y,time] space (two spatial
coordinates on the frame and the time instant) and the
dimensionality of this feature vector was further reduced to
three using the principal component analysis . The dataset
is available at www.cs.umn.edu/~aleks/inclof
Results of outlier detection on IR video trajectories are
shown in Fig. 15. Fig. 15a shows computed values of LOF(t)
using the incremental LOF algorithm, for each trajectory t at
the moment of its insertion into the database. As it can be
observed, the LOF values for trajectories 225 and 237 are
significantly larger than for the rest of the trajectories, thus
clearly indicating unusual behavior. To further illustrate
performance of incremental LOF method, we plotted all
considered trajectories in [x,y,time] space (Fig. 15b), colorcoded by computed LOF value. It can be seen that the most
intense color (and the highest LOF) corresponds to clear
Fig. 14. Result of applying incremental (blue asterisks) and static “periodic”
(red x) LOF algorithm (k=10) on video sequences from our test movie. For the
static LOF algorithm, LOF values that are shown for each frame t are
computed when the latest data record is inserted.
The third data set was 1998 DARPA Intrusion Detection
Evaluation Data . The original DARPA’98 data contains
two types: training data and test data. The training data
consists of seven weeks of network-based attacks inserted in
the normal background data. Attacks in training data are
labeled. The test data contain two weeks of network-based
attacks and normal background data. Seven weeks of data
resulted in about five million connection records. Since the
amount of available data is huge (e.g. some days have several
million connection records), we have chosen only the data
from fifth week to present our findings. We used tcptrace
utility software as the packet filtering tool in order to
extract information about packets from TCP connections and
to construct new features. The DARPA’98 training data
includes “list files” that identify the time stamps (start time
and duration), service type, source IP address and source port,
destination IP address and destination port, as well as the type
of each attack. We used this information to map the
connection records from list files to the connections obtained
using tcptrace utility software and to correctly label each
connection record as “normal” or an attack type. The same
technique was used to construct KDDCup’99 data set , but
this data set did not keep time information about the attacks.
The main reason for this procedure is to associate new
constructed features with the connection records from list files
and to create more informative data set for learning. The
complete list of the features extracted from “raw tcpdump”
data using tcptrace software is available in our previously
published results .
The results of applying the incremental LOF algorithm on
DARPA98 data set are shown in Fig. 16. We have chosen to
illustrate only two phenomena here. The first phenomenon
corresponds to time moment t = 1368, when new behavior
starts to appear. This behavior was clearly detected by
incremental LOF algorithm (Fig 16, dash-dot cyan line).
Although detected behavior did not correspond to any
intrusions, it was important to include it when updating
IEEE Symposium on Computational Intelligence and Data Mining (CIDM), April 2007
characteristics of normal behavior. The second phenomenon at
time moment t = 1937, corresponds to detection of Denial of
Service (DoS) attack (Fig. 16, red dashed line). The LOF
value at this time moment spikes very high, thus resulting in
immediate DoS detection. If the static “periodic” LOF
algorithm was applied after 2500 data records have been
inserted into the data set, this DoS attack would not be
detected (Fig 16, black dash-dot line), since it would create a
new data distribution (scenario 1 (Fig.1) in section II).
Fig. 15. Incremental LOF algorithm used for identifying unusual trajectories
in motion videos (k=10); a) The values of LOF valuer obtained for each
trajectory; b) Trajectories in [x,y,time] feature space colored according to the
values of LOF value.
V. CONCLUSIONS AND FUTURE WORK
A framework for incremental density-based outlier detection
scheme is presented. The proposed algorithm has the same
detection performance as the static “iterated” LOF algorithm
that is applied after insertion of each data record, but it is
much more computationally efficient. Experimental results on
several synthetic and real life data sets from video and
intrusion detection domains indicate that the proposed
incremental
additional
functionality that is difficult to achieve using static variants of
LOF algorithm, including detection of new behavior as well as
identification of masquerading outliers.
time snapshot
Fig. 16. Instant LOF values for all the data records in the dataset at time
instants 1368, 1937, 2500 obtained using incremental LOF algorithm (k=10)
on Network Intrusion dataset.
The fact that the number of updates in the incremental LOF
algorithm per insertion/deletion of a single data record does
not depend on the total number of data records is quite
appealing for its use in real-time data stream applications.
However, its performance crucially depends on efficient
indexing structures to support k-nearest neighbor and reverse
k-nearest neighbor queries. Due to limitations of existing
indexing structures with high data dimensionality, the
proposed incremental LOF (similar as static LOF) is not
applicable when the data have large number of dimensions.
The approximate k-NN and reverse k-NN algorithms might
applicability
incremental
multidimensional data.
Future work on deleting data records from database is
needed. More specifically, it would be interesting to design an
algorithm with exponential decay of weights, where the most
recent data records will have the highest influence on the local
density estimation. In addition, an extension of the proposed
methodology to create incremental versions of other emerging
outlier detection algorithms, (e.g., Connectivity Outlier Factor
(COF) , LOCI ), is also worth considering. Additional
real-life data sets will be used to evaluate the proposed
algorithm and ROC curves will be applied to quantify the
algorithm performance.
ACKNOWLEDGMENTS
D. Pokrajac has been partially supported by NIH (grant #2
P20 RR016472-04), DoD/DoA (award 45395-MA-ISP) and
NSF (awards # 0320991, #HRD-0310163, #HRD-0630388).
Authors would also like to thank Roland Miezianko, Terravic
Corp. for providing IR surveillance video, Brian Tjaden,
Wellesley College, for discussion about reverse k-NN queries
and David Mount, University of Maryland, for insightful
discussion about sphere covering verification.