Average Case Analysis of Multichannel Sparse
Recovery Using Convex Relaxation
Yonina C. Eldar, Senior Member, IEEE and Holger Rauhut
Abstract— In this paper, we consider recovery of jointly sparse
multichannel signals from incomplete measurements. Several
approaches have been developed to recover the unknown sparse
vectors from the given observations, including thresholding,
simultaneous orthogonal matching pursuit (SOMP), and convex
relaxation based on a mixed matrix norm. Typically, worst-case
analysis is carried out in order to analyze conditions under which
the algorithms are able to recover any jointly sparse set of vectors.
However, such an approach is not able to provide insights into
why joint sparse recovery is superior to applying standard sparse
reconstruction methods to each channel individually. Previous
work considered an average case analysis of thresholding and
SOMP by imposing a probability model on the measured signals.
In this paper, our main focus is on analysis of convex relaxation
techniques. In particular, we focus on the mixed ℓ2,1 approach
to multichannel recovery. We show that under a very mild
condition on the sparsity and on the dictionary characteristics,
measured for example by the coherence, the probability of
recovery failure decays exponentially in the number of channels.
This demonstrates that most of the time, multichannel sparse
recovery is indeed superior to single channel methods. Our
probability bounds are valid and meaningful even for a small
number of signals. Using the tools we develop to analyze the
convex relaxation method, we also tighten the previous bounds
for thresholding and SOMP.
Key Words: Multichannel sparse recovery, mixed-norm
optimization, average performance, thresholding, simultaneous orthogonal matching pursuit
I. INTRODUCTION
Recovery of sparse signals from a small number of measurements is a fundamental problem in many different signal
processing tasks such as image denoising , analog-to-digital
conversion , , , radar, compression, inpainting,
and many more. The recent framework of compressed sensing
(CS), founded in the works of Donoho , Cand`es, Romberg
and Tao , studies acquisition methods as well as efﬁcient
computational algorithms that allow reconstruction of a sparse
vector x from linear measurements y = Ax, where A ∈Rn×N
is referred to as the measurement matrix. The key observation
is that y can be relatively short, so that n < N, and still
contain enough information to recover x.
Yonina C. Eldar is with the Technion—Israel Institute of Technology, Haifa
Israel. Email: . Holger Rauhut is with the Hausdorff
Center for Mathematics and Institute for Numerical Simulation, University of
Bonn, Germany.
The work of Y. Eldar was supported by the Israel Science Foundation under
Grant no. 1081/07 and by the European Commission in the framework of
the FP7 Network of Excellence in Wireless COMmunications NEWCOM++
(contract no. 216715). H. Rauhut acknowledges support by the WWTF project
SPORTS (MA 07-004) and by the Hausdorff Center for Mathematics.
Determining the sparsest vector x consistent with the data
y = Ax is generally an NP-hard problem . To determine x
in practice, a multitude of efﬁcient algorithms have been proposed, , , , , , which achieve high recovery
rates. The basis pursuit (BP), or ℓ1-minimization approach,
is the most extensively studied recovery method , ,
 , . The use of general purpose or specialized convex
optimization techniques , allows for efﬁcient reconstruction using this strategy. Although greedy methods, such
as simple thresholding or orthogonal matching pursuit (OMP),
are faster in practice, BP provides signiﬁcantly better recovery
guarantees. In particular, there exist measurement matrices
A ∈Rn×N that allow for stable recovery of all k-sparse
vectors as long as n ≥Ck log(N/k) where C is a constant.
Such uniform recovery is not possible for simple thresholding
or OMP , . (We note, however, that the recent greedy
algorithms CoSaMP and ROMP are able to provide
such uniform guarantees.) In practice, the recovery rate of
BP when averaged over all random sparse vectors is typically
better than that predicted by the theory. This is due to the fact
that existing analysis considers the ability of BP to recover
all vectors x. On the other hand, in random simulations, the
worst-case instance of x typically does not occur. Therefore,
considering the behavior of various recovery methods over
random x often leads to more characteristic behavior.
The BP principle as well as greedy approaches have been
extended to the multichannel setup where the signal consists
of several channels with joint sparsity support , , ,
 , , , , . In the buzzword distributed
compressed sensing was coined for this setup. An alternative
approach is to ﬁrst reduce the problem to a single channel
problem that preserves the sparsity pattern, and recover the
signal support set; given the support, the measurements can
be inverted to recover the input . A variety of different
recovery results have been established that provide conditions
ensuring that the output of the proposed efﬁcient algorithms
coincides with the true signals. In a recovery result was
derived for a mixed ℓp,1 program in which the objective is to
minimize the sum of the ℓp-norms of the rows of the estimated
matrix whose columns are the unknown vectors. Recovery
results for the more general problem of block-sparsity were
developed in based on the block restricted isometry property (RIP), and in based on mutual coherence. In practice,
multichannel reconstruction techniques perform much better
than recovering each channel individually. However, the theoretical equivalence results predict no performance gain. The
reason is that these results apply to all possible input signals,
and are therefore worst-case measures. Clearly, if we input the
same signal to each channel, then no additional information
on the joint support is provided from multiple measurements.
Therefore, in this worst-case scenario there is no advantage
for multiple channels.
In order to capture more closely the true underlying behavior of existing algorithms and observe a performance
gain when using several channels, we consider an averagecase analysis. In this setting, the inputs are considered to be
random variables. The idea is to develop conditions on the
measurement matrix A such that the inputs can be recovered
with high probability given a certain input distribution.
Recently, there have been several papers that consider
sparse recovery with random ensembles. In random subdictionaries of A are considered and analyzed. This allows to
obtain average results for BP with a single input channel. In
 , average-case performance of single channel thresholding
was studied. In , extensions to two multichannel
recovery algorithms were developed: thresholding and simultaneous OMP (SOMP) , . Under a mild condition on the
sparsity and on the matrix A, the probability of reconstruction
failure decays exponentially with the number of channels. In
the present paper we contribute to this line of research by
analyzing the average-case performance of multichannel BP,
i.e., mixed ℓ2,1-minimization , , , . The tools
we derive in this context are then also used to slightly improve
previous bounds on average performance of multichannel
thresholding and SOMP.
The theoretical average-case results we develop for multichannel BP are superior to the average bounds developed on
thresholding and SOMP. For an equally mild or even milder
condition on the sparsity and on the matrix A, we obtain faster
exponential decay of the failure probability with respect to the
number of channels. Thus, in this sense, the extension of BP to
the multichannel case is superior to existing greedy algorithms,
just as in the single channel setting. Moreover, our recovery
results are applicable also in the single channel case whereas
previous results require a large number of channels to
yield meaningful (i.e., positive) probability bounds (although
our new bound for thresholding generalizing the one in 
does not suffer from this drawback). Note, however, that in
simulations SOMP often exhibits the best performance. This
may be explained by the fact that the bounds are not tight (at
least for SOMP).
To develop our probability bounds, we rely on a new
sufﬁcient condition that ensures recovery of the exact signal
set via ℓ2,1-minimization. This condition generalizes a result
of , to the multichannel setting, and is weaker than
existing multichannel recovery conditions. Our average-case
analysis is then carried out assuming that the elements of
the input signal are drawn at random. We prove that under
a certain restriction on A and the sparsity set S, the sufﬁcient
condition we develop is satisﬁed with high probability. The
restriction we impose is that the ℓ2-norm of A†
Saℓover all ℓ
not in the set S is bounded, where aℓis the ℓth column of
S is the pseudo inverse of the restriction of A to the
columns in S. This is an improvement over known worst-case
recovery conditions which require a bound on the ℓ1-norm
 , , and are therefore stronger. Loosely speaking, we
will show that while worst-case results limit the sparsity level
to order √n, average-case analysis shows that sparsity up to
order n may enable recovery with high probability. In terms
of RIP constants, instead of bounding the RIP constant for
sparsity sets of size 2k, we will only need to consider sets of
size k + 1.
The remaining of the paper is organized as follows. In
Section II we introduce our problem and brieﬂy summarize
known equivalence results between the ℓ2,1 approach for
multichannel recovery and NP-hard combinatorial optimization that recovers the true signals. A new recovery condition
is derived in Section III, which is weaker than previous
results, and will be instrumental in developing our averagecase analysis in Section IV. Since the probability bounds
we develop depend on the 2-norm of A†
Saℓ, in Section V
we derive several upper bounds on this norm. In Section VI
we use the tools developed in the previous section to derive
new bounds on the average performance of thresholding and
SOMP, that are tighter than existing results and also applicable
to a broader set of problems. We then compare our bounds on
multichannel BP to these results. Finally, in Section VII we
present several simulations demonstrating the behavior of the
different methods.
Throughout the paper, we denote by AS the submatrix of A
consisting of the columns indexed by S ⊂{1, . . ., N}, while
XS is the submatrix of X consisting of the rows of X indexed
by S. The ℓth column of A is denoted by aℓor Aℓ. For a matrix
A, ∥A∥2 is the spectral norm of A, i.e., the largest singular
value, and A∗is its conjugate transpose. The unit sphere in
RL is deﬁned by SL−1 = {x ∈RL, ∥x∥2 = 1}; the complex
counterpart is denoted SL−1
= {x ∈CL, ∥x∥2 = 1}.
II. MULTICHANNEL ℓ1-MINIMIZATION
A. Problem Formulation
We consider multichannel signal recovery where our goal
is to recover a jointly-sparse matrix X ∈CN×L from n linear
measurements per channel. Here N denotes the signal length
and L the number of channels, i.e., the number of signals.
We assume that X is jointly k-sparse, meaning that there are
at most k rows in the matrix X that are not identically zero.
More formally, we deﬁne the support of the matrix X as
where the support of the ℓth column is
supp Xℓ= {j, Xjℓ̸= 0}.
Our assumption is that ∥X∥0 := | supp X| ≤k. The measurements are given by
where A ∈Cn×N is a given measurement matrix. Each
measurement vector Yℓ= AXℓcorresponds to a measurement
of the corresponding signal Xℓ.
The natural approach to determine X given Y is to solve
the ℓ0-minimization problem
However, (4) is NP hard in general . Several alternative
methods have been proposed, that have polynomial complexity
 , , , , , , , , . A variety of
different equivalence results between the solution of the ℓ0problem and the output of the proposed efﬁcient algorithm.
In an equivalence result was derived for a mixed ℓp,1
program in which the objective is to minimize the sum of the
ℓp-norms of the rows of the estimated matrix whose columns
are the unknown vectors. The condition is based on mutual
coherence, and turns out to be the same as that obtained from
a single measurement problem, so that the joint sparsity pattern
does not lead to improved recovery capabilities as judged by
this condition. Recovery results for the more general problem
of block-sparsity were developed in based on the RIP, and
in based on mutual coherence. Reducing these results to
the multiple measurement vectors (MMV) setting leads again
to conditions that are the same as in the single measurement
case. An exception is the work in , which considers
average-case performance of thresholding and SOMP. Under
a mild condition on the sparsity and on the matrix A, the
probability of reconstruction failure decays exponentially with
the number of channels L. In Section VI we slightly improve
on these bounds using the tools developed in this paper.
In Section IV we follow a similar approach and treat the
average behavior of the mixed ℓ2,1-minimization program ,
 , deﬁned by
min ∥X∥2,1 =
subject to AX = Y,
which promotes joint sparsity, as argued for instance in .
In the single channel case L = 1 this is the usual BP principle.
Therefore, our results can also be used to deduce the averagecase behavior of the BP method. This is in contrast to ,
in which the recovery results derived are not applicable to
the single channel case. As we discuss in Section VI, our
theoretical results are superior to the previous average-case
analysis of in the sense that we use an equally mild or
even milder condition on the sparsity and on the matrix A, but
at the same time get a faster exponential decay of the failure
probability with respect to the number of channels L.
B. Recovery Results
Recovery results for the program (5) were considered in
 , , . In particular, the lemma below is derived in
 and follows also from where the more general case
of block sparsity is considered.
Proposition 2.1: Let S ⊂{1, . . . , N} and suppose that
for all ℓ/∈S,
S denoting the pseudo-inverse of AS.
Then (5) recovers all X ∈CN×L with supp X = S from
Note, that the condition above does not depend on the number
of channels. In the next section we will derive a condition
similar to (6) that involves the 2-norm instead of the 1-norm,
and is therefore weaker (namely, easier to satisfy).
Assuming the columns of A are normalized, ∥aℓ∥2 = 1, we
can guarantee that (6) holds as long as the coherence µ of A
is small enough, where 
j̸=ℓ|⟨aj, aℓ⟩|.
The following result follows from by noting that the block
coherence in this setting is equal to µ/d.
Proposition 2.2: Assume that
(2k −1)µ < 1.
Then (5) recovers all X with ∥X∥0 ≤k from Y = AX.
Under the same conditions as in Propositions 2.1 and 2.2, it
is shown in that BP will recover a single k-sparse vector.
Therefore, if (6) holds, then instead of solving (5) we can use
BP on each of the columns of Y .
The coherence is lower bounded by 
The lower bound behaves like 1/√n for large N, which limits
the Proposition 2.2 to maximal sparsities k = O(√n). To
improve on this we can generalize existing recovery results
 , based on RIP to the multichannel setup. The restricted
isometry constant δk of a matrix A is deﬁned to be the smallest
constant δk such that
(1 −δk)∥x∥2
2 ≤(1 + δk)∥x∥2
for all k-sparse vectors x. The next proposition follows from
Proposition 2.3: Assume A ∈Cn×N with δ2k <
Let X ∈CN×L, Y = AX, and let X be the minimizer of
∥X −X∥F ≤Ck−1/2∥X −ˆX(k)∥2,1
where C is a constant, ∥X∥F =
Tr(X∗X) is the Frobenius
norm of X and ˆX(k) denotes the best k-term approximation
of X, i.e., supp ˆX(k) consists of the indices corresponding to
the k largest row norms ∥Xℓ∥2. In particular, recovery is exact
if | supp X| ≤k.
It is well known that Gaussian and Bernoulli random matrices
A ∈Rn×N satisfy δ2k ≤
2−1 with high probability as long
as , 
n ≥Ck log(N/k).
For random partial Fourier matrices the respective condition is
n ≥ck log4(N) , . Therefore, Proposition 2.3 allows
for a smaller number of measurements. However, there is still
no dependency on the number of channels. Indeed, under the
same RIP condition BP will recover a single k-sparse vector
and therefore, as before, BP may as well be applied to each
of the columns of Y individually.
We conclude this overview by stressing that known equivalence results do not improve on those for single channel
sparse recovery. In , equivalence results are derived
for a mixed ℓ2,1 program when different measurement matrices
Ai are used on each channel. In this case, even worst-case
analysis shows improvement over L = 1. However, when all
measurement matrices are equal, the recovery conditions do
not show any advantage with multiple signals.
III. A RECOVERY CONDITION
Before turning to analyze the average-case behavior of
(5), we ﬁrst develop a new condition on A that allows for
perfect recovery. This formulation will be useful in deriving
the average-case results.
In the following theorem we give a sufﬁcient condition on
the minimizers of (5). This theorem generalizes a result of
 , for the L = 1 case. To this end we denote by
sgn(X) ∈CN×L the matrix with entries
sgn(X)ℓj =
∥Xℓ∥2 ̸= 0;
∥Xℓ∥2 = 0.
In this deﬁnition, each element of X is normalized by the norm
of the corresponding row. When L = 1, sgn(X) reduces to
the sign of the elements of the vector x.
Theorem 3.1: Let X ∈CN×L with supp X = S and
assume AS to be non-singular. If there exists a matrix H ∈
Cn×L such that
SH = sgn(XS),
∥H∗aℓ∥2 < 1
for all ℓ/∈S
then X is the unique solution of (5).
Before proving the theorem we note that the two conditions
on H easily imply that
∥H∗aℓ∥2 ≤1,
for all ℓ.
Proof: The proof follows the ideas of , with appropriate modiﬁcations to account for the mixed ℓ2,1 norm that
replaces the ℓ1 norm.
Let Y = AX, and assume there exists a matrix H such that
X, H satisfy (13) and (14). Let X′ be an alternative matrix
satisfying Y = AX′. Our goal is to show that ∥X∥2,1 <
∥X′∥2,1. To this end, we note that
∥X∥2,1 = ∥XS∥2,1 = Tr
 sgn(XS)(XS)∗
where Tr denotes the trace. Substituting A∗
SH = sgn(XS)
into (16), and using the cyclicity of the trace we have
∥X∥2,1 = Tr
= Tr (H∗AX′)
where we used the fact that ASXS = Y = AX′ and S′
denotes the support of X′. We next rely on the following
Lemma 3.2: Let A, B be matrices such that AB is deﬁned.
Then | Tr(BA)| ≤∥B∥2,1 maxℓ∥Aℓ∥2, with strict inequality
if ∥Aℓ∥2 < maxℓ∥Aℓ∥2 for some value of ℓfor which
∥Bℓ∥2 ̸= 0.
Proof: The proof follows from noting that
| Tr(BA)| ≤
∥Bℓ∥2∥Aℓ∥2
∥Bℓ∥2 = max
∥Aℓ∥2∥Bℓ∥2,1,
where the second inequality is a result of applying Cauchy-
Schwartz. Under the condition of the lemma, we have strict
inequality in the last inequality.
Applying Lemma 3.2 to (17), leads to
∥X∥2,1 ≤∥X′S′∥2,1 max
ℓ∈S′ ∥H∗Aℓ∥2 ≤∥X′S′∥2,1
= ∥X′∥2,1,
where the last inequality follows from (15). We have strict
inequality in the ﬁrst inequality of (18) as long as the values
∥H∗Aℓ∥2 for ℓ∈S′ are not all equal since ∥X′ℓ∥2 ̸= 0 for
all ℓ∈S′ be deﬁnition of the support.
Suppose to the contrary that ∥H∗Aℓ∥2 = a for all ℓ∈
S′. Clearly, S′ must contain at least one index ℓthat is not
contained in S; otherwise S′ ⊂S, which would contradict the
hypothesis that AS is non-singular, AS′X′ = ASX and X ̸=
X′. By our assumption ∥H∗aℓ∥2 < 1, which then implies that
a < 1 or ∥H∗Aℓ∥2 < 1, ℓ∈S′. The inequalities in (18) then
∥X∥2,1 ≤∥X′S′∥2,1 max
ℓ∈S′ ∥H∗Aℓ∥2 < ∥X′S′∥2,1 = ∥X′∥2,1.
Thus, we have shown that ∥X′∥2,1 > ∥X∥2,1 for any X′
such that Y = AX′, and therefore (5) recovers the true sparse
Choosing H = (A†
S)∗sgn(XS) in Theorem 3.1 results in
the following corollary.
Corollary 3.3: Let X ∈CN×L with supp X = S and
assume AS to be non-singular. If
∥sgn(XS)∗A†
for all ℓ/∈S,
then X is the unique minimizer of (5).
This corollary will be instrumental in proving the average-case
performance of (5). It can easily be seen that Corollary 3.3 implies Proposition 2.1. This follows from the triangle inequality,
sgn(XS)∗A†
Saℓ)j sgn(Xj)∗
Saℓ)j| ∥sgn(Xj)∥2 = ∥A†
where we used the fact that ∥sgn(Xj)∥2 = 1.
IV. AVERAGE CASE ANALYSIS
Intuitively, we would expect multichannel sparse recovery
to perform better than single channel recovery. However, in
the worst case setting this is not true as already suggested by
the results of Section II. The reason is very simple. If each
channel carries the same signal, Xℓ= x for ℓ= 1, . . . , L,
then also the components of Y = AX are all the same and
we do not have more information on the support of X than
provided by a single component Yℓ. The following proposition
establishes formally that if BP fails for a given measurement
matrix A, then multichannel optimization (5) will fail as well
so that in the worst-case, adding channels will not improve
performance.
Proposition 4.1: Suppose there exists a k-sparse vector x ∈
CN that ℓ1-minimization is not able to recover from y = Ax.
Then ℓ2,1-minimization fails to recover X = (x|x| · · · |x) ∈
CN×L from Y = AX.
If ℓ1-recovery fails on some k-sparse x then
necessarily ∥x′∥1 ≤∥x∥1 for some x′ satisfying Ax′ = Ax.
Clearly X = (x|x| · · · |x) is (jointly) k-sparse and AX = AX′
for X′ = (x′|x| · · · |x′). Furthermore,
L∥x∥1 = ∥X∥2,1
and therefore X is not the unique minimizer of the ℓ2,1minimization problem.
Realizing that (5) is not more powerful than usual BP in
the worst case, we seek an average-case analysis. This means
that we impose a probability model on the k-sparse X. In
particular, as in , we will assume that on the support S of
size k the coefﬁcients of X are chosen at random. We then
show that under a suitable probability model on the non-zero
elements of X, the condition given by Corollary 3.3 is satisﬁed
with high probability, which depends on L.
We follow the probability model used in : let S be the
joint support of cardinality k. On S the coefﬁcients are given
where Σ = diag(σj, j ∈S) ∈Rk×k is an arbitrary diagonal
matrix with positive diagonal elements σj. The matrix Φ will
be chosen at random according to one of the following models.
• Real Gaussian: each entry of Φ ∈Rk×L is chosen
independently from a standard normal distribution.
• Real spherical: the rows of Φ ∈Rk×L are chosen
independently and uniformly at random from the real
sphere SL−1.
• Complex Gaussian: the real and imaginary parts of each
entry of Φ ∈Ck×L are chosen independently according
to a standard normal distribution.
• Complex spherical: the rows of Φ ∈Ck×L are chosen
independently and uniformly at random from the complex
sphere SL−1
Note that taking Σ to be the identity matrix results in a
standard Gaussian random matrix XS, while taking arbitrary
non-zero σj’s on the diagonal of Σ allows for different
variances. The matrix Σ may be deterministic or random. In
particular, choosing Σ to be the matrix with diagonal elements
given by the inverse ℓ2-norm of the rows of Φ in the real
(complex) Gaussian model, leads to a matrix XS with a real
(complex) spherical distribution.
In Theorems 4.4 and 4.5 below we develop conditions under
which (5) recovers X from Y = AX with probability that
decays exponentially with L. The condition in both theorems
is given in terms of an upper bound on ∥A†
Saℓ∥2 for ℓnot in S.
This is in contrast to the worst-case result of Proposition 2.1
that is given in terms of ∥A†
Saℓ∥1 and therefore stronger.
The essential idea in both proofs is to show that if the
bound on ∥A†
Saℓ∥2 is satisﬁed, then the sufﬁcient condition
of Corollary 3.3 holds with high probability.
Before stating the ﬁrst theorem, we derive the following
result on the norm of sums of independent random vectors,
uniformly distributed on a sphere.
Theorem 4.2: Let a ∈Ck and let Zj, j = 1, . . . , k, be a
sequence of independent random vectors which are uniformly
distributed on the real sphere SL−1. Then for any u > 1
2 (u2 −log(u2) −1)
Proof: See Appendix I.
Theorem 4.2 generalizes the Bernstein inequality for Steinhaus sequences in [46, Theorem 13] to higher dimensions. We
may extend the estimate easily to random vectors uniformly
distributed on complex unit spheres.
Corollary 4.3: Let a ∈Ck and let Zj, j = 1, . . . , k, be a
sequence of independent random vectors which are uniformly
distributed on the complex sphere SL−1
. Then for any u > 1
 −L(u2 −log(u2) −1)
Proof: First observe that ajZj has the same distribution
as |aj|Zj. We may therefore assume without loss of generality
that aj ∈R. Next, a random vector Z ∈SL−1
is uniformly
distributed on SL−1
if and only if (Re(Z)T , Im(Z)T )T is
uniformly distributed on the real sphere S2L−1. Applying
Theorem 4.2 with L replaced by 2L yields the statement.
With this tool at hand we can now easily prove the following
average-case recovery theorem.
Theorem 4.4: Let S ⊂{1, . . . , N} be a set of cardinality k
and suppose
Saℓ∥2 ≤α < 1
for all ℓ/∈S.
Let X ∈RN×L with supp X ⊂{1, . . . , N} such that the
coefﬁcients on S are given by (21) with some diagonal matrix
Σ ∈Rk×k and Φ ∈Rk×L chosen from the real Gaussian or
spherical probability. Then with probability at least
2 (α−2 −log(α−2) −1)
(5) recovers X from Y = AX.
If the real probability model is replaced by one of the two
complex models then L/2 can be replaced by L in (23).
For α < 1 we are guaranteed that the exponent in (23) has
a negative argument, and therefore the error decays exponentially in L.
First observe that by the rotational invariance
of Gaussian random vectors the columns of sgn(XS)∗=
sgn(Φ∗) are independent and uniformly distributed on the
real sphere, and the same is also true if we use the real
spherical random model. Denote b(ℓ) = A†
Saℓfor ℓ/∈S and
by Zj, j = 1, . . . , k a sequence of independent random vectors
that are uniformly distributed on the sphere SL−1. Using the
sufﬁcient recovery condition of Corollary 3.3, the union bound
and Theorem 4.2 we can estimate the probability that ℓ2,1
minimization fails to recover X by
ℓ/∈S ∥sgn(XS)∗b(ℓ)∥2 > 1)
P(∥sgn(XS)∗b(ℓ)∥2 > 1)
> α−1∥b(ℓ)∥2
≤(N −k) exp
2 (α−2 −log(α−2) −1)
The complex case follows analogously using Corollary 4.3.
For L = 1, Theorem 4.4 is contained implicitly in [46,
Theorem 13]. The appearance of the 2-norm in (24) instead
of the 1-norm as in (6) makes the condition of the theorem
weaker than worst-case estimates (recall that ∥x∥2 ≤∥x∥1 ≤
k∥x∥2 for any length-k vector x). In Section V this will
be made more evident when we consider conditions on the
coherence µ and the RIP constant to allow for recovery with
high probability. The requirement we obtain on µ is weaker
than that of Proposition 2.2 and allows for recovery with k on
the order of n, while the worst-case results limit recovery to
order √n. Furthermore, in contrast to the worst-case results
which depend on δ2k, we will show that high-probability
recovery is possible as long as δk+1 is small enough.
It is evident from (23) that the failure probability decays
exponentially with growing number of channels L. Moreover,
the bound is also useful for small L, and in particular for
the monochannel case L = 1. Indeed, a simple algebraic
manipulation shows that the failure probability is less than
ǫ provided ∥A†
Saℓ∥2 ≤α for all ℓ/∈S with α satisfying
α−2 −log(α−2) ≥2 log(N/ǫ)
This provides a useful average-case analysis even for L = 1.
For completeness, we also state an alternative recovery result below which provides a slightly better probability estimate
than Theorem 4.4 for very large values of N. However, the
required condition on ∥A†
Saℓ∥2 is stronger.
Theorem 4.5: Let S ⊂{1, . . . , N} be a set of cardinality
k, and let X ∈RN×L be random sparse coefﬁcients with
supp X = S given by the real Gaussian probability model. If
for all ℓ/∈S, where
2Γ((L + 1)/2)
and Γ denotes the Gamma function, then with probability at
P = 1 −exp(−L/8) −k exp(−A2
(5) recovers X from Y = AX.
It follows from Stirling’s formula Γ(z) ∼
2πzzz−1/2e−z,
2Γ((L + 1)/2)
2((L + 1)/2)L/2e−(L+1)/2
(L/2)(L−1)/2e−L/2
= e−1/2 (L + 1)L/2
= e−1/2  L(1 + 1/L)L1/2 ∼
Moreover, for all L ≥1 it holds that
Note that γ =
k is monotonically increasing in
L. In addition, the probability P is also increasing (towards
1) in L. Therefore, more channels increase the probability of
success and in addition relax the requirements on the matrix
To prove the theorem we show that if (24) is
satisﬁed, then condition (20) of Corollary 3.3 holds with
probability P.
To this end, let Φ ∈Rk×L denote a random matrix with
independent standard normal distributed entries, and deﬁne D
as the k ×k diagonal matrix with diagonal elements 1/sj, j ∈
S, where sj = ∥Φj∥2 =
ℓ=1 |Φjℓ|2. We can then express
sgn(XS) = sgn(ΣΦ) = sgn(Φ) = DΦ. (This equation also
means that the diagonal matrix Σ does not play any role.)
Denoting bj = A†
T aj for j /∈S,
∥sgn(XS)∗bj∥2 = ∥Φ∗Dbj∥2 ≤∥Φ∥2∥D∥2∥bj∥2.
By the assumption of the theorem ∥bj∥2 < γ where γ is
deﬁned by (24). It therefore remains to bound ∥Φ∥2 and ∥D∥2.
¿From [10, equation (4.35)], see also , the operator norm
of Φ satisﬁes
with probability at least 1 −exp(−r2/2).
Next we consider ∥D∥2. Observe that the s2
j are χ2(L)
distributed. Therefore, denoting a χ2(L)-variable by Y ,
E[sj] = E[
2L/2Γ(L/2)
√xxL/2ex/2dx
2Γ((L + 1)/2)
As a function of Φj the sj are Lipschitz continuous, i.e.,
sj(Φj −Ψj) ≤∥Φj −Ψj∥2. Using these two observations
we rely on the following standard concentration of measure
result, see e.g. [28, eq. (2.35)] or [29, eq. (1.6)].
Theorem 4.6: Let f be a Lipschitz function on RL, i.e.,
|f(x) −f(y)| ≤B∥x −y∥2 for all x, y ∈RL. Further assume
that Z = (Z1, Z2, . . . , ZL) is a vector of independent standard
Gaussian random variables. Then
P(f(Z) ≥E[f(Z)] + t) ≤exp
P(f(Z) ≤E[f(Z)] −t) ≤exp
Our goal is to show that ∥D∥2 is bounded from above, which
is equivalent to bounding the smallest value of sj from below.
Applying Theorem 4.6 to sj,
P(sj < AL(1 −t)) ≤exp(−t2A2
where we used the fact that B = 1 and E[sj] = AL. Using a
union bound over all j, we obtain
P(sj < AL(1 −t), ∀j) = P
j=1,...,k sj < AL(1 −t)
P(sj < AL(1 −t)) = k exp(−t2A2
Assuming that minj∈S sj
≥AL(1 −t) holds, ∥D∥2 ≤
1/(AL(1 −t)). Combining this bound with (26) for r =
∥sgn(XS)A†
= (s + 1 +
Choosing s = t = 1/2,
∥sgn(XS)∗A†
Saj∥2 ≤(3 + 2
¿From (27) and Corollary 3.3, X is recoverable using (5).
The probability that (27) does not hold can be computed by
applying a union bound to the probabilities that the spectral
norms of each of the matrices Φ and D are not bounded.
This shows that (27) does not hold with probability at most
exp(−L/8) + k exp(−A2
L/8) completing the proof of the
V. BOUNDED NORM CONDITION
Both Theorems 4.4 and 4.5 state that X can be recovered
with high probability from Y , as long as ∥A†
Saℓ∥2 is bounded.
In this section we develop several different conditions under
which this holds.
Proposition 5.1: Let A ∈Cn×N have unit-norm columns
and coherence µ, and let S
⊂{1, . . ., N} be a set of
cardinality k. Assume that
k + (k −1)δ)µ < δ
for some δ > 0. Then ∥A†
Saℓ∥2 ≤δ for all ℓ/∈S.
Proof: Gershgorin’s disk theorem implies that the smallest eigenvalue λmin of A∗
SAS is bounded from below by
1 −(k −1)µ. In particular, A∗
SAS is invertible provided
(k −1)µ < 1. Further,
|⟨aℓ, aj⟩|2 ≤
since by deﬁnition, |⟨aℓ, aj⟩| ≤µ. Now, using the fact that
Saℓ∥2 ≤∥(A∗
SAS)−1∥2∥A∗
≤(1 −(k −1)µ)−1√
where the last inequality follows from the fact that (28) implies
k/(1 −(k −1)µ)−1.
Condition (28) is slightly weaker than (8) as long as δ >
k. This follows from the 2-norm that replaced the 1-norm
in the upper bound. However, (28) still suffers the square-root
bottleneck k = O(√n). To improve on this result, we next
provide a condition based on the following reﬁnement of the
RIP of A. For a set S ⊂{1, . . . , N} we let
δ(S) = ∥A∗
The restricted isometry constant δk of (10) satisﬁes δk =
max|S|≤k ∥A∗
SAS −I∥2 so that if S has cardinality k then
δ(S) ≤δk. We further deﬁne
δ∗(S) = max
ℓ/∈S δ(S ∪{ℓ}).
Clearly, δ(S) ≤δ∗(S) ≤δk+1. Finally, we make use of the
following “local” 2-coherence function,
µ2(S) = max
Saℓ∥2, max
for a subset S ⊂{1, . . ., N}, where S \ ℓdenotes the
elements in S excluding the ℓth one. From the deﬁnition of
the coherence it follows immediately that
since the magnitude of each element |⟨aℓ, aj⟩| of the vector
Saℓis bounded above by µ. In addition,
µ2(S) ≤δ∗(S).
This is a result of the fact that A∗
Saℓis a submatrix of
S∪{ℓ}AS∪{ℓ} −I for ℓ/∈S, while A∗
S\{ℓ}aℓis a submatrix
SAS −I for ℓ∈S. (They both consist of a subcolumn of
the respective matrix, that “leaves” out the diagonal element.)
We now use these deﬁnitions to bound ∥A†
Proposition 5.2: Let S ⊂{1, . . . , N}. Then:
(a) If A satisﬁes δ∗(S) ≤δ < 1/2 then
for all ℓ/∈S.
(b) If A satisﬁes δ(S) ≤δ < 1 and µ2(S) ≤η then
Denoting by λ an eigenvalue of A∗
deﬁnition of δ(S) ≤δ∗(S) ≤δ implies that |1 −λ| ≤δ.
Consequently, the smallest eigenvalue of A∗
SAS is bounded
from below by 1 −δ and therefore
SAS)−1∥2 ≤
For (a), as already noted above, A∗
Saℓfor ℓ/∈S is a
k × 1 submatrix of A∗
T ∪ℓAT ∪ℓ−I. Therefore, ∥A∗
T ∪ℓAT ∪ℓ−I∥2 ≤δ, and
Saℓ∥2 ≤∥(A∗
SAS)−1∥2∥A∗
The proof of (b) follows from the fact that ∥A∗
Saℓ∥2 ≤µ2(S).
A similar estimate as above yields ∥A†
Saℓ∥2 ≤(1 −δ)−1η.
Proposition 5.2 applies if δk+1 is small while in contrast
Theorem 2.3 works with δ2k, which is generally larger than
δk+1. By (11) the condition δk+1 ≤δ can be satisﬁed if n ≥
Cδk log(N/k). Working with δ∗(S) instead of δk+1 allows to
improve on the bound (11) for Gaussian, Bernoulli and random
spherical matrices.
Proposition 5.3: Let S ⊂{1, . . . , N} be a set of cardinality
k and suppose that A =
√nΦ ∈Rn×N, where Φ is drawn
at random according to a standard Gaussian or Bernoulli
distribution (with expectation 0 and variance 1/n). Then
δ∗(S) ≤δ with probability at least 1 −ǫ provided that
n ≥C1δ−2 max{k log(1/δ), log(N/ǫ)}
for a suitable constant.
The same statement holds (with possibly a different constant) for a random matrix whose columns are chosen independently at random according to the uniform distribution on
Proof: See Appendix II.
A straightforward extension of the proof, as in , also shows
that a random matrix A ∈Rn×N with independent columns
drawn from the uniform distribution on the sphere satisﬁes
RIP, δk ≤δ with probability at least 1 −ǫ provided n ≥
Cδ−2(k log(N/k)+log(ǫ−1)). Although this fact seems to be
known, we are not aware of reference where this is rigorously
The next result relies on a theorem by Tropp [46, Theorem
B] that uses random support sets S and allows to work with
the coherence µ alone. Note that choosing S at random is
perfectly in line with an average-case analysis.
Theorem 5.4: Let A ∈Cn×N have unit norm columns and
coherence µ. Let S ⊂{1, . . ., N} be a set of cardinality k ≥4
chosen uniformly at random. Let δ, ǫ ∈(0, 1) and assume that
µ2k log(ǫ−1) ≤cδ2,
where c = log(2)e−1/2
4·144 log(3) ≈6.64 · 10−4. Then
for all ℓ/∈S
with probability at least 1 −ǫ.
The proof relies on [46, Theorem 12]. The
formulation below follows from by setting s
log(ǫ−1)/ log(k/2) and estimating log(k/2 + 1)/ log(k/2) ≤
log(3)/ log(2) for k ≥4.
Theorem 5.5: Assume A ∈Cn×N has unit norm columns
and coherence µ. Let S ⊂{1, . . . , N} be a set of cardinality
k ≥4 chosen uniformly at random. The condition
144 log(3) log(2)−1µ2k log(ǫ−1) + k
SAS −I∥≥δ) ≤ǫ.
Using (34) and the value of c, the square-root in (36) becomes
δ/(2e1/4). Combining this with (35) shows that (36) is satis-
ﬁed. Therefore, ∥A∗
SAS −I∥2 ≤δ with probability at least
1 −ǫ, which implies that
SAS)−1∥2 ≤
Saℓ∥2 ≤∥(A∗
by using condition (34) once more.
A. Comparison With Worst-Case Results
Our average-case analysis depends on ∥A†
Saℓ∥2, while the
classical condition (6) of Proposition 2.1 depends on ∥A†
and is therefore signiﬁcantly stronger. Proposition 5.2 establishes that the 2-norm condition can be satisﬁed as long as
δk+1 < 1/2. This is clearly weaker than the worst case
condition δ2k <
2 −1 ≈0.41 of Proposition 2.3.
Let us now compare worst-case and average results based
on the coherence µ, by relying on Theorem 5.4. For simplicity,
we consider the case in which A is a unit-norm tight frame,
for which ∥A∥2
n . In this case, (35) is equivalent to k ≤
4e1/4 n. If additionally µ = c/√n, then conditions (34) and
(35) are both satisﬁed for ﬁxed ǫ, δ provided
This beats the square-root bottleneck and even removes the
log-factor present in estimates for the restricted isometry constants, see (11). Moreover, we have the additional advantage
that the coherence is much easier to estimate than the restricted
isometry constants.
Combining Theorem 5.4 with the average-case analysis of
Theorems 4.4 and 4.5 shows that for a unit norm tight frame
A of coherence µ multichannel sparse recovery by (5) can be
ensured in the average-case provided k ≤Cµ−2, which can be
as small as k ≤Cn. Moreover, the failure probability decays
exponentially in the number of channels.
In the next sections we provide further examples when we
discuss particular choices of the matrix A.
VI. COMPARISON WITH MULTICHANNEL GREEDY
ALGORITHMS
We now compare our results regarding ℓ2,1 optimization
to those obtained for the greedy algorithms p-thresholding
and p-SOMP . These are multichannel versions of simple
thresholding and orthogonal matching pursuit. For 1 ≤p ≤∞
they produce a k-sparse signal ˆX from measurements Y =
AX using a greedy search. To this end, we improve slightly
on previous average-case performance results in for these
algorithms in the noiseless setting.
A. Greedy Methods
In p-thresholding, we select a set S of k indices whose
p-correlation with Y are among the k largest:
ℓY ∥p ≥∥a∗
∀ℓ∈S, ∀j /∈S.
After the support S is determined, the non-zero coefﬁcients of
ˆX are computed via an orthogonal projection: ˆXS = A†
The p-SOMP algorithm is an iterative procedure. At each
iteration, an atom index ℓm is selected, and a residual is
updated. At the ﬁrst iteration the residual is simply Y0 = Y .
After M iterations, the set of selected atoms being SM =
m=1, the new residual is computed as YM
ASM XM = (I −PSM )Y where XM = A†
SM Y and PSM =
SM is the orthogonal projection onto the linear span of
the selected atoms. The next selected atom kM+1 is the one
which maximizes the p-correlation with the residual YM,
ℓM+1YM∥p = max
Using the probability model (21) average-case recovery
theorems for p-thresholding and p-SOMP have been proven
in , [24, Theorems 4,6,7,8]. We improve slightly on these
in the following. (Note, however, that also treats the
noisy case.) Our ﬁrst result generalizes the one in to the
multichannel setup.
Theorem 6.1: Let A ∈Cn×N have unit norm columns and
local 2-coherence function µ2(S) deﬁned in (30). Let X ∈
RN×L with supp X ⊂S where S ⊂{1, . . . , N}, and such
that the coefﬁcients on S are given by (21), XS = ΣΦ, where
we choose the real spherical model for Φ. Set Y = AX and
R = maxj σj/ minj σj. If
θ = Rµ2(S) < 1,
then the probability that 2-thresholding applied to Y fails to
recover X is bounded by
 −L/2(θ−2 −log(θ−2) −1)
If we use the complex spherical model instead of the real
spherical model then L/2 in the above probability estimate
may be replaced by L.
The probability bound of Theorem 6.1 is similar to that
of Theorem 4.4. However, in contrast to our results for ℓ2,1minimization, success of thresholding suffers a dependency
on the diagonal matrix Σ. The larger the ratio R, the stronger
the condition (39) on the maximal allowed sparsity k, and the
larger the probability of error.
Proof: We proceed similarly as in . We denote by Θ
the event that 2-thresholding fails. Clearly,
P(Θ) = P(min
i Y ∥2 < max
i Y ∥2 < ρ) + P(max
ℓY ∥2 > ρ),
where ρ will be speciﬁed later. Denote by Zj, j ∈S, a
sequence of independent random vectors which are uniformly
distributed on the unit sphere of RL. Then,
i Y ∥2 < ρ) = P
σj⟨ai, aj⟩Z∗
Substituting into (40),
i Y ∥2 < ρ)
Choosing ρ = σmin/2 and applying Theorem 4.2 we obtain
i Y ∥2 < ρ)
≤k exp(−L/2(θ−2 −log(θ−2) −1))
where we used the deﬁnition of θ and µ2(S). Similarly we
ℓY ∥2 > σmin/2)
≤(N −k) exp(−L/2(θ−2 −log(θ−2) −1)).
Combining the two estimates completes the proof for the real
case. Choosing the vectors Zj, j ∈S, from the complex unit
C and using Corollary 4.3 yields the statement for
the complex case.
We now state the corresponding result for 2-SOMP, which
slightly improves the one in for the noiseless case. (Note
that we restrict to p = 2 here, although the theorem is easily
extended to general values of p.)
Theorem 6.2: Let A be a matrix with unit norm columns
and constants δ(S), µ2(S) < 1 where S ⊂{1, . . . , N}.
Assume that
µ2(S)2 + (1 + ǫ)(1 −ǫ)−1µ2(S)
for some ǫ ∈(0, 1). Let X be a random coefﬁcient matrix
with support S that is selected according to the real Gaussian
probability model, see (21), and let Y = AX. Then 2-SOMP
applied to Y recovers X in k steps with probability at least
1 −N2k exp(−ǫ2A2
where AL ∼
L is given by (25).
If we use the complex Gaussian model instead of the
real Gaussian model then the same conclusion holds with L
replaced by 2L in (42).
Proof: See Appendix III.
Remark 6.3: (a) Due to the factor 2k the probability
bound (42) becomes effective only when the number
of channels becomes comparable to the sparsity k. This
drawback is very likely due to the analysis and is not
observed in practice. However, it seems to be very
difﬁcult to remove this factor by a more sophisticated
proof technique.
(b) We require ǫ < 1, so that the probability decay of (42)
is potentially slower than that given by Theorem 4.4.
(c) With δ = ǫ = 1/2 condition (41) is satisﬁed if µ2(Λ) ≤
1/7 while the probability estimate (42) behaves like 1 −
N2k exp(−L/4).
(d) With the estimates δ(S) ≤δ∗(S) and µ2(S) ≤δ∗(S),
(41) with ǫ = 3/11 is implied by
δ∗(S) < 1/3.
(e) By Proposition 5.2 the condition δ∗(S) < 1/3 implies
Saℓ∥2 ≤1/2 for all ℓ/∈S, i.e., the bounded norm
condition ( 22) of the average case recovery result for
mixed ℓ2,1. In other words, the condition in (d) for SOMP
is slightly stronger than the one for ℓ2,1.
B. Comparison
We now compare the average-case recovery conditions for
mixed ℓ2,1, thresholding and SOMP for the following choices
of the matrix A which we will also use in the numerical
experiments:
1) Random spherical ensemble;
2) Union of Dirac and Fourier;
3) Time-Frequency shifts of the Alltop window.
1) Random spherical ensemble: Assume that the random
columns of A
Rn×N are independent and uniformly
distributed on the sphere Sn−1. Let S be a support set of size
k. Then according to Proposition 5.2 the condition ∥A†
α < 1 of Theorem 4.4 is implied by δ∗(S) ≤
1+α < 1/2,
while by Proposition 5.3 the latter holds with probability at
least 1 −ǫ provided
n ≥max{C1(α)k, C2(α) log(N/ǫ)}.
Assuming, for example, α = 1/4, under the probability
model (21), the probability that reconstruction by ℓ2,1 fails
is bounded from above by N exp(−L/2(15 −log(16)))+ǫ =
N exp(−cL) + ǫ with c ≈6.1137.
We now compare this result with the condition of Theorem
6.1 concerning thresholding. As noted in (32), µ2(S) ≤δ∗(S).
Therefore, by Proposition 5.3 we have
θ = 2Rµ2(S) ≤2Rδ∗(S) < 1
with probability at least 1 −ǫ provided
θ2 max {k log(R/θ), log(N/ǫ)}
and the failure probability of thresholding is bounded by
N exp(−L/2(θ−2 −log(θ−2) −1)) + ǫ.
Let us ﬁnally consider Theorem 6.2 for SOMP. By Proposition 5.3 the condition δ∗(S) < 1/3 in Remark 6.3 is satisﬁed
with probability at least 1 −ǫ provided
n ≥max {C1k, C2 log(N/ǫ)}
and the failure probability of SOMP is bounded by
N2k exp(−9/121 A2
L ∼L if the real Gaussian probability model is used.
Conditions (43), (44), (45) for ℓ2,1, thresholding and SOMP
are rather similar. However, condition (44) for thresholding
involves the ratio R. If R is large then thresholding behaves
much worse compared to ℓ2,1 and SOMP. The probability
estimate (46) is the worst compared to the other two algorithms
due to the factor 2k. Therefore, ℓ2,1 gives the best known
theoretical average case result.
2) Union of Dirac and Fourier: Consider the n×2n matrix
A = (I|F), where I is the n × n identity matrix and F is
the normalized n × n Fourier matrix. The coherence of A is
easily seen to be µ = 1/√n. By Proposition 5.1 condition
Saℓ∥2 ≤α with α = 1/2 is satisﬁed for all support
sets S of cardinality at most k provided
If S is chosen at random then a much better bound (up to
constants) is obtained using Theorem 5.4. In our special case,
however, further improvement is possible. A reformulation
of a result of , see also [46, Proposition 3] shows the
following. If the support S consists of k1 arbitrary elements of
{1, . . ., n} and k2 random elements of {n + 1, . . . , 2n} then
with probability at least 1 −ǫ we have δ(S) ≤1/2 provided
k = k1 + k2 ≤
with c = 0.25. In particular k ≤n/4 and the same reasoning
as in the proof of Theorem 5.4 yields
Saℓ∥2 ≤α = 1/2.
Using one of the complex probability models in Theorem
4.4, the failure probability of ℓ2,1-minimization is bounded by
N exp(−L(4 −log(4) −1)) = N exp(−cL) with c ≈1.61.
To compute the performance of thresholding, note that
condition (39), 2Rµ2(S) ≤2Rµ
k ≤θ < 1, is satisﬁed
Assuming that the non-zero rows of the matrix Φ in the
probability model (21) on the coefﬁcients are independent and uniformly distributed on the complex unit sphere
, the failure probability of thresholding is bounded by
N exp(−L(θ−2 −log(θ−2) −1)).
Assuming δ(S) ≤δ = 1/2 and µ
k ≤1/7, i.e.,
the condition of Remark 6.3(c) is satisﬁed since by (32),
k ≤1/7. Then by Theorem 6.2 SOMP fails with
probability at most N2k exp(−A2
2L/4) assuming the complex
Gaussian probability model. Assuming as in the discussion
of ℓ2,1 that the support set is such that k1 arbitrary elements
of {1, . . ., n} and k2 random elements of {n + 1, . . . , 2n}
are chosen with k = k1 + k2 then the assumed condition
δ(S) ≤1/2 is true with probability at least 1 −ǫ provided
(47) holds.
Similar conclusions on the comparison of the three algorithms as in the previous example apply. We note, however,
that in contrast to ℓ2,1 and SOMP, the performance bound
for thresholding does not require a probability model on the
support set S.
3) Time-Frequency shifts of Alltop window: Let n ≥5 be
a prime. Denote by (Trg)ℓ= gℓ−r mod n and (Msg)ℓ=
e2πisℓ/ngℓthe cyclic shift and modulation operator, respectively. Then TrMs, r, s = 0, . . . , n −1 forms the set of timefrequency shifts. Let gℓ=
√ne2πiℓ3/n be the so-called Alltop
window. Then deﬁne A to be the n × n2 matrix with columns
being the time-frequency shifts TrMsg, r, s = 0, . . . , n −1.
The coherence of A is µ = 1/√n .
As in the Fourier-Dirac case, under condition (48) and the
complex probability model of Theorem 6.1, thresholding fails
with probability at most N exp(−L(θ−2 −log(θ−2) −1)).
For the analysis of ℓ2,1 and SOMP we assume that the
support S is chosen uniformly at random. As A is the union
of n orthonormal bases we have ∥A∥2
2 = n. Then choosing
δ = 3/4 in Theorem 5.4 yields that under the condition
n ≥Ck log(ǫ−1)
with a constant C (which also implies (35)) we have
Saℓ∥2 ≤3√c log−1/2(ǫ−1) ≤α
for all ℓ/∈S
with probability at least 1 −ǫ where α = 3√c ≈0.0773. By
Theorem 4.4, using one of the complex probability models, the
failure probability of ℓ2,1 is then bounded by N exp(−c2L)+ǫ
with c2 = α−2 −log(α−2) −1.
For the analysis of SOMP we choose δ = 1/2 in Theorem 5.5. Assuming that the square-root in (36) is less than
2 is equivalent to
n ≥Ck log(ǫ−1)
with an appropriate C, and condition (36) is satisﬁed. Then
with probability at least 1 −ǫ we have δ∗(S) ≤1/2.
Furthermore, as suggested by Remark 6.3(b) the condition
µ2(S) ≤1/12 is also implied by (50) since µ2(S) ≤
n. Assuming the complex Gaussian probability model on
the non-zero coefﬁcients of X the failure probability of SOMP
is bounded by N2k exp(−A2
2L/2) + ǫ due to Theorem 6.2.
VII. NUMERICAL SIMULATIONS
We tested the three algorithms ℓ2,1 minimization, thresholding and SOMP using the three different types of matrices
indicated in the previous section. The support set S of the
sparse coefﬁcient matrices X was always selected uniformly
at random while the non-zero coefﬁcients were selected at
random using one of the following choices of the probability
model (21), XS = ΣΦ:
1) Φ is chosen to be a real Gaussian random matrix (i.e., all
entries independent and standard normally distributed);
Σ has independent diagonal entries with standard normal
distribution.
2) Φ is chosen to be a complex Gaussian random matrix
(i.e., the real and imaginary parts of each entry are
chosen independently according to a standard normal
distribution); Σ is equal to the identity.
Note that Σ = I is favorable for thresholding, while the choice
of Σ should have no inﬂuence on the performance of ℓ2,1 and
only a mild inﬂuence on SOMP.
In the following ﬁgures the results of various simulation
runs are plotted (we always used 100 simulations for each
choice of parameters).
In Fig. 1 we plot the results when choosing A from a
random spherical ensemble of size n = 32 columns and
N = 256 rows for L = 1, 2, 4. The matrix X was generated
according to model (1). The improvement with increasing L
is clearly evident.
Sparsity Level
Empirical Recovery Rate
Recovery Rate for l2,1
Sparsity Level
Empirical Recovery Rate
Recovery Rate for SOMP
Sparsity Level
Empirical Recovery Rate
Recovery Rate for Thresholding
Multichannel recovery with X generated according to model (1)
and A chosen from a random spherical ensemble, (a) ℓ2,1, (b) SOMP, (c)
Thresholding.
In Fig. 2 we consider all three methods when A is a union
of Dirac and Fourier bases, each with 32 elements. Therefore,
n = 32 and N = 64. The matrix X was generated according
to model (2). In this setting the performance using thresholding
is reasonable, though still worse than ℓ2,1 and SOMP.
Sparsity Level
Empirical Recovery Rate
Recovery Rate for l2,1
Sparsity Level
Empirical Recovery Rate
Recovery Rate for SOMP
Sparsity Level
Empirical Recovery Rate
Recovery Rate for Thresholding
Multichannel recovery with X generated according to model (2)
and A a union of the Dirac and Fourier bases, (a) ℓ2,1, (b) SOMP, (c)
Thresholding.
Finally, in Fig. 3 we plot the results when using timefrequency shifts of the Alltop window with n = 29 and N =
292 = 841. Here the results of thresholding are extremely poor
and therefore not plotted.
In all three cases, SOMP performs better than the ℓ2,1
approach. However, both show clear performance advantage
with increasing L.
Sparsity Level
Empirical Recovery Rate
Recovery Rate for l2,1
Sparsity Level
Empirical Recovery Rate
Recovery Rate for SOMP
Multichannel recovery with X generated according to model (2) and
A chosen as time-frequency shifts of the Alltop function (a) ℓ2,1 (b) SOMP.
VIII. CONCLUSION
In this paper we analyzed the average-case performance
of ℓ2,1 recovery of multichannel signals. Our main result is
that under mild conditions on the sparsity and measurement
matrix, the probability of failure decays exponentially with
the number of channels. To develop this result we assumed
a probability model on the non-zero coefﬁcients of a jointly
sparse signal. The results we obtained appear to be the bestknown theoretical results on multichannel recovery. Using
the tools we developed for analyzing the ℓ2,1 approach, we
also improved slightly on previous performance bounds for
thresholding and SOMP.
APPENDIX I
PROOF OF THEOREM 4.2
The proof uses the following extension of Khintchine’s
inequality to higher dimensions stated in ,
for all p ≥2 and all vectors a ∈Rk. By splitting in real and
imaginary parts it easily follows that this inequality also holds
for all a ∈Ck. We may assume without loss of generality that
∥a∥2 = 1. Then an application of Markov’s inequality yields
≥exp(λLu2/2)
≤exp(−λLu2/2)E
= exp(−λLu2/2)
≤exp(−λLu2/2)
λi Γ(L/2 + i)
= exp(−λLu2/2)
= exp(−λLu2/2)
(1 −λ)L/2 ,
where (a)i = a(a + 1)(a + 2) · · · (a + i −1) denotes the
Pochhammer symbol. The last equation is due to the fact that
i! λi is the Taylor series of (1−λ)−a, which converges
for λ < 1. Minimizing (51) with respect to λ gives λ = 1 −
u−2. Inserting this value yields the statement of the theorem.
APPENDIX II
PROOF OF PROPOSITION 5.3
Consider ﬁrst the case of Gaussian or Bernoulli matrices.
According to Theorem 2.1 in (see also Lemma 5.1 in ),
we have ∥A∗
SAS −I∥2 ≥δ with probability at most 2(1 +
12/δ)k exp(−c0/9nδ2) with c0 = 7/18. A similar estimate
holds for ∥A∗
S∪ℓAS∪ℓ−I∥2 with ℓ/∈S. A union bound over
all ℓ/∈S yields δ∗(S) ≥δ with probability at most 2N(1 +
12/δ)k exp(−c0/9nδ2). This term is less than ǫ if (33) holds.
Now consider a random matrix Ψ ∈Rn×N with independent columns that are uniformly distributed on the sphere
Sn−1. Then Ψ has the same distribution as DA, where A
is Gaussian matrix as above, D = diag(s−1
1 , . . . , s−1
sj = √n∥Φj∥2 where Φj ∈Rn is a vector of independent
standard normally-distributed random variables. We now use
the following measure concentration inequality [3, Corollary
(2.3)] or [4, eq. (2.6)] for a standard Gaussian vector Z ∈Rn,
1 −γ ) ≤exp(−γ2n/4),
2 ≤(1 −γ)n) ≤exp(−γ2n/4).
By a union bound this implies that
j=1,...,N s2
j=1,...,N s2
≥1 −2N exp(−γ2n/4).
By the above reasoning, we have (1 −δ/3)∥x∥2
2 ≤∥Ax∥2 ≤
(1 + δ/3)∥x∥2
2 for all x with supp x ⊂S ∪{ℓ} for some
ℓ/∈S with probability at least 1 −ǫ provided (33) holds with
a suitable constant. If additionally 1 −γ ≤minj=1,...,N s2
maxj=1,...,N s2
1−γ for γ = δ/4 then (1 −δ)∥x∥2
2 ≤(1 + δ)∥x∥2
2 for all x with supp x ⊂
S ∪{ℓ} for some ℓ/∈S. By a union bound and (52) this
holds with probability at least 1 −2ǫ provided (33) holds and
2N exp(−δ2n/64) ≤ǫ, the latter being equivalent to n ≥
64δ2 log(2N/ǫ). Adjusting the constant in (33) completes the
APPENDIX III
PROOF OF THEOREM 6.2
We assume that until a certain step SOMP has selected only
correct indices, collected in J ⊂S. Let us ﬁrst estimate the
probability that it selects a correct element of S \ J also in
the next step.
We denote by PJ = AJA†
J the orthogonal projection onto
the span of the columns of A in J, and QJ = I −PJ. The
residual at the current iteration is given by YM = QJY =
QJASX = QJASΣΦ. SOMP selects a correct index in S \ J
in the next step if
ℓQJASΣΦ∥2 > max
ℓQJASΣΦ∥2.
By Theorem 11 in (which is proven using Theorem 4.6;
note that there is a slight error in in the computation
of the constant AL) we have the following concentration of
measure inequalities
ℓQJASΣΦ∥2 < (1 + ǫ)C2(L)×
≤exp(−ǫ2A2
ℓQJASΣΦ∥2 > (1 −ǫ)C2(L)×
≤|Sc| exp(−ǫ2A2
where AL is the constant in (25) and C2(L) = E∥Z∥2 with
Z = (Z1, . . . , ZL) being a vector of independent standard
normal variables. Now we assume that
(1 + ǫ)C2(L) max
≥(1 −ǫ)C2(L) max
Then by the above and a union bound the probability that
SOMP fails can be bounded by
ℓQJASΣΦ∥2 ≤max
ℓQJASΣΦ∥2)
≤(|Sc| + 1) exp(−ǫ2A2
Let us consider now the maximum on the right hand side of
(54). First note that PJaℓ= aℓfor all ℓ∈J, in other words
QJaℓ= 0. Hence, we can estimate
ℓ/∈S ∥ΣS\JA∗
j |⟨QJaj, aℓ⟩|2
|⟨QJaj, aℓ⟩|2.
Furthermore, for ℓ/∈S we have
|⟨QJaj, aℓ⟩|2
S\J(I −PJ)aℓ∥2
S\Jaℓ∥2 + ∥A∗
≤µ2(S \ J) + ∥A∗
S\JAJ∥2∥(A∗
JAJ)−1∥2∥A∗
1 −δ(S)µ2(S) =
where we used the fact that A∗
S\JAJ is a submatrix of A∗
Next we consider the maximum on the left hand side of
(54). We can estimate
j |⟨QJaℓ, aj⟩|2
j∈S\J |⟨QJaj, aj⟩|2.
Furthermore, for j ∈S \ J
|⟨QJaj, aj⟩| = |⟨(I −PJ)aj, aj⟩|
≥1 −µ2(S)2(1 −δ(S))−1.
Combining the above estimates, condition (54) is satisﬁed if
(1 + ǫ) µ2(S)
1 −δ(S) ≥(1 −ǫ)
which is equivalent to (41).
In order to complete the proof, we note that OMP successfully recovers the correct signal if (54) holds for all J ⊂S.
By a union bound of (55) over all those 2k subsets this is
true with probability at least 1 −N2k exp(−ǫ2A2
L) provided
condition (41) holds.
The extension to the complex valued case is straightforward.