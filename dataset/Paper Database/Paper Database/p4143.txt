HAL Id: hal-02170808
 
Submitted on 2 Jul 2019
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Data-Driven Deterministic Symbolic Regression of
Nonlinear Stress-Strain Relation for RANS Turbulence
Martin Schmelzer, Richard Dwight, Paola Cinnella
To cite this version:
Martin Schmelzer, Richard Dwight, Paola Cinnella. Data-Driven Deterministic Symbolic Regression of
Nonlinear Stress-Strain Relation for RANS Turbulence Modelling. 2018 Fluid Dynamics Conference,
2018, Atlanta, Georgia, United States. pp.AIAA 2018-2900, ￿10.2514/6.2018-2900￿. ￿hal-02170808￿
Data-Driven Deterministic Symbolic Regression of Nonlinear
Stress-Strain Relation for RANS Turbulence Modelling
Martin Schmelzer* and Richard Dwight†
Faculty of Aerospace Engineering, Section of Aerodynamics, Delft University of Technology, The Netherlands
Paola Cinnella‡
Laboratoire DynFluid, Arts et Métiers ParisTech, Paris, France
This work presents developments towards a deterministic symbolic regression method to
derive algebraic Reynolds-stress models for the Reynolds-Averaged Navier-Stokes (RANS)
equations. The models are written as tensor polynomials, for which optimal coeﬃcients are
found using Bayesian inversion. These coeﬃcient ﬁelds are the targets for the symbolic regression. A method is presented based on a regularisation strategy in order to promote sparsity
of the inferred models and is applied to high-ﬁdelity data. By being data-driven the method
reduces the assumptions commonly made in the process of model development in order to
increase the predictive ﬁdelity of algebraic models.
I. Introduction
Turbulence modelling is a key-challenge for computational ﬂuid dynamics (CFD) especially in industry. Despite
considerable progress made in the ﬁeld of high-ﬁdelity turbulence modelling, such as large-eddy simulation (LES) and
direct numerical simulation (DNS), RANS continues to be the standard approach used to predict a wide range of ﬂows
 . However, using the less-computationally demanding RANS approach comes at the price of uncertainty due to
approximate physical modelling of the turbulence closure.
Despite Reynolds-stress models (RSM), based on transport equations for the Reynolds-stress tensor components,
oﬀer the more elaborate second-moment closures for RANS, modelling of industrial ﬂows is mainly done using linear
eddy viscosity models (LEVM) . This is due to the fact that RSM involve a more complex mathematical structure,
which requires additional modelling and introduces computational challenges, whilst the method does not oﬀer a
superior predictive capability for all ﬂows . However, for ﬂows with streamline curvature, adverse pressure gradients,
ﬂow separation or rotation LEVM do not deliver reliable predictions due to their inherent inability to predict the
anisotropy of the Reynolds-stress correctly. Explicit Algebraic Reynolds-stress Models (EARSM), ﬁrst introduced
by Pope and further developed by Gatski and Speziale , have the potential to ﬁll the gap by oﬀering higher
predictive ﬁdelity compared to LEVM and being numerically more robust than RSM at similar computational costs as
LEVM . EARSM are derived from a projection of (simpliﬁed) RSM onto a set of tensorial polynomials (see below)
 , leading to a non-linear stress-strain relation for the Reynolds-stress tensor. As such, these models can be seen as
higher-order extensions of the linear eddy viscosity concept. The main simpliﬁcation is the omission of anisotropy
transport in order to enhance the numerical robustness, which however also reduces the predictive potential of this
modelling strategy .
Recently a new approach based on symbolic regression utilising genetic programming (GP) was introduced to learn
the non-linear stress-strain relationship for the anisotropy tensor based on high-ﬁdelity data from DNS and LES .
This data-driven method retains the input quantities used to derive EARSM but replaces the commonly used projection
method to ﬁnd the formal structure of the model by an evolutionary process based on model ﬁtness. In that way it
produces models similar to EARSM but with a mathematical form proven to reproduce the data it was trained on. This
method has the potential to generate numerically robust models with a high predictive ﬁdelity. Even though the method
is non-deterministic it discovers similar expressions for diﬀerent runs. However, it is not clear if this variability comes
from the data or is due to the inherent randomness of GP.
To overcome this characteristic of GP non-evolutionary methods for symbolic regression have been introduced,
e.g. Fast Function Extraction (FFX) , Elite Bases Regression (EBR) , Sparse identiﬁcation of nonlinear
*PhD Candidate, AWEP Department, TU Delft, Kluyverweg 2, 2629 HS Delft, The Netherlands.
†Associate Professor, AWEP Department, TU Delft, Kluyverweg 2, 2629 HS Delft, The Netherlands.
‡Professor, Laboratoire DynFluid, 151 Boulevard de l’Hopital, 75013 Paris, France.
dynamics (SINDy) or PDE functional identiﬁcation of nonlinear dynamics (PDE-FIND) . These methods
are all based on sparsity-promoting linear regression and show for a couple of cases similar or better performance
and higher convergence rates for high-dimensional problems than GP. Due to their deterministic nature, they discover
always the same model given the input quantities and parameters. Additionally, by varying the input parameters of
the method, a hierarchy of models of diﬀerent complexity and predictive ﬁdelity are discovered, which can be used to
assess overﬁtting for prediction.
In this work a deterministic version of symbolic regression is introduced to learn models for the nonlinear stressstrain relation. Our aim is to identify models given a set of diﬀerent input features without restricting the search by too
many modelling assumptions.
II. Nonlinear constitutive stress-strain relation
The challenge in RANS-based turbulence modelling is the closure of the RANS equations by a model for the
Reynolds-stress tensor τi j. This symmetric second order tensor can be decomposed into
τi j = 2k(ai j + 1
in which ai j represents the non-dimensionalised anisotropic part and k the turbulent kinetic energy the isotropic part.
Based on the decomposition of the mean velocity gradient tensor into the symmetric and the anti-symmetric part
∂jUi = si j + ωi j,
with the mean strain-rate tensor si j = 1
2 (∂jUi + ∂iUj) and the rotation rate tensor ωi j = 1
2 (∂jUi −∂iUj), Pope was
the ﬁrst to use the Cayley-Hamilton theorem in order to derive an integrity basis of ten nonlinear base tensors and ﬁve
corresponding invariants . Given a turbulent time scale τ to non-dimensionalise Si j = τsi j and Ωi j = τωi j the base
tensors can be combined linearly as a functional form for ai j:
ai j (Si j,Ωi j) =
The base tensors are
i j = Si j, T2
i j = SikΩk j = Ωik Sk j,
i j = Sik Sk j −1
3δi jSmnSnm,
i j = ΩikΩk j −1
3δi jΩmnΩnm,
i j = Ωik Skl Sl j −Sik SklΩl j,
i j = ΩikΩkl Sl j + SikΩklΩl j −2
3 SklΩlmΩmk,
i j = Ωik SkmΩmnΩn j −ΩikΩkmSmnΩn j,
i j = SikΩkmSmnSn j −Sik SkmΩmnSn j,
i j = ΩikΩkmSmnSn j + Sik SkmΩmnΩn j −2
3 SkmSmnΩnpΩpk,
i j = Ωik SkmSmnΩnpΩp j −ΩikΩkmSmnSnpΩp j
and the corresponding invariants read
I1 = SmnSnm, I2 = ΩmnΩnm, I3 = SkmSmnSnk
I4 = ΩkmΩmnSnk, I5 = ΩkmΩmnSnpSpk.
While the combination of base tensors is linear, according to Pope’s analysis the scalar coeﬃcient ﬁelds αn are
nonlinear functions of the invariants, i.e. αn = αn(I1,...,I5). The identiﬁcation of the functional form of the coeﬃcients
is the essential step to build a nonlinear eddy-viscosity model. Classical methods to identify the functional forms are
given in . As mentioned above these methods are based on modelling assumptions such as the weak equilibrium
hypothesis, which omits the transport of anisotropy . In the following, optimal scalar ﬁelds αn are identiﬁed for the
given test cases and further used to learn a functional form for αn using symbolic regression.
III. Test case, high-ﬁdelity data and solver
The test case, for which we want to infer and test correction models, is the ﬂow over a series of periodic hills in 2D
 , the geometry is shown in Fig. 1. The ﬂow exhibits separation on the leeward side of the hills and reattachment in
between. Both features are challenging to predict by LEV models . We consider two diﬀerent Reynolds-numbers
ReH = 2800 and 10595 based on the bulk-velocity at the hill’s crest and the hill height. At both the lower as well as
the upper wall a no-slip boundary condition is applied. The ﬂow setup is periodic in the stream-wise direction and
driven with a volume forcing. The CFD solver used is simpleFoam from the OpenFOAM toolbox . The inferred
correction models resulting from the proposed learning method are implemented in a new turbulence model based on the
k −ω model. The high-ﬁdelity data used for model-learning and validation are mean velocity ﬁelds and Reynolds-stress
ﬁelds from Breuer et al. . More precisely, DNS data for the lower Reynolds number case and LES data for the
higher Reynolds number one.
Stream-wise velocity of ﬂow over periodic hills in 2D at ReH = 10595. LES data from .
IV. Optimal coeﬃcients for the nonlinear constitutive relation
As the modelling of the anisotropy of the Reynolds-stress tensor using the linear stress-strain relation is insuﬃcient,
we introduce an additive term b∆
i j to account for the model-form error
τi j = −2νt Si j + 2
3 kδi j + 2kb∆
which is a non-dimensional symmetric tensor with zero trace. The mean strain-rate Si j, the Reynolds-stress tensor
τi j and the turbulent kinetic energy k are directly available from high-ﬁdelity simulations (LES or DNS) or from
experiments. The eddy viscosity νt can be obtained by passively solving the turbulent transport equations of a turbulence
model, e.g. k −ω, for k and the mean velocity ﬁeld Ui (frozen approach) . The model-form error b∆
i j is then readily
computable as a residual given the equation above.
The obtained model-form error is case-dependent, meaning that it only corrects the stress-strain relation for a given
setup (geometry, boundary conditions, Reynolds number, etc.). The key is to distill information from it to identify a
suitable mathematical expression as a corrective term, which can be used for prediction of other ﬂow cases. For that we
identify the coeﬃcient ﬁelds of the base tensor series given input data b∆
i j. The relation between the data and the base
tensor series is expressed by a statistical model
b∆= Ha + ϵ,
in which the sparse block-diagonal matrix H ∈R6K×M with M = N × K contains the base tensors ordered per
component and spatial location k = 1,...,K


and the vectors a ∈RM and b∆∈R6K contain the stacked coeﬃcient and tensor ﬁelds as well ordered per component
and spatial location, respectively:
a = (α1,1,. . . ,αN,1,α1,2,. . . ,αN,K )T,
xx,2,. . . ,b∆
The random variable ϵ serves as an additive error term expressing the fact, that given the best set of coeﬃcients
αn,k the base tensor series might not ﬁt the data perfectly. As the input data (Reynolds-stress and mean velocity ﬁeld) is
relatively smooth spatially, we also want to infer smooth coeﬃcients ﬁelds. In a preliminary study utilising ordinary
least-square regression, we discovered rough coeﬃcient ﬁelds, in which the values next to each other diﬀered several
orders of magnitude, which is a common issue for inverse problems . While these coeﬃcients lead to a very low
reconstruction error of the anisotropic Reynolds-stress, they are discarded as they are not useful to serve as a target
for the symbolic regression. A lack of smoothness leads to unreasonably complex models, with an increased risk of
overﬁtting the rough coeﬃcient ﬁelds. Furthermore, more complex correction models also increase the numerical
stiﬀness of the CFD solver leading to a reduction or loss of numerical stability. Even though the smoothness constraint
increases the a priori reconstruction error, it enables the discovery of sparse correction models.
The inference of the coeﬃcients given the data b∆is done via Bayesian inversion 
p(a|b∆) ∝L(b∆|a) p(a)
in which the posterior probability p(a|b∆) is the product of the likelihood L(b∆|a) based on the statistical model
deﬁned above and the prior probability p(a), which embodies the smoothness constraint. As the dimension of this
problem depends on the mesh size, which is already large for the considered 2D ﬂow case (18000 cells), we focus on
the inference of the maximum a priori estimation (MAP) of the posterior
aM AP = arg max
and not the full posterior distribution. We further simplify the problem by assuming that both the likelihood function
and the prior are normally distributed
L(b∆|a) ∝exp
2 (b∆−Ha)T R−1(b∆−Ha)
2 (a −ao)T P−1(a −ao)
Thus, also the posterior is normally distributed and, under the assumption of a prior zero mean ao = 0, it reads
p(a|b∆) ∝exp
2 (a −aM AP)T Σ−1(a −aM AP)
with mean aM AP and covariance matrix Σ
aM AP = Kb∆,
Σ = (I −KH)P,
K = PHT (R + HPHT )−1
the latter is known as the Kalman gain . In the present study the observation covariance matrix R ∈R6K is assumed
to be diagonal R = σRI implying that the underlying spatial correlation of the data b∆is omitted. Further work will
also use other covariance structures. The prior covariance P ∈RM acts to regularise the inversion problem in order to
obey the smoothness constraint on the coeﬃcient ﬁelds and is deﬁned as
P(x, x′) =
otherwise,
in which σP is the scalar variance, x represents the coordinates of location within the mesh and L is the correlation
length. The Gaussian kernel is a common choice resulting in a smooth correlation function. With a correlation length
shorter then the smallest cell size the MAP-inversion framework reduces to a regularised least-squares problem, while a
larger correlations length increases both the smoothness of the coeﬃcients and the reconstruction error.
Given values for σR, σP and L we are now able to compute aM AP with (15). As the size of the problem and
therefore the size of the matrices present in the Kalman gain in (17) scales with the number of mesh points of the ﬂow
case at hand, we need to exploit the sparsity of the matrices in order to store them eﬃciently and make the computations
tractable. Due to their structure deﬁned above especially the memory requirements for H and R can be drastically
reduced. The density of P given (18) is controlled by a cut-oﬀfor small-sized correlations. As it depends on the
correlation length L, also its memory requirements increase with a larger correlations length. For the periodic hill test
case we have tested L = 0.01 and L = 0.001, see Fig. 2a and 2b respectively. In these ﬁgures, the reconstruction error
(b∆−H ˆa)T (b∆−H ˆa)
g0.5 for an inferred set of coeﬃcients ˆa is shown against an empirical roughness criterion
s = (|∇ˆa0|)max
(|∇ˆa0|)mean
corresponding to the maximum of the gradient of ˆa0 = ˆα0,k normalised by its mean value. Unregularised inversion is
prone to small diﬀerences in the data leading to large discrepancies in the inferred coeﬃcients , i.e. localised large
gradients. The rationale of the criterion is therefore to identify if unphysically large gradients are present for a given
regularisation strategy. In general, the lower s is, the lower are the highest gradients, which contributes to smoother
coeﬃcient ﬁelds. The coloured marker-lines represent a diﬀerent number of employed base tensors ranging from N = 2
to 5, which is a common choice for EARSM for 2D , and the scalar variances of P and R are summarised to
equivalent to a Tikhonov regularisation parameter in case of regularised least-square regression .
reconstruction error
(a) L = 0.01
reconstruction error
(b) L = 0.001
Reconstruction error versus roughness metric s for two diﬀerent correlation length L. The number of
base tensors N are indicated by coloured marker lines. The regularisation parameter λ are associated to the
points next to the mentioned value.
In general, the lower the regularisation parameter λ, the lower the reconstruction error and the larger s, and vice
versa. For λ ≤0.001 the reconstruction error decreases while the roughness increases more than one order of magnitude.
By increasing the number of base tensors from N = 2 to N = 3 the reduction of the reconstruction error is signiﬁcant
for a given λ, but only minor for a further expansion. This implies that for a correction model at least N = 3 should
be considered. For λ ≥0.001 the reconstruction error increases but the smoothness settles, also the number of base
tensors is not important anymore in terms of the reconstruction error, meaning that the problem is dominated by the
regularisation.
Changing the correlation length (L = 0.01 in Fig.2a, L = 0.001 in Fig.2b) decreases the roughness of the coeﬃcient
ﬁelds slightly, but has a minor eﬀect overall. We have observed for even larger correlation lengths a further decrease in
roughness for a given λ, but also an increase in the reconstruction error. Smoothing the coeﬃcient ﬁelds by increasing
L is therefore detrimental to the endeavour of designing a prior to steer the inversion towards low roughness and low
reconstruction error (lower-left corner of the Fig. 2).
Finally, the preceding parametric study helps us in identifying suitable coeﬃcient ﬁelds to serve as targets for the
symbolic regression. We identify coeﬃcient ﬁelds given L = 0.01 and λ = 0.0001 corresponding to a reasonable
compromise between low bias according to a low reconstruction error and low roughness s. Further research will focus
on a rigorous optimisation of the parameters λ and L to ﬁnd an optimum of low reconstruction error and low roughness.
V. Deterministic Symbolic Regression of Correction Models
This Section deals with the methodology of deterministic symbolic regression. First, it is explained how a library
of candidate functions is build automatically given raw input features and a set of mathematical operations. Second,
details are given on the algorithm to identify the best linear combination of as few as possible candidate functions to ﬁt
the target coeﬃcient ﬁelds, which were identiﬁed in the preceding section.
A. Building a library of candidate functions
The deterministic symbolic regression constructs an over-complete library of possible nonlinear candidate functions
B approximating given data and identiﬁes the relevant ones by adopting a sparsity constrain. An important step is
the design of the library of candidate functions. Because the correction models we want to identify are based on the
nonlinear eddy viscosity concept, we use the invariants I1 and I2 of the nonlinear base tensor expansion as our primitive
input features, which are shown for the periodic hill test case calculated from the mean velocity of the LES data of 
at ReH = 10,595 in Fig. 3a and 3b respectively. Further candidate functions are constructed by applying mathematical
operations to the already existing ones present in the library of candidate functions, see Table 1. The operations can be
applied in a diﬀerent order and also repetitively. For example a library containing only polynomials and products of the
resulting set of candidates is encoded in
in which the vertical line | indicates that the candidate functions resulting from the operation to the left are passed on to
the operation to the right. The resulting new functions after each operation are appended to the library. This procedure
makes the construction of the library automatic similar to FFX .
raw input features
p = [±0.5,±2,...]
f = [sin,cos,exp,log,...]
Operations to build a library of candidate functions
Since the procedure can produce duplicates at diﬀerent stages of the process, we check at the end for equivalent
expressions and retrieve only a unique set of candidate functions. In addition for every candidate function it is checked,
if it can be evaluated for the ﬂow case, for which we want to ﬁnd a correction model. Functions, which are not deﬁned
for the input domain provided by the data, are deleted from the library. Obviously the nonlinearity of the resulting
library B can be determined by the type and order of the operations and by their frequency. This is illustrated in Table 2.
example of resulting most complex candidates
R | P | F | M
1 ) log(I2
R | P | A | F
Illustration of resulting most complex candidate functions for a given order of operations.
As a starting point for the present work we focus on a weakly nonlinear library using the rule described in (21) with
p = {1.0,2.0}. In addition to the two invariants I1 and I2 we also include a constant function c to the set of raw input
features. This helps to identify very sparse models, for which the form of a base tensors only needs to be scaled by a
factor and not altered by a spatially-dependent function. The resulting library B has 16 candidate functions and reads
c,I1,I2,I2
1 I2,I1I2,I3
which are evaluated using the reference data. Thus, by using p = {1.0,2.0} and allowing products of candidate function
with the same base, we eﬀectively achieve exponents p = {1.0,2.0,3.0,4.0}. The evaluated candidate functions are
stored column-wise in the library matrix
Two invariants of the nonlinear base tensor series for ﬂow over periodics hills at Re = 10,595. Data
from .
B. Deterministic Symbolic Regression
The deterministic symbolic regression identiﬁes the most relevant candidate functions by solving the optimisation
Θ(n) = arg min
in which the vector of coeﬃcients Θ(n) needs to be identiﬁed. The target is a speciﬁc coeﬃcient ﬁeld an for a given
base tensor n. The regularisation term using norm q = 1 (lasso regression) or q = 2 (ridge regression) acts to increase
the sparsity of Θ(n), i.e. increasing the number of zeros in order to turn oﬀthe corresponding candidate functions
 , proportional to the magnitude of the Tikhonov parameter λr.
We use STRidge (Sequential Threshold Ridge Regression), which solves the optimisation problem in Eq. 24 using
ridge regression q = 2 iteratively . After each step sparsity is achieved by setting coeﬃcients to zero, which have
a smaller magnitude than a given threshold tmax, and the regression is repeated for the non-zero coeﬃcients until a
suitable model is identiﬁed. For the resulting set of coeﬃcients an ordinary least-square regression, i.e. λr = 0, is
performed to achieve the unbiased values for the coeﬃcients. Once Θ(n) has been identiﬁed for every base tensor n, the
mathematical expression of the discovered correction model can be retrieved from
BΘ(n)T (n)
Using data the ﬂow over periodic hills at ReH = 10595 only one step of STRidge with λr = 0.02 was
performed and the threshold was deﬁned as tmax = ξ max(| ˆθ|) with 0 < ξ < 1 depending on the largest absolute
coeﬃcient max(| ˆθ|). This variation of STRidge allows to determine a hierarchical set of models given diﬀerent values
of ξ. Give ξ = 0.9 the expression for the resulting correction model reads
M(1) = 7.0815 I1 T (1)
i j + 4.2099 T (2)
i j + 3.6909 I1 T (3)
i j −0.0509 T (4)
and by applying the same setup for the symbolic regression to the data of ReH = 2800 we obtain
M(2) = −6.4069 I2 T (1)
i j + 4.1292 T (2)
i j −17.4622 I2 T (3)
i j + 18.1459 I2 T (4)
The models have a very sparse structure as only the raw input features and the constant have been identiﬁed from B
to serve as functions for the coeﬃcients. This is a result of a large ξ = 0.9 value. Interestingly, for M(1) only I1 with a
positive coeﬃcient and for M(2) only I2 with a negative coeﬃcient have been chosen for T (1)
i j and T (3)
i j . This choice is
reasonable, as the invariants exhibit a similar spatial structure with opposite signs shown in Fig. 3. While the coeﬃcient
i j is almost the same for both models, the identiﬁed function for T (4)
i j is diﬀerent.
The a priori error is calculated using two metrics, the root-mean squared error ϵ and the tensor alignment ρ between
the training data and the model
Mi j,k −b∆
Mmn,k Mnm,kb∆
in which the tensors components ij are evaluated at each grid point k. The results are given in Table 3. Given the
l2-norm of the distance between the model and the data in eq. 24 the metric ϵ quantiﬁes the achieved goodness-of-ﬁt of
the symbolic regression. In addition, the metric ρ calculates how well the resulting tensor shape is aligned with the data,
and therefore does not include magnitude information. In comparison to the literature the achieved a priori error is
similar to other approaches using deep learning or genetic programming based symbolic regression .
Training case
ReH = 10595
ReH = 2800
Parameters in derivation and a priori error of the models.
VI. Prediction
Fig. 4 shows predictions of the stream-wise velocity for Reynolds number ReH = 10595 using the correction
models M(1) and M(2). While M(1) has been derived from the same dataset, using it to predict the same ﬂow serves
as a veriﬁcation. In general, both models show an improvement compared to the baseline LEVM k −ω all over the
domain. The most challenging quantity of this ﬂow case is the reattachment point xa between the hills. This is reﬂected
by a vanishing wall-shear stress at the lower wall, shown in Fig. 5 and summarised in Tab. 4. Again the predictions
of the models are closer to the reference data than the baseline LEVM, which expresses the fact that the symbolic
regression successfully identiﬁed corrections of the baseline LEVM. Also for all shown quantities M(1) is closer to the
reference data than M(2) consistently. However, for the lower Reynolds-number case (ReH = 2800) shown in Fig. 6,
M(1) performs similarly well and predicts the reattachment closer to the reference than M(2). This behaviour is not
unexpected as the models are derived to be sparse corrections of the underlying LEVM in order to avoid over-ﬁtting.
Further work needs to be done in order to study the dependency of the sparsity-controlling cut-oﬀparameter ξ on the
predictive performance of the resulting models in relation to the Reynolds-number.
2Ux/Ub + x
Breuer et al. 
Predictions of the stream-wise velocity for ReH = 10595 using correction model M(1), k −ω LEVM Mo
and EARSM M E ARSM from literature . Validation data from .
For the present case, the Explicit-Algebraic Reynolds-stress model (EARSM) from literature performs worse
than the linear k −ω model all over the domain at ReH = 10595, see Fig. 4. In a benchmark study , in which
several EARSM and RSM were applied to this test case, it was also observed that not necessarily more complex models
Wall-Shear Stress
Breuer et al. 
Prediction of the wall-shear stress for ReH = 10595 using correction model M(1), k −ω LEVM Mo and
EARSM M E ARSM from literature . Validation data from .
perform better.
Predictions of reattachment point xa for ReH = 10595 using correction model M(1), M(2) and k −ω
LEVM Mo in comparison to the LES data from .
VII. Conclusion
In this contribution we show that symbolic regression is a powerful machine learning method to learn the mathematical form of models from data for the purpose of RANS-based turbulence modelling. The goal is to identify
concise models ready to be implemented in existing codes as used in industry proven in but with a deterministic
framework, which can be applied to high-dimensional regression problems.
We introduced a framework for the inference of optimal coeﬃcients ﬁelds of the base tensor series used in the
nonlinear eddy viscosity concept. By utilising a maximum a posteriori (MAP) based Bayesian inversion we conduced a
parameter study in order to achieve coeﬃcients with low roughness and low reconstruction error by using data from
high-ﬁdelity simulations of the ﬂow over periodic hills at ReH = 2800 and 10595.
2Ux/Ub + x
Breuer et al. 
Predictions of the stream-wise velocity for Reynolds number ReH = 2800 using two correction models
M(1) and M(2) and k −ω LEVM Mo. Validation data from .
The resulting coeﬃcient ﬁelds were used as targets of the symbolic regression based on sparsity-promoting
regularised least square regression. We have successfully identiﬁed models and presented the a priori error and the
predictive performance of both of them.
The method shows potential for data-driven correction of RANS-based turbulence modelling. Next steps will be
the application to ﬂow cases with a diﬀerent geometry and establishing a relation between the a priori error and the
predictive performance of a model. In addition, the two step procedure of inferring the targets and conducting the
symbolic regression separately will be merged into a single optimisation framework.