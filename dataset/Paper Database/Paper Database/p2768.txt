Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 38–45
November 16-20, 2020. c⃝2020 Association for Computational Linguistics
Transformers: State-of-the-Art Natural Language Processing
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison,
Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush
Hugging Face, Brooklyn, USA / {first-name}@huggingface.co
Recent progress in natural language processing has been driven by advances in both model
architecture and model pretraining.
Transformer architectures have facilitated building
higher-capacity models and pretraining has
made it possible to effectively utilize this capacity for a wide variety of tasks.
Transformers is an open-source library with the
goal of opening up these advances to the
wider machine learning community.
The library consists of carefully engineered stateof-the art Transformer architectures under a
uniﬁed API. Backing this library is a curated collection of pretrained models made
by and available for the community.
Transformers is designed to be extensible by researchers, simple for practitioners, and fast
and robust in industrial deployments. The library is available at 
huggingface/transformers.
Introduction
The Transformer has rapidly
become the dominant architecture for natural language processing, surpassing alternative neural
models such as convolutional and recurrent neural
networks in performance for tasks in both natural
language understanding and natural language generation. The architecture scales with training data
and model size, facilitates efﬁcient parallel training,
and captures long-range sequence features.
Model pretraining allows models to be trained on generic corpora and subsequently be easily adapted to speciﬁc
tasks with strong performance. The Transformer
architecture is particularly conducive to pretraining on large text corpora, leading to major gains in
accuracy on downstream tasks including text classi-
ﬁcation , language understanding
 , machine translation ,
coreference resolution , commonsense inference , and
summarization among others.
This advance leads to a wide range of practical
challenges that must be addressed in order for these
models to be widely utilized. The ubiquitous use of
the Transformer calls for systems to train, analyze,
scale, and augment the model on a variety of platforms. The architecture is used as a building block
to design increasingly sophisticated extensions and
precise experiments. The pervasive adoption of pretraining methods has led to the need to distribute,
ﬁne-tune, deploy, and compress the core pretrained
models used by the community.
Transformers is a library dedicated to supporting
Transformer-based architectures and facilitating the
distribution of pretrained models. At the core of
the libary is an implementation of the Transformer
which is designed for both research and production.
The philosophy is to support industrial-strength implementations of popular model variants that are
easy to read, extend, and deploy. On this foundation, the library supports the distribution and usage
of a wide-variety of pretrained models in a centralized model hub. This hub supports users to
compare different models with the same minimal
API and to experiment with shared models on a
variety of different tasks.
Transformers is an ongoing effort maintained by
the team of engineers and researchers at Hugging
Face with support from a vibrant community of
over 400 external contributors. The library is released under the Apache 2.0 license and is available
on GitHub1. Detailed documentation and tutorials
are available on Hugging Face’s website2.
1 
transformers
2 
Figure 1: Average daily unique downloads of the most downloaded pretrained models, Oct. 2019 to May 2020.
Related Work
The NLP and ML communities have a strong culture of building open-source research tools. The
structure of Transformers is inspired by the pioneering tensor2tensor library and the original source code for BERT , both from Google Research.
The concept of providing easy caching for pretrained models stemmed from AllenNLP . The library is also closely related to neural translation and language modeling
systems, such as Fairseq , Open-
NMT , Texar ,
Megatron-LM , and Marian NMT . Building on these elements, Transformers adds extra
user-facing features to allow for easy downloading,
caching, and ﬁne-tuning of the models as well as
seamless transition to production. Transformers
maintains some compatibility with these libraries,
most directly including a tool for performing inference using models from Marian NMT and Google’s
There is a long history of easy-to-use, userfacing libraries for general-purpose NLP. Two core
libraries are NLTK and
Stanford CoreNLP , which
collect a variety of different approaches to NLP in
a single package. More recently, general-purpose,
open-source libraries have focused primarily on
machine learning for a variety of NLP tasks, these
include Spacy , AllenNLP , ﬂair , and Stanza . Transformers provides similar functionality as these libraries.
Additionally, each of these libraries now uses the
Transformers library and model hub as a low-level
framework.
Since Transformers provides a hub for NLP models, it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect
framework-speciﬁc model parameters for easy use.
Unlike these hubs, Transformers is domain-speciﬁc
which allows the system to provide automatic support for model analysis, usage, deployment, benchmarking, and easy replicability.
Library Design
Transformers is designed to mirror the standard
NLP machine learning model pipeline: process
data, apply a model, and make predictions. Although the library includes tools facilitating training and development, in this technical report we
focus on the core modeling speciﬁcations. For
complete details about the features of the library
refer to the documentation available on https:
//huggingface.co/transformers/.
Every model in the library is fully deﬁned by
three building blocks shown in the diagram in Figure 2: (a) a tokenizer, which converts raw text to
sparse index encodings, (b) a transformer, which
transforms sparse indices to contextual embeddings, and (c) a head, which uses contextual embeddings to make a task-speciﬁc prediction. Most
user needs can be addressed with these three components.
Transformers
Central to the library are carefully
tested implementations of Transformer architecture
variants which are widely used in NLP. The full list
of currently implemented architectures is shown in
Figure 2 (Left). While each of these architectures
Ex. Datasets
Language Modeling
Generation
WikiText-103
Sequence Classiﬁcation
Classiﬁcation,
Sentiment Analysis
GLUE, SST,
Question Answering
y span [1 : N]
QA, Reading
Comprehension
Natural Questions
Token Classiﬁcation
NER, Tagging
OntoNotes, WNUT
Multiple Choice
Text Selection
Pretraining
Wikitext, C4
Conditional Generation
Translation,
Summarization
WMT, IWSLT,
CNN/DM, XSum
Transformers
Masked [x1:N\n ⇒xn]
 
 
Autoregressive [x1:n−1 ⇒xn]
GPT / GPT-2
 
 
 
Seq-to-Seq [∼x1:N ⇒x1:N]
 
 
 
Specialty: Multimodal
 
Specialty: Long-Distance
 
Longformer
 
Specialty: Efﬁcient
 
 
DistilBERT
 
Specialty: Multilingual
XLM/RoBERTa
 
Transformer
Tokenizers
Character-Level BPE
Byte-Level BPE
SentencePiece
Figure 2: The Transformers library. (Diagram-Right) Each model is made up of a Tokenizer, Transformer, and
Head. The model is pretrained with a ﬁxed head and can then be further ﬁne-tuned with alternate heads for different
tasks. (Bottom) Each model uses a speciﬁc Tokenizer either implemented in Python or in Rust. These often differ
in small details, but need to be in sync with pretraining. (Left) Transformer architectures specialized for different
tasks, e.g. understanding versus generation, or for speciﬁc use-cases, e.g. speed, image+text. (Top) heads allow a
Transformer to be used for different tasks. Here we assume the input token sequence is x1:N from a vocabulary V,
and y represents different possible outputs, possibly from a class set C. Example datasets represent a small subset
of example code distributed with the library.
shares the same multi-headed attention core, there
are signiﬁcant differences between them including
positional representations, masking, padding, and
the use of sequence-to-sequence design. Additionally, various models are built to target different
applications of NLP such as understanding, generation, and conditional generation, plus specialized
use cases such as fast inference or multi-lingual
applications.
Practically, all models follow the same hierarchy
of abstraction: a base class implements the model’s
computation graph from an encoding (projection
on the embedding matrix) through the series of selfattention layers to the ﬁnal encoder hidden states.
The base class is speciﬁc to each model and closely
follows the model’s original implementation which
gives users the ﬂexibility to easily dissect the inner
workings of each individual architecture. In most
cases, each model is implemented in a single ﬁle
to enable ease of extensibility.
Wherever possible, different architectures follow the same API allowing users to switch easily
between different models. A set of Auto classes
provides a uniﬁed API that enables very fast switching between models and even between frameworks.
These classes automatically instantiate with the
conﬁguration speciﬁed by the user-speciﬁed pretrained model.
Tokenizers
A critical NLP-speciﬁc aspect of the
library is the implementations of the tokenizers necessary to use each model. Tokenizer classes (each
inheriting from a common base class) can either be
instantiated from a corresponding pretrained model
or can be conﬁgured manually. These classes store
the vocabulary token-to-index map for their corresponding model and handle the encoding and decoding of input sequences according to a model’s
speciﬁc tokenization process. The tokenizers implemented are shown in Figure 2 (Right). Users
can easily modify tokenizer with interfaces to add
additional token mappings, special tokens (such as
classiﬁcation or separation tokens), or otherwise
resize the vocabulary.
Tokenizers can also implement additional useful
features for the users. These range from token type
indices in the case of sequence classiﬁcation to
maximum length sequence truncating taking into
account the added model-speciﬁc special tokens
(most pretrained Transformer models have a maximum sequence length).
For training on very large datasets, Python-based
tokenization is often undesirably slow.
most recent release, Transformers switched its implementation to use a highly-optimized tokenization library by default.
This low-level library,
available at 
tokenizers, is written in Rust to speed up the
tokenization procedure both during training and
deployment.
Each Transformer can be paired with
one out of several ready-implemented heads
with outputs amenable to common types of
These heads are implemented as additional wrapper classes on top of the base
class, adding a speciﬁc output layer, and optional loss function, on top of the Transformer’s
contextual embeddings.
The full set of implemented heads are shown in Figure 2 (Top).
These classes follow a similar naming pattern:
XXXForSequenceClassification where
XXX is the name of the model and can be used
for adaptation (ﬁne-tuning) or pretraining. Some
heads, such as conditional generation, support extra
functionality like sampling and beam search.
For pretrained models, we release the heads used
to pretrain the model itself. For instance, for BERT
we release the language modeling and next sentence prediction heads which allows easy for adaptation using the pretraining objectives. We also
make it easy for users to utilize the same core Transformer parameters with a variety of other heads for
ﬁnetuning. While each head can be used generally,
the library also includes a collection of examples
that show each head on real problems. These examples demonstrate how a pretrained model can be
adapted with a given head to achieve state-of-theart results on a large variety of NLP tasks.
Community Model Hub
Transformers aims to facilitate easy use and distribution of pretrained models. Inherently this is
a community process; a single pretraining run facilitates ﬁne-tuning on many speciﬁc tasks. The
Model Hub makes it simple for any end-user to access a model for use with their own data. This hub
now contains 2,097 user models, both pretrained
and ﬁne-tuned, from across the community. Figure 1 shows the increase and distribution of popular
transformers over time. While core models like
BERT and GPT-2 continue to be popular, other specialized models including DistilBERT , which was developed for the library, are
Figure 3: Transformers Model Hub. (Left) Example of a model page and model card for SciBERT , a pretrained model targeting extraction from scientiﬁc literature submitted by a community contributor. (Right) Example of an automatic inference widget for the pretrained BART model for
summarization. Users can enter arbitrary text and a full version of the model is deployed on the ﬂy to produce a
now widely downloaded by the community.
The user interface of the Model Hub is designed
to be simple and open to the community. To upload
a model, any user can sign up for an account and
use a command-line interface to produce an archive
consisting a tokenizer, transformer, and head. This
bundle may be a model trained through the library
or converted from a checkpoint of other popular
training tools. These models are then stored and
given a canonical name which a user can use to
download, cache, and run the model either for ﬁnetuning or inference in two lines of code. To load
FlauBERT , a BERT model pretrained on a French training corpus, the command
1 tknzr = AutoTokenizer.from_pretrained(
"flaubert/flaubert_base_uncased")
3 model = AutoModel.from_pretrained(
"flaubert/flaubert_base_uncased")
When a model is uploaded to the Model Hub, it
is automatically given a landing page describing its
core properties, architecture, and use cases. Additional model-speciﬁc metadata can be provided via
a model card that describes
properties of its training, a citation to the work,
datasets used during pretraining, and any caveats
about known biases in the model and its predictions.
An example model card is shown in Figure 3 (Left).
Since the Model Hub is speciﬁc to transformerbased models, we can target use cases that would
be difﬁcult for more general model collections. For
example, because each uploaded model includes
metadata concerning its structure, the model page
can include live inference that allows users to experiment with output of models on a real data. Figure 3 (Right) shows an example of the model page
with live inference. Additionally, model pages include links to other model-speciﬁc tools like benchmarking and visualizations. For example, model
pages can link to exBERT , a
Transformer visualization library.
Community Case Studies
The Model Hub highlights how Transformers is used by a variety of
different community stakeholders. We summarize
three speciﬁc observed use-cases in practice. We
highlight speciﬁc systems developed by users with
different goals following the architect, trainer, and
end-user distinction of Strobelt et al. :
Case 1: Model Architects AllenAI, a major NLP
research lab, developed a new pretrained model for
improved extraction from biomedical texts called
SciBERT . They were able
to train the model utilizing data from PubMed to
produce a masked language model with state-ofthe-art results on targeted text. They then used the
Model Hub to distribute the model and promote
it as part of their CORD - COVID-19 challenge,
making it trivial for the community to use.
Case 2: Task Trainers Researchers at NYU were
interested in developing a test bed for the performance of Transformers on a variety of different semantic recognition tasks. Their framework
Jiant allows them to
experiment with different ways of pretraining models and comparing their outputs. They used the
Transformers API as a generic front-end and performed ﬁne-tuning on a variety of different models,
leading to research on the structure of BERT .
Case 3: Application Users Plot.ly, a company focused on user dashboards and analytics, was interested in deploying a model for automatic document summarization. They wanted an approach
that scaled well and was simple to deploy, but had
no need to train or ﬁne-tune the model. They were
able to search the Model Hub and ﬁnd DistilBART,
a pretrained and ﬁne-tuned summarization model
designed for accurate, fast inference. They were
able to run and deploy the model directly from the
hub with no required research or ML expertise.
Deployment
An increasingly important goal of Transformers is
to make it easy to efﬁciently deploy model to production. Different users have different production
needs, and deployment often requires solving signiﬁcantly different challenges than training. The
library thereforce allows for several different strategies for production deployment.
One core propery of the libary is that models
are available both in PyTorch and TensorFlow, and
there is interoperability between both frameworks.
A model trained in one of frameworks can be saved
through standard serialization and be reloaded from
the saved ﬁles in the other framework seamlessly.
This makes it particularly easy to switch from one
framework to the other one along the model lifetime (training, serving, etc.).
Each framework has deployment recommendations. For example, in PyTorch, models are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either
in Python in a more efﬁcient way, or in a highperformance environment such as C++. Fine-tuned
models can thus be exported to production-friendly
environment, and run through TorchServing. TensorFlow includes several serving options within its
ecosystem, and these can be used directly.
Transformers can also export models to intermediate neural network formats for further compila-
Figure 4: Experiments with Transformers inference in
collaboration with ONNX.
tion. It supports converting models to the Open
Neural Network Exchange format (ONNX) for deployment. Not only does this allow the model to
be run in a standardized interoperable format, but
also leads to signiﬁcant speed-ups. Figure 4 shows
experiments run in collaboration with the ONNX
team to optimize BERT, RoBERTa, and GPT-2
from the Transformers library. Using this intermediate format, ONNX was able to achieve nearly a
4x speedup on this model. The team is also experimenting with other promising intermediate formats such as JAX/XLA and
TVM .
Finally, as Transformers become more widely
used in all NLP applications, it is increasingly important to deploy to edge devices such as phones
or home electronics. Models can use adapters to
convert models to CoreML weights that are suitable to be embedded inside a iOS application, to
enable on-the-edge machine learning. Code is also
made available3. Similar methods can be used for
Android devices.
Conclusion
As Transformer and pretraining play larger roles in
NLP, it is important for these models to be accessible to researchers and end-users. Transformers
is an open-source library and community designed
to facilitate users to access large-scale pretrained
models, to build and experiment on top of them,
and to deploy them in downstream tasks with stateof-the-art performance. Transformers has gained
signiﬁcant organic traction since its release and is
set up to continue to provide core infrastructure
while helping to facilitate access to new models.
3 
swift-coreml-transformers