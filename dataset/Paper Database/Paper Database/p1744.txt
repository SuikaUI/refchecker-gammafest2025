Revisiting Local Descriptor based Image-to-Class Measure
for Few-shot Learning
Wenbin Li1,
Lei Wang2,
Jinglin Xu3,
Jing Huo1,
Yang Gao1,
Jiebo Luo4
1Nanjing University, China,
2University of Wollongong, Australia
3Northwestern Polytechnical University, China,
4University of Rochester, USA
Few-shot learning in image classiﬁcation aims to learn
a classiﬁer to classify images when only few training
examples are available for each class. Recent work has
achieved promising classiﬁcation performance,
an image-level feature based measure is usually used.
In this paper, we argue that a measure at such a level
may not be effective enough in light of the scarcity of
examples in few-shot learning. Instead, we think a local
descriptor based image-to-class measure should be taken,
inspired by its surprising success in the heydays of local
invariant features. Speciﬁcally, building upon the recent
episodic training mechanism, we propose a Deep Nearest
Neighbor Neural Network (DN4 in short) and train it in an
end-to-end manner. Its key difference from the literature is
the replacement of the image-level feature based measure
in the ﬁnal layer by a local descriptor based image-to-class
measure. This measure is conducted online via a k-nearest
neighbor search over the deep local descriptors of convolutional feature maps. The proposed DN4 not only learns
the optimal deep local descriptors for the image-to-class
measure, but also utilizes the higher efﬁciency of such
a measure in the case of example scarcity, thanks to the
exchangeability of visual patterns across the images in
the same class.
Our work leads to a simple, effective,
and computationally efﬁcient framework for few-shot
Experimental study on benchmark datasets
consistently shows its superiority over the related stateof-the-art, with the largest absolute improvement of 17%
over the next best. The source code can be available from
 
1. Introduction
Few-shot learning aims to learn a model with good generalization capability such that it can be readily adapted to
new unseen classes (concepts) by accessing only one or few
examples. However, the extremely limited number of examples per class can hardly represent the class distribution
effectively, making this task truly challenging.
To tackle the few-shot learning task, a variety of methods have been proposed, which can be roughly divided into
two types, i.e., meta-learning based and metriclearning based .
The former type introduces
a meta-learning paradigm to learn an across-task
meta-learner for generalizing to new unseen tasks. They
usually resort to recurrent neural networks or long short
term memory networks to learn a memory network 
to store knowledge. The latter type adopts a relatively simpler architecture to learn a deep embedding space to transfer
representation (knowledge). This type of methods usually
relies on the metric learning and episodic training mechanism . Both types of methods have greatly advanced
the development of few-shot learning.
These existing methods mainly focus on making knowledge transfer , concept representation or relation measure , but have not paid sufﬁcient attention
to the way of the ﬁnal classiﬁcation. They generally take
the common practice, i.e., using the image-level pooled features or fully connected layers designed for larger-scale image classiﬁcation, for the few-shot case. Considering the
unique characteristic of few-shot learning (i.e., the scarcity
of examples for each training class), such a common practice may not be appropriate any more.
In this paper, we revisit the Naive-Bayes Nearest-
Neighbor (NBNN) approach published a decade ago,
and investigate its effectiveness in the context of the latest
few-shot learning research. The NBNN approach demonstrated a surprising success when the bag-of-features model
with local invariant features (i.e., SIFT) was popular. That
work provides two key insights. First, summarizing the local features of an image into a compact image-level representation could lose considerable discriminative information. It will not be recoverable when the number of training
examples is small. Second, in this case, directly using these
local features for classiﬁcation will not work if an image-toimage measure is used. Instead, an image-to-class measure
should be taken, by exploiting the fact that a new image can
 
be roughly “composed” using the pieces of other images in
the same class. The above two insights inspire us to review
the way of the ﬁnal classiﬁcation in the existing methods
for few-shot learning and reconsider the NBNN approach
for this task with deep learning.
Speciﬁcally, we develop a novel Deep Nearest Neighbor
Neural Network (DN4 in short) for few-shot learning. It
follows the recent episodic training mechanism and is fully
end-to-end trainable. Its key difference from the related existing methods lies in that it replaces the image-level feature based measure in the ﬁnal layer with a local descriptor
based image-to-class measure. Similar to NBNN , this
measure is computed via a k-nearest neighbor search over
local descriptors, with the difference that these descriptors
are now trained deeply via convolutional neural networks.
Once trained, applying the proposed network to new fewshot learning tasks is straightforward, consisting of local descriptor extraction and then a nearest neighbor search. Interestingly, in terms of computation, the scarcity of examples
per class now turns out to be an “advantage” making NBNN
more appealing for few-shot learning. It mitigates the computation of searching for the nearest neighbors from a huge
set of local descriptors, which is one factor of the lower
popularity of NBNN in large-scale image classiﬁcation.
Experiments are conducted on multiple benchmark
datasets to compare the proposed DN4 with the original
NBNN and the related state-of-the-art methods for the task
of few-shot learning. The proposed method again demonstrates a surprising success. It improves the 1-shot and 5shot accuracy on miniImageNet from 50.44% to 51.24%
and from 66.53% to 71.02%, respectively.
Particularly,
on ﬁne-grained datasets it achieves the largest absolute improvement over the next best method by 17%.
2. Related Work
Among the recent literature of few-shot learning, the
transfer learning based methods are most relevant to the
proposed method. Therefore, we brieﬂy review two main
branches of this kind of methods as follows.
Meta-learning based methods. As shown by the representative work , the meta-learning based
methods train a meta-learner with the meta-learning or the
learning-to-learn paradigm for few-shot learning. This is beneﬁcial for identifying how to update the
parameters of the learner’s model. For instance, Santoro et
al. trained an LSTM as a controller to interact with an
external memory module. And the work adopted an
LSTM-based meta-learner as an optimizer to train another
classiﬁer as well as learning a task-common initialization
for this classiﬁer. The work of MM-Net constructed a
contextual learner to predict the parameters of an embedding network for unlabeled images by using memory slots.
Although the meta-learning based methods can achieve
excellent results for few-shot classiﬁcation, it is difﬁcult to train their complicated memory-addressing architecture because of the temporally-linear hidden state dependency . Compared with the methods in this branch,
the proposed framework DN4 can be trained more easily
in an end-to-end manner from scratch, e.g., by only using a
common single convolutional neural networks (CNN), and
could provide quite competitive results.
Metric-learning based methods. The metric-learning
based methods mainly depend on learning an informative
similarity metric, as demonstrated by the representative
work . Speciﬁcally, to introduce
the metric-based method into few-shot learning, Koch et
al. originally utilized a Siamese Neural Network to learn
powerful discriminative representations and then generalized them to unseen classes. And then, Vinyals et al. 
introduced the episodic training mechanism into few-shot
learning and proposed the Matching Nets by combining attention and memory together. In , a Prototypical Network was proposed by taking the mean of each class as
its corresponding prototype representation to learn a metric space. Recently, Sung et al. considered the relation
between query images and class images, and presented a
Relation Network to learn a deep non-linear measure.
The proposed framework DN4 belongs to the metriclearning based methods. However, a key difference from
them is that the above methods mainly adopt the imagelevel features for classiﬁcation, while the proposed DN4 exploits deep local descriptors and the image-to-class measure
for classiﬁcation, as inspired by the NBNN approach .
As will be shown in the experimental part, the proposed
DN4 can clearly outperform the several state-of-the-art
metric-learning based methods.
3. The Proposed Method
3.1. Problem Formulation
Let S denote a support set, which contains C different
image classes and K labeled samples per class. Given a
query set Q, few-shot learning aims to classify each unlabeled sample in Q according to the set S. This setting is also
called C-way K-shot classiﬁcation. Unfortunately, when S
only has few samples per class, it will be hard to effectively
learn a model to classify the samples in Q. Usually, the
literature resorts to an auxiliary set A to learn transferable
knowledge to improve the classiﬁcation on Q. Note that
the set A can contain a large number of classes and labeled
samples, but it has a disjoint class label space with respect
to the set S.
The episodic training mechanism has been demonstrated in the literature as an effective approach to learning the transferable knowledge from A, and it will also
be adopted in this work. Speciﬁcally, at each iteration, an
episode is constructed to train the classiﬁcation model by
Support set
Query image
Local Features Ψ(𝑋𝑋)
Image-to-Class module Φ
Φ (Ψ(𝑋𝑋), 𝑐𝑐1)
Class: 𝑐𝑐1
Class: 𝑐𝑐2
Class: 𝑐𝑐3
Class: 𝑐𝑐4
Class: 𝑐𝑐5
Embedding Ψ
Φ (Ψ(𝑋𝑋), 𝑐𝑐2)
Φ (Ψ(𝑋𝑋), 𝑐𝑐3)
Φ (Ψ(𝑋𝑋), 𝑐𝑐4)
Φ (Ψ(𝑋𝑋), 𝑐𝑐5)
Figure 1. Illustration of the proposed Deep Nearest Neighbor Neural Network (DN4 in short) for a few-shot learning task in the 5-way
and 1-shot setting. As shown, this framework consists of a CNN-based embedding module Ψ(·) for learning deep local descriptors and an
image-to-class module Φ(·) for measuring the similarity between a given query image X and each of the classes, ci (i = 1, 2, · · · , 5).
simulating a few-shot learning task. The episode consists
of a support set AS and a query set AQ that are randomly
sampled from the auxiliary set A. Generally, AS has the
same numbers of ways (i.e., classes) and shots as S. In
other words, there are exactly C classes and K samples per
class in AS. During training, tens of thousands of episodes
will be constructed to train the classiﬁcation model, namely
the episodic training. In the test stage, with the support set
S, the learned model can be directly used to classify each
image in Q.
3.2. Motivation from the NBNN Approach
This work is largely inspired by the Naive-Bayes
Nearest-Neighbor (NBNN) method in . The two key observations of NBNN are described as follows, and we show
that they apply squarely to few-shot learning.
First, for the (then-popular) bag-of-features model in image classiﬁcation, local invariant features are usually quantized into visual words to generate the distribution of words
(e.g., a histogram obtained by sum-pooling) in an image.
It is observed in that due to quantization error, such
an image-level representation could signiﬁcantly lose discriminative information. If there are sufﬁcient training samples, the subsequent learning process (e.g., via support vector machines) can somehow recover from such a loss, still
showing satisfactory classiﬁcation performance. Nevertheless, when training samples are insufﬁcient, this loss is unrecoverable and leads to poor classiﬁcation.
Few-shot learning is impacted more signiﬁcantly by the
issue of example scarcity than NBNN. And the existing
methods usually pool the last convolutional feature maps
(e.g., via the global average pooling or fully connected
layer) to an image-level representation for the ﬁnal classiﬁcation. In this case, such an information loss will also occur
and is unrecoverable.
Second, as further observed in , using the local invariant features of two images, instead of their image-level representations, to measure an image-to-image similarity for
classiﬁcation will still incur a poor result. This is because
such an image-to-image similarity does not generalize beyond training samples. When the number of training samples is small, a query image could be different from any
training samples of the same class due to intra-class variation or background clutter. Instead, an image-to-class measure should be used. Speciﬁcally, the local invariant features from all training samples in the same class are collected into one pool. This measure evaluates the proximity
(e.g., via nearest-neighbor search) of the local features of a
query image to the pool of each class for classiﬁcation.
Again, this observation applies to few-shot learning.
Essentially, the above image-to-class measure breaks the
boundaries of training images in the same class, and uses
their local features collectively to provide a richer and more
ﬂexible representation for a class. As indicated in , this
setting can be justiﬁed by a fact that a new image can be
roughly “composed” by using the pieces of other images in
the same class (i.e., the exchangeability of visual patterns
across the images in the same class).
3.3. The Proposed DN4 Framework
The above analysis motivates us to review the way of
the ﬁnal classiﬁcation in few-shot learning and reconsider
the NBNN approach. This leads to the proposed framework
Deep Nearest Neighbor Neural Network (DN4 in short).
As illustrated in Figure 1, DN4 mainly consists of two
components: a deep embedding module Ψ and an imageto-class measure module Φ. The former learns deep local
descriptors for all images.
With the learned descriptors,
the latter calculates the aforementioned image-to-class measure. Importantly, these two modules are integrated into a
uniﬁed network and trained in an end-to-end manner from
scratch. Also, note that the designed image-to-class module
can readily work with any deep embedding module.
Deep embedding module.
The module Ψ routinely
learns the feature representations for query and support images. Any proper CNN can be used. Note that Ψ only contains convolutional layers but has no fully connected layer,
since we just need deep local descriptors to compute the
image-to-class measure. In short, given an image X, Ψ(X)
will be an h×w×d tensor, which can be viewed as a set of
m (m=hw) d-dimensional local descriptors as
Ψ(X) = [x1, . . . , xm] ∈Rd×m ,
where xi is the i-th deep local descriptor. In our experiments, given an image with a resolution of 84 × 84, we can
get h = w = 21 and d = 64. It means that each image has
441 deep local descriptors in total.
Image-to-Class module. The module Φ uses the deep
local descriptors from all training images in a class to construct a local descriptor space for this class. In this space,
we calculate the image-to-class similarity (or distance) between a query image and this class via k-NN, as in .
Speciﬁcally, through the module Ψ, a given query image
q will be embedded as Ψ(q) = [x1, . . . , xm] ∈Rd×m. For
each descriptor xi, we ﬁnd its k-nearest neighbors ˆxj
in a class c. Then we calculate the similarity between xi
and each ˆxi, and sum the mk similarities as the image-toclass similarity between q and the class c. Mathematically,
the image-to-class measure can be easily expressed as
cos(xi, ˆxj
cos(xi, ˆxi) =
∥xi∥· ∥ˆxi∥,
where cos(·) indicates the cosine similarity. Other similarity or distance functions can certainly be employed.
Note that in terms of computational efﬁciency, the
image-to-class measure seems more suitable for few-shot
classiﬁcation than the generic image classiﬁcation focused
in . The major computational issue in NBNN caused by
searching for k-nearest neighbors from a huge pool of local
descriptors has now been substantially weakened due to the
much smaller number of training samples in few-shot setting. This makes the proposed framework computationally
efﬁcient. Furthermore, compared with NBNN, it will be
more promising, by beneﬁting from the deep feature representations that are much more powerful than the handcrafted features used in NBNN.
Finally, it is worth mentioning that the image-to-class
module in DN4 is non-parametric. So the entire classiﬁcation model is non-parametric if not considering the embedding module Ψ. Since a non-parametric model does not
involve parameter learning, the over-ﬁtting issue in parametric few-shot learning methods (e.g., learning a fully connected layer over image-level representation) can also be
mitigated to some extent.
3.4. Network Architecture
For fair comparison with the state-of-the-art methods,
we take a commonly used four-layer convolutional neural
network as the embedding module. It contains four convolutional blocks, each of which consists of a convolutional
layer, a batch normalization layer and a Leaky ReLU layer.
Besides, for the ﬁrst two convolutional blocks, an additional
2×2 max-pooling layer is also appended, respectively. This
embedding network is named Conv-64F, since there are 64
ﬁlters of size 3 × 3 in each convolutional layer. As for the
image-to-class module, the only hyper-parameter is the parameter k, which will be discussed in the experiment.
At each iteration of the episodic training, we feed a support set S and a query image q into our model. Through the
embedding module Ψ, we obtain all the deep local representations for all these images. Then via the module Φ, we
calculate the image-to-class similarity between q and each
class by Eq. (2). For a C-way K-shot task, we can get a
similarity vector z ∈RC. The class corresponding to the
largest component of z will be the prediction for q.
4. Experimental Results
The main goal of this section is to investigate two interesting questions: (1) How does the pre-trained deep features
based NBNN without episodic training perform on the fewshot learning? (2) How does our proposed DN4 framework,
i.e., a CNN based NBNN in an end-to-end episodic training
manner, perform on the few-shot learning?
4.1. Datasets
We conduct all the experiments on four benchmark
datasets as follows.
miniImageNet. As a mini-version of ImageNet ,
this dataset contains 100 classes with 600 images per
class, and has a resolution of 84 × 84 for each image. Following the splits used in , we take 64, 16 and 20 classes
for training (auxiliary), validation and test, respectively.
Stanford Dogs. This dataset is originally used for
the task of ﬁne-grained image classiﬁcation, including 120
breeds (classes) of dogs with a total number of 20, 580 images. Here, we conduct ﬁne-grained few-shot classiﬁcation
task on this dataset, and take 70, 20 and 30 classes for training (auxiliary), validation and test, respectively.
Stanford Cars. This dataset is also a benchmark
dataset for ﬁne-grained classiﬁcation task, which consists of
196 classes of cars with a total number of 16, 185 images.
Similarly, 130, 17 and 49 classes in this dataset are split for
training (auxiliary), validation and test.
CUB-200. This dataset contains 6033 images from
200 bird species. In a similar way, we select 130, 20 and 50
classes for training (auxiliary), validation and test.
For the last three ﬁne-grained datasets, all the images in
these datasets are resized to 84 × 84 as miniImageNet.
4.2. Experimental Setting
All experiments are conducted around the C-way K-shot
classiﬁcation task on the above datasets. To be speciﬁc,
5-way 1-shot and 5-shot classiﬁcation tasks will be conducted on all these datasets. During training, we randomly
sample and construct 300, 000 episodes to train all of our
models by employing the episodic training mechanism. In
each episode, besides the K support images (shots) in each
class, 15 and 10 query images will also be selected from
each class for the 1-shot and 5-shot settings, respectively.
In other words, for a 5-way 1-shot task, there will be 5 support images and 75 query images in one training episode.
To train our model, we adopt the Adam algorithm with
an initial learning rate of 1×10−3 and reduce it by half of
every 100, 000 episodes.
During test, we randomly sample 600 episodes from the
test set, and take the top-1 mean accuracy as the evaluation
criterion. This process will be repeated ﬁve times, and the
ﬁnal mean accuracy will be reported. Moreover, the 95%
conﬁdence intervals are also reported. Notably, all of our
models are trained from scratch in an end-to-end manner,
and do not need ﬁne-tuning in the test stage.
4.3. Comparison Methods
Baseline methods. To illustrate the basic classiﬁcation
performance on the above datasets, we implement a baseline method k-NN (Deep global features). Particularly, we
adopt the basic embedding network Conv-64F and append
three additional FC layers to train a classiﬁcation network
on the corresponding training (auxiliary) dataset. During
test, we use this pre-trained network to extract features from
the last FC layer and use a k-NN classiﬁer to get the ﬁnal classiﬁcation results. Also, to answer the ﬁrst question
at the beginning of Section 4, we re-implement the NBNN
algorithm by using the pre-trained Conv-64F truncated
from the above k-NN (Deep global features) method. This
new NBNN algorithm employing the deep local descriptors
instead of the hand-crafted descriptors (i.e., SIFT), is called
NBNN (Deep local features).
Metric-learning based methods. As our method belongs to the metric-learning branch, we mainly compare
our model with four state-of-the-art metric-learning based
models, including Matching Nets FCE , Prototypical
Nets , Relation Net and Graph Neural Network
(GNN) . Note that we re-run the GNN model by using
the Conv-64F as its embedding module because the original GNN adopts a different embedding module Conv-256F,
which also has four convolutional layers but with 64, 96,
128 and 256 ﬁlters for the corresponding layers, respectively. Also, we re-run the Prototypical Nets via the same
5-way training setting instead of the 20-way training setting
in the original work for a fair comparison.
Meta-learning based methods.
Besides the metriclearning based models, ﬁve state-of-the-art meta-learning
based models are also picked for reference. These models
include Meta-Learner LSTM , Model-agnostic Metalearning (MAML) , Simple Neural AttentIve Learner
(SNAIL) , MM-Net and Dynamic-Net . As SNAIL
adopts a much more complicated ResNet-256F (a smaller
version of ResNet ) as its embedding module, we will additionally report its results based on the Conv-32F provided
in its appendix for a fair comparison. Note that Conv-32F
has the same architecture with Conv-64F, but with 32 ﬁlters
per convolutional layer, which has also been employed by
Meta-Learner LSTM and MAML to reduce over-ﬁtting.
4.4. Few-shot Classiﬁcation
The generic few-shot classiﬁcation task is conducted on
miniImageNet. The results are reported in Table 1, where
the hyper-parameter k is set as 3. From Table 1, it is amazing to see that NBNN (Deep local features) can achieve
much better results than k-NN (Deep global features), and
it is even better than Matching Nets FCE, Meta-Learner
LSTM and SNAIL (Conv-32F). This not only veriﬁes that
the local descriptors can perform better than the image-level
features (i.e., FC layer features used by k-NN), but also
shows that the image-to-class measure is truly promising.
However, NBNN (Deep local features) still has a large performance gap compared with the state-of-the-art Prototypical Nets, Relation Net and GNN. The reason is that, as a
lazy learning algorithm, NBNN (Deep local features) does
not have a training stage and also lacks the episodic training.
So far, the ﬁrst question has been answered.
On the contrary, our proposed DN4 embeds the imageto-class measure into a deep neural network, and can learn
the deep local descriptors jointly by employing the episodic
training, which indeed obtains superior results. Compared
with the metric-learning based models, our DN4 (Conv-
64F) gains 7.68%, 2.22%, 2.79% and 0.8% improvements
over Matching Nets FCE, GNN‡ (Conv-64F), Prototypical Nets‡ (i.e., via 5-way training setting) and Relation
Net on the 5-way 1-shot classiﬁcation task, respectively.
On the 5-way 5-shot classiﬁcation task, we can even get
15.71%, 7.52%, 4.49% and 5.7% signiﬁcant improvements
over these models. The reason is that these methods usually
Table 1. The mean accuracies of the 5-way 1-shot and 5-shot tasks on the miniImageNet dataset, with 95% conﬁdence intervals. The
second column refers to which kind of embedding module is employed, e.g., Conv-32F and Conv-64F etc. The third column denotes the
type of this method, i.e., meta-learning based or metric-learning based. ∗Results reported by the original work. ‡ Results re-implemented
in the same setting for a fair comparison.
5-Way Accuracy (%)
k-NN (Deep global features)
27.23±1.41
49.29±1.56
NBNN (Deep local features)
44.10±1.17
58.84±1.10
Matching Nets FCE∗ 
43.56±0.84
55.31±0.73
Prototypical Nets‡ 
48.45±0.96
66.53±0.51
Prototypical Nets∗ 
49.42±0.78
68.20±0.66
49.02±0.98
63.50±0.84
50.33±0.36
66.41±0.63
Relation Net∗ 
50.44±0.82
65.32±0.70
Our DN4 (k=3)
51.24±0.74
71.02±0.64
To take a whole picture of the-state-of-art methods
Meta-Learner LSTM∗ 
43.44±0.77
60.60±0.71
SNAIL∗ 
48.70±1.84
63.11±0.92
MM-Net∗ 
53.37±0.48
66.97±0.35
SNAIL∗ 
ResNet-256F
55.71±0.99
68.88±0.92
Dynamic-Net∗ 
ResNet-256F
55.45±0.89
70.13±0.68
Dynamic-Net∗ 
56.20±0.86
72.81±0.62
Table 2. The mean accuracies of the 5-way 1-shot and 5-shot tasks on three ﬁne-grained datasets, i.e., Stanford Dogs, Stanford Cars and
CUB-200, with 95% conﬁdence intervals. For each setting, the best and the second best methods are highlighted.
5-Way Accuracy (%)
Stanford Dogs
Stanford Cars
k-NN (Deep global features)
26.14±0.91
43.14±1.02
23.50±0.88
34.45±0.98
25.81±0.90
45.34±1.03
NBNN (Deep local features)
31.42±1.12
42.17±0.99
28.18±1.24
38.27±0.92
35.29±1.03
47.97±0.96
Matching Nets FCE‡ 
35.80±0.99
47.50±1.03
34.80±0.98
44.70±1.03
45.30±1.03
59.50±1.01
Prototypical Nets‡ 
37.59±1.00
48.19±1.03
40.90±1.01
52.93±1.03
37.36±1.00
45.28±1.03
46.98±0.98
62.27±0.95
55.85±0.97
71.25±0.89
51.83±0.98
63.69±0.94
Our DN4 (k=1)
45.41±0.76
63.51±0.62
59.84±0.80
88.65±0.44
46.84±0.81
74.92±0.64
Our DN4-DA (k=1)
45.73±0.76
66.33±0.66
61.51±0.85
89.60±0.44
53.15±0.84
81.90±0.60
use image-level features whose number is too small, while
our DN4 adopts learnable deep local descriptors which are
more abundant especially in the 5-shot setting. On the other
hand, local descriptors enjoy the exchangeability characteristic, making the distribution of each class built upon the
local descriptors more effective than the one built upon the
image-level features. Therefore, the second question can
also be answered.
To take a whole picture of the few-shot learning area, we
also report the results of the state-of-the-art meta-learning
based methods. We can see that our DN4 is still competitive with these methods. Especially in the 5-way 5-shot setting, our DN4 gains 15.82%, 10.42%, 7.91% and 4.05% improvements over SNAIL (Conv-32F), Meta-Learner LSTM,
MAML and MM-Net, respectively. As for the Dynamic-
Net, a two-stage model, it pre-trains its model with all
classes together before conducting the few-shot training,
while our DN4 does not. More importantly, our DN4 only
has one single uniﬁed network, which is much simpler than
these meta-learning based methods with additional complicated memory-addressing architectures.
4.5. Fine-grained Few-shot Classiﬁcation
Besides the generic few-shot classiﬁcation, we also conduct ﬁne-grained few-shot classiﬁcation tasks on three ﬁnegrained datasets, i.e., Stanford Dogs, Stanford Cars and
Two baseline models and three state-of-theart models are implemented on these three datasets, i.e.,
k-NN (Deep global features), NBNN (Deep local features), Matching Nets FCE , Prototypical Nets and
GNN . The results are shown in Table 2. In general,
the ﬁne-grained few-shot classiﬁcation task is more challenging than the generic one due to the smaller inter-class
and larger intra-class variations of the ﬁne-grained datasets.
It can be seen by comparing the performance of the same
methods between Tables 1 and 2. The performance of the
k-NN (Deep global features), NBNN (Deep local features)
and Prototypical Nets on the ﬁne-grained datasets is worse
than that on miniImageNet. It can also be observed that
NBNN (Deep local features) performs consistently better
than k-NN (Deep global features).
Due to the small inter-class variation of the ﬁne-grained
task, we choose k = 1 for our DN4 to avoid introducing
noisy visual patterns. From Table 2, we can see that our
DN4 performs surprisingly well on these datasets under the
5-shot setting. Especially on the Stanford Cars, our DN4
gains the largest absolute improvement over the second best
method, i.e., GNN, by 17%. Under the 1-shot setting, our
DN4 does not perform as well as in the 5-shot setting. The
key reason is that our model relies on the k-nearest neighbor
algorithm, which is a lazy learning algorithm and its performance depends largely on the number of samples. This
characteristic has been shown in Table 5, i.e., the performance of DN4 gets better and better as the number of shots
increases. Another reason is that these ﬁne-grained datasets
are not sufﬁciently large (e.g., CUB-200 only has 6033 images), resulting in over-ﬁtting when training deep networks.
To avoid over-ﬁtting, we perform data augmentation on
the training (auxiliary) sets by cropping and horizontally
ﬂipping randomly. Then, we re-train our model, i.e., DN4-
DA, on these augmented datasets but test on the original test
sets. It can be observed that our DN4-DA can obtain nearly
the best results for both 1-shot and 5-shot tasks. The ﬁnegrained recognition largely relies on the subtle local visual
patterns, and they can be naturally captured by the learnable
deep local descriptors emphasized in our model.
4.6. Discussion
Ablation study. To further verify that the image-to-class
measure is more effective than the image-to-image measure,
we perform an ablation study by developing two image-toimage (IoI for short) variants of DN4. Speciﬁcally, the ﬁrst
variant named DN4-IoI-1 concatenates all local descriptors
of an image as a high-dimensional (h × w × d) feature vector and uses the image-to-image measure. As for the second
variant (DN4-IoI-2 for short), it keeps the local descriptors
like DN4 without concatenation. The only difference between DN4-IoI-2 and DN4 is that DN4-IoI-2 restricts the
search for the k-NN of a query’s local descriptor within
each individual support image, while DN4 can search from
one entire support class. Under the 1-shot setting, DN4-IoI-
2 is identical with DN4. Both variants still adopt the k-NN
Table 3. The results of the ablation study on miniImageNet.
5-Way Accuracy (%)
37.39±0.82
50.47±0.66
51.14±0.79
69.52±0.62
51.24±0.74
71.02±0.64
Table 4. The 5-way 5-shot mean accuracy (%) of our DN4 by varying the value of k ∈{1, 3, 5, 7} during training on miniImageNet.
5-way 5-shot Accuracy (%)
search, and use k = 1 and k = 3 for 1-shot setting and
5-shot setting, respectively.
The results on miniImageNet are reported in Table 3. As
seen, DN4-IoI-1 performs clearly the worst by using the
concatenated global features with the image-to-image measure. In contrast, DN4-IoI-2 performs excellently on both
1-shot and 5-shot tasks, which veriﬁes the importance of local descriptors and the exchangeability (within one image).
Notably, DN4 is superior to DN4-IoI-2 on the 5-shot task,
which shows that utilizing the exchangeability of visual patterns within a class indeed helps to gain performance.
Inﬂuence of backbone networks.
Besides the commonly used Conv-64F, we also evaluate our model by using another deeper embedding module, i.e., ResNet-256F
used by SNAIL and Dynamic-Net .
The details
of ResNet-256F can refer to SNAIL .
When using
ResNet-256F as the embedding module, the accuracy of
DN4 reaches 54.37 ± 0.36% for the 5-way 1-shot task
and 74.44 ± 0.29% for the 5-shot task. As seen, with a
deeper backbone network, DN4 can perform better than the
case of using the shallow Conv-64F. Moreover, when using the same ResNet-256F as the embedding module, our
DN4 (ResNet-256F) can gain 4.31% improvements over
Dynamic-Net (ResNet-256F) (i.e., 70.13 ± 0.68%) under
the 5-shot setting (see Table 1).
Inﬂuence of neighbors.
In the image-to-class module, we need to ﬁnd the k-nearest neighbors in one support
class for each local descriptor of a query image. Next, we
measure the image-to-class similarity between a query image and a speciﬁc class. How to choose a suitable hyperparameter k is thus a key. For this purpose, we perform a
5-way 5-shot task on miniImageNet by varying the value
of k ∈{1, 3, 5, 7}, and show the results in Table 4. It can
be seen that the value of k has a mild impact on the performance. Therefore, in our model, k should be selected
according to the speciﬁc task.
Inﬂuence of shots. The episodic training mechanism is
popular in current few-shot learning methods. The basic
Table 5. The 5-way K-shot mean accuracy (%) of our DN4 by
varying the number of shots (K = 1, 2, 3, 4, 5) during training on
miniImageNet. For each test setting, the best result is highlighted.
rule is the matching condition between training and test. It
means that, in the training stage, the numbers of ways and
shots should keep consistent with those adopted in the test
stage. In other words, if we want to perform a 5-way 1-shot
task, the same 5-way 1-shot setting should be maintained
in the training stage. However, in the real training stage,
we still want to know the inﬂuence of mismatching conditions, i.e., under-matching condition and over-matching
condition. We ﬁnd that the over-matching condition can
achieve better performance than the matching condition,
and much better than the under-matching condition.
Basically, for the under-matching condition, we use a
smaller number of shots in the training stage, and conversely, use a larger number of shots for the over-matching
condition. We ﬁx the number of ways but vary the number of shots during training to learn several different models. Then we test these models under different shot settings,
where the number of shots is changed but the number of
ways is ﬁxed. A 5-way K-shot (K = 1, 2, 3, 4, 5) task is
conducted on miniImageNet by using our DN4. The results
are presented in Table 5, where the entries on the diagonal
are the results of the matching condition. The results in the
upper triangle are the results of the under-matching condition. Also, the lower triangle contains the results of the
over-matching condition. It can be seen that the results in
the lower triangle are better than those on the diagonal, and
the results on the diagonal are better than those in the upper
triangle. This exactly veriﬁes our statement made above.
It is also worth mentioning that if we use a 5-shot trained
model and test it on the 1-shot task, we can obtain an accuracy of 53.85%. This result is quite high in this task,
and much better than 51.24% obtained by the 1-shot trained
model using our DN4 under a matching condition.
Visualization.
We visualize the similarity matrices
learned by NBNN (Deep local features) and our DN4 under the 5-way 5-shot setting on miniImageNet.
them are image-to-class measure based models. We select
20 query images from each class (i.e., 100 query images
in total), calculate the similarity between each query image
and each class, and visualize the 5×100 similarity matrices.
From Figure 2, it can be seen that the results of DN4 are
much closer to the ground truth than those of NBNN, which
(b) Our DN4
(c) Ground Truth
Figure 2. Similarity matrices of NBNN (Deep local Features), our
DN4 and the ground truth on miniImageNet under the 5-way 5shot setting. Vertical axis denotes the ﬁve classes in the support
Horizontal axis denotes 20 query images per class.
warmer colors indicate higher similarities.
demonstrates that the end-to-end manner is more effective.
Runtime. Although NBNN performs successfully in the
literature , it did not become popular. One key reason is
the high computational complexity of the nearest-neighbor
search, especially in large-scale image classiﬁcation tasks.
Fortunately, under the few-shot setting our framework can
enjoy the excellent performance of NBNN without being
signiﬁcantly affected by its computational issue.
Generally, during training for a 5-way 1-shot or 5-shot task, one
episode (batch) time is 0.31s or 0.38s with 75 or 50 query
images on a single Nvidia GTX 1080Ti GPU and a single
Intel i7-3820 CPU. During test, it will be more efﬁcient, and
only takes 0.18s for one episode. Moreover, the efﬁciency
of our model can be further improved with optimized parallel implementation.
5. Conclusions
In this paper, we revisit the local descriptor based imageto-class measure and propose a simple and effective Deep
Nearest Neighbor Neural Network (DN4) for few-shot
We emphasize and verify the importance and
value of the learnable deep local descriptors, which are
more suitable than image-level features for the few-shot
problem and can well boost the classiﬁcation performance.
We also verify that the image-to-class measure is superior to
the image-to-image measure, owing to the exchangeability
of visual patterns within a class.
Acknowledgements
This work is partially supported by the NSF awards
1704309, 1722847, 1813709), National NSF of
China (Nos. 61432008, 61806092), Jiangsu Natural Science Foundation (No.
BK20180326), the Collaborative
Innovation Center of Novel Software Technology and Industrialization, and Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University (No.
CX201814).