The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)
DCMN+: Dual Co-Matching Network for Multi-Choice Reading Comprehension
Shuailiang Zhang,1,2,3 Hai Zhao,1,2,3∗Yuwei Wu,1,2,3
Zhuosheng Zhang,1,2,3 Xi Zhou,4 Xiang Zhou4
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China
3MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China
4CloudWalk Technology, Shanghai, China
{zsl123, will8821, zhangzs}@sjtu.edu.cn, ,
{zhouxi, zhouxiang}@cloudwalk.cn
Multi-choice reading comprehension is a challenging task
to select an answer from a set of candidate options when
given passage and question. Previous approaches usually only
calculate question-aware passage representation and ignore
passage-aware question representation when modeling the relationship between passage and question, which cannot effectively capture the relationship between passage and question.
In this work, we propose dual co-matching network (DCMN)
which models the relationship among passage, question and
answer options bidirectionally. Besides, inspired by how humans solve multi-choice questions, we integrate two reading strategies into our model: (i) passage sentence selection that ﬁnds the most salient supporting sentences to answer the question, (ii) answer option interaction that encodes
the comparison information between answer options. DCMN
equipped with the two strategies (DCMN+) obtains state-ofthe-art results on ﬁve multi-choice reading comprehension
datasets from different domains: RACE, SemEval-2018 Task
11, ROCStories, COIN, MCTest.
Introduction
Machine reading comprehension (MRC) is a fundamental
and long-standing goal of natural language understanding
which aims to teach machine to answer question automatically according to given passage .
In this paper, we focus on multi-choice MRC tasks such as
RACE which requests to choose the right
option from a set of candidate answers according to given
passage and question. Different from MRC datasets such as
SQuAD and NewsQA where the expected answer is usually in the form
of a short span from the given passage, answer in multichoice MRC is non-extractive and may not appear in the
original passage, which allows rich types of questions such
∗Corresponding author. This paper was partially supported
by National Key Research and Development Program of China
(No. 2017YFB0304100), Key Projects of National Natural Science
Foundation of China (U1836222 and 61733011).
Copyright c⃝2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Passage: Runners in a relay race pass a stick in one
direction. However, merchants passed silk, gold, fruit,
and glass along the Silk Road in more than one direction. They earned their living by traveling the famous
Silk Road. ... The Silk Road was made up of many
routes, not one smooth path. They passed through
what are now 18 countries. The routes crossed mountains and deserts and had many dangers of hot sun,
deep snow and even battles...
Question: The Silk Road became less important because
A. it was made up of different routes
B. silk trading became less popular
C. sea travel provided easier routes
D. people needed fewer foreign goods
Table 1: An example passage with related question and options from RACE dataset. The ground-truth answer and the
evidence sentences in the passage are in bold.
as commonsense reasoning and passage summarization, as
illustrated by the example in Table 1.
Pre-trained language models such as BERT and XLNet have achieved signiﬁcant improvement on various MRC tasks. Recent works on
MRC may be put into two categories, training more powerful language models or exploring effective applying pattern of the language models to solve speciﬁc task. There
is no doubt that training a better language model is essential and indeed extremely helpful but at the same time it is time-consuming
and resource-demanding to impart massive amounts of general knowledge from external corpora into a deep language
model via pre-training .
For example, training a 24-layer transformer requires 64 TPUs for 4 days. So from the practical viewpoint, given limited computing resources and a
well pre-trained model, can we improve the machine reading comprehension during ﬁne-tuning instead of via expensive full pre-training? This work starts from this viewpoint
and focuses on exploring effective applying pattern of lan-
Answer Options
Bidirectional
MaxPooling
Concatenation
Option Interaction
Sentence Selection
Bidirectional Matching
Figure 1: The framework of our model. P-Passage, Q-Question, O-Option.
guage models instead of presenting better language models
to furthermore enhance state-of-the-art multi-choice MRC.
We will show the way to use a strong pre-trained language
model may still have a heavy impact on MRC performance
no matter how strong the language model itself is.
To well handle multi-choice MRC problem, an effective
solution has to carefully model the relationship among the
triplet of three sequences, passage (P), question (Q) and answer candidate options (A) with a matching module to determine the answer. However, previous unidirectional matching strategies usually calculate question-aware passage representation and ignore passage-aware question representation when modeling the relationship between passage and
question .
To alleviate such an obvious defect in modeling the {P,
Q, A} triplet from existing work, we propose dual comatching network (DCMN) which bidirectionally incorporates all the pairwise relationships among the {P, Q, A}
triplet. In detail, we model the passage-question, passageoption and question-option pairwise relationship simultaneously and bidirectionally for each triplet, exploiting the
gated mechanism to fuse the representations from two directions. Besides, we integrate two reading strategies which humans usually use into the model. One is passage sentence selection that helps extract salient evidence sentences from the
given passage, and then matches evidence sentences with answer options. The other is answer option interaction that encodes comparison information into each option. The overall
framework is shown in Figure 1. The output of pre-trained
language model and XLNet
 ) is used as the contextualized encoding.
After passage sentence selection and answer option interaction, bidirectional matching representations are built for
every pairwise relationship among the {P, Q, A} triplet.
Our model achieves new state-of-the-art results on the
multi-choice MRC benchmark challenge RACE . We further conduct experiments on four representative multi-choice MRC datasets from different domains , SemEval-2018 Task
11 , MCTest , COIN Shared Task 1 ) and achieve the absolute improvement of 4.9% and
2.8% in average accuracy from directly ﬁne-tuned BERT
and XLNet, respectively, which indicates our method has
a heavy impact on the MRC performance no matter how
strong the pre-trained language model itself is.
Our Proposed Model
The illustration of our model is shown in Figure 1. The major components of the model are Contextualized Encoding,
Passage Sentence Selection, Answer Option Interaction and
Bidirectional Matching. We will discuss each component in
Task Deﬁnition
For the task of multi-choice reading comprehension, the machine is given a passage (P), a question (Q), and a set of answer candidate options (A) to select the correct answer from
the candidates, where P = {p1, p2, ..., pn} is the passage
composed of n sentences, A = {A1, A2, ..., Am} is the
option set with m answer candidates.
Contextualized Encoding
In this work, pre-trained language models are used as the
encoder of our model which encodes each token in passage
and question into a ﬁxed-length vector. Given an encoder,
the passage, the question, and the answer options are encoded as follows:
Hp = Encode(P), Hq = Encode(Q)
Ha =Encode(A)
where Encode(·) returns the last layer output by the encoder, which can be well pre-trained language models such
as BERT and XLNet , as using transformer as the contextualized encoder
has shown to be very powerful in language representation
 . Hp ∈R|P |×l, Hq ∈R|Q|×l, and
Ha ∈R|A|×l are sequence representation of passage, question and answer option, respectively. |P|, |Q|, |A| are the
sequence length, respectively. l is the dimension of the hidden state.
Soon after, the snack came out. I then opened the chips and
started to enjoy them, before enjoying the soda. I had a great
little snack...
What else did the person enjoy?
She lived in the house across the street that had 9 people and
3 dogs and another cat living in it. She didn’t seem very happy
there, especially with a 2 year old that chased her and grabbed
her. The other people in the house agreed...
What did the 2 year old’s
When I was hungry last week for a little snack and a soda, I
went to the closest vending machine. I felt that it was a little
overpriced, but being as though I needed something...
What’s the main idea of this
Table 2: Analysis of the sentences in passage required to answer questions on RACE and COIN. 50 examples from each dataset
are sampled randomly. N sent indicates the number of sentences required to answer the question. The evidence sentences in the
passage are in emphasis and the correct answer is with bold.
Passage Sentence Selection
Existing multi-choice MRC models learn the passage representation with all the sentences in one-shot, which is inefﬁcient and counter-intuitive. To explore how many sentences are necessarily required to answer the question, we
randomly extract 50 examples from the development set of
RACE and COIN, as shown in Table 2. Among all examples, 87% questions on RACE and 86% on COIN can be
answered within two evidence sentences. From this observation, the model should be extremely beneﬁcial if focusing
on a few key evidence sentences.
To select the evidence sentences from the passage P =
{p1, p2, .., pi, .., pn}, this module scores each sentence pi
with respect to the question Q and answer option A in parallel. The top K scored sentences will be selected. This
module shares the encoder with the whole model. For each
{pi, Q, A} triplet, Hpi ∈R|pi|×l, Hq, and Ha are all representations offered by the encoder. Here we introduce two
methods to compute the score of the triplet based on the representations.
• Cosine score: The model computes word-by-word cosine
similarity between the sentence and question-option sequence pair.
Dpa = Cosine(Ha, Hpi) ∈R|A|×|pi|
Dpq = Cosine(Hq, Hpi) ∈R|Q|×|pi|
pa = MaxPooling(Dpa) ∈R|A|
pq = MaxPooling(Dpq) ∈R|Q|
where Dpa, Dpa are the distance matrices and Dpa
cosine similarity between the i-th word in the candidate
option and the j-th word in the passage sentence.
• Bilinear score: Inspired by , we compute the bilinear weighted distance between two sequences, which can be calculated as follows:
α = SoftMax(HqW1) ∈R|Q|×l
q = αT Hq ∈Rl
j W2q ∈Rl, j ∈[1, |pi|]
pq = Max(¯P1¯P2, ..., ¯P|pi|) ∈Rl
where W1, W2 ∈Rl×l are learnable parameters, ˆP
the bilinear similarity vector between the passage sentence and question. Similarly, the vector ˆP
pa between the
passage sentence and answer can be calculated with the
same procedure. The ﬁnal score can be computed as follows:
score = W T
where W3, W4 ∈Rl are learnable parameters.
After scoring each sentence, top K scored sentences are
selected and concatenated together as an updated passage Ps
to replace original full passage. So the new sequence triplet
is {Ps, Q, A} and the new passage is represented as Hps.
Answer Option Interaction
Human solving multi-choice problem may seek help from
comparing all answer options. For example, one option has
to be picked up not because it is the most likely correct, but
all the others are impossibly correct. Inspired by such human experience, we introduce the comparison information
among answer options so that each option is not independent
of the other. Here we build bilinear representations between
any two options. Gated mechanism is used to fuse interaction representation into the original answer option representations.
The encoder encodes each answer option Ai as Hai. Then
the comparison vector between option Ai and Aj can be
computed as follows:
G = SoftMax(HaiW5Haj T ) ∈R|Ai|×|Aj|
Hai,j = ReLU(GHaj) ∈R|Ai|×l
where W5 ∈Rl×l is one learnable parameter, G is the bilinear interaction matrix between Ai and Aj, Hai,j is the
interaction representation. Then gated mechanism is used to
fuse interaction representation into the original answer option representations as follows:
ai = [{Hai,j}j̸=i] ∈R|Ai|×(m−1)l
aiW6 ∈R|Ai|×l
aiW7 + HaiW8 + b)
Hoi = g ∗Hai + (1 −g) ∗¯H
where W7, W8 ∈Rl×l and W6 ∈R(m−1)l×l are learnable
parameters, ˆH
ai is the concatenation of all the interaction
representations. g ∈R|Ai|×l is a reset gate which balances
the inﬂuence of ¯H
ai and Hai, and Hoi is the ﬁnal option representation of Ai encoded with the interaction information.
At last, we denote O = {Ho1, Ho2, ..., Hom} as the ﬁnal
answer option representation set fused with comparison information across answer options.
Bidirectional Matching
The triplet changes from {P, Q, A} to {Ps, Q, O} with
passage sentence selection and answer option interaction.
To fully model the relationship in the {Ps, Q, O} triplet,
bidirectional matching is built to get all pairwise representations among the triplet, including passage-answer, passagequestion and question-answer representation. Here shows
how to model the relationship between question-answer sequence pair as an example and it is the same for the other
two pairs.
Bidirectional matching representation between the question Hq and answer option Ho can be calculated as follows:
Gqo = SoftMax(HqW9HoT )
Goq = SoftMax(HoW10HqT )
Eq = GqoHo, Eo = GoqHq
Sq = ReLU(EqW11)
So = ReLU(EoW12)
where W9, W10, W11, W12
∈Rl×l are learnable parameters. Gqo ∈R|Q|×|O| and Goq ∈R|O|×|Q| are the
weight matrices between question and answer option. Eq ∈
R|Q|×l, Eo ∈R|A|×l represent option-aware question representation and question-aware option representation, respectively. The ﬁnal representation of question-answer pair is
calculated as follows:
Sq o = MaxPooling(Sq)
So q = MaxPooling(So)
g = σ(Sq oW13 + So qW14 + b)
Mq o = g ∗So q + (1 −g) ∗So q
where W13, W14 ∈Rl×l and b ∈Rl are three learnable parameters. After a row-wise max pooling operation, we get
the aggregation representation Mq ∈Rl and Mo ∈Rl.
g ∈Rl is a reset gate. Mq o ∈Rl is the ﬁnal bidirectional matching representation of the question-answer sequence pair.
Passage-question and passage-option sequence matching
representation Mp q, Mp o ∈Rl can be calculated in the
same procedure from Eq.(7) to Eq.(8). The framework of
this module is shown in Figure 1.
Objective Function
With the built matching representations Mp q, Mp o, Mq o
for three sequence pairs, we concatenate them as the ﬁnal
representation C ∈R3l for each passage-question-option
triplet. We denote the representation Ci for each {Ps, Q, Oi}
triplet. If Ak is the correct option, then the objective function
can be computed as follows:
C = [Mp q; Mp o; Mq o]
L(Ak|P, Q) = −log
exp(V T Ck)
j=1 exp(V T Cj)
where V ∈R3l is a learnable parameter and m is the number
of answer options.
Experiments
We evaluate our model on ﬁve multi-choice MRC datasets
from different domains. Statistics of these datasets are detailed in Table 3. Accuracy is calculated as acc = N +/N,
where N + and N are the number of correct predictions
and the total number of questions. Some details about these
datasets are shown as follows:
• RACE : RACE consists of two subsets: RACE-M and RACE-H respectively corresponding
to middle school and high school difﬁculty levels, which
is recognized as one of the largest and most difﬁcult
datasets in multi-choice reading comprehension.
• SemEval-2018 Task11 : Multichoice questions should be answered based on narrative
texts about everyday activities.
• ROCStories : This dataset
contains 98,162 ﬁve-sentence coherent stories in the training dataset, 1,871 four-sentence story contexts along with
a right ending and a wrong ending in the development and
test datasets, respectively.
• MCTest : This
task requires machines to answer questions about ﬁctional
stories, directly tackling the high-level goal of opendomain machine comprehension.
narrative text
ROCStories
everyday scenarios
Table 3: Statistics of multi-choice machine reading comprehension datasets. #o is the average number of candidate options for each question. #p and #q are the number of documents and questions in the dataset.
HAF 
MRU 
HCM 
MMN 
GPT 
RSM 
OCN 
XLNet 
BERTlarge∗
XLNetlarge∗
Our Models
BERTbase∗+ DCMN
BERTlarge∗+ DCMN
BERTlarge∗+ DCMN + PSS + AOI
XLNetlarge∗+ DCMN + PSS + AOI
Human Performance
Table 4: Experiment results on RACE test set. All the results are from single models. PSS: Passage Sentence Selection; AOI: Answer Option Interaction. ∗indicates our implementation.
• COIN Task 1 : The data for the
task is short narrations about everyday scenarios with
multiple-choice questions.
Implementation Details
Our model is evaluated based on the pre-trained language
model BERT and XLNet which both have small and large versions. The basic version BERTbase has 12-layer transformer blocks, 768
hidden-size, and 12 self-attention heads, totally 110M parameters. The large version BERTlarge has 24-layer transformer blocks, 1024 hidden-size, and 16 self-attention
heads, totally 340M parameters. Two versions of XLNet
have the similar sizes as BERT.
In our experiments, the max input sequence length is set
to 512. A dropout rate of 0.1 is applied to every BERT layer.
We optimize the model using BertAdam 
optimizer with a learning rate 2e-5. We train for 10 epochs
with batch size 8 using eight 1080Ti GPUs when BERTlarge
and XLNetlarge are used as the encoder. Batch size is set to
16 when using BERTbase and XLNetbase as the encoder1.
Evaluation and Ablation Study on RACE
Table 4 reports the experimental results on RACE and its two
subtasks: RACE-M and RACE-H. In the table, Turkers is
the performance of Amazon Turkers on a randomly sampled
subset of the RACE test set and Ceiling is the percentage of
the unambiguous questions with a correct answer in a subset
of the test set. Here we give the results of directly ﬁne-tuned
BERTbase, BERTlarge and XLNetlarge on RACE and get the
accuracy of 65.0%, 72.0% and 80.1%, respectively. Because
1Our code is at 
XLNetlarge
base encoder
66.0 (+1.4)
73.8 (+2.0)
81.5 (+1.4)
+ DCMN + P SS
66.6 (+2.0)
74.6 (+2.8)
82.1 (+2.0)
+ DCMN + P OI
66.8 (+2.2)
74.4 (+2.6)
82.2 (+2.1)
+ DCMN + ALL (DCMN+)
67.4 (+2.8)
75.4 (+3.6)
82.6 (+2.5)
Table 5: Ablation study on RACE dev set. PSS: Passage Sentence Selection. AOI: Answer Option Interaction. DCMN+:
DCMN + PSS + AOI.
of the limited computing resources, the largest batch size can
only be set to 8 in our experiments which leads to 1.7% decrease (80.1% vs. 81.8%) on XLNet compared to the result
reported in 2.
The comparison indicates that our proposed method obtains signiﬁcant improvement over pre-trained language
models (75.8% vs. 72.0% on BERTlarge and 82.8% vs. 80.1%
on XLNetlarge) and achieves the state-of-the-art result on
In Table 5, we focus on the contribution of main components (DCMN, passage sentence selection and answer option interaction) in our model. From the results, the bidirectional matching strategy (DCMN) gives the main contribution and achieves further improvement by integrating with
the two reading strategies. Finally, we have the best performance by combining all components.
Evaluation on Other Multi-choice Datasets
The results on four other multi-choice MRC challenges are
shown in Table 6. When adapting our method to the nonconventional MRC dataset ROCStories which requires to
choose the correct ending to a four-sentence incomplete
story from two answer options ,
the question context is left empty as no explicit questions
are provided. Passage sentence selection is not used in this
dataset because there are only four sentences as the passage.
Since the test set of COIN is not publicly available, we report the performance of the model on its development set.
As shown in Table 6, we achieve state-of-the-art (SOTA)
results on all datasets and obtain 3.1% absolute improvement in average accuracy over the previous average SOTA
(88.9% vs. 85.8%) by using BERT as encoder and 4.8%
(90.6% vs. 85.8%) by using XLNet as encoder. To further
investigate the contribution of our model, we also report
the results of directly ﬁne-tuned BERT/XLNet on the target
datasets. From the comparison, we can see that our model
obtains 4.9% and 2.8% absolute improvement in average accuracy over the baseline of directly ﬁne-tuned BERT (88.9%
vs. 84.0%) and XLNet (90.6% vs. 87.8%), respectively.
These results indicate our proposed model has a heavy impact on the performance no matter how strong the adopted
pre-trained language model itself is.
Comparison with Unidirectional Methods
Here we focus on whether the bidirectional matching
method works better than previous unidirectional methods.
2The implementation is very close to the result 80.3% in when using batch size 8 on RACE.
Previous STOA
DCMN XLNet
SemEval Task 11
 
91.8 (+1.3)
93.4 (+1.4)
ROCStories
 
92.4 (+1.6)
95.8 (+2.0)
MCTest-MC160
 
85.0 (+11.2)
86.2 (+5.6)
MCTest-MC500
 
86.5 (+6.1)
86.6 (+3.2)
COIN Task 1
 
88.8 (+4.5)
91.1 (+2.0)
88.9 (+4.9)
90.6 (+2.8)
Table 6: Results on the test set of SemEval Task 11, ROCStories, MCTest and the development set of COIN Task 1. The test
set of COIN is not public. DCMN BERT: BERT + DCMN + PSS + AOI. Previous SOTA: previous state-of-the-art model. All
the results are from single models.
+ Unidirectional
[SP O; SP Q; SO Q]
[SP Q; SQ O]
[SP Q; SO Q]
[SP O; SQ P ; SO Q]
[SP O; SQ O]
[SQ P ; SO Q]
[SP Q; SP O] (HCM)
[SP O; SO Q]
[SP Q; SO P ]
[SP O; SP Q; SQ O] (HAF)
[SP O; SQ P ; SQ O]
[SQ P ; SQ O]
[SQ O; SO Q; SP Q; SP O] (MMN)
+ Bidirectional
[M P Q; M P O]
[M P O; M Q O]
[M P Q; M Q O]
[M P Q; M P O; M Q O] (DCMN)
Table 7: Performance comparison with different combination methods on the RACE dev set. (HCM) ,
(HAF) , (MMN) are previous methods. We use BERTbase as our encoder here. [; ]
indicates the concatenation operation. SP O and M P O are the unidirectional and bidirectional representation referred in Eq. 8.
Table 8: Results on RACE and COIN dev set with cosine
and bilinear score in PSS. We use BERTbase as encoder here.
In Table 7, we enumerate all the combinations of unidirectional matching strategies3 which only use passage-aware
question representation SQ P or question-aware passage
representation SP Q when modeling the relationship between the passage and question. Specially, we roughly summarize the matching methods in previous work (i.e. HCM,
HAF, MMN) using our model notations which meet their
general ideas except some calculation details.
From the comparison, we observe that previous matching
strategies (HCM 64.4%, HAF 64.2%, MMN 63.2%) fail to
give further performance improvement over the strong encoder (64.6%). In contrast, all bidirectional combinations
work better than the encoder. All three pairwise matching
representations (M P Q, M P O, M Q O) are necessary and
by concatenating them together, we achieve the highest performance (67.1%).
3Here we omit the combinations with SO P because we ﬁnd the
combinations with SP O works better than SO P .
Results with Different Settings in PSS
Table 8 shows the performance comparison with different
scoring methods, and we observe that both methods have
their advantages and disadvantages. Cosine score method
works better on COIN dataset (83.5% vs. 82.8%) and bilinear score works better on RACE dataset (66.8% vs. 66.5%).
Figure 2 shows the results of passage sentence selection
(Pss) on COIN and RACE dev set with different numbers of
selected sentences (Top K). The results without Pss module
are also shown in the ﬁgure (RACE-w and COIN-w). We
observe that Pss mechanism consistently shows a positive
impact on both datasets when more than four sentences are
selected compared to the model without Pss (RACE-w and
COIN-w). The highest performance is achieved when top
3 sentences are selected on COIN and top 5 sentences on
RACE where the main reason is that the questions in RACE
are designed by human experts and require more complex
reasoning.
Why Previous Methods Break Down?
As shown in Table 7, applying previous models to a strong
BERT encoder fails to give performance increase over directly ﬁne-tuned BERT. The contrast is clear that our proposed model achieves more than 3.8% absolute increase
over the BERT baseline. We summarize the reasons resulting in such contrast as follows: (i) the unidirectional representations cannot well capture the relationship between two
sequences, (ii) previous methods use elementwise subtraction and multiplication to fuse Eq and Ho in Eq. 7 (i.e.,
Figure 2: Results of sentence selection on dev sets of
RACE and COIN when selecting different numbers of sentences (Top K). We use BERTbase as encoder and cosine
score method here. RACE/COIN-w indicates the results on
RACE/COIN without passage sentence selection module.
[Eq ⊖Ho; Eq ⊗Ho]) which is shown suboptimal as such processing breaks the symmetry of equation. Symmetric representations from both directions show essentially helpful for
bidirectional architecture.
Evaluation on Different Types of Questions
Inspired by , we further analyze the performance of the main components on different question types.
Questions are roughly divided into ﬁve categories: detail,
inference, main, attitude and vocabulary . We annotate all the instances of the
RACE development set. As shown in Figure 3, all the combinations of components work better than the BERT baseline in most question types. Bidirectional matching strategy
(DCMN) consistently improves the results across all categories. DCMN+PSS works best on the inference and attitude
categories which indicates PSS module may effectively improve the reasoning ability of the model. DCMN+AOI works
better than DCMN on detail and main categories which indicates that the model achieves better distinguish ability with
answer option interaction information.
瀉瀂濶濴濵瀈濿濴瀅瀌
濴濶濶瀈瀅濴濶瀌澳澻澸澼
Figure 3: Results on different question types, tested on the
RACE dev set. BERTlarge is used as encoder here. OI: Answer Option Interaction. SS: Passage Sentence Selection.
Related Work
Neural network based methods have been applied to several natural language processing tasks, especially to MRC
 .
The task of selecting sentences to answer the question
has been studied across several question-answering (QA)
datasets, by modeling the relevance between a sentence
and the question . apply distant supervision to generate imperfect labels and then use them to train a neural
evidence extractor. propose a simple sentence selector to select the minimal set of sentences then
feed into the QA model. They are different from our work
in that (i) we select the sentences by modeling the relevance among sentence-question-option triplet, not sentencequestion pair. (ii) Our model uses the output of language
model as the sentence embedding and computes the relevance score using these sentence vectors directly, without
the need of manually deﬁned labels. (iii) We achieve a generally positive impact by selecting sentences while previous
sentence selection methods usually bring performance decrease in most cases.
Most recent works attempting to integrate answer option
interaction information focus on building attention mechanism at word-level whose performance increase
is very limited. Our answer option interaction module is different from previous works in that: (i) we encode the comparison information by modeling the bilinear representation
among the options at sentence-level which is similar to modeling passage-question sequence relationship, other than attention mechanism. (ii) We use gated mechanism to fuse the
comparison information into the original answer option representations.
Conclusion
This paper proposes dual co-matching network integrated
with two reading strategies (passage sentence selection and
answer option interaction) to enhance multi-choice machine
reading comprehension. In terms of strong pre-trained language models such as BERT and XLNet as encoder, our proposed method achieves state-of-the-art results on ﬁve representative multi-choice MRC datasets including RACE. The
experiment results consistently indicate the general effectiveness and applicability of our model.