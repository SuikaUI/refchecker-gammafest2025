Convolutional Gaussian Embeddings for
Personalized Recommendation with Uncertainty
Junyang Jiang1 , Deqing Yang2 ∗, Yanghua Xiao1,3 and Chenlu Shen2
1School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, China
2School of Data Science, Fudan University, China
3Shanghai Institute of Intelligent Electronics & Systems, China
{jiangjy15, yangdeqing, shawyh, clshen17}@fudan.edu.cn
Most of existing embedding based recommendation models use embeddings (vectors) corresponding to a single ﬁxed point in low-dimensional
space, to represent users and items.
Such embeddings fail to precisely represent the users/items
with uncertainty often observed in recommender
systems. Addressing this problem, we propose a
uniﬁed deep recommendation framework employing Gaussian embeddings, which are proven adaptive to uncertain preferences exhibited by some
users, resulting in better user representations and
recommendation performance.
Furthermore, our
framework adopts Monte-Carlo sampling and convolutional neural networks to compute the correlation between the objective user and the candidate item, based on which precise recommendations are achieved. Our extensive experiments on
two benchmark datasets not only justify that our
proposed Gaussian embeddings capture the uncertainty of users very well, but also demonstrate its
superior performance over the state-of-the-art recommendation models.
Introduction
Recommender systems have demonstrated great commercial value in the era of information overload, because they
help users ﬁlter our their favorite items precisely from large
repositories.
No matter in traditional matrix factorization
(MF for short) based models [Lee and Seung, 2000; Koren, 2008] or in recent deep neural models [He et al., 2017;
He and Chua, 2017], users and items are generally represented as low-dimensional vectors, also known as embeddings, which are learned from observed user-item interactions or user/item features.
In these models, a user/item
representation is a single ﬁxed point of the continuous vector space, which represents a user’s preferences or an item’s
characteristics. Then, the ﬁnal recommendation results are
generated through computing the correlations between user
embeddings and item embeddings, such as inner product of
two embeddings [He et al., 2018b] or feeding them into
∗Contact Author
multi-layer perceptron (MLP for short) [Shen et al., 2019;
He et al., 2017].
Despite their successes, one unneglectable limitation of
these embedding-based models is the lack of handling uncertainty. In a recommender system, users may induce uncertainty due to some reasons. One reason is the lack of discriminative information [Zhu et al., 2018], especially for those
users who have very few or even no observed user-item interactions, e.g., historical ratings or reviews for items. Even
for the users who have sufﬁcient interactions, uncertainty
may also be caused by diversity [Bojchevski and G¨unnemann,
2018], e.g., some users exhibit many and very distinct genres
of preferences. We illustrate the example in Figure 1 to explain why the embeddings corresponding to ﬁxed points can
not well handle such cases. Suppose user u has rated movie
m1 and m2 with high scores and these two movies belong to
very distinct genres which are labeled with different colors. If
we use ﬁxed embeddings learned from observed user-movie
interactions to represent users and movies, u’s position in the
embedding space (mapped into a 2D map) may locate in the
middle of m1 and m2. If the recommendation is made based
on the distance between the embedding positions of u and the
candidate movies, u may be recommended to with movie m4
of the genre different from m1 and m2, instead of m3 of the
same genre as m2, because u is closer to m4 than m3. There
is another case that u’s position may be closer to m2 than to
m1, then m3 still has fewer chances to be recommended to u.
In recent years, some researchers have employed Gaussian embeddings to learn the representations of words [Vilnis and McCallum, 2014] and nodes in a graph [Bojchevski
and G¨unnemann, 2018; Zhu et al., 2018] because of their
fixed point of user
fixed point of movie
distribution range of
Gaussian embedding
Figure 1: The ﬁxed points of user/item embeddings can not well represent the users/items with uncertainty, resulting in inaccurate recommendation. While distribution based embeddings handle the ones
with uncertainty well.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
advantage on capturing the uncertainty in representations.
It motivates us to employ Gaussian embeddings in recommender systems to represent users and items more ﬂexibly.
In Gaussian embedding based mechanism, each user or item
embedding corresponds to a Gaussian distribution of which
the mean and the variance are learned from observed useritem interactions. In other words, each user or item is represented by a density instead of a ﬁxed point in latent feature space. The variance of the Gaussian distribution of a
user/item measures uncertainty associated to the user/item’s
representation. Recall the example in Figure 1, if u and all
movies are represented by Gaussian embeddings, their positions in the space are the distribution ranges labeled by the
dashed ellipses rather than ﬁxed points. As depicted in the
ﬁgure, u’s range may overlap m3’s range other than m4’s
Thus precise recommendation results for the users
with uncertainty are achieved.
Most of existing Gaussian embeddings based models are
learned based on ranking-based loss [Dos Santos et al., 2017;
Vilnis and McCallum, 2014], which is not applicable to the
tasks other than learning to rank, such as predicting a value
or classiﬁcation. This is because metrics used in previous
work, such as KL-divergence, take on a more limited range of
values, which is not enough for the input of a classiﬁer [Vilnis
and McCallum, 2014]. Besides, models for rate prediction
rely on an absolute, rather than relative manner. Therefore
it is not feasible to employ such a ranking scheme for the
recommendation tasks other than ranking candidate items.
In this paper, we propose a recommendation framework in
terms of implicit feedback [He et al., 2017; 2018b] rather than
only ranking candidate items. As a result, we adopt a learning principle different from previous ranking-based Gaussian
embedding models. Speciﬁcally, our framework ﬁrst learns a
Gaussian distribution for each user and item. Then, according
to the distribution, a group of samples is generated through
Monte-Carlo sampling [Hastings, 1970] for the objective user
and the candidate item, respectively. The generated samples
are used to compute the correlation between the user and
the item, based on which precise recommendation results are
achieved. Furthermore, in order to compute the correlation
between the user and the item effectively, our framework incorporates convolutional neural network (CNN for short) to
extract and compress the features from the user-item sample pair. Our experiment results have proven such convolutional operation is more effective than previous average-based
method [Oh et al., 2018].
In our framework, if the user and the item are regarded as
two objects, the correlation computed based on their Gaussian embeddings actually quantiﬁes the matching degree of
the two objects. Therefore, our framework can be extended
to other machine learning tasks such as link prediction and
classiﬁcation.
In summary, the contributions of our work include:
1. We employ Gaussian embeddings into recommender
systems to represent users and items, which is more effective
than traditional embeddings of ﬁxed points.
2. We adopt convolutional operations to learn the correlation between the Gaussian embeddings of an objective user
and a candidate item efﬁciently, which is critical to generating
precise recommendation results.
3. The extensive experiments conducted on two benchmark
datasets justify that our proposed Gaussian embeddings capture the uncertainty of some users well, and demonstrate our
framework’s superiority over the state-of-the-art recommendation models.
The rest of this paper is organized as follows. We present
the design details of our framework in Section 2, and show
our experimental results in Section 3. In Section 4, we introduce related work and conclude our work in Section 5.
Methodology
Problem Statement
In the following introduction, we use a bold uppercase to represent a matrix or a cube, and a bold lowercase to represent a
vector unless otherwise speciﬁed,.
Implicit Feedback
We design our framework in terms of implicit feedback which
is also focused in many recommendation models [Fuzheng et
al., 2016; He et al., 2017; 2018b]. Given a user u and an item
v, we deﬁne observed u’s implicit feedback to v as
if u interacts with v, such as rating or review
The task of our framework is predicting a given objective
user u’s implicit feedback to a candidate item v, which is
essentially a binary classiﬁcation model. Accordingly, our
framework should estimate the probability that u’s implicit
feedback to v is 1, which is denoted as ˆyuv in this paper.
Gaussian Embedding
In our framework, each user or item is represented by a
Gaussian distribution consisting of a expectation embedding
(vector) and a covariance matrix, i.e., g = N(µ, Σ) where
µ ∈RD, Σ ∈RD×D, and D is embedding dimension.
Such latent representations should preserve the similarities
between users and items in the embedding space, based on
which the correlations between users and items are evaluated.
As a result, given a user u and an item v, our framework tries
to evaluate ˆyuv based on gu and gv.
More speciﬁcally, to limit the complexity of the model
and reduce the computational overhead, we assume that
the embedding dimensions are uncorrelated [Bojchevski and
G¨unnemann, 2018]. Thus Σ is considered as a diagonal covariance matrix diag(Σ1, Σ2, · · · , ΣD) and can be further
represented by a D-dimensional array.
Algorithm Objective
Before describing the details of our proposed framework, we
ﬁrst summarize our algorithm’s objective. Formally, we use
p(l|u, v) to denote the probability that the matching degree
between user u and item v is l. In our scenario of implicit
feedback, l is either 1 or 0, and we denote p(l = 1|u, v) as
ˆyuv. Therefore, the p(l = 1|u, v) of high value indicates
that we should recommend v to u.
If l is labeled with a
rating score, p can be used to predict u’s rating score on v,
which reﬂects the degree of u’s preference to v. Moreover,
p(l = 1|u, v) can be used to indicate a classiﬁcation task if l
is regarded as class label.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
According to the aforementioned problem deﬁnition,
p(l|u, v) is estimated with p(l|gu, gv). Recall that Gaussian
distribution is a probability distribution of a random variable,
so we calculate p(l|gu, gv) as
p(l|gu, gv) =
p(l|zu, zv)p(zu|gu)p(zv|gv)dzudzv (1)
where zu, zv ∈RD are the vectors of random variables sampled based on Gaussian distribution gu and gv, respectively.
To approximate the integration in Eq.1, we adopt Monte-
Carlo sampling [Hastings, 1970]. Speciﬁcally, suppose that
we sample zu ∼gu and zv ∼gv for K times, then we have
p(l|gu, gv) = lim
The calculation of Eq.2 is challenging. On one hand, a
large K incurs unbearable computational cost. On the other
hand, a small K incurs bias, resulting in unsatisfactory recommendation results. What is more, it is not trivial to compute p(l|zi
v). In fact, we can rewrite Eq.2 as
p(l|gu, gv) = p
This formula implies that p is computed based on K2 correlations of vector pair (zi
v). Inspired by CNN’s power on
extracting and compressing features in image processing, we
choose a CNN fed with the K2 vector pairs to compute Eq.3,
in which the convolution kernels are used to learn the pairwise correlations of (zi
v). The computation details will
be introduced in the next subsection. Our experiment results
will prove that the CNN-based computation of Eq.3 is more
effective than computing the mean of p(l|zi
Framework Description
Embedding Layer
The ﬁrst layer is the embedding layer. At ﬁrst, a user u and
an item v are represented as a one-hot vector of D dimensions, denoted as eu ∈RD and ev ∈RD, respectively.
Besides u/v’s ID, the dimensions of value 1 in eu/v can
also correspond to u/v’s feature IDs. In our experiments,
we only input user/item IDs into our framework. Furthermore, we initialize four embedding matrices U, P ∈RM×D
and V , Q ∈RN×D where M and N are user (or user feature) number and item (or item feature) number, respectively.
Then, we have the Gaussian mean vector and variance vector
of u and v through the following lookup operation,
µu = U T eu, Σu = ELU(P T eu) + 1
µv = V T ev, Σv = ELU(QT ev) + 1
where 1 ∈RD is an array ﬁlled with value 1 and ELU is
Exponential Linear Unit. Both of them are used to guarantee
that every element of variance vector is non-negative. Thus,
we get gu = N(µu, Σu) and gv = N(µv, Σv).
feature extraction
predictionlayer
interactionlayer
convolution
convolution
Figure 2: The overview of our recommendation framework.
Interaction Layer
The second layer is the interaction layer, in which K samples
are sampled for u and v according to the Monte-Carlo sampling under hu and gv, respectively. In order to perform backpropagation, we use the reparameterization trick in [Kingma
and Max, 2013] to obtain the embedding of u’s i-th sample
as follows
u = µu + Σ
where ϵ is an auxiliary noise variable ϵ ∼N(0, 1) and varies
in each sample. So does zv.
As stated in subsection 2.1, p(l|gu, gv) is computed based
on the correlations of K2 sample pairs. Hence in this layer
we construct a big interaction map E consisting of K2 units.
Each unit E(i,j) represents the correlation of a sample pair
v), which has the following expression,
E(i,j) = [zi
where [·, ·] is the concatenation of two vectors. As a result, E
is actually a cube of K×K×2D dimension. Then, we should
utilize E to compute p(l|gu, gv), which is implemented in the
next layer.
We note that other interaction operations of two vectors,
such as inner product and element-wise product, are also
widely used in other recommendation models. But our empirical results show that concatenation outperforms other functions consistently. One possible explanation is concatenation
preserves original feature of two vectors and thus neural networks can better learn their proximity.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Feature Extraction Layer
The input of this layer is the output of the preceding interaction layer, i.e., the cube E. In fact, E contains rich features
that are beneﬁcial to compute Eq.3. It is analogous to an image containing pixel features except that the number of channels is 2D. Inspired by the usage of CNN for extracting and
compressing object features which has been proven effective
in the ﬁeld of computer image processing [Krizhevsky et al.,
2012], we employ a multi-layer CNN followed by an MLP in
this feature extraction layer.
Speciﬁcally, for each layer of the CNN, we apply T ﬁlters
G ∈Rlk×lk×c to extract speciﬁc local patterns where lk × lk
is kernel (window) size of the ﬁlter, and c is channel number.
We feed E into the ﬁrst layer of our CNN to generate its
output S1 as follows
S1 = ReLU(G1 ⊗E + b1)
where G1 ∈Rlk×lk×2D, ⊗is convolution operator and b1
is bias. In general, T is set to a large number such as 32 or
64, which helps us learn more than one correlation of each
vector pair. Besides, one ﬁlter computes the correlation of
exactly one vector pair if we set lk=1. Otherwise, the ﬁlter
extracts features from adjacent vector pairs. In different layers of the CNN, we can set different lks. Our empirical results
show that a larger kernel size greatly reduces computing cost
but contributes little to overall accuracy. Another reason for
adopting convolution is that it can reduce the dimensions with
fewer parameters.
For each of the rest layers of the CNN, its input is the output of the preceding layer. Suppose SL is the output of the
CNN’s last layer, all of SL’s features are ﬂattened through
concatenation to be fed into an MLP to obtain ﬁnal output of
this feature extraction layer, i.e.,
s = MLP([S1
L · · · ST
where s ∈RD′ and Si
L(1 ≤i ≤T) is the ﬂattened array of
feature map corresponding to the i-th ﬁlter. In the following
evaluation of our framework, we adopted a CNN of two layers, i.e., L=2. The ﬁrst layer’s lk is set to 1, and the second
layer’s lk is set to 2.
Prediction Layer
The last layer of our framework is the prediction layer, which
accepts the output of the preceding CNN, i.e., s, to generate
the ﬁnal prediction score ˆyuv. In this layer, we feed s into
a single layer perceptron and use Sigmoid function σ(·) to
compute ˆyuv as follows
ˆyuv = σ(W T
out ∈RD′ is the weight matrix and b is a bias vector. According to ˆyuv, we can decide whether v deserves being recommended to u.
Model Learning
To learn our model’s parameters including all embeddings
mentioned before, we use binary cross-entropy loss since it
is suitable for binary classiﬁcation. Speciﬁcally, we have
(u,v)∈Y+∪Y−
yuv log ˆyuv +(1−yuv) log(1−ˆyuv)
# interaction
Table 1: Statistics of experimental datasets.
where Y+ denotes the set of observed interactions (ˆyuv = 1),
and Y−denotes the set of negative instances which are sampled randomly from unobserved interactions. In our experiments, we use Adam algorithm [Kingma and Ba, 2015] to optimize Eq.11, because it has been proven to be powerful optimization algorithm for stochastic gradient descent for training
deep learning models.
Please note that our framework can be applied to various
recommendation tasks, including personalized ranking and
rating prediction, through simply modifying the loss function.
Experiments
In this section, we conduct extensive experiments to answer
the following research questions.
RQ1: Which hyper-parameters are critical and sensitive
to our framework and how do they impact the ﬁnal performance?
RQ2: Does our recommendation framework outperform
the previous state-of-the-art recommendation models in terms
of predicting implicit feedback?
RQ3: Can our proposed Gaussian embeddings well capture
the preferences of the users with uncertainty, further resulting
in better recommendation performance?
Experimental Settings
Dataset Description
We evaluated our models on two public benchmark datasets:
MovieLens 1M (ml-1m)1,and Amazon music (Music)2. The
detailed statistics of the two datasets are summarized in Table
1. In ml-1m dataset, each user has at least 20 ratings. In
Music dataset, we only reserved the users who have at least 1
rating record given its sparsity.
Evaluation Protocols
Following [He et al., 2018b; 2017], we adopted the leaveone-out evaluation. We held out the latest one interaction
of each user as the positive sample in test set, and paired it
with 99 items randomly sampled from unobserved interactions. For each positive sample of every user in training set,
we randomly sampled 4 negative samples. We then predicted
and evaluated the 100 user-item interactions of each user in
test set. We used two popular metrics evaluation measures,
i.e., Hit Ratio (HR) and Normalized Discounted Cumulative
Gain (nDCG) [Jarvelin and Kekalainen, 2002] to evaluate the
recommendation performance of all compared models. The
ranked list is truncated at 3 and 10 for both measures. Compared with Hit Ratio, nDCG is more sensitive to rank position
because it assigns higher scores for top position ranking.
1 
2 
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
1. MF-BPR: This model optimizes the standard MF with the
pairwise Bayesian Personalized Ranking (BPR for short) loss
[Rendle et al., 2012].
2. NCF: This model [He et al., 2017] has been proven to be
a powerful DNN-based CF framework consisting of a GMF
(generalized matrix factorization) layer and an MLP (multilayer perceptron). Both GMF and MLP are fed with user and
item representations initialized in random. NCF parameters
are learned based on obtained user-item interactions.
ConvNCF: This is an improved version [He et al.,
2018a] of NCF which uses outer product to explicitly model
the pairwise correlations between the dimensions of the ﬁxed
point embedding space, and then applies multi-layer CNN to
extract signal from the interaction map.
4. DeepCF: This is a deep version [Deng et al., 2019]
of CF, aiming to fuse representation learning based methods
and matching function based methods. It employs MLP to
learn the complex matching function and low-rank relations
between users and items.
5. NAIS: In this framework [He et al., 2018b], a user’s representation is the attentive sum of his/her historical favorite
items’ embeddings. A historical item’s attention is computed
based on the similarity between it and the candidate item.
Thus such representations especially for the users with many
historical favorite items, are also ﬂexible w.r.t. different candidate items.
GER: To the best of our knowledge, this baseline
[Dos Santos et al., 2017] is the only Gaussian embedding
based recommendation model.
It replaces dot product of
vectors by inner product between two Gaussian distributions
based on BPR framework. As we stated before, such rankingbased loss is not to applicable to other recommendation tasks.
MoG: This is a variant of the model in [Oh et al.,
2018], which averages predeﬁned soft contrastive loss between vector pairs to obtain matching probability between
stochastic embeddings.
We set its stochastic mappings to
Gaussian embeddings. We compared MoG with our framework to highlight the effectiveness of computing matching
probability based on convolutional operations.
In addition, we denote our framework as GeRec. In order
to achieve a fair comparison, we set the embedding dimension D=64 in all above baselines. The code package of implementing our framework is published on
 
Experimental Results
Hyper-parameter Tuning
At ﬁrst, we try to answer RQ1 through the empirical studies
of hyper-parameter tuning in our framework. Due to space
limitation, we only display the results of tuning three critical hyper-parameters of our framework GeRec, i.e., embedding dimension D, Monte-Carlo sampling number K and
our CNN’s kernel number T, which were obtained from the
evaluation on MovieLens dataset. Compared with previous
deep models, only K is additionally imported into our framework. Please note that when we tuned one hyper-parameter,
we set the rest hyper-parameters to their best values. Table 2
displays our framework’s performance of movie recommendation under different hyper-parameter settings. In general,
Table 2: GeRec’s hyper-parameter tuning results on MovieLens.
larger D and T result in better performance. But we only selected D = 64 and T = 64 in our experiments given model
training cost. And we set K = 9 in the following comparison
experiments according to the results in Table 2. In addition,
D′ is also set to 64.
Global Performance Comparisons
To answer RQ2, we compared our framework with the baselines in terms of recommendation performance. The results
listed in Table 3 were the average scores of 5 runs, showing
that our framework GeRec performs best on the two datasets.
Speciﬁcally, GeRec’s advantage over MF-BPR, NCF, ConvNCF and DeepCF shows that Gaussian embeddings represent users and items better than the embeddings of ﬁxed
points, resulting in more precise recommendation results.
GeRec’s advantage over NAIS shows that although attentionbased user representations are also ﬂexible embeddings, they
do not perform as well as Gaussian embeddings. GeRec’s superiority over GER and MoG justiﬁes that, our CNN-based
evaluation of the correlations between the Gaussian samples
of users and items is more effective than the operations in
GER and MoG.
Effectiveness on Capturing User Uncertainty
To answer RQ3, we evaluated our framework particularly
against the users with uncertain preferences. At ﬁrst, we introduce how to categorize such users. As stated in Sec. 1,
we focus on two kinds of users with uncertainty in this paper. The ﬁrst kind of such users are those with sparse observed user-item interactions, because very little information
about their preferences can be obtained from their historical
actions. The second kind of such users are those having many
distinct preferences, because we can not identify which genre
of preference is their most favorite one.
Inspired by [Zhu et al., 2018], we identiﬁed these two
kinds of uncertain users according to two metrics, respectively. Speciﬁcally, for the ﬁrst kind of users, we ﬁltered out
six user groups according to a metric o1. The users of o1 are
those who have 10o1 observed user-item interactions. Thus
small o1 indicates the users with more the ﬁrst kind of uncertainty. For the second kind of users, we also ﬁltered out
six user groups according to metric o2. We compute o2 for
a given user u as follows. For each pair (mi, mj) of movies
rated by u, suppose Gi and Gj are the genre sets of mi and
mj, respectively. Then, we set oij = 1 −|Gi∩Gj|
|Gi∪Gj|. Finally,
we use average oij of all movie pairs as u’s o2. As a result,
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
MovieLens 1M
Amazon Music
Table 3: Global Recommendation Performance results show that our GeRec outperforms all baselines on the two datasets.
1st kind of uncertain users
2nd kind of uncertain users
Table 4: The learned Gaussian variances for MovieLens users.
large o2 indicates more preference diversity, i.e., the second
kind of uncertainty. For space limitation, we only display the
results of MovieLens users in Table 4. In the table, the displayed variances are the average Gaussian variances learned
by our framework, showing that our proposed Gaussian embeddings assign larger variances to the users with more uncertainty. Thus, such distribution based embeddings represent
the users with uncertainty well, resulting in better recommendation performance.
Related Work
MF-based models constitute one important family of recommendation models, such as latent factor model [Yehuda et al.,
2009] and non-negative matrix factorization [He and Chua,
2017]. Based on these traditional MF-based models, some
improved versions had been proposed and proven more effective. For example, SVD++ [Koren, 2008] improves SVD
through taking into account the latent preferences of users
besides explicit user-item interactions. MF-BPR optimizes
standard MF with pairwise Bayesian Personalized Ranking
[Rendle et al., 2012] loss. Factorization Machine (FM) [Rendle, 2010] captures the interactions between user/item features to improve performance of model learning. All these
models represent users and items by a vector containing the
latent factors of users/items, of which the values are ﬁxed
once they are learned from user-item interactions, so are not
adaptive to the users/items with uncertainty.
In recent years, many researchers have justiﬁed that traditional recommendation models including CF and MF-based
models, can be improved by employing DNNs. For example, the authors in [Sedhain et al., 2015] proposed a novel
AutoEncoder (AE) framework for CF. DeepMF [Xue et al.,
2017] is a deep version of MF-based recommendation model.
In addition, NCF model [He et al., 2017] integrates generalized matrix factorization model (GMF) and multiple-layer
perceptron (MLP) to predict CF-based implicit feedback.
DeepCF [Deng et al., 2019] also employs MLP to learn the
complex matching function and low-rank relations between
users and items, to enhance the performance of CF. In general, these deep models also represent users/items by embeddings which are used to feed the neural networks, and their
embeddings also correspond to ﬁxed points in embedding
space without ﬂexibility. Although the models in[Shen et al.,
2019; He et al., 2018b] import attention mechanism to make
user representations more ﬂexible, such attention-based embeddings were proven not so good as Gaussian embeddings
by our experiments.
Gaussian embeddings are generally trained with ranking
objective and energy functions, such as probability product
kernel and KL-divergence. The authors in [Vilnis and Mc-
Callum, 2014] ﬁrst used a max-margin loss to learn word representations in the space of Gaussian distributions to model
uncertainty. Similarly, [He et al., 2015] and [Dos Santos et
al., 2016] learn Gaussian embeddings for knowledge graphs
and heterogeneous graphs, respectively; [Dos Santos et al.,
2017] uses Gaussian distributions to represent users and items
in ranking-based recommendation. To improve graph embedding quality, [Bojchevski and G¨unnemann, 2018] takes
into account node attributes and employs a personalized
ranking formulation, and [Zhu et al., 2018] incorporates 2-
Wasserstein distance and Wasserstein Auto-Encoders.
these methods employ ranking function and thus can not be
applied to other recommendation tasks easily. Recently, [Oh
et al., 2018] learns stochastic mappings of images with contrastive loss and also uses Gaussian embeddings.
Conclusion
In this paper, we propose a uniﬁed recommendation framework in which each user or item is represented by a Gaussian
embedding instead of a vector corresponding to a single ﬁxed
point in feature space. Moreover, convolutional operations
are adopted to effectively evaluate the correlations between
users and items, based on which precise recommendation results of both personalized ranking and rating prediction can
be obtained. Our extensive experiments not only demonstrate
our framework’s superiority over the state-of-the-art recommendation models, but also justify that our proposed Gaussian embeddings capture the preferences of the users with
uncertainty very well.
Acknowledgements
This paper was supported by National Key R&D Program of
China No. 2017YFC1201203, National NSF of China No.
U1636207, Shanghai Municipal Science and Technology Major Project (Grant No. 16JC1420400).
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)