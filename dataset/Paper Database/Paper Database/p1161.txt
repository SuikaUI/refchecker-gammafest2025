Machine Learning, 54, 45–66, 2004
c⃝2004 Kluwer Academic Publishers. Manufactured in The Netherlands.
Support Vector Data Description
DAVID M.J. TAX
 
ROBERT P.W. DUIN
 
Pattern Recognition Group, Faculty of Applied Sciences, Delft University of Technology, Lorentzweg 1,
2628 CJ Delft, The Netherlands
Editor: Douglas Fisher
Data domain description concerns the characterization of a data set. A good description covers all
target data but includes no superﬂuous space. The boundary of a dataset can be used to detect novel data or outliers.
We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classiﬁer.
It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classiﬁer it can be
made ﬂexible by using other kernel functions. The method is made robust against outliers in the training set and is
capable of tightening the description by using negative examples. We show characteristics of the Support Vector
Data Descriptions using artiﬁcial and real data.
outlier detection, novelty detection, one-class classiﬁcation, support vector classiﬁer, support vector
data description
Introduction
Muchefforthasbeenexpendedtosolveclassiﬁcationandregressiontasks.Intheseproblems
a mapping between objects represented by a feature vector and outputs (a class label or real
valued outputs) is inferred on the basis of a set of training examples. In practice, another type
of problem is of interest too: the problem of data description or One-Class Classiﬁcation
 . Here the problem is to make a description of a training
set of objects and to detect which (new) objects resemble this training set.
Obviously data description can be used for outlier detection—to detect uncharacteristic
objects from a data set. Often these outliers in the data show an exceptionally large or small
feature value in comparison with other training objects. In general, trained classiﬁers or
regressors only provide reliable estimates for input data close to the training set. Extrapolations to unknown and remote regions in feature space are very uncertain . Neural networks, for instance, can be trained to estimate posterior probabilities
 and tend to give high conﬁdence
outputs for objects which are remote from the training set. In these cases outlier detection
should be used ﬁrst to detect and reject outliers to avoid unfounded conﬁdent classiﬁcations.
Secondly, data description can be used for a classiﬁcation problem where one of the
classes is sampled very well, while the other class is severely undersampled. An example is
a machine monitoring system in which the current condition of a machine is examined. An
alarm is raised when the machine shows a problem. Measurements on the normal working
conditions of a machine are very cheap and easy to obtain. Measurements of outliers, on
D.M.J. TAX AND R.P.W. DUIN
the other hand, would require the destruction of the machine in all possible ways. It is very
expensive, if not impossible, to generate all faulty situations . Only a method which uses mainly the target data, and does not require representative
outlier data, can solve this monitoring problem.
The last possible use of outlier detection is the comparison of two data sets. Assume a
classiﬁerhasbeentrained(byalonganddifﬁcultoptimization)onsome(possiblyexpensive)
data. When a similar problem has to be solved, the new data set can be compared with the
old training set. In case of incomparable data, the training of a new classiﬁer will be needed.
In many one-class classiﬁcation problems an extra complication occurs, namely that it
is beforehand not clear what the speciﬁc distribution of the data will be in practice. The
operator of the machine can easily run the machine in different, but legal modes, covering the
complete operational area of the machine. Although it deﬁnes the normal working area, the
distribution over this area is not to be trusted. In practice the machine may stay much longer
in one mode than in another, and this mode might have been sampled sparsely during the
training phase. A data description for this type of data should therefore model the boundary
of the normal class, instead of modeling the complete density distribution.
Several solutions for solving the data description problem have been proposed. Most
often the methods focus on outlier detection. Conceptually the simplest solution for outlier
detection is to generate outlier data around the target set. An ordinary classiﬁer is then
trained to distinguish between the target data and outliers . Koch
et al. used ART-2A and a Multi-Layered Perceptron for the detection of (partially
obscured) objects in an automatic target recognition system. Unfortunately this method
requires the availability of a set of near-target objects (possibly artiﬁcial) belonging to the
outlier class. The methods scale very poorly in high dimensional problems, especially when
the near-target data has to be created and is not readily available.
In classiﬁcation or regression problems a more advanced Bayesian approach can be used
for detecting outliers . Instead of
using the most probable weight conﬁguration of a classiﬁer (or regressor) to compute the
output, the output is weighted by the probability that the weight conﬁguration is correct
given the data. This method can then provide an estimate of the probability for a certain
object given the model family. Low probabilities will then indicate a possible outlier. The
classiﬁer outputs are moderated automatically for objects remote from the training domain.
These methods are not optimized for outlier detection though; they require a classiﬁcation
(or regression) task to be solved and can be computationally expensive.
Most often the task of data description is solved by estimating a probability density of the
target data . For instance in Bishop and Tarassenko, Hayton,
and Brady the density is estimated by a Parzen density estimator, whereas in Parra,
Deco, and Miesbach one Gaussian distribution is used. In 
not only the target density is estimated, but also the outlier density. The ﬁrst drawback is that
in general (in higher dimensional feature spaces) a large number of samples is required. The
second drawback is that they will not be very resistant to training data which only deﬁnes
the area of the target data, and which does not represent the complete density distribution. It
will mainly focus on modeling the high density areas, and reject low density areas, although
they may deﬁne legal target data.
SUPPORT VECTOR DATA DESCRIPTION
Vapnik argued that in order to solve a problem, one should not try to solve a more
general problem as an intermediate step . The estimation of the complete
density instead of computing the boundary around a data set might require too much data
and could result in bad descriptions. An attempt to train just the boundaries of a data set
is made in Moya and Hush and Moya, Koch, and Hostetler . Here neural
networks are trained with extra constraints to give closed boundaries. Unfortunately this
method inherits the weak points in neural network training, i.e. the choice of the size of
the network, weight initialization, the stopping criterion, etc. Rosen made a data
description as a by-product of a classiﬁcation problem. He shows how the classiﬁcation
problem can be formulated as a convex programming problem. When the classiﬁcation
problem is not linearly separable, an ellipsoidal separation can be applied, where one of the
classes is enclosed by an ellipsoid of minimum size. We will use a similar method for the
one-class classiﬁcation problem.
Finally Sch¨olkopf et al. used an hyperplane to separate the
target objects from the origin with maximal margin. This formulation is comparable with
the Support Vector Classiﬁer by Vapnik and it is possible to deﬁne implicit mappings
to obtain more ﬂexible descriptions.
In this paper we discuss a method which obtains a spherically shaped boundary around the
complete target set with the same ﬂexibility. To minimize the chance of accepting outliers,
the volume of this description is minimized. We will show how the outlier sensitivity can
be controlled by changing the ball-shaped boundary into a more ﬂexible boundary and how
example outliers can be included into the training procedure (when they are available) to
ﬁnd a more efﬁcient description.
In Section 2 we present the basic theory, which is presented in part in Tax and Duin
 . The normal data description, and also the description using negative examples will
be explained and we will compare the method with the hyperplane approach in Sch¨olkopf
et al. . In Section 3 some basic characteristics of this data description will be shown,
and in Section 4 the method will be applied to a real life problem and compared with some
other density estimation methods. Section 5 contains some conclusions.
To begin, we ﬁx some notation. We assume vectors x are column vectors and x2 = x · x.
We have a training set {xi}, i = 1, . . . , N for which we want to obtain a description. We
further assume that the data shows variances in all feature directions.
Normal data description
To start with the normal data description, we deﬁne a model which gives a closed boundary
around the data: an hypersphere. The sphere is characterized by center a and radius R > 0.
We minimize the volume of the sphere by minimizing R2, and demand that the sphere
contains all training objects xi. This is identical to the approach which is used in Sch¨olkopf,
Burges, and Vapnik to estimate the VC-dimension of a classiﬁer (which is bounded
by the diameter of the smallest sphere enclosing the data). At the end of this section we will
D.M.J. TAX AND R.P.W. DUIN
show that this approach gives solutions similar to the hyperplane approach of Sch¨olkopf
et al. .
Analogous to the Support Vector Classiﬁer we deﬁne the error function
to minimize:
F(R, a) = R2
with the constraints:
∥xi −a∥2 ≤R2,
To allow the possibility of outliers in the training set, the distance from xi to the center a
should not be strictly smaller than R2, but larger distances should be penalized. Therefore
we introduce slack variables ξi ≥0 and the minimization problem changes into:
F(R, a) = R2 + C
with constraints that almost all objects are within the sphere:
∥xi −a∥2 ≤R2 + ξi,
The parameter C controls the trade-off between the volume and the errors.
Constraints (4) can be incorporated into Eq. (3) by using Lagrange multipliers:
L(R, a, αi, γi, ξi) = R2 + C
αi{R2 + ξi −(∥xi∥2 −2a · xi + ∥a∥2)} −
with the Lagrange multipliers αi ≥0 and γi ≥0. L should be minimized with respect to
R, a, ξi and maximized with respect to αi and γi.
Setting partial derivatives to zero gives the constraints:
C −αi −γi = 0
From the last equation αi = C −γi and because αi ≥0, γi ≥0, Lagrange multipliers γi
can be removed when we demand that
SUPPORT VECTOR DATA DESCRIPTION
Resubstituting (6)–(8) into (5) results in:
αi(xi · xi) −
αiα j(xi · x j)
subject to constraints (9). Maximizing (10) gives a set αi. When an object xi satisﬁes the
inequality ∥xi −a∥2 < R2 + ξi, the constraint is satisﬁed and the corresponding Lagrange
multiplier will be zero (αi = 0). For objects satisfying the equality ∥xi −a∥2 = R2 + ξi
the constraint has to be enforced and the Lagrange multiplier will become unequal zero
∥xi −a∥2 < R2 →αi = 0, γi = 0
∥xi −a∥2 = R2 →0 < αi < C, γi = 0
∥xi −a∥2 > R2 →αi = C, γi > 0
Equation (7) shows that the center of the sphere is a linear combination of the objects.
Only objects xi with αi > 0 are needed in the description and these objects will therefore
be called the support vectors of the description (SV’s).
To test an object z, the distance to the center of the sphere has to be calculated. A test
object z is accepted when this distance is smaller or equal than the radius:
∥z −a∥2 = (z · z) −2
αi(z · xi) +
αiα j(xi · x j) ≤R2
By deﬁnition, R2 is the distance from the center of the sphere a to (any of the support
vectors on) the boundary. Support vectors which fall outside the description (αi = C) are
excluded. Therefore:
R2 = (xk · xk) −2
αi(xi · xk) +
αiα j(xi · x j)
for any xk ∈SV<C, the set of support vectors which have αk < C.
Note that in all formulae (Eqs. (10), (14) and (15)) objects xi only appear in the form of
inner products with other objects (xi · x j). Analogous to Vapnik the inner products
can be replaced by a kernel function to obtain more ﬂexible methods. We will use this in
Section 2.3.
The left plot in ﬁgure 1 shows an example description for a small 2 dimensional bananashaped data set. For this description C = 1 is taken, indicating the hard-margin solution (by
constraint (6) the upper bound constraint on αi in (9) will always be satisﬁed for C ≥1).
The solid circles indicate the support vectors, the dashed line is the boundary of the data
description. The gray value indicates the distance to the center of the sphere; dark is close,
light is remote. Only three objects are required to describe the complete data set.
Sch¨olkopf gives an alternative approach for solving the data description problem
 . A hyperplane is placed such that it separates the dataset from
D.M.J. TAX AND R.P.W. DUIN
Example of a data description without outliers (left) and with one outlier (right).
the origin with maximal margin. Although this is not a closed boundary around the data,
it gives comparable solutions when the data is preprocessed to have unit norm .
For hyperplane w which separates the data xi from the origin with margin ρ, the following
w · xi ≥ρ −ξi
where ξi accounts for possible errors. Sch¨olkopf minimizes the structural error of the hyperplane, measured by ∥w∥. This results in the following minimization problem:
2∥w∥2 −ρ + 1
with constraints (16). The regularization parameter ν ∈(0, 1) is a user deﬁned parameter
indicating the fraction of the data that should be separated and can be compared with the
parameter C in the SVDD. Here we will call this method the ν-SVC.
An equivalent formulation of (16) and (17) is
w,ρ,ξ ρ −1
w · xi ≥ρ −ξi,
where a constraint on ∥w∥is introduced. When all the data in the original SVDD formulation
is transformed to unit norm (see also (3) and (4)), we obtain the following optimization
min R′2 + C′ 
i −a′∥2 ≤R′2 + ξ ′
SUPPORT VECTOR DATA DESCRIPTION
where x′ and a′ are normalized vectors. Rewriting gives:
max −R′2 −C′ 
i) ≥2 −R′2 −ξ ′
We deﬁne w = 2a′, ρ = 2 −R′2,
νN = C′, ξi = ξ ′
i and the following optimization
problem is obtained:
max −2 + ρ −1
w · ⃗xi ≥ρ −ξi,
which is a comparable solution to Eq. (18). It differs in the constraint on the norm of w
and in an offset of 2 in the error function. For non-normalized data the solutions become
incomparable due to the difference in model of the description (hyperplane or hypersphere).
We will come back to the ν-SVC in Section 2.3 in the discussion on kernels.
SVDD with negative examples
When negative examples (objects which should be rejected) are available, they can be
incorporated in the training to improve the description. In contrast with the training (target)
examples which should be within the sphere, the negative examples should be outside it.
This data description now differs from the normal Support Vector Classiﬁer in the fact that
the SVDD always obtains a closed boundary around one of the classes (the target class).
The support vector classiﬁer just distinguishes between two (or more) classes and cannot
detect outliers which do not belong to any of the classes.
Inthefollowingthetargetobjectsareenumeratedbyindicesi, j andthenegativeexamples
by l, m. For further convenience assume that target objects are labeled yi = 1 and outlier
objects are labeled yl = −1. Again we allow for errors in both the target and the outlier set
and introduce slack variables ξi and ξl:
F(R, a, ξi, ξl) = R2 + C1
and the constraints
∥xi −a∥2 ≤R2 + ξi,
∥xl −a∥2 ≥R2 −ξl,
ξi ≥0, ξl ≥0
These constraints are again incorporated in Eq. (22) and the Lagrange multipliers αi, αl,
γi, γl are introduced:
L(R, a, ξi, ξl, αi, αl, γi, γl) = R2 + C1
αi[R2 + ξi −(xi −a)2] −
αl[(xl −a)2 −R2 + ξl]
with αi ≥0, αl ≥0, γi ≥0, γl ≥0.
D.M.J. TAX AND R.P.W. DUIN
Setting the partial derivatives of L with respect to R, a, ξi and ξl to zero gives the
constraints:
0 ≤αi ≤C1,
When Eqs. (25)–(27) are resubstituted into Eq. (24) we obtain
αi(xi · xi) −
αl(xl · xl) −
αiα j(xi · x j)
αlα j(xl · x j) −
αlαm(xl · xm)
When we ﬁnally deﬁne new variables α′
i = yiαi (index i now enumerates both target and
outlier objects), the SVDD with negative examples is identical to the normal SVDD. The
constraints given in Eqs. (25) and (26) change into 
i = 1 and a = 
ixi and again
the testing function Eq. (14) can be used. Therefore, when outlier examples are available,
we will replace Eq. (10) by (29) and we will use α′
i instead of αi.
In the right plot in ﬁgure 1 the same data set is shown as in the left plot, extended with
one outlier object (indicated by the arrow). The outlier lies within the original description
on the left. A new description has to be computed to reject this outlier. With a minimal
adjustment to the old description, the outlier is placed on the boundary of the description. It
becomes a support vector for the outlier class and cannot be distinguished from the support
vectors from the target class on the basis of Eq. (14). Although the description is adjusted
to reject the outlier object, it does not ﬁt tightly around the rest of the target set anymore. A
more ﬂexible description is required.
Flexible descriptions
When instead of the rigid hypersphere a more ﬂexible data description is required, another
choice for the inner product (xi ·x j) can be considered. Replacing the new inner product by
a kernel function K(xi, x j) = ((xi)·(x j)) an implicit mapping  of the data into another
(possibly high dimensional) feature space is deﬁned. An ideal kernel function would map the
target data onto a bounded, spherically shaped area in the feature space and outlier objects
outside this area. Then the hypersphere model would ﬁt the data again (this is comparable
with replacing the inner product in the Support Vector classiﬁer when the classes are not
linearly separable). Several kernel functions have been proposed for the Support Vector
Classiﬁer . It appears that not all these
kernel functions map the target set in a bounded region in feature space. To show this, we
ﬁrst investigate the polynomial kernel.
SUPPORT VECTOR DATA DESCRIPTION
Data description trained on a banana shaped data set. Kernel is a polynomial kernel with varying
degrees. Support vectors are indicated by the solid circles, the dashed line is the description boundary.
The polynomial kernel is given by
K(xi, x j) = (xi · x j)d
where the free parameter d ∈N+ gives the degree of the polynomial kernel. The testing
function of the SVDD (Eq. (14)) shows that only the second term accounts for the interaction
between the test object z and the support objects xi. Recall that xi ·x j = cos(θi j)∥xi∥·∥x j∥
where θi j is the angle between object vector xi and x j. When data is not centered around
the origin, object vectors can become large. And when the θi j is small, cos(θi j) ∼1 will
stay almost constant. Then for larger degrees d, the polynomial kernel is approximated by:
(xi · x j)d = cosd(θi j)∥xi∥d · ∥x j∥d ≃∥xi∥d · ∥x j∥d
Equation (31) looses the sensitivity to θi j in the neighborhood of the training data (where
θ becomes small). The objects with the largest norm in the training set will overwhelm all
other terms in the polynomial kernel. This effect can be suppressed by centering the data
around the origin and rescaling the data to unit standard variance. Unfortunately rescaling
to unit variance might only magnify the noise in directions with small variance, and the
inﬂuence of the norms is not avoided. Finally, centering the data in the feature space by
subtracting the averaged x does not resolve the problem of
the large differences in vector norms. It can be shown that the centered SVDD is equivalent
to the original SVDD.
The inﬂuence of large norm objects is visible in ﬁgure 2. For a simple 2 dimensional
data set, descriptions are obtained using a polynomial kernel with different degrees, ranging
from d = 1.0 (left) to d = 6.0 (right). Again the solid circles indicate the support vectors,
the dashed line is the description boundary mapped in the input space. The rigid spherical
description is obtained for d = 1.0. For degree d = 6.0 the description is a sixth order
polynomial. Here the training objects most remote from the origin (the objects on the right)
become support objects and the data description only distinguishes on the basis of the norm
of the vectors. Large regions in the input space without target objects will be accepted by
the description.
D.M.J. TAX AND R.P.W. DUIN
Next we investigate the Gaussian kernel with
K(xi, x j) = exp(−∥xi −x j∥2/s2)
This kernel is independent of the position of the data set with respect to the origin, it only
utilizes the distances between objects. In the testing function (Eq. (14)) the ﬁrst term equals
1.0 and the testing function boils down to a weighted sum of Gaussians. Test object z is
accepted when:
−∥z −xi∥2
≥−R2/2 + CR
where CR depends only on the Support Vectors xi and not on z. Using the Gaussian kernel
the inﬂuence of the norms of the objects is avoided. The objects are mapped to unit norm
vectors (the norm of the mapped objects (xi) · (xi) = 1) and only the angles between
object vectors count. When Gaussian kernels are used, both the SVDD and the ν-SVC give
comparable solutions.
For small values of s, exp( −∥xi−x j∥2
) ≃0, ∀i ̸= j and Eq. (10) is optimized when all
objects become support objects with equal αi = 1
N . This is identical to the Parzen density
estimation with a small kernel width. For very large s the solution approximates the original
spherically shaped solution. This can be shown by a Taylor expansion of the Gaussian kernel:
K(xi, x j) = 1 −∥xi∥2/s2 −∥x j∥2/s2 + 2(xi · x j)/s2 + · · ·
Substituting Eq. (34) into Eq. (10) we obtain:
αi(1 −∥xi∥2/s2 −∥xi∥2/s2 + 2(xi · xi)/s2 + · · ·)
αiα j(1 −∥xi∥2/s2 −∥x j∥2/s2 + 2(xi · x j)/s2 + · · ·)
= 1 −1 + 2
αi∥xi∥2/s2 −2
αiα j(xi · x j)/s2 + · · ·
Ignoring higher orders this is equivalent to Eq. (10) (with an extra scaling factor 2/s2).
When a new regularization parameter C′ = (2C)/s2 is used, an identical solution is found.
For intermediate values of s a weighted Parzen density estimation is obtained. Both the
weights of the kernels and the choice of which training objects become support vectors are
obtained automatically by the optimization procedure.
These situations, ranging from a Parzen density estimation to the rigid hyper sphere, can
be observed in ﬁgure 3, in combination with varying values of C. In the left column of
pictures a small s = 1.0 width parameter is used, and in the right column a large s = 15
is used. In all cases, except for the limiting case where s becomes huge, the description is
tighter than the normal spherically shaped description or the description with the polynomial
kernel. Note that with increasing s the number of support vectors decreases. Secondly,
SUPPORT VECTOR DATA DESCRIPTION
Data description trained on a banana shaped data set. A Gaussian kernel with different widths (s =
1, 5, 15) and different values for C (C = 40, almost the hard margin case, and C = 0.1) are used. Support vectors
are indicated by the solid circles, the solid white line is the description boundary.
decreasing the parameter C constraints the values for αi more, and more objects become
support vector. The error on the target class increases, but the covered volume of the data
description decreases with decreasing C.
Target error estimate
When an object drawn from the target distribution is rejected by the description, it is called
an error. By applying Leave-One-Out estimation , it can be
shown that the number of support vectors is an indication of the expected error made on
the target set. For that the notion of essential support vectors has to be introduced . The expansion of the center of the description a = 
i αixi is not unique. It might
be that more objects are on the boundary of the sphere than is necessary for the description
(for instance when four objects are on a circle in a 2 dimensional feature space where 3
are sufﬁcient1). The essential support vectors are these objects which appear in all possible
expansions.
When one of the internal points (αi = 0) is left out of the training and the data description
is computed, the same solution is obtained as with the training set including this training
object. During testing this object will therefore be accepted by the description. When a nonessential support vector is left out during training, the solution including this object is still
D.M.J. TAX AND R.P.W. DUIN
obtained, using the remaining support vectors in the expansion. When an essential support
object on the boundary (a support vector with αi < C) is left out, a smaller description
is found. This support point will be rejected by the new solution. The errors, the support
objects with αi = C are already outside the sphere. When they are left out of the training
set, they will again be rejected. The fraction of objects which become (essential) support
objects and outlier, is thus an leave-one-out error estimate on the target set:
˜ELOO ≤# SV s + # errors
(where the inequality holds when not all support vectors are essential support vectors).
When the Support Vector Data Description is trained on a set with both target and outlier
objects present, only the fraction of support vectors on the target set should be considered for
the estimation of the leave-one out error. The error of the outlier class can also be estimated,
but only when a representative set of outliers is drawn from the true outlier distribution.
Because usually in the one-class classiﬁcation problem the outlier class is sampled very
sparsely, a good representative data set is often not available and a reliable estimate of the
error is not possible.
This target error estimate opens the possibility to optimize the regularization parameter
C and the width s of the Gaussian kernel. The value of the regularization parameter C can
be chosen by using the fact that for errors αi = C. Recall the constraint that 
i when negative examples are available) and therefore # errors
NC . When in a
training set no errors are expected C can be set to 1.0, indicating that all target data should
be accepted and all negative examples should be rejected. When no negative examples
are available and some outliers in the training set are expected, set C ≤
N·(fraction outlier)
beforehand. In Sch¨olkopf et al. the parameter (fraction outlier) is called ν and the
fact that this ν sets the error (here on the target class) is called the ν-property.
Secondly the kernel width s can be optimized on the basis of the required target acceptance
rate. Comparing two width values s1 > s2, we get exp(−∥xi −x j∥2/s2
1) > exp(−∥xi −
2). For large enough s1 Eq. (10) is maximized when the αi, α j corresponding to the
larger terms K(xi, x j) become zero. This means that the most separate objects become
support vector and that the number of support vectors tends to decrease with increasing s.
Using Eq. (36) this also means a decrease of the expected error on the target set (this should
be no surprise, because the covered area in feature space tends to increase when the data
model changes from a Parzen density estimate with small σ to the rigid hypersphere, see
again ﬁgure 3). Unfortunately the minimization of Eq. (10) with Gaussian kernel requires
s beforehand, so the optimization requires an iterative scheme to minimize the difference
between the fraction SV’s and the user deﬁned error fraction.
When a small fraction of the target data is to be rejected, the boundary is located in the
tails of the target distribution. In order to have reliable estimates in these boundary in the
tails, the required sample sizes become large. Consider a dataset with 10 objects in a 2
dimensional data set. In a typical training of an SVDD, it appears that the minimum number
of support vectors is 2 to 3. To obtain a target error lower than 20% on this data, more
training data is required. The accuracy of the target error estimate, the required number of
training objects and some other characteristics will be investigated in the next section.
SUPPORT VECTOR DATA DESCRIPTION
SVDD characteristics
In this section several characteristics of the SVDD will be investigated. In all experiments
the Gaussian kernel is used and an upper bound on the error on the target set is set a priori
and the width parameter s is optimized.
Number of support vectors
The question of how much data is required for ﬁnding a sufﬁciently accurate description
(in terms of classiﬁcation error) of a target class in some feature space cannot be answered
a priori. Not only does it depend on the shape of the target data set, it also depends on the
distribution of the (unknown) outlier set. However, a lower limit can be obtained, when
a rigid spherically shaped data description is considered. Here just the coordinates of the
center of the sphere and the radius of the sphere are required. Therefore in theory only two
objects are sufﬁcient to determine the sphere (independent of dimensionality). The weights
αi are positive and constrained to sum up to 1 (Eq. (6)), and therefore the sphere center has to
be within the convex hull of the support vectors. The center can therefore only be described
by two objects when it is located on the line connecting these objects. For d-dimensional
data which has variation in all directions, the required number of objects can increase up to
d + 1. For data sets in a subspace, the number becomes less (and down to two if the data is
on a one dimensional line).
In ﬁgure 4 the fraction of outliers which is accepted, the fraction of training objects which
become support vectors and the error on the target set is shown for data with different
dimensionalities, 2D and 5D. The data is drawn from the (artiﬁcially generated) banana
shaped distribution. For varying s an SVDD is trained. The error on the target set is then
estimated by drawing a new independent test set containing 200 objects. 1000 outliers are
drawn from a square block around the data.
The fraction of (independent test) outliers which is accepted, the fraction of the (training) target data
which becomes support vector and the fraction of (independent test) target data rejected vs. the width parameter
s for (left) 2 dimensional and (right) 5 dimensional data. The training set size is 50.
D.M.J. TAX AND R.P.W. DUIN
By increasing s the number of support vectors decrease, closely followed by the error on
the target set. For large s the error is bounded by the minimum number of support vectors
which is needed for the description, in this case about 3 support vectors for 2D data and
about 5 for 5D data. For a training set size of 50 this results in a minimum error of about
6% or 10%. The maximal fraction of outliers accepted is the quotient between the volume
of the hypersphere and the volume of the outlier data block. For s > 20 in the 2D data and
s > 40 in the 6D data both fractions stay constant, indicating that the maximum scale in
the data is reached.
Note that for increasing dimensionality of the feature space the volume of the outlier block
tends to grow faster than the volume of the target class. This means that the overlap between
the target and outlier data decreases and the classiﬁcation problem becomes easier. In
practice, this phenomenon is less apparent, although still present, because outliers are often
not completely randomly scattered in all feature directions. Increasing the dimensionality
to improve classiﬁcation performance does have it’s limits, because with increasing the
dimensionality also the target class boundary increases, such that for reliable estimation of
the boundary, more data is required.
In Table 1 the minimum number of support vectors is shown for different datasets and
varying dimensionalities (from 2D to the full dimensionality, using all available features).
The ﬁrst three datasets, gauss, banana and ellipse, are artiﬁcially generated. The Gauss and
Minimum number of support vectors, for different datasets (with dimensionalities from 2D to the full
dimensionality) and different target classes.
2.94 (0.64)
5.63 (0.97)
7.81 (1.69)
11.42 (2.06)
3.23 (0.71)
5.50 (1.48)
7.76 (2.61)
8.71 (5.36)
2.08 (0.04)
2.16 (0.08)
2.17 (0.07)
2.34 (0.19)
2.00 (0.00)
2.00 (0.00)
2.20 (0.63)
2.10 (0.32)
2.30 (0.67)
2.10 (0.32)
6.20 (1.23)
2.40 (0.97)
2.70 (1.34)
5.60 (0.97)
6.00 (1.33)
7.30 (1.70)
3.10 (0.32)
4.40 (0.97)
4.90 (1.37)
7.20 (1.40)
3.90 (0.32)
3.00 (0.67)
3.90 (0.32)
3.00 (0.47)
2.10 (0.32)
2.30 (0.67)
2.30 (0.48)
2.20 (0.42)
2.20 (0.42)
4.20 (0.42)
3.70 (0.48)
4.30 (0.48)
4.00 (0.47)
3.80 (0.42)
4.00 (0.47)
2.60 (1.07)
2.30 (0.67)
2.60 (1.07)
3.70 (0.67)
2.20 (0.42)
3.10 (0.57)
5.00 (0.82)
3.30 (0.67)
5.00 (0.82)
Values are averaged of 10 runs, the standard deviation is given between brackets.
SUPPORT VECTOR DATA DESCRIPTION
banana set have variance in all feature directions, the ellipse mainly in the ﬁrst direction. The
other datasets are normal classiﬁcation problems, taken from the UCI repository . A data description is trained on each of the classes separately (thus
the three class Iris dataset shows three lines of results). In the ﬁrst column the number of
SV’s in the original feature space is shown (except for the artiﬁcial datasets). In the next
columns it is shown for datasets which are reduced to the ﬁrst few principal components.
For the Gauss and banana datasets the number of SV’s increases with increasing dimensionality, but stays in the order of d/2. In the ellipse dataset the single large variance
direction causes this number to stay very low (two support vectors on the extremities of
the dataset often support the whole data description). For the other data sets the number of
support vectors is mainly determined by the dimensionality of the subspace in which the
data is distributed. For the sonar database the number of SV’s hardly changes after 25D,
for the other datasets this already happens with 5D data. Finally note that the number of
SV’s for different classes can differ, e.g. in imox this number ranges from 2 to almost 5.
When this limited amount of data is available, this minimum number of support vectors
immediately gives an indication of the target error which can minimally be achieved (by
Eq. (36)). Only using more training data can reduce the error on the target set further.
Training with outliers
To compare data descriptions trained with outlier data with standard two-class classiﬁers,
results on the classiﬁcation problems mentioned in the previous section are shown in Table 2.
One class is the target class, and all other data is outlier data. Tenfold cross validation is
used to ﬁnd the classiﬁcation error. The reported results are the classiﬁcation errors on an
independent (labeled) test set. The classiﬁers are Gaussian-density based linear classiﬁer
(called Bayes), Parzen classiﬁer and the Support Vector Classiﬁer with polynomial kernel,
degree 3. These are compared with the data descriptions, without and including example
outliers (SVDD and SVDD-neg respectively). In both these cases the Gaussian kernel was
used. The parameter σ was optimized such that approximately 10% of the data becomes
support vector. When example outliers are used, the parameters C1 and C2 are set such, that
the fraction of rejected objects on the target set should be less then 1%, while the fraction
of accepted outlier objects can be up to 50%.
To investigate the inﬂuence of the norms of the vectors in real world applications, not
only the Gaussian kernel but also the polynomial kernel (with degree 3, where the data is
rescaled to unit variance) is used. These results are given in the columns SVDD,p3 and
SVDD-neg, p3.
In most cases the classiﬁers, especially the Parzen classiﬁer and the SVC with polynomial degree 3, outperform the data descriptions. This is not surprising, because they are
constructed to give the best separation between the two classes, with equal focus on each of
the classes. They are not limited by the extra constraint to have a closed boundary around
on the target class.
The performance of the one-class classiﬁers which only use information from the target
set, perform worse, but in some cases still comparable to the classiﬁers which use information of both the target and outlier data. The data descriptions using outliers perform
D.M.J. TAX AND R.P.W. DUIN
Cross-validation error (in %) of different classiﬁers for distinguishing between one of the classes and
33.3 (0.0)
26.7 (9.4)
23.3 (8.5)
10.0 (3.5)
38.7 (8.2)
20.0 (4.4)
21.9 (8.6)
19.2 (7.8)
22.4 (8.7)
33.6 (7.8)
36.5 (9.3)
29.0 (5.7)
32.7 (8.6)
32.6 (10.2)
19.2 (6.9)
20.5 (11.2)
51.5 (5.1)
50.6 (6.1)
30.4 (7.9)
51.9 (7.0)
10.3 (4.4)
30.0 (10.9)
54.7 (6.9)
46.8 (5.3)
69.7 (4.6)
76.2 (4.1)
14.9 (3.6)
72.5 (6.0)
25.0 (11.9)
14.5 (6.4)
11.0 (7.5)
50.6 (8.5)
44.3 (8.7)
35.2 (10.6)
45.2 (8.1)
25.0 (11.9)
14.5 (6.4)
11.0 (7.5)
41.3 (6.3)
53.8 (7.0)
30.3 (11.6)
45.2 (9.2)
17.6 (8.4)
74.5 (10.6)
13.4 (8.6)
48.8 (8.6)
66.7 (7.9)
22.4 (6.1)
17.7 (8.5)
11.4 (7.1)
10.4 (5.0)
18.8 (9.0)
49.1 (15.2)
13.1 (10.9)
40.2 (12.8)
somewhat better than without example outliers, as should be expected, but in some cases
it requires careful optimization of the C1, C2 parameters (for instance for the imox dataset
which has much overlap between the classes). In most cases the data descriptions with the
polynomial kernel perform worse than with the Gaussian kernel, except for a few cases.
Note that when the polynomial kernel is used, the data is preprocessed to have zero mean and
unit variance along each of the feature directions. When this preprocessing is not applied,
the classiﬁcation performance becomes extremely poor (worse than random guessing).
Experiments
Finally in this section we investigate how the SVDD works in a real one-class classiﬁcation
problem. We focus on a machine diagnostics problem: the characterization of a submersible
water pump . Here the normal operation of the pump has to be characterized, to distinguish it from all other faulty operation conditions. Both target objects
(measurements on a normal operating pump) and negative examples (measurements on a
damaged pump) are available. In a test bed several normal and outlier situations can be
simulated. The normal situations consist of working conditions with different loads and
speeds of the pump. The outlier data contain pumping situations with loose foundation,
imbalance and a failure in the outer race of the uppermost ball bearing.
SUPPORT VECTOR DATA DESCRIPTION
The training and testing sample sizes for the different vibration datasets with their settings. A speed
setting of 46, 50, 54 Hz and a load setting of 29, 33 (0.1 kW) means that all measurements with these settings are
used (i.e. 6 situations in total).
Testing set
Speeds (Hz)
Loads (0.1 kW)
Training set
target obj.
Target obj.
Outlier obj.
46, 48, 50, 52, 54
25, 29, 33
46, 50, 54
25, 29, 33
46, 48, 50, 52, 54
To characterize the normal operation condition of the pump, the operator varies the speed
(with possible values of 46, 48, 50, 52, 54 Hz) and the load (25, 29, 33 × 0.1 kW) of the
pump. For each setting, the pump is run for some time. On the pump vibration sensors are
mounted. From the recorded time series subsamples are taken and the power spectrum is
calculated (containing 64 bins, resulting in a 64 dimensional dataset). By this procedure
the normal operation conditions can be simulated and recorded. Thus the target area in the
feature space can be indicated. It is not clear, though, what the typical working conditions
will be in practice. The pump should be operating somewhere in the target area, but it will
probably not uniformly over the whole area.
These characteristics are simulated using this dataset by sampling the training data in
different ways. In this way the personal choices of the operator vary, although they should
in all cases deﬁne the normal operation area in the feature space. In Table 3 ﬁve different
datasets are listed with their speed and load settings, and the number of training and testing
objects available. The ﬁrst dataset is considered to be the most complete, that it covers the
whole working area of the pump. The next datasets are approximations to this dataset.
To see how well the SVDD and the SVDD with negative examples (SVDD neg) perform,
we compare them with a number of other methods. The ﬁrst method is a normal density
where just the mean and the covariance matrix of the data has to be estimated. This will result
in a ellipsoidal boundary around the data. In high dimensional feature spaces the covariance
matrix of the target set can become singular. The covariance matrix is regularized by adding
a small diagonal term to the covariance matrix:
+λI. The regularization parameter
is optimized for the problem at hand and is in most cases between 10−3 and 10−6. The second
method is the Parzen density where the width of the Parzen kernel is estimated using leaveone-out optimization . The third method is a Mixture of Gaussians, optimized
using EM . The number of clusters is set to 5 beforehand, but varying this
value does not improve performance very much.
The last method is a nearest neighbor method, which compares the local density of an
object x with the density of the nearest neighbor in the target set . The
distance between the test object x and its nearest neighbor in the training set NNtr(x) is
compared with the distance between this nearest neighbor NNtr(x) and its nearest neighbor
in the training set NNtr(NNtr(x)). When the ﬁrst distance is much larger than the second
distance, the object will be regarded as an outlier. We use the quotient between the ﬁrst and
D.M.J. TAX AND R.P.W. DUIN
The ROC curves for dataset 1 (see Table 3). Left for the data in the original 64D feature space, right
the data in 15D (retaining 80% of the variance).
the second distance as indication of the validity of the object:
∥x −NNtr(x)∥
∥NNtr(x) −NNtr(NNtr(x))∥
where NNtrx is the nearest neighbor of x in the training set.
It is to be expected that when just the target class area is covered, and not sampled according to what will happen in practice, the density methods will fail and the methods which just
focus on modeling the data will still work. In the case of a good distribution representation
in the training set, the density and the boundary methods should perform about equal.
For the comparison, the trade-off between the fraction of the target class rejected (error
of the ﬁrst kind, EI) versus the fraction of the outlier accepted (error of the second kind,
EII) is investigated. The methods will be compared over a range of threshold values, from
1% target class rejection up to 50% rejection. The fraction of outliers rejected and target
objects accepted is measured afterwards on the test set.
In ﬁgure 5 the Receiver-Operating Characteristic curves are shown (ROC-curve) for the methods trained on the power spectrum features. In the left plot the results on
the full 64 dimensional feature space is shown. A good discrimination between target and
outlier objects means both a small fraction of outlier accepted and a large fraction of target
objects accepted. The optimal performing classiﬁer will be in the upper left corner of the
ROC curve. The ROC curves in the left plot show, that for this data overlap between the
classes occur, and this optimal performance cannot be obtained.
The sample size is too low to obtain a good density estimate and the Parzen density
estimator is not able to estimate the density well. It rejects all target and all outlier data.
The Gaussian density and the SVDD obtain about the same solution, indicating that in this
representation with these amounts of data, no more than an ellipse can be ﬁtted. The Mixture
of Gaussians and the kNN overﬁt and show poor performance.
In the right subplot the data dimensionality is reduced by PCA to retain 80% of the
variance. In this case, the sample size is sufﬁcient to follow the boundaries of the target
set more closely. The poor performance of the Gaussian shows, that the target distribution
SUPPORT VECTOR DATA DESCRIPTION
Integrated errors for the ﬁve outlier detection methods on the original power spectrum data and versions
with only the ﬁrst few principal components.
Number of features
Normal dens.
Mix.o.Gauss.
Parzen dens.
Best performances from the methods are shown in bold.
boundary is more complex than a ellipsoid. The Parzen density estimator has still some
problems with the sample size, but both the SVDD and the kNN give nice results.
To make a more quantitative comparison, an error measure is derived from the ROCcurves. The fraction of the outliers which is accepted (EII), is averaged over varying EI
 :
EII(EI) dEI
In Table 4 the results on the power spectrum data are shown. The methods are applied to
both the original 64 dimensional data set and to the ﬁrst principal components of that data,
ranging from 3 up to 30 features. For higher dimensionalities the normal densities and the
SVDD perform well. This indicates that for higher dimensions, approximating the target
class by one ellipse is sufﬁcient. For the Normal density, the Mixture of Gaussians and the
SVDD the error decreases with increasing dimensionality while for the Parzen estimator it
decreases. On low dimensionalities the density can be estimated relatively well by the Parzen
estimator, but it suffers from the fact that it is not capable of accepting all target objects in
the test set. This means that in order to compute the AUC error from the ROC curve, the
ROC-curve should be extended. Here it is chosen to extend it in the worst case sense (when
a certain fraction target acceptance cannot be reached, it is assumed that the fraction of
outliers accepted will be 1). Therefore the errors for the Parzen density estimator are high.
The use of the negative examples in the SVDD is problematic in this example, because
there are many outliers and some show complete overlap with the target class. Choosing
a set of ‘good’ outliers improves performance signiﬁcantly, but when some ‘poor’ outliers
are chosen (outliers which are actually inside the target class) performance may deteriorate
to random guessing. In these cases a manual optimization of C1 and C2 is required, by
judging what fraction of the outliers are genuine outliers and which are actually not. When
randomly outliers are drawn, the variance of the different outcomes is too large to be useful.
In ﬁgures 6 and 7 the ROC curves for the one-class classiﬁers are given for the other
datasets listed in Table 3. By the (sub)sampling of the training set, the distribution of
the training target data does not completely reﬂect the target distribution in the test data,
although it should deﬁne the same area in feature space. In almost all cases the SVDD,
D.M.J. TAX AND R.P.W. DUIN
The ROC curves for dataset 2 and 3 (see Table 3). The data dimensionality was reduced by PCA to
retain 90% of the variance.
The ROC curves for datasets 4 (on the left) and 5 on the right). (see Table 3). The data dimensionality
was reduced by PCA to retain 90% of the variance.
which focuses on modeling the boundary, obtains a better performance than the density
estimates (this is very clear in datasets 3 and 5). The performance of the density estimators
can be improved with respect to the SVDD by reducing the dimensionality, but in these
cases the absolute performance is so poor, that it is not useful to apply in practice.
The worst classiﬁcation performances are obtained for dataset 4, in which measurements
on just one speed (50 Hz.) was included. Using just one speed setting is insufﬁcient to deﬁne
the whole feature area, in contrast to dataset 5, where a single load is used, but where the
performance of the SVDD is still very good. These examples show that, when an area in
featurespaceshouldbecharacterized(andnotpersethecompletetargetprobabilitydensity),
the SVDD gives very good data descriptions. The density estimators on the other hand, focus
much more on the high density areas, which are not representative for the practical situation.
Conclusions
In this paper we discussed a new method to solve the multidimensional outlier detection
problem. Instead of estimating a probability density, it obtains a boundary around the data
SUPPORT VECTOR DATA DESCRIPTION
set. By avoiding the estimation of the data density, it can obtain a better data boundary. The
data description is inspired by the Support Vector Classiﬁer. The boundary is described by
a few training objects, the support vectors. It is possible to replace normal inner products
by kernel functions and thus to obtain more ﬂexible data descriptions. In contrast to the
Support Vector Classiﬁer, the Support Vector Data Description using a polynomial kernel
suffers from the large inﬂuence of the norms of the object vectors, but it shows promising
results for the Gaussian kernel. Using the Gaussian kernel, descriptions comparable to the
hyperplane solution by Sch¨olkopf et al. are obtained.
The fraction of the target objects which become support vectors is an estimate of the
fraction of target objects rejected by the description. When the maximum desired error on
the target set is known beforehand, the width parameter s can be set to give the desired
number of support vectors. When not enough objects are available, the fraction of support
vectors stays high whatever width parameter s is used. This is an indication that more data is
necessary. Extra data in the form of outlier objects can also be used to improve the Support
Vector Data Description.
Comparing the Support Vector Data Description with other outlier detection methods,
Normal density estimation, Parzen density estimation and the Nearest Neighbor method,
it shows comparable or better results for sparse and complex data sets, especially when
outlier information is used. For very small target error rates the SVDD breaks down, while
for high sample sizes a density estimation like Parzen density method is to be preferred.
Acknowledgments
This work was partly supported by the Foundation for Applied Sciences (STW) and the
Dutch Organization for Scientiﬁc Research (NWO).
1. Or actually 2, but this requires a very speciﬁc placement of the vectors, on opposite sides with equal distance
to the center.