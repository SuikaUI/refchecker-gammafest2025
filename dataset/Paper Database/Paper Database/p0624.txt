Advance Access publication August 16, 2010
Political Analysis 18:470–498
doi:10.1093/pan/mpq019
Modeling Heterogeneity and Serial Correlation in
Binary Time-Series Cross-sectional Data: A Bayesian
Multilevel Model with AR(p) Errors
Department of Politics, Princeton University, 035 Corwin Hall, Princeton, NJ 08544
e-mail: 
This paper proposes a Bayesian generalized linear multilevel model with a pth-order
autoregressive error process to analyze unbalanced binary time-series cross-sectional
(TSCS) data. The model speciﬁcation is motivated by the generic TSCS data structure and is
intended to handle the associated inefﬁciency and endogeneity problems. It accommodates
heterogeneity across units and between time periods in the form of random intercepts and
random-effect coefﬁcients. At the same time, its pth-order autoregressive error process,
employed either by itself or in concert with other dynamic methods, adequately corrects
serial correlation and improves statistical inference and forecasting. With a stationarity
restriction on the error process, the model can also be used as a residual-based
cointegration test on discrete TSCS data. This is especially valuable because cointegration
testing on discrete TSCS data is methodologically challenging and rarely conducted in
practice. To handle the estimation difﬁculties, I developed an efﬁcient Markov chain Monte
Carlo (MCMC) algorithm by orthogonalizing the error term with the Cholesky decomposition
and adding an auxiliary variable. The parameter expansion method, that is, partial group
move–multigrid Monte Carlo updating (PGM-MGMC), is employed to further improve
MCMC mixing and speed up convergence. The paper also provides a computational
scheme to approximate the Bayes’s factor for the purposes of serial correlation diagnostics,
lag order determination, and variable selection. Simulated and empirical examples are used
to assess the model and techniques.
Introduction
The popularity and importance of time-series cross-sectional (TSCS) data in political
science calls for statistical models that are able to handle heterogeneity and serial correlation and uncover the nonrandom pattern in the errors for statistical inference and forecasting. Conventional models and methods—such as the ﬁxed- and random-effect
estimators (FE and RE) and the panel-corrected standard errors (PCSE)—often treat heterogeneity and serial correlation only as ‘‘problems’’ to correct or control for. In this way,
they waste information about political dynamics and differences among units and between
Author’s note: I am grateful to Andrew D. Martin and Jeff Gill for their help and advice throughout the research
process. Thanks to Edward Greenberg, Jong Hee Park, Michael Peress, Vera Troeger, Robert Walker, and Nathan
Jensen for comments. I also wish to acknowledge the referees for useful suggestions. None of these individuals
bear responsibility for any error.
 The Author 2010. Published by Oxford University Press on behalf of the Society for Political Methodology.
All rights reserved. For Permissions, please email: 
 Published online by Cambridge University Press
time periods. This is undesirable because heterogeneity and dynamics are of substantive
importance to political scientists . Adequately modeling serial correlation not only
improves reliability of statistical inference but also reveals sub rosa political dynamic
processes. Likewise, contemporary correlation caused by time-speciﬁc characteristics
(common shocks from the system) is substantively interesting in studies of international
and comparative political economy (IPE and CPE) because globalization has a major impact on almost all signiﬁcant political economic phenomena . Heterogeneity across units is also useful for crossunit comparison. This is especially meaningful when the data contain a small or moderate
number of theoretically interesting units, such as countries, states, or legislators. Thoroughly analyzing heterogeneity is also crucial for correctly understanding dynamics.
Ignoring heterogeneity leads to ‘‘spurious dynamics’’ wherein temporal pseudodependence is simply caused by unmodeled differences instead of dynamics .
Therefore, TSCS data analysis has two equally important tasks: overcoming the methodological challenges raised by the correlated TSCS structure and taking advantage of the
unique opportunities presented by TSCS data for studying substantively interesting relationships. In the political methodology literature on TSCS analysis, most efforts are put
into the former task, but the latter one is rather under addressed.
Methodologically, most existing methods are developed for linear models. The complexities caused by nonlinearity have rarely been addressed . In the broader literature on longitudinal data
analysis, little effort has been made to thoroughly model intertemporal and contemporaneous dependencies in the nonlinear framework . However, TSCS data
in political science often have discrete outcomes (most often, dichotomous outcomes):
for example, democratization, exchange rate regime, international conﬂict, civil war, or
state failure. Compared to continuous outcomes, discrete (especially dichotomous) data
make it even more challenging to handle the already quite complex TSCS data structure.
This article proposes a Bayesian generalized linear multilevel model with a pth-order
autoregressive error process (henceforth, GLMM-AR(p)) to model unbalanced binary
TSCS data. In contrast to conventional methods that treat heterogeneity as an estimation
nuisance, this Bayesian model analyzes heterogeneity with random-effect parameters, and
their estimates (posterior distributions) can be easily used to learn the degree of heterogeneity in the time and spatial dimensions.1 In this multilevel model, covariates are included at their appropriate levels to explain the individual- or group-level variation.
The autoregressive error process is used to test and correct serial correlation. It also serves
as a residual-based TSCS cointegration test to help avoid spurious regressions. To overcome the estimation difﬁculties caused by nonlinearity and high dimensionality, I use several methods to develop an efﬁcient Markov chain Monte Carlo (MCMC) algorithm. I also
propose a computational scheme for model comparison based on the Bayes’s factor (BF).
1In this paper, I only consider contemporaneous correlation caused by time-speciﬁc shocks from the system. Contemporaneous correlation can also result from interactions among the units, which is often analyzed by applying
spatial dynamic regressions . Integrating spatial dynamics is a future
extension of the model presented in this paper.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
The model and methods are applied to simulated and empirical binary TSCS data to improve the reliability of statistical inference and forecasting.
Motivating Examples and TSCS Data Structure
The GLMM-AR(p) model proposed in this paper is based on the generic characteristics of
the TSCS data structure. Before introducing the model speciﬁcation and going through the
statistical details, it is necessary to ﬁrst discuss the structural characteristics of TSCS data
and the associated methodological issues. To do so, I use two typical TSCS data sets in
political science as motivating examples.
The ﬁrst application regards state failure in the sub-Saharan region. A large database on
state failure was released by the Political Instability Task Force (PITF) in 2001 and has
been analyzed in many studies . The sub-Saharan data set contains 40 countries repeatedly measured
from 1956 to 1995. The response variable, state failure, is dichotomous—a new occurrence
of state failure in a country-year is coded 1; otherwise, the outcome is coded 0. Because of
missing outcome data, the data structure is rather unbalanced. The minimum number of
country observations is only 3 (Eritrea and Ethiopia), and the maximum is 40 (Liberia,
South Africa, and Swaziland). In the cross-sectional dimension, only three countries
are observed in each of the 4 years from 1956 to 1959 compared to 40 countries in
1993 and 1994. One of the interesting but debated substantive questions in the IPE
and CPE literature is whether trade openness prevents state failure .
The second example is also about state failure in the sub-Saharan countries but focused on
civil war,one speciﬁc typeof state failure. The responsevariable, civil war duration, iscoded
1 if a country is in a new or ongoing civil war; otherwise, it is coded 0. The data are from
Miguel, Satyanath, and Sergenti and analyze how economic growth affects the duration of civil war. The sample countries are observed between 1980 and 1999. The minimum number of country observations is 9 (Namibia) and year observations is 36 ,
whereas the maximum number of country observations is 20 and year observations is 40.
The basic characteristics of the two TSCS data sets include multilevel and dynamic
features and an unbalanced data structure. The data are multilevel in the sense that information is gathered both at the individual (country-year) and at the group (country and year)
levels. Some variables of theoretical importance vary by time and unit, such as trade openness, economic growth, and democracy, but others are group-level characteristics, invariable across time or units, such as colonial heritage, geography, whether the state is an
oil-exporting country, and the Cold War. Because of the multilevel feature, TSCS data
are clustered in the time and spatial dimensions without nesting. Observations of the same
year or of the same country can be regarded as members of the same group. Observations in
the same group are correlated because they share the group-speciﬁc features . Furthermore, the effect of
a covariate can vary by country or year, implying varying coefﬁcients across groups (heterogeneous effects). To handle observed, unobserved, and coefﬁcient heterogeneity, we
can apply standard multilevel modeling in the classical or Bayesian frameworks . Such models can be estimated with well-developed software, such
as the lme4 R package, JAGS/BUGS, the SAS program, and so forth.
The most challenging part of discrete TSCS analysis lies in how to appropriately handle
their time-series features in the multilevel and nonlinear situation. Both the state failure and
 Published online by Cambridge University Press
the civil war duration data can be regarded as a collection of multiple time series. There is
always one time series for each unit in TSCS data, though the lengths of the time series may
vary widely; therefore, all the concerns in time-series analysis are also relevant to TSCS
data. Beck and Katz , Beck, Katz, and Tucker , and Beck et al. provide
excellent guidelines for political scientists about how to handle serial correlation and how
to use information in TSCS data to reveal political dynamics. However, several important
questions have not yet been fully addressed. First, although we can use a variety of methods
to capture dynamics in the data, most of them do not speak to whether the errors are still
serially correlated. There are multiple sources of intertemporal correlation, including unmodeled dynamics, omitted variables, or measurement error. Dynamics in the errors may
not be fully captured by lags or time splines. Serial correlation is the nonrandom temporal
pattern that is not explained in a model. Uncovering this pattern is necessary to better understand dynamics and reach more reliable inferences. Second, in the motivating examples
as well as many other empirical studies, the response variable is discrete. Methods for serial
correlation in linear models do not necessarily apply to nonlinear models. For example,
a linear model with a lagged response has the same error structure as one with an AR(1)
error process, but this is not the case in generalized linear models. Third, some of the most
often-employed methods should be used with caution. Including lagged values in a structural model should have a theoretical justiﬁcation. If the errors are still correlated, lagged
responses invite endogeneity . Last but not least, cointegration testing is necessary to avoid spurious
regressions in TSCS analysis .2 In the state failure example, it is possible that either the mean level or the
variance of the underlying failure propensity changes over time. Noncointegration is more
likely in the civil war duration study. As Fearon and Laitin point out, civil wars tend
to be cumulative and last for multiple periods. In the data set, a civil war often lasts for
several years, sometimes more than a decade. This demonstrates strong path dependence.
The underlying propensity for a country to stay in a war may drift widely, but the observed
dichotomous response variable imposes formidable challenges to unit root and cointegration testing. One possible solution is to directly model the error dynamic process to see
whether it is stationary, which is actually a residual-based cointegration test on TSCS data
 .
In general, binary TSCS data—such as the state failure and civil war duration
examples—require a dynamic generalized linear multilevel model to handle heterogeneity
and serial correlation. The model should analyze the direct and indirect effects of grouplevel characteristics and estimate the dynamic error process. The model also needs to accommodate unbalanced data structures, given that missing outcome data in the state failure
and civil war duration example are a norm rather than an exception in political TSCS analysis. However, developing such a model faces at least three major methodological challenges. With heterogeneity in two dimensions and serial correlated errors, the overall error
structure consists of three parts, namely, the unit-level errors, the time-level errors, and the
serially correlated individual-level errors. The model should be able to analyze all three
parts at the same time. Furthermore, because the covariance matrix of the individual-level
2In time-series analysis, cointegration refers to the situation that a set of variables are individually integrated of
order 1, but some linear combination of these variables results is covariance stationary. In the regression sense, if
the covariates are cointegrated with the response variable, the residual process is stationary.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
errors is not diagonal, the likelihood function is intractable. This makes conventional data
augmentation methods inefﬁcient. In addition, the complication arising from correlated
errors is further exacerbated by unbalanced data structures that increase the complexity
of the covariance matrix. The following three sections will introduce such a model and
related estimation methods to handle those computational problems.
Model Speciﬁcation and Assumptions
In this section, I introduce the GLMM-AR(p) model and the necessary assumptions for
identiﬁcation and consistency. I give a general model speciﬁcation that is designed to accommodate a variety of real data analysis situations. For a particular data set, some parts of
the general speciﬁcation may not be employed. For instance, if no group-level predictors
are available, the group-level regressions can be omitted, and the random-effect coefﬁcients are simply modeled as varying around their overall mean. Likewise, if the researcher
has substantive theories suggesting that coefﬁcients may not vary by time or unit, the model
can be simpliﬁed to only have random intercepts. It is also possible that the ﬁnal speci-
ﬁcation chosen by model comparison does not have a dynamic error process; nevertheless,
it is still necessary to ﬁrst try different lag orders to test for serial correlation. When developing the model, it is important to leave all important options open and let the model be
general and ﬂexible. In empirical studies, we should apply the model based on the particular data set and use the BF to choose a model ex post (refer to the next section for
details). An R package, GLMMarp, is available online to help specify, estimate, and compare models in the general GLMM-AR(p) framework.
General Model Speciﬁcation: GLMM-AR(p) Model
Suppose that a data set consists of N unique units indexed i, where i 5 1, 2, . . ., N. Each unit
i has Ti observations at times 1i, . . ., ti, . . ., Ti, where {1i, . . ., ti, . . ., Ti} 4 {1, 2, . . ., T}.
With an unbalanced data structure, it is likely that Ti 6¼ T and Ti 6¼ Tj for i 6¼ j. Units i and j
are observed in the same time period if and only if ti 5 tj. Therefore, ti indicates the location
of observation yi;ti in the sequence {1, 2, . . ., T}, that is, the time-dimensional cluster it
belongs to. In the model, there are two sources of clustering: each observation yi;ti belongs
to clusters i and t# (ti 5 t#) at the same time. By using the latent variable speciﬁcation
 , the GLMM-AR(p) with a probit link can be written as follows:
zi;ti 5 x#
1i;tib11w#
i;tib2;i1s#
i;tib3;ti1ni;ti;
b2;i 5 Aib21bi;
b3;ti 5 Ftib31cti;
ni;ti 5 q1ni;ti211    1qpni;ti2p1ei;ti;
where I() is the indicator function. This speciﬁcation is an extension of commonly applied
mixed-effect models in longitudinal data analysis. In this model, whether yi;ti takes value
0 or 1 is determined by a latent continuous variable zi;ti and a threshold 0. At the latent level,
zi;ti is assumed to have a linear relationship with the covariates in equation (2). The error
 Published online by Cambridge University Press
term ni;ti follows an AR(p) process in equation (5). Although non-Toeplitz errors can be
applied, the simple AR(p) error speciﬁcation is adequate to approximate a white noise
process e with an appropriate order p.3 Besides its computational convenience, the
AR(p) error speciﬁcation has several other advantages. It can serve as a serial correlation
diagnostic following the classical Box–Jenkins procedure . It is also a residual-based cointegration test because by deﬁnition the autoregressive
error process should be stationary under cointegration. Note that this error speciﬁcation
does not theoretically or mathematically exclude the use of other methods for dynamics
modeling, such as lagged responses or lagged explanatory variables. If necessary, lags can
be included as ordinary regressors in the design matrices in equation (2). As this does not
lead to any additional methodological complexity, I do not specify them as separate terms
in the model and will not discuss them as special speciﬁcations. This general speciﬁcation
allows three groups of covariates: those in x1i;ti with FEs on all the observations and those in
wi;ti or si;ti with unit- or time-speciﬁc effects. For identiﬁcation, when the model has both
time- and unit-speciﬁc intercepts, one of them has to be centered at 0; and if there is also an
overall intercept, both random intercepts are mean 0. At the unit level in equation (3),
heterogeneity across units is modeled, and the variation of random effects b2,i is further
explained by a covariate matrix Ai, a parameter vector b2, and a group-level error term bi.
The same interpretation applies to the time-level regression in equation (4).
ðx1;iti; w#
i;tiAi; s#
i;tiFtiÞ and b 5 (b1, b2, b3,), which can be written as follows:
zi;ti 5 x#
i;ticti1ni;ti;
ni;ti 5 q1ni;ti211 . . . 1qpni;ti2p1ei;ti:
Here, the ‘‘interactions’’ w#
i;tiAi and s#
i;tiFti do not cause concern because model interpretation is solely based on the structural form in equations (1)–(5). The estimation form
clearly demonstrates the complexity of the error structure. If we assume that {bi} and {ct}
are uncorrelated, the error structure can be expressed as follows: Rzi 5 W#
iRcTiSi1Rni, where Rzi, Rbi, RcTi , and Rni are the covariance matrices of zi, {bi},
fcTig, and ni, respectively.4 It is important to note that the covariance matrix Rzi reﬂects
both correlation and heteroskedasticity.
Several assumptions are required for identiﬁcation and estimation.
1. The error term ei;ti is white noise—that is, ei;ti  Normal
2. There is no cross-sectional correlation in the individual-level error term n; mathematically, that means cor
ni;ti; nk;tk
5 0, "i 6¼ k.
3. The covariates in xi;ti, wi;ti, and si;ti are sequentially exogenous. Formally, this
assumption is ðxi;ti; wi;ti; si;ti ? ni;tiÞjzi;tsi ;
4. The error term n follows a pth-order autoregressive process.
5. The autoregressive process is stationary. The stationarity assumption is restrictive
and requires that the dynamic process zi is either stationary or cointegrated with
3A Toeplitz matrix is also known as a diagonal-constant matrix, which is a matrix with constant descending
diagonals from left to right. The AR(p) and MA(q) covariance matrices are both Toeplitz.
4For model speciﬁcation and estimation, this assumption is not necessary. Here, the assumption is used only for an
easily written mathematical form as an illustration how complex the error structure is.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
the explanatory variables ; otherwise, the model cannot
be applied.5 This assumption makes the GLMM-AR(p) a cointegration test following
the line of residual-based testing . Later in this paper, I use an empirical example to illustrate how the GLMM-
AR(p) model serves as a tool to detect spurious regressions.
6. Finally, I specify prior distributions of the parameters as follows:
r : r 2 Sq
where N is the normal density, W is the Wishart density, U is a uniform density, and Sq is the
stationarity space of the p-th order autoregressive process.6 Because ei;ti, {bi}, and {ct} are
error terms, their prior means are 0. With these prior functional forms, all the parameters
except r have conditional conjugacy. Other prior speciﬁcations are possible. For instance,
the autoregressive coefﬁcients r could have a multivariate normal prior distribution truncated in the stationarity space.
MCMC Algorithm
Because the error term in this model is serially correlated, the covariance matrix Rn has
nonzero off-diagonal elements, making it challenging to estimate. One possible ‘‘solution’’
is to use so-called robust standard errors. However, this method is not as convenient as it
seems to be because when constructing the weight function, we have to overcome three
types of bias .7 More importantly, using robust standard errors discard valuable information likely contained in the
error term. Such information could have been used for analyzing dynamics and heterogeneity and for improving forecasting. Other solutions in the literature include numerical
methods, such as penalized quasilikelihood, marginal quasilikelihood, or expectationmaximization algorithms . Those methods are burdened with many approximation steps such as linear or high-order Taylor expansion,
Gauss–Hermite quadrature, pseudodata generation, and empirical Bayes’s estimation.
What is even worse is that these procedures often produce estimates and standard errors
that are biased toward zero, especially with low-order expansions . The
Bayesian approach has been widely used for analyzing multilevel models because it has the
ﬂexibility needed for the estimation of parameter-rich models , and the bias caused by evaluating the estimator at estimates rather than the true parameters (centering bias).
 Published online by Cambridge University Press
and Verbeke, 2005). In this section, I discuss these estimation problems and provide
Bayesian solutions.
Cholesky Decomposition and Auxiliary Variable Approach
When developing an MCMC algorithm for model estimation, the major problem posed by the
nondiagonal matrix Rni is the construction of the conditional distribution of {ct}. To sample
{ct}, we have to ﬁrst switch from the zi-dimension to the zt-dimension and then determine
the joint error distribution in the zt-dimensional expression of the GLMM-AR(p) model:
z1 5 x1b1w1bN11s1c11e1;
z2 5 x2b1w2bN21s2c21e2;
zT 5 xTb1wTbNT1sTcT1eT:
This is a seemingly unrelated regression (SUR) system. The errors in the Tequations are
correlated because of serial dependence. The covariance matrix is VarðejX; w; sÞ 5
Ret5IT for a balanced data set and more complicated if Nt 6¼ Nk for some t 6¼ k. Handling
unbalanced structures is not easy in SUR analysis. In the Bayesian framework, it is feasible
to calculate the covariance matrix Var(ejX, w, s) with unbalanced structures. However, it is
computationally inefﬁcient to calculate an M  M matrix
in each MCMC
iteration by decomposing Toeplitz matrices and then mapping the elements into a large
covariance matrix.
Another closely related problem is that the nondiagonal covariance matrix means that
we have to sample from truncated multivariate normal distributions to augment {zi}. Most
existing methods for doing this are inefﬁcient and mix slowly . Correlated errors also make it
difﬁcult to compute the BF for model comparison. This is because to calculate the BF, we
have to deal with the following integral in the likelihood computation:
yi;1i; . . . ; yi;Ti
zi;t1i; . . . ; zi;Tiju
dzi;t1i . . . dzi;Ti;
ai;ti; bi;ti
is the truncated region determined by the value of yi;ti, and u represents all
the parameters in the model. This high-dimensional integration is difﬁcult because yi are
not i.i.d. conditional on u—that is, f
yi;1i; . . . ; yi;Ti
. To do numerical
integration, we need random samples of z from its marginal distribution (unconditional on
y). This creates a paradox because the latent process z can only be inferred from the observed process y. One solution is the method of Geweke, Hajivassiliou, and Keane (the
GHK simulator) , which is computationally expensive, especially with a large data set.
Alternatively, if the latent response variable is updated by using a polynomial operator
zi;ti, the likelihood can be approximated with the auxiliary particle ﬁlter . However, this sampling importance resampling scheme
is numerically unstable for a high-order Markov process and often requires numerous
draws to obtain a valid approximation of the likelihood.
In this paper, I propose an algorithm which orthogonalizes the correlated errors in such
a way that z can be sampled independently. With this method, f
yi;1i; . . . ; yi;Ti
, where u# 5 (u, u) and u is an auxiliary parameter vector. The basic idea
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
is as follows: ﬁrst, I decompose the covariance matrix Rni into two parts Rj 5 Vi1jiITi,
where Vi is a positive-deﬁnite matrix and ki is an arbitrary constant; and Vi is further
decomposed into V#
iVi in which V#
i is a lower triangular matrix produced in the Cholesky
decomposition.8 Hence, Rji 5 V#
iVi1jiITi. Now equation (6) can be reexpressed as
where the new error term ei  NTið0; jiITiÞ, the auxiliary variable ui  NTið0; ITiÞ, and ei and
ui are mutually independent. In equation (10), zi has exactly the same covariance matrix as
when integrating ui out. Given ui, r and other parameters, the zs do not need to
be updated conditional on one another because the new error term ei is i.i.d. There are
general formulas to compute the Toeplitz covariance matrix Rji for AR(p) errors. In practice, I do not need to compute each Rji; instead, in each iteration, I can simply compute RT
 Tand construct the covariance matrix Rji for each i by taking the ﬁrst Ti rows and columns
of RT  T. With this method, all zs are updated in one block instead of PN
i 5 1 Ti blocks,
simulation
efﬁciency.
Importantly,
yi;1i; . . . ; yi;Tiju#
, the likelihood can be computed as simply as in
an ordinary probit model.
q1; q2; . . . ; qTi
iui, and the full MCMC algorithm is as follows:
1. b; fbig; fuigj   pðfuigjb; fbig; Þpðfbigjb; Þpðbj  Þ:9
   NK1
, where B1 5
, and Hi 5 ðVi1w#
 bijb;   NK2ðbi; DiÞ, where Di 5
bi 5 Diw#
iðViÞ21ðzi2x#
 uij   Nðui; UiÞ, where Ui 5 ðITi1ViV#
ui 5 UiViðzi2x#
2. fctgj   NK3ðct; EiÞ, where Ei 5
iðkNtINÞ21si
and ct 5 Eis#
iðkNtINÞ21
tbNt2qtÞ, where kNt 5
j1; jj; jN
i;ticti1qi;ti; ji
and E21
, where m1 5 m0 1 N,
i 5 1 bib#
21, g1 5 g0 1 T, and E1 5 ðE21
t 5 1 ctc#
5. rj   WðrÞ  Nðˆr; PÞ, I use a Metropolis–Hasting algorithm to update r with a
tailored kernel Nðˆr; PÞ as in Chib .
Improving Mixing: PGM-MGMC
There are two important issues in applying MCMC methods: convergence and mixing. To
draw reliable inferences based on MCMC samples, we ﬁrst have to ensure that the
8I choose .i/2 for ji, where .i is the smallest eigenvalue of Xi. This choice follows Chib and Jeliazkov and is
to make the algorithm numerically stable.
9Sampling b, {bi}, and {ui} in one block improves the efﬁciency of this algorithm because they are correlated by
construction; however, it is not feasible to include {ct} in this block because of the complex covariance structure
caused by the unbalanced data structure.
 Published online by Cambridge University Press
simulation output is actually from the target distribution. If the algorithm is correctly
developed, the Markov chain is guaranteed to converge eventually to the target distribution. The second major concern is mixing—how fast the chain explores the sample
space and produces a representative sample of the posterior distribution. When mixing
is slow, the chain stays in a local region for a long time. If we stop the simulation
too early, we will reach conclusions based on local draws, which is as misleading as nonconvergence. For models with only a few parameters, mixing is often not a big concern,
but for multilevel models with a large number of random-effect and low-level parameters,
the Markov chain can mix so slowly that Bayesian models may be impractical . The GLMM-AR(p) model has serially correlated erors
and random effects in two dimensions. The algorithm presented above has the slowmixing problem, as shown in preliminary simulations. This problem is quite serious
for the random-effect parameters because their within-chain correlation decreases so
slowly that autocorrelation is still above 0.5 even after 200 lags. In those simulation studies, with 500,000 post-burn-in iterations, the posterior samples still fail formal convergence tests using the coda R package. This highlights the necessity of improving the
efﬁciency of the algorithm. The solution used in this paper is to add a PGM-MGMC updating stage into the algorithm. The PGM-MGMC sampler moves the Markov chain in
a coordinate-free way in each MCMC iteration. It noticeably reduces within-chain correlation and speeds up MCMC mixing. This is illustrated in the simulated and empirical
examples later in the paper.
The basic idea of multigrid methods is to use a sequence of auxiliary ‘‘coarse-grid’’
problems in addition to the original ‘‘ﬁne-grid’’ problem so that the information is more
efﬁciently stored and convergence is accelerated . This method was ﬁrst applied in statistical physics and Euclidean quantum
physics. Goodman and Sokal extended the deterministic multigrid method into
a MGMC algorithm by applying partial resampling and ﬁber construction. Liu and Sabatti
 generalized the Gibbs sampler by using the MGMC method to decompose the sample space into disjoint orbits to facilitate information transmission.10 After a transformation
group is chosen, the Markov chain is moved by from one orbit to another without leaving
the target sample space . This enables a faster exploration of the sample
space and achieves the effects of reparameterization, blocking, and grouping, but it decomposes the sample space more freely than other methods. Liu and Sabatti applied this
method to state space models and showed that the move could be dramatic and autocorrelation was reduced. Mueller and Czado also applied the PGM-MGMC method to
reduce autocorrelation in their autoregressive ordinal probit model and demonstrated
efﬁciency gains.
The posterior distribution of the GLMM-AR(p) setup facilitates the development of
a mover distribution. From this mover distribution, we can randomly draw a mover to transform a subset of m-dimensional parameter vector x [ ({zi}, b, {b}, {c}). To apply the
PGM-MGMC method, I choose the scale group C 5 {v > 0:v(x) 5 v  x} and calculate the
unimodular Haar measure as L(dv) 5 v21 dv. Now, it is straightforward to see that the
mover, v, can be sampled from a standard gamma distribution:
10In group theory, deﬁne a group G and a set X, and an orbit of x 2 X is the set S  X to which x can be moved by the
elements of G. Disjoint orbits simply means that the orbits S1, S2, . . . are disjoint. Refer to Bogopolski or
Aschbacher for more formally deﬁnitions and detailed theories about orbits.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
v m21pðvvÞdv}vm21exp
ðvzi2xivb2wivbi2vcÞ
i ðvzi2xivb2wivbi2vcÞ
ðzi2xib2wibi2sicTiÞ
i ðzi2xib2wibi2sicTiÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
This kernel is proportional to a gamma distribution C (a, b) of random variable v2 with
parameters a 5 (m 1 1)/2 and b 5 (Q1 1 Q2)/2. I apply this PGM-MGMC sampler in each
iteration in the MCMC algorithm to further update the parameters in the following way:
i g, vbðgÞ/bðgÞ, fvbðgÞ
i g, and fvcðgÞ
Bayesian Model Comparison
Because we always have uncertainty in almost all respects of model speciﬁcation, implementing information-based criteria in model decision making is important . In
the GLMM-AR(p) model, besides the uncertainty in other aspects of model speciﬁcation,
we normally have little prior information about the order of the autoregressive error process. In linear time-series regressions, the way to handle this problem is to ﬁrst estimate
models with different lag orders and then to determine the lag order by model comparison.
However, the same procedure is not commonly performed for nonlinear models, and lag
orders are often set for convenience sake (most often the ﬁrst order). The BF is a comprehensive criterion for model comparison, but it is notoriously difﬁcult to compute because of
high-dimensional integration and numerical instability . With correlated errors, completing this type of computation often requires additional samplers, such
as importance samplers or recursive importance samplers
 . However, because of the way, I orthogonalize the errors, I
can compute the BF in such a simple way that it only requires the full- and reducedrun MCMC outputs.11 In this section, I present the computational scheme for the BF.
The BF is deﬁned as the ratio of the marginal likelihood of two competing models. The
marginal likelihood of a model is the quantity after marginalizing the parameters from the
likelihood, that is, f ðyjMjÞ 5
dh, where Mj refers to Model j. There are
various approaches to approximate the BF . Here, I use the marginal
likelihood method developed by Chib and Chib and Jeliazkov . To apply this
11Full MCMC simulations or full runs estimate all the parameters in the model, whereas reduced MCMC simulations or reduced runs refer to simulations using the same algorithms as the full runs but ﬁxing a subset of
parameters at certain values.
 Published online by Cambridge University Press
method, I ﬁrst express the marginal likelihood as the normalizing constant in the Bayesian
posterior setup: m(y) 5 f (yjh)p (h)/p (hjy). Then, I ﬁx h at h*.12 The marginal likelihood on
a logarithmic scale can be computed by using the formula:
ln ˆmðyÞ 5 ln ˆfðyjhÞ1ln ˆpðhÞ2ln ˆpðhjyÞ:
With the Cholesky decomposition and auxiliary parameter approach, the likelihood ordinate, ˆfðyjhÞ, is straightforward to compute. Denote by u all the parameters except the
auxiliary variable u, the likelihood ordinate can be approximated by integrating out u:
ˆfðyjuÞ 5 1
ðDitiÞyitið12DitiÞ12yiti ;
Because I integrate out u (q is a function of u) with respect to the conditional distribution p(uju*, z), a reduced run is required to recursively sample from p(ujz, u*) and
p(zju, u*).
To approximate the posterior ordinate p(u*jy), I partition it in the following way:
ˆpðb; b; D; r; EjyÞ 5 ˆpðrjyÞˆpðcj; r; yÞˆpðEjc; r; yÞˆpðbjE; c; r; yÞ
 ˆpðDjb; E; c; r; yÞˆpðbjD; b; E; c; r; yÞ;
and compute each term on the right-hand side:
1. ˆpðrjyÞ: denote by w all parameters except r and u:
ˆpðrjyÞ 5 J21 PN
i 5 1 ðaðrðjÞ; rjy; cðjÞ; uðjÞ; zðjÞÞqðrðjÞ; rjy; cðjÞ; uðjÞ; zðjÞÞÞ
k 5 1 ðaðr; rðkÞjy; cðkÞ; uðkÞ; zðkÞÞÞ
The numerator is the sample expectation with respect to p(w, u, zjy). The MCMC output
can be used to average out these parameters. The denominator is the sample expectation
with respect to the conditional product measure p(w, u, zjy)q(r*, rjy, w, u, z). Here, one
reduced run is needed by ﬁxing r at r* and updating all other parameters.
2. ˆpðcjr; yÞ: use the output of the reduced run conducted in Step 1 for this numerical
integration.
3. ˆpðEjc; r; z; yÞ 5 ˆpðEjc; yÞ: no reduced run is required.
bE; c; r; y
i 5 1 ˆpðb
i jc; r; zi; yiÞ: conduct a reduced run by ﬁxing E, c,
5. ˆpðDjb; E; c; r; yÞ 5 ˆpðDjb; yÞ: no reduced run is needed.
12The parameters u can be ﬁxed at any values. But for the sake of numerical stability, values in the high-density
region of the posterior are preferred. In this paper, I choose u* to be the posterior mean.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
6. ˆpðbjD; b; E; c; r; yÞ 5 ˆpðbjb; c; r; z; yÞ: conduct a reduced run by ﬁxing
D, b, E, c, and r.
Simulation Study
I examine the performance of the model and estimation techniques with two Monte Carlo
experiments. The only difference between their data-generating processes (DGPs) is the
error dynamic process—the true autoregressive coefﬁcients are q1 5 0.7 and q2 5 0.2
in the ﬁrst data set and q1 5 20.5 and q2 5 20.3 in the second one. Chib and Jeliazkov
 argue that in hierarchical models, negative autocorrelation is easier to identify.
Here, I use the two comparable data sets to check this point. In both data sets, the number
of observations is 2187, and the data structures are unbalanced. The number of observations of a unit varies from 2 to 50 and that of a time period varies from 8 to 50. Both DGPs
have a mixed-effect design: there are ﬁve covariates with FEs (xi), ﬁve with unit-speciﬁc
effects (wi), and two with time-speciﬁc effects (si). At the group levels, there are three unitlevel predictors (A) and two time-level predictors (F).
I assign diffuse priors. For the auxiliary parameter vector, ui, its prior is NTið0; ITiÞ by
design. The prior choice is also straightforward for the group-level errors {bi} and {ct}. As
residuals, their distributions are centered at 0 and their covariance matrices are treated as
hyperparameters. The coefﬁcient parameter vector b is assigned with a multivariate normal
prior centered at 0, having a diagonal covariance matrix, 400  I. This prior is vague and
does not have notable effects on the posteriors, given that the sample size is 2187. In order
to implement the PGM-MGMC algorithm, the priors of b are required to be centered at 0.
The prior on the autoregressive parameter vector b is a multivariate uniform distribution
truncated within the stationary space. For the hyperparameters D and E, their priors can
have a discernible inﬂuence on the posteriors because binary data often have very little
information about such low-level parameters. However, excessively diffuse priors, such
as setting D0 5 100  I, are likely to cause trouble in inverting matrices in the Wishart
updating. Cowles, Carlin, and Connett and Ibrahim and Klainman also observe that excessively vague priors of the covariance matrices invite numerical instability
and slow convergence. My experience from many trials suggests that priors around D0 5
20  I (and the same for E0) are good choices for balancing prior inﬂuence and numerical
stability for varying sample sizes. The sensitivity of posteriors to the prior choices was
checked by using alternative priors with reasonable changes of locations and scales.
I conducted the simulations using two Macintosh machines, each equipped with eight
2.26 GHz Intel Xeon ‘‘Nehalem’’ processors and 6 GB of memory running Mac OS X.
With the sample size 2187, the average time for one iteration is about 5.35 s. The MCMC
outputs analyzed in this section are based on 100,000 iterations after discarding 10,000
burn-in iterations for each model. Competing models were running in parallel. The running
time for each GLMM-AR(p) model is about 1 week. This computing time is acceptable
considering that the code is written completely in R. In the Metropolis-Hastings (MH) step,
because the proposal density is tailored, the acceptance rate is roughly between 75% and
90%. Multiple convergence diagnostics were conducted for all parameters except the augmented data and the auxiliary parameters u. Because there are too many such parameters
(2187  2), I randomly drew 100 of each and conducted diagnostics.
As observed by Carlin and Olsen and Schafer , the slow MCMC mixing
problem for nonlinear mixed-effect models is serious even after the improvement achieved
by the Cholesky decomposition and auxiliary parameter approach. As shown in Fig. 1, for
the FE parameters, the standard Gibbs chains are mixed well; but for the random-effect and
 Published online by Cambridge University Press
the low-level parameters, autocorrelation decreases rather slowly—even after 150 lags, it is
still as high as 0.5. By using the PGM-MGMC mover, the mixing is considerably improved,
especially for those slowly mixed parameter chains, as shown by the blue areas in Fig. 1.
To select the lag order p for the autoregressive error process, I start by estimating the
GLMM-AR(0) model with the assumption of serial independence and compute its marginal
likelihood. Then I estimate the GLMM-AR(1) and compare its marginal likelihood with that
of the GLMM-AR(0). If the marginal likelihood of the AR(0) model is larger than that of the
AR(1) model, I choose 0 as the lag order; otherwise, I increase the lag order to estimate the
GLMM-AR(2) and compare it with the GLMM-AR(1). I continue this process until an increase of the lag order leads to a decrease of the marginal likelihood and then I choose the lag
order associated with the largest marginal likelihood. This procedure illustrates how the
model can serve as a serial correlation diagnostic for binary TSCS data. It is much easier
to apply than the score test proposed in Gourieroux, Monfort, and Trognon , especially for a high-order autocorrelation diagnostics. Notice that the lag order chosen by this
process is the one that ﬁts the data best. It may not be the ‘‘true’’ lag order if the data do not
provide enough information to fully uncover the error process. The posteriors of the autoregressivecoefﬁcients can tell how strongand how lasting the autocorrelation is. In practice, I
estimated GLMM-AR(p) with the lag orders from 0 to 4 or 5 in parallel and then compared
them to decide whether a further increase is needed and what the ﬁnal lag order is.
In Table 1, I report the marginal likelihood of the competing models.13 For DGP I with
positive serial correlation, the GLMM-AR(3) is the best model among the ﬁve models with
the lag orders from 0 to 4. Meanwhile, for DGP II with negative serial correlation, the
GLMM-AR(2) ﬁts the data best compared to other competing speciﬁcations. Figure 2
Comparison of within-chain correlation: Gibbs versus Gibbs 1 PGM-MGMC.
Note. The gray shadow indicates the autocorrelation of the standard Gibbs chains, whereas the
black shadow shows the decrease of autocorrelation when applying the PGM-MGMC method. From
the left to the right, the graphs illustrate the autocorrelation of the Markov chains of a FE parameter,
a random-effect parameter, and one element in the covariance matrix E. These parameters are
randomly chosen.
13Because the BF is just a pairwise comparison of marginal likelihoods of two competing models, we can directly
look at marginal likelihoods—the biggest value represents the best model.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
summarizes the posteriors of all FE parameters and part of the mean parameters of random
effects.14 The GLMM-AR(0) assumes independent errors. It produces estimates biased
toward 0 and artiﬁcially small error bands (high certainty levels) for both DPGs. This
is for two reasons: ﬁrst, with falsely assumed serial independence, estimators of standard
errors are biased ; in
addition, with the probit link, the coefﬁcient estimates are scaled by Rni. If the errors
are correlated, the standard identiﬁcation assumption Rni 5 I leads to smaller estimates.
This is because the actual diagonal elements in Rni are greater than 1 with autocorrelated
errors so that estimates are further scaled toward 0. This kind of bias also applies to the
random coefﬁcients at the two group levels, as illustrated in Fig. 3.
In these simulated studies, I did not ﬁnd clear evidence that negative autocorrelation is
easier to identify—their directions are all estimated correctly, although their scales are not.
With a larger number of time periods or units, the autoregressive coefﬁcients will be better
estimated. In Fig. 2, most posteriors of the models with p > 0 cover true values, and they are
similar across those models. Although all the true parameters except r are exactly the same
in the two DGPs, the posteriors are slightly different for the two different data sets, implying that the two data sets are also different in terms of how informative the data are.
The simulation designs put much heterogeneity in both the time and the unit dimensions: the random effects b2i and b3t are generated from distributions NðA#
ib2; DÞ and
tb3; EÞ with large variance. Figure 3 shows that the estimated group-level residuals
vary widely across units and between time periods. The graphs also demonstrate that an
adequate correction for serial correlation is important for better identifying heterogeneity.
The posteriors of the group-speciﬁc residuals in the GLMM-AR(0) model are more concentrated around zero. This demonstrates that neglecting serial correlation results in the
underestimation of heterogeneity.
Other parameters such as the covariance matrices D and E are estimated less precisely
than the higher level parameters. They are also relatively sensitive to prior speciﬁcations.
However, the estimated correlation parameters are close to the true values. An increase of
the number of units helps better estimate D and decreases its sensitivity to priors. Similarly,
a larger number of time periods identiﬁes E more precisely.
Empirical Examples
In this section, I use the two motivating examples discussed at the beginning of the paper to
demonstrate the implementation of the model. The state failure example shows that, by
controlling for multiple sources of confounding and making good use of information
Log marginal likelihood
Simulation I
(q1 5 0.7, q2 5 0.2)
Simulation II
(q1 5 20.5, q2 5 20.3)
14There are 5  3 1 2  2 such parameters, and it would take too much space to report all of them in the paper.
 Published online by Cambridge University Press
in the error term, the GLMM-AR(p) model ﬁts the data better than conventional models
and considerably improves within-sample forecasting. The civil war example highlights
the necessity of serial correlation diagnostics in TSCS data analysis: falsely assuming independent errors ignores the possibility of nonstationarity and may result in spurious
Posteriors of GLMM-AR(p) models with different lag orders.
Note. The solid lines are 90% credible intervals and the dashed lines are 95% credible intervals.
Posteriors of the competing models are grouped by parameters. The true values of the parameters
presented in the graphs are as follows: for the FE parameters, b11 5 2, b12 5 24, b13 5 0, and b14 5 4;
for the mean parameters of the unit-speciﬁc effects, b211 5 0:5, b212 5 1, b213 5 22, b213 5 22,
b251 5 3, b252 5 24, and b253 5 0; and for the mean parameters of the time-speciﬁc effects, b311 5 23,
b312 5 0:2, b321 5 2, and b322 5 21.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
regressions. This empirical example demonstrates how the GLMM-AR(p) model helps
detect suspicious cases of noncointegration.
State Failure in the Sub-Saharan Africa
Since 1994, the PITF has been building statistical models to explain and forecast state
failure (see Phase I Findings to Phase IV Findings available at 
.edu/pitf). King and Zeng gave a comprehensive critique of the methodology
applied by PITF for drawing causal inferences and conducting prediction and provided
suggestions on how to improve state failure data analysis. From a different perspective,
Heterogeneity in unit and time dimensions (two examples).
Note. The dots in the graphs indicate posterior mean. The vertical lines are the 95% credible intervals
of the posteriors of the group-level errors, b5 and c1, measuring the unexplained variation of the
random-effect parameters, b25;i and b31;t. Each vertical line is a group-speciﬁc residual distribution.
Salient heterogeneity is reﬂected by the varying locations and scales of the posteriors.
 Published online by Cambridge University Press
I use the state failure data of the sub-Saharan countries to illustrate how the GLMM-AR(p)
model improves statistical inference and forecasting.
In the imputed data set, there are 1214 country-years, and there are 446 state failures.15
The proportion of state failure is 36.74%, which means that state failure is not a rare event
in the sub-Saharan region. Hence, case–control resampling is not necessary or preferred given the fact that it unnecessarily discards
observations and breaks the data structure. Following PITF and King and Zeng
 , I lagged all the explanatory variables for 2 years. This is not only to avoid simultaneity but also to use the statistical model to do 2-year-ahead prediction. Table 2 summarizes the within-country and within-year variation of the data.16 There are three
variables, party fractionalization, regime durability, and male secondary school enrollment, which have little cross-year variation for most countries, causing serious problems
with the FE model. The cross-sectional variation of almost all variables changes considerably from year to year, implying heterogeneity between the sample time periods. In the
literature, there is a debate about the effect of trade openness on state failure: PITF and
Beck et al. ﬁnd that it signiﬁcantly reduces the risk of state failure, but King and
Zeng argue that the effect is unclear and uncertain. If the overall effect of trade
Within-group data variation: state failure example
Within-country variation
Within-year variation
Minimum Mean Maximum Minimum Mean Maximum
State failure
Party fractionalization
Party legitimacy
Regime durability
Calories per capita
(consumed)
GDP per capita
Neighbors in conﬂict
Neighbors in civil/ethnic war neighII
Infant mortality
Political terror scale
Political discrimination
Secondary school enrollment Enroll
Change of democracy
Trade openness
Note. The table presents the data variation in the time and spatial dimensions. In the within-county (within-year)
variation column is the summary of how the variables vary within each country (year). If a variable is invariant
or slowly moving (with any 0 or close to 0 value in the row of a variable), it will cause identiﬁcation and estimation
problems for the FE estimator. GDP, gross domestic product.
15There is a large proportion of missing data in the original data set. Bayesian data augmentation is a preferred
method to handle missingness than multiple imputation . However, due to the
more than 2000 parameters in the GLMM-AR(p) model, Bayesian data augmentation for such a large proportion
of missing data will greatly increase the computing time. For this reason, I use the multiple imputation method to
handle the missingness before implementing the Bayesian MCMC simulation.
16The variables are chosen from the ‘‘candidate’’ covariates in PITF Phase III Findings (p. 24) by using stochastic
search variable selection method with 0.5 as the threshold value.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
openness is ambiguous, it may be because its effect is so heterogeneous across countries
that a cross-sectional generalization is misleading. Therefore, I assign the covariate trade
openness with a country-speciﬁc coefﬁcient. Country- and year-speciﬁc intercepts are also
included in the GLMM-AR(p) model to control for unobserved heterogeneity.
Because I do not have strong prior beliefs about the parameters in the model, the priors
used in this example are uninformative. For the purpose of model comparison, I assign the
same priors to the common parameters in competing models. I followed two basic rules
when specifying priors: the ﬁrst rule is that the prior should not affect the posterior in an
important way and the second is that the sample space deﬁned by the prior should be mathematically and substantively proper. I set b  N(0, 400  I). Centering the prior mean at
0 is conventionally interpreted as hypothesis testing on whether the coefﬁcients are different from 0 . The variance is chosen based on my experience: such a variance speciﬁcation normally ensures that the prior has negligible
inﬂuence on the posterior. About the priors of D and E, the smaller D21
0 , v0, E21
g0, the smaller effect the priors have on the posteriors (refer to Step 4 of the MCMC algorithm). However, because the sample countries are all in the sub-Saharan region, excessively large prior variances on D and E are not substantively defensible. I set D  E 
W(40, 20  I40)  W(40, 20  I40). This prior choice also helps avoid numerical instability. Other parameters are easy to choose, except the lag order p. The lag order is determined ex post by model selection instead of being parameterized in the model. I checked
robustness by using different sets of priors with reasonable changes. With the sample size
of 1214, the priors do not have visible inﬂuence on b and r, but with the number of groups
in both time and spatial dimensions equal to 40, the priors of D and E have observable
effects on their posteriors. Nevertheless, they do not have visible inﬂuences on the substantively interesting parameters.
I estimated thecompetingmodels usingthesamecomputingplatformasinthesimulation
studies. The average running time for a single iteration is about 3.72 s. Compared to the
simulated examples in the previous section, the state failure models require more iterations
(I did 200,000 iterations after 10,000 burn-in iterations). This is because colinearity in the
empirical data causes higher cross-chain correlation, which slows down mixing and increases the difﬁculty of assessing (non)convergence. Each GLMM-AR(p) model takes
about 9 days to complete the simulation process. I estimated six competing models, including the GLMM-AR(p) models with p 5 0, 1, 2, 3, the completely pooled probit model
(PROBIT), and a GLMM model without considering time heterogeneity (GLMM-CL1).
The PROBIT, GLMM-CL1, and GLMM-AR(0) all assume that the errors are serially uncorrelated. The PROBIT model does not analyze any unobserved heterogeneity either. The
GLMM-CL1 model only accommodates heterogeneity across units. The GLMM-AR(p)
models took a longer time to converge than the PROBITand GLMM-CL1 models, but with
the PGM-MGMC method, their mixing time was considerably shortened. Figure 4 shows
how much the method reduces within-chain correlations and improves MCMC mixing.
The posteriors and marginal likelihood of each model are reported in Fig. 5. Positive
serial correlation is detected: ˆEðq1Þ 5 0:37 in the GLMM-AR(1) and ˆEðq1Þ 5 0:27 and
ˆEðq2Þ 5 0:22 in the GLMM-AR(2); q1 and q2 are greater than 0 at a 95% credibility level.
The PROBIT ignores serial correlation and heterogeneity, and its estimates and standard
errors are noticeably different from the other ﬁve models. The model exaggerates the effects of almost all the variables and leads to over-conﬁdence on the estimates. The pooled
model suggests that nine explanatory variables are important with high certainty. It also
strangely supports the argument that higher male secondary school enrollment leads
to higher risk of state failure, which is contradictory to current theory . The other ﬁve models all ﬁnd that six
of the nine ‘‘important’’ variables in the PROBIT lose both importance and certainty after
controlling for heterogeneity. As shown in Fig. 5, the posteriors of the GLMM-AR(p) models with p > 0 are similar across the models, and the BFs suggest that the GLMM-AR(2) has
Mixing improvement by PGM-MGMC updating: state failure example.
Note. The gray shadow indicates the autocorrelation of the standard Gibbs chains, whereas the
black shadow shows the decrease of autocorrelation when applying the PGM-MGMC. From the left
to the right, the graphs illustrate the autocorrelation of the Markov chains of a FE parameter,
a random-effect parameter, and one element in the covariance matrix D. These parameters are
randomly chosen.
Posterior summary of six competing models: state failure example.
Note. Log marginal likelihood: PROBIT 5 2584.794, GLMM-CL1 5 2511.185, GLMM-AR(0) 5
2503.931, GLMM-AR(1) 5 2470.357, GLMM-AR(2) 5 2468.638, and GLMM-AR(3) 5
2473.560. The solid lines are 90% credible intervals and dashed lines are 95% credible intervals.
Posteriors of the competing models are grouped by parameters.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
the best goodness of ﬁt, slightly better than the GLMM-AR(1) (log10 5 0.74), but decisively better than the GLMM-AR(3) (log10 5 2.14). It ﬁts the data much better than the
multilevel models without modeling serial correlation (the log10 BF of the GLMM-AR(2)
versus the GLMM-AR(0) is 15.33 and that of the GLMM-AR(2) versus the GLMM-CL1 is
18.48). The PROBIT model has poor model quality, and the log10 BF of it versus the best
model is as small as 250.45.
Figure 6 compares the posteriors of the random-effect parameters of the GLMM-AR(2)
and the GLMM-AR(0). The graphs show that correcting serial correlation makes
Random effects, country-level residuals, and random intercepts.
Note. The vertical lines in the graphs are 90% credible intervals of the posteriors. The zero horizontal
line is a reference line, and the nonzero horizontal lines in the ﬁrst-row graphs is the overall mean of
the country-speciﬁc coefﬁcient associated to trade openness. The other three random parameters are
residuals that are mean 0 by deﬁnition, a fact that is conﬁrmed by the estimates.
 Published online by Cambridge University Press
a difference in estimating the random-effect coefﬁcients and group-level residuals. Heterogeneity is detected in both the time and the spatial dimensions. The effect of trade openness varies from country to country, a fact that can substantively be attributed to the
different structure of international trade in different countries. An open economy implies
that a country is engaged in the international community, which has been suggested to be
the mechanism through which trade openness reduces state failure risk . However, if the major products are primary goods, this
trade structure may be highly correlated with high domestic instability. This is because
primary commodity exports may imply high dependency on natural resources, a weak government, and proﬁtable rebellions for lootable resources, all of which lead to high risk of
state failure . In the
GLMM-AR(2) model, there are only three countries (Gabon, Gambia, and Malawi) in
which trade openness decreases state failure risk at a 90% credibility level. For other countries, an open market has no clear effect on state failure. The country-speciﬁc random
intercept shows relative homogeneity—there is not much unobserved heterogeneity after
the country-level errors are modeled (as shown in graphs in the second row of Fig. 6).
However, the year-speciﬁc intercept demonstrates noticeable heterogeneity between time
periods. Compared to the random effects in the GLMM-AR(2), the posteriors of the
GLMM-AR(0) have smaller error bands and consequently put more conﬁdence on
the effect of trade openness—at a 90% credibility level, trade openness has a negative
effect on state failure in 10 instead of the 3 countries in the GLMM-AR(2).
One of the main goals of the PITF studies is to build a forecasting system based on
statistical models. Prediction is another means of assessing model quality. Here, I also
compare these models by assessing their within-sample prediction performance. To avoid
setting arbitrary or post hoc thresholds for classiﬁcation, I simply report the numeric predictive probabilities of state failure of all the sample country-years. In this way, we can
compare the predictive failure probabilities of the observed failure group and the observed
nonfailure group. As shown in the upper graphs in Fig. 7, the PROBIT performs poorly in
terms of distinguishing failure cases from nonfailure ones, and the density kernels of the
two groups largely overlap. The GLMM-CL1 separates the two densities better than the
pooled model, but the two kernels are still poorly separated. The GLMM-AR(0) considers
heterogeneity both across units and between time periods, and it further reduces the overlapping area. By considering the error dynamics and using the information ignored by the
former models, the GLMM-AR(2) distinguishes the two groups much more accurately.
In the graph, the two density kernels are well separated—only their tails are slightly
connected.
An alternative way to evaluate and compare the predicting performance of competing
models is the receiver operating characteristic (ROC) curve, used by King and Zeng
 . The idea of the ROC curve is simple: at a given rate of correct classiﬁcation
of one group (say, the failure group), a model performs better than others if it has the highest rate of correct classiﬁcation of the other group (say, the nonfailure group). Graphically,
the dominating curve in an ROC graph represents the best model. This approach has the
advantage of not using arbitrary cutoff values. In Fig. 7, the diagonal line is a reference line,
indicating the extreme situation that the densities of the two groups completely overlap. In
the ﬁgure, the ROC curve of the pooled probit model has the lowest location, indicating the
worst prediction. The GLMM-CL1 model improves forecasting by modeling heterogeneity
across countries, and its curve globally dominates that of the pooled probit model. The
GLMM-AR(0) model has an ROC curve globally—but only slightly—above that of
the GLMM-CL1, but it is dominated everywhere by the curve of the GLMM-AR(2) model.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
The ROC curve of the GLMM-AR(2) model is almost a horizontal line, showing that there
is barely a trade-off between the two types of classiﬁcation.
Civil War Duration in Sub-Saharan Africa
Political scientists are fully aware of the nonstationarity problem in time-series analysis , but this concern has not been commonly extended to TSCS studies. In this
example, I use the civil war duration example to demonstrate that the GLMM-AR(p) model
can serve as a cointegration test to help detect spurious regressions. The model analyzes the
residual (error) process with the restriction that the process be stationary. Whenever this
restriction is violated, the MCMC simulation will show abnormal symptoms: the Markov
chain is halted because of the difﬁculty of sampling legitimate draws for the autoregressive
coefﬁcients, or the MCMC samples of the autoregressive coefﬁcients are highly concentrated around the edge of the unit circle. In either situation, we should change the model
speciﬁcation to achieve cointegration by altering variable selection, transforming the data
(for instance, by differencing), or changing other aspects of model speciﬁcation.
The civil war literature includes studies on war onset, war duration, and war termination
 . In the literature, some argue that civil war onset and duration are
Comparison of within-sample predictive probabilities.
Note. In the graphs of predictive probabilities, the gray lines are distributions of the predictive failure
probability of the observed nonfailure group and the black lines are those of the observed failure
group. In the ROC graph, the 45 line is a reference line, and the ROC curves plot the rate of correct
classiﬁcation of one group with a given rate of correct classiﬁcation the other.
 Published online by Cambridge University Press
different phenomena that require different theories to explain , others, such as Miguel, Satyanath, and Sergenti ,
suggest that the theories applied to civil war onset are also relevant to civil war duration.
Most empirical civil war data are TSCS. Popular models applied in the extant empirical
civil war studies include the pooled model and the FE and RE estimators, none of which
consider the cointegration issue. This paper is not intended to solve the theoretical debate.
Instead, the GLMM-AR(p) model is applied here to show that TSCS analysis of civil war
duration without cointegration testing can produce unreliable inferences, which might shed
light on the debate. The response process, civil war duration, may be unstationary, but
many of the explanatory variables have little temporal variation (as shown in Table 3).
This highlights the necessity of cointegration testing and justiﬁes the concern with spurious
regressions when applying the FE, RE, or pooled probit model.
Miguel, Satyanath, and Sergenti use rainfall as an instrumental variable for economic growth for the purpose of controlling for endogeneity. However, this is a weak
instrumental variable, and it is also difﬁcult to argue that rainfall affects civil war duration
only through economic growth (in other words, it is uncorrelated with the error term).
Furthermore, if economic growth time series cannot help achieve cointegration, it is unlikely that the rainfall data can. For these reasons, I use the lagged economic growth rate
directly. Miguel, Satyanath, and Sergenti also tested whether the effect of economic growth varies across countries. I specify the GLMM-AR(p) model with both country- and year-speciﬁc random intercepts and a random-effect coefﬁcient for the variable of
economic growth. The deﬁnitions and within-group variations of all the variables are summarized in Table 3. I do not include two often-selected variables in civil war onset
studies—a dummy variable for being a noncontiguous state and an indicator for being
new state—because they have little overall variation across the sample country-years.
How the effect of economic growth varies across countries is further explained with
Within-group data variation: civil war duration example
Within-country variation
Within-year variation
Ongoing war
Income per capita
(log)t 2 1
Population (log)t 2 1
Mountainous
mountainous
Ethnolinguistic
fractionalization
fractionalization
Oil-exporting country
Political instability
Anocracyt 2 1
Democracyt 21
GDP growth ratet 2 1
Note. The table presents the data variation in the time and spatial dimensions. In the within-county (within-year)
variation column is the summary of how the variables vary within each country (year). If a variable is invariant or
slowly moving (with any 0 or close to 0 value in the row of a variable), it will cause identiﬁcation and estimation
problems for the FE estimator. GDP, gross domestic product.
Modeling Heterogeneity and Serial Correlation in Binary TSCS Data
 Published online by Cambridge University Press
the same ﬁve variables as in Miguel, Satyanath, and Sergenti , including how mountainous a country is, whether the state is an oil-exporting country, economic development
level, male secondary school enrollment, and democracy. There are 743 observations
(country-years) in total among which are 182 ongoing wars.
With a similar prior assignment as in the state failure study, I estimated six models with
or without considering serial correlation. Figure 8 summarizes the MCMC outputs of the
competing models. The GLMM-AR(0) model controls for heterogeneities in both time and
spatial dimensions and produces different estimates from those of the pooled probit model.
Compared to the probit model, the GLMM-AR(0) has much larger error bands for most
parameters and detects fewer statistically important variables. Nonetheless, both models
suggest that theories applied to civil war onset are relevant to civil war duration—the reliability and directions of the effects of most covariates are similar to those in civil war
onset models. 17 Then I estimated GLMM-AR(p) models with p 5 1, 2, 3, 4. All the simulations were abnormally halted because of the difﬁculty of drawing legitimate samples of
the autoregressive coefﬁcients. The left graph in Fig. 8 summarizes the samples of the
autoregressive coefﬁcients before the MCMC simulations were halted. The draws in all
the GLMM-AR models are close to the edge of the unit circle, showing a signiﬁcant tendency toward going out of the stationarity space. In the middle and right graphs of Fig. 8 are
the summaries of the MCMC draws of the GLMM-AR(1) and -AR(2) models. Note that the
outputs cannot tell us anything about the parameters because they are not sampled from the
target distributions. I use them to demonstrate how different the empirical results could be
when we seriously consider the residual dynamics and the noncointegration problem.
The evidence found in the MCMC simulation process suggests that the explanatory
variables are unlikely to be cointegrated with civil war duration, and statistical inferences
Summary of MCMC outputs of competing models.
Note. Log marginal likelihood: PROBIT 5 2399.514 and GLMM-AR(0) 5 2327.632. For the
GLMM-AR(p) models with p 5 1, 2, 3, 4, because the MCMC processes were abnormally halted, the
Markov chains did not converge and no marginal likelihood was calculated.
17For the detailed empirical ﬁndings in civil war onset studies, refer to the quantitative studies on civil war onset,
such as Fearon and Laitin , Cederman and Girardin , and Fearon, Kasara, and Laitin .
 Published online by Cambridge University Press
based on the pooled probit, or GLMM-AR(0) model are spurious and misleading. The
solution could be adding a lagged response variable as a regressor in addition to the autoregressive errors or ﬁnding other time-varying explanatory variables to achieve cointegration. This example shows that the GLMM-AR(p) model can be applied for the purpose
of cointegration testing and avoiding spurious regressions in discrete TSCS analysis.
Conclusions
In TSCS analysis, modeling heterogeneity in both the time and the spatial dimensions
while adequately correcting serial correlation is required by the generic characteristics
of the TSCS data structure. However, for categorical responses, this is especially challenging because of nonlinearity and the complex error structure. Moreover, because TSCS
models analyze multiple time series, cointegration testing is necessary and important. This
paper proposes the GLMM-AR(p) model as a comprehensive solution and develops an
MCMC algorithm for model estimation. The error-orthogonalizing method facilitates
the construction of the MCMC algorithm and achieves the goal of conducting data augmentation in a single block. It also simpliﬁes BF computation, making Bayesian model
comparison easy and convenient. The application of the PGM-MGMC method further improves simulation efﬁciency. An R package GLMMarp is available on CRAN for estimating various speciﬁcations within the GLMM-AR(p) framework.18 A possible future
extension of the model could be to relax the assumption that time-speciﬁc common shocks
have the same impact on all units. This extended model may be a multifactor residual
model with a hierarchical component and serially correlated errors.
National Science Foundation (SES-0918320).