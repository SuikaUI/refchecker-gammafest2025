Published as a conference paper at ICLR 2019
IMPROVING THE GENERALIZATION OF ADVERSARIAL
TRAINING WITH DOMAIN ADAPTATION
Chuanbiao Song
Department of Computer Science
Huazhong University of Science and Technology
Wuhan 430074, China
 
Department of Computer Science
Huazhong University of Science and Technology
Wuhan 430074, China
 
Liwei Wang
Department of Machine Intelligence
Peking University
 
John E. Hopcroft
Department of Computer Science
Cornell University
Ithaca 14850, NY, USA
 
By injecting adversarial examples into training data, adversarial training is
promising for improving the robustness of deep learning models. However, most
existing adversarial training approaches are based on a speciﬁc type of adversarial
attack. It may not provide sufﬁciently representative samples from the adversarial domain, leading to a weak generalization ability on adversarial examples from
other attacks. Moreover, during the adversarial training, adversarial perturbations
on inputs are usually crafted by fast single-step adversaries so as to scale to large
datasets. This work is mainly focused on the adversarial training yet efﬁcient
FGSM adversary. In this scenario, it is difﬁcult to train a model with great generalization due to the lack of representative adversarial samples, aka the samples are
unable to accurately reﬂect the adversarial domain. To alleviate this problem, we
propose a novel Adversarial Training with Domain Adaptation (ATDA) method.
Our intuition is to regard the adversarial training on FGSM adversary as a domain adaption task with limited number of target domain samples. The main idea
is to learn a representation that is semantically meaningful and domain invariant
on the clean domain as well as the adversarial domain. Empirical evaluations on
Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can
greatly improve the generalization of adversarial training and the smoothness of
the learned models, and outperforms state-of-the-art methods on standard benchmark datasets. To show the transfer ability of our method, we also extend ATDA
to the adversarial training on iterative attacks such as PGD-Adversial Training
(PAT) and the defense performance is improved considerably.
INTRODUCTION
Deep learning techniques have shown impressive performance on image classiﬁcation and many
other computer vision tasks. However, recent works have revealed that deep learning models are
often vulnerable to adversarial examples , which are maliciously designed to deceive the target model by generating carefully crafted adversarial perturbations on original clean inputs. Moreover, adversarial examples can transfer across
models to mislead other models with a high probability .
How to effectively defense against adversarial attacks is crucial for security-critical computer vision
systems, such as autonomous driving.
As a promising approach, adversarial training defends from adversarial perturbations by training
a target classiﬁer with adversarial examples. Researchers have found that adversarial training could increase the robustness of neural networks. However, adversarial training often obtains adversarial examples by taking a speciﬁc attack
technique (e.g., FGSM) into consideration, so the defense targeted such attack and the trained model
exhibits weak generalization ability on adversarial examples from other adversaries . Tram`er et al. showed that the robustness of adversarial training can be easily circumvented by the attack that combines with random perturbation from other models. Accordingly,
for most existing adversarial training methods, there is a risk of overﬁtting to adversarial examples
crafted on the original model with the speciﬁc attack.
In this paper, we propose a novel adversarial training method that is able to improve the generalization of adversarial training. From the perspective of domain adaptation (DA) , there is a big domain gap between the distribution of clean examples and the distribution of
adversarial examples in the high-level representation space, even though adversarial perturbations
are imperceptible to humans. Liao et al. showed that adversarial perturbations are progressively ampliﬁed along the layer hierarchy of neural networks, which maximizes the distance between
the original and adversarial subspace representations. In addition, adversarial training simply injects
adversarial examples from a speciﬁc attack into the training set, but there is still a large sample space
for adversarial examples. Accordingly, training with the classiﬁcation loss on such a training set will
probably lead to overﬁtting on the adversarial examples from the speciﬁc attack. Even though Wong
& Kolter showed that adversarial training with iterative noisy attacks has stronger robustness
than the adversarial training with single-step attacks, iterative attacks have a large computational
cost and there is no theoretical analysis to justify that the adversarial examples sampled in such way
could be sufﬁciently representative for the adversarial domain.
Our contributions are focused on how to improve the generalization of adversarial training on the
simple yet scalable attacks, such as FGSM (Goodfellow et al.). The key idea of our approach is
to formulate the learning procedure as a domain adaptation problem with limited number of target
domain samples, where target domain denotes adversarial domain. Speciﬁcally, we introduce unsupervised as well as supervised domain adaptation into adversarial training to minimize the gap and
increase the similarity between the distributions of clean examples and adversarial examples. In this
way, the learned models generalize well on adversarial examples from different ℓ∞bounded attacks.
We evaluate our ATDA method on standard benchmark datasets. Empirical results show that despite
a small decay of accuracy on clean data, ATDA signiﬁcantly improves the generalization ability of
adversarial training and has the transfer ability to extend to adversarial training on PGD .
BACKGROUND AND RELATED WORK
In this section, we introduce some notations and provides a brief overview of the current advanced
attack methods, as well as the defense methods based on adversarial training.
Denote the clean data domain and the adversarial data domain by D and A respectively, we consider
a classiﬁer based on a neural network f(x) : Rd →Rk. f(x) outputs the probability distribution
for an input x ∈ d, and k denotes the number of classes in the classiﬁcation task. Let ϕ
be the mapping at the logits layer (the last neural layer before the ﬁnal softmax function), so that
f(x) = softmax(ϕ(x)). Let ϵ be the magnitude of the perturbation. Let xadv be the adversarial
image computed by perturbing the original image x. The cost function of image classiﬁcation is
denoted as J(x, y). We deﬁne the logits as the logits layer representation, and deﬁne the logit space
as the semantic space of the logits layer representation.
We divide attacks into two types: white-box attacks have the complete knowledge of the target model
and can fully access the model; black-box attacks have limited knowledge of the target classiﬁer
(e.g.,its architecture) but can not access the model weights.
ATTACK METHODS
We consider four attack methods to generate adversarial examples. For all attacks, the components
of adversarial examples are clipped in .
Published as a conference paper at ICLR 2019
Fast Gradient Sign Method (FGSM).
Goodfellow et al. introduced FGSM to generate adversarial examples by applying perturbations in the direction of the gradient.
xadv = x + ϵ · sign(∇xJ(x, ytrue))
As compared with other attack methods, FGSM is a simple, yet fast and efﬁcient adversary. Accordingly, FGSM is particularly amenable to adversarial training.
Projected Gradient Descent (PGD).
The Projected Gradient Descent (PGD) adversary was introduced by Madry et al. without random start, which is a stronger iterative variant of FGSM.
This method applies FGSM iteratively for k times with a budget α instead of a single step.
xadvt+1 = xadvt + α · sign(∇xJ(xadvt, ytrue))
xadvt+1 = clip(xadvt+1, xadvt+1 −ϵ, xadvt+1 + ϵ)
xadv = xadvk
Here clip(·, a, b) function forces its input to reside in the range of [a, b]. PGD usually yields a higher
success rate than FGSM does in the white-box setting but shows weaker capability in the black-box
RAND+FGSM (R+FGSM).
Tram`er et al. proposed R+FGSM against adversarially
trained models by applying a small random perturbation of step size α before applying FGSM.
x′ = x + α · sign(N(0d, Id))
xadv = x′ + (ϵ −α) · sign(∇xJ(xadv, ytrue))
Momentum Iterative Method (MIM).
MIM is a modiﬁcation of the iterative
FGSM and it won the ﬁrst place of NIPS 2017 Adversarial Attacks Competition. Its basic idea is to
utilize the gradients of the previous t steps with a decay factor µ to update the gradient at step t + 1
before applying FGSM with a budget α.
xadv0 = x, g0 = 0
gt+1 = µ · gt +
∇xJ(xadvt, ytrue)
∥∇xJ(xadvt, ytrue)∥1
xadvt+1 = xadvt + α · sign(gt+1)
xadvt+1 = clip(xadvt+1, xadvt+1 −ϵ, xadvt+1 + ϵ)
xadv = xadvk
PROGRESS ON ADVERSARIAL TRAINING
An intuitive technique to defend a deep model against adversarial examples is adversarial training,
which injects adversarial examples into the training data during the training process. First, Goodfellow et al. proposed to increase the robustness by feeding the model with both original and adversarial
examples generated by FGSM and by learning with the modiﬁed objective function.
ˆJ(x, ytrue) = αJ(x, ytrue) + (1 −α)J(x + ϵ sign(∇xJ(x, ytrue), ytrue)
Kurakin et al. scaled the adversarial training to ImageNet and
showed better results by replacing half the clean example at each batch with the corresponding
adversarial examples. Meanwhile, Kurakin et al. discovered the label leaking effect and
suggested not to use the FGSM deﬁned with respect to the true label ytrue. However, their approach
has weak robustness to the RAND+FGSM adversary. Tram`er et al. proposed an ensemble
adversarial training to improve robustness on black-box attacks by injecting adversarial examples
transferred from a number of ﬁxed pre-trained models into the training data.
For adversarial training, another approach is to train only with adversarial examples. Nøkland 
proposed a specialization of the method (Goodfellow et al.) that learned only with the objective
function of adversarial examples. Madry et al. demonstrated successful defenses based on
adversarial training with the noisy PGD, which randomly initialize an adversarial example within
the allowed norm ball before running iterative attack. However, this technique is difﬁcult to scale to
large-scale neural networks as the iterative attack increases the training time
Published as a conference paper at ICLR 2019
by a factor that is roughly equal to the number of iterative steps. Wong & Kolter developed
a robust training method by linear programming that minimized the loss for the worst case within
the perturbation ball around each clean data point. However, their approach achieved high test error
on clean data and it is still challenging to scale to deep or wide neural networks.
As described above, though adversarial training is promising, it is difﬁcult to select a representative
adversary to train on and most existing methods are weak in generalization for various adversaries,
as the region of the adversarial examples for each clean data is large and contiguous . Furthermore, generating a representative set of adversarial examples
for large-scale datasets is computationally expensive.
ADVERSARIAL TRAINING WITH DOMAIN ADAPTATION
In this work, instead of focusing on a better sampling strategy to obtain representative adversarial
data from the adversarial domain, we are especially concerned with the problem of how to train
with clean data and adversarial examples from the efﬁcient FGSM, so that the adversarially trained
model is strong in generalization for different adversaries and has a low computational cost during
the training.
We propose an Adversarial Training with Domain Adaptation (ATDA) method to defense adversarial
attacks and expect the learned models generalize well for various adversarial examples. Our motivation is to treat the adversarial training on FGSM as a domain adaptation task with limited number of
target domain samples, where the target domain denotes adversarial domain. We combine standard
adversarial training with the domain adaptor, which minimizes the domain gap between clean examples and adversarial examples. In this way, our adversarially trained model is effective on adversarial
examples crafted by FGSM but also shows great generalization on other adversaries.
DOMAIN ADAPTATION ON LOGIT SPACE
UNSUPERVISED DOMAIN ADAPTATION
Suppose we are given some clean training examples {xi} (xi ∈Rd) with labels {yi} from the clean
data domain D, and adversarial examples {xadv
∈Rd) from adversarial data domain A. The
adversarial examples are obtained by sampling (xi, ytrue) from D, computing small perturbations
on xi to generate adversarial perturbations, and outputting (xadv
It’s known that there is a huge shift in the distributions of clean data and adversarial data in the
high-level representation space. Assume that in the logit space, data from either the clean domain
or the adversarial domain follow a multivariate normal distribution, i.e., D ∼N(µD, ΣD), A ∼
N(µA, ΣA). Our goal is to learn the logits representation that minimizes the shift by aligning the
covariance matrices and the mean vectors of the clean distribution and the adversarial distribution.
To implement the CORrelation ALignment (CORAL), we deﬁne a covariance distance between the
clean data and the adversarial data as follows.
LCORAL(D, A) = 1
Cϕ(D) −Cϕ(A)
where Cϕ(D) and Cϕ(A) are the covariance matrices of the clean data and the adversarial data in the
logit space respectively, and ∥· ∥ℓ1 denotes the L1 norm of a matrix. Note that LCORAL(D, A) is
slightly different from the CORAL loss proposed by Sun & Saenko .
Similarly, we use the standard distribution distance metric, Maximum Mean Discrepancy
(MMD) , to minimize the distance of the mean vectors of the clean data
and the adversarial data.
LMMD(D, A) = 1
The loss function for Unsupervised Domain Adaptation (UDA) can be calculated as follows.
LUDA(D, A) = LCORAL(D, A) + LMMD(D, A)
Published as a conference paper at ICLR 2019
SUPERVISED DOMAIN ADAPTATION
Even though the unsupervised domain adaptation achieves perfect confusion alignment, there is no
guarantee that samples of the same label from clean domain and adversarial domain would map
nearby in the logit space. To effectively utilize the labeled data in the adversarial domain, we introduce a supervised domain adaptation (SDA) by proposing a new loss function, denoted as margin
loss, to minimize the intra-class variations and maximize the inter-class variations on samples of
different domains. The SDA loss is shown in Eq. (9).
LSDA(D, A) =Lmargin(D, A)
(k −1)(|D| + |A|)·
cn∈C\{cytrue}
softplus(∥ϕ(x) −cytrue∥1 −∥ϕ(x) −cn∥1)
Here softplus denotes a function ln(1 + exp(·)); cytrue ∈Rk denotes the center of ytrue class in
the logit space; C = { cj | j = 1, 2, ..., k} is a set consisting of the logits center for each class,
which will be updated as the logits changed. Similar to the center loss , we update
center cj for each class j:
x∈D∪A 1ytrue=j · that avoids
the label leaking effect to generate a new adversarial example xadv
for each clean example xi.
= xi + ϵ · sign(∇xJ(xi, ytarget))
where ytarget denotes the predicted class arg max{ϕ(xi)} of the model.
However, in this case, the sampled adversarial examples are aggressive but not sufﬁciently representative due to the fact that the sampled adversarial examples always lie at the boundary of the ℓ∞
ball of radius ϵ (see Figure 1) and the adversarial examples within the boundary are ignored. For
adversarial training, if we train a deep neural network only on the clean data and the adversarial data
from the FGSM attack, the adversarially trained model will overﬁt on these two kinds of data and
exhibits weak generalization ability on the adversarial examples sampled from other attacks. From
a different perspective, such problem can be viewed as a domain adaptation problem with limited
number of labeled target domain samples, as only some special data point can be sampled in the
adversarial domain by FGSM adversary.
Figure 1: Illustration of the adversarial sampling by FGSM for xi ∈R2. The blue dot (in the center)
represents a clean example and the red dots (along the boundary) represent the potential adversarial
examples for the clean example.
Consequently, it is natural to combine the adversarial training with domain adaptation to improve
the generalization ability on adversarial data. We generate new adversarial examples by the variant
of FGSM attack shown in Eq. (11), then we use the following loss function to meet the criteria of
domain adaptation while training a strong classiﬁer.
Published as a conference paper at ICLR 2019
L(D, A) = LC(D) + LC(A) + λ · LDA(D, A)
= LC(D) + LC(A) + λ · (LUDA(D, A) + LSDA(D, A))
LC(x|ytrue) + 1
LC(xadv|ytrue)
+ λ · (LCORAL(D, A) + LMMD(D, A) + Lmargin(D, A))
Here λ is the hyper-parameter to balance the regularization term; m is the number of input clean
examples; D indicates the input clean examples {xi}, and A the corresponding adversarial examples
}; LC denotes the classiﬁcation loss. The training process is summarized in Algorithm 1.
Algorithm 1 Adversarial training with domain adaptation on network f(x) : Rd →Rk.
Parameters: Size of the training minibatch is m.
1: Randomly initialize network f(x) and logits centers {cj | j = 1, 2, ..., k};
2: Number of iterations t ←0;
Read a minibatch of data Db = {x1, ..., xm} from the training set;
Use the current state of network f to generate adversarial examples Ab = {xadv
, ..., xadv
by the FGSM variant that avoids label leaking;
Extract logits for examples Db, Ab by performing forward-backward propagation from the
input layer to the logits layer ϕ(x);
Update parameters cj for each class j by ct+1
j −α · ∆ct
Compute the loss by Eq. (12) and update parameters of network f by back propagation;
10: until the training converges.
EXPERIMENTS
In this section, we evaluate our ATDA method on various benchmark datasets to demonstrate the
robustness and contrast its performance against other competing methods under different white-box
and black-box attacks with bounded ℓ∞norm. Code for these experiments is available at https:
//github.com/JHL-HUST/ATDA.
EXPERIMENTAL SETUP
We consider four popular datasets, namely Fashion-MNIST ,
SVHN , CIFAR-10 and CIFAR-100 . For all
experiments, we normalize the pixel values to by dividing 255.
Baselines.
To evaluate the generalization power on adversarial examples in both the white-box
and black-box settings, we report the clean test accuracy, the defense accuracy on FGSM, PGD,
R+FGSM and MIM in the non-targeted way. The common settings for these attacks are shown in
Table 5 of the Appendix. We compare our ATDA method with normal training as well as several
state-of-the-art adversarial training methods:
• Normal Training (NT). Training with cross-entropy loss on the clean training data.
• Standard Adversarial Training (SAT) (Goodfellow et al.). Training with the cross-entropy
on the clean training data and the adversarial examples from the FGSM variant with perturbation ϵ to avoid label leaking.
• Ensemble Adversarial Training (EAT) . Training with cross-entropy
on the clean training data and the adversarial examples crafted from the currently trained
model and the static pre-trained models by the FGSM variant with the perturbation ϵ to
avoid label leaking.
• Provably Robust Training (PRT) . Training with cross-entropy loss
on the worst case in the ℓ∞ball of radius ϵ around each clean training data point. It could
be seen as training with a complicated method of sampling in the ℓ∞ball of radius ϵ.
Evaluation Setup.
For each benchmark dataset, we train a normal model and various adversarial models with perturbation ϵ on a main model with ConvNet architecture, and evaluate them on
Published as a conference paper at ICLR 2019
various attacks bounded by ϵ. Moreover, for Ensemble Adversarial Training (EAT), we use two
different models as the static pre-trained models. For black-box attacks, we test trained models on
the adversarial examples transferred from a model held out during the training. All experiments are
implemented on a single Titan X GPU. For all experiments, we set the hyper-parameter λ in Eq.
(12) to 1/3 and the hyper-parameter α in Eq. (10) to 0.1. For more details about neural network
architectures and training hyper-parameters, see Appendix A. We tune the networks to make sure
they work, not to post concentrates on optimizing these settings.
COMPARISON OF DEFENSE PERFORMANCE ON ACCURACY
We evaluate the defense performance of our ATDA method from the perspective of classiﬁcation
accuracy on various datasets, and compare with the baselines.
Evaluation on Fashion-MNIST. The accuracy results on Fashion-MNIST are reported in Table 1a.
NT yields the best performance on the clean data, but generalizes poorly on adversarial examples.
SAT and EAT overﬁt on the clean data and the adversarial data from FGSM. PRT achieves lower
error against various adversaries, but higher error on the clean data. ATDA achieves stronger robustness against different ℓ∞bounded adversaries as compared to SAT (adversarial training on FGSM).
Evaluation on SVHN.
The classiﬁcation accuracy on SVHN are summarized in Table 1b. PRT
seems to degrade the performance on the clean testing data and exhibits weak robustness on various
attacks. As compared to SAT, ATDA achieves stronger generalization ability on adversarial examples from various attacks and higher accuracy on the white-box adversaries, at the same time it only
loses a negligible performance on clean data.
Evaluation on CIFAR-10. Compared with Fashion-MNIST and SVHN, CIFAR-10 is a more difﬁcult dataset for classiﬁcation. As PRT is challenging and expensive to scale to large neural networks
due to its complexity, the results of PRT are not reported. The accuracy results on CIFAR-10 are
summarized in Table 1c. ATDA outperforms all the competing methods on most adversaries, despite
a slightly lower performance on clean data.
Evaluation on CIFAR-100.
The CIFAR-100 dataset contains 100 image classes, with 600 images
per class. Our goal here is not to achieve state-of-the-art performance on CIFAR-100, but to compare
the generalization ability of different training methods on a comparatively large dataset. The results
on CIFAR-100 are summarized in Table 1d. Compared to SAT, ATDA achieves better generalization
on various adversarial examples and it does not degrade the performance on clean data.
In conclusion, the accuracy results provide empirical evidence that ATDA has great generalization
ability on different adversaries as compared to SAT and outperforms other competing methods.
FURTHER ANALYSIS ON THE DEFENSE PERFORMANCE
To further investigate the defence performance of the proposed method, we compute two other
metrics: the local loss sensitivity to perturbations and the shift of adversarial data distribution with
respect to the clean data distribution.
Local Loss Sensitivity.
One method to quantify smoothness and generalization to perturbations
for models is the local loss sensitivity . It is calculated in the clean testing data as
follows. The lower the value is, the smoother the loss function is.
∥∇xJ(xi, yi)∥2
The results of the local loss sensitivity for the aforementioned learned models are summarized in
Table 2. The results suggest that adversarial training methods do increase the smoothness of the
model as compared with the normal training and ATDA performs the best.
Distribution Discrepancy.
To quantify the dissimilarity of the distributions between the clean
data and the adversarial data, we compare our learned logits embeddings with the logits embeddings
of the competing methods on Fashion-MNIST. We use t-SNE for the
comparison on the training data, testing data and adversarial testing data from the white-box FGSM
or PGD. The comparisons are illustrated in Figure 2 and we report the detailed MMD distances
across domains in Table 3. Compared with NT, SAT and EAT actually increase the MMD distance
Published as a conference paper at ICLR 2019
across domains of the clean data and the adversarial data. In contrast, PRT and ATDA can learn
domain invariance between the clean domain and the adversarial domain. Furthermore, our learned
logits representation achieves the best performance on domain invariance.
Table 1: The accuracy of defense methods on the testing datasets and the adversarial examples
generated by various adversaries.
(a) On Fashion-MNIST. The magnitude of perturbations is 0.1 in ℓ∞norm.
White-Box Attack (%)
Black-Box Attack (%)
(b) On SVHN. The magnitude of perturbations is 0.02 in ℓ∞norm.
White-Box Attack (%)
Black-Box Attack (%)
(c) On CIFAR-10. The magnitude of perturbations is 4/255 in ℓ∞norm.
White-Box Attack (%)
Black-Box Attack (%)
(d) On CIFAR-100. The magnitude of perturbations is 4/255 in ℓ∞norm.
White-Box Attack (%)
Black-Box Attack (%)
Table 2: The local loss sensitivity analysis for defense methods.
Local loss sensitivity
Fashion-MNIST
ABLATION STUDIES ON ATDA
To individually dissect the effectiveness of different components in ATDA (Standard Adversarial
Training (SAT), Unsupervised Domain Adaptation (UDA), and Supervised Domain Adaptation
(SDA)), we conduct a series of ablation experiments in Figure 3. For each model, we report the
Published as a conference paper at ICLR 2019
Table 3: The MMD distance across domains in the logit space for defense methods on Fashion-
MNIST. D denotes the distribution of the clean testing data; AF GSM and AP GD denote the distributions of the adversarial testing data generated by the white-box FGSM and PGD, respectively.
MMD Distance
Defense Method
MMD(D, AF GSM)
MMD(D, AP GD)
(a) Training data
(b) Testing data
adversarial
(d) PGD adversarial data
Figure 2: t-SNE visualizations for the embeddings of training data, testing data, and adversarial
testing data from FGSM and PGD in the logit space for Fashion-MNIST. The ﬁrst row to the ﬁfth
row correspond to NT, SAT, EAT, PRT and ATDA, respectively.
average accuracy rates over all white-box attacks and all black-box attacks, respectively. The results
illustrate that, by aligning the covariance matrix and mean vector of the clean and adversarial examples, UDA plays a key role in improving the generalization of SAT on various attacks. In general,
the aware of margin loss on SDA can also improve the defense quality on standard adversarial training, but the effectiveness is not very stable over all datasets. By combining UDA and SDA together
with SAT, our ﬁnal algorithm ATDA can exhibits stable improvements on the standard adversarial
training. In general, the performance of ATDA is slightly better than SAT+UDA.
Published as a conference paper at ICLR 2019
(a) Fashion-MNIST
(c) CIFAR-10
(d) CIFAR-100
Figure 3: Ablation experiments for ATDA to investigate the impact of Standard Adversarial Training
(SAT), Unsupervised Domain Adaptation (UDA), and Supervised Domain Adaptation (SDA). We
report the average accuracy rates over all white-box attacks and all black-box attacks, respectively.
EXTENSION TO PGD-ADVERSARIAL TRAINING
ATDA can simply be extended to adversarial training on other adversaries. We now consider to
extend the ATDA method to PGD-Adversarial Training (PAT) : adversarial
training on the noisy PGD with perturbation ϵ. By combining adversarial training on the noisy PGD
with domain adaptation, we implement an extension of ATDA for PAT, called PATDA. For the noisy
PGD, we set the iterated step k as 10 and the budget α as ϵ/4 according to Madry et al. .
As shown in Table 4, we evaluate the defense performance of PAT and PATDA on various datasets.
On Fashion-MNIST, we observe that PATDA fails to increase robustness to most adversaries as
compared to PAT. On SVHN, PAT and PATDA fail to converge properly. The results are not surprising, as training with the hard and sufﬁcient adversarial examples (from the noisy PGD) requires the
neural networks with more parameters. On CIFAR-10 and CIFAR-100, PATDA achieves stronger
robustness to various attacks than PAT. In general, PATDA exhibits stronger robustness to various
adversaries as compared to PAT. The results indicate that domain adaptation can be applied ﬂexibly
to adversarial training on other adversaries to improve the defense performance.
Table 4: The accuracy of PAT and PATDA on the testing datasets and the adversarial examples
generated by various adversaries. The magnitude of perturbations in ℓ∞norm is 0.1 for Fashion-
MNIST, 0.02 for SVHN, and 4/255 for CIFAR-10 and CIFAR-100.
White-Box Attack (%)
Black-Box Attack (%)
Fashion-MNIST
CONCLUSION
In this study, we regard the adversarial training as a domain adaptation task with limited number
of target labeled data. By combining adversarial training on FGSM adversary with unsupervised
and supervised domain adaptation, the generalization ability on adversarial examples from various
attacks and the smoothness on the learned models can be highly improved for robust defense. In
addition, ATDA can easily be extended to adversarial training on iterative attacks (e.g., PGD) to
improve the defense performance. The experimental results on several benchmark datasets suggest
that the proposed ATDA and its extension PATDA achieve signiﬁcantly better generalization results
as compared with current competing adversarial training methods.
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation (61772219).
Published as a conference paper at ICLR 2019