Learning Convolutional Networks for Content-weighted Image Compression
Hong Kong Polytechnic University
 
Wangmeng Zuo∗
Harbin Institute of Technology
 
Shuhang Gu
Hong Kong Polytechnic University
 
Debin Zhao
Harbin Institute of Technology
 
David Zhang
Hong Kong Polytechnic University
 
Lossy image compression is generally formulated as a
joint rate-distortion optimization to learn encoder, quantizer, and decoder.
However, the quantizer is nondifferentiable, and discrete entropy estimation usually is required for rate control. These make it very challenging to
develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we
suggest that the bit rate of the different parts of the image should be adapted to local content. And the content
aware bit rate is allocated under the guidance of a contentweighted importance map. Thus, the sum of the importance
map can serve as a continuous alternative of discrete entropy estimation to control compression rate.
And binarizer is adopted to quantize the output of encoder due to
the binarization scheme is also directly deﬁned by the importance map. Furthermore, a proxy function is introduced
for binary operation in backward propagation to make it
differentiable. Therefore, the encoder, decoder, binarizer
and importance map can be jointly optimized in an end-toend manner by using a subset of the ImageNet database.
In low bit rate image compression, experiments show that
our system signiﬁcantly outperforms JPEG and JPEG 2000
by structural similarity (SSIM) index, and can produce the
much better visual result with sharp edges, rich textures,
and fewer artifacts.
∗Corresponding Author
1. Introduction
Image compression is a fundamental problem in computer vision and image processing. With the development
and popularity of high-quality multimedia content, lossy
image compression has been becoming more and more essential in saving transmission bandwidth and hardware storage. An image compression system usually includes three
components, i.e. encoder, quantizer, and decoder, to form
the codec.
The typical image encoding standards, e.g.,
JPEG and JPEG 2000, generally rely on handcrafted image transformation and separate optimization on codecs,
and thus are suboptimal for image compression. Moreover,
JPEG and JPEG 2000 perform poor for low rate image compression, and usually are inevitable in producing some visual artifacts, e.g., blurring, ringing, and blocking.
Recently, deep convolutional networks (CNNs) have
achieved great success in versatile vision tasks . As to image compression, CNN is also expected
to be more powerful than JPEG and JPEG 2000 by considering the following reasons.
First, for image encoding and decoding, ﬂexible nonlinear analysis and synthesis transformations can be easily deployed by stacking several convolutional layers. Second, it allows to jointly optimize the nonlinear encoder and decoder in an end-to-end
Furthermore, several recent advances also validate the effectiveness of deep learning in image compression .
However, there are still several issues to be addressed
in CNN-based image compression. In general, lossy image
compression can be formulated as a joint rate-distortion optimization to learn encoder, quantizer, and decoder. Even
the encoder and decoder can be represented as CNNs and
be optimized via back-propagation, the learning of non-
 
differentiable quantizer is still a challenge problem. Moreover, the system aims to jointly minimize both the compression rate and distortion, where entropy rate should also be
estimated and minimized in learning. As a result of quantization, the entropy rate deﬁned on discrete codes is also
a discrete function, and a continuous approximation is required.
In this paper, we present a novel CNN-based image compression framework to address the issues raised by quantization and entropy rate estimation. For the existing deep
learning based compression models , the discrete
code after quantization should ﬁrst have the same length
with the encoder output, and then compressed based on entropy coding. That is, the discrete code before entropy coding is spatially invariant. However, it is generally known
that the local information content is spatially variant in an
image. Thus, the bit rate should also be spatially variant
to adapt to local information content. To this end, we introduce a content-weighted importance map to guide the
allocation of local bit rate. Given an input image x, let
e = E(x) be the output of encoder network, which includes
n feature maps with size of h × w. Denote by p = P(x)
the h×w non-negative importance map. Speciﬁcally, when
L ≤pi,j < l
L, we will only encode the ﬁrst nl
L -th feature
maps at spatial location (i, j). Here, L is the number of the
importance level. And n
L is the number of bits for each importance level. The other feature maps are automatically set
with 0 and need not be saved into the codes. By this way, we
can allocate more bits to the region with rich content, which
is very helpful in preserving texture details with less sacri-
ﬁce of bit rate. Moreover, the sum of the importance map
i,j pi,j will serve as a continuous estimation of compression rate, and can be directly adopted as a compression rate
controller.
Beneﬁted from importance map, we do not require to use
any other entropy rate estimate in our objective, and can
adopt a simple binarizer for quantization. The binarizer set
those features with the possibility over 0.5 to 1 and others
to 0. Inspired by the binary CNN , we introduce a
proxy function for the binary operation in backward propagation and make it trainable. As illustrated in Figure 1, our
compression framework consists of four major components:
convolutional encoder, importance map network, binarizer,
and convolutional decoder. With the introduction of continuous importance map and proxy function, all the components can be jointly optimized in an end-to-end manner.
Note that we do not include any term on entropy rate
estimate in the training of the compression system. And
the local spatial context is also not utilized. Therefore, we
design a convolutional entropy coder to predict the current
code with its context, and apply it to context-adaptive binary arithmetic coding (CABAC) framework to further
compress the binary codes and importance map.
Our whole framework is trained on a subset of the ImageNet database and tested on the Kodak dataset. In low
bit rate image compression, our system achieves much better rate-distortion performance than JPEG and JPEG 2000
in terms of both quantitative metrics (e.g., SSIM and MSE)
and visual quality. More remarkably, the compressed images by our system are visually more pleasant in producing
sharp edges, rich textures, and fewer artifacts. Compared
with other CNN-based system , ours performs
better in retaining texture details while suppressing visual
artifacts.
To sum up, the main contribution of this paper is to introduce content-weighted importance map and binary quantization in the image compression system. The importance
map not only can be used to substitute entropy rate estimate
in joint rate-distortion optimization, but also can be adopted
to guide the local bit rate allocation. By equipping with binary quantization and the proxy function, our compression
system can be end-to-end trained, and obtain signiﬁcantly
better results than JPEG and JPEG 2000.
2. Related Work
For the existing image standards, e.g., JPEG and JPEG
2000, the codecs actually are separately optimized. In the
encoding stage, they ﬁrst perform a linear transform to an
image. Quantization and lossless entropy coding are then
utilized to minimize the compression rate. For example,
JPEG applies discrete cosine transform (DCT) on 8×8
image patches, quantizes the frequency components and
compresses the quantized codes with a variant of Huffman
encoding. JPEG 2000 uses a multi-scale orthogonal
wavelet decomposition to transform an image, and encodes
the quantized codes with the Embedded Block Coding with
Optimal Truncation. In the decoding stage, decoding algorithm and inverse transform are designed to minimize distortion. In contrast, we model image compression as a joint
rate-distortion optimization, where both nonlinear encoder
and decoder are jointly trained in an end-to-end manner.
Recently, several deep learning based image compression models have been developed.
For lossless image
compression, deep learning models have achieved state-ofthe-art performance .
For lossy image compression, Toderici et al. present a recurrent neural network
(RNN) to compress 32×32 images. Toderici et al. further introduce a set of full-resolution compression methods
for progressive encoding and decoding of images. These
methods learn the compression models by minimizing the
distortion for a given compression rate. While our model is
end-to-end trained via joint rate-distortion optimization.
The most related work is that of based on convolutional autoencoders. Ball´e et al. use generalized divisive normalization (GDN) for joint nonlinearity, and replace
rounding quantization with additive uniform noise for con-
Figure 1. Illustration of the CNN architecture for content-weighted image compression.
tinuous relaxation of distortion and entropy rate loss. Theis
et al. adopt a smooth approximation of the derivative of
the rounding function, and upper-bound the discrete entropy
rate loss for continuous relaxation. Our content-weighted
image compression system is different with in rate
loss, quantization, and continuous relaxation. Instead of
rounding and entropy, we deﬁne our rate loss on importance
map and adopt a simple binarizer for quantization. Moreover, the code length after quantization is spatially invariant
in . In contrast, the local code length in our model is
content-aware, which is useful in improving visual quality.
Our work is also related to binarized neural network
(BNN) , where both weights and activations are binarized to +1 or −1 to save memory storage and run time.
Courbariaux et al. adopt a straight-through estimator to
compute the gradient of the binarizer. In our compression
system, only the encoder output is binarized to 1 or 0, and a
similar proxy function is used in backward propagation.
3. Content-weighted Image Compression
Our content-weighted image compression framework is
composed of four components, i.e. convolutional encoder,
binarizer, importance map network, and convolutional decoder. And Figure 1 shows the whole network architecture.
Given an input image x, the convolutional encoder deﬁnes a
nonlinear analysis transform by stacking convolutional layers, and outputs E(x). The binarizer B(E(x)) assigns 1
to the encoder output higher than 0.5, and 0 to the others.
The importance map network takes the intermediate feature
maps as input, and yields the content-weighted importance
map P(x). The rounding function is adopted to quantize
P(x) and generate a mask M(P(x)) that has the same size
of B(E(x)). The binary code is then trimmed based on
M(P(x)). And the convolutional decoder deﬁnes a nonlinear synthesis transform to produce decoding result ˆx.
In the following, we ﬁrst introduce the components of the
framework and then present the formulation and learning
method of our model.
3.1. Components and Gradient Computation
Convolutional encoder and decoder
Both the encoder and decoder in our framework are
fully convolution networks and can be trained by backpropagation. The encoder network consists of three convolutional layers and three residual blocks. Following ,
each residual block has two convolutional layers. We further remove the batch normalization operations from the
residual blocks. The input image x is ﬁrst convolved with
128 ﬁlters with size 8 × 8 and stride 4 and followed by one
residual block. The feature maps are then convolved with
256 ﬁlters with size 4 × 4 and stride 2 and followed by
two residual blocks to output the intermediate feature maps
f(x). Finally, f(x) is convolved with m ﬁlters with size
1 × 1 to yield the encoder output E(x). It should be noted
that we set n = 64 for low comparison rate models with
less than 0.5 bpp, and n = 128 otherwise.
The network architecture of decoder D(c) is symmetric
to that of the encoder, where c is the code of an image x.
To upsample the feature maps, we adopt the depth to space
operation mentioned in . Please refer to the supplementary material for more details on the network architecture of
the encoder and decoder.
Due to sigmoid nonlinearity is adopted in the last convolutional layer, the encoder output e = E(x) should be in
the range of . Denote by eijk an element in e. The
binarizer is deﬁned as
if eijk > 0.5
if eijk ≤0.5
However, the gradient of the binarizer function B(eijk)
is zero almost everywhere except that it is inﬁnite when
eijk = 0.5. In the back-propagation algorithm, the gradient is computed layer by layer by using the chain rule in a
backward manner. Thus, this will make any layer before the
binarizer (i.e., the whole encoder) never be updated during
Fortunately, some recent works on binarized neural networks (BNN) have studied the issue of propagating gradient through binarization. Based on the straightthrough estimator on gradient , we introduce a proxy
function ˜B(eijk) to approximate B(eijk). Here, B(eijk) is
still used in forward propagation calculation, while ˜B(eijk)
is used in back-propagation. Inspired by BNN, we adopt a
piecewise linear function ˜B(eijk) as the proxy of B(eijk),
˜B(eijk) =
if eijk > 1
if 1 ≤eijk ≤0
if eijk < 0
Then, the gradient of ˜B(eijk) can be easily obtained by,
˜B′(eijk) =
if 1 ≤eijk ≤0
Importance map
In , the code length after quantization is spatially invariant, and entropy coding is then used to further compression the code. Actually, the difﬁculty in compressing different parts of an image should be different. The smooth
regions in an image should be easier to be compressed than
those with salient objects or rich textures. Thus, fewer bits
should be allocated to the smooth regions while more bits
should be allocated to the regions with more information
content. For example, given an image with an eagle ﬂying
in the blue sky in Figure 2, it is reasonable to allocate more
bits to the eagle and fewer bits to blue sky. Moreover, when
the whole code length for an image is limited, such allocation scheme can also be used for rate control.
We introduce a content-weighted importance map for bit
allocation and compression rate control. It is a feature map
with only one channel, and its size should be same with
that of the encoder output. The value of importance map
Figure 2. Illustration of importance map. The regions with sharp
edge or rich texture generally have higher values and should be
allocated more bits to encode.
is in the range of (0, 1). An importance map network is
deployed to learn the importance map from an input image
x. It takes the intermediate feature maps f(x) from the last
residual block of the encoder as input, and uses a network
of three convolutional layers to produce the importance map
Denote by h×w the size of the importance map p, and n
the number of feature maps of the encoder output. In order
to guide the bit allocation, we should ﬁrst quantize each
element in p to an integer no more than n, and then generate
an importance mask m with size n × h × w. Given an
element pij in p, the quantizer to importance map is deﬁned
L ≤pij < l
L, l = 1, . . . , L
if pij = 1
where L is the importance levels and (n mod L) = 0. Each
important level is corresponding to n
L bits. As mentioned
above, pij ∈(0, 1). Thus, pij ̸= 1 and Q(pij) has only L
kinds of different quantity value i.e. 0, . . . , L −1.
It should be noted that, Q(pij) = 0 indicates that zero bit
will be allocated to this location, and all its information can
be reconstructed based on its context in the decoding stage.
By this way, the importance map can not only be treated as
an alternative of entropy rate estimation but also naturally
take the context into account.
With Q(p), the importance mask m = M(p) can then
be obtained by,
The ﬁnal coding result of the image x can then be represented as c = M(p) ◦B(e), where ◦denotes the elementwise multiplication operation. Note that the quantized importance map Q(p) should also be considered in the code.
Thus all the bits with mkij = 0 can be safely excluded from
B(e). Thus, instead of n, only n
LQ(pij) bits are need for
each location (i, j). Besides, in video coding, just noticeable distortion (JND) models have also been suggested
for spatially variant bit allocation and rate control. Different from , the importance map are learned from training
data by optimizing joint rate-distortion performance.
Finally, in back-propagation, the gradient m with respect
to pij should be computed. Unfortunately, due to the quantization operation and mask function, the gradient is zero
almost everywhere. Actually, the importance map m can be
equivalently rewritten as a function of p,
n ⌉< Lpij + 1
where ⌈.⌉is the ceiling function. Analogous to binarizer,
we also adopt a straight-through estimator of the gradient,
L, if Lpij −1 ≤⌈kL
n ⌉< Lpij + 2
3.2. Model formulation and learning
Model formulation
In general, the proposed content-weighted image compression system can be formulated as a rate-distortion optimization problem. Our objective is to minimize the combination
of the distortion loss and rate loss. A tradeoff parameter γ
is introduced for balancing compression rate and distortion.
Let X be a set of training data, and x ∈X be an image
from the set. Therefore, the objective function our model is
{LD(c, x) + γLR(x)},
where c is the code of the input image x. LD(c, x) denotes
the distortion loss and LR(x) denotes the rate loss, which
will be further explained in the following.
Distortion loss. Distortion loss is used to evaluate the
distortion between the original image and the decoding result. Even better results may be obtained by assessing the
distortion in the perceptual space. With the input image x
and decoding result D(c), we simply use the squared ℓ2 error to deﬁne the distortion loss,
LD(c, x) = ∥D(c) −x∥2
Rate loss. Instead of entropy rate, we deﬁne the rate loss
directly on the continuous approximation of the code length.
Suppose the size of encoder output E(x) is n × h × w.
The code by our model includes two parts: (i) the quantized
importance map Q(p) with the ﬁxed size h × w; (ii) the
trimmed binary code with the size n
i,j Q(pij). Note that
the size of Q(p) is constant to the encoder and importance
map network. Thus n
i,j Q(pij) can be used as rate loss.
Due to the effect of quantization Q(pij), the function
i,j Q(pij) cannot be optimized by back-propagation.
Thus, we relax Q(pij) to its continuous form, and use the
sum of the importance map p = P(x) as rate loss,
For better rate control, we can select a threshold r, and penalize the rate loss in Eqn. (10) only when it is higher than
r. And we then deﬁne the rate loss in our model as,
i,j(P(x))ij−r, if P
i,j(P(x))ij >r
The threshold r can be set based on the code length for a
given compression rate. By this way, our rate loss will penalize the code length higher than r, and makes the learned
compression system achieve the comparable compression
rate to the given one.
Beneﬁted from the relaxed rate loss and the straight-through
estimator of the gradient, the whole compression system
can be trained in an end-to-end manner with an ADAM
solver . We initialize the model with the parameters pretrained on training set X without the importance map. The
model is further trained with the learning rate of 1e−4, 1e−5
and 1e−6. In each learning rate, the model is trained until the objective function does not decrease. And a smaller
learning rate is adopted to ﬁne-tune the model.
4. Convolutional entropy encoder
Due to no entropy constraint is included, the code generated by the compression system in Sec. 3 is non-optimal
in terms of entropy rate. This provides some leeway to further compress the code with lossless entropy coding. Generally, there are two kinds of entropy compression methods,
i.e. Huffman tree and arithmetic coding . Among them,
arithmetic coding can exhibit better compression rate with
a well-deﬁned context, and is adopted in this work.
4.1. Encoding binary code
The binary arithmetic coding is applied according to the
CABAC framework. Note that CABAC is originally
proposed for video compression. Let c be the code of n
binary bitmaps, and m be the corresponding importance
mask. To encode c, we modify the coding schedule, redeﬁne the context, and use CNN for probability prediction.
As to coding schedule, we simply code each binary bit map
from left to right and row by row, and skip those bits with
the corresponding important mask value of 0.
Figure 3. The CNN for convolutional entropy encoder. The red
block represents the bit to predict; dark blocks mean unavailable
bits; blue blocks represent available bits.
Context modeling. Denote by ckij be a binary bit of
the code c. We deﬁne the context of ckij as CNTX(ckij)
by considering the binary bits both from its neighbourhood
and from the neighboring maps. Speciﬁcally, CNTX(ckij)
is a 5 × 5 × 4 cuboid.
We further divide the bits in
CNTX(ckij) into two groups: the available and unavailable ones. The available ones represent those can be used to
predict ckij. While the unavailable ones include: (i) the bit
to be predicted ckij, (ii) the bits with the importance map
value 0, (iii) the bits out of boundary and (iv) the bits currently not coded due to the coding order. Here we redeﬁne
CNTX(ckij) by: (1) assigning 0 to the unavailable bits,
(2) assigning 1 to the unavailable bits with value 0, and assigning 2 to the unavailable bits with value 1.
Probability prediction. One usual method for probability prediction is to build and maintain a frequency table. As
to our task, the size of the cuboid is too large to build the
frequency table. Instead, we introduce a CNN model for
probability prediction. As shown in Figure 3, the convolutional entropy encoder En(CNTX(ckij)) takes the cuboid
as input, and output the probability that the bit ckij is 1.
Thus, the loss for learning the entropy encoder can be written as,
mkij {ckij log2(En(CNTX(ckij)))
+(1 −ckij) log2(1 −En(CNTX(ckij)))}
where m is the importance mask. The convolutional entropy encoder is trained using the ADAM solver on the
contexts of binary codes extracted from the binary feature
maps generated by the trained encoder. The learning rate
decreases from 1e−4 to 1e−6 as we do in Sec. 3.
4.2. Encoding quantized importance map
We also extend the convolutional entropy encoder to the
quantized importance map. To utilize binary arithmetic coding, a number of binary code maps are adopted to represent
the quantized importance map. The convolutional entropy
encoder is then trained to compress the binary code maps.
5. Experiments
Our content-weighted image compression model are
trained on a subset of ImageNet with about 10, 000
Figure 4. Comparison of the ratio-distortion curves by different
methods: (a) PSNR, (b) SSIM, and (c) MSE.
”Without IM” represents the proposed method without importance map.
high quality images. We crop these images into 128 × 128
patches and take use of these patches to train the network.
After training, we test our model on the Kodak PhotoCD
image dataset with the metrics for lossy image compression. The compression rate of our model is evaluated by the
metric bits per pixel (bpp), which is calculated as the total
amount of bits used to code the image divided by the number of pixels. The image distortion is evaluated with Means
Square Error (MSE), Peak Signal-to-Noise Ratio (PSNR),
and the structural similarity (SSIM) index.
In the following, we ﬁrst introduce the parameter setting
of our compression system. Then both quantitative metrics
and visual quality evaluation are provided. Finally, we further analyze the effect of importance map and convolutional
entropy encoder on the compression system.
5.1. Parameter setting
In our experiments, we set the number of binary feature
maps n according to the compression rate, i.e. 64 when the
compression rate is less than 0.5 bpp and 128 otherwise.
Then, the number of importance level is chosen based on m.
For n = 64 and n = 128, we set the number of importance
level to be 16 and 32, respectively. Moreover, different values of the tradeoff parameter γ in the range [0.0001, 0.2] are
chosen to get different compression rates. For the choice of
the threshold value r, we just set it as r0hw for n = 64 and
0.5r0hw for n = 128. r0 is the wanted compression rate
represent with bit per pixel (bpp).
5.2. Quantitative evaluation
For quantitative evaluation, we compare our model
with JPEG , JPEG 2000 , and the CNN-based
method by Ball´e et al. .
Among the different variants of JPEG, the optimized JPEG with 4:2:0 chroma subsampling is adopted. For the sake of fairness, all the results by Ball´e , JPEG, and JPEG2000 on the Kodak
dataset are downloaded from 
edu/˜lcv/iclr2017/.
Figure 5. Images produced by different compression systems at different compression rates. From the left to right: groundtruth, JPEG,
JPEG 2000, Ball´e , and ours. Our model achieves the best visual quality at each rate, demonstrating the superiority of our model in
preserving both sharp edges and detailed textures. (Best viewed on screen in color)
Using MSE, SSIM and PSNR as performance metrics, Figure 4 gives the ratio-distortion curves of these four
methods. In terms of MSE, JPEG has the worst performance. And both our system and Ball´e can be slightly
better than JPEG 2000. In terms of PSNR, the results by
JPEG 2000, Ball´e and ours are very similar, but are
much higher than that by JPEG. In terms of SSIM, our system outperforms all the three competing methods, including
JPEG, JPEG 2000, and Ball´e . Due to SSIM is more consistent with human visual perception than PSNR and MSE,
these results indicate that our system may perform better in
terms of visual quality.
5.3. Visual quality evaluation
We further compare the visual quality of the results by
JPEG, JPEG 2000, Ball´e and our system in low compression rate setting. Figure 5 shows the original images
and the results produced by the four compression systems.
Visual artifacts, e.g., blurring, ringing, and blocking, usually are inevitable in the compressed images by traditional
image compression standards such as JPEG and JPEG 2000.
And these artifacts can also be perceived in the second and
third columns of Figure 5. Even Ball´e is effective in suppressing these visual artifacts. In Figure 5, from the results
produced by Ball´e , we can observe the blurring artifacts
in row 1, 2, 3, and 5, the color distortion in row 4 and 5,
and the ringing artifacts in row 4 and 5. In contrast, the results produced by our system exhibit much less noticeable
artifacts and are visually much more pleasing.
From Figure 5, Ball´e usually produces the results by
blurring the strong edges or over-smoothing the small-scale
textures. Speciﬁcally, in row 5 most details of the necklace
have been removed by Ball´e . One possible explanation
may be that before entropy encoding it adopts a spatially
invariant bit allocation scheme. Actually, it is natural to
see that more bits should be allocated to the regions with
strong edges or detailed textures while less to the smooth
regions. Instead, in our system, an importance map is introduced to guide spatially variant bit allocation. Moreover,
instead of handcrafted engineering, the importance map are
end-to-end learned to minimize the rate-distortion loss. As
a result, our model is very promising in keeping perceptual
structures, such as sharp edges and detailed textures.
5.4. Experimental analyses on important map
To assess the role of importance map, we train a baseline model by removing the importance map network from
our framework. Both entropy and importance map based
rate loss are not included in the baseline model. Thus, the
compression rate is controlled by modifying the number
of binary feature maps. Figure 4 also provides the ratiodistortion curves of the baseline model. One can see that,
the baseline model performs poorer than JPEG 2000 and
Figure 6. The important maps obtained at different compression
rates. The right color bar shows the palette on the number of bits.
Figure 7. Performance of convolutional entropy encoder: (a) for
encoding binary codes and importance map, and (b) by comparing
with CABAC.
Ball´e in terms of MSE, PSNR, and SSIM, validating the
necessity of importance map for our model. Using the image in row 5 of Figure 5, the compressed images by our
model with and without importance map are also shown in
the supplementary material. And more detailed textures and
better visual quality can be obtained by using the importance map.
Figure 6 shows the importance map obtained at different
compression rates. One can see that, when the compression
rate is low, due to the overall bit length is very limited, the
importance map only allocates more bits to salient edges.
With the increasing of compression rate, more bits will begin to be allocated to weak edges and mid-scale textures.
Finally, when the compression rate is high, small-scale textures will also be allocated with more bits. Thus, the importance map learned in our system is consistent with human
visual perception, which may also explain the advantages of
our model in preserving the structure, edges and textures.
5.5. Entropy encoder evaluation
The model in Sec. 3 does not consider entropy rate, allowing us to further compress the code with convolutional
entropy encoder. Here, two groups of experiments are conducted. First, we compare four variants of our model: (i) the
full model, (ii) the model without entropy coding, (iii) the
model by only encoding binary codes, and (iv) the model
by only encoding importance map. From Figure 7(a), both
the binary codes and importance map can be further compressed by using our convolutional entropy encoder. And
our full model can achieve the best performance among the
four variants. Second, we compare our convolutional entropy coding with the standard CABAC with small context
(i.e. the 5 bits near the bit to encode). As shown in Figure 7(b), our convolutional entropy encoder can take larger
context into account and perform better than CABAC. Besides, we also note that our method with either CABAC or
convolutional encoder can outperform JPEG 2000 in terms
6. Conclusion
A CNN-based system is developed for content weighted
image compression. With the importance map, we suggest a
non-entropy based loss for rate control. Spatially variant bit
allocation is also allowed to emphasize the salient regions.
Using the straight-through estimator, our model can be endto-end learned on a training set. A convolutional entropy
encoder is introduced to further compress the binary codes
and the importance map. Experiments clearly show the superiority of our model in retaining structures and removing
artifacts, leading to remarkable visual quality.