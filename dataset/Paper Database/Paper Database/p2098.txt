Residual Networks Behave Like Boosting Algorithms∗
Chapman Siu
Faculty of Engineering and Information Technology
University of Technology Sydney, Australia
 
We show that Residual Networks (ResNet) is equivalent to boosting feature representation, without any modiﬁcation to the underlying ResNet training algorithm. A
regret bound based on Online Gradient Boosting theory is proved and suggests that
ResNet could achieve Online Gradient Boosting regret bounds through neural network architectural changes with the addition of a shrinkage parameter in the identity
skip-connections and using residual modules with max-norm bounds. Through
this relation between ResNet and Online Boosting, novel feature representation
boosting algorithms can be constructed based on altering residual modules. We
demonstrate this through proposing decision tree residual modules to construct a
new boosted decision tree algorithm and demonstrating generalization error bounds
for both approaches; relaxing constraints within BoostResNet algorithm to allow it
to be trained in an out-of-core manner. We evaluate convolution ResNet with and
without shrinkage modiﬁcations to demonstrate its efﬁcacy, and demonstrate that
our online boosted decision tree algorithm is comparable to state-of-the-art ofﬂine
boosted decision tree algorithms without the drawback of ofﬂine approaches.
Introduction
Residual Networks (ResNet) have previously had a lot of attention due to performance, and ability
to construct “deep” networks while largely avoiding the problem of vanishing (or exploding) gradients.
Some attempts have been made in explaining ResNets through: unravelling their representation ;
observing identity loops and ﬁnding no spurious local optima ; and reinterpreting residual modules
as weak classiﬁers which allows sequential training under boosting theory .
Empirical evidence shows that these deep residual networks, and subsequent architectures with
the same parameterizations, are easier to optimize. They also out-perform non-residual ones, and
have consistently achieved state-of-the-art performance on various computer vision tasks such as
CIFAR-10 and ImageNet .
Summary of Results
We demonstrate the equivalence of ResNet and Online Boosting in Section 3. We show that the
layer by layer boosting method of ResNet has an equivalent representation of additive modelling
approaches ﬁrst demonstrated in Logitboost which boost feature representations rather than the
label directly, i.e. ResNet can be framed as an Online Boosting algorithm with composite loss
functions. Although traditional boosting results may not apply as ResNet are not a naive weighted
ensemble, we can refer to them as Online Boosting analogues which presents regret bound guarantees.
We demonstrate that under “nice” conditions for the composite loss function, the regret bound for
Online Boosting holds, and by extension also applies for ResNet architectures.
∗This work was supported by and completed whilst author was at Suncorp Group Limited.
 
 
Taking inspiration from Online Boosting, we also modify the architecture of ResNet with an additional
learnable shrinkage parameter (vanilla ResNet can be interpreted as Online Boosting algorithm where
the shrinkage factor is ﬁxed/unlearnable and set to 1). As this approach only modiﬁes the neural
network architecture, the same underlying ResNet algorithms can still be used.
Experimentally, we compare vanilla ResNet with our modiﬁed ResNet using convolutional neural
network residual network (ResNet-CNN) on multiple image datasets. Our modiﬁed ResNet shows
some improvement over vanilla ResNet architecture.
We also compare our boosted decision tree neural decision tree residual network on multiple benchmark datasets and their results against other decision tree ensemble methods, including Deep Neural
Decision Forests , neural decision trees ensembled via AdaNet , and off-the-shelf algorithms
(gradient boosting decision tree/random forest) using LightGBM . In our experiments, neural
decision tree residual network showed superior performance to neural decision tree variants, and
comparable performance to ofﬂine tradition gradient boosting decision tree models.
Related Works
In recent years researchers have sought to understand why ResNet perform the way that they do. The
BoostResNet algorithm reinterprets ResNet as a multi-channel telescoping sum boosting problem
for the purpose of introducing a new algorithm for sequential training , providing theoretical
justiﬁcation for the representational power of ResNet under linear neural network constraints . One
interpretation of residual networks is as a collection of many paths of differing lengths which behave
like a shallow ensemble; empirical studies demonstrate that residual networks introduce short paths
which can carry gradients throughout the extent of very deep networks .
Comparison with BoostResNet and AdaNet
Combining Neural Networks and boosting has previously been explored in architectures such as
AdaNet and BoostResNet . We seek to understand why ResNet achieves their level of
performance, without altering how ResNet are trained.
In the case of BoostResNet, the distribution must be explicitly maintain over all examples during
training and parts of ResNet are trained sequentially which cannot be updated in a truly online,
out-of-core manner. And in the case of AdaNet, which do not always work for ResNet structure,
additional feature vectors are sequentially added, and chooses their own structure during learning.
In our proposed approach, we do not require these modiﬁcations, and can train the model in the
same way as an unmodiﬁed ResNet. A ResNet style architecture is a special case of AdaNet, so
AdaNet generalization guarantee applies here and our generalization analysis is built upon their
work. Furthermore we also demonstrate Neural Decision Trees belong to same family of feedforward
neural networks as AdaNet, so AdaNet generalization guarentee also applies to Neural Decision Tree
ResNet modules and our generalization analysis is built upon their work.
Preliminaries
In this section we cover the background of residual neural networks and boosting. We also explore
the conditions which enable regret bounds in Online Gradient Boosting setting and the class of
feedforward neural networks for AdaNet generalization bounds.
Residual Neural Networks
A residual neural network (ResNet) is composed of stacked entities referred to as residual blocks.
A Residual Block of ResNet contains a module and an identity loop. Let each module map its
input x to ft(x) where t denotes the level of the module, and where ft(x) is typically a sequence of
convolutions, batch normalizations or non-linearities. These formulations may differ depending on
context and the model architecture.
We denote the output of the t-th residual block to be gt+1(x)
gt+1(x) = ft(gt(x)) + gt(x)
where x is the input of the ResNet.
Output of ResNet has a recursive relation speciﬁed in equation 1, then output of the T-th residual
block is equal to the summation of lower module outputs, i.e., gT +1(x) = PT
t=0 ft(gt(x)), where
g0(x) = 0 and f0(g0(x)) = x. For classiﬁcation tasks, the output of a ResNet is rendered after a
linear classiﬁer w ∈Rn×C on representation gT +1(x) where C is the number of classes, and n is
the number of channels:
ˆy = ˜σ(w⊤gT +1(x)) = ˜σ
w⊤ft(gt(x))
where σ(·) denotes a map from classiﬁer output to labels. For example, σ could represent a softmax
The goal of boosting is to combine weaker learners into a strong learner. There are many variations to
boosting. For example, in AdaBoost and its derivatives, we require the boosting algorithm to choose
training sets for the weak classiﬁer to force it to make novel inferences . This was the approached
used by BoostResNet . In gradient boosting, this requirement is removed through training against
pseudo-residual and can even be extended to the online learning setting .
In either scenario, boosting can be viewed as an additive model or linear combinations of T models
˜F(x) = PT
t=1 αtht(x), where ht(x) is a function of the input x and αt is the corresponding
multiplier for the t-th model .
LogitBoost is an algorithm ﬁrst introduced in “Additive Logistic Regression: A Statistical View
of Boosting” , which introduces boosting on the input feature representation, including neural
networks. In the general binary classiﬁcation scenario, the formulation relies on boosting over the
logit or softmax transformation
F (x) = ˜σ( ˜F(x)) = ˜σ
Where ˜σ represents the softmax function. This form is similar to the linear classiﬁer layer which is
used by ResNet algorithm.
Online Boosting introduces a framework for training boosted models in an online manner. Within
this formulation, there are two adjustments which are required to make ofﬂine boosting models online.
First, the partial sums ˆyi−1 (where ˆyi represents the predictions of the i-th model) is multiplied by a
shrinkage factor, which is tuned using gradient descent. Second, the partial sums ˆyi outputs are to be
bounded .
The bounds presented for online gradient boosting are based on regret. The regret RA(T) of a learner
is deﬁned as the difference between the total loss from the learner and the total learner of the best
hypothesis in hindsight
ℓt(ht(xt)) −min
ℓ(f ∗(xt))
Online gradient boosting regret bounds applies can be applied to any linear combination of a give
base weak learner with a convex, linear loss function that is Lipschitz constant bounded by 1.
Corollary 2.1 (From Corollary 1 ) Let the learning rate η ∈[ 1
N , 1], number of weak learners
N, be given parameters. Algorithm 1 is an online learning algorithm for span(F) for set of convex,
linear loss functions with Lipschitz constant bounded by 1 with the following regret bound for any
f ∈span(F):
f(T) ≤(1 −
O(||f||1 · (ηT + R(T) +
where ∆0 := PT
t (f(xt)), or the initial error, and R(T) is the regret or excess loss for
the base learner algorithm.
The regret bound in this theorem depends on several conditions; the requirement that for any weak
learner A, that it has a ﬁnite upper bound, i.e. ||A(x)|| ≤D, for some D, and the set of loss functions
constraints an efﬁciently computable subgradient ∇ℓ(y) has a ﬁnite upper bound.
Compared with boosting approach used in BoostResNet which is based on AdaBoost , the usage
of the online gradient boosting algorithm does not require maintaining an explicit distribution of
weights over the whole training data set and is a “true” online, out-of-core algorithm. Leveraging
online gradient boosting allows us to overcome the constraints of BoostResNet approach.
AdaNet Generalization Bounds for feedforward neural networks deﬁned to be a multi-layer architecture where units in each layer are only connected to those in the layer below has been provided by
 . It requires the weights of each layer to be bounded by lp-norm, with p ≥1, and all activation
functions between each layer to be coordinate-wise and 1-Lipschitz activation functions. This yields
the following generalization error bounds provided by Lemma 2 from :
Corollary 2.2 (From Lemma 2 ) Let D be distribution over X × Y and S be a sample of m
examples chosen independently at a random according to D. With probability at least 1 −δ, for
θ > 0, the strong decision tree classiﬁer F(x) satisﬁes that
R(f) ≤ˆRS,ρ(f) + 4
|wk|1Rm( ˜Hk) + 2
+C(ρ, l, m, δ)
where C(ρ, l, m, δ) =
ρ2 log(ρ2m
log l )⌉log l
As this bound depends only on the logarithmically on the depth for the network l this demonstrates
the importance of strong performance in the earlier layers of the feedforward network.
Now that we have a formulation for ResNet and boosting, we explore further properties of ResNet,
and how we may evolve and create more novel architectures.
ResNet are Equivalent to a Boosted Model
As we recall from equations 2 and 3, ResNet indeed have a similar form to LogitBoost. In this
scenario, both formulations aim to boost the underlying feature representation. One consequence of
the ResNet formulation is that the linear classiﬁer w, would be a shared linear classiﬁer across all all
ResNet modules.
Assumption 3.1 The t-th residual module with a trainable linear classiﬁer layer w deﬁned by
˜ht(x) := w⊤ft(gt(x))
Is a weak learner for all t ≥0. We will call this weak learner the hypothesis module.
This assumption is required to ensure that ˜ht(x) is a weak learner to adhere to learning bounds
proposed in Corollary 2.1. We show that different ResNet modules variants used in our experiments
assumption in Sections 6.3.
Overall this demonstrates that the proposed framework is equivalent to traditional boosting frameworks which boost on the feature representation. However, to further analyse the algorithmic
results, we need to ﬁrst consider additional restrictions which are placed within the “Online Boosting
Algorithm” framework.
Online Boosting Considerations
Our representation is a special case of online gradient boosting as shown in Algorithm 1, our regret
bound analysis is built upon work in . The regret bounds for an online boosting algorithm that
competes with linear combination of the base weak learner applies when used for a class of convex,
linear loss function with Lipschitz constant bounded by 1.
Algorithm 1 Online Boosting for Composite Loss Functions for span(F)
1: Maintain N copies of the algorithm A, denoted A1, A2, . . . , AN and choose step size parameter
2: For each i, initialize θi = 0.
3: for t = 1 to T do
Receive example xt
for i = 1 to N do
t = (1 −θi)Fi−1
+ ηAi(xt), where (1 −θi) is our shrinkage factor for algorithm Ai
Predict yt = ψ−1(FN
Obtain loss function ℓψ
t and the model suffers loss ℓψ
t (Ft), which is equivalent to equivalently
for i = 1 to N do
Pass loss based on partial sums of Fi−1
to Ai, i.e. in descent direction ∇ℓψ
Update θi ∈[0, η] using online gradient descent
15: end for
This algorithm yields the regret bound for algorithm 1, which is directly from Corollary 1 from 
in Corollary 2.1. We will provide further analysis of this algorithm; in particular the validity of
composite loss functions in Section 4.
Analysis of Online Boosting for Composite Loss Functions
In this section we provide analysis on corollary 2.1. Corollary 2.1 holds for the learning algorithm
with losses in C, where C is deﬁned to be set of convex, linear loss functions with Lipschitz constant
bounded by 1. Next we describe the conditions in which composite loss functions ℓψ belongs in C.
A composite loss function ℓψ, where ψ is the link function, belongs to C if ψ is the canonical link
function. This has been shown to be a sufﬁcient but not necessary condition for canonical link to lead
to convex composite loss functions .
Lemma 4.1 Composite loss functions retain smoothness
Proof: If ψ−1 satisﬁes Lipschitz continuous function (e.g. logistic function/softmax, as its derivative
is bounded everywhere), then the composite loss is also Lipschitz constant, as composition of
functions which are Lipschitz constant is also Lipschitz constant. As if f2 has Lipschitz constant L2
and f1 has Lipschitz constant L1 then
|f2(f1(x)) −f2(f1(y))| ≤L2|f(x) −f1(y)|
≤L1L2|x −y|
Hence, if ψ−1 has Lipschitz constant bounded by 1, then the composition of the particular loss
function with Lipschitz constant of 1 also has Lipschitz constant bounded by 1 and belongs to the
base loss function class. An example of such a link function is the logit function, which has a
Lipschitz constant of 1 and is the canonical link function for log loss (cross entropy loss), which
suggests that the composite loss function is indeed convex and belongs in loss function class C
This demonstrates ResNet which boost on the feature representation and have a logit link satisﬁes
regret bound as shown in Collorary 2.1.
Recovering Loss For Intermediary Residual Modules
Figure 1: The architecture of a modiﬁed residual network (three residual modules) with shrinkage
parameter and shared linear classiﬁer
When analysing the ResNet and Online Boosting algorithm, the Online Boosting algorithm requires
the gradient of the underlying boosting function to be recovered as part of the update process. This
is shown in line 12 within Algorithm 1. One approach to tackle this challenge was suggested in
BoostResNet where a common auxiliary linear classiﬁer is used across all residual modules, however
this approach was not explored in the work as BoostResNet was focused on sequential training of
residual modules, and such a constraint was deemed inappropriate. Instead BoostResNet would
construct different linear classiﬁer layers which were dropped at every stage when the residual
modules have been trained.
Our approach to remediate this is to formulate the ResNet architecture as a single-input, multi-output
training problem, whereby each residual module will have an explicit ‘shortcut’ to the output labels
whilst sharing the same linear classiﬁer layer. This architecture is shown in Figure 1.
Remark: It has been demonstrated that through carefully constructing a ResNet, the last layer need
not be trained, and instead can be a ﬁxed random projection . This has been demonstrated through
theoretical justiﬁcations in linear neural networks.
In our ResNet algorithm, if the linear classiﬁer layer is shared, then the model would be framed as a
single input, multi-output residual network, where the outputs are all predicting the same output y.
The predicted output of the network, which corresponds to each of the weak learners wT ˜
correspond to yi
t on lines 8 and 9 of Algorithm 2. Through this setup, it allows each residual module,
Ai to be updated by back propagation with respect to the label y in the same manner as line 12 in
Algorithm 1. In a similar manner the shrinkage layers θi in Algorithm 2 would be updated as shown
in Algorithm 1 as per line 13.
Through unravelling a ResNet, the paths of a ResNet are distributed in a binomial manner , that
is, there is one path that passes through n modules and n paths that go through one module, with
an average path length of n/2 . This means that even without a shared layer, there will be paths
within the ResNet framework where the gradient is recovered to the residual modules. This approach
is shown by ﬁgure 2, and has an identical setup as algorithm 2 except in the back propagation step,
we update all layers based on the whole network using output yN
Algorithm 2 Online Boosting Algorithm as ResNet with Shrinkage
1: Maintain N ResNet Modules ˜
A, N shrinkage layers θ, linear classiﬁer layer w and choose step
size parameter η ∈[ 1
N , 1], constructed as per Figure 2
2: For each i, initialize shrinkage layer θi = 0.
3: Deﬁne F0
4: for t = 1 to T do
{Feed Forward}
Receive example xt
for i = 1 to N do
t = (1 −θi)Fi−1
Predict and output yi
t = ψ−1(Fi
{Back Propagation}
for i = 1 to N do
Update all layers in the subnetwork up to ResNet module i via back propagation using the
ﬁnal prediction output yi
15: end for
Remark: if a residual network is reframed as the Online Boosting Algorithm 1, it would be equivalent
to choosing η = 1, with θi = 0 being ﬁxed or untrainable. For the regret bounds to hold, we require
shrinkage parameter θi to be trainable, and the outputs of each residual module to be bounded by a
predetermined max-norm.
In Section 6 we will provide empirical evidence validating both approaches.
Neural Decision Tree
Another popular application of boosting algorithms is through the construction of decision trees. In
order to demonstrate how ResNet could be used to boost a variety of models with different residual
module representations, we describe our construction for our Neural Decision Tree ResNet and the
associated generalization error analysis.
Construction of Neural Decision Tree and Generalization Error Analysis
To demonstrate decision tree formulation based on Deep Neural Decision Forests belongs to this
family of neural network models, consider the residual module is shown by Figure 5, where the
split functions are realized by randomized multi-layer perceptrons . This construction is a neural
network has 3 sets of layers that belongs to family of artiﬁcial neural networks deﬁned by ; which
require the weights of each layer to be bounded by lp-norm, with p ≥1, and all activation functions
between each layer to be coordinate-wise and 1-Lipschitz activation functions. The size of these
layers are based on a predetermined number of nodes n with a corresponding number of leaves
ℓ= n + 1. Let the input space be X and for any x ∈X, let h0 ∈Rk denote the corresponding
feature vector.
The ﬁrst layer is decision node layer. This is deﬁned by trainable parameters Θ = {W, b}, with
W ∈Rk×n and b ∈Rn. Deﬁne ˜W = [W ⊕−W] and ˜b = [b ⊕−b], which represent the positive
and negative routes of each node. Then the output of the ﬁrst layer is H1(x) = ˜W ⊤x + ˜b. This is
interpreted as the linear decision boundary which dictates how each node is to be routed.
The next is the probability routing layer, which are all untrainable, and are a predetermined binary
matrix Q ∈R2n×(n+1). This matrix is constructed to deﬁne an explicit form for routing within a
decision tree. We observe that routes in a decision tree are ﬁxed and pre-determined. We introduce
a routing matrix Q which is a binary matrix which describes the relationship between the nodes
and the leaves. If there are n nodes and ℓleaves, then Q ∈{0, 1}(ℓ×2n), where the rows of Q
represents the presence of each binary decision of the n nodes for the corresponding leaf ℓ. We deﬁne
the activation function to be φ2(x) = (log ◦softmax)(x). Then the output of the second layer is
H2(x) = Q⊤(φ2 ◦H1)(x). As log(x) is 1-Lipschitz bounded function in the domain (0, 1) and the
Figure 2: The architecture of a modiﬁed residual network two modules and with shrinkage layers.
Figure 3: Left: Iris Decision Tree by Scikit-Learn, Right: Corresponding Parameters for our Neural
Network. Changing the softmax function to a deterministic routing will yield precisely the same
result as the Scikit-Learn decision tree.
range of softmax ∈(0, 1), then by extension, φ2(x) is a 1-Lipschitz bounded function for x ∈R. As
Q is a binary matrix, then the output of H2(x) must also be in the range (−∞, 0).
Figure 4: Decision Tree as a three layer Neural Network. The Neural Network has two trainable
layers: the decision tree nodes, and the leaf nodes.
The ﬁnal output layer is the leaf layer, this is a fully connected layer to the previous layer, which is
deﬁned by parameter π ∈Rn+1, which represents the number of leaves. The activation function
is deﬁned to be φ3(x) = exp(x). The the output of the last layer is deﬁned to be H3(x) =
π⊤(φ3 ◦H2(x)). Since H2(x) has range (−∞, 0), then φ3(x) is a 1-Lipschitz bounded function as
exp(x) is 1-Lipschitz bounded in the domain (−∞, 0). As each activation function is 1-Lipschitz
functions, then our decision tree neural network belongs to the same family of artiﬁcial neural
networks deﬁned by , and thus our decision trees have the corresponding generalisation error
bounds related to AdaNet.
The formulation of these equations and their parameters is shown in ﬁgure 3 which demonstrates
how a decision tree trained in Python Scikit-Learn can have its parameters be converted to a neural
decision tree, and ﬁgure 4 demonstrates the formulation of the three layer network which constructs
this decision tree.
Extending Neural Decision Trees to ResNet
For our Neural Decison Tree ResNet, in order to ensure that the feature representation is invariant to
the number of leaves in the tree, we add a linear projection to ensure that the shortcut connection
match the dimensions, as suggested in the original ResNet implementation .
In this way, we have demonstrated construction of our variations of residual modules retain generalization bounds proved by and retain true out-of-core online boosted learning, compared with
other existing algorithms such as BoostResNet .
Experiments
Below, we perform experiments on two different ResNet architectures.
First, we examine the ResNet convolution network variant , with and without the addition of
trainable shrinkage parameter. Both models are assessed over street view house numbers SVHN ,
and CIFAR-10 benchmark datasets.
Second, we examine the efﬁcacy of creating boosted decision tree models in ResNet framework.
Our approach was compared against other neural decision tree ensemble models and ofﬂine models
Figure 5: Decision Tree Residual Module based on Decision Tree Algorithm in “Deep Neural
Decision Forests” 
Table 1: Accuracies of SVHN Task. All trained with same number of iterations (200 epoch, with
learning schedule as deﬁned in original ResNet model.)
RESNET (SHRINKAGE)
RESNET (SHARED)
including Deep Neural Decision Forests , neural decision trees ensembled via AdaNet , and
off-the-shelf algorithms (gradient boosting decision tree/random forest) using LightGBM . All
models were assess using UCI datasets which are detailed in Section B of the appendix.
In both scenarios, the datasets were divided using a 70 : 30 split into a training and test dataset
respectively.
Convolution Network ResNet
In both the CIFAR-10 and SVHN datasets we ﬁt the same 20-layer ResNet. This ResNet consists of
one 3 × 3 convolution, followed by stacks of 18 layers with 3 × 3 convolutions of the feature maps
sizes of {32, 16, 8} respectively, with 6 layers for each feature map size. The number of ﬁlters are
{16, 32, 64}. The subsampling is performed by convolutions with a stride of 2 and the network ends
with global average pooling, a 10-way fully connected layer and softmax. The implementation is
taken directly from the Keras CIFAR-10 ResNet sample code. The model was run without image
augmentation, and with a batch size of 32 for 200 epochs. To compare the original ResNet, we
augment the ResNet model by adding a trainable shrinkage parameter as described in Section 5
(ResNet-Shrinkage), and our augmented ResNet model with both shrinkage parameter and shared
linear layer (ResNet-Shared).
We ﬁnd that the model with shrinkage only has marginally higher accuracy than the vanilla ResNet-20
implementation in both datasets. For the ResNet-Shared model, it is comparable to the SVHN task,
however falls short in the CIFAR-10 task. In general, adding shrinkage does not impact performance
of ResNet models and in certain cases, it improves the performance.
Table 2: Accuracies of CIFAR-10 Task. All trained with same number of iterations (200 epoch, with
learning schedule as deﬁned in original ResNet model.)
RESNET (SHRINKAGE)
RESNET (SHARED)
Neural Decision Tree ResNet
The next experiment conducted was to address whether ResNet could be used to boost a variety of
models with different residual module representations. We compared our decision tree in ResNet
(ResNet-DT), and ResNet with shared linear classiﬁer layer (ResNet-DT Shared) with Deep Neural
Decision Forests (DNDF), neural decision trees ensembled via AdaNet (AdaNet-DT), and
off-the-shelf algorithms (gradient boosting decision tree/random forest) using LightGBM which
we denote as LightGBDT, LightRF respectively.
For ResNet-DT, ResNet-DT Shared, DNDF, LightGBM and LightRF, all models used an ensemble of
15 trees with a maximum depth of 5 (i.e. 32 nodes). For each of these models, they were run for 200
For AdaNet-DT, the candidate sub-networks used are decision trees identical to implementation
in DNDF. This means that at every iteration, a candidate neural decision tree was either added or
discarded with no change to the ensemble. The complexity measure function r was deﬁned to be
d(h) where d is the number of hidden layers (i.e. number of nodes) in the decision tree
 . For AdaNet-DT, the algorithm started with 1 tree, and was run 14 times with 20 epoch per
iteration, allowing AdaNet to build up to 15 trees. Once the ﬁnal neural network structure was chosen,
it was run for another 200 epoch and used for comparison with the other models.
To assess the efﬁcacy, we used a variety of datasets from the UCI repository. Full results for the
training and test data sets are provided in section B of the appendix.
Figure 6: Boxplot of Relative Performance with Deep Neural Decision Forest Model as Baseline on
train dataset. High values indicate better performance.
In order to construct a baseline for all models to be comparable, the results presented are on the
average and median error improvement compared with DNDF models, as they were the worse
performing model based on these benchmarks. From the results in Table 4, LightGBM performed
the best with the best average improvement on error relative to the baseline DNDF model. What is
interesting is that both our ResNet-DT model performed second best, beating LightRF and AdaNet-
DT models.It is important to note that our setup for AdaNet-DT only allowed a “bushy” candidate
model, this did not allow AdaNet-DT to build deeper layers compared with ResNet-DT approach;
Figure 7: Boxplot of Relative Performance with Deep Neural Decision Forest Model as Baseline on
test dataset. High values indicate better performance.
Table 3: Mean Improvement compared with Deep Neural Decision Forest Model and Mean Reciprocal
Rank on train datasets.
RECIPROCAL RANK
RESNET-DT (SHARED)
only allowing it to build a wider and shallow architect through appending additional decision trees.
Despite this, the AdaNet-DT implementation did outperform the DNDF implementation.
When examining relative improvement, it is important to understand how the values are then distributed. Figures 6 and 7 contain the boxplots of relative performance based on the train and test
datasets respectively. From our empirical experiments, it suggests that the difference between the
ResNet-DT and ResNet-DT Shared are around the variance in the results. One interpretation is
through joint training, the variability in performance is lowered and may possible provide more stable
models. As to whether joint training should be used or not, we believe it should be considered to be a
optional parameter that is learned in training time instead.
In general, it would appear our ResNet-DT performance is comparable to LightGBM models whilst
providing the ability to update the tree ensemble in an online manner and producing non-greedy
decision splits. As this approach can be performed in an online or mini-batch manner, it can be used
to incrementally update and train over large datasets compared with LightGBM models which can
only operate in an ofﬂine manner.
Weak Learning Condition Check
We present a summarised proof demonstrating ResNet-CNN and ResNet-DT satisfy the weak learning
condition as stated in Assumption 3.1. The full proof is provided in Section A of the appendix.
For both cases, it is sufﬁcient to demonstrate that there exists a parameterization such that
the residual module f(x) = x.
Applying this parameterization over the recursive relation
gT +1(x) = PT
i=0 fi(gi(x)), suggests there exists a parameterization of the residual module ˜g
such that w⊤˜gT +1(x) ∝w⊤x. As w is a learnable weight and a linear model, which is a known
weak learner , demonstrating that hypothesis modules created through residual modules are weak
Table 4: Mean Improvement compared with Deep Neural Decision Forest Model and Mean Reciprocal
Rank on test datasets.
RECIPROCAL RANK
RESNET-DT (SHARED)
Figure 8: A ResNet-CNN module as deﬁned in 
ResNet-CNN: We will brieﬂy demonstrate that with dense layers in a ResNet setup can recover
the identity. We defer demonstrating convolutional layers scenario to section A of the appendix.
We will ignore the batch normalization function in ResNet, noting that batch normalization layer with
centering value of 0 and scale of 1 is a valid parameterization. As such the residual module can be
expressed as
f(x) ≡W ⊤σ(V ⊤x + Vb) + Wb
Where W, V are the appropriate weights matrices with Wb, Vb being the respective biases and
σ is ReLu activation. Suppose W, V is chosen to be the identity matrix and Wb is chosen to be
a matrix containing a single value representing min(x, 0), and Vb = −Wb. Hence there exists a
parameterization of ResNet-CNN where f(x) = x as required.
Remark: ResNet built under constraints of a linear residual module with only convolution layers
and ReLu activations have been shown to have perfect ﬁnite sample expressivity; which is a much
stronger condition than recovering only the identity .
ResNet-DT: The weak learning condition can be trivially demonstrated through routing the input
x in a deterministic manner to a single leaf with probability 1. Under this condition the ﬁnal linear
projection layer, project only the target leaf, would result in an identity mapping. This demonstrates
a decision tree which routes only to one leaf will have a parameterization f(x) = x. This can also
interpreted as a “decision stump” which is commonly used in boosting applications.
Conclusions and Future Work
We have demonstrated the equivalence between ResNet and Online Boosting algorithm, and provided
a regret bound for ResNet based on the interpretation of residual modules with the linear classiﬁer
as weak learners. We have proposed the addition of shrinkage parameters to ResNet, which based
on initial results demonstrating it as a promising approach in reﬁning ResNet models. We have also
demonstrated a method to remove “ofﬂine” restriction of BoostResNet of requiring maintaining
distribution of all training data weights through extending it to an online gradient boosting algorithm.
Together these provide insight into the interpretation of ResNet as well as extensions of residual
modules to new and novel feature representations, such as neural decision trees. These representations
allow us to create new boosting variations of decision trees. We have additionally demonstrated that
this approach is superior to other neural network decision tree ensemble variants and comparable
with state-of-the-art ofﬂine variations without the drawbacks of ofﬂine approaches. In addition we
have also provided generalization bounds for our residual module implementations. The insights into
the relation between boosting and ResNet could spur other changes to the default ResNet architecture,
such as challenging the default size of the step parameter in the identity skip-connect. These insights
may also change how residual modules are optimized and built, and encourage developments into
new residual modules architectures.