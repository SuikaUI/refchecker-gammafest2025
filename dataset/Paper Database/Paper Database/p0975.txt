Panoptic Feature Pyramid Networks
Alexander Kirillov
Ross Girshick
Kaiming He
Piotr Doll´ar
Facebook AI Research (FAIR)
The recently introduced panoptic segmentation task has
renewed our community’s interest in unifying the tasks of
instance segmentation (for thing classes) and semantic segmentation (for stuff classes).
However, current state-ofthe-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation,
without performing any shared computation. In this work,
we aim to unify these methods at the architectural level,
designing a single network for both tasks. Our approach
is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using
a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for
instance segmentation, but also yields a lightweight, topperforming method for semantic segmentation. In this work,
we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for
both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and
aid future research in panoptic segmentation.
1. Introduction
Our community has witnessed rapid progress in semantic segmentation, where the task is to assign each pixel a
class label (e.g. for stuff classes), and more recently in instance segmentation, where the task is to detect and segment
each object instance (e.g. for thing classes). These advances
have been aided by simple yet powerful baseline methods,
including Fully Convolutional Networks (FCN) and
Mask R-CNN for semantic and instance segmentation,
respectively. These methods are conceptually simple, fast,
and ﬂexible, serving as a foundation for much of the subsequent progress in these areas. In this work our goal is
to propose a similarly simple, single-network baseline for
the joint task of panoptic segmentation , a task which
encompasses both semantic and instance segmentation.
While conceptually straightforward, designing a single network that achieves high accuracy for both tasks is
(a) Feature Pyramid Network
(b) Instance Segmentation Branch
(c) Semantic Segmentation Branch
Figure 1: Panoptic FPN: (a) We start with an FPN backbone , widely used in object detection, for extracting
rich multi-scale features.
(b) As in Mask R-CNN ,
we use a region-based branch on top of FPN for instance
segmentation. (c) In parallel, we add a lightweight denseprediction branch on top of the same FPN features for semantic segmentation. This simple extension of Mask R-
CNN with FPN is a fast and accurate baseline for both tasks.
challenging as top-performing methods for the two tasks
have many differences. For semantic segmentation, FCNs
with specialized backbones enhanced by dilated convolutions dominate popular leaderboards . For
instance segmentation, the region-based Mask R-CNN 
with a Feature Pyramid Network (FPN) backbone
has been used as a foundation for all top entries in recent recognition challenges . While there have
been attempts to unify semantic and instance segmentation
 , the specialization currently necessary to achieve
top performance in each was perhaps inevitable given their
parallel development and separate benchmarks.
Given the architectural differences in these top methods,
one might expect compromising accuracy on either instance
or semantic segmentation is necessary when designing a
single network for both tasks. Instead, we show a simple,
ﬂexible, and effective architecture that can match accuracy
for both tasks using a single network that simultaneously
generates region-based outputs (for instance segmentation)
and dense-pixel outputs (for semantic segmentation).
 
Figure 2: Panoptic FPN results on COCO (top) and Cityscapes (bottom) using a single ResNet-101-FPN network.
Our approach starts with the FPN backbone popular for instance-level recognition and adds a branch
for performing semantic segmentation in parallel with the
existing region-based branch for instance segmentation, see
Figure 1. We make no changes to the FPN backbone when
adding the dense-prediction branch, making it compatible
with existing instance segmentation methods. Our method,
which we call Panoptic FPN for its ability to generate both
instance and semantic segmentations via FPN, is easy to implement given the Mask R-CNN framework .
While Panoptic FPN is an intuitive extension of Mask R-
CNN with FPN, properly training the two branches for simultaneous region-based and dense-pixel prediction is important for good results.
We perform careful studies in
the joint setting for how to balance the losses for the two
branches, construct minibatches effectively, adjust learning
rate schedules, and perform data augmentation. We also explore various designs for the semantic segmentation branch
(all other network components follow Mask R-CNN). Overall, while our approach is robust to exact design choices,
properly addressing these issues is key for good results.
When trained for each task independently, our method
achieves excellent results for both instance and semantic
segmentation on both COCO and Cityscapes . For
instance segmentation, this is expected as our method in this
case is equivalent to Mask R-CNN. For semantic segmentation, our simple dense-prediction branch attached to FPN
yields accuracy on par with the latest dilation-based methods, such as the recent DeepLabV3+ .
For panoptic segmentation , we demonstrate that
with proper training, using a single FPN for solving both
tasks simultaneously yields accuracy equivalent to training
two separate FPNs, with roughly half the compute. With
the same compute, a joint network for the two tasks outperforms two independent networks by a healthy margin.
Example panoptic segmentation results are shown in Fig. 2.
Panoptic FPN is memory and computationally efﬁcient,
incurring only a slight overhead over Mask R-CNN. By
avoiding the use of dilation, which has high overhead, our
method can use any standard top-performing backbone (e.g.
a large ResNeXt ). We believe this ﬂexibility, together
with the fast training and inference speeds of our method,
will beneﬁt future research on panoptic segmentation.
We used a preliminary version of our model (semantic
segmentation branch only) as the foundation of the ﬁrstplace winning entry in the COCO Stuff Segmentation 
track in 2017. This single-branch model has since been
adopted and generalized by several entries in the 2018
COCO and Mapillary Challenges1, showing its ﬂexibility
and effectiveness. We hope our proposed joint panoptic segmentation baseline is similarly impactful.
2. Related Work
Panoptic segmentation: The joint task of thing and stuff
segmentation has a rich history, including early work on
scene parsing , image parsing , and holistic scene
understanding . With the recent introduction of the joint
panoptic segmentation task , which includes a simple
task speciﬁcation and carefully designed task metrics, there
has been a renewed interest in the joint task.
This year’s COCO and Mapillary Recognition Challenge featured panoptic segmentation tracks that
proved popular. However, every competitive entry in the
panoptic challenges used separate networks for instance
and semantic segmentation, with no shared computation.1
Our goal is to design a single network effective for both
tasks that can serve as a baseline for future work.
1For details of not yet published winning entries in the 2018 COCO and
Mapillary Recognition Challenge please see: 
org/workshop/coco-mapillary-eccv-2018.html. TRI-ML
used separate networks for the challenge but a joint network in their recent
updated tech report (which cites a preliminary version of our work).
Instance segmentation: Region-based approaches to object detection, including the Slow/Fast/Faster/Mask R-CNN
family , which apply deep networks on
candidate object regions, have proven highly successful.
All recent winners of the COCO detection challenges have
built on Mask R-CNN with FPN , including in
2017 and 2018.1 Recent innovations include Cascade R-CNN , deformable convolution , and sync
batch norm . In this work, the original Mask R-CNN
with FPN serves as the starting point for our baseline, giving
us excellent instance segmentation performance, and making our method fully compatible with these recent advances.
An alternative to region-based instance segmentation is
to start with a pixel-wise semantic segmentation and then
perform grouping to extract instances . This direction is innovative and promising. However, these methods tend to use separate networks to predict the instancelevel information (e.g., use a separate network to
predict instance edges, bounding boxes, and object breakpoints, respectively). Our goal is to design a single network
for the joint task. Another interesting direction is to use
position-sensitive pixel labeling to encode instance information fully convolutionally; build on this.
Nevertheless, region-based approaches remain dominant
on detection leaderboards . While this motivates us to start with a region-based approach to instance
segmentation, our approach would be fully compatible with
a dense-prediction branch for instance segmentation.
Semantic segmentation: FCNs serve as the foundation of modern semantic segmentation methods. To increase
feature resolution, which is necessary for generating highquality results, recent top methods rely heavily on the use of dilated convolution (also known as
atrous convolution ). While effective, such an approach
can substantially increase compute and memory, limiting
the type of backbone network that can be used. To keep this
ﬂexibility, and more importantly to maintain compatibility
with Mask R-CNN, we opt for a different approach.
As an alternative to dilation, an encoder-decoder or
‘U-Net’ architecture can be used to increase feature
resolution . Encoder-decoders progressively
upsample and combine high-level features from a feedforward network with features from lower-levels, ultimately
generating semantically meaningful, high-resolution features (see Figure 5). While dilated networks are currently
more popular and dominate leaderboards, encoder-decoders
have also been used for semantic segmentation .
In our work we adopt an encoder-decoder framework,
namely FPN . In contrast to ‘symmetric’ decoders ,
FPN uses a lightweight decoder (see Fig. 5). FPN was designed for instance segmentation, and it serves as the default
backbone for Mask R-CNN. We show that without changes,
FPN can also be highly effective for semantic segmentation.
Multi-task learning: Our approach is related to multi-task
learning. In general, using a single network to solve multiple diverse tasks degrades performance , but various
strategies can mitigate this . For related tasks, there
can be gains from multi-task learning, e.g. the box branch in
Mask R-CNN beneﬁts from the mask branch , and joint
detection and semantic segmentation of thing classes also
shows gains . Our work studies the beneﬁts of
multi-task training for stuff and thing segmentation.
3. Panoptic Feature Pyramid Network
Our approach, Panoptic FPN, is a simple, single-network
baseline whose goal is to achieve top performance on both
instance and semantic segmentation, and their joint task:
panoptic segmentation . Our design principle is to start
from Mask R-CNN with FPN, a strong instance segmentation baseline, and make minimal changes to also generate a
semantic segmentation dense-pixel output (see Figure 1).
3.1. Model Architecture
Feature Pyramid Network: We begin by brieﬂy reviewing FPN . FPN takes a standard network with features
at multiple spatial resolutions (e.g., ResNet ), and adds
a light top-down pathway with lateral connections, see Figure 1a. The top-down pathway starts from the deepest layer
of the network and progressively upsamples it while adding
in transformed versions of higher-resolution features from
the bottom-up pathway. FPN generates a pyramid, typically
with scales from 1/32 to 1/4 resolution, where each pyramid
level has the same channel dimension (256 by default).
Instance segmentation branch: The design of FPN, and
in particular the use of the same channel dimension for all
pyramid levels, makes it easy to attach a region-based object
detector like Faster R-CNN . Faster R-CNN performs
region of interest (RoI) pooling on different pyramid levels
and applies a shared network branch to predict a reﬁned box
and class label for each region. To output instance segmentations, we use Mask R-CNN , which extends Faster
R-CNN by adding an FCN branch to predict a binary segmentation mask for each candidate region, see Figure 1b.
Panoptic FPN: As discussed, our approach is to modify
Mask R-CNN with FPN to enable pixel-wise semantic segmentation prediction. However, to achieve accurate predictions, the features used for this task should: (1) be of suitably high resolution to capture ﬁne structures, (2) encode
sufﬁciently rich semantics to accurately predict class labels,
and (3) capture multi-scale information to predict stuff regions at multiple resolutions. Although FPN was designed
for object detection, these requirements – high-resolution,
rich, multi-scale features – identify exactly the characteristics of FPN. We thus propose to attach to FPN a simple and
fast semantic segmentation branch, described next.
conv→2×→conv→2×→conv→2×
256 × 1/16
conv→2×→conv→2×
Figure 3: Semantic segmentation branch. Each FPN level
(left) is upsampled by convolutions and bilinear upsampling
until it reaches 1/4 scale (right), theses outputs are then
summed and ﬁnally transformed into a pixel-wise output.
Semantic segmentation branch: To generate the semantic segmentation output from the FPN features, we propose
a simple design to merge the information from all levels of
the FPN pyramid into a single output. It is illustrated in
detail in Figure 3. Starting from the deepest FPN level (at
1/32 scale), we perform three upsampling stages to yield
a feature map at 1/4 scale, where each upsampling stage
consists of 3×3 convolution, group norm , ReLU, and
2× bilinear upsampling. This strategy is repeated for FPN
scales 1/16, 1/8, and 1/4 (with progressively fewer upsampling stages). The result is a set of feature maps at the same
1/4 scale, which are then element-wise summed. A ﬁnal
1×1 convolution, 4× bilinear upsampling, and softmax are
used to generate the per-pixel class labels at the original image resolution. In addition to stuff classes, this branch also
outputs a special ‘other’ class for all pixels belonging to objects (to avoid predicting stuff classes for such pixels).
Implementation details: We use a standard FPN conﬁguration with 256 output channels per scale, and our semantic segmentation branch reduces this to 128 channels. For
the (pre-FPN) backbone, we use ResNet/ResNeXt 
models pre-trained on ImageNet using batch norm
(BN) . When used in ﬁne-tuning, we replace BN with a
ﬁxed channel-wise afﬁne transformation, as is typical .
3.2. Inference and Training
Panoptic inference: The panoptic output format requires each output pixel to be assigned a single class label
(or void) and instance id (the instance id is ignored for stuff
classes). As the instance and semantic segmentation outputs
from Panoptic FPN may overlap; we apply the simple postprocessing proposed in to resolve all overlaps. This
post-processing is similar in spirit to non-maximum suppression and operates by: (1) resolving overlaps between
different instances based on their conﬁdence scores, (2) resolving overlaps between instance and semantic segmentation outputs in favor of instances, and (3) removing any stuff
regions labeled ‘other’ or under a given area threshold.
Mult-adds ×1012
Symmetric decoder
Output scale
Activations ×109
Output scale
Figure 4: Backbone architecture efﬁciency.
We compare methods for increasing feature resolution for semantic segmentation, including dilated networks, symmetric decoders, and FPN, see Figure 5. We count multiply-adds and
memory used when applying ResNet-101 to a 2 megapixel
FPN at output scale 1/4 is similar computationally to dilation-16 (1/16 resolution output), but produces a
4× higher resolution output. Increasing resolution to 1/8
via dilation uses a further ∼3× more compute and memory.
Joint training: During training the instance segmentation
branch has three losses : Lc (classiﬁcation loss), Lb
(bounding-box loss), and Lm (mask loss). The total instance
segmentation loss is the sum of these losses, where Lc and
Lb are normalized by the number of sampled RoIs and Lm is
normalized by the number of foreground RoIs. The semantic segmentation loss, Ls, is computed as a per-pixel cross
entropy loss between the predicted and the ground-truth labels, normalized by the number of labeled image pixels.
We have observed that the losses from these two
branches have different scales and normalization policies.
Simply adding them degrades the ﬁnal performance for
one of the tasks. This can be corrected by a simple loss
re-weighting between the total instance segmentation loss
and the semantic segmentation loss. Our ﬁnal loss is thus:
L = λi (Lc + Lb + Lm) + λsLs. By tuning λi and λs it
is possible to train a single model that is comparable to two
separate task-speciﬁc models, but at about half the compute.
3.3. Analysis
Our motivation for predicting semantic segmentation using FPN is to create a simple, single-network baseline
that can perform both instance and semantic segmentation.
However, it is also interesting to consider the memory and
computational footprint of our approach relative to model
architectures popular for semantic segmentation. The most
common designs that produce high-resolution outputs are
dilated convolution (Figure 5b) and symmetric encoderdecoder models that have a mirror image decoder with lateral connections (Figure 5c). While our primary motivation is compatibility with Mask R-CNN, we note that FPN
is much lighter than a typically used dilation-8 network,
∼2× more efﬁcient than the symmetric encoder-decoder,
and roughly equivalent to a dilation-16 network (while producing a 4× higher resolution output). See Figure 4.
b5×1024×1/32
b4×512×1/16
b3×256×1/8
(a) Original
(b) +Dilation
(c) +Symmetric Decoder
(d) +Assymetric Decoder (FPN)
b5×1024×1/8
b4×512×1/8
b3×256×1/8
b5×1024×1/32
b4×512×1/16
b3×256×1/8
b5×1024×1/32
b4×512×1/16
b3×256×1/8
b5×1024×1/32
b4×512×1/16
b3×256×1/8
1×256×1/32
1×256×1/16
Figure 5: Backbone architectures for increasing feature resolution. (a) A standard convolutional network (dimensions are
denoted as #blocks×#channels×resolution). (b) A common approach is to reduce the stride of select convolutions and use
dilated convolutions after to compensate. (c) A U-Net style network uses a symmetric decoder that mirrors the bottom-up
pathway, but in reverse. (d) FPN can be seen as an asymmetric, lightweight decoder whose top-down pathway has only one
block per stage and uses a shared channel dimension. For a comparison of the efﬁciency of these models, please see Figure 4.
4. Experiments
Our goal is to demonstrate that our approach, Panoptic
FPN, can serve as a simple and effective single-network
baseline for instance segmentation, semantic segmentation,
and their joint task of panoptic segmentation . For instance segmentation, this is expected, since our approach
extends Mask R-CNN with FPN. For semantic segmentation, as we simply attach a lightweight dense-pixel prediction branch (Figure 3) to FPN, we need to demonstrate it
can be competitive with recent methods. Finally, we must
show that Panoptic FPN can be trained in a multi-task setting without loss in accuracy on the individual tasks.
We therefore begin our analysis by testing our approach
for semantic segmentation (we refer to this single-task variant as Semantic FPN). Surprisingly, this simple model
achieves competitive semantic segmentation results on the
COCO and Cityscapes datasets. Next, we analyze the integration of the semantic segmentation branch
with Mask R-CNN, and the effects of joint training. Lastly,
we show results for panoptic segmentation, again on COCO
and Cityscapes. Qualitative results are shown in Figures 2
and 6. We describe the experimental setup next.
4.1. Experimental Setup
COCO: The COCO dataset was developed with a focus on instance segmentation, but more recently stuff annotations were added . For instance segmentation, we
use the 2017 data splits with 118k/5k/20k train/val/test images and 80 thing classes. For semantic segmentation, we
use the 2017 stuff data with 40k/5k/5k splits and 92 stuff
classes. Finally, panoptic segmentation uses all 2017
COCO images with 80 thing and 53 stuff classes annotated.
Cityscapes: Cityscapes is an ego-centric street-scene
dataset. It has 5k high-resolution images with ﬁne pixel-accurate annotations: 2975 train, 500
val, and 1525 test. An additional 20k images with coarse
annotations are available, we do not use these in our experiments. There are 19 classes, 8 with instance-level masks.
Single-task metrics: We report standard semantic and instance segmentation metrics for the individual tasks using
evaluation code provided by each dataset. For semantic segmentation, the mIoU (mean Intersection-over-Union) 
is the primary metric on both COCO and Cityscapes. We
also report fIoU (frequency weighted IoU) on COCO 
and iIoU (instance-level IoU) on Cityscapes . For instance segmentation, AP (average precision averaged over
categories and IoU thresholds) is the primary metric
and AP50 and AP75 are selected supplementary metrics.
Panoptic segmentation metrics: We use PQ (panoptic
quality) as the default metric to measure Panoptic FPN performance, for details see . PQ captures both recognition
and segmentation quality, and treats both stuff and thing categories in a uniﬁed manner. This single, uniﬁed metric allows us to directly compare methods. Additionally, we use
PQSt and PQTh to report stuff and thing performance separately. Note that PQ is used to evaluate Panoptic FPN predictions after the post-processing merging procedure is applied to the outputs of the semantic and instance branches.
COCO training: We use the default Mask R-CNN 1×
training setting with scale jitter (shorter image side in
 ). For semantic segmentation, we predict 53 stuff
classes plus a single ‘other’ class for all 80 thing classes.
Cityscapes training: We construct each minibatch from
32 random 512×1024 image crops (4 crops per GPU) after randomly scaling each image by 0.5 to 2.0×. We train
for 65k iterations starting with a learning rate of 0.01 and
dropping it by a factor of 10 at 40k and 55k iterations. This
differs from the original Mask R-CNN setup but is effective for both instance and semantic segmentation. For the
largest backbones for semantic segmentation, we perform
color augmentation and crop bootstrapping . For
semantic segmentation, predicting all thing classes, rather
than a single ‘other’ label, performs better (for panoptic inference we discard these predictions). Due to the high variance of the mIoU (up to 0.4), we report the median performance of 5 trials of each experiment on Cityscapes.
DeeplabV3 
ResNet-101-D8
PSANet101 
ResNet-101-D8
Mapillary 
WideResNet-38-D8
DeeplabV3+ 
Semantic FPN
ResNet-101-FPN
Semantic FPN
ResNeXt-101-FPN
(a) Cityscapes Semantic FPN. Performance is reported on the val set and
all methods use only ﬁne Cityscapes annotations for training. The backbone notation includes the dilated resolution ‘D’ (note that uses both
dilation and an encoder-decoder backbone). All top-performing methods
other than ours use dilation. FLOPs (multiply-adds ×1012) and memory
(# activations ×109) are approximate but informative. For these larger
FPN models we train with color and crop augmentation. Our baseline is
comparable to state-of-the-art methods in accuracy and efﬁciency.
Vllab 
Stacked Hourglass
DeepLab VGG16 
Oxford 
ResNeXt-101
G-RMI 
Inception ResNet v2
Semantic FPN
ResNeXt-152-FPN
(b) COCO-Stuff 2017 Challenge results. We submitted an early version
of Semantic FPN to the 2017 COCO Stuff Segmentation Challenge held
at ECCV . Our entry won ﬁrst place without ensembling, and we outperformed competing
methods by at least a 2 point margin on all reported metrics.
Cityscapes
(c) Ablation (mIoU): Channel width
of 128 for the features in the semantic branch strikes a good balance between accuracy and efﬁciency.
Cityscapes COCO
(d) Ablation (mIoU): Sum aggregation of the feature maps in the
semantic branch is marginally better and is more efﬁcient.
Table 1: Semantic Segmentation using FPN.
4.2. FPN for Semantic Segmentation
Cityscapes: We start by comparing our baseline Semantic FPN to existing methods on the Cityscapes val split
in Table 1a. We compare to recent top-performing methods, but not to competition entires which typically use ensembling, COCO pre-training, test-time augmentation, etc.
Our approach, which is a minimal extension to FPN, is
able to achieve strong results compared to systems like
DeepLabV3+ , which have undergone many design iterations. In terms of compute and memory, Semantic FPN is
lighter than typical dilation models, while yielding higher
resolution features (see Fig. 4). We note that adding dilation into FPN could potentially yield further improvement
but is outside the scope of this work. Moreover, in our baseline we deliberately avoid orthogonal architecture improvements like Non-local or SE , which would likely
yield further gains. Overall, these results demonstrate that
our approach is a strong baseline for semantic segmentation.
COCO: An earlier version of our approach won the 2017
COCO-Stuff challenge. Results are reported in Table 1b.
As this was an early design, the the semantic branch differed slightly (each upsampling module had two 3×3 conv
layers and ReLU before bilinear upscaling to the ﬁnal resolution, and features were concatenated instead of summed,
please compare with Figure 3). As we will show in the ablations shortly, results are fairly robust to the exact branch
design. Our competition entry was trained with color augmentation and at test time balanced the class distribution and used multi-scale inference. Finally, we note that
at the time we used a training schedule speciﬁc to semantic
segmentation similar to our Cityscapes schedule (but with
double learning rate and halved batch size).
Ablations: We perform a few ablations to analyze our proposed semantic segmentation branch (shown in Figure 3).
For consistency with further experiments in our paper, we
use stuff annotations from the COCO Panoptic dataset
(which as discussed differ from those used for the COCO
Stuff competition).
Table 1c shows ResNet-50 Semantic FPN with varying number of channels in the semantic
branch. We found that 128 strikes a good balance between
accuracy and efﬁciency. In Table 1d we compare elementwise sum and concatenation for aggregating feature maps
from different FPN levels. While accuracy for both is comparable, summation is more efﬁcient. Overall we observe
that the simple architecture of the new dense-pixel labelling
branch is robust to exact design choices.
4.3. Multi-Task Training
Single-task performance of our approach is quite effective; for semantic segmentation the results in the previous
section demonstrate this, for instance segmentation this is
known as we start from Mask R-CNN. However, can we
jointly train for both tasks in a multi-task setting?
To combine our semantic segmentation branch with the
instance segmentation branch in Mask R-CNN, we need
to determine how to train a single, uniﬁed network. Previous work demonstrates that multi-task training is often
challenging and can lead to degraded results . We
likewise observe that for semantic or instance segmentation,
adding the secondary task can degrade the accuracy in comparison with the single-task baseline.
In Table 2 we show that with ResNet-50-FPN, using a
simple loss scaling weight on the semantic segmentation
loss, λs, or instance segmentation loss, λi, we can obtain
a re-weighting that improves results over single-task baselines. Speciﬁcally, adding a semantic segmentation branch
with the proper λs improves instance segmentation, and
vice-versa. This can be exploited to improve single-task
results. However, our main goal is to solve both tasks simultaneously, which we explore in the next section.
(a) Panoptic FPN on COCO for instance segmentation (λi = 1).
(b) Panoptic FPN on Cityscapes for instance segmentation (λi = 1).
(c) Panoptic FPN on COCO for semantic segmentation (λs = 1).
(d) Panoptic FPN on Cityscapes for semantic segmentation (λs = 1).
Table 2: Multi-Task Training: (a,b) Adding a semantic segmentation branch can slightly improve instance segmentation
results over a single-task baseline with properly tuned λs (results bolded). Note that λs indicates the weight assigned to the
semantic segmentation loss and λs = 0.0 serves as the single-task baseline. (c,d) Adding an instance segmentation branch
can provide even stronger beneﬁts for semantic segmentation over a single-task baseline with properly tuned λi (results
bolded). As before, λi indicates the weight assigned to the instance segmentation loss and λi = 0.0 serves as the single-task
baseline. While promising, we are more interested in the joint task, for which results are shown in Table 3.
Cityscapes
(a) Panoptic Segmentation: Panoptic R50-FPN vs. R50-FPN×2. Using a single FPN network for solving both tasks simultaneously yields
comparable accuracy to two independent FPN networks for instance
and semantic segmentation, but with roughly half the compute.
Cityscapes
(b) Panoptic Segmentation: Panoptic R101-FPN vs. R50-FPN×2.
Given a roughly equal computational budget, a single FPN network for
the panoptic task outperforms two independent FPN networks for instance and semantic segmentation by a healthy margin.
Cityscapes
(c) Training Panoptic FPN. During training, for each minibatch we can
either combine the semantic and instances loss or we can alternate which
loss we compute (in the latter case we train for twice as long). We ﬁnd
that combining the losses in each minibatch performs much better.
Cityscapes
(d) Grouped FPN. We test a variant of Panoptic FPN where we group
the 256 FPN channels into two sets and apply the instance and semantic
branch to its own dedicated group of 128. While this gives mixed gains,
we expect better multi-task strategies can improve results.
Table 3: Panoptic FPN Results.
Figure 6: More Panoptic FPN results on COCO (top) and Cityscapes (bottom) using a single ResNet-101-FPN network.
MPS-TU Eindhoven 
Panoptic FPN
(a) Panoptic Segmentation on COCO test-dev. We submit Panoptic FPN
to the COCO test-dev leaderboard (for details on competing entries, please
 
We only compare to entires that use a single network for the joint task.
We do not compare to competition-level entires that utilize ensembling
(including methods that ensemble separate networks for semantic and
instance segmentation). For methods that use one network for panoptic
segmentation, our approach improves PQ by an ∼9 point margin.
DIN 
Panoptic FPN
(b) Panoptic Segmentation on Cityscapes. For Cityscapes, there is no
public leaderboard for panoptic segmentation at this time. Instead, we compare on val to the recent work of Arnab and Torr who develop a novel
approach for panoptic segmentation, named DIN. DIN is representative of
alternatives to region-based instance segmentation that start with a pixelwise semantic segmentation and then perform grouping to extract instances
(see the related work). Panoptic FPN, without extra coarse training data or
any bells and whistles, outperforms DIN by a 4.3 point PQ margin.
Table 4: Comparisons of ResNet-101 Panoptic FPN to the state of the art.
4.4. Panoptic FPN
We now turn to our main result: testing Panoptic FPN
for the joint task of panoptic segmentation , where the
network must jointly and accurately output stuff and thing
segmentations. For the following experiments, for each setting we select the optimal λs and λi from {0.5, 0.75, 1.0},
ensuring that results are not skewed by ﬁxed choice of λ’s.
Main results: In Table 3a we compare two networks
trained separately to Panoptic FPN with a single backbone.
Panoptic FPN yields comparable accuracy but with roughly
half the compute (the backbone dominates compute, so the
reduction is almost 50%). We also balance computational
budgets by comparing two separate networks with ResNet-
50 backbones each and Panoptic FPN with ResNet-101,
see Table 3b. Using roughly equal computational budget,
Panoptic FPN signiﬁcantly outperforms two separate networks. Taken together, these results demonstrate that the
joint approach is strictly beneﬁcial, and that our Panoptic
FPN can serve as a solid baseline for the joint task.
Ablations: We perform additional ablations on Panoptic
FPN with ResNet-50. First, by default, we combine the
instance and semantic losses together during each gradient update. A different strategy is to alternate the losses on
each iteration (this may be useful as different augmentation
strategies can be used for the two tasks). We compare these
two options in Table 3c; the combined loss demonstrates
better performance. Next, in Table 3d we compare with an
architecture where FPN channels are grouped into two sets,
and each task uses one of the two features sets as its input.
While the results are mixed, we expect more sophisticated
multi-task approaches could give stronger gains.
Comparisons: We conclude by comparing Panoptic FPN
with existing methods.
For these experiments, we use
Panoptic FPN with a ResNet-101 backbone and without
bells-and-whistles. In Table 4a we show that Panoptic FPN
substantially outperforms all single-model entries in the recent COCO Panoptic Segmentation Challenge. This establishes a new baseline for the panoptic segmentation task. On
Cityscapes, we compare Panoptic FPN with an approach for
panoptic segmentation recently proposed in in Table 4b.
Panoptic FPN outperforms by a 4.3 point PQ margin.
5. Conclusion
We introduce a conceptually simple yet effective baseline for panoptic segmentation.
The method starts with
Mask R-CNN with FPN and adds to it a lightweight semantic segmentation branch for dense-pixel prediction. We
hope it can serve as a strong foundation for future research.