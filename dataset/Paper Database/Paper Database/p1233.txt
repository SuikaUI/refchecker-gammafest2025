WORKING PAPER
SOME NUMERICAL EXPERIMENTS WITH
VARIABLE STORAGE QUASI-NEWTON
ALGORITHMS
Jean Charles Gilbert
Claude LemarCchal
August 1988
I n t e r n a t i o n a l I n s t i t u t e
for Applied Systems Analysis
NOT FOR QUOTATION
WITHOUT PERMISSION
OF THE AUTHOR
SOME NUMERICAL EXPERIMENTS WITH
VARIABLE STORAGE QUASI-NEWTON
ALGORITHMS
Jean Charles Gilbert
Claude Lemare'chal
August 1988
Working Papera are interim reports on work of the International Institute for
Applied Systems Analysis and have received only limited review. Views or
opinions expressed herein do not necessarily represent those of the Institute
or of its National Member Organizations.
INTERNATIONAL INSTITUTE FOR APPLIED SYSTEMS ANALYSIS
A-2361 Laxenburg, Austria
This paper relates some numerical experiments with variable storage quasi-
Newton methods for the optimization of large-scale models. The basic idea of the
recommended algorithm is to start bfgs updates on a diagonal matrix, itself generated
by an update formula and adjusted to Rayleigh's ellipsoid of the local Hessian of the
objective function in the direction of the change in the gradient.
A variational derivation of some rank one and rank two updates in Hilbert spaces
is also given.
Alexander B. Kurzhanski
System and Decision Sciences Program
SOME NUMERICAL EXPERIMENTS WITH
VARIABLE STORAGE QUASI-NEWTON ALGORITHMS *
Jean Charles GILBERT
International Institute for Applied Systems Analysis
A - 2361 Lazenburg (Austria)
Claude LEMA R ~ C H A
Institut National de Recherche en Informatique et en Automatique
F - 78153 Le Chesnay (France)
Abbreviated title. Variable storage QN algorithms.
Key words. Conjugate Gradient, Diagonal Updates, Large-scale Problems, Lim-
ited Memory, Unconstrained Optimization, Variable Metric Algorithms, Variable
Subject classification AMS (MOS): 49D05, 65K05.
* Work supported in part by INRIA (Institut National de Recherche en Informatique et en Automatique),
France and in part by the FNRS (Fonds National de la Recherche Scientifique), Belgium.
1. Introduction
This paper relates some numerical experiments with variable storage quasi-
Newton methods for finding a minimum of a smooth real-valued function f defined on
These methods are intended for large-scale problems (that is to say, problems
with a large number of variables, say, more than 500) when the Hessian of the objec-
tive function has no particular structure. In particular, in their general setting, these
methods do not try to take advantage of the possible sparsity of the Hessian. It is
thought that this type of algorithms may help in filling the gap between, on the one
hand, conjugate gradient (CG) methods, which use few locations in memory, O(n),
but converge rather slowly and require exact line search, and, on the other hand,
quasi-Newton (QN) or variable metric methods, which have the converse features: fast
rate of convergence (theoretically superlinear), no exact line search requirement but
cumbersome in memory since 0(n2) storage locations are needed.
Variable storage QN algorithms are QN-like methods in the sense that they use
the change in the gradient to obtain information on the local Hessian of the objective
function. However, they do not store any matrix of order n because this is supposed
to be either impossible or too expensive. Rather, they are able to operate with a vari-
able amount of storage which typically is a multiple of n. A priori, if CG and QN
methods are regarded as two extremes, a small (resp. a large) amount of storage
should make them resemble CG (resp. QN) methods. In any case, it seems reasonable
to expect an increase in performance if more storage is used.
Among the papers dealing with variable storage methods, let us mention the
works by Buckley , Nocedal , Buckley and Lenir . The papers by
Shanno , Shanno and Phua and Gill and Murray have also some
connection to the subject. We shall come back in details on the methods proposed in
these papers.
This study is definitely experimental in the sense that we have tried to bring
improvements to an existing method by observing the effect of the implementation of
some ideas on a set of test problems.
The resulting algorithms of this study have been included in the Frensh library
Modulopt of INRIA under the names of mlqn2 and mlqn3. This library has the
important attraction to put a battery of test problems to the disposal of optimization
code writers and this, for each of the four classes of optimization problems: without
constraints, with bound constraints, with linear constraints and with nonlinear con-
straints. These applications usually come from real-world models. This has not only
advantages. Indeed, the solutions of the problems are not exactly known, neither is
the nature of the spectrum of the Hessian around a solution, the gradient of the objec-
tive function may be spoilt by rounding errors due to the large amount of computation
and, last but not least, computation time and memory storage may be deterrent fac-
tors. However, those problems are the one to be solved and their large scale is not
artificially obtained. Moreover, the problems from Modulopt library are written in a
fully portable form (Fortran 77), which should allow comparison with further
developped codes. The problems we used essentially come from physics: fluid mechan-
ics, meteorology and crystallography.
We started with an algorithm proposed by Nocedal in order to compare its
efficiency with the algorithm conmin of Shanno and Phua and the algorithm of
Buckley and Lenir , called mlgc3 in Modulopt library. Strictly speaking, con-
min has not the "variable" storage property but, as we shall see, is rather a "tw*
storage" method. However, the code is intended for large-scale problems and, like
variable storage methods, it is inspired by QN methods to enhance the performances of
CG methods. Furthermore, the comparison of the performances of mlgc3 and
Nocedal's algorithm with those of conmin will allow us to see to what extent the
decreasing of the objective function can benefit from the availability of memory space.
Contrary to conmin and mlgc3, Nocedal's method has, in our opinion, the con-
ceptually nice feature of not requiring any restart during computation, a concept
inherited from CG methods. This makes the algorithm closer to QN methods. Our
numerical experiments will show that, at least on our test problems, the "basic"
Nocedal's method (called mlqn2.a below) generally behaves better than conmin and
The codes mlqn2 and mlqn3 that we validate by this study are polished versions
of Nocedal's algorithm. Like this one, it builds the current approximation of the Hes-
sian by updating m times a given matrix 80, using the last m couples (y,s), where s is
the change in the variable z and y is the corresponding change in the gradient of the
objective function. A particular attention is given here in the choice of the starting
matrix 80. In mlqn2, Ho is just a multiple of the identity matrix. In mlqn3, 80 is a
diagonal matrix, itself updated at each iteration using a diagonal update formula. We
have found that the most efficient way to compute 80 is to use a "diagonal bfgs update
Without wishing to go into what will be said in the conclusion of the paper, we
may say, however, that our experiments suggest that the marginal profit yielded by
increasing the number m of updates is clearly decreasing with m and may become
negative. If we take into account the computing time, the best algorithms for our test
problems, which have dimensions n equal to 34, 403, 455, 500, 1559 and 1875, should
use between 5 and 10 updates. Increasing the number of updates does not increase
significantly the performances or even decreases them and increases substantially the
computing time. In other words, the algorithms do not seem to take much advantage
of the possible avaibility of storage. On the other hand, a good choice of the starting
(diagonal) matrix HO was determining for the performances of the methods. It is
indeed not unusual to observe a better decrease of the objective function with mlqn2
amd mlqn3 than with bfgs algorithm.
The paper is organized as follows. In Section 2, we give more details on the algo-
rithms mentioned above. In Section 9, we briefly describe the test problems and dis-
cuss numerical experiments made with mlgc3 and bfgs on these problems. In Section
4, we introduce several ways of choosing an initial matrix for QN type methods and we
propose several formulae for updating diagonal matrices. Numerical experiments are
related. In Annez, we show how to obtain some variable metric update formulae in
Hilbert spaces by means of a variational formulation.
2. Some variable storage quasi-Newton methods
2.1. Notation
Let EI be a Hilbert space over lU, equipped with a real scalar product <-,.> and
its associated norm I . 1.
We shall denote by L(H), the space of continuous linear
operators on H. Being given two vectors u and u in H, we shall use the bracket
operator [u,u] E L(H) defined by [u,u] : H -+ H : d + [u,u] d := <u,d> u. We
shall say that B E L(H) is positive if <Bu,u> is positive, for all nonzero u E H.
If M has finite dimension n and if (ei)l<is, is an orthonormal basis (ONB) of H,
the Frobenius norm associated to the scalar product
of a linear operator B is
defined by
This quantity does not depend on the choice of the ONB and is actually a norm. If
I I . ) ( denotes thenormof L ( q , we haveclearly IIB(( 5 ) I B I J F , V B â‚¬ L(H),and
for the bracket operator, we have
2.2. Quasi-Newton methods in Hilbert spaces
We consider the problem of minimizing a smooth real-valued function f on H:
min{ f(z) : z E E l ) ,
and we shall denote by z, a local solution of this problem.
Quasi-Newton (or secant) methods generate two sequences: a sequence (zk) E H
of approximations of z, and a sequence (Bk)
L ( B ) of bijective approximations of
B* := v2f(z,), the Hessian of f at z,. We shall suppose that B, is nonsingular and we
shall note H, := B,~. Starting with a couple (zo, Bo), the sequences are calculated
In (2.5), pk is a positive step-size determined by a search on f along the direction
dk := - BK' gk and gk := g(q), where g(z) := Vf(z) is the gradient of f at z. In
(2.5), U represents an update formula that calculates Bk+l from Bk, using yk :=
gk+l - gk and sk := q+l - 3. If Hk is the inverse of Bk, it is generally possible to
update Hk instead of Bk using the inverse update formula 0 of U:
Let us make some comments to see to what extent QN theory depends on the
scalar structure of El. Formula (2.4) uses the gradient g(z) of f at z = zk, which is
defined by Riesz theorem ' for instance) as the unique represen-
tative in H of the continuous linear form f'(z), the derivative of f at z:
fl(z)-(u) = <g(z),u> , v u E El.
Formula (2.4) also uses the operator Bk, which is an approximation of the Hessian
B(z) of f at z = z,. B(z) is the unique representative in L(El) of the continuous bil-
inear form f"(z), the second order derivative of f at z:
Therefore, the choice of the scalar product in El affects the value of B(z,) that Bk
should approach. So, Bk should also depend on the Hilbert structure of El. Now, by
Taylor's theorem and formulae (2.7) and (2.8), we have:
This relation allows to understand why the basic idea of QN methods is to find update
formulae U such that Bk+l given by (2.5) satisfies the QN equation (or secant equa-
Therefore, if a change of scalar product does not affect the secant equation (2.9), it
changes Bk+l by changing yk, via formula (2.7). Moreover, the form of the formulae U
and D in (2.5) and (2.6) also depends on the scalar product. Indeed, the properties
that characterize a given update formula are generally expressed in terms of the Hil-
bert structure of El. If we take as a guideline the preservation of these properties, the
form of the formulae will reflect its dependance on the scalar product. This point of
vue is taken in Annez, where some classical rank one and rank two update formulae
are derived.
One of them is the bfgs formula, which is thought to be one of the best secant
updates in optimization ). For us, U will stand for the bfgs
will stand for the inverse bfgs formula:
These formulae clearly show their dependance on the scalar product < a , . > .
Formulae (2.10) and (2.11) have the property to transmit the positivity of Bk to
Bk+l (resp. of Hk to Hksl), if and only if <yk,Sk> is positive. Having Bk positive is
important to make dk a descent direction of f at zk: fr(%).(dk) < 0. For this reason,
the stepsize pk in (2.4) is generally determined such that Wolfe's conditions are
satisfied:
where 0 < al < 112 and al < a2 < 1. Clearly, inequality (2.13) implies the positivity
of <yk,sk>
In practice, IH is a Hilbert space of finite dimension n. If the number n of vari-
ables is large, it may turn out to be impossible or too expensive to store in memory the
full current approximation Hk of the inversed Hessian. Because the initial matrix Ho
takes generally little place in memory (it is most commonly a multiple of the identity
matrix) and because Hk is formed from Ho and k couples { (y;,s,) : 0 < i < k ), it can
be thought of memorizing these elements instead of Hk and of computing Hk gk by an
appropriate algorithm. Of course, when the number of iteration increases, these pieces
of information become more and more cumbersome in memory and we must think to
truncate the sequence of couples { (yi,si) : 0 5 i < k ) or to get rid of some of them.
We shall say that it is an m-storage QN method if only m of these couples are used to
form Hk from an initial matrix. Note that in this type of methods, the inverse update
formula (2.11) is preferably used to the direct update formula (2.10) because the inver-
sion of Bk may be problematic.
The variable storage QN methods we present hereafter, all fit into this framework
and differ in the selection of the couples (yi,si), in the choice of the starting matrix H,,,
in the way Hk gk is computed and in the presence or absence of restarts.
2.3. The algorithm of Shanno
Motivated by the search of a conjugate gradient type method without exact line
searches, Shanno recommended, on the basis of a large amount of computa-
tional results, to take dk := - Hk gk as descent direction at iteration k, with the follow-
ing formulae for Hk:
where 0 stands for the inverse bfgs formula (2.11), rk is the index of the last restart
iteration preceding iteration k and 6ik-i is the evaluation at iteration rk-1 of
The algorithm is restarted at iteration k when Powell's restart criterion is
satisfied, i.e. when I < g k , ~ - ~ >
I 2 0.2 1 gk12.
Then, rk is set to k and formula (2.14) is
used. Otherwise, rk is set to rk-l and formula (2.15) is used. The scaling factor (2.16)
was experimented by Shanno and Phua who motivated it by the self-scaling
variable metric algorithms of Oren and Spedicato .
So, when k > rk, the algorithm is clearly a 2-storage bfgs method using succes-
sively the couples (yrk-l,srk-l) and (yk-l,~-l) to build Hk.
Formulae (2.14) and (2.15) are directly inspired by Beale's formulae to res-
tart the CG method at iteration rk. It can be proved indeed ) that
for f quadratic and exact line searches, the search directions obtained by (2.14) and
(2.15) with any scaling factor 6 are identical to Beale's directions, scaled by 6. The
advantage of Shanno's method over Beale's method is then to generate automatically
descent direction of f without requiring exact line searches, as long as <yk,sk> is posi-
tive at each iteration, which can be provided by the line search.
This algorithm is a part of the code conmin, name by which we shall refer to. It
requires 7n locations in memory: for 9,
gk, dk, z ~ + ~ ,
gk+l, yrk and srk. The Euclidean
scalar product is used: < u,u> := u u. The stepsize pk is determined to satisfy (2.12)
with cr2 = 0.9, which is more restrictive than Wolfe's condition (2.13). At restart
iterations, the first trial step-size is p:
:= 1, whereas for nonrestart iterations, the first
trial step-size is chosen to be ):
It is also important to mention that always at least two trial step-sizes are required
before accepting a step-size satisfying (2.12) and (2.17). Therefore, at least one qua-
dratic interpolation can be done at each step, which gives an exact line search in case f
is quadratic.
2.4. The algorithm of Buckley and Lenir
The algorithm of Shanno uses exactly two couples of vectors y and s to build its
current approximation of the metric. Therefore, it cannot take advantage of extra
locations that would be possibly available in memory. The algorithm of Buckley and
Lenir remedies to this deficiency and may be seen as an extension of Shanno's
Following the presentation of the authors, we shall say that the algorithm is
cyclic, each cycle being composed of a QN-part followed by a CG-part. The QN-part
builds a preconditioner for the CG-part. The decision to restart a cycle is taken during