Partial Transfer Learning with Selective Adversarial
Zhangjie Cao†, Mingsheng Long†, Jianmin Wang†, and Michael I. Jordan♯
†KLiss, MOE; TNList; School of Software, Tsinghua University, China
♯University of California, Berkeley, Berkeley, USA
 
{mingsheng,jimwang}@tsinghua.edu.cn
 
Adversarial learning has been successfully embedded into deep networks to learn
transferable features, which reduce distribution discrepancy between the source
and target domains. Existing domain adversarial networks assume fully shared
label space across domains. In the presence of big data, there is strong motivation
of transferring both classiﬁcation and representation models from existing big
domains to unknown small domains. This paper introduces partial transfer learning,
which relaxes the shared label space assumption to that the target label space is only
a subspace of the source label space. Previous methods typically match the whole
source domain to the target domain, which are prone to negative transfer for the
partial transfer problem. We present Selective Adversarial Network (SAN), which
simultaneously circumvents negative transfer by selecting out the outlier source
classes and promotes positive transfer by maximally matching the data distributions
in the shared label space. Experiments demonstrate that our models exceed stateof-the-art results for partial transfer learning tasks on several benchmark datasets.
Introduction
Deep networks have signiﬁcantly improved the state of the art for a wide variety of machine learning
problems and applications. At the moment, these impressive gains in performance come only when
massive amounts of labeled data are available. Since manual labeling of sufﬁcient training data
for diverse application domains on-the-ﬂy is often prohibitive, for problems short of labeled data,
there is strong motivation to establishing effective algorithms to reduce the labeling consumption,
typically by leveraging off-the-shelf labeled data from a different but related source domain. This
promising transfer learning paradigm, however, suffers from the shift in data distributions across
different domains, which poses a major obstacle in adapting classiﬁcation models to target tasks .
Existing transfer learning methods assume shared label space and different feature distributions across
the source and target domains. These methods bridge different domains by learning domain-invariant
feature representations without using target labels, and the classiﬁer learned from source domain can
be directly applied to target domain. Recent studies have revealed that deep networks can learn more
transferable features for transfer learning , by disentangling explanatory factors of variations
behind domains. The latest advances have been achieved by embedding transfer learning in the
pipeline of deep feature learning to extract domain-invariant deep representations .
In the presence of big data, we can readily access large-scale labeled datasets such as ImageNet-1K.
Thus, a natural ambition is to directly transfer both the representation and classiﬁcation models from
large-scale dataset to our target dataset, such as Caltech-256, which are usually small-scale and with
unknown categories at training and testing time. From big data viewpoint, we can assume that the
large-scale dataset is big enough to subsume all categories of the small-scale dataset. Thus, we
Preliminary work. Copyright by the author(s).
 
source domain
target domain
soccer-ball
binoculars
soccer-ball
binoculars
Figure 1: The partial transfer learning problem, where source label space subsumes target label space.
introduce a novel partial transfer learning problem, which assumes that the target label space is a
subspace of the source label space. As shown in Figure 1, this new problem is more general and
challenging than standard transfer learning, since outlier source classes (“sofa”) will result in negative
transfer when discriminating the target classes (“soccer-ball” and “binoculars”). Thus, matching the
whole source and target domains as previous methods is not an effective solution to this new problem.
This paper presents Selective Adversarial Networks (SAN), which largely extends the ability of deep
adversarial adaptation to address partial transfer learning from big domains to small domains.
SAN aligns the distributions of source and target data in the shared label space and more importantly,
selects out the source data in the outlier source classes. A key improvement over previous methods
is the capability to simultaneously promote positive transfer of relevant data and alleviate negative
transfer of irrelevant data, which can be trained in an end-to-end framework. Experiments show that
our models exceed state-of-the-art results for partial transfer learning on public benchmark datasets.
Related Work
Transfer learning bridges different domains or tasks to mitigate the burden of manual labeling
for machine learning , computer vision and natural language processing .
The main technical difﬁculty of transfer learning is to formally reduce the distribution discrepancy
across different domains. Deep networks can learn abstract representations that disentangle different
explanatory factors of variations behind data and manifest invariant factors underlying different
populations that transfer well from original tasks to similar novel tasks . Thus deep networks have
been explored for transfer learning , multimodal and multi-task learning , where
signiﬁcant performance gains have been witnessed relative to prior shallow transfer learning methods.
However, recent advances show that deep networks can learn abstract feature representations that can
only reduce, but not remove, the cross-domain discrepancy , resulting in unbounded risk for
target tasks . Some recent work bridges deep learning and domain adaptation ,
which extends deep convolutional networks (CNNs) to domain adaptation by adding adaptation layers
through which the mean embeddings of distributions are matched , or by adding a
subnetwork as domain discriminator while the deep features are learned to confuse the discriminator
in a domain-adversarial training paradigm . While performance was signiﬁcantly improved,
these state of the art methods may be restricted by the assumption that the source and target domains
share the same label space. This assumption is violated in partial transfer learning, which transfers
both representation and classiﬁcation models from existing big domains to unknown small domains.
To our knowledge, this is the ﬁrst work that addresses partial transfer learning in adversarial networks.
Partial Transfer Learning
In this paper, we propose partial transfer learning, a novel transfer learning paradigm where the
target domain label space Ct is a subspace of the source domain label space Cs i.e. Ct ⊂Cs. This new
paradigm ﬁnds wide applications in practice, as we usually need to transfer a model from a large-scale
dataset (e.g. ImageNet) to a small-scale dataset (e.g. CIFAR10). Similar to standard transfer learning,
in partial transfer learning we are also provided with a source domain Ds = {(xs
labeled examples associated with |Cs| classes and a target domain Dt = {xt
i=1 of nt unlabeled
examples associated with |Ct| classes, but differently, we have |Cs| > |Ct| in partial transfer learning.
The source domain and target domain are sampled from probability distributions p and q respectively.
In standard transfer learning, we have p ̸= q; and in partial transfer learning, we further have pCt ̸= q,
where pCt denotes the distribution of the source domain labeled data belonging to label space Ct.
The goal of this paper is to design a deep neural network that enables learning of transfer features
f = Gf (x) and adaptive classiﬁer y = Gy (f) to bridge the cross-domain discrepancy, such that the
target risk Pr(x,y)∼q [Gy (Gf(x)) ̸= y] is minimized by leveraging the source domain supervision.
In standard transfer learning, one of the main challenges is that the target domain has no labeled
data and thus the source classiﬁer Gy trained on source domain Ds cannot be directly applied to
target domain Dt due to the distribution discrepancy of p ̸= q. In partial transfer learning, another
more difﬁcult challenge is that we even do not know which part of the source domain label space Cs
is shared with the target domain label space Ct because Ct is not accessible during training, which
results in two technical difﬁculties. On one hand, the source domain labeled data belonging to outlier
label space Cs\Ct will cause negative transfer effect to the overall transfer performance. Existing
deep transfer learning methods generally assume source domain and target domain
have the same label space and match the whole distributions p and q, which are prone to negative
transfer since the source and target label spaces are different and thus cannot be matched in principle.
Thus, how to eliminate or at least decrease the inﬂuence of the source labeled data in outlier label
space Cs\Ct is the key to alleviating negative transfer. On the other hand, reducing the distribution
discrepancy between pCt and q is crucial to enabling knowledge transfer in the shared label space Ct.
In summary, there are two essential challenges to enabling partial transfer learning. (1) Circumvent
negative transfer by ﬁltering out the unrelated source labeled data belonging to the outlier label space
Cs\Ct. (2) Promote positive transfer by maximally matching the data distributions pCt and q in the
shared label space Ct. We propose a novel selective adversarial network to address both challenges.
Domain Adversarial Network
Domain adversarial networks have been successfully applied to transfer learning by extracting
transferable features that can reduce the distribution shift between the source domain and the target
domain. The adversarial learning procedure is a two-player game, where the ﬁrst player is the domain
discriminator Gd trained to distinguish the source domain from the target domain, and the second
player is the feature extractor Gf ﬁne-tuned simultaneously to confuse the domain discriminator.
To extract domain-invariant features f, the parameters θf of feature extractor Gf are learned by
maximizing the loss of domain discriminator Gd, while the parameters θd of domain discriminator
Gd are learned by minimizing the loss of the domain discriminator. In addition, the loss of label
predictor Gy is also minimized. The objective of domain adversarial network is the functional:
C0 (θf, θy, θd) = 1
Ly (Gy (Gf (xi)) , yi) −
xi∈(Ds∪Dt)
Ld (Gd (Gf (xi)) , di),
where λ is a trade-off parameter between the two objectives that shape the features during learning.
After training convergence, the parameters ˆθf, ˆθy, ˆθd will deliver a saddle point of the functional (1):
(ˆθf, ˆθy) = arg min
θf ,θy C0 (θf, θy, θd) ,
(ˆθd) = arg max
θd C0 (θf, θy, θd) .
Domain adversarial networks are among the top-performing architectures for standard transfer
learning where the source domain label space and target domain label space are the same, Cs = Ct.
Selective Adversarial Network
In partial transfer learning, however, the target domain label space is a subset of the source domain
label space, Ct ⊂Cs. Thus, matching the whole source domain distribution p and target domain
back-propagation
Figure 2: The architecture of the proposed Selective Adversarial Networks (SAN) for partial transfer
learning, where f is the extracted deep features, ˆy is the predicted data label, and ˆd is the predicted
domain label; Gf is the feature extractor, Gy and Ly are the label predictor and its loss, Gk
are the domain discriminator and its loss; GRL stands for Gradient Reversal Layer. The blue part
shows the class-wise adversarial networks (|Cs| in total) designed in this paper. Best viewed in color.
distribution q will result in negative transfer caused by the outlier label space Cs\Ct. The larger the
outlier label space Cs\Ct compared to the target label space Ct, the severer the negative transfer effect
will be. To combat negative transfer, we should ﬁnd a way to select out the outlier source classes as
well as the associated source labeled data in Cs\Ct when performing domain adversarial adaptation.
To match the source and target domains of different label spaces Cs ̸= Ct, we need to split the domain
discriminator Gd in Equation (1) into |Cs| class-wise domain discriminators Gk
d, k = 1, . . . , |Cs|,
each is responsible for matching the source and target domain data associated with label k, as shown
in Figure 2. Since the target label space Ct is inaccessible during training while the target domain
data are fully unlabeled, it is not easy to decide which domain discriminator Gk
d is responsible for
each target data point. Fortunately, we observe that the output of the label predictor ˆyi = Gy(xi) to
each data point xi is a probability distribution over the source label space Cs. This distribution well
characterizes the probability of assigning xi to each of the |Cs| classes. Therefore, it is natural to use
ˆyi as the probability to assign each data point xi to the |Cs| domain discriminators Gk
d, k = 1, . . . , |Cs|.
The assignment of each point xi to different discriminators can be implemented by a probabilityweighted domain discriminator loss for all |Cs| domain discriminators Gk
d, k = 1, . . . , |Cs| as follows,
d (Gf (xi)) , di
d is the k-th domain discriminator while Lk
d is its cross-entropy loss, and di is the domain
label of point xi. Compared with the single-discriminator domain adversarial network in Equation (1),
the proposed multi-discriminator domain adversarial network enables ﬁne-grained adaptation where
each data point xi is matched only by those relevant domain discriminators according to its probability
ˆyi. This ﬁne-grained adaptation may introduce three beneﬁts. (1) It avoids the hard assignment of
each point to only one domain discriminator, which tends to be inaccurate for target domain data.
(2) It circumvents negative transfer since each point is only aligned to one or several most relevant
classes, while the irrelevant classes are ﬁltered out by the probability-weighted domain discriminator
loss. (3) The probability-weighted domain discriminator loss puts different losses to different domain
discriminators, which naturally learns multiple domain discriminators with different parameters θk
these domain discriminators with different parameters can promote positive transfer for each instance.
Besides the instance-level weighting mechanism described above, we introduce another class-level
weighting method to further remove the negative inﬂuence of outlier source classes Cs\Ct and the
associated source data. We observe that only the domain discriminators responsible for the target
classes Ct are effective for promoting positive transfer, while the other discriminators responsible for
the outlier source classes Cs\Ct only introduce noises and deteriorate the positive transfer between the
source domain and the target domain in the shared label space Ct. Therefore, we need to down-weight
the domain discriminators responsible for the outlier source classes, which can be implemented by
class-level weighting of these domain discriminators. Since target data are not likely to belong to
the outlier source classes, their probabilities yk
i , k ∈Cs\Ct are also sufﬁciently small. Thus, we can
down-weight the domain discriminators responsible for the outlier source classes as follows,
xi∈(Ds∪Dt)
d (Gf (xi)) , di
i is the class-level weight for class k, which is small for the outlier source classes.
Although the multiple domain discriminators introduced in Equation (4) can selectively transfer
relevant knowledge to target domain by decreasing the negative inﬂuence of outlier source classes
Cs\Ct and by effectively transferring knowledge of shared label space Ct, it highly depends on the
probability ˆyi = Gy(xi). Thus, we further reﬁne the label predictor Gy by exploiting the entropy
minimization principle which encourages low-density separation between classes. This criterion
is implemented by minimizing the conditional-entropy E of probability ˆyk
i on target domain Dt as
H (Gy (Gf (xi)))
where H(·) is the conditional-entropy loss functional H (Gy (Gf (xi))) = −P|Cs|
minimizing entropy (5), the label predictor Gy(xi) can directly access target unlabeled data and will
amend itself to pass through the target low-density regions to give more accurate probability ˆyi.
Integrating all things together, the objective of the proposed Selective Adversarial Network (SAN) is
θf, θy, θk
Ly (Gy (Gf (xi)), yi) + 1
H (Gy (Gf (xi)))
d (Gf (xi)) , di
where λ is a hyper-parameter that trade-offs the two objectives in the uniﬁed optimization problem.
The optimization problem is to ﬁnd the parameters ˆθf, ˆθy and ˆθk
d(k = 1, 2, ..., |Cs|) that satisfy
(ˆθf, ˆθy) = arg min
θf, θy, θk
d, ..., ˆθ|Cs|
d,...,θ|Cs|
θf, θy, θk
The selective adversarial network (SAN) successfully enables partial transfer learning, which simultaneously circumvents negative transfer by ﬁltering out outlier source classes Cs\Ct, and promotes
positive transfer by maximally matching the data distributions pCt and q in the shared label space Ct.
Experiments
We conduct experiments on three benchmark datasets to evaluate the efﬁcacy of our approach against
several state-of-the-art deep transfer learning methods. Codes and datasets will be available online.
The evaluation is conducted on three public datasets: Ofﬁce-31, Caltech-Ofﬁce and ImageNet-Caltech.
Ofﬁce-31 is a standard benchmark for domain adaptation in computer vision, consisting of 4,652
images and 31 categories collected from three distinct domains: Amazon (A), which contains images
downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images taken by web
camera and digital SLR camera with different settings, respectively. We denote the three domains
with 31 categories as A 31, W 31 and D 31. Then we use the ten categories shared by Ofﬁce-31 and
Caltech-256 and select images of these ten categories in each domain of Ofﬁce-31 as target domains,
denoted as A 10, W 10 and D 10. We evaluate all methods across six transfer tasks A 31 →W 10, D
31 →W 10, W 31 →D 10, A 31 →D 10, D 31 →A 10 and W 31 →A 10. These tasks represent
the performance on the setting where both source and target domains have small number of classes.
Caltech-Ofﬁce is built by using Caltech-256 (C 256) as source domain and the three domains
in Ofﬁce 31 as target domains. We use the ten categories shared by Caltech-256 and Ofﬁce-31 and
select images of these ten categories in each domain of Ofﬁce-31 as target domains .
Denoting source domains as C 256, we can build 3 transfer tasks: C 256 →W 10, C 256 →A 10
and C 256 →D 10. This setting aims to test the performance of different methods on the task setting
where source domain has much more classes than the target domain.
ImageNet-Caltech is constructed with ImageNet-1K dataset containing 1000 classes and
Caltech-256 containing 256 classes. They share 84 common classes, thus we form two transfer learning tasks: ImageNet 1000 →Caltech 84 and Caltech 256 →ImageNet 84. To prevent the
effect of the pre-trained model on ImageNet, we use ImageNet validation set when ImageNet is used
as target domain and ImageNet training set when ImageNet is used as source domain. This setting
represents the performance on tasks with large number of classes in both source and target domains.
We compare the performance of SAN with state of the art transfer learning and deep learning methods:
Convolutional Neural Network (AlexNet ), Deep Adaptation Network (DAN) , Reverse Gradient (RevGrad) and Residual Transfer Networks (RTN) . DAN learns transferable features
by embedding deep features of multiple task-speciﬁc layers to reproducing kernel Hilbert spaces
(RKHSs) and matching different distributions optimally using multi-kernel MMD. RevGrad improves
domain adaptation by making the source and target domains indistinguishable for a discriminative
domain classiﬁer via an adversarial training paradigm. RTN jointly learns transferable features and
adapts different source and target classiﬁers via deep residual learning . All prior methods do
not address partial transfer learning where the target label space is a subspace of the source label
space. To go deeper with the efﬁcacy of selective mechanism and entropy minimization, we perform
ablation study by evaluating two variants of SAN: (1) SAN-selective is the variant without selective
mechanism; (2) SAN-entropy is the variant without entropy minimization.
We follow standard protocols and use all labeled source data and all unlabeled target data for
unsupervised transfer learning . We compare average classiﬁcation accuracy of each transfer
task using three random experiments. For MMD-based methods (DAN and RTN), we use Gaussian
kernel with bandwidth b set to median pairwise squared distances on training data, i.e. median
heuristic . For all methods, we perform cross-valuation on labeled source data to select parameters.
We implement all deep methods based on the Caffe deep-learning framework, and ﬁne-tune from
Caffe-provided models of AlexNet pre-trained on ImageNet. We add a bottleneck layer between
the fc7 and fc8 layers as RevGrad except for the task ImageNet 1000 →Caltech 84 since the
pre-trained model is trained on ImageNet dataset and it can fully exploit the advantage of pre-trained
model with the original fc7 and fc8 layer. For SAN, we ﬁne-tune all the feature layers and train
the bottleneck layer, the classiﬁer layer and the adversarial networks. Since these new layers and
networks are trained from scratch, we set their learning rate to be 10 times that of the other layers.
We use mini-batch stochastic gradient descent (SGD) with momentum of 0.9 and the learning rate
annealing strategy implemented in RevGrad : the learning rate is not selected through a grid search
due to high computational cost: it is adjusted during SGD using the following formula: ηp =
where p is the training progress linearly changing from 0 to 1, η0 = 0.001, α = 10 and β = 0.75,
which is optimized for low error on the source domain. As SAN can work stably across different
transfer tasks, the penalty of adversarial networks is increased from 0 to 1 gradually as RevGrad .
The classiﬁcation results on the six tasks of Ofﬁce-31, the three tasks of Caltech-Ofﬁce and the two
tasks of ImageNet-Caltech are shown in Table 1 and 2. The SAN model outperforms all comparison
methods on all the tasks. In particular, SAN substantially improves the accuracy by huge margins
on tasks with small source domain and small target domain, e.g. A 31 →W 10 , A 31 →D 10,
and tasks with large source domain and small target domain, e.g. C 31 →W 10. And it achieves
considerable accuracy gains on tasks with large-scale source domain and target domain, e.g. I 1000
Table 1: Accuracy (%) of partial transfer learning tasks on Ofﬁce-31
A 31 →W 10
D 31 →W 10
W 31 →D 10
A 31 →D 10
D 31 →A 10
W 31 →A 10
AlexNet 
RevGrad 
SAN-selective
SAN-entropy
Table 2: Accuracy (%) of partial transfer learning tasks on Caltech-Ofﬁce and ImageNet-Caltech
Caltech-Ofﬁce
ImageNet-Caltech
C 256 →W 10
C 256 →A 10
C 256 →D 10
I 1000 →C 84
C 256 →I 84
AlexNet 
RevGrad 
SAN-selective
SAN-entropy
→C 84. These results suggest that SAN can learn transferable features for partial transfer learning
in all the tasks under the setting where the target label space is a subspace of the source label space.
The results reveal several interesting observations. (1) Previous deep transfer learning methods
including those based on adversarial-network like RevGrad and those based on MMD like DAN
perform worse than standard AlexNet, which demonstrates the inﬂuence of negative transfer effect.
These methods try to transfer knowledge from all classes of source domain to target domain but there
are classes in source domain that do not exist in the target domain, a.k.a. outlier source data. Fooling
the adversarial network to match the distribution of outlier source data and target data will make the
classiﬁer more likely to classify target data in these outlier classes, which is prone to negative transfer.
Thus these previous methods perform even worse than standard AlexNet. However, SAN outperforms
them by large margins, indicating that SAN can effectively avoid negative transfer by eliminating the
outlier source classes irrelevant to target domain. (2) RTN performs better than AlexNet because it
executes entropy minimization criterion which can avoid the impact of outlier source data to some
degree. But comparing RTN with SAN-selective which only has entropy minimization loss, we
observe that SAN-selective outperforms RTN in most tasks, demonstrating that RTN also suffers
from negative transfer effect and even the residual branch of RTN cannot learn the large discrepancy
between source and target domain. SAN outperforms RTN in all the tasks, proving that our selective
adversarial mechanism can jointly promote positive transfer from relevant source domain data to
target domain and circumvent negative transfer from outlier source domain data to target domain.
We go deeper into different modules of SAN by comparing the results of SAN variants in Tables 1
and 2. (1) SAN outperforms SAN-selective, proving that using selective adversarial mechanism can
selectively transfer knowledge from source data to target data. It can successfully select the source
data belonging to the classes shared with target classes by the corresponding domain discriminators.
(2) SAN outperforms SAN-entropy especially in tasks where source and target domains have very
large distribution gap in terms of the different numbers of classes, e.g. I 1000 →C 84. Entropy
minimization can effectively decrease the probability of predicting each point to irrelevant classes
especially when there are a large number of irrelevant classes, which can in turn boost the performance
of the selective adversarial mechanism. This explains the improvement from SAN-entropy to SAN.
Accuracy for Different Numbers of Target Classes: We investigate a wider spectrum of partial
transfer learning by varying the number of target classes. Figure 3(a) shows that when the number
of target classes decreases, the performance of RevGrad degrades quickly, meaning that negative
transfer becomes severer when the domain gap is enlarged. The performance of SAN degenerates
when the number of target classes decreases from 31 to 20, where negative transfer problem arises but
the transfer problem itself is still hard; the performance of SAN increases when the number of target
classes decreases from 20 to 10, where the transfer problem itself becomes easier. The margin that
SAN outperforms RevGrad becomes larger when the number of target classes decreases. SAN also
outperforms RevGrad in standard transfer learning setting when the number of target classes is 31.
Convergence Performance: We examine the convergence of SAN by studying the test error through
training process. As shown in Figure 3(b), the test errors of DAN and RevGrad are increasing due to
negative transfer. RTN converges very fast depending on the entropy minimization, but converges to
a higher test error than SAN. SAN converges fast and stably to a lowest test error, meaning it can be
trained efﬁciently and stably to enable positive transfer and alleviate negative transfer simultaneously.
Number of Target Classes
(a) Accuracy w.r.t #Target Classes
Number of Iterations
Test Error
(b) Test Error
Figure 3: Empirical analysis: (a) Accuracy by varying #target domain classes; (b) Target test error.
(b) RevGrad
Figure 4: The t-SNE visualization of DAN, RevGrad, RTN, and SAN with class information.
(b) RevGrad
Figure 5: The t-SNE visualization of DAN, RevGrad, RTN, and SAN with domain information.
Feature Visualization: We visualize the t-SNE embeddings of the bottleneck representations
by DAN, RevGrad, RTN and SAN on transfer task A 31 →W 10 in Figures 4(a)–4(d) (with class
information) and Figures 5(a)–5(d) (with domain information). We randomly select ﬁve classes in
the source domain not shared with target domain and ﬁve classes shared with target domain. We can
make intuitive observations. (1) Figure 4(a) shows that the bottleneck features are mixed together,
meaning that DAN cannot discriminate both source and target data very well; Figure 5(a) shows that
the target data are aligned to all source classes including those outlier ones, which embodies the
negative transfer issue. (2) Figures 4(b)– 4(c) show that both RevGrad and RTN discriminate the
source domain well but the features of most target data are very close to source data even to the wrong
source classes; Figures 5(b)– 5(c) further indicate that both RevGrad and RTN tend to draw target
data close to all source classes even to those not existing in target domain. Thus, their performance
on target data degenerates due to negative transfer. (3) Figures 4(d) and 5(d) demonstrate that SAN
can discriminate different classes in both source and target while the target data are close to the right
source classes, while the outlier source classes cannot inﬂuence the target classes. These promising
results demonstrate the efﬁcacy of both selective adversarial adaptation and entropy minimization.
Conclusion
This paper presented a novel selective adversarial network approach to partial transfer learning. Unlike
previous adversarial adaptation methods that match the whole source and target domains based on the
shared label space assumption, the proposed approach simultaneously circumvents negative transfer
by selecting out the outlier source classes and promotes positive transfer by maximally matching the
data distributions in the shared label space. Our approach successfully tackles partial transfer learning
where source label space subsumes target label space, which is testiﬁed by extensive experiments.