Ternary Neural Networks for Resource-Efﬁcient AI Applications
Hande Alemdar1, Vincent Leroy1, Adrien Prost-Boucle2, Frédéric Pétrot2
1 Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, F-38000 Grenoble, France
2Univ. Grenoble Alpes, CNRS, Grenoble INP, TIMA, F-38000 Grenoble, France
{name.surname}@univ-grenoble-alpes.fr
Abstract—The computation and storage requirements for Deep
Neural Networks (DNNs) are usually high. This issue limits their
deployability on ubiquitous computing devices such as smart
phones, wearables and autonomous drones. In this paper, we
propose ternary neural networks (TNNs) in order to make deep
learning more resource-efﬁcient. We train these TNNs using a
teacher-student approach based on a novel, layer-wise greedy
methodology. Thanks to our two-stage training procedure, the
teacher network is still able to use state-of-the-art methods such
as dropout and batch normalization to increase accuracy and
reduce training time. Using only ternary weights and activations,
the student ternary network learns to mimic the behavior of its
teacher network without using any multiplication. Unlike its {-1,1}
binary counterparts, a ternary neural network inherently prunes
the smaller weights by setting them to zero during training. This
makes them sparser and thus more energy-efﬁcient. We design a
purpose-built hardware architecture for TNNs and implement it
on FPGA and ASIC. We evaluate TNNs on several benchmark
datasets and demonstrate up to 3.1× better energy efﬁciency with
respect to the state of the art while also improving accuracy.
I. INTRODUCTION
Deep neural networks (DNNs) have achieved state-of-theart results on a wide range of AI tasks including computer
vision , speech recognition and natural language processing . As DNNs become more complex, their number
of layers, number of weights, and computational cost increase.
While DNNs are generally trained on powerful servers with the
support of GPUs, they can be used for classiﬁcation tasks on a
variety of hardware. However, as the networks get bigger, their
deployability on autonomous mobile devices such as drones
and self-driving cars and mobile phones diminishes due to
the extreme hardware resource requirements imposed by high
number of synaptic weights and ﬂoating point multiplications.
Our goal in this paper is to obtain DNNs that are able to
classify at a high throughput on low-power devices without
compromising too much accuracy.
In recent years, two main directions of research have been explored to reduce the cost of DNNs classiﬁcations. The ﬁrst one
preserves the ﬂoating point precision of DNNs, but drastically
increases sparsity and weights sharing for compression , .
This has the advantage of signiﬁcantly diminishing memory
and power consumption while preserving accuracy. However,
the power savings are limited by the need for ﬂoating-point
operation. The second direction reduces the need for ﬂoatingpoint operations using weight discretization , , , ,
with extreme cases such as binary neural networks completely
eliminating the need for multiplications , , . The
main drawback of these approaches is a signiﬁcant degradation
in the classiﬁcation accuracy in return for a limited gain in
resource efﬁciency.
This paper introduces ternary neural networks (TNNs) to
address these issues and makes the following contributions:
• We propose a teacher-student approach for obtaining
Ternary NNs with weights and activations constrained to
{−1, 0, 1}. The teacher network is trained with stochastic
ﬁring using back-propagation, and can beneﬁt from all
techniques that exist in the literature such as dropout ,
batch normalization , and convolutions, The student
network has the same architecture and, for each neuron,
mimics the behavior of the equivalent neuron in the teacher
network without using any multiplications,
• We design a specialized hardware that is able to process
TNNs at up to 2.7× better throughput, 3.1× better energy
efﬁciency and 635× better area efﬁciency than state-ofthe-art and with competitive accuracy,
• We make the training code publicly available 1 and provide
a demonstration hardware design for TNNs using FPGA. 2
The rest of this paper is organized as follows. In the following
section, we introduce our procedure for training the ternary
NNs detailing our use of teacher-student paradigm to eliminate
the need for multiplications altogether during test time, while
still beneﬁting all state-of-the-art techniques such as batch
normalization and dropout during training. In Section III,
we provide a survey of related works that we compare our
performance with. We present our experimental evaluation on
ternarization and the classiﬁcation performance on ﬁve different
benchmark datasets in Section IV. In Section V, we describe
our purpose-built hardware that is able to handle both fully
connected multi-layer perceptrons (MLPs) and convolutional
NNs (CNNs) with a high throughput and a low-energy budget.
Finally, we conclude with a discussion and future studies in
Section VI.
II. TRAINING TERNARY NEURAL NETWORKS
We use a two-stage teacher-student approach for obtaining
TNNs. First, we train the teacher network with stochastically
ﬁring ternary neurons. Then, we let the student network learn
how to imitate the teacher’s behavior using a layer-wise greedy
algorithm. Both the teacher and the student networks have
the same architecture. The student network’s weights are
the ternarized version of the teacher network’s weights. The
1 
2 
 
TERNARY NEURAL NETWORK DEFINITIONS FOR A SINGLE NEURON i
Teacher network
Student Network
Wi = [wj], wj ∈R
Wi = [wj], wj ∈{−1, 0, 1}
with prob. −ρ if ρ < 0
with prob. ρ if ρ > 0
if yi < bi
if yi > bi
where ρ = tanh(yi), ρ ∈(−1, 1)
student network uses a step function with two thresholds as
the activation function. In Table I, we provide our notations
and descriptions. In order to emphasize the difference, we
denote the discrete valued parameters and inputs with a bold
font. Real-valued parameters are denoted by normal font. We
use [.] to denote a matrix or a vector. We use subscripts for
enumeration purposes and superscripts for differentiation. nt
is deﬁned as the output of neuron i in teacher network and ns
is the output of neuron i in the student network. We detail the
two stages in the following subsections.
A. The Teacher Network
The teacher network can have any architecture with any
number of neurons, and can be trained using any of the standard
training algorithms. We train the teacher network with a single
constraint only: it has stochastically ﬁring ternary neurons with
output values of −1, 0, or 1. The beneﬁt of this approach is that
we can use any technique that already exists for efﬁcient NN
training, such as batch normalization , dropout , etc. In
order to have a ternary output for teacher neuron i denoted as
i , we add a stochastic ﬁring step after the activation step. For
achieving this stochastically, we use tanh (hyperbolic tangent),
hard tanh, or soft-sign as the activation function of the teacher
network so that the neuron output has (−1, 1) range before
ternarization. We use this range to determine the ternary output
of the neuron as described in Table I. Although we do not
require any restrictions for the weights of the teacher network,
several studies showed that it has a regularization effect and
reduces over-ﬁtting , . Our approach is compatible with
such a regularization technique as well.
B. The Student Network
After the teacher network is trained, we begin the training
of the student network. The goal of the student network is to
predict the output of the teacher real-valued network. Since
we use the same architecture for both networks, there is a
one-to-one correspondence between the neurons of both. Each
student neuron denoted as ns
i learns to mimic the behavior
of the corresponding teacher neuron nt
i individually and
independently from the other neurons. In order to achieve
this, a student neuron uses the corresponding teacher neuron’s
weights as a guide to determine its own ternary weights using
two thresholds tlo
(for the lower threshold) and thi
higher one) on the teacher neuron’s weights. This step is called
tanh(W T x + b)
y!; y0; y+
Fig. 1. Example ternarization for a single neuron
the weight ternarization. In order to have a ternary neuron
output, we have a step activation function of two thresholds
hi. The output ternarization step determines these.
Figure 1 depicts the ternarization procedure for a sample
neuron. In the top row, we plot the distributions of the weights,
activations and ternary output of a sample neuron in the teacher
network respectively. The student neuron’s weight distribution
that is determined by tlo
is plotted below the teacher’s
weight distribution. We use the transfer function output of
the student neuron, grouped according to the teacher neuron’s
output on the same input, to determine the thresholds for
the step activation function. In this way, the resulting output
distribution for both the teacher and the student neurons are
similar. In the following subsections we detail each step.
1) Output Ternarization: The student network uses a twothresholded step activation function to have ternary output
as described in Table I. Output ternarization ﬁnds the step
activation function’s thresholds bi
hi, for a ternary
neuron i, for a given set of ternary weights W. In order to
achieve this, we compute three different transfer function output
distributions for the student neuron, using the teacher neuron’s
ternary output value on the same input. We use y−to denote
the set of transfer function outputs of the student neuron for
which the teacher neuron’s output value is −1. y0 and y+ are
deﬁned in the same way for teacher neuron output values 0
and 1, respectively.
We use a simple classiﬁer to ﬁnd the boundaries between
these three clusters of student neuron transfer function outputs,
and use the boundaries as the two thresholds bi
the step activation function. The classiﬁcation is done using a
linear discriminant on the kernel density estimates of the three
distributions. The discriminant between y+ and y0 is selected
hi, and the discriminant between y−and y0 gives
2) Weight Ternarization: During weight ternarization, the
order and the sign of the teacher network’s weights are
preserved. We ternarize the weights of the ith neuron of the
teacher network using two thresholds tlo
min(Wi) ⩽tlo
⩽0 and 0 ⩽thi
⩽max(Wi). The weights
for the ith student neuron are obtained by weight ternarization
as follows
ternarize(Wi|tlo
i ) = Wi = [wj]
if wj < tlo
i ⩾wj ⩾thi
if wj > thi
We ﬁnd the optimal threshold values for the weights by
evaluating the ternarization quality with a score function. For
a given neuron with p positive weights and n negative weights,
the total number of possible ternarization schemes is np since
we respect the original sign and order of weights. For a given
conﬁguration of the positive and negative threshold values thi
and tlo for a given neuron, we calculate the following score for
assessing the performance of the ternary network, mimicking
the original network.
Stlo,thi =
p(nt = ±1|xt
d)I(ns=±1|xs
d)p(nt = 0|xt
d)I(ns=0|xs
where nt and ns denote the output of the teacher neuron and
student neuron, respectively.
d is the dth input sample for the teacher neuron, and xs
is the input dth input sample for the student neuron. Note that
d after the ﬁrst layer. Since we ternarize the network
in a feed-forward manner, in order to prevent ternarization
errors from propagating to upper layers, we always use the
teacher’s original input to determine its output probability
distribution. The output probability distribution for the teacher
neuron for input d, p(nt|xt
d), is calculated using stochastic
ﬁring as described in Table I. The output probability distribution
for the student neuron for input d, p(ns|xs
d) is calculated using
the ternary weights W with the current conﬁguration of tlo, thi,
and the step activation function thresholds. These thresholds,
bhi and blo are selected according to the current ternary weight
conﬁguration W.
The output probability values are accumulated as scores over
all input samples only when the output of the student neuron
matches the output of the teacher neuron. The optimal ternarization of weights is determined by selecting the conﬁguration
with the maximum score.
W∗= arg max
The worst-case time complexity of the algorithm is
O(∥W∥2). We propose using a greedy dichotomic search
strategy instead of a fully exhaustive one. We make a search
grid over n candidate values for tlo by p values for thi.
We select two equally-spaced pivot points along one of the
dimensions, n or p. Using these pivot points, we calculate the
maximum score along the other axis. We reduce the search
space by selecting the region in which the maximum point lies.
Since we have two points, we reduce the search space to twothirds at each step. Then, we repeat the search procedure in the
reduced search space. This faster strategy runs in O(log2∥W∥),
and when there are no local maxima it is guaranteed to ﬁnd
the optimal solution. When there are multiple local extremum,
it may get stuck. Fortunately, we can detect the possible suboptimal solutions, using the score values obtained for the
student neuron. By using a threshold on the output score for a
student neuron, we can selectively use exhaustive search on a
subset of neurons. Empirically, we ﬁnd these cases to be rare.
We provide a detailed analysis in Section IV-A.
The ternarization of the output layer is slightly different
since it is a soft-max classiﬁer. In the ternarization process,
instead of using the teacher network’s output, we use the actual
labels in the training set. Again, we treat neurons independently
but we make several iterations over each output neuron in a
round-robin fashion. After each iteration we check against
convergence. In our experiments, we observed that the method
converges after a few passes over all neurons.
Our layer-wise approach allows us to update the weights of
the teacher network before ternarization of any layer. For this
optional weight update, we use a staggered retraining approach
in which only the non-ternarized layers are modiﬁed. After
the teacher network’s weights are updated, input to a layer for
both teacher and student networks become equal, xt
We use early stopping during this optional retraining and we
ﬁnd that a few dozen of iterations sufﬁce.
III. RELATED WORK
In this section, we give a brief survey on several related
works in energy-efﬁcient NNs. In Table III, we provide a
comparison between our approach and the related works
that use binary or ternary weights in the deployment phase
by summarizing the constraints put on inputs, weights and
activations during training and testing.
Courbariaux et al. propose the BinaryConnect (BC)
method for binarizing only the weights, leaving the inputs
and the activations as real-values. The same idea is also
used as TernaryConnect (TC) in and Ternary Weight
Networks (TWN) in with ternary {−1, 0, 1} weights instead
of binary {−1, 1}. They use the back-propagation algorithm
with an additional weight binarization step. In the forward
pass, weights are binarized either deterministically using their
sign, or stochastically. Stochastic binarization converts the realvalued weights to probabilities with the use of the hard-sigmoid
function, and then decides the ﬁnal value of the weights with
this. In the back-propagation phase, a quantization mechanism
is used so that the multiplication operations are converted to bitshift operations. While this binarization scheme helps reducing
the number of multiplications during training and testing, it is
not fully hardware-friendly since it only reduces the number
of ﬂoating point multiplication operations. Recently, the same
idea is extended to the activations of the neurons also . In
Binarized NN, the sign activation function is used for obtaining
binary neuron activations. Also, shift-based operations are used
during both training and test time in order to gain energyefﬁciency. Although this method helps improving the efﬁciency
in multiplications it does not eliminate them completely.
XNOR-Nets provide a solution to convert convolution
operations in CNNs to bitwise operations. The method ﬁrst
learns a discrete convolution together with a set of real-valued
scaling factors (K, α ∈R). After the convolution calculations
are handled using bit-wise operations, the scaling factors are
COMPARISON OF SEVERAL APPROACHES FOR RESOURCE-EFFICIENT NEURAL NETWORKS
Deployment
Activations
Activations
BC , TC , TWN 
{−1, 0, 1}
{−1, 0, 1}
Binarized NN 
XNOR-Net 
{−1, 1} with K, α ∈R
{−1, 1} with K, α ∈R
{−1, 0, 1}
Bitwise NN 
{−1, 0, 1}
{−1, 0, 1}
TrueNorth 
{−1, 0, 1}
TNN (This Work)
{−1, 0, 1}
{−1, 0, 1}
{−1, 0, 1}
{−1, 0, 1}
{−1, 0, 1}
applied to obtain actual result. This approach is very similar
to Binarized NN and helps reducing the number of ﬂoating
point operations to some extent.
Following the same goal, DoReFa-Net and Quantized
Neural Networks (QNN) propose using n-bit quantization
for weights, activations as well as gradients. In this way, it is
possible to gain speed and energy efﬁciency to some extent not
only during training but also during inference time. Han et al.
 combine several techniques to achieve both quantization and
compression of the weights by setting the relatively unimportant
ones to 0. They also develop a dedicated hardware called
Efﬁcient Inference Engine (EIE) that exploits the quantization
and sparsity to gain large speed-ups and energy savings, only
on fully connected layers currently .
Soudry et al. propose Expectation Backpropagation
(EBP), an algorithm for learning the weights of a binary
network using a variational Bayes technique. The algorithm
can be used to train the network such that, each weight can
be restricted to be binary or ternary values. The strength of
this approach is that the training algorithm does not require
any tuning of hyper-parameters, such as learning rate as in
the standard back-propagation algorithm. Also, the neurons
in the middle layers are binary, making it hardware-friendly.
However, this approach assumes the bias is real and it is not
currently applicable to CNNs.
All of the methods described above are only partially
discretized, leading only to a reduction in the number of ﬂoating
point multiplication operations. In order to completely eliminate
the need for multiplications which will result in maximum
resource efﬁciency, one has to discretize the network completely
rather than partially. Under these extreme limitations, only a
few studies exist in the literature.
Kim and Smaragdis propose Bitwise NN which is a
completely binary approach, where all the inputs, weights, and
the outputs are binary. They use a straightforward extension of
back-propagation to learn bitwise network’s weights. First, a
real-valued network is trained by constraining the weights of
the network using tanh. Also tanh non-linearity is used for the
activations to constrain the neuron output to (−1, 1). Then, in
a second training step, the binary network is trained using the
real-valued network together with a global sparsity parameter.
In each epoch during forward propagation, the weights and the
activations are binarized using the sign function on the original
constrained real-valued parameters and activations. Currently,
CNNs are not supported in Bitwise-NNs.
IBM announced an energy efﬁcient TrueNorth chip, designed
for spiking neural network architectures . Esser et al.
 propose an algorithm for training networks that are
compatible with IBM TrueNorth chip. The algorithm is based
on backpropagation with two modiﬁcations. First, Gaussian
approximation is used for the summation of several Bernoulli
neurons, and second, values are clipped to satisfy the boundary
requirements of TrueNorth chip. The ternary weights are
obtained by introducing a synaptic connection parameter that
determines whether a connection exits. If the connection exists,
the sign of the weight is used. Recently, the work has been
extended to CNN architectures as well .
IV. EXPERIMENTAL ASSESSMENT OF TERNARIZATION AND
CLASSIFICATION
The main goals of our experiments are to demonstrate, (i)
the performance of the ternarization procedure with respect
to the real-valued teacher network, and (ii) the classiﬁcation
performance of fully discretized ternary networks.
We perform our experiments on several benchmarking
datasets using both multi-layer perceptrons (MLP) in a
permutation-invariant manner and convolutional neural networks (CNN) with varying sizes. For the MLPs, we experiment
with different architectures in terms of depth and neuron count.
We use 250, 500, 750, and 1000 neurons per layer for 2, 3,
and 4 layer networks. For the CNNs, we use the following
VGG-like architecture proposed by :
(2 × nC3) −MP2 −(2 × 2nC3) −MP2 −(2 × 4nC3) −
MP2 −(2 × 8nFC) −L2SVM
where C3 is a 3 × 3 convolutional layer, MP2 is a 2 × 2 maxpooling layer, FC is a fully connected layer. We use L2SVM
as our output layer. We use two different network sizes with
this architecture with n = 64 and n = 128. We call these
networks CNN-Small and CNN-Big, respectively.
We perform our experiments on the following datasets:
MNIST database of handwritten digits is a well-studied
database for benchmarking methods on real-world data. MNIST
has a training set of 60K examples, and a test set of 10K
examples of 28 × 28 gray-scale images. We use the last 10K
samples of the training set as a validation set for early stopping
and model selection.
CIFAR-10 and CIFAR-100 are two color-image classi-
ﬁcation datasets that contain 32×32 RGB images. Each dataset
consists of 50K images in training and 10K images in test sets.
In CIFAR-10, the images come from a set of 10 classes that
contain airplanes, automobiles, birds, cats, deer, dogs, frogs,
horses, ships and trucks. In CIFAR-100, the number of image
classes is 100.
SVHN (Street View House Numbers) consists of 32×
32 RGB color images of digits cropped from Street View
images. The total training set size is 604K examples (with
531K less difﬁcult samples to be used as extra) and the test
set contains 26K images.
GTSRB (German Trafﬁc Sign Recognition Benchmark
Dataset) is composed of 51839 images of German road
signs in 43 classes. The images have great variability in terms
of size and illumination conditions. Also, the dataset has
unbalanced class frequencies. The images in the dataset are
extracted from 1-second video sequences recorded at 30 fps.
In order to have a representative validation set, we extract 1
track at random per trafﬁc sign for validation. The number of
images in train, validation and test set are 37919, 1290 and
12630 respectively.
In order to allow a fair comparison against related works,
we perform our experiments in similar conﬁgurations. On
MNIST, we only use MLPs. We minimize cross entropy loss
using stochastic gradient descent with a mini-batch size of 100.
During training we use random rotations up to ±10 degrees. We
report the test error rate associated with the best validation error
rate after 1000 epochs. We do not perform any preprocessing
on MNIST and we use a threshold-based binarization on the
For other datasets, we use two CNN architectures: CNN-
Small and CNN-Big. As before, we train a teacher network
before obtaining the student TNN. For the teacher network,
we use a modiﬁed version of Binarized NN’s algorithm 
and ternarize the weights during training. In this way, we
obtain a better accuracy on the teacher network and we gain
considerable speed-up while obtaining the student network.
Since we already have the discretized weights during the teacher
network training, we only mimic the output of the neurons using
the step activation function with two thresholds for the student
network. During teacher network training, we minimize squared
Hinge loss with adam with mini-batches of size 200. We train at
most 1000 epochs and report the test error rate associated with
the best validation epoch. For input binarization, we use the
approach described in with either 12 or 24 (on CIFAR100)
transduction ﬁlters. We do not use any augmentation on the
A. Ternarization Performance
The ternarization performance is the ability of the student
network to imitate the behavior of its teacher. We measure
this by using the accuracy difference between the teacher
network and the student network. Table III shows this difference
between the teacher and student networks on training and
test sets for three different exhaustive search threshold values.
0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99
Neurons (%)
Percentage of Neurons for Exhaustive Search
0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99
Performance on Training Data
0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99
Performance on Test Data
The effect of threshold values on run-time and classiﬁcation
performance
ε = 1 corresponds to the fully exhaustive search case whereas
ε = 0 represents fully dichotomic search. The results show
that the ternarization performance is better for deeper networks.
Since we always use the teacher network’s original output as
a reference, errors are not ampliﬁed in the network. On the
contrary, deeper networks allow the student network to correct
some of the mistakes in the upper layers, dampening the errors.
Also, we perform a retraining step with early stopping before
ternarizing a layer, since it slightly improves the performance.
The ternarization performance generally decreases with lower
ε threshold values, but the decrease is marginal. On occasion,
ACCURACY GAP DUE TO TERNARIZATION FOR DIFFERENT ε
performance even increases. This is due to teacher network’s
weight update, that allows the network to escape from local
minima. In order to demonstrate the effect of ε in terms of
run-time and classiﬁcation performance, we conduct a detailed
analysis without the optional staggered retraining. Figure 2
shows the distribution of the ratio of neurons that are ternarized
exhaustively with different ε, together with the performance
gaps on training and test datasets. The optimal trade-off is
achieved with ε = 0.95. Exhaustive search is used for only
20% of the neurons, and the expected value of accuracy gaps
is practically 0. For the largest layer with 1000 neurons, the
ternarization operations take 2 min and 63 min for dichotomic
and exhaustive search, respectively, on a 40-core Intel(R)
Xeon(R) CPU E5-2650 v3 @ 2.30GHz server with 128 GB
RAM. For the output layer, the ternarization time is reduced
to 21 min with exhaustive search.
B. Classiﬁcation Performance
The classiﬁcation performance in terms of error rate (%)
on several benchmark datasets is provided in Table IV. We
compare our results to several related methods that we described
in the previous section. We make a distinction between the
fully discretized methods and the partially discretized ones
because only in the latter, the resulting network is completely
discrete and requires no ﬂoating points and no multiplications,
providing maximum energy efﬁciency.
Since the benchmark datasets we use are the most studied
ones, there are several known techniques and tricks that give
performance boosts. In order to eliminate unfair comparison
among methods, we follow the majority’s lead and we do
not use any data augmentation in our experiments. Moreover,
using an ensemble of classiﬁers is a common technique for
performance boosting in almost all classiﬁers and is not unique
to neural networks . For that reason, we do not use an
ensemble of networks and we cite the compatible results in
other related works.
MNIST is by far the most studied dataset in deep learning
literature. The state-of-the-art is already down to 21 erroneous
classiﬁcations (0.21%) which is extremely difﬁcult to obtain
without extensive data augmentation. TNN’s error rate on
MNIST is 1.67% with a single 3-layer MLP with 750 neurons
CLASSIFICATION PERFORMANCE - ERROR RATES (%)
Fully Discretized
TNN (This Work)
TrueNorth , 
Bitwise NN 
Partially Discretized
Binarized NN 
XNOR-Net 
DoReFa-Net 
in each layer. Bitwise NNs with 1024 neurons in 3
layers achieves a slightly better performance. TNN with an
architecture that has similar size to Bitwise NN is worse due to
over-ﬁtting. Since TNN selects a different sparsity level for each
neuron, it can perform better on smaller networks, and larger
networks cause over-ﬁtting on MNIST. Bitwise NN’s global
sparsity parameter has a better regularization effect on MNIST
for relatively bigger networks. Its performance with smaller
networks or on other datasets is unknown. TrueNorth with
a single network achieves only 7.30% error rate. To alleviate
the limitations of single network performance, a committee of
networks can be used, reducing the error rate to 0.58% with
64 networks.
The error rate of TNN on CIFAR10 is 12.11%. When
compared to partially discretized alternatives, a fully discretized
TNN is obtained at the cost a few points in the accuracy and
exceeds the performance of TrueNorth by more than 4%. On
SVHN, it has a similar achievement with lower margins. For
CIFAR100, on the other hand, it does not perform better than
TrueNorth. Given the relatively lower number of related works
that report results on CIFAR100 as opposed to CIFAR10, we
can conclude that this is a more challenging dataset for resourceefﬁcient deep learning with a lot of room for improvement.
TNN has the most remarkable performance on GTSRB dataset.
With 0.98% error rate, CNN-Big model exceeds the human
performance which is at 1.16%.
Partially discretized approaches use real-valued input which
contains more information. Therefore, it is expected that they
are able to get higher classiﬁcation accuracy. When compared to
partially discretized studies, TNNs only lose a small percentage
of accuracy and in return they provide better energy efﬁciency.
Next, we describe the unique hardware design for TNNS and
investigate to which extent TNNs are area and energy efﬁcient.
V. PURPOSE-BUILT HARDWARE FOR TNN
We designed a hardware architecture for TNNs that is
optimized for ternary neuron weights and activation values
{−1, 0, +1}. In this section we ﬁrst describe the purpose-built
hardware we designed and evaluate its performance in terms
of latency, throughput and energy and area efﬁciency.
A. Hardware Architecture
Figure 3 outlines the hardware architecture of a fullyconnected layer in a multi-layer NN. The design forms a
pipeline that corresponds to the sequence of NN processing
steps. For efﬁciency reasons, the number of layers and the
maximum layer dimensions (input size and number of neurons)
are decided at synthesis time. For a given NN architecture, the
Activation
Output Layer
Activation
Fig. 3. Hardware implementation scheme of ternary neural network
design is still user-programmable: each NN layer contains a
memory that can be programmed at run-time with neuron
weights or output ternarization thresholds blo and bhi. As
seen in the previous experiments of Section IV, a given NN
architecture can be reused for different datasets with success.
Ternary values are represented with 2 bits using usual two’s
complement encoding. That way, the compute part of each
neuron is reduced to just one integer adder/subtractor and one
register, both of width ⌈log2 k⌉+ 1 bits, where k is the input
size for the neuron. So each neuron is only a few tens of
ASIC gates, which is very small. Inside each layer, all neurons
work in parallel so that one new input item is processed per
clock cycle. Layers are pipelined in order to simultaneously
work on different sets of inputs, i.e. layer 2 processes image n
while layer 1 processes image n + 1. The ternarization block
processes the neuron outputs sequentially, so it consists of the
memory of threshold values, two signed comparators and a
multiplexer.
We did a generic register transfer level (RTL) implementation
that can be synthesized on both Field-Programmable Gate
Array (FPGA) and Application-speciﬁc Integrated Circuit
(ASIC) technologies. FPGAs are reprogrammable off-theshelf circuits and are ideal for general-purpose hardware
acceleration. Typically, high-performance cloud solutions use
high-end FPGA tightly coupled with general-purpose multicore
processors , while ASIC is used for more throughput or
in battery-powered embedded devices.
B. Hardware Performance
For the preliminary measurements, we used the dataset
MNIST and the FPGA board Sakura-X because it features
precise power measurements capabilities. It can accommodate
a 3-layer fully connected NN with 1024 neurons per layer
(for a total of 3082 neurons), using 81% of the Kintex-7 160T
The performance of our FPGA design in terms of latency,
throughput and energy efﬁciency is given in Table V. With
a 200 MHz clock frequency, the throughput (here limited
by the number of neurons) is 195 K images/s with a power
consumption of 3.8 W and a classiﬁcation latency of 20.5 µs.
FPGA HARDWARE PERFORMANCE OF MLPS ON SAKURA-X
Throughput
Latency µs
(per image)
(per image)
COMPARISON OF SEVERAL HARDWARE SOLUTIONS FOR MLP
TrueNorth EIE 64PE EIE 256PE 
Technology
Virtex-7 ST 28 nm
Clock (MHz)
Quantization
Throughput (fps)
Energy Eff. (fps/W)
Area (mm2)
Area Eff. (fps/mm2)
We know that TrueNorth can operate at the two extremes
of power consumption and accuracy. It consumes 0.268 µJ with
a network of low accuracy (92.7%), and consumes as high as
108 µJ with a committee of 64 networks that achieves 99.4%.
Our hardware cannot operate at these two extremes, yet in the
middle operating zone, we outperform TrueNorth both in terms
of energy-efﬁciency - accuracy trade-off and speed. TrueNorth
consumes 4 µJ per image with 95% accuracy with a throughput
of 1000 images/s, and with 1 ms latency. Our TNN hardware,
consuming 3.63 µJ per image achieves 98.14% accuracy at a
rate of 255 102 images/s, and a latency of 8.09 µs.
For the rest of the FPGA experiments, the larger board
VC709 equipped with the Virtex-7 690T FPGA is used because
it can support much larger designs. We also synthesized
the design as ASIC using STMicroelectronics 28 nm FDSOI
manufacturing technology. The results are given in Table VI.
We compare our FPGA and ASIC solutions with the state of
the art: TrueNorth and EIE .
The ASIC version compares very well with TrueNorth on
throughput, area efﬁciency (fps/mm2) and energy efﬁciency
(fps/W). Even though EIE uses 16-bit precision, it achieves
high throughput because it takes advantage of weight sparsity
and skips many useless computations. However, we achieve
better energy and area efﬁciencies since all our hardware
elements (memories, functional units etc.) are signiﬁcantly
reduced thanks to ternarization. Our energy results would be
even better if taking into account weight sparsity and zeroactivations (e.g. when input values are zero) like done in EIE
Finally, we implemented the CNN-Big and CNN-Small
described in Section IV, on both FPGA and ASIC. Results
are given in Table VII. We give worst-case FPGA results
because this is important for users of general-purpose hardware
accelerators. For ASIC technology, we took into account perdataset zero-activations to reduce power consumption, similar
to what was done in EIE works. We compare with TrueNorth
because only paper gives ﬁgures of merit related to CNNs
on ASIC. The TrueNorth area is calculated according to the
number of cores used. Using different CNN models than
TrueNorth’s, we achieve better accuracy on three datasets out of
four, while having higher throughput, better energy efﬁciency
and much better area efﬁciency.
HARDWARE PERFORMANCE OF CNNS
TNN FPGA 200 MHz
TNN ASIC ST 28 nm 500 MHz
TrueNorth 
Throughput (fps)
Energy per image (µJ)
Energy Efﬁciency (fps/W)
Area (mm2)
Area Efﬁciency (fps/mm2)
Accuracy (%)
VI. DISCUSSIONS AND FUTURE WORK
In this study, we propose TNNs for resource-efﬁcient
applications of deep learning. Energy efﬁciency and area
efﬁciency are brought by not using any multiplication nor any
ﬂoating-point operation. We develop a student-teacher approach
to train the TNNs and devise a purpose-built hardware for
making them available for embedded applications with resource
constraints. Through experimental evaluation, we demonstrate
the performance of TNNs both in terms of accuracy and
resource-efﬁciency, with CNNs as well as MLPs. The only
other related work that has these two features is TrueNorth
 , since Bitwise NNs do not support CNNs . In terms of
accuracy, TNNs perform better than TrueNorth with relatively
smaller networks in all of the benchmark datasets except one.
Unlike TrueNorth and Bitwise NNs, TNNs use ternary neuron
activations using a step function with two thresholds. This
allows each neuron to choose a sparsity parameter for itself
and gives an opportunity to remove the weights that have very
little contribution. In that respect, TNNs inherently prune the
unnecessary connections.
We also develop a purpose-built hardware for TNNs that
offers signiﬁcant throughput and area efﬁciency and highly
competitive energy efﬁciency. As compared to TrueNorth, our
TNN ASIC hardware offers improvements of 147× to 635×
on area efﬁciency, 1.4× to 3.1× on energy efﬁciency and 2.1×
to 2.7× on throughput. It also often has higher accuracy with
our new training approach.
ACKNOWLEDGMENT
This project is being funded in part by Grenoble Alpes
Métropole through the Nano2017 Esprit project. The authors
would like to thank Olivier Menut from ST Microelectronics
for his valuable inputs and continuous support.