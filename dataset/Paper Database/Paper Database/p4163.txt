Benchmarking Graph Neural Networks
Vijay Prakash Dwivedi1
 
Chaitanya K. Joshi2
 
Anh Tuan Luu1
 
Thomas Laurent3
 
Yoshua Bengio4
 
Xavier Bresson5
 
1Nanyang Technological University, Singapore, 2University of Cambridge, UK, 3Loyola Marymount
University, USA, 4Mila, University of Montréal, Canada, 5National University of Singapore
In the last few years, graph neural networks (GNNs) have become the standard toolkit
for analyzing and learning from data on graphs. This emerging ﬁeld has witnessed an
extensive growth of promising techniques that have been applied with success to computer
science, mathematics, biology, physics and chemistry. But for any successful ﬁeld to become
mainstream and reliable, benchmarks must be developed to quantify progress. This led us
in March 2020 to release a benchmark framework that i) comprises of a diverse collection
of mathematical and real-world graphs, ii) enables fair model comparison with the same
parameter budget to identify key architectures, iii) has an open-source, easy-to-use and
reproducible code infrastructure, and iv) is ﬂexible for researchers to experiment with new
theoretical ideas. As of December 2022, the GitHub repository1 has reached 2,000 stars and
380 forks, which demonstrates the utility of the proposed open-source framework through
the wide usage by the GNN community. In this paper, we present an updated version of our
benchmark with a concise presentation of the aforementioned framework characteristics, an
additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with
a real-world measured chemical target, and discuss how this framework can be leveraged to
explore new GNN designs and insights. As a proof of value of our benchmark, we study the
case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark
and has since spurred interest of exploring more powerful PE for Transformers and GNNs
in a robust experimental setting.
Graph Neural Networks, Benchmarking, Graph Datasets, Exploration Tool
1. Introduction
Graph neural networks have beneﬁtted from a great interest recently with numerous methods
being developed for diverse domains including chemistry , physics , social sciences , transportation , knowledge graphs , recommendation , and
neuroscience . Developing powerful and expressive GNN architectures is a
key concern towards practical applications and real-world adoption of graph machine learning.
1. The framework is hosted at 
©2022 Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier
License: CC-BY 4.0, see 
 
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
However, tracking progress is often challenging in the absence of a community-standard
benchmark as models that are evaluated on traditionally-used datasets with inconsistent
experimental comparisons make it diﬃcult to diﬀerentiate complex, simple and graph-agnostic
architectures .
Benchmarking Graph Neural Networks Framework
- Imports methods from other
modules: data, configs,nets,
- Executes 1 run of a single
experiment
- Selected config file decides
which GNN network to run on
which dataset.
- Customizable, eg. execution
of a different set of modules.
- 1 file per task-dataset,
ensuring the same fair
comparison of all GNNs for
the task-dataset chosen.
- Data loading files
- Customizable, eg. add
positional encodings
- Add custom dataset
- Config files for gnn,
dataset, parameters
and hyperparams.
- Customizable, eg.
tweak hyperparams.
- Add custom configs
- GNN network files of
multiple layers
- Customizable, eg.
try combination of
different layers
- GNN layer definition
- Customizable, eg.
adjust aggregation fn,
or activations.
- Add custom layer
- Train and Eval scripts
- Different scripts for
sparse MP-GNNs and
dense WL-GNNs
- Customizable, eg.
tweak eval metrics
DGL graph learning library based on PyTorch
Each module can be customized, and
extended to add new datasets (in data
module), or new GNNs (in layers module),
as few examples.
Figure 1: Overview sketch of the proposed GNN benchmarking framework with diﬀerent modular components. This
benchmark is built upon DGL and PyTorch libraries.
opensource
benchmarking
framework (see Fig 1) that
brings forward a set of diverse
medium-scale datasets which
are discriminative to benchmark diﬀerent GNN models
when compared fairly on ﬁxed
parameter budgets. The existing collection of datasets, the
protocol to use the same parameter budgets for comparison, and the modular coding
infrastructure has been widely
used to prototype powerful
GNN ideas and develop new
insights, as shown by 2000+ stars and 380+ forks of the GitHub repository from its ﬁrst
release in March 2020, and 470+ citations gathered by the ArXiv technical report according
to Google Scholar. Aspects of the benchmark have led to facilitating several interesting
studies for GNNs such as on (i) the aggregation functions and ﬁlters , (ii) improving expressive power of GNNs , (iii) pooling mechanisms , (iv) graph-speciﬁc normalization and regularization , and (v) GNNs’ robustness and eﬃciency among other ideas contributed in the literature. In this paper,
we provide an updated overview of the proposed framework that extends on the previous
collection of datasets to (a) include a number of essential mathematical datasets which can
be used to test speciﬁc theoretical graph properties, and (b) incorporate another molecular
dataset, AQSOL that has real-world experimental solubility targets
unlike ZINC’s computed targets, resulting in a collection of 12 datasets (see Table 1). The
remainder of the paper discusses a proof of concept of the benchmark that can be used to
explore and develop new insights for GNNs.
2. Overview of GNN Benchmarking Framework
Datasets. Collecting representative, realistic and medium-to-large scale graph datasets
presents several challenges. It is unclear what theoretical tools can deﬁne the quality of a
dataset or validate its statistical representativeness for a given task. Similarly, there are
several arbitrary choices when preparing graphs, such as node and edge features. Finally,
very large graph datasets also present a computational challenge and require extensive GPU
resources to be studied .
Benchmarking Graph Neural Networks
A. Real World Graphs
Graph Regression
Social/Academic Networks
OGBL-COLLAB
Edge Classiﬁcation
Node Classiﬁcation
Computer Vision
Graph Classiﬁcation
B. Mathematical Graphs
Mathematical Modelling
Node Classiﬁcation
Combinatorial Optimization
Edge Classiﬁcation
Isomorphism
Graph Classiﬁcation
Cycles in Graphs
Graph Classiﬁcation
Multi Graph Properties
GraphTheoryProp
Multi Node/Graph Task
Table 1: Summary statistics of datasets included in the benchmark. Additional details
in Appendix Table 2 and Sec. C.
On account of such challenges, we present
in our benchmark a collection of 12 graph
datasets, listed in Table 1, which are (i) collected from real-world sources and generated
from mathematical models, (ii) of mediumscale size suitable for academic research, (iii)
representative of the three fundamental learning tasks at graph-level, node-level and edgelevel, and (iv) from diverse end-application
domains. These datasets are appropriate to
statistically separate the performance of GNNs
on speciﬁc graph properties, hence fulﬁlling the
academic mission to identify ﬁrst principles.
Coding Infrastructure. Our benchmarking
infrastructure builds upon PyTorch and DGL , and has been developed with the following
fundamental objectives: (a) Ease-of-use and modularity, enabling new users to experiment
and study the building blocks of GNNs; (b) Experimental rigour and fairness for all models
being benchmarked; and (c) Being future-proof and comprehensive for tracking the progress
of graph ML tasks and new GNNs. At a high level as sketched in Fig 1, our benchmark
uniﬁes independent components for: (i) Data pipelines; (ii) GNN layers and models; (iii)
Training and evaluation functions; (iv) Network and hyperparameter conﬁgurations; and (v)
Scripts for reproducibility. This standardized framework has been of immense help to the
community as aforementioned about its wide community usage. It has enabled researchers
to explore new ideas at any stage of the pipeline without setting up everything else. We
direct readers to the README user manual included in our GitHub repository for detailed
instructions on using the coding infrastructure.
Parameter Budgets for Fair Comparison. One goal of this benchmark is not to ﬁnd
the optimal hyperparameters for a speciﬁc model (which is computationally expensive), but
to compare the model and their building blocks within a budget of parameters. Therefore,
we decide on using two model parameter budgets: i) 100k for each GNN for all the datasets,
and ii) 500k for GNNs where the scalability of a model to larger parameters and deeper layers
are investigated. The layers and dimensions are selected accordingly to match these budgets.
Discussion on Design Choices. First, our motivation behind the medium-scale datasets
in the benchmark is to enable swift yet reliable prototyping of GNN research ideas as we can
achieve statistical diﬀerence in GNN performance within 12 hours of single experiment runs
(see Appendix I). Medium-scale datasets are arguably more informative than small datasets
and more feasible than large-scale datasets in the academic-scale research. Second, our
coding infrastucture with standard protocols has enabled fair comparison of GNNs something
that was lacking in prior literature . Third, a ﬁxed budget of model
parameters for each GNN model allows for fair comparison of diﬀerent architectures. In
the absence of such design choice, it is comparatively diﬃcult to conclude whether a better
performing model’s gain arises from its architectural design or extra learning capacity brought
by additional model parameters. Finally, the aforementioned decisions can be reﬁned and
extended to allow further ﬂexibility as elaborated in Appendix H.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
3. How can the benchmark be used to explore new insights?
The proposed benchmarking framework can be used to test new research ideas at the level
of data preprocessing, improving the GNN layers and normalization schemes, or even to
substantiate the performance of a novel GNN model. Such studies are conveniently facilitated
given the set of diverse datasets and the rigorous comparison of diﬀerent experiments on same
parameter budgets. At any stage, a modular component of the framework, such as data,
layers, etc., can be modiﬁed and multiple experiments on the datasets can be conducted
fairly and with ease. Indeed, we employ the framework to perform multiple studies, out
of which we present here the insight of positional encodings for GNNs using Laplacian
eigenvectors, for an example, while the remainder is included in the appendix.
Graph Positional Encoding. Nodes in a graph do not have any canonical positional
information. In the absence of available features, nodes are anonymous, such as the nodes in
CSL, CYCLES or GraphTheoryProp datasets in our benchmark. As such, message passing
based GCNs perform either poorly or fail completely to detect the class of the graph, such
as isomorphic class, or cycles . We proposed the use of
Laplacian eigenvectors as node positional encoding by building on
top of corresponding dataset ﬁles in the data module as shown in the pseudo-code snippet
alongside. In other words, the positional encoding pi for a node i can be added to its features
xi as xi = xi + pi. Similarly, other ideas can be explored by leveraging respective modules of
the framework (in Fig 1) for which we direct to README of our GitHub repository.
class NameOfDataset(torch.utils.data.Dataset):
def __init__(self, name=‘name_of_dataset’):
# existing code to load dataset
def _add_positional_encodings(self, args):
# new code that precomputes and adds
# positional encoding using eigenvectors
Figure 2: Primary code block in data
module to implement Graph PE.
We used the benchmark to validate and also
quantiﬁed the improvement provided by this
idea. The Laplacian PE eﬀectively improved
the MP-GCNs (message-passing based Graph
Convolutional Networks) on the the 3 synthetics datasets mentioned previously and other
real-world datasets, including the newly added
AQSOL dataset. A detailed presentation of the PE with experiments are in Appendix E.1.
After the introduction of Laplacian PE through this benchmark, new ideas followed up in the
literature for improving PE , thus demonstrating how the identiﬁcation
of ﬁrst principles using the proposed benchmark can steer GNN research.
4. Conclusion
This paper introduces an open-source benchmarking framework for Graph Neural Networks
that is modular, easy-to-use, and can be leveraged to quickly yet robustly test new GNN
ideas and explore insights that direct further research. The benchmark led us to propose
graph PE that has remained an interesting avenue of exploration since the ﬁrst release of our
benchmark. We also perform additional studies on investigation of diﬀerent GNN categories,
and edge representations for link prediction, the details of which are included in the appendix
for interested readers.
Benchmarking Graph Neural Networks
Acknowledgments
XB is supported by NRF Fellowship NRFF2017-10, NUS-R-252-000-B97-133 and A*STAR
Grant ID A20H4g2141. This research is supported by Nanyang Technological University,
under SUG Grant (020724-00001). The authors thank the reviewers and the editor for their
comments and suggestions, which greatly improved the manuscript.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
A. Related Work
In the last few years, graph neural networks (GNNs) have seen a great surge of interest with
promising methods being developed for myriad of domains including chemistry , physics ,
social sciences , knowledge graphs , recommendation , and
neuroscience . Historically, three classes of GNNs have been developed.
The ﬁrst models aimed at extending the original
convolutional neural networks to graphs. The second class enhanced
the original models with anisotropic operations on graphs , such
as attention and gating mechanisms . The recent third
class has introduced GNNs that improve upon theoretical limitations of previous models . Speciﬁcally, the ﬁrst two classes can only diﬀerentiate simple
non-isomorphic graphs and cannot separate automorphic nodes. Developing powerful and
theoretically expressive GNN architectures is a key concern towards practical applications
and real-world adoption of graph machine learning. However, tracking recent progress has
been challenging as most models are evaluated on small datasets such as Cora, Citeseer and
TU, which are inappropriate to diﬀerentiate complex, simple and graph-agnostic architectures
 , and do not have consensus on a unifying
experimental setting .
Consequently, our motivation is to benchmark GNNs to identify and quantify what types
of architectures, ﬁrst principles or mechanisms are universal, generalizable, and scalable
when we move to larger and more challenging datasets. Benchmarking provides a strong
paradigm to answer these fundamental questions. It has proved to be beneﬁcial for driving
progress, identifying essential ideas, and solving domain-speciﬁc problems in several areas of
science . Recently, the famous 2012 ImageNet challenge has provided a benchmark dataset that has triggered the deep learning revolution
 . Nevertheless, designing successful benchmarks is
highly challenging as it requires both a coding framework with a rigorous experimental setting
for fair comparisons, all while being reproducible, as well as using appropriate datasets that
can statistically separate model performance. The lack of benchmarks has been a major issue
in GNN literature as the aforementioned requirements have not been rigorously enforced.
B. Graph Neural Network Pipeline
In this section, we describe the experimental pipeline for the two broad classes of GNN
architectures that are benchmarked in this framework as representative GNN classes – Message
Passing Graph Convolutional Networks (MP-GCNs), which are based on the message passing
framework formalized in Gilmer et al. , and Weisfeiler Lehman GNNs (WL-GNNs),
which improves the theoretical limitations of MP-GCNs and align expressivity power to the
Benchmarking Graph Neural Networks
WL-tests to distinguish non-isomorphic graphs. The two pipelines are illustrated in Figure 3
for GCNs and Figure 4 for WL-GNNs.
In Section B.1, we describe the components of the setup of the GCN class with vanilla
GCN , GraphSage , MoNet , GAT , and GatedGCN , including
the input layers, the GNN layers and the task based MLP classiﬁer layers. We also include
the description of GIN in this section as this model can be interpreted as a
GCN, although it was designed to diﬀerentiate non-isomorphic graphs. In Section B.2, we
present the GNN layers and the task based MLP classiﬁer layers for the class of WL-GNN
models with Ring-GNNs and 3WL-GNNs .
Node feat.
Edge feat.
Layer ℓ: {
Layer ℓ+ 1 : {
Node Predictions
Graph Prediction
Edge Predictions
Input Layer
Prediction Layer
Figure 3: A standard experimental pipeline for GCNs, which embeds the graph node and
edge features, performs several GNN layers to compute convolutional features, and ﬁnally
makes a prediction through a task-speciﬁc MLP layer.
B.1 Message-Passing GCNs
For this class, we consider the widely used message passing-based graph convolutional
networks (MP-GCNs), which update node representations from one layer to the other
according to the formula: hℓ+1
j}j∈Ni). Note that the update equation is local,
only depending on the neighborhood Ni of node i, and independent of graph size, making
the space/time complexity O(E) reducing to O(n) for sparse graphs. Thus, MP-GCNs are
highly parallelizable on GPUs and are implemented via sparse matrix multiplications in
modern graph machine learning frameworks .
MP-GCNs draw parallels to ConvNets for computer vision by considering
a convolution operation with shared weights across the graph domain.
B.1.1 Input Layer
Given a graph, we are given node features αi ∈Ra×1 for each node i and (optionally) edge
features βij ∈Rb×1 for each edge connecting node i and node j. The input features αi
and βij are embedded to d-dimensional hidden features hℓ=0
via a simple linear
projection before passing them to a graph neural network:
i = U0αi + u0 ; e0
ij = V 0βij + v0,
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Node feat.
Edge feat.
Input Tensor
WL-GNN Layer
Prediction Layer
Input 3D tensor
Node Predictions
Graph Prediction
Edge Predictions
*Details in Section B.2.3
Figure 4: A standard experimental pipeline for WL-GNNs, which inputs to a GNN a graph
with all node and edge information (if available) represented by a dense tensor, performs
several GNN layer computations over the dense tensor, and ﬁnally makes a prediction through
a task-speciﬁc MLP layer.
where U0 ∈Rd×a, V 0 ∈Rd×b and u0, v0 ∈Rd. If the input node/edge features are one-hot
vectors of discrete variables, then biases u0, v0 are not used.
B.1.2 GCN layers
Each GCN layer computes d-dimensional representations for the nodes/edges of the graph
through recursive neighborhood diﬀusion (or message passing), where each graph node
gathers features from its neighbors to represent local graph structure. Stacking L GCN layers
allows the network to build node representations from the L-hop neighborhood of each node.
Figure 5: A generic graph neural network layer. Figure adapted from Bresson and Laurent
i denote the feature vector at layer ℓassociated with node i. The updated features
at the next layer ℓ+ 1 are obtained by applying non-linear transformations to the
central feature vector hℓ
i and the feature vectors hℓ
j for all nodes j in the neighborhood of
node i (deﬁned by the graph structure). This guarantees the transformation to build local
Benchmarking Graph Neural Networks
reception ﬁelds, such as in standard ConvNets for computer vision, and be invariant to both
graph size and vertex re-indexing.
Thus, the most generic version of a feature vector hℓ+1
at vertex i at the next layer in
the GNN is:
where {j →i} denotes the set of neighboring nodes j pointed to node i, which can be
replaced by {j ∈Ni}, the set of neighbors of node i, if the graph is undirected. In other
words, a GNN is deﬁned by a mapping f taking as input a vector hℓ
i (the feature vector
of the center vertex) as well as an un-ordered set of vectors {hℓ
j} (the feature vectors of
all neighboring vertices), see Figure 5. The arbitrary choice of the mapping f deﬁnes an
instantiation of a class of GNNs.
Graph ConvNets (GCN) 
In the simplest formulation of
GNNs, vanilla Graph ConvNets iteratively update node features via an isotropic averaging
operation over the neighborhood node features, i.e.,
UℓMeanj∈Ni hℓ
where Uℓ∈Rd×d (a bias is also used, but omitted for clarity purpose), degi is the in-degree
of node i, see Figure 6. Eq. (3) is called a convolution as it is a linear approximation of a
localized spectral convolution. Note that it is possible to add the central node features hℓ
the update (3) by using self-loops or residual connections.
The GCN model in Kipf and Welling use symmetric normalization instead of the
isotropic averaging, to result in the following node update equation:
GraphSage 
GraphSage improves upon the simple GCN model
by explicitly incorporating each node’s own features from the previous layer in its update
i, Meanj∈Ni hℓ
where Uℓ∈Rd×2d, see Figure 7. Observe that the transformation applied to the central node
features hℓ
i is diﬀerent to the transformation carried out to the neighborhood features hℓ
The node features are then projected onto the ℓ2-unit ball before being passed to the next
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Figure 6: GCN Layer
Figure 7: GraphSage Layer
The authors also deﬁne more sophisticated neighborhood aggregation functions, such as
Max-pooling or LSTM aggregators:
i, Maxj∈NiReLU
where V ℓ∈Rd×d and the LSTMℓcell also uses learnable weights. In our experiments, we
use the Max-pooling version of GraphSage, Eq.(8).
Graph Attention Network (GAT) 
GAT uses an attention
mechanism to introduce anisotropy in the neighborhood aggregation
function. The network employs a multi-headed architecture to increase the learning capacity,
similar to the Transformer . The node update equation is
where Uk,ℓ∈R
K ×d are the K linear projection heads, and ek,ℓ
ij are the attention coeﬃcients
for each head deﬁned as:
j′∈Ni exp 
The MoNet model introduces a general architecture to
learn on graphs and manifolds using the Bayesian Gaussian Mixture Model (GMM) . In the case of graphs, the node update equation is deﬁned as:
Aℓ(deg−1/2
where Uk,ℓ∈Rd×d, µℓ
k)−1, aℓ∈R2 and Aℓ∈R2×2 are the (learnable) parameters of the
GMM, see Figure 9.
Gated Graph ConvNet (GatedGCN) 
considers residual connections, batch normalization and edge gates to design another anisotropic variant of GCN. The authors propose to explicitly update
edge features along with node features:
where Uℓ, V ℓ∈Rd×d, ⊙is the Hadamard product, and the edge gates eℓ
ij are deﬁned as:
j′∈Ni σ(ˆeℓ
where σ is the sigmoid function, ε is a small ﬁxed constant for numerical stability, Aℓ, Bℓ, Cℓ∈
Rd×d, see Figure 10. Note that the edge gates (17) can be regarded as a soft attention process,
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Figure 10: GatedGCN Layer
Figure 11: GIN Layer
related to the standard sparse attention mechanism . Diﬀerent from
other anisotropic GNNs, the GatedGCN architecture explicitly maintains edge features ˆeij
at each layer, following Bresson and Laurent ; Joshi et al. .
Graph Isomorphism Networks (GIN) 
The GIN architecture is
based the Weisfeiler-Lehman Isomorphism Test to study the
expressive power of GNNs. The node update equation is deﬁned as:
(1 + ϵ) hℓ
where ϵ is a learnable constant, Uℓ, V ℓ∈Rd×d, BN denotes Batch Normalization. See Figure
11 for illustration of the update equation.
Normalization and Residual Connections
As a ﬁnal note, we augment each messagepassing GCN layer with batch normalization (BN) and residual
connections . As such, we consider a more speciﬁc class of GCNs than (2):
where σ is a non-linear activation function and gGCN is a speciﬁc message-passing GCN layer.
Benchmarking Graph Neural Networks
B.1.3 Task-based Layer
The ﬁnal component of each network is a prediction layer to compute task-dependent outputs,
which are given to a loss function to train the network parameters in an end-to-end manner.
The input of the prediction layer is the result of the ﬁnal message-passing GCN layer for
each node of the graph (except GIN, which uses features from all intermediate layers).
Graph classiﬁer layer
To perform graph classiﬁcation, we ﬁrst build a d-dimensional
graph-level vector representation yG by averaging over all node features in the ﬁnal GCN
The graph features are then passed to a MLP, which outputs un-normalized logits/scores
ypred ∈RC for each class:
ypred = P ReLU (Q yG) ,
where P ∈Rd×C, Q ∈Rd×d, C is the number of classes. Finally, we minimize the cross-entropy
loss between the logits and groundtruth labels.
Graph regression layer
For graph regression, we compute yG using Eq.(23) and pass it
to a MLP which gives the prediction score ypred ∈R:
ypred = P ReLU (Q yG) ,
where P ∈Rd×1, Q ∈Rd×d. The L1-loss between the predicted score and the groundtruth
score is minimized during the training.
Node classiﬁer layer
For node classiﬁcation, we independently pass each node’s feature
vector to a MLP for computing the un-normalized logits yi,pred ∈RC for each class:
yi,pred = P ReLU
where P ∈Rd×C, Q ∈Rd×d. The cross-entropy loss weighted inversely by the class size is
used during training.
Edge classiﬁer layer
To make a prediction for each graph edge eij, we ﬁrst concatenate
node features hi and hj from the ﬁnal GNN layer. The concatenated edge features are then
passed to a MLP for computing the un-normalized logits yij,pred ∈RC for each class:
yij,pred = P ReLU
where P ∈Rd×C, Q ∈Rd×2d.
The standard cross-entropy loss between the logits and
groundtruth labels is used.
B.2 Weisfeiler-Lehman GNNs
Weisfeiler-Lehman GNNs are the second GNN class we include in our benchmarking framework
which are based on the WL test . Xu et al. introduced
GIN–Graph Isomorphism Network, a provable 1-WL GNN, which can distinguish two nonisomorphic graphs w.r.t.
the 1-WL test.
Higher k-WL isomorphic tests lead to more
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Figure 12: 3WL-GNN Layer
Figure 13: RingGNN Layer
discriminative k-WL GNNs in . However, k-WL
GNNs require the use of tensors of rank k, which is intractable in practice for k > 2. As
a result, Maron et al. proposed a model, namely 3-WL GNNs, that uses rank-2
tensors while being 3-WL provable. This 3-WL model improves the space/time complexities
of Morris et al. from O(n3)/O(n4) to O(n2)/O(n3) respectively. We use 3WLGNNs
 and RingGNNs as the GNN instances in this class,
the experimental pipeline of which are described as follows.
B.2.1 Input Tensor
For a given graph with adjacency matrix A ∈Rn×n, node features hnode ∈Rn×d and edge
features hedge ∈Rn×n×de, the input tensor to the RingGNN and 3WL-GNN networks is
hℓ=0 ∈Rn×n×(1+d+de),
i,j,1 = Aij ∈R,
i,j,2:d+1 =
i,j,d+2:d+de+1 = hedge
B.2.2 WL-GNN layers
3WL-GNNs 
These networks introduced an architecture that
can distinguish two non-isomorphic graphs with the 3-WL test. The layer update equation
of 3WL-GNNs is deﬁned as:
hℓ+1 = Concat
1(hℓ) . MW ℓ
2(hℓ), MW ℓ
Benchmarking Graph Neural Networks
where hℓ, hℓ+1 ∈Rn×n×d, and MW are 2-layer MLPs applied along the feature dimension:
MW={Wa,Wb}(h) = σ
where Wa, Wb ∈Rd×d. As h ∈Rn×n×d, the MLP (33) is implemented with a standard
2D-convolutional layer with 1 × 1 kernel size. Eventually, the matrix multiplication in (32) is
carried out along the ﬁrst and second dimensions such that:
MW1(h) . MW2(h)
with complexity O(n3).
Ring-GNNs 
These models proposed to improve the order-2
equivariant GNNs of Maron et al. with the multiplication of two equivariant linear
layers. The layer update equation of Ring-GNNs is designed as:
1(hℓ) + wℓ
2(hℓ).LW ℓ
where hℓ, hℓ+1 ∈Rn×n×d, wℓ
1,2 ∈R, and LW are the equivariant linear layers deﬁned as
where W ∈Rd×d×17 and {Li}15
i=1 is the set of all basis functions for all linear equivariant
functions from Rn×n →Rn×n for the complete list
of these 15 operations) and {Li}17
i=16 are the basis for the bias terms. Matrix multiplication
in (35) also implies a time complexity O(n3).
B.2.3 Task-based network layers
We describe the ﬁnal network layers depending on the task at hand. The loss functions
corresponding to the task are the same as the GCNs, and presented in Section B.1.3.
Graph classiﬁer layer
We have followed the original author implementations in Maron
et al. ; Chen et al. to design the classiﬁer layer for 3WL-GNNs and Ring-
GNNs. Similar to Xu et al. , the classiﬁer layer for Ring-GNNs uses features
from all intermediate layers and then passes the features to a MLP:
P ReLU (Q yG) ∈RC,
where P ∈Rd×C, Q ∈RLd×d, C is the number of classes.
For 3WL-GNNs, Eqn. (37) is replaced by a diagonal and oﬀ-diagonal max pooling readout
Maron et al. at every layer:
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
and the ﬁnal prediction score is deﬁned as:
where P ℓ∈R2d×C, C is the number of classes.
Graph regression layer
Similar to the graph classiﬁer layer with P ∈Rd×1 for Ring-
GNNs, and P ℓ∈R2d×1 for 3WL-GNNs.
Node classiﬁer layer
For node classiﬁcation, the prediction in Ring-GNNs is done as
where P ∈Rd×C, Q ∈RLd×d, C is the number of classes.
In 3WL-GNNs, the ﬁnal prediction score is deﬁned as:
P ℓynode,ℓ
where P ℓ∈Rd×C, C is the number of classes.
Edge classiﬁer layer
For link prediction, for both Ring-GNNs and 3WL-GNNs, the edge
features are obtained by concatenating the node features such as:
where P ∈Rd×C, Q ∈R2Ld×d, C is the number of classes.
C. Datasets and Benchmarking Experiments
In this section, we provide details on the datasets included in the benchmarking framework
(Table 1) and the numerical results of the experiments using the GNN described in Section
B, which also consists experiments from a simple graph-agnostic baseline for every dataset
that parallelly applies an MLP on each node’s feature vector, independent of other nodes.
For complete statistics of the data, see Table 2. The experimental overview in terms of the
training strategy, reporting of results and the parameter budget used for fair comparison are
described ﬁrst, as follows.
Benchmarking Graph Neural Networks
Avg. Nodes
Avg. Edges
Node feat. (dim)
Edge feat. (dim)
Atom Type (28)
Bond Type (4)
Atom Type (65)
Bond Type (5)
OGBL-COLLAB
2358104.00
Word Embs (128)
Year & Weight (2)
Word Embs (300)
Pixel+Coord (3)
Node Dist (1)
Pixel[RGB]+Coord (5)
Node Dist (1)
Node Attr (3)
Node Attr (7)
Node Dist (1)
GraphTheoryProp
Source S.P.+Random(2)
Table 2: Summary statistics of all datasets. Numbers in parentheses of Node features and Edge
features are the dimensions. S.P. denotes shortest path.
Training. We use the Adam optimizer with the same learning
rate decay strategy for all models. An initial learning rate is selected in {10−2, 10−3, 10−4}
which is reduced by half if the validation loss does not improve after a ﬁxed number of epochs,
in the range 5-25. We do not set a maximum number of epochs – the training is stopped
either when the learning rate has reached the small value of 10−6, or the computational time
reaches 12 hours. We run each experiment with 4 diﬀerent seeds and report the statistics of
the 4 results. More details are provided in each experimental sub-sections.
Task-based network layer. The node representations generated by the ﬁnal layer
of GCNs, or the dense tensor obtained at the ﬁnal layer of the higher order WL-GNNs,
are passed to a network suﬃx which is usually a downstream MLP of 3 layers. For GIN,
RingGNN, and 3WL-GNN, we follow the original instructions of network suﬃxes to consider
feature outputs from each layer of the network, similar to that of Jumping Knowledge
Networks . Refer to the equations in the Sections B.1.3 and B.2.3 for more
Parameter budgets. Our goal is not to ﬁnd the optimal set of hyperparameters for a
speciﬁc GNN model (which is computationally expensive), but to compare and benchmark
the model and/or their building blocks within a budget of parameters.
Therefore, we
decide on using two parameter budgets: (1) 100k parameters for each GNNs for all the
tasks, and (2) 500k parameters for GNNs for which we investigate scaling a model to
larger parameters and deeper layers. The number of hidden layers and hidden dimensions
are selected accordingly to match these budgets. The conﬁguration details of each single
experiment can be found in our modular coding infrastructure on GitHub.
C.1 Graph Regression with ZINC dataset
For the ZINC dataset in our benchmark, we use a subset (12K) of ZINC molecular graphs
(250K) dataset to regress a molecular property known as the constrained
solubility which is the term logP −SA −cycle (octanol-water partition coeﬃcients, logP,
penalized by the synthetic accessibility score, SA, and number of long cycles, cycle). For
each molecular graph, the node features are the types of heavy atoms and the edge features
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Test MAE±s.d.
Train MAE±s.d.
Epoch/Total
0.706±0.006
0.644±0.005
1.01s/0.03hr
vanilla GCN
0.459±0.006
0.343±0.011
2.89s/0.16hr
0.367±0.011
0.128±0.019
12.78s/0.71hr
0.468±0.003
0.251±0.004
3.74s/0.15hr
0.398±0.002
0.081±0.009
16.61s/0.68hr
0.416±0.006
0.313±0.011
1.53s/0.07hr
0.278±0.003
0.101±0.011
3.66s/0.16hr
0.397±0.010
0.318±0.016
1.97s/0.10hr
0.292±0.006
0.093±0.014
10.82s/0.52hr
0.475±0.007
0.317±0.006
2.93s/0.11hr
0.384±0.007
0.067±0.004
12.98s/0.53hr
0.435±0.011
0.287±0.014
5.76s/0.28hr
GatedGCN-E
0.375±0.003
0.236±0.007
5.37s/0.29hr
0.282±0.015
0.074±0.016
20.50s/0.96hr
GatedGCN-E-PE
0.214±0.013
0.067±0.019
10.70s/0.56hr
0.387±0.015
0.319±0.015
2.29s/0.10hr
0.526±0.051
0.444±0.039
10.22s/0.42hr
0.512±0.023
0.383±0.020
327.65s/8.32hr
0.363±0.026
0.243±0.025
366.29s/9.76hr
0.353±0.019
0.236±0.019
293.94s/6.63hr
0.407±0.028
0.272±0.037
286.23s/8.88hr
0.256±0.054
0.140±0.044
334.69s/10.90hr
0.303±0.068
0.173±0.041
329.49s/11.08hr
0.303±0.057
0.246±0.043
811.27s/12.15hr
Table 3: Benchmarking results for ZINC for graph regression. Results (lower is better) are averaged
over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models. The suﬃx -E denotes
the use of available edge features, and the suﬃx -PE denote the use of Laplacian Eigenvectors as
node positional encodings with dimension 8.
are the types of bonds between them. ZINC has been used popularly for research related to
molecular graph generation .
Splitting. ZINC has 10, 000 train, 1, 000 validation and 1, 000 test graphs.
Training. For the learning rate strategy across all GNNs, an initial learning rate is set to
1 × 10−3, the reduce factor is 0.5, and the stopping learning rate is 1 × 10−5. The patience
value is 5 for 3WLGNN and RingGNN, and 10 for all other GNNs.
Performance Measure. The performance measure is the mean absolute error (MAE)
between the predicted and the groundtruth constrained solubility for each molecular graph.
Results. The numerical results are presented in Table 3 and analysed in Section D collectively
with other benchmarking results.
C.2 Graph Regression with AQSOL dataset
AQSOL dataset is based on AqSolDB which is a standardized database
of 9, 982 molecular graphs with their aqueous solubility values, collected from 9 diﬀerent
data sources. The aqueous solubility targets are collected from experimental measurements
and standardized to LogS units in AqSolDB. We use these ﬁnal values as the property to
regress in the AQSOL dataset which is the resultant collection after we ﬁlter out few graphs
Benchmarking Graph Neural Networks
with no edges (bonds) and a small number of graphs with missing node feature values. Thus,
the total molecular graphs are 9, 823. For each molecular graph, the node features are the
types of heavy atoms and the edge features are the types of bonds between them.
Splitting. We provide a scaﬀold splitting of the dataset in the ratio 8 : 1 : 1
to have 7, 831 train, 996 validation and 996 test graphs.
Training. For the learning rate strategy across all GNNs, an initial learning rate is set to
1 × 10−3, the reduce factor is 0.5, and the stopping learning rate is 1 × 10−5. The patience
value is 5 for 3WLGNN and RingGNN, and 10 for all other GNNs.
Performance Measure. Similar to ZINC, the performance measure is the mean absolute
error (MAE) between the predicted and the actual aqueous solubility values.
Results. The numerical results are presented in Table 4 and analysed in Section D.
TestMAE±s.d.
TrainMAE±s.d.
Epoch/Total
1.744±0.016
1.413±0.042
0.61s/0.02hr
vanilla GCN
1.483±0.014
0.791±0.034
1.14s/0.04hr
1.458±0.011
0.567±0.027
2.83s/0.10hr
1.431±0.010
0.666±0.027
1.51s/0.05hr
1.402±0.013
0.402±0.013
3.20s/0.10hr
1.372±0.020
0.593±0.030
1.28s/0.05hr
1.333±0.013
0.382±0.018
3.31s/0.13hr
1.395±0.027
0.557±0.022
1.68s/0.06hr
1.501±0.056
0.444±0.024
3.62s/0.11hr
1.441±0.023
0.678±0.021
1.92s/0.06hr
1.403±0.008
0.386±0.014
4.44s/0.14hr
1.352±0.034
0.576±0.056
2.28s/0.09hr
1.355±0.016
0.465±0.038
5.52s/0.16hr
GatedGCN-E
1.295±0.016
0.544±0.033
2.29s/0.08hr
1.308±0.013
0.367±0.012
5.61s/0.18hr
GatedGCN-E-PE
0.996±0.008
0.372±0.016
5.70s/0.30hr
1.894±0.024
0.660±0.027
1.55s/0.05hr
1.962±0.058
0.850±0.054
3.97s/0.14hr
20.264±7.549
0.625±0.018
113.99s/1.76hr
3.769±1.012
0.470±0.022
125.17s/2.26hr
1.154±0.050
0.434±0.026
130.92s/2.48hr
1.108±0.036
0.405±0.031
131.12s/2.62hr
1.042±0.064
0.307±0.024
139.04s/2.70hr
1.052±0.034
0.287±0.023
140.43s/2.67hr
Table 4: Benchmarking results for AQSOL for graph regression. Results (lower is better) are averaged
over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models. The suﬃx -E denotes
the use of available edge features, and the suﬃx -PE denote the use of Laplacian Eigenvectors as
node positional encodings with dimension 4.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
OGBL-COLLAB
Test Hits±s.d.
Train Hits±s.d.
Epoch/Total
20.350±2.168
29.807±3.360
2.09s/0.09hr
vanilla GCN
50.422±1.131
92.112±0.991
351.05s/12.04hr
51.618±0.690
99.949±0.052
277.93s/11.87hr
48.956±1.143
87.385±2.056
7.66s/0.31hr
36.144±2.191
61.156±3.973
26.69s/1.26hr
51.501±0.962
97.851±1.114
18.12s/0.80hr
52.635±1.168
96.103±1.876
453.47s/12.09hr
GatedGCN-PE
52.849±1.345
96.165±0.453
452.75s/12.08hr
GatedGCN-E
49.212±1.560
88.747±1.058
451.21s/12.03hr
41.730±2.284
70.555±4.444
8.66s/0.34hr
RingGNN and 3WLGNN rely
on dense tensors which leads
to OOM on both GPU and
CPU memory.
Matrix Fact.
44.206±0.452
100.000±0.000
2.66s/0.21hr
Table 5: Benchmarking results for OGBL-COLLAB for link prediction. Results (higher is better)
are averaged over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models. The
suﬃx -E denotes the use of available edge features, and the suﬃx -PE denote the use of Laplacian
Eigenvectors as node positional encodings with dimension 20.
C.3 Link Prediction with OGBL-COLLAB dataset
OGBL-COLLAB is a link prediction dataset proposed by OGB corresponding to a collaboration network between approximately 235K scientists, indexed by
Microsoft Academic Graph . Nodes represent scientists and edges denote
collaborations between them. For node features, OGB provides 128-dimensional vectors,
obtained by averaging the word embeddings of a scientist’s papers. The year and number of
co-authored papers in a given year are concatenated to form edge features. The graph can
also be viewed as a dynamic multi-graph, since two nodes may have multiple temporal edges
between if they collaborate over multiple years.
Splitting. We use the realistic training, validation and test edge splits provided by OGB.
Speciﬁcally, they use collaborations until 2017 as training edges, those in 2018 as validation
edges, and those in 2019 as test edges.
Training. All GNNs use a consistent learning rate strategy: an initial learning rate is set to
1 × 10−3, the reduce factor is 0.5, the patience value is 10, and the stopping learning rate is
Performance Measure. We use the evaluator provided by OGB, which aims to measure
a model’s ability to predict future collaboration relationships given past collaborations.
Speciﬁcally, they rank each true collaboration among a set of 100,000 randomly-sampled
negative collaborations, and count the ratio of positive edges that are ranked at K-place or
above (Hits@K, with K = 50).
Matrix Factorization Baseline. In addition to GNNs, we report performance for a simple
matrix factorization baseline , which trains 256-dimensional embeddings for
Benchmarking Graph Neural Networks
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
59.452±2.327
85.347±5.440
0.01s/0.03hr
vanilla GCN
77.103±0.830
98.918±0.619
0.05s/0.10hr
74.767±0.950
99.976±0.095
0.06s/0.12hr
77.469±0.854
98.925±0.590
0.06s/0.11hr
77.431±0.669
98.737±0.710
0.17s/0.36hr
77.481±0.712
98.767±0.726
0.19s/0.81hr
76.908±0.821
99.914±0.262
1.12s/2.22hr
75.857±0.577
99.575±0.388
0.06s/0.13hr
Table 6: Benchmarking results for WikiCS for node classiﬁcation. Results (higher is better) are
averaged over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models. The suﬃx
-PE denote the use of Laplacian Eigenvectors as node positional encodings with dimension 20.
each of the 235K nodes. Comparing GNNs to matrix factorization tells us whether models
leverage node features in addition to graph structure, as matrix factorization can be thought
of as feature-agnostic.
Results. The numerical results are presented in Table 5 and discussed in Section D.
C.4 Node Classiﬁcation with WikiCS dataset
WikiCS is a node classiﬁcation dataset based on an extracted subset of Wikipedia’s Computer
Science articles . It is a single graph dataset with 11, 701 nodes
and 216, 123 edges where each node corresponds to an article, and each edge corresponds to
a hyperlink. Each node belongs to a label out of total 10 classes representing the article’s
category. The average of the article text’s pre-trained GloVe word embeddings is assigned as 300-dimensional node features. Compared to previous single-graph
node classiﬁcation benchmarks such as Cora and Citeseer, WikiCS dataset has denser node
neighborhoods and each node’s connectivity is spread across nodes from varying class labels.
Additionally, as shown in Mernyei and Cangea , the average shortest path length
in WikiCS is smaller compared to Cora and Citeseer. Thus, on average, a larger node
neighborhood and smaller shortest path length makes WikiCS an appropriate benchmark to
test out neighborhood computation functions in GNNs’ design.
Splitting. We follow the splitting deﬁned in Mernyei and Cangea that has 20 diﬀerent
training, validation and early stopping splits consisting of 5% nodes, 22.5% nodes and 22.5%
nodes of each class respectively. 50% nodes from each class, which are not in the training or
validation split, are assigned as test splits. We combine the two original validation (22.5%
nodes) and early stopping (22.5% nodes) splits to make the new validation (45% nodes)
splits since we do not use separate early stopping splits in our benchmark.
Training. As consistent learning rate strategy across GNNs, an initial learning rate is set to
1 × 10−2, the reduce factor is 0.5, the patience value is 25, and the stopping learning rate is
1 × 10−5. Since there are 20 diﬀerent training and validation splits, the training is done 20
times using these splits, and evaluated on the single test split. This is done for 4 times with
4 diﬀerent seeds. Finally, the average of the 20 × 4 = 80 runs is reported.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Performance Measure. The performance measure is the classiﬁcation accuracy between
the predicted and groundtruth label for each node.
Results. The numerical results are presented in Table 6 and discussed in Section D.
C.5 Graph Classiﬁcation with Super-pixel (MNIST/CIFAR10) datasets
The super-pixels datasets test graph classiﬁcation using the popular MNIST and CIFAR10
image classiﬁcation datasets. Our main motivation to use these datasets is as sanity-checks:
we expect most GNNs to perform close to 100% for MNIST and well enough for CIFAR10.
Besides, the use of super-pixel image datasets is suggestive of the way image datasets can be
used for graph learning research.
The original MNIST and CIFAR10 images are converted to graphs using super-pixels.
Super-pixels represent small regions of homogeneous intensity in images, and can be extracted
with the SLIC technique . We use SLIC super-pixels from 2. For each sample, we build a k-nearest neighbor adjacency matrix with
−∥xi −xj∥2
where xi, xj are the 2-D coordinates of super-pixels i, j, and σx is the scale parameter deﬁned
as the averaged distance xk of the k nearest neighbors for each node. We use k = 8 for both
MNIST and CIFAR10, whereas the maximum number of super-pixels (nodes) are 75 and 150
for MNIST and CIFAR10, respectively. The resultant graphs are of sizes 40-75 nodes for
MNIST and 85-150 nodes for CIFAR10. Figure 14 presents visualizations of the super-pixel
Splitting. We use the standard splits of MNIST and CIFAR10. MNIST has 55, 000 train,
5, 000 validation, 10, 000 test graphs and CIFAR10 has 45, 000 train, 5, 000 validation, 10, 000
test graphs. The 5, 000 graphs for validation set are randomly sampled from the training set
and the same splits are used for every GNN.
Training. The learning decay rate strategy is adopted with an initial learning rate of
1 × 10−3, reduce factor 0.5, patience value 10, and the stopping learning rate 1 × 10−5 for all
GNNs, except for 3WLGNN and RingGNN where we experienced a diﬃculty in training,
leading us to slightly adjust their learning rate schedule hyperparameters. For both 3WLGNN
and RingGNN, the patience value is changed to 5. For RingGNN, the initial learning rate is
changed to 1 × 10−4 and the stopping learning rate is changed to 1 × 10−6.
Performance Measure. The classiﬁcation accuracy between the predicted and groundtruth
label for each graph is the performance measure.
Results. The numerical results are presented in Table 7 and discussed in Section D.
C.6 Node Classiﬁcation with SBM (PATTERN/CLUSTER) datasets
The SBM datasets consider node-level tasks of graph pattern recognition – PATTERN and semi-supervised graph clustering – CLUSTER. The graphs are
generated with the Stochastic Block Model (SBM) , which is widely used to
model communities in social networks by modulating the intra- and extra-communities
2. 
Benchmarking Graph Neural Networks
(b) CIFAR10
Figure 14: Sample images and their superpixel graphs. The graphs of SLIC superpixels (at
most 75 nodes for MNIST and 150 nodes for CIFAR10) are 8-nearest neighbor graphs in the
Euclidean space and node colors denote the mean pixel intensities.
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
95.340±0.138
97.432±0.470
22.74s/1.48hr
56.340±0.181
65.113±1.685
29.48s/1.53hr
vanilla GCN
90.705±0.218
97.196±0.223
83.41s/2.99hr
55.710±0.381
69.523±1.948
109.70s/4.39hr
97.312±0.097
100.000±0.000
113.12s/3.13hr
65.767±0.308
99.719±0.062
124.61s/3.29hr
90.120±0.145
96.459±1.020
37.06s/1.22hr
54.142±0.394
70.163±3.429
47.16s/1.86hr
90.805±0.032
96.609±0.440
93.19s/3.82hr
54.655±0.518
65.911±2.515
97.13s/3.85hr
95.535±0.205
99.994±0.008
42.26s/1.25hr
64.223±0.455
89.114±0.499
55.27s/1.62hr
97.340±0.143
100.000±0.000
128.79s/3.50hr
67.312±0.311
94.553±1.018
154.15s/4.22hr
96.485±0.252
100.000±0.000
39.22s/1.41hr
55.255±1.527
79.412±9.700
52.12s/2.07hr
11.350±0.000
11.235±0.000
2945.69s/12.77hr
19.300±16.108
19.556±16.397
3112.96s/13.00hr
91.860±0.449
92.169±0.505
2575.99s/12.63hr
39.165±17.114
40.209±17.790
2998.24s/12.60hr
95.075±0.961
95.830±1.338
1523.20s/12.40hr
59.175±1.593
63.751±2.697
1506.29s/12.60hr
95.002±0.419
95.692±0.677
1608.73s/12.42hr
58.043±2.512
61.574±3.575
2091.22s/12.55hr
Table 7: Benchmarking results for Super-pixels datasets for graph classiﬁcation. Results (higher is
better) are averaged over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models.
connections, thereby controlling the diﬃculty of the task.
A SBM is a random graph
which assigns communities to each node as follows: any two vertices are connected with
the probability p if they belong to the same community, or they are connected with the
probability q if they belong to diﬀerent communities (the value of q acts as the noise level).
PATTERN: The graph pattern recognition task, presented in Scarselli et al. ,
aims at ﬁnding a ﬁxed graph pattern P embedded in larger graphs G of variable sizes. For
all data, we generate graphs G with 5 communities with sizes randomly selected between
 . The SBM of each community is p = 0.5, q = 0.35, and the node features on G are
generated with a uniform random distribution with a vocabulary of size 3, i.e. {0, 1, 2}. We
randomly generate 100 patterns P composed of 20 nodes with intra-probability pP = 0.5 and
extra-probability qP = 0.5 (i.e., 50% of nodes in P are connected to G). The node features
for P are also generated as a random signal with values {0, 1, 2}. The graphs are of sizes
44-188 nodes. The output node labels have value 1 if the node belongs to P and value 0 if it
CLUSTER: For the semi-supervised clustering task, we generate 6 SBM clusters with
sizes randomly selected between and probabilities p = 0.55, q = 0.25. The graphs are
of sizes 40-190 nodes. Each node can take an input feature value in {0, 1, 2, .., 6}. If the value
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
50.519±0.000
50.487±0.014
8.95s/0.11hr
20.973±0.004
20.938±0.002
5.83s/0.07hr
vanilla GCN
63.880±0.074
65.126±0.135
118.85s/3.51hr
53.445±2.029
54.041±2.197
65.72s/1.30hr
71.892±0.334
78.409±1.592
492.19s/11.31hr
68.498±0.976
71.729±2.212
270.28s/6.08hr
50.516±0.001
50.473±0.014
93.41s/1.17hr
50.454±0.145
54.374±0.203
53.56s/0.97hr
50.492±0.001
50.487±0.005
391.19s/5.19hr
63.844±0.110
86.710±0.167
225.61s/3.70hr
85.498±0.045
85.598±0.043
19.21s/0.36hr
47.828±1.510
48.258±1.607
12.84s/0.23hr
85.614±0.032
86.034±0.087
37.08s/0.70hr
69.026±1.372
73.749±2.570
30.20s/0.66hr
85.482±0.037
85.569±0.044
35.71s/0.90hr
58.064±0.131
58.454±0.183
24.29s/0.52hr
85.582±0.038
85.720±0.068
68.49s/1.58hr
66.407±0.540
67.727±0.649
47.82s/1.05hr
75.824±1.823
77.883±1.632
20.92s/0.57hr
57.732±0.323
58.331±0.342
14.17s/0.27hr
78.271±0.186
90.212±0.476
50.33s/0.77hr
70.587±0.447
76.074±1.362
35.94s/0.75hr
84.480±0.122
84.474±0.155
139.01s/3.09hr
60.404±0.419
61.618±0.536
79.97s/2.13hr
85.568±0.088
86.007±0.123
644.71s/11.91hr
73.840±0.326
87.880±0.908
400.07s/6.81hr
GatedGCN-PE
86.508±0.085
86.801±0.133
647.94s/12.08hr
76.082±0.196
88.919±0.720
399.66s/6.58hr
85.590±0.011
85.852±0.030
15.24s/0.40hr
58.384±0.236
59.480±0.337
10.71s/0.23hr
85.387±0.136
85.664±0.116
25.14s/0.62hr
64.716±1.553
65.973±1.816
20.67s/0.47hr
86.245±0.013
86.118±0.034
573.37s/12.17hr
42.418±20.063
42.520±20.212
428.24s/8.79hr
86.244±0.025
86.105±0.021
595.97s/12.15hr
22.340±0.000
22.304±0.000
501.84s/6.22hr
85.661±0.353
85.608±0.337
304.79s/7.88hr
57.130±6.539
57.404±6.597
219.51s/6.52hr
85.341±0.207
85.270±0.198
424.23s/9.56hr
55.489±7.863
55.736±8.024
319.98s/5.79hr
Table 8: Benchmarking results for SBMs datasets for node classiﬁcation. Results (higher is better)
are averaged over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models. The
suﬃx -PE denote the use of Laplacian Eigenvectors as node positional encodings with dimension 2
for PATTERN and 20 for CLUSTER.
is 1, the node belongs to class 0, value 2 corresponds to class 1, . . . , value 6 corresponds to
class 5. Otherwise, if the value is 0, the class of the node is unknown and will be inferred by
the GNN. There is only one labelled node that is randomly assigned to each community and
most node features are set to 0. The output node labels are deﬁned as the community/cluster
class labels.
Splitting. The PATTERN dataset has 10, 000 train, 2, 000 validation, 2, 000 test graphs
and CLUSTER dataset has 10, 000 train, 1, 000 validation, 1, 000 test graphs. We save the
generated splits and use the same sets in all models for fair comparison.
Training. As presented in the standard experimental protocol in Section C, we use Adam
optimizer with a learning rate decay strategy. For all GNNs, an initial learning rate is set to
1 × 10−3, the reduce factor is 0.5, the patience value is 5, and the stopping learning rate is
Performance Measure. The performance measure is the average node-level accuracy
weighted with respect to the class sizes.
Results. Our numerical results are presented in Table 8 and discussed in Section D together
with other benchmark results.
C.7 Edge Classiﬁcation/Link Prediction with TSP dataset
Leveraging machine learning for solving NP-hard combinatorial optimization problems (COPs)
has been the focus of intense research in recent years . Recently proposed learning-driven solvers for COPs combine GNNs with classical search to predict approximate solutions
directly from problem instances (represented as graphs). Consider the intensively studied
Travelling Salesman Problem (TSP), which asks the following question: “Given a list of cities
and the distances between each pair of cities, what is the shortest possible route that visits
Benchmarking Graph Neural Networks
(b) TSP200
(c) TSP500
Figure 15: Sample graphs from the TSP dataset. Nodes are colored blue and edges on the
groundtruth TSP tours are colored red.
each city and returns to the origin city?" Formally, given a 2D Euclidean graph, one needs
to ﬁnd an optimal sequence of nodes, called a tour, with minimal total edge weights (tour
length). TSP’s multi-scale nature makes it a challenging graph task which requires reasoning
about both local node neighborhoods as well as global graph structure.
For our experiments with TSP, we follow the learning-based approach to COPs described
in Joshi et al. , where a GNN is the backbone architecture for assigning probabilities
to each edge as belonging/not belonging to the predicted solution set. The probabilities are
then converted into discrete decisions through graph search techniques. Each instance is a
graph of n node locations sampled uniformly in the unit square S = {xi}n
i=1 and xi ∈ 2.
We generate problems of varying size and complexity by uniformly sampling the number of
nodes n ∈ for each instance.
In order to isolate the impact of the backbone GNN architectures from the search
component, we pose TSP as a binary edge classiﬁcation task, with the groundtruth value
for each edge belonging to the TSP tour given by Concorde . For
scaling to large instances, we use sparse k = 25 nearest neighbor graphs instead of full graphs,
following . See Figure 15 for sample TSP instances of various sizes.
Splitting. TSP has 10, 000 train, 1, 000 validation and 1, 000 test graphs.
Training. All GNNs use a consistent learning rate strategy: an initial learning rate is set to
1 × 10−3, the reduce factor is 0.5, the patience value is 10, and the stopping learning rate is
Performance Measure. Given the high class imbalance, i.e., only the edges in the TSP
tour have positive label, we use the F1 score for the positive class as our performance measure.
Non-learnt Baseline. In addition to reporting performance of GNNs, we compare with a
simple k-nearest neighbor heuristic baseline, deﬁned as follows: Predict true for the edges
corresponding to the k nearest neighbors of each node, and false for all other edges. We set
k = 2 for optimal performance. Comparing GNNs to the non-learnt baseline tells us whether
models learn something more sophisticated than identifying a node’s nearest neighbors.
Results. The numerical results are presented in Table 9 and analysed in Section D.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Test F1±s.d.
Train F1±s.d.
Epoch/Total
0.544±0.001
0.544±0.001
50.15s/2.31hr
vanilla GCN
0.630±0.001
0.631±0.001
152.89s/11.15hr
0.665±0.003
0.669±0.003
157.26s/11.68hr
0.643±0.001
0.645±0.002
57.84s/4.23hr
0.641±0.002
0.643±0.002
84.46s/6.65hr
0.671±0.002
0.673±0.002
68.23s/6.25hr
0.791±0.003
0.793±0.003
218.20s/9.72hr
GatedGCN-E
0.808±0.003
0.811±0.003
218.51s/12.04hr
GatedGCN-E
0.838±0.002
0.850±0.001
807.23s/12.17hr
0.656±0.003
0.660±0.003
72.73s/5.56hr
0.643±0.024
0.644±0.024
17850.52s/17.19hr
0.704±0.003
0.705±0.003
12835.53s/16.08hr
0.694±0.073
0.695±0.073
17468.81s/16.59hr
0.288±0.311
0.290±0.312
17190.17s/16.51hr
k-NN Heuristic
Test F1: 0.693
Table 9: Benchmarking results for TSP for edge classiﬁcation. Results (higher is better) are averaged
over 4 runs with 4 diﬀerent seeds. Red: the best model, Violet: good models. The suﬃx -E denotes
the use of available edge features.
C.8 Graph Classiﬁcation and Isomorphism Testing with CSL dataset
The Circular Skip Link dataset is a symmetric graph dataset introduced in Murphy et al.
 to test the expressivity of GNNs. Each CSL graph is a 4-regular graph with edges
connected to form a cycle and containing skip-links between nodes. Formally, it is denoted by
GN,C where N is the number of nodes and C is the isomorphism class which is the skip-link
length of the graph. We use the same dataset G41,C with C ∈{2, 3, 4, 5, 6, 9, 11, 12, 13, 16}.
The dataset is class-balanced with 15 graphs for every C resulting in a total of 150 graphs.
Splitting. We perform a 5-fold cross validation split, following Murphy et al. , which
gives 5 sets of train, validation and test data indices in the ratio 3 : 1 : 1. We use stratiﬁed
sampling to ensure that the class distribution remains the same across splits. The indices
are saved and used across all experiments for fair comparisons.
Training. For the learning rate strategy across all GNNs, an initial learning rate is set to
5 × 10−4, the reduce factor is 0.5, the patience value is 5, and the stopping learning rate is
1 × 10−6. We train on the 5-fold cross validation with 20 diﬀerent seeds of initialization,
following Chen et al. .
Performance Measure. We use graph classiﬁcation accuracy between the predicted labels
and groundtruth labels as our performance measure. The model performance is evaluated
on the test split of the 5 folds at every run, and following Murphy et al. ; Chen et al.
 , we report the maximum, minimum, average and the standard deviation of the 100
scores, i.e., 20 runs of 5-folds.
Results. The numerical results are reported in Table 10 and analyzed in Section E.1. In this
paper, we use CSL primarily to validate the impact of having Graph Positional Encodings
Benchmarking Graph Neural Networks
Test Accuracy
Train Accuracy
Node Positional Encoding with Laplacian Eigenvectors
22.567±6.089
30.389±5.712
0.16s/0.03hr
100.000±0.000
100.000±0.000
0.40s/0.07hr
99.933±0.467
100.000±0.000
0.50s/0.11hr
99.967±0.332
100.000±0.000
0.49s/0.09hr
99.933±0.467
100.000±0.000
0.61s/0.12hr
99.600±1.083
100.000±0.000
0.66s/0.14hr
99.333±1.333
100.000±0.000
0.44s/0.04hr
17.233±6.326
26.122±14.382
2.93s/0.50hr
25.167±7.399
54.533±18.415
3.11s/0.51hr
30.533±9.863
99.644±1.684
2.33s/0.25hr
30.500±8.197
100.000±0.000
2.38s/0.23hr
No Node Positional Encoding
All MP-GCNs
10.000±0.000
10.000±0.000
10.000±0.000
10.000±0.000
3.09s/0.45hr
10.000±0.000
10.000±0.000
3.28s/0.42hr
95.700±14.850
95.700±14.850
2.29s/1.51hr
97.800±10.916
97.800±10.916
2.28s/0.90hr
Table 10: Results for the CSL dataset, with and without Laplacian Positional Encodings. Results are
from 5-fold cross validation, run 20 times with diﬀerent seeds. Red: the best model, Violet: good
models. The dimension of node positional encoding with Laplacian eigenvectors is 20.
(Section E.1) that is proposed as a demonstration of our benchmarking framework to steer
new GNN research.
C.9 Cycle Detection with CYCLES dataset
The CYCLES is a dataset synthetically generated by Loukas which contains equal
number of graphs with and without cycles of ﬁxed lengths. The task is a binary classiﬁcation
task to detect whether a graph has cycle or not. Though there are several forms of the dataset
used in Loukas in terms of the number of nodes and cycle lengths, we select the
dataset variant marked with having node size 56 and cycle length 6, based on the diﬃculty
results shown by the author. The graphs have nodes in the range 37-65.
Splitting. We use the same dataset splits as in Loukas . Originally there 10,000
graphs each in the training and test sets. We sample 1,000 class balanced graphs from the
training set to be used as validation samples. Therefore, the resulting CYCLES dataset
has 9,000 train/ 1,000 validation/10,000 test graphs with all the sets having class-balanced
samples. We show results on diﬀerent sizes of training samples following the original author
of CYCLES dataset.
Training. For the learning rate strategy, an initial learning rate is set to 1 × 10−4, the
reduce factor is 0.5, the patience value is 10, and the stopping learning rate is 1 × 10−6.
Following Loukas , we train using a varying sample size from 200 to 5, 000 out of the
training graphs and report the results accordingly. The reported results are based on 4 runs
with 4 diﬀerent seeds.
Performance Measure. The classiﬁcation accuracy between the predicted and groundtruth
label for whether a graph has cycle or not is the performance measure.
Results. Similar to the CSL dataset (Section C.8), we use the CYCLES dataset mainly
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
for the validation of the Graph Positional Encodings (Section E.1) proposed as an outcome
of this benchmarking framework. As such, we train only a subset of MP-GCNs (GINs and
GatedGCNs) and report the respective results. The numerical results are reported in Table
11 and analyzed in Section E.1.
Train samples →
Test Acc±s.d.
70.585±0.636
74.995±1.226
78.083±1.083
86.130±1.140
86.720±3.376
95.960±0.393
97.998±0.300
99.570±0.089
50.000±0.000
50.000±0.000
50.000±0.000
50.000±0.000
GatedGCN-PE
95.082±0.346
96.700±0.381
98.230±0.473
99.725±0.027
Table 11: Test accuracy on the CYCLES dataset. Results (higher is better) are averaged over 4 runs
with 4 diﬀerent seeds. The performance on test sets with models trained on varying train data size is
show, following Vignac et al. . Bold shows the best result out of a GNN’s two model instances
that use and not use PE. The dimension for PE is 20.
C.10 Multi-task graph properties with GraphTheoryProp dataset
Corso et al. proposed a synthetic dataset of undirected and unweighted graphs of diverse
types randomly generated for a multi-task benchmarking of 6 graph-theoretic properties,
3 at the node-level and 3 at the graph-level. We call this dataset as GraphTheoryProp.
The node-level tasks are to determine single source shortest paths (Dist.), node eccentricity
(Ecc.), and Laplacian features LX given a node feature vector X (Lap.) The graph-level
tasks are graph connectivity (Conn.), diameter (Diam.) and spectral radius (Rad.). The
dataset has graph sizes in the range of 15-24 nodes which have random identiﬁers as input
features. This dataset is crucial to benchmark the robustness of a GNN to predict speciﬁc
or overall of all the 6 properties, as these may share subroutines such as graph traversals,
despite the tasks being diﬀerent graph properties .
-3.19±0.11
-2.81±0.11
-2.42±0.09
-4.39±0.18
-2.07±0.13
-3.06±0.11
-4.39±0.13
-3.21±0.13
-2.87±0.03
-2.83±0.07
-3.99±0.04
-2.00±0.15
-3.27±0.07
-4.31±0.15
-3.22±0.13
-2.76±0.17
-2.36±0.12
-3.92±0.15
-2.65±0.11
-3.35±0.16
-4.31±0.08
GatedGCN-PE
-3.51±0.11
-3.23±0.08
-3.35±0.08
-4.03±0.21
-2.60±0.12
-3.57±0.05
-4.32±0.13
Table 12: Mean Log10MSE for each task over 4 runs with 4 diﬀerent seeds. Average denotes the
combined average of all the tasks. Log10MSE is on the test set (lower is better). Bold shows the
best result out of a GNN’s two model instances that use and not use PE. The dimension for PE is 12.
Splitting. We use the same splitting sets as in Corso et al. which has 5,120 train,
640 validation, 1,280 test graphs.
Training. For the learning rate strategy, an initial learning rate is set to 1 × 10−3, the
reduce factor is 0.5, the patience value is 15, and the stopping learning rate is 1 × 10−6. The
reported results are based on 4 runs with 4 diﬀerent seeds.
Performance Measure. For performance measure, Log10MSE is reported between the
Benchmarking Graph Neural Networks
predicted and groundtruth values for each single task. Besides, an average performance
measure is reported which is the combined average of all the 6 tasks.
Results. As with the CSL and CYCLES datasets (Sections C.8, C.9), we use GraphTheoryProp in this paper for the validation of Graph Positional Encodings, Section E.1. The
numerical results are reported in Table 12 and analyzed in Section E.1.
D. Analysis and Discussion of Benchmarking Results
This section highlights the main take-home messages from the experiments in Section C on
the datasets in the proposed framework, which evaluate the GNNs from Section B with the
experimental setup described in Section C and respective sub-sections of each datasets.
Graph-agnostic NNs perform poorly. As a sanity check, we compare all GNNs
to a simple graph-agnostic MLP baseline which updates each node independent of oneother, hℓ+1
, and passes these features to the task-based layer. MLP presents
consistently low scores across all datasets (Tables 3-10), which shows the necessity to use
graph structure for these tasks. All proposed datasets used in our study are appropriate to
statistically separate GNN performance, which has remained an issue with the widely used
but small graph datasets .
GCNs outperform WL-GNNs on the proposed datasets. Although provably
powerful in terms of graph isomorphism tests and invariant function approximation , the recent 3WLGNNs and RingGNNs
were not able to outperform GCNs for our medium-scale datasets, as shown in Tables 3-5 and
7-9. These new models are limited in terms of space/time complexities, with O(n2)/O(n3)
respectively, not allowing them to scale to larger datasets. On the contrary, GCNs with
linear complexity w.r.t. the number of nodes for sparse graphs, can scale conveniently to 16
layers and show the best performance on all datasets. 3WL-GNNs and RingGNNs face loss
divergence and/or out-of-memory errors when trying to build deeper networks.
Anisotropic mechanisms improve GCNs. Among the models in the GCN class, the
best results point towards the anisotropic models, particularly GAT and GatedGCN, which
are based on sparse and dense attention mechanisms, respectively. For instance, results for
ZINC, AQSOL, WikiCS, MNIST, CIFAR10, PATTERN and CLUSTER in respective Tables
3, 4, 6, 7, 8 show that the performance of the 100K-parameter anisotropic GNNs (GCN with
symmetric normalization, GAT, MoNet, GatedGCN) are consistently better than the isotropic
models (vanilla GCN, GraphSage), except for vanilla GCN-WikiCS, GraphSage-MNIST and
MoNet-CIFAR10. Table 14, discussed later, dissects and demonstrates the importance of
anisotropy for the link prediction tasks, TSP and COLLAB. Overall, our results suggest that
understanding the expressive power of attention-based neighborhood aggregation functions
is a meaningful avenue of research.
Underlying challenges for training WL-GNNs. We consistently observe a relatively
high standard deviation in the performance of WL-GNNs (recall that we average across
4 runs using 4 diﬀerent seeds). We attribute this ﬂuctuation to the absence of universal
training procedures like batching and batch normalization, as these GNNs operate on dense
rank-2 tensors of variable sizes. On the other hand, GCNs running on sparse tensors better
leverage batched training and normalization for stable and fast training. Leading graph
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
machine learning libraries represent batches of graphs as sparse block diagonal matrices,
enabling batched training of GCNs through parallelized computation .
Dense tensors are incompatible with the prevalent approach, disabling the use of batch
normalization for WL-GNNs. We experimented with layer normalization 
but without success. We were also unable to train WL-GNNs on CPU memory for the single
COLLAB graph, see Table 5. Practical applications of the new WL-GNNs may require
redesigning the best practices and common building blocks of deep learning, i.e. batching of
variable-sized data, normalization schemes, and residual connections.
3WL-GNNs perform the best among their class. Among the models in the WL-
GNN class, 3WL-GNN provide better results than its similar counter-part RingGNN and
achieves close to the best performance for AQSOL, see Table 4. The GIN model, while being
less expressive, is able to scale better and provides overall good performance.
E. Studies using the Benchmarking Framework
One of the primary goals of this benchmarking framework is to facilitate researchers to perform
new explorations conveniently and develop insights that improve our overall understanding
of graph neural networks. This section provides a demonstration of two such studies that
we carry out by leveraging the datasets and the coding infrastructure which are part of this
framework. First, we explore the absence of positional information in graphs for MP-GCNs
which induces their low representation power. As a result, we develop a new insight that
Laplacian eigenvectors can very simply be used as graph positional encodings and improve
MP-GCNs. This insight has been received keenly in the recent literature and there are
a number of works that propose positional encoding schemes with some addressing the
challenges of using Laplacian eigenvectors . Second, we study and show how the modiﬁcation of existing MP-GCNs with
joint edge representations help the models perform comparatively better than their vanilla
counterparts.
E.1 Laplacian Positional Encodings
As discussed in Section D, MP-GCNs outperforms WL-GNNs on the diverse collection of
datasets included in our proposed benchmark despite having theoretical limitations derived
from the alignment of MP-GCNs to the WL-tests. Also, WL-GNNs were found to be
computationally infeasible on medium and large scale datasets. Motivated by these results,
we propose ‘Graph Positional Encodings’ using Laplacian eigenvectors, thus referred as
Laplacian Positional Encodings, to improve the theoretical shortcomings of MP-GCNs, which
allows us to retain the computationally eﬃciency oﬀered by the message-passing framework
and improve the MP-GCNs performance.
E.1.1 Related Work
In Murphy et al. ; Srinivasan and Ribeiro , it was pointed out that standard
MP-GCNs might perform poorly when dealing with graphs that exhibit some symmetries
in their structures, such as node or edge isomorphism. This is related to the limitation of
MP-GCNs due to their equivalence to the 1-WL test .
Benchmarking Graph Neural Networks
The equivalence is based on the condition when MP-GCNs handle anonymous nodes , i.e. nodes do not have unique node features. To address this issue of anonymous MP-
GCNs, Murphy et al. introduced a framework, called Graph Relational Pooling (GRP),
that assigns to each node an identiﬁer that depends on the index ordering. This approach
can be computationally expensive as it requires to account for all n! node permutations, thus
requiring some sampling in practice. You et al. proposed learnable position-aware
embeddings based on random anchor sets of nodes for pairwise node (or, link) tasks. However,
the random selection of anchor sets has limitations and their approach is not applicable
on inductive node tasks. Similarly, one could think of using full or partial random node
identiﬁers for breaking node-anonymity. Yet, it suﬀers from generalization to unseen graphs
 . Li et al. proposed the use of distance encoding
as node attributes which captures distances between nodes using power(s) of random walk
matrix. However, their failure on distance regular graphs and the cost of
computing the power matrices may be limiting to scale to diverse and medium to large-scale
graphs. We improve upon these works and propose the use of Laplacian eigenvectors as
positional encodings.
E.1.2 Laplacian eigenvectors as Positional Encodings
We keep the overall MP-GCN architecture and simply add positional features to each node
before processing the graph through the MP-GCN. Intuitively, the positional features should
be chosen such that nodes which are far apart in the graph have diﬀerent positional features
whereas nodes which are nearby have similar positional features. As node positional features,
we propose to use graph Laplacian eigenvectors , which have less
ambiguities and which better describe the distance between nodes on the graph. Formally,
Laplacian eigenvectors are spectral techniques that embed the graphs into the Euclidean
space. These vectors form a meaningful local coordinate system, while preserving the global
graph structure. Mathematically, they are deﬁned via the factorization of the graph Laplacian
∆= I −D−1/2AD−1/2 = UT ΛU,
where A is the n × n adjacency matrix, D is the degree matrix, and Λ, U correspond
respectively to the eigenvalues and eigenvectors. Laplacian eigenvectors also represent a
natural generalization of the Transformer positional encodings (PE)
for graphs as the eigenvectors of a discrete line (NLP graph) are the cosine and sinusoidal
functions. The computational complexity O(E3/2), with E being the number of edges, can be
improved with, e.g. the Nystrom method . The eigenvectors are deﬁned
up to the factor ±1 (after being normalized to unit length), so the sign of eigenvectors will
be randomly ﬂipped during training. For the experiments, we use the k smallest non-trivial
eigenvectors, where the k value is given in the respective experiment tables in Section C as
the dimensions of the PE. The smallest eigenvectors provide smooth encoding coordinates of
neighboring nodes. See Section E.1.4 for additional discussion about positional encodings
and the reasoning behind our decision to use random sign ﬂipping.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
E.1.3 Experiments and Analysis
We ﬁrst use the mathematical graphs such as CSL, CYCLES and GraphTheoryProp included
in our benchmark (Sections C.8-C.10) to validate the proposed Laplacian PE as simple
augmentations in MP-GCNs to improve their performance on the datasets. On CSL dataset,
Table 10 compares the MP-GCNs using the Laplacian eigenvectors as PE and the WL-GNNs.
The MP-GCN models were the most accurate with 99% of mean accuracy, while 3WL-GNN
obtained 97% and RingGNN 25% with our experimental setting. Similarly, in Table 11 for
CYCLES dataset and Table 12 for GraphTheoryProp dataset, where we simply select 2
representative MP-GCNs (GINs and GatedGCNs), we observe a consistent improvement
in the performance when GINs and GatedGCNs are augmented with Laplacian PE. This
demonstrates the importance of positional features to successfully detect cycles in a graph,
and also predict critical theoretical and geometric properties in a graph.
Next, we study ZINC, AQSOL, WikiCS, PATTERN, CLUSTER and COLLAB with
PE (note that MNIST, CIFAR10 and TSP do not need PE as the nodes in these graphs
already have features describing their positions in R2). We observe a boost of performance
for ZINC, AQSOL and CLUSTER ), an improvement for PATTERN, and statistically the same
result for COLLAB, see the respective tables in Section C. This way, MP-GCNs can be
augmented with Laplacian PE to overcome their limitations of not being able to detect simple
graph symmetries. Additionally, PEs also boost the models’ performance on real-world graph
learning tasks.
E.1.4 Challenges with using Laplacian eigenvectors
Ideally, positional encodings (PEs) should be unique for each node, and nodes which are
far apart in the graph should have diﬀerent positional features whereas nodes which are
nearby have similar positional features. Note that in a graph that has some symmetries,
positional features cannot be assigned in a canonical way. For example, if node i and node j
are structurally symmetric, and we have positional features pi = a, pj = b that diﬀerentiate
them, then it is also possible to arbitrary choose pi = b, pj = a since i and j are completely
symmetric by deﬁnition. In other words, the PE is always arbitrary up to the number of
symmetries in the graph. As a consequence, the network will have to learn to deal with
these ambiguities during training. The simplest possible positional encodings is to give an
(arbitrary) ordering to the nodes, among n! possible orderings. During training, the orderings
are uniformly sampled from the n! possible choices in order for the network to learn to be
independent to these arbitrary choices .
We propose an alternative to reduce the sampling space, and therefore the amount of
ambiguities to be resolved by the network. Laplacian eigenvectors are hybrid positional and
structural encodings, as they are invariant by node re-parametrization. However, they are
also limited by natural symmetries such as the arbitrary sign of eigenvectors (after being
normalized to have unit length). The number of possible sign ﬂips is 2k, where k is the
number of eigenvectors. In practice we choose k ≪n, and therefore 2k is much smaller n!
(the number of possible ordering of the nodes). During the training, the eigenvectors will be
uniformly sampled at random between the 2k possibilities. If we do not seek to learn the
invariance w.r.t. all possible sign ﬂips of eigenvectors, then we can remove the sign ambiguity
Benchmarking Graph Neural Networks
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
10.000±0.000
10.000±0.000
0.58s/0.05hr
EigVecs-20
68.633±7.143
99.811±0.232
0.59s/0.09hr
Rand sign(EigVecs)
99.767±0.394
99.689±0.550
0.59s/0.16hr
Abs(EigVecs)
99.433±1.133
100.000±0.000
0.60s/0.12hr
Fixed node ordering
10.533±4.469
76.056±14.136
0.59s/0.05hr
Rand node ordering
11.133±2.571
10.944±2.106
0.60s/0.08hr
85.605±0.105
85.999±0.145
646.03s/11.36hr
86.029±0.085
86.955±0.227
645.36s/11.94hr
Rand sign(EigVecs)
86.508±0.085
86.801±0.133
647.94s/12.08hr
Abs(EigVecs)
86.393±0.037
87.011±0.172
645.90s/11.41hr
Fixed node ordering
80.133±0.202
98.416±0.141
643.23s/8.27hr
Rand node ordering
85.767±0.044
85.998±0.063
645.09s/11.79hr
73.684±0.348
88.356±1.577
399.44s/6.97hr
EigVecs-20
75.520±0.395
89.332±1.297
400.50s/5.70hr
Rand sign(EigVecs)
76.082±0.196
88.919±0.720
399.66s/6.58hr
Abs(EigVecs)
73.796±0.234
91.125±1.248
398.97s/6.68hr
Fixed node ordering
69.232±0.265
92.298±0.712
400.40s/5.82hr
Rand node ordering
74.656±0.314
82.940±1.718
397.75s/6.88hr
52.635±1.168
96.103±1.876
453.47s/12.09hr
EigVecs-20
52.326±0.678
96.700±1.296
452.40s/12.10hr
Rand sign(EigVecs)
52.849±1.345
96.165±0.453
452.75s/12.08hr
Abs(EigVecs)
51.419±1.109
95.984±1.157
451.36s/12.07hr
Test MAE±s.d.
Train MAE±s.d.
Epoch/Total
0.354±0.012
0.095±0.012
10.52s/0.49hr
0.319±0.010
0.038±0.007
10.62s/0.43hr
Rand sign(EigVecs)
0.214±0.013
0.067±0.019
10.70s/0.56hr
Abs(EigVecs)
0.214±0.009
0.035±0.011
10.61s/0.50hr
Fixed node ordering
0.431±0.007
0.044±0.009
10.62s/0.35hr
Rand node ordering
0.321±0.015
0.177±0.015
10.55s/0.55hr
Table 13: Study of positional encodings (PEs) with the GatedGCN model . Performance reported on the test sets of CSL, ZINC, PATTERN, CLUSTER and COLLAB
(higher is better, except for ZINC). Red: the best model.
of eigenvectors by taking the absolute value. This choice seriously degrades the expressivity
power of the positional features.
Numerical results for diﬀerent positional encodings are reported in Table 13. For all
results, we use the GatedGCN model . We study 5 types of
positional encodings; EigVecs-k corresponds to the smallest non-trivial k eigenvectors, Rand
sign(EigVecs) randomly ﬂips the sign of the k smallest non-trivial eigenvectors in each batch,
Abs(EigVecs) takes the absolute value of the k eigenvectors, Fixed node ordering uses the
original node ordering of graphs, and Rand node ordering randomly permutes ordering of
nodes in each batch. We observed that the best results are consistently produced with the
Laplacian PEs with random sign ﬂipping at training. For index PEs, randomly permuting
the ordering of nodes also improves signiﬁcantly the performances over keeping ﬁxed the
original node ordering. However, Laplacian PEs clearly outperform index PEs.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
E.2 Edge representations for link prediction.
E.2.1 With GatedGCN and GAT
The TSP and COLLAB edge classiﬁcation tasks present an interesting empirical result for
GCNs: Isotropic models (vanilla GCN, GraphSage) are consistently outperformed by their
Anisotropic counterparts which use joint representations of adjacent nodes as edge features
during aggregation (GAT, GatedGCN). In this section, we systematically study the impact
of anisotropy by instantiating three variants of GAT and GatedGCN:
(1) Isotropic aggregation ) with node updates
of the form:
identiﬁed by (E.Feat,E.Repr=x,x) in Table 14;
(2) Anisotropy using edge features ) with
node updates as:
j) · W ℓhℓ
with (E.Feat,E.Repr=✓,x);
and (3) Anisotropy with edge features and explicit edge representations updated at each
layer with node/edge updates as (such as in GatedGCN by default (Bresson and Laurent,
ij · W ℓhℓ
with (E.Feat,E.Repr=✓,✓).
The formal update equations of the three variants of GatedGCN are:
Isotropic, similar to vanilla GCNs with sum aggregation:
where Uℓ, V ℓ∈Rd×d.
Anisotropic with intermediate edge features computed as joint representations of adjacent
node features at each layer:
j′∈Ni σ(ˆeℓ
ij = Aℓhℓ−1
where Uℓ, V ℓ∈Rd×d, ⊙is the Hadamard product, and eℓ
ij are the edge gates.
Anisotropic with edge features as well as explicit edge representations updated across layers
in addition to node features, as in GatedGCN by default, Eq.(16):
j′∈Ni σ(ˆeℓ
Benchmarking Graph Neural Networks
where Uℓ, V ℓ∈Rd×d, ⊙is the Hadamard product, and eℓ
ij are the edge gates. The input
edge features from the datasets (e.g. distances for TSP, collaboration year and frequency for
COLLAB) can optionally be used to initialize the edge representations ˆeℓ=0
ij . Note that there
may be a multitude of approaches to instantiating anisotropic GNNs and using edge features
 besides the ones
we consider.
The formal update equations of the three variants of GAT are:
Isotropic, similar to multi-headed vanilla GCNs with sum aggregation:
where Uk,ℓ∈R
Anisotropic with intermediate edge features computed as joint representations of adjacent
node features at each layer, as in GAT by default, Eq.(10):
j′∈Ni exp(ˆek,ℓ
ij = LeakyReLU
V k,ℓConcat
where Uk,ℓ∈R
K ×d, V k,ℓ∈R
K are the K linear projection heads and ek,ℓ
ij are the attention
coeﬃcients for each head.
Anisotropic with edge features as well as explicit edge representations updated across layers
in addition to node features:
Bk,ℓConcat
ij, Uk,ℓhℓ
j′∈Ni exp(ˆak,ℓ
V k,ℓConcat
ij, Uk,ℓhℓ
where Uk,ℓ∈R
K ×d, V k,ℓ∈R
K , Ak,ℓ∈R
K ×d, Bk,ℓ∈R
K are the K linear projection
heads and ak,ℓ
ij are the attention coeﬃcients for each head. The input edge features from the
datasets can optionally be used to initialize the edge representations eℓ=0
Numerical Experiments and Analysis
In Table 14, we show the experiments of the three variants of GatedGCN and GAT
on TSP and COLLAB. GatedGCN-E and GAT-E in Table are models using input edge
features from the datasets to initialize the edge representations eij. As maintaining edge
representations comes with a time and memory cost for the large COLLAB graph, all models
use a reduced budget of 27K parameters to ﬁt the GPU memory, and are allowed to train
for a maximum of 24 hours for convergence.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
0.646±0.002
0.648±0.002
150.83s/8.34hr
0.757±0.009
0.760±0.009
197.80s/12.06hr
0.791±0.003
0.793±0.003
218.20s/9.72hr
GatedGCN-E
0.808±0.003
0.811±0.003
218.51s/12.04hr
0.643±0.001
0.644±0.001
325.22s/12.10hr
0.671±0.002
0.673±0.002
68.23s/6.25hr
0.748±0.022
0.749±0.022
462.22s/12.10hr
0.782±0.006
0.783±0.006
438.37s/12.11hr
35.989±1.549
60.586±4.251
263.62s/10.90h
50.668±0.291
96.128±0.576
384.39s/18.44hr
51.537±1.038
96.524±1.704
376.67s/19.85hr
GatedGCN-E
47.212±2.016
85.801±0.984
377.04s/16.49hr
41.141±0.701
70.344±1.837
371.50s/15.97hr
50.662±0.687
96.085±0.499
403.52s/19.69hr
49.674±0.105
92.665±0.719
349.19s/19.59hr
44.989±1.395
82.230±4.941
328.29s/11.10hr
Table 14: Study of anisotropy and edge representations for link prediction on TSP and COLLAB.
Red: the best model, Violet: good models.
On both TSP and COLLAB, upgrading isotropic models with edge features signiﬁcantly
boosts performance given the same model parameters (e.g. 0.75 vs. 0.64 F1 score on TSP,
50.6% vs. 35.9% Hits@50 on COLLAB for GatedGCN with edge features vs. the isotropic
variant). Maintaining explicit edge representations across layers further improves F1 score for
TSP, especially when initializing the edge representations with euclidean distances between
nodes (e.g. 0.78 vs. 0.67 F1 score for GAT-E vs. standard GAT). On COLLAB, adding
explicit edge representations and inputs degrades performance, suggesting that the features
(collaboration frequency and year) are not useful for the link prediction task (e.g. 47.2
vs. 51.5 Hits@50 for GatedGCN-E vs. GatedGCN). As suggested by Hu et al. , it
would be interesting to treat COLLAB as a multi-graph with temporal edges, motivating
the development of task-speciﬁc anisotropic edge representations beyond generic attention
and gating mechanisms.
E.2.2 With GraphSage
Interestingly, in Table 5 for COLLAB, we found that the isotropic GraphSage with max
aggregation performs close to GAT and GatedGCN models, both of which perform anisotropic
mean aggregation. On the other hand, models which use sum aggregation (GIN, MoNet) are
unable to beat the simple matrix factorization baseline. This result indicates that aggregation
functions which are invariant to node degree (max and mean) provide a powerful inductive
bias for COLLAB.
We instantiate two anisotropic variants of GraphSage, as described in the following
paragraphs, and compare them to GAT and GatedGCN on COLLAB in Table 15. We ﬁnd
that upgrading max aggregators with edge features does not signiﬁcantly boost performance.
On the other hand, maintaining explicit edge representations across layers hurts the models,
presumably due to using very small hidden dimensions. (As previously mentioned, maintaining
representations for both 235K nodes and 2.3M edges leads to signiﬁcant GPU memory usage
and requires using smaller hidden dimensions.)
Benchmarking Graph Neural Networks
Aggregation
Train Acc.
35.989±1.549
60.586±4.251
263.62s/10.90h
Weighted Mean
50.668±0.291
96.128±0.576
384.39s/18.44hr
Weighted Mean
51.537±1.038
96.524±1.704
376.67s/19.85hr
GatedGCN-E
Weighted Mean
47.212±2.016
85.801±0.984
377.04s/16.49hr
41.141±0.701
70.344±1.837
371.50s/15.97hr
Weighted Mean
50.662±0.687
96.085±0.499
403.52s/19.69hr
Weighted Mean
49.674±0.105
92.665±0.719
349.19s/19.59hr
Weighted Mean
44.989±1.395
82.230±4.941
328.29s/11.10hr
50.908±1.122
98.617±1.763
241.49s/10.62hr
Weighted Max
50.997±0.875
99.158±0.694
366.24s/11.46hr
Weighted Max
48.530±1.919
90.990±9.273
359.18s/11.88hr
GraphSage-E
Weighted Max
47.315±1.939
93.475±5.884
359.10s/12.07hr
Table 15: Study of anisotropic edge features and representations for link prediction on COLLAB,
including GraphSage models. Red: the best model, Violet: good models.
Isotropic, as in GraphSage by default, Eq.(8):
i, Maxj∈Ni ReLU
where Uℓ∈Rd×2d, V ℓ∈Rd×d.
Anisotropic with intermediate edge features computed as joint representations of adjacent node features at each layer:
i, Maxj∈Ni ReLU
where Uℓ∈Rd×2d, V ℓ, Aℓ∈Rd×d, ⊙is the Hadamard product, and eℓ
ij are the edge gates.
Anisotropic with edge features as well as explicit edge representations updated across
layers in addition to node features:
i, Maxj∈Ni ReLU
where Uℓ∈Rd×2d, V ℓ, Aℓ, Bℓ∈Rd×d, ⊙is the Hadamard product, and ˆeℓ
ij are the edge
gates. The input edge features from the datasets can optionally be used to initialize the edge
representations eℓ=0
F. Experiments on TU datasets
Apart from the proposed datasets in our benchmark (Section C), we perform experiments
on 3 TU datasets for graph classiﬁcation – ENZYMES, DD and PROTEINS. Our goal
is to empirically highlight some of the challenges of using these conventional datasets for
benchmarking GNNs.
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
Test Acc.±s.d.
Train Acc.±s.d.
Epoch/Total
55.833±3.516
93.062±7.551
0.18s/0.17hr
53.833±4.717
87.854±10.765
0.19s/0.18hr
vanilla GCN
65.833±4.610
97.688±3.064
0.69s/0.67hr
64.833±7.089
93.042±4.982
0.74s/0.70hr
65.000±4.944
100.000±0.000
1.62s/1.34hr
68.167±5.449
100.000±0.000
1.76s/1.42hr
63.000±8.090
95.229±5.864
0.53s/0.49hr
62.167±4.833
93.562±5.897
0.68s/0.62hr
68.500±5.241
100.000±0.000
0.70s/0.59hr
68.500±4.622
100.000±0.000
0.76s/0.66hr
65.667±4.899
99.979±0.062
2.31s/2.05hr
70.000±4.944
99.979±0.062
2.63s/2.30hr
65.333±6.823
100.000±0.000
0.53s/0.61hr
67.667±5.831
100.000±0.000
0.60s/0.68hr
18.667±1.795
20.104±2.166
7.12s/6.71hr
45.333±4.522
56.792±6.081
8.05s/11.16hr
61.000±6.799
98.875±1.571
9.22s/9.83hr
57.667±9.522
96.729±5.525
11.80s/11.09hr
72.239±3.854
73.816±1.015
6.36s/6.61hr
72.408±3.449
73.880±0.623
1.13s/1.11hr
vanilla GCN
72.758±4.083
100.000±0.000
3.56s/2.66hr
73.168±5.000
100.000±0.000
3.81s/2.88hr
73.433±3.429
100.000±0.000
11.50s/8.59hr
71.900±3.647
100.000±0.000
6.60s/4.90hr
71.736±3.365
81.003±2.593
3.30s/2.34hr
71.479±2.167
81.268±2.295
2.83s/2.01hr
75.900±3.824
95.851±2.575
6.31s/3.56hr
74.198±3.076
96.964±1.544
2.84s/1.75hr
72.918±2.090
82.796±2.242
12.05s/10.13hr
71.983±3.644
83.243±3.716
8.78s/7.93hr
71.910±3.873
99.851±0.136
5.28s/4.08hr
70.883±2.702
99.883±0.088
2.31s/1.79hr
75.644±2.681
79.847±1.551
0.42s/0.29hr
75.823±2.915
79.442±1.443
0.35s/0.24hr
vanilla GCN
76.098±2.406
81.387±2.451
1.55s/1.53hr
75.912±3.064
82.140±2.706
1.46s/1.42hr
75.289±2.419
85.827±0.839
3.36s/2.30hr
75.559±1.907
85.118±1.171
3.44s/2.35hr
76.452±2.898
78.206±0.548
1.23s/1.06hr
76.453±2.892
78.273±0.695
1.26s/1.03hr
76.277±2.410
83.186±2.000
1.47s/1.42hr
75.557±3.443
84.253±2.348
1.51s/1.41hr
76.363±2.904
79.431±0.695
5.03s/4.13hr
76.721±3.106
78.689±0.692
4.78s/3.64hr
74.117±3.357
75.351±1.267
1.02s/1.20hr
71.241±4.921
71.373±2.835
1.04s/1.06hr
67.564±7.551
67.607±4.401
28.61s/12.08hr
56.063±6.301
59.289±5.560
19.08s/11.88hr
61.712±4.859
62.427±4.548
12.82s/7.58hr
64.682±5.877
65.034±5.253
13.05s/7.32hr
Table 16: Performance on the TU datasets with 10-fold cross validation (higher is better). Two
runs of all the experiments using the same hyperparameters but diﬀerent random seeds are shown
separately to note the diﬀerences in ranking and variation for reproducibility. The top 3 performance
scores are highlighted as First, Second, Third.
Splitting. Since the 3 TU datasets that we use do not have standard splits, we perform
a 10-fold cross validation split which gives 10 sets of train, validation and test data indices in
the ratio 8 : 1 : 1. We use stratiﬁed sampling to ensure that the class distribution remains the
same across splits. The indices are saved and used across all experiments for fair comparisons.
There are 480 train/60 validation/60 test graphs for ENZYMES, 941 train/118 validation/119
test graphs for DD, and 889 train/112 validation/112 test graphs for PROTEINS datasets in
each of the folds.
Training. We use Adam optimizer with a similar learning rate strategy as used in our
benchmark’s experimental protocol. An initial learning rate is tuned from a range of 1×10−3
to 7 × 10−5 using grid search for every GNN models. The learning rate reduce factor is 0.5,
the patience value is 25 and the stopping learning rate is 1 × 10−6.
Performance Measure. We use classiﬁcation accuracy between the predicted labels and
groundtruth labels as our performance measure. The model performance is evaluated on the
test split of the 10 folds for all TU datasets, and reported as the average and the standard
deviation of the 10 scores.
Our numerical results on the TU datasets – ENZYMES, DD and PROTEINS are presented
in Table 16. We observe all NNs have similar statistical test performance as the standard
deviation is quite large. We also report a second run of these experiments with the same
experimental protocol, i.e. the same 10-fold splitting and hyperparameters but diﬀerent
initialization (seed). We observe a change of model ranking, which we attribute to the
small size of the datasets and the non-determinism of gradient descent optimizers. We also
observe that, for DD and PROTEINS, the graph-agnostic MLP baselines perform as good as
Benchmarking Graph Neural Networks
GNNs. Our observations reiterate how experiments on the small TU datasets are diﬃcult to
determine which GNNs are powerful and robust.
G. A Note on Graph Size Normalization
Intuitively, batching graphs of variable sizes may lead to node representation at diﬀerent
scales, making it diﬃcult to learn the optimal statistics µ and σ for BatchNorm across
irregular batch sizes and variable graphs. A preliminary version of this work introduced a
graph size normalization technique called GraphNorm, which normalizes the node features
i w.r.t. the graph size, i.e.,
where V is the number of graph nodes. The GraphNorm layer is placed before the BatchNorm
We would like to note that GraphNorm does not have any concrete theoretical basis as
of now, and was proposed based on initially promising empirical results on datasets such
as ZINC and CLUSTER. Future work shall investigate more principled approaches towards
designing normalization layers for graph structured data.
H. Elaboration on Benchmarking Design Choices
In Section 2, we provided a brief overview on the design choices that we had to make to
build the proposed benchmarking framework. In particular, the decisions on the selection
of the speciﬁc graph datasets that we have included in this framework, the necessity to
constraint model parameters for comparison of GNNs’ performance, and whether a standard
codebase with data, training, evaluation pipelines is required can be derived from several
reasonings. In this section, we provide an elaborate discussion on these factors and how
possible extensions can be developed in future with ease and as per required by a research
Datasets. Our collection of datasets is based on medium-scale size and criteria of diversity in
terms of the end-application domains, learning tasks at graph-, edge-, or node-levels, and their
source of construction being real or mathematical. The medium-scale size of datasets enables
quick prototyping of novel ideas and robust analysis could be generated in single experiments
in as less as 12 hours of maximum time per experiment. Similarly, the diversity ensures a
model can be tested on not just one end-application domain but a number of such domains.
However, despite the best eﬀorts, after any collection of datasets in such a research area where
a general GNN architecture is expected to be robust to a variety of tasks and domains, there
could always be need of additional datasets. Due to this necessity, the proposed framework
can be extended with new datasets conveniently by any researchers adopting it. We have also
observed the open-sourced GitHub repo of our framework being used accordingly with an
example repo being which extends the framework with additional node classiﬁcation datasets as well as
adopts it in Pytorch Geometric instead of DGL. Such adoption of our
framework demonstrates its ﬂexibility and the supported convenient extensions. We provide
Dwivedi, Joshi, Luu, Laurent, Bengio and Bresson
detailed instructions on adding new datasets to the framework in our GitHub repository’s
Parameter Budgets. As we have already mentioned, we designed the framework with the
objective that it is used to conveniently ‘identify ﬁrst principles’ in GNNs’ research and
not drive a model towards achieving SOTA performance. To enforce this, a straightforward
and sound choice is to constraint model parameters and ﬁx it to a speciﬁc number (as
eg. 100k and 500k) when comparing two or more GNNs. With this choice, we can likely
rely on the inference that performance gains are coming from architectural designs and not
merely large trainable parameters. The parameter budgeting also tells that the proposed
framework may not be ideal to optimize a model to achieve SOTA by tuning hyperparameters,
increasing model size to as much parameters as a server can ﬁt, etc. However, we believe we
condition the framework to be suitable for identifying performance trends and infer which
ﬁrst principles work robustly across diﬀerent model experiments. Once such principles are
identiﬁed, models can further be scaled without any constraints to achieve SOTA performance
targeted benchmarks, beyond the datasets we included here.
Codebase. A major contribution of this work is the release of the open-source coding
infrastructure on GitHub. As observed since the ﬁrst release in March 2020, the framework
has been used extensively to develop new ideas in the ﬁeld. In the existing literature prior
to this work , it was a major issue that diﬀerent research papers in this
ﬁeld adopted inconsistent model comparison methods. Our framework addresses this need of
having a standard codebase that helps in training and evaluating GNNs on a collection of
appropriate datasets with consistent settings. While a limiting perspective to such codebase
can be that it restricts on the diverse choices which researchers often adopt in deep learning
to fully realise the capabilities of a model, we understand that we have set out speciﬁc
objectives of the need of the proposed coding infrastructure and any extensions with other
training settings to the codebase can be done by augmenting methods or modules that applies
to each model in a fair and consistent way.
I. Hardware
Timing research code can be tricky due to diﬀerences of implementations and hardware
acceleration. Nonetheless, we take a practical view and report the average wall clock time
per epoch and the total training time for each model. All experiments were implemented in
DGL/PyTorch. We run experiments for MNIST, CIFAR10, ZINC, AQSOL, TSP, COLLAB,
WikiCS, CSL, CYCLES, GraphTheoryProp and TUs on an Intel Xeon CPU E5-2690 v4
server with 4 Nvidia 1080Ti GPUs (11 GB), and for PATTERN and CLUSTER on an Intel
Xeon Gold 6132 CPU with 4 Nvidia 2080Ti (11 GB) GPUs. Each experiment was run on a
single GPU and 4 experiments were run on the server at any given time (on diﬀerent GPUs).
We run each experiment for a maximum of 12 hours.
J. Memory Usage
For datasets that contain graphs with variable sizes, the memory consumed during training
by the GPU device changes at each batch of graphs. We report in Figure 16 the GPU memory
consumption during the training of GCN, GAT and GatedGCN on two datasets–ZINC and
Benchmarking Graph Neural Networks
Batch Iteration
GPU Memory (in MB)
Batch Iteration
SBM-CLUSTER
Figure 16: Memory consumed by the GPU device in a forward pass of a batch during training.
All GNNs shown here have 500k learnable parameters. Batch size is 128 for ZINC and 64 for
SBM-CLUSTER.
SBM-CLUSTER. The plots show the memory allocated during the model’s forward pass
using a batch of graphs (128 graphs for ZINC and 64 for SBM-CLUSTER) and is computed
by using PyTorch’s torch.cuda.memory_allocated(device) functionality. Overall, it can
be observed that GatedGCN is a relatively higher memory-intensive model as compared with
GCN and GAT, see Section B.1 for the respective models’ equations.