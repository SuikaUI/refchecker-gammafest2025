Learning Deep Representations for Graph Clustering
University of Science
and Technology of China
 
Microsoft Research
 
Tsinghua University
 
Enhong Chen
University of Science
and Technology of China
 
Tie-Yan Liu
Microsoft Research
 
Recently deep learning has been successfully adopted in
many applications such as speech recognition and image classiﬁcation. In this work, we explore the possibility of employing deep learning in graph clustering.
We propose a simple method, which ﬁrst learns a nonlinear embedding of the original graph by stacked autoencoder, and then runs k-means algorithm on the embedding to obtain clustering result. We show that this
simple method has solid theoretical foundation, due to
the similarity between autoencoder and spectral clustering in terms of what they actually optimize. Then, we
demonstrate that the proposed method is more efﬁcient
and ﬂexible than spectral clustering. First, the computational complexity of autoencoder is much lower than
spectral clustering: the former can be linear to the number of nodes in a sparse graph while the latter is super quadratic due to eigenvalue decomposition. Second, when additional sparsity constraint is imposed, we
can simply employ the sparse autoencoder developed
in the literature of deep learning; however, it is nonstraightforward to implement a sparse spectral method.
The experimental results on various graph datasets show
that the proposed method signiﬁcantly outperforms conventional spectral clustering, which clearly indicates the
effectiveness of deep learning in graph clustering.
Introduction
Deep learning has been a hot topic in the communities
of machine learning and artiﬁcial intelligence. Many algorithms, theories, and large-scale training systems towards
deep learning have been developed and successfully adopted
in real tasks, such as speech recognition ,
image classiﬁcation , and natural language processing . However, to our knowledge, the adoption of deep
learning in clustering has not been adequately investigated
∗This work was done when the two authors were visiting Microsoft Research Asia.
Copyright c⃝2014, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
yet. The goal of this work is to conduct some preliminary
investigations along this direction.
Clustering aims to group similar patterns among massive data points. Graph clustering is a key branch of clustering, which tries to ﬁnd disjoint partitions of graph nodes
such that the connections between nodes within the same
partition are much denser than those across different partitions. On one hand, many real-world problems can be cast as
graph clustering such as image segmentation , community detection , and
VLSI design ; on the other
hand, it is easy to transform a clustering problem in the vector space to a clustering problem on the similarity graph built
from the vector representations of the data points. Therefore,
we choose to put our focus on graph clustering in this work,
and in particular, we investigate the use of stacked sparse
autoencoder to perform graph clustering.
Our proposal is motivated by the similarity between autoencoder and spectral clustering, a state-of-the-art graph
clustering method, in terms of what they actually optimize.
Among many existing graph clustering algorithms 
 , spectral clustering has attracted people’s great attention in the past decades due to its solid theoretical foundation
and global optimal solution. Given an n-node graph, spectral clustering method runs an Eigenvalue Decomposition
(EVD) on the normalized graph Laplacian matrix; then the
eigenvectors corresponding to the k smallest non-zero eigenvalues are extracted as the representation of the graph nodes,
where k is the predeﬁned number of clusters; 1 after that, a
k-means method is run on the graph representations to get
the clusters results.
Note that these k eigenvectors are also the eigenvectors of
the normalized graph similarity matrix, whereas corresponding to its k largest eigenvalues. Therefore these eigenvectors
can be regarded as an encoding of the normalized graph similarity matrix, and according to the Eckart-Young-Mirsky
1Some researchers also suggest selecting the eigenvectors corresponding to the ⌊
k⌋smallest non-zero eigenvalues.
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence
theorem, this encoding can lead to the optimal rank-k reconstruction of the original normalized graph similarity matrix.
People familiar with autoencoder may immediately realize
that spectral clustering is very similar to autoencoder: autoencoder also attempts to ﬁnd a low-dimensional encoding
of the input data that can keep the most information of the
original set of data through reconstruction. Actually, our theoretical study shows that the objective functions of spectral
clustering and autoencoder are actually very similar when
they are used to solve graph clustering problems (i.e., the
reconstruction error of the normalized similarity matrix under the Frobenuis norm by the encoding), and both of them
can achieve the desired solution when the optimization processes are well done.2
While autoencoder is similar to spectral clustering in
theory, it is much more efﬁcient and ﬂexible in practice.
First, as we know, spectral clustering is computationally
expensive because it involves an eigenvalue decomposition
(EVD). The complexity of a straightforward implementation
of EVD is cubic to the number of nodes in the graph, while
even the fastest implementation to our knowledge requires a
super-quadratic computational complexity. In contrast, autoencoder is highly efﬁcient due to its back-propagation
framework, whose computational complexity can be linear
to the number of nodes in the graph when the nodes are
sparsely connected. Second, When dealing with very largescale data, we usually hope to get some sparse representations so as to improve the efﬁciency of the data processing.
However, the encoding produced by spectral clustering cannot guarantee the sparsity property because the eigenvectors
of the graph Laplacian are probably very dense. In addition,
it is non-straightforward to incorporate sparsity constraints
into spectral clustering without destroying its nature of being a spectral method. In contrast, it is very easy to fulﬁll
the sparsity requirement by using the autoencoder. Actually,
one can simply use the sparse autoencoder developed in the
literature of deep learning for this purpose, which basically
introduces a L1 regularization term to the original objective
function of autoencoder. Furthermore, one can stack multiple layers of sparse autoencoders, to achieve additional beneﬁt from deep structures.
Based on the above discussions, we propose a method
called GraphEncoder for graph clustering. First, we feed
the normalized graph similarity matrix into a deep neural network (DNN) which takes sparse autoencoder as the
building block. Then through a greedy layer-wise pretraining process, we seek the best non-linear graph representations that can approximate the input matrix through reconstruction and achieve the desired sparsity properties. After
stacking several layers of sparse autoencoders, we run kmeans on the sparse encoding output by the ﬁnal layer to
obtain the clustering results. To verify the effectiveness of
the proposed method, we conducted extensive experiments
on various real-world graph datasets. The experimental results show that the proposed algorithm can signiﬁcantly out-
2Note that spectral clustering can obtain the global optimum;
however, autoencoder usually leads to a local optimum due to the
back-propagation algorithm it employs.
perform the baseline algorithms like spectral clustering. The
results also indicate that the stacked deep structure can boost
the clustering results, i.e., the results become more and more
accurate when going from the shallow layers to the deep layers.
To the best of our knowledge, this is the ﬁrst work that
investigates how to use deep learning for graph clustering.
It enriches our understanding on the power of deep learning, by opening a door to use unsupervised pre-training techniques like stacked sparse autoencoder to deal with the clustering problems.
Background and Related Work
Given an undirected weighted graph G = (V,E), where V =
{v1,v2,...,vn} is the node set and E = {ei j} is the edge set,
graph clustering aims to ﬁnd a disjoint partition {Vi}k
V, where k is cluster number. As has been mentioned before, our proposed model is highly related to spectral clustering and deep learning, so we will brieﬂy review these two
Spectral Clustering
We use S = {si j} to denote the similarity matrix of graph G,
and thus si j (i, j = 1,2,··· ,n) is the similarity score between
node i and j. Let di = ∑j si j be the degree of node i, based
on which we measure the capacity of a subset A of V, i.e.,
vol(A) = ∑i∈A di. For any disjoint subsets A,B ⊂V, we de-
ﬁne link(A,B) = 1
2 ∑i∈A,j∈B si j. One of the commonly-used
clustering objective that spectral clustering aims to minimize
is the Normalized Cut (NCut):
NCut(V1,...,Vk) =
link(Vi, ¯Vi)
Here Vi ∩¯Vi = /0 and Vi ∪¯Vi = V. To achieve this goal, spectral clustering converts the above objective function to the
following discrete optimization problem:
H∈Rn×k Trace(HT LH)
HT DH = I.
(i = 1,2,··· ,n; j = 1,2,··· ,k);
D = diag(d1,d2,··· ,dn);
Here L is the so-called graph Laplacian matrix. It can be
seen that the discrete optimization problem in (2) is NP-Hard
 . Therefore, spectral clustering
turns to relax the condition in (2) by allowing Hi j to take any
real values. According to some matrix calculus, the solution
yields H to consist the eigenvectors corresponding to the k
smallest non-zero eigenvalues of the normalized Laplacian
matrix D−1L. The ﬁnal clustering results are then obtained
through running the k-means algorithm on the graph embedding matrix H.
There are works towards efﬁcient implementation of spectral clustering such as , in which the authors’ aim is to construct a sparse similarity graph as the
input to the parallelized spectral clustering method, whereas
what we aim to achieve is to replace the expensive EVD
in spectral clustering, leading to the difference with former
Deep Learning
Recently deep learning has won great success in many applications such as image classiﬁcation, speech recognition,
and natural language processing . One of the strategies that make training deep
architectures possible and effective is the greedy layerwised
unsupervised pretraining 
 . This strategy aims to learn useful representations one layer at a time, and then to set the output
features to be the input of the next layer. Each layer in this
process involves some kind of non-linearity, such as nonlinear activation function (e.g., sigmoid or tanh) or some
regularization on features ). By stacking these non-linear single layers
together, deep learning are believed to yield better representations ).
In the greedy layerwised pretraining process, autoencoder
 is
commonly used as a basic unit to generate new representations in each layer, and it is also the main building block
in our model. In the autoencoder framework, a feature extraction function f(·;θ1) is ﬁrstly implemented on original
feature vector xi, i = 1,2,··· ,n (n is the number of training samples), yielding a new representation f(xi;θ1). Function f(·;θ1) is named as encoder. After that, f(xi;θ1) is
transformed back into the input space by another function
g(·;θ2), which is called as decoder. The aim of autoencoder
is to minimize the reconstruction loss between the original
data and the reconstructed data from the new representations, i.e.,
Loss(θ1,θ2) =
l (xi,g(f(xi;θ1);θ2)).
Here l(·) is the sample-wise loss function. Usually encoder
and decoder are composed of a linear transformation, followed by an activation function. That is, f(x;θ1) = f0(Wx+
b), g(x;θ2) = g0(Mx + d), where f0 and g0 are activation
functions like the element-wise sigmoid function. In this
sense, θ1 = {W,b} and θ2 = {M,d} are the parameters to be
learned in the training process. Furthermore, there are works
on sparse autoencoder, which aims to penalize the large hidden layer outputs .
Model Description
In this section, we introduce our proposed deep learning
method for graph clustering, including the motivation, the
basic building block, and the implementation details.
Motivation
As mentioned in the introduction, our proposal is motivated
by the similarity between autoencoder and spectral clustering. To show this, we will ﬁrst explain spectral clustering
in the viewpoint of matrix reconstruction, and then demonstrate that autoencoder is a better choice than spectral clustering in the scenario of large-scale graph clustering.
We start with some notations. Let Q = D−1L, where L
is the graph Laplacian matrix and D is the diagonal matrix
with the node degrees in the corresponding diagonal elements. According to the properties of the Laplacian matrix,
Q is symmetric and its rank is assumed to be n −r, where
r is the number of connected components in the graph G.
We write the eigenvalue decomposition of Q as Q = YΛY T,
where Y ∈Rn×n stacks the N eigenvectors of Q in columns
and Y TY = I, Λ = diag(λ1,λ2,··· ,λn), λ1 > λ2 > ··· > λn
are the n eigenvalues of Q. Let Λk denote the diagonal matrix
with the k smallest non-zero eigenvalues in Λ in its diagonal elements, and Yk ∈Rn×k be the matrix containing the k
columns from Y corresponding to the k non-zero eigenvalues in Λk. Thus Yk is the embedding matrix used in spectral
clustering when minimizing the normalized cut in order to
cluster the graph nodes into k groups.
Note that the non-zero requirement on the eigenvalues
does not impact the clustering result by much when k is relatively large. The reason is as follows. The zero eigenvalue of
Q corresponds to the eigenvector with all its elements equal
to 1. If we put this eigenvector into Yk, it will have no effect
on the clustering result except that it will squeeze out the
least informational eigenvector. For simplicity, in the following discussions we will directly talk about the k smallest eigenvalues in Λ without requiring the eigenvalues to
be non-zero. Further note that Q = D−1L = D−1(D −S) =
I −D−1S. Therefore, the k smallest eigenvalues of Q are
exactly the k largest eigenvalues of the normalized graph
similarity matrix D−1S, and accordingly Yk contains the k
eigenvectors of D−1S corresponding to its k largest eigenvalues. The Eckart-Young-Mirsky Theorem explains the reconstruction nature of spectral clustering, which is related to the low-rank approximation of a matrix.
Theorem 1 (Eckart-Young-Mirsky). For a rank-r matrix
P ∈Rm×n, with singular value decomposition (SVD) P =
UΣV T, UTU = I, V TV = I; if k < r, we have:
rank( ˜P) = k
||P−˜P||F = U ˜ΣV T
where ˜Σ is the same matrix as Σ except that it contains only
the k largest singular values and the other singular values
are replaced with 0.
The above theorem shows that the matrix reconstruction
by the truncated largest k singular vectors in SVD is the best
rank-k approximation of the original matrix. Furthermore, if
a matrix Z is symmetric, i.e., Z = PPT, there exists orthogonal decomposition Z = UΣ2UT=UΛUT, where UTU = I
and Λ is diagonal matrix with the eigenvalues of Z in the
diagonal elements. This is the well-known fact that the SVD
of a matrix P is highly related to the EVD of the symmetric
matrix PPT. Speciﬁcally, for symmetric matrix Z, the matrix reconstruction by the truncated largest k eigenvectors in
EVD is also the best rank-k approximation under the Frobe-
nuis norm. According to the above discussions, we obtain
the following corollary.
Corollary 2 YkΛkY T
k is the best reconstruction of the symmetric matrix D−1S in terms of the Frobenuis norm among
all rank-k matrices, where Yk is the embedding matrix in
spectral clustering.
On one hand, Corollary 2 tells us that spectral clustering
can be regarded as a process of matrix reconstruction for
D−1S. From the new embedding matrix Yk obtained through
eigenvalue decomposition, it can build the best rank-k matrix approximation towards the normalized graph similarity
matrix D−1S in terms of Frobenuis norm. On the other hand,
imagine that if we use the normalized graph similarity matrix D−1S as the input feature matrix for an autoencoder, we
are actually also seeking for the best matrix reconstruction
for D−1S in terms of Frobenuis norm by solving the autoencoder. That is, both autoencoder and spectral clustering minimize the reconstruction error for the original normalized
similarity matrix D−1S. We therefore say that autoencoder
and spectral clustering are similar in terms of what they optimize.
While autoencoder is similar to spectral clustering in theory, the former is much more ﬂexible in incorporating additional constraints. In many real large-scale graph clustering
problems, it is desirable to seek some sparse representations
as the graph embedding. On one aspect, this can greatly improve the efﬁciency of the system in terms of both storage
and data processing. On another aspect, it usually also improves the clustering accuracy, since it can remove some
noisy information that hurts the clustering results. However,
the embedding produced by spectral clustering cannot guarantee the sparsity property since the eigenvectors of Q are
probably very dense. Furthermore, it is non-straightforward
to introduce sparsity constraint to spectral clustering; if we
do it in a stiff way (e.g., directly add a sparsity constraint to
the objective function of spectral clustering), it is very likely
that we cannot get a spectral method any more. In sharp contrast, we will have a much easier life if we use autoencoder.
That is, we can simply adopt the sparse autoencoder developed in the literature of deep learning. It basically introduces a sparsity regularization term to the original objective
function of autoencoder and can still beneﬁt from the efﬁcient back-propagation algorithm for the optimization. Furthermore, one can stack multiple layers of sparse autoencoders, to achieve additional beneﬁt from deep structures.
The above discussions motivate us to adopt an autoencoderbased method for graph clustering.
GraphEncoder
In this subsection we introduce the autoencoder-based graph
clustering model called GraphEncoder. In general, the key
component of GraphEncoder is a deep neural network with
sparse autoencoder as its building block. As stated in the
previous sections, given an n-node graph G with its similarity matrix S, we can treat S as the training set containing n instances s1,s2,··· ,sn, si ∈Rn, i = 1,2,··· ,n. Note
that si = {sij}, j = 1,2,··· ,n. Then we feed the normalized
training set D−1S into the deep neural network and use the
Table 1: Clustering with GraphEncoder
n-node graph G, with similarity matrix S ∈Rn×n and
degree matrix D = diag{d1,d2,··· ,dn}, where di is the
degree of node i; DNN layers number Γ, with number of
nodes n(j) in layer j; n(1) = n; X(j) ∈Rn×n(j) is the input
to layer j. X(1) = D−1S.
For j = 1 to Γ
1. Build a three layer sparse autoencoder with input data X(j).
2. Train the sparse autoencoder by optimizing (6) with backpropagation. Obtain the hidden layer activations h(j).
3. Set X(j+1) = h(j).
Run k-means on XΓ ∈Rn×n(Γ).
Final clustering result.
output features in the deepest layer of DNN as the graph embedding. At last, k-means clustering is implemented on the
graph embedding of G to produce the ﬁnal clustering result.
Speciﬁcally, we consider the autoencoder in each layer
of the deep neural network. Let xi be the i-th input vector
of this layer, and f0 and g0 be the activations of the hidden layer and the output layer respectively. We have hi =
f0(Wxi + b) and yi = g0(Mhi + d), where Θ = {θ1,θ2} =
{W,b,M,d} are the parameters to be learned, f0 and g0
are the non-linear operators such as the sigmoid function
(sigmoid(z) = 1/(1+exp(−z))) or tanh function (tanh(z) =
(ez −e−z)/(ez +e−z)). Then the optimization goal is to minimize the reconstruction error between the original data xi
and the reconstructed data yi from the new representation
||yi −xi||2.
We also impose the sparsity constraints to the activation
in the hidden layer. That is, we add a regularization term to
the reconstruction error in (5),
||yi −xi||2 +βKL(ρ|| ˆρ),
where β controls the weight of the sparsity penalty, ρ is set
to be a small constant such as 0.01, ˆρ = 1
j=1 hj is the average of the hidden layer activations, and KL(ρ|| ˆρ) is deﬁned
KL(ρ|| ˆρ) =
+(1−ρ)log 1−ρ
Note that (6) can be solved by standard back-propagation
algorithms. After the training of the current layer is completed, we use the hidden layer activations as the inputs to
train the next layer. This greedy layer-wise training process
forms the model of the Stacked Sparse Autoencoder . When all the layers are trained in this manner, we use the output of the ﬁnal layer as the new graph
representation and run k-means on it to get the clustering results. The whole procedure is summarized as the algorithm
in Table 1.
The proposed model GraphEncoder has the following advantages:
• Usually in autoencoder, the dimension of the hidden layer
is lower than that of the input layer. This captures the intuition that not all edges in the original graph are necessary in clustering. For a certain node, it is possible that
only its relationships with part of the other nodes (e.g., the
nodes with the highest degrees) determine which cluster
it should belong to. The optimization of (5) captures this
useful low dimensional information.
• The sparsity penalty in (6) not only strengthens the requirements of deleting edges as stated above, but also
makes the computation more efﬁcient given the sparsity
target ρ is a small value and the activations approach zero.
• The stacked structures provide a smooth way of eliminating edges. Graph representations are expected to become
clearer and clearer for clustering as the training goes from
shallow layers to deep layers. We will use an example in
the experimental session to demonstrate this effect of the
deep structure.
• GraphEncoder is much more efﬁcient than spectral clustering. Spectral clustering relies on EVD. To our knowledge, the computational complexity of the fastest EVD
solver is O(n2.367) where n is the number of nodes in the
graph, when the graph has some special sparse and dense
structure like Toeplitz matrix . In
contrast, it is not difﬁcult to see that the complexity of
GraphEncoder is O(ncd), where d is the maximum number of hidden layer nodes in DNN, and c is the average
degree of the graph. Usually c can be regarded as a ﬁxed
value: for example, in the top-k similarity graph, c = k;
and in a social network graph, c, the average friend number of people, is also bounded. Parameter d is related to
the predeﬁned number of clusters (more clusters lead to
larger d), but not related to n. Therefore we can regard cd
as some constant that does not increase with n, and the
overall complexity of GraphEncoder is linear to the number of nodes. In addition, whereas EVD is hard to parallel, the stochastic gradient descent (SGD) 
training of DNN is comparatively easy to parallel, as have
been well explored in the literature of DNN.
Experimental Evaluation
We report the experimental results in this section. We ﬁrst
introduce the datasets and the benchmark algorithms used in
the experiments, and then give the experiment settings. After
that, we show the clustering performance of our proposed
clustering algorithm compared with benchmark algorithms.
In addition, we give an example to show the power of deep
structures in clustering.
To test the performance of our DNN-based model, we evaluated its performance on several real world datasets.
1. Wine. This is a dataset from UCI Machine Learning
Repository , consisting of
178 instances with 13 attributes. Every instance corresponds to a certain wine with its chemical analysis information as the attributes. All instances are labeled with 3
wine categories. We built a cosine similarity graph using
these instances and used the labels as the groundtruth.
2. 20-Newsgroup. This dataset is a collection of about
20,000 newsgroup documents. The documents are partitioned into 20 different groups according to their topics.
We represented every document as a vector of tf-idf scores
of each word and built the cosine similarity graph based
on the tf-idf scores. To demonstrate the robustness of our
algorithms with different targeting cluster numbers, we
constructed three graphs built from 3, 6, and 9 different
newsgroups respectively. The newsgroup names in each
graph are listed as the following, where the abbreviation
NG used in graph names is short for Newsgroup.
• 3-NG: corp.graphics, rec.sport.baseball, talk.politics.guns.
• 6-NG: alt.atheism, comp.sys.mac.hardware, rec.motorcycles,
rec.sport.hockey, soc.religion.christian, talk.religion.misc.
• 9-NG: talk.politics.mideast, talk.politics.misc, comp.os.mswindows.misc,
comp.sys.ibm.pc.hardware,
sci.electronics,
sci.crypt, sci.med, sci.space, misc.forsale
For each chosen group, we randomly selected 200 documents from it, and thus the three graphs contain 600,
1,200, and 1,800 nodes respectively. The document labels
are used as the groundtruth.
3. DIP. This is an unweighted protein-protein interaction
(PPI) network from the Database of Interacting Proteins
 . The average degree of nodes is
4. BioGrid. The last dataset is another PPI network obtained
from the BioGrid Database . We removed the nodes without any connections to other nodes
from the original graph and got a ﬁnal graph of 5,964
nodes. The average degree is approximately 65, which is
much higher than that of DIP. We used the protein complex data in CYC2008 as the groundtruth
for the two PPI networks.
The detailed information of all these graphs are summarized in Table 2. Since all these datasets have groundtruth,
we evaluated the performance of a clustering algorithm by
the Normalized Mutual Information (NMI) of its clustering
results. The range of NMI values is . The higher the
NMI is, the better the corresponding clustering results are.
Table 2: Datasets Information.
Weighted or Not
Fully Connected
Fully Connected
Fully Connected
Fully Connected
Unweighted
Unweighted
Benchmark Algorithms
We used the following graph clustering methods as the
benchmark algorithms.
1. Spectral Clustering. We used two versions of spectral
clustering:
unnormalized
clustering
 , which aims to minimize ratio
cut, and normalized spectral clustering , which aims to minimize normalized cut. The better
NMI value output by the two versions was recorded as
the benchmark results.
2. k-means. Our algorithm runs k-means on the new graph
embedding in the deep layers. To show the power of the
deep structures, we also run the k-means algorithm directly on the original graph (i.e., k-means on the n × n
normalized similarity matrix) as a benchmark method.
Experiment Settings
We implemented GraphEncoder based on Sparse Autoencoder (SAE), which is the autoencoder model that penalizes
both the reconstruction error and the sparsity error in the hidden layer. In our experiments, we tuned two parameters of
SAE: sparsity penalty, which tradeoffs the weight of the two
errors in the optimization, and sparsity target, which is the
target value that the hidden layer activations aim to reach.
The neural networks for Wine and 3-NG have 3 layers and
the neural networks for all the other graphs have 5 layers.
The number of nodes in each layer are listed in Table 3. All
the neural networks use element-wise sigmoid function as
activations of each layer.
Table 3: Neural Network Structures
#nodes in each layer
178-128-64
600-512-256
1,200-1,024-512-256-128
1,800-1,024-512-256-128
4,741-2,048-1,024-512-256
5,964-2,048-1,024-256-128
For Wine and the three 20-Newsgroup graphs, since their
underlying cluster numbers by the groundtruth labels are
small, we do not vary the target cluster numbers in the experiments. We just set the cluster numbers to their real cluster
numbers, i.e. Wine with 3, the three 20-Newsgroup graphs
with 3, 6, and 9 respectively.
For the two PPI networks, we checked the algorithm performance with various predeﬁned cluster numbers. That is,
we varied the target cluster number from 5 to 400 (with average interval between two consecutive clustering numbers
to be 50), and plotted the NMI values varying with the target
cluster number for each algorithm.
Experimental Results
Clustering Performance.
The clustering results on Wine
and 20-Newsgroup datasets are summarized in Table 4. It
can be observed that the best performances of each dataset,
listed in bolded digits, are all obtained by GraphEncoder
built on sparse autoencoder.
Table 4: Clustering Results on Wine and 20-Newsgroup.
(Measured by NMI)
Spectral Clustering
The experimental results on the two PPI networks are
shown in Figure 1 and Figure 2 respectively, with horizontal
Spectral Clustering
Figure 1: Clustering Results on DIP.
Spectral Clustering
Figure 2: Clustering Results on BioGrid.
axis to be predeﬁned clusters number, and vertical axis to be
the corresponding NMI value.
We can see that for all the 6 graphs: (i) GraphEncoder
based on sparse autoencoder beats spectral clustering. This
is exactly the empirical justiﬁcation of our claim that sparsity on graph embedding can help to improve clustering results. (ii) GraphEncoder based on sparse autoencoder beats
k-means directly running on original normalized similarity
graph, showing that deep structures can help to get even better graph representations.
Power of Deep Structures.
To show the power of the
stacked deep structures, we list the NMI values of k-means
clustering on each layer’s embedding by GraphEncoder in
Table 5, in which we aim to cluster DIP and BioGrid
datasets into 50 groups. Note that DNN for both datasets
has ﬁve layers, with the shallowest layer to be layer 1 and
deepest layer to be layer 5. The NMI value on layer 1 is
exactly the result of directly running k-means on the input
normalized similarity graph. From Table 5 we can observe
that the NMI values become larger when the layer goes from
shallower to deeper, showing that the deep structures play an
important role in generating good graph representations for
clustering.
Table 5: Layer-wise NMI Values on Two PPI Networks with
#cluster= 50.
Conclusion
In this paper, we have proposed a novel graph clustering
method based on deep neural network, which takes the
sparse autoencoder as its building block. We introduced a
layer-wise pretraining scheme to map the input graph similarity matrix to the output graph embedding. Experimental
results on several real world datasets have shown that the
proposed method outperforms several state-of-the-art baselines including spectral clustering.