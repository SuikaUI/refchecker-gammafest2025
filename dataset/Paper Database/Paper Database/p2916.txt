Fuzzy Transfer Learning: Methodology and Application
Corresponding Author: Jethro Shell1,∗, Simon Coupland1,∗∗
De Montfort University,United Kingdom
Producing a methodology that is able to predict output using a model is a well studied area in Computational Intelligence (CI). However, a number of real-world applications require a model but have little or no data available of the
speciﬁc environment. Predominantly, standard machine learning approaches focus on a need for training data for such
models to come from the same domain as the target task. Such restrictions can severely reduce the data acquisition
making it extremely costly, or in certain situations, impossible. This impedes the ability of these approaches to model
such environments. It is on this particular problem that this paper is focussed.
In this paper two concepts, Transfer Learning (TL) and Fuzzy Logic (FL) are combined in a framework, Fuzzy
Transfer Learning (FuzzyTL), to address the problem of learning tasks that have no prior direct contextual knowledge.
Through the use of a FL based learning method, uncertainty that is evident in dynamic environments is represented.
By applying a TL approach through the combining of labelled data from a contextually related source task, and little or
no unlabelled data from a target task, the framework is shown to be able to accomplish predictive tasks using models
learned from contextually diﬀerent data.
Keywords: fuzzy logic, transfer learning, context, online adaptation, sensor networks, intelligent environments
94D05 MSC: ,
68T05 MSC:
1. Introduction
The availability of information impacts on the understanding of a problem. Further to this, a lack of information
reduces the ability to understand a problem. Diﬀerences in information and understanding about one problem domain
and a second similar domain can be deﬁned as being contained within a knowledge gap. This paper presents a novel
composition of methods that use a combination of inductive and deductive learning mechanisms that draw inspiration
from human approaches to problem solving, to bridge the knowledge gap. Two concepts, Transfer Learning (TL), a
methodology that allows information gained in diﬀerent contextual situations to assist new learning tasks2, and Fuzzy
Logic (FL), an approach to capture imprecision and uncertainty, are brought together in a novel framework to address
the problem of learning tasks that have no prior direct contextual knowledge.
Real-world applications often consist of many unknowns increasing the knowledge gap. To predict or classify
based on the information gathered from these applications can be diﬃcult. Standard machine learning approaches
require that there is a form of training data. Predominantly such training data has to come from the same domain.
Some applications make the procurement of a priori labelled training data demanding, or in some cases, not possible
at all. For example, to measure certain physical areas such as remote forest locations, impromptu set ups such as
disaster zones, or small user groups that have very deﬁned requirements such as disabled users.
∗+44 (0)116 255 1551
∗∗+44 (0)116 207 8419
Email addresses: (Corresponding Author: Jethro Shell), (Simon Coupland)
1The Centre for Computational Intelligence, De Montfort University, The Gateway, Leicester, LE1 9BH, United Kingdom
2In this research the term task is referred to as any action the learning method is required to accomplish
 
May 23, 2014
The procurement of training data produces an interesting problem. If there is a requirement to classify or predict
the output from such environments, how can a model be produced? The examples given previously present situations
where labelled data from the same distribution is costly. The lack of information within the same problem domain can
cause standard supervised learning to be ineﬀective. The process of labelling data can be costly. For example, there
may be few images that are labelled from a given feature space . Recognising that supervised methods require
that training data is supplied from the same domain, Semi-Supervised Learning (SSL) was introduced to approach
this issue . SSL sits in between supervised and unsupervised learning. Unlike supervised learning, where the goal
is to learn a mapping from x to y, given a training set of pairs (xi, yi), SSL is supplied with unlabelled data. yi ∈Y
is referred to as the labels of xi. Typically the focus of unsupervised learning is to ﬁnd structure in the unlabelled
data, X = (x1, . . ., xn) where xi ∈X for all of i ∈n . Additionally, large quantities of unlabelled data may not be
available. The initial reduced quantity of unlabelled domain data also renders the use of SSL and unsupervised less
The contexts discussed, however, can be related to other implementations which may contain previously discovered knowledge. The transferral of knowledge from one context to another is in keeping with the concept of a more
humanist style of learning, to reuse and repurpose information. Within the study of human learning, ordinary learning
is viewed as being ordinary when it is within the same context (a student may solve similar problems that are at the
end of a chapter that have appeared previously), whereas TL occurs outside of a single context (problems are solved
when they occur mixed with others at the end of the course) . Studies have shown that humans often draw upon
more than just training data for generalisation . In recent years there has been signiﬁcant quantities of research in
the area of TL and its application to real-world problems in the area of Computational Intelligence (CI) . TL
can be broadly deﬁned as a learning technique that uses knowledge from a source domain to increase the performance
of learning within the target task domain. The methodology allows the domains, tasks and distributions used within
the training and testing to be diﬀerent. The research within this paper presents a novel use of a TL method to model
scenarios where little or no information is initially known.
There is a strong relationship between context and uncertainty. As individuals endeavour to learn a new task they
often aﬀord uncertainty to it. There is a clear codependency on the level of certainty in any learning activity and
the amount of information that is available. Problems with little information can have a high degree of uncertainty
 . Dynamic applications such as Intelligent Environments (IEs) can exhibit this uncertainty in the sensors that
are used and the decision structures that are applied. The incorporation of a FL system is proposed to assist in the
modelling of environments in the presence of uncertainty and vagueness. The use of FL allows for the incorporation
of approximation and a greater expressiveness of the uncertainty within the data .
To summarise, this paper presents a novel framework, Fuzzy Transfer Learning (FuzzyTL), that uses the methods
within FL and TL to bridge the knowledge gap between the learning process of one context to another. Whilst the
abilities of the framework have been shown to be applied to predictive tasks (as illustrated in Section ??), there is a
belief that the generic nature of the framework allows it to be applied to problem spaces beyond these conﬁnes. The
paper illustrates that the use of FuzzyTL can outperform other state of the art methodologies in the prediction of data
for contexts that have little or no apriori knowledge.
The structure of the paper is as follows. The next section, Section 2 gives an overview of the current research in
this area. Section 3 describes the methodology used within the FuzzyTL framework. The experiments and results
carried out to analyse the proposed methodology are discussed in Section 4. The ﬁnal two sections conclude the paper
and outline future work.
2. Background
2.1. Context
The deﬁning of context is broad and far reaching. The study of context is a multi-disciplinary pursuit, ranging
from psychology and linguistics, to computing (especially within Computational Intelligence).
The concept of context plays an important role in both FL and TL. There is, however, no single consensus of how
context should be deﬁned. The structure of the FuzzyTL framework has foundations in the notion of context. TL has
the ability to use information from one domain to close the information gap in a learning process from a diﬀering but
similar domain. The domains can be deﬁned as contexts. To analyse the contexts, a valid deﬁnition of a context must
be put forward.
There has been considerable work surrounding context and computing. Dey puts forward a deﬁnition of
“Context is any information that can be used to characterise the situation of an entity. An entity is a
person, place, or object that is considered relevant to the interaction between a user and an application,
including the user and applications themselves.”
The deﬁnition by Dey relates to Dourish’s representational view of context . Dourish uses the representational
nature of software systems to represent and encode context. This view has similarities to deﬁnitions by Schilit et al.
 and Jang as each describe context through its relationship to information which is formed or expressed, to
varying degrees of abstraction. Dourish asserts four assumptions regarding context that are based upon these types of
deﬁnition. They are:
1. Context is a form of information. It is something that can be known and therefore represented.
2. Context is delineable. For an application, or set of applications, the context of the activities which the application
supports can be deﬁned.
3. Context is stable. Variation may occur within elements of the application from application to application,
however they do not vary from instance to instance.
4. Context and activity are separable. Context describes the features of an environment that an activity occurs
2.1.1. Deﬁning Context
To consolidate the varying descriptions of context, a high level, abstract deﬁnition is given, and subsequently used
within this paper. Taking inﬂuence from the work of Dey , Dourish and Bettini et al. , context is deﬁned
1. Information: Each context consists of deﬁnable variables that are relevant and measurable.
2. Behaviour: The context embodies an entity, application, service or group thereof that is aﬀected by the behaviour of the associated information.
3. Variation: Diﬀerences within the structure of the variables can occur between context to context, but not from
instance to instance within a context itself. This would be deﬁned as a new context.
This can be represented formally as:
α1 = {ai, b, c} j . . . α2 = {a, b, d}k
where α is a context containing multiple variables (a, b, c, or d) and varying quantities of those variables j and k.
Each context is deﬁnable as it contains variables that are measurable. The two contexts show variation as they contain
diﬀering variable structures. α contains a variable a with a diﬀering structure to α2. This variation does not occur
within the context itself, however.
Context is directly linked to the measurement of the real-world. The measuring of any application must take into
account the context in which it exists. As both the real-world and subsequently the context it embodies are evaluated,
the uncertainty and vagueness that are contained become more apparent. There is a need to capture and eﬀectively
represent this uncertainty and vagueness.
2.2. Uncertainty
Much of science requires the pursuit of precision and exactness. However, humans live in a world that is formed by
imprecision, vagueness and uncertainty. Real world applications are particularly at the mercy of this world. As people
endeavour to measure the world, imprecision emerges. The cost associated with the pursuit of increasing precision
rises in equal measure. As an example, parking a car is a simple task as it only requires that the ﬁnal placement of
the vehicle is imprecise. Generally, parking spaces allow for a large margin of error. Decreasing the error margin
from many centimetres to only a few millimetres, and so increasing the precision, would drastically increase the cost
associated in terms of execution . Similarly, uncertainty is codependent on the quantity of information that is
available. As more information about a problem is acquired, individuals become more certain about its formulation
and solution. Problems with less information have a higher degree of uncertainty .
By its very nature, uncertainty increases with a lack of knowledge. Uncertainty can be considered as existing in
the knowledge gap. A knowledge gap can be broadly deﬁned as the level of understanding that is exhibited based
on the information that is known, compared to an optimum level of understanding. Using the previous car parking
example, the optimum level of understanding may be considered to be how a driving instructor will park a vehicle
having full vision of the parking bay. Comparably, a student may have a reduced level of understanding. In this scenario, information may be limited, conﬂicting and vague because of limited driving experience and vehicles partially
obscuring the view of the parking bay. This alters the learning and understanding of the task, manifesting itself as
uncertainty. The diﬀerence in the understanding of the task that the instructor and the student exhibit is expressed by
the knowledge gap.
To tackle the uncertainty and vagueness that is exhibited within the real-world, the framework proposed in this
paper is based upon the use of FL. FL is used to capture the uncertainty and vagueness that exits in data. The FL
elements of the framework are brought together in a Fuzzy Inference System (FIS). A strength of the FIS is the ability
to handle linguistic concepts and perform non-linear mapping between inputs and outputs . FIS takes crisp inputs
and maps them to crisp outputs. A FIS principally contains four components: fuzzy rules, a fuzziﬁer, an inference
engine, and a defuzziﬁer . For a full description of a FIS, see and . Various automated learning methods
can be used to generate the main elements of a FIS, namely the fuzzy sets and the rulebase. Each learning method
approaches one or a number of these areas of the FIS. The framework proposed in this paper uses a Ad-Hoc Data
Driven Learning (ADDL) method.
2.3. Wang-Mendel Methodology
The FuzzyTL framework identiﬁes the structure of the FIS through the use of an automated learning process.
Numerical source data is used to form fuzzy rules and fuzzy sets. The process is based on an algorithm proposed by
Wang and Mendel .
The basis of the Wang-Mendel (WM) process is the formation of fuzzy sets and fuzzy rules that constitute the
main components of the FIS. The main element of the method is the generation of fuzzy rules from numerical pairs
which in turn are formed into a rulebase. The approach is a generalised one and is in keeping with the requirements of
the TL structure used in the FuzzyTL framework. Wang and Mendel proposed a multi-step procedure to produce
the fuzzy rules and fuzzy sets. This is described in Figure 1.
The WM initially divides each domain interval into fuzzy regions, each containing the membership functions for
that input or output. In order to automate this step, the domain is equally divided based upon the minimum and
maximum values of the interval and the deﬁned number of regions. Rules are also created by generating membership
values from each data point. To remove conﬂicts and reduce the rulebase to a manageable size, the WM algorithm
retains those rules with the highest membership in the same group. The ﬁnal stage is to map the input to the output
values. This is achieved through a defuzziﬁcation process.
Although a number of data driven learning methods were considered for the FuzzyTL framework, the simplistic
and easy approach to the implementation of the WM method allows for a quick adoption of the framework. Equally,
the ability to modify its components are fundamental to the FuzzyTL frameworks adaptive approach. The swiftness in
the execution within the early stages of the preliminary fuzzy modelling process allows for subsequent adaptation of
the model by other methods . Its level of generality and maturity have allowed it to be applied to a large number
of applications, which is applicable to the FuzzyTL approach.
2.4. Transfer Learning
The motivation of transfer learning is to improve the learning in a target domain by acquiring information from a
diﬀering but related domain. Traditional machine learning strategies work under a number of assumptions. Mihalkova
et al. propose that the learning of each new task begins from scratch. Many machine learning techniques also
require that training and testing data is sourced from the same feature space. This can be diﬃcult, or in some cases
impossible. TL oﬀers the ability to apply previously acquired knowledge to areas where little or no information is
available, improving the learning. TL has been applied to varying domains: activity recognition , eye tracking ,
gaming and image classiﬁcation to name but a few. A major motivation behind the FuzzyTL framework
comes from environments that lack any prior knowledge in the form of labelled training data.
Labelled Data
Fuzzy Rules
Fuzzy Regions
of Rulebase
Map Output
Defuzziﬁcation
From Data Pairs
Figure 1: Multi-step Wang-Mendel Process.
2.4.1. Measures, Deﬁnition and Foundations
Transfer learning contains two principle elements, a Domain and a Task. According to Pan and Yang , a
Domain can be deﬁned as consisting of two components: a feature space x and a marginal probability distribution
P(x) where X = {x1, . . ., xn} ∈X. A Task consists of a label space Y and a predictive function f(.). The predictive
function can be learned from the training data which is constructed as data pairs {xi, yi} where xi ∈X and yi ∈Y. The
source domain can be deﬁned as Ds = {(xs1, ys1), . . ., (xsn, ysn)} where xs ∈Xs is the data point and ys ∈Ys is the
corresponding label.
Based on these deﬁnitions transfer learning can be deﬁned as:
Given a source domain Ds and a learning task Ts, a target domain Dt and a learning task Tt, transfer
learning aims to improve the learning of a new task Tt through the transfer of knowledge from a related
task Ts by the learning of the predictive function in the target domain Dt, where Ds , Dt or Ts , Tt
There are a number of diﬀering types of TL. The FuzzyTL framework uses an Inductive Informed Unsupervised
Transfer Learning method.
2.4.2. Transductive Informed Unsupervised Transfer Learning
Transductive transfer learning is derived from classical transductive learning. The process of transductive learning
uses both a labelled training set of data combined with an auxiliary unlabelled set [? ]. This is in contrast to inductive
learning which is the process of acquiring knowledge by drawing inductive inferences from teacher or environmentprovided facts [? ]. In the context of TL, this is where labelled data exists in the training data and within the testing
data, but the test data is hidden from the process. As with standard machine learning approaches, there are number
of variations based on the availability of training data. Within this paper, we will use the terminology of Informed
Unsupervised (IU), Informed Supervised (IS), Uninformed Unsupervised (UU) and Uninformed Supervised (US).
The term IU transfer learning was introduced by Cook, Feuz and Krishnan . Deviating from the standard terms
of supervised and unsupervised learning, they introduced the use of informed and uninformed which are applied to the
availability of labelled data in the source and target areas. IS transfer learning implies that labelled data is available
in both the target and source domains. Labelled data in the target domain is required to induce an objective predictive
model. In transductive transfer learning both Ds = {(xs, ys)} and Dt = {(xt)} are known where Ds is the source data
and Dt is the training data. Additionally there is auxiliary unlabelled data that is not part of the training set .
IU transfer learning, however deﬁnes that the labelled data is only available in the source domain. By contrast, US
learning implies that labelled data is available only in the target domain with UU transfer learning implying that there
is no availability of labelled data in either domains.
2.4.3. Limited-Data Transfer Learning Methods
Within TL there are a number of sparse and limited-data methods. In this context, limited-data refers to scenarios
with datasets that are a low percentage of the overall quantity. One-Shot learning highlights this approach . The
basis of One-Shot Learning can be identiﬁed by drawing parallels with the abilities of humans to identify objects
under a wide variety of conditions after seeing only a single point. The process acquires knowledge from one setting
and uses it in another. The methodology models new classes of objects based only on samples of related or support
classes through probability densities.
Larochelle et al. expand the concept of limited data with the introduction of Zero-data Learning. Zero-data
learning is based on the premise that a model must generalise to classes or tasks where there is no availability of
training data, only a description of the data. It is assumed that the situation may occur that no labelled data is available
so descriptions are used. There are similarities between the problem proposed within this paper and that approached
by Larochelle et al. In , the authors assumed that the descriptions that are used within the classiﬁcation process
are predeﬁned. Within Zero-data Learning the hierarchical deﬁnitions can often come from expert opinion, diﬀering
from the automated, data-driven strategy chosen within the FuzzyTL approach.
3. Fuzzy Transfer Learning Methodology
A facet of learning is the ability to transfer information from one context to another. Information gained through
learning can be generalised, absorbing inconsistencies and anomalies. The hypothesis of the framework is that what
has been learnt can be adapted in order to accomplish the new task, building upon and adapting the previous knowledge. In this section the major elements of the FuzzyTL methodology are outlined.
3.1. Overview
The FuzzyTL methodology is contained within a framework structure. The key components can be seen in Figure
In this structure there are two distinct processes: the transferring of the fuzzy concepts and their relationships, and
the adaptation of the fuzzy components using knowledge of the application context.
In the ﬁrst stage the system uses a source of labelled data to instigate a learning process. The learning process uses
this source data to construct a FIS. The structure of the FIS consists of fuzzy sets and fuzzy rules. The FIS is used to
capture the knowledge from the source, and transfer it to the target task. This process of transferring information is a
fundamental aspect of the FuzzyTL methodology, and highlights the use of an IU TL method.
Unlike IS TL where a quantity of labelled data is required from both the source and the target task, IU TL describes
situations where no labelled data is available from the target task. The FuzzyTL methodology captures information
from the source task to act as an initial learning point for the target task. This is the basis for the TL process.
The second stage of the framework addresses the adaptation of the FIS. The adaptation process uses knowledge
from the unlabelled task dataset coupled with previously learnt information. This process adapts the individual components of the FIS to capture the variations in the data. Alterations and variations from situation to situation, are
absorbed through changes made within the domains of the fuzzy sets and adaptations to the rulebase. Using this
structure, the FuzzyTL methodology is shown to be able to use the transfer of information to assist in bridging the
knowledge gap. Through an online adaptation process, newly accrued information can be absorbed.
To continue the discussion of the novel FuzzyTL methodology, a series of elements need to be predeﬁned. A
source domain Ds can be deﬁned as
Fuzzy Rules
Fuzzy Sets
Source Task
Unlabelled
Processing
Target Task
Predictive
Transferable
Adaptation
Labelled Data
Exhaustive
Figure 2: Overview of the Fuzzy Transfer Learning Framework.
where x ∈X are data inputs, y ∈Y is an output, n is the number of inputs, m is the number of outputs, and P is the
number of data tuples within the domain. Equally, the target Dt and adaptive domains Da can be deﬁned in the same
Additionally, the domain can be deﬁned through the use of intervals. Intervals are used to represent the domains
to allow for the use of interval arithmetic. This has been demonstrated previously within the application of fuzzy
temporal relationships . Within this paper, an interval is referred to as a bounded set of real numbers
A = [aL, aR] = {a : aL ≤a ≤aR, a ∈R}
B = [bL, bR] = {b : bL ≤b ≤bR, b ∈R}
where aL and aR are the left and right limits of the interval A . Two intervals A and B are considered equal if their
corresponding endpoints are equal. So, A = B if aL = bL and aR = bR. The intersection of two intervals is empty
A ∩B = ∅, if aR < bL or bL > aR .
Based on this deﬁnition and using the interval notation, a source domain interval can be deﬁned as
A domain will also be deﬁned through its relationship to fuzzy sets. A source domain with n inputs, and m outputs
is deﬁned as
Ds = { f Xs
where f Xs
n is a set of fuzzy input sets, and f Ym is a set of fuzzy output sets. f Xs
n can be deﬁned as
where vs to vl are the sets very small, small, medium, large and very large respectively. These sets can be any
description that is suitable to the context. Within this paper, the fuzzy sets are constructed as normal, continuous and
triangular.
A target domain with l inputs is deﬁned as
Dt = { f Xt
where f Xt
n is a set of fuzzy input sets.
The rulebases used within the subsequent sections are deﬁned using the same notation. A rulebase that contains n
antecedents and m consequent sets is depicted as
R = { f Xr
where R is the rulebase, X is a data input, Y is the corresponding output, and P is the number of rules.
3.2. Transferring Fuzzy Concepts
The ﬁrst stage of the FuzzyTL process is the construction of the FIS (Fuzzy Inference System). Fuzzy rules and
fuzzy sets are formed via the use of an ADDL process which is calculated from numerical data. The method uses
numerical data to form the sets and rules, a procedure based on an algorithm proposed by WM .
The basis of the WM process is the formation of fuzzy sets and fuzzy rules from numerical data. The use of the
method is not restricted to an individual application domain and has been shown to be applicable to a broad number
of implementations .
The FuzzyTL framework transfers information from context through the use of a model based on a FIS. The WM
algorithm uses numerical data to extract the fuzzy sets and a rulebase that form constituent parts of the FIS.
The FuzzyTL framework follows the standard WM process for the production of the fuzzy sets and fuzzy rules.
A full description of the WM process can be found in .
A fundamental aspect of the WM process is the reduction of the produced of the fuzzy rulebase. Under the standard
WM process, a weighted measure is used to produce a Reduced Rule Base (RRB). This emphasises the rules with
the highest antecedent and consequent membership values, reducing the exhaustive rulebase to a reduced set of rules.
Previous research has shown that within the exhaustive rulebase there exists further information that can inform both
the RRB construction and further enhance the transfer process. The FuzzyTL framework adds to the WM method by
supplementing the process with a fuzzy frequency measure. The addition of the fuzzy frequency measure endeavours
to remove the possibility of anomalous data inﬂuencing the production of fuzzy rules. An in depth explanation of this
process can be found in .
To capture more of the information contained within the exhaustive rulebase, the FuzzyTL framework uses a frequency value. This value gains more information from the original rulebase by focusing on the number of occurrences
of each rule.
3.3. Adaptation Through Learning
As outlined in Section 3.2, the transferral of the FIS embodies the TL component of the FuzzyTL methodology.
Using the notation in Section 2.4.1, this can be described as transferring a source domain Ds to model a predictive
function of a target domain Dt. The relationship between Ds and Dt inﬂuences the output of the model. If there exists
some relationship, explicit or implicit, between the two domains this is categorised as being related. The nature of
the relationship will dictate the necessity for the adaptation of the knowledge contained within the source domain and
the learning task. If the domains are equal and the learning tasks are approaching the same problem, no adaptation
is required, however, this is rare within real world applications. Separation of the domains results in the need for an
adaptation process.
In order for the framework to absorb such changes from the source to the target contexts, the elements of the
transferable FIS are adapted. Using the knowledge housed within the exhaustive rulebase, the FIS itself and newly
acquired information, changes are made in order for the framework to output the required data. The adaptation process
consists of ﬁve stages.
3.3.1. External Input Domain Adjustment: Stage One
A knowledge gap can occur during the transfer of learning structures from one contextual domain to another.
This can be captured as both diﬀerences in the domains themselves, and diﬀerences in the learning structure. To
absorb such contextual diﬀerences in the source and target tasks, the FuzzyTL adapts the minimum and maximum
values within the domain. Taking each input instance of the dataset, the framework adjusts the range of the interval
according to any diﬀerence calculated between the transferable FIS and the new input values.
A source domain Ds can be represented as data tuples (x1, x2, y), where x1 and x2 are inputs and y represents the
output. A new domain is formed based on Ds incorporating the alterations made through the adaptation process. This
is deﬁned as Da. Da represents missing information between the source and target tasks.
Each data point is analysed to extract information. The input interval is adapted if the value extends beyond the
left (xL) or right (xR) boundaries. This produces a new set structure. Any adaptation to the domain results in an equal
distribution across the sets. This is due to the equal spacing. Extension of the domain requires a simple change to the
footprint of each set.
Unevenly distributed membership functions require a scaling function in order to adapt the sets. In the FuzzyTL
framework triangular functions are used, however, other functions are applicable. A triangular function can be deﬁned
when a −s ≤x ≤a + s
where x is the input value, a is the centre of the function and s is the width. In a similar manner to equally distributed
sets, the centre of the function a is used as the anchor point. If the domain is shifted in a negative or positive direction,
the sets are moved by the centre points. Each point is moved an equal distance. Any extension or compression of the
domain requires that the sets are altered according to the scaling. This process is shown in Figure 3a. Here, three sets
are shown in the X universe. The sets shown (Small, Medium and Large) are uneven and have diﬀering footprints.
The example shows the domain increased by 30%. If the domain of the target task is contained within the source task,
an alternative strategy is required to adapt both set structure and domains.
3.3.2. Internal Input Domain Adjustment: Stage Two
The second adaptation stage also focuses on the input domains. The transferring of source domains can require
adaptation to remove the knowledge gap. The knowledge gap can be represented by diﬀerences in the domain intervals. In Figure 3b, interval DI
t1 is shown to be a subset of DI
t2 partially overlaps DI
s . This can
be represented as DI
t2R. Where necessary, stage one increases the overall size of the
domain interval either by decreasing the left limit or increasing the right limit to reduce the diﬀerences. However, in
transferring the source to the target, there may be a need to reduce the domain to within the source extremities, either
partially or wholly. In Figure 3b, the source domain DI
s , has been reduced to form DI
t2 is shifted in a positive
direction along the axis. The left limit DI
t2L has been moved in a positive direction from DI
sL. This is accomplished by
stage two of the adaptation process. The right limit DI
t2R has also been shifted in a positive direction to the outside of
the source interval. This is accomplished by stage one.
The process used for internal input domain adjustment is illustrated in Figure 4. This diagram shows the ﬂow of
the process through three key steps.
• Step One: Initialisation This step initialises the system by processing the data from the source domain to gain an
input interval. The source domain input interval can be deﬁned as DI
s (X) = [xL, xR] with xL being the minimum
value and xR being the maximum value of each input value in the domain.
• Step Two: Correlation Unlike the source task, the target task has extremely limited data availability. To address
this lack of knowledge, the adaptation system uses local minimum and maximum values to compare to the
source values. As the target task acquires data points, local minima and maxima are calculated. If one, or both
of these values fall within the interval that is represented by the source values xL and xR, a proximity measure
is produced to ascertain whether the domain is adapted. The proximity is based upon a membership function.
The function can take any form chosen, although within the FuzzyTL framework a Gaussian function is used.
The membership function is based on the source input domain interval.
The output from the proximity function is compared to a predeﬁned threshold. When the threshold value is
reached, the domains are adapted. The adaptation is based on the values from the target source.
• Step Three: Negative Inﬂuence Adaptation of the input domains is monitored based on its impact on the overall
fuzzy system. To ascertain the inﬂuence of this adaptation, a comparison is made between the maximum
membership of the rulebase previous to the update, to the same value following the changes. A reduction in
Medium Large
(a) Adaptation of Unevenly Distributed Sets Based on New Minimum and Maximum Input Values.
(b) Example of Internal Domain Containment.
Figure 3: Elements of Adaptive Stages 1 and 2.
value returns the system to its previous state. This allows for the system to police the adaptation, and endeavour
to move away from a state of negative transfer.
The ﬁrst two stages of the adaptation focus on the input variable domains and as a result the antecedent sets.
Data is available to produce adaptation within these domains. The unlabelled nature of the data impedes the ability
for direct adaptation of the target consequent domains. The third adaptation stage combines data produced by the
framework with new task information to approach this problem.
3.3.3. Output Domain Adaptation Through Gradient Control : Stage Three
The third adaptation process focusses on the manipulation of the consequent sets. The process uses information
from the target domain coupled with data from the framework itself. This allows for feedback from within the
adaptation framework. The process can be summarised as three steps:
• Step One: Data Gathering A predeﬁned n sized sliding window SL of data is collected from the source domain
Ds and the target domain Dt for each input variable x ∈X and output y ∈Y. The output value for the target
domain is taken from the FuzzyTL system. The source output is recorded from the labelled data provided.
Gradients are formed based on the sliding window data between the input and the output value. This provides
an understanding of the relationship at each data point. The gradients are the basis of the consequent adaptation.
• Step Two: Gradient Production Gradients are formed for each source and target input, and the source output.
Output from the FuzzyTL framework is used to produce the target output gradient. The data is normalised using
Initialise System - Source Data Ds
Extract input intervals
S (X) = [xL, xR]
Process target
domain - Dt
End of process.
Get target domain minimum
t(xL) and maximum DI
Skip comparison.
Compare the
overall membership
values of the input.
adaptation
Return domains to
previous values.
Output fuzzy sets.
Figure 4: Inner Domain Adaptation Process.
a standard score method. This is deﬁned as:
where z is the output, x is the input value, ¯x is the mean of the sliding window and σ is the standard deviation
of the sliding window.
• Step Three: Gradient Comparison Using the gradients gained across each source and target domain input and
output variable, a comparison is made at each individual input value.
• Step Four: Consequent Adaptation A mapping is made from the source input and output values, to the target
input and output values based on the gradients of the values. By mapping the source gradient to the target
gradient, diﬀerences highlight the necessity to adapt the consequent sets. Diﬀerences between the source and
target consequent gradients produce adaptations to the target consequent domain interval.
The adaptation of the consequent can be stated as:
(gsi −gti)
where ϕ is a learning parameter to weight the impact of the gradient delta. This can be user deﬁned although the
default is 0.1. gs is the gradient of the source sliding window for n inputs that can be represented as gsi...n ∈[−1, 1] ,gt
is the gradient of the target sliding window for n inputs that can be represented as gti...n ∈[−1, 1] and dDa is the delta
used to adapt the consequent sets.
The initial three stages of the adaptation process approach the alteration of the fuzzy sets. In order to absorb the
contextual changes within each implementation, the system additionally adapts the fuzzy rulebase.
3.3.4. Rule Base Modiﬁcation Via Source Rule Comparison : Stage Four
Previous stages focussed on the adaptation of the domains. Diﬀerences can also occur within the structure of the
rulebase. The knowledge of the FuzzyTL framework is held within the fuzzy sets and fuzzy rules. By altering the
rulebase, the knowledge gaps can be ﬁlled in order to apply the transferable FIS to the new context.
The rulebase is modiﬁed by examining previously pruned rules by applying the target domain data. The use of the
exhaustive rulebase is ﬁrmly in keeping with the TL ethos of the frameworks construction. Algorithm 1 expresses the
adaptation of the rules using the exhaustive transferable rulebase.
The ﬁrst stage of the process is to examine the exhaustive rulebase A to identify any rules that ﬁre using the data
from the target domain Dt. The rule that ﬁres with the highest membership value from each data point is retained
in the adaptive rulebase C. The grouped rules are compared to the reduced rulebase B based on those with the same
antecedent values. Each of the reduced rulebase rules that ﬁres is compared to the adaptive rulebase. Those rules
that have greater membership values are retained, removing the comparable rule from the adaptive rulebase. The
greater weighting indicates a greater applicability to the new domain. If the identiﬁed rule in the reduced rulebase B
is not within the adaptive rulebase C, this is added. The addition of the rules from the exhaustive rulebase assists in
supplying missing knowledge areas required by the new task.
The ﬁnal stage of the adaptation again focuses on the fuzzy rulebase.
3.3.5. Rule Adaptation Using Euclidean Distance Measure : Stage Five
Previously learnt information can provide data to partially ﬁll gaps in the knowledge to complete a new task. To
remove incompleteness, and strive to capture all of the segments where disparities lay, new information is required.
In the FuzzyTL framework, information of the new domain may only be partially represented in the fuzzy rulebase.
To reinforce the rulebase, new rules need to be constructed. As the task domain is an unlabelled dataset, this process
relies on the use of the combined learning from the newly accumulated information and the use of previous knowledge
in the form of the exhaustive rulebase. To produce antecedent and consequent fuzzy sets, separate strategies are used.
The initial stage of the process is to gain an output from each of the input variables. The process can be demonstrated using a simple example. The domain is segmented into fuzzy sets, (2N + 1). Within this example ﬁve sets
are used. These are deﬁned as { Very Low, Low, Medium, High, Very High }. The input value is applied to each set
iteratively. The set with the highest output corresponds to the antecedent set for the new rule. This can be described
x1 = {< Very Low, 0.0 >< Low, 0.0 >, < Medium, 0.35 >, < High, 0.65 >, < Very High, 0.0 >}
x2 = {< Very Low, 0.0 >< Low, 0.8 >, < Medium, 0.2 >, < High, 0.0 >, < Very High, 0.0 >}
The example shows two domains, x1 and x2 within the target task Dt. The set with the highest ﬁring strength produces
the output for the antecedent sets of the rule. If sets of jointly strong ﬁring strength are found, the initially discovered
Algorithm 1 Adaptation Algorithm: Stage Four Adaptation Using Exhaustive Rule Base.
Input Variable x ∈X
Target Domain Dt = {(xt
Exhaustive Rule Base A = { f Xa
Reduced Rule Base B = { f Xb
Adaptive Rule Base C = { f Xc
Membership Degree µ
Rule r = { f X1, f X2, fY}
for h = 1; h < M; h + +
for i = 1; i < N; i + +
⊲Combine the membership of the antecedent values from the exhaustive rule
if e > 0 then
⊲Check if the rule produces output.
for u = 0 →Q; u ←i + u
⊲Combine the membership of the antecedent values from the adaptive
rule base.
iYa(e) > µf
uYc(g) then
⊲Compare membership of A and C rules.
if C , ∅then
⊲Remove current rule.
⊲Add rule from exhaustive rulebase.
for j = 1; j < P; j + +
for k = 1; k < Q; k + +
⊲Check if set labels are the same.
⊲Combine the membership of the antecedent values from the
reduced rule base.
pYb(w) > µf
kYc(e) then
⊲Remove current rule.
⊲Add rule from reduced rulebase.
set is used. In the example shown, two antecedent sets relating to two input variables are formed from the highest
memberships of the x1 and x2 domains. In the x1 domain, the High set produces the highest membership. In x2, the
highest is the set Low. As a result the High and Low are placed into the rule.
The adaptation of the antecedent sets is based on information previously gained. The lack of consequent data
requires a diﬀerent strategy.
There is a mapping between the input values, and the consequent output from a corresponding rule. By using a
euclidean distance based on the input values from the unlabelled data, a mapping can be generated to ﬁnd the closest
corresponding consequent set.
During the formation of the exhaustive rulebase, each set is assigned a corresponding input value. Figure 5 shows
the relationship between the source and target data values. From the source data, a mapping can be produced from
the original input data values to the corresponding antecedent sets. Through the use of an n dimensional euclidean
distance, the closet target input values can output sets based on these values. In the example, the sets are represented
by the grid structure. Each square shows the relationship of the input values, and the corresponding sets. Shown
in Figure 5 are two target antecedent values, t1 and t2. These values are not represented within the current reduced
rulebase. By mapping them to source input values, the antecedent sets can be found. Using the smallest euclidean
value, Figure 5 shows that t1 can be mapped to s1 via the distance d1. Equally, t2 can be mapped to s2 via the distance
d4. Through this procedure, a combination of antecedent sets is formed. Using these sets as a comparative value, a
consequent set can be extracted from the exhaustive rulebase. By extracting the consequent set in this manner, a new
rule is formed that draws knowledge from the source dataset.
Very Small
Very Large
Very Small
Very Large
Time (Hrs:Mins)
Light (Lms)
b Source Input Values
Target Input Values
Figure 5: Example of the Gaining of Antecedent Sets to Map Consequent Sets Using Euclidean Distance.
4. Application of Fuzzy Transfer Learning to Intelligent Environments
To illustrate the application of the Fuzzy Transfer Learning methodology, the FuzzyTL was applied to a complex,
uncertain and dynamic environment. IEs embody this type of domain. The nature of Intelligent Environment (IE)
applications have produced a number of sparse data problems . Remote locations such as environmental monitoring
 , ad hoc structures within disaster recovery scenarios and highly speciﬁc user groups, for example disabled
individuals, reduce, or in certain circumstances eliminate, the quantity of data that is available to produce a model.
In this section, the FuzzyTL methodology is shown to be able to predict output from IEs using diﬀering contextual
information.
4.1. Experimental Design
The motivation for the FuzzyTL framework is to address the issue of environments where little or no knowledge
is known a-priori, though there is a need to produce a prediction or classify the target data. Torrey and Shavlik 
suggest three metrics that can be used to measure the performance of a TL system. These are deﬁned as:
1. The initial performance achievable in the target task using only the transferred knowledge, before any further
learning is done, compared to the initial performance of an ignorant agent.
2. The amount of time it takes to fully learn the target task given the transferred knowledge compared to the
amount of time to learn it from scratch.
3. The ﬁnal performance level achievable in the target task compared to the ﬁnal level without transfer.
These metrics are closely associated with the construction of learning processes, and the composition and quantity
of the available data. The metrics proposed by Torrey and Shavlik require the TL structure to be IS, where information
is available from both the source and target domains. IU transfer learning, as used in the FuzzyTL framework, makes
the comparison of ignorant and informed agents not possible. To compare the performance of a starting ignorant
agent is not feasible, as there is no information to model the agent upon. To learn an agent from scratch also requires
a level of known data. This research approaches a problem where there is no labelled data within the target domain,
and initially little or no unlabelled data. For this reason, the second metric proposed by Torrey and Shavlik is not
applicable. On this basis, a diﬀerent set of metrics are used, however, they are broadly based on the those proposed in
The metrics use a comparison of the output of FuzzyTL framework, against actual known sensor readings from
the IEs. Input values are given to the system producing a predictive value. This is compared to actual recorded sensor
readings. The error indicates the accuracy of the FuzzyTL system. To evaluate the use of the FuzzyTL framework,
two real world IE datasets were chosen to demonstrate the applicability of the spatial and temporal contextual transfer
process. The ﬁrst dataset was taken from a publicly available source .
4.1.1. Intel Berkeley Research Laboratory Dataset
The Intel dataset was based upon information collected from 54 sensors deployed in the Intel Berkeley Research
Laboratory (hereby referred to as the Intel Laboratory) between the 25th February and the 5th April, 2004. The
network used XBow Mica2dot weatherboard based nodes to record environmental data across the internal structure
of the laboratory. Four parameters were measured: time-stamped temperature (in degrees Celsius), humidity ranging
from 0-100%, light (measured in Lux), and residual power of each sensor unit expressed in Volts. The data was
collected using the TinyDB in-network query processing system built onto the TinyOS platform which recorded
information every 31 seconds .
Additional to the sensor readings, the Intel Laboratory provides the x, y co-ordinates of the sensor locations. These
values were used in the experiments to construct a context measure. A section of network was identiﬁed to examine
the inﬂuence of variations in the spatial aspect of the contexts. Data was also taken from a section of the dataset that
related to a speciﬁc time period. This allowed for the investigation of temporal changes in the context. To achieve both
of these, the output of Sensors 7, 9, 12 , 24 , 34 , 42 and 51 where examined across seven days from 28th February to
5th March, 2004 including the 29th February.
A quantity of preprocessing was undertaken to be able to place the dataset into the FuzzyTL framework. Each
sensor was isolated based on its moteid. The unused variables were also removed from the data resulting in only the
time, light and temperature remaining. The time variable was converted to seconds to allow for ease of processing.
The millisecond component was removed allowing for this process. A number of diﬀerent experimental set ups were
used to illustrate the eﬀect of a greater and lesser availability of labelled data in the source domain.
4.2. De Montfort University Robotics Laboratory Dataset
The second dataset is based on a sensor network constructed in the Robotics Laboratory of the Centre for Computational Intelligence of De Montfort University, United Kingdom. Again the sensor network is focussed on the
monitoring of environmental conditions. The De Montfort University Robotics Laboratory Sensor Network (here
after referred to as the Robotics Laboratory) was composed of nine sensors in total (Sn1 - Sn9). Sensors 3, 5 and 6
(represented as Sn3, Sn5 and Sn6) are composed of Phidget light and temperature sensors. Sensors 1, 2 and 4 (Sn1,
Sn2 and Sn4) are single temperature sensors. Five days of data were collected between the 15th and 19th October,
2011. Three of the sensors were used in order to apply the input variables of time and light, and an output variable
of temperature. These sensors were Sn3, Sn5 and Sn6. All superﬂuous information was removed from the raw data
leaving each sensor, and the day of the week isolated.
The construction of the framework to process both the Intel Laboratory and Robotics Laboratory datasets were
constructed and run using C++ via Code:Blocks (Version 8.02) and compiled through GNU GCC on Ubuntu LTS
Version 10.04.
4.2.1. Experiment Structure
To understand the structure of the methodology and its capacity, two hypotheses were proposed and tested.
Hypothesis 1: Where minimal unlabelled data is available within a target task, data in the form of a TL process from
contextually related but diﬀering source tasks, can be used to learn predictive tasks.
Hypothesis 2: Adaptation of the transferred source domain through the use of unlabelled new data can increase the
performance FuzzyTL in predicting target tasks.
To address these hypotheses, three broad main experimental groups were carried out: performance, context impact and adaptation. Hypothesis 1 was evaluated primarily through the use of the performance and context impact
experiments. Hypothesis 2 was evaluated using the adaptation experiments.
Deﬁnitions. This section will deﬁne a number of elements used through out the remainder of this section. To calculate the diﬀerence between the predicted value produced by the FuzzyTL framework and the actual values that are
observed, a Root Mean Squared Error (RMSE) is used. The RMSE takes the errors between each of the points in the
dataset, and aggregates them into a single measure. RMSE can be deﬁned as
were n is the number of data points in the dataset, x1 is the observed dataset and x2 is the predicted value.
To understand the impact of the contextual change, a metric was produced to measure the temporal-spatial difference between context structure. A normalised euclidean distance was used. Three separate inputs were given, the
speciﬁcation of the sensor location using the x and y co-ordinate, and a date measurement. This is constructed as
((a1 −a2) −sup A)
inf A −sup A
+ ((b1 −b2) −sup B)
inf B −sup B
+ ((c1 −c2) −supC)
inf C −supC
were a is the x and b is the y co-ordinate, c is the date, and CD is the Context Distance (CD). The supremum and
inﬁmum are calculated to produce a normalised distance. This allows for diﬀerent values to be used. A and B deﬁne
the spatial inputs of the context. C is the temporal input. The context can be composed of n inputs. The context used
in these experiments consists of spatial and temporal elements. Diﬀerent contexts may contain diﬀerent quantities of
variables.
Context has been deﬁned previously in Section 2.1. Based on this criteria, two further sub-types of context are
used, inter and intra. An inter context can be deﬁned internally within an environment such as section of network or a
room in an IE. An intra context is deﬁned as encapsulating the inter context such as building, time scale or composition
of these. Within this section, an inter contextual comparison is composed within the individual locations of the sensor
networks. Each context is then deﬁned by time and location of the sensors within this location. This falls in line with
the third criteria of a context. An intra contextual comparison uses an abstract deﬁnition to compare the contexts.
Unlike the inter comparison, a categorical deﬁnition is given. These context deﬁnitions are used for comparison.
Each comparison takes the form of assessing the performance of the FuzzyTL framework based on diﬀerences in the
individually deﬁned contexts. The diﬀerence is calculated based on the deﬁned variables that constituent the context.
This allows for contexts with behavioural diﬀerences to be compared.
Within the following sections, further explanation of the construction of each individual experiment will be given,
with the results gained.
4.2.2. Performance
To measure the performance of the FuzzyTL framework, two datasets were used. Across both datasets, a system
was constructed using two input variables (time and light), and a single output variable, temperature. For the performance measure, inter contextual comparisons were produced. The initial value output of the system was based on
zero prior unlabelled data. Each data point from the dataset was fed into the system to simulate real time operation. To
assess the performance of the FuzzyTL framework the predicted value at each data point was compared to the actual
observed output from the dataset. Any error produced was consolidated into a single value using the RMSE process.
To produce a benchmark to compare the system against, each of the datasets were processed using the adapted
Fuzzy Frequency WM system . The learning process was, however, altered. The source data was supplied from
the target domain. This produced an output that, unlike the FuzzyTL, has labelled knowledge of the target learning
task. This allows a comparison to be made. The FuzzyTL framework that was supplied target data perceived to
produce an output closest to the actual sensor value. A comparison was made to demonstrate the context of the output
from the contextually diﬀerent source FuzzyTL framework.
Intel Laboratory Data Comparison to Observed Values. For the Intel Laboratory dataset, individual contexts were
formed using each of the sensors in the spatial grouping (Sensors 7, 9, 12 , 24 , 34 , 42 and 51), and for each day
between 28th February to 5th March, 2004. Extraneous source and target contexts were removed from the experiment.
A single context is formed from a single day and a single sensor. A value was predicted for a single sensor taken from
within the group, across the deﬁned time period. This produced 2352 diﬀering context comparisons. In the event that
the system is unable to predict a reading, a predeﬁned value of −1 is given.
To assess the performance of the FuzzyTL framework, source contexts were compared to the benchmark values.
The adapted WM method produced 49 separate RMSE values (seven sensors × seven days). These related to each
sensor (7, 9, 12 , 24 , 34 , 42 and 51), and each day within the speciﬁed time interval . 2352 contexts were produced for the Intel Laboratory comparison. These
were composed of the 49 separate contexts from the source data (seven sensors × seven days) and the target data
(seven sensors × seven days) with 49 contexts removed where the source and target context matched. From the 2352
RMSE values calculated from the FuzzyTL framework, the lowest RMSE value was taken for each context. This
produced 49 contexts to compare to the benchmark set. By isolating the lowest RMSE values, this equated to the best
performing contexts. As the benchmark dataset represents the optimum data conditions for the learning process, the
best performing contexts were chosen to compare against them. The focus of this process is to establish whether the
FuzzyTL framework can ﬁrstly output a predictive value, and to then contextualise the performance.
The data for the experiment was composed of two sample population datasets, the benchmark and Intel Laboratory
datasets. To compare the benchmark of the Intel Laboratory Data and the output of the FuzzyTL, the datasets was
normalised using a power transform, log6(x).
To compare the two sample populations, a paired t-test was used. The paired t-test produced an value of 0.0015.
As this value was below the α of 0.05 it was concluded that the benchmark and FuzzyTL datasets were from nonidentical populations. Looking closer at the data, 37 contexts were highlighted as producing a negative diﬀerence.
This showed that the best performing FuzzyTL output was greater in 37 contexts than the benchmark. However, 12 of
the contexts, 24.4898%, produced a lower RMSE than the benchmark. In these cases the FuzzyTL system was able
to use contextually diﬀerent source data to produce better performing output than the benchmark dataset.
Figure 6 shows a single context comparison where the best FuzzyTL output out performed the benchmark dataset.
This is the source data 24, 28th February, 2004, and the target data Sensor 34, 28th February, 2004.
Time (Secs)
Temperature ◦C
Figure 6: Comparison of Benchmark and Best FuzzyTL to Sensor Readings Target Data Sensor 34, 28th February,
The ability for the framework to adapt the sets according to new data improved the output beyond the benchmark.
The ﬁnal values of the input domains were in close proximity to the target sensor as is expected. The output domain
had an increased diﬀerence. The left temperature interval value (tmL) diﬀered by 0.18◦C, and the right temperature
interval value diﬀered by 0.86◦C. As the system is dynamic, this is absorbed within the process.
Of the 2352 contexts, 66.1990% (1557 out of 2352) produced a RMSE that was equal, or within the minimum
and maximum interval of the benchmark dataset. This indicates that the FuzzyTL was able to use diﬀering contextual
source data to produce comparable predictive output. The lowest of those contexts produced a RMSE of 0.5139◦C.
The data used was source data from sensor 42 on the 3rd March, 2004 and target data from the sensor 7 on the 2nd
March, 2004. In comparison, the highest error produced was a RMSE of 10.5014◦C. The source data was provided by
sensor 7 on the 4th March, 2004, and the target data by sensor 24 on the 2nd March, 2004. The returned RMSE can
be attributed to the nature of the target data. The diﬀerent structure of the interval domains of the source and target
consequent sets, produced a variation in the output compared to the actual value.
The highest source consequent had a left limit value ys1
L of 17.2640◦C. The right limit value ys1
R was 36.1584◦C.
The target interval was an intersection of this interval. The left target limit value yt
L was 14.3044◦C and the right target
limit was 21.2624◦C. In contrast, the lowest error rate had a closer consequent domain interval. The source had a left
L of 16.9308◦C and a right limit ys2
R of 31.5034◦C. The target had a left limit of 17.8520◦C and a right limit
of 24.8100◦C. The knowledge in the consequent domain is dependent on the source task. The closer the source and
target consequent domain intervals, the smaller the RMSE that is produced.
Robotics Laboratory Data Comparison to Observed Values. A similar analysis was undertaken using the Robotics
Laboratory dataset. The adapted WM methodology produced 12 RMSE values (four days × three sensors) based on
the sensors in the Robotics laboratory and across the deﬁned number of days. These values were compared to the
lowest, and so best, output produced by the FuzzyTL framework using diﬀerent contextual data.
As with the Intel dataset, a paired t-test was carried out between the Robotics Laboratory dataset and the output
of the FuzzyTL. The p-value that was produced was lower than the deﬁned α signiﬁcance value of 0.05. This rejects
the null hypothesis that the Robotics Laboratory benchmark dataset is the same distribution as the best output of the
FuzzyTL framework. Further analysis showed that all contexts produced a higher RMSE value than the benchmark.
Focussing closer on the data showed that individual source contexts produced RMSE values that were comparable
to the benchmark and similar to the actual sensor output. Overall, the lowest RMSE value produced was using source
data from sensor 1 on the 18th October 2011, to predict values for sensor 1 on 17th October 2011. The RMSE value
for the context was 0.1862◦C. The benchmark produced RMSE values in the range of a RMSE of 0.0987 to 0.2212◦C.
The RMSE for this context is 0.0987◦C. The Robotics laboratory produces a more consistent temperature than found
within readings recorded in the Intel dataset. The variation across the context shown was 0.67◦C. The FuzzyTL
framework replicates the narrow variation producing a value of 0.55◦C. The initial value calculated, based on zero
data, produced an error of only 0.11◦C.
The highest RMSE produced was a value of 3.3447, using source data from sensor 2 on the 16th October, 2011,
to predict values for sensor 1 on 17th October, 2011. The variation across both the output of the sensor and predictive
value is again low, 0.89 and 0.83◦C respectively. The impact of the size of the output interval will be discussed in the
following section.
5. Adaptation
The FuzzyTL framework is grounded on the transfer and adaptation of information. To investigate the performance
gain through the use of the adaptation process, a comparison was made between a non-adapted system, and the full
FuzzyTL framework.
5.1. Comparison of Non-Adaptive FuzzyTL to Full FuzzyTL Framework: Intel Laboratory Data
The non-adaptive system was composed of a transferred FIS. The learning processes involved in forming the
structure of the fuzzy system remained unchanged to those previously used. Supplementary online adaptation and
learning was removed providing a base to compare to. The ﬁrst experiment was based on the Intel laboratory dataset.
The structure outlined in Section 4.1.1 was used to form the basis for the comparison. Using a similar performance
metric, each of the diﬀerent contexts were compared to the sensor readings. The non-adaptive RMSE values were
subsequently compared to the values previously gained from using the full FuzzyTL system. A total of 2352 contexts
were used for comparison.
The previous experiments focussed on samples of the datasets. This experiment used the whole set of both nonadapted and adapted data. To compare the two datasets, a Wilcoxon signed-rank test was chosen. The p-value for the
Wilcoxon signed-rank test was lower than the deﬁned alpha value of 0.05. This indicated that the two datasets were
from diﬀerent distributions. Further analysis showed that 87.6700% of contexts exhibited a decrease
in RMSE when the adaptation was applied.
Isolating a single context for comparison, Figure 7 shows the comparison of the non-adaptive and full FuzzyTL
systems using source data from sensor 7 on 2nd March, 2004, and target data from sensor 24 on 2nd March, 2004.
This illustrates the non-adaptive systems inability to cope with the initial prediction, producing a -1 value. This is due
to the nature of the input, and output domain intervals. Issues arise within the non-adaptive system as the input values
fall outside of the light interval domain. The minimum of the target light interval (lts
L ) sits outside of the non-adapted
light interval (lna
L . The lts
L = 97.52 and lna
L = 158.23. The results of this are shown in Figure 7 between 51 to
27260 seconds (00:00:51 - 07:34:20). The non-adaptive system can not produce an output based on these inputs.
Taking on board the transferred information, and the new data, the FuzzyTL framework adapts the all interval
domains. Driven by the inputs from both the target and source data, the output interval domain is shown, in this
example, to move toward the parameters of the sensor interval. The minimum and the maximum FuzzyTL light
values (l ftl
L and l ftl
R ) are equal actual sensor. The output values, are are additionally closer to the actual sensor interval
minimum and maximum values, than the non-adapted system. Overall, the incorporation of the adaptation stages into
the FuzzyTL improved the predictive capability when used with the Intel Laboratory dataset.
5.2. Comparison of Non-Adaptive FuzzyTL to Full FuzzyTL Framework:
Robotics Laboratory Data
In a similar approach to the Intel laboratory dataset, a comparison was made between a non-adaptive and full
FuzzyTL system using the Robotics laboratory data. The non-adaptive system was based on the FuzzyTL framework
with the removal of the adaptive stages.
Time (Secs)
Temperature ◦C
Non-Adapted
Figure 7: Comparison of Non-Adapted System, FuzzyTL and Sensor Source Data Sensor 7, 2nd March, 2004 and
Target Data Sensor 24, 2nd March, 2004 based on the Intel Laboratory Dataset.
To compare the data, a Wilcoxon signed-rank test was used. The p-value calculated was lower than the alpha value
of 0.05. Taking the null hypothesis stated that both datasets come from the same distribution, this can be rejected.
The diﬀerences between the datasets showed that 82.5757% (109 of 132) of the contexts produced a lower RMSE
when the adaptive stages were applied. Of those, the greatest reduction was an RMSE of 14.2349◦C. By comparison,
of the 17.4242% that produced a higher RMSE, the greatest increase was 1.1318◦C. These ﬁndings substantiate the
previous conclusions drawn from the Intel Laboratory dataset.
Drilling down into the contexts, Figure 8 shows an example of non-adapted system output compared to the actual
sensor reading. This ﬁgure shows how, in certain conditions, the non-adapted system is unable to output a value for a
portion of the target (Within Figure 8, the missing output is not displayed).
As the light input value steps outside of the domain, the system fails to produce an output. The transferred light
domain is lL = −0.01 and lR = 811.01. At 36842 seconds (10:14:02) the input light level reaches 863, beyond the
light domain interval. This causes the system to fail to output a value.
5.3. Comparison to Other Regression Methods
To demonstrate the eﬀective nature of FuzzyTL, two other regression based methods were chosen to compare
the approach to. The authors chose to compare the FuzzyTL method to an implementation of K-Nearest Neighbour
(KNN) and Support Vector Regression (SVR). These methods were chosen as they come from diﬀering areas of
machine learning which are mature and established methodologies. Each method was applied to the Intel dataset as
deﬁned in Section 4.1.1. The implementations of both KNN and SVR used in this paper are taken from Pedregosa et
al’s [? ] Python based data mining and analysis tool set, Scikit-learn.
5.3.1. Implementation of K-Nearest Neighbour
The use of the KNN method for regression has been applied to a number of applications from predicting log
volumes [? ] to uses with large data sources in astronomy [? ].
KNN implements learning based on the k nearest neighbours of each query point, where k is an integer value
speciﬁed by the user. The target is predicted by local interpolation through the use of the targets associated with
Time (Secs)
Temperature ◦C
Non-Adapted
Figure 8: Comparison of Non-Adapted System, FuzzyTL and Sensor Source Data Sensor 2, 16th October, 2011 and
Target Data Sensor 3, 17th October, 2004 based on the Robotics Laboratory Dataset.
the nearest neighbour in the training set. In this case, a value of ﬁve was speciﬁed. For standard KNN, each point
in the local neighbourhood contributes equally to the classiﬁcation of the query point. The implementation used
in this paper, however, alternatively weighted the points such that nearby points contribute more to the regression
than faraway points. This gives greater emphasis to the data closer to the current information. The Scikit-learn
implementation achieves this through weights proportional to the inverse of the distance from the query point [? ].
The sparse nature of the data forced the use of a brute-force search to locate the neighbours in the training data.
Although ineﬃcient in large datasets, the nature of the training data lends itself to this approach.
5.3.2. Implementation of Support Vector Regression
The use of Support Vector Machine (SVM) and additionally the use of SVR has become increasingly wide spread.
They have successfully been applied to many applications that require classiﬁcation, for example text classiﬁcation
[? ], gene selection for Cancer classiﬁcation [? ] and gesture detection [? ]. The derivative of SVM, SVR, has been
equally applied to real-world problems. SVR has been applied to applications ranging from face detection [? ] and
ﬁnancial forecasting [? ], to aﬀective computing [? ].
SVM is a method that was developed at AT&T Bell laboratories by Vapnik and a number of co-workers during the
1990’s . In 1996 Drucker et al. [? ] introduced a regression
extension of the SVM. In simple terms, SVR looks to ﬁnd a function, f(x), with at most ǫ-deviation from the target y
based on training data (x1, y1) . . . , (xl, yl) ∈X where X denotes the space of the input patterns. The vector is required
to be as ﬂat as possible. Errors are accepted as long as they are less than ǫ but no deviation can be greater. A detailed
examination and explanation of the methodology of SVR is beyond the scope of this paper. For a greater insight into
SVR, see [? ].
A number of parameters were set to deﬁne the SVR algorithm within the Scikit-learn implementation. There is a
large body of research into the tuning of these hyperparameters . A discussion of tuning these parameters, is again
beyond the scope of this research. As a result a generalised approach was taken to the parameter structure. A C value
of 1.0 was set as the penalty of the error term. A value of 0.1 was speciﬁed for the epsilon-tube. The kernel function
was deﬁned as a Guassian Radial Basis Function. This is a popular kernel method for SVR implementations.
Other parameters used the default settings.
5.3.3. Results
The comparisons to both the KNN and SVR methodologies used the same dataset as discussed in Section 4.2.2.
2352 comparisons were made using the Intel Laboratory dataset. The results illustrated that the FuzzyTL produced a
lower RMSE value in 54.5493% (1283 out of 2352) of the comparisons made with the KNN method and 67.8996%
(1597 out of 2352) compared to the SVR approach. Looking closer at the results, it can be seen that in a high proportion, the performance of the FuzzyTL approach was near comparable to both the KNN and SVR. In an additional
26.4856% of the comparisons for the KNN method and 11.1630% in the case of SVR, the FuzzyTL methodology
was within 0.5% RMSE. This highlights that even in those contexts that the FuzzyTL framework produced a greater
overall error, the margin was comparable to the performance of the other methods.
6. Conclusion
The following sections discuss the ﬁndings of the experiments and oﬀer some discussion surrounding the key
6.1. Contextually Diﬀering Environments Can Act as Source Information
A main focus of this paper was to understand whether the FuzzyTL framework could produce a predictive output
based on little knowledge of the target domain. To asses the frameworks output, a comparison was made against
two benchmark datasets using a RMSE value. The Intel Laboratory dataset comparison showed that in 24.4898% of
cases, the FuzzyTL produced a lower RMSE value than the benchmark. Additionally, of the Intel Laboratory contexts
analysed (2352 in total), 66.1990% contextually diﬀerent source datasets produced an RMSE output that was equal,
or within the minimum or maximum interval of the benchmark dataset. The analysis highlighted that the FuzzyTL
framework was able to produce output that matched or surpassed the deﬁned benchmark.
Overall, the Robotics Laboratory dataset substantiated the Intel Laboratory ﬁndings. Again a paired t-test was
carried out to compare the best FuzzyTL output to the benchmark results. Unlike the Intel Laboratory comparison, all
of the contexts studied produced a higher RMSE when using contextually diﬀerent source data. Drilling down into
the data, however, showed that individual contextual instances produced RMSE values that were comparable to the
benchmark.
The FuzzyTL was shown to be able to output predictive values using contextually diﬀerent source data. Output
from the FuzzyTL framework was comparable to a benchmark formed using target information. Although stronger
within the Intel Laboratory dataset, the FuzzyTL framework was shown to produce predictive output across two
diﬀering real-world datasets.
6.2. Online Adaptation Decreases the Error of the FuzzyTL Output
To investigate the impact of the adaptation process, and test the second hypothesis, a series of experiments based
on a non-adapted version of the FuzzyTL framework were used. The Non-Adaptive (N-A) framework was constructed
from a transferred FIS.
A comparison was made between the performance of the N-A framework and the full FuzzyTL framework.
The output of each context was assessed against the actual sensor readings. Of the 2352 contexts compared, 2062
(87.6700%) produced a lower RMSE when the adaptive stages were incorporated. This clearly demonstrated that
the introduction of the adaptive stages decreased the error produced. Failures in the N-A framework produced high
RMSE values. The strict structure of the N-A framework resulted in failures. Target values that were beyond the
source domain failed to produce an output. The adaptive process allowed the FuzzyTL to alter the domains to the
target data, producing an output. In a minority of speciﬁc cases, the N-A frameworks out performed the full FuzzyTL
framework. These special cases required the source and target interval domains to be in close proximity. Additionally,
the target input domains were required to be proper subsets of the source input domains.
The same comparison was carried out on the Robotics Laboratory dataset. Of the diﬀerences deﬁned, 82.5757%
(109 of the 132 contexts) produced a lower RMSE when the adaptive process was applied. Across the contexts, the
largest reduction was 14.2349◦C.
6.3. FuzzyTL can outperform other regression based algorithms using contextually diﬀerent sparse data
A comparison was made between the FuzzyTL framework and two mature algorithms, KNN and SVR. 2352
contextually diﬀering comparisons were made using the Intel Laboratory dataset. Predictive values were produced
from each diﬀerent methodology and compared to the actual output from the real-world data. When compared to
the SVR approach, the FuzzyTL framework produced a lower RMSE in 1597 out of 2352 (67.8996%) contexts. The
same comparison process was used between FuzzyTL and a KNN implementation. Of the 2352 context, the FuzzyTL
framework produced a lower RMSE in 1283 (54.5493%).
Further analysis showed that the FuzzyTL framework was able to outperform the other methodologies in contexts
were signiﬁcant adaptation was required.
7. Future Work
A number of areas of the Fuzzy Transfer Learning can be expanded.
7.1. Comparison of Wang-Mendel Method to Other Rule Generation Methodologies
Within the FuzzyTL framework, a WM methodology was used to extract a FIS using numerical data. This research
carried out a preliminary comparison of FIS production methods. Further methods exist outside of this study, though
the comparison of these was outside the scope of this paper. An in depth comparison of extraction methodologies
would highlight other applicable methodologies. This may allow for the further extension of the FuzzyTL framework.
7.2. Automation of Set Extraction From Data
The current FuzzyTL framework uses a simple system to deﬁne the number of sets that are used within the FIS.
This is based on a preassigned value. Previous research has been carried out into the extraction of fuzzy sets from
labelled data. Of interest is the application of such methods to automate the process of assigning set quantities. The
removal of the need to use expert knowledge to assign set quantities would further automate the framework. This area
of research would extend the data driven nature of the framework structure.
7.3. Use of Multiple Context Data
The composition of the source data has been shown to have a direct eﬀect on the outcome of the predictive value
of the FuzzyTL framework. The proximity of the source and target domains can have a direct eﬀect on the error
produced. Certain conditions of a source domain can have adverse aﬀects on the learning of the target domain.
Anomalous or erroneous data can produce a model that is incorrect. To tackle this issue, research has been conducted
into the use of multiple source domains within TL . The use of multiple sources can increase the chance
of discovering a source domain that is close to the target. The extension of the FuzzyTL framework to incorporate
multiple source data may reduce the impact of negative transfer.