DeePCG: constructing coarse-grained models via deep neural networks
Linfeng Zhang and Jiequn Han
Program in Applied and Computational Mathematics,
Princeton University, Princeton, NJ 08544, USA
Institute of Applied Physics and Computational Mathematics,
Fenghao East Road 2, Beijing 100094, P.R. China and
CAEP Software Center for High Performance Numerical Simulation, Huayuan Road 6, Beijing 100088, P.R. China
Roberto Car
Department of Chemistry, Department of Physics,
Program in Applied and Computational Mathematics,
Princeton Institute for the Science and Technology of Materials,
Princeton University, Princeton, NJ 08544, USA
Department of Mathematics and Program in Applied and Computational Mathematics,
Princeton University, Princeton, NJ 08544, USA and
Beijing Institute of Big Data Research, Beijing, 100871, P.R. China
We introduce a general framework for constructing coarse-grained potential models without ad
hoc approximations such as limiting the potential to two- and/or three-body contributions. The
scheme, called Deep Coarse-Grained Potential (abbreviated DeePCG), exploits a carefully crafted
neural network to construct a many-body coarse-grained potential. The network is trained with
full atomistic data in a way that preserves the natural symmetries of the system. The resulting
model is very accurate and can be used to sample the conﬁgurations of the coarse-grained variables
in a much faster way than with the original atomistic model. As an application we consider liquid
water and use the oxygen coordinates as the coarse-grained variables, starting from a full atomistic
simulation of this system at the ab initio molecular dynamics level. We ﬁnd that the two-body,
three-body, and higher-order oxygen correlation functions produced by the coarse-grained and full
atomistic models agree very well with each other, illustrating the eﬀectiveness of the DeePCG model
on a rather challenging task.
INTRODUCTION
In molecular dynamics (MD), we are often faced with
two types of coarse-graining tasks. In a ﬁrst set of applications we are interested in evaluating the Landau free
energy, which is a function of a small subset of coarsegrained (CG) variables. In this case the CG variables
are either scalar or low dimensional vector variables. In
a second set of applications we are interested in sampling
with molecular dynamics (MD) or with Monte Carlo the
conﬁgurations of an extensive set of CG variables. In this
case the dimensionality of the CG space is proportional
to the size of the system but is reduced relative to the full
space of atomistic coordinates. The ﬁrst type of CG variables is typically adopted to study problems like phase
transitions, where the objective is to perform detailed
analyses of the Landau free energy surface by ﬁnding the
metastable states, the free energy barriers between these
∗Electronic address: wang 
†Electronic address: 
states, the transition pathways, etc. Take the melting
of a solid as an example, the Steinhardt order parameters have been used as CG variables to diﬀerentiate
solid (crystal) and liquid phases. The second type of CG
variables is typically used to accelerate conﬁgurational
sampling relative to full atomistic simulations. For example, one may coarse-grain a polymer by replacing the
monomers with point-like particles, or beads, connected
by springs.
For a good description of the Landau free energy
surface one needs to ﬁnd good order parameters acting as CG variables and address the issues associated
with crossing high energy barriers. Typically these approaches are limited to a few CG variables, but recent
work demonstrated that machine learning methods allow us to describe the functional dependence of the Landau free energy surface on several CG variables .
When considering extensive CG variables, the diﬃculty
is often associated with ﬁnding an accurate free energy
function in the space of the CG variables. Such free energy function usually depends on the CG variables in
a complex and nonlinear way.
Therefore, ﬁnding a
good representation of this function often requires sub-
stantial physical/chemical intuition
In principle, machine learning methods can address this problem
more accurately and in an automated way , but
most machine learning approaches so far have focused
on the representation of the potential energy surface in
the space of the atomistic degrees of freedom rather than
the representation of the free energy surface in the space
of the CG variables. For example, the Deep Potential
method , a model based on deep neural networks, has
made it possible to parametrize an atomistic potential
energy function derived from quantum mechanics without ad hoc approximations. A subsequent development
of this approach, called Deep Potential Molecular Dynamics (DeePMD) , has allowed us to perform MD
simulations of comparable quality to ab initio molecular
dynamics (AIMD) at the cost of classical empirical
force ﬁelds.
The free energy surface, rather than the potential energy surface, is the key physical quantity that we need
to represent when dealing with CG variables.
work, we introduce the Deep Coarse-Grained Potential
(DeePCG) scheme, an approach that generalizes the
Deep Potential and DeePMD methods to representations
of the free energy surface in the space of the CG variables, a quantity that will be called the CG potential in
the following. A related method to represent the manybody character of the CG potential in molecules was recently reported in Ref. .
In our approach, similar
to the Deep Potential and DeePMD methods, no ad hoc
approximations are required, in addition to the network
model itself, to represent the CG potential. The scheme
is very accurate as demonstrated by the almost perfect
agreement of the many-body correlations extracted from
CG simulations with the corresponding correlations extracted from the original atomistic model. In the present
work, we use liquid water as an example to illustrate the
approach. We choose AIMD as the underlying atomistic
model, and replace the individual water molecules with
point-like particles located at the oxygen sites in the CG
model. The excellent agreement of the second-, third-,
and higher-order correlation functions between CG and
atomistic models shows the promise of the DeePCG approach.
METHODOLOGY
Basic Theory
We consider a d-dimensional system with N atoms in
the constant-volume canonical (NV T ) ensemble.
coordinates of the atoms, in the laboratory frame, are
q = {q1, q2, . . . , qdN} ∈RdN. The conﬁgurational distribution function is deﬁned by
Z e−βV (q),
e−βV (q) dq is the partition function.
coarse-grained
{ξ1(q), ξ2(q), . . . , ξM(q)} are a reduced set of coordinates (M < dN). M can be ﬁnite and independent of
the system size or it can be extensive with the system
size, as in the two cases discussed in the introduction.
When M is ﬁnite the CG variables are the so-called
order parameters of the system. When M is extensive,
the CG variables replace molecular objects with simpler
sub-objects.
The conﬁgurational distribution of the
CG system is the projection of the conﬁgurational
distribution of the microscopic (atomistic) system onto
the space of the CG variables:
e−βV (q)δ(ξ(q) −ξ) dq.
The probability distribution in Eq. (2) allows us to de-
ﬁne the CG potential and the forces acting on the CG
variables as:
β ln p(ξ),
F (ξ) = −∇ξU(ξ),
respectively. Eq. (3) tells us that a good CG potential
should reproduce accurately the full conﬁgurational distribution of the CG variables in the atomistic model.
Testing the quality of the full conﬁgurational distribution of the CG variables is diﬃcult, and, typically tests
have been based only on two- and three-body correlation
functions .
U(ξ) is uniquely speciﬁed by the underlying atomistic
model and the deﬁnition of the CG degrees of freedoms.
Therefore, constructing a CG model involves two steps:
(1) the choice of an appropriate CG-potential representation, and (2) the optimization of the parameters that
deﬁne the potential representation. The way in which
these two issues are addressed diﬀerentiates alternative
We notice that, even if we knew exactly U(ξ), we would
not have a closed deterministic form for the equation of
motion of the CG variables due to the dN −M missing
degrees of freedom in the CG potential. The issue of the
dynamics of the CG variables has been addressed in the
literature. See, e.g., Refs. . Further assumptions,
like a time-scale separation between the CG variables and
the remaining degrees of freedom, are usually required to
recover dynamical information of the atomistic model. In
the following we shall focus on the accurate construction
of the CG potential. We will leave to future studies the
investigation of CG dynamics.
CG potential representation
We adopt a neural network representation U w(ξ) for
the CG potential U(ξ). Here w are the parameters to
be optimized by the training process. U w(ξ) should be
constructed using in input only the generalized coordinates ξ, without any human intervention in the optimization process.
The U w(ξ) constructed in this way
should preserve the symmetry properties of U(ξ). In this
manuscript we limit ourselves to considering CG objects
that behave as point particles and have only positional
dependence. In this case, the ξ variables are the coordinates of the CG particles. More general choices of the
CG objects have been suggested in the literature 
when dealing with, e.g., polymers, biological molecules,
or colloidal particles. In these cases it may be useful to
consider rods, ellipsoids, particles connected by springs,
etc., as the CG objects.
In principle, all these cases
could be treated with the present formalism. In the setup
adopted here of point-like CG objects, the CG potential
U w is extensive, intrinsically many-body, and should preserve the translational and rotational invariance, as well
as the permutational symmetry of the CG objects.
All the properties of the CG potential described above
are preserved by the Deep Potential model . To illustrate how it works, we use the example of liquid water.
We write the CG potential as a sum of the local contributions of the CG particles, i.e., U w(ξ) = P
i (ξ), the potential contribution of the CG particle i,
is constructed in two steps. First, the coordinates of the
CG particle i and its neighbors within a cut-oﬀradius Rc
are transformed into the descriptors {Dij} of the local
environment of the CG particle i. We call this procedure local frame transformation and refer to Fig. 1 for
more details. In the following we use the symbol Di to
denote the entire set of descriptors for atom i. Next, as
illustrated in Fig. 2, the descriptors Di are given in input
to a fully connected feedforward neural network to compute the potential contribution of the CG particle i. The
mathematical formulation of the network structure is also
presented in Fig. 2, where the operation of each layer of
the network corresponds to a linear mapping of the output from the previous layer combined with a nonlinear
mapping. The translational and rotational symmetries
are preserved by the local frame transformation.
permutational symmetry is preserved because: (a) for
each CG particle i, its descriptors Dij are sorted in ascending order according to the inverse distances between
particles i and j; (b) the subnetworks associated with
the same type of particles share the same parameters w;
(c) U w(ξ) = P
i (ξ) is an additive relationship. More
details on the Deep Potential method can be found in
Refs. . Due to the adoption of a ﬁnite cutoﬀradius, the simulation cost of the DeePCG model scales
linearly with the system size.
CG potential optimization
The construction of the CG potential U w(ξ), introduced in Subsection B, has many similarities with the
construction of the potential energy V (q), using the
FIG. 1: Schematic plot of the neural network input for the
environment of CG particle i, using water as an example.
Red and white balls represent the oxygen and the hydrogen
atoms of the microscopic system, respectively. Purple balls
denote CG particles, which, in our example, are centered at
the positions of the oxygens. We ﬁrst sort all the CG particles within the cutoﬀradius Rc centered at i, according to
their inverse distances from i. These particles constitute the
neighbors of i. i(a) and i(b) are the ﬁrst and the second nearest neighbor, respectively, of i. j and k are generic neighbors
i, i(a), and i(b) deﬁne the local frame of i.
local frame, i is the origin; the arrow from i to i(a) deﬁnes
the x axis; the directional normal to the plane containing i,
i(a), and i(b) deﬁnes the z axis; the y axis is then assigned
with the right-hand rule. Site j is close to i and is described
with full radial and angular information by the descriptor
Dij = {1/Rij, xij/R2
ij, yij/R2
ij, zij/R2
ij}, where (xij, yij, zij)
are Cartesian coordinates in the local frame of i. Site k is far
from i and is described with radial information only by the
descriptor Dik = {1/Rik}.
DeePMD method. There is, however, a very important
diﬀerence in the two cases.
In the DeePMD case the
potential energy V (q) is directly available from the underlying AIMD simulations. In the DeePCG case, the
CG potential is a free energy and is not directly available. Therefore, the optimization for the CG potential
requires a speciﬁc formulation.
We adopt a forcematching scheme like the one in the multi-scale coarsegraining method introduced by Voth et al. . In addition, we pay special attention to the fact that a neural
network representation U w(ξ) may have tens of thousands, or more, variational parameters, and a suitable
optimization algorithm is needed.
A straightforward force-matching approach would consist in ﬁtting accurate mean forces from atomistic simula-
FIG. 2: Schematic plot of the sub-network structure for the
CG particle i. Di (see deﬁnition in text) is the input and Ui
is the output. In this sub-network, data ﬂow from the input
layer (Di) to the output layer (Ui) through K hidden layers,
where each layer is a composition of a linear transformation
and a piecewise nonlinear operation ψ(· · · ). We use the hyperbolic tangent for the nonlinear function ψ. This procedure
is adopted for all the hidden layers. In the ﬁnal step going
from the last hidden layer to Ui, only the linear transformation is applied.
tions. There have been many eﬀorts in this direction . Of particular interest is a simple formula proposed
by Ciccotti et al. , in which a set of dN-dimensional
vectors bj(q) that satisfy
∇qξi(q) · bj(q) = δij,
i, j = 1, . . . M,
is introduced. Then the mean force on ξi(q), namely the
negative gradient of U(ξ) with respect to the position of
the i-th CG particle, can be expressed as
Fi(ξ) = −∂iU(ξ) = ⟨Fi(q)⟩ξ=ξ(q),
with an instantaneous force estimator
Fi(q) = −bi(q) · ∇qV (q) + 1
β ∇q · bi(q).
Here ⟨· · · ⟩ξ=ξ(q) denotes conditional expectation over the
equilibrium distribution of the system restricted to the
hypersurface ξ = ξ(q). To train the DeePCG model one
needs to minimize the so-called loss function with respect
to the model parameters w. The most natural choice of
loss function in terms of force-matching is
Fi(ξn) + ∂iU w(ξn)
where D is the number of conﬁgurations of CG variables
ξn in the dataset and the mean force Fi(ξn) is estimated
with Eq. (6).
We notice that the sample of CG con-
ﬁgurations in Eq. (8) is very general in the sense that it
does not need to be an equilibrium sample at the thermodynamic conditions of the atomistic simulation. For
example, the sample could include, in an enhanced way,
accessible CG conﬁgurations that have a small probability of occurrence at the thermodynamic conditions of
interest, such as in the case of rare events. We stress,
however, that the sampling of the microscopic degree
of freedom orthogonal to ξ in Eq. (6) must be done at
the appropriate thermodynamic conditions. In practice,
the diﬀerent conﬁgurations ξn in the dataset can be extracted from unconstrained MD or Monte Carlo (MC)
simulations of the microscopic atomistic model at diﬀerent temperatures.
The above straightforward approach is not convenient
when the conditional expectation values in Eq. (6) require computationally expensive constrained/restrained
simulations.
In this situation we ﬁnd it more convenient to approximate the ensemble average ⟨· · · ⟩q with
the average ( 1
n=1 · · · ) over the conﬁgurations ξn (see
Eq. (10) below).
The latter average does not require
constrained/restrained simulations, but it requires ξn to
be extracted from equilibrium atomistic simulations at
the temperature selected in Eq. (1). Then the mean force
Fi in the loss function (8) can be replaced by the instantaneous force Fi.
In other words, this corresponds to
using an instantaneous version of the loss function
ˆLins(w) =
Fi(ξn) + ∂iU w(ξn)
With a suﬃciently large representative dataset, we expect that the ensemble average of the diﬀerence between
predicted and instantaneous forces should be approximated quite well by ˆLins(w), i.e.:
Lins(w) :=
Fi(ξ(q)) + ∂iU w(ξ(q))
≈ˆLins(w).
This amounts to an ergodicity requirement for the atomistic system and is always valid if the system samples an
equilibrium thermodynamic state.
By deﬁnition, ˆLins(w) is much easier to compute than
ˆL(w). Below we argue that ˆLins(w) is also a valid loss
function to optimize CG potential. To see this, note that
the instantaneous force can be viewed as the mean force
plus a random error R, which depends on the microscopic
conﬁguration q, i.e.,
Fi(q) = Fi(ξ(q)) + Ri(q).
By using Eq. (6), the average ⟨Ri(q)⟩ξ=ξ(q) in the constrained ensemble vanishes, so the average ⟨Ri(q)⟩q also
vanishes. By inserting (11) into (10), the instantaneous
loss function (10) becomes
Lins(w) = L(w) +
Fi(ξ(q)) + ∂iU w(ξ(q))
Since the second term on the right hand side of Eq. (12)
is independent of w, Lins(w) and L(w) have the same
minimizer. This equivalence justiﬁes our usage of ˆLins(w)
as the loss function.
In the application example that we discuss in the next
section we use CG variables that depend linearly on the
microscopic coordinates, similar to Ref. . However,
the method that we have illustrated in Eqs. (6) and (7)
can deal with non-linear dependencies as well, as in the
method discussed in Ref. .
In practice, we ﬁnd that the stochastic gradient descent method is very eﬃcient to optimize loss function
(9), which is a highly non-convex function corresponding
to a rugged landscape in the large parameter space due
to the nonlinearity of the neural network interpolation.
This ruggedness does not seem to constitute an essential diﬃculty since the diﬀerent local minima found with
the stochastic gradient descent (SGD) method approximate equally well the physics associated to the target
function. We will discuss this issue in more detail later.
Within our approach, the stochastic gradients ∇wl(w),
applied to update the parameters at each step, are provided by the average over a small batch B, a subset of
the whole dataset:
Fi(ξ(qα)) + ∂iU w(ξ(qα))
where |B| denotes the batch size. The above procedure is
diﬀerent from the scheme adopted in Ref. , in which
the full gradients ∇w ˆLins(w) are applied to update the
parameters at each step. We ﬁnd that SGD greatly reduces the number of gradient evaluations that are required.
We note that other systematic procedures to optimize
the parameters have been discussed in the literature. Of
particular interest is the iterative Boltzmann inversion
method , which works by iteratively optimizing the
CG interactions until the radial distribution functions
of the CG system match those of the target atomistic
simulation. By construction, it provides accurate twobody correlations.
COARSE-GRAINING OF LIQUID WATER
To show how we construct the CG potential for an extensive CG system, we use the coarse graining of a liquid
water model from an ab initio density functional theory
(DFT) based simulation into eﬀective “water particles” as an example. Because of its importance as a
solvent in chemical and biological systems and its unique
properties, the study of water is of wide interest. The
DFT potential energy surface is intrinsically many-body.
Developing an accurate CG model that represents a water molecule by a single particle is an ever-evolving and
ongoing quest .
Constructing eﬀective interactions to achieve this goal
has usually required a large amount of human eﬀort combined with substantial physical/chemical intuition. For
example, in the mW monatomic potential , which
has been successfully used to study crystallization of water , a specially designed Lennard-Jones-like form is
used for two-body interactions while three-body interactions are adapted from the Stillinger-Weber potential
In principle, coarse graining approaches that do
not require physical/chemical intuition are possible by
exploiting general variational principles, such as the one
adopted in the multi-scale coarse-graining (MS-CG) 
or the iterative Boltzmann inversion methods.
However, even when using general variational principles to
bridge atomistic and CG scales, the faithfulness of distribution of the CG variables may still depend on the
CG representation. For example, in the applications of
the MS-CG method to liquid water, the two- and threebody distribution functions of the CG variables still show
non-negligible deviations from the corresponding target
microscopic distributions .
The DFT dataset in our example comes from Ref. .
The electronic structure of the water system is modeled by DFT with the PBE0 exchange-correlation functional and includes the long-range dispersion interactions self-consistently using the Tkatchenko-Scheﬄer
model .
The corresponding AIMD simulation 
adopts periodic boundary conditions and deuterons replace protons for a larger integration time step (0.5 fs).
The simulation data consist of snapshots from a 20 pslong trajectory in the NVT ensemble, where N = 192 (64
H2O molecules), V = 1.9275 nm3 (simple cubic periodic
simulation cell), and T = 330 K. In total 40,000 snapshots are recorded.
The important diﬀerence between
training DeePMD and DeePCG is that in DeePCG one
is attempting to estimate mean forces that correspond
to conditioned averages of ﬂuctuating atomic forces for
ﬁxed CG conﬁgurations, while in DeePMD one is attempting to estimate the deterministic atomic force in
a ﬁxed atomic conﬁguration. Therefore, a short AIMD
trajectory is not suﬃcient to train a DeePCG model with
satisfactory accuracy, but this diﬃculty is circumvented
by constructing a DeePMD model from the AIMD
data and sampling the conﬁgurations with a much longer
DeePMD trajectory (15 ns).
Figs. 3, 4, and 5 com-
pare DeePMD and AIMD conﬁgurations in terms of the
O-O radial distribution function (RDF), O-O-O angular distribution functions (ADFs), and the distributions
of two averaged local Steinhardt parameters (deﬁned in
Appendix A) , respectively. It is observed that the
conﬁgurations sampled by DeePMD are in almost perfect agreement with the AIMD data. Therefore, when
considering the oxygen conﬁgurations, training with the
data generated by DeePMD is essentially indistinguishable from that with data generated by AIMD.
Now we construct the DeePCG model. We use oxygen
as the CG particle. We deﬁne the local environment of
an O atom with the same cutoﬀradius adopted in the
DeePMD model, i.e., Rc = 6˚A. We use the full radial
and angular information for the 16 CG particles closest to the particle at the origin (see, e.g., particle j in
Fig. 1), while retaining only radial information for all the
other particles within Rc, (see, e.g., particle k in Fig. 1).
Next, the local environment of each CG particle deﬁnes a
sub-network, and we use 4 hidden layers with decreasing
number of nodes per layer, i.e., 120, 60, 30, and 15 nodes
from the innermost to the outermost layer, to construct
the corresponding contribution to the CG potential.
The training process minimizes ˆLins(w) deﬁned in
Eq. (9). The force on each oxygen in the atomistic model
serves as the instantaneous estimator Fi in Eq. (7). We
employ the stochastic gradient descent method with the
Adam optimizer to update the parameters of each
layer, with a batch size of 4 and a learning rate that exponentially decays with the training step. In our current
implementation, the training process requires 15 hours on
a ThinkPad p50 laptop computer with an Intel Core i7-
6700HQ CPU and 32 GB memory. The DeePMD-kit 
is used for optimizations and MD simulations of both the
DeePMD and the DeePCG models.
After training, we perform an NVT simulation on the
CG variables. The initial snapshot for this simulation is
taken directly from a snapshot selected along the AIMD
trajectory. The CG force is generated directly by analytical gradient of the CG potential, the volume and the
temperature are the same of the AIMD simulation, and
the temperature is controlled using a Langevin thermostat with a damping time τ = 0.1ps. In addition, using
the same strategy, we perform an NVT simulation on 512
CG variables, where the only diﬀerence is that the number of CG variables and the size of the simulation region
are 8 times larger than those of the AIMD simulation.
DISCUSSION
In Figs. 3, 4, and 5, we show that the DeePCG model
reproduces very well the oxygen correlation functions
of the atomistic DeePMD model and, by extension, of
the underlying AIMD model.
In addition to comparing 2- and 3-body correlations, as done in standard protocols , we also perform tests on how well the
DeePCG model preserves higher order distribution prop-
DeePCG (large sys.)
FIG. 3: Upper panel: the O-O RDFs of liquid water from
AIMD and DeePMD for a system with 64 water molecules,
and from DeePCG simulation for systems with 64 and 512
CG particles; lower panel: the deviations of DeePMD and of
two DeePCG models relative to the AIMD result.
rc = 0.27 nm
rc = 0.456 nm
rc = 0.37 nm
rc = 0.60 nm
The O-O-O ADFs of liquid water from AIMD,
DeePMD, and DeePCG simulations. The results for four different cutoﬀradii are provided.
erties. In this regard, we calculate the sample averaged
local Steinhardt bond order parameters ¯q4 and ¯q6, and
ﬁnd satisfactory agreement between the DeePCG and
DeePMD models.
In the example that we discussed above we use ˆLins(w)
to optimize a CG model of water. We ﬁnd that to base
the optimization on ˆL(w) deﬁned in Eq. (8) is signiﬁcantly less eﬃcient. This is because when the oxygens are
the CG variables, very long constrained simulations using
Eq. (6) are required to sample exhaustively the allowed
conﬁgurations of the hydrogen bond network (HBN).
Typically, when the oxygen positions are ﬁxed, as in a
constrained simulation, diﬀerent HBN conﬁgurations are
compatible with the ﬁxed oxygen conﬁgurations, but it
takes a very long time, typically of the order of a few
nanoseconds, for the system to sample diﬀerent HBN
4) or P(q-
FIG. 5: Upper panel: the ¯q4 and ¯q6 distribution function of
liquid water from AIMD, DeePMD, and DeePCG simulations;
lower panel: deviations of DeePMD and of DeepCG from the
AIMD results.
conﬁgurations.
This is because of the long-range correlations imposed on the HBN by the Pauling ice rules
(i.e., each oxygen has two nearer and two more distant
hydrogen neighbors) . Thus, the scheme used here
for matching the on-the-ﬂy instantaneous forces is much
more eﬃcient.
It is well-known that neural network models are highly
nonlinear functions of the parameters w.
Multiple local minima exist in the landscape of the loss functions
L(w) or Lins(w). Indeed, diﬀerent initializations of the
weights often lead to diﬀerent local minimizers of the loss
function. This, however, does not seem to be a serious
problem as demonstrated by the test described below.
In this test, we prepare 1000 conﬁgurations randomly
selected from the DeePMD data and pick up oxygen positions to deﬁne the CG particle conﬁgurations. For a
CG particle i in each conﬁguration, we deﬁne the model
deviation Σi to be the standard deviation of the force on
CG particle i predicted by CG models that only diﬀer
among themselves by the initialization of the simulation
procedure, i.e.,
where the ensemble average ⟨· · · ⟩w is taken with respect to models obtained from the same training process, the same training data set , but diﬀerent initialization of the parameters w. In this way, 64,000 instances of the model deviation Σi are computed, and they
are used to show the consistency of the predictions of
diﬀerent DeePCG models quantitatively. As shown by
Fig. 6, with DeePMD data corresponding to 6 independent 2.5 ns-long trajectories, 99.3% of the model deviations, i.e., a large majority of them, are below 50 meV/˚A.
Moreover, the deviations do not become more signiﬁcant
when the magnitude of the CG force is large (inset in
Fig. 6). Therefore, the diﬀerences of the CG forces pre-
Distribution
Model deviation [meV/Å]
Model devi. [meV/Å]
|F| [meV/Å]
The distributions of the model deviation Σi of the
DeePCG model for liquid water, using training data from trajectories of total length 2 ns, 5 ns, and 15 ns. The 2 ns data
are generated from a single trajectory; The 5 ns data are generated from 2 independent 2.5 ns-long trajectories; The 15 ns
data are generated from 6 independent 2.5 ns-long trajectories. Inset: the correlation between the magnitude of the CG
force and the model deviation, using 15 ns training data.
dicted by diﬀerent DeePCG models are generally consistent.
Indeed the conﬁgurational distribution functions
generated by DeePCG models that diﬀer only in the initialization are indistinguishable. If we use shorter trajectories, the model deviations increase, as shown in Fig. 6
for DeePMD data corresponding to 2 independent 2.5 nslong trajectories, and for DeePMD data corresponding to
a single 2 ns-long trajectory. This conﬁrms that longer
trajectories give better approximations of the ensemble
average for Lins(w).
In terms of computational cost and scalability, in the
current implementation, DeePCG accelerates DeePMD
7.5 times. Since all the physical quantities in DeePCG
are sums of local contributions, upon training, the
DeePCG model can be directly applied to much larger
systems with linear scaling of cost. To test the reliability
of DeePCG for larger systems, we perform a 1 ns-long
NVT CGMD simulation on a system containing 512 water beads. This system is at the same temperature of
the original DeePMD data, but is 8 times larger than the
system used to construct the DeePCG model. The corresponding RDF, as shown in Fig. 3, is only very slightly
less structured than the DeePCG result with 64 water
beads, but tends to unity at large separation with a
longer tail as we expect. This is consistent with the result
in Ref. , which shows that the pair correlation function is almost converged in a 64-water ﬁxed-cell system
and larger cells only loosen the structure very slightly.
Finally, like in the case of the Deep Potential and
DeePMD schemes , in our implementation discontinuities are present in the forces, due to adoption of
a sharp cutoﬀradius, limitation of angular information
to a ﬁxed number of atoms, and abrupt changes in the
atomic lists due to sorting. Upon training, these discontinuities become much smaller than the thermal ﬂuctua-
tions and can be subsumed in the thermal noise applied
by the stochastic thermostat used to sample the canonical ensemble. The accuracy of the canonical distributions
of the CG variables reported in Figs. 3, 4, and 5, relative to the corresponding canonical distributions in the
underlying atomistic simulation, validates our approach.
While irrelevant for canonical sampling, the discontinuities make the CG potential a piece-wise continuous function of the CG coordinates, whereas in principle it should
be a fully continuous function. We have recently generalized the present neural network representation in order to
construct potentials that are fully continuous both in the
space of the microscopic variables and in that of the CG
degrees of freedom . We leave to future work a discussion of applications using the fully continuous (smooth)
version of our approach.
CONCLUSION AND FUTURE WORK
In summary, DeePCG is a promising tool for parameterizing the CG potential and sampling CG conﬁgurations via MD. Due to the generality of the procedure
adopted to construct the CG potential function, we expect DeePCG to be useful for a wide variety of tasks.
In the case of water, we note that one reason for the
great success of the mW potential is that it allows us
to accelerate ice nucleation by several orders of magnitude because the absence of the hydrogen coordinates in
the CG coordinate set eliminates the constraint imposed
by the Pauling ice rules . It would be interesting to
investigate whether the CG water model introduced in
this paper could describe not only the liquid but also the
crystalline ice phase, and whether the freezing temperature of the CG model could approximate closely that of
the underlying microscopic model. Direct ice nucleation
studies would be greatly facilitated by the CG model.
Coarse grained models are often used to describe the
conformations of polymers, represented for example by
a sequence of beads and springs. Until now these models are typically constructed phenomenologically by requiring that a small set of force constants match experimental and/or molecular simulation data. The DeePCG
model presented here has the potential to completely
eliminate phenomenological assumptions such as the restriction to harmonic spring interactions, by systematically constructing a many-body potential for the beads
depending on their conﬁgurations. We leave these studies and a more rigorous investigation of the dynamical
properties of the CG models to future work.
Acknowledgments
The work of L. Zhang, J. Han, and W. E is supported in part by ONR grant N00014-13-1-0338, DOE
grants de-sc0008626 and de-sc0009248, and NSFC grants
U1430237 and 91530322.
The work of R. Car is supported in part by DOE-SciDAC grant de-sc0008626. The
work of H. Wang is supported by the National Science Foundation of China under Grants 11501039 and
91530322, the National Key Research and Development
Program of China under Grants 2016YFB0201200 and
2016YFB0201203, and the Science Challenge Project No.
JCKY2016212A502.
Steinhardt,
Ronchetti.
Bond-orientational order in liquids and
glasses. Physical Review B, 28(2):784, 1983.
 T Stecher, N Bernstein, and G Cs´anyi. Free energy surface reconstruction from umbrella samples using gaussian
process regression. Journal of chemical theory and computation, 10(9):4079, 2014.
 L Mones, N Bernstein, and G Cs´anyi. Exploration, sampling, and reconstruction of free energy surfaces with
gaussian process regression.
J Chem Theory Comput,
12:5100–5110, 2016.
 Tobias Lemke and Christine Peter. Neural network based
prediction of conformational free energies-a new route toward coarse-grained simulation models. Journal of chemical theory and computation, 13(12):6213–6221, 2017.
 Raimondas Galvelis and Yuji Sugita.
Neural network
and nearest neighbor algorithms for enhancing sampling of molecular dynamics. J. Chem. Theory Comput,
13(6):2489–2500, 2017.
 Elia Schneider, Luke Dai, Robert Q Topper, Christof
Drechsel-Grau, and Mark E Tuckerman. Stochastic neural network approach for learning high-dimensional free
energy surfaces. Physical Review Letters, 119(15):150601,
 Linfeng Zhang, Han Wang, and Weinan E. Reinforced
dynamics for enhanced sampling in large atomic and
molecular systems.
The Journal of Chemical Physics,
148(12):124113, 2018.
 Julija Zavadlav, Siewert J. Marrink, and Matej Praprotnik.
Multiscale simulation of protein hydration using
the swinger dynamical clustering algorithm.
of Chemical Theory and Computation, 14(3):1754–1761,
2018. PMID: 29439560.
 Alexander P Lyubartsev and Aatto Laaksonen. Calculation of eﬀective interaction potentials from radial distribution functions: A reverse monte carlo approach. Physical Review E, 52(4):3730, 1995.
 Robert E Rudd and Jeremy Q Broughton.
Coarsegrained molecular dynamics and the atomic limit of ﬁnite
elements. Physical review B, 58(10):R5893, 1998.
 I Pagonabarraga and D Frenkel. Dissipative particle dynamics for interacting systems. The Journal of Chemical
Physics, 115(11):5015–5026, 2001.
 Dirk Reith, Mathias P¨utz, and Florian M¨uller-Plathe.
potentials
atomistic simulations.
Journal of computational chemistry,
24(13):1624–1636, 2003.
 Steve O Nielsen, Carlos F Lopez, Goundla Srinivas, and
Michael L Klein. Coarse grain models and the computer
simulation of soft materials. Journal of Physics: Condensed Matter, 16(15):R481, 2004.
 Wataru Shinoda, Russell DeVane, and Michael L Klein.
Coarse-grained molecular modeling of non-ionic surfactant self-assembly. Soft Matter, 4(12):2454–2462, 2008.
 WG Noid, Jhih-Wei Chu, Gary S Ayton, Vinod Krishna,
Sergei Izvekov, Gregory A Voth, Avisek Das, and Hans C
The multiscale coarse-graining method. i.
a rigorous bridge between atomistic and coarse-grained
models. The Journal of chemical physics, 128(24):244114,
 WG Noid, Pu Liu, Yanting Wang, Jhih-Wei Chu, Gary S
Ayton, Sergei Izvekov, Hans C Andersen, and Gregory A
Voth. The multiscale coarse-graining method. ii. numerical implementation for coarse-grained molecular models.
The Journal of chemical physics, 128(24):244115, 2008.
 M Scott Shell. The relative entropy is fundamental to
multiscale and inverse thermodynamic problems.
Journal of chemical physics, 129(14):144108, 2008.
 V Molinero, EB Moore, et al. Water modeled as an intermediate element between carbon and silicon. Journal
of Physical Chemistry B, 113(13):4008–4016, 2009.
 Luca Larini, Lanyuan Lu, and Gregory A Voth.
multiscale coarse-graining method. vi. implementation of
three-body coarse-grained potentials.
The Journal of
chemical physics, 132(16):164107, 2010.
 Avisek Das and Hans C Andersen. The multiscale coarsegraining method. ix. a general method for construction
of three body coarse-grained force ﬁelds. The Journal of
chemical physics, 136(19):194114, 2012.
 Mohammadhasan Dinpajooh and Marina G Guenza. On
the density dependence of the integral equation coarsegraining eﬀective potential.
The Journal of Physical
Chemistry B, 2017.
 Michael R DeLyser and William G Noid.
pressure-matching to inhomogeneous systems via localdensity potentials.
The Journal of chemical physics,
147(13):134111, 2017.
 Jacob W Wagner, Thomas Dannenhoﬀer-Lafage, Jaehyeok Jin, and Gregory A Voth. Extending the range
and physical accuracy of coarse-grained models: Order
parameter dependent interactions. The Journal of chemical physics, 147(4):044113, 2017.
 Tanmoy Sanyal and M Scott Shell. Coarse-grained models using local-density potentials optimized with the relative entropy: Application to implicit solvation. The Journal of chemical physics, 145(3):034109, 2016.
 Joshua D Moore, Brian C Barnes, Sergei Izvekov, Martin
L´ısal, Michael S Sellers, DeCarlos E Taylor, and John K
Brennan. A coarse-grain force ﬁeld for rdx: Density dependent and energy conserving. The Journal of Chemical
Physics, 144(10):104501, 2016.
 J¨org Behler and Michele Parrinello. Generalized neuralnetwork representation of high-dimensional potentialenergy surfaces. Physical review letters, 98(14):146401,
 Albert P Bart´ok, Mike C Payne, Risi Kondor, and G´abor
Cs´anyi. Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons. Physical review letters, 104(13):136403, 2010.
 Kristof T Sch¨utt, Farhad Arbabzadah, Stefan Chmiela,
Klaus R M¨uller, and Alexandre Tkatchenko. Quantumchemical insights from deep tensor neural networks. Nature Communications, 8:13890, 2017.
 Stefan
Tkatchenko,
Sauceda, Igor Poltavsky, Kristof T Sch¨utt, and Klaus-
Robert M¨uller.
Machine learning of accurate energyconserving molecular force ﬁelds.
Science Advances,
3(5):e1603015, 2017.
 Jequn Han, Linfeng Zhang, Roberto Car, and Weinan E.
Deep potential: a general representation of a many-body
potential energy surface. Communications in Computational Physics, 23(3):629–639, 2018.
 Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car,
and Weinan E. Deep potential molecular dynamics: A
scalable model with the accuracy of quantum mechanics.
Phys. Rev. Lett., 120:143001, Apr 2018.
 Roberto Car and Michele Parrinello. Uniﬁed approach for
molecular dynamics and density-functional theory. Physical Review Letters, 55(22):2471, 1985.
 Sebastian T John and Gabor Csanyi.
coarse-grained interactions using gaussian approximation potentials. The Journal of Physical Chemistry B,
121(48):10934–10949, 2017.
 Carmen Hij´on, Pep Espa˜nol, Eric Vanden-Eijnden, and
Rafael Delgado-Buscalioni. Mori–zwanzig formalism as
a practical computational tool.
Faraday discussions,
144:301–322, 2010.
 Jianfeng Lu and Eric Vanden-Eijnden. Exact dynamical coarse-graining without time-scale separation.
Journal of chemical physics, 141(4):07B619 1, 2014.
 Gregory A Voth. Coarse-graining of condensed phase and
biomolecular systems. CRC press, 2008.
 Patrick T Underhill and Patrick S Doyle. On the coarsegraining of polymers into bead-spring chains. Journal of
non-newtonian ﬂuid mechanics, 122(1):3–31, 2004.
 Ian Goodchild, Laura Collier, Sarah L Millar, Ivan
Prokeˇs, Jason CD Lord, Craig P Butts, James Bowers,
John RP Webster, and Richard K Heenan. Structural
studies of the phase, aggregation and surface behaviour
of 1-alkyl-3-methylimidazolium halide+ water mixtures.
Journal of colloid and interface science, 307(2):455–468,
 BL Bhargava and Michael L. Klein. Formation of micelles in aqueous solutions of a room temperature ionic
liquid: a study using coarse grained molecular dynamics.
Molecular Physics, 107(4-6):393–401, 2009.
 Giovanni Ciccotti, Raymond Kapral, and Eric Vanden-
reaction coordinates, and unbiased constrained dynamics.
ChemPhysChem, 6(9):1809–1814, 2005.
 Luca Maragliano and Eric Vanden-Eijnden. A temperature accelerated method for sampling free energy and determining reaction pathways in rare events simulations.
Chemical physics letters, 426(1):168–175, 2006.
 Jerry B Abrams and Mark E Tuckerman. Eﬃcient and
direct generation of multidimensional free energy surfaces via adiabatic dynamics without coordinate transformations.
The Journal of Physical Chemistry B,
112(49):15742–15757, 2008.
 Evangelia Kalligiannaki, Vagelis Harmandaris, Markos A
Katsoulakis, and Petr Plech´aˇc.
The geometry of generalized force matching and related information metrics
in coarse-graining of molecular systems. The Journal of
chemical physics, 143(8):084105, 2015.
 Walter Kohn and Lu Jeu Sham. Self-consistent equations
including exchange and correlation eﬀects. Physical Review, 140(4A):A1133, 1965.
 Emily B Moore and Valeria Molinero. Structural transformation in supercooled water controls the crystallization rate of ice. Nature, 479(7374):506–508, 2011.
 Frank H Stillinger and Thomas A Weber. Computer simulation of local order in condensed phases of silicon. Physical Review B, 31(8):5262, 1985.
 Robert A DiStasio Jr, Biswajit Santra, Zhaofeng Li, Xifan Wu, and Roberto Car. The individual and collective
eﬀects of exact exchange and dispersion interactions on
the ab initio structure of liquid water. The Journal of
chemical physics, 141(8):084502, 2014.
 Carlo Adamo and Vincenzo Barone.
Toward reliable
density functional methods without adjustable parameters: The pbe0 model. The Journal of Chemical Physics,
110(13):6158–6170, 1999.
 Alexandre Tkatchenko and Matthias Scheﬄer. Accurate
molecular van der waals interactions from ground-state
electron density and free-atom reference data. Physical
Review Letters, 102:073005, Feb 2009.
 Wolfgang Lechner and Christoph Dellago. Accurate determination of crystal structures based on averaged local
bond order parameters. The Journal of chemical physics,
129(11):114707, 2008.
 Diederik Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
 Han Wang, Linfeng Zhang, Jiequn Han, and Weinan E.
Deepmd-kit: A deep learning package for many-body potential energy representation and molecular dynamics.
arXiv preprint arXiv:1712.03641, 2017.
 Linus Pauling. The shared-electron chemical bond. Proceedings of the national academy of sciences, 14(4):359–
362, 1928.
 Thomas D K¨uhne, Matthias Krack, and Michele Parrinello.
Static and dynamical properties of liquid water from ﬁrst principles by a novel car- parrinello-like
approach. Journal of chemical theory and computation,
5(2):235–241, 2009.
 Linfeng Zhang, Jiequn Han, Han Wang, Wissam A Saidi,
Roberto Car, and Weinan E. End-to-end symmetry preserving inter-atomic potential energy model for ﬁnite
and extended systems. arXiv preprint arXiv:1805.09003,
 The training data set is the same, but the batches used at
each step of the Adam iteration are picked from the data
set randomly and independently for diﬀerent models.
Appendix A: Deﬁnition of the local averaged
Steinhardt parameters
The bond orientational order of particle i (atom or
molecule) in a condensed environment is often described
by a local Steinhardt parameter ql(i) , deﬁned as
|qlm(i)|2i1/2
j∈Nb(i) s(rij)Ylm(ˆr ij)
j∈Nb(i) s(rij)
Here Nb(i) denotes the set of neighbors of particle i,
Ylm(ˆr ij) are spherical harmonics, and s(rij) is a switching function deﬁned by
rmax −rmin
rmin ≤r < rmax,
In this work we take rmin = 0.31 nm and rmax = 0.36 nm,
and adopt the modiﬁcation of the local Steinhardt parameter proposed by Lechner and Dellago , which is
more sensitive than the original bond order parameter in
distinguishing diﬀerent crystal structures. The modiﬁed
Steinhardt parameter is deﬁned by
|¯qlm(i)|2i 1
Nb(i) s(rij)qlm(j)
Nb(i) s(rij)
where ˜Nb(i) includes Nb(i) and the tagged particle i. In
the full expansion of the local averaged Steinhardt parameters, 4-body terms like Ylm(ˆr ik) · Ylm(ˆr jl), i ̸= j ̸=
k ̸= l are found. Therefore, the distribution of the value
of the local averaged Steihardt parameters includes the
eﬀect of 4-body angular correlations.