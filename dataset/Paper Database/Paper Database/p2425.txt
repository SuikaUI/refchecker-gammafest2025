StarGAN: Uniﬁed Generative Adversarial Networks
for Multi-Domain Image-to-Image Translation
Yunjey Choi1,2 Minje Choi1,2 Munyoung Kim2,3 Jung-Woo Ha2 Sunghun Kim2,4 Jaegul Choo1,2
1 Korea University
2 Clova AI Research, NAVER Corp.
3 The College of New Jersey
4 Hong Kong University of Science & Technology
Figure 1. Multi-domain image-to-image translation results on the CelebA dataset via transferring knowledge learned from the RaFD dataset.
The ﬁrst and sixth columns show input images while the remaining columns are images generated by StarGAN. Note that the images are
generated by a single generator network, and facial expression labels such as angry, happy, and fearful are from RaFD, not CelebA.
Recent studies have shown remarkable success in imageto-image translation for two domains. However, existing
approaches have limited scalability and robustness in handling more than two domains, since different models should
be built independently for every pair of image domains. To
address this limitation, we propose StarGAN, a novel and
scalable approach that can perform image-to-image translations for multiple domains using only a single model.
Such a uniﬁed model architecture of StarGAN allows simultaneous training of multiple datasets with different domains
within a single network. This leads to StarGAN’s superior
quality of translated images compared to existing models as
well as the novel capability of ﬂexibly translating an input
image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute
transfer and a facial expression synthesis tasks.
1. Introduction
The task of image-to-image translation is to change a
particular aspect of a given image to another, e.g., changing
the facial expression of a person from smiling to frowning
(see Fig. 1). This task has experienced signiﬁcant improvements following the introduction of generative adversarial
networks (GANs), with results ranging from changing hair
color , reconstructing photos from edge maps , and
changing the seasons of scenery images .
Given training data from two different domains, these
models learn to translate images from one domain to the
other. We denote the terms attribute as a meaningful feature inherent in an image such as hair color, gender or age,
and attribute value as a particular value of an attribute, e.g.,
black/blond/brown for hair color or male/female for gender.
We further denote domain as a set of images sharing the
same attribute value. For example, images of women can
 
represent one domain while those of men represent another.
Several image datasets come with a number of labeled
attributes. For instance, the CelebA dataset contains 40
labels related to facial attributes such as hair color, gender,
and age, and the RaFD dataset has 8 labels for facial
expressions such as ‘happy’, ‘angry’ and ‘sad’. These settings enable us to perform more interesting tasks, namely
multi-domain image-to-image translation, where we change
images according to attributes from multiple domains. The
ﬁrst ﬁve columns in Fig. 1 show how a CelebA image can
be translated according to any of the four domains, ‘blond
hair’, ‘gender’, ‘aged’, and ‘pale skin’. We can further extend to training multiple domains from different datasets,
such as jointly training CelebA and RaFD images to change
a CelebA image’s facial expression using features learned
by training on RaFD, as in the rightmost columns of Fig. 1.
However, existing models are both inefﬁcient and ineffective in such multi-domain image translation tasks. Their
inefﬁciency results from the fact that in order to learn all
mappings among k domains, k(k−1) generators have to
be trained. Fig. 2 (a) illustrates how twelve distinct generator networks have to be trained to translate images among
four different domains. Meanwhile, they are ineffective that
even though there exist global features that can be learned
from images of all domains such as face shapes, each generator cannot fully utilize the entire training data and only
can learn from two domains out of k. Failure to fully utilize training data is likely to limit the quality of generated
images. Furthermore, they are incapable of jointly training domains from different datasets because each dataset is
partially labeled, which we further discuss in Section 3.2.
As a solution to such problems we propose StarGAN, a
novel and scalable approach capable of learning mappings
among multiple domains. As demonstrated in Fig. 2 (b), our
model takes in training data of multiple domains, and learns
the mappings between all available domains using only a
single generator. The idea is simple. Instead of learning
a ﬁxed translation (e.g., black-to-blond hair), our generator
takes in as inputs both image and domain information, and
learns to ﬂexibly translate the image into the corresponding domain. We use a label (e.g., binary or one-hot vector)
to represent domain information. During training, we randomly generate a target domain label and train the model to
ﬂexibly translate an input image into the target domain. By
doing so, we can control the domain label and translate the
image into any desired domain at testing phase.
We also introduce a simple but effective approach that
enables joint training between domains of different datasets
by adding a mask vector to the domain label. Our proposed
method ensures that the model can ignore unknown labels
and focus on the label provided by a particular dataset. In
this manner, our model can perform well on tasks such
as synthesizing facial expressions of CelebA images us-
(a) Cross-domain models
(b) StarGAN
Figure 2. Comparison between cross-domain models and our proposed model, StarGAN. (a) To handle multiple domains, crossdomain models should be built for every pair of image domains.
(b) StarGAN is capable of learning mappings among multiple domains using a single generator. The ﬁgure represents a star topology connecting multi-domains.
ing features learned from RaFD, as shown in the rightmost columns of Fig. 1. As far as our knowledge goes, our
work is the ﬁrst to successfully perform multi-domain image translation across different datasets.
Overall, our contributions are as follows:
• We propose StarGAN, a novel generative adversarial
network that learns the mappings among multiple domains using only a single generator and a discriminator, training effectively from images of all domains.
• We demonstrate how we can successfully learn multidomain image translation between multiple datasets by
utilizing a mask vector method that enables StarGAN
to control all available domain labels.
• We provide both qualitative and quantitative results on
facial attribute transfer and facial expression synthesis tasks using StarGAN, showing its superiority over
baseline models.
2. Related Work
Generative Adversarial Networks. Generative adversarial networks (GANs) have shown remarkable results
in various computer vision tasks such as image generation
 , image translation , super-resolution
imaging , and face image synthesis . A
typical GAN model consists of two modules: a discriminator and a generator. The discriminator learns to distinguish
between real and fake samples, while the generator learns to
generate fake samples that are indistinguishable from real
samples. Our approach also leverages the adversarial loss
to make the generated images as realistic as possible.
Conditional GANs. GAN-based conditional image generation has also been actively studied. Prior studies have pro-
Input image
Target domain
Depth-wise concatenation
Fake image
Fake image
Depth-wise concatenation
Reconstructed
Fake image
classification
Real / Fake
(b) Original-to-target domain
(c) Target-to-original domain
(d) Fooling the discriminator
classification
Real / Fake
Fake image
Real image
(a) Training the discriminator
Figure 3. Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between
real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain
label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to
reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from
real images and classiﬁable as target domain by D.
vided both the discriminator and generator with class information in order to generate samples conditioned on the class
 . Other recent approaches focused on generating
particular images highly relevant to a given text description
 . The idea of conditional image generation has also
been successfully applied to domain transfer , superresolution imaging , and photo editing . In this
paper, we propose a scalable GAN framework that can ﬂexibly steer the image translation to various target domains,
by providing conditional domain information.
Image-to-Image Translation. Recent work have achieved
impressive results in image-to-image translation . For instance, pix2pix learns this task in a supervised manner using cGANs .
It combines an adversarial loss with a L1 loss, thus requires paired data samples. To alleviate the problem of obtaining data pairs, unpaired image-to-image translation frameworks 
have been proposed. UNIT combines variational autoencoders (VAEs) with CoGAN , a GAN framework where two generators share weights to learn the joint
distribution of images in cross domains. CycleGAN 
and DiscoGAN preserve key attributes between the input and the translated image by utilizing a cycle consistency
loss. However, all these frameworks are only capable of
learning the relations between two different domains at a
time. Their approaches have limited scalability in handling
multiple domains since different models should be trained
for each pair of domains. Unlike the aforementioned approaches, our framework can learn the relations among multiple domains using only a single model.
3. Star Generative Adversarial Networks
We ﬁrst describe our proposed StarGAN, a framework to
address multi-domain image-to-image translation within a
single dataset. Then, we discuss how StarGAN incorporates
multiple datasets containing different label sets to ﬂexibly
perform image translations using any of these labels.
3.1. Multi-Domain Image-to-Image Translation
Our goal is to train a single generator G that learns mappings among multiple domains. To achieve this, we train G
to translate an input image x into an output image y conditioned on the target domain label c, G(x, c) →y. We randomly generate the target domain label c so that G learns
to ﬂexibly translate the input image. We also introduce an
auxiliary classiﬁer that allows a single discriminator to
control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D : x →{Dsrc(x), Dcls(x)}. Fig. 3 illustrates
the training process of our proposed approach.
Adversarial Loss. To make the generated images indistinguishable from real images, we adopt an adversarial loss
Ladv = Ex [log Dsrc(x)] +
Ex,c[log (1 −Dsrc(G(x, c)))],
where G generates an image G(x, c) conditioned on both
the input image x and the target domain label c, while D
tries to distinguish between real and fake images. In this
paper, we refer to the term Dsrc(x) as a probability distribution over sources given by D. The generator G tries to
minimize this objective, while the discriminator D tries to
maximize it.
Domain Classiﬁcation Loss. For a given input image x
and a target domain label c, our goal is to translate x into
an output image y, which is properly classiﬁed to the target
domain c. To achieve this condition, we add an auxiliary
classiﬁer on top of D and impose the domain classiﬁcation
loss when optimizing both D and G. That is, we decompose
the objective into two terms: a domain classiﬁcation loss of
real images used to optimize D, and a domain classiﬁcation
loss of fake images used to optimize G. In detail, the former
is deﬁned as
cls = Ex,c′[−log Dcls(c′|x)],
where the term Dcls(c′|x) represents a probability distribution over domain labels computed by D. By minimizing
this objective, D learns to classify a real image x to its corresponding original domain c′. We assume that the input
image and domain label pair (x, c′) is given by the training
data. On the other hand, the loss function for the domain
classiﬁcation of fake images is deﬁned as
cls = Ex,c[−log Dcls(c|G(x, c))].
In other words, G tries to minimize this objective to generate images that can be classiﬁed as the target domain c.
Reconstruction Loss. By minimizing the adversarial and
classiﬁcation losses, G is trained to generate images that
are realistic and classiﬁed to its correct target domain. However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input
images while changing only the domain-related part of the
inputs. To alleviate this problem, we apply a cycle consistency loss to the generator, deﬁned as
Lrec = Ex,c,c′[||x −G(G(x, c), c′)||1],
where G takes in the translated image G(x, c) and the original domain label c′ as input and tries to reconstruct the original image x. We adopt the L1 norm as our reconstruction
loss. Note that we use a single generator twice, ﬁrst to translate an original image into an image in the target domain
and then to reconstruct the original image from the translated image.
Full Objective. Finally, the objective functions to optimize
G and D are written, respectively, as
LD = −Ladv + λcls Lr
LG = Ladv + λcls Lf
cls + λrec Lrec,
where λcls and λrec are hyper-parameters that control the
relative importance of domain classiﬁcation and reconstruction losses, respectively, compared to the adversarial loss.
We use λcls = 1 and λrec = 10 in all of our experiments.
3.2. Training with Multiple Datasets
An important advantage of StarGAN is that it simultaneously incorporates multiple datasets containing different
types of labels, so that StarGAN can control all the labels
at the test phase. An issue when learning from multiple
datasets, however, is that the label information is only partially known to each dataset. In the case of CelebA and
RaFD , while the former contains labels for attributes
such as hair color and gender, it does not have any labels
for facial expressions such as ‘happy’ and ‘angry’, and vice
versa for the latter. This is problematic because the complete information on the label vector c′ is required when
reconstructing the input image x from the translated image
G(x, c) (See Eq. (4)).
Mask Vector. To alleviate this problem, we introduce a
mask vector m that allows StarGAN to ignore unspeciﬁed
labels and focus on the explicitly known label provided by
a particular dataset. In StarGAN, we use an n-dimensional
one-hot vector to represent m, with n being the number of
datasets. In addition, we deﬁne a uniﬁed version of the label
as a vector
˜c = [c1, ..., cn, m],
where [·] refers to concatenation, and ci represents a vector
for the labels of the i-th dataset. The vector of the known
label ci can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes.
For the remaining n−1 unknown labels we simply assign
zero values. In our experiments, we utilize the CelebA and
RaFD datasets, where n is two.
Training Strategy. When training StarGAN with multiple
datasets, we use the domain label ˜c deﬁned in Eq. (7) as input to the generator. By doing so, the generator learns to
ignore the unspeciﬁed labels, which are zero vectors, and
focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset,
except for the dimension of the input label ˜c. On the other
hand, we extend the auxiliary classiﬁer of the discriminator to generate probability distributions over labels for all
datasets. Then, we train the model in a multi-task learning
setting, where the discriminator tries to minimize only the
classiﬁcation error associated to the known label. For example, when training with images in CelebA, the discriminator minimizes only classiﬁcation errors for labels related
to CelebA attributes, and not facial expressions related to
RaFD. Under these settings, by alternating between CelebA
and RaFD the discriminator learns all of the discriminative
features for both datasets, and the generator learns to control all the labels in both datasets.
Figure 4. Facial attribute transfer results on the CelebA dataset. The ﬁrst column shows the input image, next four columns show the single
attribute transfer results, and rightmost columns show the multi-attribute transfer results. H: Hair color, G: Gender, A: Aged.
4. Implementation
Improved GAN Training. To stabilize the training process
and generate higher quality images, we replace Eq. (1) with
Wasserstein GAN objective with gradient penalty de-
Ladv = Ex[Dsrc(x)] −Ex,c[Dsrc(G(x, c))]
−λgp Eˆx[(||▽ˆxDsrc(ˆx)||2 −1)2] ,
where ˆx is sampled uniformly along a straight line between
a pair of a real and a generated images. We use λgp = 10
for all experiments.
Network Architecture.
Adapted from CycleGAN ,
StarGAN has the generator network composed of two convolutional layers with the stride size of two for downsampling, six residual blocks , and two transposed convolutional layers with the stride size of two for upsampling. We
use instance normalization for the generator but no normalization for the discriminator. We leverage PatchGANs
 for the discriminator network, which classiﬁes
whether local image patches are real or fake. See the appendix (Section 7.2) for more details about the network architecture.
5. Experiments
In this section, we ﬁrst compare StarGAN against recent
methods on facial attribute transfer by conducting user studies. Next, we perform a classiﬁcation experiment on facial expression synthesis. Lastly, we demonstrate empirical
results that StarGAN can learn image-to-image translation
from multiple datasets. All our experiments were conducted
by using the model output from unseen images during the
training phase.
5.1. Baseline Models
As our baseline models, we adopt DIAT and Cycle-
GAN , both of which performs image-to-image translation between two different domains. For comparison, we
trained these models multiple times for every pair of two
different domains. We also adopt IcGAN as a baseline
which can perform attribute transfer using a cGAN .
DIAT uses an adversarial loss to learn the mapping from
x ∈X to y ∈Y , where x and y are face images in two
different domains X and Y , respectively. This method has
a regularization term on the mapping as ||x −F(G(x))||1
to preserve identity features of the source image, where F
is a feature extractor pretrained on a face recognition task.
CycleGAN also uses an adversarial loss to learn the mapping between two different domains X and Y . This method
regularizes the mapping via cycle consistency losses,
||x −(GY X(GXY (x)))||1 and ||y −(GXY (GY X(y)))||1.
This method requires two generators and discriminators for
each pair of two different domains.
IcGAN combines an encoder with a cGAN model.
cGAN learns the mapping G : {z, c} →x that generates
an image x conditioned on both the latent vector z and the
conditional vector c. In addition, IcGAN introduces an encoder to learn the inverse mappings of cGAN, Ez : x →z
and Ec : x →c. This allows IcGAN to synthesis images
by only changing the conditional vector and preserving the
latent vector.
Figure 5. Facial expression synthesis results on the RaFD dataset.
5.2. Datasets
CelebA. The CelebFaces Attributes (CelebA) dataset 
contains 202,599 face images of celebrities, each annotated
with 40 binary attributes. We crop the initial 178 × 218 size
images to 178×178, then resize them as 128×128. We randomly select 2,000 images as test set and use all remaining
images for training data. We construct seven domains using
the following attributes: hair color (black, blond, brown),
gender (male/female), and age (young/old).
RaFD. The Radboud Faces Database (RaFD) consists
of 4,824 images collected from 67 participants. Each participant makes eight facial expressions in three different gaze
directions, which are captured from three different angles.
We crop the images to 256 × 256, where the faces are centered, and then resize them to 128 × 128.
5.3. Training
All models are trained using Adam with β1 = 0.5
and β2 = 0.999. For data augmentation we ﬂip the images horizontally with a probability of 0.5. We perform one
generator update after ﬁve discriminator updates as in .
The batch size is set to 16 for all experiments. For experiments on CelebA, we train all models with a learning rate of
0.0001 for the ﬁrst 10 epochs and linearly decay the learning rate to 0 over the next 10 epochs. To compensate for the
lack of data, when training with RaFD we train all models
for 100 epochs with a learning rate of 0.0001 and apply the
same decaying strategy over the next 100 epochs. Training
takes about one day on a single NVIDIA Tesla M40 GPU.
5.4. Experimental Results on CelebA
We ﬁrst compare our proposed method to the baseline
models on a single and multi-attribute transfer tasks. We
train the cross-domain models such as DIAT and Cycle-
GAN multiple times considering all possible attribute value
pairs. In the case of DIAT and CycleGAN, we perform
multi-step translations to synthesize multiple attributes (e.g.
transferring a gender attribute after changing a hair color).
Qualitative evaluation. Fig. 4 shows the facial attribute
transfer results on CelebA. We observed that our method
provides a higher visual quality of translation results on test
data compared to the cross-domain models. One possible
reason is the regularization effect of StarGAN through a
multi-task learning framework. In other words, rather than
training a model to perform a ﬁxed translation (e.g., brownto-blond hair), which is prone to overﬁtting, we train our
model to ﬂexibly translate images according to the labels
of the target domain. This allows our model to learn reliable features universally applicable to multiple domains of
images with different facial attribute values.
Furthermore, compared to IcGAN, our model demonstrates an advantage in preserving the facial identity feature
of an input. We conjecture that this is because our method
maintains the spatial information by using activation maps
from the convolutional layer as latent representation, rather
than just a low-dimensional latent vector as in IcGAN.
Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format
using Amazon Mechanical Turk (AMT) to assess single
and multiple attribute transfer tasks. Given an input im-
Figure 6. Facial expression synthesis results of StarGAN-SNG and StarGAN-JNT on CelebA dataset.
age, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer
in attribute(s), and preservation of a ﬁgure’s original identity. The options were four randomly shufﬂed images generated from four different methods. The generated images
in one study have a single attribute transfer in either hair
color (black, blond, brown), gender, or age.
In another
study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions
with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user
study is 146 and 100 in single and multiple transfer tasks,
respectively.
Hair color
Table 1. AMT perceptual evaluation for ranking different models
on a single attribute transfer task. Each column sums to 100%.
Table 2. AMT perceptual evaluation for ranking different models
on a multi-attribute transfer task. H: Hair color; G: Gender; A:
Quantitative results. Tables 1 and 2 show the results of
our AMT experiment on single- and multi-attribute transfer tasks, respectively. StarGAN obtained the majority of
votes for best transferring attributes in all cases. In the case
of gender changes in Table 1, the voting difference between
our model and other models was marginal, e.g., 39.1% for
StarGAN vs. 31.4% for DIAT. However, in multi-attribute
changes, e.g., the ‘G+A’ case in Table 2, the performance
difference becomes signiﬁcant, e.g., 49.8% for StarGAN vs.
20.3% for IcGAN), clearly showing the advantages of Star-
GAN in more complicated, multi-attribute transfer tasks.
This is because unlike the other methods, StarGAN can handle image translation involving multiple attribute changes
by randomly generating a target domain label in the training phase.
5.5. Experimental Results on RaFD
We next train our model on the RaFD dataset to learn the
task of synthesizing facial expressions. To compare Star-
GAN and baseline models, we ﬁx the input domain as the
‘neutral’ expression, but the target domain varies among the
seven remaining expressions.
Qualitative evaluation. As seen in Fig. 5, StarGAN clearly
generates the most natural-looking expressions while properly maintaining the personal identity and facial features of
the input. While DIAT and CycleGAN mostly preserve the
identity of the input, many of their results are shown blurry
and do not maintain the degree of sharpness as seen in the
input. IcGAN even fails to preserve the personal identity in
the image by generating male images.
We believe that the superiority of StarGAN in the image
quality is due to its implicit data augmentation effect from
a multi-task learning setting. RaFD images contain a relatively small size of samples, e.g., 500 images per domain.
When trained on two domains, DIAT and CycleGAN can
only use 1,000 training images at a time, but StarGAN can
use 4,000 images in total from all the available domains for
its training. This allows StarGAN to properly learn how to
maintain the quality and sharpness of the generated output.
Quantitative evaluation. For a quantitative evaluation, we
compute the classiﬁcation error of a facial expression on
synthesized images. We trained a facial expression classiﬁer on the RaFD dataset (90%/10% splitting for training
and test sets) using a ResNet-18 architecture , resulting
in a near-perfect accuracy of 99.55%. We then trained each
of image translation models using the same training set and
performed image translation on the same, unseen test set.
Finally, we classiﬁed the expression of these translated images using the above-mentioned classiﬁer. As can be seen in
Table 3, our model achieves the lowest classiﬁcation error,
indicating that our model produces the most realistic facial
expressions among all the methods compared.
Classiﬁcation error
# of parameters
52.6M × 14
Real images
Table 3. Classiﬁcation errors [%] and the number of parameters on
the RaFD dataset.
Another important advantage of our model is the scalability in terms of the number of parameters required. The
last column in Table 3 shows that the number of parameters
required to learn all translations by StarGAN is seven times
smaller than that of DIAT and fourteen times smaller than
that of CycleGAN. This is because StarGAN requires only
a single generator and discriminator pair, regardless of the
number of domains, while in the case of cross-domain models such as CycleGAN, a completely different model should
be trained for each source-target domain pair.
5.6. Experimental Results on CelebA+RaFD
Finally, we empirically demonstrate that our model can
learn not only from multiple domains within a single
dataset, but also from multiple datasets. We train our model
jointly on the CelebA and RaFD datasets using the mask
vector (see Section 3.2). To distinguish between the model
trained only on RaFD and the model trained on both CelebA
and RaFD, we denote the former as StarGAN-SNG (single)
and the latter as StarGAN-JNT (joint).
Effects of joint training.
Fig. 6 shows qualitative comparisons between StarGAN-SNG and StarGAN-JNT, where
the task is to synthesize facial expressions of images in
CelebA. StarGAN-JNT exhibits emotional expressions with
high visual quality, while StarGAN-SNG generates reasonable but blurry images with gray backgrounds. This difference is due to the fact that StarGAN-JNT learns to translate
CelebA images during training but not StarGAN-SNG. In
other words, StarGAN-JNT can leverage both datasets to
improve shared low-level tasks such facial keypoint detection and segmentation. By utilizing both CelebA and RaFD,
StarGAN-JNT can improve these low-level tasks, which is
beneﬁcial to learning facial expression synthesis.
Learned role of mask vector. In this experiment, we gave a
one-hot vector c by setting the dimension of a particular facial expression (available from the second dataset, RaFD) to
Figure 7. Learned role of the mask vector. All images are generated by StarGAN-JNT. The ﬁrst row shows the result of applying
the proper mask vector, and the last row shows the result of applying the wrong mask vector.
one. In this case, since the label associated with the second
data set is explicitly given, the proper mask vector would be
 . Fig. 7 shows the case where this proper mask vector
was given and the opposite case where a wrong mask vector
of was given. When the wrong mask vector was used,
StarGAN-JNT fails to synthesize facial expressions, and it
manipulates the age of the input image. This is because the
model ignores the facial expression label as unknown and
treats the facial attribute label as valid by the mask vector.
Note that since one of the facial attributes is ‘young’, the
model translates the image from young to old when it takes
in a zero vector as input. From this behavior, we can con-
ﬁrm that StarGAN properly learned the intended role of a
mask vector in image-to-image translations when involving
all the labels from multiple datasets altogether.
6. Conclusion
In this paper, we proposed StarGAN, a scalable imageto-image translation model among multiple domains using
a single generator and a discriminator. Besides the advantages in scalability, StarGAN generated images of higher
visual quality compared to existing methods ,
owing to the generalization capability behind the multi-task
learning setting. In addition, the use of the proposed simple
mask vector enables StarGAN to utilize multiple datasets
with different sets of domain labels, thus handling all available labels from them. We hope our work to enable users
to develop interesting image translation applications across
multiple domains.
Acknowledgements. This work was mainly done while the
ﬁrst author did a research internship at Clova AI Research,
NAVER. We thank all the researchers at NAVER, especially
Donghyun Kwak, for insightful discussions. This work was
partially supported by the National Research Foundation
of Korea (NRF) grant funded by the Korean government
(MSIP) (No. NRF2016R1C1B2015924). Jaegul Choo is
the corresponding author.