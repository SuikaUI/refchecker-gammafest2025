Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78–87,
Gothenburg, Sweden, April 26-30 2014. c⃝2014 Association for Computational Linguistics
Multi-Granular Aspect Aggregation in Aspect-Based Sentiment Analysis
John Pavlopoulos and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
 
Aspect-based sentiment analysis estimates
the sentiment expressed for each particular aspect (e.g., battery, screen) of an entity (e.g., smartphone).
Different words
or phrases, however, may be used to refer to the same aspect, and similar aspects may need to be aggregated at coarser
or ﬁner granularities to ﬁt the available
space or satisfy user preferences. We introduce the problem of aspect aggregation at multiple granularities. We decompose it in two processing phases, to allow previous work on term similarity and
hierarchical clustering to be reused. We
show that the second phase, where aspects
are clustered, is almost a solved problem, whereas further research is needed
in the ﬁrst phase, where semantic similarity measures are employed.
introduce a novel sense pruning mechanism for WordNet-based similarity measures, which improves their performance
in the ﬁrst phase. Finally, we provide publicly available benchmark datasets.
Introduction
Given a set of texts discussing a particular entity (e.g., reviews of a laptop), aspect-based sentiment analysis (ABSA) attempts to identify the most
prominent (e.g., frequently discussed) aspects of
the entity (e.g., battery, screen) and the average
sentiment (e.g., 1 to 5 stars) for each aspect or
group of aspects, as in Fig. 1. Most ABSA systems
perform all or some of the following :
subjectivity detection to retain only sentences (or
other spans) expressing subjective opinions; aspect extraction to extract (and possibly rank) terms
corresponding to aspects (e.g., ‘battery’); aspect
aggregation to group aspect terms that are nearsynonyms (e.g., ‘price’, ‘cost’) or to obtain aspects
Figure 1: Aspect groups and scores of an entity.
at a coarser granularity (e.g., ‘chicken’,‘steak’,
and ‘ﬁsh’ may be replaced by ‘food’ in restaurant
reviews); and aspect sentiment score estimation to
estimate the average sentiment for each aspect or
group of aspects. In this paper, we focus on aspect
aggregation, the least studied stage of the four.
Aspect aggregation is needed to avoid reporting
separate sentiment scores for aspect terms that are
very similar. In Fig. 1, for example, showing separate lines for ‘money’, ‘price’, and ‘cost’ would
be confusing. The extent to which aspect terms
should be aggregated, however, also depends on
the available space and user preferences. On devices with smaller screens, it may be desirable to
aggregate aspect terms that are similar, though not
necessarily near-synonyms (e.g., ‘design’, ‘color’,
‘feeling’) to show fewer lines (Fig. 1), but ﬁner aspects may be preferable on larger screens. Users
may also wish to adjust the granularity of aspects,
e.g., by stretching or narrowing the height of Fig. 1
on a smartphone to view more or fewer lines.
Hence, aspect aggregation should be able to produce groups of aspect terms for multiple granularities. We assume that the aggregated aspects are
displayed as lists of terms, as in Fig. 1. We make
no effort to order (e.g., by frequency) the terms in
each list, nor do we attempt to produce a single
(more general) term to describe each aggregated
aspect, leaving such tasks for future work.
ABSA systems usually group synonymous (or
near-synonymous) aspect terms . Ag-
gregating only synonyms (or near-synonyms),
however, does not allow users to select the desirable aspect granularity, and ignores the hierarchical relations between aspect terms. For example,
‘pizza’ and ‘steak’ are kinds of ‘food’ and, hence,
the three terms can be aggregated to show fewer,
coarser aspects, even though they are not synonyms. Carenini et al. used a predeﬁned
domain-speciﬁc taxonomy to hierarchically aggregate aspect terms, but taxonomies of this kind
are often not available. By contrast, we use only
general-purpose taxonomies (e.g., WordNet), term
similarity measures based on general-purpose taxonomies or corpora, and hierarchical clustering.
We deﬁne multi-granular aspect aggregation to
be the task of partitioning a given set of aspect
terms (generated by a previous aspect extraction
stage) into k non-overlapping clusters, for multiple values of k. A further constraint is that the
clusters have to be consistent for different k values, meaning that if two aspect terms t1, t2 are
placed in the same cluster for k = k1, then t1
and t2 must also be grouped together (in the same
cluster) for every k = k2 with k2 < k1, i.e., for
every coarser grouping. For example, if ‘waiter’
and ‘service’ are grouped together for k = 5, they
must also be grouped together for k = 4, 3, 2
and (trivially) k = 1, to allow the user to feel
that selecting a smaller number of aspect groups
(narrowing the height of Fig. 1) has the effect of
zooming out (without aspect terms jumping unexpectedly to other aspect groups), and similarly
for zooming in.1 This requirement is satisﬁed by
using agglomerative hierarchical clustering algorithms , which in our case produce term hierarchies
like the ones of Fig. 2. By using slices (nodes at a
particular depth) of the hierarchies that are closer
to the root or the leaves, we obtain fewer or more
clusters. The vertical dotted lines of Fig. 2 illustrate two slices for k = 4. By contrast, ﬂat clustering algorithms (e.g., k-means) do not satisfy the
consistency constraint for different k values.
Agglomerative clustering algorithms require a
measure of the distance between individuals, in
our case a measure of how similar two aspect
terms are, and a linkage criterion to specify which
clusters should be merged to form larger (coarser)
clusters. To experiment with different term sim-
1We also require the clusters to be non-overlapping to
make this zooming in and out metaphor clearer to the user.
Figure 2: Example aspect hierarchies produced by
agglomerative hierarchical clustering.
Table 1: An aspect term similarity matrix.
ilarity measures and linkage criteria, we decompose multi-granular aspect aggregation in two processing phases. Phase A ﬁlls in a symmetric matrix, like the one of Table 1, with scores showing the similarity of each pair of input aspect
terms; the matrix in effect deﬁnes the distance
measure to be used by agglomerative clustering.
In Phase B, the aspect terms are grouped into k
non-overlapping clusters, for varying values of k,
given the matrix of Phase A and a linkage criterion; a hierarchy like the ones of Fig. 2 is ﬁrst
formed via agglomerative clustering, and fewer or
more clusters (for different values of k) are then
obtained by using different slices of the hierarchy,
as already discussed. Our two-phase decomposition can also accommodate non-hierarchical clustering algorithms, provided that the consistency
constraint is satisﬁed, but we consider only agglomerative hierarchical clustering in this paper.
The decomposition in two phases has three
main advantages. Firstly, it allows reusing previous work on term similarity measures , which can be used to ﬁll in the matrix of Phase A. Secondly, the decomposition allows different linkage criteria to be experimentally compared (in Phase B) using the same similarity matrix (of Phase A), i.e., the same distance
measure. Thirdly, the decomposition leads to high
inter-annotator agreement, as we show experimentally. By contrast, in preliminary experiments we
found that asking humans to directly evaluate aspect hierarchies produced by hierarchical clustering, or to manually create gold aspect hierarchies
led to poor inter-annotator agreement.
We show that existing term similarity measures
perform reasonably well in Phase A, especially
when combined, but there is a large scope for improvement. We also propose a novel sense pruning
method for WordNet-based similarity measures,
which leads to signiﬁcant improvements in Phase
A. In Phase B, we experiment with agglomerative clustering using four different linkage criteria,
concluding that they all perform equally well and
that Phase B is almost a solved problem when the
gold similarity matrix of Phase A is used; however, further improvements are needed in the similarity measures of Phase A to produce a sufﬁciently good similarity matrix. We also make publicly available the datasets of our experiments.
Our main contributions are:
(i) to the best
of our knowledge, we are the ﬁrst to consider
multi-granular aspect aggregation (not just merging near-synonyms) in ABSA without manually
crafted domain-speciﬁc ontologies; (ii) we propose a two-phase decomposition that allows previous work on term similarity and hierarchical clustering to be reused and evaluated with high interannotator agreement; (iii) we introduce a novel
sense pruning mechanism that improves WordNetbased similarity measures; (iv) we provide the ﬁrst
public datasets for multi-granular aspect aggregation; (v) we show that the second phase of our decomposition is almost a solved problem, and that
research should focus on the ﬁrst phase. Although
we experiment with customer reviews of products
and services, ABSA and the work of this paper in
particular are, at least in principle, also applicable
to texts expressing opinions about other kinds of
entities (e.g., politicians, organizations).
Section 2 below discusses related work. Sections 3 and 4 present our work for Phase A and B,
respectively. Section 5 concludes.
Related work
Most existing approaches to aspect aggregation
aim to produce a single, ﬂat partitioning of aspect terms into aspect groups, rather than aspect
groups at multiple granularities. The most common approaches are to aggregate only
synonyms or near-synonyms, using WordNet , statistics from corpora ,
or semi-supervised learning , or to cluster the aspect terms
using (latent) topic models . Topic models do not perform
better than other methods , and
their clusters may overlap.2 The topic model of
Titov et al. uses two granularity levels;
we consider many more (3–10 levels).
Carenini et al. used a predeﬁned domainspeciﬁc taxonomy and similarity measures to aggregate related terms. Yu et al. used a tailored version of an existing taxonomy. By contrast, we assume no domain-speciﬁc taxonomy.
Kobayashi et al. proposed methods to extract aspect terms and relations between them, including hierarchical relations. They extract, however, relations by looking for clues in texts (e.g.,
particular phrases). By contrast, we employ similarity measures and hierarchical clustering, which
allows us to group similar aspect terms even when
they do not cooccur in texts.
Also, in contrast
to Kobayashi et al. , we respect the consistency constraint discussed in Section 1.
A similar task is taxonomy induction.
Cimiano and Staab automatically construct taxonomies from texts via agglomerative clustering,
much as in our Phase B, but not in the context of
ABSA, and without trying to learn a similarity matrix ﬁrst. They also label the hierarchy’s concepts,
a task we do not consider. Klapaftis and Manandhar show how word sense induction can be
combined with agglomerative clustering to obtain
more accurate taxonomies, again not in the context of ABSA. Our sense pruning method was in-
ﬂuenced by their work, but is much simpler than
their word sense induction. Fountain and Lapata
 study unsupervised methods to induce concept taxonomies, without considering ABSA.
We now discuss our work for Phase A. Recall that
in this phase the input is a set of aspect terms and
2Topic models are typically also used to perform aspect
extraction, apart from aspect aggregation, but simple heuristics (e.g., most frequent nouns) often outperform them in aspect extraction .
the goal is to ﬁll in a matrix (Table 1) with scores
showing the similarity of each pair of aspect terms.
Datasets used in Phase A
We used two benchmark datasets that we had previously constructed to evaluate ABSA methods for
subjectivity detection, aspect extraction, and aspect score estimation, but not aspect aggregation.
We extended them to support aspect aggregation,
and we make them publicly available.3
The two original datasets contain sentences
from customer reviews of restaurants and laptops,
respectively. The reviews are manually split into
sentences, and each sentence is manually annotated as ‘subjective’ (expressing opinion) or ‘objective’ (not expressing opinion). The restaurants
dataset contains 3,710 English sentences from the
restaurant reviews of Ganu et al. . The laptops dataset contains 3,085 English sentences from
394 customer reviews, collected from sites that
host customer reviews. In the experiments of this
paper, we use only the 3,057 (out of 3,710) subjective restaurant sentences and the 2,631 (out of
3,085) subjective laptop sentences.
For each subjective sentence, our datasets show
the words that human annotators marked as aspect
terms. For example, in “The dessert was divine!”
the aspect term is ‘dessert’, and in “Really bad
waiter.” it is ‘waiter’. Among the 3,057 subjective
restaurant sentences, 1,129 contain exactly one aspect term, 829 more than one, and 1,099 no aspect
term; a subjective sentence may express an opinion about the restaurant (or laptop) being reviewed
without mentioning a speciﬁc aspect (e.g., “Really
nice restaurant!”), which is why no aspect terms
are present in some subjective sentences. There
are 558 distinct multi-word aspect terms and 431
distinct single-word aspect terms in the subjective
restaurant sentences. Among the 2,631 subjective
sentences of the laptop reviews, 823 contain exactly one aspect term, 389 more than one, and
1,419 no aspect term. There are 273 distinct multiword aspect terms and 330 distinct single-word aspect terms in the subjective laptop sentences.
From each dataset, we selected the 20 (distinct)
aspect terms that the human annotators had annotated most frequently, taking annotation frequency
to be an indicator of importance; there are only
two multi-word aspect terms (‘hard drive’, ‘bat-
3The datasets are available at 
aueb.gr/software.html.
tery life’) among the 20 most frequent ones in the
laptops dataset, and none among the 20 most frequent aspect terms of the restaurants dataset. We
then formed all the 190 possible pairs of the 20
terms and constructed an empty similarity matrix
(Fig. 1), one for each dataset, which was given
to three human judges to ﬁll in (1: strong dissimilarity, 5: strong similarity).4 For each aspect
term, all the subjective sentences mentioning the
term were also provided, to help the judges understand how the terms are used in the particular domains (e.g., ‘window’ and ‘Windows’ have
domain-speciﬁc meanings in laptop reviews).
The Pearson correlation coefﬁcient indicated
high inter-annotator agreement (0.81 for restaurants, 0.74 for laptops). We also measured the absolute inter-annotator agreement a(l1, l2), deﬁned
below, where l1, l2 are lists containing the scores
(similarity matrix values) of two judges, N is the
length of each list, and vmax, vmin are the largest
and smallest possible scores (5 and 1).
a(l1, l2) = 1
1 −|l1(i) −l2(i)|
vmax −vmin
The absolute interannotator agreement was also
high (0.90 for restaurants, 0.91 for laptops).5 With
both measures, we compute the agreement of each
judge with the averaged (for each matrix cell)
scores of the other two judges, and we report the
mean of the three agreement estimates. Finally, we
created the gold similarity matrix of each dataset
by placing in each cell the average scores that the
three judges had provided for that cell.
In preliminary experiments, we gave aspect
terms to human judges, asking them to group any
terms they considered near-synonyms. We then
asked the judges to group the aspect terms into
fewer, coarser groups by grouping terms that could
be viewed as direct hyponyms of the same broader
term (e.g., ‘pizza’ and ‘steak’ are both kinds of
‘food’), or that stood in a hyponym-hypernym relation (e.g., ‘pizza’ and ‘food’).
We used the
Dice coefﬁcient to measure inter-annotator agreement, and we obtained reasonably good agreement
for near-synonyms (0.77 for restaurants, 0.81 for
laptops), but poor agreement for the coarser as-
4The matrix is symmetric; hence, the judges had to ﬁll in
only half of it. The guidelines and an annotation tool that
were given to the judges are available upon request.
5The Pearson correlation ranges from −1 to 1, whereas
the absolute inter-annotator agreement ranges from 0 to 1.
pects (0.25 and 0.11).6 In other preliminary experiments, we asked human judges to rank alternative aspect hierarchies that had been produced
by applying agglomerative clustering with different linkage criteria to 20 aspect terms, but we obtained very poor inter-annotator agreement (Pearson score −0.83 for restaurants and 0 for laptops).
Phase A methods
We employed ﬁve term similarity measures. The
ﬁrst two are WordNet-based . The next two combine WordNet with
statistics from corpora. The ﬁfth one is a corpusbased distributional similarity measure.
The ﬁrst measure is Wu and Palmer’s . It
is actually a sense similarity measure (a term may
have multiple senses). Given two senses sij, si′j′
of terms ti, ti′, the measure is deﬁned as follows:
WP(sij, si′j′) = 2 ·
depth(lcs(sij, si′j′))
depth(sij) + depth(sij),
where lcs(sij, si′j′) is the least common subsumer, i.e., the most speciﬁc common ancestor of
the two senses in WordNet, and depth(s) is the
depth of sense s in WordNet’s hierarchy.
Most terms have multiple senses, however,
and word sense disambiguation methods are not yet robust enough. Hence, when
given two aspect terms ti, ti′, rather than particular
senses of the terms, a simplistic greedy approach
is to compute the similarities of all the possible
pairs of senses sij, si′j′ of ti, ti′, and take the similarity of ti, ti′ to be the maximum similarity of
the sense pairs . We use this greedy approach
with all the WordNet-based measures, but we also
propose a sense pruning mechanism below, which
improves their performance. In all the WordNetbased measures, if a term is not in WordNet, we
take its similarity to any other term to be zero.7
The second measure, PATH (sij, si′j′), is simply the inverse of the length (plus one) of the shortest path connecting the senses sij, si′j′ in WordNet
 . Again, the greedy approach
can be used with terms having multiple senses.
6The Dice coefﬁcient ranges from 0 to 1. There was a very
large number of possible responses the judges could provide
and, hence, it would be inappropriate to use Cohen’s K.
7This never happened in the restaurants dataset. In the
laptops dataset, it only happened for ‘hard drive’ and ‘battery life’. We use the NLTK implementation of the ﬁrst four
measures (see and our own implementation of the distributional similarity measure.
The third measure is Lin’s , deﬁned as:
LIN (sij, si′j′) = 2 · ic(lcs(sij, si′j′))
ic(sij) + ic(si′j′) ,
sij, si′j′
lcs(sij, si′j′) is the least common subsumer of
sij, si′j′ in WordNet, and ic(s) = −log P(s) is
the information content of sense s , estimated from a corpus. When the corpus is not sense-tagged, we follow the common
approach of treating each occurrence of a word as
an occurrence of all of its senses, when estimating ic(s).8 We experimented with two variants of
Lin’s measure, one where the ic(s) scores were
estimated from the Brown corpus , and one where they were estimated from
the (restaurant or laptop) reviews of our datasets.
The fourth measure is Jiang and Conrath’s
 , deﬁned below. Again, we experimented
with two variants of ic(s), as above.
JCN (sij, si′j′) =
ic(sij) + ic(si′j′) −2 · lcs(sij, si′j′)
For all the above WordNet-based measures, we
experimented with a sense pruning mechanism,
which discards some of the senses of the aspect
terms, before applying the greedy approach. For
each aspect term ti, we consider all of its Word-
Net senses sij. For each sij and each other aspect
term ti′, we compute (using PATH ) the similarity between sij and each sense si′j′ of ti′, and we
consider the relevance of sij to ti′ to be:9
rel(sij, ti′) =
si′j′ ∈senses(ti′) PATH (sij, si′j′)
The relevance of sij to all of the N other aspect
terms ti′ is taken to be:
rel(sij) = 1
rel(sij, ti′)
For each aspect term ti, we retain only its senses
sij with the top rel(sij) scores, which tends to
8 
README-WN-IC-30.txt. We use the default counting.
9We also experimented with other similarity measures
when computing rel(sij, ti′), instead of PATH , but there
was no signiﬁcant difference. We use NLTK to tokenize, remove punctuation, and stop-words.
without SP
LIN @domain
LIN @Brown
JCN @domain
JCN @Brown
Table 2: Phase A results (Pearson correlation to
gold similarities) with and without sense pruning.
prune senses that are very irrelevant to the particular domain (e.g., laptops). This sense pruning mechanism is novel, and we show experimentally that it improves the performance of all the
WordNet-based similarity measures we examined.
We also implemented a distributional similarity measure , for
each aspect term t, we create a vector ⃗v(t) =
⟨PMI (t, w1), . . . , PMI (t, wn)⟩. The vector components are the Pointwise Mutual Information
scores of t and each word wi of a corpus:
PMI (t, wi) = −log
P(t) · P(wi)
We treat P(t, wi) as the probability of t, wi cooccurring in the same sentence, and we use the (laptop or restaurant) reviews of our datasets as the
corpus to estimate the probabilities. The distributional similarity DS(t, t′) of two aspect terms t, t′
is the cosine similarity of ⃗v(t),⃗v(t′).10
Finally, we tried combinations of the similarity
measures: AVG is the average of all ﬁve; WN is
the average of the ﬁrst four, which employ Word-
Net; and WNDS is the average of WN and DS;
all the scores range in . We also tried regression (e.g., SVR), but there was no improvement.
Phase A experimental results
Each similarity measure was evaluated by computing its Pearson correlation with the scores of the
gold similarity matrix. Table 2 shows the results.
Our sense pruning consistently improves all
four WordNet-based measures. It does not apply to
10We also experimented with Euclidean distance, a normalized PMI , and the Brown corpus, but
there was no improvement.
DS, which is why the DS results are identical with
and without pruning. A paired t test indicates that
the other differences (with and without pruning) of
Table 2 are statistically signiﬁcant (p < 0.05). We
used the senses with the top ﬁve rel(sij) scores for
each aspect term ti during sense pruning. We also
experimented with keeping fewer senses, but the
results were inferior or there was no improvement.
Lin’s measure performed better when information content was estimated on the (much
larger, but domain-independent) Brown corpus
(LIN @Brown), as opposed to using the (domainspeciﬁc) reviews of our datasets (LIN @domain),
but we observed no similar consistent pattern for
JCN . Given its simplicity, PATH performed remarkably well in the restaurants dataset; it was
the best measure (including combinations) without
sense pruning, and the best uncombined measure
with sense pruning. It performed worse, however,
compared to several other measures in the laptops
dataset. Similar comments apply to WP, which is
among the top-performing uncombined measures
in restaurants, both with and without sense pruning, but the worst overall measure in laptops. DS
is the best overall measure in laptops when compared to measures without sense pruning, and the
third best overall when compared to measures that
use sense pruning, but the worst overall in restaurants both with and without pruning.
JCN , which use both WordNet and corpus statistics, have a more balanced performance across the
two datasets, but they are not top-performers in
any of the two. Combinations of similarity measures seem more stable across domains, as the results of AVG, WN , and WNDS indicate, though
experiments with more domains are needed to investigate this issue.
WNDS is the best overall
method with sense pruning, and among the best
three methods without pruning in both datasets.
To get a better view of the performance of
WNDS with sense pruning, i.e., the best overall
measure of Table 2, we compared it to two state of
the art semantic similarity systems. First, we applied the system of Han et al. , one of the
best systems of the recent *Sem 2013 semantic
text similarity competition, to our Phase A data.
The performance (Pearson correlation with gold
similarities) of the same system on the widely used
WordSim353 word similarity dataset is 0.73, much higher than the same system’s
performance on our Phase A data (see Table 3),
Restaurants
Han et al. 
WNDS with SP
Table 3: Phase A results (Pearson correlation to
gold similarities) of WNDS with SP against semantic similarity systems and human judges.
which suggests that our data are more difﬁcult.11
We also employed the recent Word2Vec system, which computes continuous vector space representations of words from large corpora and has
been reported to improve results in word similarity
tasks . We used the English
Wikipedia to compute word vectors with 200 features.12 The similarity between two aspect terms
was taken to be the cosine similarity of their vectors. This system performed better than Han et
al.’s with laptops, but not with restaurants.
Table 3 shows that WNDS (with sense pruning) performed clearly better than the system of
Han et al. and Word2Vec. Table 3 also shows
the Pearson correlation of each judge’s scores to
the gold similarity scores, as an indication of the
best achievable results. Although WNDS (with
sense pruning) performs reasonably well in both
domains,13 there is large scope for improvement.
In Phase B, the aspect terms are to be grouped
into k non-overlapping clusters, for varying values of k, given a Phase A similarity matrix. We
experimented with both the gold similarity matrix
of Phase A and similarity matrices produced by
WNDS (with SP), the best Phase A method.
Phase B methods
We experimented with agglomerative clustering
and four linkage criteria: single, complete, average, and Ward . Let d(t1, t2) be the distance of
11The system of Han et al. is available from
 
SimService/; we use the STS similarity.
12Word2Vec
 
google.com/p/word2vec/.
We used the continuous
bag of words model with default parameters, the ﬁrst billion
characters of the English Wikipedia, and the preprocessing of
 
13Recall that the Pearson correlation ranges from −1 to 1.
two individual instances t1, t2; in our case, the instances are aspect terms and d(t1, t2) is the inverse
of the similarity of t1, t2, deﬁned by the Phase A
similarity matrix (gold or produced by WNDS).
Different linkage criteria deﬁne differently the distance of two clusters D(C1, C2), which affects
the choice of clusters that are merged to produce
coarser (higher-level) clusters:
Dsingle(C1, C2)
t1∈C1,t2∈C2 d(t1, t2)
Dcompl(C1, C2)
t1∈C1,t2∈C2 d(t1, t2)
Davg(C1, C2)
Complete linkage tends to produce more compact
clusters, compared to single linkage, with average
linkage being in between. Ward minimizes the total in-cluster variance; consult Milligan for
further details.14
Phase B experimental results
To evaluate the k clusters produced at each aspect
granularity by the different linkage criteria, we
used the Silhouette Index (SI ) ,
a cluster evaluation measure that considers both
inter- and intra-cluster coherence.15 Given a set of
clusters {C1, . . . , Ck}, each SI (Ci) is deﬁned as:
max(bj, aj),
where aj is the mean distance from the j-th instance of Ci to the other instances in Ci, and bj is
the mean distance from the j-th instance of Ci to
the instances in the cluster nearest to Ci. Then:
SI ({C1, . . . , Ck}) = 1
We always use the correct (gold) distances of the
instances (terms) when computing the SI scores.
As shown in Fig. 3, no linkage criterion clearly
outperforms the others, when the gold matrix of
Phase A is used; all four criteria perform reasonably well. Note that the SI ranges from −1 to
14We used the SCIPY implementations of agglomerative clustering with the four criteria (see 
scipy.org), relying on maxclust to obtain the slice of the
resulting hierarchy that leads to k (or approx. k) clusters.
15We used the SI
implementation of Pedregosa et
al. ; see We also
experimented with the Dunn Index and the
Davies-Bouldin Index , but we obtained similar results.
(a) restaurants
(b) laptops
Figure 3: Silhouette Index (SI) results for Phase
B, using the gold similarity matrix of Phase A.
(a) restaurants
(b) laptops
Figure 4: SI results for Phase B, using the WNDS
(with SP) similarity matrix of Phase A.
1, with higher values indicating better clustering.
Figure 4 shows that when the similarity matrix of
WNDS (with SP) is used, the SI scores deteriorate signiﬁcantly; again, there is no clear winner
among the linkage criteria, but average and Ward
seem to be overall better than the others.
(a) Restaurants
(b) Laptops
Figure 5: Human evaluation of aspect groups.
In a ﬁnal experiment, we showed clusterings
of varying granularities (k values) to four human
judges (graduate CS students).
The clusterings
were produced by two systems: one that used the
gold similarity matrix of Phase A and agglomerative clustering with average linkage in Phase B,
and one that used the similarity matrix of WNDS
(with SP) and again agglomerative clustering with
average linkage. We showed all the clusterings
to all the judges. Each judge was asked to evaluate each clustering on a 1–5 scale. We measured
the absolute inter-annotator agreement, as in Section 3.1, and found high agreement in all cases
(0.93 and 0.83 for the two systems, respectively,
in restaurants; 0.85 for both in laptops).16
Figure 5 shows the average human scores of
the two systems for different granularities. The
judges considered the aspect groups always perfect or near-perfect when the gold similarity matrix of Phase A was used, but they found the aspect groups to be of rather poor quality when
the similarity matrix of the best Phase A measure was used. These results, along with those of
Fig. 3–4, show that more effort needs to be devoted
to improving the similarity measures of Phase A,
whereas Phase B is in effect an almost solved
problem, if a good similarity matrix is available.
Conclusions
We considered a new, more demanding form of
aspect aggregation in ABSA, which aims to aggregate aspects at multiple granularities, as opposed
to simply merging near-synonyms, and without assuming that manually crafted domain-speciﬁc ontologies are available. We decomposed the problem in two processing phases, which allow previous work on term similarity and hierarchical
clustering to be reused and evaluated appropriately with high inter-annotator agreement.
showed that the second phase, where we used agglomerative clustering, is an almost solved problem, whereas further research is needed in the ﬁrst
phrase, where term similarity measures are employed. We also introduced a sense pruning mechanism that signiﬁcantly improves WordNet-based
similarity measures, leading to a measure that outperforms state of the art similarity methods in the
ﬁrst phase of our decomposition. We also made
publicly available the datasets of our experiments.
Acknowledgments
We thank G. Batistatos, A. Zosakis, and G. Lampouras for their annotations in Phase A. We thank
A. Kosmopoulos, G. Lampouras, P. Malakasiotis,
and I. Lourentzou for their annotations in Phase B.
16The Pearson correlation cannot be computed, as several
judges gave the same rating to the ﬁrst system, for all k.