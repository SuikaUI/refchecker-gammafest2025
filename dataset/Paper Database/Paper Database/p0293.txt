Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5851–5861
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Low-resource Deep Entity Resolution with Transfer and Active Learning
Jungo Kasai♥∗
Sairam Gurajada♣
Yunyao Li♣
Lucian Popa♣
♥Paul G. Allen School of Computer Science & Engineering, University of Washington
♣IBM Research – Almaden
 
{qian.kun,Sairam.Gurajada}@ibm.com
{yunyaoli,lpopa}@us.ibm.com
Entity resolution (ER) is the task of identifying different representations of the same
real-world entities across databases.
key step for knowledge base creation and text
Recent adaptation of deep learning methods for ER mitigates the need for
dataset-speciﬁc feature engineering by constructing distributed representations of entity
records. While these methods achieve stateof-the-art performance over benchmark data,
they require large amounts of labeled data,
which are typically unavailable in realistic ER
applications.
In this paper, we develop a
deep learning-based method that targets lowresource settings for ER through a novel combination of transfer learning and active learning.
We design an architecture that allows
us to learn a transferable model from a highresource setting to a low-resource one. To further adapt to the target dataset, we incorporate
active learning that carefully selects a few informative examples to ﬁne-tune the transferred
model. Empirical evaluation demonstrates that
our method achieves comparable, if not better, performance compared to state-of-the-art
learning-based methods while using an order
of magnitude fewer labels.
Introduction
Entity Resolution (ER), also known as entity
matching, record linkage , and merge-purge , identiﬁes and links different representations of the same real-world entities. ER yields
a uniﬁed and consistent view of data and serves as
a crucial step in downstream applications, including knowledge base creation, text mining , and social media analysis . For instance, seen in Table 1 are
citation data records from two databases, DBLP
and Google Scholar.
If one intends to build a
system that analyzes citation networks of publications, it is essential to recognize publication overlaps across the databases and to integrate the data
records .
Recent work demonstrated that deep learning
(DL) models with distributed representations of
words are viable alternatives to other machine
learning algorithms,
including support vector
machines and decision trees, for performing ER
 . The
DL models provide a universal solution to ER
across all kinds of datasets that alleviates the necessity of expensive feature engineering, in which
a human designer explicitly deﬁnes matching
functions for every single ER scenario. However,
DL is well known to be data hungry; in fact, the
DL models proposed in Ebraheem et al. ;
Mudgal et al. achieve state-of-the-art performance by learning from thousands of labels.1
Unfortunately, realistic ER tasks have limited access to labeled data and would require substantial
labeling effort upfront, before the actual learning
of the ER models. Creating a representative training set is especially challenging in ER problems
due to the data distribution, which is heavily
skewed towards negative pairs (i.e. non-matches)
as opposed to positive pairs (i.e. matches).
This problem limits the applicability of DL
methods in low-resource ER scenarios. Indeed,
we will show in a later section that the performance of DL models degrades signiﬁcantly as
compared to other machine learning algorithms
when only a limited amount of labeled data is
available. To address this issue, we propose a DLbased method that combines transfer learning and
117k labels were used for the DBLP-Scholar scenario.
M Carey, D Dewitt, J Naughton, M
Asgarian, P Brown, J Gehrke, D Shah
Object-relational
Benchmark (Experience Paper)
SIGMOD Conference
A Netz, S Chaudhuri, J Bernhardt, U
Integration of Data Mining with
Database Technology
Google Scholar
MJ Carey, DJ Dewitt, JF Naughton,
M Asgarian, P
The Bucky Object Relational
Proceedings of the SIGMOD Conference on Management of Data
A Netz, S Chaudhuri, J Bernhardt, U
Integration of Data Mining and
Relational Databases
Table 1: Data record examples from DBLP-Scholar (citation genre). The ﬁrst records from DBLP and Google
Scholar (red) refer to the same publication even though the information is not identical. The second ones (blue and
brown) record different papers with the same authors and year.
active learning. We ﬁrst develop a transfer learning methodology to leverage a few pre-existing
scenarios with abundant labeled data, in order to
use them in other settings of similar nature but
with limited or no labeled data. More concretely,
through a carefully crafted neural network architecture, we learn a transferable model from
multiple source datasets with cumulatively abundant labeled data.
Then we use active learning
to identify informative examples from the target
dataset to further adapt the transferred model
to the target setting. This novel combination of
transfer and active learning in ER settings enables
us to learn a comparable or better performing
DL model while using signiﬁcantly fewer target
dataset labels in comparison to state-of-the-art DL
and even non-DL models. We also note that the
two techniques are not dependent on each other.
For example, one could skip transfer learning if no
high-resource dataset is available and directly use
active learning. Conversely, one could use transfer
learning directly without active learning. We evaluate these cases in the experiments. Speciﬁcally,
we make the following contributions:
• We propose a DL architecture for ER that
learns attribute agnostic and transferable representations from multiple source datasets using dataset (domain) adaptation.
• To the best of our knowledge, we are the ﬁrst
to design an active learning algorithm for deep
ER models.
Our active learning algorithm
searches for high-conﬁdence examples and uncertain examples, which provide a guided way
to improve the precision and recall of the transferred model to the target dataset.
• We perform extensive empirical evaluations
over multiple benchmark datasets and demonstrate that our method outperforms state-ofthe-art learning-based models while using an
order of magnitude fewer labels.
Background and Related Work
Entity Resolution
Let D1 and D2 be two collections of entity
records. The task of ER is to classify the entity
record pair ⟨e1, e2⟩, ∀e1 ∈D1, e2 ∈D2, into
a match or a non-match.
This is accomplished
by comparing entity record e1 to e2 on their
corresponding attributes. In this paper, we assume
records in D1 and D2 share the same schema
(set of attributes).
In cases where they have
different attributes, one can use schema matching
techniques to ﬁrst
align the schemas, followed by data exchange
techniques .
Each attribute
value is a sequence of words.
Table 1 shows
examples of data records from an ER scenario,
DBLP-Scholar from the
citation genre and clearly depicts our assumption
of datasets handled in this paper.
Since the entire Cartesian product D1 × D2
often becomes large and it is infeasible to run
a high-recall classiﬁer directly,
we typically
decompose the problem into two steps: blocking
and matching. Blocking ﬁlters out obvious nonmatches from the Cartesian product to obtain a
candidate set. Attribute-level or record-level tf-idf
and jaccard similarity can be used for blocking criteria. For example, in the DBLP-Scholar scenario,
one blocking condition could be based on applying equality on “Year”. Hence, two publications in
different years will be considered as obvious nonmatches and ﬁltered out from the candidate set.
Then, the subsequent matching phase classiﬁes
the candidate set into matches and non-matches.
Figure 1: Deep ER model architecture with dataset
adaptation via gradient reversal. Only two attributes
are shown. Ws indicate word vectors.
Learning-based Entity Resolution
As described above, after the blocking step, ER
reduces to a binary classiﬁcation task on candidate pairs of data records. Prior work has proposed learning-based methods that train classiﬁers
on training data, such as support vector machines,
naive bayes, and decision trees .
These learningbased methods ﬁrst extract features for each record
pair from the candidate set across attributes in the
schema, and use them to train a binary classiﬁer.
The process of selecting appropriate classiﬁcation
features is often called feature engineering and it
involves substantial human effort in each ER scenario. Recently, Ebraheem et al. and Mudgal et al. have proposed deep learning models that use distributed representations of entity
record pairs for classiﬁcation. These models beneﬁt from distributed representations of words and
learn complex features automatically without the
need for dataset-speciﬁc feature engineering.
Deep ER Model Architecture
We describe the architecture of our DL model that
classiﬁes each record pair in the candidate set into
a match or a non-match. As shown in Fig. 1, our
model encompasses a sequence of steps that computes attribute representations, attribute similarity
and ﬁnally the record similarity for each input pair
⟨e1, e2⟩. A matching classiﬁer uses the record similarity representation to classify the pair. For an
extensive list of hyperparameters and training details we chose, see the appendix.
Input Representations.
For each entity record
pair ⟨e1, e2⟩, we tokenize the attribute values and
vectorize the words by external word embeddings
to obtain input representations (Ws in Fig. 1). We
use the 300 dimensional fastText embeddings , which capture subword information by producing word vectors via character n-grams.
This vectorization has the beneﬁt
of well representing out-of-vocabulary words that frequently appear in ER
attributes. For instance, venue names SIGMOD
and ACL are out of vocabulary in the publicly
available GloVe vectors ,
but we clearly need to distinguish them.
Attribute Representations. We build a universal
bidirectional RNN on the word input representations of each attribute value and obtain attribute
vectors (attr1 and attr2 in Fig. 1) by concatenating the last hidden units from both directions. Crucially, the universal RNN allows for transfer learning between datasets of different schemas without
error-prone schema mapping. We found that gated
recurrent units ) yielded
the best performance on the dev set as compared to
simple recurrent neural networks ) and Long Short-Term Memory networks
 ).
We also found that using BiGRU with multiple
layers did not help, and we will use one-layer Bi-
GRUs with 150 hidden units throughout the experiments below.
Attribute Similarity. The resultant attribute representations are then used to compare attributes of
each entity record pair. In particular, we compute
the element-wise absolute difference between the
two attribute vectors for each attribute and construct attribute similarity vectors (sim1 and sim2
in Fig. 1). We also considered other comparison
mechanisms such as concatenation and elementwise multiplication, but we found that absolute
difference performs the best in development, and
we will report results from absolute difference.
Record Similarity. Given the attribute similarity
vectors, we now combine those vectors to represent the similarity between the input entity record
pair. Here, we take a simple but effective approach
of adding all attribute similarity vectors (sim in
Fig. 1). This way of combining vectors ensures
that the ﬁnal similarity vector is of the same dimensionality regardless of the number of attributes
and facilitates transfer of all the subsequent parameters.
For instance, the DBLP-Scholar and
Cora2 datasets have four and eight attributes respectively, but the networks can share all weights
and biases between the two. We also tried methods
such as max pooling and average pooling, but none
of them outperformed the simple addition method.
Matching Classiﬁcation. We ﬁnally feed the similarity vector for the two records to a two-layer
multilayer perceptron (MLP) with highway connections and classify the
pair into a match or a non-match (“Matching Classiﬁer” in Fig. 1). The output from the ﬁnal layer
of the MLP is a two dimensional vector and we
normalize it by the softmax function to obtain a
probability distribution. We will discuss dataset
adaptation for transfer learning in the next section.
Training Objectives. We train the networks to
minimize the negative log-likelihood loss. We use
the Adam optimization algorithm with batch size 16 and an initial learning rate of 0.001, and after each epoch we evaluate
our model on the dev set. Training terminates after
20 epochs, and we choose the model that yields the
best F1 score on the dev set and evaluate the model
on the test data.
Deep Transfer Active Learning for ER
We introduce two orthogonal frameworks for our
deep ER models in low resource settings: transfer
and active learning. We also introduce the notion
of likely false positives and likely false negatives,
and provide a principled active labeling method in
the context of deep ER models, which contributes
to stable and high performance.
Adversarial Transfer Learning
The architecture described above allows for simple transfer learning: we can train all parameters
in the network on source data and use them to
classify a target dataset. However, this method of
transfer learning can suffer from dataset-speciﬁc
properties. For example, the author attribute in
the DBLP-ACM dataset contains ﬁrst names while
that in the DBLP-Scholar dataset only has ﬁrst initials. In such situations, it becomes crucial to construct network representations that are invariant
with respect to idiosyncratic properties of datasets.
To this end, we apply the technique of dataset (domain) adaptation developed in image recognition
2 
data/cora-refs.tar
 . In particular, we
build a dataset classiﬁer with the same architecture as the matching classiﬁer (“Dataset Classi-
ﬁer” in Fig. 1) that predicts which dataset the input pair comes from. We replace the training objective by the sum of the negative log-likelihood
losses from the two classiﬁers.
We add a gradient reversal layer between the similarity vector
and the dataset classiﬁer so that the parameters
in the dataset classiﬁer are trained to predict the
dataset while the rest of the network is trained to
mislead the dataset classiﬁer, thereby developing
dataset-independent internal representations. Crucially, with dataset adaptation, we feed pairs from
the target dataset as well as the source to the network. For the pairs from the target, we disregard
the loss from the matching classiﬁer.
Active Learning
Since labeling a large number of pairs for each ER
scenario clearly does not scale, prior work in ER
has adopted active learning as a more guided approach to select examples to label .
Designing an effective active learning algorithm
for deep ER models is particularly challenging because ﬁnding informative examples is very dif-
ﬁcult (especially for positive examples due to
the extremely low matching ratio in realistic ER
tasks), and we need more than a handful of both
negative and positive examples in order to tune a
deep ER model with many parameters.
To address this issue, we design an iterative
active learning algorithm (Algorithm 1) that
searches for two different types of examples from
unlabeled data in each iteration: (1) uncertain examples including likely false positives and likely
false negatives, which will be labeled by human
annotators; (2) high-conﬁdence examples including high-conﬁdence positives and high-conﬁdence
negatives.
We will not label high-conﬁdence
examples and use predicted labels as a proxy.
We will show below that those carefully selected
examples serve different purposes.
Uncertain examples and high-conﬁdence examples are characterized by the entropy of the
conditional probability distribution given by the
current model. Let K be the sampling size and the
unlabeled dataset consisting of candidate record
pairs be DU = {xi}N
i=1. Denote the probability
that record pair xi is a match according to the
current model by p(xi).
Then, the conditional
entropy of the pair H (xi) is computed by:
−p(xi) log p(xi) −(1 −p(xi)) log(1 −p(xi))
Uncertain examples and high-conﬁdence examples are associated with high and low entropy.
Given this notion of uncertainty and high conﬁdence, one can simply select record pairs with top
K entropy as uncertain examples and those with
bottom K entropy as high-conﬁdence examples.
Namely, take
as sets of uncertain and high-conﬁdence examples
respectively. However, these simple criteria can
introduce an unintended bias toward a certain direction, resulting in unstable performance. For example, uncertain examples selected solely on the
basis of entropy can sometimes contain substantially more negative examples than positive ones,
leading the network to a solution with low recall.
To address this instability problem, we propose a
partition sampling mechanism. We ﬁrst partition
the unlabeled data DU into two subsets: D
DU, consisting of pairs that the model predicts as
matches and non-matches respectively. Namely,
U = {x ∈DU|p(x) ≥0.5}, DU = {x ∈
DU|p(x) < 0.5}.
Then, we pick top/bottom k = K/2 examples
from each subset with respect to entropy. Uncertain examples are now:
where the two criteria select likely false positives
and likely false negatives respectively.
false positives and likely false negatives are useful
for improving the precision and recall of ER
models . However, the deep ER
models do not have explicit features, and thus we
use entropy to identify the two types of examples
in contrast to the feature-based method used in
Qian et al. . High-conﬁdence examples are
identiﬁed by:
where the two criteria correspond to highconﬁdence positives and high-conﬁdence negatives respectively. These sampling criteria equally
partition uncertain examples and high-conﬁdence
examples into different categories. We will show
that the partition mechanism contributes to stable
and better performance in a later section.
Algorithm 1 Deep Transfer Active Learning
Unlabeled data DU, sampling size K, batch size B,
max. iteration number T, max. number of epochs I.
Denote the deep ER parameters and the set of
respectively.
Update(W, DL, B)
function that optimizes the negative log-likelihood of
the labeled data DL with batch size B. Set k = K/2.
1: Initialize W via transfer learning.
Initialize also
2: for t ∈{1, 2, ..., T} do
Select k likely false positives and k likely false
negatives from DU and remove them from DU.
Label those examples and add them to DL.
Select k high-conﬁdence positives and k highconﬁdence negatives from DU and add them with
positive and negative labels to DL.
for t ∈{1, 2, ..., I} do
W ←Update(W, DL, B)
Run deep ER model on DL with W and get the
if the F1 score improves then
13: end for
14: return W
High-conﬁdence examples prevent the network
from overﬁtting to selected uncertain examples
 .
Moreover, they can give
the DL model more labeled data without actual
manual effort. Note that we avoid using any entropy level thresholds to select examples, and instead ﬁx the number of examples.
In contrast,
the active learning framework for neural network
image recognition in Wang et al. uses
entropy thresholds.
Such thresholds necessitate
ﬁne-tuning for each target dataset: Wang et al.
 use different thresholds for different image recognition datasets. However, since we do
not have sufﬁcient labeled data for the target in
low-resource ER problems, the necessity of ﬁnetuning thresholds would undermine the applicability of the active learning framework.
DBLP-Scholar
Fodors-Zagats
restaurant
Zomato-Yelp
restaurant
Amazon-Google
Table 2: Post-blocking statistics of the ER datasets we
used. (attr denotes the number of attributes.)
Experiments
Experimental Setup
For all datasets, we ﬁrst conduct blocking to reduce the Cartesian product to a candidate set.
Then, we randomly split the candidate set into
training, development, and test data with a ratio of 3:1:1.
For the datasets used in Mudgal et al. (DBLP-ACM, DBLP-Scholar,
Fodors-Zagats, and Amazon-Google), we adopted
the same feature-based blocking strategies and
random splits to ensure comparability with the
state-of-the-art method. The candidate set of Cora
was obtained by randomly sampling 50,000 pairs
from the result of the jaccard similarity-based
blocking strategy described in Wang et al. .
The candidate set of Zomato-Yelp was taken from
Das et al. .3 All dataset statistics are given
in Table 2. For evaluation, we compute precision,
recall, and F1 score on the test sets. In the active learning experiments, we hold out the test sets
a priori and sample solely from the training data
to ensure fair comparison with non-active learning
methods. The sampling size K for active learning
is 20. As preprocessing, we tokenize with NLTK
 and lowercase all attribute values. For every conﬁguration, we run experiments
with 5 random initializations and report the average. Our DL models are all implemented using the
publicly available deepmatcher library.4
We establish baselines using a state-of-the-art
learning-based ER package, Magellan . We experimented with the following
6 learning algorithms: Decision Tree, SVM, Ran-
3We constructed Zomato-Yelp by merging Restaurants 1
and 2, which are available in Das et al. . Though the
two datasets share the same source, their schemas slightly
differ: Restaurants 1 has an address attribute that contains
zip code, while Restaurants 2 has a zip code attribute and
an address attribute. We put a null value for the zip code
attribute in Restaurants 1 and avoid merging errors.
4 
deepmatcher
1000 2000 3000 4000 5000 6000 7000
# Labeled Training examples
Deep Learning
Decision Tree
Random Forest
Naive Bayes
Logistic Regression
Linear Regression
Figure 2: Performance vs. data size (DBLP-ACM).
dom Forest, Naive Bayes, Logistic Regression,
and Linear Regression. We use the same feature
set as in Mudgal et al. . See the appendix
for extensive lists of features chosen.
Results and Discussions
Model Performance and Data Size. Seen in Fig.
2 is F1 performance of different models with varying data size on DBLP-ACM. The DL model improves dramatically as the data size increases and
achieves the best performance among the 7 models when 7000 training examples are available. In
contrast, the other models suffer much less from
data scarcity with an exception of Random Forest.
We observed similar patterns in DBLP-Scholar
and Cora. These results conﬁrm our hypothesis
that deep ER models are data-hungry and require
a lot of labeled data to perform well.
Transfer Learning. Table 3 shows results from
our transfer learning framework when used in isolation (i.e., without active learning, which we will
discuss shortly). Our dataset adaptation method
substantially ameliorates performance when the
target is DBLP-Scholar (from 41.03 to 53.84 F1
points) or Cora (from 38.3 to 43.13 F1 points)
and achieves the same level of performance on
DBLP-ACM. Transfer learning with our dataset
adaptation technique achieves a certain level of
performance without any target labels, but we
still observe high variance in performance (e.g.
6.21 standard deviation in DBLP-Scholar) and a
huge discrepancy between transfer learning and
training directly on the target dataset. To build a
reliable and stable ER model, a certain amount of
target labels may be necessary, which leads us to
apply our active learning framework.
Active Learning. Fig. 3 shows results from our
active learning as well as the 7 algorithms trained
on labeled examples of corresponding size that are
DBLP-Scholar
Train on Source
92.32±1.15
41.03±6.33
38.30±3.77
+Adaptation
92.31±1.36
53.84±6.21
43.13±3.62
Train on Target
98.45±0.22
92.94±0.47
98.68±0.26
Mudgal et al. 
Table 3: Transfer learning results (citation genre). We report standard deviations of the F1 scores. For each target
dataset, the source is given by the other two datasets (e.g., the source for DBLP-ACM is DBLP-Scholar and Cora.)
# Labeled Training examples
(a) DBLP-ACM
# Labeled Training examples
(b) DBLP-Scholar
# Labeled Training examples
Deep Transfer Active
Deep Active
Deep Learning
Decision Tree
Random Forest
Naive Bayes
Logistic Regression
Linear Regression
Figure 3: Low-resource performances on different datasets.
randomly sampled.5 Deep transfer active learning
(DTAL) initializes the network parameters by
transfer learning whereas deep active learning
(DAL) starts with a random initialization.
can observe that DTAL models remedy the data
scarcity problem as compared to DL models with
random sampling in all three datasets. DAL can
achieve competitive performance to DTAL at the
expense of faster convergence.
Seen in Table 4 is performance comparison
of different algorithms in low-resource and highresource settings.
(We only show the SVM results since SVM performed best in each conﬁguration among the 6 non-DL algorithms.) First,
deep transfer active learning (DTAL) achieves the
best performance in the low-resource setting of
each dataset. In particular, DTAL outperforms the
others to the greatest degree in Cora (97.68 F1
points) probably because Cora is the most complex dataset with 8 attributes in the schema. Non-
DL algorithms require many interaction features,
which lead to data sparsity. Deep active learning
(DAL) also outperforms SVM and yields comparable performance to DTAL. However, the standard deviations in performance of DAL are substantially higher than those of DTAL (e.g. 4.15
5We average the results over 5 random samplings.
vs. 0.33 in DBLP-ACM), suggesting that transfer
learning provides useful initializations for active
learning to achieve stable performance.
One can argue that DTAL performs best in the
low-resource scenario, but the other algorithms
can also boost their low-resource performance by
active learning. While there are many approaches
to active learning on feature-based (non-DL) ER
 ; Qian et al. ) that
yield strong performance under certain condition,
it requires further research to quantify how these
methods perform with varying datasets, genres,
and blocking functions. It should be noted, however, that in DBLP-Scholar and Cora, DTAL in
the low-resource setting even signiﬁcantly outperforms SVM (and the other 5 algorithms) in the
high-resource scenario. These results imply that
DTAL would signiﬁcantly outperform SVM with
active learning in the low-resource setting since
the performance with the full training data with labels serves as an upper bound. Moreover, we can
observe that DTAL with a limited amount of data
(less than 6% of training data in all datasets), performs comparably to DL models with full training
data. Therefore, we have demonstrated that a deep
ER system with our transfer and active learning
frameworks can provide a stable and reliable solu-
Train Size
97.89±0.33
95.35±4.15
93.40±2.61
96.97±0.69
98.45±0.22
98.35±0.14
89.54±0.39
88.76±0.76
83.33±1.26
85.36±0.32
92.94±0.47
DBLP-Scholar
88.56±0.46
97.68±0.39
97.05±0.64
84.35±4.25
87.66±3.15
98.68±0.26
95.39±0.31
Table 4: Low-resource (shaded) and high-resource (full
training data) performance comparison. DTAL, DAL,
and DL denote deep transfer active learning, deep active learning, and deep learning (random sampling).
tion to entity resolution with low annotation effort.
Other Genre Results. We present results from
the restaurant and software genres.6
Table 5 are results of transfer and active learning
from Zomato-Yelp to Fodors-Zagats.
to our extensive experiments in the citation
genre, the dataset adaptation technique facilitates
transfer learning signiﬁcantly, and only 100 active
learning labels are needed to achieve the same
performance as the model trained with all target
labels (894 labels).
Fig. 4 shows low-resource
performance in the software genre. The relative
performance among the 6 non-DL approaches
differs to a great degree as the best non-DL model
is now logistic regression, but deep active learning
outperforms the rest with 1200 labeled examples
(10.4% of training data). These results illustrate
that our low-resource frameworks are effective in
other genres as well.
Active Learning Sampling Strategies.
discussed in a previous section, we adopted highconﬁdence sampling and a partition mechanism
for our active learning. Here we analyze the effect
of the two methods. Table 6 shows deep transfer
active learning performance in DBLP-ACM with
varying sampling strategies. We can observe that
high-conﬁdence sampling and the partition mech-
6We intend to apply our approaches to more genres, but
unfortunately we lack large publicly available ER datasets in
other genres than citation. Applications to non-English languages are also of interest. We leave this for future.
Train on Src
11.76±6.84
+Adaptation
70.13±19.89
+100 active labels
100.00±0.00
Train on Tgt
100.00±0.00
Mudgal et al. 
Table 5: Transfer and active learning results in the
restaurant genre. The target and source datasets are
Fodors-Zagats and Zomato-Yelp respectively.
800 1000 1200 1400
# Labeled Training examples
Deep Active
Deep Learning
Decision Tree
Random Forest
Naive Bayes
Logistic Regression
Linear Regression
Figure 4: Low-resource performance (software genre).
anism contribute to high and stable performance
as well as good precision-recall balance. Notice
that there is a huge jump in recall by adding
partition while precision stays the same (row 4 to
row 3). This is due to the fact that the partition
mechanism succeeds in ﬁnding more false negatives. The breakdown of labeled examples (Table
7) shows that is indeed the case. It is noteworthy
that the partition mechanism lowers the ratio of
misclassiﬁed examples (FP+FN) in the labeled
sample set because partitioning encourages us to
choose likely false negatives more aggressively,
yet false negatives tend to be more challenging to
ﬁnd in entity resolution due to the skewness toward the negative . We observed
similar patterns in DBLP-Scholar and Cora.
Further Related Work
Transfer learning has proven successful in ﬁelds
such as computer vision and natural language
processing, where networks for a target task
is pretrained on a source task with plenty of
training data and language modeling to
ours has also proposed transfer learning on top
of the features from distributed representations,
but they focused on classical machine learning
classiﬁers (e.g., logistic regression, SVMs, decision trees, random forests) and they did not con-
Sampling Method
High-Conﬁdence
95.19±2.21
96.61±0.57
High-Conf.+Part.
97.73±0.43
Top K Entropy
92.07±9.73
Table 6: Low-resource performance (300 labeled examples) of different sampling strategies (DBLP-ACM).
Table 7: Breakdown of 300 labeled samples (uncertain
samples) from deep transfer active learning in DBLP-
ACM. Part, FP, TP, FN, and TN denote the partition
mechanism, false positives, true positives, false negatives, and true negatives respectively.
sider active learning. Their distributed representations are computed in a “bag-of-words” fashion,
which can make applications to textual attributes
more challenging . Moreover, their method breaks attribute boundaries for
tuple representations in contrast to our approach
that computes a similarity vector for each attribute
in an attribute-agnostic manner. In a complex ER
scenario, each entity record is represented by a
large number of attributes, and comparing tuples
as a single string can be infeasible. Other prior
work also proposed a transfer learning framework
for linear model-based learners in ER .
Conclusion
We presented transfer learning and active learning
frameworks for entity resolution with deep learning and demonstrated that our models can achieve
competitive, if not better, performance as compared to state-of-the-art learning-based methods
while only using an order of magnitude less labeled data. Although our transfer learning alone
did not sufﬁce to construct a reliable and stable entity resolution system, it contributed to faster convergence and stable performance when used together with active learning. These results serve
as further support for the claim that deep learning
can provide a uniﬁed data integration method for
downstream NLP tasks. Our frameworks of transfer and active learning for deep learning models
are potentially applicable to low-resource settings
beyond entity resolution.
Acknowledgments
We thank Sidharth Mudgal for assistance with
the DeepMatcher/Magellan libraries and replicating experiments. We also thank Vamsi Meduri,
Phoebe Mulcaire, and the anonymous reviewers
for their helpful feedback. JK was supported by
travel grants from the Masason Foundation fellowship.