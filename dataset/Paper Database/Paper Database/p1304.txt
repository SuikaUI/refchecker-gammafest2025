Springer Texts in Statistics
George Casella Stephen Fienberg Ingram Olkin
Springer Science+Business Media, LLC
Springer Texts in Statistics
Alfred: Elements of Statistics for the Life and Social Sciences
Berger: An Introduction to Probability and Stochastic Processes
Bilodeau and Brenner: Theory of Multivariate Statistics
Blom: Probability and Statistics: Theory and Applications
Brockwell and Davis: An Introduction to Times Series and Forecasting
Chow and Teicher: Probability Theory: Independence, Interchangeability,
Martingales, Third Edition
Christensen: Plane Answers to Complex Questions: The Theory of Linear
Models, Second Edition
Christensen: Linear Models for Multivariate, Time Series, and Spatial Data
Christensen: Log-Linear Models and Logistic Regression, Second Edition
Creighton: A First Course in Probability Models and Statistical Inference
Dean and Voss: Design and Analysis of Experiments
du Toit, Steyn, and Stump!" Graphical Exploratory Data Analysis
Durrett: Essentials of Stochastic Processes
Edwards: Introduction to Graphical Modelling
Finkelstein and Levin: Statistics for Lawyers
Flury: A First Course in Multivariate Statistics
Jobson: Applied Multivariate Data Analysis, Volume I: Regression and
Experimental Design
Jobson: Applied Multivariate Data Analysis, Volume II: Categorical and
Multivariate Methods
Kalbfleisch: Probability and Statistical Inference, Volume I: Probability,
Second Edition
Kalbfleisch: Probability and Statistical Inference, Volume II: Statistical
Inference, Second Edition
Karr: Probability
Keyfitz: Applied Mathematical Demography, Second Edition
Kiefer: Introduction to Statistical Inference
Kokoska and Nevison: Statistical Tables and Formulae
Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems
Lehmann: Elements of Large-Sample Theory
Lehmann: Testing Statistical Hypotheses, Second Edition
Lehmann and Casella: Theory of Point Estimation, Second Edition
Lindman: Analysis of Variance in Experimental Design
Lindsey: Applying Generalized Linear Models
Madansky: Prescriptions for Working Statisticians
McPherson: Statistics in Scientific Investigation: Its Basis, Application, and
Interpretation
Mueller: Basic Principles of Structural Equation Modeling
Nguyen and Rogers: Fundamentals of Mathematical Statistics: Volume I:
Probability for Statistics
Nguyen and Rogers: Fundamentals of Mathematical Statistics: Volume II:
Statistical Inference
(continued after index)
Christian P. Robert
George Casella
Monte Carlo
Statistical Methods
With 65 Figures
Christian P. Robert
CREST-INSEE
Laboratoire de Statistique
75675 Paris Cedex 14
Dept. de Mathematique
UFR des Sciences
Universite de Rouen
76821 Mont Saint Aignan cedex
Editorial Board
George Casella
Biometrics Unit
Cornell University
Ithaca, NY 14853-7801
George Casella
Biometrics Unit
Comell University
Ithaca, NY 14853-7801
Stephen Fienberg
Department of Statistics
Carnegie Mellon University
Pittsburgh, PA 15213
Library of Congress Cataloging-in-Publication Data
Robert, Christian P., 1961-
Monte Cario statistical methods / Christian P. Robert, George
(Springer texts in statistics)
Includes bib1iographica1 references (p.
) and index.
ISBN 978-1-4757-3073-9
ISBN 978-1-4757-3071-5 (eBook)
DOI 10.1007/978-1-4757-3071-5
Ingram Olkin
Department of Statistics
Stanford University
Stanford, CA 94305
1. Mathematica1 statistics.
2. Monte Carlo method.
1. Casella,
III. Series.
QA276.R575
519.5-dc21
Printed on acid-free paper.
AII rights reserved. This work may not be translated or copied in whole or in part without
the writtenpermission of the publisher Springer Science+Business Media, LLC.
except for brief excerpts in connection with reviews or scholarly
analysis. U se in connection with any form of information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or hereafter deve10ped is forbidden.
The use of general descriptive names, trade names, trademarks, etc., in this publication, even
if the former are not especially identified, is not to be taken as a sign that such names, as
understood by the Trade Marks and Merchandise Marks Act, may accordingly be used freely
Production managed by Alian Abrams; manufacturing supervised by Jeffrey Taub.
Photocomposed pages prepared from the authors' TeX files.
ISBN 978-1-4757-3073-9
SPIN 10707599
© 1999 Springer Science+Business Media New York
Originally published by Springer-Verlag New York, Inc. in 1999
Sof'tcover reprint of the hardcover 1st edition 1999
In memory of our colleague and dear friend, Costas Goutis 
To Benjamin, Joachim, Rachel and Samh, our favorite mndom
genemtors!
Monte Carlo statistical methods, particularly those based on Markov chains,
have now matured to be part of the standard set of techniques used by
statisticians. This book is intended to bring these techniques into the classroom, being (we hope) a self-contained logical development of the subject,
with all concepts being explained in detail, and all theorems, etc. having
detailed proofs. There is also an abundance of examples and problems, relating the concepts with statistical practice and enhancing primarily the
application of simulation techniques to statistical problems of various difficulties.
This is a textbook intended for a second-year graduate course. We do not
assume that the reader has any familiarity with Monte Carlo techniques
(such as random variable generation) or with any Markov chain theory.
We do assume that the reader has had a first course in statistical theory
at the level of Statistical Inference by Casella and Berger . Unfortunately, a few times throughout the book a somewhat more advanced notion is needed. We have kept these incidents to a minimum and have posted
warnings when they occur. While this is a book on simulation, whose actual
implementation must be processed through a computer, no requirement is
made on programming skills or computing abilities: algorithms are presented in a program-like format but in plain text rather than in a specific
programming language. (Most of the examples in the book were actually
implemented in C, with the S-Plus graphical interface.)
Chapters 1-3 are introductory. Chapter 1 is a review of various statistical methodologies and of corresponding computational problems. Chapters 2 and 3 contain the basics of random variable generation and Monte
Carlo integration. Chapter 4, which is certainly the most theoretical in the
book, is an introduction to Markov chains, covering enough theory to allow the reader to understand the workings and evaluate the performance
of Markov chain Monte Carlo (MCMC) methods. Section 4.1 is provided
for the reader who already is familiar with Markov chains, but needs a rex
fresher, especially in the application of Markov chain theory to Monte Carlo
calculations. Chapter 5 covers optimization and provides the first application of Markov chains to simulation methods. Chapters 6 and 7 cover the
heart of MCMC methodology, the Metropolis-Hastings algorithm and the
Gibbs sampler. Finally, Chapter 8 presents the state-of-the-art methods for
monitoring convergence of the MCMC methods and Chapter 9 shows how
these methods apply to some statistical settings which cannot be processed
otherwise, namely the missing data models.
Each chapter concludes with a section of notes that serve to enhance the
discussion in the chapters, describe alternate or more advanced methods,
and point the reader to further work that has been done, as well as to
current research trends in the area. The level and rigor of the notes are
variable, with some of the material being advanced.
The book can be used at several levels and can be presented in several
ways. For example, Chapters 1-3 and most of Chapter 5 cover standard simulation theory, and hence serve as a basic introduction to this topic. Chapters 6-9 are totally concerned with MCMC methodology. A one-semester
course, assuming no familiarity with random variable generation or Markov
chain theory could be based on Chapters 1-7, with some illustrations from
Chapters 8 and 9. For instance, after a quick introduction with examples
from Chapter 1 or §3.1, and a description of Accept-Reject techniques of
§2.3, the course could cover Monte Carlo integration (§3.2, §3.3 [except
§3.3.3], §3.4, §3.7), Markov chain theory through either §4.1 or §4.2-§4.8
(while adapting the depth to the mathematical level of the audience), mention stochastic optimization via §5.3, and describe Metropolis-Hastings and
Gibbs algorithms as in Chapters 6 and 7 (except §6.5, §7.1.5, and §7.2.4).
Dpending on the time left, the course could conclude with some diagnostic
methods of Chapter 8 (for instance, those implemented in CODA) and/or
some models of Chapter 9 (for instance, the mixture models of §9.3 and
§9.4). Alternatively, a more advanced audience could cover Chapter 4 and
Chapters 6-9 in one semester and have a thorough introduction to MCMC
theory and methods.
Much of the material in this book had its original incarnation as the
French monograph Methodes de Monte Carlo par Chaines de Markov by
Christian Robert , which has been tested for several years on graduate audiences (in France, Quebec, and even Norway).
Nonetheless, it constitutes a major revision of the French text, with the inclusion of problems, notes, and the updating of current techniques, to keep
up with the advances that took place in the past 2 years (like Langevin
diffusions, perfect sampling, and various types of monitoring).
Throughout the preparation of this book, and its predecessor, we were
fortunate to have colleagues who provided help. Sometimes this was in
the form of conversations or references (thanks to Steve Brooks and Sid
Chib!), and a few people actually agreed to read through the manuscript.
Our colleague and friend, Costas Goutis, provided many helpful comments
and criticisms, mostly on the French version, but these are still felt in this
version. We are also grateful to Brad Carlin, Dan Fink, Jim Hobert, and
Krishanu Maulik for detailed reading of parts of the manuscript, to our
historian Walter Piegorsch, and to Richard Tweedie, who taught from the
manuscript and provided many helpful suggestions, and to his students,
Alex Trindade, Sandy Thompson, Nicole Benton, Sarah Streett, and Sue
Taylor. Virginie Brai"do, Arnaud Doucet, Jean-Louis Foulley, Ana Justel,
Anne Philippe, Sandrine Micaleff, and Randall Douc pointed out typos
and mistakes in the French version, but should not be held responsible for
those remaining! Part of Chapter 8 has a lot of common with a "reviewww"
written by Christian Robert with Chantal Guihenneuc-Jouyaux and Kerrie
Mengersen for the Valencia Bayesian meeting (and the Internet!). The input
of the French working group "MC Cube," whose focus is on convergence
diagnostics, can also be felt in several places of this book. Wally Gilks and
David Spiegelhalter granted us permission to use their graph (Figure 2.3.1)
and examples as Problems 7.44-7.55, for which we are grateful. Agostino
Nobile kindly provided the data on which Figures 7.3.2 and 7.3.2 are based.
Finally, Arnoldo Frigessi (from Roma) made the daring move of teaching
(in English) from the French version in Olso, Norway; not only providing us
with very helpful feedback but also contributing to making the European
Union more of a reality!
Christian P. Robert
George Casella
December 1998
List of Tables
List of Figures
1 Introduction
Statistical Models
Likelihood Methods
Bayesian Methods
1.4 Deterministic Numerical Methods
2 Random Variable Generation
Basic Methods
Introduction
2.1.2 The Kiss Generator
2.1.3 Beyond Uniform Distributions
Transformation Methods
2.3 Accept-Reject Methods
General Principle
2.3.2 Envelope Accept-Reject Methods
2.3.3 Log-Concave Densities
2.4 Problems
3 Monte Carlo Integration
Introduction
3.2 Classical Monte Carlo Integration
3.3 Importance Sampling
Principles
3.3.2 Finite Variance Estimators
3.3.3 Comparing Importance Sampling with Accept-Reject
3.4 Riemann Approximations
Laplace Approximations
3.6 The Saddlepoint Approximation
An Edgeworth Derivation
3.6.2 Tail Areas
3.7 Acceleration Methods
3.7.1 Antithetic Variables
3.7.2 Control Variates
3.7.3 Conditional Expectations
3.8 Problems
4 Markov Chains
Essentials for MCMC
4.2 Basic Notions
Irreducibility, Atoms, and Small Sets
4.3.1 Irreducibility
4.3.2 Atoms and Small Sets
4.3.3 Cycles and Aperiodicity
4.4 Transience and Recurrence
Classification of Irreducible Chains
4.4.2 Criteria for Recurrence
4.4.3 Harris Recurrence
4.5 Invariant Measures
4.6 Ergodicity and Convergence
4.6.1 Ergodicity
4.6.2 Geometric Convergence
4.6.3 Uniform Ergodicity
4.7 Limit Theorems
4.7.1 Ergodic Theorems
4.7.2 Central Limit Theorems
Covariance in Markov Chains
4.9 Problems
4.10 Notes
5 Monte Carlo Optimization
Introduction
5.2 Stochastic Exploration
A Basic Solution
5.2.2 Gradient Methods
5.2.3 Simulated Annealing
5.2.4 Prior Feedback
5.3 Stochastic Approximation
5.3.1 Missing Data Models and Demarginalization
5.3.2 Monte Carlo Approximation
5.3.3 The EM Algorithm
5.3.4 Monte Carlo EM
5.4 Problems
6 The Metropolis-Hastings Algorithm
Monte Carlo Methods Based on Markov Chains
The Metropolis-Hastings algorithm
Definition
Convergence Properties
6.3 A Collection of Metropolis-Hastings Algorithms
6.3.1 The Independent Case
6.3.2 Random Walks
6.3.3 ARMS: A General Metropolis-Hastings Algorithm
6.4 Optimization and Control
Optimizing the Acceptance Rate
Conditioning and Accelerations
6.5 Further Topics
Reversible Jumps
6.5.2 Langevin Algorithms
7 The Gibbs Sampler
General Principles
Definition
7.1.2 Completion
7.1.3 Convergence Properties
7.1.4 Gibbs Sampling and Metropolis-Hastings
7.1.5 The Hammersley-Clifford Theorem
7.1.6 Hierarchical Structures
The Two-Stage Gibbs Sampler
Dual Probability Structures
7.2.2 Reversible and Interleaving Chains
7.2.3 Monotone Covariance and Rao-Blackwellization
7.2.4 The Duality Principle
7.3 Hybrid Gibbs Samplers
Comparison with Metropolis-Hastings Algorithms
7.3.2 Mixtures and Cycles
7.3.3 Metropolizing the Gibbs Sampler
7.3.4 Reparameterization
Improper Priors
8 Diagnosing Convergence
Stopping the Chain
Convergence Criteria
8.1.2 Multiple Chains
8.1.3 Conclusions
8.2 Monitoring Convergence to the Stationary Distribution
Graphical Methods
8.2.2 Nonparametric Tests of Stationarity
8.2.3 Renewal Methods
8.2.4 Distance Evaluations
8.3 Monitoring Convergence of Averages
8.3.1 Graphical Methods
8.3.2 Multiple Estimates
8.3.3 Renewal Theory
8.3.4 Within and Between Variances
8.4 Simultaneous Monitoring
8.4.1 Binary Control
8.4.2 Valid Discretization
8.5 Problems
9 Implementation in Missing Data Models
Introduction
9.2 First Examples
9.2.1 Discrete Data Models
9.2.2 Data Missing at Random
9.3 Finite Mixtures of Distributions
9.4 A Reparameterization of Mixtures
9.5 Extensions
9.5.1 Hidden Markov Chains
9.5.2 Changepoint models
9.5.3 Stochastic Volatility
9.6 Problems
A Probability Distributions
B Notation
B.1 Mathematical
B.2 Probability
B.3 Distributions
B.4 Markov Chains
B.5 Statistics
B.6 Algorithms
C References
Subject Index
Author Index
List of Tables
1.6.1 Some conjugate families
3.2.1 Evaluation of some normal quantiles
3.3.1 Comparison of instrumental distributions
3.3.2 Comparison between Monte Carlo and importance sampling
estimators
3.5.1 Laplace approximation of a Gamma integral
3.6.1 Saddlepoint approximation of a noncentral chi squared
5.2.1 Stochastic gradient runs
5.2.2 Simulated annealing runs
5.2.3 Sequence of Bayes estimators for the gamma distribution
5.2.4 Average grades of first year students
5.2.5 Maximum likelihood estimates of mean grades
5.3.1 Estimation result for the factor ARCH model (5.3.10)
5.4.1 Tree swallow movements
5.4.2 Selected batting average data
6.3.1 Monte Carlo saddlepoint approximation of a noncentral chi
squared integral
6.3.2 Estimators of the mean and the variance of a normal
distribution using a random walk Metropolis-Hastings
6.4.1 Estimation in the inverse Gaussian distribution IN(fh'(h)
by the Metropolis-Hastings algorithm
6.4.2 Decrease of squared error risk associated with Rao-
Blackwellization
6.4.3 Improvement in quadratic risk from Rao-Blackwellization
6.6.1 Braking distances.
6.6.2 Galaxy data
6.7.1 Performance of the Metropolis-Hastings algorithm [A.28]
7.1.1 Convergence of estimated quantiles simulated from the slice
sampler [A.33]
LIST OF TABLES
7.1.2 Failures of pumps in a nuclear plant
7.2.1 Frequencies of passage for 360 consecutive observations.
7.3.1 Interquantile ranges for the Gibbs sampling and the modification of Liu
7.5.1 Occurrences of clinical mastitis in dairy herds
8.3.1 Estimation of the asymptotic variance 'Y~ for h( B) = Band
renewal control
8.3.2 Estimators of 'Y~ for h(x) = x, obtained by renewal in io.
8.3.3 Approximations by Gibbs sampling of posterior expectations
and evaluation of variances by the renewal method
8.4.1 Minimum test sample sizes for the diagnostic of Raftery and
8.4.2 Parameters of the binary control method for three different
parameters of the grouped multinomial model
8.4.3 Evolution of initializing and convergence times
9.2.1 Observation of two characteristics of the habitat of 164
9.2.2 Average incomes and numbers of responses/nonresponses
to a survey on the income by age, sex, and marital status
9.6.1 Independent observations of Z = (X, Y) rv N2(0,~) with
missing data
9.6.2 Yearly number of mining accidents in England, 1851 - 1962 445
List of Figures
1.2.1 Cauchy likelihood
2.1.1 Plot of chaotic pairs
2.1.2 Representation of the line y = 69069x mod 1 by uniform
2.1.3 Plots of pairs from the Kiss generator
2.3.1 Lower and upper envelopes of a log-concave density
3.2.1 Convergence of the Bayes estimator of 110112
3.2.2 Empirical cdf's of log-likelihoods
3.3.1 Approximations squared error risks, exponential and lognormal 83
3.3.2 Convergence of three estimators of IEj [IX/(1 - XW/2]
3.3.3 Convergence of four estimators of IE/[X5][X~2.1]
3.3.4 Convergence of four estimators of IE/[h3 (X)]
3.3.5 Convergence of four estimators of IE/[h5(X)]
3.3.6 Convergence of estimators of IE[X/(1 + X)]
3.3.7 Convergence of the estimators of IE[X/(1 + X)]
3.4.1 Convergence of the estimators of IE[X log(X)]
3.4.2 Convergence of estimators of IEy[(1 + eX)Ix<o]
3.4.3 Convergence of estimators of IEy[(1 + eX)Ix::;o]
3.6.1 Student's t saddlepoint approximation
3.7.1 Approximate risks of truncated James-Stein estimators
3.7.2 Comparison of an antithetic and standard iid estimate
3.7.3 Convergence of estimators of IE[exp( _X2)]
3.7.4 Comparisons of an Accept-Reject and importance sampling
3.9.1 Graphs of the variance coefficients
5.2.1 Representation of the function hex, y) of Example 5.2.1
5.2.2 Stochastic gradient paths
5.2.3 Simulated annealing sequence
5.3.1 Estimated slices of the log-likelihood ratio for the factor
ARCH model (5.3.10)
LIST OF FIGURES
6.3.1 Convergence of Accept-Reject and Metropolis-Hastings
estimators for the algorithms [A.26] and [A.27]
6.3.2 Convergence of Accept-Reject and Metropolis-Hastings
estimators for the algorithms [A.26] and [A.27]
6.3.3 Histograms of three samples produced by the
algorithm [A.28]
6.3.4 90% confidence envelopes of the means produced by the
random walk Metropolis-Hastings algorithm [A.24]
6.5.1 Linear regression with reversible jumps
6.5.2 Convergence of the empirical averages for the Langevin
random walk and stochastic gradient Metropolis-Hastings
algorithms
6.5.3 Convergence of the empirical averages for the Langevin
Metropolis-Hastings and iid simulation algorithms
7.1.1 Comparison ofthe Box-Muller algorithm and the
slice sampler
7.2.1 Evolution of the estimator Orb of [A.35]
7.3.1 Successive moves of a Gibbs chain
7.3.2 Gibbs chain for the probit model
7.3.3 Hybrid chain for the probit model
7.3.4 Comparison of the Gibbs sampling and of the modification
7.4.1 Iterations of the chains of a divergent Gibbs sampling
7.4.2 Evolution of ({J(t») for a random effects model
7.6.1 Cross-representation of a quasi-random sequence
8.2.1 Witch's hat distribution
8.2.2 Evolution of the chain (O~t») around the mode of a witch's
hat distribution
8.2.3 Plot of successive Kolmogorov-Smirnov statistics
8.2.4 Plot of renewal probabilities for the pump failure data
8.2.5 Stationarity of the indicator Jt for m = 50 parallel chains
converging to the witch's hat distribution
8.2.6 Evolutions of convergence indicators for the model of Gaver
and O'Muircheartaigh
8.2.7 Convergence of empirical averages for the algorithm [[A.30]]
and the Metropolis-Hastings algorithm
8.3.1 Evolution of the D; criterion for the chain produced by
8.3.2 Evolution of the CUSUM criterion D; for the chain produced
by algorithm [[A.43]]
8.3.3 Comparison of the density (8.3.6) and of the histogram from
a sample of 20, 000 points simulated by Gibbs sampling
8.3.4 Convergence of four estimators of the expectation under
LIST OF FIGURES
8.3.5 Convergence of four estimators of IE[(X(t))O.8] after elimination of the first 200,000 iterations
8.3.6 Evolutions of RT and WT for the posterior
distribution (8.3.6)
8.3.7 Evolutions of RT and WT for the witch's hat distribution
8.4.1 Convergence of the mean et for a chain generated
from [A.30]
8.4.2 Discretization of a continuous Markov chain, based on three
small sets
9.4.1 Evolution of the estimation of the density (9.4.1)
9.4.2 Convergence of estimators of the parameters of the mixture
9.5.1 Simulated sample of a switching AR model
9.5.2 The number of moves of a lamb fetus during 240 successive
5-second periods
9.5.3 Simulated sample of a stochastic volatility process
9.5.4 Gelman and Rubin's shrink factors for a stochastic
volatility model
9.5.5 Geweke's convergence indicators for a stochastic
volatility model
9.5.6 Successive values of the minimum p-value evaluating the
stationarity of a sample
9.5.7 Allocation map and average versus true allocation for a
stochastic volatility model