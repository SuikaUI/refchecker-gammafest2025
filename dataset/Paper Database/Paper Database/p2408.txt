A Survey on Network Embedding
Peng Cui1, Xiao Wang1, Jian Pei2, Wenwu Zhu1
1Department of Computer Science and Technology, Tsinghua University, China
2School of Computing Science, Simon Fraser University, Canada
 , ,
 , 
Network embedding assigns nodes in a network to lowdimensional representations and effectively preserves the
network structure. Recently, a signiﬁcant amount of progresses have been made toward this emerging network analysis paradigm. In this survey, we focus on categorizing and
then reviewing the current development on network embedding methods, and point out its future research directions.
We ﬁrst summarize the motivation of network embedding.
We discuss the classical graph embedding algorithms and
their relationship with network embedding. Afterwards and
primarily, we provide a comprehensive overview of a large
number of network embedding methods in a systematic manner, covering the structure- and property-preserving network
embedding methods, the network embedding methods with
side information and the advanced information preserving
network embedding methods. Moreover, several evaluation
approaches for network embedding and some useful online
resources, including the network data sets and softwares, are
reviewed, too. Finally, we discuss the framework of exploiting these network embedding methods to build an effective
system and point out some potential future directions.
Introduction
Many complex systems take the form of networks, such as
social networks, biological networks, and information networks. It is well recognized that network data is often sophisticated and thus is challenging to deal with. To process
network data effectively, the ﬁrst critical challenge is to ﬁnd
effective network data representation, that is, how to represent networks concisely so that advanced analytic tasks,
such as pattern discovery, analysis and prediction, can be
conducted efﬁciently in both time and space.
Traditionally, we usually represent a network as a graph
G = ⟨V, E⟩, where V is a vertex set representing the nodes
in a network, and E is an edge set representing the relationships among the nodes. For large networks, such as those
with billions of nodes, the traditional network representation
poses several challenges to network processing and analysis.
• High computational complexity. The nodes in a network
are related to each other to a certain degree, encoded by
the edge set E in the traditional network representation.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
These relationships cause most of the network processing or analysis algorithms either iterative or combinatorial computation steps, which result in high computational
complexity. For example, a popular way is to use the
shortest or average path length between two nodes to represent their distance. To compute such a distance using the
traditional network representation, we have to enumerate
many possible paths between two nodes, which is in nature a combinatorial problem. As another example, many
studies assume that a node with links to important nodes
tends to be important, and vice versa. In order to evaluate the importance of a node using the traditional network
representation, we have to iteratively conduct a stochastic
node traversal process until reaching a convergence. Such
methods using the traditional network representation result in high computational complexity that prevents them
from being applicable to large-scale real-world networks.
• Low parallelizability. Parallel and distributed computing is de facto to process and analyze large-scale data.
Network data represented in the traditional way, however, casts severe difﬁculties to design and implementation of parallel and distributed algorithms. The bottleneck
is that nodes in a network are coupled to each other explicitly reﬂected by E. Thus, distributing different nodes
in different shards or servers often causes demandingly
high communication cost among servers, and holds back
speed-up ratio. Although some limited progress is made
on graph parallelization by subtly segmenting large-scale
graphs (Staudt, Sazonovs, and Meyerhenke ), the luck of
these methods heavily depends on the topological characteristics of the underlying graphs.
• Inapplicability of machine learning methods. Recently,
machine learning methods, especially deep learning, are
very powerful in many areas. These methods provide standard, general and effective solutions to a broad range of
problems. For network data represented in the traditional
way, however, most of the off-the-shelf machine learning
methods may not applicable. Those methods usually assume that data samples can be represented by independent vectors in a vector space, while the samples in network data (i.e., the nodes) are dependant to each other
to some degree determined by E. Although we can simply represent a node by its corresponding row vector in
the adjacency matrix of the network, the extremely high
dimensionality of such a representation in a large graph
with many nodes makes the in sequel network processing
and analysis difﬁcult.
The traditional network representation has become a
bottleneck in large-scale network processing and analysis
nowadays. Representing the relationships explicitly using a
set of edges in the traditional representation is the upmost
To tackle the challenge, substantial effort has been committed to develop novel network embedding, i.e., learning
low-dimensional vector representations for network nodes.
In the network embedding space, the relationships among
the nodes, which were originally represented by edges or
other high-order topological measures in graphs, is captured
by the distances between nodes in the vector space, and the
topological and structural characteristics of a node are encoded into its embedding vector. An example is shown in
Fig. 1. After embedding the karate club network into a twodimensional space, the similar nodes marked by the same
color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the
two-dimensional embedding space.
Network embedding, as a promising way of network
representation, is capable of supporting subsequent network processing and analysis tasks such as node classiﬁcation ,
node clustering , network visualization and link prediction . If this goal is fulﬁlled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2. The traditional topology based network representation usually directly uses the observed adjacency matrix, which may contain noise or redundant information. The embedding based
representation ﬁrst aims to learn the dense and continuous
representations of nodes in a low dimensional space, so that
the noise or redundant information can be reduced and the
intrinsic structure information can be preserved. As each
node is represented by a vector containing its information
of interest, many iterative or combinatorial problems in network analysis can be tackled by computing mapping functions, distance metrics or operations on the embedding vectors, and thus avoid high complexity. As the nodes are not
coupling any more, it is convenient to apply main-stream
parallel computing solutions for large-scale network analysis. Furthermore, network embedding can open the opportunities for network analysis to be beneﬁted from the rich
literature of machine learning. Many off-the-shelf machine
learning methods such as deep learning models can be directly applied to solve network problems.
In order to make the embedding space well support network analysis tasks, there are two goals for network embedding. First, the original network can be reconstructed from
the learned embedding space. It requires that, if there is an
edge or relationship between two nodes, then the distance
of these two nodes in the embedding space should be rel-
(a) Input: karate network
(b) Output: representations
Figure 1: An example of network embedding on a karate
network. Images are extracted from DeepWalk .
atively small. In this way, the network relationships can be
well preserved. Second, the learned embedding space can
effectively support network inference, such as predicting unseen links, identifying important nodes, and inferring node
labels. It should be noted that an embedding space with only
the goal of network reconstruction is not sufﬁcient for network inference. Taking the link prediction problem as an example, if we only consider the goal of network reconstruction, the embedding vectors learned by SVD tend to ﬁt all
the observed links and zero values in the adjacency matrix,
which may lead to overﬁtting and cannot infer unseen links.
In this paper, we survey the state-of-the-art works on network embedding and point out future research directions. In
Section 2, we ﬁrst categorize network embedding methods
according to the types of information preserved in embedding, and summarize the commonly used models. We brieﬂy
review the traditional graph embedding methods and discuss the difference of these methods with the recent network
embedding methods in Section 3. Then, in Sections 4, 5
and 6, we respectively review the methods on structure and
property preserving network embedding, network embedding with side information, as well as advanced information
preserving network embedding. In Section 7, we present a
few evaluation scenarios and some online resources, including the data sets and codes, for network embedding. We conclude and discuss a series of possible future directions in
Section 8.
Categorization and The Models
To support network inference, more information beyond
nodes and links needs to be preserved in embedding space.
Figure 2: A comparison between network topology based network analysis and network embedding based network analysis.
Most research works on network embedding develop along
this line in recent years. There are multiple ways to categorize them. In this paper, according to the types of information that are preserved in network embedding, we categorize the existing methods into three categories, that is, (1)
network structure and properties preserving network embedding, (2) network embedding with side information and (3)
advanced information preserving network embedding.
The Categorization of Network Embedding
As mentioned before, network embedding usually has two
goals, i.e., network reconstruction and network inference.
The traditional graph embedding methods, mainly focusing
on network reconstruction, has been widely studied. We will
brieﬂy review those methods in Section 3. Fu and Ma present a more detailed survey. In this paper, we focus on the recently proposed network embedding
methods aiming to address the goal of network inference.
The categorization structure of the related works is shown in
Structure and property preserving network embedding
Among all the information encoded in a network, network
structures and properties are two crucial factors that largely
affect network inference. Consider a network with only
topology information. Many network analysis tasks, such
as identifying important nodes and predicting unseen links,
can be conducted in the original network space. However, as
mentioned before, directly conducting these tasks based on
network topology has a series of problems, and thus poses a
question that whether we can learn a network embedding
space purely based on the network topology information,
such that these tasks can be well supported in this low dimensional space. Motivated by this, attempts are proposed
to preserve rich structural information into network embedding, from nodes and links to neighborhood structure , highorder proximities of nodes ,
and community structures . All these
types of structural information have been demonstrated useful and necessary in various network analysis tasks. Besides
this structural information, network properties in the original network space are not ignorable in modeling the formation and evolution of networks. To name a few, network
transitivity (i.e. triangle closure) is the driving force of link
formation in networks , and structural
balance property plays an important role in the evolution of
signed networks . Preserving
these properties in a network embedding space is, however,
challenging due to the inhomogeneity between the network
space and the embedding vector space. Some recent studies
begin to look into this problem and demonstrate the possibility of aligning these two spaces at the property level .
Network Embedding with Side Information
network topology, some types of networks are accompanied with rich side information, such as node content or
labels in information networks , node and
edge attributes in social networks , as
well as node types in heterogeneous networks . Side information provides useful clues for characterizing relationships among network nodes, and thus is
helpful in learning embedding vector spaces. In the cases
where the network topology is relatively sparse, the importance of the side information as complementary information sources is even more substantial. Methodologically,
the main challenge is how to integrate and balance the
topological and side information in network embedding.
Some multimodal and multisource fusion techniques are explored in this line of research .
Advanced Information Preserving Network Embedding
In the previous two categories, most methods learn network
embedding in an unsupervised manner. That is, we only take
the network structure, properties, and side information into
account, and try to learn an embedding space to preserve
the information. In this way, the learned embedding space
Figure 3: An overview of different settings of network embedding.
is general and, hopefully, able to support various network
applications. If we regard network embedding as a way of
network representation learning, the formation of the representation space can be further optimized and conﬁned towards different target problems. Realizing this idea leads
to supervised or pseudo supervised information (i.e. the advanced information) in the target scenarios. Directly designing a framework of representation learning for a particular
target scenario is also known as an end-to-end solution , where high-quality supervised information is exploited to learn the latent representation space from scratch.
End-to-end solutions have demonstrated their advantages in
some ﬁelds, such as computer vision and
natural language processing (NLP) . Similar ideas are also feasible for network applications. Taking
the network node classiﬁcation problem as an example, if we
have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent
middle layer, and the resulted network embedding is speciﬁc
for node classiﬁcation. Some recent works demonstrate the
feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction .
In general, network structures and properties are the fundamental factors that need to be considered in network embedding. Meanwhile, side information on nodes and links, as
well as advanced information from target problem is helpful
to enable the learned network embedding work well in real
applications.
Commonly Used Models in Network Embedding
To transform networks from original network space to embedding space, different models can be adopted to incorporate different types of information or address different goals.
The commonly used models include matrix factorization,
random walk, deep neural networks and their variations.
Matrix Factorization
An adjacency matrix is commonly
used to represent the topology of a network, where each column and each row represent a node, and the matrix entries
indicate the relationships among nodes. We can simply use a
row vector or column vector as the vector representation of a
node, but the formed representation space is N-dimensional,
where N is the total number of nodes. Network embedding,
aiming to learn a low-dimensional vector space for a network, is eventually to ﬁnd a low-rank space to represent a
network, in contrast with the N-dimensional space. In this
sense, matrix factorization methods, with the same goal of
learning low-rank space for the original matrix, can naturally
be applied to solve this problem. In the series of matrix factorization models, Singular Value Decomposition (SVD) is
commonly used in network embedding due to its optimality
for low-rank approximation . Non-negative
matrix factorization is often used because of its advantages
as an additive model .
Random Walk
As mentioned before, preserving network
structure is a fundamental requirement for network embedding. Neighborhood structure, describing the local structural
characteristics of a node, is important for network embedding. Although the adjacency vector of a node encodes the
ﬁrst-order neighborhood structure of a node, it is usually
a sparse, discrete, and high-dimensional vector due to the
nature of sparseness in large-scale networks. Such a representation is not friendly to subsequent applications. In the
ﬁeld of natural language processing (NLP), the word representation also suffers from similar drawbacks. The development of Word2Vector signiﬁcantly
improves the effectiveness of word representation by transforming sparse, discrete and high-dimensional vectors into
dense, continuous and low-dimensional vectors. The intuition of Word2Vector is that a word vector should be able
to reconstruct the vectors of its neighborhood words which
are deﬁned by co-occurence rate. Some methods in network
embedding borrow these ideas. The key problem is how to
deﬁne “neighborhood” in networks.
To make analogy with Word2Vector, random walk models
are exploited to generate random paths over a network. By
regarding a node as a word, we can regard a random path as
a sentence, and the node neighborhood can be identiﬁed by
co-occurence rate as in Word2Vector. Some representative
methods include DeepWalk and Node2Vec .
Deep Neural Networks
By deﬁnition, network embedding is to transform the original network space into a lowdimensional vector space. The intrinsic problem is to learn a
mapping function between these two spaces. Some methods,
like matrix factorization, assume the mapping function to be
linear. However, the formation process of a network is complicated and highly nonlinear, thus a linear function may not
be adequate to map the original network to an embedding
If seeking for an effective non-linear function learning
model, deep neural networks are certainly useful options because of their huge successes in other ﬁelds. The key challenges are how to make deep models ﬁt network data, and
how to impose network structure and property-level constraints on deep models. Some representative methods, such
as SDNE , SDAE , and SiNE , propose deep
learning models for network embedding to address these
challenges. At the same time, deep neural networks are also
well known for their advantages in providing end-to-end solutions. Therefore, in the problems where advanced information is available, it is natural to exploit deep models to
come up with an end-to-end network embedding solution.
For instance, some deep model based end-to-end solutions
are proposed for cascade prediction and network alignment .
The network embedding models are not limited to those
mentioned in this subsection. Moreover, the three kinds of
models are not mutually exclusive, and their combinations
are possible to make new solutions. More models and details
will be discussed in later sections.
Network Embedding v.s. Graph
The goal of graph embedding is similar as network embedding, that is, to embed a graph into a low-dimensional vector
space . There is a rich literature in graph embedding. Fu and Ma provide a thorough
review on the traditional graph embedding methods. Here
we only present some representative and classical methods
on graph embedding, aiming to demonstrate the critical differences between graph embedding and the current network
embedding.
Representative Graph Embedding Methods
Graph embedding methods are originally studied as dimension reduction techniques. A graph is usually constructed
from a feature represented data set, like image data set.
Isomap ﬁrst
constructs a neighborhood graph G using connectivity algorithms such as K nearest neighbors (KNN), i.e., connecting
data entries i and j if i is one of the K nearest neighbors
of j. Then based on G, the shortest path dG
ij of entries i and
j in G can be computed. Consequently, for all the N data
entries in the data set, we have the matrix of graph distances
ij}. Finally, the classical multidimensional scaling (MDS) method is applied to DG to obtain the coordinate
vector ui for entry i, which aims to minimize the following
ij −∥ui −uj∥)2.
Indeed, Isomap learns the representation ui of entry i, which
approximately preserves the geodesic distances of the entry
pairs in the low-dimensional space.
The key problem of Isomap is its high complexity due
to the computing of pair-wise shortest pathes. Locally linear embedding (LLE) is proposed
to eliminate the need to estimate the pairwise distances between widely separated entries. LLE assumes that each entry
and its neighbors lie on or close to a locally linear patch of a
mainfold. To characterize the local geometry, each entry can
be reconstructed from its neighbors as follows:
where the weight Wij measures the contribution of the entry xj to the reconstruction of entry xi. Finally, in the
low-dimensional space, LLE constructs a neighborhoodpreserving mapping based on locally linear reconstruction
as follows:
By optimizing the above function, the low-dimensional representation matrix U, which preserves the neighborhood
structure, can be obtained.
Laplacian eigenmaps (LE) also
begins with constructing a graph using ǫ-neighborhoods or
K nearest neighbors. Then the heat kernel is utilized to choose the weight Wij of
nodes i and j in the graph. Finally, the representation ui of
node i can be obtained by minimizing the following function:
∥ui −uj∥2Wij = tr(UT LU),
where L = D −W is the Laplacian matrix, and D is the
diagonal matrix with Dii = P
j Wji. In addition, the constraint UT DU = I is introduced to avoid trivial solutions.
Furthermore, the locality preserving projection (LPP) , a linear approximation of the nonlinear
LE, is proposed. Also, it introduces a transformation matrix
A such that the representation ui of entry xi is ui = AT xi.
LPP computes the transformation matrix A ﬁrst, and ﬁnally
the representation ui can be obtained.
These methods are extended in the rich literature of graph
embedding by considering different characteristics of the
constructed graphs .
Figure 4: Overview of DeepWalk. Image extracted from .
Major Differences
Network embedding and graph embedding have substantial
differences in objective and assumptions. As mentioned before, network embedding has two goals, i.e. reconstructing
original networks and support network inference. The objective functions of graph embedding methods mainly target the goal of graph reconstruction. As discussed before,
the embedding space learned for network reconstruction is
not necessarily good for network inference. Therefore, graph
embedding can be regarded as a special case of network embedding, and the recent research progress on network embedding pays more attention to network inference.
Moreover, graph embedding mostly works on graphs constructed from feature represented data sets, where the proximity among nodes encoded by the edge weights are well de-
ﬁned in the original feature space. In contrast, network embedding mostly works on naturally formed networks, such
as social networks, biology networks, and e-commerce networks. In those networks, the proximities among nodes are
not explicitly or directly deﬁned. The deﬁnition of node
proximities depends on speciﬁc analytic tasks and application scenarios. Therefore, we have to incorporate rich information, such as network structures, properties, side information and advanced information, in network embedding to
facilitate different problems and applications.
In the rest of the paper, we mainly focus on the network
embedding methods with the goal of supporting network inference.
Structure and Property Preserving
Network Embedding
In essence, one basic requirement of network embedding
is to appropriately preserve network structures and capture properties of networks. Often, network structures include ﬁrst-order structure and higher-order structure, such as
second-order structure and community structure. Networks
with different types have different properties. For example,
directed networks have the asymmetric transitivity property.
The structural balance theory is widely applicable to signed
In this section, we review the representative methods of
structure preserving network embedding and property preserving network embedding.
Structure Preserving Network Embedding
Network structures can be categorized into different groups
that present at different granularities. The commonly exploited network structures in network embedding include
neighborhood structure, high-order node proximity and network communities.
Neighborhood Structures and High-order Node Proximity
DeepWalk is proposed for learning the representations of nodes in a network,
which is able to preserve the neighbor structures of nodes.
DeepWalk discovers that the distribution of nodes appearing
in short random walks is similar to the distribution of words
in natural language. Motivated by this observation, Skip-
Gram model , a widely used word
representation learning model, is adopted by DeepWalk to
learn the representations of nodes. Speciﬁcally, as shown in
Fig. 4, DeepWalk adopts a truncated random walk on a network to generate a set of walk sequences. For each walk
sequence s = {v1, v2, ..., vs}, following Skip-Gram, Deep-
Walk aims to maximize the probability of the neighbors of
node vi in this walk sequence as follows:
Pr({vi−w, ..., vi+w}\vi|φ(vi)) = Πi+w
j=i−w,j̸=iPr(vj|φ(vi)),
where w is the window size, φ(vi) is the current representation of vi and {vi−w, ..., vi+w}\vi is the local context nodes
of vi. Finally, hierarchical soft-max 
is used to efﬁciently infer the embeddings.
Node2vec demonstrates that
DeepWalk is not expressive enough to capture the diversity
of connectivity patterns in a network. Node2vec deﬁnes a
ﬂexible notion of a node’s network neighborhood and designs a second order random walk strategy to sample the
neighborhood nodes, which can smoothly interpolate between breadth-ﬁrst sampling (BFS) and depth-ﬁrst sampling
(DFS). Node2vec is able to learn the representations that
embed nodes with same network community closely, and to
learn representations where nodes sharing similar roles have
similar embeddings.
Figure 5: An example of the ﬁrst-order and second-order
structures in a network. Image extracted from is proposed for large scale network embedding, and can preserve the ﬁrst and second order
proximities. The ﬁrst order proximity is the observed pairwise proximity between two nodes, such as the observed
edge between nodes 6 and 7 in Fig. 5. The second order
proximity is determined by the similarity of the “contexts”
(neighbors) of two nodes. For example, the second order
similarity between nodes 5 and 6 can be obtained by their
neighborhoods 1, 2, 3, and 4 in Fig. 5. Both the ﬁrst order and second order proximities are important in measuring
the relationships between two nodes. The ﬁrst order proximity can be measured by the joint probability distribution
between two nodes vi and vj as
p1(vi, vj) =
1 + exp(−uT
The second order proximity is modeled by the probability of
the context node vj being generated by node vi, that is,
p2(vj|vi) =
The conditional distribution implies that nodes with similar
distributions over the contexts are similar to each other. By
minimizing the KL-divergence of the two distributions and
the empirical distributions respectively, the representations
of nodes that are able to preserve the ﬁrst and second order
proximities can be obtained.
Considering that LINE only preserves the ﬁrst-order and
second-order proximities, GraRep 
demonstrates that k-step (k > 2) proximities should also
be captured when constructing the global representations of
nodes. Given the adjacency matrix A, the k-step probability
transition matrix can be computed by Ak = A...A
element Ak
ij refers to the transition probability pk(j|i) from
a current node i to a context node j and the transition consists of k steps. Moreover, motivated by the Skip-Gram
model , the k-step loss function of
node i is deﬁned as
pk(j|i) log σ(uT
j ui))+λEj′∼pk(V )[log σ(−uT
where σ(x) = (1 + e−x)−1, pk(V ) is the distribution over
the nodes in the network and j′ is the node obtained from
negative sampling. Furthermore, GraRep reformulates the
loss function as the matrix factorization problem, for each
k-step loss function, SVD can be directly used to infer the
representations of nodes. By concentrating the representations learned from each function, the global representations
can be obtained.
Network Communities
Wang et al. 
propose a modularized nonnegative matrix factorization (M-
NMF) model for network embedding, which aims to preserve both the microscopic structure, i.e., the ﬁrst-order and
second-orderproximities of nodes, and the mesoscopic community structure . To preserve
the microscopic structure, they adopt the NMF model to factorize the pairwise node similarity
matrix and learn the representations of nodes. Meanwhile,
the community structure is detected by modularity maximization . Then, based on the assumption
that if the representation of a node is similar to that of a community, the node may have a high propensity to be in this
community, they introduce an auxiliary community representation matrix to bridge the representations of nodes with
the community structure. In this way, the learned representations of nodes are constrained by both the microscopic structure and community structure, which contains more structural information and becomes more discriminative.
The aforementioned methods mainly adopt the shallow models, consequently, the representation ability is limited. SDNE proposes a deep
model for network embedding, so as to address the high
non-linearity, structure-preserving, and sparsity issues. The
framework is shown in Fig. 6. Speciﬁcally, SDNE uses the
deep autoencoder with multiple non-linear layers to preserve
the neighbor structures of nodes. Given the input adjacency
nodes xi of node i, the hidden representations for each layer
can be obtained by
= σ(W(1)xi + b(1))
= σ(W(k)y(k−1)
+ b(k)), k = 2, ..., K.
Then the output representation ˆxi can be obtained by reversing the calculation process of encoder. To impose more
penalty to the reconstruction error of the non-zero elements
than that of zero elements, SDNE introduces the penalty
vector bi = {bij}n
j=1 (bij is larger than a threshold if there is
an edge between nodes i and j) and gives rise to the following function that can preserve the second-order proximity
∥(ˆxi −xi) ⊙bi∥2.
To preserve the ﬁrst-order proximity of nodes, the idea of
Laplacian eigenmaps is adopted.
By exploiting the ﬁrst-order and second-order proximities
jointly into the learning process, the representations of nodes
can be ﬁnally obtained.
Cao et al. propose a network
embedding method to capture the weighted graph structure
Figure 6: The framework of SDNE. Image extracted
from .
and represent nodes of non-linear structures. As shown in
Fig. 7, instead of adopting the previous sampling strategy
that needs to determine certain hyper parameters, they considers a random surﬁng model motivated by the PageRank
model. Based on this random surﬁng model, the representation of a node can be initiatively constructed by combining the weighted transition probability matrix. After that, the
PPMI matrix can be computed.
Finally, the stacked denoising autoencoders that partially corrupt the input data before taking the
training step are applied to learn the latent representations.
In order to make a general framework on network embedding, Chen et al. propose a network
embedding framework that uniﬁes some of the previous algorithms, such as LE, DeepWalk and Node2vec. The proposed framework, denoted by GEM-D[h(·), g(·), d(·, ·)], involves three important building blocks: h(·) is a node proximity function based on the adjacency matrix; g(·) is a warping function that warps the inner products of network embeddings; and d(·, ·) measures the differences between h
and g. Furthermore, they demonstrate that the high-order
proximity for h(·) and the exponential function for g(·) are
more important for a network embedding algorithm. Based
on these observations, they propose UltimateWalk=GEM-
D[Q(L), exp(x), dwf(·, ·)], where Q(L) is a ﬁnite-step
transition matrix, exp(x) is an exponential function and
dwf(·, ·) is the warped Frobenius norm.
In summary, many network embedding methods aim to
preserve the local structure of a node, including neighborhood structure, high-order proximity as well as community structure, in the latent low-dimensional space. Both linear and non-linear models are attempted, demonstrating the
large potential of deep models in network embedding.
Property Preserving Network Embedding
Among the rich network properties, the properties that are
crucial for network inference are the focus in property preserving network embedding. Speciﬁcally, most of the existing property preserving network embedding methods focus
on network transitivity in all types of networks and the structural balance property in signed networks.
Ou et al. aim to preserve the nontransitivity property via latent similarity components. The
non-transitivity property declares that, for nodes A, B and
C in a network where (A, B) and (B, C) are similar pairs,
(A, C) may be a dissimilar pair. For example, in a social
network, a student may connect with his classmates and his
family, while his classmates and family are probably very
different. To address this, they use a set of linear projection matrices to extract M hash tables, and thus, each pair
of nodes can have M similarities {Sm
m=1 based on those
hash tables. Then the ﬁnal similarity between two nodes can
be aggregated from {Sm
m=1. Finally they approximate the
aggregated similarity to the semantic similarity based on the
observation that if two nodes have a large semantic similarity, at least one of the similarities Sm
ij from the hash tables is
large, otherwise, all of the similarities are small.
Preserving the asymmetric transitivity property of directed network is considered by HOPE .
Asymmetric transitivity indicates that, if there is a directed
edge from node i to node j and a directed edge from j to v,
there is likely a directed edge from i to v, but not from v to i.
In order to measure this high-order proximity, HOPE summarizes four measurements in a general formulation, that is,
Katz Index , Rooted PageRank , Common Neighbors , and Adamic-Adar . With the high-order proximity, SVD can be directly
applied to obtain the low dimensional representations. Furthermore, the general formulation of high-order proximity
enables HOPE to transform the original SVD problem into a
generalized SVD problem , such
that the time complexity of HOPE is largely reduced, which
means HOPE is scalable for large scale networks.
SiNE is proposed for signed network embedding, which considers both positive and negative edges in a network. Due to the negative edges, the social theories on signed network, such as structural balance
theory , are
very different from the unsigned network. The structural balance theory demonstrates that users in a signed social network should be able to have their “friends” closer than their
“foes”. In other words, given a triplet (vi, vj, vk) with edges
eij = 1 and eik = −1, the similarity f(vi, vj) between
nodes vi and vj is larger than f(vi, vk). To model the structural balance phenomenon, a deep learning model consisting
of two deep networks with non-linear functions is designed
to learn the embeddings and preserve the network structure
property, which is consistent with the extended structural
balance theory. The framework is shown in Fig. 8.
The methods reviewed in this subsection demonstrate the
importance of maintaining network properties in network
embedding space, especially the properties that largely affect the evolution and formation of networks. The key challenge in is how to address the disparity and heterogeneity of
the original network space and the embedding vector space
at property level.
Figure 7: Overview of the method proposed by Cao et al. . Image extracted from .
Generally, most of the structure and property preserving
methods take high order proximities of nodes into account,
which demonstrate the importance of preserving high order structures in network embedding. The difference is the
strategy of obtaining the high order structures. Some methods implicitly preserve high-order structure by assuming a
generative mechanism from a node to its neighbors, while
some other methods realize this by explicitly approximating
high-order proximities in the embedding space. As topology
structures are the most notable characteristic of networks,
structure-preserving network methods embody a large part
of the literature. Comparatively, property preserving network embedding is a relatively new research topic and is
only studied lightly. As network properties usually drive the
formation and evolution of networks, it shows great potential for future research and applications.
Network Embedding with Side
Information
Besides network structures, side information is another important information source for network embedding. Side information in the context of network embedding can be divided into two categories: node content and types of nodes
and edges. In this section, we review the methods that take
side information into network embedding.
Network Embedding with Node Content
In some types of networks, like information networks, nodes
are acompanied with rich information, such as node labels, attributes or even semantic descriptions. How to combine them with the network topology in network embedding
arouses considerable research interests.
Tu et al. propose a semi-supervised
network embedding algorithm, MMDW, by leveraging labeling information of nodes. MMDW is also based on
the DeepWalk-derived matrix factorization. MMDW adopts
support vector machines (SVM) and incorporates the label information to ﬁnd an optimal classifying boundary. By optimizing the max-margin classiﬁer of
SVM and matrix factorization based DeepWalk simultaneously, the representations of nodes that have more discriminative ability can be learned.
Le et al. propose a generative model
for document network embedding, where the words associated with each documents and the relationships between
documents are both considered. For each node, they learn
its low-rank representation ui in a low dimensional vector
space, which can reconstruct the network structure. Also,
they learn the representation of nodes in the topic space
based on the Relational Topic Model (RTM) , where each topic z is associated with a probability distribution over words. To integrate the two aspects,
they associate each topic z with a representation ϕz in the
same low dimensional vector space and then have the following function:
2∥ui −ϕz∥2)
2∥ui −ϕz∥2).
Finally, in a uniﬁed generative process, the representations
of nodes U can be learned.
Besides network structures, Yang et al. 
propose TADW that takes the rich information (e.g., text)
associated with nodes into account when they learn the low
dimensional representations of nodes. They ﬁrst prove that
DeepWalk is equivalent to factorizing the matrix M whose
element Mij = log([ei(A + A2 + ... + At)]j/t), where A
is the adjacency matrix, t denotes the t steps in a random
walk and ei is a row vector where all entries are 0 except
the i-th entry is 1. Then, based on the DeepWalk-derived
Sun et al. . Image extracted from .
matrix factorization and motivated by the inductive matrix
completion , they incorporate
rich text information T into network embedding as follows:
W,H ∥M −WT HT∥2
Finally, they concatenate the optimal W and HT as the representations of nodes.
TADW suffers from high computational cost and the node
attributes just simply incorporated as unordered features lose
the much semantic information. Sun et al. 
consider the content as a special kind of nodes, and give rise
to an augmented network, as shown in Fig. 9. With this augmented network, they are able to model the node-node links
and node-content links in the latent vector space. They use
a logistic function to model the relationship in the new augmented network, and by combining with negative sampling,
they can learn the representations of nodes in a joint objective function, such that the representations can preserve
the network structure as well as the relationship between the
node and content.
Pan et al. propose a coupled deep model
that incorporates network structure, node attributes and node
labels into network embedding. The architecture of the proposed model is shown in Fig. 10. Consider a network with
N nodes {vi}i=1,...,N, where each node is associated with
a set of words {wi}, and some nodes may have |L| labels
{ci}. To exploit this information, they aim to maximize the
following function:
−b≤j≤b,j̸=0
log P(vi+j|vi)
log P(wj|vi) + α
log P(wj|ci),
where S is the random walks generated in the network and
b is the window size of sequence. Speciﬁcally, function
P, which captures the probability of observing contextual
nodes (or words) given the current node (or label), can be
computed using the soft-max function. In Eq. 13, the ﬁrst
term is also motivated by Skip-Gram, similar to DeepWalk,
Inter-Node Relationship Modeling
Figure 10: The framework of TriDNR . Image extracted from .
which models the network structure. The second term models the node-content correlations and the third term models
the label-node correspondences. As a result, the learned representations is enhanced by network structure, node content,
and node labels.
LANE is also proposed to incorporate the label information into the attributed network
embedding. Unlike the previous network embedding methods, LANE is mainly based on spectral techniques . LANE adopts the cosine similarity to construct the
corresponding afﬁnity matrices of the node attributes, network structure, and labels. Then, based on the corresponding
Laplacian matrices, LANE is able to map the three different
sources into different latent representations, respectively. In
order to build the relationship among those three representations, LANE projects all these latent representations into
a new common space by leveraging the variance of the projected matrix as the correlation metric. The learned representations of nodes are able to capture the structure proximities
as well as the correlations in the label informed attributed
Although different methods adopt different strategies to
integrate node content and network topology, they all assume that node content provides additional proximity information to constrain the representations of nodes.
Heterogeneous Information Network Embedding
Different from networks with node content, heterogeneous
networks consist of different types of nodes and links. How
to unify the heterogeneous types of nodes and links in network embedding is also an interesting and challenging problem.
Yann et al. propose
a heterogeneous social network embedding algorithm for
classifying nodes. They learn the representations of all types
of nodes in a common vector space, and perform the inference in this space. In particular, for the node ui with type
ti, they utilize a linear classiﬁcation function f ti
θ to predict
its label and adopt the hinge-loss function ∆to measure the
Figure 11: Overview of the method proposed by Chang et al. . Image extracted from .
loss with the true label yi:
θ (ui), yi),
where l is the number of labeled nodes. To preserve the local structures in the latent space, they impose the following
smoothness constraint, which enforces that two nodes i and
j will be close in the latent space if they have a large weight
Wij in the heterogeneous network:
Wij∥ui −uj∥2.
In this way, different types of nodes are mapped into a common latent space. The overall loss function combines the
classiﬁcation and regularization losses Eq. (14) and Eq. (15).
A stochastic gradient descent method is used here to learn
the representations of nodes in a heterogeneous network for
classifying.
Chang et al. propose a deep embedding algorithm for heterogeneous networks, whose nodes
have various types. The main goal of the heterogeneous network embedding is to learn the representations of nodes with
different types such that the heterogeneous network structure can be well preserved. As shown in Fig. 11, given a
heterogeneous network with two types of data (e.g., images
and texts), there are three types of edges, i.e., image-image,
text-text, and image-text. The nonlinear embeddings of images and texts are learned by a CNN model and the fully
connected layers, respectively. By cascading the extra linear embedding layer, the representations of images and texts
can be mapped to a common space. In the common space,
the similarities between data from different modalities can
be directly measured, so that if there is an edge in the original heterogeneous network, the pair of data has similar representations.
Huang and Mamoulis propose a meta path similarity preserving heterogeneous information network embedding algorithm. To model a particular
relationship, a meta path is a sequence of
object types with edge types in between. They develop a fast
dynamic programming approach to calculate the truncated
meta path based proximities, whose time complexity is linear to the size of the network. They adopt a similar strategy
as LINE to preserve the proximity in the
low dimensional space.
Xu et al. propose a network embedding method for coupled heterogeneous network. The coupled heterogeneous network consists of two different but related homogeneous networks. For each homogeneous network, they adopt the same function (Eq. (6)) as LINE to
model the relationships between nodes. Then the harmonious embedding matrix is introduced to measure the closeness between nodes of different networks. Because the internetwork edges are able to provide the complementary information in the presence of intra-network edges, the learned
embeddings of nodes also perform well on several tasks.
In the methods preserving side information, side information introduces additional proximity measures so that the
relationships between nodes can be learned more comprehensively. Their difference is the way of integrating network
structures and side information. Many of them are naturally extensions from structure preserving network embedding methods.
Advanced Information Preserving
Network Embedding
In this section, we review network embedding methods that
take additional advanced information into account so as to
solve some speciﬁc analytic tasks. Different from side information, the advanced information refers to the supervised or
pseudo supervised information in a speciﬁc task.
Information Diffusion
Information diffusion is an ubiquitous
phenomenon on the web, especially in social networks.
Many real applications, such as marketing, public opinion
formation, epidemics, are related to information diffusion.
Most of the previous studies on information diffusion are
conducted in original network spaces.
Recently, Simon et al. propose a
social network embedding algorithm for predicting information diffusion. The basic idea is to map the observed information diffusion process into a heat diffusion process modeled by a diffusion kernel in the continuous space. Specifically, the diffusion kernel in a d-dimensional Euclidean
space is deﬁned as
K(t, j, i) = (4Πt)−d
2 e−∥j−i∥2
It models the heat at location i at time t when an initial unit
heat is positioned at location j, which also models how information spreads between nodes in a network.
The goal of the proposed algorithm is to learn the representations of nodes in the latent space such that the diffusion kernel can best explain the cascades in the training
set. Given the representation uj of the initial contaminated
node j in cascade c, the contamination score of node i can
be computed by
K(t, j, i) = (4Πt)−d
The intuition of Eq. (17) is that the closer a node in the latent
space is from the source node, the sooner it is infected by
information from the source node. As the cascade c offers a
guidance for the information diffusion of nodes, we expect
the contamination score to be as closely consistent with c
as possible, which gives rise to the following empirical risk
∆(K(., j, .), c),
where function ∆is a measure of the difference between the
predicted score and the observed diffusion in c. By minimizing the Eq. (18) and reformulating it as a ranking problem,
the optimal representations U of nodes can be obtained.
The cascade prediction problem here is deﬁned as predicting the increment of cascade size after a given time interval . Li et al. argue that
the previous work on cascade prediction all depends on the
bag of hand-crafting features to represent the cascade and
network structures. Instead, they present an end-to-end deep
learning model to solve this problem using the idea of network embedding, as illustrated in Fig. 12. Similar to Deep-
Walk , they perform
a random walk over a cascade graph to sample a set of
paths. Then the Gated Recurrent Unite (GRU) , a speciﬁc type of recurrent neural
network , is applied to these paths and
learn the embeddings for these paths. The attention mechanism is then used to assemble these embeddings to learn the
representation of this cascade graph. Once the representation
of this cascade is known, a multi-layer perceptron can be adopted to output the ﬁnal predicted size of
this cascade. The whole procedure is able to learn the representation of cascade graph in an end-to-end manner. The experimental results on the Twitter and Aminer networks show
promising performance on this task.
Anomaly Detection
Anomaly detection has been widely investigated in previous work . Anomaly
detection in networks aims to infer the structural inconsistencies, which means the anomalous nodes that connect to various diverse inﬂuential communities , such as the red node in Fig. 13. Hu et al. propose a network embedding based method for
anomaly detection. In particular, in the proposed model, the
k-th element uk
i in the embedding ui of node i represents the
correlation between node i and community k. Then, they assume that the community memberships of two linked nodes
should be similar. Therefore, they can minimize the following objective function:
∥ui −uj∥2 + α
(∥ui −uj∥−1)2. (19)
This optimization problem can be solved by the gradient descent method. By taking the neighbors of a node into account, the embedding of the node can be obtained by a
weighted sum of the embeddings of all its neighbors. An
anomaly node in this context is one connecting to a set of different communities. Since the learned embedding of nodes
captures the correlations between nodes and communities,
based on the embedding, they propose a new measure to
indicate the anomalousness level of a node. The larger the
value of the measure, the higher the propensity for a node
being an anomaly node.
Network Alignment
The goal of network alignment is to establish the correspondence between the nodes from two networks.
Man et al. propose a network embedding
algorithm to predict the anchor links across social networks.
The same users who are shared by different social networks
naturally form the anchor links, and these links bridge the
different networks. As illustrated in Fig. 14, the anchor link
prediction problem is, given source network Gs and target
network Gt and a set of observed anchor links T , to identify
the hidden anchor links across Gs and Gt.
First, Man et al. extend the original
sparse networks Gs and Gt to the denser networks. The basic idea is that given a pair of users with anchor links, if they
have a connection in one network, so do their counterparts
in the other network , in this way, more
links will be added to the original networks. For a pair of
nodes i and j whose representations are ui and uj, respectively, by combining the negative sampling strategy, they use
the following function to preserve the structures of Gs and
Gt in a vector space:
Evk∝Pn(v)[log(1 −σ(uT
i uk))], (20)
where σ(x) = 1/(1 + exp(−x)). The ﬁrst term models the
observed edges, and the second term samples K negative
Then given the observed anchor links . Image extracted from .
Figure 13: The anomalous (red) nodes in embedding, and
A, B, C, D are four communities . Image
extracted from .
function φ parameterized by θ so as to bridge these two representations. The loss function is deﬁned as:
∥φ(ui; θ) −uj∥F .
The mapping function can be linear or non-linear via Multi-
Layer Perceptron (MLP) . By optimizing
Eq. (20) and Eq. (21) simultaneously, the representations
that can preserve the network structure and respect the observed anchor links can be learned.
Advanced information preserving network embedding usually consists of two parts. One is to preserve the network
structure so as to learn the representations of nodes. The
other is to establish the connection between the representations of nodes and the target task. The ﬁrst one is similar to
structure and property preserving network embedding, while
the second one usually needs to consider the domain knowledge of a speciﬁc task. The domain knowledge encoded by
the advanced information makes it possible to develop endto-end solutions for network applications. Compared with
the hand-crafted network features, such as numerous network centrality measures, the combination of advanced information and network embedding techniques enables repre-
Figure 14: The illustrative diagram of network embedding
for anchor link prediction proposed by Man et al. . Image extracted from .
sentation learning for networks. Many network applications
may be beneﬁtted from this new paradigm.
Network Embedding in Practice
In this section, we summarize the data sets, benchmarks, and
evaluation tasks that are commonly used in developing new
network embedding methods.
Real World Data Sets
Getting real network data sets in academic research is always far from trivial. Here, we describe some most popular
real world networks currently used in network embedding
literature. The data sets can be roughly divided into four
groups according to the nature of the networks: social networks, citation networks, language networks, and biological
networks. A summary of these data sets can be found in Table 2. Please note that, the same name may be used to refer to different variants in different studies. Here we aim to
provide an overview of the networks, and do not attempt to
describe all of those variants in detail.
Social Networks
• BLOGCATALOG . This is a network of social relationships of the bloggers listed on
the BlogCatalog website. One instance of this data set
can be found at 
edu/datasets/BlogCatalog3.
• FLICKR . This is a network of
the contacts between users of the photo sharing websites Flickr. One instance of the network can be downloaded at 
datasets/Flickr.
• YOUTUBE . This is a network between users of the popular video sharing website, Youtube. One instance of the network can be
found at 
datasets/YouTube2.
• Twitter . This is a network between users on a social news website Twitter.
downloaded at 
datasets/Twitter.
Citation Networks
• DBLP . This network represents the
citation relationships between authors and papers. One
instance of the data set can be found at http://
arnetminer.org/citation.
• Cora . This network represents the
citation relationships between scientiﬁc publications. Besides the link information, each publication is also associated with a word vector indicating the absence/presence
of the corresponding words from the dictionary. One instance of the data set can be found at 
soe.ucsc.edu/node/236.
• Citeseer . This network, similar to Cora, also consists of scientiﬁc publications and
their citation relationships. One instance of the data set
can be downloaded at 
edu/node/236.
 . This is the collaboration
network constructed from the ArXiv website. One instance of the data set can be found at 
stanford.edu/data/ca-AstroPh.html.
Language Networks
• Wikipedia
 . This is
word cooccurrence network from the English Wikipedia pages.
One instance of the data set can be found at http:
//www.mattmahoney.net/dc/textdata.
Biological Networks
• PPI . This is a subgraph of the
biological network that represents the pairwise physical
interactions between proteins in yeast. One instance of
the data set can be downloaded at 
uni-koblenz.de/networks/maayan-vidal.
Node Classiﬁcation
Given some nodes with known labels in a network, the node
classiﬁcation problem is to classify the rest nodes into different classes. Node classiﬁcation is one of most primary
applications for network embedding . Essentially, node classiﬁcation based on network embedding for can be divided into
three steps. First, a network embedding algorithm is applied to embed the network into a low dimensional space.
Then, the nodes with known labels are used as the training
set. Last, a classiﬁer, such as Liblinear , is
learned from the training set. Using the trained classiﬁer, we
can infer the labels of the rest nodes.
evaluation
multilabel classiﬁcation problem include Micro-F1 and Macro-
F1 . Speciﬁcally, for an overall label
set C and a label A, let T P(A), FP(A), and FN(A) be
the number of true positives, false positives, and false negatives in the instances predicted as A, respectively. Then the
Micro-F1 is deﬁned as
A∈C T P(A)
A∈C(T P(A) + FP(A)),
A∈C T P(a)
A∈C(T P(A) + FN(A)),
Micro-F1 = 2 ∗Pr ∗R
The Macro-F1 measure is deﬁned as
Macro-F1 =
where F1(A) is the F1-measure for the label A.
The multi-label classiﬁcation application has been successfully tested on four categories of data sets, namely social networks ,
Table 1: A summary of real world networks
structure and property
preserving network
network embedding
information
classiﬁcation
link prediction
clustering
visualization
BLOGCATALOG
FLICKR , and YOUTUBE ), citation networks ,
Cora , and Citeseer ), language networks ),
and biological networks ).
Speciﬁcally, a social network usually is a communication
network among users on online platforms. DeepWalk , GraRep , SDNE , node2vec , and LANE conduct classiﬁcation on BLOGCATALOG to evaluate the performance. Also, the classiﬁcation performance
on FLICKR has been assessed in . Some studies apply their algorithms to the Youtube network, which
also achieves promising classiﬁcation results. A citation network usually represents the citation relationships between
authors or between papers. For example, use the DBLP network to test the classi-
ﬁcation performance. Cora is used in
 . Citeseer is used in . The classiﬁcation performance on language networks, such as Wikipedia, is also
widely studied . The Protein-Protein Interactions (PPI) is used in . Based
on NUS-WIDE , a heterogeneous network
extracted from Flickr, Chang et al. validated the superior classiﬁcation performanceof network embedding on heterogeneous networks.
To summarize, network embedding algorithms have been
widely used on various networks and have been well demonstrated their effectiveness on node classiﬁcation.
Link Prediction
Link prediction, as one of the most fundamental problems
on network analysis, has received a considerable amount of
attention . It aims to estimate the likelihood of the existence
of an edge between two nodes based on observed network
structure . Since network embedding algorithms are able to learn the vector based features
for each node, the similarity between nodes can be easily
estimated, for example, by the inner product or the cosine
similarity. A larger similarity implies that the two nodes may
have a higher propensity to be linked.
Generally, precision@k and Mean Average Precision
(MAP) are used to evaluate the link prediction performance , which are deﬁned as
precision@k(i) = |{j|i, j ∈V, index(j) ≤k, △i(j) = 1}|
where V is the set of nodes, index(j) is the ranked index
of the j-th node and △i(j) = 1 indicates that nodes i and j
have an edge.
j precision@j(i) ∗△i(j)
|{△i(j) = 1}|
where Q is the query set.
The popularly used real networks for the link prediction task can be divided into three categories: citation networks and DBLP1), social networks
 ,
Facebook , Epinions3, and Slashdot4), and biological networks ). Speciﬁcally, and test the effectiveness on ARXIV5.
HOPE applies network embedding to
link prediction on two directed networks SN-Twitter, which
is a subnetwork of Twitter6, and SN-TWeibo, which is
1 
2 
kddcup2012-track1/data
3 
4 
5 
6 
(c) DeepWalk
(d) GraRep
Figure 15: Network visualization of 20-NewsGroup by different network embedding algorithms, i.e., SDNE , LINE , DeepWalk , GraRep ,
LE . Image extracted from SDNE .
a subnetwork of the social network in Tencent Weibo7.
Node2vec tests the performance of link prediction on a social network Facebook and
a biological network PPI. EOE uses DBLP
to demonstrate the effectiveness on citation networks. Based
on two social networks, Epinions and Slashdot, SiNE shows the superior performance of signed network embedding on link prediction.
To sum up, network embedding is able to capture inherent network structures, and thus naturally it is suitable for
link prediction applications. Extensive experiments on various networks have demonstrated that network embedding
can tackle link prediction effectively.
Node Clustering
Node clustering is to divide the nodes in a network into clusters such that the nodes within the same cluster are more
similar to each other than the nodes in different clusters. Network embedding algorithms learn representations of nodes
in low dimensional vector spaces, so many typical clustering
methods, such as Kmeans , can
be directly adopted to cluster nodes based on their learned
representations.
Many evaluation criteria have been proposed for clustering evaluation. Accuracy (AC) and normalized mutual information (NMI) are frequently used to assess
the clustering performance on graphs and networks. Speciﬁcally, AC is used to measure the percentage of correct labels
obtained. Given n data, let li and ri be the obtained cluster
label and the ground truth label, respectively. AC is deﬁned
i=1 δ(ri, map(li))
where δ(x, y) equals one if x = y and equals zero otherwise, and map(li) is the permutation mapping function that
maps each cluster label li to the equivalent label from the
data, which can be found using the Kuhn-Munkres algorithm .
Given the set of clusters obtained from the ground truth
and obtained from the algorithm, respectively, denoted by C
and C′, the NMI can be deﬁned as
NMI(C, C′) =
max(H(C), H(C′)),
7 
where H(C) is the entropy of C, and MI(C, C′) is the mutual information metric of C and C′.
The node clustering performance is tested on three
types of networks: social networks and YELP ), citation networks ),
and document networks ). In particular, extracts a social
network from a social blogging site. It uses the TF-IDF features extracted from the blogs as the features of blog users
and the “following” behaviors to construct the linkages. It
successfully applies network embedding to the node clustering task. uses the Facebook social
network to demonstrate the effectiveness of community preserving network embedding on node clustering. is applied to more social networks including MOVIE, a network extracted from YAGO that contains knowledge about movies, YELP, a network extracted from YELP that is about reviews given to
restaurants, and GAME, extracted from Freebase that is related to video games. tests the node clustering performance on a document
network, 20-NewsGroup network, which consists of documents. The node clustering performance on citation networks is tested by clustering
authors in DBLP. The results show the superior clustering
performance on citation networks.
In summary, node clustering based on network embedding is tested on different types of networks. Network embedding has become an effective method to solve the node
clustering problem.
Network Visualization
Another important application of network embedding is network visualization, that is, generating meaningful visualization that layouts a network on a two dimensional space. By
applying the visualization tool, such as t-SNE , to the learned low dimensional representations of nodes, it is easy for users to see a big picture of
a sophisticated network so that the community structure or
node centrality can be easily revealed.
More often than not, the quality of network visualization
by different network embedding algorithms is evaluated visually. Fig. 15 is an example by SDNE where SDNE is applied to 20-NewsGroup. In Fig. 15,
Figure 16: Relationship among different types of network
embedding methods.
each document is mapped into a two dimensional space as a
point, and different colors on the points represent the labels.
As can be seen, network embedding preserves the intrinsic structure of the network, where similar nodes are closer
to each other than dissimilar nodes in the low-dimensional
space. Also, LINE , GraRep , and EOE are applied to a citation network DBLP and generate meaningful layout of the
network. Pan et al. show the visualization
of another citation network Citeseer-M10 consisting of scientiﬁc publications from ten distinct
research areas.
Open Source Software
In Table 2, we provide a collection of links where one can
ﬁnd the source code of various network embedding methods.
Conclusions and Future Research
Directions
The above survey of the state-of-the-art network embedding
algorithms clearly shows that it is still a young and promising research ﬁeld. To apply network embedding to tackle
practical applications, a frontmost question is to select the
appropriate methods. In Fig. 16 we show the relationship
among different types of network embedding methods discussed in this survey.
The structure and property preserving network embedding is the foundation. If one cannot preserve well the network structure and retain the important network properties,
in the embedding space serious information is loss, which
hurts the analytic tasks in sequel. Based on the structure and
property preserving network embedding, one may apply the
off-the-shelf machine learning methods. If some side information is available, it can be incorporated into network embedding. Furthermore, the domain knowledge of some certain applications as advanced information can be considered.
In the rest of this section, we discuss several interesting
directions for future work.
More Structures and Properties
Although various methods are proposed to preserve structures and properties, such as ﬁrst order and high order proximities, communities, asymmetric transitivity, and structural
balance, due to the complexity of real world networks, there
are still some particular structures that are not fully considered in the existing network embedding methods. For example, how to incorporate network motifs , one of the most common higher-order
structures in a network, into network embedding remains an
open problem. Also, more complex local structures of a node
can be considered to provide higher level constraints. The
current assumption of network embedding is usually based
on the pairwise structure, that is, if two nodes have a link,
then their representations are similar. This assumption can
work well for some applications, such as link prediction, but
it cannot encode the centrality information of nodes, because
the centrality of a node is usually related to a more complex
structure. As another example, in several real world applications, an edge may involve more than two nodes, known as
a hyperedge. Such a hypernetwork naturally indicates richer
relationships among nodes and has its own characteristics.
Hypernetwork embedding is important for some real applications.
The power law distribution property indicates that most
nodes in a network are associated with a small number of
edges. Consequently, it is hard to learn an effective representation for a node with limited information. How this property affects the performance of network embedding and how
to improve the embeddings of the minority nodes are still
largely untouched.
The Effect of Side Information
Section 5 discusses a series of network embedding algorithms that preserve side information in embedding. All the
existing methods assume that there is an agreement between
network structure and side information. To what extent the
assumption holds in real applications, however, remains an
open question. The low correlation of side information and
structures may degrade the performance of network embedding. Moreover, it is interesting to explore the complementarity between network structures and side information.
More often than not, each information may contain some
knowledge that other information does not have.
Besides, in a heterogeneous information network, to measure the relevance of two objects, the meta path, a sequence
of object types with edge types in between, has been widely
used. However, meta structure , which is
essentially a directed acyclic graph of object and edge types,
provides a higher-order structure constraint. This suggests a
huge potential direction for improving heterogeneous information network embedding.
More Advanced Information and Tasks
In general, most of network embedding algorithms are designed for general purposes, such as link prediction and node
classiﬁcation. These network embedding methods mainly
focus on general network structures and may not be speciﬁc to some target applications. Another important research
Table 2: A summary of the source code
Structure and property preserving network embedding
Source code
DeepWalk 
 
LINE 
 
GraRep 
 
SDNE 
 
Node2vec 
 
DNGR 
 
M-NMF 
 
GED 
 
Ou et al. 
 
HOPE 
 
Network embedding with side information
Source code
MMDW 
 
TADW 
 
TriDNR 
 
Advanced information preserving network embedding
Source code
Information diffusion 
 
Cascade prediction 
 
Anomaly detection 
 
Collaboration prediction 
 
direction is to explore the possibility of designing network
embedding for more speciﬁc applications. For example,
whether network embedding is a new way to detect rumors
in social network ? Can we use network embedding to infer social ties ? Each real
world application has its own characteristics, and incorporating their unique domain knowledge into network embedding is a key. The technical challenges here are how to model
the speciﬁc domain knowledge as advanced information that
can be integrated into network embedding in an effective
Dynamic Network Embedding
Although many network embedding methods are proposed,
they are mainly designed for static networks. However, in
real world applications, it is well recognized that many networks are evolving over time. For example, in the Facebook network, friendships between users always dynamically change over time, e.g., new edges are continuously
added to the social network while some edges may be
deleted. To learn the representations of nodes in a dynamic network, the existing network embedding methods
have to be run repeatedly for each time stamp, which is
very time consuming and may not meet the realtime processing demand. Most of the existing network embedding
methods cannot be directly applied to large scale evolving
networks. New network embedding algorithms, which are
able to tackle the dynamic nature of evolving networks, are
highly desirable.
More embedding spaces
The existing network embedding methods embed a network
into the Euclidean space. In general, the principle of network
embedding can be extended to other target spaces. For example, recently some studies assume
that the underlying structure of a network is in the hyperbolic
space. Under this assumption, heterogeneous degree distributions and strong clustering emerge naturally, as they are
the simple reﬂections of the negative curvature and metric
property of the underlying hyperbolic geometry. Exploring
other embedding space is another interesting research direction.