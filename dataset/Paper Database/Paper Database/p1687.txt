Published as a conference paper at ICLR 2024
ITRANSFORMER: INVERTED TRANSFORMERS ARE
EFFECTIVE FOR TIME SERIES FORECASTING
Yong Liu∗, Tengge Hu∗, Haoran Zhang∗, Haixu Wu, Shiyu Wang§, Lintao Ma§, Mingsheng LongB
School of Software, BNRist, Tsinghua University, Beijing 100084, China
§Ant Group, Hangzhou, China
{liuyong21,htg21,z-hr20,whx20}@mails.tsinghua.edu.cn
{weiming.wsy,lintao.mlt}@antgroup.com, 
The recent boom of linear forecasting models questions the ongoing passion for
architectural modifications of Transformer-based forecasters. These forecasters
leverage Transformers to model the global dependencies over temporal tokens of
time series, with each token formed by multiple variates of the same timestamp.
However, Transformers are challenged in forecasting series with larger lookback
windows due to performance degradation and computation explosion. Besides, the
embedding for each temporal token fuses multiple variates that represent potential
delayed events and distinct physical measurements, which may fail in learning
variate-centric representations and result in meaningless attention maps. In this
work, we reflect on the competent duties of Transformer components and repurpose
the Transformer architecture without any modification to the basic components. We
propose iTransformer that simply applies the attention and feed-forward network
on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture
multivariate correlations; meanwhile, the feed-forward network is applied for each
variate token to learn nonlinear representations. The iTransformer model achieves
state-of-the-art on challenging real-world datasets, which further empowers the
Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice
alternative as the fundamental backbone of time series forecasting. Code is available at this repository: 
INTRODUCTION
iTransformer
Transformer
Figure 1: Performance of iTransformer. Average results (MSE) are
reported following TimesNet .
Transformer has achieved tremendous success in natural language processing and
computer vision , growing into the foundation model that follows the scaling law .
Inspired by the immense success in extensive fields, Transformer
with strong capabilities of depicting pairwise dependencies and
extracting multi-level representations in sequences is emerging
in time series forecasting .
However, researchers have recently begun to question the validity
of Transformer-based forecasters, which typically embed multiple
variates of the same timestamp into indistinguishable channels
and apply attention on these temporal tokens to capture temporal
dependencies. Considering the numerical but less semantic relationship among time points, researchers find that simple linear
layers, which can be traced back to statistical forecasters , have exceeded complicated Transformers on both performance and efficiency . Meanwhile, ensuring the independence of variate and utilizing mutual
∗Equal Contribution
 
Published as a conference paper at ICLR 2024
Time Steps
Attention over
Temporal Tokens
Attention over
Variate Tokens
FFN on Series
Representations
Transformer
iTransformer
Variate-Mixed
Representation
Variate-Unmixed
Representation
FFN on Multivariate
Representations
Figure 2: Comparison between the vanilla Transformer (top) and the proposed iTransformer (bottom).
Transformer embeds the temporal token, which contains the multivariate representation of each time
step. iTransformer embeds each series independently to the variate token, such that the attention module depicts the multivariate correlations and the feed-forward network encodes series representations.
information is ever more highlighted by recent research that explicitly models multivariate correlations to achieve accurate forecasting , but this goal can
be hardly achieved without subverting the vanilla Transformer architecture.
Considering the disputes of Transformer-based forecasters, we reflect on why Transformers perform
even worse than linear models in time series forecasting while acting predominantly in many other
fields. We notice that the existing structure of Transformer-based forecasters may be not suitable for
multivariate time series forecasting. As shown on the top of Figure 2, it is notable that the points
of the same time step that basically represent completely different physical meanings recorded by
inconsistent measurements are embedded into one token with wiped-out multivariate correlations.
And the token formed by a single time step can struggle to reveal beneficial information due to
excessively local receptive field and time-unaligned events represented by simultaneous time points.
Besides, while series variations can be greatly influenced by the sequence order, permutationinvariant attention mechanisms are improperly adopted on the temporal dimension . Consequently, Transformer is weakened to capture essential series representations and portray
multivariate correlations, limiting its capacity and generalization ability on diverse time series data.
Concerning the potential risks of embedding multivariate points of a timestamp as a (temporal) token,
we take an inverted view on time series and embed the whole time series of each variate independently
into a (variate) token, the extreme case of Patching that enlarges local receptive field.
By inverting, the embedded token aggregates the global representations of series that can be more
variate-centric and better leveraged by booming attention mechanisms for multivariate correlating.
Meanwhile, the feed-forward network can be proficient enough to learn generalizable representations
for distinct variates encoded from arbitrary lookback series and decoded to predict future series.
Based on the above motivations, we believe it is not that Transformer is ineffective for time series
forecasting, but rather it is improperly used. In this paper, we revisit the structure of Transformer and
advocate iTransformer as a fundamental backbone for time series forecasting. Technically, we embed
each time series as variate tokens, adopt the attention for multivariate correlations, and employ the
feed-forward network for series representations. Experimentally, the proposed iTransformer achieves
state-of-the-art performance on real-world forecasting benchmarks shown in Figure 1 and surprisingly
tackles the pain points of Transformer-based forecasters. Our contributions lie in three aspects:
• We reflect on the architecture of Transformer and refine that the competent capability of
native Transformer components on multivariate time series is underexplored.
• We propose iTransformer that regards independent time series as tokens to capture multivariate correlations by self-attention and utilize layer normalization and feed-forward network
modules to learn better series-global representations for time series forecasting.
• Experimentally, iTransformer achieves comprehensive state-of-the-art on real-world benchmarks. We extensively analyze the inverted modules and architecture choices, indicating a
promising direction for the future improvement of Transformer-based forecasters.
Published as a conference paper at ICLR 2024
RELATED WORK
With the progressive breakthrough made in natural language processing and computer vision areas,
elaboratively designed Transformer variants are proposed to tackle ubiquitous time series forecasting
applications. Going beyond contemporaneous TCNs and RNNbased forecasters , Transformer has
exhibited powerful sequence modeling capability and promising model scalability, leading to the
trend of passionate modifications adapted for time series forecasting.
Through a systematical review of Transformer-based forecasters, we conclude that existing modifications can be divided into four categories by whether to modify the component and architecture.
As shown in Figure 3, the first category , which
is the most common practice, mainly concerns the component adaptation, especially the attention
module for the temporal dependency modeling and the complexity optimization on long sequences.
Nevertheless, with the rapid emergence of linear forecasters , the impressive performance and efficiency continuously
challenge this direction. Soon afterward, the second category attempts to fully utilize Transformer.
It pays more attention to the inherent processing of time series, such as Stationarization , Channel Independence, and Patching , which bring about consistently
improved performance. Moreover, faced with the increasing significance of the independence and
mutual interactions of multiple variates, the third category refurbishes Transformer in both aspects of
component and architecture. Representative explicitly captures the cross-time
and cross-variate dependencies by the renovated attention mechanism and architecture.
Unlike previous works, iTransformer modifies none of the native components of Transformer. Instead,
we adopt the components on the inverted dimensions with the altered architecture, as the only one that
belongs to the fourth category to our best knowledge. We believe the capabilities of the components
have stood the test extensively, the truth is that the architecture of Transformer is improperly adopted.
(II) PatchTST, NSTransformer,…
Modified Attn
Feed-forward
Add & Norm
Series Processing
Original Arch
Transformer
Modified Architecture
(I) Autoformer, Informer,…
(III) Crossformer,…
(IV) iTransformer (Ours)
No Modified Architecture
No Modified
Modified Arch
Figure 3: Transformer-based forecasters categorized by component and architecture modifications.
ITRANSFORMER
In multivariate time series forecasting, given historical observations X = {x1, . . . , xT } ∈RT ×N
with T time steps and N variates, we predict the future S time steps Y = {xT +1, . . . , xT +S} ∈
RS×N. For convenience, we denote Xt,: as the simultaneously recorded time points at the step t, and
X:,n as the whole time series of each variate indexed by n. It is notable that Xt,: may not contain
time points that essentially reflect the same event in real-world scenarios because of the systematical
time lags among variates in the dataset. Besides, the elements of Xt,: can be distinct from each other
in physical measurements and statistical distributions, for which a variate X:,n generally shares.
STRUCTURE OVERVIEW
Our proposed iTransformer illustrated in Figure 4 adopts the encoder-only architecture of Transformer , including the embedding, projection, and Transformer blocks.
Embedding the whole series as the token
Most Transformer-based forecasters typically regard
multiple variates of the same time as the (temporal) token and follow the generative formulation of
forecasting tasks. However, we find the approach on the numerical modality can be less instructive for
Published as a conference paper at ICLR 2024
Variate Tokens
Temporal LayerNorm
Multivariate
Correlations
Act & Drop
Projection
Multivariate
Feed-forward
Figure 4: Overall structure of iTransformer, which shares the same modular arrangement with the
encoder of Transformer. (a) Raw series of different variates are independently embedded as tokens.
(b) Self-attention is applied to embedded variate tokens with enhanced interpretability revealing
multivariate correlations. (c) Series representations of each token are extracted by the shared feedforward network. (d) Layer normalization is adopted to reduce the discrepancies among variates.
learning attention maps, which is supported by increasing applications of Patching that broadens the respective field. Meanwhile, the triumph of linear forecasters
also challenges the necessity of adopting a heavy encoder-decoder Transformer for generating tokens.
Instead, our proposed encoder-only iTransformer focuses on representation learning and adaptive
correlating of multivariate series. Each time series driven by the underlying complicated process
is firstly tokenized to describe the properties of the variate, applied by self-attention for mutual
interactions, and individually processed by feed-forward networks for series representations. Notably,
the task to generate the predicted series is essentially delivered to linear layers, which has been proven
competent by previous work and we provide a detailed analysis in the next section.
Based on the above considerations, in iTransformer, the process of predicting future series of each
specific variate ˆY:,n based on the lookback series X:,n is simply formulated as follows:
n = Embedding(X:,n),
Hl+1 = TrmBlock(Hl), l = 0, . . . , L −1,
ˆY:,n = Projection(hL
where H = {h1, . . . , hN} ∈RN×D contains N embedded tokens of dimension D and the superscript denotes the layer index. Embedding : RT 7→RD and Projection : RD 7→RS are
both implemented by multi-layer perceptron (MLP). The obtained variate tokens interact with each
other by self-attention and are independently processed by the shared feed-forward network in each
TrmBlock. Specifically, as the order of sequence is implicitly stored in the neuron permutation of the
feed-forward network, the position embedding in the vanilla Transformer is no longer needed here.
iTransformers
The architecture essentially presupposes no more specific requirements on Transformer variants, other than the attention is applicable for multivariate correlation. Thus, a bundle of
efficient attention mechanisms can be the plugins,
reducing the complexity when the variate number grows large. Besides, with the input flexibility of
attention, the token number can vary from training to inference, and the model is allowed to be trained
on arbitrary numbers of variates. The inverted Transformers, named iTransformers, are extensively
evaluated in experiments of Section 4.2 and demonstrate advantages on time series forecasting.
INVERTED TRANSFORMER COMPONENTS
We organize a stack of L blocks composed of the layer normalization, feed-forward network, and
self-attention modules. But their duties on the inverted dimension are carefully reconsidered.
Published as a conference paper at ICLR 2024
Layer normalization
Layer normalization is originally proposed to increase
the convergence and training stability of deep networks. In typical Transformer-based forecasters,
the module normalizes the multivariate representation of the same timestamp, gradually fusing the
variates with each other. Once the collected time points do not represent the same event, the operation
will also introduce interaction noises between noncausal or delayed processes. In our inverted version,
the normalization is applied to the series representation of individual variate as Equation 2, which
has been studied and proved effective in tackling non-stationary problems . Besides, since all series as (variate) tokens are normalized to a Gaussian distribution,
the discrepancies caused by inconsistent measurements can be diminished. By contrast, in previous
architecture, different tokens of time steps will be normalized, leading to oversmooth time series.
LayerNorm(H) =
hn −Mean(hn)
n = 1, . . . , N
Feed-forward network
Transformer adopts the feed-forward network (FFN) as the basic building
block for encoding token representation and it is identically applied to each token. As aforementioned,
in the vanilla Transformer, multiple variates of the same timestamp that form the token can be
malpositioned and too localized to reveal enough information for predictions. In the inverted version,
FFN is leveraged on the series representation of each variate token. By the universal approximation
theorem , they can extract complicated representations to describe a time series. With
the stacking of inverted blocks, they are devoted to encoding the observed time series and decoding
the representations for future series using dense non-linear connections, which work effectively as
the recent works completely built on MLPs .
More interestingly, the identical linear operation on independent time series, which serves as the
combination of the recent linear forecasters and Channel Independence , can be instructive for us to understand the series representations. Recent revisiting on linear
forecasters highlights that temporal features extracted by MLPs are supposed to be
shared within distinct time series. We propose a rational explanation that the neurons of MLP are
taught to portray the intrinsic properties of any time series, such as the amplitude, periodicity, and even
frequency spectrums (neuron as a filter), serving as a more advantageous predictive representation
learner than the self-attention applied on time points. Experimentally, we validate that the division of
labor helps enjoy the benefits of linear layers in Section 4.3, such as the promoted performance if
providing enlarged lookback series, and the generalization ability on unseen variates.
Self-attention
While the attention mechanism is generally adopted for facilitating the temporal
dependencies modeling in previous forecasters, the inverted model regards the whole series of one
variate as an independent process. Concretely, with comprehensively extracted representations of
each time series H = {h0, . . . , hN} ∈RN×D, the self-attention module adopts linear projections to
get queries, keys, and values Q, K, V ∈RN×dk, where dk is the projected dimension.
With denotation of qi, kj ∈Rdk as the specific query and key of one (variate) token, we notice
that each entry of the pre-Softmax scores is formulated as Ai,j = (QK⊤/√dk)i,j ∝q⊤
i kj. Since
each token is previously normalized on its feature dimension, the entries can somewhat reveal the
variate-wise correlation, and the whole score map A ∈RN×N exhibits the multivariate correlations
between paired variate tokens. Consequently, highly correlated variate will be more weighted for the
next representation interaction with values V. Based on this intuition, the proposed mechanism is
believed to be more natural and interpretable for multivariate series forecasting. We further provide
the visualization analysis of the score map in Section 4.3 and Appendix E.1.
EXPERIMENTS
We thoroughly evaluate the proposed iTransformer on various time series forecasting applications,
validate the generality of the proposed framework and further dive into the effectiveness of applying
the Transformer components on the inverted dimensions of time series.
We extensively include 7 real-world datasets in our experiments, including ECL, ETT (4
subsets), Exchange, Traffic, Weather used by Autoformer , Solar-Energy datasets
Published as a conference paper at ICLR 2024
proposed in LSTNet , and PEMS (4 subsets) evaluated in SCINet .
We also provide the experiments on Market (6 subsets) in Appendix F.4. It records the minutesampled server load of Alipay online transaction application with hundreds of variates, where we
consistently outperform other baselines. Detailed dataset descriptions are provided in Appendix A.1.
FORECASTING RESULTS
In this section, we conduct extensive experiments to evaluate the forecasting performance of our
proposed model together with advanced deep forecasters.
We carefully choose 10 well-acknowledged forecasting models as our benchmark,
including (1) Transformer-based methods: Autoformer , FEDformer , Stationary , Crossformer , PatchTST ;
(2) Linear-based methods: DLinear , TiDE , RLinear ; and (3) TCN-based methods: SCINet , TimesNet .
Main results
Comprehensive forecasting results are listed in Table 1 with the best in red and the
second underlined. The lower MSE/MAE indicates the more accurate prediction result. Compared
with other forecasters, iTransformer is particularly good at forecasting high-dimensional time series.
Besides, PatchTST as the previous state-of-the-art, fails in many cases of PEMS, which can stem from
the extremely fluctuating series of the dataset, and the patching mechanism of PatchTST may lose
focus on specific locality to handle rapid fluctuation. By contrast, the proposed model aggregating
the whole series variations for series representations can better cope with this situation. Notably, as
the representative that explicitly captures multivariate correlations, the performance of Crossformer
is still subpar to iTransformer, indicating the interaction of time-unaligned patches from different
multivariate will bring about unnecessary noise for forecasting. Therefore, the native Transformer
components are competent for temporal modeling and multivariate correlating, and the proposed
inverted architecture can effectively tackle real-world time series forecasting scenarios.
Table 1: Multivariate forecasting results with prediction lengths S ∈{12, 24, 36, 48} for PEMS and
S ∈{96, 192, 336, 720} for others and fixed lookback length T = 96. Results are averaged from all
prediction lengths. Avg means further averaged by subsets. Full results are listed in Appendix F.4.
iTransformer
Crossformer
Stationary
Autoformer
MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
0.178 0.270 0.219 0.298 0.205 0.290 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338
0.383 0.399 0.380 0.392 0.381 0.397 0.685 0.578 0.482 0.470 0.391 0.404 0.442 0.444 0.689 0.597 0.408 0.428 0.471 0.464 0.465 0.459
0.360 0.403 0.378 0.417 0.367 0.404 0.940 0.707 0.370 0.413 0.416 0.443 0.354 0.414 0.750 0.626 0.519 0.429 0.461 0.454 0.613 0.539
0.428 0.282 0.626 0.378 0.481 0.304 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379
0.258 0.278 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382
Solar-Energy 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711
PEMS (Avg) 0.119 0.218 0.514 0.482 0.217 0.305 0.220 0.304 0.375 0.440 0.148 0.246 0.320 0.394 0.121 0.222 0.224 0.327 0.151 0.249 0.614 0.575
ITRANSFORMERS GENERALITY
In this section, we evaluate iTransformers by applying our framework to Transformer and its variants, which generally address the quadratic complexity of the self-attention mechanism, including
Reformer , Informer , Flowformer and FlashAttention . Surprising and promising discoveries are exhibited, indicating the simple
inverted perspective can enhance Transformer-based forecasters with promoted performance with
efficiency, generalization on unseen variates, and better utilization of historical observations.
Performance promotion
We evaluate Transformers and the corresponding iTransformers with the
reported performance promotions in Table 2. It is notable that the framework consistently improves
various Transformers. Overall, it achieves averaged 38.9% promotion on Transformer, 36.1% on
Reformer, 28.5% on Informer, 16.8% on Flowformer and 32.2% on Flashformer, revealing the
previous improper usage of the Transformer architecture on time series forecasting. Moreover, since
the attention mechanism is adopted on the variate dimension in our inverted structure, the introduction
of efficient attentions with linear complexity essentially addresses the computational problem due to
Published as a conference paper at ICLR 2024
numerous variates, which is prevalent in real-world applications but can be resource-consuming for
Channel Independence . Therefore, the idea of iTransformer can be widely practiced
on Transformer-based forecasters to take advantage of booming efficient attention mechanisms.
Table 2: Performance promotion obtained by our inverted framework. Flashformer means Transformer
equipped with hardware-accelerated FlashAttention . We report the average
performance and the relative MSE reduction (Promotion). Full results can be found in Appendix F.2.
Transformer
Flowformer
Flashformer
Variate generalization
By inverting vanilla Transformers, it is notable that the models are empowered with the generalization capability on unseen variates. Firstly, benefiting from the flexibility of the
number of input tokens, the amount of variate channels is no longer restricted and thus feasible to vary
from training and inference. Besides, feed-forward networks are identically applied on independent
variate tokens in iTransformer. As aforementioned, the neurons as filters learn the intrinsic patterns
of any time series, which are inclined to be shared and transferable among distinct variates.
To verify the hypothesis, we compare inverting with another generalizing strategy: Channel Independence, training a shared backbone to forecast all variates. We partition the variates of each dataset
into five folders, train models with only 20% of variates of one folder, and directly forecast all
variates without fine-tuning. We compare the performance in Figure 5 and each bar presents the
averaged results of all folders to avoid the randomness of partition. CI-Transformers take a long time
to predict each variate one by one during inference while iTransformers directly predict all variates
and generally present smaller increases, indicating FFN is competent to learn transferable time series
representations. It leaves a potential direction to build a foundation model upon iTransformer, where
diverse multivariate time series with different numbers of variates can be feasibly trained together.
Transformer
Flowformer
Transformer
Flowformer
Transformer
Flowformer
Solar-Energy
CI-Transformers (100% variate)
CI-Transformers (20% variate)
iTransformers (100% variate)
iTransformers (20% variate)
Figure 5: Performance of generalization on unseen variates. We partition the variates of each dataset
into five folders, train models with 20% variates, and use the partially trained model to forecast all
varieties. iTransformers can be trained efficiently and forecast with good generalizability.
Increasing lookback length
Previous works have witnessed the phenomenon that the forecasting
performance does not necessarily improve with the increase of lookback length on Transformers , which can be attributed to the distracted attention on the growing input.
However, the desired performance improvement is generally held on linear forecasts, theoretically
supported by statistical methods with enlarged historical information to be
Published as a conference paper at ICLR 2024
utilized. As the working dimensions of attention and feed-forward network are inverted, we evaluate
the performance of Transformers and iTransformer in Figure 6 with increased lookback length. The
results surprisingly verify the rationality of leveraging MLPs on the temporal dimension such that
Transformers can benefit from the extended lookback window for more precise predictions.
iTransformer
iFlowformer
Transformer
Flowformer
Figure 6: Forecasting performance with the lookback length T ∈{48, 96, 192, 336, 720} and fixed
prediction length S = 96. While the performance of Transformer-based forecasters does not
necessarily benefit from the increased lookback length, the inverted framework empowers the vanilla
Transformer and its variants with improved performance on the enlarged lookback window.
MODEL ANALYSIS
Ablation study
To verify the rational business of Transformer components, we provide detailed
ablations covering both replacing components (Replace) and removing components (w/o) experiments.
The results are listed in Table 3. iTransformer that utilizes attention on the variate dimension and
feed-forward on the temporal dimension generally achieves the best performance. Notably, the
performance of vanilla Transformer (the third row) performs the worst among these designs, revealing
the potential risks of the conventional architecture, which we describe in detail in Appendix E.3.
Table 3: Ablations on iTransformer. We replace different components on the respective dimension
to learn multivariate correlations (Variate) and series representations (Temporal), in addition to
component removal. The average results of all predicted lengths are listed here.
Solar-Energy
iTransformer
Analysis of series representations
To further validate the claim that feed-forward networks are
more favored to extract the series representations. We conduct representation analysis based on
the centered kernel alignment (CKA) similarity . A higher CKA indicates
more similar representations. For Transformer variants and iTransformers, we calculate the CKA
between the output features of the first and the last block. Notably, previous works have demonstrated
that time series forecasting, as a low-level generative task, prefers the higher CKA similarity for the better performance. As shown in Figure 7, a clear division
line is exhibited, implying that iTransformers have learned more appropriate series representations
by inverting the dimension and thus achieve more accurate predictions. The results also advocate
inverting Transformer deserves a fundamental renovation of the forecasting backbone.
Published as a conference paper at ICLR 2024
Analysis of multivariate correlations
By assigning the duty of multivariate correlation to the
attention mechanism, the learned map enjoys enhanced interpretability. We present the case visualization on series from Solar-Energy in Figure 7, which has distinct correlations in the lookback and
future windows. It can be observed that in the shallow attention layer, the learned map shares lots of
similarities to the correlations of raw input series. As it dives into deeper layers, the learned map
become gradually alike to the correlations of future series, which validates the inverted operation
empowers interpretable attention for correlating, and the processes of encoding the past and decoding
for the future are essentially conducted in series representations during feed-forwarding.
Lookback Correlations
Future Correlations
Score Map of Layer 1
Score Map of Layer L
Figure 7: Analysis of series representations and multivariate correlations. Left: MSE and CKA
similarity of representations comparison between Transformers and iTransformers. A higher CKA
similarity indicates more favored representations for accurate predictions. Right: A case visualization
of multivariate correlations of raw time series and the learned score maps by inverted self-attention.
Efficient training strategy
Due to the quadratic complexity of self-attention, it can be overwhelming for training on numerous variates, which is very common in real-world scenarios. In addition to
efficient attention mechanisms, we propose a novel training strategy for high-dimensional multivariate
series by taking advantage of previously demonstrated variate generation capability. Concretely, we
randomly choose part of the variates in each batch and only train the model with selected variates.
Since the number of variate channels is flexible because of our inverting, the model can predict all
the variates for predictions. As shown in Figure 8, the performance of our proposed strategy is still
comparable with full-variate training, while the memory footprint can be reduced significantly.
Sample Ratio
Sample Ratio
Solar-Energy
Figure 8: Analysis of the efficient training strategy. While the performance (left) remains stable on
partially trained variates of each batch with different sampled ratios, the memory footprint (right) can
be cut off greatly. We provide the comprehensive model efficiency analysis in Appendix D.
CONCLUSION AND FUTURE WORK
Considering the characteristics of multivariate time series, we propose iTransformer that inverts the
structure of Transformer without modifying any native modules. iTransformer regards independent
series as variate tokens to capture multivariate correlations by attention and utilize layer normalization
and feed-forward networks to learn series representations. Experimentally, iTransformer achieves
state-of-the-art performance and exhibits remarkable framework generality supported by promising
analysis. In the future, we will explore large-scale pre-training and more time series analysis tasks.
Published as a conference paper at ICLR 2024
ETHICS STATEMENT
Our work only focuses on the time series forecasting problem, so there is no potential ethical risk.
REPRODUCIBILITY STATEMENT
In the main text, we have strictly formalized the model architecture with equations. All the implementation details are included in the Appendix, including dataset descriptions, metrics, model, and
experiment configurations. The code will be made public once the paper is accepted.
ACKNOWLEDGMENTS
This work was supported by the National Key Research and Development Plan (2021YFB1715200),
the National Natural Science Foundation of China (U2342217 and 62022050), the BNRist Innovation Fund (BNR2024RC01010), Ant Group through CCF-Ant Research Fund, and the National
Engineering Research Center for Big Data Software.