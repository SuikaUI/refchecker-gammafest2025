MIT Open Access Articles
ImageNet Large Scale Visual Recognition Challenge
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Russakovsky, Olga et al. “ImageNet Large Scale Visual Recognition Challenge.”
International Journal of Computer Vision 115.3 : 211–252.
As Published: 
Publisher: Springer US
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of Use: Article is made available in accordance with the publisher's policy and may be
subject to US copyright law. Please refer to the publisher's site for terms of use.
Int J Comput Vis 115:211–252
DOI 10.1007/s11263-015-0816-y
ImageNet Large Scale Visual Recognition Challenge
Olga Russakovsky1 · Jia Deng2 · Hao Su1 · Jonathan Krause1 ·
Sanjeev Satheesh1 · Sean Ma1 · Zhiheng Huang1 · Andrej Karpathy1 ·
Aditya Khosla3 · Michael Bernstein1 · Alexander C. Berg4 · Li Fei-Fei1
Received: 31 August 2014 / Accepted: 12 March 2015 / Published online: 11 April 2015
© Springer Science+Business Media New York 2015
The ImageNet Large Scale Visual Recognition
Challenge is a benchmark in object category classiﬁcation
and detection on hundreds of object categories and millions
of images. The challenge has been run annually from 2010
to present, attracting participation from more than ﬁfty institutions. This paper describes the creation of this benchmark
dataset and the advances in object recognition that have been
possible as a result. We discuss the challenges of collecting
large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed
analysis of the current state of the ﬁeld of large-scale image
classiﬁcation and object detection, and compare the state-ofthe-art computer vision accuracy with human accuracy. We
conclude with lessons learned in the 5 years of the challenge,
and propose future directions and improvements.
Dataset · Large-scale · Benchmark · Object
recognition · Object detection
1 Introduction
Overview The ImageNet Large Scale Visual Recognition
Challenge (ILSVRC) has been running annually for 5 years
Communicated by M. Hebert.
Olga Russakovsky and Jia Deng authors contributed equally.
B Olga Russakovsky
 
Stanford University, Stanford, CA, USA
University of Michigan, Ann Arbor, MI, USA
Massachusetts Institute of Technology, Cambridge, MA, USA
UNC Chapel Hill, Chapel Hill, NC, USA
 and has become the standard benchmark for
large-scale object recognition.1 ILSVRC follows in the footsteps of the PASCAL VOC challenge , established in 2005, which set the precedent for standardized evaluation of recognition algorithms in the form of
yearly competitions. As in PASCAL VOC, ILSVRC consists
of two components: (1) a publically available dataset, and
(2) an annual competition and corresponding workshop. The
dataset allows for the development and comparison of categorical object recognition algorithms, and the competition
and workshop provide a way to track the progress and discuss
the lessons learned from the most successful and innovative
entries each year.
The publically released dataset contains a set of manually annotated training images. A set of test images is also
released, with the manual annotations withheld.2 Participants train their algorithms using the training images and
then automatically annotate the test images. These predicted
annotations are submitted to the evaluation server. Results
of the evaluation are revealed at the end of the competition period and authors are invited to share insights at the
workshop held at the International Conference on Computer
Vision (ICCV) or European Conference on Computer Vision
(ECCV) in alternate years.
ILSVRC annotations fall into one of two categories: (1)
image-level annotation of a binary label for the presence or
absence of an object class in the image, e.g., “there are cars
in this image” but “there are no tigers,” and (2) object-level
1 In this paper, we will be using the term object recognition broadly to
encompass both image classiﬁcation (a task requiring an algorithm to
determine what object classes are present in the image) as well as object
detection (a task requiring an algorithm to localize all objects present
in the image).
2 In 2010, the test annotations were later released publicly; since then
the test annotation have been kept hidden.
Int J Comput Vis 115:211–252
annotation of a tight bounding box and class label around
an object instance in the image, e.g., “there is a screwdriver
centeredatposition(20,25)withwidthof50pixelsandheight
of 30 pixels”.
Large-Scale Challenges and Innovations In creating the
dataset, several challenges had to be addressed. Scaling up
from 19,737 images in PASCAL VOC 2010 to 1,461,406
in ILSVRC 2010 and from 20 object classes to 1000 object
classes brings with it several challenges. It is no longer feasible for a small group of annotators to annotate the data as is
done for other datasets . Instead we turn
to designing novel crowdsourcing approaches for collecting
large-scale annotations . New evaluation criteria have to
be deﬁned to take into account the facts that obtaining perfect
manual annotations in this setting may be infeasible.
Once the challenge dataset was collected, its scale allowed
for unprecedented opportunities both in evaluation of object
recognition algorithms and in developing new techniques.
Novel algorithmic innovations emerge with the availability
of large-scale training data. The broad spectrum of object categories motivated the need for algorithms that are even able to
distinguish classes which are visually very similar. We highlight the most successful of these algorithms in this paper,
and compare their performance with human-level accuracy.
Finally, the large variety of object classes in ILSVRC
allows us to perform an analysis of statistical properties of
objects and their impact on recognition algorithms. This type
of analysis allows for a deeper understanding of object recognition, and for designing the next generation of general object
recognition algorithms.
Goals This paper has three key goals:
(1) To discuss the challenges of creating this large-scale
object recognition benchmark dataset,
(2) To highlight the developments in object classiﬁcation
and detection that have resulted from this effort, and
(3) To take a closer look at the current state of the ﬁeld of
categorical object recognition.
The paper may be of interest to researchers working on creating large-scale datasets, as well as to anybody interested
in better understanding the history and the current state of
large-scale object recognition.
The collected dataset and additional information about
ILSVRC can be found at:
 
1.1 Related Work
We brieﬂy discuss some prior work in constructing benchmark image datasets.
Image Classiﬁcation Datasets Caltech 101 was among the ﬁrst standardized datasets for multicategory image classiﬁcation, with 101 object classes and
commonly 15–30 training images per class. Caltech 256
 increased the number of object classes
to 256 and added images with greater scale and background
variability. The TinyImages dataset 
contains 80 million 32 × 32 low resolution images collected
from the internet using synsets in WordNet 
as queries. However, since this data has not been manually
veriﬁed, there are many errors, making it less suitable for
algorithm evaluation. Datasets such as 15 Scenes or recent Places provide a single
scene category label (as opposed to an object category).
The ImageNet dataset is the backbone
of ILSVRC. ImageNet is an image dataset organized according to the WordNet hierarchy . Each concept
in WordNet, possibly described by multiple words or word
phrases, is called a “synonym set” or “synset”. ImageNet
populates 21,841 synsets of WordNet with an average of 650
manually veriﬁed and full resolution images. As a result,
ImageNet contains 14,197,122 annotated images organized
by the semantic hierarchy of WordNet .
ImageNet is larger in scale and diversity than the other image
classiﬁcation datasets. ILSVRC uses a subset of ImageNet
images for training the algorithms and some of ImageNet’s
image collection protocols for annotating additional images
for testing the algorithms.
Image Parsing Datasets Many datasets aim to provide richer
image annotations beyond image-category labels. LabelMe
 contains general photographs with multiple objects per image. It has bounding polygon annotations
around objects, but the object names are not standardized:
annotators are free to choose which objects to label and what
to name each object. The SUN2012 dataset
contains 16,873 manually cleaned up and fully annotated
images more suitable for standard object detection training
and evaluation. SIFT Flow contains 2,688
images labeled using the LabelMe system. The LotusHill
dataset contains very detailed annotations
Int J Comput Vis 115:211–252
of objects in 636,748 images and video frames, but it is not
available for free. Several datasets provide pixel-level segmentations: for example, MSRC dataset 
with 591 images and 23 object classes, Stanford Background
Dataset with 715 images and 8 classes,
and the Berkeley Segmentation dataset 
with 500 images annotated with object boundaries. Open-
Surfaces segments surfaces from consumer photographs and
annotates them with surface properties, including material,
texture, and contextual information .
The closest to ILSVRC is the PASCAL VOC dataset
 , which provides a standardized test bed for object detection, image classiﬁcation, object
segmentation, person layout, and action classiﬁcation. Much
of the design choices in ILSVRC have been inspired by
PASCAL VOC and the similarities and differences between
the datasets are discussed at length throughout the paper.
ILSVRC scales up PASCAL VOC’s goal of standardized
training and evaluation of recognition algorithms by more
than an order of magnitude in number of object classes
and images: PASCAL VOC 2012 has 20 object classes and
21,738 images compared to ILSVRC2012 with 1000 object
classes and 1,431,167 annotated images.
The recently released COCO dataset 
contains more than 328,000 images with 2.5 million object
instances manually segmented. It has fewer object categories
than ILSVRC (91 in COCO versus 200 in ILSVRC object
detection) but more instances per category (27K on average
compared to about 1K in ILSVRC object detection). Further,
it contains object segmentation annotations which are not
currently available in ILSVRC. COCO is likely to become
another important large-scale benchmark.
Large-Scale Annotation ILSVRC makes extensive use of
Amazon Mechanical Turk to obtain accurate annotations
 . Works such as describe
quality control mechanisms for this marketplace. Vondrick
et al. provides a detailed overview of crowdsourcing video annotation. A related line of work is to obtain
annotations through well-designed games, e.g. . Our novel approaches to crowdsourcing accurate image annotations are in Sects. 3.1.3, 3.2.1
and 3.3.3.
Standardized Challenges There are several datasets with
standardized online evaluation similar to ILSVRC: the aforementioned PASCAL VOC , Labeled
Faces in the Wild for unconstrained
face recognition, Reconstruction meets Recognition for 3D reconstruction and KITTI forcomputervisioninautonomousdriving.These
datasets along with ILSVRC help benchmark progress in different areas of computer vision. Works such as emphasize the importance of examining the bias
inherent in any standardized dataset.
1.2 Paper Layout
We begin with a brief overview of ILSVRC challenge tasks
in Sect. 2. Dataset collection and annotation are described at
length in Sect. 3. Section 4 discusses the evaluation criteria
of algorithms in the large-scale recognition setting. Section 5
provides an overview of the methods developed by ILSVRC
participants.
Section6containsanin-depthanalysisofILSVRCresults:
Sect. 6.1 documents the progress of large-scale recognition
over the years, Sect. 6.2 concludes that ILSVRC results are
statistically signiﬁcant, Sect. 6.3 thoroughly analyzes the current state of the ﬁeld of object recognition, and Sect. 6.4
compares state-of-the-art computer vision accuracy with
human accuracy. We conclude and discuss lessons learned
from ILSVRC in Sect. 7.
2 Challenge Tasks
The goal of ILSVRC is to estimate the content of photographs
for the purpose of retrieval and automatic annotation. Test
images are presented with no initial annotation, and algorithms have to produce labelings specifying what objects are
present in the images. New test images are collected and
labeled especially for this competition and are not part of the
previously published ImageNet dataset .
ILSVRC over the years has consisted of one or more of
the following tasks (years in parentheses):3
(1) Image classiﬁcation : Algorithms produce a
list of object categories present in the image.
(2) Single-object localization : Algorithms produce a list of object categories present in the image, along
with an axis-aligned bounding box indicating the position
and scale of one instance of each object category.
(3) Object detection : Algorithms produce a list
of object categories present in the image along with an
axis-aligned bounding box indicating the position and
scale of every instance of each object category.
This section provides an overview and history of each of the
three tasks. Table 1 shows summary statistics.
3 In addition, ILSVRC in 2012 also included a taster ﬁne-grained classiﬁcation task, where algorithms would classify dog photographs into
one of 120 dog breeds . Fine-grained classiﬁcation
has evolved into its own Fine-Grained classiﬁcation challenge in 2013
 , which is outside the scope of this paper.
Int J Comput Vis 115:211–252
Table 1 Overview of the
provided annotations for each of
the tasks in ILSVRC
classiﬁcation
Single-object
localization
Object detection
Manual labeling
on training set
Number of object classes
annotated per image
Locations of annotated
All instances on
some images
All instances on
all images
Manual labeling
on validation
and test sets
Number of object classes
annotated per image
All target classes
Locations of annotated
All instances on
all images
All instances on
all images
2.1 Image Classiﬁcation Task
Data for the image classiﬁcation task consists of photographs
collected from Flickr4 and other search engines, manually
labeled with the presence of one of 1000 object categories.
Each image contains one ground truth label.
For each image, algorithms produce a list of object categories present in the image. The quality of a labeling is
evaluated based on the label that best matches the ground
truth label for the image (see Sect. 4.1).
Constructing ImageNet was an effort to scale up an image
classiﬁcation dataset to cover most nouns in English using
tens of millions of manually veriﬁed photographs . The image classiﬁcation task of ILSVRC came as a
direct extension of this effort. A subset of categories and
imageswaschosenandﬁxedtoprovideastandardizedbenchmark while the rest of ImageNet continued to grow.
2.2 Single-Object Localization Task
The single-object localization task, introduced in 2011, built
off of the image classiﬁcation task to evaluate the ability of
algorithms to learn the appearance of the target object itself
rather than its image context.
Data for the single-object localization task consists of
the same photographs collected for the image classiﬁcation
task, hand labeled with the presence of one of 1000 object
categories.Eachimagecontainsonegroundtruthlabel.Additionally, every instance of this category is annotated with an
axis-aligned bounding box.
For each image, algorithms produce a list of object categories present in the image, along with a bounding box
indicating the position and scale of one instance of each
object category. The quality of a labeling is evaluated based
on the object category label that best matches the ground
truth label, with the additional requirement that the location
of the predicted instance is also accurate (see Sect. 4.2).
4 www.ﬂickr.com.
2.3 Object Detection Task
The object detection task went a step beyond single-object
localization and tackled the problem of localizing multiple
object categories in the image. This task has been a part of
the PASCAL VOC for many years on the scale of 20 object
categories and tens of thousands of images, but scaling it up
by an order of magnitude in object categories and in images
proved to be very challenging from a dataset collection and
annotation point of view (see Sect. 3.3).
Data for the detection tasks consists of new photographs
collected from Flickr using scene-level queries. The images
are annotated with axis-aligned bounding boxes indicating
the position and scale of every instance of each target object
category. The training set is additionally supplemented with
(a) data from the single-object localization task, which contains annotations for all instances of just one object category,
and (b) negative images known not to contain any instance
of some object categories.
For each image, algorithms produce bounding boxes indicating the position and scale of all instances of all target
object categories. The quality of labeling is evaluated by
recall, or number of target object instances detected, and
precision, or the number of spurious detections produced by
the algorithm (see Sect. 4.3).
3 Dataset Construction at Large Scale
Our process of constructing large-scale object recognition
image datasets consists of three key steps.
The ﬁrst step is deﬁning the set of target object categories. To do this, we select from among the existing
ImageNet categories. By using WordNet
as a backbone , ImageNet already takes care of
disambiguating word meanings and of combining together
synonyms into the same object category. Since the selection
of object categories needs to be done only once per challenge task, we use a combination of automatic heuristics and
manual post-processing to create the list of target categories
appropriate for each task. For example, for image classiﬁca-
Int J Comput Vis 115:211–252
tion we may include broader scene categories such as a type
of beach, but for single-object localization and object detection we want to focus only on object categories which can be
unambiguously localized in images (Sects. 3.1.1, 3.3.1).
The second step is collecting a diverse set of candidate
images to represent the selected categories. We use both
automatic and manual strategies on multiple search engines
to do the image collection. The process is modiﬁed for the
different ILSVRC tasks. For example, for object detection
we focus our efforts on collecting scene-like images using
generic queries such as “African safari” to ﬁnd pictures likely
to contain multiple animals in one scene (Sect. 3.3.2).
The third (and most challenging) step is annotating the
millions of collected images to obtain a clean dataset. We
carefully design crowdsourcing strategies targeted to each
individual ILSVRC task. For example, the bounding box
annotation system used for localization and detection tasks
consists of three distinct parts in order to include automatic crowdsourced quality control (Sect. 3.2.1). Annotating
images fully with all target object categories (on a reasonable
budget) for object detection requires an additional hierarchical image labeling system (Sect. 3.3.3).
We describe the data collection and annotation procedure
for each of the ILSVRC tasks in order: image classiﬁcation
(Sect. 3.1), single-object localization (Sect. 3.2), and object
detection (Sect. 3.3), focusing on the three key steps for each
3.1 Image Classiﬁcation Dataset Construction
The image classiﬁcation task tests the ability of an algorithm
to name the objects present in the image, without necessarily
localizing them.
We describe the choices we made in constructing the
ILSVRC image classiﬁcation dataset: selecting the target
object categories from ImageNet (Sect. 3.1.1), collecting a
diverse set of candidate images by using multiple search
engines and an expanded set of queries in multiple languages
(Sect. 3.1.2), and ﬁnally ﬁltering the millions of collected
images using the carefully designed crowdsourcing strategy
of ImageNet (Sect. 3.1.3).
3.1.1 Deﬁning Object Categories for the Image
Classiﬁcation Dataset
The 1000 categories used for the image classiﬁcation task
were selected from the ImageNet categories. The 1000 synsets are selected such that there is no
overlap between synsets: for any synsets i and j, i is not
an ancestor of j in the ImageNet hierarchy. These synsets
are part of the larger hierarchy and may have children in
ImageNet; however, for ILSVRC we do not consider their
child subcategories. The synset hierarchy of ILSVRC can
be thought of as a “trimmed” version of the complete ImageNet hierarchy. Figure 1 visualizes the diversity of the
ILSVRC2012 object categories.
The exact 1000 synsets used for the image classiﬁcation
and single-object localization tasks have changed over the
years. There are 639 synsets which have been used in all
ﬁve ILSVRC challenges so far. In the ﬁrst year of the challenge synsets were selected randomly from the available
ImageNet synsets at the time, followed by manual ﬁltering to make sure the object categories were not too obscure.
With the introduction of the object localization challenge in
2011 there were 321 synsets that changed: categories such
as “New Zealand beach” which were inherently difﬁcult to
localize were removed, and some new categories from ImageNet containing object localization annotations were added.
In ILSVRC2012, 90 synsets were replaced with categories
corresponding to dog breeds to allow for evaluation of more
ﬁne-grained object classiﬁcation, as shown in Fig. 2. The
synsets have remained consistent since year 2012. Appendix 1 provides the complete list of object categories used in
ILSVRC2012-2014.
3.1.2 Collecting Candidate Images for the Image
Classiﬁcation Dataset
Image collection for ILSVRC classiﬁcation task is the same
as the strategy employed for constructing ImageNet . Training images are taken directly from ImageNet. Additional images are collected for the ILSVRC using
this strategy and randomly partitioned into the validation and
test sets.
We brieﬂy summarize the process; contains further details. Candidate images are collected from the
Internet by querying several image search engines. For each
synset, the queries are the set of WordNet synonyms. Search
engines typically limit the number of retrievable images (on
the order of a few hundred to a thousand). To obtain as many
images as possible, we expand the query set by appending
the queries with the word from parent synsets, if the same
wordappearsintheglossaryofthetargetsynset.Forexample,
whenquerying“whippet”,accordingtoWordNet’sglossarya
“small slender dog of greyhound type developed in England”,
we also use “whippet dog” and “whippet greyhound.” To
further enlarge and diversify the candidate pool, we translate
the queries into other languages, including Chinese, Spanish, Dutch and Italian. We obtain accurate translations using
WordNets in those languages.
3.1.3 Image Classiﬁcation Dataset Annotation
Annotatingimageswithcorrespondingobjectclassesfollows
the strategy employed by ImageNet . We
summarize it brieﬂy here.
Int J Comput Vis 115:211–252
Fig. 1 The diversity of data in
the ILSVRC image
classiﬁcation and single-object
localization tasks. For each of
the eight dimensions, we show
example object categories along
the range of that property.
Object scale, number of
instances and image clutter for
each object category are
computed using the metrics
deﬁned in Sect. 3.2.2 and in
Appendix 1. The other
properties were computed by
asking human subjects to
annotate each of the 1000 object
categories (Russakovsky et al.
To collect a highly accurate dataset, we rely on humans to
verify each candidate image collected in the previous step for
a given synset. This is achieved by using Amazon Mechanical Turk (AMT), an online platform on which one can put
up tasks for users for a monetary reward. With a global user
base, AMT is particularly suitable for large scale labeling.
In each of our labeling tasks, we present the users with a set
of candidate images and the deﬁnition of the target synset
(including a link to Wikipedia). We then ask the users to
verify whether each image contains objects of the synset.
We encourage users to select images regardless of occlusions, number of objects and clutter in the scene to ensure
diversity.
While users are instructed to make accurate judgment, we
need to set up a quality control system to ensure this accuracy. There are two issues to consider. First, human users
make mistakes and not all users follow the instructions. Second, users do not always agree with each other, especially
for more subtle or confusing synsets, typically at the deeper
levels of the tree. The solution to these issues is to have multiple users independently label the same image. An image
is considered positive only if it gets a convincing majority
of the votes. We observe, however, that different categories
require different levels of consensus among users. For example, while ﬁve users might be necessary for obtaining a good
consensus on Burmese cat images, a much smaller number
Int J Comput Vis 115:211–252
Fig. 2 The ILSVRC dataset contains many more ﬁne-grained classes compared to the standard PASCAL VOC benchmark; for example, instead
of the PASCAL “dog” category there are 120 different breeds of dogs in ILSVRC2012-2014 classiﬁcation and single-object localization tasks
is needed for cat images. We develop a simple algorithm to
dynamically determine the number of agreements needed for
different categories of images. For each synset, we ﬁrst randomly sample an initial subset of images. At least 10 users
are asked to vote on each of these images. We then obtain a
conﬁdence score table, indicating the probability of an image
being a good image given the consensus among user votes.
For each of the remaining candidate images in this synset, we
proceed with the AMT user labeling until a pre-determined
conﬁdence score threshold is reached.
Empirical Evaluation Evaluation of the accuracy of the
large-scale crowdsourced image annotation system was done
on the entire ImageNet . A total of 80
synsets were randomly sampled at every tree depth of the
mammal and vehicle subtrees. An independent group of
subjects veriﬁed the correctness of each of the images. An
average of 99.7% precision is achieved across the synsets.
We expect similar accuracy on ILSVRC image classiﬁcation
dataset since the image annotation pipeline has remained the
same. To verify, we manually checked 1500 ILSVRC2012-
2014 image classiﬁcation test set images (the test set has
remained unchanged in these 3 years). We found 5 annotation errors, corresponding as expected to 99.7% precision.
3.1.4 Image Classiﬁcation Dataset Statistics
Using the image collection and annotation procedure
described in previous sections, we collected a large-scale
dataset used for ILSVRC classiﬁcation task. There are 1000
object classes and approximately 1.2 million training images,
50 thousand validation images and 100 thousand test images.
Table 2 documents the size of the dataset over the years of
the challenge.
3.2 Single-Object Localization Dataset Construction
The single-object localization task evaluates the ability of an
algorithm to localize one instance of an object category. It
was introduced as a taster task in ILSVRC 2011, and became
an ofﬁcial part of ILSVRC in 2012.
The key challenge was developing a scalable crowdsourcing method for object bounding box annotation. Our
three-step self-verifying pipeline is described in Sect. 3.2.1.
Having the dataset collected, we perform detailed analysis in
Sect. 3.2.2 to ensure that the dataset is sufﬁciently varied to
be suitable for evaluation of object localization algorithms.
Object Classes and Candidate Images The object classes
for single-object localization task are the same as the object
classes for image classiﬁcation task described above in
Sect. 3.1. The training images for localization task are a subset of the training images used for image classiﬁcation task,
and the validation and test images are the same between both
Bounding Box Annotation Recall that for the image classiﬁcation task every image was annotated with one object class
label, corresponding to one object that is present in an image.
For the single-object localization task, every validation and
Int J Comput Vis 115:211–252
Table 2 Scale of ILSVRC image classiﬁcation task (minimum per class - maximum per class)
Train images
(per class)
Val images
(per class)
Test images
(per class)
Image classiﬁcation annotations (1000 object classes)
ILSVRC2010
1,261,406 (668–3047)
50,000 (50)
150,000 (150)
ILSVRC2011
1,229,413 (384–1300)
50,000 (50)
100,000 (100)
ILSVRC2012-14
1,281,167 (732–1300)
50,000 (50)
100,000 (100)
The numbers in parentheses correspond to (minimum per class–maximum per class). The 1000 classes change from year to year but are consistent
between image classiﬁcation and single-object localization tasks in the same year. All images from the image classiﬁcation task may be used for
single-object localization
Table 3 Scale of additional annotations for the ILSVRC single-object localization task (minimum per class - maximum per class)
Train images
annotations
(per class)
Train bboxes
annotated(per
Val images
annotations
(per class)
Val bboxes
(per class)
Test images
annotations
Additional annotations for single-object localization (1000 object classes)
ILSVRC2011
315,525 (104–1256)
344,233 (114–1502)
50,000 (50)
55,388 (50–118)
ILSVRC2012-14
523,966 (91–1268)
593,173 (92–1418)
50,000 (50)
64,058 (50–189)
The numbers in parentheses correspond to (minimum per class–maximum per class). The 1000 classes change from year to year but are consistent
between image classiﬁcation and single-object localization tasks in the same year. All images from the image classiﬁcation task may be used for
single-object localization
test image and a subset of the training images are annotated
with axis-aligned bounding boxes around every instance of
this object.
Every bounding box is required to be as small as possible while including all visible parts of the object instance.
An alternate annotation procedure could be to annotate the
full (estimated) extent of the object: e.g., if a person’s legs
are occluded and only the torso is visible, the bounding box
could be drawn to include the likely location of the legs. However, this alternative procedure is inherently ambiguous and
ill-deﬁned, leading to disagreement among annotators and
among researchers (what is the true “most likely” extent of
this object?). We follow the standard protocol of only annotating visible object parts .5
3.2.1 Bounding Box Object Annotation System
We summarize the crowdsourced bounding box annotation
system described in detail in Su et al. . The goal is
to build a system that is fully automated, highly accurate,
and cost-effective. Given a collection of images where the
5 Some datasets such as PASCAL VOC 
and LabelMe are able to provide more detailed
annotations: for example, marking individual object instances as being
truncated. We chose not to provide this level of detail in favor of annotating more images and more object instances.
object of interest has been veriﬁed to exist, for each image
the system collects a tight bounding box for every instance
of the object.
There are two requirements:
– Quality Each bounding box needs to be tight, i.e. the
smallest among all bounding boxes that contains all visible parts of the object. This facilitates the object detection
learning algorithms by providing the precise location of
each object instance;
– Coverage Every object instance needs to have a bounding
box. This is important for training localization algorithms
because it tells the learning algorithms with certainty
what is not the object.
The core challenge of building such a system is effectively
controlling the data quality with minimal cost. Our key observation is that drawing a bounding box is signiﬁcantly more
difﬁcult and time consuming than giving answers to multiple choice questions. Thus quality control through additional
veriﬁcationtasksismorecost-effectivethanconsensus-based
algorithms. This leads to the following workﬂow with simple
basic subtasks:
(1) Drawing A worker draws one bounding box around one
instance of an object on the given image.
Int J Comput Vis 115:211–252
(2) QualityveriﬁcationAsecondworkerchecksifthebounding box is correctly drawn.
(3) Coverage veriﬁcation A third worker checks if all object
instances have bounding boxes.
The sub-tasks are designed following two principles. First,
the tasks are made as simple as possible. For example, instead
of asking the worker to draw all bounding boxes on the same
image, we ask the worker to draw only one. This reduces
the complexity of the task. Second, each task has a ﬁxed and
predictable amount of work. For example, assuming that the
input images are clean (object presence is correctly veriﬁed)
and the coverage veriﬁcation tasks give correct results, the
amount of work of the drawing task is always that of providing exactly one bounding box.
Quality control on Tasks 2 and 3 is implemented by
embedding “gold standard” images where the correct answer
is known. Worker training for each of these subtasks is
described in detail in Su et al. .
Empirical Evaluation The system is evaluated on 10 categories with ImageNet : balloon, bear, bed,
bench, beach, bird, bookshelf, basketball hoop, bottle, and
people. A subset of 200 images are randomly sampled from
each category. On the image level, our evaluation shows that
97.9% images are completely covered with bounding boxes.
For the remaining 2.1%, some bounding boxes are missing.
However, these are all difﬁcult cases: the size is too small,
the boundary is blurry, or there is strong shadow.
On the bounding box level, 99.2% of all bounding boxes
are accurate (the bounding boxes are visibly tight). The
remaining 0.8% are somewhat off. No bounding boxes are
found to have less than 50% intersection over union overlap
with ground truth.
Additional evaluation of the overall cost and an analysis
of quality control can be found in Su et al. .
3.2.2 Single-Object Localization Dataset Statistics
Using the annotation procedure described above, we collect
a large set of bounding box annotations for the ILSVRC
single-object classiﬁcation task. All 50 thousand images in
the validation set and 100 thousand images in the test set
are annotated with bounding boxes around all instances of
the ground truth object class (one object class per image). In
addition, in ILSVRC2011 25% of training images are annotated with bounding boxes the same way, yielding more than
310 thousand annotated images with more than 340 thousand
annotated object instances. In ILSVRC2012 40% of training images are annotated, yielding more than 520 thousand
annotated images with more than 590 thousand annotated
object instances. Table 3 documents the size of this dataset.
In addition to the size of the dataset, we also analyze the level of difﬁculty of object localization in these
images compared to the PASCAL VOC benchmark. We compute statistics on the ILSVRC2012 single-object localization
validation set images compared to PASCAL VOC 2012 validation images.
Real-world scenes are likely to contain multiple instances
of some objects, and nearby object instances are particularly difﬁcult to delineate. The average object category in
ILSVRC has 1.61 target object instances on average per
positive image, with each instance having on average 0.47
neighbors (adjacent instances of the same object category).
This is comparable to 1.69 instances per positive image and
0.52 neighbors per instance for an average object class in
As described in Hoiem et al. , smaller objects tend
to be signiﬁcantly more difﬁcult to localize. In the average
object category in PASCAL the object occupies 24.1% of
the image area, and in ILSVRC 35.8%. However, PASCAL
has only 20 object categories while ILSVRC has 1000. The
537 object categories of ILSVRC with the smallest objects
on average occupy the same fraction of the image as PAS-
CAL objects: 24.1%. Thus even though on average the object
instances tend to be bigger in ILSVRC images, there are more
than 25 times more object categories than in PASCAL VOC
with the same average object scale.
Appendix 1 and Russakovsky et al. have additional
comparisons.
3.3 Object Detection Dataset Construction
The ILSVRC task of object detection evaluates the ability
of an algorithm to name and localize all instances of all
target objects present in an image. It is much more challenging than object localization because some object instances
may be small/occluded/difﬁcult to accurately localize, and
the algorithm is expected to locate them all, not just the one
it ﬁnds easiest.
There are three key challenges in collecting the object
detection dataset. The ﬁrst challenge is selecting the set
of common objects which tend to appear in cluttered photographs and are well-suited for benchmarking object detection performance. Our approach relies on statistics of the
object localization dataset and the tradition of the PASCAL
VOC challenge (Sect. 3.3.1).
The second challenge is obtaining a much more varied
set of scene images than those used for the image classiﬁcation and single-object localization datasets. Section 3.3.2
describes the procedure for utilizing as much data from
the single-object localization dataset as possible and supplementing it with Flickr images queried using hundreds of
manually designed high-level queries.
Int J Comput Vis 115:211–252
The third, and biggest, challenge is completely annotating
this dataset with all the objects. This is done in two parts.
Section 3.3.3 describes the ﬁrst part: our hierarchical strategy
for obtaining the list of all target objects which occur within
every image. This is necessary since annotating in a straightforward way by creating a task for every (image, object class)
pair is no longer feasible at this scale. Appendix 1 describes
the second part: annotating the bounding boxes around these
objects, using the single-object localization bounding box
annotationpipelineofSect.3.2.1alongwithextraveriﬁcation
to ensure that every instance of the object is annotated with
exactly one bounding box.
3.3.1 Deﬁning Object Categories for the Object Detection
There are 200 object classes hand-selected for the detection
task, eacg corresponding to a synset within ImageNet. These
were chosen to be mostly basic-level object categories that
would be easy for people to identify and label. The rationale is that the object detection system developed for this
task can later be combined with a ﬁne-grained classiﬁcation
model to further classify the objects if a ﬁner subdivision is
desired.6 As with the 1000 classiﬁcation classes, the synsets
are selected such that there is no overlap: for any synsets i
and j, i is not an ancestor of j in the ImageNet hierarchy.
The selection of the 200 object detection classes in 2013
was guided by the ILSVRC 2012 classiﬁcation and localization dataset. Starting with 1000 object classes and their
bounding box annotations we ﬁrst eliminated all object
classes which tended to be too “big” in the image (on average the object area was greater than 50% of the image area).
These were classes such as T-shirt, spiderweb, or manhole
cover. We then manually eliminated all classes which we
did not feel were well-suited for detection, such as hay,
barbershop, or poncho. This left 494 object classes which
were merged into basic-level categories: for example, different species of birds were merged into just the “bird” class.
The classes remained the same in ILSVRC2014. Appendix 1 contains the complete list of object categories used in
ILSVRC2013-2014 (in the context of the hierarchy described
in Sect. 3.3.3).
Staying mindful of the tradition of the PASCAL VOC
dataset we also tried to ensure that the set of 200 classes
contains as many of the 20 PASCAL VOC classes as possible. Table 4 shows the correspondences. The changes that
were done were to ensure more accurate and consistent
crowdsourced annotations. The object class with the weakest
correspondence is “potted plant” in PASCAL VOC, corre-
6 Some of the training objects are actually annotated with more detailed
classes: for example, one of the 200 object classes is the category “dog,”
and some training instances are annotated with the speciﬁc dog breed.
Table 4 Correspondences between the object classes in the PASCAL
VOC and the ILSVRC detection task
Class name in
PASCAL VOC
(20 classes)
Closest class in
ILSVRC-DET
(200 classes)
Average object scale (%)
PASCAL VOC ILSVRC-DET
Watercraft
Wine bottle
Domestic cat
Dining table
Potted plant
Flower pot
TV/monitor
TV or monitor
Object scale is the fraction of image area (reported in percent) occupied by an object instance. It is computed on the validation sets of
PASCAL VOC 2012 and of ILSVRC-DET. The average object scale is
24.1% across the 20 PASCAL VOC categories and 20.3% across the 20
corresponding ILSVRC-DET categories. Sect. 3.3.4 reports additional
dataset statistics
sponding to “ﬂower pot” in ILSVRC. “Potted plant” was
one of the most challenging object classes to annotate consistently among the PASCAL VOC classes, and in order to
obtain accurate annotations using crowdsourcing we had to
restrict the deﬁnition to a more concrete object.
3.3.2 Collecting Images for the Object Detection Dataset
Many images for the detection task were collected differently than the images in ImageNet and the classiﬁcation
and single-object localization tasks. Figure 3 summarizes
the types of images that were collected. Ideally all of these
images would be scene images fully annotated with all target
categories. However, given budget constraints our goal was
to provide as much suitable detection data as possible, even
if the images were drawn from a few different sources and
distributions.
The validation and test detection set images come from
two sources (percent of images from each source in parentheses). The ﬁrst source (77%) is images from ILSVRC2012
Int J Comput Vis 115:211–252
Fig. 3 Summary of images collected for the detection task. Images in green (bold) boxes have all instances of all 200 detection object classes fully
annotated. Table 5 lists the complete statistics
single-object localization validation and test sets corresponding to the 200 detection classes (or their children in the
ImageNet hierarchy). Images where the target object occupied more than 50% of the image area were discarded, since
they were unlikely to contain other objects of interest. The
second source (23%) is images from Flickr collected specifically for detection task. We queried Flickr using a large set
of manually deﬁned queries, such as “kitchenette” or “Australian zoo” to retrieve images of scenes likely to contain
several objects of interest. Appendix 1 contains the full list.
We also added pairwise queries, or queries with two target
object names such as “tiger lion,” which also often returned
cluttered scenes.
Figure 4 shows a random set of both types of validation
images. Images were randomly split, with 33% going into
the validation set and 67% into the test set.7
The training set for the detection task comes from three
sources of images (percent of images from each source in
parentheses). The ﬁrst source (63%) is all training images
from ILSVRC2012 single-object localization task corresponding to the 200 detection classes (or their children in
the ImageNet hierarchy). We did not ﬁlter by object size,
allowing teams to take advantage of all the positive examples available. The second source (24%) is negative images
which were part of the original ImageNet collection process
but voted as negative: for example, some of the images were
collected from Flickr and search engines for the ImageNet
synset “animals” but during the manual veriﬁcation step did
7 The validation/test split is consistent with ILSVRC2012: validation
images of ILSVRC2012 remained in the validation set of ILSVRC2013,
and ILSVRC2012 test images remained in ILSVRC2013 test set.
not collect enough votes to be considered as containing an
“animal.” These images were manually re-veriﬁed for the
detection task to ensure that they did not in fact contain the
target objects. The third source (13%) is images collected
from Flickr speciﬁcally for the detection task. These images
were added for ILSVRC2014 following the same protocol
as the second type of images in the validation and test set.
This was done to bring the training and testing distributions
closer together.
3.3.3 Complete Image-Object Annotation for the Object
Detection Dataset
The key challenge in annotating images for the object detection task is that all objects in all images need to be labeled.
Suppose there are N inputs (images) which need to be annotated with the presence or absence of K labels (objects). A
naïve approach would query humans for each combination of
input and label, requiring N K queries. However, N and K can
be very large and the cost of this exhaustive approach quickly
becomes prohibitive. For example, annotating 60,000 validation and test images with the presence or absence of 200
object classes for the detection task naïvely would take 80
times more effort than annotating 150,000 validation and
test images with 1 object each for the classiﬁcation task—
and this is not even counting the additional cost of collecting
bounding box annotations around each object instance. This
quickly becomes infeasible.
In Deng et al. we study strategies for scalable multilabel annotation, or for efﬁciently acquiring multiple labels
from humans for a collection of items. We exploit three key
Int J Comput Vis 115:211–252
Fig. 4 Random selection of images in ILSVRC detection validation set. The images in the top four rows were taken from ILSVRC2012 single-object
localization validation set, and the images in the bottom four rows were collected from Flickr using scene-level queries
observations for labels in real world applications (illustrated
in Fig. 5):
(1) Correlation Subsets of labels are often highly correlated. Objects such as a computer keyboard, mouse and
monitor frequently co-occur in images. Similarly, some
labels tend to all be absent at the same time. For example, all objects that require electricity are usually absent
in pictures taken outdoors. This suggests that we could
potentially ﬁll in the values of multiple labels by grouping them into only one query for humans. Instead of
checking if dog, cat, rabbit etc. are present in the photo,
we just check about the “animal” group If the answer is
no, then this implies a no for all categories in the group.
(2) Hierarchy The above example of grouping dog, cat, rabbit etc. into animal has implicitly assumed that labels can
be grouped together and humans can efﬁciently answer
queries about the group as a whole. This brings up
our second key observation: humans organize semantic concepts into hierarchies and are able to efﬁciently
Int J Comput Vis 115:211–252
Fig. 5 Consider the problem of
binary multi-label annotation.
For each input (e.g., image) and
each label (e.g., object), the goal
is to determine the presence or
absense (plus or minus) of the
label (e.g., decide if the object is
present in the image).
Multi-label annotation becomes
much more efﬁcient when
considering real-world structure
of data: correlation between
labels, hierarchical organization
of concepts, and sparsity of
Fig. 6 Our algorithm dynamically selects the next query to efﬁciently determine the presence or absence of every object in every image. Green
denotes a positive annotation and red denotes a negative annotation. This toy example illustrates a sample progression of the algorithm for one
label (cat) on a set of images
categorize at higher semantic levels ,
e.g. humans can determine the presence of an animal in
an image as fast as every type of animal individually.
This leads to substantial cost savings.
(3) Sparsity The values of labels for each image tend to be
sparse, i.e. an image is unlikely to contain more than a
dozen types of objects, a small fraction of the hundreds
of object categories. This enables rapid elimination of
many objects by quickly ﬁlling in no. With a high degree
of sparsity, an efﬁcient algorithm can have a cost which
growslogarithmicallywiththenumberofobjectsinstead
of linearly.
We propose algorithmic strategies that exploit the above
intuitions. The key is to select a sequence of queries for
humans such that we achieve the same labeling results with
only a fraction of the cost of the naïve approach. The main
challenges include how to measure cost and utility of queries,
how to construct good queries, and how to dynamically order
them. A detailed description of the generic algorithm, along
with theoretical analysis and empirical evaluation, is presented in Deng et al. .
Application of the Generic Multi-class Labeling Algorithm
to Our Setting The generic algorithm automatically selects
the most informative queries to ask based on object label statistics learned from the training set. In our case of 200 object
classes, since obtaining the training set was by itself challenging we chose to design the queries by hand. We created
a hierarchy of queries of the type “is there a... in the image?”
For example, one of the high-level questions was “is there an
animal in the image?” We ask the crowd workers this question about every image we want to label. The children of the
“animal” question would correspond to speciﬁc examples of
animals: for example, “is there a mammal in the image?” or
“is there an animal with no legs?” To annotate images efﬁciently, these questions are asked only on images determined
to contain an animal. The 200 leaf node questions correspond
to the 200 target objects, e.g., “is there a cat in the image?”.
A few sample iterations of the algorithm are shown in Fig. 6.
Algorithm 1 is the formal algorithm for labeling an image
with the presence or absence of each target object category.
With this algorithm in mind, the hierarchy of questions was
constructed following the principle that false positives only
add extra cost whereas false negatives can signiﬁcantly affect
the quality of the labeling. Thus, it is always better to stick
with more general but less ambiguous questions, such as “is
there a mammal in the image?” as opposed to asking overly
speciﬁc but potentially ambiguous questions, such as “is
there an animal that can climb trees?” Constructing this hierarchy was a surprisingly time-consuming process, involving
multiple iterations to ensure high accuracy of labeling and
avoid question ambiguity. Appendix 1 shows the constructed
hierarchy.
Int J Comput Vis 115:211–252
Input: Image i, queries Q, directed graph G over Q
Output: Labels L : Q →{“yes”, “no”}
Initialize labels L(q) = ∅∀q ∈Q;
Initialize candidates C = {q: q ∈Root(G)};
while C not empty do
Obtain answer A to query q∗∈C;
L(q∗) = A; C = C\{q∗};
if A is “yes” then
Chldr = {q ∈Children(q∗, G): L(q) = ∅};
C = C ∪Chldr;
Des = {q ∈Descendants(q∗, G): L(q) = ∅};
L(q) = “no′′ ∀q ∈Des;
C = C\Des;
Algorithm 1: The algorithm for complete multi-class annotation. This is a special case of the algorithm described in
Deng et al. . A hierarchy of questions G is manually
constructed. All root questions are asked on every image. If
the answer to query q∗on image i is “no” then the answer is
assumed to be “no” for all queries q such that q is a descendant of q∗in the hierarchy. We continue asking the queries
until all queries are answered. For images taken from the
single-object localization task we used the known object
label to initialize L.
Bounding Box Annotation Once all images are labeled with
the presence or absence of all object categories we use the
bounding box system described in Sect. 3.2.1 along with
some additional modiﬁcations of Appendix 1 to annotate the
location of every instance of every present object category.
3.3.4 Object Detection Dataset Statistics
Using the procedure described above, we collect a largescale dataset for ILSVRC object detection task. There are 200
object classes and approximately 450K training images, 20K
validation images and 40K test images. Table 5 documents
the size of the dataset over the years of the challenge. The
major change between ILSVRC2013 and ILSVRC2014 was
the addition of 60,658 fully annotated training images.
Prior to ILSVRC, the object detection benchmark was the
PASCAL VOC challenge . ILSVRC
has 10 times more object classes than PASCAL VOC (200 vs
20), 10.6 times more fully annotated training images (60,658
vs 5,717), 35.2 times more training objects (478,807 vs
13,609), 3.5 times more validation images (20,121 vs 5823)
and 3.5 times more validation objects (55,501 vs 15,787).
ILSVRC has 2.8 annotated objects per image on the validation set, compared to 2.7 in PASCAL VOC. The average
object in ILSVRC takes up 17.0% of the image area and in
PASCAL VOC takes up 20.7%; Table 4 contains per-class
comparisons. Additionally, ILSVRC contains a wide variety
of objects, including tiny objects such as sunglasses (1.3%
of image area on average), ping-pong balls (1.5% of image
area on average) and basketballs for image classiﬁcation and PASCAL VOC for both
image classiﬁcation and object detection. To adapt these procedures to the large-scale setting we had to address three
key challenges. First, for the image classiﬁcation and singleobject localization tasks only one object category could be
labeled in each image due to the scale of the dataset. This
created potential ambiguity during evaluation (addressed in
Sect. 4.1). Second, evaluating localization of object instances
is inherently difﬁcult in some images which contain a cluster
of objects (addressed in Sect. 4.2). Third, evaluating localization of object instances which occupy few pixels in the
image is challenging (addressed in Sect. 4.3).
In this section we describe the standardized evaluation criteria for each of the three ILSVRC tasks. We elaborate further
on these and other more minor challenges with large-scale
evaluation. Appendix 1 describes the submission protocol
and other details of running the competition itself.
Table 5 Scale of ILSVRC object detection task
Train images (per class)
Train bboxes annotated
(per class)
Val images (per class)
Val bboxes annotated
(per class)
Test images
Object detection annotations (200 object classes)
ILSVRC2013
395909 (417-561-66911
pos, 185-4130-10073 neg)
(438-660-73799)
21121 (23-58-5791 pos,
55501 (31-111-12824)
ILSVRC2014
456567 (461-823-67513
pos, 42945-64614-70626
(502-1008-74517)
21121 (23-58-5791 pos,
55501 (31-111-12824)
Numbers in parentheses correspond to (minimum per class–median per class–maximum per class)
Int J Comput Vis 115:211–252
Fig. 7 Tasks in ILSVRC. The ﬁrst column shows the ground truth labeling on an example image, and the next three show three sample outputs
with the corresponding evaluation score
4.1 Image Classiﬁcation
The scale of ILSVRC classiﬁcation task (1000 categories and
more than a million of images) makes it very expensive to
label every instance of every object in every image. Therefore, on this dataset only one object category is labeled in
each image. This creates ambiguity in evaluation. For example, an image might be labeled as a “strawberry” but contain
both a strawberry and an apple. Then an algorithm would not
know which one of the two objects to name. For the image
classiﬁcation task we allowed an algorithm to identify multiple (up to 5) objects in an image and not be penalized as
long as one of the objects indeed corresponded to the ground
truth label. Figure 7 (top row) shows some examples.
Concretely, each image i has a single class label Ci. An
algorithm is allowed to return 5 labels ci1, . . . ci5, and is considered correct if ci j = Ci for some j.
Let the error of a prediction di j = d(ci j, Ci) be 1 if ci j ̸=
Ci and 0 otherwise. The error of an algorithm is the fraction
of test images on which the algorithm makes a mistake:
We used two additional measures of error. First, we evaluatedtop-1error.Inthiscasealgorithmswerepenalizediftheir
highest-conﬁdence output label ci1 did not match ground
truth class Ci. Second, we evaluated hierarchical error. The
intuition is that confusing two nearby classes (such as two
different breeds of dogs) is not as harmful as confusing a dog
for a container ship. For the hierarchical criteria, the cost of
one misclassiﬁcation, d(ci j, Ci), is deﬁned as the height of
the lowest common ancestor of ci j and Ci in the ImageNet
hierarchy. The height of a node is the length of the longest
path to a leaf node (leaf nodes have height zero).
However, in practice we found that all three measures
of error (top-5, top-1, and hierarchical) produced the same
ordering of results. Thus, since ILSVRC2012 we have been
exclusively using the top-5 metric which is the simplest and
most suitable to the dataset.
Int J Comput Vis 115:211–252
Fig. 8 Images marked as “difﬁcult” in the ILSVRC2012 single-object localization validation set. Please refer to Sect. 4.2 for details
4.2 Single-Object Localization
The evaluation for single-object localization is similar to
object classiﬁcation, again using a top-5 criteria to allow
the algorithm to return unannotated object classes without
penalty. However, now the algorithm is considered correct
only if it both correctly identiﬁes the target class Ci and
accurately localizes one of its instances. Figure 7 (middle
row) shows some examples.
Concretely, an image is associated with object class Ci,
withall instances of this object class annotatedwithbounding
boxes Bik. An algorithm returns {(ci j, bi j)}5
j=1 of class labels
ci j and associated locations bi j. The error of a prediction j
di j = max(d(ci j, Ci), min
d(bi j, Bik))
Here d(bi j, Bik) is the error of localization, deﬁned as 0 if the
area of intersection of boxes bi j and Bik divided by the areas
of their union is greater than 0.5, and 1 otherwise . The error of an algorithm is computed as
Evaluating localization is inherently difﬁcult in some
images. Consider a picture of a bunch of bananas or a carton
of apples. It is easy to classify these images as containing bananas or apples, and even possible to localize a few
instances of each fruit. However, in order for evaluation
to be accurate every instance of banana or apple needs to
be annotated, and that may be impossible. To handle the
images where localizing individual object instances is inherently ambiguous we manually discarded 3.5% of images
since ILSVRC2012. Some examples of discarded images are
shown in Fig. 8.
4.3 Object Detection
The criteria for object detection was adopted from PASCAL
VOC . It is designed to penalize
the algorithm for missing object instances, for duplicate
detections of one instance, and for false positive detections.
Figure 7(bottom row) shows examples.
For each object class and each image Ii, an algorithm
returns predicted detections (bi j, si j) of predicted locations
Input: Bounding box predictions with conﬁdence scores
{(b j, s j)}M
j=1 and ground truth boxes B on image I for a
given object class.
Output: Binary results {z j}M
j=1 of whether or not prediction j is
a true positive detection
Let U = B be the set of unmatched objects;
Order {(b j, s j)}M
j=1 in descending order of s j;
for j=1 …M do
Let C = {Bk ∈U : IOU(Bk, b j) ≥thr(Bk)};
if C ̸= ∅then
Let k∗= arg max{k : Bk∈C} IOU(Bk, b j);
Set U = U\Bk∗;
Set z j = 1 since true positive detection;
Set z j = 0 since false positive detection;
Algorithm 2: The algorithm for greedily matching object
detection outputs to ground truth labels. The standard
thr(Bk) = 0.5 . ILSVRC computes thr(Bk) using Eq. 5 to better handle low-resolution
bi j with conﬁdence scores si j. These detections are greedily
matched to the ground truth boxes {Bik} using Algorithm 2.
Foreverydetection j onimagei thealgorithmreturns zi j = 1
if the detection is matched to a ground truth box according
to the threshold criteria, and 0 otherwise. For a given object
class, let N be the total number of ground truth instances
across all images. Given a threshold t, deﬁne recall as the
fraction of the N objects detected by the algorithm, and precision as the fraction of correct detections out of the total
detections returned by the algorithm. Concretely,
Recall(t) =
i j 1[si j ≥t]zi j
Precision(t) =
i j 1[si j ≥t]zi j
i j 1[si j ≥t]
The ﬁnal metric for evaluating an algorithm on a given
object class is average precision over the different levels of
recall achieved by varying the threshold t. The winner of
each object class is then the team with the highest average
Int J Comput Vis 115:211–252
precision, and then winner of the challenge is the team that
wins on the most object classes.8
Difference with PASCAL VOC Evaluating localization of
object instances which occupy very few pixels in the image
is challenging. The PASCAL VOC approach was to label
such instances as “difﬁcult” and ignore them during evaluation. However, since ILSVRC contains a more diverse set of
object classes including, for example, “nail” and “ping pong
ball” which have many very small instances, it is important
to include even very small object instances in evaluation.
In Algorithm 2, a predicted bounding box b is considered
to have properly localized by a ground truth bounding box
B if I OU(b, B) ≥thr(B). The PASCAL VOC metric uses
the threshold thr(B) = 0.5. However, for small objects even
deviations of a few pixels would be unacceptable according
to this threshold. For example, consider an object B of size
10 × 10 pixels, with a detection window of 20 × 20 pixels
which fully contains that object. This would be an error of
approximately 5 pixels on each dimension, which is average
human annotation error. However, the IOU in this case would
be 100/400 = 0.25, far below the threshold of 0.5. Thus for
smaller objects we loosen the threshold in ILSVRC to allow
for the annotation to extend up to 5 pixels on average in each
direction around the object. Concretely, if the ground truth
box B is of dimensions w × h then
thr(B) = min
(w + 10)(h + 10)
In practice, this changes the threshold only on objects which
are smaller than approximately 25 × 25 pixels, and affects
5.5% of objects in the detection validation set.
Practical Consideration One additional practical consideration for ILSVRC detection evaluation is subtle and comes
directly as a result of the scale of ILSVRC. In PASCAL,
algorithms would often return many detections per class on
the test set, including ones with low conﬁdence scores. This
allowed the algorithms to reach the level of high recall at
least in the realm of very low precision. On ILSVRC detection test set if an algorithm returns 10 bounding boxes per
object per image this would result in 10×200×40K = 80M
detections. Each detection contains an image index, a class
index, 4 bounding box coordinates, and the conﬁdence score,
so it takes on the order of 28 bytes. The full set of detections
would then require 2.24Gb to store and submit to the evaluation server, which is impractical. This means that algorithms
8 In this paper we focus on the mean average precision across all
categories as the measure of a team’s performance. This is done for
simplicity and is justiﬁed since the ordering of teams by mean average
precision was always the same as the ordering by object categories won.
are implicitly required to limit their predictions to only the
most conﬁdent locations.
The ILSVRC dataset and the competition has allowed significant algorithmic advances in large-scale image recognition
and retrieval.
5.1 Challenge Entries
This section is organized chronologically, highlighting the
particularly innovative and successful methods which participated in the ILSVRC each year. Tables 6, 7 and 8 list all the
participating teams. We see a turning point in 2012 with the
development of large-scale convolutional neural networks.
ILSVRC2010 The ﬁrst year the challenge consisted of just
the classiﬁcation task. The winning entry from NEC team
 used SIFT and LBP features with two non-linear coding representations and a stochastic
SVM. The honorable mention XRCE team used an improved Fisher vector representation along with PCA dimensionality
reduction and data compression followed by a linear SVM.
Fisher vector-based methods have evolved over 5 years of
the challenge and continued performing strongly in every
ILSVRC from 2010 to 2014.
ILSVRC2011 The winning classiﬁcation entry in 2011 was
the 2010 runner-up team XRCE, applying high-dimensional
image signatures with compression
using product quantization 
and one-vs-all linear SVMs. The single-object localization
competition was held for the ﬁrst time, with two brave entries.
The winner was the UvA team using a selective search
approach to generate class-independent object hypothesis
regions , followed by dense sampling and vector quantization of several color SIFT features
 , pooling with spatial pyramid
matching , and classifying with a
histogram intersection kernel SVM 
trained on a GPU .
ILSVRC2012 This was a turning point for large-scale object
recognition, when large-scale deep neural networks entered
the scene. The undisputed winner of both the classiﬁcation
and localization tasks in 2012 was the SuperVision team.
They trained a large, deep convolutional neural network on
RGB values, with 60 million parameters using an efﬁcient
GPU implementation and a novel hidden-unit dropout trick
Int J Comput Vis 115:211–252
Table 6 Teams participating in ILSVRC2010-2012, ordered alphabetically
Insitutions
Contributors and references
ILSVRC 2010
Massachusetts Institute of Technology
Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan
Salakhutdinov, Stan Bileschi, Hueihan Jhuang
IBM research†, Georgia Tech‡
Lexing Xie†, Hua Ouyang‡, Apostol Natsev†
Intelligent Systems and Informatics Lab., The
University of Tokyo
Tatsuya Harada, Hideki Nakayama, Yoshitaka
Ushiku, Yuya Yamashita, Jun Imura, Yasuo
Harbin Institute of Technology
Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang,
Bingquan Liu, Chengjie Sun
Laboratoire d’Informatique de Grenoble
Georges Quénot
NEC Labs America†, University of Illinois at
Urbana-Champaign‡, Rutgers∓
Yuanqing Lin†, Fengjun Lv†, Shenghuo Zhu†, Ming
Yang†, Timothee Cour†, Kai Yu†, LiangLiang
Cao‡, Zhen Li‡, Min-Hsuan Tsai‡, Xi Zhou‡,
Thomas Huang‡, Tong Zhang∓ 
National Institute of Informatics,
Tokyo,Japan†, Hefei Normal Univ. Heifei,
Cai-Zhi Zhu†, Xiao Zhou‡, Shiníchi Satoh†
CeMNet, SCE, NTU, Singapore
Zhengxiang Wang, Liang-Tien Chia
University of California Irvine
Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes
Xerox Research Centre Europe
Jorge Sanchez, Florent Perronnin, Thomas Mensink
 
ILSVRC 2011
Intelligent Systems and Informatics lab,
University of Tokyo
Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku,
Yuya Yamashita, Sho Inaba, Hiroshi Muraoka,
Yasuo Kuniyoshi
National Institute of Informatics, Japan
Duy-Dinh Le, Shiníchi Satoh
University of Amsterdam†, University of
Koen E. A. van de Sande†, Jasper R. R. Uijlings‡,
Arnold W. M. Smeulders†, Theo Gevers†, Nicu
Sebe‡, Cees Snoek† 
Xerox Research Centre Europe†, CIII‡
Florent Perronnin†, Jorge Sanchez†‡ 
ILSVRC 2012
University of Tokyo†, JST PRESTO‡
Naoyuki Gunji†, Takayuki Higuchi†, Koki
Yasumoto†, Hiroshi Muraoka†, Yoshitaka Ushiku†,
Tatsuya Harada†‡, Yasuo Kuniyoshi† 
LEAR INRIA Grenoble†, TVPA Xerox
Research Centre Europe‡
Thomas Mensink†‡, Jakob Verbeek†, Florent
Perronnin‡, Gabriela Csurka‡ 
University of Oxford
Karen Simonyan, Yusuf Aytar, Andrea Vedaldi,
Andrew Zisserman 
SuperVision
University of Toronto
Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton
 
University of Amsterdam
Koen E. A. van de Sande, Amir Habibian, Cees G.
M. Snoek 
Xerox Research Centre Europe†, LEAR
Florent Perronnin†, Zeynep Akata†‡, Zaid
Harchaoui‡, Cordelia Schmid‡(Perronnin et al.
Each method is identiﬁed with a codename used in the text. We report ﬂat top-5 classiﬁcation and single-object localization error, in percents (lower
is better). For teams which submitted multiple entries we report the best score. In 2012, SuperVision also submitted entries trained with the extra
data from the ImageNet Fall 2011 release, and obtained 15.3% classiﬁcation error and 33.5% localization error. Key references are provided where
available. More details about the winning entries can be found in Sect. 5.1
Int J Comput Vis 115:211–252
Table 7 Teams participating in ILSVRC2013, ordered alphabetically
Insitutions
Contributors and references
ILSVRC 2013
Adobe†, University of Illinois at
Urbana-Champaign‡
Hailin Jin†, Zhe Lin†, Jianchao Yang†, Tom
Paine‡ 
Andrew Howard Consulting
Andrew Howard
Beijing University of Posts and
Telecommunications†, Orange Labs
International Center Beijing‡
Chong Huang†, Yunlong Bian†, Hongliang Bai‡, Bo
Liu†, Yanchao Feng†, Yuan Dong†
Matthew Zeiler 
Saint Petersburg State University
Evgeny Smirnov, Denis Timoshenko, Alexey
Korolev 
National Tsing Hua University
Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping
Kang, Tzu-Wei Huang, Ci-Hong Deng, Hao-Che
University of Illinois at Urbana-Champaign†,
IBM Watson Research Center‡, IBM Haifa
Research Center∓
Zhicheng Yan†, Liangliang Cao‡, John R Smith‡,
Noel Codella‡,Michele Merler‡, Sharath
Pankanti‡, Sharon Alpert∓, Yochay Tzur∓,
University of Tokyo
Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta,
Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada
Peking University†, Microsoft Research‡,
Shanghai Jiao Tong University∓, XiDian
University§, Harbin Institute of
Technologyς
Tianjun Xiao†‡, Minjie Wang∓‡, Jianpeng Li§‡,
Yalong Baiς ‡, Jiaxing Zhang‡, Kuiyuan Yang‡,
Chuntao Hong‡, Zheng Zhang‡ 
NEC Labs America†, University of Missouri ‡
Xiaoyu Wang†, Miao Sun‡, Tianbao Yang†,
Yuanqing Lin†, Tony X. Han‡, Shenghuo
Zhu† 
National University of Singapore
Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang,
Wei Xia, Shuicheng Yan (* = equal contribution)
 
Orange Labs International Center Beijing†,
Beijing University of Posts and
Telecommunications‡
Hongliang BAI†, Lezi Wang‡, Shusheng Cen‡,
YiNan Liu‡, Kun Tao†, Wei Liu†, Peng Li†, Yuan
New York University
Pierre Sermanet, David Eigen, Michael Mathieu,
Xiang Zhang, Rob Fergus, Yann LeCun 
Self-employed†, Student in Troy High
School, Fullerton, CA‡
Henry Shu†, Jerry Shu‡ 
Sun Yat-Sen University, China.
Xiaolong Wang 
University of Toronto
Yichuan Tang*, Nitish Srivastava*, Ruslan
Salakhutdinov (* = equal contribution)
The Third Research Institute of the Ministry
of Public Security, P.R. China
Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei
Wang, Lin Mei, Chuanping Hu
University of California Los Angeles
Yukun Zhu, Jun Zhu, Alan Yuille
University of Illinois at Urbana-Champaign
Thomas Paine, Kevin Shih, Thomas Huang
 
Int J Comput Vis 115:211–252
Table 7 continued
Insitutions
Contributors and references
University of Amsterdam, Euvision
Technologies
Koen E. A. van de Sande, Daniel H. F. Fontijne,
Cees G. M. Snoek, Harro M. G. Stokman, Arnold
W. M. Smeulders 
Visual Geometry Group, University of Oxford
Karen Simonyan, Andrea Vedaldi, Andrew
Zisserman 
New York University
Matthew D Zeiler, Rob Fergus 
Each method is identiﬁed with a codename used in the text. For classiﬁcaton and single-object localization we report ﬂat top-5 error, in percents
(lower is better). For detection we report mean average precision, in percents (higher is better). Even though the winner of the challenge was
determined by the number of object categories won, this correlated strongly with mAP. Parentheses indicate the team used outside training data and
was not part of the ofﬁcial competition. Some competing teams also submitted entries trained with outside data: Clarifai with 11.2% classiﬁcation
error, NEC with 20.9% detection mAP. Key references are provided where available. More details about the winning entries can be found in Sect. 5.1
Table 8 Teams participating in ILSVRC2014, ordered alphabetically
Insitutions
Contributors and references
ILSVRC 2014
Adobe†, UIUC‡
Hailin Jin†, Zhaowen Wang‡, Jianchao
Yang†, Zhe Lin†
Howard Vision Technologies
Andrew Howard 
Institute for Infocomm
Research†, Universit Pierre et
Marie Curie‡
Olivier Morre†‡, Hanlin Goh†, Antoine
Veillard‡, Vijay
Chandrasekhar† 
UC Berkeley
Ross Girshick, Jeff Donahue, Sergio
Guadarrama, Trevor Darrell, Jitendra Malik
 
KAIST department of EE
Jun-Cheol Park, Yunhun Jang, Hyungwon
Choi, JaeYoung Jun 
Chinese Academy of Science†,
Southeast University‡
Peihao Huang†, Yongzhen Huang†, Feng
Liu‡, Zifeng Wu†, Fang Zhao†, Liang
Wang†, Tieniu Tan† 
CRIPAC, CASIA
Weiqiang Ren, Chong Wang, Yanhua Chen,
Kaiqi Huang, Tieniu Tan 
The Chinese University of Hong
Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi
Qiu, Yonglong Tian, Hongsheng Li, Shuo
Yang, Zhe Wang, Yuanjun Xiong, Chen
Qian, Zhenyao Zhu, Ruohui Wang,
Chen-Change Loy, Xiaogang Wang, Xiaoou
Tang 
University of Warwick
Ben Graham 115:211–252
Table 8 continued
Insitutions
Contributors and references
DeepInsight
NLPR†, HKUST‡
Junjie Yan†, Naiyan Wang‡, Stan Z. Li†,
Dit-Yan Yeung‡ 
Fengjun Lv Consulting
Fengjun Lv 
Christian Szegedy, Wei Liu, Yangqing Jia,
Pierre Sermanet, Scott Reed, Drago
Anguelov, Dumitru Erhan, Andrew
Rabinovich 
Hong Kong U. of Science
and Tech.†, Chinese U. of
H. K.‡, Stanford U.∓
Cewu Lu†, Hei Law*†, Hao Chen*‡,
Qifeng Chen*∓, Yao Xiao*†Chi Keung
Tang† 
libccv.org
Liu Liu 
The University of Tokyo†,
IIT Guwahati‡
Senthil Purushwalkam†‡, Yuichiro
Tsuchiya†, Atsushi Kanehira†, Asako
Kanezaki†, Tatsuya Harada† 
The University of Tokyo
Riku Togashi, Keita Iwamoto, Tomoaki
Iwase, Hideki Nakayama 
National University of
Singapore†, IBM
Research Australia‡
Jian Dong†, Yunchao Wei†, Min Lin†,
Qiang Chen‡, Wei Xia†, Shuicheng
Yan† 
National Univ. of
Singapore†, Beijing
Samsung Telecom R&D
Min Lin†, Jian Dong†, Hanjiang Lai†,
Junjun Xiong‡, Shuicheng Yan† 
Orange Labs Beijing†,
BUPT China‡
Hongliang Bai†, Yinan Liu†, Bo Liu‡,
Yanchao Feng‡, Kun Tao†, Yuan
Dong† 
LENOVO†, HKUST‡, U. of
Lin Sun†‡, Zhanghui Kuang†, Cong
Zhao†, Kui Jia∓, Oscar C.Au‡ 
South China Univ. of
Technology
Guo Lihua, Liao Qijun, Ma Qianli, Lin
Southeast U.†, Chinese A.
of Sciences‡
Feng Liu†, Zifeng Wu‡, Yongzhen
Sun Yat-Sen University
Liliang Zhang, Tianshui Chen, Shuye
Zhang, Wanglan He, Liang Lin,
Dengguang Pang, Lingbo Liu
The Third Research
Institute of the Ministry of
Public Security
Jie Shao, Xiaoteng Zhang, JianYing Zhou,
Jian Wang, Jian Chen, Yanfeng Shang,
Wenfei Wang, Lin Mei, Chuanping Hu
 
Toyota Technological
Institute at Chicago†,
Ecole Centrale Paris‡
George Papandreou†, Iasonas
Kokkinos‡ 
Int J Comput Vis 115:211–252
Table 8 continued
Insitutions
Contributors and references
University of Isfahan
Fatemeh Shaﬁzadegan, Elham Shabaninia
 
U. of Amsterdam and
Euvision Tech.
Koen van de Sande, Daniel Fontijne, Cees
Snoek, Harro Stokman, Arnold
Smeulders 
University of Oxford
Karen Simonyan, Andrew Zisserman
 
The University of
Queensland
Zhongwen Xu and Yi Yang 
Each method is identiﬁed with a codename used in the text. For classiﬁcaton and single-object localization we report ﬂat top-5 error, in percents
(lower is better). For detection we report mean average precision, in percents (higher is better). CLSo,LOCo,DETo corresponds to entries using
outside training data (ofﬁcially allowed in ILSVRC2014). ◦means localization error greater than 60% (localization submission was required with
every classiﬁcation submission). Key references are provided where available. More details about the winning entries can be found in Sect. 5.1
 . The second
place in image classiﬁcation went to the ISI team, which
used Fisher vectors and a
streamlined version of Graphical Gaussian Vectors , along with linear classiﬁers using
Passive-Aggressive (PA) algorithm .
The second place in single-object localization went to the
VGG, with an image classiﬁcation system including dense
SIFT features and color statistics , a Fisher
vector representation , and a
linear SVM classiﬁer, plus additional insights from . Both
ISI and VGG used for object
localization; SuperVision used a regression model trained
to predict bounding box locations. Despite the weaker detection model, SuperVision handily won the object localization
task. A detailed analysis and comparison of the SuperVision
and VGG submissions on the single-object localization task
can be found in Russakovsky et al. . The inﬂuence of
the success of the SuperVision model can be clearly seen in
ILSVRC2013 and ILSVRC2014.
ILSVRC2013 There were 24 teams participating in the
ILSVRC2013 competition, compared to 21 in the previous
3yearscombined.Followingthesuccessofthedeeplearningbased method in 2012, the vast majority of entries in 2013
used deep convolutional neural networks in their submission.
The winner of the classiﬁcation task was Clarifai, with several large deep convolutional networks averaged together.
The network architectures were chosen using the visualization technique of , and they were
trained on the GPU following using the
dropout technique .
The winning single-object localization OverFeat submissionwasbasedonanintegratedframeworkforusingconvolutional networks for classiﬁcation, localization and detection
with a multiscale sliding window approach . They were the only team tackling all three tasks.
The winner of object detection task was UvA team, which
utilized a new way of efﬁcient encoding densely sampled color descriptors pooled using a multi-level spatial pyramid in a selective search framework . The detection
results were rescored using a full-image convolutional network classiﬁer.
ILSVRC2014 2014 attracted the most submissions, with 36
teams submitting 123 entries compared to just 24 teams in
2013—a 1.5× increase in participation.9 As in 2013 almost
all teams used convolutional neural networks as the basis for
their submission. Signiﬁcant progress has been made in just
1 year: image classiﬁcation error was almost halved since
ILSVRC2013 and object detection mean average precision
almost doubled compared to ILSVRC2013. Please refer to
Sect. 6.1 for details.
In 2014 teams were allowed to use outside data for training
their models in the competition, so there were six tracks: provided and outside data tracks in each of image classiﬁcation,
single-object localization, and object detection tasks.
The winning image classiﬁcation with provided data team
was GoogLeNet, which explored an improved convolutional
neural network architecture combining the multi-scale idea
with intuitions gained from the Hebbian principle. Additional dimension reduction layers allowed them to increase
both the depth and the width of the network signiﬁcantly
without incurring signiﬁcant computational overhead. In the
image classiﬁcation with external data track, CASIAWS won
by using weakly supervised object localization from only
classiﬁcation labels to improve image classiﬁcation. MCG
9 Table 8 omits 4 teams which submitted results but chose not to ofﬁcially participate in the challenge.
Int J Comput Vis 115:211–252
Fig. 9 Performance of winning entries in the ILSVRC2010-2014 competitions in each of the three tasks (details about the entries and
numerical results are in Sect. 5.1). There is a steady reduction of error
every year in object classiﬁcation and single-object localization tasks,
and a 1.9× improvement in mean average precision in object detection.
There are two considerations in making these comparisons. (1) The
object categories used in ISLVRC changed between years 2010 and
2011, and between 2011 and 2012. However, the large scale of the data
(1000 object categories, 1.2 million training images) has remained the
same, making it possible to compare results. Image classiﬁcation and
single-object localization entries shown here use only provided training data. (2) The size of the object detection training data has increased
signiﬁcantly between years 2013 and 2014 (Sect. 3.3). Section 6.1 discusses the relative effects of training data increase versus algorithmic
improvements
region proposals pretrained on PAS-
CAL VOC 2012 data are used to extract region proposals,
regions are represented using convolutional networks, and a
multiple instance learning strategy is used to learn weakly
supervised object detectors to represent the image.
In the single-object localization with provided data track,
the winning team was VGG, which explored the effect of
convolutional neural network depth on its accuracy by using
three different architectures with up to 19 weight layers with
rectiﬁed linear unit non-linearity, building off of the implementation of Caffe . For localization they used
per-class bounding box regression similar to OverFeat . In the single-object localization with
external data track, Adobe used 2000 additional ImageNet
classes to train the classiﬁers in an integrated convolutional
neural network framework for both classiﬁcation and localization, with bounding box regression. At test time they used
k-means to ﬁnd bounding box clusters and rank the clusters
according to the classiﬁcation scores.
In the object detection with provided data track, the winning team NUS used the RCNN framework with the network-in-network method 
and improvements of . Global context information was incorporated following . In the
object detection with external data track, the winning team
was GoogLeNet (which also won image classiﬁcation with
provided data). It is truly remarkable that the same team was
able to win at both image classiﬁcation and object detection,
indicating that their methods are able to not only classify the
imagebasedonsceneinformationbutalsoaccuratelylocalize
multiple object instances. Just like most teams participating
in this track, GoogLeNet used the image classiﬁcation dataset
as extra training data.
5.2 Large Scale Algorithmic Innovations
ILSVRC over the past 5 years has paved the way for several
breakthroughs in computer vision.
The ﬁeld of categorical object recognition has dramatically evolved in the large-scale setting. Section 5.1 documents the progress, starting from coded SIFT features and
evolving to large-scale convolutional neural networks dominating at all three tasks of image classiﬁcation, single-object
localization, and object detection. With the availability of so
muchtrainingdata(alongwithanefﬁcientalgorithmicimplementation and GPU computing resources) it became possible
to learn neural networks directly from the image data, without needing to create multi-stage hand-tuned pipelines of
extracted features and discriminative classiﬁers. The major
breakthrough came in 2012 with the win of the SuperVision
team on image classiﬁcation and single-object localization
tasks , and by 2014 all of the top
contestants were relying heavily on convolutional neural networks.
Further, over the past few years there has been a lot of
focus on large-scale recognition in the computer vision community . Best paper awards at top vision conferences in
2013 were awarded to large-scale recognition methods: at
CVPR 2013 to “Fast, Accurate Detection of 100,000 Object
Classes on a Single Machine” and at
ICCV 2013 to “From Large Scale Image Categorization to
Entry-Level Categories” . Additionally,
several inﬂuential lines of research have emerged, such as
large-scale weakly supervised localization work of which was awarded the best paper award in
ECCV 2012 and large-scale zero-shot learning, e.g., .
Int J Comput Vis 115:211–252
6 Results and Analysis
6.1 Improvements over the Years
State-of-the-art accuracy has improved signiﬁcantly from
ILSVRC2010 to ILSVRC2014, showcasing the massive
progress that has been made in large-scale object recognition over the past 5 years. The performance of the winning
ILSVRC entries for each task and each year are shown in
Fig. 9. The improvement over the years is clearly visible. In
this section we quantify and analyze this improvement.
6.1.1 Image Classiﬁcation and Single-Object Localization
Improvement over the Years
There has been a 4.2× reduction in image classiﬁcation error
(from 28.2 to 6.7%) and a 1.7× reduction in single-object
localization error (from 42.5 to 25.3%) since the beginning
of the challenge. For consistency, here we consider only
teams that use the provided training data. Even though the
exact object categories have changed (Sect. 3.1.1), the large
scale of the dataset has remained the same (Table 3), making
the results comparable across the years. The dataset has not
changed since 2012, and there has been a 2.4× reduction in
image classiﬁcation error (from 16.4 to 6.7%) and a 1.3× in
single-object localization error (from 33.5 to 25.3%) in the
past 3years.
6.1.2 Object Detection Improvement over the Years
Object detection accuracy as measured by the mean average
precision (mAP) has increased 1.9× since the introduction of
this task, from 22.6% mAP in ILSVRC2013 to 43.9% mAP
in ILSVRC2014. However, these results are not directly comparable for two reasons. First, the size of the object detection
training data has increased signiﬁcantly from 2013 to 2014
(Sect. 3.3). Second, the 43.9% mAP result was obtained
with the addition of the image classiﬁcation and single-object
localization training data. Here we attempt to understand the
relative effects of the training set size increase versus algorithmic improvements. All models are evaluated on the same
ILSVRC2013-2014 object detection test set.
First, we quantify the effects of increasing detection training data between the two challenges by comparing the
same model trained on ILSVRC2013 detection data versus
ILSVRC2014 detection data. The UvA team’s framework
from 2013 achieved 22.6% with ILSVRC2013 data (Table 7)
and 26.3% with ILSVRC2014 data and no other modiﬁcations.10 The absolute increase in mAP was 3.7%. The RCNN
model achieved 31.4% mAP with ILSVRC2013 detection
plus image classiﬁcation data and
10 Personal communication with members of the UvA team.
34.5% mAP with ILSVRC2014 detection plus image classiﬁcation data (Berkeley team in Table 8). The absolute
increase in mAP by expanding ILSVRC2013 detection data
to ILSVRC2014 was 3.1%.
Second, we quantify the effects of adding in the external
data for training object detection models. The NEC model in
2013 achieved 19.6% mAP trained on ILSVRC2013 detection data alone and 20.9% mAP trained on ILSVRC2013
detection plus classiﬁcation data (Table 7). The absolute
increase in mAP was 1.3%. The UvA team’s best entry in
2014 achieved 32.0% mAP trained on ILSVRC2014 detection data and 35.4% mAP trained on ILSVRC2014 detection
plus classiﬁcation data. The absolute increase in mAP was
Thus, we conclude based on the evidence so far that
expandingtheILSVRC2013detectionsettotheILSVRC2014
set, as well as adding in additional training data from the
classiﬁcation task, all account for approximately 1–4% in
absolute mAP improvement for the models. For comparison,
we can also attempt to quantify the effect of algorithmic innovation. The UvA team’s 2013 framework achieved 26.3%
mAP on ILSVRC2014 data as mentioned above, and their
improved method in 2014 obtained 32.0% mAP (Table 8).
This is 5.8% absolute increase in mAP over just 1 year from
algorithmic innovation alone.
Insummary, weconcludethat theabsolute21.3%increase
in mAP between winning entries of ILSVRC2013 (22.6%
mAP) and of ILSVRC2014 (43.9% mAP) is the result
of impressive algorithmic innovation and not just a consequence of increased training data. However, increasing
the ISLVRC2014 object detection training dataset further is
likely to produce additional improvements in detection accuracy for current algorithms.
6.2 Statistical Signiﬁcance
One important question to ask is whether results of different
submissions to ILSVRC are statistically signiﬁcantly different from each other. Given the large scale, it is no surprise
that even minor differences in accuracy are statistically signiﬁcant;weseektoquantifyexactlyhowmuchofadifference
is enough.
FollowingthestrategyemployedbyPASCALVOC , for each method we obtain a conﬁdence
interval of its score using bootstrap sampling. During each
bootstrap round, we sample N images with replacement from
all the available N test images and evaluate the performance
of the algorithm on those sampled images. This can be done
very efﬁciently by precomputing the accuracy on each image.
Given the results of all the bootstrapping rounds we discard
the lower and the upper α fraction. The range of the remaining results represents the 1 −2α conﬁdence interval. We run
a large number of bootstrapping rounds 115:211–252
Table 9 We use bootstrapping to construct 99.9each ILSVRC task in
99.9% Conf Int
Image classiﬁcation
DeeperVision
10.87–11.53
11.03–11.69
11.13–11.80
11.25–11.91
11.41–12.08
12.60–13.30
13.14–13.87
13.20–13.91
13.83–14.54
14.43–15.17
SuperVision†
14.94–15.69
SuperVision
16.04–16.80
25.71–26.65
26.53–27.43
26.60–27.52
29.09–30.04
Single-object localization
24.87–25.78
25.98 – 26.92
29.38–30.35
29.61–30.58
31.40–32.40
SuperVision†
33.05–34.04
33.24–34.25
SuperVision
33.67–34.69
34.97–35.99
41.69–42.75
42.18–43.24
45.90–46.95
49.50–50.57
53.10–54.17
61.44–62.48
Object detection
GoogLeNet†
42.92–45.65
39.68–42.30
DeepInsight†
39.49–42.06
36.29–38.80
34.63–36.92
34.36–36.70
Table 9 continued
99.9% Conf Int
33.67–36.12
31.28–33.49
29.70–31.93
28.03–30.20
22.00–23.82
20.40–22.15
19.14–20.85
18.82–20.61
10.98–12.34
10.04–11.32
9.48–10.77
† Means the entry used external training data. The winners using the
provided data for each track and each year are bolded. The difference
between the winning method and the runner-up each year is signiﬁcant
even at the 99.9% level
convergence). Table 9 shows the results of the top entries to
each task of ILSVRC2012-2014. The winning methods are
statistically signiﬁcantly different from the other methods,
even at the 99.9% level.
6.3 Current State of Categorical Object Recognition
Besides looking at just the average accuracy across hundreds
of object categories and tens of thousands of images, we can
also delve deeper to understand where mistakes are being
made and where researchers’ efforts should be focused to
expedite progress.
To do so, in this section we will be analyzing an
“optimistic” measurement of state-of-the-art recognition
performance instead of focusing on the differences in individual algorithms. For each task and each object class, we
compute the best performance of any entry submitted to
any ILSVRC2012-2014, including methods using additional
training data. Since the test sets have remained the same, we
can directly compare all the entries in the past 3 years to
obtain the most “optimistic” measurement of state-of-the-art
accuracy on each category.
For consistency with the object detection metric (higher
is better), in this section we will be using image classiﬁcation and single-object localization accuracy instead of error,
where accuracy = 1 −error.
6.3.1 Range of Accuracy Across Object Classes
Figure 10 shows the distribution of accuracy achieved by the
“optimistic” models across the object categories. The image
classiﬁcation model achieves 94.6% accuracy on average
Int J Comput Vis 115:211–252
Fig. 10 For each object class, we consider the best performance of any
entry submitted to ILSVRC2012-2014, including entries using additional training data. The plots show the distribution of these “optimistic”
per-class results. Performance is measured as accuracy for image classi-
ﬁcation (left) and for single-object localization (middle), and as average
precision for object detection (right). While the results are very promising in image classiﬁcation, the ILSVRC datasets are far from saturated:
many object classes continue to be challenging for current algorithms
(or 5.4% error), but there remains a 41.0% absolute difference inaccuracy between the most and least accurate object
class. The single-object localization model achieves 81.5%
accuracy on average (or 18.5% error), with a 77.0% range
in accuracy across the object classes. The object detection
model achieves 44.7% average precision, with an 84.7%
range across the object classes. It is clear that the ILSVRC
dataset is far from saturated: performance on many categories
has remained poor despite the strong overall performance of
the models.
6.3.2 Qualitative Examples of Easy and Hard Classes
Figures 11 and 12 show the easiest and hardest classes for
each task, i.e., classes with the best and worst results obtained
with the “optimistic” models.
For image classiﬁcation, 121 out of 1000 object classes
have 100% image classiﬁcation accuracy according to the
optimistic estimate. Figure 11 (top) shows a random set of 10
of them. They contain a variety of classes, such as mammals
like “red fox” and animals with distinctive structures like
“stingray”. The hardest classes in the image classiﬁcation
task, with accuracy as low as 59.0%, include metallic and
see-through man-made objects, such as “hook” and “water
bottle,”thematerial“velvet”andthehighlyvariedsceneclass
“restaurant.”
For single-object localization, the 10 easiest classes with
99.0–100% accuracy are all mammals and birds. The hardest classes include metallic man-made objects such as “letter
opener” and “ladle”, plus thin structures such as “pole” and
“spacebar” and highly varied classes such as “wing”. The
most challenging class “spacebar” has a only 23.0% localization accuracy.
Object detection results are shown in Fig. 12. The easiest classes are living organisms such as “dog” and “tiger”,
plus “basketball” and “volleyball” with distinctive shape and
color, and a somewhat surprising “snowplow.” The easiest
class “butterﬂy” is not yet perfectly detected but is very close
with 92.7 % AP. The hardest classes are as expected small
thin objects such as “ﬂute” and “nail”, and the highly varied “lamp” and “backpack” classes, with as low as 8.0 %
6.3.3 Per-Class Accuracy as a Function of Image
Properties
We now take a closer look at the image properties to try to
understand why current algorithms perform well on some
object classes but not others. One hypothesis is that variation
inaccuracycomesfromthefactthatinstancesofsomeclasses
tend to be much smaller in images than instances of other
classes, and smaller objects may be harder for computers to
recognize. In this section we argue that while accuracy is
correlated with object scale in the image, not all variation in
accuracy can be accounted for by scale alone.
For every object class, we compute its average scale, or
the average fraction of image area occupied by an instance
of the object class on the ILSVRC2012-2014 validation set.
Sincetheimagesandobjectclassesintheimageclassiﬁcation
and single-object localization tasks are the same, we use the
bounding box annotations of the single-object localization
dataset for both tasks. In that dataset the object classes range
from “swimming trunks” with scale of 1.5 % to “spider web”
withscaleof 85.6 %. Intheobject detectionvalidationdataset
theobjectclassesrangefrom“sunglasses”withscaleof1.3 %
to “sofa” with scale of 44.4 %.
Figure 13 shows the performance of the “optimistic”
method as a function of the average scale of the object in the
image. Each dot corresponds to one object class. We observe
a very weak positive correlation between object scale and
image classiﬁcation accuracy: ρ = 0.14. For single-object
localization and object detection the correlation is stronger,
Int J Comput Vis 115:211–252
Fig. 11 For each object
category, we take the best
performance of any entry
submitted to
ILSVRC2012-2014 (including
entries using additional training
data). Given these “optimistic”
results we show the easiest and
harder classes for each task. The
numbers in parentheses indicate
classiﬁcation and localization
accuracy. For image
classiﬁcation the 10 easiest
classes are randomly selected
from among 121 object classes
with 100% accuracy. Object
detection results are shown in
at ρ = 0.40 and ρ = 0.41 respectively. It is clear that
not all variation in accuracy can be accounted for by scale
alone. Nevertheless, in the next section we will normalize
for object scale to ensure that this factor is not affecting our
conclusions.
6.3.4 Per-Class Accuracy as a Function of Object
Properties
Besides considering image-level properties we can also
observe how accuracy changes as a function of intrinsic
Int J Comput Vis 115:211–252
Fig. 12 For each object
category, we take the best
performance of any entry
submitted to ILSVRC2012-2014
(including entries using
additional training data). Given
these “optimistic” results we
show the easiest and harder
classes for the object detection
task, i.e., classes with best and
worst results. The numbers in
parentheses indicate average
precision. Image classiﬁcation
and single-object localization
results are shown in Fig. 11
Fig. 13 Performance of the “optimistic” method as a function of object
scale in the image, on each task. Each dot corresponds to one object
class. Average scale (x-axis) is computed as the average fraction of
the image area occupied by an instance of that object class on the
ILSVRC2014 validation set. “Optimistic” performance (y-axis) corresponds to the best performance on the test set of any entry submitted
to ILSVRC2012-2014 (including entries with additional training data).
The test set has remained the same over these 3 years. We see that accuracy tends to increase as the objects get bigger in the image. However,
it is clear that far from all the variation in accuracy on these classes can
be accounted for by scale alone
object properties. We deﬁne three properties inspired by
human vision: the real-world size of the object, whether it’s
deformable within instance, and how textured it is. For each
property, the object classes are assigned to one of a few bins
(listed below). These properties are illustrated in Fig. 1.
Human subjects annotated each of the 1000 image classiﬁcation and single-object localization object classes from
ILSVRC2012-2014 with these properties . By construction (see Sect. 3.3.1), each of the 200
object detection classes is either also one of 1000 object
classes or is an ancestor of one or more of the 1000 classes in
the ImageNet hierarchy. To compute the values of the properties for each object detection class, we simply average the
annotated values of the descendant classes.
Int J Comput Vis 115:211–252
In this section we draw the following conclusions about
state-of-the-art recognition accuracy as a function of these
object properties:
– Real-world size XS for extra small (e.g. nail), small (e.g.
fox), medium (e.g. bookcase), large (e.g. car) or XL for
extra large (e.g. church)
The image classiﬁcation and single-object localization
“optimistic” models performs better on large and extra
large real-world objects than on smaller ones. The “optimistic” object detection model surprisingly performs better on extra small objects than on small or medium ones.
– Deformability within instance Rigid (e.g., mug) or
deformable (e.g., water snake)
The “optimistic” model on each of the three tasks performs statistically signiﬁcantly better on deformable
objects compared to rigid ones. However, this effect disappears when analyzing natural objects separately from
man-made objects.
– Amount of texture none (e.g. punching bag), low (e.g.
horse), medium (e.g. sheep) or high (e.g. honeycomb)
The “optimistic” model on each of the three tasks is
signiﬁcantly better on objects with at least low level of
texture compared to untextured objects.
These and other ﬁndings are justiﬁed and discussed in
detail below.
Experimental Setup We observed in Sect. 6.3.3 that objects
that occupy a larger area in the image tend to be somewhat
easier to recognize. To make sure that differences in object
scale are not inﬂuencing results in this section, we normalize each bin by object scale. We discard object classes with
the largest scales from each bin as needed until the average
object scale of object classes in each bin across one property is the same (or as close as possible). For real-world size
property for example, the resulting average object scale in
each of the ﬁve bins is 31.6–31.7% in the image classiﬁcation and single-object localization tasks, and 12.9–13.4% in
the object detection task.11
Figure 14 shows the average performance of the “optimistic” model on the object classes that fall into each bin for
each property. We analyze the results in detail below. Unless
otherwise speciﬁed, the reported accuracies below are after
the scale normalization step.
To evaluate statistical signiﬁcance, we compute the 95%
conﬁdence interval for accuracy using bootstrapping: we
repeatedly sample the object classes within the bin with
11 For rigid versus deformable objects, the average scale in each bin
is 34.1–34.2% for classiﬁcation and localization, and 13.5–13.7% for
detection. For texture, the average scale in each of the four bins is 31.1–
31.3%forclassiﬁcationandlocalization,and12.7–12.8%fordetection.
replacement, discard some as needed to normalize by scale,
and compute the average accuracy of the “optimistic” model
on the remaining classes. We report the 95 % conﬁdence
intervals (CI) in parentheses.
Real-World Size In Fig. 14 (top, left) we observe that in the
image classiﬁcation task the “optimistic” model tends to perform signiﬁcantly better on objects which are larger in the
real-world.Theclassiﬁcationaccuracyis93.6–93.9%onXS,
S and M objects compared to 97.0 % on L and 96.4 % on XL
objects. Since this is after normalizing for scale and thus
can’t be explained by the objects’ size in the image, we conclude that either (1) larger real-world objects are easier for the
model to recognize, or (2) larger real-world objects usually
occur in images with very distinctive backgrounds.
To distinguish between the two cases we look Fig. 14 (top,
middle). We see that in the single-object localization task,
the L objects are easy to localize at 82.4 % localization accuracy. XL objects, however, tend to be the hardest to localize
with only 73.4 % localization accuracy. We conclude that the
appearance of L objects must be easier for the model to learn,
while XL objects tend to appear in distinctive backgrounds.
The image background make these XL classes easier for the
image-level classiﬁer, but the individual instances are difﬁcult to accurately localize. Some examples of L objects are
“killer whale,” “schooner,” and “lion,” and some examples of
XL objects are “boathouse,” “mosque,” “toyshop” and “steel
arch bridge.”
In Fig. 14 (top,right) corresponding to the object detection task, the inﬂuence of real-world object size is not as
apparent. One of the key reasons is that many of the XL and
L object classes of the image classiﬁcation and single-object
localization datasets were removed in constructing the detection dataset (Sect. 3.3.1) since they were not basic categories
well-suitedfordetection.Therewereonly3XLobjectclasses
remaining in the dataset (“train,” “airplane” and “bus”), and
none after scale normalization.We omit them from the analysis. The average precision of XS, S, M objects (44.5, 39.0,
and 38.5% mAP respectively) is statistically insigniﬁcant
from average precision on L objects: 95 % conﬁdence interval of L objects is 37.5–59.5%. This may be due to the fact
that there are only 6 L object classes remaining after scale
normalization; all other real-world size bins have at least 18
object classes.
Finally, it is interesting that performance on XS objects
of 44.5 mAP (CI 40.5–47.6%) is statistically signiﬁcantly
better than performance on S or M objects with 39.0 and
38.5 % mAP respectively. Some examples of XS objects are
“strawberry,” “bow tie” and “rugby ball.”
Deformability Within Instance In Fig. 14(second row) it is
clear that the “optimistic” model performs statistically signiﬁcantly worse on rigid objects than on deformable objects.
Int J Comput Vis 115:211–252
Fig. 14 Performance of the
“optimistic” computer vision
model as a function of object
properties. The x-axis
corresponds to object properties
annotated by human labelers for
each object class and illustrated in
Fig. 1. The y-axis is the average
accuracy of the “optimistic”
model. Note that the range of the
y-axis is different for each task
to make the trends more visible.
The black circle is the average
accuracy of the model on all
object classes that fall into each
bin. We control for the effects of
object scale by normalizing the
object scale within each bin
(details in Sect. 6.3.4). The
color bars show the model
accuracy averaged across the
remaining classes. Error bars
show the 95 % conﬁdence
interval obtained with
bootstrapping. Some bins are
missing color bars because less
than 5 object classes remained
in the bin after scale
normalization. For example, the
bar for XL real-world object
detection classes is missing
because that bin has only 3
object classes (airplane, bus,
train) and after normalizing by
scale no classes remain
Int J Comput Vis 115:211–252
Image classiﬁcation accuracy is 93.2 % on rigid objects (CI
92.6–93.8%), much smaller than 95.7% on deformable ones.
Single-object localization accuracy is 76.2 % on rigid objects
(CI 74.9–77.4%), much smaller than 84.7 % on deformable
ones. Object detection mAP is 40.1 % on rigid objects (CI
37.2–42.9%), much smaller than44.8 %on deformableones.
We can further analyze the effects of deformability after
separating object classes into “natural” and “man-made” bins
based on the ImageNet hierarchy. Deformability is highly
correlated with whether the object is natural or man-made:
0.72 correlation for image classiﬁcation and single-object
localization classes, and 0.61 for object detection classes.
Figure 14(third row) shows the effect of deformability on
performance of the model for man-made and natural objects
separately.
Man-made classes are signiﬁcantly harder than natural
classes: classiﬁcation accuracy 92.8 % (CI 92.3–93.3%) for
man-made versus 97.0 % for natural, localization accuracy
75.5 % (CI 74.3–76.5%) for man-made versus 88.5 % for
natural, and detection mAP 38.7 % (CI 35.6–41.3%) for
man-made versus 50.9 % for natural. However, whether the
classes are rigid or deformable within this subdivision is no
longer signiﬁcant in most cases. For example, the image classiﬁcation accuracy is 92.3 % (CI 91.4–93.1%) on man-made
rigid objects and 91.8 % on man-made deformable objects—
not statistically signiﬁcantly different.
There are two cases where the differences in performance
are statistically signiﬁcant. First, for single-object localization, natural deformable objects are easier than natural rigid
objects: localization accuracy of 87.9 % (CI 85.9–90.1%) on
natural deformable objects is higher than 85.8 % on natural
rigid objects—falling slightly outside the 95% conﬁdence
interval. This difference in performance is likely because
deformable natural animals tend to be easier to localize than
rigid natural fruit.
Second, for object detection, man-made rigid objects
are easier than man-made deformable objects: 38.5 % mAP
(CI 35.2–41.7%) on man-made rigid objects is higher than
33.0 % mAP on man-made deformable objects. This is
because man-made rigid objects include classes like “traf-
ﬁc light” or “car” whereas the man-made deformable objects
contain challenging classes like “plastic bag,” “swimming
trunks” or “stethoscope.”
Amount of Texture Finally, we analyze the effect that object
texture has on the accuracy of the “optimistic” model. Figure 14(fourth row) demonstrates that the model performs
better as the amount of texture on the object increases.
The most signiﬁcant difference is between the performance
on untextured objects and the performance on objects with
low texture. Image classiﬁcation accuracy is 90.5 % on
untextured objects (CI 89.3–91.6%), lower than 94.6 % on
low-textured objects. Single-object localization accuracy is
71.4 % on untextured objects (CI 69.1–73.3%), lower than
80.2 % on low-textured objects. Object detection mAP is
33.2 % on untextured objects (CI 29.5–35.9%), lower than
42.9 % on low-textured objects.
Texture is correlated with whether the object is natural or
man-made, at 0.35 correlation for image classiﬁcation and
single-object localization, and 0.46 correlation for object
detection. To determine if this is a contributing factor, in
Fig. 14(bottom row) we break up the object classes into natural and man-made and show the accuracy on objects with no
texture versus objects with low texture. We observe that the
model is still statistically signiﬁcantly better on low-textured
object classes than on untextured ones, both on man-made
and natural object classes independently.12
6.4 Human Accuracy on Large-Scale Image
Classiﬁcation
Recent improvements in state-of-the-art accuracy on the
ILSVRC dataset are easier to put in perspective when compared to human-level accuracy. In this section we compare
the performance of the leading large-scale image classi-
ﬁcation method with the performance of humans on this
To support this comparison, we developed an interface
that allowed a human labeler to annotate images with up
to ﬁve ILSVRC target classes. We compare human errors to
those of the winning ILSRC2014 image classiﬁcation model,
GoogLeNet (Sect. 5.1). For this analysis we use a random
sample of 1500 ILSVRC2012-2014 image classiﬁcation test
set images.
Annotation Interface Our web-based annotation interface
consists of one test set image and a list of 1000 ILSVRC categories on the side. Each category is described by its title, such
as“cowboyboot.”Thecategoriesaresortedinthetopological
order of the ImageNet hierarchy, which places semantically
similar concepts nearby in the list. For example, all motor
vehicle-related classes are arranged contiguously in the list.
Every class category is additionally accompanied by a row of
13 examples images from the training set to allow for faster
visual scanning. The user of the interface selects 5 categories
from the list by clicking on the desired items. Since our interface is web-based, it allows for natural scrolling through the
list, and also search by text.
Annotation Protocol We found the task of annotating images
with one of 1000 categories to be an extremely challenging
12 Natural object detection classes are removed from this analysis
because there are only 3 and 13 natural untextured and low-textured
classes respectively, and none remain after scale normalization. All
other bins contain at least 9 object classes after scale normalization.
Int J Comput Vis 115:211–252
Table 10 Human classiﬁcation results on the ILSVRC2012-2014 classiﬁcation test set, for two expert annotators A1 and A2
Relative confusion
Human succeeds, GoogLeNet succeeds
Human succeeds, GoogLeNet fails
Human fails, GoogLeNet succeeds
Human fails, GoogLeNet fails
Total number of images
Estimated GoogLeNet classiﬁcation error
Estimated human classiﬁcation error
We report top-5 classiﬁcation error
task for an untrained annotator. The most common error that
an untrained annotator is susceptible to is a failure to consider
a relevant class as a possible label because they are unaware
of its existence.
Therefore, in evaluating the human accuracy we relied
primarily on expert annotators who learned to recognize a
large portion of the 1000 ILSVRC classes. During training,
the annotators labeled a few hundred validation images for
practice and later switched to the test set images.
6.4.1 Quantitative Comparison of Human and Computer
Accuracy on Large-Scale Image Classiﬁcation
We report results based on experiments with two expert
annotators. The ﬁrst annotator (A1) trained on 500 images
and annotated 1500 test images. The second annotator (A2)
trained on 100 images and then annotated 258 test images.
The average pace of labeling was approximately 1 image per
minute, but the distribution is strongly bimodal: some images
are quickly recognized, while some images (such as those of
ﬁne-grained breeds of dogs, birds, or monkeys) may require
multiple minutes of concentrated effort.
The results are reported in Table 10.
Annotator 1 Annotator A1 evaluated a total of 1500 test set
images. The GoogLeNet classiﬁcation error on this sample
was estimated to be 6.8 % (recall that the error on full test
set of 100,000 images is 6.7 %, as shown in Table 8). The
human error was estimated to be 5.1%. Thus, annotator A1
achieves a performance superior to GoogLeNet, by approximately 1.7 %. We can analyze the statistical signiﬁcance of
this result under the null hypothesis that they are from the
same distribution. In particular, comparing the two proportions with a z-test yields a one-sided p-value of p = 0.022.
Thus, we can conclude that this result is statistically signiﬁcant at the 95 % conﬁdence level.
Annotator 2 Our second annotator (A2) trained on a smaller
sample of only 100 images and then labeled 258 test set
images. As seen in Table 10, the ﬁnal classiﬁcation error
is signiﬁcantly worse, at approximately 12.0 % Top-5 error.
The majority of these errors (48.8 %) can be attributed to the
annotator failing to spot and consider the ground truth label
as an option.
Thus, we conclude that a signiﬁcant amount of training
time is necessary for a human to achieve competitive performance on ILSVRC. However, with a sufﬁcient amount of
training, a human annotator is still able to outperform the
GoogLeNet result (p = 0.022) by approximately 1.7 %.
Annotator Comparison We also compare the prediction
accuracy of the two annotators. Of a total of 204 images that
both A1 and A2 labeled, 174 (85 %) were correctly labeled
by both A1 and A2, 19 (9 %) were correctly labeled by A1
but not A2, 6 (3 %) were correctly labeled by A2 but not A1,
and 5 (2 %) were incorrectly labeled by both. These include
2 images that we consider to be incorrectly labeled in the
ground truth.
In particular, our results suggest that the human annotators do not exhibit strong overlap in their predictions. We
can approximate the performance of an “optimistic” human
classiﬁer by assuming an image to be correct if at least one
of A1 or A2 correctly labeled the image. On this sample of
204 images, we approximate the error rate of an “optimistic”
human annotator at 2.4 %, compared to the GoogLeNet error
rate of 4.9 %.
6.4.2 Analysis of Human and Computer Errors
on Large-Scale Image Classiﬁcation
We manually inspected both human and GoogLeNet errors to
gain an understanding of common error types and how they
compare. For purposes of this section, we only discuss results
based on the larger sample of 1500 images that were labeled
by annotator A1. Examples of representative mistakes are
in Fig. 15. The analysis and insights below were derived
speciﬁcally from GoogLeNet predictions, but we suspect that
many of the same errors may be present in other methods.
Types of Errors in Both Computer and Human Annotations
(1) Multiple objects Both GoogLeNet and humans struggle with images that contain multiple ILSVRC classes
(usually many more than ﬁve), with little indication of
which object is the focus of the image. This error is only
present in the Classiﬁcation setting, since every image is
constrained to have exactly one correct label. In total, we
attribute 24 (24 %) of GoogLeNet errors and 12 (16 %)
of human errors to this category. It is worth noting that
humans can have a slight advantage in this error type,
since it can sometimes be easy to identify the most salient
object in the image.
Int J Comput Vis 115:211–252
Fig. 15 Representative validation images that highlight common
sources of error. For each image, we display the ground truth in blue,
and top 5 predictions from GoogLeNet follow (red wrong, green right).
GoogLeNet predictions on the validation set images were graciously
provided by members of the GoogLeNet team. From left to right:
Images that contain multiple objects, images of extreme closeups and
uncharacteristic views, images with ﬁlters, images that signiﬁcantly
beneﬁt from the ability to read text, images that contain very small
and thin objects, images with abstract representations, and example of
a ﬁne-grained image that GoogLeNet correctly identiﬁes but a human
would have signiﬁcant difﬁculty with
(2) Incorrect annotations We found that approximately 5
out of 1500 images (0.3 %) were incorrectly annotated in
the ground truth. This introduces an approximately equal
number of errors for both humans and GoogLeNet.
Types of Errors that the Computer is More Susceptible to
than the Human
(1) Object small or thin GoogLeNet struggles with recognizing objects that are very small or thin in the image,
even if that object is the only object present. Examples of
this include an image of a standing person wearing sunglasses, a person holding a quill in their hand, or a small
ant on a stem of a ﬂower. We estimate that approximately
22 (21 %) of GoogLeNet errors fall into this category,
while none of the human errors do. In other words, in our
sample of images, no image was mislabeled by a human
because they were unable to identify a very small or thin
object. This discrepancy can be attributed to the fact that
a human can very effectively leverage context and affordances to accurately infer the identity of small objects
(for example, a few barely visible feathers near person’s
handasverylikelybelongingtoamostlyoccludedquill).
(2) Image ﬁlters Many people enhance their photos with
ﬁlters that distort the contrast and color distributions
of the image. We found that 13 (13 %) of the images
that GoogLeNet incorrectly classiﬁed contained a ﬁlter.
Thus, weposit that GoogLeNet is not veryrobust tothese
distortions. In comparison, only one image among the
human errors contained a ﬁlter, but we do not attribute
the source of the error to the ﬁlter.
(3) Abstract representations. GoogLeNet struggles with
images that depict objects of interest in an abstract form,
such as 3D-rendered images, paintings, sketches, plush
toys,orstatues.Anexampleistheabstractshapeofabow
drawn with a light source in night photography, a 3Drendered robotic scorpion, or a shadow on the ground, of
a child on a swing. We attribute approximately 6 (6 %)
of GoogLeNet errors to this type of error and believe
that humans are signiﬁcantly more robust, with no such
errors seen in our sample.
(4) Miscellaneous sources Additional sources of error that
occur relatively infrequently include extreme closeups
of parts of an object, unconventional viewpoints such
as a rotated image, images that can signiﬁcantly beneﬁt
from the ability to read text (e.g. a featureless container
identifying itself as “face powder”), objects with heavy
occlusions, and images that depict a collage of multiple images. In general, we found that humans are more
robust to all of these types of error.
Types of Errors that the Human is More Susceptible to than
the Computer
(1) Fine-grained recognition We found that humans are
noticeably worse at ﬁne-grained recognition (e.g. dogs,
monkeys,snakes,birds),evenwhentheyareinclearview.
To understand the difﬁculty, consider that there are more
than 120 species of dogs in the dataset. We estimate that
28(37 %)ofthehumanerrorsfallintothiscategory,while
only 7 (7 %) of GoogLeNet errors do.
(2) Class unawareness The annotator may sometimes be
unawareofthegroundtruthclasspresentasalabeloption.
When pointed out as an ILSVRC class, it is usually clear
that the label applies to the image. These errors get progressively less frequent as the annotator becomes more
familiar with ILSVRC classes. Approximately 18 (24 %)
of the human errors fall into this category.
(3) Insufﬁcient training data Recall that the annotator is
only presented with 13 examples of a class under every
Int J Comput Vis 115:211–252
category name. However, 13 images are not always
enoughtoadequatelyconveytheallowedclass variations.
For example, a brown dog can be incorrectly dismissed as
a “Kelpie” if all examples of a “Kelpie” feature a dog with
black coat. However, if more than 13 images were listed
it would have become clear that a “Kelpie” may have
brown coat. Approximately 4 (5 %) of human errors fall
into this category.
6.4.3 Conclusions from Human Image Classiﬁcation
Experiments
We investigated the performance of trained human annotators on a sample of 1500 ILSVRC test set images. Our results
indicate that a trained human annotator is capable of outperforming the best model (GoogLeNet) by approximately
1.7 % (p = 0.022).
Weexpectthatsomesourcesoferrormayberelativelyeasily eliminated (e.g. robustness to ﬁlters, rotations, collages,
effectively reasoning over multiple scales), while others may
prove more elusive (e.g. identifying abstract representations
of objects). On the other hand, a large majority of human
errors come from ﬁne-grained categories and class unawareness. We expect that the former can be signiﬁcantly reduced
with ﬁne-grained expert annotators, while the latter could
be reduced with more practice and greater familiarity with
ILSVRC classes. Our results also hint that human errors are
not strongly correlated and that human ensembles may further reduce human error rate.
It is clear that humans will soon outperform state-ofthe-art ILSVRC image classiﬁcation models only by use of
signiﬁcant effort, expertise, and time. One interesting followup question for future investigation is how computer-level
accuracy compares with human-level accuracy on more complex image understanding tasks.
7 Conclusions
In this paper we described the large-scale data collection
process of ILSVRC, provided a summary of the most successful algorithms on this data, and analyzed the success and
failure modes of these algorithms. In this section we discuss some of the key lessons we learned over the years of
ILSVRC, strive to address the key criticisms of the datasets
and the challenges we encountered over the years, and conclude by looking forward into the future.
7.1 Lessons Learned
Thekeylessonofcollectingthedatasetsandrunningthechallenges for 5 years is this: All human intelligence tasks need
to be exceptionally well-designed. We learned this lesson
both when annotating the dataset using Amazon Mechanical
Turk workers (Sect. 3) and even when trying to evaluate human-level image classiﬁcation accuracy using expert
labelers (Sect. 6.4). The ﬁrst iteration of the labeling interface
was always bad—generally meaning completely unusable. If
there was any inherent ambiguity in the questions posed (and
there almost always was), workers found it and accuracy suffered. If there is one piece of advice we can offer to future
research, it is to very carefully design, continuously monitor,
and extensively sanity-check all crowdsourcing tasks.
The other lesson, already well-known to large-scale
researchers, is this: Scaling up the dataset always reveals
unexpected challenges. From designing complicated multistepannotationstrategies(Sect.3.2.1)tohavingtomodifythe
evaluation procedure (Sect. 4), we had to continuously adjust
to the large-scale setting. On the plus side, of course, the
major breakthroughs in object recognition accuracy (Sect. 5)
and the analysis of the strength and weaknesses of current
algorithms as a function of object class properties (Sect. 6.3)
would never have been possible on a smaller scale.
7.2 Criticism
In the past 5 years, we encountered three major criticisms
of the ILSVRC dataset and the corresponding challenge:
(1) the ILSVRC dataset is insufﬁciently challenging, (2) the
ILSVRC dataset contains annotation errors, and (3) the rules
of ILSVRC competition are too restrictive. We discuss these
The ﬁrst criticism is that the objects in the dataset tend
to be large and centered in the images, making the dataset
insufﬁciently challenging. In Sect. 3.2.2 and 3.3.4 we tried
to put those concerns to rest by analyzing the statistics of
the ILSVRC dataset and concluding that it is comparable
with, and in many cases much more challenging than, the
long-standing PASCAL VOC benchmark (Everingham et al.
The second is regarding the errors in ground truth labeling.
We went through several rounds of in-house post-processing
of the annotations obtained using crowdsourcing, and corrected many common sources of errors (e.g., Appendix 1).
The major remaining source of annotation errors stem from
ﬁne-grained object classes, e.g., labelers failing to distinguish different species of birds. This is a tradeoff that had
to be made: in order to annotate data at this scale on a reasonable budget, we had to rely on non-expert crowd labelers.
However, overall the dataset is encouragingly clean. By our
estimates, 99.7 % precision is achieved in the image classi-
ﬁcation dataset (Sects. 3.1.3, 6.4) and 97.9 % of images that
went through the bounding box annotation system have all
instances of the target object class labeled with bounding
boxes (Sect. 3.2.1).
Int J Comput Vis 115:211–252
The third criticism we encountered is over the rules of
the competition regarding using external training data. In
ILSVRC2010-2013, algorithms had to only use the provided
training and validation set images and annotations for training their models. With the growth of the ﬁeld of large-scale
unsupervised feature learning, however, questions began to
arise about what exactly constitutes “outside” data: for example, are image features trained on a large pool of “outside”
images in an unsupervised fashion allowed in the competition? After much discussion, in ILSVRC2014 we took the
ﬁrst step towards addressing this problem. We followed the
PASCAL VOC strategy and created two tracks in the competition: entries using only “provided” data and entries using
“outside” data, meaning any images or annotations not provided as part of ILSVRC training or validation sets. However,
in the future this strategy will likely need to be further revised
as the computer vision ﬁeld evolves. For example, competitions can consider allowing the use of any image features
which are publically available, even if these features were
learned on an external source of data.
7.3 The Future
Given the massive algorithmic breakthroughs over the past
5 years, we are very eager to see what will happen in the next
5 years. There are many potential directions of improvement
and growth for ILSVRC and other large-scale image datasets.
First, continuing the trend of moving towards richer image
understanding (from image classiﬁcation to single-object
localization to object detection), the next challenge would
be to tackle pixel-level object segmentation. The recently
released large-scale COCO dataset is
already taking a step in that direction.
Second, as datasets grow even larger in scale, it may
become impossible to fully annotate them manually. The
scale of ILSVRC is already imposing limits on the manual
annotations that are feasible to obtain: for example, we had to
restrict the number of objects labeled per image in the image
classiﬁcation and single-object localization datasets. In the
future, with billions of images, it will become impossible to
obtain even one clean label for every image. Datasets such
as Yahoo’s Flickr Creative Commons 100M,13 released with
weak human tags but no centralized annotation, will become
more common.
The growth of unlabeled or only partially labeled largescale datasets implies two things. First, algorithms will have
to rely more on weakly supervised training data. Second,
even evaluation might have to be done after the algorithms
make predictions, not before. This means that rather than
evaluating accuracy (how many of the test images or objects
13 
datatype=i\&did=67.
did the algorithm get right) or recall (how many of the desired
images or objects did the algorithm manage to ﬁnd), both of
which require a fully annotated test set, we will be focusing
moreonprecision: of thepredictions that thealgorithmmade,
how many were deemed correct by humans.
We are eagerly awaiting the future development of object
recognition datasets and algorithms, and are grateful that
ILSVRC served as a stepping stone along this path.
Acknowledgments
We thank Stanford University, UNC Chapel Hill,
Google and Facebook for sponsoring the challenges, and NVIDIA for
providing computational resources to participants of ILSVRC2014. We
thank our advisors over the years: Lubomir Bourdev, Alexei Efros,
Derek Hoiem, Jitendra Malik, Chuck Rosenberg and Andrew Zisserman. We thank the PASCAL VOC organizers for partnering with us
in running ILSVRC2010-2012. We thank all members of the Stanford
vision lab for supporting the challenges and putting up with us along
the way. Finally, and most importantly, we thank all researchers that
have made the ILSVRC effort a success by competing in the challenges
and by using the datasets to advance computer vision.
Appendix 1: ILSVRC2012-2014 Image
Classiﬁcation and Single-Object Localization
Object Categories
abacus, abaya, academic gown, accordion, acorn, acorn squash, acoustic guitar, admiral, affenpinscher, Afghan hound,
African chameleon, African crocodile, African elephant, African grey, African hunting dog, agama, agaric, aircraft carrier,
Airedale, airliner, airship, albatross, alligator lizard, alp, altar, ambulance, American alligator, American black bear, American chameleon, American coot, American egret, American lobster, American Staffordshire terrier, amphibian, analog
clock, anemone ﬁsh, Angora, ant, apiary, Appenzeller, apron, Arabian camel, Arctic fox, armadillo, artichoke, ashcan,
assault riﬂe, Australian terrier, axolotl, baboon, backpack, badger, bagel, bakery, balance beam, bald eagle, balloon,
ballplayer, ballpoint, banana, Band Aid, banded gecko, banjo, bannister, barbell, barber chair, barbershop, barn, barn spider, barometer, barracouta, barrel, barrow, baseball, basenji, basketball, basset, bassinet, bassoon, bath towel, bathing cap,
bathtub, beach wagon, beacon, beagle, beaker, bearskin, beaver, Bedlington terrier, bee, bee eater, beer bottle, beer glass,
bell cote, bell pepper, Bernese mountain dog, bib, bicycle-built-for-two, bighorn, bikini, binder, binoculars, birdhouse,
bison, bittern, black and gold garden spider, black grouse, black stork, black swan, black widow, black-and-tan coonhound,
black-footed ferret, Blenheim spaniel, bloodhound, bluetick, boa constrictor, boathouse, bobsled, bolete, bolo tie, bonnet,
book jacket, bookcase, bookshop, Border collie, Border terrier, borzoi, Boston bull, bottlecap, Bouvier des Flandres, bow,
bow tie, box turtle, boxer, Brabancon griffon, brain coral, brambling, brass, brassiere, breakwater, breastplate, briard, Brittany spaniel, broccoli, broom, brown bear, bubble, bucket, buckeye, buckle, bulbul, bull mastiff, bullet train, bulletproof
vest, bullfrog, burrito, bustard, butcher shop, butternut squash, cab, cabbage butterﬂy, cairn, caldron, can opener, candle,
cannon, canoe, capuchin, car mirror, car wheel, carbonara, Cardigan, cardigan, cardoon, carousel, carpenter’s kit, carton,
cash machine, cassette, cassette player, castle, catamaran, cauliﬂower, CD player, cello, cellular telephone, centipede,
chain, chain mail, chain saw, chainlink fence, chambered nautilus, cheeseburger, cheetah, Chesapeake Bay retriever, chest,
chickadee, chiffonier, Chihuahua, chime, chimpanzee, china cabinet, chiton, chocolate sauce, chow, Christmas stocking, church, cicada, cinema, cleaver, cliff, cliff dwelling, cloak, clog, clumber, cock, cocker spaniel, cockroach, cocktail
shaker, coffee mug, coffeepot, coho, coil, collie, colobus, combination lock, comic book, common iguana, common newt,
computer keyboard, conch, confectionery, consomme, container ship, convertible, coral fungus, coral reef, corkscrew,
corn, cornet, coucal, cougar, cowboy boot, cowboy hat, coyote, cradle, crane, crane, crash helmet, crate, crayﬁsh, crib,
cricket, Crock Pot, croquet ball, crossword puzzle, crutch, cucumber, cuirass, cup, curly-coated retriever, custard apple,
daisy, dalmatian, dam, damselﬂy, Dandie Dinmont, desk, desktop computer, dhole, dial telephone, diamondback, diaper,
digital clock, digital watch, dingo, dining table, dishrag, dishwasher, disk brake, Doberman, dock, dogsled, dome, doormat, dough, dowitcher, dragonﬂy, drake, drilling platform, drum, drumstick, dugong, dumbbell, dung beetle, Dungeness
crab, Dutch oven, ear, earthstar, echidna, eel, eft, eggnog, Egyptian cat, electric fan, electric guitar, electric locomotive,
electric ray, English foxhound, English setter, English springer, entertainment center, EntleBucher, envelope, Eskimo dog,
espresso, espresso maker, European ﬁre salamander, European gallinule, face powder, feather boa, ﬁddler crab, ﬁg, ﬁle,
ﬁre engine, ﬁre screen, ﬁreboat, ﬂagpole, ﬂamingo, ﬂat-coated retriever, ﬂatworm, ﬂute, ﬂy, folding chair, football helmet,
forklift, fountain, fountain pen, four-poster, fox squirrel, freight car, French bulldog, French horn, French loaf, frilled
lizard, frying pan, fur coat, gar, garbage truck, garden spider, garter snake, gas pump, gasmask, gazelle, German shepherd,
German short-haired pointer, geyser, giant panda, giant schnauzer, gibbon, Gila monster, go-kart, goblet, golden retriever,
goldﬁnch, goldﬁsh, golf ball, golfcart, gondola, gong, goose, Gordon setter, gorilla, gown, grand piano, Granny Smith,
grasshopper, Great Dane, great grey owl, Great Pyrenees, great white shark, Greater Swiss Mountain dog, green lizard,
green mamba, green snake, greenhouse, grey fox, grey whale, grille, grocery store, groenendael, groom, ground beetle,
guacamole, guenon, guillotine, guinea pig, gyromitra, hair slide, hair spray, half track, hammer, hammerhead, hamper,
hamster, hand blower, hand-held computer, handkerchief, hard disc, hare, harmonica, harp, hartebeest, harvester, harvestman, hatchet, hay, head cabbage, hen, hen-of-the-woods, hermit crab, hip, hippopotamus, hog, hognose snake, holster,
home theater, honeycomb, hook, hoopskirt, horizontal bar, hornbill, horned viper, horse cart, hot pot, hotdog, hourglass,
house ﬁnch, howler monkey, hummingbird, hyena, ibex, Ibizan hound, ice bear, ice cream, ice lolly, impala, Indian cobra,
Indian elephant, indigo bunting, indri, iPod, Irish setter, Irish terrier, Irish water spaniel, Irish wolfhound, iron, isopod,
Italian greyhound, jacamar, jack-o’-lantern, jackfruit, jaguar, Japanese spaniel, jay, jean, jeep, jellyﬁsh, jersey, jigsaw
puzzle, jinrikisha, joystick, junco, keeshond, kelpie, Kerry blue terrier, killer whale, kimono, king crab, king penguin,
king snake, kit fox, kite, knee pad, knot, koala, Komodo dragon, komondor, kuvasz, lab coat, Labrador retriever, lacewing,
ladle, ladybug, Lakeland terrier, lakeside, lampshade, langur, laptop, lawn mower, leaf beetle, leafhopper, leatherback
turtle, lemon, lens cap, Leonberg, leopard, lesser panda, letter opener, Lhasa, library, lifeboat, lighter, limousine, limpkin,
liner, lion, lionﬁsh, lipstick, little blue heron, llama, Loafer, loggerhead, long-horned beetle, lorikeet, lotion, loudspeaker,
loupe, lumbermill, lycaenid, lynx, macaque, macaw, Madagascar cat, magnetic compass, magpie, mailbag, mailbox, maillot, maillot, malamute, malinois, Maltese dog, manhole cover, mantis, maraca, marimba, marmoset, marmot, mashed
potato, mask, matchstick, maypole, maze, measuring cup, meat loaf, medicine chest, meerkat, megalith, menu, Mexican
hairless, microphone, microwave, military uniform, milk can, miniature pinscher, miniature poodle, miniature schnauzer,
minibus, miniskirt, minivan, mink, missile, mitten, mixing bowl, mobile home, Model T, modem, monarch, monastery,
mongoose, monitor, moped, mortar, mortarboard, mosque, mosquito net, motor scooter, mountain bike, mountain tent,
Int J Comput Vis 115:211–252
mouse, mousetrap, moving van, mud turtle, mushroom, muzzle, nail, neck brace, necklace, nematode, Newfoundland,
night snake, nipple, Norfolk terrier, Norwegian elkhound, Norwich terrier, notebook, obelisk, oboe, ocarina, odometer,
oil ﬁlter, Old English sheepdog, orange, orangutan, organ, oscilloscope, ostrich, otter, otterhound, overskirt, ox, oxcart,
oxygen mask, oystercatcher, packet, paddle, paddlewheel, padlock, paintbrush, pajama, palace, panpipe, paper towel,
papillon, parachute, parallel bars, park bench, parking meter, partridge, passenger car, patas, patio, pay-phone, peacock,
pedestal, Pekinese, pelican, Pembroke, pencil box, pencil sharpener, perfume, Persian cat, Petri dish, photocopier, pick,
pickelhaube, picket fence, pickup, pier, piggy bank, pill bottle, pillow, pineapple, ping-pong ball, pinwheel, pirate, pitcher,
pizza, plane, planetarium, plastic bag, plate, plate rack, platypus, plow, plunger, Polaroid camera, pole, polecat, police
van, pomegranate, Pomeranian, poncho, pool table, pop bottle, porcupine, pot, potpie, potter’s wheel, power drill, prairie
chicken, prayer rug, pretzel, printer, prison, proboscis monkey, projectile, projector, promontory, ptarmigan, puck, puffer,
pug, punching bag, purse, quail, quill, quilt, racer, racket, radiator, radio, radio telescope, rain barrel, ram, rapeseed, recreational vehicle, red fox, red wine, red wolf, red-backed sandpiper, red-breasted merganser, redbone, redshank, reel, reﬂex
camera, refrigerator, remote control, restaurant, revolver, rhinoceros beetle, Rhodesian ridgeback, riﬂe, ringlet, ringneck
snake, robin, rock beauty, rock crab, rock python, rocking chair, rotisserie, Rottweiler, rubber eraser, ruddy turnstone,
ruffed grouse, rugby ball, rule, running shoe, safe, safety pin, Saint Bernard, saltshaker, Saluki, Samoyed, sandal, sandbar,
sarong, sax, scabbard, scale, schipperke, school bus, schooner, scoreboard, scorpion, Scotch terrier, Scottish deerhound,
screen, screw, screwdriver, scuba diver, sea anemone, sea cucumber, sea lion, sea slug, sea snake, sea urchin, Sealyham
terrier, seashore, seat belt, sewing machine, Shetland sheepdog, shield, Shih-Tzu, shoe shop, shoji, shopping basket,
shopping cart, shovel, shower cap, shower curtain, siamang, Siamese cat, Siberian husky, sidewinder, silky terrier, ski,
ski mask, skunk, sleeping bag, slide rule, sliding door, slot, sloth bear, slug, snail, snorkel, snow leopard, snowmobile,
snowplow, soap dispenser, soccer ball, sock, soft-coated wheaten terrier, solar dish, sombrero, sorrel, soup bowl, space
bar, space heater, space shuttle, spaghetti squash, spatula, speedboat, spider monkey, spider web, spindle, spiny lobster,
spoonbill, sports car, spotlight, spotted salamander, squirrel monkey, Staffordshire bullterrier, stage, standard poodle,
standard schnauzer, starﬁsh, steam locomotive, steel arch bridge, steel drum, stethoscope, stingray, stinkhorn, stole, stone
wall, stopwatch, stove, strainer, strawberry, street sign, streetcar, stretcher, studio couch, stupa, sturgeon, submarine, suit,
sulphur butterﬂy, sulphur-crested cockatoo, sundial, sunglass, sunglasses, sunscreen, suspension bridge, Sussex spaniel,
swab, sweatshirt, swimming trunks, swing, switch, syringe, tabby, table lamp, tailed frog, tank, tape player, tarantula,
teapot, teddy, television, tench, tennis ball, terrapin, thatch, theater curtain, thimble, three-toed sloth, thresher, throne,
thunder snake, Tibetan mastiff, Tibetan terrier, tick, tiger, tiger beetle, tiger cat, tiger shark, tile roof, timber wolf, titi,
toaster, tobacco shop, toilet seat, toilet tissue, torch, totem pole, toucan, tow truck, toy poodle, toy terrier, toyshop, tractor,
trafﬁc light, trailer truck, tray, tree frog, trench coat, triceratops, tricycle, triﬂe, trilobite, trimaran, tripod, triumphal arch,
trolleybus, trombone, tub, turnstile, tusker, typewriter keyboard, umbrella, unicycle, upright, vacuum, valley, vase, vault,
velvet, vending machine, vestment, viaduct, vine snake, violin, vizsla, volcano, volleyball, vulture, wafﬂe iron, Walker
hound, walking stick, wall clock, wallaby, wallet, wardrobe, warplane, warthog, washbasin, washer, water bottle, water
buffalo, water jug, water ouzel, water snake, water tower, weasel, web site, weevil, Weimaraner, Welsh springer spaniel,
West Highland white terrier, whippet, whiptail, whiskey jug, whistle, white stork, white wolf, wig, wild boar, window
screen, window shade, Windsor tie, wine bottle, wing, wire-haired fox terrier, wok, wolf spider, wombat, wood rabbit,
wooden spoon, wool, worm fence, wreck, yawl, yellow lady’s slipper, Yorkshire terrier, yurt, zebra, zucchini
Appendix 2: Additional Single-Object Localization
Dataset Statistics
We consider two additional metrics of object localization dif-
ﬁculty: chance performance of localization and the level of
clutter. We use these metrics to compare ILSVRC2012-2014
single-object localization dataset to the PASCAL VOC 2012
object detection benchmark. The measures of localization
difﬁculty are computed on the validation set of both datasets.
According to both of these measures of difﬁculty there is
a subset of ILSVRC which is as challenging as PASCAL
but more than an order of magnitude greater in size. Figure 16 shows the distributions of different properties (object
scale, chance performance of localization and level of clutter)
across the different classes in the two datasets.
Chance Performance of Localization (CPL) Chance performance on a dataset is a common metric to consider. We deﬁne
theCPLmeasureastheexpectedaccuracyofadetectorwhich
ﬁrst randomly samples an object instance of that class and
then uses its bounding box directly as the proposed localization window on all other images (after rescaling the images
to the same size). Concretely, let B1, B2, . . . , BN be all the
bounding boxes of the object instances within a class, then
j̸=i I OU(Bi, B j) ≥0.5
Some of the most difﬁcult ILSVRC categories to localize
according to this metric are basketball, swimming trunks,
ping pong ball and rubber eraser, all with less than 0.2 %
CPL. This measure correlates strongly (ρ = 0.9) with the
average scale of the object (fraction of image occupied by
object). The average CPL across the 1000 ILSVRC categories is 20.8 %. The 20 PASCAL categories have an average
CPL of 8.7 %, which is the same as the CPL of the 562 most
difﬁcult categories of ILSVRC.
Clutter Intuitively, even small objects are easy to localize
on a plain background. To quantify clutter we employ the
objectness measure of , which is a classgeneric object detector evaluating how likely a window in the
image contains a coherent object (of any class) as opposed to
background (sky, water, grass). For every image m containing target object instances at positions Bm
2 , . . . , we use
the publicly available objectness software to sample 1000
windows W m
2 , . . . W m
1000, in order of decreasing probability of the window containing any generic object. Let
obj(m) be the number of generic object-looking windows
sampled before localizing an instance of the target category,
i.e., obj(m) = min{k : maxi iou(W m
i ) ≥0.5}. For
a category containing M images, we compute the average
number of such windows per image and deﬁne
Clutter = log2
The higher the clutter of a category, the harder the objects
are to localize according to generic cues. If an object can’t
be localized with the ﬁrst 1000 windows (as is the case for
1 % of images on average per category in ILSVRC and 5 %
in PASCAL), we set obj(m) = 1001. The fact that more than
95 % of objects can be localized with these windows imply
that the objectness cue is already quite strong, so objects that
require many windows on average will be extremely difﬁcult
to detect: e.g., ping pong ball (clutter of 9.57, or 758 windows on average), basketball (clutter of 9.21), puck (clutter
of 9.17) in ILSVRC. The most difﬁcult object in PASCAL
is bottle with clutter score of 8.47. On average, ILSVRC has
clutter score of 3.59. The most difﬁcult subset of ILSVRC
with 250 object categories has an order of magnitude more
categories and the same average amount of clutter (of 5.90)
as the PASCAL dataset.
Appendix 3: Manually Curated Queries
for Obtaining Object Detection Scene Images
In Sect. 3.3.2 we discussed three types of queries we used
for collecting the object detection images: (1) single object
category name or a synonym; (2) a pair of object category
names; (3) a manual query, typically targetting one or more
Int J Comput Vis 115:211–252
Fig. 16 Distribution of various measures of localization difﬁculty
on the ILSVRC2012-2014 single-object localization (dark green) and
PASCAL VOC 2012 (light blue) validation sets. Object scale is fraction
of image area occupied by an average object instance. Chance performance of localization and level of clutter are deﬁned in Appendix 1. The
plots on top contain the full ILSVRC validation set with 1000 classes;
the plots on the bottom contain 200 ILSVRC classes with the lowest
chance performance of localization. All plots contain all 20 classes of
PASCAL VOC
object categories with insufﬁcient data. Here we provide a
list of the 129 manually curated queries:
afternoon tea, ant bridge building, armadillo race, armadillo yard, artist studio, auscultation, baby room, banjo orchestra,
banjo rehersal, banjo show, califone headphones & media player sets, camel dessert, camel tourist, carpenter drilling,
carpentry, centipede wild, coffee shop, continental breakfast toaster, continental breakfast wafﬂes, crutch walking, desert
scorpion, diner, dining room, dining table, dinner, dragonﬂy friendly, dragonﬂy kid, dragonﬂy pond, dragonﬂy wild,
drying hair, dumbbell curl, fan blow wind, fast food, fast food restaurant, ﬁrewood chopping, ﬂu shot, goldﬁsh aquarium,
goldﬁsh tank, golf cart on golf course, gym dumbbell, hamster drinking water, harmonica orchestra, harmonica rehersal,
harmonica show, harp ensemble, harp orchestra, harp rehersal, harp show, hedgehog cute, hedgehog ﬂoor, hedgehog
hidden, hippo bird, hippo friendly, home improvement diy drill, horseback riding, hotel coffee machine, hotel coffee
maker, hotel wafﬂe maker, jellyﬁsh scuba, jellyﬁsh snorkling, kitchen, kitchen counter coffee maker, kitchen counter
toaster, kitchenette, koala feed, koala tree, ladybug ﬂower, ladybug yard, laundromat, lion zebra friendly, lunch, mailman,
making breakfast, making wafﬂes, mexican food, motorcycle racing, ofﬁce, ofﬁce fan, opossum on tree branch, orchestra,
panda play, panda tree, pizzeria, pomegranate tree, porcupine climbing trees, power drill carpenter, purse shop, red panda
tree, riding competition, riding motor scooters, school supplies, scuba starﬁsh, sea lion beach, sea otter, sea urchin habitat,
shopping for school supplies, sitting in front of a fan, skunk and cat, skunk park, skunk wild, skunk yard, snail ﬂower,
snorkling starﬁsh, snowplow cleanup, snowplow pile, snowplow winter, soccer game, south american zoo, starﬁsh sea
world, starts shopping, steamed artichoke, stethoscope doctor, strainer pasta, strainer tea, syringe doctor, table with food,
tape player, tiger circus, tiger pet, using a can opener, using power drill, wafﬂe iron breakfast, wild lion savana, wildlife
preserve animals, wiping dishes, wombat petting zoo, zebra savana, zoo feeding, zoo in australia
Appendix 4: Hierarchy of Questions for Full Image
Annotation
The following is a hierarchy of questions manually constructed for crowdsourcing full annotation of images with
the presence or absence of 200 object detection categories
in ILSVRC2013 and ILSVRC2014. All questions are of the
form “is there a ... in the image?” Questions marked with •
are asked on every image. If the answer to a question is determined to be “no” then the answer to all descendant questions
is assumed to be “no”. The 200 numbered leaf nodes correspond to the 200 object detection categories.
The goal in the hierarchy construction is to save cost (by
asking as few questions as possible on every image) while
avoiding any ambiguity in questions which would lead to
false negatives during annotation. This hierarchy is not treestructured; some questions have multiple parents.
Hierarchy of questions • ﬁrst aid/ medical items
◦(1) stethoscope
◦(2) syringe
◦(3) neck brace
◦(4) crutch
◦(5) stretcher
◦(6) band aid: an adhesive bandage to cover small cuts or blisters
• musical instruments
◦(7) accordion (a portable box-shaped free-reed instrument; the reeds are made to vibrate by air from the bellows
controlled by the player)
◦(8) piano, pianoforte, forte-piano
◦percussion instruments: chimes, maraccas, drums, etc
◦(9) chime: a percussion instrument consisting of a set of tuned bells that are struck with a hammer; used as an
orchestral instrument
◦(10) maraca
◦(11) drum
◦stringed instrument
◦(12) banjo, the body of a banjo is round. please do not confuse with guitar
◦(13) cello: a large stringed instrument; seated player holds it upright while playing
◦(14) violin: bowed stringed instrument that has four strings, a hollow body, an unfretted ﬁngerboard and is
played with a bow. please do not confuse with cello, which is held upright while playing
◦(15) harp
◦(16) guitar, please do not confuse with banjo. the body of a banjo is round
Int J Comput Vis 115:211–252
◦wind instrument: a musical instrument in which the sound is produced by an enclosed column of air that is moved
by the breath (such as trumpet, french horn, harmonica, ﬂute, etc)
◦(17) trumpet: a brass musical instrument with a narrow tube and a ﬂared bell, which is played by means of
valves. often has 3 keys on top
◦(18) french horn: a brass musical instrument consisting of a conical tube that is coiled into a spiral, with a ﬂared
bell at the end
◦(19) trombone: a brass instrument consisting of a long tube whose length can be varied by a u-shaped slide
◦(20) harmonica
◦(21) ﬂute: a high-pitched musical instrument that looks like a straight tube and is usually played sideways (please
do not confuse with oboes, which have a distinctive straw-like mouth piece and a slightly ﬂared end)
◦(22) oboe: a slender musical instrument roughly 65cm long with metal keys, a distinctive straw-like mouthpiece
and often a slightly ﬂared end (please do not confuse with ﬂutes)
◦(23) saxophone: a musical instrument consisting of a brass conical tube, often with a u-bend at the end
• food: something you can eat or drink (includes growing fruit, vegetables and mushrooms, but does not include living
◦food with bread or crust: pretzel, bagel, pizza, hotdog, hamburgers, etc
◦(24) pretzel
◦(25) bagel, beigel
◦(26) pizza, pizza pie
◦(27) hotdog, hot dog, red hot
◦(28) hamburger, beefburger, burger
◦(29) guacamole
◦(30) burrito
◦(31) popsicle (ice cream or water ice on a small wooden stick)
◦(33) pineapple, ananas
◦(34) banana
◦(35) pomegranate
◦(36) apple
◦(37) strawberry
◦(38) orange
◦(39) lemon
◦vegetables
◦(40) cucumber, cuke
◦(41) artichoke, globe artichoke
◦(42) bell pepper
◦(43) head cabbage
◦(44) mushroom
• items that run on electricity (plugged in or using batteries); including clocks, microphones, trafﬁc lights, computers,
◦(45) remote control, remote
◦electronics that blow air
◦(46) hair dryer, blow dryer
◦(47) electric fan: a device for creating a current of air by movement of a surface or surfaces (please do not
consider hair dryers)
◦electronics that can play music or amplify sound
◦(48) tape player
◦(49) iPod
◦(50) microphone, mike
◦computer and computer peripherals: mouse, laptop, printer, keyboard, etc
◦(51) computer mouse
◦(52) laptop, laptop computer
◦(53) printer (please do not consider typewriters to be printers)
◦(54) computer keyboard
◦(55) lamp
◦electric cooking appliance (an appliance which generates heat to cook food or boil water)
◦(56) microwave, microwave oven
◦(57) toaster
◦(58) wafﬂe iron
◦(59) coffee maker: a kitchen appliance used for brewing coffee automatically
◦(60) vacuum, vacuum cleaner
◦(61) dishwasher, dish washer, dishwashing machine
◦(62) washer, washing machine: an electric appliance for washing clothes
◦(63) trafﬁc light, trafﬁc signal, stoplight
◦(64) tv or monitor: an electronic device that represents information in visual form
◦(65) digital clock: a clock that displays the time of day digitally
• kitchen items: tools,utensils and appliances usually found in the kitchen
◦electric cooking appliance (an appliance which generates heat to cook food or boil water)
◦(56) microwave, microwave oven
◦(57) toaster
◦(58) wafﬂe iron
◦(59) coffee maker: a kitchen appliance used for brewing coffee automatically
◦(61) dishwasher, dish washer, dishwashing machine
◦(66) stove
◦things used to open cans/bottles: can opener or corkscrew
◦(67) can opener (tin opener)
◦(68) corkscrew
◦(69) cocktail shaker
◦non-electric item commonly found in the kitchen: pot, pan, utensil, bowl, etc
◦(70) strainer
◦(71) frying pan (skillet)
◦(72) bowl: a dish for serving food that is round, open at the top, and has no handles (please do not confuse with
a cup, which usually has a handle and is used for serving drinks)
◦(73) salt or pepper shaker: a shaker with a perforated top for sprinkling salt or pepper
◦(74) plate rack
◦(75) spatula: a turner with a narrow ﬂexible blade
◦(76) ladle: a spoon-shaped vessel with a long handle; frequently used to transfer liquids from one container to
◦(77) refrigerator, icebox
• furniture (including benches)
◦(78) bookshelf: a shelf on which to keep books
◦(79) baby bed: small bed for babies, enclosed by sides to prevent baby from falling
◦(80) ﬁling cabinet: ofﬁce furniture consisting of a container for keeping papers in order
◦(81) bench (a long seat for several people, typically made of wood or stone)
◦(82) chair: a raised piece of furniture for one person to sit on; please do not confuse with benches or sofas, which
are made for more people
◦(83) sofa, couch: upholstered seat for more than one person; please do not confuse with benches (which are made
of wood or stone) or with chairs (which are for just one person)
◦(84) table
• clothing, article of clothing: a covering designed to be worn on a person’s body
◦(85) diaper: Garment consisting of a folded cloth drawn up between the legs and fastened at the waist; worn by
infants to catch excrement
◦swimming attire: clothes used for swimming or bathing (swim suits, swim trunks, bathing caps)
◦(86) swimming trunks: swimsuit worn by men while swimming
◦(87) bathing cap, swimming cap: a cap worn to keep hair dry while swimming or showering
◦(88) maillot: a woman’s one-piece bathing suit
◦necktie: a man’s formal article of clothing worn around the neck (including bow ties)
◦(89) bow tie: a man’s tie that ties in a bow
◦(90) tie: a long piece of cloth worn for decorative purposes around the neck or shoulders, resting under the shirt
collar and knotted at the throat (NOT a bow tie)
◦headdress, headgear: clothing for the head (hats, helmets, bathing caps, etc)
◦(87) bathing cap, swimming cap: a cap worn to keep hair dry while swimming or showering
◦(91) hat with a wide brim
◦(92) helmet: protective headgear made of hard material to resist blows
◦(93) miniskirt, mini: a very short skirt
◦(94) brassiere, bra: an undergarment worn by women to support their breasts
◦(95) sunglasses
• living organism (other than people): dogs, snakes, ﬁsh, insects, sea urchins, starﬁsh, etc.
◦living organism which can ﬂy
◦(97) dragonﬂy
◦(98) ladybug
◦(99) butterﬂy
◦(100) bird
◦living organism which cannot ﬂy (please don’t include humans)
◦living organism with 2 or 4 legs (please don’t include humans):
◦mammals (but please do not include humans)
◦feline (cat-like) animal: cat, tiger or lion
◦(101) domestic cat
◦(102) tiger
◦(103) lion
◦canine (dog-like animal): dog, hyena, fox or wolf
◦(104) dog, domestic dog, canis familiaris
◦(105) fox: wild carnivorous mammal with pointed muzzle and ears and a bushy tail (please do not
confuse with dogs)
◦animals with hooves: camels, elephants, hippos, pigs, sheep, etc
◦(106) elephant
◦(107) hippopotamus, hippo
◦(108) camel
◦(109) swine: pig or boar
◦(110) sheep: woolly animal, males have large spiraling horns (please do not confuse with antelope
which have long legs)
◦(111) cattle: cows or oxen (domestic bovine animals)
◦(112) zebra
◦(113) horse
◦(114) antelope: a graceful animal with long legs and horns directed upward and backward
◦(115) squirrel
◦(116) hamster: short-tailed burrowing rodent with large cheek pouches
◦(117) otter
◦(118) monkey
◦(119) koala bear
◦(120) bear (other than pandas)
◦(121) skunk (mammal known for its ability fo spray a liquid with a strong odor; they may have a single
thick stripe across back and tail, two thinner stripes, or a series of white spots and broken stripes
◦(122) rabbit
◦(123) giant panda: an animal characterized by its distinct black and white markings
◦(124) red panda: Reddish-brown Old World raccoon-like carnivore
◦(125) frog, toad
◦(126) lizard: please do not confuse with snake (lizards have legs)
◦(127) turtle
◦(128) armadillo
◦(129) porcupine, hedgehog
◦living organism with 6 or more legs: lobster, scorpion, insects, etc.
◦(130) lobster: large marine crustaceans with long bodies and muscular tails; three of their ﬁve pairs of legs
have claws
◦(131) scorpion
◦(132) centipede: an arthropod having a ﬂattened body of 15 to 173 segments each with a pair of legs, the
foremost pair being modiﬁed as prehensors
◦(133) tick (a small creature with 4 pairs of legs which lives on the blood of mammals and birds)
◦(134) isopod: a small crustacean with seven pairs of legs adapted for crawling
◦(135) ant
◦living organism without legs: ﬁsh, snake, seal, etc. (please don’t include plants)
◦living organism that lives in water: seal, whale, ﬁsh, sea cucumber, etc.
◦(136) jellyﬁsh
◦(137) starﬁsh, sea star
◦(138) seal
◦(139) whale
◦(140) ray: a marine animal with a horizontally ﬂattened body and enlarged winglike pectoral ﬁns with
gills on the underside
◦(141) goldﬁsh: small golden or orange-red ﬁshes
◦living organism that slides on land: worm, snail, snake
◦(142) snail
◦(143) snake: please do not confuse with lizard (snakes do not have legs)
• vehicle: any object used to move people or objects from place to place
◦a vehicle with wheels
◦(144) golfcart, golf cart
◦(145) snowplow: a vehicle used to push snow from roads
◦(146) motorcycle (or moped)
◦(147) car, automobile (not a golf cart or a bus)
◦(148) bus: a vehicle carrying many passengers; used for public transport
◦(149) train
◦(150) cart: a heavy open wagon usually having two wheels and drawn by an animal
◦(151) bicycle, bike: a two wheeled vehicle moved by foot pedals
◦(152) unicycle, monocycle
◦a vehicle without wheels (snowmobile, sleighs)
◦(153) snowmobile: tracked vehicle for travel on snow
◦(154) watercraft (such as ship or boat): a craft designed for water transportation
◦(155) airplane: an aircraft powered by propellers or jets
• cosmetics: toiletry designed to beautify the body
◦(156) face powder
◦(157) perfume, essence (usually comes in a smaller bottle than hair spray
◦(158) hair spray
◦(159) cream, ointment, lotion
◦(160) lipstick, lip rouge
• carpentry items: items used in carpentry, including nails, hammers, axes, screwdrivers, drills, chain saws, etc
◦(161) chain saw, chainsaw
◦(162) nail: pin-shaped with a head on one end and a point on the other
◦(163) axe: a sharp tool often used to cut trees/ logs
◦(164) hammer: a blunt hand tool used to drive nails in or break things apart (please do not confuse with axe, which
◦(165) screwdriver
◦(166) power drill: a power tool for drilling holes into hard materials
• school supplies: rulers, erasers, pencil sharpeners, pencil boxes, binders
◦(167) ruler,rule: measuring stick consisting of a strip of wood or metal or plastic with a straight edge that is used
for drawing straight lines and measuring lengths
◦(168) rubber eraser, rubber, pencil eraser
Int J Comput Vis 115:211–252
◦(169) pencil sharpener
◦(170) pencil box, pencil case
◦(171) binder, ring-binder
• sports items: items used to play sports or in the gym (such as skis, raquets, gymnastics bars, bows, punching bags,
◦(172) bow: weapon for shooting arrows, composed of a curved piece of resilient wood with a taut cord to propel
◦(173) puck, hockey puck: vulcanized rubber disk 3 inches in diameter that is used instead of a ball in ice hockey
◦(174) ski
◦(175) racket, racquet
◦gymnastic equipment: parallel bars, high beam, etc
◦(176) balance beam: a horizontal bar used for gymnastics which is raised from the ﬂoor and wide enough to
◦(177) horizontal bar, high bar: used for gymnastics; gymnasts grip it with their hands (please do not confuse
with balance beam, which is wide enough to walk on)
◦(178) golf ball
◦(179) baseball
◦(180) basketball
◦(181) croquet ball
◦(182) soccer ball
◦(183) ping-pong ball
◦(184) rugby ball
◦(185) volleyball
◦(186) tennis ball
◦(187) punching bag, punch bag, punching ball, punchball
◦(188) dumbbell: An exercising weight; two spheres connected by a short bar that serves as a handle
• liquid container: vessels which commonly contain liquids such as bottles, cans, etc.
◦(189) pitcher: a vessel with a handle and a spout for pouring
◦(190) beaker: a ﬂatbottomed jar made of glass or plastic; used for chemistry
◦(191) milk can
◦(192) soap dispenser
◦(193) wine bottle
◦(194) water bottle
◦(195) cup or mug (usually with a handle and usually cylindrical)
◦(196) backpack: a bag carried by a strap on your back or shoulder
◦(197) purse: a small bag for carrying money
◦(198) plastic bag
• (199) person
• (200) ﬂower pot: a container in which plants are cultivated
Appendix 5: Modiﬁcation to Bounding Box System
for Object Detection
The bounding box annotation system described in Sect. 3.2.1
is used for annotating images for both the single-object localization dataset and the object detection dataset. However, two
additional manual post-processing are needed to ensure accuracy in the object detection scenario:
Ambiguous Objects The ﬁrst common source of error was
that workers were not able to accurately differentiate some
object classes during annotation. Some commonly confused
labels were seal and sea otter, backpack and purse, banjo and
guitar, violin and cello, brass instruments (trumpet, trombone, french horn and brass), ﬂute and oboe, ladle and
spatula. Despite our best efforts (providing positive and negative example images in the annotation task, adding text
explanations to alert the user to the distinction between these
categories) these errors persisted.
In the single-object localization setting, this problem was
not as prominent for two reasons. First, the way the data was
collected imposed a strong prior on the object class which
was present. Second, since only one object category needed
to be annotated per image, ambiguous images could be discarded: for example, if workers couldn’t agree on whether or
not a trumpet was in fact present, this image could simply be
removed. In contrast, for the object detection setting consensus had to be reached for all target categories on all images.
To ﬁx this problem, once bounding box annotations were
collected we manually looked through all cases where the
bounding boxes for two different object classes had significant overlap with each other (about 3 % of the collected
boxes). About a quarter of these boxes were found to correspond to incorrect objects and were removed. Crowdsourcing
this post-processing step (with very stringent accuracy constraints) would be possible but it occurred in few enough
casesthatitwasfaster(andmoreaccurate)todothisin-house.
Duplicate Annotations The second common source of error
were duplicate bounding boxes drawn on the same object
instance. Despite instructions not to draw more than one
bounding box around the same object instance and constraints in the annotation UI enforcing at least a 5 pixel
difference between different bounding boxes, these errors
persisted.Onereasonwasthatsometimestheinitialbounding
box was not perfect and subsequent labelers drew a slightly
improved alternative.
This type of error was also present in the single-object
localization scenario but was not a major cause for concern.
A duplicate bounding box is a slightly perturbed but still
correct positive example, and single-object localization is
only concerned with correctly localizing one object instance.
For the detection task algorithms are evaluated on the ability
to localize every object instance, and penalized for duplicate
detections, so it is imperative that these labeling errors are
corrected (even if they only appear in about 0.6 % of cases).
Approximately 1 % of bounding boxes were found to have
signiﬁcant overlap of more than 50 % with another bounding
box of the same object class.We again manually veriﬁed all
of these cases in-house. In approximately 40 % of the cases
the two bounding boxes correctly corresponded to different
people in a crowd, to stacked plates, or to musical instruments
nearby in an orchestra. In the other 60 % of cases one of the
boxes was randomly removed.
These veriﬁcation steps complete the annotation procedure of bounding boxes around every instance of every object
class in validation, test and a subset of training images for
the detection task.
Training Set Annotation With the optimized algorithm of
Sect. 3.3.3 we fully annotated the validation and test sets.
However, annotating all training images with all target object
classes was still a budget challenge. Positive training images
taken from the single-object localization dataset already had
bounding box annotations of all instances of one object class
on each image. We extended the existing annotations to the
detection dataset by making two modiﬁcation. First, we corrected any bounding box omissions resulting from merging
ﬁne-grained categories: i.e., if an image belonged to the
“dalmatian” category and all instances of “dalmatian” were
annotated with bounding boxes for single-object localization, we ensured that all remaining “dog” instances are also
annotated for the object detection task. Second, we collected
signiﬁcantly more training data for the person class because
Int J Comput Vis 115:211–252
the existing annotation set was not diverse enough to be representative (the only people categories in the single-object
localization task are scuba diver, groom, and ballplayer). To
compensate, we additionally annotated people in a large fraction of the existing training set images.
Appendix 6: Competition Protocol
Competition Format At the beginning of the competition
period each year we release the new training/validation/test
images, training/validation annotations, and competition
speciﬁcation for the year. We then specify a deadline for submission, usually approximately 4 months after the release of
data. Teams are asked to upload a text ﬁle of their predicted
annotations on test images by this deadline to a provided
server. We then evaluate all submissions and release the
For every task we released code that takes a text ﬁle of
automatically generated image annotations and compares it
with the ground truth annotations to return a quantitative
measure of algorithm accuracy. Teams can use this code to
evaluate their performance on the validation data.
As described in Everingham et al. , there are three
options for measuring performance on test data: (i) Release
test images and annotations, and allow participants to assess
performance themselves; (ii) Release test images but not
test annotations—participants submit results and organizers
assess performance; (iii) Neither test images nor annotations
are released—participants submit software and organizers
run it on new data and assess performance. In line with the
PASCAL VOC choice, we opted for option (ii). Option (i)
allows too much leeway in overﬁtting to the test data; option
(iii) is infeasible, especially given the scale of our test set
(40K–100K images).
We released ILSVRC2010 test annotations for the image
classiﬁcationtask,butallothertestannotationshaveremained
hidden to discourage ﬁne-tuning results on the test data.
Evaluation Protocol After the Challenge After the challenge period we set up an automatic evaluation server that
researchers can use throughout the year to continue evaluating their algorithms against the ground truth test annotations.
We limit teams to 2 submissions per week to discourage parameter tuning on the test data, and in practice we have never
had a problem with researchers abusing the system.