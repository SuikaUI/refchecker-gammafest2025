A Survey on Ensemble Learning under the Era of Deep Learning
Yongquan Yanga, Haijun Lvb, Ning Chenc
a Institution of Clinical Pathology, West China Hospital, Sichuan University, 37 Guo
Xue Road, 610041 Chengdu, China
b AIDP, Baidu Co., Ltd, 701 Na Xian Road, 201210 Shanghai, China
c School of Electronics and Information, Xi’an Polytechnic University, 19 Jin Hua Road,
710048 Xi’an, China
Due to the dominant position of deep learning (mostly deep neural networks) in
various artificial intelligence applications, recently, ensemble learning based on deep
neural networks (ensemble deep learning) has shown significant performances in
improving the generalization of learning system. However, since modern deep neural
networks usually have millions to billions of parameters, the time and space overheads
for training multiple base deep learners and testing with the ensemble deep learner
are far greater than that of traditional ensemble learning. Though several algorithms
of fast ensemble deep learning have been proposed to promote the deployment of
ensemble deep learning in some applications, further advances still need to be made
for many applications in specific fields, where the developing time and computing
resources are usually restricted or the data to be processed is of large dimensionality.
An urgent problem needs to be solved is how to take the significant advantages of
ensemble deep learning while reduce the required expenses so that many more
applications in specific fields can benefit from it. For the alleviation of this problem, it
is essential to know about how ensemble learning has developed under the era of
deep learning. Thus, in this article, we present fundamental discussions focusing on
data analyses of published works, methodologies, recent advances and unattainability
of traditional ensemble learning and ensemble deep learning. We hope this article will
be helpful to realize the intrinsic problems and technical challenges faced by future
developments of ensemble learning under the era of deep learning.
Correspondence to: Yongquan Yang ( )
1 Introduction
Ensemble learning , a machine-learning
technique that utilizes multiple base learners to form an ensemble learner for the
achievement of better generalization of learning system, has achieved great success in
various artificial intelligence applications and received extensive attention from the
machine learning community. However, with the increase in the number of base
learners, the costs for training multiple base learners and testing with the ensemble
learner also increases rapidly, which inevitably hinders the universality of the usage of
ensemble learning in many artificial intelligence applications. Especially when deep
learning ) is dominating the development of various artificial intelligence
applications, the usage of ensemble learning based on deep neural networks
(ensemble deep learning) is facing severe obstacles.
Since modern deep neural networks usually have millions to billions of parameters, the time
and space overheads for training multiple base deep learners and testing with the
ensemble deep learner is far greater than that of traditional ensemble learning. Fast
ensemble deep learning algorithms like Snapshot , fast geometric
ensembling (FGE) and stochastic weight averaging (SWA)
 have promoted the deployment of ensemble
deep learning in some artificial intelligence applications to some extent. However, due
to the large expenses compared with traditional ensemble learning, the deployment
of ensemble deep learning algorithms still needs further advances in some specific
fields, where the developing time and computing resources are usually restricted ) or the data to be processed is of
large dimensionality ). In these specific fields, it is sometimes still difficult to deploy the
traditional ensemble learning algorithms, which makes the deployment of ensemble
deep learning even more challenging. Thus, an urgent problem needs to be solved is
how to make full usage of the significant advantages of ensemble deep learning while
reducing the expenses required for both training and testing so that many more
artificial intelligence applications in specific fields can benefit from it.
To alleviate this problem, it is essential to know about how ensemble learning has
developed under the era of deep learning. In this article, we achieve this by presenting
discussions with our observations of various existing ensemble learning algorithms
and related applications. Different from existing review articles that mostly discussed about traditional ensemble learning, reviews
 that particularly discussed ensemble deep learning in bioinformatics,
or specific technical innovations with ensemble learning introduced, such as GAN-
Ensembles , this article aims
to reveal the intrinsic problems and technical challenges of deploying ensemble
learning under the era of deep learning to a wider range of specific fields, with more
fundamental discussions about the developing routes of both traditional ensemble
learning and ensemble deep learning. First, through data analysis of published works,
we show the prosperity of ensemble learning and the gap between ensemble deep
learning and traditional ensemble learning. Second, we discuss the methodologies,
recent advances and unattainability of traditional ensemble learning by analyzing
existing well-known approaches, applications and giving some of our observations.
Third, we discuss the methodologies, recent advances and unattainability of ensemble
deep learning with analyses and observations in the context of usual and fast solutions.
Finally, we discuss the whole paper, pointing out some possible future research
directions.
2 Data Analysis of Published Works
In this section, we present some data analysis of published works to show the
prosperity of ensemble learning (EL) and the gap between ensemble deep learning
(EDL) and traditional ensemble learning (TEL).
2.1 Prosperity of EL
The research history of EL can be traced back to 1990 . Due to its significant advantages in improving the generalization of
learning systems, the research on the theoretical algorithms and applications of EL has
always been a research hotspot. Dietterich , an authoritative scholar
in the field of machine learning, once listed EL as the first of four research directions
in the field of machine learning in AI Magazine. In the past 30 years, the research of EL
has made remarkable progress. As shown in Figure 1, the number of papers related to
the topic of ‘ensemble learning’ and published in the core set of Web of Science from
1990 to 2019 has been increasing steadily year by year, which reflects an exponentially
growing prosperity of EL.
Figure. 1. The number of papers published in the core set of Web of Science from 1990 to 2019 for
the topic of ‘ensemble learning’.
'ensemble learning'
Number of papers
2.2 Gap between EDL and TEL
The research history of ensembles of neural networks can be traced back to the
early research stage of ensemble learning in 1990 .
However, the research progress of ensembles of deep neural networks (ensemble
deep learning, EDL) had been very slow before 2014. Later benefited from the great
success of deep neural networks with popularization of big data and high-performance
computing resources, the research progress of EDL began to sprout rapidly. Figure 2
shows the comparison between the number of papers related to ‘ensemble deep
learning’ or ‘ensemble learning’ not ‘deep’ (traditional ensemble learning, TEL) and
 
Figure 2 that a large gap exists between EDL and TEL. Compared with TEL, the
development of EDL is severely lagging behind. The primary reason for this
phenomenon is that modern deep neural networks usually have parameters ranging from
millions to billions which makes the time and space overheads for the training and
testing stages of EDL much greater than that of TEL, which severely hinders the
deployment of EDL in various artificial intelligence applications in specifical fields.
Figure 2. The comparison between the number of papers published in the core set of Web of
Science from 1990 to 2019 for the topic of ‘traditional ensemble learning’ (’ensemble learning’ not
‘deep’) and the topic of ‘ensemble deep learning’.
3 Traditional Ensemble Learning
Traditional ensemble learning (TEL) has been playing a major role in the research
history of ensemble learning (EL). In this section, starting with the paradigm of usual
machine learning (UML), we respectively present the methodology of TEL, well-known
implementations for the methodology of TEL, recent advances of TEL and
unattainability of TEL.
3.1 Preliminary
'ensemble learning' not 'deep'
'ensemble deep learning'
Number of papers
Let us first consider the paradigm of usual machine learning (UML), where a raw
data set 𝐷= {𝑑1, ⋯, 𝑑𝑛} and its corresponding target set 𝑇= {𝑡1, ⋯, 𝑡𝑛} are given.
Specifically, 𝑑𝑛 is a raw data point of 𝐷 and 𝑡𝑛 is the target corresponding to 𝑑𝑛.
In the UML paradigm, two components are normally essential: 1) feature extraction
which converts raw data points into corresponding learnable representations; and 2)
model development which evolves a learner that can map representations into
corresponding targets.
The feature extraction consists of a number of extracting methods , and a converting procedure. An extracting method is responsible for
extracting a type of representations from raw data points; and the converting
procedure is responsible for incorporating the results of extracting methods into
learnable representations for the whole raw data set. The model development
component consists of a learning algorithm, a learning strategy, and an evolving
procedure. The learning algorithm is responsible for the construction and optimization
of a learner; the learning strategy is responsible for configurating rules to carry out the
evolving procedure; and the evolving procedure is responsible for, under the learning
strategy, updating the parameters of the learner to be appropriate for mapping
representations into corresponding targets. The outline for the methodology of UML
is shown as Figure. 3.
Figure. 3. The methodology of usual machine learning (UML), which consists of feature extraction
and model development.
Formally, let 𝐸𝑀= {𝑒𝑚1(∗; 𝜃1
𝑒𝑚), ⋯, 𝑒𝑚𝑚(∗; 𝜃𝑚
𝑒𝑚)} denote various extracting
methods. Particularly, 𝑒𝑚𝑚(∗; 𝜃𝑚
𝑒𝑚) signifies an extracting method 𝑒𝑚𝑚(∗)
parameterized by 𝜃𝑚
𝑒𝑚. The feature extraction component can be expressed as
𝐹= 𝐹𝑒𝑎𝑡𝑢𝑟𝑒𝐸𝑥𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛(𝐷, 𝐸𝑀 ) = {𝑓1, ⋯, 𝑓𝑛}.
Thereinto, 𝑓𝑛 , which signifies the representation of 𝑑𝑛 , is constructed by the
converting procedure that can be more specifically expressed as
𝑓𝑛= 𝑐𝑜𝑛𝑣𝑒𝑟𝑡𝑖𝑛𝑔(𝑑𝑛, 𝐸𝑀)
= {𝑒𝑚1(𝑑𝑛; 𝜃1
𝑒𝑚), ⋯, 𝑒𝑚𝑚(𝑑𝑛; 𝜃𝑚
= {𝑥𝑛,1, ⋯, 𝑥𝑛,𝑘} 𝑠. 𝑡. 𝑘≥𝑚.
Formally, let 𝐿𝐴= {𝑙(∗; 𝜃𝑙), 𝑜𝑝𝑡(∗,∗; 𝜃𝑜𝑝𝑡)} denote the learning algorithm and
learning algorithm
feature extraction
model development
learning strategy
training data
converting
extracting methods
𝐿𝑆= {𝑙𝑠(𝜃𝑙𝑠)} denote the learning strategy. Specifically, 𝑙(∗; 𝜃𝑙) signifies the
construction of a learner 𝑙(∗) parameterized by 𝜃𝑙 and 𝑜𝑝𝑡(∗,∗; 𝜃𝑜𝑝𝑡) signifies the
learner’s optimization procedure 𝑜𝑝𝑡(∗,∗) parameterized by 𝜃𝑜𝑝𝑡 for updates of 𝜃𝑙,
and 𝑙𝑠(𝜃𝑙𝑠) signifies a learning strategy 𝑙𝑠 parameterized by 𝜃𝑙𝑠 . Note, here
𝑜𝑝𝑡(∗,∗) implicitly consists of an objective function constructed for classification or
regression and corresponding optimization of the objective function. The model
development component can be expressed as
𝐿= 𝑀𝑜𝑑𝑒𝑙𝐷𝑒𝑣𝑒𝑙𝑜𝑝𝑚𝑒𝑛𝑡(𝐹, 𝑇, 𝐿𝐴, 𝐿𝑆) = {𝑙(∗; 𝜃𝑢
Thereinto, 𝑙(∗; 𝜃𝑢
𝑙) signifies the learner 𝑙(∗) parameterized by 𝜃𝑢
𝑙, which is updated
by the evolving procedure that can be more specifically expressed as
𝑙= 𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐹, 𝑇, 𝑙(∗; 𝜃𝑙), 𝑜𝑝𝑡(∗,∗; 𝜃𝑜𝑝𝑡) | 𝑙𝑠(𝜃𝑙𝑠))
𝜃𝑙(𝑙(𝐹; 𝜃𝑙), 𝑇; 𝜃𝑜𝑝𝑡 | 𝑙𝑠(𝜃𝑙𝑠)).
The developed learner 𝑙(∗; 𝜃𝑢
𝑙) forms the mapping between representations 𝐹 and
corresponding targets 𝑇.
At testing, given a test raw data point 𝑑𝑡𝑒𝑠𝑡 , the corresponding target 𝑡𝑡𝑒𝑠𝑡
predicted by the evolved learner can be formally expressed as follows
𝑓𝑡𝑒𝑠𝑡= {𝑒𝑚1(𝑑𝑡𝑒𝑠𝑡; 𝜃1
𝑒𝑚), ⋯, 𝑒𝑚𝑚(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑚
𝑡𝑡𝑒𝑠𝑡= 𝑙(𝑓𝑡𝑒𝑠𝑡; 𝜃𝑢
Note, in the expressions of this subsection, each 𝜃 denotes the parameters
corresponding to the implementation of respective expression.
3.2 Methodology of TEL
The paradigm of traditional ensemble learning (TEL) and the paradigm of UML
primarily differ in the model development. TEL improves UML by replacing the model
development of UML with generating base learners and forming ensemble learner.
Generating base learners is responsible for evolving multiple base learners that have
diversity in mapping the extracted representations into corresponding targets.
Forming ensemble learner is responsible for integrating the base leaners into an
ensemble leaner that can achieve better generalization.
With appropriate learning algorithms (𝐿𝐴) and learning strategies (𝐿𝑆), generating
base learners can be roughly divided into two categories: one is to use different types
of learning algorithms to generate 'heterogeneous' base learners; and the other is to
use the same learning algorithm to generate 'homogeneous' base learners. Forming
ensemble learner consists of an ensembling criteria and an integrating procedure. An
ensembling criteria is responsible for the construction and configuration of an
ensemble learner. With the ensembling criteria, an integrating procedure is
responsible for forming the final ensemble learner which is appropriate for mapping
from predictions of base learners into corresponding targets. The outline for the
methodology of TEL is shown as Figure. 4.
Formally, generating base learners can be expressed as
𝐿𝑏= 𝐵𝑎𝑠𝑒𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐹, 𝑇, 𝐿𝐴, 𝐿𝑆) = {𝑙1(∗; 𝜃𝑢
𝑙1), ⋯, 𝑙𝑏(∗; 𝜃𝑢
Thereinto, 𝑙𝑏(∗; 𝜃𝑢
𝑙𝑏) signifies a base learner 𝑙𝑏(∗) parameterized by 𝜃𝑢
𝑙𝑏, which is
updated by the evolving procedure. More specifically, the details of generating
'heterogeneous' base learners can be expressed as follows
𝐿𝐴= {{𝑙1(∗; 𝜃𝑙1), 𝑜𝑝𝑡1(∗,∗; 𝜃𝑜𝑝𝑡1)}, ⋯, {𝑙𝑏(∗; 𝜃𝑙𝑏), 𝑜𝑝𝑡𝑏(∗,∗; 𝜃𝑜𝑝𝑡𝑏)} },
𝐿𝑆= {𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1), ⋯, 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)},
𝑙𝑏= 𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐹, 𝑇, 𝑙𝑏(∗; 𝜃𝑙𝑏), 𝑜𝑝𝑡𝑏(∗,∗; 𝜃𝑜𝑝𝑡𝑏) | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏))
(𝑙𝑏(𝐹; 𝜃𝑙𝑏), 𝑇; 𝜃𝑜𝑝𝑡𝑏 | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)) .
And, the details of generating 'homogeneous' base learners can be expressed as
𝐿𝐴= {{𝑙0(∗; 𝜃𝑙0), 𝑜𝑝𝑡0(∗,∗; 𝜃𝑜𝑝𝑡0)}},
𝐿𝑆= {𝑙𝑠ℎ𝑜𝑚𝑜,1(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,1), ⋯, 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)},
𝑙𝑏(∗) = 𝑙0(∗),
𝑙𝑏= 𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐹, 𝑇, 𝑙0(∗; 𝜃𝑙0), 𝑜𝑝𝑡0(∗,∗; 𝜃𝑜𝑝𝑡0) | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏))
(𝑙0(𝐹; 𝜃𝑙0), 𝑇; 𝜃𝑜𝑝𝑡0 | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)).
Figure. 4. The methodology of traditional ensemble learning (TEL), which consists of feature
extraction, generating base learners and forming ensemble learner. In generating base learners,
the learning strategy for evolving heterogeneous base learners is usually simpler than the learning
strategy for evolving homogeneous base learners. In forming ensemble learner, building
appropriate ensembling criteria is the key point to form the ensemble learner.
Let 𝐸𝐶= {𝑙𝑒(∗; 𝜃𝑙𝑒), 𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑐𝑜𝑛𝑓)} denote the ensembling criteria for
forming ensemble learner, and 𝑇̃ = {𝑡1̃, ⋯, 𝑡𝑛̃ }, 𝑡𝑛̃ = {𝑙1(𝑓𝑛; 𝜃𝑢
𝑙1), ⋯, 𝑙𝑏(𝑓𝑛; 𝜃𝑢
signify the predictions of the base learners. Specifically, 𝑙𝑒(∗; 𝜃𝑙𝑒) signifies the
construction of an ensemble learner 𝑙𝑒(∗) parameterized by 𝜃𝑙𝑒 and 𝑐𝑜𝑛𝑓(∗,∗
algorithms
ensemble learner
feature extraction
generating base learners
forming ensemble learner
base learners
strategies
training data
converting
heterogeneously
homogeneously
integrating
ensembling
extracting
; 𝜃𝑐𝑜𝑛𝑓) signifies the ensemble learner’s configuration 𝑐𝑜𝑛𝑓(∗,∗) parameterized by
𝜃𝑐𝑜𝑛𝑓 for updates of 𝜃𝑙𝑒. Forming ensemble learning can be formally expressed as
𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛(𝑇̃, 𝑇, 𝐸𝐶) = {𝑙𝑒(∗; 𝜃𝑢
Thereinto, 𝑙𝑒(∗; 𝜃𝑢
𝑙𝑒) signifies an ensemble learner 𝑙𝑒(∗) parameterized by 𝜃𝑢
which is configured by the integrating procedure. More specifically, the integrating
procedure can be expressed as
𝑙𝑒= 𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑡𝑖𝑛𝑔(𝑇̃, 𝑇, 𝑙𝑒(∗; 𝜃𝑙𝑒), 𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑐𝑜𝑛𝑓))
(𝑙𝑒(𝑇̃; 𝜃𝑙𝑒), 𝑇; 𝜃𝑐𝑜𝑛𝑓).
At testing, given a test raw data point 𝑑𝑡𝑒𝑠𝑡 , the corresponding target 𝑡𝑡𝑒𝑠𝑡
predicted by the formed ensemble learner can be expressed formally as follows
𝑓𝑡𝑒𝑠𝑡= {𝑒𝑚1(𝑑𝑡𝑒𝑠𝑡; 𝜃1
𝑒𝑚), ⋯, 𝑒𝑚𝑚(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑚
𝑡̃𝑡𝑒𝑠𝑡= {𝑙1(𝑓𝑡𝑒𝑠𝑡; 𝜃𝑢
𝑙1), ⋯, 𝑙𝑏 since it is beyond the topic of the methodology
of TEL. Particularly in discussion of generating base learners, we primarily elaborate
on various learning strategies, as discussion of machine learning algorithms is a topic
more appropriate in UML. The discussed well-known learning strategies and
ensembling criteria for generating base learners and integrating base learners are
summarized in Table. 1 and Table. 2.
Table. 1. Various well-known learning strategies (LS) for generating base learners
Specific LS
Manipulating at data level
Sampling data points
Bagging 
Dagging 
Weighting data points
Boosting 
Wagging 
Multiboosting 
Manipulating at feature level
Random Subspace 
Manipulating at both data and feature levels
Random Forests 
Rotation Forests 
Negative correlation learning 
ANN-based random initialization 
Table. 2. Various well-known ensembling criteria (EC) for forming ensemble learner
Specific EC
Weighting methods
Majority voting / averaged summarization 
Meta-learning methods
Stacking 
Mixture of expert 
Weighting-based learning 
Ensemble selection 
methods 
Ranking-based criteria 
Selection based criteria . While generating multiple base
learners, the learning algorithm and the learning strategy have to be considered. The
learning algorithm used to generate the base learners can be a decision tree (DT)
 , artificial neural network (ANN) , support vector
machine (SVM) or related variants. The used learning algorithm
basically determines the upper limit of the predictive performance of the base learners.
Generally, the predictive performance of the base learners is required to be at least
better than random prediction, because, theoretically, a number of random
predictions cannot be integrated to produce a well-regular prediction. The learning
strategy that ensures the diversity of the base learners plays a decisive role in
constructing the ensemble learner, especially when the accuracy of the base learners
reaches certain limit. This is because, theoretically, a better ensemble learner cannot
be summarized from a number of identical base learners. Usually, the learning strategy
for generating base learners can be heterogeneous or homogeneous. When the
learning strategy employs different learning algorithms to generate ‘heterogeneous’
base learners, the diversity of the base learners can be well ensured with very simple
learning strategy like independently learning, because good diversity can exist among
the base learners via the discrepancies of their diverse structures in addition to
different parameters. However, when the learning strategy uses the same learning
algorithm to generate ‘homogeneous’ base learners, the learning strategy to ensure
the diversity of the base learners is the key point to success, because the diversity of
the base learners is only kept in discrepancies of their parameters as they enjoy the
same structure. Theoretically, both heterogeneous learning strategy and
homogeneous learning strategy can be used at any time to generate base learners.
However, the homogeneous learning strategy is the most commonly used, due to the
fact that it only requires one learning algorithm to be implemented in practice. Thus,
in the following contents of this subsection, we discuss various learning strategies for
the diversity of ‘homogeneous’ base learners, respectively in the context of
manipulating at data level, manipulating at feature level, manipulating at both data
and feature levels, and others, which are summarized in Table 1.
(1) Manipulating at data level
Breiman et al. and Freund et al. ,
ensured the diversity of the base learners by sampling or weighting the training set
multiple times to generate new training sets for training different base learners. The
two learning strategies are known as Bagging and Boosting . Bagging uses the simple random sampling to generate new data
sets for the training of base learners. A known variant of Bagging is Dagging , which generates disjoint data sets by random sampling without
replacement. Boosting starts with equal weights assigned to the samples of the
training data and reweights the samples of the training data referring to the
performance of previous base learner to focus on difficult data points. A known variant
of Bagging is Wagging , which assigns random weights to the
samples in the training data using Gaussian distribution or Poisson distribution.
Combining Bagging and Wagging, Webb proposed Multiboosting, which
firstly uses Boosting to assign weights to the samples in the training data, and then
applies Wagging to update the weights of the assigned samples.
(2) Manipulating at feature level
Different from manipulating at the training data level , Ho upgraded the manipulation at data level to feature level (aka, Random Subspace)
by sampling the features extracted from the training data and then using the sampled
features as the input of the learning algorithm to generate various base learners.
(3) Manipulating at both data and feature levels
The two learning strategies of manipulating at data or feature level significantly
promoted the development of ensemble learning. By combining the advantages of
them, Breiman et al. constructed a learning strategy by sampling the
training data as well as the features extracted from the training data at the same time,
and proposed the famous random forests (RF) ensemble learning approach, which is
still widely used in various application fields. Later, Rodriguez et al. proposed rotation forests (RoF), the learning strategy of which first randomly
splits the training features into several disjoint subsets and then principle component
analysis (PCA) is applied to each subset and axis rotation is employed
to form new features for the training of base learners.
(4) Others
Another known learning strategy that can effectively ensure the diversity of base
learners is to encourage negative correlation in the predictive error of base learners
by complementary learning of the training data . Besides, when
artificial neural network (ANN) is employed as the learning
algorithm, a commonly used learning strategy to obtain diverse base learners is
training the ANN with different random initializations, which tends to differentiate the
errors of the base learners .
3.3.2 Ensemble learner formation
To form the ensemble learner from the based learners, an appropriate ensembling
criterion is the key to success. Basically, known ensembling criteria for the ensemble
learner formation can be roughly divided into three categories: weighting methods,
meta-learning methods, and ensemble selection methods. In the following contents
of this subsection, we respectively discuss these three categories, which are
summarized in Table 2, and summarize corresponding usage.
(1) Weighting methods
The essence of the weighting methods for ensembling criteria is to combine the
outputs of the base learners by assigning different weights to obtain better predictions.
The majority voting rule commonly used for classification problem is the simplest
implementation of the weighting method. The class with the most votes from the base
learners will be taken as the final output of the ensemble learner. As for regression
problem, the outputs of the base learners can be averaged to produce the final output of the ensemble learner. Another commonly used
weighting method is to assign the weight of a base learner in the ensemble learner
according to its predictive ability on a validation data set. In addition, Mao et al. used the error between the weighted output of multiple base learners and
the true value as the objective function, where the weights of the base learners are
subject to ∑𝜔𝑖= 1 and −1 < 𝜔𝑖< 1 , and minimized the objective function to
determine the weights of the base learners for the ensemble learner. Weighting
methods are simple to use, but they may not be able to fully exploit the information
of the base learners, especially when large amount of data is available.
(2) Meta-learning methods
Unlike the weighting methods which simply assign different weights to the base
learners, the fundamental idea of meta-learning methods for ensembling criteria is by
learning from the meta-knowledge that has been learnt by the base learners (i.e., the
predictions of the base learners) and corresponding targets to further reduce the
generalization error. Via learning to map the predictions of the base learners into
corresponding targets, meta-learning methods can further reduce the generalization
error when large amount of data is available, by fully exploiting the information of the
base learners to overcome the limitation of weighting methods. As the essence of
meta-learning methods follows the paradigm of machine learning, they are
appropriate to form ensemble learners for both classification and regression problems.
In 1992, Wolpert proposed the first meta-learning method Stacking to
integrate multiple base learners. A popular variant of the Stacking method is the
mixture of experts (ME) , which first uses a divideand-conquer strategy to split the problem into multiple sub-problems, and
respectively trains an expert (base learner) for each sub-problem, and then use
another machine learning algorithm to learn from the outputs of the base learners
and corresponding ground-truths to further reduce the generalization error.
Combining the advantages of both the weighting method and the meta-learning
method, Omari and FIgueiras-Vidal first weighted the
outputs of multiple base learners to obtain an initial ensemble, and then used another
machine learning algorithm to learn from the initial ensemble to further reduce the
generalization error. Meta-learning methods can fully exploit the information from the
base learners, but it will take much more expenses to evolve the ensemble criteria,
especially when the number of base learners is relatively large.
(3) Ensemble selection methods
In the early research stage of ensemble learning, most ensembling criteria used
all the generated base learners to form an ensemble learner. Although this simple
selection strategy can obtain an ensemble learner that is significantly better than a
single base learner with appropriate ensembling criterion, the prediction time and
storage space of the ensemble learner increased rapidly with the increase in the
number of base learners. To alleviate this problem, in 2002, Zhou et al proposed the concept of ensemble selection for the first time, affirming that
using a small number of base learners can as well achieve better ensemble
performance. This caused a strong focus in the community of ensemble learning and
opened a new research direction of selective ensemble learning . The strategies for selecting the base learner can be roughly divided into:
clustering-based methods, ranking-based methods, and selection-based methods. The
clustering-based method needs to consider how to measure the similarity between
the prediction results of two base learners (subsets), which clustering algorithm to
choose, and how to determine the number of subsets for the base learners . The ranking-based method first uses a certain measurement (such
as accuracy) to rank the base learners, and then uses a suitable stopping criterion to
select a number of base learners for ensemble . The
selection-based method leverages a certain selection strategy, such as greedy strategy
 or dynamic programming , to select only a proportion
of the base learners to participate in integrating . Ensemble selection methods can favorably reduce the prediction
time and storage space of the ensemble learner, but it may not achieve the optimal
solution by ignoring the information of some base learners.
(4) Summarization of usage
In summary, there are no stationary rules for deciding which ensembling criterion
is better than others. One has to construct the appropriate ensembling criterion by
referring to specific situations. Theoretically, weighting methods can be used without
any prerequisites. However, weighting methods may not be able to fully exploit the
information of the base learners when large amount of collected data is available. In
this situation, meta-learning methods are better than weighting methods. In addition,
when the efficiency of the ensemble learner is a key factor to be considered, ensemble
selection methods should also be united with weighting methods or meta-learning
methods for a more appropriate ensembling criterion.
3.4 Recent advances of TEL
3.4.1 Material collection
Apart from the well-known implementation of TEL, in recent years, a lot of TEL
advances have been proposed for various artificial intelligence applications . As the number of proposed TEL advances
can reach up to hundreds or thousands in a single year, it is quite difficult for a survey
to contain all of these advances. Thus, we propose to review a number of highly cited
recent TEL advances and summarize corresponding key innovations to show some
underlying trends of recent development of TEL. That highly cited recent advances are
selected is based on one of the assumptions behind the PageRank .
In the case of this paper, that is the more citations a paper gets the more people agree
with the approach presented in the paper. Thus, we consider summarizations from the
review of the selection of highly cited recent TEL advances can more appropriately and
stably reflect the primary underlying trends in recent development of TEL than the
collection of all the recent TEL advances. The selected works for review of recent TEL
advances were suggested by searching on Web of Science with the filtering rules including being published in the last ten years and being
highly cited or being a hotspot.
3.4.2 Review
Referring to the methodology of TEL presented in Figure. 4, but different from the
discussion in Section 3.1 that focused more on learning strategies for generating base
learners and ensembling criteria for forming ensemble learner, we add additional
descriptions of extracting methods for feature representations as well as depictions of
learning algorithms for generating base learners, since they also provide essential basis
for TEL approaches to be carried out in specific applications.
For the 3D human action recognition, Wang et al. proposed an
ensemble learning approach called actionlet ensemble. Actionlet was defined as a
conjunction of the features for a subset of the joints of the human skeleton. The 3D
joint position feature (3D-JPF) and local occupancy patter (LOP) were extracted from
the training images, and further transformed into temporal patterns for training via
the proposed Fourier temporal pyramid (FTP) . Multiple actionlet
classifiers based on an support vector machine (SVM) were
trained with a proposed data mining strategy which aims to discover discriminative
actionlet classifiers on the training dataset. A convex linear combination of the pretrained multiple actionlet classifiers were proposed to form the final actionlet
ensemble classifier.
For the prediction of protein-protein interactions, You et al. 
employed auto covariance (AC) , conjoint triad (CT) ,
local descriptor (LD) and Moran autocorrelation (MA) to extract features from protein sequences. The extracted features were used
to train multiple base learners based on a single hidden layer feed-forward neural
networks, extreme learning machine (ELM) , with
different random initializations. Majority voting was employed to integrate the trained
multiple base learners into the final ensemble learner.
For the identification of DNase I hypersensitive sites, Liu et al. 
proposed an ensemble learning algorithm entitled iDHS-EL. Multiple base learners
based on the random forests (RF) algorithm were trained using three
types of features including Kmer , reverse complement Kmer (RC-Kmer)
 and pseudo dinucleotide composition (PseDNC) 
that reflect the characteristics of a DNA sequence from different angles. The final
ensemble learner was formed by employing the grid search strategy to integrate the
pre-trained multiple base learners. For a similar task of identifying N6methyladenosine sites, Wei et al. proposed an ensemble learning
algorithm named M6APred-EL. Three feature representation algorithms, including
PS(kmer)NP, PCPs , and RFHC-GACs , were used to
extract diverse features from RNA sequences. The extracted features were respectively
employed to train SVM for multiple base predictors to identify
N6-methyladenosine sites. The obtained multiple base predictors were integrated by
majority voting to form the ensemble predictor.
For the landslide susceptibility mapping, Chen et al. proposed
several ensembles using multiple base learners that were heterogeneously trained
with artificial neural network (ANN) , maximum entropy
(MaxEnt) and SVM . Based on previous
studies, eleven landslide conditioning factors such as elevation, slope degree, aspect,
profile and plan curvatures, topographic wetness index (TWI), distance to roads,
distance to rivers, normalized difference vegetation index (NDVI), land use, and
lithology were selected for training. The proposed ensembles include ANN-SVM, ANN-
MaxEnt, ANN-MaxEnt-SVM, and SVM-MaxEnt which were integrated by testing
different basic mathematics (multiplication, division, addition and subtraction). For
the same task, Pham et al. assessed the application of the ensemble
learning technique in the landslide problem, constructing several ensemble learners
by combining the base classifier of multilayer perceptron (MLP) with
Bagging , Dagging , Boosting , Multiboosting and Rotation Forests . The factors used for training were selected based on topography, physicmechanical properties, locations and meteorology. The majority voting was employed
to form the ensemble learner. In addition, Dou et al. also constructed
three types of ensemble learners for this task, combining SVM 
respectively with Bagging , Boosting and
Stacking ensembling strategies. The factors used for training were
selected by referring to previous literatures, and the ensembling criteria employed to
form the ensemble learner include majority voting and meta-learning.
For the short-term load forecasting, Li et al. proposed an ensemble
learning algorithm based on wavelet transform , extreme
learning machine (ELM) and partial least squares
regression (PLSR) . Various wavelet transform specifications
were used to generated different types of features from input load series. Each type
of the generated features was individually leveraged to train multiple predictors based
on extreme learning machine (ELM) . PLSR, which
tackle the high degree of correlation between the individual forecast, was used to
weight the multiple predictors to establish an accurate ensemble forecast.
For the multi-class imbalance learning, Bi et al. proposed an
ensemble learning algorithm named diversified error correcting output codes (DECOC)
by introducing the error correcting output codes (ECOC) 
into the ensemble learning algorithm diversified one-against-one (DOVO) . ECOC is a decomposition strategy that builds a codeword for each class by
maximizing the distance (such as Hamming distance) between various classes and, by
selecting bits from the built codewords, divides the original problem into multiple subproblems which can be further individually solved via machine learning. In original
DOVO, one-vs-one (OVO) was employed to divide the original problem into multiple
sub-problems which are further solved with a variety of learning algorithms. OVO is a
decomposition strategy that build a sub-problem by only selecting instances for each
pair of classes from the original data. DECOC is formed by replacing OVO of EOVO with
ECOC, and the diverse learning algorithms that DECOC employs include SVM , k-nearest neighbor (KNN) , Logistic Regression (LR) , C4.5 , AdaBoost (AB) , Random
Forests (RF) , and multilayer perceptron (MLP) . The
obtained multiple based learners were weighted by minimizing the error in favor of
the minority classes to form the final ensemble learner. For experiments, 17 public
datasets were used, which had clearly defined features, the number of which ranges
from 3 to 128.
For the soil moisture forecasting, Prasad et al. proposed and
ensemble learning approach based on ensemble empirical mode decomposition
(EEMD) and complete ensemble empirical mode
decomposition with adaptive noise (CEEMDAN) . EEMD and
CEEMDAN are two improved variants of empirical mode decomposition (EMD) , which decomposes intact soil moisture time series into several intrinsic
mode functions (IMFs) to extract instantaneous frequency features. Each of the IMFs
and residual were separately processed by partial autocorrelation function (PACF)
 to help selecting features with statistically significant relationship. The
processed IMFs and residual were used to train extreme learning machine (ELM) and Random Forests (RF) to generate
multiple base learners. To form the ensemble learner, the generated multiple base
learners were integrated via averaged summation.
For the flood susceptibility mapping, Shahabi et al. proposed
an ensemble learning algorithm which employed four types of k-nearest neighbor
(KNN) classifiers as the base learning algorithm and utilized Bagging 
strategy to generate sub-datasets for training multiple base learners. Ten task-related
conditioning factors were chosen for training, including distance
to river, elevation, slope, lithology, curvature, rainfall, topographic wetness index (TWI),
stream power index (SPI), land use/land cover, and river density. The employed four
types of KNN classifiers include coarse KNN which defines the nearest
neighbor among all classes as the classifier, cosine KNN (CoKNN) which
uses the cosine distance metric as the nearest neighbor classifier, cubic KNN (CuKNN)
 which uses the cubic distance metric as the nearest neighbor classifier,
and weighted KNN which uses the weighted Euclidean distance as the nearest
neighbor classifier. The obtained multiple base learners were integrated by majority
voting to form the final ensemble learner.
For the automatic detection of lung cancer from biomedical dataset, Shakeel et al.
 employed generalized neural network (GNN) as the learning algorithm and trained multiple base learners using the
ensemble learning process proposed in , which generated multiple
base learners with subsets equally divided from the input features collected by
examining lung cancer effects on people’s genetic changes and other impacts. The
obtained multiple base learners were integrated by majority voting to form the final
ensemble learner.
For the hyperspectral image classification, Su et al. proposed two
types of ensemble learners which employed the tangent space collaborative
representation classifier (TCRC) as the learning algorithm and
respectively utilized Bagging and Boosting 
to train multiple base learners. The features used for training were the narrow spectral
bands captured by hyperspectral images. TCRC is an improved collaborative
representation classifier (CRC) , the principle of which is that a testing
sample can be approximated by training samples. CRC classifies a testing sample by
assigning it to the class whose labeled training samples provide the smallest
representation residual. TCRC improves CRC by using simplified tangent distance to
take advantage of the local manifold in the tangent space of the testing sample . The majority voting was employed to form the two types of ensemble
For the wind power forecasting, Wang et al. proposed a hybrid
approach named BMA-EL based on Bayesian model averaging (BMA) and ensemble learning (EL). For training, the task-related
features include wind speed, wind direction and ambient temperature. Artificial neural
network (ANN) , radial basis function neural network (RBFNN)
 , and SVM were employed as the learning algorithms. For
generating more diverse base learners, the training set were divided into three subsets
using clustering of self-organizing map and K-fold crossvalidation . ANN, RBFNN and SVM were respectively optimized on the
three generated subsets to produce three heterogeneous base learners. BMA was
utilized to integrate the three base learners and the parameters of the BMA model
were optimized on a validation dataset to form the final ensemble learner.
3.4.3 Summary
The information of the reviewed recent works of TEL is listed in Table. 3. The
feature extraction methods adopted by the TEL approaches proposed in the reviewed
works are listed in Table. 4. The learning algorithms and learning strategies employed
to generate base learners are respectively listed in Table. 5 and Table. 6. The
ensembling criteria utilized to integrate base learners are listed in Table. 7. Finally,
referring to Table. 3-7, the key innovations of the TEL approaches proposed in the
reviewed works are summarized in Table. 8.
Table. 3. The information of the applications in the reviewed recent TEL works (TA)
App. Description
Citations Published Year
3D human action recognition 
Prediction of protein-protein interactions 
Identification of hypersensitive sites
 
 
Landslide susceptibility mapping
 
 
 
Short-term load forecasting 
Multi-class imbalance learning 
Soil moisture forecasting 
Flood susceptibility mapping 
Detection of lung cancer 
Hyperspectral image classification 
Wind power forecasting 
Table. 4. The methods for feature extraction (FE) of the TEL (TFE) approaches proposed in the
reviewed works
Description
Temporal patterns transformed by Fourier temporal pyramid (FTP) 
Auto covariance 
Conjoint triad 
Local descriptor 
Moran autocorrelation 
Kmer 
Reverse complement Kmer 
Pseudo dinucleotide composition 
PS(kmer)NP 
PCPs 
RFHC-GACs 
Landslide related factors 
Factors selected based on various properties of landslide 
Factors selected by referring to previous literatures 
Wavelet decompositions 
Factors predefined on public datasets 
EEMD / CEEMDAN followed with PACF 
Flood related factors 
Factors of cancer effects 
Narrow spectral bands of hyperspectral images 
Wind power related factors 
Table. 5. The learning algorithms (LA) of the TEL (TLA) approaches proposed in the reviewed works
Description
Support vector machine 
Extreme learning machine 
Random forest 
Artificial neural network 
Maximum entropy 
Multilayer perceptron 
K nearest neighbor 
Logistic regression 
An improved iterative Dichotomiser 3 
AdaBoost 
KNN based on cosine distance 
KNN based on cubic distance 
KNN based on weighted Euclidean distance
Generalized neural network 
Tangent space collaborative representation classifier 
Radial basis function neural network 
Table. 6. The learning strategies (LS) of the TEL approaches proposed in the reviewed works
Code Description
Discovering discriminative learner on the training dataset 
Training with randomly initialized parameters on the same dataset
Manipulating at both the data and feature levels with random selections 
Training multiple learners on the same dataset
Manipulating at the feature level with division
Manipulating at the data level with divide-and-conquer strategy
Manipulating at the data level with Bagging strategy 
Manipulating at the data level with Dagging strategy 
Manipulating at the data level with Boosting strategy 
Manipulating at the data level with Multioosting strategy 
Manipulating at both the data and feature level with axis rotation 
Manipulating at the data level with self-organizing map and crossvalidation 
Table. 7. The ensembling criteria (EC) of the TEL (TEC) approaches proposed in the reviewed works
Description
Linear combination
Majority voting
Grid search
Basic mathematics
Stacking 
Partial least squares regression 
Minimizing the error in favor of the minority classes
Averaged summation
Bayesian model averaging 
Table. 8. The key innovations of the TEL approaches proposed in the reviewed works
Homo. by LS1
W. by TEC1
Homo. by LS2
W. by TEC2
Homo. by LS3
W. by TEC3
Homo. by LS5
W. by TEC2
Hetero. by LS4
W. by TEC4
Homo. by LS7-10
W. by TEC2
Homo. by LS7,9
W. by TEC2 / M. by TEC5
Homo. by LS5
W. by TEC6
TLA1,3,7-10
Hetero. by LS6
W. by TEC7
Homo. by LS5
W. by TEC8
TLA7,11-13
Hetero. / Homo. by LS7
W. by TEC2
Homo. by LS5
W. by TEC2
Homo. by LS7,9
W. by TEC2
Hetero. by LS12
W. by TEC9
Based on these tables, we can summarize: The representations of data employed
by the proposed TEL approaches of the reviewed works for training mostly were
various hand-crafted features; The learning algorithms of these TEL approaches that
were frequently employed are classic SVM, KNN, and neural networks (ELM, ANN, MLP
and GNN); Manipulating at the data level and the feature level by resampling were the
frequently learning strategies used by these proposed TEL approaches; While the
majority of these proposed TEL approaches trained homogenous (Homo.) base
learners, three of them trained heterogenous (Hetero.) base learners; While the
majority of these TEL approaches employed various weighting (W.) methods as
ensembling criteria to form the final ensemble learner, only one of them tried metalearning (M.) methods; And ensemble selection methods were rarely used, probably
because the number of base learners was not massive enough to employ ensemble
selection. These summarizations reflect the primary underlying trends for the recent
development of TEL.
3.5 Unattainability
Based on the primary underlying trends of TEL summarized in the subsection 3.4.3,
in this subsection, we discuss the unattainability of TEL. While most of recent advances
of TEL focused on proposing solutions for specific applications based on combinatorial
innovations by leveraging existing learning strategies for base learner generation and
existing ensembling criteria for ensemble learner formation, few recent advances of
TEL proposed new learning strategies for base learner generation or new ensembling
criteria for ensemble learner formation. It seems that recent advances for the intrinsic
problems of TEL have reached to a bottleneck, although TEL is still prosperously
developing in various applications. Besides, the primary issue associated with the
methodology of TEL comes from the nature of UML, which evolves a learner based on
hand-crafted features which are usually difficult to design and not expressive enough.
4 Usual Ensemble Deep Learning
With the popularization of big data, computing resources, and deep learning ), which improves
the paradigm of UML, has achieved unprecedented success in the field of machine
learning. This will give birth to a huge number of approaches for ensembles of deep
neural networks (ensemble deep learning, EDL) and promote the research of
ensemble learning into a new era. The usual way to evolve the methodology of EDL
(usual ensemble deep learning, UEDL) is directly applying DL to the methodology of
TEL. In this section, starting with the paradigm of deep learning (DL), we respectively
discuss the methodology, recent works and unattainability of UEDL.
4.1 Preliminary
Different from the paradigm of UML (Fig. 3), the paradigm of DL embeds the
feature extraction into model development to form an end-to-end framework, which
is able to learn task-specifically oriented features that are more expressive when
massive training data is available. The paradigm of DL is shown as Fig. 5.
Figure. 5. The methodology of deep learning (DL), which embeds the feature extraction into deep
model development to form an end-to-end framework.
Formally, let 𝐷𝐿𝐴= {𝑑𝑙(∗; 𝜃𝑑𝑙), 𝑑𝑜𝑝𝑡(∗,∗; 𝜃𝑑𝑜𝑝𝑡)} denote the deep learning
algorithm and 𝐿𝑆= {𝑙𝑠(𝜃𝑙𝑠)} denote the learning strategy. Specifically, 𝑑𝑙(∗; 𝜃𝑑𝑙)
signifies the construction of a deep learner 𝑑𝑙(∗) parameterized by 𝜃𝑑𝑙 and
𝑑𝑜𝑝𝑡(∗,∗; 𝜃𝑑𝑜𝑝𝑡) signifies the deep learner’s optimization procedure 𝑑𝑜𝑝𝑡(∗,∗)
parameterized by 𝜃𝑑𝑜𝑝𝑡 for updates of 𝜃𝑑𝑙 . Note, here 𝑑𝑜𝑝𝑡(∗,∗) implicitly
consists of an objective function constructed for classification or regression and
corresponding optimization of the objective function. The deep model development
component can be expressed as
𝐷𝐿= 𝐷𝑒𝑒𝑝𝑀𝑜𝑑𝑒𝑙𝐷𝑒𝑣𝑒𝑙𝑜𝑝𝑚𝑒𝑛𝑡(𝐷, 𝑇, 𝐷𝐿𝐴, 𝐿𝑆) = {𝑑𝑙(∗; 𝜃𝑢
Thereinto, 𝑑𝑙(∗; 𝜃𝑢
𝑑𝑙) signifies the deep learner 𝑑𝑙(∗) parameterized by 𝜃𝑢
is updated by the deeply evolving procedure that can be more specifically expressed
𝑑𝑙= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙(∗; 𝜃𝑑𝑙), 𝑑𝑜𝑝𝑡(∗,∗; 𝜃𝑑𝑜𝑝𝑡) | 𝑙𝑠(𝜃𝑙𝑠))
𝜃𝑑𝑙(𝑙(𝐹; 𝜃𝑑𝑙), 𝑇; 𝜃𝑑𝑜𝑝𝑡 | 𝑙𝑠(𝜃𝑙𝑠)).
The developed deep learner 𝑑𝑙(∗; 𝜃𝑢
𝑑𝑙) forms the mapping between data 𝐷 and
deeply evolving
deep learning algorithm
deep model development
deep learner
learning strategy
training data
corresponding targets 𝑇. Note the data 𝐷 here can be raw data or features extracted
from raw data.
At testing, given a test data point 𝑑𝑡𝑒𝑠𝑡, the corresponding target 𝑡𝑡𝑒𝑠𝑡 predicted
by the evolved deep learner can be expressed formally as follows
𝑡𝑡𝑒𝑠𝑡= 𝑑𝑙(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢
Note, in the expressions of this subsection, each 𝜃 denotes the parameters
corresponding to the implementation of respective expression.
4.2 Methodology of UEDL
Introducing DL to TEL, three basic patterns have emerged to evolve the
methodology of UEDL. As shown in Fig. 6, the three basic patterns include: A) feature
extraction based on DL; B) generating base learners based on DL; and C) forming
ensemble learner based on DL.
Let 𝐷𝐸𝑀= {𝑑𝑒𝑚1(∗; 𝜃1
𝑑𝑒𝑚), ⋯, 𝑑𝑒𝑚𝑚(∗; 𝜃𝑚
𝑑𝑒𝑚)} denote various DL-based
extracting methods. Particularly, 𝑑𝑒𝑚𝑚(∗; 𝜃𝑚
𝑑𝑒𝑚) signifies a DL-based extracting
𝑑𝑒𝑚𝑚(∗) parameterized by
𝑑𝑒𝑚 . Specifically,
𝑑𝑒𝑚= {𝑑𝑙𝑚(∗
𝑒𝑚} , which signifies that 𝑑𝑒𝑚𝑚(∗) consists of a developed deep learner
𝑑𝑙𝑚(∗) parameterized by 𝜃𝑢
𝑑𝑙𝑚 and a hyperparameter 𝜃𝑚
𝑒𝑚 indicating what
features to extract from 𝑑𝑙𝑚(∗; 𝜃𝑢
𝑑𝑙𝑚). The pattern A can be formally expressed as
𝐹= 𝐷𝑒𝑒𝑝𝐹𝑒𝑎𝑡𝑟𝑢𝑒𝐸𝑥𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛(𝐷, 𝐷𝐸𝑀 ) = {𝑓1, ⋯, 𝑓𝑛}.
Thereinto, 𝑓𝑛 , which signifies the deep representation of 𝑑𝑛 , is constructed by a
deeply converting procedure that can be more specifically expressed as
𝑓𝑛= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑐𝑜𝑛𝑣𝑒𝑟𝑡𝑖𝑛𝑔(𝑑𝑛, 𝐷𝐸𝑀)
= {𝑑𝑒𝑚1(𝑑𝑛; 𝜃1
𝑑𝑒𝑚), ⋯, 𝑑𝑒𝑚𝑚(𝑑𝑛; 𝜃𝑚
= {𝑑𝑥𝑛,1, ⋯, 𝑑𝑥𝑛,𝑘} 𝑠. 𝑡. 𝑘≥𝑚.
Figure. 6. Three basic patterns to evolve the methodology of usual ensemble deep learning (UEDL)
by applying DL to TEL. Pattern A: feature extraction based on DL. Pattern B: generating base
learners based on DL. Pattern C: forming ensemble learner based on DL.
With appropriate deep learning algorithms (𝐷𝐿𝐴) and learning strategies (𝐿𝑆), the
pattern B can be formally expressed as
feature extraction
generating base learners
(hetero/homogeneously)
forming ensemble learner
feature extraction
with deep learning
generating base learners
with deep learning
(hetero/homogeneously)
forming ensemble learner
with deep learning
deep learning
:pattern A
:pattern B
:pattern C
𝐿𝑏= 𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐷, 𝑇, 𝐷𝐿𝐴, 𝐿𝑆)
= {𝑑𝑙1(∗; 𝜃𝑢
𝑑𝑙1), ⋯, 𝑑𝑙𝑏(∗; 𝜃𝑢
Thereinto, 𝑑𝑙𝑏(∗; 𝜃𝑢
𝑑𝑙𝑏) signifies a base deep learner 𝑑𝑙𝑏(∗) parameterized by 𝜃𝑢
which is updated by the deeply evolving procedure. Identically, the data 𝐷 here can
be raw data or features extracted from raw data. More specifically, the details of
generating 'heterogeneous' base deep learners can be expressed as follows
𝐷𝐿𝐴= {{𝑑𝑙1(∗; 𝜃𝑑𝑙1), 𝑑𝑜𝑝𝑡1(∗,∗; 𝜃𝑑𝑜𝑝𝑡1)}, ⋯, {𝑑𝑙𝑏(∗; 𝜃𝑑𝑙𝑏), 𝑑𝑜𝑝𝑡𝑏(∗,∗; 𝜃𝑑𝑜𝑝𝑡𝑏)} },
𝐿𝑆= {𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1), ⋯, 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)},
𝑑𝑙𝑏= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙𝑏(∗; 𝜃𝑑𝑙𝑏), 𝑑𝑜𝑝𝑡𝑏(∗,
∗; 𝜃𝑑𝑜𝑝𝑡𝑏) | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑑𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏))
(𝑑𝑙𝑏(𝐷; 𝜃𝑑𝑙𝑏), 𝑇; 𝜃𝑑𝑜𝑝𝑡𝑏 | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)).
And, the details of generating 'homogeneous' deep base learners can be expressed as
𝐷𝐿𝐴= {{𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0)}},
𝐿𝑆= {𝑙𝑠ℎ𝑜𝑚𝑜,1(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,1), ⋯, 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)},
𝑑𝑙𝑏(∗) = 𝑑𝑙0(∗)
𝑑𝑙𝑏= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,
∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏))
(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)).
Let 𝐷𝐸𝐶= {𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒), 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓)} denote the ensemble criteria for
ensemble deep learner formation, and 𝑇̃ = {𝑡1̃, ⋯, 𝑡𝑛̃ } signify the predictions of the
base learners. Specifically, 𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒) signifies the construction of a DL-based
ensemble learner 𝑑𝑙𝑒(∗) parameterized by 𝜃𝑑𝑙𝑒 and 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓) signifies
the DL-based ensemble learner’s configuration 𝑑𝑐𝑜𝑛𝑓(∗,∗) parameterized by
𝜃𝑑𝑐𝑜𝑛𝑓 for updates of 𝜃𝑑𝑙𝑒. The pattern C can be formally expressed as
𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛(𝑇̃, 𝑇, 𝐷𝐸𝐶) = {𝑑𝑙𝑒(∗; 𝜃𝑢
Thereinto, 𝑑𝑙𝑒(∗; 𝜃𝑢
𝑑𝑙𝑒) signifies an ensemble deep learner 𝑑𝑙𝑒(∗) parameterized
𝑑𝑙𝑒, which is configured by the deeply integrating procedure. More specifically,
the deeply integrating procedure can be expressed as
𝑑𝑙𝑒= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑡𝑖𝑛𝑔(𝑇̃, 𝑇, 𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒), 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓))
= 𝑎𝑟𝑔𝑑𝑐𝑜𝑛𝑓
(𝑑𝑙𝑒(𝑇̃; 𝜃𝑑𝑙𝑒), 𝑇; 𝜃𝑑𝑐𝑜𝑛𝑓).
At testing, given a test data point 𝑑𝑡𝑒𝑠𝑡 , the corresponding procedures for the
three individual patterns can be expressed formally as follows
𝑓𝑡𝑒𝑠𝑡= {𝑑𝑒𝑚1(𝑑𝑡𝑒𝑠𝑡; 𝜃1
𝑑𝑒𝑚), ⋯, 𝑑𝑒𝑚𝑚(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑚
𝑡̃𝑡𝑒𝑠𝑡= {𝑑𝑙1(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢
𝑑𝑙1), ⋯, 𝑑𝑙𝑏 have been proposed for UEDL, we follow the strategy presented for the
material collection of recent advances of TEL in section 3.4.1. Thus, we review a
number of highly cited recent works that are related to the topic of UEDL and
summarize corresponding key innovations to more appropriately and stably reflect the
primary underlying trends in recent development of UEDL. Identically, the reviewed
works were suggested by searching on Web of Science 
with filtering rules including being published in last ten years, and being highly cited
or being a hotspot.
4.3.2 Review
Referring to the methodology of TEL presented in Figure. 6, in addition to learning
strategies for generating base learners and ensembling criteria for forming ensemble
learner, we also add additional descriptions of extracting methods for feature
representations as well as depictions of learning algorithms for generating base
learners whenever they are necessary to provide essential basis for UEDL approaches
to be carried out in specific applications.
For the probabilistic wind power forecasting, Wang et al. 
proposed an ensemble deep learning approach based on different deep convolutional
neural network (DCNN) architectures. Multiple base deep learners
are independently trained using features extracted from raw wind power data by
Mallat wavelet decomposition and converted into 2D image. The
employed DCNN architectures were different in terms of the number of hidden layers,
the number of maps in each layer and the size of the input. For the training of each
base deep learner, separated wavelet features were used. To form the ensemble deep
learner, the predictions of the multiple base deep learners were fused by wavelet
reconstruction .
For the electricity load demand forecasting, Qiu et al. proposed a
deep ensemble learning approach based on empirical mode decomposition (EMD)
 . First, via EMD, load demand time series were decomposed into
several intrinsic mode functions (IMFs) and one residual which can be regarded as
instantaneous frequency features extracted from the non-stationary and nonlinear
raw data. Each of the IMFs and residual was separately used to train a deep belief
network (DBN) which was composed of restricted Boltzmann
machines (RBMs) and multilayer
perceptron (MLP) , resulting in a series of base deep
learners. The base deep learners were integrated via averaged summation or weighted
linear combination to form the ensemble learner.
For the sentiment analysis in social applications, Araque et al. 
trained multiple base deep learners based on word2vec / doc2vec
 combined with logistic regression (word2vec/doc2vec-LR).
Word2vec was used for representations of short texts and doc2vec was used for
representations of long texts. The obtained base deep learners were integrated to
form the final ensemble deep learner through weighting by majority voting or metalearning by random forests .
For the melanoma recognition in dermoscopic images, Codella et al. employed convolutional architecture for fast feature embedding (Caffe) and deep residual network (ResNet) pretrained on ImageNet
 and convolutional network of U-shape (U-Net) pretrained on lesion segmentation task for feature
representations, in addition to several types of traditional feature representations
including color histogram (CH) , edge histogram (EH) , multi-scale variant of color local binary patters (MSLBP) and a
type of sparse coding (SC) . The deep CNN based feature representations
and the traditional feature representations were respectively employed to train SVM
 for multiple base learners. The predictions of the obtained base
learners were averaged to form the final ensemble deep learner.
For the wind speed forecasting, Chen et al. utilized long shortterm memory (LSTM) to
exploit the implicit information of collected wind speed time series. A cluster of LSTM
with diverse number of hidden layers and neurons in each hidden layer were built to
train multiple base deep learners on the collected raw data. A meta-learning strategy
was proposed to form the final ensemble deep learner. The meta-learning strategy was
composed of a support vector regression machine (SVRM) that took the forecasting results of the trained base deep learners as inputs and
was optimized by extremal optimization (EO) .
For the crude oil price forecasting, Li et al. formulated an
ensemble deep learning approach named stacked denoising autoencoder bagging
(SDAE-B). Employing the bagging strategy , multiple base deep learners
were trained based on SDAE . Referring to , 198 factors, including price, flow, stock macroeconomic and financial
series, were selected as exogenous variables for training. The predictions of the
obtained base learners were averaged to form the final ensemble deep learner.
For the cancer prediction based on gene expression, Xiao et al. 
proposed to integrate multiple base learners using deep learning. The DEseq was employed to select informative genes from a given sequence for
training. K nearest neighbor (KNN) , support vector machine (SVM)
 , decision tree (DT) , random forests (RF) , and gradient boosting decision tree (GBDT) were employed to
train five base learners with model selection using K-fold cross-validation . To form the ensemble learner, a five-layer neural network (5-NN) was built as
an implementation of the meta-learning strategy, which took the predictions of the
five obtained base learners as inputs and output normal or tumor.
For the prediction of melt index (MI) in industrial polymerization processes, Liu et
al. proposed an ensemble deep kernel learning (EDKL) approach. A
total of 11 factors correlated with MI were chosen for training, including the reactor
pressure, reactor temperature, liquid level, and flow rate of the main catalysts. The
deep belief network (DBN) comprising a series of individual
restricted Boltzmann machines (RBM) 
was adopted to extracted features from the chosen factors in a unsupervised manner.
With the extracted features, multiple base learners were trained based on a kernel
learning soft-sensing model (KLSM) by bagging . The
predictions of the obtained base learners were averaged to form the final ensemble
deep learner.
For the brain disease diagnosis based on MRI image, Suk et al. as
well proposed to integrate multiple base learners using deep learning. A series of
techniques (details can be found in the preprocessing section) were employed to
extract 93-dimensional volumetric features from an MRI image. Based on the
extracted features, sparse regression model (SRM) was utilized to train multiple base learners with different values of the
regularization control parameter. To form the ensemble learner, a six-layer
convolutional neural network (6-CNN) was built as an implementation of the metalearning strategy, which took the predictions of the obtained base learners as inputs
and output disease diagnosis.
For the red lesion detection in fundus images, Orlando et al. 
formed an ensemble learning approach enhanced by deep learning. An unsupervised
candidate lesion detection approach was proposed based on morphological
operations to identify potential lesion areas. A five-layer convolutional neural network
(5-CNN) was built to learn features from patches around the detected potential lesion
areas. A 63-dimensional feature vector was as well extracted from lesion candidates
using descriptors explored in related literature . Both convolutional and hand-crafted features are used by
random forests (RF) to form the ensemble learner.
For bearing fault diagnosis, Xu et al. prosed an ensemble learning
approach based on deep learning and random forests (RF) . Morlet, a
type of continuous wavelet transform, was employed to convert raw vibration signals
of bearing, which was one-dimensional in time-domain, into spectrum of abundant
condition information, which was two-dimensional in time-frequency domain . A built five-layer CNN model, which consists of two convolutional layers,
two pooling layers and one fully connected layer, was trained with the converted
spectrums for feature extraction. The features of the two pooling layers and the fully
connected layer were independently employed by RF to produce three
RF base learners. To form the ensemble learner, the outputs of the three base learners
were integrated by the winner-take-all strategy . Ma et al. also proposed an ensemble learning approach for the same task. Employing
a multi-objective evolutionary algorithm (MOEA/DD) , multiple base
deep learners were generated based on three deep learning architectures, including
DBN , ResNet and deep auto-encoder (DAE) . The three architectures all have five layers (including three hidden layers).
Regarding to the number of hidden neurons in each hidden layer and the learning rate
for back propogation, MOEA/DD was leveraged to evolve a population of base deep
learners with maximum accuracy and diversity simultaneously. Frequency spectrums
calculated from the in-situ monitoring signals were used as input features for the
training. To form the ensemble deep learner, the ensemble weights for the based
learners were optimized by differential evolution (DE) with the
objective of average training accuracy and selected by a designed selection strategy
with three constraints including: (1) prediction accuracy, (2) diversity and (3) training
For vehicle type classification, Liu et al., constructed an ensemble
deep learning approach based on three types of ResNet architectures,
including ResNet-50, ResNet-101, ResNet-152. With respective initializations
pretrained on ImageNet , the three architectures were
independently trained on the same training set which had been augmented with
balance sampling to obtain diverse base learners. The majority voting was employed
as the ensembling criterion to form the ensemble learner.
For vehicle type classification, Qummar et al., constructed
an ensemble deep learning approach based on five deep CNN architectures, including
ResNet-50 , Inception-v3 , Xception ,
Dense-121 and Dense-169 . With respective pretrained
initializations, the five architectures were independently trained on the same fundus
images of Kaggle ( to
generate five different base learners. To form the ensemble learner, Stacking was employed as the ensembling criterion.
For the prediction of neuromuscular disorders, Khamparia et al. also proposed to integrate multiple base learners using deep learning.
Bhattacharya coefficient was employed to select top gene features and the obtained
coefficient was given as input to generate gene features related to muscular disorder.
Based on the extracted features, KNN , DT , linear
discriminant analysis (LDA), quadratic discriminant analysis (QDA), RF ,
variants of SVM was utilized to train multiple base learners. To
form the ensemble learner, a five-layer convolutional neural network (5-CNN) was built
as an implementation of the meta-learning strategy, which took the predictions of the
obtained base learners as inputs and output disease diagnosis.
4.3.3 Summary
The information of the reviewed recent works of UEDL is listed in Table. 9. In
addition to Table. 4, the feature extraction methods adopted by the UDEL approaches
proposed in the reviewed works are listed in Table. 10. In addition to Table. 5 and Table.
6, the learning algorithms and learning strategies employed to generate base learners
are respectively listed in Table. 11 and Table. 12. In addition to Table. 7, the ensembling
criteria utilized to integrate base learners are listed in Table. 13. Referring to Table. 9-
13, the key innovations of the UEDL approaches proposed in the reviewed works are
summarized in Table. 14. Finally, referring to the paradigm of UEDL presented in Fig. 5,
the basic patterns of applying deep learning to TEL to evolve the UEDL approaches
proposed in the reviewed works are listed in Table. 15.
Table. 9. The information of the applications in the reviewed recent UEDL works (DA)
Code Description
Citations Published
Probabilistic wind power forecasting 
Electricity load demand forecasting 
Sentiment analysis in social applications 
Melanoma recognition in dermoscopic images 
Wind speed forecasting 
Crude oil price forecasting 
Cancer prediction based on gene expression 
Prediction of melt index in industrial polymerization processes 
Red lesion detection in fundus images 
Bearing fault diagnosis
 
 
Vehicle type classification 
Diabetic retinopathy detection 
Prediction of neuromuscular disorders 
Table. 10. In addition to Table. 4, the methods for feature extraction (FE) of the UEDL (TFE/DFE)
approaches proposed in the reviewed works
Description
Mallat wavelet decomposition 
Empirical mode decomposition (EMD) 
Color histogram 
Edge histogram 
Multi-scale variant of color local binary patters (MSLBP) 
Sparse coding (SC) 
Raw wind speed series 
Factors including price, flow, stock, macroeconomic and financial series 
DEseq 
Volumetric features extracted from MRI image 
Volumetric features extracted from fundus images 
Morlet wavelet transform 
Frequency spectrums calculated from the in-situ monitoring signals 
Raw fundus images
Raw vehicle images
gene features related to muscular disorder 
Word2vec 
Doc2vec 
Caffe pretrained on ImageNet 
ResNet pretrained on ImageNet 
U-Net pretrained on lesion segmentation task 
Unsupervised learning features from chosen factors via DBN 
Table. 11. In addition to Table. 5, the learning algorithms (LA) of the UEDL (TLA/DLA) approaches
proposed in the reviewed works
Description
Decision tree 
Gradient boosting decision tree 
kernel learning soft-sensing model 
Sparse regression model 
Linear discriminant analysis
Quadratic discriminant analysis
Deep convolutional neural network 
Deep belief network 
Long short-term memory 
Stacked denoising autoencoder 
Deep residual network 
Deep auto-encoder 
Inception-v3
Rethinking the Inception Architecture for Computer Vision 
Deep learning with depthwise separable convolutions 
Densely Connected Convolutional Networks 
Table. 12. In addition to Table. 6, the learning strategies (LS) of the UEDL approaches proposed in
the reviewed works
Description
Training with differently initialized values of regularization parameters 
Multi-objective evolutionary algorithm (MOEA/DD) 
Table. 13. In addition to Table. 7, the ensembling criteria (EC) of the UEDL (TEC/DEC) approaches
proposed in the reviewed works
Description
Wavelet reconstruction 
Random forests 
Support vector regression machine optimized by extremal optimization
 
Winner-take-all 
Differential evolution 
Ensemble selection with constraints 
Five-layer neural network 
Six-layer convolutional neural network 
Five-layer convolutional neural network 
Table. 14. The key innovations of the UEDL approaches proposed in the reviewed works
Hetero. by LS5
W. by TEC10
Homo. by LS5
W. by TEC1,8
Homo. by LS4
M. by TEC11
TFE24-27, DFE3-5
Homo. by LS5
W. by TEC8
Hetero. by LS4
M. by TEC12
Homo. by LS7
W. by TEC8
TLA1,3,7,17,18
Homo. by LS4
M. by DEC1
Homo. by LS7
W. by TEC8
Homo. by LS13
M. by DEC2
TFE32, DFE7
Homo. by LS3
W. by TEC2
TFE33, DFE7
Homo. by LS3
W. by TEC13
Hetero. by LS14
W. by TEC14 / S. by TEC15
Hetero. by LS4
W. by TEC2
Hetero. by LS4
W. by TEC2
TLA7,17,21,22
Hetero. by LS4
M. by DEC3
Table. 15. The basic patterns of applying deep learning to TEL to evolve the UEDL approaches
proposed in the reviewed works
Pattern App.6-10 Pattern App.11-14 Pattern
Based on these tables, we can summarize: Five of the reviewed works evolved
UEDL approaches by employing pattern A, which utilized pretrained or unsupervised
trained deep models to extract more expressive features from raw data; Eight of the
reviewed works evolved UEDL approaches by employing pattern B, which replaced the
traditional learning algorithms with deep learning algorithms to generate base deep
learners with hand-crafted features or raw data; Three of the reviewed works evolved
UEDL approaches by employing pattern C, which constructed meta-learning based
ensembling criteria to form deep ensemble learner by deep learning algorithms; And
one work of DA11 proposed an ensemble selection (S.) based ensembling criterion to
reduce the costs of ensemble deep learner at testing, as it was more expensive than
traditional ensemble learner. These summarizations reflect the primary underlying
trends for the recent development of UEDL.
4.4 Unattainability
Based on the primary underlying trends of UEDL summarized in subsection 4.3.3,
in this subsection, we discuss the unattainability of UEDL. Although a few learning
strategies and ensemble criteria that are more appropriate for the methodology of
UEDL have been proposed, the primary issue associated with the methodology of
UEDL is that it still retains the paradigm of TEL by simply introducing DL to TEL. This
nature of UEDL considerably increases the time and space demands of training
multiple base deep learners and testing the ensemble deep learner. To address this,
the concept of knowledge distillation has become popular in
many UEDL approaches. The key idea of knowledge distillation is using a student
learner, which is often simpler, to distil knowledge of multiple teacher learners
selected from a pool of pre-optimized teacher learners which are often more complex
 . Though knowledge distillation can reduce the
costs at the testing stage of ensemble deep learning, it still requires large extra
expenses during the process of training which prevents adequate usage of UEDL in
specific fields. Thus, new methodology for fast ensemble deep learning needs to be
further studied.
5. Fast Ensemble Deep Learning
Retaining the paradigm of TEL, UEDL ignores the intrinsic characteristics of DL. To
address this, taking the inherent characteristics of DL into consideration, fast ensemble
deep learning (FEDL) emerges, which more intrinsically reduce the time and space
overheads of EDL.
5.1 Existing FEDL approaches
To reduce the time overhead of EDL in training multiple base deep learners, Huang
et al. proposed an FEDL approach named Snapshot, arguing that
the local minima found on the optimizing path of stochastic gradient descent (SGD)
 can benefit ensembles of deep neural
networks. Snapshot utilizes the non-convex nature of deep neural networks and the
ability of SGD to converge and escape from local minima as needed. By allowing SGD
to converge to local minima multiple times along the optimizing path via cyclic learning
rates , instead of training multiple deep
learners independently from scratch, they effectively reduced the time cost of
obtaining multiple base deep learners. Garipov et al. found that
paths with lower loss values existing between the local minima in the loss plane of
deep neural networks, and proposed the fast geometric ensembling (FGE) algorithm
to find local minima along these paths via cyclic learning rates. As a result, multiple
base deep learners can be obtained with less time overhead. Although Snapshot and
FGE can effectively reduce the time of training multiple base deep learners, in order
to make full use of Snapshot and FGE to obtain better generalization performance, we
need to store the obtained multiple base deep learners and average their predictions
to form the final ensemble deep learner. This substantially increases the expenses for
the testing stage of the final ensemble deep learner. Observing that the local minima
found at the end of each learning rate cycle tend to accumulate at the boundary of the
low-value area on the loss plane of deep neural networks, the stochastic weight
averaging (SWA) algorithm was proposed to
average the local minima at the boundary of low-value area to form the ensemble
deep learner. It has been proved that SWA can produce ensemble deep learner with
better generalization. At the end of each learning rate cycle, SWA saves the weights of
the currently optimized deep neural network and averages them with the weights
saved at the end of the last learning rate cycle to form the current ensemble deep
learner. SWA can obtain multiple base deep learners under the time expense of
training one deep learner and the space expense of additional one deep learner, and
effectively reduces the time and space cost for the testing stage of the final ensemble
deep learner.
5.2 Optional methodology of FEDL
Although several FEDL approaches have been proposed, there still lacks a clear
definition for FEDL. Alleviating this situation, Yang et al. presented
an optional definition and inferred evaluations of FEDL based on observations of
existing FEDL approaches. To present a clear definition, the problem of FEDL were
divided into three procedures, including: A) training a base deep learner; B) starting
from the pre-trained base deep learner to find local minima in the entire parameter
space to obtain extra base deep learners; C) integrating the obtained multiple base
deep learners to form the ensemble deep learner. The methodology of this definition
for FEDL is shown in Fig. 7.
With a deep learning algorithm (𝐷𝐿𝐴= {𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0)}) and a
learning strategy (𝐿𝑆1 = {𝑙𝑠0(𝜃𝑙𝑠0)} ), the procedure A) of FEDL can be formally
expressed as
𝐷𝐿0 = 𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝑀𝑜𝑑𝑒𝑙𝐷𝑒𝑣𝑒𝑙𝑜𝑝𝑚𝑒𝑛𝑡(𝐷, 𝑇, 𝐷𝐿𝐴, 𝐿𝑆1) = {𝑑𝑙0(∗; 𝜃𝑢
𝑑𝑙0 = 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠0(𝜃𝑙𝑠0))
= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡0
(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠0(𝜃𝑙𝑠0)).
With the pre-trained 𝐷𝐿0 , 𝐷𝐿𝐴 and additional learning strategies ( 𝐿𝑆2 =
{𝑙𝑠1(𝜃𝑙𝑠1), ⋯, 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)}), the procedure B) of FEDL can be formally expressed as
𝐷𝐿𝑏= 𝐸𝑥𝑡𝑟𝑎𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐷, 𝑇, 𝐷𝐿0, 𝐷𝐿𝐴, 𝐿𝑆2) =
{𝑑𝑙1(∗; 𝜃𝑢
𝑑𝑙1), ⋯, 𝑑𝑙𝑏(∗; 𝜃𝑢
𝑑𝑙𝑏(∗) = 𝑑𝑙0(∗),
𝑑𝑙𝑏= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0|𝜃𝑢
𝑑𝑙𝑏−1), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏))
= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡0
(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0|𝜃𝑢
𝑑𝑙𝑏−1), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)).
Both ensemble criteria 𝐸𝐶= {𝑙𝑒(∗; 𝜃𝑙𝑒), 𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑐𝑜𝑛𝑓)} for TEL and 𝐷𝐸𝐶=
{𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒), 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓)} for UEDL can be leveraged for fast ensemble deep
learner formation. In the context of FEDL, these two criteria are also named as
ensemble in model space. Another ensemble criterion that is used by FEDL is ensemble
in parameter space, which fuses multiple base deep learners into a single deep learner.
Formally, let 𝐸𝐶𝑃𝑆= {𝑓𝑢𝑠𝑖𝑛𝑔(∗, 𝜃𝑓𝑢𝑠𝑖𝑛𝑔)} denote the ensemble criterion in
parameter space, the procedure C) of FEDL in parameter space can be formally
expressed as follows
𝐹𝐷𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛({𝐷𝐿0, 𝐷𝐿𝑏}, 𝐸𝐶𝑃𝑆)
= {𝑓𝑑𝑙𝑒(∗; 𝜃𝑢
𝑓𝑑𝑙𝑒(∗) = 𝑑𝑙0(∗),
𝑓𝑑𝑙𝑒= 𝑓𝑢𝑠𝑖𝑛𝑔({𝜃𝑢
𝑑𝑙1, ⋯, 𝜃𝑢
𝑑𝑙𝑏}; 𝜃𝑓𝑢𝑠𝑖𝑛𝑔).
Figure. 7. The methodology of fast ensemble deep learning (FEDL). Deep model development: pretraining a base deep learner. Generating base deep learners: starting from the pre-trained base
deep learner to find local minima in the entire parameter space to generate extra base deep
learners. Forming ensemble deep learner: integrating the obtained multiple base deep learners to
form the final ensemble deep learner.
At testing, given a test data point 𝑑𝑡𝑒𝑠𝑡, the inference procedures of the ensemble
deep learner integrated in model space include computing the predictions of all base
deep learners which can be expressed as
𝑡̃𝑡𝑒𝑠𝑡= {𝑑𝑙0(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢
𝑑𝑙0), 𝑑𝑙1(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢
𝑑𝑙1), ⋯, 𝑑𝑙𝑏(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢
and mapping 𝑡̃𝑡𝑒𝑠𝑡 to the final prediction 𝑡𝑡𝑒𝑠𝑡 by the learner integrated in model
space. Whereas, the inference procedure of the ensemble deep learner fused in
parameter space can be simply expressed as
𝑡𝑡𝑒𝑠𝑡= 𝑓𝑑𝑙𝑒(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢
ensembling criteria
in model space
in parameter space
starting from the pre-trained base
deep learner to find local minima in
the entire parameter space ( , )
deeply evolving
:deep model development
:generating base deep learners
:forming ensemble deep learner
:training data
:deep learning algorithm
:learning strategy
:base deep learners
:ensemble deep learner integrated in model or parameter space
5.3 Finding local minima in the sub-parameter space for FEDL
Existing FEDL approaches (like Snapshot, FGE and SWA) have promoted the
deployment of ensemble deep learning in some artificial intelligence applications to
some extent. However, due to the large expenses compared with TEL, FEDL still needs
further advances in some specific fields, where the developing time and computing
resources are usually restricted )
or the data to be processed is of large dimensionality ). In these specific fields, it is
sometimes still difficult to deploy the TEL approaches, which makes the deployment
of existing FEDL approaches still challenging.
To alleviate this, Yang et al argued that local minima
found in the sub-parameter space (LoMiFoSS) can be effective for FEDL, and proposed
the concept of LoMiFoSS-based fast ensemble deep learning (LoMiFoSS-FEDL).
Referring to the optional methodology presented for FEDL, the problem of LoMiFoSS-
FEDL were also divided into three procedures to present the methodology of
LoMiFoSS-FEDL, including: A) pre-training a base deep learner; B) starting from the
pre-trained base deep learner to find local minima in the sub-parameter space; C)
fusing the multiple base deep learners of the found local minima in the sub-parameter
space to form the final ensemble deep learner. Different from current state-of-the-art
FEDL approaches that optimize the entire parameters of a deep neural network architecture,
LoMiFoSS-FEDL only optimizes partial parameters to further reduce the costs required
for training multiple base deep learners. LoMiFoSS-FEDL provides an addition to
current FEDL approaches. An optional methodology of LoMiFoSS-FEDL is shown in Fig.
Formally, the procedure A) of LoMiFoSS-FEDL can be expressed exactly as the
procedure A) of FEDL. With the pre-trained 𝐷𝐿0 , 𝐷𝐿𝐴 , a sub-parameter space
𝑆𝑃𝑆= {𝜃𝑑𝑙0,𝑠|𝜃𝑑𝑙0,𝑠∈𝜃𝑑𝑙0; 𝜃𝑑𝑙0,𝑓∈𝜃𝑑𝑙0; (𝜃𝑑𝑙0,𝑠∪𝜃𝑑𝑙0,𝑓) == 𝜃𝑑𝑙0} , and additional
learning strategies (𝐿𝑆2 = {𝑙𝑠1(𝜃𝑙𝑠1), ⋯, 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)}), the procedure B) of LoMiFoSS-
FEDL can be formally expressed as
𝐷𝐿𝑏= 𝐸𝑥𝑡𝑟𝑎𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐷, 𝑇, 𝐷𝐿0, 𝐷𝐿𝐴, 𝑆𝑃𝑆, 𝐿𝑆2) =
{𝑑𝑙1(∗; 𝜃𝑢
𝑑𝑙1), ⋯, 𝑑𝑙𝑏(∗; 𝜃𝑢
𝑑𝑙𝑏(∗) = 𝑑𝑙0(∗),
𝑑𝑙𝑏,𝑠= 𝑝𝑎𝑟𝑡𝑖𝑎𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0,𝑠|𝜃𝑢
𝑑𝑙𝑏−1), 𝑑𝑜𝑝𝑡0(∗,
∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏))
= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡0
(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0,𝑠|𝜃𝑢
𝑑𝑙𝑏−1), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)).
With the ensemble criterion in parameter space 𝐸𝐶𝑃𝑆, the procedure C) of LoMiFoSS-
FEDL in sub-parameter space can be formally expressed as
𝐹𝐷𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛({𝐷𝐿0, 𝐷𝐿𝑏}, 𝐸𝐶𝑃𝑆)
= {𝑓𝑑𝑙𝑒(∗; 𝜃𝑢
𝑓𝑑𝑙𝑒(∗) = 𝑑𝑙0(∗),
𝑑𝑙𝑒,𝑠= 𝑓𝑢𝑠𝑖𝑛𝑔({𝜃𝑢
𝑑𝑙1,𝑠, ⋯, 𝜃𝑢
𝑑𝑙𝑏,𝑠} ; 𝜃𝑓𝑢𝑠𝑖𝑛𝑔).
Figure. 8. The methodology of LoMiFoSS-FDEL for fast ensemble deep learning. Deep model
development: pre-training a base deep learner. Generating base deep learners: starting from the
pre-trained base deep learner to find local minima in the sub-parameter space to generate extra
base deep learners. Forming ensemble deep learner: integrating the obtained multiple base deep
learners in the sub-parameter space to form the final ensemble deep learner.
At testing, the inference procedure of the ensemble deep learner for LoMiFoSS-
FDEL can be expressed the same as the inference procedure of the ensemble deep
learner fused in parameter space for FDEL.
5.4 Remaining issues
The primary remaining issue associated with FEDL is the imperfection for its
definition. From the existing methodologies (Fig. 7 and 8) presented for FEDL, we can
notice that the found extra base deep learners are tightly related to the pre-trained
base deep learner. However, we cannot be sure whether the found extra base deep
learners and the pre-trained base deep learner can be united to produce the optimal
ensemble solution. In addition, one may argue that finding multiple deep learners that
starting from the pre-trained base
deep learner to find local minima in
the subparameter space ( )
projection
ensembling criteria
in parameter space
:deep model development
:generating base deep learners
:forming ensemble deep learner
:training data
:deep learning algorithm
:learning strategy
:base deep learners
:ensemble deep learner
are less tightly related to each other can achieve better ensemble performances. Thus,
the two presented methodologies can only provide some optional perspectives for the
definition of FEDL, and new methodologies from different angles for ensemble deep
learning are still needed.
6 Discussion
This article is different from existing review articles that mostly discussed about traditional ensemble learning, reviews that particularly discussed ensemble deep learning in bioinformatics, or
specific technical innovations with ensemble learning introduced such as GAN-
Ensembles . With more
fundamental discussions about the developing routes of both traditional ensemble
learning and ensemble deep learning, in this article, we aim to reveal the intrinsic
problems and technical challenges of deploying ensemble learning under the era of
deep learning to a wider range of specific fields.
Data analyses of published works show that ensemble learning is still prosperously
developing while the research of ensemble deep learning (EDL) compared with the
research of traditional ensemble learning (TEL) is severely lagging behind. The primary
reason for this phenomenon lies in that the time and space overheads for the training
and testing stages of EDL are much larger than that of TEL.
Recent advances of TEL have achieved remarkable progresses in various
applications. However, most of these advances focused on proposing solutions for
specific applications, based on combinatorial innovations by leveraging existing
learning strategies for base learner generation and existing ensembling criteria for
ensemble learner formation. Few proposed new learning strategies for base learner
generation or new ensembling criteria for ensemble learner formation. This
phenomenon reflects that the advances for the intrinsic problems of TEL seem to have
reached to a bottleneck, although TEL is still prosperously developing in various
applications. Besides, the primary issue associated with the methodology of TEL comes
from the nature of usual machine learning (UML), which evolves a learner based on
hand-crafted features which are usually difficult to design and not expressive enough.
Directly introducing deep learning (DL) to enhance the methodology of TEL by
three basic patterns (introducing DL to feature extraction, introducing DL to base
learner generation or introducing DL to ensemble learner formation), recent advances
of usual EDL (UEDL) have promoted the solutions for various applications. However,
the primary issue of the methodology of UEDL is that its nature still retains the
paradigm of TEL, which considerably increases the costs of training multiple base deep
learners and testing the ensemble deep learner. This can also be confirmed by the fact
that knowledge distillation and ensemble selection based
ensembling criterion for ensemble deep learner formation
appeared in some recent advances of UEDL to reduce the testing overhead. A
promising future direction is to more intrinsically reduce the expenses of EDL by taking
into consideration the inherent characteristics of DL.
Fast ensemble deep learning (FEDL) emerges to more intrinsically reduce the costs
of EDL, considering some inherent characteristics of DL. Finding local minima along the
(global or local) optimization path of SGD for multiple base deep learners, existing
advances of FEDL have provided preferrable solutions to promote the deployment of
EDL in more artificial intelligence applications of specific fields. However, the primary
remaining issue of FEDL is the imperfection for its definition, since currently existing
alternative methodologies for FEDL can only provide uncomprehensive perspectives.
A promising future direction is to comprehensively propose more appropriate
definitions for FEDL.
Notably, many brilliant ideas have been proposed for TEL, another interesting
direction is to introduce some ideas of TEL into DL for FEDL, with the reverse direction
of the development of UEDL that introduces DL into TEL. For example, recently, Zhang
et al introduced negative correlation learning to
DL with a divide-and-conquer strategy and proposed a solution of deep negative
correlation learning for FEDL.
Data Availability
Data sharing not applicable to this article as no datasets were generated or
analysed during the current study.
Competing interests
The authors declare that they have no competing interests.
Alam M, Samad MD, Vidyaratne L, et al Survey on Deep Neural Networks in
Neurocomputing
417:302–321.
 
Altman NS An introduction to kernel and nearest-neighbor nonparametric
regression. Am Stat. 
Anders S, Huber W Differential expression analysis for sequence count data.
Nat Preced. 
Araque O, Corcuera-Platas I, Sánchez-Rada JF, Iglesias CA Enhancing deep
learning sentiment analysis with ensemble techniques in social applications.
Expert Syst Appl. 
Bakker B, Heskes T Clustering ensembles of neural network models. Neural
Networks. 
Barata C, Ruela M, Francisco M, et al Two systems for the detection of
melanomas in dermoscopy images using texture and color features. IEEE Syst J.
 
Bauer E, Kohavi R Empirical comparison of voting classification algorithms:
bagging, boosting, and variants. Mach Learn
Behera S, Mohanty MN Detection of ocular artifacts using bagged tree
ensemble model. In: Proceedings - 2019 International Conference on Applied
Machine Learning, ICAML 2019
Behler J, Parrinello M Generalized neural-network representation of highdimensional
potential-energy
 
Bi J, Zhang C An empirical comparison on state-of-the-art multi-class imbalance
learning algorithms and a new diversified ensemble learning scheme.
Knowledge-Based Syst. 
Bishop CM Pattern Recoginiton and Machine Learning
Boettcher S, Percus A Nature’s way of optimizing. Artif Intell 119:275–286.
 
Boettcher S, Percus AG Optimization with extremal dynamics. Phys Rev Lett.
 
Boettcher S, Percus AG Optimization with extremal dynamics. Complexity.
 
Borş AG, Pitas I Median radial basis function neural network. IEEE Trans Neural
Networks. 
predictors.
 
 
Cao Y, Geddes TA, Yang JYH, Yang P Ensemble deep learning in bioinformatics.
Nat. Mach. Intell.
Chang CC, Lin CJ LIBSVM: A Library for support vector machines. ACM Trans
Intell Syst Technol. 
Chen J, Zeng GQ, Zhou W, et al Wind speed forecasting using nonlinear-learning
ensemble of deep learning time series prediction and extremal optimization.
Energy Convers Manag. 
Chen W, Feng P, Ding H, Lin H Identifying N 6-methyladenosine sites in the
Arabidopsis
transcriptome.
 
Chen W, Feng PM, Lin H, Chou KC IRSpot-PseDNC: Identify recombination spots
dinucleotide
composition.
 
Chen W, Pourghasemi HR, Kornejady A, Zhang N Landslide spatial modeling:
Introducing new ensembles of ANN, MaxEnt, and SVM machine learning
techniques. Geoderma. 
Chollet F Xception: Deep learning with depthwise separable convolutions. In:
Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition,
Codella NCF, Gutman D, Celebi ME, et al Skin lesion analysis toward melanoma
detection: A challenge at the 2017 International symposium on biomedical
imaging (ISBI), hosted by the international skin imaging collaboration (ISIC). In:
2018 IEEE 15th International Symposium on Biomedical Imaging . IEEE,
pp 168–172
Codella NCF, Nguyen QB, Pankanti S, et al Deep learning ensembles for
recognition
dermoscopy
 
Cox DR The Regression Analysis of Binary Sequences. J R Stat Soc Ser B.
 
Da Silva NFF, Hruschka ER, Hruschka ER Tweet sentiment analysis with classifier
ensembles. Decis Support Syst. 
Das A, Mohapatra SK, Mohanty MN Design of deep ensemble classifier with
fuzzy decision method for biomedical image classification. Appl Soft Comput
115:108178. 
Daubechies I, Bates BJ Ten Lectures on Wavelets. J Acoust Soc Am 93:1671–
1671. 
Davies MN, Secker A, Freitas AA, et al Optimizing amino acid groupings for
classification.
Bioinformatics.
 
Deng J, Dong W, Socher R, et al ImageNet: A large-scale hierarchical image
database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition.
IEEE, pp 248–255
Dietterich TG Ensemble methods in machine learning. In: Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial Intelligence and
Lecture Notes in Bioinformatics)
Dietterich TG Machine-learning research: Four current directions. AI Mag
Dietterich TG, Bakiri G Solving Multiclass Learning Problems via Error-
Correcting Output Codes. J Artif Intell Res. 
Ding S, Zhao H, Zhang Y, et al Extreme learning machine: algorithm, theory and
applications. Artif Intell Rev. 
Dong X, Yu Z, Cao W, et al A survey on ensemble learning. Front. Comput. Sci.
Dos Santos EM, Sabourin R, Maupin P A dynamic overproduce-and-choose
strategy for the selection of classifier ensembles. Pattern Recognit.
 
Dou J, Yunus AP, Bui DT, et al Improved landslide assessment using support
vector machine with bagging, boosting, and stacking ensemble machine learning
framework in a mountainous watershed, Japan. Landslides 17:641–658.
 
Duchi J, Hazan E, Singer Y Adaptive subgradient methods for online learning
and stochastic optimization. In: COLT 2010 - The 23rd Conference on Learning
Durugkar I, Gemp I, Mahadevan S Generative multi-adversarial networks. In:
5th International Conference on Learning Representations, ICLR 2017 -
Conference Track Proceedings
Freund Y, Schapire RE A Decision-Theoretic Generalization of On-Line Learning
Application
 
Freund Y, Schapire RE Experiments with a New Boosting Algorithm. Proc 13th
Int Conf Mach Learn. 
Friedman JH Greedy function approximation: A gradient boosting machine. Ann
Stat. 
Frosst N, Hinton G rey Distilling a neural network into a soft decision tree. In:
CEUR Workshop Proceedings
Gardner MW, Dorling SR Artificial neural networks (the multilayer perceptron)
- a review of applications in the atmospheric sciences. Atmos Environ.
 
Garipov T, Izmailov P, Podoprikhin D, et al Loss surfaces, mode connectivity,
and fast ensembling of DNNs. In: Advances in Neural Information Processing
Geladi P, Kowalski BR Partial least-squares regression: a tutorial. Anal Chim
Acta. 
Ghosh A, Kulharia V, Namboodiri V, et al Multi-agent Diverse Generative
Adversarial Networks. In: Proceedings of the IEEE Computer Society Conference
on Computer Vision and Pattern Recognition
Granitto PM, Verdes PF, Ceccatto HA Neural network ensembles: Evaluation of
aggregation algorithms. Artif Intell. 
Guo C, Yang Y, Pan H, et al Fault analysis of High Speed Train with DBN
hierarchical ensemble. In: Proceedings of the International Joint Conference on
Neural Networks
Guo Y, Yu L, Wen Z, Li M Using support vector machine combined with auto
covariance to predict protein-protein interactions from protein sequences.
Nucleic Acids Res. 
Gupta S, Dennis J, Thurman RE, et al Predicting human nucleosome occupancy
 
Guyon I, Elisseeff A Feature Extraction, Foundations and Applications: An
introduction to feature extraction. Stud Fuzziness Soft Comput
Han X, Chen X, Liu L-P GAN Ensemble for Anomaly Detection. Proc AAAI Conf
Artif Intell 35:4090–4097. 
Hansen LK, Salamon P Neural Network Ensembles. IEEE Trans Pattern Anal
Mach Intell. 
He K, Zhang X, Ren S, Sun J Deep Residual Learning for Image Recognition. In:
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,
pp 770–778
Hernández-Lobato D, Martinez-Muñoz G, Suárez A Statistical instance-based
pruning in ensembles of independent classifiers. IEEE Trans Pattern Anal Mach
Intell. 
Hinton GE A Practical Guide to Training Restricted Boltzmann Machines. In:
Computer. pp 599–619
Hinton GE, Osindero S, Teh YW A fast learning algorithm for deep belief nets.
Neural Comput. 
Hinton GE, Salakhutdinov RR Reducing the dimensionality of data with neural
networks. Science (80- ). 
Ho TK The random subspace method for constructing decision forests. IEEE
Trans Pattern Anal Mach Intell. 
Hochreiter S, Schmidhuber J Long Short-Term Memory. Neural Comput.
 
Hoeting JA, Madigan D, Raftery AE, Volinsky CT Bayesian model averaging: A
tutorial. Stat Sci. 
Hu LY, Huang MW, Ke SW, Tsai CF The distance function effect on k-nearest
classification
Springerplus.
 
Huang G, Huang G Bin, Song S, You K Trends in extreme learning machines: A
review. Neural Networks
Huang G, Li Y, Pleiss G, et al Snapshot Ensembles: Train 1, Get M for Free. In:
International Conference on Learning Representations 2017
Huang G, Liu Z, Maaten L van der, Weinberger KQ Densely Connected
Convolutional Networks. In: 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE, pp 2261–2269
Huang NE, Shen Z, Long SR, et al The empirical mode decomposition and the
Hubert spectrum for nonlinear and non-stationary time series analysis. Proc R Soc
A Math Phys Eng Sci. 
Izmailov P, Podoprikhin D, Garipov T, et al Averaging weights leads to wider
optima and better generalization. In: 34th Conference on Uncertainty in Artificial
Intelligence 2018, UAI 2018
Jia Deng, Wei Dong, Socher R, et al ImageNet: A large-scale hierarchical image
database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition.
IEEE, pp 248–255
Jia F, Lei Y, Lin J, et al Deep neural networks: A promising tool for fault
characteristic mining and intelligent diagnosis of rotating machinery with massive
data. Mech Syst Signal Process. 
Jia Y, Shelhamer E, Donahue J, et al Caffe: Convolutional architecture for fast
feature embedding. In: MM 2014 - Proceedings of the 2014 ACM Conference on
Multimedia
Jungnickel D The Greedy Algorithm. pp 129–153
Kang S, Cho S, Kang P Constructing a multi-class classifier using one-againstone
classifiers.
Neurocomputing.
 
Khamparia A, Singh A, Anand D, et al A novel deep learning-based multi-model
ensemble method for the prediction of neuromuscular disorders. Neural Comput
Appl. 
Khan A, Sohail A, Zahoora U, Qureshi AS A survey of the recent architectures of
deep convolutional neural networks. Artif Intell Rev 53:5455–5516.
 
Kingma DP, Ba JL Adam: A method for stochastic gradient descent. ICLR Int Conf
Learn Represent
Kumar PR Dynamic programming. In: The Control Systems Handbook: Control
System Advanced Methods, Second Edition
Le Q, Mikolov T Distributed representations of sentences and documents. In:
31st International Conference on Machine Learning, ICML 2014
LeCun Y, Bengio Y, Hinton G Deep learning. Nature 521:436–444.
 
LeCun Y, Bottou L, Bengio Y, Haffner P Gradient-based learning applied to
document recognition. Proc IEEE. 
Lee D, Karchin R, Beer MA Discriminative prediction of mammalian enhancers
from DNA sequence. Genome Res. 
Li K, Deb K, Zhang Q, Kwong S An evolutionary many-objective optimization
algorithm based on dominance and decomposition. IEEE Trans Evol Comput.
 
Li T, Qian Z, He T Short-Term Load Forecasting with Improved CEEMDAN and
Complexity.
 
Li W, Du Q, Zhang F, Hu W Hyperspectral Image Classification by Fusing
Collaborative and Sparse Representations. IEEE J Sel Top Appl Earth Obs Remote
Sens. 
LIN J, QU L FEATURE EXTRACTION BASED ON MORLET WAVELET AND ITS
APPLICATION FOR MECHANICAL FAULT DIAGNOSIS. J Sound Vib 234:135–148.
 
Liu B, Long R, Chou KC IDHS-EL: Identifying DNase i hypersensitive sites by
fusing three different modes of pseudo nucleotide composition into an ensemble
framework.
Bioinformatics.
 
Liu W, Wang Z, Liu X, et al A survey of deep neural network architectures and
applications.
Neurocomputing
234:11–26.
 
Liu W, Zhang M, Luo Z, Cai Y An Ensemble Deep Learning Method for Vehicle
Type Classification on Visual Traffic Surveillance Sensors. IEEE Access 5:24417–
24425. 
Liu Y, Gao Z, Chen J Development of soft-sensors for online quality prediction
of sequential-reactor-multi-grade industrial processes. Chem Eng Sci.
 
Liu Y, Yang C, Gao Z, Yao Y Ensemble deep kernel learning with application to
quality prediction in industrial polymerization processes. Chemom Intell Lab Syst.
 
Liu Y, Yao X Ensemble learning via negative correlation. Neural Networks.
 
Liu Z, Xiao X, Yu DJ, et al pRNAm-PC: Predicting N6-methyladenosine sites in
physical-chemical
properties.
 
Loshchilov I, Hutter F SGDR: Stochastic gradient descent with warm restarts. In:
5th International Conference on Learning Representations, ICLR 2017 -
Conference Track Proceedings
Ma S, Chu F Ensemble deep learning-based fault diagnosis of rotor bearing
systems. Comput Ind. 
Maddox W, Garipov T, Izmailov P, et al A Simple Baseline for Bayesian
Uncertainty in Deep Learning. Adv Neural Inf Process Syst
Mairal J Sparse Modeling for Image and Vision Processing
Mallat SG A Theory for Multiresolution Signal Decomposition: The Wavelet
Representation.
 
Mao S, Jiao L, Xiong L, et al Weighted classifier ensemble based on quadratic
form. Pattern Recognit. 
Martinez-Muñoz G, Hernández-Lobato D, Suarez A An analysis of ensemble
pruning techniques based on ordered aggregation. IEEE Trans Pattern Anal Mach
Intell. 
Masoudnia S, Ebrahimpour R Mixture of experts: A literature survey. Artif Intell
Rev. 
Mendes-Moreira J, Soares C, Jorge AM, De Sousa JF Ensemble approaches for
regression: A survey. ACM Comput. Surv.
Mikolov T, Chen K, Corrado G, Dean J Efficient estimation of word
representations in vector space. In: 1st International Conference on Learning
Representations, ICLR 2013 - Workshop Track Proceedings
Mohammed A, Kora R An effective ensemble deep learning framework for text
classification.
 
Mohapatra SK, Khilar R, Das A, Mohanty MN Design of Gradient Boosting
Ensemble Classifier with Variation of Learning Rate for Automated Cardiac Data
Classification. In: 2021 8th International Conference on Signal Processing and
Integrated Networks (SPIN). IEEE, pp 11–14
Naser H Estimating and forecasting the real prices of crude oil: A data rich
model using a dynamic model averaging (DMA) approach. Energy Econ.
 
Niemeijer M, Van Ginneken B, Staal J, et al Automatic detection of red lesions
photographs.
 
Omari A, Figueiras-Vidal AR Post-aggregation of classifier ensembles. Inf Fusion.
 
Orlando JI, Prokofyeva E, del Fresno M, Blaschko MB An ensemble deep
learning based approach for red lesion detection in fundus images. Comput
Methods Programs Biomed. 
Page L, Brin S, Motwani R, Winograd T The PageRank Citation Ranking: Bringing
 
Panda S, Das A, Mishra S, Mohanty MN Epileptic Seizure Detection using Deep
Ensemble Network with Empirical Wavelet Transform. Meas Sci Rev 21:110–116.
 
Parisotto E, Ba J, Salakhutdinov R Actor-mimic deep multitask and transfer
reinforcement learning. In: 4th International Conference on Learning
Representations, ICLR 2016 - Conference Track Proceedings
Pham BT, Tien Bui D, Prakash I, Dholakia MB Hybrid integration of Multilayer
Perceptron Neural Networks and machine learning ensembles for landslide
susceptibility assessment at Himalayan area (India) using GIS. Catena.
 
Phillips SJ, Dudík M, Schapire RE A maximum entropy approach to species
distribution modeling. In: Proceedings, Twenty-First International Conference on
Machine Learning, ICML 2004
Prasad R, Deo RC, Li Y, Maraseni T Soil moisture forecasting by a hybrid machine
learning technique: ELM integrated with ensemble empirical mode
decomposition. Geoderma. 
Qiu X, Ren Y, Suganthan PN, Amaratunga GAJ Empirical Mode Decomposition
based ensemble deep learning for load demand time series forecasting. Appl Soft
Comput J. 
 
Qummar S, Khan FG, Shah S, et al A Deep Learning Ensemble Approach for
Retinopathy
Detection.
 
Rahmati O, Pourghasemi HR, Zeinivand H Flood susceptibility mapping using
frequency ratio and weights-of-evidence models in the Golastan Province, Iran.
Geocarto Int. 
Ramsey FL Characterization of the Partial Autocorrelation Function. Ann Stat
2:. 
Rawat W, Wang Z Deep convolutional neural networks for image classification:
A comprehensive review. Neural Comput.
Rodríguez JJ, Kuncheva LI, Alonso CJ Rotation forest: A New classifier ensemble
 
Ronneberger O, Fischer P, Brox T U-Net: Convolutional Networks for
Biomedical Image Segmentation. In: Lecture Notes in Computer Science
(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in
Bioinformatics). pp 234–241
Rosenblatt The Perceptron: A Theory of Statistical Separability in Cognitive
Rumelhart DE, Hinton GE, Williams RJ Learning representations by backpropagating errors. Nature. 
Sagi O, Rokach L Ensemble learning: A survey. Wiley Interdiscip. Rev. Data Min.
Knowl. Discov.
Sainath TN, Vinyals O, Senior A, Sak H Convolutional, Long Short-Term Memory,
fully connected Deep Neural Networks. In: 2015 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp 4580–4584
Salzberg SL C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan
Publishers,
 
Sandler M, Howard A, Zhu M, et al MobileNetV2: Inverted Residuals and Linear
Bottlenecks. In: Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition
SB S, Singh V Automatic Detection of Diabetic Retinopathy in Non-dilated RGB
Retinal Fundus Images. Int J Comput Appl. 
Learnability.
 
Seoud L, Hurtut T, Chelbi J, et al Red Lesion Detection Using Dynamic Shape
Features for Diabetic Retinopathy Screening. IEEE Trans Med Imaging.
 
Shahabi H, Shirzadi A, Ghaderi K, et al Flood detection and susceptibility
mapping using Sentinel-1 remote sensing data and a machine learning approach:
Hybrid intelligence of bagging ensemble based on K-Nearest Neighbor classifier.
Remote Sens. 
Shakeel PM, Tolba A, Al-Makhadmeh Z, Jaber MM Automatic detection of lung
cancer from biomedical data set using discrete AdaBoost optimized ensemble
generalized
 
Shen J, Zhang J, Luo X, et al Predicting protein-protein interactions based only
information.
 
Shen Z, He Z, Xue X MEAL: Multi-Model ensemble via adversarial learning. In:
33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative
Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2019
Smith LN Cyclical learning rates for training neural networks. In: Proceedings -
2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017
Storn R, Price K Differential Evolution - A Simple and Efficient Heuristic for
Optimization
Continuous
 
Su H, Yu Y, Du Q, Du P Ensemble Learning for Hyperspectral Image Classification
Using Tangent Collaborative Representation. IEEE Trans Geosci Remote Sens.
 
Su H, Zhao B, Du Q, Sheng Y Tangent Distance-Based Collaborative
Representation for Hyperspectral Image Classification. IEEE Geosci Remote Sens
Lett. 
Suk H Il, Lee SW, Shen D Deep ensemble learning of sparse regression models
diagnosis.
 
Szegedy C, Ioffe S, Vanhoucke V, Alemi AA Inception-v4, inception-ResNet and
the impact of residual connections on learning. In: 31st AAAI Conference on
Artificial Intelligence, AAAI 2017
Szegedy C, Vanhoucke V, Ioffe S, et al Rethinking the Inception Architecture for
Computer Vision. In: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, pp 2818–2826
Tan M, Le Q V. EfficientNetV2: Smaller Models and Faster Training.
 
Ting KM, Witten IH Stacking bagged and dagged models. Proc of ICML’97
Torres ME, Colominas MA, Schlotthauer G, Flandrin P A complete ensemble
empirical mode decomposition with adaptive noise. In: 2011 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp 4144–
Vesanto J, Alhoniemi E Clustering of the self-organizing map. IEEE Trans Neural
Networks. 
Vincent P, Larochelle H, Bengio Y, Manzagol PA Extracting and composing
robust features with denoising autoencoders. In: Proceedings of the 25th
International Conference on Machine Learning
Vincent P, Larochelle H, Lajoie I, et al Stacked denoising autoencoders: Learning
Useful Representations in a Deep Network with a Local Denoising Criterion. J
Mach Learn Res
Wang G, Jia R, Liu J, Zhang H A hybrid wind power forecasting approach based
on Bayesian model averaging and ensemble learning. Renew Energy.
 
Wang H, Nie F, Huang H, et al Identifying AD-sensitive and cognition-relevant
imaging biomarkers via joint classification and regression. In: Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial Intelligence and
Lecture Notes in Bioinformatics)
Wang H zhi, Li G qiang, Wang G bing, et al Deep learning based ensemble
probabilistic
forecasting.
 
Wang J, Liu Z, Wu Y, Yuan J Learning actionlet ensemble for 3D human action
recognition.
 
Wasserman L Bayesian model selection and model averaging. J Math Psychol.
 
Webb GI MultiBoosting: a technique for combining boosting and wagging.
Mach Learn. 
Wei L, Chen H, Su R M6APred-EL: A Sequence-Based Predictor for Identifying
N6-methyladenosine Sites Using Ensemble Learning. Mol Ther - Nucleic Acids.
 
Wold S, Esbensen K, Geladi P Principal component analysis. Chemom Intell Lab
Syst 2:37–52. 
generalization.
 
Wu X, Kumar V, Ross QJ, et al Top 10 algorithms in data mining. Knowl Inf Syst.
 
WU Z, HUANG NE ENSEMBLE EMPIRICAL MODE DECOMPOSITION: A NOISE-
ASSISTED DATA ANALYSIS METHOD. Adv Adapt Data Anal 01:1–41.
 
Xia J-F, Han K, Huang D-S Sequence-Based Prediction of Protein-Protein
Interactions by Means of Rotation Forest and Autocorrelation Descriptor. Protein
Pept Lett. 
Xia J, Yokoya N, Iwasaki A A novel ensemble classifier of hyperspectral and
LiDAR data using morphological features. In: ICASSP, IEEE International
Conference on Acoustics, Speech and Signal Processing - Proceedings
Xiao Y, Wu J, Lin Z, Zhao X A deep learning-based multi-model ensemble
method for cancer prediction. Comput Methods Programs Biomed 153:1–9.
 
Xie S, Girshick R, Dollar P, et al Aggregated Residual Transformations for Deep
Neural Networks. In: 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, pp 5987–5995
Xu G, Liu M, Jiang Z, et al Bearing Fault Diagnosis Method Based on Deep
Convolutional Neural Network and Random Forest Ensemble Learning. Sensors
19:1088. 
Xu Y, Yang W, Wang J Air quality early-warning system for cities in China. Atmos
Environ. 
Yang Y, Chen N, Jiang S Collaborative strategy for visual object tracking.
Multimed Tools Appl 77:7283–7303. 
Yang Y, Lv H, Chen N, et al Local minima found in the subparameter space can
be effective for ensembles of deep convolutional neural networks. Pattern
Recognit 109:107582. 
Yang Y, Lv H, Chen N, et al FTBME: feature transferring based multi-model
79:18767–18799.
 
Yang Y, Wu Y, Chen N Explorations on visual localization from active to passive.
Multimed Tools Appl 78:2269–2309. 
Yang Y, Yang Y, Chen J, et al Handling Noisy Labels via One-Step Abductive
Multi-Target Learning: An Application to Helicobacter Pylori Segmentation
Yang Y, Yang Y, Yuan Y, et al Detecting helicobacter pylori in whole slide images
via weakly supervised multi-task learning. Multimed Tools Appl 79:26787–26815.
 
You ZH, Lei YK, Zhu L, et al Prediction of protein-protein interactions from
amino acid sequences with ensemble extreme learning machines and principal
component analysis. BMC Bioinformatics. 
Zagaglia P Macroeconomic factors and oil futures prices: A data-rich model.
Energy Econ. 
Zhang CX, Zhang JS A survey of selective ensemble learning algorithms. Jisuanji
Xuebao/Chinese J. Comput.
Zhang D, Shen D Multi-modal multi-task learning for joint prediction of multiple
regression and classification variables in Alzheimer’s disease. Neuroimage.
 
Zhang L, Shi Z, Cheng MM, et al Nonlinear Regression via Deep Negative
Correlation
 
Zhang P, He Z Using data-driven feature enrichment of text representation and
ensemble technique for sentence-level polarity classification. J Inf Sci.
 
Zhang X, Wang J, Zhang K Short-term electric load forecasting based on singular
spectrum analysis and support vector machine optimized by Cuckoo search
algorithm. Electr Power Syst Res. 
Zhao Y, Li J, Yu L A deep learning ensemble approach for crude oil price
forecasting. Energy Econ. 
Zhou ZH Ensemble methods: Foundations and algorithms
Zhou ZH Ensemble Learning. In: Encyclopedia of Biometrics
Zhou ZH, Wu J, Tang W Ensembling neural networks: Many could be better than
all. Artif Intell. 
Zhu C, Bichot CE, Chen L Multi-scale color local binary patterns for visual object
classes recognition. In: Proceedings - International Conference on Pattern
Recognition
Zoph B, Vasudevan V, Shlens J, Le Q V. Learning Transferable Architectures for
Scalable Image Recognition. In: 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition. IEEE, pp 8697–8710