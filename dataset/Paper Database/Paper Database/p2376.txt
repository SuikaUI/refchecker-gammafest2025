CBAM: Convolutional Block Attention Module
Sanghyun Woo*1, Jongchan Park*†2, Joon-Young Lee3, and In So Kweon1
1 Korea Advanced Institute of Science and Technology, Daejeon, Korea
{shwoo93, iskweon77}@kaist.ac.kr
2 Lunit Inc., Seoul, Korea
 
3 Adobe Research, San Jose, CA, USA
 
Abstract. We propose Convolutional Block Attention Module (CBAM),
a simple yet eﬀective attention module for feed-forward convolutional
neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel
and spatial, then the attention maps are multiplied to the input feature
map for adaptive feature reﬁnement. Because CBAM is a lightweight and
general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with
base CNNs. We validate our CBAM through extensive experiments on
ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets.
Our experiments show consistent improvements in classiﬁcation and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.
Keywords: Object recognition, attention mechanism, gated convolution
Introduction
Convolutional neural networks (CNNs) have signiﬁcantly pushed the performance of vision tasks based on their rich representation power. To enhance performance of CNNs, recent researches have mainly investigated three
important factors of networks: depth, width, and cardinality.
From the LeNet architecture to Residual-style Networks so far, the
network has become deeper for rich representation. VGGNet shows that stacking blocks with the same shape gives fair results. Following the same spirit,
ResNet stacks the same topology of residual blocks along with skip connection to build an extremely deep architecture. GoogLeNet shows that width
is another important factor to improve the performance of a model. Zagoruyko
and Komodakis propose to increase the width of a network based on the
ResNet architecture. They have shown that a 28-layer ResNet with increased
*Both authors have equally contributed.
†The work was done while the author was at KAIST.
 
Woo, Park, Lee, Kweon
Convolutional Block Attention Module
Input Feature
Refined Feature
Fig. 1: The overview of CBAM. The module has two sequential sub-modules:
channel and spatial. The intermediate feature map is adaptively reﬁned through
our module (CBAM) at every convolutional block of deep networks.
width can outperform an extremely deep ResNet with 1001 layers on the CI-
FAR benchmarks. Xception and ResNeXt come up with to increase the
cardinality of a network. They empirically show that cardinality not only saves
the total number of parameters but also results in stronger representation power
than the other two factors: depth and width.
Apart from these factors, we investigate a diﬀerent aspect of the architecture
design, attention. The signiﬁcance of attention has been studied extensively in
the previous literature . Attention not only tells where to focus, it also
improves the representation of interests. Our goal is to increase representation
power by using attention mechanism: focusing on important features and suppressing unnecessary ones. In this paper, we propose a new network module,
named “Convolutional Block Attention Module”. Since convolution operations
extract informative features by blending cross-channel and spatial information
together, we adopt our module to emphasize meaningful features along those two
principal dimensions: channel and spatial axes. To achieve this, we sequentially
apply channel and spatial attention modules (as shown in Fig. 1), so that each
of the branches can learn ‘what’ and ‘where’ to attend in the channel and spatial
axes respectively. As a result, our module eﬃciently helps the information ﬂow
within the network by learning which information to emphasize or suppress.
In the ImageNet-1K dataset, we obtain accuracy improvement from various
baseline networks by plugging our tiny module, revealing the eﬃcacy of CBAM.
We visualize trained models using the grad-CAM and observe that CBAMenhanced networks focus on target objects more properly than their baseline
networks. Taking this into account, we conjecture that the performance boost
comes from accurate attention and noise reduction of irrelevant clutters. Finally,
we validate performance improvement of object detection on the MS COCO and
the VOC 2007 datasets, demonstrating a wide applicability of CBAM. Since we
have carefully designed our module to be light-weight, the overhead of parameters and computation is negligible in most cases.
Contribution. Our main contribution is three-fold.
1. We propose a simple yet eﬀective attention module (CBAM) that can be
widely applied to boost representation power of CNNs.
Convolutional Block Attention Module
2. We validate the eﬀectiveness of our attention module through extensive ablation studies.
3. We verify that performance of various networks is greatly improved on the
multiple benchmarks by plugging our light-weight module.
Related Work
Network engineering. “Network engineering” has been one of the most important vision research, because well-designed networks ensure remarkable performance improvement in various applications. A wide range of architectures has
been proposed since the successful implementation of a large-scale CNN .
An intuitive and simple way of extension is to increase the depth of neural
networks . Szegedy et al.
 introduce a deep Inception network using a
multi-branch architecture where each branch is customized carefully. While a
naive increase in depth comes to saturation due to the diﬃculty of gradient
propagation, ResNet proposes a simple identity skip-connection to ease the
optimization issues of deep networks. Based on the ResNet architecture, various
models such as WideResNet , Inception-ResNet , and ResNeXt have been
developed. WideResNet proposes a residual network with a larger number of
convolutional ﬁlters and reduced depth. PyramidNet is a strict generalization
of WideResNet where the width of the network gradually increases. ResNeXt 
suggests to use grouped convolutions and shows that increasing the cardinality
leads to better classiﬁcation accuracy. More recently, Huang et al. propose
a new architecture, DenseNet. It iteratively concatenates the input features with
the output features, enabling each convolution block to receive raw information
from all the previous blocks. While most of recent network engineering methods
mainly target on three factors depth , width , and cardinality , we focus on the other aspect, ‘attention’, one of the curious facets
of a human visual system.
Attention mechanism. It is well known that attention plays an important
role in human perception . One important property of a human visual
system is that one does not attempt to process a whole scene at once. Instead,
humans exploit a sequence of partial glimpses and selectively focus on salient
parts in order to capture visual structure better .
Recently, there have been several attempts to incorporate attention
processing to improve the performance of CNNs in large-scale classiﬁcation tasks.
Wang et al. propose Residual Attention Network which uses an encoderdecoder style attention module. By reﬁning the feature maps, the network not
only performs well but is also robust to noisy inputs. Instead of directly computing the 3d attention map, we decompose the process that learns channel
attention and spatial attention separately. The separate attention generation
process for 3D feature map has much less computational and parameter over-
Woo, Park, Lee, Kweon
head, and therefore can be used as a plug-and-play module for pre-existing base
CNN architectures.
More close to our work, Hu et al. introduce a compact module to exploit
the inter-channel relationship. In their Squeeze-and-Excitation module, they use
global average-pooled features to compute channel-wise attention. However, we
show that those are suboptimal features in order to infer ﬁne channel attention,
and we suggest to use max-pooled features as well. They also miss the spatial
attention, which plays an important role in deciding ‘where’ to focus as shown in
 . In our CBAM, we exploit both spatial and channel-wise attention based on
an eﬃcient architecture and empirically verify that exploiting both is superior to
using only the channel-wise attention as . Moreover, we empirically show that
our module is eﬀective in detection tasks (MS-COCO and VOC). Especially, we
achieve state-of-the-art performance just by placing our module on top of the
existing one-shot detector in the VOC2007 test set.
Convolutional Block Attention Module
Given an intermediate feature map F ∈RC×H×W as input, CBAM sequentially
infers a 1D channel attention map Mc ∈RC×1×1 and a 2D spatial attention
map Ms ∈R1×H×W as illustrated in Fig. 1. The overall attention process can
be summarized as:
F′ = Mc(F) ⊗F,
F′′ = Ms(F′) ⊗F′,
where ⊗denotes element-wise multiplication. During multiplication, the attention values are broadcasted (copied) accordingly: channel attention values are
broadcasted along the spatial dimension, and vice versa. F′′ is the ﬁnal reﬁned
output. Fig. 2 depicts the computation process of each attention map. The following describes the details of each attention module.
Channel attention module. We produce a channel attention map by exploiting the inter-channel relationship of features. As each channel of a feature map
is considered as a feature detector , channel attention focuses on ‘what’ is
meaningful given an input image. To compute the channel attention eﬃciently,
we squeeze the spatial dimension of the input feature map. For aggregating spatial information, average-pooling has been commonly adopted so far. Zhou et al.
 suggest to use it to learn the extent of the target object eﬀectively and Hu et
al. adopt it in their attention module to compute spatial statistics. Beyond
the previous works, we argue that max-pooling gathers another important clue
about distinctive object features to infer ﬁner channel-wise attention. Thus, we
use both average-pooled and max-pooled features simultaneously. We empirically conﬁrmed that exploiting both features greatly improves representation
power of networks rather than using each independently (see Sec. 4.1), showing
the eﬀectiveness of our design choice. We describe the detailed operation below.
Convolutional Block Attention Module
Channel Attention
Channel Attention Module
[MaxPool, AvgPool]
Spatial Attention
Spatial Attention Module
Input feature F
Channel-refined
feature F’
Shared MLP
Fig. 2: Diagram of each attention sub-module. As illustrated, the channel
sub-module utilizes both max-pooling outputs and average-pooling outputs with
a shared network; the spatial sub-module utilizes similar two outputs that are
pooled along the channel axis and forward them to a convolution layer.
We ﬁrst aggregate spatial information of a feature map by using both averagepooling and max-pooling operations, generating two diﬀerent spatial context descriptors: Fc
avg and Fc
max, which denote average-pooled features and max-pooled
features respectively. Both descriptors are then forwarded to a shared network
to produce our channel attention map Mc ∈RC×1×1. The shared network is
composed of multi-layer perceptron (MLP) with one hidden layer. To reduce
parameter overhead, the hidden activation size is set to RC/r×1×1, where r is
the reduction ratio. After the shared network is applied to each descriptor, we
merge the output feature vectors using element-wise summation. In short, the
channel attention is computed as:
Mc(F) = σ(MLP(AvgPool(F)) + MLP(MaxPool(F)))
= σ(W1(W0(Fc
avg)) + W1(W0(Fc
where σ denotes the sigmoid function, W0 ∈RC/r×C, and W1 ∈RC×C/r. Note
that the MLP weights, W0 and W1, are shared for both inputs and the ReLU
activation function is followed by W0.
Spatial attention module. We generate a spatial attention map by utilizing
the inter-spatial relationship of features. Diﬀerent from the channel attention,
the spatial attention focuses on ‘where’ is an informative part, which is complementary to the channel attention. To compute the spatial attention, we ﬁrst
apply average-pooling and max-pooling operations along the channel axis and
concatenate them to generate an eﬃcient feature descriptor. Applying pooling
operations along the channel axis is shown to be eﬀective in highlighting informative regions . On the concatenated feature descriptor, we apply a convolution
Woo, Park, Lee, Kweon
Channel attention
ResBlock + CBAM
conv blocks
conv blocks
Spatial attention
Fig. 3: CBAM integrated with a ResBlock in ResNet . This ﬁgure shows
the exact position of our module when integrated within a ResBlock. We apply
CBAM on the convolution outputs in each block.
layer to generate a spatial attention map Ms(F) ∈RH×W which encodes where
to emphasize or suppress. We describe the detailed operation below.
We aggregate channel information of a feature map by using two pooling
operations, generating two 2D maps: Fs
avg ∈R1×H×W and Fs
max ∈R1×H×W .
Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer,
producing our 2D spatial attention map. In short, the spatial attention is computed as:
Ms(F) = σ(f 7×7([AvgPool(F); MaxPool(F)]))
= σ Top-5 Error(%)
ResNet50 (baseline)
ResNet50 + AvgPool (SE )
ResNet50 + MaxPool
ResNet50 + AvgPool & MaxPool
Table 1: Comparison of diﬀerent channel attention methods. We observe
that using our proposed method outperforms recently suggested Squeeze and
Excitation method .
Woo, Park, Lee, Kweon
Description
Param. GFLOPs Top-1 Error(%) Top-5 Error(%)
ResNet50 + channel (SE )
ResNet50 + channel
ResNet50 + channel + spatial (1x1 conv, k=3) 28.10M
ResNet50 + channel + spatial (1x1 conv, k=7) 28.10M
ResNet50 + channel + spatial (avg&max, k=3) 28.09M
ResNet50 + channel + spatial (avg&max, k=7) 28.09M
Table 2: Comparison of diﬀerent spatial attention methods. Using the
proposed channel-pooling (i.e. average- and max-pooling along the channel axis)
along with the large kernel size of 7 for the following convolution operation
performs best.
Description
Top-1 Error(%) Top-5 Error(%)
ResNet50 + channel (SE )
ResNet50 + channel + spatial
ResNet50 + spatial + channel
ResNet50 + channel & spatial in parallel
Table 3: Combining methods of channel and spatial attention. Using both
attention is critical while the best-combining strategy (i.e. sequential, channel-
ﬁrst) further improves the accuracy.
of max-pooled features. We argue that max-pooled features which encode the degree of the most salient part can compensate the average-pooled features which
encode global statistics softly. Thus, we suggest to use both features simultaneously and apply a shared network to those features. The outputs of a shared
network are then merged by element-wise summation. We empirically show that
our channel attention method is an eﬀective way to push performance further
from SE without additional learnable parameters. As a brief conclusion, we
use both average- and max-pooled features in our channel attention module with
the reduction ratio of 16 in the following experiments.
Spatial attention. Given the channel-wise reﬁned features, we explore an eﬀective method to compute the spatial attention. The design philosophy is symmetric with the channel attention branch. To generate a 2D spatial attention map,
we ﬁrst compute a 2D descriptor that encodes channel information at each pixel
over all spatial locations. We then apply one convolution layer to the 2D descriptor, obtaining the raw attention map. The ﬁnal attention map is normalized by
the sigmoid function.
We compare two methods of generating the 2D descriptor: channel pooling
using average- and max-pooling across the channel axis and standard 1 × 1 convolution reducing the channel dimension into 1. In addition, we investigate the
eﬀect of a kernel size at the following convolution layer: kernel sizes of 3 and 7.
In the experiment, we place the spatial attention module after the previously
Convolutional Block Attention Module
designed channel attention module, as the ﬁnal goal is to use both modules
Table 2 shows the experimental results. We can observe that the channel
pooling produces better accuracy, indicating that explicitly modeled pooling
leads to ﬁner attention inference rather than learnable weighted channel pooling
(implemented as 1 × 1 convolution). In the comparison of diﬀerent convolution
kernel sizes, we ﬁnd that adopting a larger kernel size generates better accuracy
in both cases. It implies that a broad view (i.e. large receptive ﬁeld) is needed
for deciding spatially important regions. Considering this, we adopt the channelpooling method and the convolution layer with a large kernel size to compute
spatial attention. In a brief conclusion, we use the average- and max-pooled
features across the channel axis with a convolution kernel size of 7 as our spatial
attention module.
Arrangement of the channel and spatial attention. In this experiment,
we compare three diﬀerent ways of arranging the channel and spatial attention
submodules: sequential channel-spatial, sequential spatial-channel, and parallel
use of both attention modules. As each module has diﬀerent functions, the order
may aﬀect the overall performance. For example, from a spatial viewpoint, the
channel attention is globally applied, while the spatial attention works locally.
Also, it is natural to think that we may combine two attention outputs to build
a 3D attention map. In the case, both attentions can be applied in parallel, then
the outputs of the two attention modules are added and normalized with the
sigmoid function.
Table 3 summarizes the experimental results on diﬀerent attention arranging methods. From the results, we can ﬁnd that generating an attention map
sequentially infers a ﬁner attention map than doing in parallel. In addition, the
channel-ﬁrst order performs slightly better than the spatial-ﬁrst order. Note that
all the arranging methods outperform using only the channel attention independently, showing that utilizing both attentions is crucial while the best-arranging
strategy further pushes performance.
Final module design. Throughout the ablation studies, we have designed the
channel attention module, the spatial attention module, and the arrangement of
the two modules. Our ﬁnal module is as shown in Fig. 1 and Fig. 2: we choose
average- and max-pooling for both channel and spatial attention module; we use
convolution with a kernel size of 7 in the spatial attention module; we arrange
the channel and spatial submodules sequentially. Our ﬁnal module(i.e. ResNet50
+ CBAM) achieves top-1 error of 22.66%, which is much lower than SE (i.e.
ResNet50 + SE), as shown in Table 4.
Image Classiﬁcation on ImageNet-1K
We perform ImageNet-1K classiﬁcation experiments to rigorously evaluate our
module. We follow the same protocol as speciﬁed in Sec. 4.1 and evaluate our
Woo, Park, Lee, Kweon
Architecture
Param. GFLOPs Top-1 Error (%) Top-5 Error (%)
ResNet18 
ResNet18 + SE 
ResNet18 + CBAM
ResNet34 
ResNet34 + SE 
ResNet34 + CBAM
ResNet50 
ResNet50 + SE 
ResNet50 + CBAM
ResNet101 
ResNet101 + SE 
ResNet101 + CBAM
WideResNet18 (widen=1.5)
WideResNet18 (widen=1.5) + SE 26.07M
WideResNet18 (widen=1.5) + CBAM 26.08M
WideResNet18 (widen=2.0)
WideResNet18 (widen=2.0) + SE 45.97M
WideResNet18 (widen=2.0) + CBAM 45.97M
ResNeXt50 (32x4d)
ResNeXt50 (32x4d) + SE 
ResNeXt50 (32x4d) + CBAM
ResNeXt101 (32x4d)
ResNeXt101 (32x4d) + SE 
ResNeXt101 (32x4d) + CBAM
* all results are reproduced in the PyTorch framework.
Table 4: Classiﬁcation results on ImageNet-1K. Single-crop validation errors are reported.
module in various network architectures including ResNet , WideResNet ,
and ResNext .
Table 4 summarizes the experimental results. The networks with CBAM
outperform all the baselines signiﬁcantly, demonstrating that the CBAM can
generalize well on various models in the large-scale dataset. Moreover, the models
with CBAM improve the accuracy upon the one of the strongest method – SE 
which is the winning approach of the ILSVRC 2017 classiﬁcation task. It implies
that our proposed approach is powerful, showing the eﬃcacy of new pooling
method that generates richer descriptor and spatial attention that complements
the channel attention eﬀectively.
Fig. 4 depicts the error curves of various networks during ImageNet-1K training. We can clearly see that our method exhibits lowest training and validation
error in both error plots. It shows that CBAM has greater ability to improve
generalization power of baseline models compared to SE .
We also ﬁnd that the overall overhead of CBAM is quite small in terms of both
parameters and computation. This motivates us to apply our proposed module
CBAM to the light-weight network, MobileNet . Table 5 summarizes the
experimental results that we conducted based on the MobileNet architecture.
We have placed CBAM to two models, basic and capacity-reduced model(i.e.
adjusting width multiplier(α) to 0.7). We observe similar phenomenon as shown
Convolutional Block Attention Module
(a) ResNet50 
(b) MobileNet 
Fig. 4: Error curves during ImageNet-1K training. Best viewed in color.
Architecture
Parameters GFLOPs Top-1 Error (%) Top-5 Error (%)
MobileNet α = 0.7
MobileNet α = 0.7 + SE 
MobileNet α = 0.7 + CBAM
MobileNet 
MobileNet + SE 
MobileNet + CBAM
* all results are reproduced in the PyTorch framework.
Table 5: Classiﬁcation results on ImageNet-1K using the light-weight
network, MobileNet . Single-crop validation errors are reported.
in Table 4. CBAM not only boosts the accuracy of baselines signiﬁcantly but also
favorably improves the performance of SE . This shows the great potential
of CBAM for applications on low-end devices.
Network Visualization with Grad-CAM 
For the qualitative analysis, we apply the Grad-CAM to diﬀerent networks
using images from the ImageNet validation set. Grad-CAM is a recently proposed
visualization method which uses gradients in order to calculate the importance
of the spatial locations in convolutional layers. As the gradients are calculated
with respect to a unique class, Grad-CAM result shows attended regions clearly.
By observing the regions that network has considered as important for predicting
a class, we attempt to look at how this network is making good use of features.
We compare the visualization results of CBAM-integrated network (ResNet50 +
CBAM) with baseline (ResNet50) and SE-integrated network (ResNet50 + SE).
Fig. 5 illustrate the visualization results. The softmax scores for a target class
are also shown in the ﬁgure.
In Fig. 5, we can clearly see that the Grad-CAM masks of the CBAMintegrated network cover the target object regions better than other methods.
That is, the CBAM-integrated network learns well to exploit information in
target object regions and aggregate features from them. Note that target class
Woo, Park, Lee, Kweon
P = 0.96340
P = 0.19994
P = 0.93707
P = 0.35248
P = 0.87490
P = 0.53005
P = 0.99085
P = 0.59662
P = 0.96039
P = 0.59790
P = 0.84387
P = 0.71000
P = 0.98482
P = 0.90806
P = 0.78636
P = 0.98567
Tailed frog
Toilet tissue
Loudspeaker
Spider web
American egret
Space heater
Croquet ball
Hammerhead
Eskimo dog
Snow leopard
Boat paddle
Daddy longlegs
School bus
Fig. 5: Grad-CAM visualization results. We compare the visualization results of CBAM-integrated network (ResNet50 + CBAM) with baseline
(ResNet50) and SE-integrated network (ResNet50 + SE). The grad-CAM visualization is calculated for the last convolutional outputs. The ground-truth label
is shown on the top of each input image and P denotes the softmax score of each
network for the ground-truth class.
Convolutional Block Attention Module
mAP@.5 mAP@.75 mAP@[.5, .95]
ResNet50 
Faster-RCNN 
ResNet50 + CBAM
Faster-RCNN 
ResNet101 
Faster-RCNN 
ResNet101 + CBAM Faster-RCNN 
* all results are reproduced in the PyTorch framework.
Table 6: Object detection mAP(%) on the MS COCO validation set. We
adopt the Faster R-CNN detection framework and apply our module to the
base networks. CBAM boosts mAP@[.5, .95] by 0.9 for both baseline networks.
mAP@.5 Parameters (M)
StairNet 
StairNet + SE 
StairNet + CBAM
MobileNet SSD 
MobileNet StairNet 
MobileNet StairNet + SE 
MobileNet StairNet + CBAM
* all results are reproduced in the PyTorch framework.
Table 7: Object detection mAP(%) on the VOC 2007 test set. We adopt
the StairNet detection framework and apply SE and CBAM to the detectors.
CBAM favorably improves all the strong baselines with negligible additional
parameters.
scores also increase accordingly. From the observations, we conjecture that the
feature reﬁnement process of CBAM eventually leads the networks to utilize
given features well.
MS COCO Object Detection
We conduct object detection on the Microsoft COCO dataset . This dataset
involves 80k training images and 40k validation images . The average mAP over diﬀerent IoU thresholds from 0.5 to 0.95 is used
for evaluation. According to , we trained our model using all the training
images as well as a subset of validation images, holding out 5,000 examples for
validation. Our training code is based on and we train the network for 490K
iterations for fast performance validation. We adopt Faster-RCNN as our
detection method and ImageNet pre-trained ResNet50 and ResNet101 as our
baseline networks. Here we are interested in performance improvement by plugging CBAM to the baseline networks. Since we use the same detection method in
all the models, the gains can only be attributed to the enhanced representation
power, given by our module CBAM. As shown in the Table 6, we observe signiﬁcant improvements from the baseline, demonstrating generalization performance
of CBAM on other recognition tasks.
Woo, Park, Lee, Kweon
VOC 2007 Object Detection
We further perform experiments on the PASCAL VOC 2007 test set. In this experiment, we apply CBAM to the detectors, while the previous experiments (Table 6) apply our module to the base networks. We adopt the StairNet framework, which is one of the strongest multi-scale method based on the SSD .
For the experiment, we reproduce SSD and StairNet in our PyTorch platform
in order to estimate performance improvement of CBAM accurately and achieve
77.8% and 78.9% mAP@.5 respectively, which are higher than the original accuracy reported in the original papers. We then place SE and CBAM right before every classiﬁer, reﬁning the ﬁnal features which are composed of up-sampled
global features and corresponding local features before the prediction, enforcing
model to adaptively select only the meaningful features. We train all the models
on the union set of VOC 2007 trainval and VOC 2012 trainval (“07+12”), and
evaluate on the VOC 2007 test set. The total number of training epochs is 250.
We use a weight decay of 0.0005 and a momentum of 0.9. In all the experiments,
the size of the input image is ﬁxed to 300 for the simplicity.
The experimental results are summarized in Table 7. We can clearly see
that CBAM improves the accuracy of all strong baselines with two backbone
networks. Note that accuracy improvement of CBAM comes with a negligible
parameter overhead, indicating that enhancement is not due to a naive capacityincrement but because of our eﬀective feature reﬁnement. In addition, the result
using the light-weight backbone network again shows that CBAM can be
an interesting method to low-end devices.
Conclusion
We have presented the convolutional bottleneck attention module (CBAM), a
new approach to improve representation power of CNN networks. We apply
attention-based feature reﬁnement with two distinctive modules, channel and
spatial, and achieve considerable performance improvement while keeping the
overhead small. For the channel attention, we suggest to use the max-pooled
features along with the average-pooled features, leading to produce ﬁner attention than SE . We further push the performance by exploiting the spatial
attention. Our ﬁnal module (CBAM) learns what and where to emphasize or
suppress and reﬁnes intermediate features eﬀectively. To verify its eﬃcacy, we
conducted extensive experiments with various state-of-the-art models and con-
ﬁrmed that CBAM outperforms all the baselines on three diﬀerent benchmark
datasets: ImageNet-1K, MS COCO, and VOC 2007. In addition, we visualize
how the module exactly infers given an input image. Interestingly, we observed
that our module induces the network to focus on target object properly. We hope
CBAM become an important component of various network architectures.
Convolutional Block Attention Module