HAL Id: inria-00323349
 
Submitted on 21 Sep 2008
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Whole-History Rating: A Bayesian Rating System for
Players of Time-Varying Strength
Rémi Coulom
To cite this version:
Rémi Coulom.
Whole-History Rating:
A Bayesian Rating System for Players of Time-Varying
Strength. Computer and Games, Sep 2008, Beijing, China. pp.113–124. ￿inria-00323349￿
Whole-History Rating: A Bayesian Rating
System for Players of Time-Varying Strength
R´emi Coulom
Universit´e Charles de Gaulle, INRIA SEQUEL, CNRS GRAPPA, Lille, France
Abstract. Whole-History Rating (WHR) is a new method to estimate
the time-varying strengths of players involved in paired comparisons. Like
many variations of the Elo rating system, the whole-history approach is
based on the dynamic Bradley-Terry model. But, instead of using incremental approximations, WHR directly computes the exact maximum a
posteriori over the whole rating history of all players. This additional accuracy comes at a higher computational cost than traditional methods,
but computation is still fast enough to be easily applied in real time to
large-scale game servers (a new game is added in less than 0.001 second).
Experiments demonstrate that, in comparison to Elo, Glicko, TrueSkill,
and decayed-history algorithms, WHR produces better predictions.
Introduction
Institutions that organize competitive activities, such as sports or games, often
rely on ratings systems. Rating systems provide an estimation of the strength of
competitors. This strength estimation makes it possible to set up more balanced
matches, motivate competitors by providing them with a measurement of their
progress, and make predictions about the outcomes of future competitions.
Almost every institution designed its own rating system, so many algorithms
exist. The following discussion summarizes the main kinds of rating systems.
Static Rating Systems. Static rating systems do not consider the variation
in time of the strengths of players. They are appropriate for rating humans
over a short period of time, or for rating computers. An eﬀective method for a
static rating system consists in using Bayesian inference with the Bradley-Terry
model . But static rating systems are not adapted to situations where players
may make signiﬁcant progress.
Incremental Rating Systems. Incremental rating systems, such as the FIDE
rating system , Glicko , or TrueSkill store a small amount of data for
each player (one number indicating strength, and sometimes another indicating uncertainty). After each game, this data is updated for the participants in
the game. The rating of the winner is increased, and the rating of the loser is
decreased.
Incremental rating systems can handle players of time-varying strength, but
do not make optimal use of data. For instance, if two players, A and B, enter the
rating system at the same time and play many games against each other, and
none against established opponents, then their relative strength will be correctly
estimated, but not their strength with respect to the other players. If player A
then plays against established opponents, and its rating changes, then the rating
of player B should change too. But incremental rating systems would leave B’s
rating unchanged.
Decayed-history Rating Systems. In order to ﬁx the deﬁciencies of incremental rating systems, a static rating algorithm may be applied, limited to recent
games. This idea may be reﬁned by giving a decaying weight to games, either
exponential or linear1. With this decay, old games are progressively forgotten,
which allows to measure the progress of players.
This decayed-history approach solves some problems of incremental rating
systems, but also has some ﬂaws. The main problem is that the decay of game
weights generates a very fast increase in the uncertainty of player ratings. This
is unlike incremental systems that assume that rating uncertainty grows like
the square root of time. With the decayed-history approach, players who stop
playing for a while may experience huge jumps in their ratings when they start
playing again, and players who play very frequently may have the feeling that
their rating is stuck. If players don’t all play at the same frequency, there is no
good way to tune the speed of the decay.
Accurate Bayesian Inference. Another approach that may be more theoretically sound than decayed history consists in using the same model as incremental
algorithms, but with less approximations. The weakness of algorithms like Glicko
and TrueSkill lies in the inaccuracies of representing the probability distribution with just one value and one variance for every player, ignoring covariance.
Authors of incremental algorithms already proposed to correct inaccuracies by
running several passes of the algorithm forward and backward in time . Edwards , with Edo ratings, proposed a method to directly estimate the
maximum a posteriori on the exact model.
The WHR algorithm presented in this paper is similar in principle to Edo
ratings, although the numerical method is diﬀerent (Edo uses MM , whereas
WHR uses the more eﬃcient Newton’s method). On his web site, Edwards wrote
that “Edo ratings are not particularly suitable to the regular updating of current
ratings”. Experiments presented in this paper clearly indicate that he underestimated his idea: evaluating ratings of the past more accurately helps to evaluate
current ratings: the prediction rate obtained with WHR outperforms decayed
history and incremental algorithms. Also, the WHR algorithm allows to rapidly
update ratings after the addition of one game, making it suitable for large-scale
real-time estimation of ratings.
1 This idea is often credited to Ken Thompson (for instance, by Sonas ), but I
could not ﬁnd a more precise reference
Whole-History Ratings
Paper Outline. Section 2 presents the dynamic Bradley-Terry model, Section 3
is the WHR algorithm, and Section 4 presents experiment results on data of the
KGS Go server.
The Dynamic Bradley-Terry Model
This section brieﬂy presents the dynamic Bradley-Terry model that is the
basis for the WHR system.
– Player number: i ∈{1, . . . , N}, integer index
– Elo rating of player i at time t: Ri(t), real number.
– γ rating of player i at time t: γi(t), deﬁned by γi(t) = 10
– Natural rating of player i at time t: ri(t) = ln γi(t) = Ri(t) ln 10
Elo ratings are familiar to chess players, but are on a rather arbitrary and
inconvenient scale. Natural ratings will be used most of the time in this paper,
because they make calculations easier.
Bradley-Terry Model
The Bradley-Terry model for paired comparisons gives the probability of winning
a game as a function of ratings:
P(player i beats player j at time t) =
γi(t) + γj(t) .
The Bradley-Terry model may be generalized to handle draws, advantage of
playing ﬁrst, teams, and multi-player games . In this paper, only the simple
formulation will be used, but it would be straightforward to generalize the WHR
algorithm to those more complex situations.
Bayesian Inference
The principle of Bayesian Inference consists in computing a probability distribution over player ratings (γ) from the observation of game results (G) by inverting
the model thanks to Bayes formula:
p(γ|G) = P(G|γ)p(γ)
In this formula, p(γ) is a prior distribution over γ (lower-case p is used
for probability densities), and P(G) is a normalizing constant. P(G|γ) is the
Bradley-Terry model described in the previous section. p(γ|G) is called the posterior distribution of γ. The value of γ that maximizes p(γ|G) is the maximum a
posteriori, and may be used as an estimation of the strengths of players, derived
from the observation of their game results.
In the dynamic Bradley-Terry model, the prior has two roles. First, a prior
probability distribution over the range of ratings is applied. This way, the rating
of a player with 100% wins does not go to inﬁnity. Also, a prior controls the
variation of the ratings in time, to avoid huge jumps in ratings.
In the dynamic Bradley-Terry model, the prior that controls the variation of
ratings in time is a Wiener process:
ri(t2) −ri(t1) ∼N
 0, |t2 −t1|w2
w is a parameter of the model, that indicates the variability of ratings in time.
The extreme case of w = 0 would mean static ratings.
Some realizations of a Wiener process are plotted on Figure 1. The Wiener
process is a model for Brownian motion, and can be simulated by adding an
independent normally-distributed random value at each time step. This means
that the variance increases linearly with time, so the conﬁdence interval grows
like the square root of time. A Wiener process is said to be memoryless (or
Markovian), that is to say, if t1 < t2 < t3,
r(t1), r(t2)
r(t2), r(t3)
Fig. 1. Three realizations of a Wiener process, with r(0) = 0. The dashed line indicates
the 95% conﬁdence interval for p(r(t)|r(0) = 0).
The WHR algorithm consists in computing, for each player, the γ(t) function
that maximizes p(γ|G). Once this maximum a posteriori has been computed,
the variance around this maximum is also estimated, which is a way to estimate
rating uncertainty.
Whole-History Ratings
(a) History of a player
(b) optimizing one by one
(c) Newton’s method
Fig. 2. Newton’s method applied to the history of one player. (a) The rating history
of a player who has played 4 games is deﬁned by 4 ratings r1, r2, r3, and r4, at the
four times of the four games. (b) Two ratings that are close in time, such as r2 and r3,
are strongly correlated. So, methods such as MM that optimize parameters one by
one, are very ineﬃcient. (c) Since the optimized function is very similar to a quadratic
form, Newton’s method is extremely eﬃcient.
Optimization Method
The ﬁrst step of the WHR algorithm consists in computing the maximum a
posteriori for all the γi(t). Since γi are functions of time, this is an inﬁnitedimensional optimization problem. But it is easy to reduce it to a ﬁnite-dimensional
problem, since knowing the values of γ at the times of games is enough. Rating estimation between two consecutive games may be done with interpolation
formulas provided in Appendix C.
Figure 2 illustrates how optimization is performed with Newton’s method.
Since the Wiener process is Markovian, the Hessian matrix
  ∂2 log p
is tridiagonal, so Newton’s method has a cost linear in the number of ratings. Formally,
Newton’s method consists in updating the rating vector r of one player (the
vector of ratings at times when that player played a game) according to this
−1 ∂log p
The complete details of how to perform this update are provided in Appendices
A and B. Since p is very similar to a Gaussian, log p is very similar to a quadratic
form, so only one iteration of Newton’s method is enough to get a very good
value of r. The overall optimization algorithm consists in applying this Newton
update to every player in turn, and iterate until convergence.
Incremental Updates
The global optimization algorithm described in the previous section may take
a few minutes to converge on a big database of games (see Section 4). So, it
may be too slow to restart the algorithm from scratch in order to estimate new
ratings when one new game is added to the database.
In order to let WHR work in real time when one new game is added, a simple
solution consists in keeping the rating estimations obtained before the addition,
and applying Newton’s method once to every player of this game. This is orders
of magnitude faster, although a little less accurate. If computation time allows,
more iterations of Newton’s method may be applied from time to time. For
instance, in experiments of incremental WHR described in the next section, one
iteration was run on every player every 1000 games.
Estimating Rating Uncertainty
Once the maximum a posteriori has been found, rating uncertainty may be
estimated. Since the posterior probability is similar to a Gaussian, its covariance
matrix may be approximated by the opposite of the inverse of the Hessian.
In practice it is not possible to compute the whole covariance matrix for all
the parameters at the same time. But rating uncertainty of one player can be
estimated by considering the Hessian of the ratings of this player only, assuming
opponents have a ﬁxed rating equal to the maximum a posteriori. The detailed
formulas for this operation can be found in Appendix B.2.
This numerical algorithm is extremely similar to a technique developed by
physicists to estimate functions from noisy observations .
Experiments in the Game of Go
Speed of Convergence
The WHR algorithm was tested on the database of games of the KGS Go server.
This database contains all rated games since 2000 and until October, 2007. It
contains 213,426 players, and 10.8 million games. Computations were performed
on an Intel Core2 Duo at 2.4 GHz (using only one thread), and took about 7
minutes for 200 iterations. The prior was one virtual win and one virtual loss
against a virtual player of rating zero, on the day of the ﬁrst game. The Wiener
process had a variance of w2 = 60 Elo2 per day.
Figure 3 shows the speed of converge of the log-likelihood. Figure 4 shows the
result of player Crazy Stone, who made a lot of progress in year 2007. This ﬁgure
shows that rating uncertainty increases slowly during long periods of inactivity.
Prediction Ability
The prediction ability of WHR was compared experimentally with the basic
Elo algorithm , TrueSkill , Glicko , Bayeselo , and decayed history.
Bayeselo may be considered as a special case of WHR, with w2 = 0, or a special case of decayed history, with an inﬁnitely long decay. Decayed history was
implemented with an exponential decay, that is to say each game was weighted
with a coeﬃcient e(t−t0)/τ, where τ is a parameter of the algorithm.
Bayeselo, WHR and decayed history all used the same scheme for incremental
update of ratings: after each addition of a game, the ratings of the two participants were optimized with one step of Newton’s method. Before each prediction
of the outcome of a game, the ratings of the two participants were optimized,
too. Every 1000 games, the ratings of all participants were optimized, one by
Whole-History Ratings
Fig. 3. Speed of optimization. One point is plotted every 10 iterations. Log-likelihood
is the diﬀerence between the current log-likelihood and the initial log-likelihood, when
all ratings were set to zero.
Fig. 4. Whole-History Rating estimation of player CrazyStone. Dots indicate days
when games were played.
The method to compare algorithms consisted in measuring their prediction
rates over a database of games. The prediction rate is the proportion of games
whose most likely winner was predicted correctly (when two players have the
same rating, this counts as 0.5 correct prediction). Parameters of the algorithm
were ﬁrst tuned to optimize prediction rate over a training database, then prediction rate was measured on a diﬀerent test database.
The training set was made of the 726,648 rated even games with komi 6.5
played on KGS between 2000-11-07 and 2005-05-20. The test set consisted of
the 2,331,757 rated even games with komi 6.5 played between 2005-05-21 and
2007-10-01. The time resolution of the database is one day, so if a player played
several games in one day, they were considered as simultaneous.
Results are summarized in Table 1. It shows that WHR signiﬁcantly outperforms the other algorithms. Algorithms that remember all the game results
outperform the fast incremental methods.
Table 1. Prediction performance of some rating algorithms. Prior = 1 means one
virtual win and one virtual loss against a player of rating zero. 95% conﬁdence of
superiority is obtained with a diﬀerence in prediction rate of 0.163% in the training set,
and 0.091% in the test set. Since the same data was used to measure the performances of
all algorithms, there is some variance reduction, so diﬀerences may be more signiﬁcant.
Time was measured on the training set.
Time Training
Optimal parameters
0.41 s 56.001% 55.121% k = 20
0.73 s 56.184% 55.522% σ0 = 150 Elo, w2 = 20 Elo2/day
0.40 s 56.212% 55.536% β2 = 1, σ2
0 = 0.5, w2 = 0.000975/game
88.66 s 56.216% 55.671% prior = 1
Decayed history
89.86 s 56.260% 55.698% prior = 1, τ = 400 days
252.00 s 56.356% 55.793% prior = 1.2, w2 = 14 Elo2/day
Performance on the test set is inferior to performance on the training set.
This probably cannot be explained by overﬁtting alone. Because these two sets
of games correspond to diﬀerent periods of time, they don’t have the same statistical properties. It may be that the KGS rating system improved, and since
matches are automatically balanced by the server, recent games are more balanced, so they are more diﬃcult to predict.
A remarkable aspect of these results is that parameters that optimize prediction rate give a very low variance to the Wiener process. The static Bayeselo
algorithm even outperformed incremental algorithms on the test set. This is surprising, because many players on KGS are beginners, and made very big progress
during the 2.5 years of the test set. The high number of experienced players probably outweighed beginners. This is, by the way, an important inaccuracy in the
dynamic Bradley Terry model: it does not take those diﬀerent abilities to make
progress into consideration.
Conclusion
WHR is a new rating algorithm that directly computes player ratings as a function of time. It is computationally more costly than incremental and decayedhistory algorithms, but more accurate, and fast enough to be applied in real time
to large-scale game servers.
A research direction would be to apply WHR to more data sets, and compare
it empirically to more alternative rating algorithms, such as Edo, and TrueSkill
Through Time (TTT). It is likely that WHR would not outperform Edo and
TTT in terms of prediction rate, since their models are almost identical. But
the main diﬀerence may be in computation time. Previous experiments with Edo
and TTT were run with a time resolution of one year, whereas WHR operates
with a time resolution of one day. Such a short time period between ratings
induces a very strong correlation between parameters, which Newton’s method
may handle more eﬃciently than MM or approximate message passing.
Whole-History Ratings
Another research direction would be to improve the model. An eﬃcient application of WHR to Go data would require some reﬁnements of the dynamic
Bradley-Terry model, that the KGS rating algorithm already has. In particular, it should be able to
– take handicap, komi, and time control into consideration,
– deal with outliers,
– handle the fact that beginners make faster progress than experts.
Acknowledgments
I thank William Shubert for providing KGS data. I am also very grateful to the
reviewers, whose remarks helped to improve this paper a lot.