Low-Dose X-ray CT Reconstruction via Dictionary Learning
Institute of Image Processing and Pattern Recognition, Xi’an Jiaotong University, Xi’an, Shaanxi
710049, China, and also with the Biomedical Imaging Division, VT-WFU School of Biomedical
Engineering and Sciences, Wake Forest University Health Sciences, Winston-Salem, NC 27157
Hengyong Yu [Senior Member, IEEE],
Biomedical Imaging Division, VT-WFU School of Biomedical Engineering and Sciences, and the
Department of Radiology, Division of Radiologic Sciences, Wake Forest University Health
Sciences, Winston-Salem, NC 27157 USA
Xuanqin Mou,
Institute of Image Processing and Pattern Recognition, Xi’an Jiaotong University, Xi’an, Shaanxi
710049, China
Lei Zhang [Member, IEEE],
Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong,
Jiang Hsieh [Senior Member, IEEE], and
GE Healthcare Technology, Waukesha, WI 53188 USA
Ge Wang [Fellow, IEEE]
Biomedical Imaging Division, VT-WFU School of Biomedical Engineering and Sciences, Virginia
Tech, Blacksburg, VA 24061 USA, and also with the Wake Forest University Health Sciences,
Winston-Salem, NC 27157 USA
Qiong Xu: ; Hengyong Yu: ; Xuanqin Mou: ; Lei
Zhang: cslzhang@comp. polyu.edu.hk; Jiang Hsieh: ; Ge Wang: 
Although diagnostic medical imaging provides enormous benefits in the early detection and
accuracy diagnosis of various diseases, there are growing concerns on the potential side effect of
radiation induced genetic, cancerous and other diseases. How to reduce radiation dose while
maintaining the diagnostic performance is a major challenge in the computed tomography (CT)
field. Inspired by the compressive sensing theory, the sparse constraint in terms of total variation
(TV) minimization has already led to promising results for low-dose CT reconstruction. Compared
to the discrete gradient transform used in the TV method, dictionary learning is proven to be an
effective way for sparse representation. On the other hand, it is important to consider the statistical
property of projection data in the low-dose CT case. Recently, we have developed a dictionary
learning based approach for low-dose X-ray CT. In this paper, we present this method in detail
and evaluate it in experiments. In our method, the sparse constraint in terms of a redundant
dictionary is incorporated into an objective function in a statistical iterative reconstruction
framework. The dictionary can be either predetermined before an image reconstruction task or
adaptively defined during the reconstruction process. An alternating minimization scheme is
developed to minimize the objective function. Our approach is evaluated with low-dose X-ray
© 2012 IEEE
Correspondence to: Hengyong Yu, ; Xuanqin Mou, .
NIH Public Access
Author Manuscript
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
 
IEEE Trans Med Imaging. 2012 September ; 31(9): 1682–1697. doi:10.1109/TMI.2012.2195669.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
projections collected in animal and human CT studies, and the improvement associated with
dictionary learning is quantified relative to filtered backprojection and TV-based reconstructions.
The results show that the proposed approach might produce better images with lower noise and
more detailed structural features in our selected cases. However, there is no proof that this is true
for all kinds of structures.
Index Terms
Compressive sensing (CS); computed tomography (CT); dictionary learning; low-dose CT; sparse
representation; statistical iterative reconstruction
I. Introduction
Nowadays, X-ray computed tomography (CT) is widely used in hospitals and clinics for
diagnosis and intervention. It is well known that X-ray radiation can be harmful which may
induce genetic, cancerous, and other diseases – . Therefore, the radiation risk issue is
receiving more and more attention. As a result, the well-known ALARA (as low as
reasonably achievable) principle is applied to avoid excessive radiation dose in the medical
community. Since X-ray imaging is a quantum accumulation process, the signal-to-noise
ratio (SNR) depends on the X-ray dose quadratically. Given other conditions being identical,
reducing the X-ray dose will degrade image quality. Consequently, how to reconstruct
adequate CT images at a minimum radiation dose level is a hot topic in the CT field.
There are two strategies for radiation dose reduction: the first one is to reduce the X-ray flux
towards each detector element, and the second one is to decrease the number of X-ray
attenuation measurements across a whole object to be reconstructed. The former is usually
implemented by adjusting the operating current, the operating potential and exposure time of
an X-ray tube, leading to noisy projections. The latter necessarily produces insufficient
projection data, suffering from few-view, limited-angle, interior scan, or other problems.
These problems can co-exist in one dataset, representing a major opportunity for algorithmic
To reconstruct images from noisy projections, various reconstruction algorithms were
proposed. These algorithms can be categorized into two classes in terms of the variables
used in the optimization process.
The first class focuses on preprocessing projection data before image reconstruction. The
optimization variables are projection data. Hsieh proposed an adaptive filtering approach
where the filter parameterization was adjusted according to the noise property .
Kachelriess et al. developed a multi-dimensional adaptive filtering approach to refine
projection data along the detector row, view, and longitudinal directions . La Riviere
developed a penalized likelihood technique to smooth a sinogram . Wang et al. presented
a penalized weighted least-squares approach to reduce sinogram noise .
Different from these preprocessing methods, the second class is referred to as statistical
iterative reconstruction (SIR). It optimizes the maximum-likelihood or penalized-likelihood
function formulated according to the statistical characteristics of projection data. Its
optimization variables are image pixels or voxels. Kamphues and Beekman proposed an
ordered-subsets convex algorithm . Nuyts etal. studied a gradient ascent algorithm .
Erdogan and Fessler developed a monotonic algorithm based on separable paraboloidal
surrogate . Recently, Thibault et al. applied the SIR schemeto real multi-slice helical
data . These results show that the statistical framework promises an optimal
reconstruction quality from noisy projection data.
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Compressive sensing (CS) theory has now become popular, and has been instrumental for
image reconstruction from incomplete and noisy datasets. CS theory allows a sparse signal
to be accurately reconstructed from samples far less than what is required by the Shannon/
Nyquist sampling theorem , . The key for the success of CS is the sparsity of a
signal under study. Although an object is not sparse in general, often times a sparsifying
transform can be used to convert it into a domain in which the signal has a sparse
representation. One common sparsifying transform is the discrete gradient transform (DGT)
whose coefficients can be summed up to form the so-called total variation (TV). Inspired by
CS theory, various TV minimization algorithms were suggested to solve the few-view,
limited-angle, and interior problems. For example, Chen et al. proposed a prior image
constrained compressed sensing (PICCS) algorithm for dynamic CT application . Yu
and Wang proved that a piecewise constant interior region of interest (ROI ) can be uniquely
reconstructed by a TV minimizing technique , . Xu et al. extended this CS based
interior tomography formulation into a SIR framework . Ritschl et al. proposed an
improved TV method within the ASD-POCS framework for clinical applications .
Although these TV-based algorithms are successful in a number of cases, the power of the
TV minimization constraint is still limited. First, the TV constraint is a global requirement,
which cannot directly reflect structures of an object. Second, the DGT operation cannot
distinguish true structures and image noise. Consequently, images reconstructed with the TV
constraint may lose some fine features and generate a blocky appearance in incomplete and
noisy cases. Hence, it is necessary to investigate superior sparsifying methods for CSinspired image reconstruction.
Recently, the sparse representation in terms of a redundant dictionary has attracted an
increasing interest in the image processing, imaging analysis and magnetic resonance
imaging (MRI) fields. Such a dictionary is an over-complete basis. The elements in this
basis are called atoms, which are learned from application-specific training images. Then, an
object image can be sparsely represented as a linear combination of these atoms. Usually, an
object image is decomposed into small overlapped patches. The dictionary learning
approach acts on these patches, and an average of the corresponding values in the
overlapped patches is computed at a given location. Since the dictionary is learned from
training images, it is expected to have a better sparsifying capability than any generic sparse
transform. Also, the redundancy of the atoms facilitates a sparser representation. More
importantly, the dictionary tends to capture local image features effectively because of the
patch-based analysis and most importantly the structural self-similarity in many cases.
Given the aforementioned advantages, dictionary learning based techniques have led to
excellent results in image/video denoising, inpainting, restoration, face recognition, texture
classification, MRI, and other areas. In 2006, Elad and Aharon addressed the image
denoising problem using a dictionary learned using the K-SVD algorithm . Mairal et al.
extended this method for color image restoration . Then, Mairal et al. generalized it into
a multi-scale framework for image and video restoration . Wright et al. used the
dictionary learning based sparse representation in human face recognition . As far as
medical imaging is concerned, the dictionary learning based work just began to emerge,
initially for MRI. In 2010, Chen et al. combined a dictionary learning method and a TVbased MRI scheme to improve image quality . In 2011, Ravishankar and Bresler
proposed an adaptive dictionary learning based method to reconstruct MRI images from
highly under-sampled k-space data .
It is our hypothesis that the dictionary learning technique can be used for low-dose CT
reconstruction. Recently, we proposed a dictionary learning based reconstruction method to
address the low-dose CT reconstruction problem . Our method consists of two
components. The first component is the SIR process that enforces the statistical knowledge.
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
The second component is the dictionary-based sparsification whose role is similar to the TV
minimization . In our work, two types of dictionaries are used. The first type is a global
dictionary learned from a pre-specified set of training images, and it will not change during
the reconstruction process. The second type is an adaptive dictionary learned from an
intermediate image and updated iteratively in the reconstruction process.
The rest of this paper is organized as follows. In Section II, we will introduce the
background on dictionary learning and SIR. In Section III, we will describe our
reconstruction approach and its implementation. In Section IV, we will report representative
results from preclinical and clinical low-dose projections, and quantify the performance of
our proposed methodology. Finally, in Section V we will discuss relevant problems and
conclude the paper.
II. Background
A. Dictionary Learning and Sparse Representation
Let N and K be integers, and ℝ be the real space. A dictionary is a matrix D ∈ ℝN×K whose
column dk ∈ ℝ N×1 (k = 1, …, K) is a N dimensional vector, which is called an atom.
Usually, the dictionary is redundant or over-complete; that is, N ≪ K. An image patch of
pixels can be expressed as a N dimensional vector x ∈ ℝN×1. Suppose that a patch
x can be exactly or approximately represented as a sparse linear combination of the atoms in
the dictionary D; that is
where ε ≥ 0 is a small error bound, and the representation vector α ∈ ℝK×1 has few nonzero
entries, ||α||0 ≪ N ≪ K with ||·||0 being the l0-norm. The dictionary redundancy implies that
the number of atoms is larger than the length of an atom.
Finding a sparse representation α ∈ ℝK×1 of an image patch x ∈ ℝN×1 with respect to a
given dictionary D ∈ ℝN×K is equivalent to solve the following optimization problem:
By the Lagrange method, (2) can be rewritten in an unconstraint form
where ν is the Lagrange multiplier. It is pointed out that the above two problems are
equivalent when a suitable ν is chosen. Since solving (2) or (3) directly is NP-hard, an
approximate alternative strategy is desirable. In this regard, there are several greedy
algorithms available, such as matching pursuit (MP) and orthogonal matching pursuit
(OMP) , algorithms. Also, the l0-norm can be replaced by the l1-norm to make the
problem convex and manageable using a basis pursuit (BP) algorithm.
Given a training set of S patches, the so-called dictionary learning is to seek a dictionary that
makes each patch in the training set be sparsely represented by the atoms in this dictionary.
Denote the given patch set as a matrix X ∈ ℝN×S with a patch xs ∈ ℝN×1 (s = 1, …, S) being
a column vector of X, and the corresponding sparse representation vector as a matrix α ∈
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
ℝK×S with the representation αs ∈ ℝK×1 of a patch being a column vector of α ∈ ℝK×S.
Then, the dictionary learning is to solve
Equation (4) is basically equivalent to either of the following problems:
where L0 and ε are the sparsity and precision of the sparse representation, respectively. The
l0-norm in (4)–(6) can be replaced by the l1-norm to make them easier to solve. There are
many algorithms available for dictionary learning, including the classical K-SVD method
 and the fast online learning technique .
A successful application of the dictionary learning technique is image denoising . Let a
vector z ∈ ℝM×1 represent a noise image of H × W pixels and a vector x ∈ ℝM×1 denote its
corresponding filtered version, M = H × W. A set of small overlapping patches can be
extracted from the image. With a sliding distance of one pixel, we will have
patches. It is assumed that the patches extracted from the filtered
image can be sparely represented in terms of a dictionary, and the filtered image should be
close to the original noisy image. Hence, the denoising procedure is to minimize the
following objective function:
where Es ∈ ℝN×M is the matrix to extract a patch from the image x,
, and λ is a regularization parameter related to the noise level
of z. The dictionary D in (7) can be determined in two ways. One way is to predetermine it
from a training set, which should contain representative structures in the image to be
filtered. The other way is to construct the dictionary during the denoising procedure.
B. Statistical Reconstruction and Objective Function
Without loss of generality, we only assume a monochromatic source. Approximately,
measured data follow the Poisson distribution:
where yi is the measurement along the ith X-ray path, and bi is the blank scan factor,
is the integral of the X-ray linear attenuation coefficients, A = {aij} is
the system matrix, μ = (μ1, … μJ)T is a linear attenuation coefficient distribution, ri accounts
for read-out noise, I and J are the number of projections and pixels, respectively.
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Assuming that the noise distributions along different paths are statistically independent, the
Poisson log-likelihood function of the joint probability distribution can be written as 
where P (·) is the probability function, and ȳi = bie−li + ri is the expected value of yi.
Ignoring the constant terms, we obtain
From the statistical perspective, an image can be reconstructed by maximizing a posteriori
(MAP) of the function P(μ|y). According to the Bayesian rule P(μ|y) = P(y|μ) P(μ)/P(y) and
the monotonic increment property of the natural logarithm, the reconstruction is equivalent
to maximize the following objective function:
where ln P(μ) is a regularization term based on the prior knowledge. Let R(μ) = − ln P(μ),
the task can be converted into minimizing the following objective function:
Performing a second-order Taylor expansion of gi (l) = (bie−l + ri) − yi ln (bie−l + ri) with
respect to an estimated line integral l̂i = ln(bi/(yi − ri)) , (11) becomes
where wi = (yi − ri)2/yi is the statistical weight for each X-ray path.
While the simultaneous algebraic iterative technique (SART) minimizes a least square
function, SIR deals with a statistically weighted least square function defined by (12). The
statistical weight represents the confidence of the projection measurement along each path.
The projection data through denser paths would have lower signal-to-noise-ratios (SNR).
While methods were developed based on physical experiments to calculate statistical
weights , , , in this feasibility study we use the Poisson monochromatic model for
SIR. For practice applications, the measured realistic data may not fully satisfy the Poisson
model. The compound Poisson model may be more accurate . However, the Poisson
model is well accepted in the CT field .
III. Methodology
A. GDSIR and ADSIR
In Section II-B the regularization term R (μ) in (12) represents prior information on
reconstructed images. Various assumptions about the prior information lead to different
reconstruction algorithms. For example, the assumption of smooth variation over adjacent
pixels suggests a regularization in terms of quadratic differences between adjacent pixels.
The piecewise constant assumption supports a TV regularization. A more general form of
regularization is the q-generalized Gaussian Markov field (q-GGMRF) prior , which has
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
two adjustable parameters. The quadratic and TV regularization functions are special cases
of q-GGMRF. Because all these regularization means are based on the relationship of
adjacent pixels, it is difficult for them to distinguish weak structures and strong noise.
On the other hand, the dictionary learning and sparse representation techniques perform well
in sensing structures and suppressing noise. Here, we propose to use the sparsity constraint
in terms of a redundant dictionary as the regularization term of (12). In reference to (7), we
have the following minimization problem:
is an operator to extract patches from an image.
Similar to the comments we made on (7), the dictionary D in (13) can be either
predetermined or dynamically defined. According to these two options, we can perform
either global dictionary based statistical iterative reconstruction (GDSIR) or adaptive
dictionary based statistical iterative reconstruction (ADSIR). Because structures in various
subjects are quite similar in a given application, a training set of patches for construction of
a global dictionary can be extracted from images of similar objects. On the other hand, the
training set of patches for construction of an adaptive dictionary can be extracted from an
intermediate image and dynamically updated during the reconstruction process.
Algorithm Description—For GDSIR, the image reconstruction process is equivalent to
solve the following optimization problem:
where there are two variables μ and α. An alternating minimization scheme can be used to
optimize the two variables. First, an intermediate reconstructed image μ with a fixed sparse
representation α̃ is updated to reduce the data discrepancy. Thus, the objective function (14)
By the separable paraboloid surrogate method , (15) can be iteratively solved as
where the superscript t = 1,2, …, T is the iteration index. Second, the intermediate image μt
is re-expressed in terms of the dictionary for a better sparse representation. Thus, the
objective function (14) becomes
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
which is a sparse representation problem. As an example, the OMP algorithm can be used to
find the sparse representation of each patch. The above two steps are alternatingly
performed until a stopping criterion is satisfied.
For ADSIR, the image reconstruction process is equivalent to solve the following
optimization problem:
where there are three variables μ, D, and α. Similar to GDSIR, an alternating minimization
scheme is used to optimize the three variables. First, an intermediate reconstructed image μ
is updated to reduce the data discrepancy with a fixed sparse representation α̃ and a current
dictionary D̃. This step is exactly the same as that for GDSIR expressed by (16). Second,
keeping the intermediate image μt unchanged, D and α are estimated by
Equation (19) is the generic dictionary learning and sparse representation problem, which
can be solved with respect to α and D alternatingly using the classic K-SVD algorithm. For
fast convergence, we first use the fast online algorithm , to tra in a dictionary from
the patches extracted from an intermediate image μt. Then, we fix the dictionary to up date
the sparse representation as in (17) using the OMP algorithm. The above procedures are
alternatingly performed until a stopping criterion is satisfied.
B. Parameter Selection
Similar to other regularized iterative reconstruction algorithms, the regularization parameter
λ is to balance the data fidelity and prior information terms. The final reconstruction
depends on the parameter λ. It is an interesting problem how to choose an optimal
parameter. Usually, it is empirically selected in practice. For GDSIR and ADSIR, we can
also empirically select λ. Because the data fidelity term is proportional to the noise standard
deviation in the projection domain, λ should be increased with the noise increment.
Specifically, our parameter selection problem can be reformulated as follows. After some
variable exchanges and constant additions, we can rewrite (15) as
where B = wA, L = wl̂,
, l̂ = (l̂1, …, l̂I)T, μ̃ = {(1/N)Σs∈ΩjFsjDα̃s}, N is the
number of pixels in a patch, Ωj denotes the set of patches covering a pixel j, and Fsj ∈ ℝ1×N
extracts a pixel j from a patch s. Note that in (19) we can discard the pixels in margins and
only consider the pixels in the intersection between the rows from
columns from
for an H × W image. Let ξ = μ − μ̃ and L̃ = L − Bμ̃. Then,
(20) becomes
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Equation (21) is a classical regularization problem, which can be solved using existing
methods , such as the Miller method and the generalized cross-validation method
 . Nevertheless, λ is empirically selected in this feasibility study.
The dictionary redundancy improves the sparsity of representation. To ensure the
redundancy, the number of atoms in a dictionary should be much greater than that of pixels
in a patch, that means K ≫ N. In the image processing field, K = 4N is a conventional
choice. On the one hand, a larger patch size corresponds to a larger number of atoms in a
dictionary, which would increase the computational cost. On the other hand, if the patch size
is too small, it could not effectively catch features in an image. In , it was pointed out
that there was no significant difference between the results with N = 8×8, K = 256 and N =
16×16, K = 1024, and a larger patch size may lead to an over-smoothed image. Hence, N =
8×8, K = 256, are used in this paper. In the dictionary learning process, we solve the
optimization problem (5) by minimizing the representation error with a fixed sparsity level
. The sparsity level is selected as 5~10 atoms. The aforementioned parameter selection for
has been applied in many applications , , and also worked well in our
experiments to be described below.
Besides, there are two parameters in the sparse representation step of image reconstruction,
which are the sparsity level
and the precision level ε. The OMP process will stop when
. The sparsity level
is the number of atoms involved
in representing a patch, which is empirically determined according to the complexity of an
image to be reconstructed and the property of the dictionary. Usually,
is greater than or
equal to the
in dictionary learning, and smaller than N/2 for sparsity. The precision level
ε represents the tolerance of the difference between the reconstructed and original images,
which is determined by the image noise level and the property of the dictionary. The
stronger the noise is, the greater ε is. We can estimate the image noise level over a flat
region in a reconstructed image.
Based on the above discussions, the parameter selection guidelines are summarized in Table
C. Monotonic Decrement of the Objective Function
Since the objective function is monotonically minimized in each part of the alternating
minimization process, the value of the objective function is also monotonically reduced
using the proposed algorithms.
In the GDSIR case, let μt denote an intermediate image and αt the sparse representation after
t (t = 1,2, …, T) iterations. Since the updating formula (16) monotonically minimizes the
objective function (15) , we have
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Then, in the sparse representation step using the OMP method, the following problem will
be optimized:
Clearly, no matter
or not, the solution of (23) satisfies
Therefore, we have
which shows that GDSIR monotonically decreases its objective function. In the ADSIR
case, we can follow similar steps to reach the same conclusion.
The above-mentioned monotonic decrement in the objective function value does not mean
the convergence of the reconstruction process for neither GDSIR nor ADSIR. The
convergence of GDSIR or ADSIR is much more difficult to prove, and is considered beyond
the scope of this paper. However, our numerical and experimental studies to be reported
below seem suggesting the convergence of our GDSIR and ADSIR algorithms.
D. Stopping Criterion and Computational Cost
In practice, we can simply stop the reconstruction process after a number of iterations when
the change of the objective function or the reconstructed image becomes very small. This
stopping rule has been widely used in the iterative reconstruction community. As far as the
computational cost is concerned, after image updating GDSIR needs one OMP step for
sparse representation, and ADSIR not only needs one OMP step but also one dictionary
learning step. The computational cost of OMP and dictionary learning depends on the sparse
representation level, precision level, patch size, dictionary size, etc. The computational cost
of the OMP step is usually less than that for image updating which involves forward and
backward projections. Therefore, the computational cost of GDSIR is comparable to other
iterative methods. In this work, the online dictionary learning method was used , ,
which is significantly faster than other dictionary learning methods. As a result, the
computational time of ADSIR is comparable to that of GDSIR.
E. Overall Work flow
The specific flowcharts for GDSIR and ADSIR are presented in Tables II and III,
respectively.
IV. Experiments
Low-dose raw projections from sheep lung perfusion and patient cardiac angiography
studies were used to evaluate and validate our proposed algorithms. Two numerical
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
simulation studies were designed to evaluate the performance of the proposed algorithms.
Also, the popular TV regularization method and a non-statistical version of GDSIR were
compared with GDSIR and ADSIR. Furthermore, low-count and few-view datasets were
used to demonstrate the potential of the proposed techniques.
A. Sheep Lung Study
1) Data Acquisition—In a sheep lung perfusion study, an anesthetized sheep was scanned
at normal and low doses respectively on a SIEMENS Somatom Sensation 64-slice CT
scanner in a circular cone-beam scanning mode. A scan protocol was developed for lowdose studies with ECG gating: time point 1 for a normal X-ray dose scan (100kV/150 mAs)
before a contrast agent injection, and time points 2–21 for low-dose scans (80 kV/17 mAs)
after the contrast agent injection. All the sinograms of the central slice were extracted, which
were in a fan-beam geometry. The radius of the trajectory was 57 cm. Over a 360° range,
1160 projections were uniformly collected. For each projection, 672 detector elements were
equi-angularly distributed defining a field-of-view (FOV) of 25.05 cm in radius. In this
experiment, the reconstructed images were matrixes of 768×768 pixels covering a
43.63×43.63 cm2 region. The sparsity constraint was enforced on the entire lung region of
500×370 pixels.
2) Global Dictionary Learning—In this study, a baseline image was reconstructed from
the normal-dose sinogram using the filtered backprojection (FBP) algorithm to construct a
global dictionary, and the performance of the proposed techniques was evaluated with other
low-dose sinograms. Because of the discrepancy in the normal and low-dose X-ray kVp
settings and injection of the contrast agent, the attenuation maps of the low-dose images
were quite different from that of the baseline image. Also, the physiological motion of the
sheep most likely introduced structural differences. As such, this group of sinograms
actually offers a challenging opportunity to evaluate the robustness of GDSIR.
First, a set of overlapping patches were extracted from the lung region in the baseline image.
The patch size was of 8×8 pixels. The patches with very small variance were removed from
the extracted patch set. The direct current (dc) component was removed from each patch.
Then, a global dictionary of 256 atoms was constructed using the online dictionary learning
method with a fixed sparsity level
 . The lung region and the final dictionary are
shown in Fig. 1. Finally, a dc atom was added to the dictionary.
3) Low-Dose Results—For comparison, five reconstruction techniques were applied to
the aforementioned low-dose sinograms. As the benchmark, low dose images were
reconstructed using the FBP method. The corresponding reconstructions using the other four
reconstruction techniques were described as follows.
First, the GDSIR algorithm with a prelearned global dictionary (Section IV-A2) was
employed to reconstruct low-dose images with the following empirical parameters: λ = 0.04,
ε = 2.5 × 10−5, and
. The initial image was from the FBP method. An ordered-subset
strategy was used . The number of subsets was 40. The iterative process was stopped after
50 iterations.
Then, the ADSIR algorithm was tested with the same low-dose sinograms. In each iteration,
the dictionary was learned in real-time from the set of patches extracted from an
intermediate image. The parameters for dictionary learning were the same as those in
Section IV-A2. Because there was strong noise in an intermediate image, the atoms in this
dictionary are noisy. Therefore, the error control item for ADSIR was made smaller than
that for GDSIR in which the dictionary was learned from the normal-dose image. On the
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
other hand, since the dictionary was learned from the reconstructed image itself, there was
no need to use many atoms to capture the structures. The sparsity level parameter
made much smaller than that for GDSIR.
Taking all these factors into account, the parameter were empirically chosen as λ = 0.04, ε =
1 × 10−5, and
. The number of subsets was again 40. The iterative process was also
stopped after 50 iterations.
Third, the popular TV regularization algorithm was included to demonstrate the merits of
the proposed methods. For that purpose, the TV minimization constraint wasused as the
regularization term in (12), and enforced using the soft-threshold filtering based alternating
minimization algorithm , . We denote this method as TVSIR.
Fourth, to evaluate the effect of the statistical reconstruction technique in this dictionary
learning based reconstruction framework, we replaced the log-likelihood term
in (13) with an unweighted l2-norm data fidelity term
The global dictionary based algorithm was modified with this constant weighting scheme,
which we denote as GDNSIR. The adaptive dictionary based algorithm can be modified in a
similar way. The regularization parameter was empirically set to λ = 0.35 given the weight
change of the data fidelity term in the objective function.
The results from a representative low-dose sinogram are in Fig. 2. It can be seen that there is
strong noise in the FBP reconstruction, and streak artifacts along high attenuation structures,
such as around bones. This kind of streak artifacts can be easily identified from the
difference between the FBP image and the results with the SIR methods. The dictionary
learning based algorithms generally performed well with low-dose data. While GDSIR did
better in preserving structures and suppressing noise, ADSIR kepts lightly more structures
than GDSIR (see the region indicated by the arrow “A”). ADSIR generated a little less
uniformity than GDSIR in the whole image (see the region indicated by the arrow “B”), and
some edges were obscurer than those with GDSIR. The performance of GDNSIR was not
much different from that of GDSIR. However, there were some streak artifacts with
GDNSIR as in the FBP reconstruction (see the difference from the FBP image), especially
around the bone (see the region indicated by the arrow “C”). The image reconstructed by
TVSIR had much less noise than the FBP result, but it was a little blocky and had an inferior
visibility compared to the dictionary learning based methods (see the regions indicated by
the arrows “D” and “E”). Some bony structures in the TVSIR result were obscure or
invisible (see the region indicated by the arrow “F”).
4) Few-View Test—Reducing the number of projection views is an important strategy to
reduce image time and radiation dose, giving the few-view problem. To evaluate the
proposed dictionary learning based algorithms for few-view tomography, the number of
low-dose views was down-sampled from 1160 to 580, 290 and 116, respectively. The
GDSIR and ADSIR methods were then applied. Also, the FBP and TVSIR methods were
performed for comparison. The results are in Fig. 3.
It is seen that the FBP reconstruction results became worse and worse when the number of
views was gradually decreased from 1160 to 116. The GDSIR, ADSIR and TVSIR results
were much better than the FBP reconstruction. In the case of 580 views, the GDSIR and
ADSIR results were almost as good as that reconstructed from 1160 views in Fig. 2.
However, in the cases of 290 and 116 views, some details were lost. The TVSIR results
always had more noise and fewer structures than the dictionary learning based
reconstructions (see the regions indicated by the arrows).
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
5) Plots of the Terms in the Objective Function—To monitor the convergence of the
proposed dictionary learning based algorithms, we took GDSIR as an example. Fig. 4 plots
the convergence curves of the log-likelihood term
and the sparse
representation error term
with respect to the iteration number. These curves
show that both the log-likelihood term and the sparse representation error term would
decrease monotonically. After ~40 iterations, both the two terms changed little with further
iterations. Practically, the reconstruction process can be stopped after a fixed number of
iterations when the log-likelihood term and sparse representation error term do not decrease
insignificantly.
B. Numerical Simulations
To evaluate the performance of the proposed algorithms, we conducted two comprehensive
numerical studies. One is the sheep lung simulation to evaluate the performance of the
proposed methods and demonstrate the parameter selection. The other is the human thorax
simulation focused on the robustness of GDSIR.
1) Sheep Lung Simulation
Data Acquisition: In this study, the FBP reconstruction from the normal-dose sinogram in
Section IV-A2 [see Fig. 1(a)] was chosen as a numerical phantom. A fan-beam geometry
was defined with the same parameters as presented in Section IV-A1. A monochromatic
source was used. The Poisson noise was superimposed onto the raw projection data to
synthesize low-dose projections assuming 1.0 × 104 photons from the X-ray source towards
each detector element. Furthermore, this set of low-dose data was down-sampled from 1160
views to 580, 290, and 116 views, respectively.
Image Reconstruction: The FBP, GDSIR, ADSIR, and TVSIR algorithms were all
performed on each of the four datasets. For GDSIR, the global dictionary was learned from
the patches extracted from the TVSIR result in Fig. 2. The key details for dictionary learning
were the same as that in Section IV-A2. The numerical phantom, the image for dictionary
learning and the learned dictionary are shown in Fig. 5.
All the reconstructed images and associated differences are shown in Fig. 6. The
performances of these algorithms can be seen from not only the reconstructed images (the
regions indicated by the arrows) but also the difference images. Clearly, the FBP algorithm
had the worst results, and TVSIR had more noise and was more blocky than the dictionary
learning based methods. When the number of views was decreased, heavy noise and artifacts
compromised adaptive dictionary learning, which caused the loss of some fine structures in
the ADSIR results. As a result, it seems that ADSIR works better for low-dose datasets
while GDSIR is preferred in the cases of ultra-low-dose datasets.
Quantitative Indexes Evaluation: The aforementioned algorithms were quantitatively
evaluated using two indexes. One is the root mean square error (RMSE)
is a reconstructed value,
is the phantom value of the jth pixel, and J is the
number of pixels in an image. The other is the image quality assessment (IQA) index for
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
structural similarity (SSIM) (the closer to 1, the higher structural similarity) , which is
proved to be consistent with visual perception. With the image phantom as the reference, we
evaluated all the reconstructed images in terms of the RMSE and SSIM indexes.
The RMSE and SSIM indexes of these results are shown in Tables IV and V, respectively.
Quantitatively, GDSIR and ADSIR outperformed TVSIR, all of which outperformed FBP.
GDSIR and ADSIR had basically the same performance. In the cases of 1160 and 580
views, ADSIR kept slightly less noise and more features than GDSIR, but in the cases of
290 and 116 views GDSIR did better than ADSIR. In brief, GDSIR and ADSIR would
produce less noise and more structures than the TVSIR and FBP.
Parameter Comparison: To evaluate how the parameter selection affects the performance
of GDSIR, here we describe our analysis on the reconstructed results from 290 views as an
example. The regularization parameter λ, the sparsity level
and the precision level were
first empirically selected as Case 1 in Table VI. Then, these three parameters were
individually perturbed around that in Case 1 to form Cases 2–7, respectively. Finally, we
kept the parameters as Case 1, but changed the global dictionary to the dictionary learned
from the phantom itself (that is, the dictionary in Section IV-A2) to form Case 8.
The reconstructed results and difference images are in Fig. 7. The RMSE and SSIM indexes
are in Table VI. It can be seen that there are not much differences among Case 1–5 and 7.
Case 6 is the worst one, which was caused by the smaller precision level ε. A too small
precision level would introduce fake structures, and keep more noise. Case 8 is the best one.
The reason is that the global dictionary in this case was learned from the phantom image
itself, and most useful structures were kept. From these results, we see that decent results
can be obtained as long as the parameters do not deviate from the proper ranges as
summarized in Table I.
Computational Cost Comparison: Our codes ran on a PC with one i7–2600 CPU and a 4
GB RAM. In Table VII, we recorded the average computational times for the image
updating (including forward and backward projections), OMP, dictionary learning and TV
minimization steps in one iteration with 1160 views. It can be seen that an OMP step cost
about half of the time for image updating, while the TV minimization only took one percent
of the time for OMP. For TVSIR, most of the time was consumed in image updating.
GDSIR took almost 1.5 times of the overhead of TVSIR. For ADSIR, the dictionary
learning step needed almost twice of the OMP overhead.
2) Human Thorax Simulation
Data Acquisition: In this study, a human thorax perfusion image was downloaded from
 to generate a numerical phantom. The thorax region in the original image was kept,
including 351×512 pixels. The pixel values were mapped onto [0 2], and the phantom
covered a region of 20.57×30 cm2 (Fig. 8). A fan-beam geometry with equi-angularly
distributed detectors was assumed. The distance from the X-ray source to the system origin
was 55cm. Over a 360° full scan, 1100 projections were uniformly collected. For each
projection, 600 detector elements spanned an FOV of 19.71 cm in radius. The Poisson noise
was superimposed onto raw projections to synthesize low-dose data. Furthermore, this lowdose dataset was down-sampled from 1100 views to 550, 275, and 110 views, respectively.
The reconstructed images were in the same matrix as the original phantom. The sparsity
constraint was enforced. In this simulation, we did not use any patient specific information,
and the dictionary in the sheep lung study [Fig. 1(b)] was taken as the global dictionary for
GDSIR reconstruction. Because the sheep lung image is significantly different from the
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
human thorax image, the robustness of the GDSIR algorithm was actually over-tested in this
arrangement.
Reconstructed Results: The results reconstructed by the FBP and the proposed dictionary
learning based algorithms as well as the associated differences images are shown in Fig. 9. It
can be seen that the dictionary learning based reconstructions suppressed noise (such as the
region indicated by the arrow “a” in Fig. 8) and the streak artifacts (the region indicated by
the arrow “b” in Fig. 8), and preserved structures effectively (such as the region indicated by
the arrow “c” in Fig. 8, which is a superior vena cava highlighted by the contrast agent, and
can be clearly reconstructed using the GDSIR and ADSIR algorithms in the case of 110
views). Surprisingly, the global dictionary learned from the sheep lung image performed
well in this human thorax study. Also, GDSIR had similar results as those with ADSIR. In
the case of 110 views, due to the degraded dictionary quality, ADSIR gave worse results
than GDSIR.
C. Clinical Study
1) Data Acquisition: Under the approval of the Institutional Review Board (IRB), Wake
Forest University Health Science, the proposed algorithms were evaluated with a group of
existing raw projections collected in cardiac perfusion CT studies for other purposes. The
patient was scanned by a state-of-the-art GE discovery CT750 HD scanner in a helical mode
to examine the coronary artery. After appropriate pre-processing, we obtained 64-slice fanbeam sinograms. These slices had substantial structural differences. The radius of the
scanning trajectory was 53.852 cm. Over a 360° range, 2200 projections were uniformly
acquired. For each projection, 888 detector elements were equi-angularly distributed over an
FOV of 24.92 cm in radius. In this study, images were reconstructed in a matrix of 600×800
pixels over a 45×60 cm2 region.
2) Global Dictionary Learning: To evaluate the robustness of the GDSIR, we randomly
selected slice #20 for global dictionary learning and use the resultant dictionary to
reconstruct other slices without loss of generality. To provide an excellent training set for
construction of a global dictionary, we first reconstructed slice #20 using the TVSIR
algorithm from all the 2200 views, and removed speckle noise by a 3×3 median filter. Then,
the patches were extracted from the refined image to construct a global dictionary, as
described in Section IV-A2. The image used to extract patches and construct the dictionary
is shown in Fig. 10.
3) Few-View Test: All the 64 sinograms were down-sampled from 2200 views to 440 and
220 views, respectively. Images were reconstructed from the down-sampled datasets using
the FBP, TVSIR, GDSIR, and ADSIR algorithms, respectively. As an example, here we
present the results from slice #32.
In the case of 440 views, the parameters for GDSIR were chosen as λ = 0.3, ε = 5 × 10−8,
, and the parameters for ADSIR were as λ = 0.3, ε = 3 × 10−8,
. Fig. 11 shows
the reconstructed results for slice #32 after 100 iterations with 10 subsets. It is seen that
while the FBP result had more noise and streak artifacts, the TVSIR result was substantially
better. However, the TVSIR result was slightly blocky and had more noise than the GDSIR
and ADSIR counterparts.
In the case of 220 views, the parameters for GDSIR were chosen as λ = 0.4, ε = 5 × 10−8,
, and the parameters for ADSIR were as λ = 0.4, ε = 5 × 10−8,
reconstructed images for slice #32 after 200 iterations with five subsets are in Fig. 12. Our
study seems indicating that useful results can be obtained from 1/10 of the original views
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
using the proposed dictionary learning based reconstruction algorithms. The magnified
cardiac regions are in Fig. 13. Clearly, GDSIR and ADSIR outperformed FBP and TVSIR,
more effectively suppressing image noise while keeping subtle structures.
V. Discussions and Conclusion
Based on the CS theory, the TV regularization method was widely used for CT
reconstruction, and produced good results from incomplete and noisy data. Our results in
Section IV have also verified that the TV regularization method can generate better results
than the conventional FBP algorithm from low-dose and/or few-view datasets. However,
because the TV regularization method is based on a piecewise constant image model, it may
produce blocky results in practical applications when there is too much noise. Moreover, the
TV constraint uniformly penalizes the image gradient, and is not capable of distinguishing
structural details from noise and artifacts. These problems dampen the enthusiasm for the
clinical application of the TV regularization method. While many efforts have been devoted
to improving the TV based CT reconstruction , , , , , the soft-threshold
method , was selected as an example to produce the TVSIR results in our
experiments for comparison with our proposed dictionary learning based algorithms.
Different from the TV regularization method, the dictionary learning approach aims at
capturing localized structural information and suppressing image noise. The sparse
representation in terms of a redundant dictionary is able to keep the atoms reflecting
structural features and avoid the other atoms. Use of the dictionary-based sparse
representation as the regularization term for SIR is a new mechanism to improve image
quality. Moreover, any missed structural information due to the enforcement of the sparsity
constraint will be compensated for in the subsequent updating steps. SIR is very effective to
eliminate streak artifacts, and works well with a dictionary. In principle, the dictionary
learning process should lead to a sparser representation of an underlying image in a specific
application. Our simulation results for monoenergetic imaging and for one imaging region
have shown that both the GDSIR and ADSIR algorithms outperformed TVSIR from lowdose and/or few-view data. However, it should be noted that there remain differences
between the true image and the GDSIR and ADSIR results, as seen in Section IV-B. The
basic idea of the dictionary learning based approach is to find a best match to a true image
from the dictionary-spanned image space. When the true image is outside the dictionary
image space, the reconstructed image can be viewed as its projection on the dictionary
image space with an unavoidable error. Therefore, some structures may be lost while
artifacts may be introduced although the reconstructed results often have less noise and more
structural information. A proper dictionary should represent the structural information of an
object as much as possible. In this way, the reconstruction with a sparse representation in
terms of the dictionary can perform well. With a global dictionary, the structural differences
between its training images and a true image would affect the final reconstruction quality.
Practically, both the aforementioned real data and simulation studies have demonstrated that
GDSIR perform robustly well with a dictionary learned from a quite different image.
Usually, it is not difficult to prepare an excellent training set with sufficiently many
structural features and less noise for construction of a global dictionary. Since GDSIR does
not need to update the dictionary in each iteration step, it is much faster than ADSIR. On the
other hand, it is necessary to use an adaptive dictionary when a global dictionary does not
match a specific application closely. As indicated in Section IV, ADSIR can reveal details
which are invisible in the GDSIR reconstruction. However, the image noise from dictionary
learning may affect the final reconstruction. When the sinogram is rather noisy, the
dictionary learning process will no longer be able to extract high-quality structural
information, and will degrade the image quality.
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
In our proposed dictionary learning based algorithms, there are some parameters to be
determined. How to optimize parameters is an interesting problem for almost all the
regularized reconstruction algorithms. Similar to other regularized methods, the
regularization parameter λ is very important. A simple and effective way is to select λ
empirically based on heuristic guidelines. For a fixed scanning geometry, λ should be
increased with the decrement of the projection SNR. Besides, the proposed objective
function can be transformed into a classical regularization problem as shown in (21), which
makes it possible to use established general methods in our context. In this paper, the OMP
method was used for sparse representation with two parameters: sparsity
and precision ε.
These two parameters are related to not only the dictionary but also the image features and
noise level. A too small
or too large ε could not represent the structures sufficiently. On
the other hand, a too large
or too small ε may introduce fake structures. Usually,
related to the sparsity level of the dictionary learning
, and ε depends on the image noise
level. As far as the dictionary is concerned, the dictionary size K and patch size N should be
determined. The empirical values N = 8 × 8, K = 256 have been well applied in many
applications and also validated in our experiments. Additionally, it appears possible to
analyze the relationship between noise and structural information in each iteration and
develop an adaptive parameter selection method.
Some image preprocessing techniques can be involved in the proposed algorithms. Because
a predetermined dictionary is not required before the ADSIR reconstruction, and the
dictionary is learned from intermediate images during the iterations, there is little image
preprocessing issue. For the GDSIR reconstruction, we need to predetermine a global
dictionary from a training set and keep the dictionary unchanged during the iterations. To
achieve a best performance, the dictionary is generally learned from well-reconstructed
images with structures similar to objects of interest. In practice, the images for dictionary
training may contain non-structured noise or structure-like artifacts such as speckle noise.
For the non-structured noise, we do not need any image preprocessing because the
dictionary learning process is inherently good at suppressing noise (e.g., the sheep lung
study in Section IV-A). For artifacts, an image preprocessing step is helpful to avoid
unexpected atoms. In the clinical study for this paper, there are speckle noises in the TVSIR
results. This kind of noise would lead to speckle-like atoms in the global dictionary, which
may match speckle noise well in the sparse representation and lead to speckle noise in the
GDSIR results. Since median filtering is a conventional way to remove speckle noise
without losing important structures, a 3×3 median filter was used to improve the image
quality for global dictionary learning.
The computational cost is a common problem for all the iterative reconstruction methods.
The total cost of any iterative algorithm can be expressed as the product between the
computation time for each iteration and the total iteration number. In our implementation the
image updating step adopted a finite-detector-based high-accuracy area model for the
forward and backward projections . If there is no requirement for high resolution, the
well-known distance-driven method can be used to replace the time-consuming area model
 , which can speed up the image updating step by an order of magnitude. The OMP and
dictionary learning operations take comparable time as the image updating step. Rapidly,
GPU and other hardware based acceleration methods have been developed, which can be
applied to our algorithms.
The results from low-dose and/or few-view datasets in this paper are only exemplary
applications of the proposed dictionary learning based reconstruction methods. We believe
that they will perform well for other CT applications, such as limited-angle, interior
tomography, and so on. Besides, more clinical applications are possible, such as ultra-low-
IEEE Trans Med Imaging. Author manuscript; available in PMC 2013 September 19.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
dose lung cancer screening. While our work has been presented in the context of X-ray CT,
our results could be extended to other modalities such as phase-contrast CT, PET, and
In conclusion, we have proposed two novel dictionary learning based SIR algorithms for
low-dose CT and other applications. The sparsity constraint has been introduced into the
SIR framework in terms of either a global or adaptive dictionary. Our approach has
produced promising results in terms of preserving structural details and suppressing image
noise. Especially, it has been verified that the proposed approach has outperformed the TV
minimization method for low-dose CT. However, there is no proof that this is true for all
kinds of structures. Further improvement and extension are underway.
Acknowledgments
This work was supported in part by National Natural Science Foundation of China (NSFC) (No. 61172163), in part
by the Research Fund for the Doctoral Program of Higher Education of China (No. 20110201110011), in part by
the National Science Foundation/Major Research Instrumentation (NSF/MRI) program (CMMI-0923297), in part
by the National Institutes of Health/National Institute of Biomedical Imaging and Bioengineering (NIH/NIBIB)
under Grant EB011785, and in part by the Hong Kong Research Grants Council (RGC) General Research Fund
(PolyU 5375/09E).
The authors would like to thank Dr. E. Hoffman for the sheep lung dataset acquired at University of Iowa, Iowa
City. The authors would also like to thank S. Ellis, Dr. D. Entrikin, and Dr. B. Liu who acquired and preprocessed
the patient CT data. Finally, the authors thank the anonymous reviewers for constructive suggestions.