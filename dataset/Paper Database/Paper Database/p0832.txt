Anchored Neighborhood Regression for Fast Example-Based Super-Resolution
Radu Timofte 1,2, Vincent De Smet 1, and Luc Van Gool 1,2
1 KU Leuven, ESAT-PSI / iMinds, VISICS
2 ETH Zurich, D-ITET, Computer Vision Lab
Recently there have been signiﬁcant advances in image
upscaling or image super-resolution based on a dictionary
of low and high resolution exemplars. The running time of
the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast
super-resolution methods while making no compromise on
quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than
the Euclidean distance. Moreover, we show that most of
the current approaches reach top performance for the right
parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing
the super-resolution mapping to a precomputed projective
matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding
of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-ofthe-art methods on standard images. We obtain similar or
improved quality and one or two orders of magnitude speed
improvements.
1. Introduction
Super-resolution (SR) is a popular branch of image reconstruction that focuses on the enhancement of image resolution. In general, it takes one or more low resolution (LR)
images as input and maps them to a high resolution (HR)
output image. Super-resolution algorithms can be roughly
subdivided into three subclasses: interpolation methods like
Lanczos upsampling and New Edge Directed Interpolation (NEDI) , multi-frame methods which
make use of the presence of aliasing in multiple frames of
the same scene to produce one high resolution image, and ﬁnally learning-based methods. The latter use machine learning techniques and comprise methods like Gradient Proﬁle
Prior , which try to learn edge statistics from natural
Speed (1/s)
Yang et al.
Zeyde et al.
Figure 1. Speed vs. PSNR for the tested methods. Our ANR and
GR methods (shown in red) provide both high speed and quality.
More details in Table 1.
images, but also the recent and very popular dictionary- or
example-based learning methods. Most of these dictionarybased methods build on the work of Freeman et al. and
Baker and Kanade .
Dictionary-based methods use a patch- or feature-based
approach to learn the relationship between local image details in low resolution and high resolution versions of the
same scene. An input image is typically subdivided into
overlapping patches, which together form a Markov Random Field (MRF) framework.
By searching for nearest
neighbors in a low resolution dictionary, a number of corresponding high resolution candidates can be retrieved. This
results in an MRF with a number of HR candidate patches
for each node. After associating a data cost to each candidate and a continuity cost for neighboring candidates the
MRF can be solved by using techniques such as belief propagation or graph cuts.
One downside of these methods is their high computational complexity.
Several methods have been proposed
to overcome this problem, most notably neighbor embedding and sparse encoding approaches. Neighbor embedding super-resolution methods do not always explicitly focus on lowering computational complexity, but because of their inherent interpolation of the patch subspace
they can be used to lower the number of image patch exemplars needed, thus reducing the algorithm’s execution time.
Sparse coding methods try to ﬁnd a sparse coding
for the input patches based on a compact dictionary . We give an overview of these methods in section 2. Bevilacqua et al. compare the running times for
several example-based super-resolution algorithms, mentioning minutes to hours for most state-of-the-art methods,
depending on the size of the input image.
We propose a new method for example-based superresolution that focuses on low computation time while keeping the qualitative performance of recent state-of-the-art approaches. We reach an average improvement in speed of
between one and two orders of magnitude over other stateof-the-art methods (see Figure 1).
The remainder of the paper is organized as follows: we
take a closer look at recent approaches for neighbor embedding and sparse coding for super-resolution in Section 2, we
explain our proposed method in Section 3 and show experimental results in Section 4. The conclusions are drawn in
Section 5.
2. Dictionary-based Super-Resolution
In this section we shortly review dictionary-based methods for SR. As we have brieﬂy presented Freeman’s original
method in the introduction, here we focus on neighbor
embedding and sparse coding approaches.
2.1. Neighbor embedding approaches
Neighbor embedding (NE) approaches assume that small
image patches from a low resolution image and its high resolution counterpart form low-dimensional nonlinear manifolds with similar local geometry. Chang et al. proposed a super-resolution method based on this principle, using the manifold learning method Locally Linear Embedding (LLE) . The LLE algorithm assumes that when
enough samples are available, each sample and its neighbors lie on or near a locally linear patch of the manifold.
Since the manifolds in LR and HR feature space are assumed to have a similar local geometry, this means that as
long as enough samples are available, patches in the HR
feature domain can be reconstructed as a weighted average
of local neighbors using the same weights as in the LR feature domain. Chang et al. search for a set of K nearest
neighbors for each input patch in LR feature space, compute K appropriate weights for reconstructing the LR patch
by ﬁnding a constrained least squares solution, and eventually create an HR patch by applying these weights in HR
feature space. The result image is then created by using
the computed HR patches and averaging their contributions
where they overlap. The recent Nonnegative Neighbor Embedding approach is another example of NE used for
super-resolution. It is based on the assumption that the local nonnegative least squares decomposition weights over
the local neighborhood in LR space also hold for the corresponding neighborhood in HR space.
2.2. Sparse coding approaches
The NE approaches from the previous section use a dictionary of sampled patches from low and high resolution
image pairs. These dictionaries can quickly become very
large, especially when more or bigger training images are
added to improve performance.
Sparse coding (SC) approaches try to overcome this by using a learned compact
dictionary based on sparse signal representation. Yang et
al. proposed an approach for super-resolution based on
this idea. Low resolution patches are sparsely reconstructed
from a learned dictionary using the following formulation:
∥FDlα −Fy∥2
2 + λ∥α∥0,
where F is a feature extraction operator, Dl is the learned
low resolution dictionary, α is the sparse representation, y
is the low resolution input patch and λ is a weighting factor.
The l0-norm constraint leads to a NP-hard problem and, in
practice, is relaxed to an l1-norm constraint. Equation (1)
is eventually also extended with a term which encourages
similarity in the overlapping regions of nearby patches.
Sparse dictionaries are jointly learned for low and high
resolution image patches, with the goal of having the same
sparse representation for low resolution patches as their corresponding high resolution patches. This goal is reached for
a set of training image patch pairs Xh, Yl (high and low resolution patches resp.) by minimizing
N ∥Xh −DhZ∥2
M ∥Yl −DlZ∥2
where N and M are the dimensionality of the low and high
resolution patches and Z is the coefﬁcient vector representing the sparsity constraint. The resulting dictionary has a
ﬁxed size and thus the algorithm has the capability of learning from many training patches while avoiding long processing times due to an ever growing dictionary. Unfortunately, solving this sparse model still takes a large amount
Zeyde et al. build upon this framework and improve
the execution speed by adding several modiﬁcations. The
most important changes include using different training approaches for the dictionary pair (K-SVD for the low
resolution dictionary and direct approach using the pseudoinverse for the high resolution dictionary), performing dimensionality reduction on the patches through PCA and using Orthogonal Matching Pursuit for the sparse coding. They also show an improvement in quality with less
artifacts and a higher average Peak Signal-to-Noise Ratio
(PSNR) when compared to the results of Yang et al. .
3. Proposed Methods
We propose an anchored neighborhood regression
method that conveys two situations, one being the general
behavior where a neighborhood size is set and the other being the so called global case, where the neighborhood coincides with the whole dictionary in use. We refer to these
in the following as the Anchored Neighborhood Regression
(ANR) and its extreme case, the Global Regression (GR).
We start with the global case for simplicity of the formulation, and then we consider the neighborhoods.
3.1. Global Regression
For most NE and SC approaches, the least squares (LS)
problems are constrained or regularized using the l1-norm
of the coefﬁcients, similar to equation (1). This is computationally demanding. We can reformulate the problem
as a least squares regression regularized by the l2-norm of
the coefﬁcients. Thus, we use Ridge Regression (also
known as Collaborative Representation ) and have a
closed-form solution. The problem becomes
β ∥yF −Nlβ∥2
2 + λ∥β∥2,
where Nl corresponds to the neighborhood in LR space that
we choose to solve this problem, which in the case of neighborhood embedding would refer to the K nearest neighbors
of the input feature yF and in the case of sparse coding
would refer to the LR dictionary. The parameter λ allows
us to alleviate the singularity (ill-posed) problems and stabilizes the solution, which is the coefﬁcient vector β. The
algebraic solution is given by
l Nl + λI)−1NT
The HR patches can then be computed using the same coef-
ﬁcients on the high resolution neighborhood Nh
where x is the HR output patch and Nh the HR neighborhood corresponding to Nl.
If we use the whole LR dictionary for this, meaning
(Nh, Nl) = (Dh, Dl), we get a global solution for the
problem. An important observation here is that from equation (4) and equation (5), we obtain:
l Dl + λI)−1DT
where the projection matrix
PG = Dh(DT
l Dl + λI)−1DT
can be computed ofﬂine. This means that during the execution of the SR algorithm we only need to multiply the
precomputed projection matrix PG with the LR input feature vector, yF , to calculate the HR output patches x. This
formulation is the Global Regression (GR) approach, the
extreme case of our ANR method.
3.2. Anchored Neighborhood Regression
The Global Regression approach reduces the superresolution process to a projection of each input feature into
the HR space by multiplication with a precomputed matrix.
It is however a global solution and thus not tuned
towards speciﬁc input features, but rather the entire dictionary, which is a representation of the features occurring in
the training images. If instead of considering the whole dictionary as starting point for computing the projective matrix
we consider local neighborhoods of a given size we allow
more ﬂexibility of the approach at the expense of increased
computation – we will have more than one projective matrix
and neighborhoods.
We start by grouping the dictionary atoms into neighborhoods. More speciﬁcally, for each atom in the dictionary we
compute its K nearest neighbors, which will represent its
neighborhood. If we start from a learned sparse dictionary,
as in the sparsity approaches of Yang et al. and Zeyde et
al. , we ﬁnd the nearest neighbors based on the correlation between the dictionary atoms rather than the Euclidean
distance. The reason for this is that the atoms are a learned
basis consisting of l2-normalized vectors. If, conversely, we
have a dictionary of features taken straight from the training
patches, like in the NE approaches of Chang et al. and
Bevilacqua et al. , then the Euclidean distance is an appropriate distance measure. Once the neighborhoods are de-
ﬁned, we can calculate a separate projection matrix Pj for
each dictionary atom dj, based on its own neighborhood.
This can be done in the same way as in the previous section
by using only the dictionary atoms that occur in the neighborhood rather than the entire dictionary, and can again be
computed ofﬂine.
The super-resolution problem can then be solved by calculating for each input patch feature yiF its nearest neighbor atom, dj, in the dictionary, followed by the mapping to
HR space using the stored projection matrix Pj:
xi = PjyiF .
This is a close approximation of the NE approach, with a
very low complexity and thus a vast improvement in execution time. We call our approach the Anchored Neighborhood Regression (ANR), since the neighborhoods are anchored to the dictionary atoms and not directly to the low
resolution patches as in the other NE approaches.
4. Experiments
In this section we show experimental results1 of our
method and compare it quantitatively and qualitatively to
other state-of-the-art methods. We ﬁrst discuss some of the
1Source codes, images, and results are available at:
 
Dictionary size
Zeyde et al.
Dictionary size
Dictionary size
Running time (s)
Zeyde et al.
Dictionary size
Running time (s)
a) trained dictionary
b) random dictionary
Figure 2. Dictionary size vs. average PSNR and average running time performance on the 14 images from Set14 with magniﬁcation ×3.
Bicubic is our reference. All the methods were used with their best neighborhood size. For the trained dictionaries, ANR uses a size of 40,
NE+LS uses 12, NE+NNLS - 24, and NE+LLE - 24. For the random dictionaries, ANR uses a size of 128, NE+LS - 5, NE+NNLS - 24,
and NE+LLE - 128, resp. For the running times we subtracted the shared processing time (collecting patches, combining the output) for
all the methods.
details surrounding the algorithm such as used features, dictionary choices, similarity measures, size for neighborhood
calculation and different patch embeddings.
4.1. Conditions
One aspect which can inﬂuence the performance is the type
of features used to represent the image patches. These features are almost always calculated from the luminance component of the image, while the color components are interpolated using a regular interpolation algorithm such as bicubic interpolation . This is because the human
visual system is much less sensitive to high frequency color
changes than high frequency intensity changes, so for the
magniﬁcation factors used in most papers the perceived difference between bicubic interpolation and SR of the color
channels is negligible.
The most basic feature to use is the patch itself. This
however does not give the feature good generalization properties, so a popular choice is to subtract the mean and
to normalize the contrast by e.g. dividing by the standard deviation. An often used similar feature is the ﬁrstand second order derivative of the patch . Both
of these feature types seem to lead to similar performance,
while Bevilacqua et al. show that using only ﬁrst order derivatives gives slightly worse performance than using only mean subtraction. We use the same features as
Zeyde et al. , who start from the ﬁrst- and second order
gradients and apply PCA dimensionality reduction, projecting the features onto a low-dimensional subspace while preserving 99.9% of the average energy. This usually leads to
features of about 30 dimensions for upscaling factor 3 and
3 × 3 low resolution patch sizes.
We subtract the bicubically interpolated LR image from
the HR image to create normalized HR patches.
patches resulting from the SR process (i.e. equation (6) for
GR) are added to the bicubically interpolated LR input image (with overlapping parts averaged) to create the output.
We use Zeyde et al.’s algorithm and their provided
sources as a starting point for our implementations.
Embeddings
Apart from our comparisons with the sparse methods of
Yang et al. and Zeyde et al. , we also compare
our results to neighbor embedding approaches adapted to
our dictionary choices. The original LLE-based SR method
of Chang et al. does not use a learned dictionary, instead the dictionary consists simply of the training patches
themselves. This makes direct comparison to our method
and the sparse methods difﬁcult, because the question then
arises “which dictionary should we use to have a fair comparison?”. The same can be said when we wish to compare
to the nonnegative neighbor embedding of Bevilacqua et
al. . The solution is to use the same learned dictionary as
Zeyde et al. and our method, with the respective SR methods of Chang and Bevilacqua implemented to solve the SR
regression. We will refer to these as NE + LLE (Neighbor Embedding with Locally Linear Embedding) and NE
Neighborhood size
Yang et al.
Zeyde et al.
Neighborhood size
Neighborhood size
Running time (s)
Yang et al.
Zeyde et al.
Neighborhood size
Running time (s)
a) trained dictionary of size 1024
b) random dictionary of size 1024
Figure 3. Neighborhood size vs. average PSNR and average running time performance on the 14 images from Set14 with magniﬁcation
×3. Bicubic, Yang et al., Zeyde et al., and GR are reported for reference. Yang et al. uses the original dictionary of 1022, while the other
methods, except Bicubic interpolation, share the same dictionary. The decrease in the running time of the ANR method from random
dictionary experiments is caused by applying a subsampling step of max{1, neighborhoodsize
} for the anchor atoms, while for the trained
one the step is 1. For the running times we subtracted the shared processing time (collecting patches, combining the output) for all the
dictionary based methods, leaving only the encoding time for each method.
+ NNLS (Neighbor Embedding with NonNegative Least
Squares), and we add results for a similar implementation
that uses unconstrained least squares to solve the regression,
to which we refer as NE + LS.
Dictionaries
The choice of the dictionary is critical for the performance
of any SR method. Usually, the larger the dictionary the
better the performance, however this comes with a higher
computational cost. The dictionary can be built using the
LR input image itself, in this case we have an “internal”
dictionary. Glasner et al. and their intensive exploitation of “patch redundancy” are the main advocates for this.
However, many approaches prefer to build “external” dictionaries, external to the input query, using diverse images.
In our settings we work with the same set of external
images as used by Zeyde et al. and Yang et al. .
Also, we consider both randomly sampled dictionaries and
learned dictionaries. For learning we use the K-SVD/OMP
learning approach of Zeyde et al. .
In Fig. 2 we depict the effect of the dictionary on the performance. As expected, usually the performance increases
with the size of the dictionary. Moreover, we see again
that using a learned dictionary is highly beneﬁcial for all
the methods – it allows for a reasonably high performance
for small dictionary sizes. One needs a 16× larger random
sampled dictionary to reach the same performance as with
the trained dictionary. Most of the methods exhibit a similar log-linear increasing trend with the dictionary size and
the PSNR difference among ANR, Zeyde et al. and the NE
approaches is quite small for their best settings (using optimal neighborhood size). The difference is made by the
running time, where ANR and GR are the clear winners.
GR is the fastest method, but as a global method it has its
weaknesses, and for large dictionaries tends not to reach
competitive PSNR levels.
Neighborhoods
As explained in Section 3.2, our ANR algorithm ﬁnds the
nearest neighbor (atom) in the dictionary for each input feature and borrows the neighborhood and the precomputed
projection matrix from this neighbor. The NE approaches
also rely on the neighborhood to the input LR feature. The
performance of the embedding methods, and hence the performance of the SR method, depends on the dimensionality
of these neighborhoods.
The computation of the nearest neighbors is based on a
similarity measure. The Euclidean distance is the choice
of most NE approaches working directly with large dictionaries. We use the Euclidean distance for the setups with
randomly sampled dictionaries. In the case of the learned
sparse dictionaries, we obtain l2-normalized atoms meant
to form a basis spanning the space of the training samples
while minimizing the reconstruction error. For this case our
option is to obtain the nearest neighbors using the correlation expressed as the inner product.
Table 1. Magniﬁcation ×3 performance in terms of PSNR (dB) and running time (s) per image on the Set14 dataset. All the original
methods use the same training images from . The methods share the same trained dictionary of 1024, except Bicubic interpolation and
Yang et al. with a dictionary of 1022. The neighborhood sizes are as in Fig. 2. ANR is 5 times faster than Zeyde et al. If we consider only
the encoding time, our ANR method takes 0.27s on average, being 13 times faster than Zeyde et al., and 10 times faster than NE+LS.
Yang et al. 
Zeyde et al. 
coastguard
average performance
ANR speedup
average time for encoding
ANR speedup for encoding
The neighborhood size is the major parameter for the NE
techniques and for ANR as well. We show the effect of this
size in Figure 3 for dictionaries of size 1024. The methods
behave differently under the same settings. Moreover, the
curves are not monotonic – as noticed also in – and more
investigation in this phenomenon is due. On the learned dictionary, NN + LS peaks at 12, NE + LLE and NE + NNLS
at 24, while ANR peaks at 40. On the random dictionary,
NN + LS peaks at 5, NE + LLE at 128, NE + NNLS at 24,
while ANR peaks at 128. We will use these neighborhood
sizes for the further experiments. The behavior of ANR and
GR is also inﬂuenced by the choice of the regularization
parameter λ, in all our experiments empirically set to 0.01.
4.2. Performance
In this section we will show quantitative and qualitative
results as well as running times for our proposed method
and compare them to the other discussed dictionary-based
SR algorithms. More speciﬁcally, we compare our results
to the sparse coding algorithms of Yang et al. and
Zeyde et al. , as well as to our implementations based
on the LS regressions used by Chang et al. (NE + LLE)
and Bevilacqua et al. (NE + NNLS) as described in Section 4.1.2. Tables 1 and 2 summarize the results, showing
PSNR and running time values for a number of test images.
The images are divided into two sets; Set14 was used by
Zeyde et al. to show their results and Set5 was used by
Bevilacqua et al. The effect of dictionary size is explored in
Fig. 2, while Fig. 3 shows the relationship between neighborhood size, PSNR and time.
When using the optimal neighborhood size for each method
the PSNR of Zeyde et al. , NE + LLE, NE + LS, and
our ANR method reach comparable average values. The
approach of Zeyde et al. reaches the highest PSNR in all
experiments, slightly above our ANR method. On the Set14
dataset Zeyde et al. get an average of 28.67 dB, while ANR
gets 28.65 dB, and NE + LS and NE + LLE get an average
of 28.6 dB. Our GR method and Yang et al. get the same
average PSNR of 28.31 dB, while NE + NNLS lies in between with 28.44 dB. A similar behavior can be seen on the
Set5 database, where ANR can be better than Zeyde et al.
Visual examples are shown in Figures 4, 5, and 6. From
these we can conclude that ANR gets very similar quality
performance as the top methods it was compared to.
Running Time
Our implementation of NNLS has similar computation time
as what is reported by Bevilacqua et al. , which is in the
order of 10 seconds for a magniﬁcation factor of 3×. This
can also be observed in Figure 2 and Figure 3. We compare with their algorithm because it is a very recent method
aimed at low complexity and high processing speed while
still keeping high quality results, and is therefore an ideal
candidate for reference.
When we compare the processing times it is clear that
our Global Regression algorithm is the fastest by far, followed by our Anchored Neighborhood Regression. The last
row of Table 1 as well as Fig. 2 and 3 show the difference
of the encoding time, which is the processing time that is
left after subtracting the shared processing time of 0.63 seconds of the algorithms (pre/post processing, bicubic interpolation, patch extraction, etc.).
The Global Regression algorithm is useful when speed is
the most important aspect, however the general ANR algorithm gives a better speed-performance trade-off. That being said, when we look at the results for 3× magniﬁcation,
GR reaches a speedup of 350× when compared to Zeyde et
al., 9000× when compared to Yang et al., 560× when compared to the NE + LLE method inspired by Chang et al. and
3500× to NE + NNLS inspired by Bevilacqua et al. For the
Table 2. Magniﬁcation ×2, ×3, and ×4 performance in terms of PSNR (dB) and running time (s) per image on the Set5 dataset. All
the original methods use the same training images from . All the methods share the same trained dictionary of 1024, except Bicubic
interpolation and Yang et al. with a dictionary of 1022. We use the same neighborhood sizes as in Fig. 2. For upscaling factor 3, ANR is 5
times faster than Zeyde et al. being 94 times faster than Yang et al. and 4 times faster than NE+LS with 12 neighbors.
Yang et al. 
Zeyde et al. 
same magniﬁcation, ANR reaches speed improvements of
13×, 330×, 21×, and 131×, resp.
5. Conclusions
We proposed a new example-based method for superresolution called Anchored Neighbor Regression which focuses on fast execution while retaining the qualitative performance of recent state-of-the-art methods. We also proposed an extreme variant of this called Global Regression
which focuses purely on high execution speed in exchange
for some visual quality loss. The main contributions of this
paper are twofold: i) we present the ANR approach, which
uses ridge regression to learn exemplar neighborhoods of-
ﬂine and uses these neighborhoods to precompute projections to map LR patches onto the HR domain, and ii) we
show through our analysis of existing neighborhood embedding SR methods that most of these can reach a similar top
performance based on using the appropriate neighborhood
size and dictionary; the sparse learned dictionaries in combination with neighbor embeddings methods were shown to
be a faster alternative to full sparse coding methods.
We plan to extend our method to make full use of the
extra dimension of time for the case of video sequences,
with real-time streaming super-resolved video as a goal.
Acknowledgment. The authors are grateful for support by
the Flemish iMinds framework and FWO Levenslijn.