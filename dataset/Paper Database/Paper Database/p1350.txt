Eﬃcient Convolution Kernels for Dependency
and Constituent Syntactic Trees
Alessandro Moschitti
Department of Computer Science
University of Rome “Tor Vergata”, Italy
 
Abstract. In this paper, we provide a study on the use of tree kernels
to encode syntactic parsing information in natural language learning. In
particular, we propose a new convolution kernel, namely the Partial Tree
(PT) kernel, to fully exploit dependency trees. We also propose an eﬃcient algorithm for its computation which is futhermore sped-up by applying the selection of tree nodes with non-null kernel. The experiments
with Support Vector Machines on the task of semantic role labeling and
question classiﬁcation show that (a) the kernel running time is linear on
the average case and (b) the PT kernel improves on the other tree kernels
when applied to the appropriate parsing paradigm.
Introduction
Literature work shows several attempts (e.g. ) to deﬁne linking theories between the syntax and semantics of natural languages. As no complete theory
has yet been deﬁned the design of syntactic features to learn semantic structures requires a remarkable research eﬀort and intuition. Tree kernels have been
applied to reduce such eﬀort for several natural language tasks, e.g. syntactic
parsing re-ranking , relation extraction , named entity recognition and
Semantic Role Labeling .
These studies show that the kernel ability to generate large feature sets is
useful to quickly model new and not well understood linguistic phenomena in
learning machines. However, it is often possible to manually design features for
linear kernels that produce high accuracy and fast computation time whereas
the complexity of tree kernels may prevent their application in real scenarios.
In general, the poor tree kernel results depend on the speciﬁc application but
also on the absence of studies that suggest which tree kernel type should be applied.
For example, the subtree (ST) kernel deﬁned in is characterized by structures
that contain all the descendants of the target root node until the leaves whereas
the subset trees (SSTs) deﬁned in may contain internal subtrees, with no leaves.
How do such diﬀerent spaces impact on natural language tasks? Does the parsing
paradigm (constituent or dependency) aﬀect the accuracy of diﬀerent kernels?
Regarding the complexity problem, although the SST kernel computation time
has been proven to be inherently quadratic in the number of tree nodes , we
may design algorithms that run fast on the average case.
J. F¨urnkranz, T. Scheﬀer, and M. Spiliopoulou (Eds.): ECML 2006, LNAI 4212, pp. 318–329, 2006.
⃝Springer-Verlag Berlin Heidelberg 2006
Eﬃcient Syntactic Tree Kernels
In this paper, we study the impact of the ST and SST kernels on the modeling
of syntactic information in Support Vector Machines. To carry out a comprehensive investigation, we have deﬁned a novel tree kernel based on a general
form of substructures, namely, the partial tree (PT) kernel. Moreover, to solve
the computation problems, we propose algorithms which, on the average case,
evaluate the above kernels in a running time linear in the number of nodes of
the two input parse trees.
We experimented with such kernels and Support Vector Machines (SVMs) on
(a) the classiﬁcation of semantic roles deﬁned in PropBank and FrameNet 
and (b) the classiﬁcation of questions from Question Answering scenarios. We
used both gold standard trees from the Penn Treebank and automatic trees
derived with the Collins and Stanford parsers. The results show that:
(1) the SST kernel is more appropriate to exploit syntactic information from
constituent trees. (2) The new PT kernel is slightly less accurate than the SST
one on constituent trees but much more accurate on dependency structures. (3)
Our fast algorithms show a linear running time.
In the remainder of this paper, Section 2 introduces the diﬀerent tree kernel
spaces. Section 3 describes the kernel functions and our fast algorithms for their
evaluation. Section 4 introduces the Semantic Role Labeling (SRL) and Question
Classiﬁcation (QC) problems and their solution along with the related work.
Section 5 shows the comparative kernel performance in terms of execution time
and accuracy. Finally, Section 6 summarizes the conclusions.
Tree Kernel Spaces
The kernels that we consider represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees)
belongs to the feature space that we intend to generate. For such purpose, the
desired fragments need to be described. We consider three important characterizations: the subtrees (STs), the subset trees (SSTs) and a new tree class, i.e.
the partial trees (PTs).
As we consider syntactic parse trees, each node with its children is associated
with a grammar production rule, where the symbol at the left-hand side corresponds to the parent and the symbols at the right-hand side are associated with
the children. The terminal symbols of the grammar are always associated with
the tree leaves.
We deﬁne as a subtree (ST) any node of a tree along with all its descendants.
For example, Figure 1 shows the parse tree of the sentence "Mary brought a cat"
together with its 6 STs. A subset tree (SST) is a more general structure since
its leaves can be non-terminal symbols.
For example, Figure 2 shows 10 SSTs (out of 17) of the subtree of Figure
1 rooted in VP. The SSTs satisfy the constraint that grammatical rules cannot
be broken. For example, [VP [V NP]] is an SST which has two non-terminal
symbols, V and NP, as leaves whereas [VP [V]] is not an SST. If we relax the
constraint over the SSTs, we obtain a more general form of substructures that we
A. Moschitti
Fig. 1. A syntactic parse tree with its
subtrees (STs)
Fig. 2. A tree with some of its subset
trees (SSTs)
Fig. 3. A tree with some of its partial
trees (PTs)
direct stock purchase
Fig. 4. A dependency tree of a question
call partial trees (PTs). These can be generated by the application of partial
production rules of the grammar, consequently [VP [V]] and [VP [NP]] are
valid PTs. Figure 3 shows that the number of PTs derived from the same tree as
before is still higher (i.e. 30 PTs). These diﬀerent substructure numbers provide
an intuitive quantiﬁcation of the diﬀerent information levels among the treebased representations.
Fast Tree Kernel Functions
The main idea of tree kernels is to compute the number of common substructures
between two trees T1 and T2 without explicitly considering the whole fragment
space. We have designed a general function to compute the ST, SST and PT
kernels. Our fast evaluation of the PT kernel is inspired by the eﬃcient evaluation
of non-continuous subsequences (described in ). To increase the computation
speed of the above tree kernels, we also apply the pre-selection of node pairs
which have non-null kernel.
The Partial Tree Kernel
The evaluation of the common PTs rooted in nodes n1 and n2 requires the
selection of the shared child subsets of the two nodes, e.g. [S [DT JJ N]] and
[S [DT N N]] have [S [N]] (2 times) and [S [DT N]] in common. As the order
of the children is important, we can use subsequence kernels for their generation.
More in detail, let F = {f1, f2, .., f|F|} be a tree fragment space of type PTs and
let the indicator function Ii(n) be equal to 1 if the target fi is rooted at node n
and 0 otherwise, we deﬁne the PT kernel as:
K(T1, T2) =
Δ(n1, n2),
Eﬃcient Syntactic Tree Kernels
where NT1 and NT2 are the sets of nodes in T1 and T2, respectively and Δ(n1, n2) =
i=1 Ii(n1)Ii(n2), i.e. the number of common fragments rooted at the n1 and n2
nodes. We can compute it as follows:
- if the node labels of n1 and n2 are diﬀerent then Δ(n1, n2) = 0;
Δ(n1, n2) = 1 +
J 1,J 2,l(J1)=l(J 2)
Δ(cn1[J1i], cn2[J2i])
where J1 = ⟨J11, J12, J13, ..⟩and J2 = ⟨J21, J22, J23, ..⟩are index sequences
associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively,
J1i and J2i point to the i-th children in the two sequences, and l(·) returns the
sequence length.
We note that (1) Eq. 2 is a convolution kernel according to the deﬁnition and
the proof given in . (2) Such kernel generates a richer feature space than
those deﬁned in . Additionally, we add two decay factors: μ for the
height of the tree and λ for the length of the child sequences. It follows that
Δ(n1, n2) = μ
J1,J2,l(J1)=l(J2)
λd(J1)+d(J2)
Δ(cn1[J 1i], cn2[J 2i])
where d(J1) = J1l(J 1) −J11 and d(J2) = J2l(J 2) −J21. In this way, we penalize both larger trees and subtrees built on child subsequences that contain
gaps. Moreover, to have a similarity score between 0 and 1, we also apply the
normalization in the kernel space, i.e. K′(T1, T2) =
K(T1,T1)×K(T2,T2).
Eﬃcient Tree Kernel Computation
Clearly, the na¨ıve approach to evaluate Eq. 3 requires exponential time. We can
eﬃciently compute it by considering that the summation in Eq. 3 can be distributed with respect to diﬀerent types of sequences, e.g. those composed by p
children; it follows that Δ(n1, n2) = μ
p=1 Δp(cn1, cn2)
where Δp evaluates the number of common subtrees rooted in subsequences of
exactly p children (of n1 and n2) and lm = min{l(cn1), l(cn2)}. Also note that
if we only consider the contribution of the longest child sequence from node
pairs that have the same children, we implement the SST kernel. For the STs
computation we also need to remove the λ2 term from Eq. 4.
Given the two child sequences s1a = cn1 and s2b = cn2 (a and b are the last
children),
Δp(s1a, s2b) = Δ(a, b) ×
λ|s1|−i+|s2|−r × Δp−1(s1[1 : i], s2[1 : r]),
where s1[1 : i] and s2[1 : r] are the child subsequences from 1 to i and from 1
to r of s1 and s2. If we name the double summation term as Dp, we can rewrite
the relation as:
A. Moschitti
Δp(s1a, s2b) =
Δ(a, b)Dp(|s1|, |s2|) if a = b;
otherwise.
Note that Dp satisﬁes the recursive relation: Dp(k, l) =
Δp−1(s1[1 : k], s2[1 : l]) + λDp(k, l −1) + λDp(k −1, l) + λ2Dp(k −1, l −1) (5)
By means of the above relation, we can compute the child subsequences of two
sequences s1 and s2 in O(p|s1||s2|). This means that the worst case complexity
of the PT kernel is O(pρ2|NT1||NT2|), where ρ is the maximum branching factor
of the two trees. Note that the average ρ in natural language parse trees is very
small and the overall complexity can be reduced by avoiding the computation
of node pairs with diﬀerent labels. The next section shows our fast algorithm to
ﬁnd non-null node pairs.
Table 1. Pseudo-code for fast evaluation of the node pairs with non-null kernel (FTK)
function Evaluate Pair Set(Tree T1, T2)
LIST L1,L2;
NODE PAIR SET Np;
L1 = T1.ordered list;
L2 = T2.ordered list; // lists sorted at loading time
n1 = extract(L1); // get the head element and remove it from the list
n2 = extract(L2);
while (n1 and n2 are not NULL)
if (label(n1) > label(n2))
then n2 = extract(L2 );
else if (label(n1) < label(n2))
then n1 = extract(L1 );
while (label(n1) == label(n2))
while (label(n1) == label(n2))
add(⟨n1, n2⟩, Np);
n2=get next elem(L2); /*get the head element and
move the pointer to the next element*/
n1 = extract(L1);
reset(L2); //set the pointer at the ﬁrst element
return Np ;
Fast Non-null Node Pair Computation
To compute the tree kernels, we sum the Δ function for each pair ⟨n1, n2⟩∈
NT1 × NT2 (Eq. 1). When the labels associated with n1 and n2 are diﬀerent,
we can avoid evaluating Δ(n1, n2) since it is 0. Thus, we look for a node pair
set Np ={⟨n1, n2⟩∈NT1 × NT2 : label(n1) = label(n2)}. Np can be evaluated by
(i) extracting the L1 and L2 lists of nodes from T1 and T2, (ii) sorting them in
alphanumeric order and (iii) scanning them to derive the node intersection. Step
(iii) may require only O(|NT1| + |NT2|) time, but, if label(n1)=label(n2) appears
r1 times in T1 and r2 times in T2, the number of pairs will be r1 ×r2. The formal
algorithm (FTK) is shown in Table 1.
Note that the list sorting can be done only once at data preparation time
(i.e. before training) in O(|NT1| × log(|NT1|)). The worst case occurs when the
Eﬃcient Syntactic Tree Kernels
two parse trees are both generated by only one production rule since the two
internal while cycles generate |NT1| × |NT2| pairs. Moreover, the probability of
two identical production rules is lower than that of two identical nodes, thus, we
can furthermore speed up the SST (and ST) kernel by (a) sorting the node list
with respect to production rules and (b) replacing the label(n) function with
production at(n).
Partial Tree Kernel Remarks
In order to model a very fast PT kernel computation, we have deﬁned the algorithm in Section 3.2 to evaluate it eﬃciently and we apply the selection of
non-null node pairs (algorithm in Table 1) which can be also applied to the ST
and SST kernels.
Our algorithm in Section 3.2 allows us to evaluate PT kernel in O(ρ3|NT1||NT2|),
where ρ is the maximum branching factor of the two trees T1 and T2. It should be
emphasized that the na¨ıve approach for the evaluation of the PT function is exponential. Therefore, a fairer comparison of our approach should be carried out
against the eﬃcient algorithm proposed in for the evaluation of relation extraction kernels (REKs). These are not convolution kernels and produce a much lower
number of substructures than the PT kernel. The complexity of REK was O(ρ4)
when applied to only two nodes. If we applied it to all the node pairs of two trees
(as we do with the PT kernel), we would obtain a complexity of O(ρ4|NT1||NT2|)
which is higher than the one produced by our method. Consequently, our solution
is very eﬃcient and produces larger substructure spaces.
Moreover, to further speed up the kernel computation, we apply Eq. 4 to node
pairs for which the output is not null. A similar approach was suggested in 
for the computation of the SST kernel. However, its impact on such kernel has
not been clearly shown by an extensive experimentation and the eﬀect on the
new PT kernel should also be measured. For this purpose, in sections 5.1 and
5.2 we report the running time experiments for the evaluation of the SST and
PT kernels and the training time that they generate in SVMs.
Semantic Applications of Parse Tree Kernels
Semantic Role Labeling (SRL) and Question Classiﬁcation (QC) are two interesting natural language tasks in which the impact of tree kernels can be measured.
The former relates to the classiﬁcation of the predicate argument structures de-
ﬁned in PropBank or FrameNet . For example, Figure 5 shows the parse
tree of the sentence: "Mary brought a cat to school" along with the predicate
argument annotation proposed in the PropBank project. Only verbs are considered as predicates whereas arguments are labeled sequentially from Arg0 to Arg5.
Additionally, adjuncts are labeled with several ArgM labels, e.g. ArgM-TMP or
In FrameNet predicate/argument information is described by means of rich
semantic structures called Frames. These are schematic representations of situations involving various participants, properties and roles in which a word may
A. Moschitti
Fig. 5. Tree substructure space for predicate argument classiﬁcation
typically be used. Frame elements or semantic roles are arguments of target
words, i.e. the predicates. For example the following sentence is annotated according to the Arrest Frame:
[T ime One Saturday night] [ Authorities police in Brooklyn ] [T arget apprehended ]
[ Suspect sixteen teenagers].
The semantic roles Suspect and Authorities are speciﬁc to this Frame.
The common approach to learn the classiﬁcation of predicate arguments relates to the extraction of features from syntactic parse trees of the training
sentences . An alternative representation based on tree kernels selects the
minimal partial tree that includes a predicate with only one of its arguments .
For example, in Figure 5, the semantic/syntactic substructures associated with
the three arguments of the verb to bring, i.e. SArg0, SArg1 and SArgM, are shown
inside the three boxes. Note that such representation is quite intuitive.
Another interesting task is the classiﬁcation of questions in the context of
Question Answering (QA) systems. Detecting the type of a question, e.g. whether
it asks for a person or for an organization, is critical to locate and extract the
right answer from the available documents. The long tradition of QA in TREC
has produced a large question set used in several researches. These are categorized according to diﬀerent taxonomies of diﬀerent grains. We consider the coarse
grained classiﬁcation scheme described in : Abbreviations, Descriptions
(e.g. deﬁnition and manner), Entity (e.g. animal, body and color), Human (e.g.
group and individual), Location (e.g. city and country) and Numeric (e.g. code
and date).
The idea of using tree kernels for Question Classiﬁcation is to encode questions
by means of their whole syntactic parse tree. This is simpler than tailoring the
subtree around the semantic information provided by PropBank or FrameNet for
the SRL task. Additionally, we can easily experiment with other kind of parsing
paradigms, e.g. the dependency parsing. A dependency tree of a sentence is a
syntactic representation that denotes grammatical relations between words. For
example, Figure 4 shows a dependency tree of the question ”What is an offer
of direct stock purchase plan?”.
We note that (1) the father-children node relationship encodes the dependency
between the head, e.g. plan, and its modiﬁers, e.g. direct, stock and purchase.
In our approximation, we only consider the dependency structure by removing the link labels, i.e. we do not use either ”of” between oﬀer and plan or
the other labels like ”object” and ”subject”. (2) It is clear that the SST and ST
Eﬃcient Syntactic Tree Kernels
kernels cannot fully exploit the representational power of a dependency tree since
from subtrees like [plan [direct stock purchase]], they cannot generate
substructures like [plan [stock purchase]] or [plan [direct purchase]].
In contrast, the PT kernel can generate all of these subsequences allowing SVMs
to better generalize on dependency structures although the strong specialization
of the SST kernel may be superior in some tasks. The experiments of Section 5
conﬁrm our observations.
Related Work
In , the SST kernel was experimented with the Voted Perceptron for the parsetree re-ranking task. The combination with the original PCFG model improved
the syntactic parsing. In , an interesting algorithm that speeds up the average
running time is presented. Such algorithm uses the explicit fragment space to
compute the kernel between small trees. The results show an increase of the
speed similar to the one produced by our methods. In , two kernels over
syntactic shallow parser structures were devised for the extraction of linguistic
relations, e.g. person-aﬃliation. To measure the similarity between two nodes,
the contiguous string kernel and the sparse string kernel were used. In such
kernels were slightly generalized by providing a matching function for the node
pairs. The time complexity for their computation limited the experiments on a
data set of just 200 news items. In , a feature description language was used to
extract structural features from the syntactic shallow parse trees associated with
named entities. The experiments on named entity categorization showed that
too many irrelevant tree fragments may cause overﬁtting. In the SST kernel
was ﬁrstly proposed for semantic role classiﬁcation. The combination between
such kernel and a polynomial kernel of standard features improved the stateof-the-art. To complete such work, an analysis of diﬀerent tree kernel spaces as
carried out here was required. In , the computational complexity problem is
addressed by considering only selected trees and the RankBoost algorithm.
The Experiments
In these experiments, we study tree kernels in terms of (a) average running time,
(b) accuracy on the classiﬁcation of predicate argument structures of PropBank
(gold trees) and FrameNet (automatic trees) and (c) accuracy of QC on automatic question trees.
The experiments were carried out with the SVM-light-TK software available
at which encodes ST, SST and PT
kernels in the SVM-light software . We adopted the default regularization parameter and we tried a few cost-factor values (i.e., {1, 3, 7, 10, 30, 100}) to adjust
the rate between Precision and Recall on the development set. We modeled the
multiclassiﬁers by training an SVM for each class according to the ONE-vs-ALL
scheme and by selecting the class associated with the maximum score.
For the ST, SST and PT kernels, we found that the best λ values (see Section
3) on the development set were 1, 0.4 and 0.8, respectively, whereas the best μ
A. Moschitti
was 0.4. We measured the performance by using the F1 measure1 for the single
arguments and the accuracy for the ﬁnal multiclassiﬁers.
Kernel Running Time Experiments
To study the FTK running time, we
Number of Tree Nodes
Fig. 6. Average time in μseconds for the
QTK, FTK and FTK-PT evaluations
extracted from the Penn Treebank 2
 several samples of 500 trees containing exactly n nodes. Each point
of Figure 6 shows the average computation time2 of the kernel function
applied to the 250,000 pairs of trees
of size n. It clearly appears that the
FTK and FTK-PT (i.e. FTK applied
to the PT kernel) average running
time has linear behavior whereas, as
expected, the algorithm (QTK) which
does not use non-null pair selection
shows a quadratic curve.
Experiments on ProbBank
The aim of these experiments is to measure the impact of kernels on the semantic
role classiﬁcation accuracy. We used PropBank (www.cis.upenn.edu/∼ace) along
with the gold standard parses of the Penn Treebank.
The corpus contains about 53,700 sentences and a ﬁxed split between training
and testing used in other researches, e.g. . Sections from 02 to 21 are used for
training, Section 23 for testing and Section 22 as development set for a total of
122,774 and 7,359 arguments in training and testing, respectively. We considered
arguments from Arg0 to Arg5, ArgA and ArgM. This latter refers to all adjuncts
collapsed together, e.g. adverb, manner, negation, location and so on (13 diﬀerent
Figure 7 illustrates the learning curves associated with the above kernels for
the SVM multiclassiﬁers. We note that: (a) the SST and linear kernels show the
highest accuracy, (b) the richest kernel in terms of substructures, i.e. the one
based on PTs, shows lower accuracy than the SST and linear kernels but higher
than the ST kernel and (c) the results using all training data are comparable
with those obtained in , i.e. 87.1% (role classiﬁcation) but we should take
into account the diﬀerent treatment of ArgMs.
Regarding the convergence complexity, Figure 8 shows the learning time of
SVMs using QTK, FTK and FTK-PT for the classiﬁcation of one large argument
(Arg0), according to diﬀerent sizes of training data. With 70% of the data, FTK
is about 10 times faster than QTK. With all the data FTK terminated in 6 hours
1 F1 assigns equal importance to Precision P and Recall R, i.e. f1 = 2P ×R
2 We run the experiments on a Pentium 4, 2GHz, with 1 Gb ram.
Eﬃcient Syntactic Tree Kernels
% Training Data
Fig. 7. Multiclassiﬁer accuracy according
to diﬀerent training set percentages
% Training Data
Fig. 8. Arg0 classiﬁer learning time according to diﬀerent training percentages
and kernel algorithms
whereas QTK required more than 1 week. However, the real complexity burden
relates to working in the dual space. To alleviate such problem interesting and
eﬀective approaches have been proposed .
Classiﬁcation Accuracy with Automatic Trees on FrameNet
As PropBank arguments are deﬁned with respect to syntactic considerations, we
should verify that the syntactic information provided by tree kernels is also effective to detect other forms of semantic structures. For this purpose, we experimented with our models and FrameNet data (www.icsi.berkeley.edu/∼framenet)
which is mainly produced based on semantic considerations. We extracted all
24,558 sentences from the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org). We considered the 18 most
frequent roles, for a total of 37,948 examples (30% of the sentences for testing
and 70% for training/validation). The sentences were processed with the Collins’
parser to generate automatic parse trees.
Table 2 reports the F1 measure of some argument classiﬁers and the accuracy
of the multiclassiﬁer using all available training data for linear, ST, SST and PT
kernels. We note that: (1) the F1 of the single arguments across the diﬀerent
kernels follows a behavior similar to the accuracy of the global multiclassiﬁer.
(2) The high F1 measures of tree kernels on automatic trees of FrameNet show
that they are robust with respect to parsing errors.
Experiments on Question Classiﬁcation
We used the data set available at uiuc.edu/∼cogcomp/Data/QA/QC/.
This contains 5,500 training and 500 test questions from the TREC 10 QA competition. As we adopted the question taxonomy known as coarse grained introduced
in Section 4, we can compare with literature results, e.g. .
These experiments show that the PT kernel can be superior to the SST kernel
when the source of syntactic information is expressed by dependency rather
than constituent trees. For this purpose, we run the Stanford Parser (available
A. Moschitti
Table 2. Evaluation of kernels on 18
FrameNet semantic roles
Linear ST SST
86.9 87.8 86.2
76.1 79.2 79.4
79.9 82.0 81.7
85.6 87.7 86.6
80.0 81.2 79.9
Table 3. Kernel evaluation on Question
Classiﬁcation according to diﬀerent parsing approaches
Depend. BOW
Kernels SST PT SST
88.2 87.2 82.1 90.4
at to generate both
parse types. Moreover, we used an SVM with the linear kernel over the bagof-words (BOW) as baseline. Columns 2 and 3 of Table 3 show the accuracy
of the SST and PT kernels over the constituent trees, columns 4 and 5 report
the accuracy on the dependency data and Column 6 presents the BOW kernel
We note that (1) the SST kernel is again superior to the PT kernel when
using constituent trees. If we apply the SST kernel on the dependency trees
the resulting accuracy is rather lower than the one of the PT kernel (82.1% vs.
90.4%). This is quite intuitive as the SST kernel cannot generate the features
needed to represent all the possible n-ary relations derivable from father-children
relations. Overall, the accuracy produced by the dependency trees is higher than
the one attainable with the constituent trees. Nevertheless, when the SST kernel
applied to the dependency structures is combined with BOW, the SVM accuracy
reaches 90% as well .
Conclusions
In this paper, we have studied the impact of diverse tree kernels for the learning
of syntactic/semantic linguistic structures. We used the subtree (ST) and the
subset tree (SST) kernels deﬁned in previous work, and we designed a novel
general tree kernel, i.e. the partial tree (PT) kernel. Moreover, we improved the
kernel usability by designing fast algorithms which process syntactic structures
in linear average time.
The experiments with Support Vector Machines on the PropBank and
FrameNet predicate argument structures show that richer kernel spaces are more
accurate, e.g. SSTs and PTs produce higher accuracy than STs. However, if such
structures are not relevant for the representation of the target linguistic objects
improvement does not occur, e.g. PTs are not better than SSTs to describe constituent trees. On the contrary, as suggested by the experiments on Question
Classiﬁcation, the richer space provided by PTs produces a much higher accuracy than SSTs when applied to dependency trees. This because the SST kernel
seems not adequate to process such data.
Finally, the running time experiments show that our fast tree kernels can be
eﬃciently applied to hundreds of thousands of instances.
Eﬃcient Syntactic Tree Kernels