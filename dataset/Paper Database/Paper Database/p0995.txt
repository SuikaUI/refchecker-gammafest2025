UC Riverside
UC Riverside Previously Published Works
Towards Visually Explaining Variational Autoencoders
 
Liu, Wenqian
Zheng, Meng
Publication Date
2020-06-13
10.1109/cvpr42600.2020.00867
Peer reviewed
eScholarship.org
Powered by the California Digital Library
University of California
Towards Visually Explaining Variational Autoencoders
Wenqian Liu∗
Northeastern University
University of California Riverside
Meng Zheng
Rensselaer Polytechnic Institute
Srikrishna Karanam
United Imaging Intelligence
United Imaging Intelligence
University of California Riverside
Richard J. Radke
Rensselaer Polytechnic Institute
Octavia Camps
Northeastern University
Recent advances in Convolutional Neural Network
(CNN) model interpretability have led to impressive
progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention
maps as a means for visual explanations. A key problem,
however, is these methods are designed for classiﬁcation
and categorization tasks, and their extension to explaining
generative models, e.g., variational autoencoders (VAE) is
not trivial.
In this work, we take a step towards bridging this crucial gap, proposing the ﬁrst technique to visually explain VAEs by means of gradient-based attention.
We present methods to generate visual attention from the
learned latent space, and also demonstrate such attention
explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art
performance on the MVTec-AD dataset. We also show how
they can be infused into model training, helping bootstrap
the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset.
1. Introduction
Dramatic progress in computer vision, driven by deep
learning , has led to widespread adoption of
the associated algorithms in real-world tasks, including
healthcare, robotics, and autonomous driving 
contributed
 ,
 , ,
 ,
 ,
 , ,
 .
among others.
Applications in many such safety-critical
and consumer-focusing areas demand a clear understanding of the reasoning behind an algorithm’s predictions, in
addition certainly to robustness and performance guarantees. Consequently, there has been substantial recent interest in devising ways to understand and explain the underlying why driving the output what.
Explanation
Figure 1. We propose to visually explain Variational Autoencoders. Each element in the latent vector can be explained separately with attention maps generated by our method. Examples
in this ﬁgure show that the attention maps can visualize consistent explanations of each latent variable (z1 −z3) across different
Following the work of Zeiler and Fergus , much recent effort has been expended in developing ways to visualize feature activations in convolutional neural networks
(CNNs). One line of work that has seen increasing adoption involves network attention , typically visualized
by means of attention maps that highlight feature regions
considered (by the trained model) to be important for satisfying the training criterion. Given a trained CNN model,
these techniques are able to generate attention maps that visualize where a certain object, e.g., a cat, is in the image,
helping explain why this image is classiﬁed as belonging to
 
the cat category. Some extensions provide ways to
use the generated attention maps as part of trainable constraints that are enforced during model training, showing
improved model generalizability as well as visual explainability. While Zheng et al. used a classiﬁcation module
to show how one can generate a pair of such attention maps
to explain why two images of people are similar/dissimilar,
all these techniques, by design, need to perform classiﬁcation to guide model explainability, limiting their use to
object categorization problems.
Starting from such classiﬁcation model explainability,
one would naturally like to explain a wider variety of neural
network models and architectures. For instance, there has
been an explosion in the use of generative models following the work of Kingma and Welling and Goodfellow
et al. , and subsequent successful applications in a variety of tasks . While progress in algorithmic
generative modeling has been swift , explaining
such generative algorithms is still a relatively unexplored
ﬁeld of study. There are certainly some ongoing efforts in
using the concept of visual attention in generative models
 , but the focus of these methods is to use attention
as an auxiliary information source for the particular task of
interest, and not visually explain the generative model itself.
In this work, we take a step towards bridging this crucial
gap, developing new techniques to visually explain Variational Autoencoders (VAE) . Note that while we use
VAEs as an instantiation of generative models in our work,
some of the ideas we discuss are not limited to VAEs and
can certainly be extended to GANs as well. Our intuition is that the latent space of a trained VAE encapsulates key properties of the VAE and that generating explanations conditioned on the latent space will help explain the
reasoning for any downstream model predictions. Given a
trained VAE, we present new ways to generate visual attention maps from the latent space by means of gradient-based
attention. Speciﬁcally, we given the learned Gaussian distribution, we use the reparameterization trick to sample a latent code. We then backpropagate the activations in
each dimension of the latent code to a convolutional feature
layer in the model and aggregate all the resulting gradients
to generate the attention maps. While these visual attention
maps serve as means to explain the VAE, we can do much
more than just that. A classical application of a VAE is in
anomaly localization, where the intuition is that any input
data that is not from the standard Gaussian distribution used
to train the VAE should be anomalous in the inferred latent
space. Given this inference, we can now generate attention
maps helping visually explain why this particular input is
anomalous. We then also go a step further, presenting ways
in which to use these explanations as cues to precisely localize where the anomaly is in the image. We conduct extensive experiments on the recently proposed MVTec anomaly
detection dataset and present state-of-the-art anomaly localization results with just the standard VAE without any bells
and whistles.
Latent space disentanglement is another important area
of study with VAEs and has seen much recent progress
 . With our visual attention explanations conditioned on the learned latent space, our intuition that using these attention maps as part of trainable constraints
will lead to improved latent space disentanglement. To this
end, we present a new learning objective we call attention
disentanglement loss and show how one can train existing
VAE models with this new loss. We demonstrate its impact
in learning a disentangled embedding by means of experiments on the Dsprites dataset .
To summarize, the key contributions of this work include:
• We take a step towards solving the relatively unexplored problem of visually explaining generative models, presenting new methods to generate visual attention maps conditioned on the latent space of a variational autoencoder.
• Going beyond visual explanations, we show how our
visual attention maps can be put to multipurpose use.
• We present new ways to localize anomalies in images by using our attention maps as cues, demonstrating state-of-the-art localization performance on the
MVTec-AD dataset .
• We present a new learning objective called the attention disentanglement loss, showing how one incorporate it into standard VAE models, and demonstrate improved disentanglement performance on the Dsprites
dataset .
2. Related Work
CNN Visual Explanations.
Much recent effort has
been expended in explaining CNNs as they have come
to dominate performance on most vision tasks.
widely adopted methods that attempt to visualize intermediate CNN feature layers included the work of Zeiler and
Fergus and Mahendran and Vedaldi , where methods to understand the activity within the layers of convolutional nets were presented. Some more recent extensions of
this line of work include visual-attention-based approaches
 , most of which can be categorized into either gradient-based methods or response-based methods.
Gradient-based method such as GradCAM compute
and visualize gradients backpropagated from the decision
unit to a feature convolutional layer. On the other hand,
response-based approaches typically add additional trainable units to the original CNN architecture to
compute the attention maps. In both cases, the goal is to
localize attentive and informative image regions that contribute the most to the model prediction. However, these
methods and their extensions , while able to
explain classiﬁcation/categorization models, cannot be trivially extended to explaining deep generative models such as
VAEs. In this work, we present methods, using the philosophy of gradient-based network attention, to compute and visualize attention maps directly from the learned latent embedding of the VAE. Furthermore, we make the resulting
attention maps end-to-end trainable and show how such a
change can result in improved latent space disentanglement.
Detection.
Unsupervised
anomaly detection still remains challenging.
recent work in anomaly detection is based on either
classiﬁcation-based or reconstruction-based approaches.
Classiﬁcation-based approaches aim to progressively learn representative one-class decision boundaries like hyperplanes or hyperspheres around the
normal-class input distribution to tell outliers/anomalies
However, it was also shown that these methods have difﬁculty dealing with high-dimensional data.
Reconstruction-based models, on the other hand, assume input data that are anomalous cannot be reconstructed well by
a model that is trained only with normal input data. This
principle has been used by several methods based on the
traditional PCA , sparse representation , and more
recently deep autoencoders . In this work, we take a
different approach to tackling this problem. We use the attention maps generated by our proposed VAE visual explanation generation method as cues to localize anomalies. Our
intuition is that representations of anomalous data should be
reﬂected in latent embedding as being anomalous, and that
generating input visual explanations from such an embedding gives us the information we need to localize the particular anomaly.
VAE Disentanglement. Much effort has been expended
in understanding latent space disentanglement for generative models. Early work of Schmidhuber et al. proposed a principle to disentangle latent representations by
minimizing the predictability of one latent dimension given
other dimensions. Desjardins et. al generalized an
approach based on restricted Boltzmann machines to factor the latent variables. Chen et. al extended GAN 
framework to design the InfoGAN to maximise the mutual information between a subset of latent variables and
the observation.
Some of the more recent unsupervised
methods for disentanglement include β-VAE which attempted to explore independent latent factors of variation
in observed data. While still a popular unsupervised framework, β-VAE sacriﬁced reconstruction quality for obtaining
better disentanglement. Chen et. al extended β-VAE to
β-TCVAE by introducing a total correlation-based objective, whereas Mathieu et al. explored decomposition
of the latent representation into two factors for disentanglement, and Kim et al. proposed FactorVAE that encouraged the distribution of representations to be factorial and
independent across the dimensions. While these methods
focus on factorizing the latent representations provided by
each individual latent neuron, we take a different approach.
We enforce learning a disentangled space by formulating
disentanglement constraints based on our proposed visual
explanations, i.e., visual attention maps. To this end, we
propose a new attention disentanglement learning objective
that we quantitatively show provides superior performance
when compared to existing work.
3. Technical Approach
In this section, we present our method to generate explanations for a VAE by means of gradient-based attention. We ﬁrst begin with a brief review of VAEs in Sections 3.1 followed by our proposed method to generate VAE
attention. We discuss our framework for localizing anomalies in images with these attention maps and conduct extensive experiments on the MVTec-AD anomaly detection
dataset , establishing state-of-the-art anomaly localization performance. Next, we show how our generated attention visualizations can assist in learning a disentangled
latent space by optimizing our new attention disentanglement loss. Here, we conduct experiments on the Dsprites
 dataset and quantitatively demonstrate improved disentanglement performance when compared to existing approaches.
3.1. One-Class Variational Autoencoder
A vanilla VAE is essentially an autoencoder that is
trained with the standard autoencoder reconstruction objective between the input and decoded/reconstructed data, as
well as a variational objective term attempts to learn a standard normal latent space distribution. The variational objective is typically implemented with Kullback-Leibler distribution metric computed between the latent space distribution and the standard Gaussian. Given input data x, the
conditional distribution qφ(z|x) of the encoder, the standard
Gaussian distribution p(z), and the reconstructed data ˆx, the
vanilla VAE optimizes:
L = Lr + KL(qφ(z|x)||p(z))
where KL(; ) is the Kullback-Leibler divergence and Lr is
the reconstruction term:
xilog(ˆxi) + (1 −xi)log(1 −ˆxi)
where N is the total number of data samples.
Element-wise Attention
Figure 2. Element-wise Attention generation for VAE.
3.2. Generating VAE Attention
We propose a new technique to generate VAE visual attention by means of gradient-based attention computation.
Our proposed approach is substantially different from all
existing techniques compute the attention maps
by backpropagating the score from a classiﬁcation model,
thus requires class-speciﬁc information (score).
other hand, we are not restricted to such class-wise information, develop an attention mechanism directly using the
learned space, thereby not needing an additional classiﬁcation module that these existing techniques do. As illustrated
in Figure 2 and discussed below, we compute a score from
the latent space, which is then used to calculate gradients
and obtain the attention map.
Speciﬁcally, for each element zi in the latent vector z, we
ﬁrst generate an attention map by computing the gradients
backpropagated to the last convolutional layer as:
αi = GAP{ Relu
where A is the input feature map at the last convolutional
layer of the encoder in the VAE, and GAP{·} and Relu(·)
denote Global Average Pooling and ReLU operations, respectively. We then perform an average pooling operation
to aggregate the attention maps generated from each latent
dimension zi to get the overall VAE attention map as:
where D is the dimensionality of the latent space.
3.3. Generating Anomaly Attention Explanations
We now move on to discuss how our gradient-based
attention generation mechanism can be used to localize
anomaly regions given a trained one-class VAE. Inference
with such a one-class VAE with data it was trained for, i.e.,
normal data (digit “1” for instance), should ideally result
in the learned latent space representing the standard normal
distribution. Consequently, given a testing sample from a
different class (abnormal data, digit “5” for instance), the
latent representation inferred by the learned encoder should
have a large difference when compared to the learned normal distribution.
Given an abnormal input image y ∈Y as input to a
one-class VAE trained on normal images x ∈X, the encoder will infer the corresponding mean and variance µy
i for each latent variable zi to describe the abnormal
data. Given that the learned latent distribution q(zi|x) follows N(µx
i ) in latent space and any anomalies should
deviate from this distribution in the latent space, we deﬁne
a normal difference distribution in sampling the latent code
for anomaly attention generation:
Pq(zi|x)−q(zi|y)(u) = e−[u−(µx
i )]2/[2((σx
i )2 + (σy
for each latent variable zi. Given a latent code sampled
from Pq(zi|X)−q(zi|Y ), we use Equation 4 to compute the
attention map M for abnormal images. Figure 3 provides a
visual summary.
In this section, we evaluate our proposed method to generate
visual explanations as well as perform anomaly localization
with VAEs.
Metrics: We adopt the commonly used the area under the
receiver operating characteristic curve (ROC AUC) for all
quantitative performance evaluation. We deﬁne true positive rate (Tpr) as the percentage of pixels that are correctly classiﬁed as anomalous across the whole testing class,
whereas the false positive rate (Fpr) the percentage of pixels that are wrongly classiﬁed as anomalous. In addition, we
also compute the best intersection-over-union (IOU) score
by searching for the best threshold based on our ROC curve.
Note that we ﬁrst begin with qualitative (visual) evaluation
on the MNIST and UCSD datasets, and then proceed to a
more thorough quantitative evaluation on the MVTec-AD
MNIST. We start by qualitatively evaluating our visual attention maps on the MNIST dataset . Using training images from one single digit class, we train our one-class VAE
model, which will be used to test on all the digit numbers’
Aggregation
Learned Distribution
Element-wise
Attention Generation
Figure 3. Attention generation with One-Class VAE.
Figure 4. Anomaly localization results from the MNIST dataset.
Figure 5. Qualitative results from UCSD Ped1 dataset. L-R: Original test image, ground-truth masks, our anomaly attention localization maps, and difference between input and the VAE’s reconstruction . The anomalies in these samples are moving cars, bicycle, and wheelchair.
testing images. We reshape all the training and testing images to resolution of 28 × 28 pixels.
In Figure 4 (top), we show results with a model trained
on the digit “1” (normal class) and test on all other digits
(each of which becomes an abnormal class). For each test
image, we infer the latent vector using our trained encoder
and use Equation 4 to generate the attention map. As can be
observed in the results, the attention maps computed with
the proposed method is intuitively satisfying. For instance,
let us consider the attention maps generated with digit “7”
as the test image. Our intuition tells us that a key difference between the “1” and the “7” is the top-horizontal bar
in “7”, and our generated attention map indeed highlights
this region. Similarly, the differences between an image of
the digit “2” and the “1” are the horizontal base and the topround regions in the “2”. From the generated attention maps
for “2”, we notice that we are indeed able to capture these
differences, highlighting the top and bottom regions in the
images for “2”. We also show testing results with other digits (e.g., “4”,“9”) as well as with a model trained on digit
“3” and tested on the other digits in the same ﬁgure. We
note similar observations can be made from these results
as well, suggesting that our proposed attention generation
mechanism is indeed able to highlight anomalous regions,
thereby capturing the features in the underlying latent space
that cause a certain data sample to be abnormal.
UCSD Ped1 Dataset: We next test our proposed method
on the UCSD Ped 1 pedestrian video dataset, where the
videos were captured with a stationary camera to monitor
a pedestrian walkway. This dataset includes 34 training sequences and 36 testing sequences, with about 5500 “normal” frames and 3400 “abnormal” frames. We resize the
data to 100 ∗100 pixels for training and testing.
We ﬁrst qualitatively evaluate the performance of our
proposed attention generation method in localizing anomalies. As we can see from Figure 5 (where the corresponding
anomaly of interest is annotated on the left, e.g., bicycle,
Car etc.), our anomaly localization technique with attention
maps performs substantially better than simply computing
the difference between the input and its reconstruction (this
result is annotated as Vanilla-VAE in the ﬁgure). We note
more precise localization of the high-response regions in
our generated attention maps, and these high-response regions indeed correspond to anomalies in these images.
We next conduct a simple ablation study using the
pixel-level segmentation AUROC score against the baseline
method of difference between input data and the reconstruction. We test our proposed attention generation mechanism
with varying levels of spatial resolution by backpropagating to each of the encoder’s convolutional layers: 50 × 50,
25 × 25, and 12 × 12. The results are shown in Table 1
where we see our proposed mechanism gives better performance than the baseline technique.
Vanilla-VAE
Ours(Conv1)
Ours(Conv2)
Ours(Conv3)
Table 1. Anomaly localization results on UCSD Ped1 using pixellevel segmentation AUROC score. We compare results obtained
using our anomaly attention generated with different target network layers to reconstruction-based anomaly localization using
Vanilla-VAE.
MVTec-AD Dataset:
We consider the recently released comprehensive anomaly detection dataset: MVTec
Anomaly Detection (MVTec AD) that provides multiobject, multi-defect natural images and pixel-level ground
truth. This dataset contains 5354 high-resolution color images of different objects/textures, with both normal and defect (abnormal) images provided in the testing set. We resize all images to 256 × 256 pixels for training and testing.
We conduct extensive qualitative and quantitative experiments and summarize results below.
We train a VAE with ResNet18 as our feature encoder and a 32-dimensional latent space. We further use
random mirroring and random rotation, as done in the original work , to generate an augmented training set. Given
a test image, we infer its latent representation z and use
Equation 4 to generate the anomaly attention map. Given
our anomaly attention maps, we generate binary anomaly
localization maps using a variety of thresholds on the pixel
response values, which is encapsulated in the ROC curve.
We then compute and report the area under the ROC curve
(ROC AUC) and generate the best IOU number for our
method based on FPR and TPR from the ROC curve.
The results are shown in Table2, where we compare our
performance with the techniques evaluated in the benchmark paper of Bergmann et al. . From the results, we
note that with our anomaly localization approach using the
proposed VAE attention, we obtain better results on most
of the object categories than the competing methods. It is
worth noting here that some of these methods are specifically designed for the anomaly localization task, whereas
we train a standard VAE and generate our VAE attention
maps for localization. Despite this simplicity, our method
achieves competitive performance, demonstrating the potential of such an attention generation technique to be useful
for tasks other than just model explanation.
We also show some qualitative results in Figure 6. We
show results from 6 categories - three textures and three objects. For each category, we also show four types of defects
provided by the dataset. We plot from the top row to the bottom the original images, ground truth segmentation masks,
and our anomaly attention maps plotted upon input image.
Given a variety types of defects of multiple different categories, our attention maps locate accurately upon anomalies and even more reﬁned than the gt maps(note example
Wood-Scratch, the gt map indicates a much bigger anomaly
area than the actual scratch defect, yet our attention map
captures perfectly the shape of the defect.)
Dictionary
Metal Nut 0.89
Toothbrush0.92
Transistor 0.90
Table 2. Quantitative results for pixel level segmentation on 15
categories from MVTec-AD dataset. For each category, we report
the area under ROC AUC curve on the top row, and best IOU on
the bottom row. We adopt comparison scores from .
3.4. Attention Disentanglement
In the previous section, we discussed how one can generate visual explanations, by means of gradient-based attention, as well as anomaly attention maps for VAEs. We
also discussed and experimentally evaluated using these
anomaly attention maps for anomaly localization on a variety of datasets. We next discuss another application of
Figure 6. Qualitative results from MVTec-AD dataset. In this ﬁgure, we provide results from three texture categories and three
object categories: Wood, Tile, Leather, Hazelnut, Pill, and Metal
Nut. Additionally, for each of these category, we show four different type of defects. As can be seen from the ﬁgure, our anomaly
attention maps accurately localize anomalies through out all the
categories and defects, even more reﬁned than ground truth defect
our proposed VAE attention: VAE latent space disentanglement. Existing approaches for learning disentangled representations of deep generative models focus on formulating factorised, independent latent distributions so as to learn
interpretable data representations. Some examples include
β-VAE , InfoVAE , and FactorVAE , among
others, all of which attempt to model the latent prior with
factorial probability distribution. In this work, we present
an alternative technique, based on our proposed VAE attention, called the attention disentanglement loss. We show
how it can be integrated with existing baselines, e.g., FactorVAE, and demonstrate the resulting impact by means of
qualitative attention maps and quantitatively performance
characterization with standard disentanglement evaluation
Disentanglement
Per-Factor
VAE Losses
Figure 7. Training pipeline for VAE with Attention Disentanglement.
Training with Attention Disentanglement
As we showed earlier, our proposed VAE attention, by
means of gradient-based attention, generates attention maps
that can explain the underlying latent space represented by
the trained VAE. We showed how attention maps intuitively
represent different regions of normal and abnormal images,
directly corresponding to differences in the latent space
(since we generate attention from the latent code). Consequently, our intuition is that using these attention maps
to further bootstrap the training process of the VAE model
should help boost latent space disentanglement. To this end,
our big-picture idea is to use these attention maps as trainable constraints to explicitly force the attention computed
from the various dimensions in latent space to be as disentangled, or separable as possible. Our hypothesis is that
if we are able to achieve this, we will be able to learn an
improved disentangled latent space. To realize this objective, we propose a new loss called the attention disentanglement loss (LAD) that can be easily integrated with existing
VAE-type models (see Figure 7). Note that while we use
the FactorVAE for demonstration in this work, the proposed attention disentanglement loss is no way limited to
this model and can be used in conjunction with other models as well (e.g., β-VAE ). The proposed LAD takes two
attention maps A1 and A2 (each computed from a certain
dimension in the latent space) as input, and attempts to separate the high-response pixel regions in them as much as
possible. This can be mathematically expressed as:
where · is the scalar product operation, and A1
are the (i, j)th pixel in the attention maps A1 and A2 respectively. The proposed LAD is differentiable and can be
directly integrated with the standard FactorVAE training objective LFV, giving us an overall learning objective that can
be expressed as:
L = LFV + LAD
We now train the FactorVAE with our proposed overall
learning objective of Equation 7, and evaluate the impact of
LAD by comparisons with the baseline FactorVAE trained
only with LFV. For this purpose, we use the same evaluation
metric discussed in FactorVAE .
Data: We use the Dsprites dataset for experimental
evaluation. This is a standard dataset used in the disentanglement literature, providing 737,280 binary 64 × 64 2D
shape images.
Quantitative Results: In Table 3, we compare the disentanglement performance of our proposed method with other
competing approaches: baseline FactorVAE and β-
VAE . From Table 3, we note that training with our
proposed LAD results in substantially higher disentanglement scores when compared to the baseline FactorVAE that
is trained with only LFV under the same experimental settings. Speciﬁcally, the average disentanglement score of
our proposed method is around 0.93, signiﬁcantly higher
than the the 0.82 for baseline FactorVAE (γ=40). We also
note our proposed method obtains a higher disentanglement
score when compared to β-VAE as well, which has 0.73 for
β=4. These results demonstrate the potential of both our
proposed VAE attention as well as the associated disentanglement loss LAD in improving the performance of existing
methods in the disentanglement literature. These improved
results are also reﬂected in the qualitative attention map results we discuss next.
β-VAE† 
FactorVAE 
AD-FactorVAE
Table 3. Average disentanglement score on Dsprites Dataset using
Kim et al. metrics. The higher disentanglement score means
the better disentanglement performance. β-VAE† : we reference the performance of β-VAE reported in FactorVAE using
different β values.
Qualitative Results:
Figure 8 shows some attention
maps generated using the baseline FactorVAE and as well
FactorVAE trained with our proposed LAD (called AD-
FactorVAE) using the pipeline discussed above. The ﬁrst
row shows 5 input images, and the next 4 rows show results with our proposed method and the baseline Factor-
VAE. Row 2 shows attention maps generated with AD-
FactorVAE by backpropagating from the latent dimension
with the highest response, whereas row 3 shows attention
Figure 8. Attention separations on Dsprites Dataset. Top row: the
original shape images. Middle two rows: attention separations
obtained from pre-trained AD-FactorVAE. Bottom two rows: attention separations obtained from pre-trained FactorVAE.
maps generated by backpropagating from the latent dimension with the next highest response. Rows 4 and 5 show the
corresponding attention maps with baseline FactorVAE. We
can observe that our proposed method can give better attention separation when compared to the baseline FactorVAE,
with high-response regions in different areas in the image.
4. Summary
We presented new techniques to visually explain variational autoencoders, taking a ﬁrst step towards explaining
deep generative models by means of gradient-based network attention. We showed how one can use the learned latent representation to compute gradients and generate VAE
attention maps, without relying on classiﬁcation-kind of
models existing works use.
We also showed we can go
beyond using the resulting attention maps for explaining
VAEs by demonstrating applicability and performance on
two tasks: anomaly localization and latent space disentanglement. In anomaly localization, we used the fact that an
abnormal input will result in latent variables that do not
conform to the standard Gaussian in gradient backpropagation and attention generation. These anomaly attention
maps were then used as cues to generate pixel-level binary anomaly masks. We evaluated our method on a variety of datasets, showing competitive performance. In latent space disentanglement, we showed how we can use our
VAE attention from each latent dimension in enforcing a
new attention disentanglement learning objective, resulting
in improved attention separability as well as disentanglement performance.