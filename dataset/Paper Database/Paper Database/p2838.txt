Semi-Supervised Kernel Matching for Domain Adaptation
Min Xiao and Yuhong Guo
Department of Computer and Information Sciences
Temple University
Philadelphia, PA 19122, USA
{minxiao, yuhong}@temple.edu
In this paper, we propose a semi-supervised kernel
matching method to address domain adaptation problems where the source distribution substantially differs
from the target distribution. Speciﬁcally, we learn a prediction function on the labeled source data while mapping the target data points to similar source data points
by matching the target kernel matrix to a submatrix of
the source kernel matrix based on a Hilbert Schmidt
Independence Criterion. We formulate this simultaneous learning and mapping process as a non-convex integer optimization problem and present a local minimization procedure for its relaxed continuous form. Our
empirical results show the proposed kernel matching
method signiﬁcantly outperforms alternative methods
on the task of across domain sentiment classiﬁcation.
Introduction
Domain adaptation addresses the problem of exploiting information in a source domain where we have plenty labeled data to help learn a prediction model in a target domain where we have little labeled data . The need for domain adaptation is
prevailing in various applied machine learning areas, such as
natural language processing ,
computer vision and WiFi localization
 .
In many practical domain adaptation problems, the data
distribution in the source domain is substantially different
from the data distribution in the target domain. A key challenge raised in such problems is the feature divergence issue. That is, one cannot ﬁnd support in the source domain
for some critical discriminative features of the target domain while the discriminative features of the source domain are not informative or do not appear in the target domain. This is very common in natural language processing, where different genres often use very different vocabulary to describe similar concepts. For example, in sentiment
classiﬁcation data of product reviews, terms like “harmonious” or “melodic” are positive indicators in Music domain,
Copyright c⃝2012, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
but not in Books domain; similarly, terms like “noise” or
“yelling” are negative indicators in Music domain, but not
in Books domain. In this situation, most domain adaptation
algorithms seek to bridge the gap between the two domains
by re-weighting source instances , self-labeling target instances , inducing a new feature
representation and many other ways.
In this paper, we address the problem of feature representation divergence between the two domains from a novel
perspective. We assume we have a source domain that contains a much larger number of labeled instances and unlabeled instances comparing to the target domain. Instead of
focusing on bridging the cross domain feature divergence,
we employ kernelized representations for instances in each
domain to eliminate the feature representation divergence issue. Speciﬁcally, we ﬁrst produce two kernel matrices with
a given kernel function, one over the instances in the source
domain and one over the instances in the target domain.
Then we learn a prediction function from the labeled source
instances while mapping each target instance to a source instance by matching the target kernel matrix to a submatrix
of the source kernel matrix based on a Hilbert Schmidt Independence Criterion (HSIC). The labeled instances in the
target domain perform as pivot points for class separation.
Each labeled instance in the target domain is guaranteed to
be mapped into a source instance with the same class label. Through the kernel afﬁnity measure, we expect unlabeled target instances to be most likely mapped into corresponding source instances with same labels as well. Moreover, we perform semi-supervised learning by minimizing
the training loss on labeled instances in both domains while
using graph Laplacian regularization terms to incorporate
geometric information from unlabeled instances. Each graph
Laplacian regularizer reﬂects the intrinsic structure of the
instance distribution in each domain. We formulate this simultaneous semi-supervised learning and mapping process
as a non-convex integer optimization problem and present
a local minimization procedure for its relaxed continuous
form. We empirically evaluate two versions of the proposed
method on across domain sentiment classiﬁcation data of
Amazon product reviews, where one tries to extract opinion-
Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence
oriented or sentiment polarity information from a given review text. Our experimental results suggest the proposed approach signiﬁcantly outperforms the feature representation
based across domain sentiment classiﬁcation approaches.
Related Work
Domain adaptation has recently been popularly studied in
machine learning and related ﬁelds. Many domain adaptation approaches have been developed in the literature to cope
with the feature distribution divergence between the source
domain and the target domain. Covariate shift methods attempt to bridge the gap between domains by putting more
weights on source instances that are in the dense region of
the target domain .
These methods however perform poorly for highly divergent
domains characterized by missing features under source distribution for target instances.
Self-labeling adaptation methods, on the other hand, focus on target instances. They train an initial model on labeled source instances and then use it to label target instances. The newly labeled target instances will be used
to update the initial model through self-training or co-training . Their performances greatly depend on the initial model trained from source labeled data and they are not
best suitable for highly divergent domains either.
A number of domain adaptation algorithms address the
domain divergence issue directly from feature representation
learning perspective, including structural correspondence
learning methods , coupled subspace methods and others. They seek to learn a shared representation and distinguish domain-speciﬁc features by exploiting the large
amount of unlabeled data from both the source and target domains. The efﬁcacy of these methods nevertheless depends
on the existence of a certain amount of pivot features that are
used to induce shared feature representations. In addition to
these, the transfer learning work in 
exploits the Hilbert Schmidt Independence Criterion to learn
the mapping of selected features from two domains.
The approach we develop in this work is related to the
feature representation learning methods. But instead of exploring cross domain feature similarities, we focus on cross
domain instance similarities according to kernel representations. Our approach does not need pivot features or feature
correspondence information, but needs only a very small set
of labeled pivot instances from the target domain. Our empirical study shows the proposed approach is more effective
than the feature learning based domain adaptation methods
on across domain sentiment classiﬁcation.
Notation and Setting
In this paper, we consider cross domain prediction model
learning in two domains, a source domain DS and a target domain DT . In the source domain, we have ls labeled instances { , we
have one basic manifold assumption in both domains: if
two points x1, x2 are close in the intrinsic geometry of the
marginal distribution PX, then the conditional distributions
P(Y |x1) and P(Y |x2) are similar. We utilize properties of
Reproducing Kernel Hilbert Spaces (RKHS) to construct our
semi-supervised learning objective which has three types of
components: a kernel matching criterion, prediction losses,
and graph Laplacian regularizers.
Kernel Matching Criterion
The kernel matching criterion is developed to map each instance in the target domain into one instance in the source
domain, according to their geometric similarities expressed
in kernel matrices. In particular, we conduct instance mapping by maximizing a Hilbert Schmidt Independence Criterion (HSIC) over the kernel matrix of the target instances
and the kernel matrix of the mapped source instances. HSIC
 originally measures the independence
between given random variables based on the eigenspectrum of covariance operators in Reproducing Kernel Hilbert
Spaces. proposed an unsupervised kernel sorting method to match object pairs from two
sources of observations by maximizing their dependence
based on the HSIC. In this work we exploit this criterion
in a semi-supervised manner to map pairs of instances to
each other without exact correspondence requirement (since
we do not have two sets of parallel objects in two domains)
but ensuring class separation. We require each labeled instance in the target domain is guaranteed to be mapped into
a source instance with the same class label. The labeled instances in the target domain thus perform as pivot points
for class separation. Through the kernel afﬁnity measures
between instances, we expect unlabeled target instances to
be most likely mapped into corresponding source instances
with same labels as well, following the similar pivot points.
Speciﬁcally, we construct two kernel matrices in the two
domains Ks = Φ(Xs)Φ(Xs)⊤and Kt = Φ(Xt)Φ(Xt)⊤,
where Φ is a feature map function that maps feature vectors
into a Reproducing Kernel Hilbert Space. Then the kernel
matching can be conducted by
(nt −1)−2tr(MKsM ⊤HKtH)
s.t. M ∈{0, 1}nt,ns; M1 = 1; M(1 : lt, 1 : ls)ys = yt
where H = I −
nt 11⊤, I denotes a nt × nt identity matrix, and 1 denotes column vectors with all 1 entries. The
objective function here is a biased estimate of HSIC. It is
known to be sensitive to diagonal dominance. To address
this problem, we can modify the biased HSIC objective in
(1) to reduce bias by removing the main diagonal terms of
the kernel matrices, as suggested in ,
which leads to the following problem
(nt −1)−2tr(M bKsM ⊤H bKtH)
s.t. M ∈{0, 1}nt,ns; M1 = 1; M(1 : lt, 1 : ls)ys = yt
ij(1 −δij) and bKs
ij(1 −δij) are the
kernel matrices with main diagonal terms removed.
Prediction Losses
Supervised learning is conducted on the labeled instances.
We propose to learn a prediction function f : x −→y on the
labeled instances in the source domain, while minimizing
the training losses not only on the labeled source instances,
but also on the labeled target instances that have mapped
prediction values. That is, giving the mapping matrix M, we
conduct supervised training as below
ℓ(M(i, :)f(Xs), yt
where ℓ(·, ·) is a loss function, H is the Reproducing Kernel Hilbert Space (RKHS) associated with the kernel function that produces the kernel matrix Ks; the RKHS norm
H measures the complexity of f function. Penalizing
the RKHS norm imposes smoothness conditions on possible solutions. By the Representer Theorem, the solution to
this minimization problem can be written in terms of kernel
αjKs(j, i),
where α is a ns × 1 coefﬁcient parameter vector. Here we
used a more general form of representation to take the unlabeled instances into account as well. The RKHS norm of f
can then be re-expressed as
Then using a square loss function, the minimization problem
(3) can be rewritten as
∥ys −JsKsα∥2 + βα⊤Ksα
+η∥yt −JtMKsα∥2
where Js is an ls × ns matrix whose ﬁrst ls columns form
an identity matrix and all other entries are 0s; Jt is an lt×nt
matrix whose ﬁrst lt columns form an identity matrix and all
other entries are 0s.
Graph Laplacian Regularization
In addition to the kernel matching criterion and supervised prediction losses presented above, we consider to
incorporate information about the geometric structures of
the marginal distributions, Ps
X, in each domain,
based on the manifold assumption . Speciﬁcally, we will
incorporate the following graph Laplacian terms which approximate manifold regularization
Gs + γt∥Mf∥2
The graphs Gs and Gt denote the afﬁnity graphs constructed
on the source domain and target domain respectively. These
Laplacian terms work as a smoothness functional to ensure
the f function changes smoothly not only on the graph that
approximates the manifold in the source distribution, but
also on the graph that approximates the manifold in the target distribution.
Let G =< V, E > be a weighted adjacency graph on
n vertices. The graph Laplacian L of G is deﬁned as L =
D−W, where W is the edge weight matrix and D is a diagonal matrix such that Dii = P
j Wji. It is easy to see that L
is a symmetric and positive semideﬁnite matrix. Following
this procedure, the graph Laplacian matrices Ls and Lt associated with Gs and Gt can be generated correspondingly.
The graph Laplacian regularization terms in (7) can then be
rewritten as
Gs + γt∥Mf∥2
γsf ⊤Lsf + γtf ⊤M ⊤LtMf
γsα⊤KsLsKsα + γtα⊤KsM ⊤LtMKsα (8)
Finally, combing the three components (2), (6) and (8)
together, we obtain the following joint optimization problem
for semi-supervised kernel matching
M,α ∥ys −JsKsα∥2 + η∥yt −JtMKsα∥2
+ βα⊤Ksα −µtr(M bKsM ⊤H bKtH)
+ γsα⊤KsLsKsα + γtα⊤KsM ⊤LtMKsα
s.t. M ∈{0, 1}nt,ns; M1 = 1; JtMJs⊤ys = yt.
The goal of this optimization problem is to learn a kernel
mapping matrix M as well as a kernelized prediction model
parameterized by α to minimize the regularized training
losses in both domains in a semi-supervised manner.
Optimization Algorithm
The optimization problem (9) we formulated above is an integer optimization problem. Moreover, the objective function is not jointly convex in M and α. Let h(M, α) denote
the objective function of (9). We ﬁrst relax the integer constraints to obtain a continuous relaxation
M,α h(M, α)
s.t. 0 ≤M ≤1; M1 = 1; JtMJs⊤ys = yt.
Then we propose a ﬁrst order local minimization algorithm
to solve the relaxed non-convex optimization problem (10).
First we treat (10) as a non-smooth minimization problem
over M, and re-express the optimization problem as
s.t. 0 ≤M ≤1; M1 = 1; JtMJs⊤ys = yt.
g(M) = min
Note α can be viewed as a function of M, i.e., α(M). For
a given M, a closed-form solution of α(M) can be obtained
by setting the partial derivative of h(M, α) with respect to
α∗(M) = Q−1(KsJs⊤ys + ηKsM ⊤Jt⊤yt)
Q =KsJs⊤JsKs + ηKsM ⊤Jt⊤JtMKs + βKs
+ γsKsLsKs + γtKsM ⊤LtMKs
We then solve the minimization problem (11) using a ﬁrst
order local minimization algorithm with backtracking line
search. The algorithm is an iterative procedure, starting from
a feasible initial point M (0). At the (k + 1)th iteration, we
approximate the objective function g(M) in the close neighborhood of point M (k) using the ﬁrst order Taylor series expansion
g(M) ≈g(M (k)) + tr(G(M (k))⊤(M −M (k)))
where G(M (k)) denotes the gradient of g(M) at point M (k)
(i.e. the gradient of h(M, α∗(M (k)))
G(M (k)) =2ηJt⊤JtM (k)Ksαα⊤Ks −2ηJt⊤ytα⊤Ks
−2µH bKtHM (k) bKs + 2γtLtM (k)Ksαα⊤Ks
Given the gradient at point M (k), we minimize the local linearization (15) to seek a feasible descending direction of M
regarding the constraints,
M = arg min
tr(G(M (k))⊤M)
s.t. 0 ≤M ≤1; M1 = 1; JtMJs⊤ys = yt.
The optimization problem above is a standard convex linear
programming and can be solved using a standard optimization toolbox. The update direction for the (k + 1)th iteration
can be determined as
We then employ a standard backtracking line search to seek an optimal step size ρ∗to
obtain M (k+1) along the direction D in the close neighborhood of M (k): M (k+1) = M (k) + ρ∗D. The line search
procedure will guarantee the M (k+1) leads to an objective
value no worse than before in terms of the original objective
function g(M) = h(M, α∗(M)). The overall algorithm for
minimizing (11) is given in Algorithm 1.
Algorithm 1: Local Optimization Procedure
Input: ys, yt, Ks, Kt; M (0), ϵ; µ, β, γs, γt; MaxIters
Output: M ∗
Initialize k = 0, NoChange = 0 ;
1. Compute gradient G(M (k)) according to Eq. (16).
2. Solve the linear optimization (17) to get c
3. Compute descend direction D using Eq. (18).
4. Conduct backtracking line search to obtain M (k+1).
5. if ∥M (k+1) −M (k)∥2 < ϵ then NoChange = 1.
6. k = k + 1.
Until NoChange = 1 or k > MaxIters
M ∗= M (k).
Algorithm 2: Heuristic Greedy Rounding Procedure
Input: M ∈Rnt×ns, ys, yt.
Output: M ∗∈(0, 1)nt×ns.
Initialize: Set M ∗as a nt × ns matrix with all 0s.
for k = 1 to lt do
Find indices d, s.t. ys(d) = yt(k).
Compute v = arg maxv∈d(M(k, v)).
Set M ∗(k, v) = 1, M(k, :) = −inf.
for k = lt to nt do
Identify the largest value v = max(M(:)).
Identify the indices (d, r) of v in M.
Set M ∗(d, r) = 1, M(d, :) = −inf.
After obtaining the local optimal solution M ∗, we need
to round it back to an integer solution satisfying the linear
constraints in (9). We use a simple heuristic greedy procedure to conduct the rounding. The procedure is described in
Algorithm 2. The quality of the local solution we obtained
depends greatly on the initial M (0). In our experiments, we
used 100 random initializations to pick the best feasible initial M (0) that minimizes the training objective.
Experiments
In this section, we present our experimental results on across
domain sentiment classiﬁcations. We ﬁrst describe our experimental setting and then present results and discussions.
Experimental Setting
We used the across domain sentiment classiﬁcation dataset from in our experiments. The dataset contains reviews in 3 domains (Books,
DVD and Music), and have 4 different language versions
(English, German, French and Japanese). Each domain contains 2000 positive views and 2000 negative reviews, each
of which is represented as a term-frequency (TF) vector. We
used the English version and constructed 6 source-target ordered domain pairs based on the original 3 domains: B2D
(Books to DVD), D2B (DVD to Books), B2M (Books to
Music), M2B (Music to Books), D2M (DVD to Music), and
Table 1: Test accuracies for 6 domain adaptation tasks.
TargetOnly
SourceOnly
SourceTarget
Coupled Subspace
52.40 ± 0.96
71.77 ± 0.43
72.85 ± 0.65
73.63 ± 0.61
74.36 ± 0.47
79.27 ± 0.32
79.34 ± 0.36
51.23 ± 0.52
72.27 ± 0.50
72.15 ± 0.46
72.85 ± 0.52
76.03 ± 0.55
80.04 ± 0.26
79.93 ± 0.23
52.43 ± 0.75
71.16 ± 0.57
71.30 ± 0.57
71.44 ± 0.54
76.75 ± 0.54
78.14 ± 0.46
77.97 ± 0.50
51.23 ± 0.52
68.25 ± 1.30
68.90 ± 0.39
69.38 ± 0.65
75.70 ± 0.52
77.47 ± 0.28
77.34 ± 0.28
52.43 ± 0.75
71.86 ± 0.39
72.44 ± 0.46
72.49 ± 0.37
77.80 ± 0.45
79.70 ± 0.34
79.63 ± 0.29
52.40 ± 0.96
72.12 ± 0.45
72.89 ± 0.50
73.44 ± 0.49
74.59 ± 0.42
78.54 ± 0.32
77.85 ± 0.37
M2D (Music to DVD). For each pair of domains, we built
an unigram vocabulary from combined reviews in both domains. We further preprocessed the data by removing features that appear less than twice in either domain, replacing
TF features with TF-IDF features, and normalizing each attribute into .
The divergence of each pair of domains can be measured
with A−distance . We adopted the
same method in to computed approximate
A−distance values. We ﬁrst trained a linear separator to separate source and target domains with all instances from both.
The average per-instance hinge-loss for this separator subtracted from 1 was used as an estimate of proxy A−distance.
It is a number in the interval of with larger values indicating larger domain divergence. Table 2 presents the vocabulary size and proxy A−distance for each pair of domains
we used in the experiments. We can see that all three pairs
of domains present substantial divergences.
Table 2: Statistics for different domain pairs.
Vocabulary Size
A-distance
Books vs. DVD
Books vs. Music
DVD vs. Music
Approaches
In our experiments, we compared the performance of the following approaches.
• TargetOnly: trained on labeled data in target domain.
• SourceOnly: trained on labeled data in source domain.
• SourceTarget: trained on labeled data in both domains.
adaptation
in .
• Coupled Subspace: the domain adaptation method proposed in .
• SSKMDA1: the proposed semi-supervised kernel matching for domain adaptation.
• SSKMDA2: in addition to SSKMDA1, we also tested another version of semi-supervised kernel matching method
for domain adaptation by replacing the unbiased HSIC
component in Eq.(2) with the unbiased HSIC used in
 .
We used Matlab SVM toolbox for the ﬁrst three baselines
with default hyper-parameters. For Coupled Subspace, we
used the software package provided by 1. There are 2 parameters to set in this package, the top k representative features, and the size of source
and target projectors. We used the same values that are used
in : 1000 for the top representative features and 100 for the dimension of projectors.
For our proposed approach, we used Gaussian kernels to
construct the kernel matrices, K(x1, x2) = exp(−|x1 −
x2|2/(2σ2)), where the parameter σ was set to 0.05. We
used K-nearest-neighbors (KNN) with binary weights to
construct Laplacian graphs Gs and Gt for the source and target domains respectively. We used 20 as the number of nearest neighbors in our experiments. For the tradeoff parameters in our formulated optimization (9), we used β = 0.045,
γs = 0.05, γt = 0.05, η = 1, and µ = 5.
Across Domain Classiﬁcation Results
As we introduced before, our semi-supervised learning is actually a transductive learning. We conducted training with ls
labeled source instances and us unlabeled source instances
as well as lt labeled target instances and ut unlabeled target instances. The performance of the trained classiﬁer was
evaluated on the ut unlabeled target instances.
In the experiments, we used ls = 1390, us = 10, ns =
ls+us = 1400, lt = 10, ut = 990, nt = lt+ut = 1000. We
randomly chose ns instances from the source domain, with
the ﬁrst ls instances labeled and the rest unlabeled. Similarly,
we randomly chose nt instances from the target domain,
with the ﬁrst lt instances labeled and the rest unlabeled. All
approaches were tested using the same data. Each experiment was repeated 10 times. The average test accuracies and
standard deviations for all 6 experiments are reported in Table 1. We can see that neither a few labeled target instances
nor a large amount of labeled source instances alone are
enough to train a good sentiment classiﬁer for the target domain. By simply training over both labeled source instances
and target instances can have very limited improvement. The
EA++ approach demonstrates improvements over the three
baselines, but the improvement is not signiﬁcant. The Coupled Subspace domain adaptation method however presents
signiﬁcant improvement over the ﬁrst three baselines. Nevertheless, it is not as good as our proposed approach (two
1 
Figure 1: Test accuracies with varying number of labeled instances in the target domain for 6 domain adaptation tasks.
versions). Both versions of our proposed domain adaptation
method perform consistently and signiﬁcantly better than all
other approaches over all 6 tasks. For the task B2D, our
approach increases the accuracy by more than 5% comparing to Coupled Subspace. For tasks D2B and M2D, our approach increases the accuracy by about 4%; and about 2%
for tasks B2M, M2D and D2M. The two versions of the
proposed approach achieved very similar results, although
SSKMDA1 is slightly better than SSKMDA2.
Classiﬁcation Results vs Label Complexity
As we introduced before, the labeled target instances perform as pivot points for kernel matching in our proposed
approach. Then we may ask: is the proposed approach sensitive to the number of pivot points? To answer this question and study the target domain label complexity of the proposed approach, we conducted another sets of experiments
with varying number of labeled target instances. In the experiments above, we used lt = 10 which is a reasonably
small number. We thus conducted tests with a set of values lt = {10, 50, 100, 200, 500} here. We still used 1390 labeled instances and 10 unlabeled instances from the source
domain, and used 990 unlabeled instances from the target
domain. The classiﬁcation results are reported on the unlabeled 990 instances from the target domain as well.
We reported the average results over 10 times’ repeats in
Figure 1 for the versions of the proposed approach and four
others: TargetOnly, SourceTarget, EA++ and Coupled Subspace. We can see that both versions of the proposed approach consistently outperform all the other methods over
all 6 domain adaptation tasks and across a set of different
lt values. Moreover, increasing the number of labeled target instances leads to signiﬁcant performance improvement
for the TargetOnly method. The performances of SourceTarget, EA++ and Couple Subspace vary in a small degree due
to the fact there are a lot more labeled source instances,
and these labeled source instances and the labeled target
instances have to work out a compatible solution between
them. The performances of the proposed SSKMDA1 and
SSKMDA2 are quite stable across different lt values. This
suggests the proposed method only requires a very few pivot
points to produce a good prediction model for the target instances. The empirical label complexity of the proposed approach is very small from this perspective.
All these results suggest our proposed method is more effective to handle domain divergence than the feature representation based methods and require much less labeled data
from the target domain.
Conclusion
In this paper, we addressed a key challenge in domain adaptation, the problem of feature representation divergence between two domains, from a novel perspective. We developed a semi-supervised kernel matching method for domain
adaptation based on a Hilbert Schmidt Independence Criterion (HSIC). By mapping the target domain points into
corresponding source domain points in a transductive (semisupervised) way, the classiﬁer trained in the source domain
can reasonably classify the instances in the target domain
as well. The two versions of the proposed method both
achieved superior results on across domain sentiment classiﬁcation tasks comparing to other domain adaptation methods. The empirical results also suggest the proposed method
has a low label complexity in the target domain, and can
greatly reduce human annotation effort.