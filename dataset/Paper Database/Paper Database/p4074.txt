Fast, Accurate and Lightweight Super-Resolution
with Neural Architecture Search
Xiangxiang Chu, Bo Zhang, Hailong Ma, Ruijun Xu
Xiaomi AI Lab
Beijing, China
Email: {chuxiangxiang,zhangbo11,mahailong,xuruijun}@xiaomi.com
Qingyuan Li
Xiaomi IoT
Beijing, China
Email: 
Abstract—Deep convolutional neural networks demonstrate
impressive results in the super-resolution domain. A series of
studies concentrate on improving peak signal noise ratio (PSNR)
by using much deeper layers, which are not friendly to constrained resources. Pursuing a trade-off between the restoration
capacity and the simplicity of models is still non-trivial. Recent
contributions are struggling to manually maximize this balance,
while our work achieves the same goal automatically with neural
architecture search. Speciﬁcally, we handle super-resolution with
a multi-objective approach. We also propose an elastic search
tactic at both micro and macro level, based on a hybrid controller
that proﬁts from evolutionary computation and reinforcement
learning. Quantitative experiments help us to draw a conclusion
that our generated models dominate most of the state-of-the-art
methods with respect to the individual FLOPS.
I. INTRODUCTION AND RELATED WORK
As a classical task in computer vision, single image superresolution (SISR) is aimed to restore a high-resolution image
from a degraded low-resolution one, which is known as an illposed inverse procedure. Most of the recent works on SISR
have shifted their approaches to deep learning, and they have
surpassed other SISR algorithms with big margins , , ,
Nonetheless, these human-designed models are tenuous to
ﬁne-tune or to compress. Meantime, neural architecture search
has produced dominating models in classiﬁcation tasks , .
Following this trend, a novel work by has shed light on the
SISR task with a reinforced evolutionary search method, which
has achieved results outperforming some notable networks
including VDSR . We are distinct to by stepping forward
to design a dense search space which allows searching in both
macro and micro level, which has led to signiﬁcantly better
visual results.
In this paper, we dive deeper into the SISR task with
elastic neural architecture search, hitting a record comparable
to CARN and CARN-M 1. Our main contributions can be
summarized in the following four aspects,
• releasing several fast, accurate and lightweight superresolution architectures and models (FALSR-A being the
best regarding visual effects), which are highly competitive with recent state-of-the-art methods,
• performing elastic search by combining micro and macro
space on the cell-level to boost capacity,
1Our models are released at 
• building super-resolution as a constrained multi-objective
optimization problem and applying a hybrid model generation method to balance exploration and exploitation,
• producing high-quality models that can meet various
requirements under given constraints within a single run.
II. PIPELINE ARCHITECTURE
Like most of Neural Architecture Search (NAS) approaches,
our pipeline contains three principle ingredients: an elastic
search space, a hybrid model generator and a model evaluator
based on incomplete training. It is explained in detail in the
following sections.
Similar to , , we also apply NSGA-II to solve the
multi-objective problem. Our work differs from them by using
a hybrid controller and a cell-based elastic search space that
enables both macro and micro search.
We take three objectives into account for the superresolution task,
• quantitative metric to reﬂect the performance of models
• quantitative metric to evaluate the computational cost of
each model (mult-adds),
• number of parameters.
In addition, we consider the following constraints,
• minimal PSNR for practical visual perception,
• maximal mult-adds regarding resource limits.
III. ELASTIC SEARCH SPACE
Our search space is designed to perform both micro and
macro search. The former is used to choose promising cells
within each cell block, which can be viewed as a feature
extraction selector. In contrast, the latter is aimed to search
backbone connections for different cell blocks, which plays a
role of combining features at selected levels. In addition, we
use one cell block as our minimum search element for two
reasons: design ﬂexibility, and broad representational capacity.
Typically, the super-resolution task can be divided into
three sub-procedures: feature extraction, nonlinear mapping,
and restoration. Since most of the deep learning approaches
concentrate on the second part, we design our search space
to describe the mapping while ﬁxing others. Figure 1 depicts
our main ﬂow for super-resolution. Thus, a complete model
 
Feature Extractor
Subpixel Upsampling
Fig. 1: Neural Architecture of Super-Resolution (the arrows denote skip connections).
contains a predeﬁned feature extractor (a 2D convolution with
32 3 × 3 ﬁlters), n cell blocks drawn from the micro search
space which are joined by the connections from macro search
space, and subpixel-based upsampling and restoration2.
A. Cell-Level Micro Search Space
For simplicity, all cell blocks share the same cell search
space S. In speciﬁc, the micro search space comprises the
following elements:
• convolutions: 2D convolution, grouped convolution with
groups in {2, 4}, inverted bottleneck block with an expansion rate of 2,
• channels: {16, 32, 48, 64},
• kernels: {1, 3},
• in-cell residual connections:{True, False},
• repeated blocks:{1, 2, 4}.
Therefore, the size of micro space for n cell blocks is 192n.
B. Intercell Macro Search Space
The macro search space deﬁnes the connections among
different cell blocks. Speciﬁcally, for the i-th cell block CBi,
there are n + 1 −i choices of connections to build the
information ﬂow from the input of CBi to its following cell
blocks3. Furthermore, we use cj
i to represent the path from
input of CBi to CBj. We set cj
i = 1 if there is a connection
path between them, otherwise 0. Therefore, the size of macro
space for n cell blocks is 2n(n+1)/2. In summary, the size of
the total space is 192n × 2n(n+1)/2.
IV. MODEL GENERATOR
Our model generator is a hybrid controller involving both
reinforcement learning (RL) and an evolutionary algorithm
(EA). The EA part handles the iteration process and RL is
used to bring exploitation. To be speciﬁc, the iteration is controlled by NSGA-II , which contains four sub-procedures:
population initialization, selection, crossover, and mutation. To
avoid verbosity, we only cover our variations to NSGA-II.
2Our upsampling contains a 2D convolution with 32 3×3 ﬁlters, followed
by a 3 × 3 convolution with one ﬁlter of unit stride.
3Here, i starts with 1.
A. Model Meta Encoding
One model is denoted by two parts: forward-connected
cells and their information connections. We use the indices
of operators from the operator set to encode the cells, and a
nested list to depict the connections. Namely, given a model M
with n cells, its corresponding chromosome can be depicted by
(Mmic, Mmac), where Mmic and Mmac are deﬁned as follows,
Mmic = (x1, x2, ..., xn)
Mmac = (c1:n
2 , ..., cn
B. Initialization
We begin with N populations and we emphasize the diversities of cells. In effect, to generate a model, we randomly
sample a cell from S and repeat it for n times. In case N
is larger than the size of S, models are arbitrarily sampled
without repeating cells.
As for connections, we sample from a categorical distribution. While in each category, we pick uniformly, i.e.
p ∼U(0, 1). To formalize, the connections are built based
on the following rules,
random connections
dense connections
pr ≤p < pr + pden
no connnections
pr + pden ≤p < 1
C. Tournament Selection
We calculate the crowding distance as noted in to
render a uniform distribution of our models, and we apply
tournament selection (k = 2) to control the evolution pressure.
D. Crossover
exploration,
single-point
crossovers
simultaneously
space. Given two models A (Mmic(A), Mmac(A)) and B
(Mmic(B), Mmac(B)), a new chromosome C can be generated
Mmic(C) = (x1A, x2A, ..., xiB, ..., xnA)
Mmac(C) = (c1:n
2A , ..., cj:n
jB , ..., cn
where i and j are chosen positions respectively for micro and
macro genes. Informally, the crossover procedure contributes
more to exploitation than to exploration.
E. Mutation
We again apply a categorical distribution to balance exploration and exploitation.
1) Exploration: To encourage exploration, we combine
random mutation with roulette wheel selection (RWS). Since
we treat super-resolution as a multi-objective problem, FLOPS
and the number of parameters are two objectives that can
be evaluated soon after meta encodings are available. In
particular, we also sample from a categorical distribution to
determine mutation strategies, i.e. random mutation (with an
upper-bound probability pmr) or mutated by roulette wheel
selection to handle FLOPS (lower than pmf) or parameters.
random mutation
0 ≤p < pmr
RWS for FLOPS
pmr ≤p < pmf
RWS for params
pmf ≤p < 1
Whenever we need to mutate a model M by RWS, we keep
Mmac unchanged. Since each cell shares the same operator
set S, we perform RWS on S for n times to generate Mmic.
Strictly speaking, given Mmac, it’s intractable to execute a
complete RWS (involving 192n models). Instead, it can be
approximated based on S (involving 192 basic operators).
Besides, we scale FLOPS and the number of parameters
logarithmically before RWS.
2) Exploitation: To enhance exploitation, we apply a reinforcement driven mutation.
We use a neural controller to mutate, which is shown in
Figure 2. Speciﬁcally, the embedding features for Mmic are
concatenated, and then are injected into 3 fully-connected
layers to generate Mmac. The last layer has n(n + 1)/2
neutrons to represent connections, with its output denoted as
connection
Fig. 2: The controller network to generate cells and connections.
The network parameters can be partitioned into two groups,
θmic and θmac. The probability of selecting Si for cell j is
p(celli = Si|θmic) and for the connection cj
i = 1, we have
i = 1|θmac) = Omac
(i−1)∗(n+1−0.5∗i)+j. Thus, the gradient
g(θ) can be calculated as follows:
g(θ) = −∇θ[
log p(celli = Si|θmic) ∗Ri+
cj log Omac
(1 −cj) log(1 −Omac
In Equation 6, Ri and Rj are the discounted accumulated
rewards. Here, we set the discount parameter γ = 1.0.
V. EVALUATOR
The evaluator calculates the scores of the models generated
by the controller. In the beginning, we attempted to train an
RNN regressor to predict the performances of models, with
data collected in previous pipeline execution. However, its
validation error is too high to continue. Instead, each model is
trained for a relatively short time (see the ‘incomplete training’
part in Section VI-A) to roughly differentiate various models.
At the end of the incomplete training, we evaluate mean square
errors on test datasets.
VI. EXPERIMENTS
In our experiment, about 10k models are generated in total,
where the population for each iteration is 64. The Pareto-front
of all the models is shown in Fig. 6. It takes less than 3 days
on a Tesla-V100 with 8 GPUs to execute the pipeline once.
We use DIV2K as our training set.
During an incomplete training, each model is trained with
a batch size of 16 for 200 epochs. In addition, we apply
Adam optimizer (β1 = 0.9, β2 = 0.999) to minimize the
L1 loss between the generated high-resolution images and its
ground truth. The learning rate is initialized as 10−4 and kept
unchanged at this stage.
As for the full train, we choose 4 models with a large
crowding distance in the Pareto front between mean squared
error and mult-adds, which was generated at the incomplete
training stage. These models are trained based on DIV2K
dataset for 24000 epochs with a batch-size of 16 and it takes
less than 1.5 days. Moreover, the standard deviation of weights
w is initialized as 0.02 and the bias 0.
B. Comparisons with State-of-the-Art Super-Resolution Methods
After being fully trained, our model are compared with the
state-of-the-art methods on the commonly used test dataset
for super-resolution (See Table I and Figure 5). To be fair, we
only consider the models with comparable FLOPS. Therefore,
too deep and large models such as RDN , RCAN 
are excluded here. We choose PSNR and SSIM as metrics by
convention . The comparisons are made on the ×2 task.
Note that all mult-adds are measured based on a 480 × 480
TABLE I: Comparisons with the state-of-the-art methods based on ×2 super-resolution task.
36.66/0.9542
32.42/0.9063
31.36/0.8879
29.50/0.8946
FSRCNN 
37.00/0.9558
32.63/0.9088
31.53/0.8920
29.88/0.9020
37.53/0.9587
33.03/0.9124
31.90/0.8960
30.76/0.9140
37.63/0.9588
33.04/0.9118
31.85/0.8942
30.75/0.9133
LapSRN 
37.52/0.9590
33.08/0.9130
31.80/0.8950
30.41/0.9100
37.74/0.9591
33.23/0.9136
32.05/0.8973
31.23/0.9188
SelNet 
37.89/0.9598
33.61/0.9160
32.08/0.8984
37.76/0.9590
33.52/0.9166
32.09/0.8978
31.92/0.9256
CARN-M 
37.53/0.9583
33.26/0.9141
31.92/0.8960
31.23/0.9194
MoreMNAS-A 
37.63/0.9584
33.23/0.9138
31.95/0.8961
31.24/0.9187
AWSRN-M 
38.04/0.9605
33.66/0.9181
32.21/0.9000
32.23/0.9294
FALSR-A (ours)
37.82/0.9595
33.55/0.9168
32.12/0.8987
31.93/0.9256
FALSR-B (ours)
37.61/0.9585
33.29/0.9143
31.97/0.8967
31.28/0.9191
FALSR-C (ours)
37.66/0.9586
33.26/0.9140
31.96/0.8965
31.24/0.9187
feature extraction
conv f64 k3 b4 isskip
conv f48 k1 b1 isskip
conv f64 k3 b4 isskip
conv f64 k3 b4 isskip
conv f64 k3 b4 isskip
conv f64 k1 b4 noskip
conv f64 k3 b4 isskip
sub-pixel upsampling
Fig. 3: The model FALSR-A (the one with best visual effects)
comparable to CARN. Note for instance, ‘conv f64 k3 b4 isskip’
represents a block of 4 convolution layers, each with a ﬁlter size of
64 and a kernel size of 3×3, including a skip connection to form
residual structure.
At a comparable level of FLOPS, our model called FALSR-
A (Figure 3) outperforms CARN with higher scores. In
addition, it dominates DRCN and MoreMNAS-A over
three objectives on four datasets. Moreover, it achieves higher
PSNR and SSIM with fewer FLOPS than VDSR , DRRN
 and many others.
For a more lightweight version, one model called FALSR-
B (Figure 4) dominates CARN-M, which means with fewer
FLOPS and a smaller number of parameters it scores equally to
feature extraction
invertBotConE2 f16 k3 b1 isskip
invertBotConE2 f48 k1 b2 isskip
conv f16 k1 b2 isskip
invertBotConE2 f32 k3 b4 noskip
conv f64 k3 b2 noskip
groupConG4 f16 k3 b4 noskip
conv f16 k3 b1 isskip
sub-pixel upsampling
Fig. 4: The model FALSR-B comparable to CARN-M.
or higher than CARN-M. Besides, its architecture is attractive
and the complexity of connections lies in between residual
and dense connections. This means a dense connection is
not always the optimal way to transmit information. Useless
features from lower layers could make trouble for high layers
to restore super-resolution results.
Another lightweight model called FALSR-C (not drawn
because of space) also outperforms CARN-M. This model uses
relatively sparse connections (8 in total). We conclude that this
sparse ﬂow works well with the selected cells.
Figure 7 shows the qualitative results against other methods.
MultAdds (G)
PSNR on Set5
MoreMNAS-A
MoreMNAS-B
MoreMNAS-C
MoreMNAS-D
Fig. 5: FALSR A, B, C (shown in salmon) vs. others (light blue)
Fig. 6: The Pareto-front of of all the models during the evolution,
paired every two objectives.
C. Discussions
1) Cell Diversity:
Our experiments show that a good
cell diversity also helps to achieve better results for superresolution, same for classiﬁcation tasks . In fact, we have
trained several models with repeated blocks, however, they
underperform the models with diverse cells. We speculate
that different types of cells can handle input features more
effectively than monotonous ones.
2) Optimal Information Flow: Perhaps under given current
technologies, dense connections are not optimal in most cases.
In principle, a dense connection has the capacity to cover other
non-dense conﬁgurations, however, it’s usually difﬁcult to train
a model to ignore useless information.
3) Good Assumption?: Super-resolution is different from
feature extraction domains such as classiﬁcation, where more
details need to be restored at pixel level. Therefore, it rarely
applies downsampling operations to reduce the feature dimensions and it is more time-consuming than classiﬁcation tasks
like on CIFAR-10.
Regarding the time, we use incomplete training to differentiate various models. This strategy works well under an implicit
assumption: models that perform better when fully trained
also behave well with a large probability under an incomplete
training. Luckily, most of deep learning tasks share this good
feature. For the rest, we must train models as fully as possible.
VII. CONCLUSIONS
To sum up, we presented a novel elastic method for NAS
that incorporates both micro and macro search, dealing with
neural architectures in multi-granularity. The result is exciting
as our generated models dominate the newest state-of-theart SR methods. Different from human-designed and singleobjective NAS models, our methods can generate different
tastes of models by one run, ranging from fast and lightweight
to relatively large and more accurate. Therefore, it offers a feasible way for engineers to compress existing popular humandesigned models or to design various levels of architectures
accordingly for constrained devices.
Our future work will focus on training a model regressor,
which estimates the performance of models, to speed up the