A Survey on Deep Transfer Learning
Chuanqi Tan1, Fuchun Sun2, Tao Kong1,
Wenchang Zhang1, Chao Yang1, and Chunfang Liu2
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology (TNList)
Department of Computer Science and Technology, Tsinghua University
1{tcq15, kt14, zhangwc14, yang-c15}@mails.tsinghua.edu.cn
2{fcsun, cﬂiu1985}@tsinghua.edu.cn
Abstract. As a new classiﬁcation platform, deep learning has recently
received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics
and robotics, it is very diﬃcult to construct a large-scale well-annotated
dataset due to the expense of data acquisition and costly annotation,
which limits its development. Transfer learning relaxes the hypothesis
that the training data must be independent and identically distributed
(i.i.d.) with the test data, which motivates us to use transfer learning
to solve the problem of insuﬃcient training data. This survey focuses
on reviewing the current researches of transfer learning by using deep
neural network and its applications. We deﬁned deep transfer learning,
category and review the recent research works based on the techniques
used in deep transfer learning.
Keywords: Deep Transfer Learning, Transfer Learning, Survey.
Introduction
Deep learning has recently received increasing attention from researchers and has
been successfully applied to numerous real-world applications. Deep learning
algorithms attempt to learn high-level features from mass data, which make
deep learning beyond traditional machine learning. It can automatic extract
data features by unsupervised or semi-supervised feature learning algorithm and
hierarchical feature extraction. In contrast, traditional machine learning methods
need to design features manually that seriously increases the burden on users.
It can be said that deep learning is an representation learning algorithm based
on large-scale data in machine learning.
Data dependence is one of the most serious problem in deep learning. Deep
learning has a very strong dependence on massive training data compared to
traditional machine learning methods, because it need a large amount of data
to understand the latent patterns of data. An interesting phenomenon can be
found that the scale of the model and the size of the required amount of data has
a almost linear relationship. An acceptable explanation is that for a particular
problem, the expressive space of the model must be large enough to discover
 
Chuanqi Tan et al.
the patterns under the data . The pre-order layers in the model can identify
high-level features of training data, and the subsequent layers can identify the
information needed to help make the ﬁnal decision.
Insuﬃcient training data is a inescapable problem in some special domains.
The collection of data is complex and expensive that make it is extremely diﬃcult
to build a large-scale, high-quality annotated dataset. For example, each sample
in bioinformatics dataset often demonstration a clinical trial or a painful patient.
In addition, even we obtain training dataset by paid an expensive price, it is very
easy to get out of date and thus cannot be eﬀectively applied in the new tasks.
Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates
us to use transfer learning to against the problem of insuﬃcient training data.
In transfer learning, the training data and test data are not required to be i.i.d.,
and the model in target domain is not need to trained from scratch, which can
signiﬁcantly reduce the demand of training data and training time in the target
In the past, most studies of transfer learning were conducted in traditional
machine learning methods. Due to the dominance position of deep learning in
modern machine learning methods, a survey on deep transfer learning and its
applications is particularly important. The contributions of this survey paper
are as follows:
– We deﬁne the deep transfer learning and categorizing it into four categories
for the ﬁrst time.
– We reviewing the current research works on each category of deep transfer learning, and given a standardized description and sketch map of every
Deep Transfer Learning
Transfer learning is an important tool in machine learning to solve the basic
problem of insuﬃcient training data. It try to transfer the knowledge from the
source domain to the target domain by relaxing the assumption that the training
data and the test data must be i.i.d. This will leads to a great positive eﬀect on
many domains that are diﬃcult to improve because of insuﬃcient training data.
The learning process of transfer learning illustrated in the Fig. 1.
Some notations used in this survey need to be clearly deﬁned. First of all,
we give the deﬁnitions of a domain and a task respectively: A domain can be
represented by D = {χ, P(X)}, which contains two parts: the feature space χ
and the edge probability distribution P(X) where X = {x1, ..., xn} ∈χ. A task
can be represented by T = {y, f(x)}. It consists of two parts: label space y
and target prediction function f(x). f(x) can also be regarded as a conditional
probability function P(y|x). Then, the transfer learning can be formal deﬁned
as follows:
A Survey on Deep Transfer Learning
Learning Task
Learning Task
Target Domain
Source Domain
Fig. 1. Learning process of transfer learning.
Deﬁnition 1. (Transfer Learning). Given a learning task Tt based on Dt,
and we can get the help from Ds for the learning task Ts. Transfer learning aims
to improve the performance of predictive function fT (·) for learning task Tt by
discover and transfer latent knowledge from Ds and Ts, where Ds ̸= Dt and/or
Ts ̸= Tt. In addition, in the most case, the size of Ds is much larger than the
size of Dt, Ns ≫Nt.
Surveys and divide the transfer learning methods into three major
categories with the relationship between the source domain and the target domain, which has been widely accepted. These suverys are good summary of the
past works on transfer learning, which introduced a number of classic transfer
learning methods. Further more, many newer and better methods have been proposed recently. In recent years, transfer learning research community are mainly
focused on the following two aspects: domain adaption and multi-source domains
Nowadays, deep learning has achieved dominating situation in many research
ﬁelds in recent years. It is important to ﬁnd how to eﬀectively transfer knowledge
by deep neural network, which called deep transfer learning that deﬁned as
Deﬁnition 2. (Deep Transfer Learning). Given a transfer learning task de-
ﬁned by ⟨Ds, Ts, Dt, Tt, fT (·)⟩. It is a deep transfer learning task where fT (·) is
a non-linear function that reﬂected a deep neural network.
Categories
Deep transfer learning studies how to utilize knowledge from other ﬁelds by
deep neural networks. Since deep neural networks have become popular in various ﬁelds, a considerable amount of deep transfer learning methods have been
proposed that it is very important to classify and summarize them. Based on the
techniques used in deep transfer learning, this paper classiﬁes deep transfer learning into four categories: instances-based deep transfer learning, mapping-based
Chuanqi Tan et al.
deep transfer learning, network-based deep transfer learning, and adversarialbased deep transfer learning, which are shown in Table 1.
Table 1. Categorizing of deep transfer learning.
Approach category Brief description
Some related works
Instances-based
Utilize instances in source domain by appropriate weight.
 , , , ,
 , , 
Mapping-based
Mapping instances from two domains into a
new data space with better similarity.
 , , , , 
Network-based
Reuse the partial of network pre-trained in
the source domain.
 , , , ,
 , , 
Adversarial-based
Use adversarial technology to ﬁnd transferable features that both suitable for two domains.
 , 
Instances-based deep transfer learning
Instances-based deep transfer learning refers to use a speciﬁc weight adjustment strategy, select partial instances from the source domain as supplements
to the training set in the target domain by assigning appropriate weight values
to these selected instances. It is based on the assumption that ”Although there
are diﬀerent between two domains, partial instances in the source domain can
be utilized by the target domain with appropriate weights.”. The sketch map of
instances-based deep transfer learning are shown in Fig. 2.
Source Domain
Target Domain
Fig. 2. Sketch map of instances-based deep transfer learning. Instances with light blue
color in source domain meanings dissimilar with target domain are exclude from training dataset; Instances with dark blue color in source domain meanings similar with
target domain are include in training dataset with appropriate weight.
TrAdaBoost proposed by use AdaBoost-based technology to ﬁlter out instances that are dissimilar to the target domain in source domains. Re-weighted
A Survey on Deep Transfer Learning
instances in source domain to compose a distribution similar to target domain.
Finally, training model by using the re-weighted instances from source domain
and origin instances from target domain. It can reduce the weighted training error on diﬀerent distribution domains that preserving the properties of AdaBoost.
TaskTrAdaBoost proposed by is a fast algorithm promote rapid retraining
over new targets. Unlike TrAdaBoost is designed for classiﬁcation problems,
ExpBoost.R2 and TrAdaBoost.R2 were proposed by to cover the regression
problem. Bi-weighting domain adaptation (BIW) proposed can aligns the
feature spaces of two domains into the common coordinate system, and then
assign an appropriate weight of the instances from source domain. propose
a enhanced TrAdaBoost to handle the problem of interregional sandstone microscopic image classiﬁcation. propose a metric transfer learning framework
to learn instance weights and a distance of two diﬀerent domains in a parallel
framework to make knowledge transfer across domains more eﬀective. introduce an ensemble transfer learning to deep neural network that can utilize
instances from source domain.
Mapping-based deep transfer learning
Mapping-based deep transfer learning refers to mapping instances from the
source domain and target domain into a new data space. In this new data space,
instances from two domains are similarly and suitable for a union deep neural
network. It is based on the assumption that ”Although there are diﬀerent between
two origin domains, they can be more similarly in an elaborate new data space.”.
The sketch map of instances-based deep transfer learning are shown in Fig. 3.
Source Domain
Target Domain
New Data Space
Fig. 3. Sketch map of mapping-based deep transfer learning. Simultaneously, instances
from source domain and target domain are mapping to a new data space with more
similarly. Consider all instances in the new data space as the training set of the neural
Chuanqi Tan et al.
Transfer component analysis (TCA) introduced by and TCA-based methods had been widely used in many applications of traditional transfer learning. A natural idea is extend the TCA method to deep neural network. 
extend MMD to comparing distributions in a deep neural network, by introduces an adaptation layer and an additional domain confusion loss to learn a
representation that is both semantically meaningful and domain invariant. The
MMD distance used in this work is deﬁned as
DMMD(XS, XT ) =
and the loss function is deﬁned as
L = LC(XL, y) + λD2
MMD(XS, XT ).
 improved previous work by replace MMD distance with multiple kernel variant MMD (MK-MMD) distance proposed by . The hidden layer related with
the learning task in the convolutional neural networks (CNN) is mapped into
the reproducing kernel Hilbert space (RKHS), and the distance between diﬀerent
domains is minimized by the multi-core optimization method. propose joint
maximum mean discrepancy (JMMD) to measurement the relationship of joint
distribution. JMMD was used to generalize the transfer learning ability of the
deep neural networks (DNN) to adapt the data distribution in diﬀerent domain
and improved the previous works. Wasserstein distance proposed by can be
used as a new distance measurement of domains to ﬁnd better mapping.
Network-based deep transfer learning
Network-based deep transfer learning refers to the reuse the partial network that
pre-trained in the source domain, including its network structure and connection
parameters, transfer it to be a part of deep neural network which used in target
domain. It is based on the assumption that ”Neural network is similar to the
processing mechanism of the human brain, and it is an iterative and continuous
abstraction process. The front-layers of the network can be treated as a feature
extractor, and the extracted features are versatile.”. The sketch map of networkbased deep transfer learning are shown in Fig. 4.
 divide the network into two parts, the former part is the language-independent
feature transform and the last layer is the language-relative classiﬁer. The languageindependent feature transform can be transfer between multi languages. 
reuse front-layers trained by CNN on the ImageNet dataset to compute intermediate image representation for images in other datasets, CNN are trained to
learning image representations that can be eﬃciently transferred to other visual
recognition tasks with limited amount of training data. proposed a approach
to jointly learn adaptive classiﬁers and transferable features from labeled data
in the source domain and unlabeled data in the target domain, which explicitly
learn the residual function with reference to the target classiﬁer by plugging
A Survey on Deep Transfer Learning
Source Domain
Target Domain
Fig. 4. Sketch map of network-based deep transfer learning. First, network was trained
in source domain with large-scale training dataset. Second, partial of network pretrained for source domain are transfer to be a part of new network designed for target
domain. Finally, the transfered sub-network may be updated in ﬁne-tune strategy.
several layers into deep network. learning domain adaptation and deep hash
features simultaneously in a DNN. proposed a novel multi-scale convolutional
sparse coding method. This method can automatically learns ﬁlter banks at different scales in a joint fashion with enforced scale-speciﬁcity of learned patterns,
and provides an unsupervised solution for learning transferable base knowledge
and ﬁne-tuning it towards target tasks. apply deep transfer learning to transfer knowledge from real-world object recognition tasks to glitch classiﬁer for the
detector of multiple gravitational wave signals. It demonstrate that DNN can
be used as excellent feature extractors for unsupervised clustering methods to
identify new classes based on their morphology, without any labeled examples.
Another very noteworthy result is that point out the relationship between
network structure and transferability. It demonstrated that some modules may
not inﬂuence in-domain accuracy but inﬂuence the transferability. It point out
what features are transferable in deep networks and which type of networks
are more suitable for transfer. Given an conclusion that LeNet, AlexNet, VGG,
Inception, ResNet are good chooses in network-based deep transfer learning.
Adversarial-based deep transfer learning
Adversarial-based deep transfer learning refers to introduce adversarial technology inspired by generative adversarial nets (GAN) to ﬁnd transferable representations that is applicable to both the source domain and the target domain.
It is based on the assumption that ”For eﬀective transfer, good representation
should be discriminative for the main learning task and indiscriminate between
the source domain and target domain.” The sketch map of adversarial-based
deep transfer learning are shown in Fig. 5.
Chuanqi Tan et al.
Source Domain
Target Domain
Source label
Target label
Domain label
Adversarial Layer
Fig. 5. Sketch map of adversarial-based deep transfer learning. In the training process
on large-scale dataset in the source domain, the front-layers of network is regarded as
a feature extractor. It extracting features from two domains and sent them to adversarial layer. The adversarial layer try to discriminates the origin of the features. If the
adversarial network achieves worse performance, it means a small diﬀerence between
the two types of feature and better transferability, and vice versa. In the following
training process, the performance of the adversarial layer will be considered to force
the transfer network discover general features with more transferability.
The adversarial-based deep transfer learning has obtained the ﬂourishing
development in recent years due to its good eﬀect and strong practicality. 
introduce adversarial technology to transfer learning for domain adaption, by
using a domain adaptation regularization term in the loss function. proposed
an adversarial training method that suitable for most any feed-forward neural
model by augmenting it with few standard layers and a simple new gradient
reversal layer. proposed a approach transfer knowledge cross-domain and
cross-task simultaneity for sparsely labeled target domain data. A special joint
loss function was used in this work to force CNN to optimize both the distance
between domains which deﬁned as LD = Lc +λLadver, where Lc is classiﬁcation
loss, Ladver is domain adversarial loss. Because the two losses stand in direct
opposition to one another, an iterative optimize algorithm are introduced to
update one loss when ﬁxed another. proposed a new GAN loss and combine with discriminative modeling to a new domain adaptation method. 
proposed a randomized multi-linear adversarial networks to exploit multiple feature layers and the classiﬁer layer based on a randomized multi-linear adversary
to enable both deep and discriminative adversarial adaptation. utilize a
domain adversarial loss, and generalizes the embedding to novel task using a
metric learning-based approach to ﬁnd more tractable features in deep transfer
Conclusion
In this survey paper, we have review and category current researches of deep
transfer learning. Deep transfer learning is classiﬁed into four categories for the
A Survey on Deep Transfer Learning
ﬁrst time: instances-based deep transfer learning, mapping-based deep transfer learning, network-based deep transfer learning, and adversarial-based deep
transfer learning. In most practical applications, the above multiple technologies
are often used in combination to achieve better results. Most current researches
focuses on supervised learning, how to transfer knowledge in unsupervised or
semi-supervised learning by deep neural network may attract more and more
attention in the future. Negative transfer and transferability measures are important issues in traditional transfer learning. The impact of these two issues in
deep transfer learning also requires us to conduct further research. In addition,
a very attractive research area is to ﬁnd a stronger physical support for transfer
knowledge in deep neural network, which requires the cooperation of physicists,
neuroscientists and computer scientists. It can be predicted that deep transfer
learning will be widely applied to solve many challenging problems with the
development of deep neural network.