HAL Id: hal-01256422
 
Submitted on 28 Apr 2022
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Collaborative Filtering with Stacked Denoising
AutoEncoders and Sparse Inputs
Florian Strub, Jérémie Mary, Preux Philippe
To cite this version:
Florian Strub, Jérémie Mary, Preux Philippe. Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs.
NIPS Workshop on Machine Learning for eCommerce, Dec 2015,
Montreal, Canada. ￿hal-01256422v2￿
Collaborative Filtering with Stacked
Denoising AutoEncoders and Sparse Inputs
Florian Strub
Univ-Lille, CRISTaL
Inria - SequeL
 
Jeremie Mary
Univ-Lille, CRISTaL
Inria, SequeL
 
Philippe Preux
Univ-Lille, CRISTaL
Inria, SequeL
 
Neural networks have received little attention in Collaborative Filtering.
For instance, no paper using neural networks was published during the
Netﬂix Prize apart from Salakhutdinov et al’s work on Restricted Boltzmann Machine (RBM) . While deep learning has tremendous success
in image and speech recognition, sparse inputs received less attention and
remains a challenging problem for neural networks.
Nonetheless, sparse
inputs are critical for collaborative ﬁltering. In this paper, we introduce a
neural network architecture which computes a non-linear matrix factorization from sparse rating inputs. We show experimentally on the movieLens
and jester dataset that our method performs as well as the best collaborative ﬁltering algorithms. We provide an implementation of the algorithm
as a reusable plugin for Torch , a popular neural network framework.
Introduction
Recommendation systems try to give advice to users on which items (movies, music, products, etc.) users are more likely to be interested in. A good recommendation system may
dramatically increase the amount of sales of a company and retain customers. One of the
most important topics in recommendation system is collaborative ﬁltering. It aims at predicting the preference of one user by combining his feedback on a few items and the feedback
of all other users. For instance, if someone rated only a few books, can we estimate the
ratings he would have given to thousands of other books by using the ratings of all the
other readers? Can we turn a sparse matrix of past ratings into a dense matrix of estimate
The most successful approach to collaborative ﬁltering is to retrieve potential latent factors
from the sparse matrix of ratings. Book latent factors are likely to encapsulate the book
genre (spy novel, fantasy, etc.) or some writing styles. Common latent factor techniques are
to compute a low-rank rating matrix by either applying Singular Value Decomposition SVD
 or Regularized Alternative Least Square algorithm . However, it was argued that
these methods are linear and cannot catch subtle factors. Newer algorithms were explored
to face those constraints such as Non Linear Probabilistic Matrix Factorization (NL-PMF)
 , Local Low Rank Matrix Approximation (LLORMA) or Factorization Machines
(FM) which are more ﬂexible extensions of this approach.
Neural networks have already been studied in recommendation systems such as RankNet
 to rank users’ preferences. Yet, few works have tried to apply neural networks to collaborative ﬁltering. It is all the more surprising considering that deep neural networks are
able to discover non-linear latent variables . In a preliminary work, Salakhutdinov 
tackled the Netﬂix challenge using Restricted Boltzmann Machines. This algorithm is well
known in the deep learning community as an important tool to initialize neural networks.
Yet, the initial paper claimed that they did not manage to turn it into a neural network.
One natural approach to deal with collaborative ﬁltering with neural networks is to use
autoencoders. Those networks are trained to reconstruct their inputs through a dimension
reduction. Thus, they perform a Non Linear Principal Component Analysis (NPCA) .
In our case, the autoencoder is fed with sparse rating inputs and it aims at reconstructing
dense rating vectors. Very few papers exist that tackled this sparsity constraint . In
most previous works, sparsity was avoided in neural networks. Even so, autoencoders are
becoming an important topic in collaborative ﬁltering. Hao Wang used autoencoders to
learn a representation of bag of words of plot to estimate the ratings of movies. AutoRec ,
which was developed independently from our work, shares similarities with our approach.
Yet, they use diﬀerent error constraints to train their autoencoders.
In this paper, we develop a training strategy to perform collaborative ﬁltering using Stacked
Denoising AutoEncoders neural networks (SDAE) with sparse inputs. We show that neural
networks provide excellent experimental results. Benchmarks are done on RMSE metric
which is commonly used to evaluate collaborative ﬁltering algorithms. We developed several
new Torch modules as the framework lacks some important tools to deal with sparse inputs.
Reusable source code is provided to reproduce the results and to be used to build new
networks. First, we introduce denoising autoencoders, we then explain the constraints we
made to enforce sparsity in section 2. After explaining the benchmark models, we describe
our experimental settings and results, respectively in section 3 and 4. Finally, we provide
an underlying motivation which entails future potential works in section 5.
Denoising autoencoders for sparse inputs
Denoising Autoencoders
Autoencoders are feed-forward neural networks popularized by Kramer . They are unsupervised networks where the output of the network only needs to reconstruct the initial
input. The network is constrained to use narrow hidden layers, forcing an implicit dimensionality reduction on the data. The network is trained using squared error loss on the
reconstruction error and back-propagation.
Recent works in deep learning advocate to stack autoencoders to pre-train Deep Neural
Networks . This process enables the lowest layers of the network to ﬁnd low-dimensional
representations. It experimentally increases the quality of the whole network. Yet, classic
autoencoders often degenerate into identity networks and they fail to learn the relationship
between data. Pascal Vincent et al. tackle the issue by corrupting inputs, pushing
the network to denoise the ﬁnal outputs. Three processes are described to corrupt data:
 Gaussian Noise : Gaussian Noise is added to a subset of the input
 Masking Noise : A fraction ν of the input is randomly forced to be zero.
 Salt-and-Pepper Noise : A fraction ν of the input is randomly forced to be one of
the input maximum/minimum.
Therefore, the Denoising AutoEncoder (DAE) loss function is modiﬁed to emphasize the
denoising aspect of the network. It is based on two main hyperparameters α, β. They
balance whether the network would focus on denoising the input (α) or reconstructing the
input (β).
L2,α,β(x, ˜x) = α
[nn(˜x)j −xj]2
[nn(˜x)j −xj]2
Where nn(x)k is the kth output of the network, ˜x is the corrupted input x, J are the indexes
of the corrupted element of x.
Sparse Inputs
There is no standard approach for using sparse vector as input of neural networks. Most
of the papers dealing with sparse inputs got around by pre-computing an estimate of the
missing values . In our case, we want the autoencoder to handle itself this prediction
issue. Such problems have already been studied in industry . However, the amount of
missing values was very low (less than 5%) and all the missing values were known during
the training. In collaborative ﬁltering, input vectors are very sparse and target vectors have
a great number of missing values.
The following subsections provide a training framework to tackle the training of sparse
autoencoders by
 Inhibiting the edges of the input layers by zeroing out values in the input
 Inhibiting the edges of the output layers by zeroing out back-propagated values
 Using a denoising loss to emphasize rating prediction over rating reconstruction
One way to inhibit the input edges is to turn missing values to zero. To keep the autoencoder
from always returning zero, we use an empirical loss that disregards the loss of unknown
values. No error is backpropagated for missing values. Therefore, the error back-propagation
will be propagated for actual zero values while it is discarded for missing values. In other
words, missing values do not bring information to the network.
Using masking noise has two great advantages in our current issue. First, it works as a
strong regularizer. Second, it trains the autoencoder to predict missing values. Therefore,
the loss of the denoising autoencoder (DAE) becomes a promising objective function. To
handle sparse inputs, the error of unknown values is discarded.
After regularization, the ﬁnal training loss is:
L2,α,β(x, ˜x) = α
j∈J (˜x)∩K(x)
[nn(˜x)j −xj]2
j̸∈J (˜x)∩K(x)
[nn(˜x)j −xj]2
where K(x) are the indexes of known values of x and λ is the regulizer hyperparameter.
User preferences are encoded by a sparse matrix of ratings R. Given N users and M items,
the rating rij is the rating given by the ith user for the jth item. A user is then described by
a sparse vector ui and an item is represented by a sparse vector vj. Therefore, the goal is
to predict an estimate ˆrij for every missing rating. In other words, the goal is to complete
the users’(or items) sparse vectors. Thus, we deﬁne two autoencoders to compute ˆR:
 The Uencoder that takes the sparse vector ui as input and compute a dense vector
ˆui as output. This network learns a user representation.
 The Vencoder that takes the sparse vector vj as input and compute a dense vector
ˆvj as output. This networks learn an item representation.
Two Mean Square Errors (MSE) co-exist for autoencoders and one must be careful to use
the right estimator for benchmarking.
 Prediction Loss. This loss is actually unusual with autoencoders as it uses both the
training and testing dataset. Nonetheless, this is an important metric in collaborative ﬁltering. We use it to evaluate both the baselines and our method.
MSEpred(Xtest, Xtrain) =
k∈K(xtest)
[nn(xtrain)k −xtest,k]2
 Reconstruction Loss. This is the basic autoencoder loss. This error has little interest
for Collaborative ﬁltering but it provides useful information during the training
MSErec(Xtest) =
k∈K(xtest)
[nn(xtest)k −xtest,k]2
Where ∥Rtest∥, ∥Rtrain∥are the number of rating in the testing dataset and training dataset.
Benchmarked Models
Latent factor matrix factorization seeks a low-rank matrix R = UT V of rank k where
U ∈RN×k is the users’ representation and V ∈RM×k is the items’ representation.
We implemented Singular Value Decomposition (SVD) and Alternating-Least-Squares with
Weighted-λ-Regularization (ALS-WR) models for benchmarking. Indeed, a recent comparative study of collaborative ﬁltering pointed out that SVD technique remains the
most eﬃcient in the general case. Yet, we also reported NL-PMF as it is a non-linear
algorithm and AutoRec as it is based on neural networks. They both use an equivalent
setting in their publication.
SVD looks for U and V by optimizing the following function through gradient
i vj)2 + λ(∥ui∥2
F ro + ∥vj∥2
Where I × J are the set of indexes of known ratings and ∥ui∥F ro is the Froebenius norm.
Additional parameters such as the user/item bias or handcrafted features may be added in
the objective function to improve the ﬁnal results. .
ALS-WR solves the low-rank matrix factorization problem by alternatively
ﬁxing U and V and solving the resulting linear problem. Tikhonov regularization is often
used as it empirically provides good results .
i vj)2 + λ(ni∥ui∥2
F ro + nj∥vj∥2
Where ni is the number of rating for the ith user and nj is the number of rating for the jth
Neural networks actually compute a low rank approximation of R. If we ignore the output
transfer function, ˆR is iteratively built by the scalar product of the weight matrix of the
last layer and the activation of the second to last layer. For instance, given a Uencoder,
the V item representation is the uppermost weight matrix and U is the activation layers.
The situation is reversed for Vencoders. One key point that is not clear is the link between
these two representations.
The user representation and the item representation do not
lie in the same space. One representation belongs to the weight matrix whereas the other
representation belongs to a dense reconstruction of the sparse inputs through previous layers.
The task becomes more complicated if we compare the U representation from the activation
of the Uencoders and the U representation from the Vencoder weights. Adding back the
non-linearity makes the problem even harder. Yet, the ﬁnal output of the autoencoder can
still be considered as a latent non linear factor matrix factorization.
Experimental Setting
Training parameters
We train four-layer autoencoders. The ﬁrst encoding layer is 1/10th of the input size, the
second layer is 1/12th of the input size. The decoding layers have the same dimension in the
Figure 1: Training description. The ﬁrst step consists in optimizing a two-layer autoencoders. A clean output is then retrieved. It is used to train another two-layer autoencoder.
The ﬁnal autoencoder is built by mixing the outer and inner layers. The network is ﬁnally
ﬁne-tuned by retraining the full autoencoder. This process can be recursively applied for
bigger autoencoders. The third step diﬀers from classic SDAE where the decoder is usually
replaced by a classiﬁer .
reverse order. Weights are initialized using the fan-in rule Wij ∼
 . Transfer
functions are hyperbolic tangents. The neural network is optimized with stochastic backpropagation on minibatch of size twenty. The momentum is set to 0.8. Even if DAE loss
and sparsity already entail a strong regularization, a weight decay is required. The training
is done using stacked autoencoders as described in the ﬁgure 1. The other hyperparameters
of the training are provided in the table1 to reproduce the experiments.
hyperparameters
Outer Layers
Inner Layers
4-layers SDAE
learning rates*
learning decay
Weight decay
Gaussian noise
0.8 (std:0.02)
Masking Noise
Salt-and-Pepper
Table 1: Hyperparameters for training. The following parameters are set empirically to
provide good results. The same parameters are used for both autoencoders for the movieLens
and jester dataset. The layer ratio for the Vencoder of jester are respectively 1/1000th and
1/1500th of the input size.
Source code
Torch is a powerful framework written in Lua to quickly prototype Deep Neural Networks.
It is a widely used (Facebook, Deep Mind, Google) industry standard. However, Torch lacked
some important tools to deal with sparse inputs. Thus, we developed several new modules
to deal with DAE loss, sparse DAE loss and sparse inputs. They can easily be plugged
into existing code. Implementing our methods can now be done in a half-day work. An
out-of-the-box tutorial is also available to directly run the experiments. The code is freely
available on Github and Luarocks 1.
1To install the module on torch, type luarocks install nnsparse
MovieLens-1M
SVD (gradient)
0.852 ± 0.003
4.117 ± 0.04
0.850 ± 0.004
4.108 ± 0.02
0.879 ± 0.008
0.874 ± 0.003
0.831 ± 0.003
0.858 ± 0.003
4.107 ± 0.03
0.837 ± 0.004
5.001 ± 0.10
Table 2: RMSE Error with a training/testing set of 0.9/0.1 with cross-validation. The best
ALS-WR results are obtained with a rank of 20 and λreg of 0.03. SVD is performed with a
rank of 15, a learning rate of 0.02 and λreg of 0.02. Final MAE for Uencoders are Vencoders
on the movieLens dataset are respectively 0.676 ± 0.02 and 0.656 ± 0.02.
Experimental results
Two dataset are used to perform collaborative ﬁltering. The Jester Joke dataset
provides 4.1 million continuous ratings (-10.00 to +10.00) of 100 jokes from 73,496 users.
The MovieLens-1M dataset provides 1 million discrete ratings (1 to 5) from 6 thousand
users on 4 thousand movies.
Preprocessing
The full dataset is considered and the ratings are normalized from -1 to
1. We split it into random 90%10% train-test dataset. Inputs are then unbiased before
the training process. Given that ¯r is the mean over the training set bui is the mean of
the ith user and bvi is the mean of the vth user, we apply ˆrij,unbiased = ˆrui + ¯r −bui −bvj
for SVD/ALS algorithms, ˆrij,unbiased = ˆrui −bui for Uencoders, ˆrij,unbiased = ˆrui −bvi for
Vencoders.
Postprocessing
The bias computed on the training set was added back while evaluating
the ﬁnal RMSE.
The most important result is that autoencoders have excellent performance in our experiments, and are competitive compared to state-of-the-art methods. This is an improvement
regarding previous work in neural networks. Indeed, Salakhutdinov et al. faced important overﬁtting issues while turning a RBM into auto-encoders.
The second important point is that Uencoders and Vencoders may have diﬀerent results
regarding the dataset.
Vencoders performs poorly on jester while they are excellent on
movieLens. More research must be done to check whether Uencoders and Vencoders have
similar errors. Indeed, their representation spaces, as described above, diﬀers a lot.
We observe that 4-layer autoencoders return slightly better scores than 2-layer autoencoders.
More importantly, they are far more robust to a change in hyperparameters. Even for a
bad choice of hyperparameters, increasing the number of layers eventually provide excellent
As shown in ﬁgure 2, we observe that DAE loss can speed up the learning process. However, the hyperparameters α and β must be well-balanced. If the reconstruction error (β
) is ignored, the prediction error cannot fully compensate the loss of information. From
our experience, the ratio ν.α/(1 −ν)β = 1/3, where ν is the corruption ratio, is a good
equilibrium. Furthermore, the DAE loss also improve the ﬁnal RMSE. For instance, the
Uencoders return better results when the inputs are corrupted.
Discussion
Autoencoders face some limitations that are worth to mention. Compare to ALS-WR or
SVD, they are slow to train and less scalable. The training process also requires a high
epoches. α, β are respectively the hyperparameters for the prediction and
reconstruction error. ν is
the corruption ratio.
is composed of 75% of
masking noise and 25%
of SaltAndPepper noise.
The left-part of the plot is
the ﬁrst training step corresponding to a 2-layer autoencoders, the right-part
of the plot is the third
training step corresponding to the stacked 4-layer
autoencoder.
number of hyperparameters which makes them diﬃcult to tune ﬁnely. Whenever new items
occurs, Vencoders need to be retrained since it changes the size of the input/ouput layer.
Yet, Vencoders immediately provide additional estimates for new users. The situation is
reversed for Uencoders.
While studying Restricted Boltzmann Machine , the authors pointed out that the errors
made by RBM were signiﬁcantly diﬀerent from the errors made by SVD. We would like to
investigate if the same behavior occurs with autoencoders. In the same way, Uencoders and
Vencoders may also return diﬀerent errors since they learn two diﬀerent user/item representations. It may be worth ﬁnding a suitable network to mix both encoders. Autoencoders
can also be used as a pre-initialization step for more complex networks.
In ALS-WR , Tikhonov regularization prevents the algorithm from overﬁtting the data.
It diﬀers from classic L2 regularization since it takes into account the sparsity of the samples.
We tried this regularization on neural networks but it did not work. However, we strongly
assume that regularizing the backpropagation by using the density of the output may lead
to better results.
Finally, other metrics exist to estimate the quality of collaborative ﬁltering to ﬁt real-world
constraints. Normalized Discounted Cumulative Gain (NDCG) or Fscore are often preferred
to RMSE and should be benchmarked.
Conclusion
We introduce a neural network architecture that performs as well as the best algorithms
in collaborative ﬁltering. We then point out the complex relationship which lies between
users/items’ representations that are learned. We describe the autoencoders architecture.
We then explain how to add additional constraints on top of it to make the network handle
sparse inputs. We detail the full training process and we provide the source code for replicating the results. We also provide Torch modules to allow other teams to build complex
network using sparse inputs. Finally, we detail the main limitations of the algorithm and
we suggest some leads requiring further investigations.
Acknowledgements
The authors would like to acknowledge the stimulating environment provided by SequeL
research group, Inria and CRIStAL. This work was supported by French Ministry of Higher
Education and Research, by CPER Nord-Pas de Calais/FEDER DATA Advanced data
science and technologies 2015-2020, and by FUI Herm`es.
Experiments were carried out
using Grid’5000 tested, supported by Inria, CNRS, RENATER and several universities as
well as other organizations.