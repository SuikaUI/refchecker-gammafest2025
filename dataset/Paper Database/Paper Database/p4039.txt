Soft Contextual Data Augmentation for Neural Machine Translation
Jinhua Zhu1,∗, Fei Gao2,∗, Lijun Wu3, Yingce Xia4, Tao Qin4,
Wengang Zhou1, Xueqi Cheng2, Tie-Yan Liu4
1University of Science and Technology of China,
2Institute of Computing Technology, Chinese Academy of Sciences;
3Sun Yat-sen University, 4Microsoft Reserach Asia;
1{teslazhu@mail., zhwg@}ustc.edu.cn,
2{gaofei17b, cxq}@ict.ac.cn, ,
4{Yingce.Xia, taoqin, tyliu}@microsoft.com
While data augmentation is an important trick
to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited.
this paper, we present a novel data augmentation method for neural machine translation.
Different from previous augmentation methods that randomly drop, swap or replace words
with other words in a sentence, we softly augment a randomly chosen word in a sentence
by its contextual mixture of multiple related
words. More accurately, we replace the onehot representation of a word by a distribution (provided by a language model) over the
vocabulary, i.e., replacing the embedding of
this word by a weighted combination of multiple semantically similar words.
weights of those words depend on the contextual information of the word to be replaced,
the newly generated sentences capture much
richer information than previous augmentation methods.
Experimental results on both
small scale and large scale machine translation datasets demonstrate the superiority of our
method over strong baselines1.
Introduction
Data augmentation is an important trick to boost
the accuracy of deep learning methods by generating additional training samples. These methods
have been widely used in many areas.
For example, in computer vision, the training data are
augmented by transformations like random rotation, resizing, mirroring and cropping .
While similar random transformations have also
been explored in natural language processing
∗The ﬁrst two authors contributed equally to this work.
This work is conducted at Microsoft Research Asia.
1Our code can be found at 
teslacool/SCA
(NLP) tasks , data augmentation
is still not a common practice in neural machine
translation (NMT). For a sentence, existing methods include randomly swapping two words, dropping word, replacing word with another one and
so on. However, due to text characteristics, these
random transformations often result in signiﬁcant
changes in semantics.
A recent new method is contextual augmentation , which replaces words with other words that are predicted
using language model at the corresponding word
While such method can keep semantics based on contextual information, this kind of
augmentation still has one limitation: to generate new samples with adequate variation, it needs
to sample multiple times. For example, given a
sentence in which N words are going to be replaced with other words predicted by one language
model, there could be as many as exponential candidates. Given that the vocabulary size is usually
large in languages, it is almost impossible to leverage all the possible candidates for achieving good
performance.
In this work, we propose soft contextual data
augmentation, a simple yet effective data augmentation approach for NMT. Different from the previous methods that randomly replace one word
to another, we propose to augment NMT training
data by replacing a randomly chosen word in a
sentence with a soft word, which is a probabilistic distribution over the vocabulary. Such a distributional representation can capture a mixture of
multiple candidate words with adequate variations
in augmented data. To ensure the distribution reserving similar semantics with original word, we
calculate it based on the contextual information by
using a language model, which is pretrained on the
training corpus.
To verify the effectiveness of our method, we
 
conduct experiments on four machine translation tasks, including IWSLT2014 German to English, Spanish to English, Hebrew to English and
WMT2014 English to German translation tasks.
In all tasks, the experimental results show that our
method can obtain remarkable BLEU score improvement over the strong baselines.
Related Work
We introduce several related works about data
augmentation for NMT.
Artetxe et al. and Lample et al. 
randomly shufﬂe (swap) the words in a sentence,
with constraint that the words will not be shufﬂed
further than a ﬁxed small window size. Iyyer et al.
 and Lample et al. randomly drop
some words in the source sentence for learning an
autoencoder to help train the unsupervised NMT
model. In Xie et al. , they replace the word
with a placeholder token or a word sampled from
the frequency distribution of vocabulary, showing that data noising is an effective regularizer for
NMT. Fadaee et al. propose to replace a
common word by low-frequency word in the target sentence, and change its corresponding word
in the source sentence to improve translation quality of rare words. Most recently, Kobayashi 
propose an approach to use the prior knowledge
from a bi-directional language model to replace a
word token in the sentence. Our work differs from
their work that we use a soft distribution to replace
the word representation instead of a word token.
In this section, we present our method in details.
Background and Motivations
Given a source and target sentence pair (s, t)
where s = (s1, s2, ..., sT ) and t = (t1, t2, ..., tT ′),
a neural machine translation system models
the conditional probability p(t1, ..., tT ′|s1, ..., sT ).
NMT systems are usually based on an encoderdecoder framework with an attention mechanism
 .
In general, the encoder ﬁrst transforms the input
sentence with words/tokens s1, s2, ..., sT into a sequence of hidden states {ht}T
t=1, and then the decoder takes the hidden states from the encoder as
input to predict the conditional distribution of each
target word/token p(tτ|ht, t<τ) given the previous ground truth target word/tokens. Similar to
the NMT decoder, a language model is intended
to predict the next word distribution given preceding words, but without another sentence as a
conditional input. In NMT, as well as other NLP
tasks, each word is assigned with a unique ID,
and thus represented as an one-hot vector. For example, the i-th word in the vocabulary (with size
|V |) is represented as a |V |-dimensional vector
(0, 0, ..., 1, ..., 0), whose i-th dimension is 1 and
all the other dimensions are 0.
Existing augmentation methods generate new
training samples by replacing one word in the original sentences with another word . However, due to the sparse nature of
words, it is almost impossible for those methods to
leverage all possible augmented data. First, given
that the vocabulary is usually large, one word usually has multiple semantically related words as replacement candidates. Second, for a sentence, one
needs to replace multiple words instead of a single
word, making the number of possible sentences after augmentation increases exponentially. Therefore, these methods often need to augment one
sentence multiple times and each time replace a
different subset of words in the original sentence
with different candidate words in the vocabulary;
even doing so they still cannot guarantee adequate
variations of augmented sentences. This motivates
us to augment training data in a soft way.
Soft Contextual Data Augmentation
Inspired by the above intuition, we propose to augment NMT training data by replacing a randomly
chosen word in a sentence with a soft word. Different from the discrete nature of words and their
one-hot representations in NLP tasks, we deﬁne a
soft word as a distribution over the vocabulary of
|V | words. That is, for any word w ∈V , its soft
version is P(w) = (p1(w), p2(w), ..., p|V |(w)),
where pj(w) ≥0 and P|V |
j=1 pj(w) = 1.
Since P(w) is a distribution over the vocabulary, one can sample a word with respect to this
distribution to replace the original word w, as done
in Kobayashi . Different from this method,
we directly use this distribution vector to replace a
randomly chosen word from the original sentence.
Suppose E is the embedding matrix of all the |V |
words. The embedding of the soft word w is
ew = P(w)E =
which is the expectation of word embeddings over
the distribution deﬁned by the soft word.
The distribution vector P(w) of a word w can
be calculated in multiple ways. In this work, we
leverage a pretrained language model to compute
P(w) and condition on all the words preceding w.
That is, for the t-th word xt in a sentence, we have
pj(xt) = LM(wj|x<t),
where LM(wj|x<t) denotes the probability of the
j-th word in the vocabulary appearing after the sequence x1, x2, · · · , xt−1. Note that the language
model is pretrained using the same training corpus
of the NMT model. Thus the distribution P(w)
calculated by the language model can be regarded
as a smooth approximation of the original one-hot
representation, which is very different from previous augmentation methods such as random swapping or replacement. Although this distributional
vector is noisy, the noise is aligned with the training corpus.
Figure 1 shows the architecture of the combination of the encoder of the NMT model and the
language model. The decoder of the NMT model
is similarly combined with the language model. In
experiments, we randomly choose a word in the
training data with probability γ and replace it by
its soft version (probability distribution).
At last, it is worth pointing out that no additional monolingual data is used in our method.
This is different from previous techniques, such
as back translation, that rely on monolingual data
 ;
• Dropout: Randomly drop word tokens ;
• Blank: Randomly replace word tokens with a
placeholder token ;
• Smooth: Randomly replace word tokens with
a sample from the unigram frequency distribution over the vocabulary ;
• LMsample: Randomly replace word tokens
sampled from the output distribution of one
language model .
All above introduced methods except Swap incorporate a hyper-parameter, the probability γ
of each word token to be replaced in training phase.
We set γ with different values in
{0, 0.05, 0.1, 0.15, 0.2}, and report the best result
for each method. As for swap, we use 3 as window
size following Lample et al. .
For our proposed method, we train two language models for each translation task. One for
source language, and the other one for target language. The training data for the language models
is the corresponding source/target data from the
bilingual translation dataset.
experiments
{German, Spanish, Hebrew} to English ({De,
Table 1: BLEU scores on four translation tasks.
Es, He}→En) and WMT2014 English to German
(En→De) translation tasks to verify our approach.
We follow the same setup in Gehring et al. 
for IWSLT2014 De→En task.
The training
data and validation data consist of 160k and 7k
sentence pairs. tst2010, tst2011, tst2012, dev2010
and dev2012 are concatenated as our test data.
For Es→En and He→En tasks, there are 181k
and 151k parallel sentence pairs in each training
set, and we use tst2013 as the validation set,
tst2014 as the test set. For all IWSLT translation
tasks, we use a joint source and target vocabulary
with 10K byte-pair-encoding (BPE) types.
For WMT2014 En→De
translation, again, we follow Gehring et al. 
to ﬁlter out 4.5M sentence pairs for training. We
concatenate newstest2012 and newstest2013 as
the validation set and use newstest2014 as test set.
The vocabulary is built upon the BPE with 40k
sub-word types.
Model Architecture and Optimization
We adopt the sate-of-the-art Transformer architecture for language models and NMT models in our experiments.
IWSLT tasks, we take the transformer base
conﬁguration, except a) the dimension of the inner MLP layer is set as 1024 instead of 2048 and
b) the number of attention heads is 4 rather than
As for the WMT En→De task, we use the
default transformer big conﬁguration for the
NMT model, but the language model is conﬁgured
with transformer base setting in order to speed
up the training procedure. All models are trained
by Adam optimizer with
default learning rate schedule as Vaswani et al.
 . Note that after training the language models, the parameters of the language models are
ﬁxed while we train the NMT models.
Main Results
The evaluation results on four translation tasks
are presented in Table 1.
As we can see, our
method can consistently achieve more than 1.0
BLEU score improvement over the strong Transformer base system for all tasks. Compared with
other augmentation methods, we can ﬁnd that 1)
our method achieves the best results on all the
translation tasks and 2) unlike other methods that
may not be powerful in all tasks, our method
universally works well regardless of the dataset.
Specially, on the large scale WMT 2014 En→De
dataset, although this dataset already contains a
large amount of parallel training sentence pairs,
our method can still outperform the strong base
system by +1.3 BLEU point and achieve 29.70
BLEU score. These results clearly demonstrate
the effectiveness of our approach.
As mentioned in Section 4, we set different probability value of γ to see the effect of our approach and other methods in this subsection. Figure 2 shows the BLEU scores on IWSLT De→En
dataset of each method, from which we can see
that our method can observe a consistent BLEU
improvement within a large probability range and
obtain a strongest performance when γ = 0.15.
However, other methods are easy to lead to performance drop over the baseline if γ > 0.15, and the
improvement is also limited for other settings of
γ. This can again prove the superior performance
of our method.
Probability
Figure 2: BLEU scores of each method on IWSLT
De→En dataset with different replacing probability.
Conclusions and Future Work
In this work, we have presented soft contextual
data augmentation for NMT, which replaces a randomly chosen word with a soft distributional representation. The representation is a probabilistic
distribution over vocabulary and can be calculated
based on the contextual information of the sentence. Results on four machine translation tasks
have veriﬁed the effectiveness of our method.
In the future, besides focusing on the parallel
bilingual corpus for the NMT training in this work,
we are interested in exploring the application of
our method on the monolingual data. In addition,
we also plan to study our approach in other natural
language tasks, such as text summarization.