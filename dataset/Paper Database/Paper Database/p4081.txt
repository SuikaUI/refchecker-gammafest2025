Bayesian Multi-Object Tracking Using Motion Context from Multiple Objects
Ju Hong Yoon
 
Ming-Hsuan Yang
 
Jongwoo Lim
Hanyang University
 
Kuk-Jin Yoon
 
Online multi-object tracking with a single moving camera is a challenging problem as the assumptions of 2D conventional motion models (e.g., ﬁrst or second order models)
in the image coordinate no longer hold because of global
camera motion. In this paper, we consider motion context
from multiple objects which describes the relative movement between objects and construct a Relative Motion Network (RMN) to factor out the effects of unexpected camera motion for robust tracking. The RMN consists of multiple relative motion models that describe spatial relations
between objects, thereby facilitating robust prediction and
data association for accurate tracking under arbitrary camera movements. The RMN can be incorporated into various multi-object tracking frameworks and we demonstrate
its effectiveness with one tracking framework based on a
Bayesian ﬁlter. Experiments on benchmark datasets show
that online multi-object tracking performance can be better
achieved by the proposed method.
1. Introduction
Multi-object tracking (MOT) is of great importance for
numerous computer vision tasks with applications such as
surveillance, trafﬁc safety, automotive driver assistance systems, and robotics. Thanks to advances of object detectors , detection-based MOT methods have been extensively studied in recent years. In this approach, the goal is to
determine the trajectories and identities of target instances
throughout an image sequence using the detection results of
each frame as observations.
In general, detection-based tracking methods can be categorized into online and batch methods. The online methods solve the MOT problem using only the past frames up
to the current frame . In contrast, the batch or
delayed-output methods utilize the visual information in the
entire sequence or the future frames; hence, they iteratively
optimize the detection assignment of the current frame using the future information . In terms of
tracking accuracy, the methods in the second group are usually more accurate as forward-backward visual information
is available for disambiguation. However, for online applications such as driver assistance systems and service robots,
the online approach is more suitable since the tracking results in the current frame are available instantly. In this paper, we address this online MOT problem.
In detection-based MOT, as each trajectory is constructed by matching multiple detected objects of the same
class across frames, data association plays an essential role
for robust tracking. For data association, both appearance
and motion models are typically used and thus of critical
importance. In many situations, appearance models alone
are not adequate to discriminate objects, particularly for
separating instances of the same class (e.g., pedestrians),
since their shape and texture look similar. This problem is
more critical in online MOT methods because the information to reduce such ambiguities is limited compared to batch
MOT methods.
With such ambiguities in appearance, motions and positions must be used to correctly associate the confusing detections to the objects. In previous works, 2D object motion
in image plane is typically described by a ﬁrst or second order model based on the past tracking results .
These 2D conventional motion models are effective when
the objects are continuously detected and when the camera
is stationary or slowly moving, e.g., the objects in the red
boxes in Fig. 1. However, they quickly become unreliable
when objects are occluded or undetected for several frames
and at the same time the camera moves or ﬂuctuates. In
many MOT application scenarios, the camera may be on a
moving platform, such as a vehicle or a pan-tilt unit, and the
unpredicted global motion cause many existing MOT algorithms to fail because the predicted object position by the
simplistic motion models is often far from the re-detected
object position, e.g., the green box in Fig. 1. Nevertheless,
considerably less attention has been paid to motion modeling than appearance models in MOT, especially for scenes
with moving cameras. To resolve this problem, in this paper, we propose a novel method for online MOT in complex moving scenes, which can be applied to various scenarios without knowing either scene dynamics or camera
(a) Bahnhof seq.(# 140, # 171)
(b) Jelmoli seq.(# 256, # 278)
Figure 1. Examples for effectiveness of a relative motion. Black box: ground truth, Red box: a well-tracked object, Magenta box: prediction
based on the well-tracked object with the proposed RMN, Green box: prediction by conventional self-motion model. The distance error in
each graph shows that the prediction based on well-tracked object with the RMN is much more accurate than the prediction based on the
conventional self-motion model.
motion. The proposed Relative Motion Network (RMN) algorithm accounts for the motion context from multiple moving objects which are mostly insensitive to unexpected camera motions. Two examples of successful predictions using
the proposed RMN algorithm are shown in Fig. 1 where the
tracks of occluded or undetected objects are recovered after several frames. Note that the proposed algorithm does
not consider very abrupt camera motions and ﬂuctuations,
but consider realistic scenarios where the camera moves at
a moderate speed with some ﬂuctuations (as in the supplementary video) where at least a few objects can be tracked
with continuous detections and predicted well, e.g., the objects in the red boxes of Fig. 1. In such situations, the
RMN helps tracking other undetected objects after the objects are re-detected again. Furthermore, we also incorporate the proposed RMN into Bayesian framework to formulate general online MOT algorithm.
Numerous quantitative evaluations against several stateof-the-art methods on benchmark datasets show that the
proposed online MOT algorithm can handle aforementioned
challenges effectively.
2. Related Works and Problem Context
We introduce representative MOT algorithms that focus
on motion models, which can be categorized based on static
or dynamic camera assumptions as considered in this work.
Static camera: Takala et al. propose to measure directional smoothness and speed of each object based on the
current location and the past trajectory to track multiple objects. In , nonlinear motion patterns of each object and
the entry/exit maps are trained by exploiting past and future object trajectories. The trained trajectory information
is used in correcting mis-detections In , object velocity is used to generate conﬁdence maps of future trajectories for tracking. Despite demonstrated success in multiobject tracking, the aforementioned methods only works
with static cameras.
Dynamic camera: Duan et al. use both individual and
mutual relation models to handle occlusions and large shape
changes but they are not used in disambiguating the objects
in data association. Their mutual relation model has a limitation that it only works when the objects move in the same
direction. In , the pairwise relative object motion model
is developed as an additional similarity function for MOT,
which uses both past and future tracklets. Leal-Taix´e et al.
 propose robust motion models which are trained in of-
ﬂine manner based on motion training samples. On the contrary, our method does not need training data for constructing motion models.
In this paper, we exploit all relative motions between objects for online MOT. For single object tracking, the relative
distance between feature points in image has been used to
reduce tracking drifts during occlusion or drastic appearance change . For multi-object tracking, utilizes relative spatial constraint between objects. However, they do
not utilize a detector; hence, the data association between
objects and detections is not considered. However, different from , the proposed MOT algorithm considers
data association problem between detections and objects,
and the RMN is used to enhance the data association performance. In addition, we design relative motion weights
to consider different contributions from other objects, and
object states and relative motion weights are continuously
estimated within the Bayesian framework.
3. Relative Motion Network for Tracking
In this paper, the state of object i (i.e.
i-th object)
is deﬁned as xi
t]⊤, where (ui
Figure 2. Prediction of object state transition based on RMN and
(4): Rt = ∪3
t where R1
t = {⟨1, 1⟩, ⟨1, 2⟩, ⟨1, 3⟩}, R2
{⟨2, 2⟩, ⟨2, 3⟩}, R3
t = {⟨2, 3⟩, ⟨3, 3⟩}. A detection event of the
i-th object is deﬁned as oi
t ∈{0, 1} in (14). When the object is
detected oi
t = 1; otherwise, oi
t = 0. In this example, the 1-st
object is not detected ;hence, the relative motion models from the
1-st object are not included in R.
t) denote the center position and velocity, respectively; wi
t represent the width and height of the object bounding box; and t is the frame or time index. The
proposed online MOT algorithm uses the relative motion
between two objects i and j based on the position and velocity difference as
where (µ⟨i,j⟩
) and ( ˙µ⟨i,j⟩
) represent the spatial
and velocity difference, respectively. The i-th object has a
set of relative motion vectors with respect to other objects
and each one is used as a motion model for the i-th object.
When we have N objects, a relative motion network (RMN)
Rt is deﬁned as
= {⟨i, i⟩} ∪{⟨i, j⟩| oj
t = 1, 1 ≤j ≤N},
where the RMN represents a set of linked edges between
objects. Here, we include the ⟨i, j⟩relative motion model
t only if the j-th object is detected at frame t, which
is represented by the detection event oj
t = 1 (deﬁned in
(14)). Otherwise, we do not include the ⟨i, j⟩relative motion model in Rt because detection failures are caused by
various reasons such as disappearance and occlusion. The
self motion model (i.e., denoted by ⟨i, i⟩) is always included
in case there exists only one object. Since the motion correlations between pairs of objects are different, we consider
the motion correlations by using relative weights θ⟨i,j⟩
t = {θ⟨i,j⟩
|⟨i, j⟩∈Ri
t, 1 ≤j ≤N},
where we set initial relative weights uniformly based on the
t as θ⟨i,j⟩
t| denotes the cardinality of a
t). With the relative motion in the RMN, we design
the object motion model in (4) that enables the i-th object
state transition from the previous j-th object state selected
from ⟨i, j⟩∈Rt. One example is shown in Fig. 2. With
one of relative motion models from the RMN, the motion
transition is formulated by
t, ⟨i, j⟩) + w
t ] + r⟨i,j⟩
is the transition matrix based on a constant velocity motion
model; the object width and height are independent from
the relative motion; and w represents the assumed white
Gaussian noise model in this paper.
4. Online Multi-object Tracking with RMN
As mentioned, the goal of online MOT is to estimate a
set of object states Xt from a set of current observations Zt.
The set of N object states and the set of M observations are
expressed as Xt = {x1
t, . . . , xN
t } and Zt = {z1
t, . . . , zM
respectively. In this paper, we utilize the RMN to achieve
robust multi-object state estimation.
4.1. Bayesian Filter with the RMN
We solve the multi-object tracking problem within the
Bayesian framework. Assuming that object states are independent of each other in a way similar to other existing
methods , the goal is to maximize the posterior probability of the object state xi
t given the observation history
Z0:t = {Z0, . . . , Zt} and the RMN. The posterior probability is deﬁned by
t|Z0:t, R∗
t, ⟨i, j⟩)p(xi
t|Z0:t−1, ⟨i, j⟩),
where the RMN before the update is deﬁned by R∗
Rt−1. The posterior probability is decomposed with the
relative motion models and their weights θ⟨i,j⟩
. The prior
probability of each object state is modeled with the relative
motion model based on a ﬁrst-order Markov chain:
t|Z0:t−1, ⟨i, j⟩)
t−1, ⟨i, j⟩)p(xj
t−1|Z0:t−1, ⟨i, j⟩)dxj
where the transition density p(xi
t−1, ⟨i, j⟩) is described
by the motion model in (4).
The likelihood function p(Zt|xi
t, ⟨i, j⟩) is designed by considering association
events that the k-th observation is assigned to the i-th object
with the ⟨i, j⟩relative motion model. This event probability is denoted by Pk(E⟨i,j⟩
). We also consider the event
that none of the observation is associated with the i-th object, which is denoted by P0(E⟨i,j⟩
). Then, the likelihood
function is composed of the probability of these association
t, ⟨i, j⟩) ≜P0(E⟨i,j⟩
t, ⟨i, j⟩),
where the likelihood function of the k-th observation and
the i-th object is p(zk
t, ⟨i, j⟩), which is used for object
state update via a Kalman ﬁlter in Algorithm 1.
The relative weight θ⟨i,j⟩
in (5) is also updated with the
event probabilities and observations by
≜P(⟨i, j⟩|Z0:t) =
P (⟨i,j⟩|Z0:t−1)P (Zt|⟨i,j⟩)
t P (⟨i,j⟩|Z0:t−1)P (Zt|⟨i,j⟩),
P(Zt|⟨i, j⟩) = P0(E⟨i,j⟩) + P
k Pk(E⟨i,j⟩
t |⟨i, j⟩),
where the prior relative weight is deﬁned by θ⟨i,j⟩
P(⟨i, j⟩|Z0:t−1) and the model likelihood function is computed by the PASCAL score as
t |⟨i, j⟩) = area(T (zk
t )∩T (x⟨i,j⟩
area(T (zk
t )∪T (x⟨i,j⟩
where the i-th object state from the j-th object state is computed by x⟨i,j⟩
t−1, ⟨i, j⟩) from (4) and T(·) denotes
a bounding box of the state vector x or the observation z.
To update the object states and relative weights, the event
probabilities are determined by solving the data association
in the next section.
4.2. Event Probability via Data Association
To solve the online MOT problem, we need to associate
each observation to an object. The similarity function for
the data association is deﬁned as
where we also consider the size similarity Ps(zk
ﬁned in (17)) and appearance similarity Pa(zk
t) (deﬁned
in (16)) together in a way similar to existing MOT methods.
In the proposed algorithm, we consider the motion similarity Pm(zk
t ) based on the RMN. We select the most
important and contributive relative motion model of the i-th
object to the k-th observation according to the updated relative weight θ⟨i,j⟩
t ) to minimize the cost function in (12).
This is because that the contributions from the other objects
are not equal, and the predicted states from less contributive
relative motion models are less reliable and less related to
the k-th observation. Thus, by selecting the most contributive relative motion model, we exclude the predicted states
from the less contributive relative motion models in data association,
t |⟨i, j⟩k),
⟨i, j⟩k = arg
t |⟨i,j⟩)θ⟨i,j⟩
t |⟨i,j⟩)θ⟨i,j⟩
where the motion similarity is computed by the PASCAL
score P(zk
t |⟨i, j⟩) from (9) and ⟨i, j⟩k is the selected relative motion model index. In this paper, we solve the data
association problem as a bijective matching task by using
Hungarian algorithm. Based on the similarity function in
(10), we obtain the cost function between the i-th object and
the k-th observation as Ci,k
= −ln Λ(zk
t). We compute
the assignment matrix A = [ai,k]N×M that minimizes the
ai,k = 1, ∀i and
ai,k = 1, ∀k,
where an assignment indicator is deﬁned as ai,k ∈{0, 1}.
The association between objects and observations are determined as follows. When ai,k = 1, the observation assignment is obtained by following two cases.
< τ, the i-th object xi
t is associated with the
k-th observation zk
t . Then, the assignment observation is
= 1. (Note that we empirically select the threshold τ
as and ﬁx it in all the experiments.)
> τ, the i-th object is not associated with the
k-th observation. The observation assignment is γi,k
Since the detection event represents the association between the i-th object and the k-th observation, we utilize
the observation assignment γi,k
from the data association
in computing the event probability. The event probability is
computed by
t |, P0(E⟨i,j⟩
t and we divide the event probability by
t | (the cardinality of a set of the relative motion models used for the i-th object) to make the total sum of event
probabilities along the relative motion models ⟨i, j⟩∈Ri∗
always be 1. These event probabilities are used for the update of object states in (7) and relative weights in (8). If
the i-th object is associated with any observations, the i-th
object is successfully detected. Hence, the detection event
is simply obtained by the event probabilities as follows
Figure 3. (a) Examples of time-varying relative motion in a moving camera: Although the camera moves or ﬂuctuates, the spatial
difference changes following a certain dynamic model. (b) Objects moving in a group: Although two objects move coherently in
a group, their spatial difference changes according to the geometric relation between the objects and a camera.
where the detection event of the i-th object is consequently
deﬁned as oi
t ∈{0, 1}. If the object is associated with any
observation after the data association, then oi
t = 1; otherwise, oi
t = 0. As a result, we obtain a set of detection
events {oi
t| i = 1, . . . , N} with which the links between the
objects in the RMN are updated based on (2). This updated
RMN Rt is used as R∗
t+1 = Rt for the next time step. All
non-associated observations are used for new object initialization.
5. Relative Motion Update
Existing MOT methods assume that the relative
motion between two objects is static.
However, this assumption does not generally hold because of different directions and speed of object motion or geometric relation
with a camera as shown in Fig. 3. For these reasons, in
this work, we consider the time-varying relative motion to
deal with general situations. The relative motion typically
changes in piecewise linear patterns as shown in Fig. 3(a)
and thus we model their variation with a constant velocity
model and update the relative motion using a Kalman ﬁlter
 with the following transition and observation models,
= Frr⟨i,j⟩
t−1 + vr =
= Hrr⟨i,j⟩
where (uz, vz) is an observation position; ki represents
the associated observation index with the i-th object when
γi,k = 1 from the data association; Fr denotes a constant
velocity motion model; Hr converts the relative motion to
the relative observation; and vr and wr are the assumed
white Gaussian noise terms. If one of objects is not detected, then the relative motion is simply estimated by prediction using the motion model r⟨i,j⟩
= Frr⟨i,j⟩
6. Implementation
Since our algorithm is formulated based on the Bayesian
framework, it can be implemented with one of various ﬁltering methods such as Kalman and particle ﬁlters. In this
paper, we adopt and modify a Kalman ﬁlter to approximate
the proposed tracking algorithm. The Kalman ﬁlter have
been applied to multi-object tracking to estimate an object
trajectory or motion with object detectors. The main
steps of our algorithm are summarized in Algorithm 1.
Appearance and size similarity: The appearance of an object is represented by a color histogram, and the similarity between detections and objects are computed by Bhattacharyya coefﬁcient .
where Hj(zk
t ) and Hj(xi
t) denotes histogram of the i-th
object and the k-th detection; j represents the j-th bin; and
NH is the number of bins. In this work, we use 64 bins
for each color space. Hence, 192 bins are totally used to
represent object appearance.
We assume the aspect ratio between width and height is
constant. The size similarity is computed by
t−1 denotes the height of the i-th object state and
z,t represents the height of the k-th observation.
Initialization and termination: In this work, objects are
managed in a way similar to , and a relative motion
model is created when a new object is initialized. If previous and current observations have overlaps for a few frames
and are not associated with the other existing objects, a new
instance is created. Relative motion models between the
new object with all the others are then generated. If an object is not associated with any observation for a certain period, it is terminated and the corresponding relative motion
models are removed.
7. Experiments
Datasets: We use 7 benchmark sequences to demonstrate
the effectiveness of the proposed RMN algorithm.
benchmark sequences were recored by a moving camera, i.e., ETH dataset (Bahnhof, Sunnyday, and Jelmoli 1) and two sequences from Youtube (i.e., Marathon1,
1 
Algorithm 1 Online RMN Multi-Object Tracking (RMOT)
2: – RMN: R∗
t , . . . , RN∗
} where R∗
3: – Relative Weight:
t−1, . . . , ΘN
t−1} where Θi
t−1 |⟨i, j⟩∈Ri
t−1, 1 ≤j ≤N}.
4: – Object: xi
t−1 ∼N(¯xi
t−1), i = 1, . . . , N,
covariance Pi
t−1 of the i-th object state
5: – Observation: Zt = {z1
t , . . . , zM
6: • Object State Prediction with RMN
for i = 1 : N
for ⟨i, j⟩∈R∗
t|t−1 = f(¯xj
t−1, ⟨i, j⟩) in (4)
t|t−1 = F ¯Pi
▷F is from (4) and covariance Q
– χi = {(¯x⟨i,j⟩
t|t−1, P⟨i,j⟩
t|t−1)|⟨i, j⟩∈Ri∗
7: • Data Association
– Using χi and Θt−1, computing the cost matrix [Ci,k
]N×M from the similarity function in (10)-(11)
– Observation assignments [γi,k
]N×M are obtained from data association (12).
– Event probabilities (i.e., Pk(E⟨i,j⟩
) and P0(E⟨i,j⟩
) in (13) and detection
events {oi
t|i = 1, . . . , N} in (14)
8: • Update of Object States and RMN
for i = 1 : N
– A set of relative weights θ⟨i,j⟩
t is updated in (8).
– ⟨i, j⟩∗= max⟨i,j⟩∈R∗
(Kalman ﬁlter update)
t = P⟨i,j⟩∗
t|t−1 H⊤(HP⟨i,j⟩∗
t|t−1 H⊤+ R)−1
▷Noise covariance R
t = ¯x⟨i,j⟩∗
t −H¯x⟨i,j⟩∗
t = P⟨i,j⟩∗
t(HP⟨i,j⟩∗
t|t−1 H⊤+ R)Ki⊤
t = ¯x⟨i,i⟩
t = P⟨i,i⟩
– Updating the RMN Rt in (2) with detection events {oi
t|i = 1, . . . , N}.
9: • Relative Motion Update
– Updating each relative motion with the transition and the observation model in
(15) via a Kalman ﬁlter for a given relative observation.
10: Parameter: The observation matrix H =
and Q are given in the supplementary material.
Marathon2). We consider real situations where the camera moves at a reasonable speed and some jitters (as in the
supplementary video). These sequences contain ﬂuctuated
scenes as a result of camera motion.
The other two sequences are TUD and PETL1 dataset which were obtained
by a static camera 2. For the ETH dataset, we only use
sequences from the left camera without any information regarding depth, scene dynamics, and camera motion. Detection results and the ground truth of the Bahnhof, the Sunnyday, the TUD, and the PETL1 sequences from the website
2. For the Jelmoli sequence, we use the detector from 
for tests.
To generate detections for the Marathon1 and
Marathon2 sequences, we use a face detector .
Trackers: We compare the proposed MOT algorithm with
a baseline tracker which utilizes the self motion model
(SMM), and the baseline tracker is named as SMM-MOT
2 
(SMOT). For fair comparisons, the SMOT is also implemented based on the same MOT framework described in
Algorithm 1 but with the self motion model. We also compare our method (RMOT) with other state-of-the-art methods, i.e., two Online methods (StructMOT and MOT-
TBD ) and four Ofﬂine methods (PRIMPT , OnlineCRF , and CemTracker ), we use the reported
results in their paper. For new benchmark sequences (Jelmoli, Marathon1, Marathon2), we compare the proposed
RMOT with the SMOT. To achieve fair comparisons, we
use the same detection results and same ground truth.
To facilitate understanding of the proposed RMOT, the
MATLAB code, datasets, ground truth data will be made
available to the public 
project/rmot.html.
Runtime: All the experiments are carried out on a Intel 3.4
GHz PC with 8 G memory. Given the detections, the average computation time of the current MATLAB implementation is approximately 2.64 × 10−2 seconds to obtain the
tracking results without any code optimization. To be speciﬁc, for K objects, the RMN update takes approximately
6.0 × 10−4 × K(K−1)
seconds. The object state estimation approximately 5.1×10−5 ×K seconds. Therefore, the
proposed algorithm can be applied to online and real-time
applications.
Evaluation metrics and software: For evaluation, we use
well-known metrics which are widely used in MOT evaluation , which consists of Recall (correctly tracked objects over total ground truth), Precision (correctly tracked
objects over total tracking results), and false positives per
frame (FPF). We also report the number of identity switches
(IDS) and the number of fragmentations (Frag) of ground
truth trajectories. The ratio of tracks with success- fully
tracked parts for more than 80% (mostly tracked (MT)),
less than 20% (mostly lost (ML)), or less than 80 % and
more than 20 % (partially tracked (PT)). The number of
ground truth (GT) is reported in Table 2 and 2. We utilize
the same evaluation software 2 used in the other previous
papers because different evaluation softwares measure performance differently .
7.1. Comparison with the Baseline Tracker
We evaluate the accuracy of the proposed RMN algorithm against a conventional self motion model (SMM) in
terms of distance error on the ETH dataset as shown in Fig.
4. We note that numerous online MOT methods 
are based on SMM. The errors are computed by the distance
between a predicted object position and a ground truth position based on certain criteria. We compute two kinds of
mean distance errors. The ﬁrst one (D1) is evaluated from
error distances that are measured when an object is associated with a detection again after it is not associated with
Bahnhof, Sunnyday, and Jelmoli
Marathon1 and Marathon2
Table 1. Comparison with the SMOT (i.e., a baseline tracker) on datasets from a moving camera.
(a) Distance D1
(b) Distance D2
Figure 4. (a) Prediction accuracy of RMN and SMM after longterm mis-detections. (b) Prediction accuracy of RMN and SMM
when objects are well tracked.
any observation for more than 5 frames as shown in Fig. 1.
The second one (D2) is evaluated from error distances that
are measured when the object is well tracked. As shown
in Fig. 1 and 4, the prediction based on the RMN is more
accurate than that by the SMM, and its efﬁciency is greater
when mis-detections occur.
As shown in Table 1, the RMOT outperforms the SMOT
in most of metrics because the RMOT can overcome camera motion problems using the RMN as shown in Fig. 1.
Examples of our qualitative results are shown in Fig. 5.
When mis-detections occur with camera motions, the self
motion model used in the SMOT method becomes unreliable because the SMOT cannot compensate the motion
changes caused by the camera without associated observations.
Hence, even if the corresponding object is redetected, the tracker cannot locate the object near the detection response with the prediction based on the SMM due
to inaccurate prediction as shown in Fig. 1 and 4.
7.2. Comparison with the State-of-the-art Tracker
Online methods: Table 2 demonstrates quantitative results.
The StructMOT and MOT-TBD methods are also designed
for online MOT, and they also do not require known camera motion either. However, different from the RMOT and
the MOT-TBD, the StructMOT uses the cost function that
should be trained in ofﬂine manner. Multiple features (i.e.,
LBP, 3D RGB, HOF), 2D motion information, bounding
box, and centroid Euclidean distance are used to train the
cost function. Although the RMOT only utilizes RGB histogram and motion information, and does not require any
trained cost functions, the RMOT shows the comparable
performance in most of metrics. For the sequences from
a moving camera (the ETHZ datasets), the RMOT shows
better performance in Recall, Precision, MT, ML and Frag
because when the long-term mis-detections occur due to occlusions or detection failures, the RMN model helps better
predict object states from the other well-tracked objects in
data association.
Ofﬂine methods: Accroding to the results in Table 2, although the RMOT is an online method, it shows comparable performance in most of metrics in comparison with the
ofﬂine methods except for the Frag and IDS. The RMOT
tends to have more fragments and ID switches compared
to those of the OnlineCRF and the PRIMPT. This is natural because our method is an online method which does not
uses any future information. Therefore, some short tracks
are not fused together leading to a higher number of Frag,
and some of tracks sometimes follow same objects causing
a higher number of IDS.
8. Conclusion
In this paper, we exploit the motion context from multiple objects, which describes the relative movements between objects to account for camera motions and misdetections. From the tracked objects, we obtain a set of
relative motion and construct the RMN model which in
turn helps predict the object states and associate observations for tracking under camera motions with natural ﬂuctuations. For concreteness, we incorporate the RMN model
within the Bayesian ﬁltering framework and a data association method for online multi-object tracking. Experimental
results on challenging sequences demonstrate that the proposed algorithm achieves favorable and comparable performance over several state-of-the-art methods.
Acknowledgment. This work was partially supported by ICT
R&D program of MSIP/IITP [14-824-09-006, Novel Computer
Vision and Machine Learning Technology with the Ability to Predict and Forecast], the Center for Integrated Smart Sensors as
Global Frontier Project , and the IT R&D
Program of MKE/KEIT (10040246). M.-H. Yang is supported
in part by NSF CAREER Grant #1149783 and NSF IIS Grant