Recurrent Neural Networks for Time Series
Forecasting
G´abor Petneh´azi∗
Doctoral School of Mathematical and Computational Sciences
University of Debrecen
Time series forecasting is diﬃcult. It is diﬃcult even for recurrent neural networks with their inherent ability to learn sequentiality. This article
presents a recurrent neural network based time series forecasting framework covering feature engineering, feature importances, point and interval
predictions, and forecast evaluation. The description of the method is followed by an empirical study using both LSTM and GRU networks.
Introduction
Recurrent neural networks are well suited to supervised learning problems where
the dataset has a sequential nature. Time series forecasting should not be an
exception.
RNNs are essentially neural networks with memory. They can remember things
from the past, which is obviously useful for predicting time-dependent targets.
Yet, applying them to time series forecasting is not a trivial task. The aim
of this article is to review some practices that can help RNNs deliver useful
forecasts.
Neural networks for forecasting
The objective of supervised learning is to predict something from data. A training set of an output (target) and some input variables is fed to an algorithm
that learns to predict the target values. The output may be categorical (classi-
ﬁcation) or continuous (regression). The task of the algorithm is to deliver high
quality predictions, all by itself, extracting the required knowledge solely from
the available data.
Neural networks are a popular framework for supervised learning — a networklike system of weighted summations and diﬀerentiable functions that can learn
astoundingly complex things. Usually, variants of gradient descent together with
backpropagation (chain rule) are used to ﬁnd the optimal values of the network
weights. This is all simple and intuitive, yet the resulting networks are usually
∗ 
 
diﬃcult for people to understand. There are so many weights and connections
that we just wonder how the system produced the results. We don’t understand
neural networks, but we like them. The reason for their ever high popularity is
simple: they are good. They can learn arbitrarily complex functions, and they
often provide excellent predictions for pretty diﬃcult machine learning problems.
NNs are widely used in machine learning, time series prediction is just one example application. Werbos and Werbos made a pioneering work in
the ﬁeld of neural networks by developing a general formulation of backpropagation. Werbos applied the method to forecasting, and compared it to traditional
forecasting methods. Tang et al. also made a neural networks vs. Box-
Jenkins comparison and found that NNs outperform the Box-Jenkins model for
series with short memory.
For series with long memory, both methods produced similar results. Faraway and Chatﬁeld compared neural networks
with Box-Jenkins and Holt-Winters methods for forecasting and found that the
design of network architecture and the choice of input variables require great
care, and so applying neural networks in black box mode is not a good idea.
They also found that increasing the number of hidden nodes may deteriorate
out-of-sample performance.
Zhang and Qi found that neural networks are not able to capture seasonality by default, and deseasonalization and detrending can help their forecasting
performance. According to Balkin and Ord , diﬀerencing is unnecessary
for neural network based forecasting, but a log transformation may be beneﬁcial.
Zhang et al. gives a detailed review of neural networks for forecasting.
Gamboa provides a more recent review of the applications of deep learning
to time series data.
Recurrent neural networks
RNNs are neural networks for sequential data — hereby we apply them to time
series. The main idea behind recurrent neural networks is using not only the input data, but also the previous outputs for making the current prediction. This
idea makes a lot sense — we could build neural networks passing values forward
in time. However, such simple solutions usually do not work as expected. They
are hard to train and they are forgetful. Rather, we need to have a system with
some kind of memory.
There are two popular and eﬃcient RNN models that work really well: long
short-term memory and gated recurrent unit.
Long short-term memory (Hochreiter and Schmidhuber ) is a gated memory unit for neural networks. It has 3 gates that manage the contents of the
memory. These gates are simple logistic functions of weighted sums, where the
weights might be learnt by backpropagation.
It means that, even though it
seems a bit complicated, the LSTM perfectly ﬁts into the neural network and
its training process. It can learn what it needs to learn, remember what it needs
to remember, and recall what it needs to recall, without any special training
or optimization. The input gate (1) and the forget gate (2) manage the cell
state (4), which is the long-term memory. The output gate (3) produces the
output vector or hidden state (5), which is the memory focused for use. This
memory system enables the network to remember for a long time, which was
badly missing from vanilla recurrent neural networks.
it = sigmoid (Wixt + Uiht−1 + bi)
ft = sigmoid (Wfxt + Ufht−1 + bf)
ot = sigmoid (Woxt + Uoht−1 + bo)
ct = ft ⊙ct−1 + it ⊙tanh (Wcxt + Ucht−1 + bc)
ht = ot ⊙tanh (ct)
Gated recurrent unit (Cho et al. ) is essentially a simpliﬁed LSTM. It has
the exact same role in the network. The main diﬀerence is in the number of
gates and weights — GRU is somewhat simpler. It has 2 gates. Since it does
not have an output gate, there is no control over the memory content. The
update gate (6) controls the information ﬂow from the previous activation, and
the addition of new information as well (8), while the reset gate (7) is inserted
into the candidate activation. Overall, it is pretty similar to LSTM. From these
diﬀerences alone, it is hard to tell, which one is the better choice for a given
problem. For a comparison, see Chung et al. .
zt = sigmoid (Wzxt + Uzht−1 + bz)
rt = sigmoid (Wrxt + Urht−1 + br)
ht = zt ⊙ht−1 + (1 −zt) ⊙tanh (Whxt + Uh(rt ⊙ht−1) + bh)
Recurrent neural networks for forecasting
Though it is probably not their primary application, LSTM and GRU networks are often used for time series forecasting. Gers et al. used LSTMs
with peephole connections to learn temporal distances. Malhotra et al. 
used stacked LSTM networks to detect anomalies in time series. Guo et al.
 proposed an adaptive gradient learning method for RNNs that enables
them to make robust predictions for time series with outliers and change points.
Hsu incorporated autoencoder into LSTM to improve its forecasting performance. Cinar et al. proposed an extended attention mechanism to
capture periods and model missing values in time series. Bandara et al. 
used LSTMs on groups of similar time series identiﬁed by clustering techniques.
Laptev et al. applied RNNs to special event forecasting and found that
neural networks might be a better choice than classical time series methods
when the number, the length and the correlation of the time series are high.
Che et al. built a GRU-based model with a decay mechanism to capture
informative missingness in multivariate time series.
Several attempts have been made on better understanding RNNs. Karpathy
et al. explored the source of recurrent neural networks’ performance with
performance comparisons and error analysis on character level language models.
van der Westhuizen and Lasenby used diﬀerent datasets to visualize the
operations of LSTMs. Greﬀet al. compared the performance of several
LSTM variants, and found that the forget gate and the output activation function are the most important elements of an LSTM block. Chang et al. 
proposed a feature ranking method using variational dropout (Hinton et al.
Some studies tried to ﬁnd ways to measure the uncertainty associated with
the time series forecasts of recurrent neural networks. Zhu and Laptev 
made uncertainty estimates using Monte Carlo dropout and an encoder-decoder
framework of LSTM units. Caley et al. constructed prediction intervals
for convolutional LSTMs using bootstrapping.
Here we are going to explore diﬀerent aspects of RNN-based time series forecasting, and introduce an end-to-end framework for producing meaningful forecasts.
Feature engineering
LSTM and GRU networks can learn and memorize the characteristics of time
series. It is not so easy though, especially when we only have a short series of
values to learn from. Smart feature engineering can help.
There are very few things whose future values we certainly know.
one such thing — we always know how it passes. Therefore, we can use it to
make forecasts, even for multiple steps ahead into the future, without increasing
uncertainty. All we have to do is extracting useful features that our algorithm
can easily interpret.
Time series components, such as trend or seasonality can be encoded into input
variables, just like any deterministic event or condition. Time-shifted values of
the target variable might also be useful predictors.
Features are usually normalized before being fed to the neural network.
is beneﬁcial for the training process.
Two popular choices for rescaling the
variables are the minmax scaler (9) and the standard scaler (10).
max(x) −min(x)
˜x = x −mean(x)
Lagging means going some steps back in time. To predict the future, the past is
our best resource — it is not surprising that lagged values of the target variable
are quite often used as inputs for forecasting. Lags of any number of time steps
might be used. The only drawback of using lagged variables is that we lose the
ﬁrst observations — those whose shifted values are unknown. This might be a
matter when the time series is short.
Trend can be grabbed by features indicating the passage of time.
variable of equidistant increasing numbers might be enough for that.
Seasonality
We may try to ﬁnd repetitive patterns in the time series by encoding seasonal
variations. There are diﬀerent ways to do this.
One-hot encoding is a reasonable choice. Hereby, we treat seasonality as categorical variables, and use dummy variables to indicate the current time interval
in the seasonal cycle. It is simple and intuitive. However, it can not really
grab cyclicality, since the distance between intervals does not matter during the
encoding. It means that two successive, and possibly similar, time intervals are
represented by just as independent values as any two randomly chosen time
intervals. Also, one-hot encoding uses an individual variable to represent each
unique value, which may be inconvenient when we have a large number of time
intervals. These deﬁciencies of the dummy variable approach lead us to another
encoding method.
We can place the values on a single continuous scale instead of using several
binary variables. By assigning increasing equidistant values to the successive
time intervals, we can grab the similarity of adjacent pairs, but this encoding
sees the ﬁrst and the last intervals as being farthest from each other, which
is a bad mistake. This may be healed by transforming the values using either
the sine (11) or the cosine (12) transformation. In order to have each interval
uniquely represented, we should use both.
 2 · π · x
 2 · π · x
Dummy indicators
We can use simple indicator variables for events or conditions that we consider
important. Holidays are always special and are spent in unusual ways. Hence, a
binary variable indicating holidays may carry information about the time series.
Also, an indicator of working days or working hours could be useful.
Feature importances
Neural networks consist of connected simple functions of weighted sums. It is
not a diﬃcult structure, yet interpreting the meaning and role of the numerous
backpropagation learnt weights is pretty hard. It is the reason for too often
calling them black boxes, and not even trying to understand them. Recurrent
networks are even a bit more complicated.
This diﬃculty of interpretation makes neural networks somewhat less valuable.
We should naturally always prefer a simpler, more interpretable model, when
having multiple choices with about the same forecasting performance. So, we
should brighten the black box, at least partially.
A measure of variable importance could tell something about what is happening within the neural network, yet quantifying importance is not trivial. Several
methods have been proposed, see Gevrey et al. or Olden et al. for
a comparison.
Here we are going to use mean decrease accuracy — a method that is usually
applied to random forests (Breiman ). It is also called permutation accuracy. We are permuting the variables, one by one, and calculate a measure
of accuracy for each deliberately corrupted model. The feature who’s random
permutation leads to the largest drop in accuracy is considered the most important. It is a simple, intuitive measure, and it can easily be applied not just
to random forests, but to any neural network or any other supervised learning
algorithm as well.
Two metrics will be used as measures of accuracy: R2 and MDA. R2 or coef-
ﬁcient of determination is a goodness of ﬁt measure for regression. It usually
ranges from 0 to 1, though it can take on negative values as well. MDA or
mean directional accuracy is the accuracy of the binary variable indicating the
directional change of consecutive values. Mean decrease accuracy can be calculated from both accuracy measures.
Variables are permuted separately, so the importance scores should be interpreted and evaluated independently.
Variable importances do not add up.
Hence, when the same piece of information is encoded into multiple variables,
we are going to calculate the maximum of the scores, and use it as the importance of that group of variables.
Feature importances can be estimated from both regression accuracy and directional accuracy of one-step (and maybe also multi-step) predictions on diﬀerent
validation sets. It means there are quite some combinations, and it is likely that
not the exact same variables will prove to be important in all cases. Therefore,
it is worth computing some descriptive statistics of the diﬀerent calculations to
get a summarized view of the actual roles of input features.
The importance scores are usually normalized to sum to 1.
Prediction
Point predictions
Our recurrent neural networks are primarily suited for one-step-ahead forecasting. We have a single output variable containing the value that immediately
follows the corresponding sequence of inputs.
This simple framework can also be used for multi-step-ahead forecasting in an
iterative (recursive) manner. When the target variable’s lagged values are the
only non-deterministic features, we may use the predicted values as inputs for
making further predictions. It is a bit risky though, since we roll the errors
Prediction intervals
Neural networks have an important disadvantage: they only provide point estimates.
We only get some forecasted values, without any indicators of the
predictions’ conﬁdence.
Prediction intervals contain future values with a given probability. Such interval
estimates could make our forecasts more meaningful, though producing them for
neural networks is not easy. We are going to use the computationally expensive
method of bootstrapping.
The idea of bootstrapping was introduced by Efron et al. . This is actually
a very simple method for estimating the quality of estimates. Bootstrapping
takes resamples with replacement from the original dataset to make separate
forecasts. The variability of those independent estimates is a good measure of
the conﬁdence of the estimation.
Efron and Tibshirani wrote a comprehensible review of bootstrapping,
focusing on applications. They state that the bootstrap method has very nice
properties for estimating standard errors, and computational intensity, being
its main weakness, is less and less of a case as computation is getting cheaper
and faster. Efron and Tibshirani also note that the bootstrap can be used to
construct conﬁdence intervals in an automatic way, while alternative methods
require several tricks to perform satisfactorily.
Bootstrapping is not only useful for quantifying the conﬁdence in our predictions, but also for improving the forecasts themselves. Ensemble methods combine multiple algorithms to deliver better predictions. Bootstrapping is one way
to construct an ensemble.
There are 2 main approaches to ensemble learning (Dietterich ). Either
we can create models in a coordinated fashion, and take a weighted vote of
the components, or we can construct independent models with a diverse set
of results, then combine them to let the disagreements cancel out. Bootstrap
resampling can be used to construct an ensemble of this latter type.
Bagging or bootstrap aggregating is an ensemble method that trains a learning
algorithm on multiple independent bootstrap samples, and aggregates the resulting predictors by voting or averaging. It was introduced by Breiman .
This simple ensemble can improve unstable single predictors, but it can slightly
degrade the performance of more stable ones. Hence, applying bagging is not
always a good idea, but it can work very well for some learning algorithms. Dietterich remarks that since the generalization ability of neural networks
is very good, they may beneﬁt less from ensemble methods. Still, we may hope
that bagging can bring some improvement to our point predictions, but the
main reason for applying the method is the construction of prediction intervals.
Bootstrapping has been applied to compute conﬁdence and prediction intervals
for neural networks by, e.g., Paass , Heskes , Carney et al. .
Khosravi et al. compared four leading techniques for constructing prediction intervals, including the bootstrap. They conclude that there is no method
that outperforms all the others in all respects. Tibshirani compared two
diﬀerent bootstraps to two other methods for estimating the standard error of
neural network predictions.
Tibshirani concludes that the bootstraps
perform best.
Here we are going to use a method similar to the one proposed by Heskes .
We take bootstrap samples of sequences, rather than individual observations. A
separate RNN is trained on each bootstrap sample. Our ﬁnal (point) prediction
is a simple average of all individual model predictions.
We must make a distinction between conﬁdence intervals and prediction intervals. These two are easily confused.
Conﬁdence intervals quantify how well we can approximate the true regression.
The conﬁdence is in the estimate of the regression — the mean of the target
distribution.
To compute the intervals, we ﬁrst construct the bagged estimator by averaging
the resulting estimates of all bootstrap runs (13).
Now we have the center, and we just need to ﬁnd the variance (14) and an appropriate value from the t-distribution to calculate the endpoints of the conﬁdence
interval (15). We assume that the true regression follows a normal distribution
given the estimate.
CIi = [ˆyi −tconf · σˆyi, ˆyi + tconf · σˆyi]
Prediction intervals quantify how well we can approximate the target values.
This measure is more important in practice, but it is a bit more diﬃcult to
estimate. While the construction of conﬁdence intervals required nothing more
than the means and standard deviations of our resampled estimates, here we
need some more sophisticated computations to estimate the noise variance of
the regression (16).
ˆϵ ≃E[(y −ˆy)2] −σ2
We train another network with almost the same structure as the one used for
making the time series predictions, and use it to predict the remaining residuals
(17) from the input values.
i = max((yi −ˆyi)2 −σ2
This residual predictor is trained on the validation set of observations randomly
left out in the current bootstrap run.
It is a smart data recycling method.
Heskes proposed loglikelihood as the loss function, we apply the similar
formula (18) used by Khosravi et al. . The output activation of this neural
network is exponential, so that all predicted error variances are positive.
The output variances and the neural network’s predictions are added together
(19) to yield estimates of the variance of real interest — the distance of target
values from our bagged forecast estimates (20).
PIi = [ˆyi −tconf · σi, ˆyi + tconf · σi]
Validation
Point forecasts
We are going to obtain a train and a test set by splitting the time series at one
point. It means that we always test the future. We would expect the algorithm
to tell the future, so this choice of validation is natural for such forecasting
problems. Yet, the test set consists of a single time period, so this method may
not be entirely suﬃcient for evaluating the model performance.
Bootstrapping provides an alternative validation set.
An average bootstrap
sample contains about 63.2% of the individual observations, or in our case, it
contains about 63.2% of all available data subsequences. The remaining subsequences do not participate in the training process, so we may use them for
validation purposes.
We are going to call the bootstrap left-out dataset validation set, and the future
dataset test set — just for the sake of distinction. They have the same evaluation purpose.
Both sets will be used to evaluate the one-step-ahead forecasting ability of our
recurrent neural networks. The separate test set, being a complete chronologically ordered series of subseries, may also be used for iterative multi-step-ahead
forecasting.
Regression and classiﬁcation metrics are going to be applied in order to evaluate
the forecasted values and the predicted changes of direction as well.
We are calculating root mean squared error (21), symmetric mean absolute percentage error (22), coeﬃcient of determination (23), mean absolute error (24)
and median absolute error (25) regression metrics to measure the forecast ﬁt.
We are also calculating the accuracy (MDA) (26), precision (27), recall (28)
and F1 (29) classiﬁcation scores to evaluate the directional forecasts. The classiﬁcation metrics are computed on the following variables: ˜yt = 1yt−yt−1>0 and
˜ˆyt = 1ˆyt−yt−1>0.
RMSE(y, ˆy) =
(yi −ˆyi)2
SMAPE(y, ˆy) = 100
([yi] + |ˆyi|)/2
R2(y, ˆy) = 1 −
i=1(yi −ˆyi)2
i=1(yi −¯y)2
MAE(y, ˆy) = 1
MedAE(y, ˆy) = median(|y1 −ˆy1|, . . . , |yn −ˆyn|)
accuracy(˜y, ˜ˆy) = 1
precision(˜y, ˜ˆy) =
i=1 1˜yi=1 and ˜ˆyi=1
i=1 1˜ˆyi=1
recall(˜y, ˜ˆy) =
i=1 1˜yi=1 and ˜ˆyi=1
i=1 1˜yi=1
F1(˜y, ˜ˆy) = 2 · precision · recall
precision + recall
MSE is used as the loss function during the training process. Applying other
evaluation metrics is also reasonable, since they all have diﬀerent properties and
interpretation. MAE (the average of absolute errors) is easier to interpret than
the square root of MSE, MedAE is more robust to outliers, R2 score measures
the proportion of variance explained by the model, SMAPE measures the error
in percentage terms, ranging from 0 to 200%.
Classiﬁcation accuracy score simply measures the proportion of changes whose
direction we guessed right. Precision, recall and F1 are binary metrics. An
increase in the target variable is now treated as the positive class. Hence, in our
case, precision is the proportion of forecasted rises that were right, and recall
is the proportion of actual rises that we guessed right. F1 score is the harmonic
mean of precision and recall. We may get a better view of the forecasts’ quality
by using these various metrics together. Also, we can draw a confusion matrix
to evaluate all kinds of errors and correct forecasts at once.
Interval forecasts
The quality of prediction intervals should also be evaluated. We use the same
prediction interval assessment measures as Khosravi et al. .
PICP or prediction interval coverage probability (30) is the proportion of observations that fall into the interval. The wider the interval, the higher the
coverage — it is reasonable to quantify the size of the interval as well. MPIW
or mean prediction interval width (31) does exactly that. NMPIW or normalized mean prediction interval width (32) is the width normalized by the range of
the target variable. This metric allows for comparisons across datasets. CWC
or coverage width-based criterion (33) is a combined metric that takes into account both coverage and width. It has 2 hyperparameters that we can set: µ
corresponds to the nominal conﬁdence level, while η magniﬁes the diﬀerence
between µ and PICP. The lower CWC the higher quality.
1yi∈[Li,Hi]
NMPIW = MPIW
CWC = NMPIW(1 + 1P ICP <µexp(−η(PICP −µ)))
Empirical study
Our forecasting framework was implemented in Python (Van Rossum and Drake Jr
This section presents an example application on the Bike Sharing
Dataset (Fanaee-T and Gama ) available in the UCI Machine Learning
Repository (Dheeru and Karra Taniskidou ).
day of week
week of year
lagged (recent)
lagged (distant)
working hour
working day
month start
quarter start
Figure 1: Feature importances, detailed [LSTM]
The dataset is available in an hourly resolution, which allows us to construct
several seasonal variables. It contains counts of bike rentals for Capital Bikeshare at Washington, D.C., USA for 2 years. We disregard the available weather
information, and use time-determined features only. The pandas library (McKinney et al. ) was used during the data preparation.
Cyclical features were encoded using the sine and cosine transformations. The
following time components are encoded: season of year, month of year, week of
year, day of week and hour of day. Using all these features together is probably
a bit redundant, but hopefully the neural network is smart enough to handle
it. Some binary variables are used to indicate if the time is afternoon, working
hour, holiday, working day, start of quarter or start of month. Lagged values of
the target variable are also used as inputs: the preceding 3 timesteps’ values,
and lagged values of 24, 48 and 168 hours.
Each feature was scaled between zero and one using minmax scaler.
Variable importances were calculated on the bootstrap left-out validation set
and on the separate test set as well, using the single step forecasts. Two accuracy metrics were applied: R2 as a measure of goodness-of-ﬁt, and MDA as
a measure of directional accuracy. This setting led to 4 diﬀerent estimates of
variable importance (Figures 1 and 2). Those estimates were then averaged into
a single list of importances (Figures 3 and 4).
Feature importances are displayed with the following notation: r – regression
metric (R2); c – classiﬁcation metric (accuracy); v – bootstrap left-out validation set; t – future test set. These importance scores were only computed for
one-step forecasts, since shuﬄing the values of the iterative multi-step forecasts
was inconvenient.
Lagged values were divided into 2 groups: recent values (lags of 1, 2 and 3
day of week
week of year
lagged (recent)
lagged (distant)
working hour
working day
month start
quarter start
Figure 2: Feature importances, detailed [GRU]
day of week
week of year
lagged (recent)
lagged (distant)
working hour
working day
month start
quarter start
Figure 3: Feature importances, summary [LSTM]
day of week
week of year
lagged (recent)
lagged (distant)
working hour
working day
month start
quarter start
Figure 4: Feature importances, summary [GRU]
hours) and distant values (lags of 24, 48 and 168 hours). Naturally, the sine and
cosine transformed values of cyclical features were also treated together. The
maximum mean decrease accuracies are reported for these grouped variables.
Recent lagged values constitute the most important group of variables. They
are especially important for the value forecasts (regression). Seasonal variables
also seem to play an important role — intraday features like the hour of day,
afternoon or working hour got large importance scores. The trend variable does
not seem to have that much importance, though it was evident during model
building attempts that it can add to the RNNs’ forecasting capability.
Variable importances were pretty similar for the LSTM and GRU networks. We
can hardly see any disagreements in the feature rankings.
Predictions
Our recurrent neural networks consist of a one-layer LSTM/GRU of 32 units,
followed by a dense layer of a single unit with a linear activation. A dropout of
0.5 was applied to the non-recurrent connections. The learning rate was set to
0.001. The batch size and the number of epochs were 128. The mean squared
error loss function was minimized using the Adam optimizer (Kingma and Ba
 ). 16-step unrolled sequences were fed to the algorithm.
These hyperparameters were not optimized. It is just a reasonable setting for
showcasing the method on the given dataset.
The model was trained on 70% of the available data, the last 30% was used as
a test set. 50 bootstrap samples were taken from the training set.
Forecasts were made with approximate 90% prediction intervals.
Multi-step
forecasts were generated for the test set only. Some predictions are shown in
Figure 5 and Figure 6.
(a) One-step forecasts
(b) Multi-step forecasts
Figure 5: First 100 steps of test set forecasts with 90% prediction intervals
(a) One-step forecasts
(b) Multi-step forecasts
Figure 6: First 100 steps of test set forecasts with 90% prediction intervals
The recurrent neural networks were built and trained using Keras (Chollet et al.
 ) with TensorFlow backend (Abadi et al. ).
Evaluation
Several measures of forecast accuracy and directional accuracy were computed
on both evaluation sets.
MSE, SMAPE, R2, MAE and MedAE measure the forecasts’ ﬁt, while
accuracy, F1, precision and recall evaluate the quality of the forecasts’ changes
of direction. Most of these evaluation metrics were computed using scikit-learn
(Pedregosa et al. ).
The results of the regression evaluations are available in Table 1 and Table 2.
Table 3 and Table 4 show the change-of-direction binary metrics for the test
set. Confusion matrices of the directional predictions are displayed in Figure 7
and Figure 8.
The notations for the tables are the following: b – bagged estimator; i – individual estimators; v – bootstrap left-out validation set; t – future test set; o –
one-step forecasts; m – multi-step forecasts.
Table 1: Regression evaluation metrics [LSTM]
Table 2: Regression evaluation metrics [GRU]
Multi-step forecasts have consistently higher errors than the one-step-ahead
predictions. It is not surprising — just consider the accumulating errors of the
iterative forecasting procedure. Lagged values of the target variable proved to
be important predictors, especially for forecasting values, rather than just directions. Hence, the accuracy of values that we predict and reuse as inputs,
matters a lot. And anyway, the distant future obviously holds much more uncertainty, than the next timestep.
All R2 values are close to 0.95 for the single-step predictions, and are around 0.8
for the multi-step predictions. The multi-step RMSE is about twice as large
as the one-step. The directional accuracies are roughly 85% for the one-step,
while around 75% for the multi-step forecasts. For multiple steps, the direction
of change forecasts seem to work somewhat better than the value forecasts.
We have averaged the resulting estimates from all bootstrap samples, and used
the averages as our ﬁnal predictions — we used bagging. By comparing the
performance of the bagged estimator to the averaged performance of individual
estimators, we might evaluate the usefulness of this ensemble method for recurrent neural networks.
The bagged estimators produced consistently better results than the individual
neural networks. This is true for the regression and the classiﬁcation problem
as well. Thus, it seems that bagging can improve RNN-predictions.
Table 3: Classiﬁcation evaluation metrics [LSTM]
Table 4: Classiﬁcation evaluation metrics [GRU]
(a) One-step, LSTM
(b) Multi-step, LSTM
(c) One-step, GRU
(d) Multi-step, GRU
Figure 7: Confusion matrices [individual estimates]
(a) One-step, LSTM
(b) Multi-step, LSTM
(c) One-step, GRU
(d) Multi-step, GRU
Figure 8: Confusion matrices [bagged estimates]
The GRU and the LSTM networks are quite close to each other in terms of forecasting performance. The two networks seem to produce very similar forecasts.
Papadopoulos et al. found that the bootstrap consistently overestimates
prediction interval coverage. It seems to be conﬁrmed by our results, since all
of our prediction intervals have a higher coverage than the targeted 90%. Our
residual predictor neural network could probably have been further optimized,
in order to generate intervals closer to the desired coverage.
CWC’s µ was set to .9, since we aim to generate 90% prediction intervals. η
was set to the arbitrary value of 50.
The CWC metrics suggest that the prediction intervals of one-step forecasts are
better. It is hardly surprising, since the multi-step forecasts reached similar
coverage by producing much wider intervals. The coverage of our prediction
intervals exceeds the desired level of 90% in each case, so CWC equals NMPIW.
The prediction intervals’ evaluation metrics are available in Tables 5 and 6.
Table 5: PI evaluation metrics [LSTM]
Table 6: PI evaluation metrics [GRU]
Conclusions and Future Perspectives
This study aimed to explore and describe several aspects of the application of
recurrent neural networks to time series forecasting, though it is by far not comprehensive.
Recurrent neural networks are much more ﬂexible and much better suited to
time series forecasting than the linear models usually applied. Yet, several practices might help their application, some of which have been presented in this
We may do time series analysis with the aim of either forecasting future values
or understanding the processes driving the time series. Neural networks are
particularly bad in the latter. Feature importance measures solve this problem
partly. We computed permutation importance scores (mean decrease accuracy).
The target variable’s lagged values were the most important predictors in our
empirical experiment. Seasonality features also seemed important.
Another shortcoming of neural networks is the lack of prediction conﬁdence
measures. Interval forecasts can be useful for quantifying uncertainty, though
producing them for neural networks is nontrivial. Bootstrapping is a simple,
yet computationally expensive method that can do it. However, it produced
PIs with consistently higher coverage than what we targeted.
Multiple-step forecasts generated higher errors than single-step forecasts, as expected.
The gaps between the errors seemed smaller in case of direction of
change predictions.
We found that recurrent neural networks can beneﬁt from bagging.
The LSTM and GRU networks showed about the same forecasting performance.
It is hard to argue for either of them.
This forecasting framework might be enhanced in several ways.
During the training process, the states of the LSTM/GRU cells were reset in
each batch, so the dependencies between sequences in diﬀerent batches were not
taken into account. We would expect higher accuracies if we took advantage of
these relationships in the dataset, though it was diﬃcult with our bootstrapping
framework.
Our iterative method is probably not the best solution for making multi-step
forecasts.
A sequence to sequence learning model (e.g., Cho et al. ,
Sutskever et al. ) might be a better choice.
We could have constructed further input features. Feature engineering is crucial, and there is always room for improvement. Though neural networks are
very ﬂexible, so it didn’t seem so necessary.
Feature importances were only computed for one-step forecasts. It would be
worth exploring, if diﬀerent forecasting horizons require diﬀerent features to
make high quality forecasts. Other measures of variable importance could also
be applied.
Feature importances are only a tiny step towards understanding recurrent neural networks. The mechanism of RNN cells could and should be explored in
much more depth.
Bootstrapping is computationally intensive, but with today’s ever improving
GPUs, it is a feasible algorithm for time series datasets of manageable size.
Yet, it is a brute force method, so smarter solutions would be welcome.
There are several hyperparameters to optimize — it is also a disadvantage of
neural networks. In this article, we did not aim to ﬁnd the best parameters.
Grid search, or rather random search (Bergstra and Bengio ) could have
helped in ﬁnding the ideal settings.
The (small) size of real world datasets hinders deep learning methods in the
ﬁeld of time series forecasting. If our variable of interest were only observed
quarterly or yearly, we would have to wait several lifetimes to acquire a reasonable amount of data. Even this 2-year hourly bike sharing dataset was way too
small to exploit the capabilities of a neural network. It would be very useful
if we could train an algorithm on multiple similar datasets and gather some
collective knowledge that could be used to make better forecasts for the individual time series. This process of gaining knowledge and applying it to solve
diﬀerent problems is called transfer learning. It is already commonly used in,
for example, computer vision (Thrun ). Transfer learning is most useful
when the training data is scarce, so applying it to time series forecasting seems
very promising.
There is so much left to be done. RNNs clearly deserve a seat in the toolbox of
time series forecasting.