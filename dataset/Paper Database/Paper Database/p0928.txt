A simple yet effective baseline for 3d human pose estimation
Julieta Martinez1, Rayat Hossain1, Javier Romero2, and James J. Little1
1University of British Columbia, Vancouver, Canada
2Body Labs Inc., New York, NY
 , , , 
Following the success of deep convolutional networks,
state-of-the-art methods for 3d human pose estimation have
focused on deep end-to-end systems that predict 3d joint
locations given raw image pixels. Despite their excellent
performance, it is often not easy to understand whether
their remaining error stems from a limited 2d pose (visual)
understanding, or from a failure to map 2d poses into 3dimensional positions.
With the goal of understanding these sources of error,
we set out to build a system that given 2d joint locations
predicts 3d positions. Much to our surprise, we have found
that, with current technology, “lifting” ground truth 2d joint
locations to 3d space is a task that can be solved with a
remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by
about 30% on Human3.6M, the largest publicly available
3d pose estimation benchmark. Furthermore, training our
system on the output of an off-the-shelf state-of-the-art 2d
detector (i.e., using images as input) yields state of the art
results – this includes an array of systems that have been
trained end-to-end speciﬁcally for this task. Our results indicate that a large portion of the error of modern deep 3d
pose estimation systems stems from their visual analysis,
and suggests directions to further advance the state of the
art in 3d human pose estimation.
1. Introduction
The vast majority of existing depictions of humans are
two dimensional, e.g. video footage, images or paintings.
These representations have traditionally played an important role in conveying facts, ideas and feelings to other people, and this way of transmitting information has only been
possible thanks to the ability of humans to understand complex spatial arrangements in the presence of depth ambiguities. For a large number of applications, including virtual and augmented reality, apparel size estimation or even
autonomous driving, giving this spatial reasoning power to
machines is crucial. In this paper, we will focus on a particular instance of this spatial reasoning problem: 3d human
pose estimation from a single image.
More formally, given an image – a 2-dimensional representation – of a human being, 3d pose estimation is the
task of producing a 3-dimensional ﬁgure that matches the
spatial position of the depicted person. In order to go from
an image to a 3d pose, an algorithm has to be invariant to
a number of factors, including background scenes, lighting,
clothing shape and texture, skin color and image imperfections, among others. Early methods achieved this invariance
through features such as silhouettes , shape context ,
SIFT descriptors or edge direction histograms .
While data-hungry deep learning systems currently outperform approaches based on human-engineered features on
tasks such as 2d pose estimation (which also require these
invariances), the lack of 3d ground truth posture data for images in the wild makes the task of inferring 3d poses directly
from colour images challenging.
Recently, some systems have explored the possibility of
directly inferring 3d poses from images with end-to-end
deep architectures , and other systems argue that 3d
reasoning from colour images can be achieved by training
on synthetic data . In this paper, we explore the
power of decoupling 3d pose estimation into the well studied problems of 2d pose estimation , and 3d pose
estimation from 2d joint detections, focusing on the latter.
Separating pose estimation into these two problems gives
us the possibility of exploiting existing 2d pose estimation
systems, which already provide invariance to the previously
mentioned factors. Moreover, we can train data-hungry algorithms for the 2d-to-3d problem with large amounts of
3d mocap data captured in controlled environments, while
working with low-dimensional representations that scale
well with large amounts of data.
Our main contribution to this problem is the design and
analysis of a neural network that performs slightly better
than state-of-the-art systems (increasing its margin when
 
the detections are ﬁne-tuned, or ground truth) and is fast (a
forward pass takes around 3ms on a batch of size 64, allowing us to process as many as 300 fps in batch mode), while
being easy to understand and reproduce. The main reason
for this leap in accuracy and performance is a set of simple
ideas, such as estimating 3d joints in the camera coordinate
frame, adding residual connections and using batch normalization. These ideas could be rapidly tested along with other
unsuccessful ones (e.g. estimating joint angles) due to the
simplicity of the network.
The experiments show that inferring 3d joints from
groundtruth 2d projections can be solved with a surprisingly
low error rate – 30% lower than state of the art – on the
largest existing 3d pose dataset. Furthermore, training our
system on noisy outputs from a recent 2d keypoint detector yields results that slightly outperform the state-of-the-art
on 3d human pose estimation, which comes from systems
trained end-to-end from raw pixels.
Our work considerably improves upon the previous best
2d-to-3d pose estimation result using noise-free 2d detections in Human3.6M, while also using a simpler architecture. This shows that lifting 2d poses is, although far
from solved, an easier task than previously thought. Since
our work also achieves state-of-the-art results starting from
the output of an off-the-shelf 2d detector, it also suggests
that current systems could be further improved by focusing on the visual parsing of human bodies in 2d images.
Moreover, we provide and release a high-performance, yet
lightweight and easy-to-reproduce baseline that sets a new
bar for future work in this task. Our code is publicly available at 
3d-pose-baseline.
2. Previous work
Depth from images
The perception of depth from purely
2d stimuli is a classic problem that has captivated the attention of scientists and artists at least since the Renaissance,
when Brunelleschi used the mathematical concept of perspective to convey a sense of space in his paintings of Florentine buildings.
Centuries later, similar perspective cues have been exploited in computer vision to infer lengths, areas and distance ratios in arbitrary scenes . Apart from perspective
information, classic computer vision systems have tried to
use other cues like shading or texture to recover
depth from a single image. Modern systems 
typically approach this problem from a supervised learning
perspective, letting the system infer which image features
are most discriminative for depth estimation.
Top-down 3d reasoning
One of the ﬁrst algorithms for
depth estimation took a different approach: exploiting the
known 3d structure of the objects in the scene . It has
been shown that this top-down information is also used by
humans when perceiving human motion abstracted into a
set of sparse point projections . The idea of reasoning
about 3d human posture from a minimal representation such
as sparse 2d projections, abstracting away other potentially
richer image cues, has inspired the problem of 3d pose estimation from 2d joints that we are addressing in this work.
2d to 3d joints
The problem of inferring 3d joints from
their 2d projections can be traced back to the classic work
of Lee and Chen . They showed that, given the bone
lengths, the problem boils down to a binary decision tree
where each split correspond to two possible states of a
joint with respect to its parent.
This binary tree can be
pruned based on joint constraints, though it rarely resulted
in a single solution.
Jiang used a large database
of poses to resolve ambiguities based on nearest neighbor queries.
Interestingly, the idea of exploiting nearest
neighbors for reﬁning the result of pose inference has been
recently revisited by Gupta et al. , who incorporated
temporal constraints during search, and by Chen and Ramanan . Another way of compiling knowledge about 3d
human pose from datasets is by creating overcomplete bases
suitable for representing human poses as sparse combinations , lifting the pose to a reproducible
kernel Hilbert space (RHKS) or by creating novel priors from specialized datasets of extreme human poses .
Deep-net-based 2d to 3d joints
Our system is most related to recent work that learns the mapping between 2d
and 3d with deep neural networks.
Pavlakos et al. 
introduced a deep convolutional neural network based on
the stacked hourglass architecture that, instead of regressing 2d joint probability heatmaps, maps to probability distributions in 3d space. Moreno-Noguer learns
to predict a pairwise distance matrix (DM) from 2-to-3dimensional space.
Distance matrices are invariant up
to rotation, translation and reﬂection; therefore, multidimensional scaling is complemented with a prior of human
poses to rule out unlikely predictions.
A major motivation behind Moreno-Noguer’s DM regression approach, as well as the volumetric approach of
Pavlakos et al., is the idea that predicting 3d keypoints
from 2d detections is inherently difﬁcult.
For example,
Pavlakos et al. present a baseline where a direct 3d
joint representation (such as ours) is used instead (Table 1
in ), with much less accurate results than using volumetric regression1 Our work contradicts the idea that regressing 3d keypoints from 2d joint detections directly should
1This approach, however, is slightly different from ours, as the input is
still image pixels, and the intermediate 2d body representation is a series
of joint heatmaps – not joint 2d locations.
Batch norm
Dropout 0.5
Batch norm
Dropout 0.5
Figure 1. A diagram of our approach. The building block of our network is a linear layer, followed by batch normalization, dropout and a
RELU activation. This is repeated twice, and the two blocks are wrapped in a residual connection. The outer block is repeated twice. The
input to our system is an array of 2d joint positions, and the output is a series of joint positions in 3d.
be avoided, and shows that a well-designed and simple network can perform quite competitively in the task of 2d-to-3d
keypoint regression.
2d to 3d angular pose
There is a second branch of algorithms for inferring 3d pose from images which estimate
the body conﬁguration in terms of angles (and sometimes
body shape) instead of directly estimating the 3d position
of the joints . The main advantages of these
methods are that the dimensionality of the problem is lower
due to the constrained mobility of human joints, and that
the resulting estimations are forced to have a human-like
structure. Moreover, constraining human properties such
as bone lengths or joint angle ranges is rather simple with
this representation . We have also experimented with
such approaches; however in our experience the highly nonlinear mapping between joints and 2d points makes learning
and inference harder and more computationally expensive.
Consequently, we opted for estimating 3d joints directly.
3. Solution methodology
Our goal is to estimate body joint locations in 3dimensional space given a 2-dimensional input. Formally,
our input is a series of 2d points x ∈R2n, and our output
is a series of points in 3d space y ∈R3n. We aim to learn
a function f ∗: R2n →R3n that minimizes the prediction
error over a dataset of N poses:
L (f(xi) −yi) .
In practice, xi may be obtained as ground truth 2d joint
locations under known camera parameters, or using a 2d
joint detector. It is also common to predict the 3d positions
relative to a ﬁxed global space with respect to its root joint,
resulting in a slightly lower-dimensional output.
We focus on systems where f ∗is a deep neural network,
and strive to ﬁnd a simple, scalable and efﬁcient architecture
that performs well on this task. These goals are the main
rationale behind the design choices of our network.
3.1. Our approach – network design
Figure 1 shows a diagram with the basic building blocks
of our architecture.
Our approach is based on a simple, deep, multilayer neural network with batch normalization , dropout and Rectiﬁed Linear Units (RE-
LUs) , as well as residual connections . Not depicted are two extra linear layers: one applied directly to the
input, which increases its dimensionality to 1024, and one
applied before the ﬁnal prediction, that produces outputs
of size 3n. In most of our experiments we use 2 residual
blocks, which means that we have 6 linear layers in total,
and our model contains between 4 and 5 million trainable
parameters.
Our architecture beneﬁts from multiple relatively recent
improvements on the optimization of deep neural networks,
which have mostly appeared in the context of very deep
convolutional neural networks and have been the key ingredient of state-of-the-art systems submitted to the ILSVRC
(Imagenet ) benchmark. As we demonstrate, these contributions can also be used to improve generalization on our
2d-to-3d pose estimation task.
2d/3d positions
Our ﬁrst design choice is to use 2d and
3d points as inputs and outputs, in contrast to recent work
that has used raw images or
2d probability distributions as inputs, and 3d probabilities , 3d motion parameters or basis pose coef-
ﬁcients and camera parameter estimation 
as outputs.
While 2d detections carry less information,
their low dimensionality makes them very appealing to
work with; for example, one can easily store the entire Human3.6M dataset in the GPU while training the network,
which reduces overall training time, and considerably allowed us to accelerate the search for network design and
training hyperparameters.
Linear-RELU layers
Most deep learning approaches to
3d human pose estimation are based on convolutional neural networks, which learn translation-invariant ﬁlters that
can be applied to entire images , or 2dimensional joint-location heatmaps .
since we are dealing with low-dimensional points as inputs
and outputs, we can use simpler and less computationally
expensive linear layers. RELUs are a standard choice
to add non-linearities in deep neural networks.
Residual connections
We found that residual connections, recently proposed as a technique to facilitate the training of very deep convolutional neural networks , improve generalization performance and reduce training time.
In our case, they helped us reduce error by about 10%.
Batch normalization and dropout
While a simple network with the three components described above achieves
good performance on 2d-to-3d pose estimation when
trained on ground truth 2d positions, we have discovered
that it does not perform well when trained on the output
of a 2d detector, or when trained on 2d ground truth and
tested on noisy 2d observations. Batch normalization 
and dropout improve the performance of our system in
these two cases, while resulting in a slight increase of trainand test-time.
Max-norm constraint
We also applied a constraint on
the weights of each layer so that their maximum norm is
less than or equal to 1. Coupled with batch normalization,
we found that this stabilizes training and improves generalization when the distribution differs between training and
test examples.
3.2. Data preprocessing
We apply standard normalization to the 2d inputs and 3d
outputs by subtracting the mean and dividing by the standard deviation. Since we do not predict the global position
of the 3d prediction, we zero-centre the 3d poses around
the hip joint (in line with previous work and the standard
protocol of Human3.6M).
Camera coordinates
In our opinion, it is unrealistic to
expect an algorithm to infer the 3d joint positions in an arbitrary coordinate space, given that any translation or rotation of such space would result in no change in the input
data. A natural choice of global coordinate frame is the
camera frame since this makes the
2d to 3d problem similar across different cameras, implicitly enabling more training data per camera and preventing
overﬁtting to a particular global coordinate frame. We do
GT/GT + N(0, 5)
GT/GT + N(0, 10)
GT/GT + N(0, 15)
GT/GT + N(0, 20)
GT/CPM 
GT/SH 
Table 1. Performance of our system on Human3.6M under protocol #2. (Top) Training and testing on ground truth 2d joint locations plus different levels of additive gaussian noise. (Bottom)
Training on ground truth and testing on the output of a 2d detector.
this by rotating and translating the 3d ground-truth according to the inverse transform of the camera. A direct effect
of inferring 3d pose in an arbitrary global coordinate frame
is the failure to regress the global orientation of the person, which results in large errors in all joints. Note that the
deﬁnition of this coordinate frame is arbitrary and does not
mean that we are exploiting pose ground truth in our tests.
2d detections
We obtain 2d detections using the stateof-the-art stacked hourglass network of Newell et al. ,
pre-trained on the MPII dataset . Similar to previous
work , we use the bounding boxes provided with H3.6M to estimate the centre of the person in the
image. We crop a square of size 440 × 440 pixels around
this computed centre to the detector (which is then resized
to 256 × 256 by stacked hourglass). The average error between these detections and the ground truth 2d landmarks
is 15 pixels, which is slightly higher than the 10 pixels reported by Moreno-Noguer using CPM on the same
dataset. We prefer stacked hourglass over CPM because (a)
it has shown slightly better results on the MPII dataset, and
(b) it is about 10 times faster to evaluate, which allowed us
to compute detections over the entire H3.6M dataset.
We have also ﬁne-tuned the stacked hourglass model on
the Human3.6M dataset (originally pre-trained on MPII),
which obtains more accurate 2d joint detections on our target dataset and further reduces the 3d pose estimation error.
We used all the default parameters of stacked hourglass, except for minibatch size which we reduced from 6 to 3 due
to memory limitations on our GPU. We set the learning rate
to 2.5 × 10−4, and train for 40 000 iterations.
Training details
We train our network for 200 epochs using Adam , a starting learning rate of 0.001 and exponential decay, using mini-batches of size 64. Initially, the
weights of our linear layers are set using Kaiming initialization . We implemented our code using Tensorﬂow,
which takes around 5ms for a forward+backward pass, and
Protocol #1
Direct. Discuss Eating Greet Phone Photo
Pose Purch. Sitting SitingD Smoke
Wait WalkD Walk WalkT
LinKDE (SA)
132.3 164.4 162.1 205.9 150.6
162.1 170.7
127.9 162.1
Li et al. (MA)
96.9 124.7
Tekin et al. (SA)
88.8 125.3 118.0 182.7 112.4
118.4 138.8
65.8 125.0
Zhou et al. (MA)
87.1 103.2 116.2 143.3 106.9
107.4 118.1
97.7 113.0
Tekin et al. (SA)
91.4 121.7
Ghezelghieh et al. (SA)
Du et al. (SA)
104.9 122.1 139.1 135.9 105.9
120.0 117.7
106.5 126.5
Park et al. (SA)
90.0 116.5 115.3 149.5 117.6
105.8 125.1
96.2 117.3
Zhou et al. (MA)
98.8 113.4 125.2
99.0 107.3
Pavlakos et al. (MA)
Ours (SH detections) (SA)
Ours (SH detections) (MA)
Ours (SH detections FT) (MA)
Ours (GT detections) (MA)
Table 2. Detailed results on Human3.6M under Protocol #1 (no rigid alignment in post-processing). SH indicates that we trained and
tested our model with Stacked Hourglass detections as input, and FT indicates that the 2d detector model was ﬁne-tuned on H3.6M.
GT detections denotes that the groundtruth 2d locations were used. SA indicates that a model was trained for each action, and MA indicates
that a single model was trained for all actions.
around 2ms for a forward pass on a Titan Xp GPU. This
means that, coupled with a state-of-the-art realtime 2d detector (e.g., ), our network could be part of full pixelsto-3d system that runs in real time.
One epoch of training on the entire Human3.6M dataset
can be done in around 2 minutes, which allowed us to extensively experiment with multiple variations of our architecture and training hyperparameters.
4. Experimental evaluation
Datasets and protocols
We focus our numerical evaluation on two standard datasets for 3d human pose estimation:
HumanEva and Human3.6M . We also show qualitative results on the MPII dataset , for which the ground
truth 3d is not available.
Human3.6M is, to the best of our knowledge, currently
the largest publicly available datasets for human 3d pose
estimation. The dataset consists of 3.6 million images featuring 7 professional actors performing 15 everyday activities such as walking, eating, sitting, making a phone call
and engaging in a discussion. 2d joint locations and 3d
ground truth positions are available, as well as projection
(camera) parameters and body proportions for all the actors. HumanEva, on the other hand, is a smaller dataset that
has been largely used to benchmark previous work over the
last decade. MPII is a standard dataset for 2d human pose
estimation based on thousands of short youtube videos.
On Human3.6M we follow the standard protocol, using
subjects 1, 5, 6, 7, and 8 for training, and subjects 9 and
11 for evaluation. We report the average error in millimetres between the ground truth and our prediction across all
joints and cameras, after alignment of the root (central hip)
joint. Typically, training and testing is carried out independently in each action. We refer to this as protocol #1. However, in some of our baselines, the prediction has been further aligned with the ground truth via a rigid transformation
(e.g. ). We call this post-processing protocol #2. Similarly, some recent methods have trained one model for all
the actions, as opposed to building action-speciﬁc models.
We have found that this practice consistently improves results, so we report results for our method under these two
variations. In HumanEva, training and testing is done on
all subjects and in each action separately, and the error is
always computed after a rigid transformation.
4.1. Quantitative results
An upper bound on 2d-to-3d regression
Our method,
based on direct regression from 2d joint locations, naturally
depends on the quality of the output of a 2d pose detector,
and achieves its best performance when it uses ground-truth
2d joint locations.
We followed Moreno-Noguer and tested under different levels of Gaussian noise a system originally trained
with 2d ground truth.
The results can be found in Table 1. Our method largely outperforms the Distance-Matrix
method for all levels of noise, and achieves a peak
performance of 37.10 mm of error when it is trained on
ground truth 2d projections. This is about 43% better than
the best result we are aware of reported on ground truth 2d
joints . Moreover, note that this result is also about 30%
better than the 51.9 mm reported by Pavlakos et al. ,
which is the best result on Human3.6M that we aware of –
however, their result does not use ground truth 2d locations,
which makes this comparison unfair.
Although every frame is evaluated independently, and
we make no use of time, we note that the predictions produced by our network are quite smooth.
A video with
Protocol #2
Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD Smoke
Wait WalkD Walk WalkT
Akhter & Black * (MA) 14j
177.6 161.8 197.8 176.2 186.5 195.4 167.3
177.8 181.9
176.2 198.6
192.7 181.1
Ramakrishna et al. * (MA) 14j
149.3 141.6 154.3 157.7 158.9 141.8 158.1
160.4 161.7
150.0 174.8
150.2 157.3
Zhou et al. * (MA) 14j
87.9 116.8 108.3 107.3
106.0 102.2
106.5 110.4
115.2 106.7
Bogo et al. (MA) 14j
Moreno-Noguer (MA) 14j
Pavlakos et al. (MA) 17j
Ours (SH detections) (SA) 17j
Ours (SH detections) (MA) 17j
Ours (SH detections FT) (MA) 17j
Ours (SH detections) (SA) 14j
Table 3. Detailed results on Human3.6M under protocol #2 (rigid alignment in post-processing). The 14j (17j) annotation indicates
that the body model considers 14 (17) body joints. The results of all approaches are obtained from the original papers, except for (*), which
were obtained from .
Radwan et al. 
75.1 99.8 93.8 79.2 89.8 99.4 89.5
Wang et al. 
71.9 75.7 85.3 62.6 77.7 54.4 71.3
Simo-Serra et al. 65.1 48.6 73.5 74.2 46.6 32.2 56.7
Bo et al. 
46.4 30.3 64.9 64.5 48.0 38.2 48.7
Kostrikov et al. 
44.0 30.9 41.7 57.2 35.0 33.3 40.3
Yasin et al. 
35.8 32.4 41.6 46.6 41.4 35.4 38.9
Moreno-Noguer 
19.7 13.0 24.9 39.7 20.0 21.0 26.9
Pavlakos et al. 
22.1 21.9 29.0 29.8 23.6 26.0 25.5
Ours (SH detections)
19.7 17.4 46.8 26.9 18.2 18.6 24.6
Table 4. Results on the HumanEva dataset, and comparison
with previous work.
these and more qualitative results can be found at https:
//youtu.be/Hmi3Pd9x1BE.
Robustness to detector noise
To further analyze the robustness of our approach, we also experimented with testing
the system (always trained with ground truth 2d locations)
with (noisy) 2d detections from images. These results are
also reported at the bottom of Table 1.2 In this case, we also
outperform previous work, and demonstrate that our network can perform reasonably well when trained on ground
truth and tested on the output of a 2d detector.
Training on 2d detections
While using 2d ground truth
at train and test time is interesting to characterize the performance of our network, in a practical application our system has to work with the output of a 2d detector.
report our results on protocol #1 of Human3.6M in Table 2. Here, our closest competitor is the recent volumetric prediction method of Pavlakos et al. , which uses
a stacked-hourglass architecture, is trained end-to-end on
Human3.6M, and uses a single model for all actions. Our
2This was, in fact, the protocol used in the main result of .
method outperforms this state-of-the-art result by 4.4 mm
even when using out-of-the-box stacked-hourglass detections, and more than doubles the gap to 9.0 mm when the
2d detector is ﬁne-tuned on H3.6M. Our method also consistently outperforms previous work in all but one of the 15
actions of H3.6M.
Our results on Human3.6M under protocol #2 (using a
rigid alignment with the ground truth), are shown in Table 3.
Although our method is slightly worse than previous work
with out-of-the-box detections, it comes ﬁrst when we use
ﬁne-tuned detections.
Finally, we report results on the HumanEva dataset in
Table 4. In this case, we obtain the best result to date in 3
out of 6 cases, and overall the best average error for actions
Jogging and Walking. Since this dataset is rather small, and
the same subjects show up on the train and test set, we do
not consider these results to be as signiﬁcant as those obtained by our method in Human3.6M.
Ablative and hyperparameter analysis
We also performed an ablative analysis to better understand the impact
of the design choices of our network. Taking as a basis our
non-ﬁne tuned MA model, we present those results in Table 5. Removing dropout or batch normalization leads to
3-to8 mm of increase in error, and residual connections account for a gain of about 8 mm in our result. However, not
pre-processing the data to the network in camera coordinates results in error above 100 mm – substantially worse
than state-of-the-art performance.
Last but not least, we analyzed the sensitivity of our network to depth and width. Using a single residual block results in a loss of 6 mm, and performance is saturated after 2
blocks. Empirically, we observed that decreasing the layers
to 512 dimensions gave worse performance, while layers
with 2 048 units were much slower and did not seem to increase the accuracy.
Figure 2. Example output on the test set of Human3.6M. Left: 2d observation. Middle: 3d ground truth. Right (green): our 3d predictions.
error (mm)
w/o batch norm
w/o dropout
w/o batch norm w/o dropout
w/o residual connections
w/o camera coordinates
2 blocks (Ours)
Table 5. Ablative and hyperparameter sensitivity analysis.
4.2. Qualitative results
Finally, we show some qualitative results on Human3.6M in Figure 2, and from images “in the wild” from
the test set of MPII in Figure 3. Our results on MPII reveal
some of the limitations of our approach; for example, our
system cannot recover from a failed detector output, and it
has a hard time dealing with poses that are not similar to
any examples in H3.6M (e.g. people upside-down). Finally,
in the wild most images of people do not feature full bodies,
but are cropped to some extent. Our system, trained on full
body poses, is currently unable to deal with such cases.
5. Discussion
Looking at Table 2, we see a generalized increase in error when training with SH detections as opposed to training
with ground truth 2d across all actions – as one may well
expect. There is, however, a particularly large increase in
the classes taking photo, talking on the phone, sitting and
sitting down. We hypothesize that this is due to the severe self-occlusions in these actions – for example, in some
phone sequences, we never get to see one of the hands of
the actor. Similarly, in sitting and sitting down, the legs are
often aligned with the camera viewpoint, which results in
large amounts of foreshortening.
Further improvements
The simplicity of our system
suggests multiple directions of improvement in future work.
For example, we note that stacked hourglass produces ﬁnal joint detection heatmaps of size 64 × 64, and thus a
larger output resolution might result in more ﬁne-grained
detections, moving our system closer to its performance
when trained on ground truth. Another interesting direction is to use multiple samples from the 2d stacked hourglass heatmaps to estimate an expected gradient – `a la policy gradients, commonly used in reinforcement learning –
so as to train a network end-to-end. Yet another idea is to
emulate the output of 2d detectors using 3-dimensional mocap databases and “fake” camera parameters for data augmentation, perhaps following the adversarial approach of
Shrivastava et al. . Learning to estimate coherently the
depth of each person in the scene is an interesting research
path, since it would allow our system to work on 3d pose
estimation of multiple people. Finally, our architecture is
simple, and it is likely that further research into network
design could lead to better results on 2d-to-3d systems.
5.1. Implications of our results
We have demonstrated that a relatively simple deep feedforward neural network can achieve a remarkably low error
rate on 3d human pose estimation. Coupled with a state-ofthe-art 2d detector, our system obtains the best results on 3d
pose estimation to date.
Figure 3. Qualitative results on the MPII test set. Observed image, 2d detection with Stacked Hourglass , (in green) our 3d prediction.
The bottom 3 examples are typical failure cases, where either the 2d detector has failed badly (left), or slightly (right). In the middle, the 2d
detector does a ﬁne job, but the person is upside-down and Human3.6M does not provide any similar examples – the network still seems
to predict an average pose.
Our results stand in contrast to recent work, which has
focused on deep, end-to-end systems trained from pixels to
3d positions, and contradicts the underlying hypothesis that
justify the complexity of recent state-of-the-art approached
to 3d human pose estimation. For example, the volumetric
regression approach of et al. is based on the hypothesis that directly regressing 3d points is inherently difﬁcult,
and regression in a volumetric space would provide easier
gradients for the network (see Table 1 in ). Although
we agree that image content should help to resolve challenging ambiguous cases (consider for example the classic
turning ballerina optical illusion), competitive 3d pose estimation from 2d points can be achieved with simple high
capacity systems. This might be related to the latent information about subtle body and motion traits existing in
2d joint stimuli, such as gender, which can be perceived
by people . Similarly, the use of a distance matrix as
a body representation in is justiﬁed by the claim that
invariant, human-designed features should boost the accuracy of the system. However, our results show that well
trained systems can outperform these particular features in
a simple manner. It would be interesting to see whether a
combination of joint distances and joint positions boost the
performance even further – we leave this for future work.
6. Conclusions and future work
We have shown that a simple, fast and lightweight deep
neural network can achieve surprisingly accurate results in
the task of 2d-to-3d human pose estimation; and coupled
with a state-of-the-art 2d detector, our work results in an
easy-to-reproduce, yet high-performant baseline that outperforms the state of the art in 3d human pose estimation.
Our accuracy in 3d pose estimation from 2d ground
truth suggest that, although 2d pose estimation is considered a close to solved problem, it remains as one of the
main causes for error in the 3d human pose estimation task.
Moreover, our work represents poses in simple 2d and 3d
coordinates, which suggests that ﬁnding invariant (and more
complex) representations of the human body, as has been
the focus of recent work, might either not be crucial, or have
not been exploited to its full potential.
Finally, given its simplicity and the rapid development
in the ﬁeld, we like to think of our work as a future baseline, rather than a full-ﬂedged system for 3d pose estimation. This suggests multiple directions of future work.
For one, our network currently does not have access to visual evidence; we believe that adding this information to
our pipeline, either via ﬁne-tuning of the 2d detections or
through multi-sensor fusion will lead to further gains in performance. On the other hand, our architecture is similar to a
multi-layer perceptron, which is perhaps the simplest architecture one may think of. We believe that a further exploration of the network architectures will result in improved
performance. These are all interesting areas of future work.
Acknowledgments
The authors thank NVIDIA for the
donation of GPUs used in this research. Julieta was supported in part by the Perceiving Systems group at the Max
Planck Institute for Intelligent Systems. This research was
supported in part by the Natural Sciences and Engineering
Research Council of Canada (NSERC).