Technische Universit¨at M¨unchen
Fakult¨at f¨ur Informatik
Lehrstuhl VI: Echtzeitsysteme und Robotik
Supervised Sequence Labelling
with Recurrent Neural Networks
Alex Graves
Vollst¨andiger Abdruck der von der Fakult¨at f¨ur Informatik der Technischen
Universit¨at M¨unchen zur Erlangung des akademischen Grades eines
Doktors der Naturwissenschaften (Dr. rer. nat.)
genehmigten Dissertation.
Vorsitzender:
Univ.-Prof. B. Br¨ugge, Ph.D
Pr¨ufer der Dissertation:
1. Univ.-Prof. Dr. H. J. Schmidhuber
2. Univ.-Prof. Dr. St. Kramer
Die Dissertation wurde am 14.01.2008 bei der Technischen Universit¨at M¨unchen
eingereicht und durch die Fakult¨at f¨ur Informatik am 19.06.2008 angenommen.
Recurrent neural networks are powerful sequence learners. They are able
to incorporate context information in a ﬂexible way, and are robust to localised distortions of the input data. These properties make them well suited
to sequence labelling, where input sequences are transcribed with streams of
labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output
events, and thereby access long range context. The aim of this thesis is to
advance the state-of-the-art in supervised sequence labelling with recurrent
networks in general, and long short-term memory in particular.
main contributions are (1) a new type of output layer that allows recurrent
networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension
of long short-term memory to multidimensional data, such as images and
video sequences. Experimental results are presented on speech recognition,
online and oﬄine handwriting recognition, keyword spotting, image segmentation and image classiﬁcation, demonstrating the advantages of advanced
recurrent networks over other sequential algorithms, such as hidden Markov
Acknowledgements
I would like to thank my supervisor J¨urgen Schmidhuber for his guidance
and support. I would also like to thank my co-authors Santi, Tino, Nicole
and Doug, and everyone else at IDSIA for making it a stimulating and
creative place to work. Thanks to Tom Schaul for proofreading the thesis,
and Marcus Hutter for his mathematical assistance during the connectionist
temporal classiﬁcation chapter. I am grateful to Marcus Liwicki and Horst
Bunke for their expert collaboration on handwriting recognition. A special
mention goes to Fred and Matteo and all the other Idsiani who helped me
ﬁnd the good times in Lugano. Most of all, I would like to thank my family
and my wife Alison for their constant encouragement, love and support.
This research was supported in part by the Swiss National Foundation,
under grants 200020-100249, 200020-107534/1 and 200021-111968/1.
Acknowledgements
List of Tables
List of Figures
List of Algorithms
Introduction
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
Overview of Thesis . . . . . . . . . . . . . . . . . . . . . . . .
Supervised Sequence Labelling
Supervised Learning . . . . . . . . . . . . . . . . . . . . . . .
Pattern Classiﬁcation
. . . . . . . . . . . . . . . . . . . . . .
Probabilistic Classiﬁcation . . . . . . . . . . . . . . . .
Training Probabilistic Classiﬁers . . . . . . . . . . . .
Generative and Discriminative Models . . . . . . . . .
Sequence Labelling . . . . . . . . . . . . . . . . . . . . . . . .
A Taxonomy of Sequence Labelling Tasks . . . . . . .
Sequence Classiﬁcation . . . . . . . . . . . . . . . . . .
Segment Classiﬁcation . . . . . . . . . . . . . . . . . .
Temporal Classiﬁcation
. . . . . . . . . . . . . . . . .
Neural Networks
Multilayer Perceptrons . . . . . . . . . . . . . . . . . . . . . .
Forward Pass . . . . . . . . . . . . . . . . . . . . . . .
Output Layers
. . . . . . . . . . . . . . . . . . . . . .
Objective Functions
. . . . . . . . . . . . . . . . . . .
Backward Pass . . . . . . . . . . . . . . . . . . . . . .
Recurrent Neural Networks
. . . . . . . . . . . . . . . . . . .
Forward Pass . . . . . . . . . . . . . . . . . . . . . . .
Backward Pass . . . . . . . . . . . . . . . . . . . . . .
Bidirectional RNNs
. . . . . . . . . . . . . . . . . . .
Sequential Jacobian
. . . . . . . . . . . . . . . . . . .
Network Training . . . . . . . . . . . . . . . . . . . . . . . . .
Gradient Descent Algorithms . . . . . . . . . . . . . .
Generalisation
. . . . . . . . . . . . . . . . . . . . . .
Input Representation . . . . . . . . . . . . . . . . . . .
Weight Initialisation . . . . . . . . . . . . . . . . . . .
Long Short-Term Memory
The LSTM Architecture . . . . . . . . . . . . . . . . . . . . .
Inﬂuence of Preprocessing . . . . . . . . . . . . . . . . . . . .
Gradient Calculation . . . . . . . . . . . . . . . . . . . . . . .
Architectural Enhancements . . . . . . . . . . . . . . . . . . .
LSTM Equations . . . . . . . . . . . . . . . . . . . . . . . . .
Forward Pass . . . . . . . . . . . . . . . . . . . . . . .
Backward Pass . . . . . . . . . . . . . . . . . . . . . .
Framewise Phoneme Classiﬁcation
Experimental Setup
. . . . . . . . . . . . . . . . . . . . . . .
Network Architectures . . . . . . . . . . . . . . . . . . . . . .
Computational Complexity . . . . . . . . . . . . . . .
Range of Context . . . . . . . . . . . . . . . . . . . . .
Output Layers
. . . . . . . . . . . . . . . . . . . . . .
Network Training . . . . . . . . . . . . . . . . . . . . . . . . .
Retraining . . . . . . . . . . . . . . . . . . . . . . . . .
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Comparison with Previous Work . . . . . . . . . . . .
Eﬀect of Increased Context . . . . . . . . . . . . . . .
Weighted Error . . . . . . . . . . . . . . . . . . . . . .
Hidden Markov Model Hybrids
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Experiment: Phoneme Recognition . . . . . . . . . . . . . . .
Experimental Setup
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
Connectionist Temporal Classiﬁcation
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
From Outputs to Labellings . . . . . . . . . . . . . . . . . . .
Role of the Blank Labels . . . . . . . . . . . . . . . . .
CTC Forward-Backward Algorithm . . . . . . . . . . . . . . .
Log Scale . . . . . . . . . . . . . . . . . . . . . . . . .
CTC Objective Function . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Best Path Decoding
. . . . . . . . . . . . . . . . . . .
Preﬁx Search Decoding
. . . . . . . . . . . . . . . . .
Constrained Decoding . . . . . . . . . . . . . . . . . .
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Phoneme Recognition
. . . . . . . . . . . . . . . . . .
Phoneme Recognition with Reduced Label Set
Keyword Spotting
. . . . . . . . . . . . . . . . . . . .
Online Handwriting Recognition
. . . . . . . . . . . .
Oﬄine Handwriting Recognition
. . . . . . . . . . . .
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Multidimensional Recurrent Networks
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The MDRNN architecture . . . . . . . . . . . . . . . . . . . .
Multidirectional MDRNNs
. . . . . . . . . . . . . . .
Multidimensional Long Short-Term Memory . . . . . .
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Air Freight Data . . . . . . . . . . . . . . . . . . . . .
MNIST Data . . . . . . . . . . . . . . . . . . . . . . . 100
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 101
Conclusions and Future Work
Bibliography
List of Tables
Phoneme classiﬁcation error rate on TIMIT . . . . . . . . . .
Comparison of BLSTM with previous neural network results
Phoneme error rate on TIMIT
. . . . . . . . . . . . . . . . .
Phoneme error rate on TIMIT
. . . . . . . . . . . . . . . . .
Folding the 61 phonemes in TIMIT onto 39 categories . . . .
Reduced phoneme error rate on TIMIT
. . . . . . . . . . . .
Keyword error rate on Verbmobil . . . . . . . . . . . . . . . .
CTC Character error rate on IAM-OnDB . . . . . . . . . . .
Word error rate on IAM-OnDB . . . . . . . . . . . . . . . . .
Word error rate on IAM-DB . . . . . . . . . . . . . . . . . . .
Image error rate on MNIST . . . . . . . . . . . . . . . . . . . 101
List of Figures
Sequence labelling
. . . . . . . . . . . . . . . . . . . . . . . .
Taxonomy of sequence labelling tasks . . . . . . . . . . . . . .
Importance of context in segment classiﬁcation . . . . . . . .
Multilayer perceptron
. . . . . . . . . . . . . . . . . . . . . .
Neural network activation functions
. . . . . . . . . . . . . .
Recurrent neural network
. . . . . . . . . . . . . . . . . . . .
Standard and bidirectional RNNs . . . . . . . . . . . . . . . .
Sequential Jacobian for a bidirectional RNN . . . . . . . . . .
Overﬁtting on training data . . . . . . . . . . . . . . . . . . .
Vanishing gradient problem for RNNs
. . . . . . . . . . . . .
LSTM memory block with one cell . . . . . . . . . . . . . . .
Preservation of gradient information by LSTM
. . . . . . . .
Various networks classifying an excerpt from TIMIT . . . . .
Framewise phoneme classiﬁcation results on TIMIT . . . . . .
Learning curves on TIMIT . . . . . . . . . . . . . . . . . . . .
BLSTM network classifying the utterance “one oh ﬁve”
CTC and framewise classiﬁcation . . . . . . . . . . . . . . . .
CTC forward-backward algorithm
. . . . . . . . . . . . . . .
Evolution of the CTC error signal during training . . . . . . .
Problem with best path decoding . . . . . . . . . . . . . . . .
Preﬁx search decoding . . . . . . . . . . . . . . . . . . . . . .
CTC outputs for keyword spotting on Verbmobil . . . . . . .
Sequential Jacobian for keyword spotting on Verbmobil
CTC network labelling an excerpt from IAM-OnDB
CTC Sequential Jacobian from IAM-OnDB . . . . . . . . . .
7.10 CTC Sequential Jacobian from IAM-OnDB . . . . . . . . . .
2D RNN forward pass . . . . . . . . . . . . . . . . . . . . . .
2D RNN backward pass . . . . . . . . . . . . . . . . . . . . .
Sequence ordering of 2D data . . . . . . . . . . . . . . . . . .
LIST OF FIGURES
Context available to a 2D RNN with a single hidden layer . .
Axes used by the hidden layers in a multidirectional 2D RNN
Context available to a multidirectional 2D RNN . . . . . . . .
Frame from the Air Freight database . . . . . . . . . . . . . .
MNIST image before and after deformation . . . . . . . . . . 100
2D RNN applied to an image from the Air Freight database . 101
8.10 Sequential Jacobian of a 2D RNN for an image from MNIST
List of Algorithms
BRNN Forward Pass . . . . . . . . . . . . . . . . . . . . . . .
BRNN Backward Pass . . . . . . . . . . . . . . . . . . . . . .
Preﬁx Search Decoding Algorithm. . . . . . . . . . . . . . . .
CTC Token Passing Algorithm . . . . . . . . . . . . . . . . .
MDRNN Forward Pass . . . . . . . . . . . . . . . . . . . . . .
MDRNN Backward Pass . . . . . . . . . . . . . . . . . . . . .
Multidirectional MDRNN Forward Pass . . . . . . . . . . . .
Multidirectional MDRNN Backward Pass
. . . . . . . . . . .
Introduction
In machine learning, the term sequence labelling encompasses all tasks where
sequences of data are transcribed with sequences of labels. Well-known examples include speech and handwriting recognition, protein secondary structure prediction and part-of-speech tagging. Supervised sequence labelling
refers speciﬁcally those cases where a set of inputs with target transcriptions is provided for algorithm training. What distinguishes such problems
from the traditional framework of supervised pattern classiﬁcation is that
the individual data points cannot be assumed to be independent and identically distributed (i.i.d.).
Instead, both the inputs and the labels form
strongly correlated sequences. In speech recognition for example, the input
(a speech signal) is produced by the continuous motion of the vocal tract,
while the labels (a sequence of words) are constrained by the laws of syntax
and grammar. A further complication is that in many cases the alignment
between inputs and labels is unknown. This requires the use of algorithms
able to determine the location as well as the identity of the output labels.
Recurrent neural networks (RNNs) have several properties that make
them an attractive choice for sequence labelling. They are able to incorporate contextual information from past inputs (and future inputs too, in
the case of bidirectional RNNs), which allows them to instantiate a wide
range of sequence-to-sequence maps. Furthermore, they are robust to localised distortions of the input sequence along the time axis. The purpose
of this thesis is to extend and apply RNNs to real-world tasks in supervised
sequence labelling.
One shortcoming of RNNs is that the range of contextual information
they have access to is in practice quite limited. Long Short-Term Memory
(LSTM) is an RNN architecture speciﬁcally designed to overcome this limitation. In various synthetic tasks, LSTM has been shown capable of bridging
very long time lags between relevant input and target events. Moreover, it
has also proved advantageous in real-world domains such as speech processing and bioinformatics. For this reason, LSTM will be the RNN of choice
CHAPTER 1. INTRODUCTION
throughout the thesis, and a major subgoal is to analyse, apply and extend
the LSTM architecture.
Contributions
In the original formulation of LSTM 
an approximate form of the error gradient was used for training. We present
equations for the exact gradient, evaluated with backpropagation through
time . As well as being more accurate, the exact
gradient has the advantage of being easier to debug. We also combine LSTM
with bidirectional RNNs to give bidirectional
LSTM (BLSTM), thereby providing access to long range context in both
input directions. We compare the performance of BLSTM to other neural
network architectures on the task of framewise phoneme classiﬁcation.
Framewise phoneme classiﬁcation requires the alignment between the
speech signal and the output labels (the phoneme classes) to be known in
advance, e.g. using hand labelled data. However, for full speech recognition,
and many other real-world sequence labelling tasks, the alignment is unknown. So far, the most eﬀective way to apply RNNs to such tasks has been
to combine them with hidden Markov models (HMMs) in the so-called hybrid approach . We investigate the application
of HMM-LSTM hybrids to phoneme recognition.
More importantly, we introduce an output layer, known as connectionist
temporal classiﬁcation (CTC), that allows RNNs to be trained directly for
sequence labelling tasks with unknown input-label alignments, without the
need for HMM hybrids. For some tasks, we want to constrain the output
label sequence to obey some probabilistic grammar. In speech recognition,
for example, we usually want the output to be a sequence of dictionary
words, and may wish to weight the probabilities of diﬀerent word sequences
according to a probabilistic language model. We present an eﬃcient decoding algorithm that constrains CTC outputs according to a dictionary and, if
necessary, a bigram language model. In several experiments on speech and
handwriting recognition, we ﬁnd that a BLSTM network with a CTC output
layer outperforms HMMs and HMM-RNN hybrids, often by a substantial
In particular, we obtain excellent results in online handwriting
recognition, both with and without task-speciﬁc preprocessing.
RNNs were originally designed for one-dimensional time series. However, some of their advantages in time-series learning, such as robustness to
input distortion, and the use of contextual information, are also desirable
in multidimensional domains, such as image and video processing. Multidimensional RNNs (MDRNNs), a special case of directed acyclic graph RNNs
 , are a generalisation of RNNs to
multidimensional data. MDRNNs have a separate recurrent connection for
CHAPTER 1. INTRODUCTION
every spatio-temporal dimension in the data, thereby matching the structure
of the network to the structure of the input. In particular, multidirectional
MDRNNs give the network access to context information in all input directions. We introduce multidimensional LSTM by adding extra recurrent
connections to the internal structure of the LSTM memory blocks, thereby
bringing the beneﬁts of long range context to MDRNNs. We apply multidimensional LSTM to two image segmentation tasks, and ﬁnd that it is more
robust to distortion than a state-of-the-art image classiﬁcation algorithm.
Overview of Thesis
The chapters are roughly grouped into three parts: background material is
presented in Chapters 2–4, Chapters 5 and 6 are primarily experimental,
and new methods are introduced in Chapters 7 and 8.
Chapter 2 brieﬂy reviews supervised learning in general, and supervised
pattern classiﬁcation in particular. It also provides a formal deﬁnition of supervised sequence labelling, and presents a taxonomy of sequence labelling
tasks that arise under diﬀerent assumptions about the relationship between
the input and label sequences. Chapter 3 provides background material for
feedforward and recurrent neural networks, with emphasis on their application to classiﬁcation and labelling tasks. It also describes the use of the
sequential Jacobian, which is the matrix of partial derivatives of a particular output with respect to a sequence of inputs, as a tool for analysing
the use of contextual information by RNNs. Chapter 4 describes the LSTM
architecture, presents equations for the full LSTM error gradient, and introduces bidirectional LSTM (BLSTM). Chapter 5 contains an experimental
comparison of BLSTM to other neural network architectures on the task of
framewise phoneme classiﬁcation. Chapter 6 investigates the use of LSTM
in hidden Markov model-neural network hybrids. Chapters 7 and 8 introduce connectionist temporal classiﬁcation and multidimensional RNNs respectively, and concluding remarks and directions for future work are given
in Chapter 9.
Supervised Sequence
This chapter provides the background material and literature review for
supervised sequence labelling. Section 2.1 brieﬂy reviews supervised learning
in general.
Section 2.2 covers the classical, non-sequential framework of
supervised pattern classiﬁcation.
Section 2.3 deﬁnes supervised sequence
labelling, and presents a taxonomy of sequence labelling tasks that arise
under diﬀerent assumptions about the label sequences, discussing algorithms
and error measures suitable for each one.
Supervised Learning
Machine learning problems where a set of input-target pairs is provided
for training are referred to as supervised learning tasks.
This is distinct
from reinforcement learning, where only a positive or negative reward value
is provided for training, and unsupervised learning, where no task-speciﬁc
training signal exists at all, and the algorithm attempts to uncover the
structure of the data by inspection alone.
The nature and degree of supervision provided by the targets varies
greatly between supervised learning tasks. For example, training a supervised learner to correctly label every pixel corresponding to an aeroplane
in an image requires a much more informative target than simply training
it recognise whether or not an aeroplane is present. To distinguish these
extremes, people sometimes refer to weakly and strongly labelled data. Although we will not use this terminology in the thesis, the output layer introduced in Chapter 7 can be thought of
A standard supervised learning task consists of a training set S of inputtarget pairs (x, z), where x is an element of the input space X and z is an
element of the target space Z, and a disjoint test set S′, both drawn independently from the same input-target distribution DX×Z. In some cases
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
an extra validation set is drawn from the training set to validate the performance of the learning algorithm.
The goal of the task is to use the
training set to minimise some task-speciﬁc error measure E evaluated on
the test set. For example, in a regression task, the usual error measure is
the sum-of-squares, or squared Euclidean distance between the algorithm
outputs and the target vectors. For parametric algorithms (such as neural
networks) the usual approach to error minimisation is to incrementally adjust the algorithm parameters to optimise an objective function O on the
training set, where O is related, but not necessarily identical, to E. The
ability of an algorithm to transfer performance from the training set to the
test set is referred to as generalisation, and is discussed in the context of
neural networks in Section 3.3.2.
Pattern Classiﬁcation
Pattern classiﬁcation, also known as pattern recognition, has been extensively studied in the machine learning literature , and various pattern classiﬁcation algorithms, such as multilayer perceptrons and support vector machines , are routinely used for commercial tasks.
Although pattern classiﬁcation deals with non-sequential data, much of
the practical and theoretical framework underlying it carries over to the
sequential case. It is therefore instructive to brieﬂy review this framework
before we turn to sequence labelling proper.
Since we will deal exclusively with supervised pattern classiﬁcation, we
assume the presence of a training set S of input-target pairs (x, z), where
each input x is a real-valued vector of some ﬁxed length M, and each target
z represents a single class drawn from a set of K classes. A fundamental
assumption underlying most work on supervised pattern classiﬁcation is that
the input-target pairs are independent and identically distributed (i.i.d.).
Then let h be a pattern classiﬁer that maps from input vectors onto class
labels, and S′ be a test set of input-target pairs that are not contained in
S. For pattern classiﬁcation tasks where all misclassiﬁcations are equally
bad, the aim is to use S to train a classiﬁer in such a way that minimises
classiﬁcation error rate Eclass on S′
Eclass(h, S′) =
0 if h(x) = z
1 otherwise
In the more general case where diﬀerent mistakes have diﬀerent weights,
we can deﬁne a loss matrix L whose elements lij are the losses incurred by
assigning a pattern with true class Ci to class Cj. In this situation we seek
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
instead to minimise the classiﬁcation loss Eloss on S′
Eloss(h, L, S′) =
Note that (2.1) can be recovered as a special case of (2.2), where all diagonal
elements of L are 0, and all others are 1.
Probabilistic Classiﬁcation
One approach to pattern classiﬁcation is to to train a classiﬁer that directly
maps from input patterns onto class labels. This type of classiﬁer, of which
support vector machines are a well known example, is referred to as a discriminant function. Another approach is probabilistic classiﬁcation, where
the conditional probabilities p(Ck|x) of the K classes given the input pattern
x are ﬁrst determined, and the most probable is then chosen as the classiﬁer
output h(x):
h(x) = arg max
There are numerous advantages to the probabilistic approach. One of these
is that retaining the probabilities of the diﬀerent labels allows diﬀerent classiﬁers to be combined in a consistent way. Another is that the probabilities
can be used to account for the diﬀerent losses associated with diﬀerent types
of misclassiﬁcation. In the latter case Eqn. (2.3) is modiﬁed by the addition
of the loss matrix L:
h(x) = arg min
Lkjp(Ck|x)
Training Probabilistic Classiﬁers
A further beneﬁt of probabilistic classiﬁcation is that it provides a consistent methodology for algorithm training. If a probabilistic classiﬁer hw with
adjustable parameters w yields a particular conditional probability distribution p(Ck|x, w) over the class labels Ck, we can take a product over the
i.i.d. input-target pairs in the training set S to get
p(z|x, w),
which we can invert with Bayes’ theorem to obtain
p(w|S) = p(S|w)p(w)
In theory, the posterior distribution over classes for some new input x can
then be found by integrating over all possible values of w:
p(Ck|x, S) =
p(Ck|x, w)p(w|S)dw
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
In practice though, w is usually very high dimensional and calculating the
above distribution, known as the predictive distribution of h, is intractable.
A common approximation, known as the maximum a priori (MAP) approximation, is to ﬁnd the single parameter vector wMAP that maximises (2.6)
and use this to make predictions:
p(Ck|x) = p(Ck|x, wMAP )
Since p(S) is independent of w, we have
wMAP = arg max
p(S|w)p(w)
The parameter prior p(w) is usually referred to as a regularisation term.
Its eﬀect is to weight the classiﬁer towards those parameter values which
are deemed a priori more probable. In accordance with Occam’s razor, we
usually assume that more complex parameters (where ‘complex’ is typically
interpreted as ‘requiring more information to accurately describe’) are inherently less probable. For this reason p(w) is sometimes referred to as an
Occam factor or complexity penalty. In the particular case of a Gaussian
parameter prior, where p(w) ∝|w|2, the p(w) term is referred to as weight
decay. If, on the other hand, we assume a uniform prior over parameters, we
can drop the p(w) term from (2.9) to obtain the maximum likelihood (ML)
parameter vector wML
wML = arg max
p(S|w) = arg max
The standard procedure for ﬁnding wML is to minimise an objective function
O deﬁned as the negative logarithm of p(S|w)
p(z|x, w) = −
ln p(z|x, w),
where ln is the logarithm to base e.
Note that, since the logarithm is
monotonically increasing, minimising −ln p(S|w) is equivalent to maximising p(S|w).
We will return to the maximum likelihood approximation in Chapters 3
and 7, where we will use it to derive objective functions for neural networks.
Generative and Discriminative Models
Algorithms that directly calculate the class probabilities p(Ck|x) (often referred to as the posterior class probabilities) are called discriminative models.
In some cases however, it is preferable to ﬁrst calculate the class conditional
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
densities p(x|Ck) and then use Bayes’ theorem, together with the prior class
probabilities p(Ck) to ﬁnd the posterior values
p(Ck|x) = p(x|Ck)p(Ck)
p(x|Ck)p(Ck),
Algorithms following this approach are known as generative models, because
the input priors p(x) can be used to generate artiﬁcial input data.
advantage of the generative approach is that each class can be trained independently of the others, whereas discriminative models have to be retrained
every time a new class is added. However, discriminative methods typically
give better results for classiﬁcation tasks, because they concentrate all their
modelling eﬀort on ﬁnding the correct class boundaries.
This thesis focuses on discriminative models for sequence labelling. However, we will refer at various points to the well known generative model
hidden Markov models .
Sequence Labelling
The goal of sequence labelling is to assign a sequence of labels, drawn from
a ﬁxed and ﬁnite alphabet, to a sequence of input data.
For example,
one might wish to label a sequence of acoustic features with spoken words
(speech recognition), or a sequence of video frames with hand gestures (gesture recognition). Although such tasks commonly arise when analysing time
series, they are also found in domains with non-temporal sequences, such as
protein secondary structure prediction.
For some problems the precise alignment of the labels with the input data
must also be determined by the learning algorithm. In this thesis however,
we limit our attention to tasks where the alignment is either predetermined,
by some manual or automatic preprocessing, or it is unimportant, in the
sense that we require only the ﬁnal sequence of labels, and not the times at
which they occur.
Sequence labelling diﬀers from pattern classiﬁcation in that the inputs
are sequences x of ﬁxed size, real-valued vectors, while the targets are sequences z of discrete labels, drawn from some ﬁnite alphabet L. (From now
on we will use a bold typeface to denote sequences). As with pattern classiﬁcation, we assume that the input-target pairs (x, z) are i.i.d. Of course
we do not assume the individual data-points within each sequence to be
i.i.d. While the i.i.d. assumption may not be entirely correct (e.g. when the
input sequences represent turns in a spoken dialogue, or lines in a handwritten form) it is reasonable as long as the sequence boundaries are sensibly
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
Figure 2.1: Sequence labelling. The algorithm receives a stream of input
data, and outputs a sequence of discrete labels.
chosen. We further assume that the label sequences are at most as long as
the corresponding input sequences. With these restrictions in mind we can
formalise the task of sequence labelling as follows:
Let S be a set of training examples drawn independently from a ﬁxed
distribution DX×Z. The input space X = (RM)∗is the set of all sequences
of size M real-valued vectors. The target space Z = L∗is the set of all
sequences over the (ﬁnite) alphabet L of labels. We refer to elements of L∗
as label sequences or labellings. Each element of S is a pair of sequences
The target sequence z = (z1, z2, ..., zU) is at most as long as the
input sequence x = (x1, x2, ..., xT ), i.e. U ≤T. Regardless of whether the
data is a time series, we refer to the distinct points in the input sequence as
timesteps.
The task is then to use S to train a sequence labelling algorithm h : X 7→
Z to label the sequences in a test set S′ ⊂DX×Z, disjoint from S, in a way
that minimises some task-dependent error measure.
A Taxonomy of Sequence Labelling Tasks
So far we have described sequence labelling in its most general form. However, in some cases we can apply further constraints to the type of label
sequences required. These constraints aﬀect both the choice of algorithm
and the error measures used to assess performance. In what follows, we
provide a taxonomy of three classes of sequence labelling task, and discuss
algorithms and error measures suitable for each class.
The taxonomy is
outlined in Figure 2.2
Sequence Classiﬁcation
In the most restrictive case, the label sequences are constrained to be length
one. We refer to this as sequence classiﬁcation, since each input sequence is
assigned to a single class.
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
Figure 2.2: Taxonomy of sequence labelling tasks. Sequence classiﬁcation, where each input sequence is assigned a single class, is a special case
of segment classiﬁcation, where each of a predeﬁned set of input segments
is given a label. Segment classiﬁcation is a special case of temporal classiﬁcation, where any alignment between input and label sequences is allowed.
Examples of sequence classiﬁcation tasks include the identiﬁcation of a
single face and the recognition of an individual handwritten letter. A key
feature of sequence classiﬁcation is that the entire sequence can be processed
before the classiﬁcation is made.
If the input sequences are of ﬁxed length, or can be easily padded to
a ﬁxed length, they can be collapsed into a single input vector and any
of the standard pattern classiﬁcation algorithms mentioned in Section 2.2
can be applied. A prominent testbed for ﬁxed-length sequence classiﬁcation is the MNIST isolated digits dataset . Numerous
pattern classiﬁcation algorithms have been applied to MNIST, including
convolutional neural networks 
and support vector machines =
0 if h(x) = z
1 otherwise
which we refer to as the sequence error rate.
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
Figure 2.3: Importance of context in segment classiﬁcation.
word ‘defence’ is clearly legible. However the letter ‘n’ in isolation is ambiguous.
Segment Classiﬁcation
In many cases however, multiple classiﬁcations are required for a single input
sequence. For example, we might wish to classify each letter in a line of
handwriting, or each phoneme in a spoken utterance. If the positions of
the input segments for which the classiﬁcations are required are known in
advance, we refer to such tasks as segment classiﬁcation.
A crucial element of segment classiﬁcation, missing from sequence classiﬁcation, is the use of context — i.e. data on either side of the segments
to be classiﬁed. The eﬀective use of context is vital to the success of segment classiﬁcation algorithms, as illustrated in Figure 2.3. This presents a
problem for standard pattern classiﬁcation algorithms, which are designed
to process only one input at a time. A simple solution is to collect the data
on either side of the segment into a time-window, and use this as an input
pattern. However, as well as the aforementioned issue of shifted or distorted
data, the time-window approach suﬀers from the fact that the range of useful
context (and therefore the required time-window size) is generally unknown,
and may vary widely from segment to segment. Consequently the case for
sequential algorithms is stronger here than in sequence classiﬁcation.
An obvious error measure for segment classiﬁcation tasks is the segment
error rate Eseg(h, S′), which simply counts the number of misclassiﬁed segments
Eseg(h, S′) = 1
HD(h(x), z)
Where Z is the total length of all target sequences in the test set, and
HD(p, q) is the hamming distance between two equal length sequences p
and q (i.e. the number of places in which they diﬀer).
In speech recognition, the classiﬁcation of each acoustic frame as a separate segment is often known as framewise phoneme classiﬁcation. In this
context the segment error rate is usually referred to as the frame error rate.
We apply various neural network architectures to framewise phoneme classi-
ﬁcation in Chapter 5. In image processing, the classiﬁcation of each pixel, or
block of pixels, as a separate segment is known as image segmentation. We
CHAPTER 2. SUPERVISED SEQUENCE LABELLING
apply multidimensional recurrent neural networks to image segmentation in
Chapter 8.
Temporal Classiﬁcation
In the most general case, nothing can be assumed about the label sequences
except that their length is less than or equal to that of the input sequences.
They may even be empty. We refer to this situation as temporal classiﬁcation .
The key distinction between temporal classiﬁcation and segment classi-
ﬁcation is that the former requires an algorithm that can decide where in
the input sequence the classiﬁcations should be made. This in turn requires
an implicit or explicit model of the global structure of the sequence.
For temporal classiﬁcation, the segment error rate is inapplicable, since
the alignment between the input and label sequences is unknown. Instead
we measure the number of label substitutions, insertions and deletions that
would be required to turn one sequence into the other, giving us the label
error rate Elab(h, S′):
Elab(h, S′) = 1
Where Z is the total length of all target sequences in the test set, and
LEV (p, q) is the edit orLevenshtein distance between the two sequences p
and q (i.e. the minimum number of insertions, substitutions and deletions
required to change p into q). See for example for eﬃcient
edit distance calculation algorithms.
A family of similar error measures can be deﬁned by introducing other
types of edit operation, such as transpositions (caused by e.g. typing errors),
or by weighting the relative importance of the operations. For the purposes
of this thesis however, the label error rate is suﬃcient.
We will usually
refer to the label error rate according to the type of label in question, e.g.
phoneme error rate, word error rate etc.
For some temporal classiﬁcation tasks a completely correct labelling is
required (for example when reading a postcode from an envelope) and the
degree of error is unimportant. In this cases the sequence error rate (2.14)
should be used to assess performance.
We investigate the use of hidden Markov model-recurrent neural network
hybrids for temporal classiﬁcation in Chapter 6, and introduce a neural
network only approach to temporal classiﬁcation in Chapter 7.
Neural Networks
This chapter provides the background material and literature review for
neural networks, with particular emphasis on their application to classiﬁcation and labelling tasks. Section 3.1 reviews multilayer perceptrons and
their application to pattern classiﬁcation. Section 3.2 reviews recurrent neural networks and their application to sequence labelling. It also describes
the sequential Jacobian, an analytical tool for studying the use of context
information by RNNs. Section 3.3 discusses various issues, such as generalisation and input data representation, that are essential to eﬀective network
Multilayer Perceptrons
Artiﬁcial neural networks (ANNs) were originally developed as mathematical
models of the information processing capabilities of biological brains .
Although it is now clear that ANNs bear little resemblance to real biological
neurons, they enjoy continuing popularity as pattern classiﬁers.
The basic structure of an ANN is a network of small processing units,
or nodes, which are joined to each other by weighted connections. In terms
of the original biological model, the nodes represent neurons, and the connection weights represent the strength of the synapses between the neurons.
The network is activated by providing an input to some or all of the nodes,
and this activation then spreads throughout the network along the weighted
connections. The electrical activity of biological neurons typically follows a
series of sharp ‘spikes’, and the activation of an ANN node was originally
intended to model the average ﬁring rate of these spikes.
Many varieties of ANNs have appeared over the years, with widely varying properties. One important distinction is between ANNs whose connections form cycles, and those whose connections are acyclic.
cycles are referred to as feedback, recursive, or recurrent, neural networks,
CHAPTER 3. NEURAL NETWORKS
Figure 3.1: Multilayer perceptron
and are dealt with in Section 3.2. ANNs without cycles are referred to as
feedforward neural networks (FNNs). Well known examples of FNNs include
perceptrons , radial basis function networks , Kohonen maps and Hopﬁeld nets . The most widely used form of FNN, and the one we focus on
in this section, is the multilayer perceptron .
The units in a multilayer perceptron are arranged in layers, with connections feeding forward from one layer to the next (Figure 3.1).
patterns are presented to the input layer, and the resulting unit activations
are propagated through the hidden layers to the output layer. This process
is known as the forward pass of the network. The units in the hidden layers
have (typically nonlinear) activation functions that transform the summed
activation arriving at the unit. Since the output of an MLP depends only
on the current input, and not on any past or future inputs, MLPs are more
suitable for pattern classiﬁcation than for sequence labelling. We will discuss
this point further in Section 3.2.
An MLP can be thought of as a function that maps from input to output vectors. Since the behaviour of the function is parameterised by the
connection weights, a single MLP is capable of instantiating many diﬀerent
functions. Indeed it has been proven that an MLP
with a single hidden layer containing a suﬃcient number of nonlinear units
can approximate any continuous function on a compact input domain to
arbitrary precision. For this reason MLPs are said to be universal function
CHAPTER 3. NEURAL NETWORKS
Figure 3.2: Neural network activation functions
approximators.
Forward Pass
Consider an MLP with I input units, activated by input vector x. Each
unit in the ﬁrst hidden layer calculates a weighted sum of the input units.
For hidden unit h, we refer to this sum as the network input to unit h, and
denote it ah. The activation function θh is then applied, yielding the ﬁnal
activation bh of the unit. Denoting the weight from unit i to unit j as wij,
bh = θh(ah)
The two most common choices of activation function are the hyperbolic
tanh(x) = e2x −1
and the logistic sigmoid
Note that these functions are related by the linear transform tanh(x) =
2σ(2x) −1, and are therefore essentially equivalent (any function computed
by a network with a hidden layer of σ units can be computed by another
network with tanh units and vice-versa). Although there are diﬀerences in
CHAPTER 3. NEURAL NETWORKS
the way a network learns with the two functions (e.g. because tanh has a
steeper slope) we have found that they give the same results in practice, and
the two are used interchangeably throughout the thesis.
One vital feature of σ and tanh is their nonlinearity. Nonlinear neural
networks are far more powerful than linear ones since they can, for example, ﬁnd nonlinear classiﬁcation boundaries and model nonlinear equations.
Moreover, any combination of linear operators is itself a linear operator,
which means that any MLP with multiple linear hidden layers is exactly
equivalent to some other MLP with a single linear hidden layer. This contrasts with nonlinear networks, which can gain considerable power by using
successive hidden layers to re-represent the input data .
Another key property is that both functions are diﬀerentiable, which allows the network to be trained with gradient descent. Their ﬁrst derivatives
tanh′(a) = 1 −tanh(a)2
σ′(a) = σ(a)(1 −σ(a))
Because of the way they squash an inﬁnite input domain into a ﬁnite output range, neural network activation functions are sometimes referred to as
squashing functions.
Having calculated the activations of the units in the ﬁrst hidden layer,
the process of summation and activation is then repeated for the rest of the
hidden layers in turn, e.g. for unit h in hidden layer l
bh = θh(ah),
Where Hl is the number of units in hidden layer l.
Output Layers
The output vector y of an MLP is given by the activation of the units in the
output layer. The network input ak to each output unit k is calculated by
summing over the units connected to it, exactly as for a hidden unit, i.e.
for a network with m hidden layers.
Both the number of units in the output layer and the choice of output
activation function depend on the task. For binary classiﬁcation tasks, the
standard conﬁguration is a single unit with a logistic sigmoid activation
CHAPTER 3. NEURAL NETWORKS
(Eqn. (3.4)). Since the range of the logistic sigmoid is the open interval (0, 1),
the activation of the output unit can be interpreted as an estimate of the
probability that the input vector belongs to the ﬁrst class (and conversely,
one minus the activation estimates the probability that it belongs to the
second class)
p(C1|x) = y = σ(a)
p(C2|x) = 1 −y
The use of the logistic sigmoid as a binary probability estimator is referred
to in the statistics literature as logistic regression, or a logit model. If we use
a coding scheme for the target vector z where z = 1 if the correct class is C1
and z = 0 if the correct class is C2, we can combine the above expressions
p(z|x) = yz(1 −y)1−z
For classiﬁcation problems with K > 2 classes, the convention is to have K
output units, and normalise the output activations with the softmax function to obtain estimates of the class probabilities:
p(Ck|x) = yk =
k′=1 eak′ ,
which is termed a multinomial logit model by statisticians. A 1-of-K coding
scheme represent the target class z as a binary vector with all elements
equal to zero except for element k, corresponding to the correct class Ck,
which equals one. For example, if K = 5 and the correct class is C2, z
is represented by (0, 1, 0, 0, 0). Using this scheme we obtain the following
convenient form for the target probabilities:
Given the above deﬁnitions, the use of MLPs for pattern classiﬁcation is
straightforward. Simply feed in an input vector, activate the network, and
choose the class label corresponding to the most active output unit.
Objective Functions
The derivation of objective functions for MLP training follows the steps outlined in Section 2.2.2. Although attempts have been made to approximate
the full predictive distribution of Eqn. (2.7) for neural networks , we will here focus on objective functions derived using
maximum likelihood. Note that maximum likelihood can be trivially extended to the MAP approximation of Eqn. (2.9) by adding a regularisation
term, such as weight-decay, which corresponds to a suitably chosen prior over
CHAPTER 3. NEURAL NETWORKS
the weights. For binary classiﬁcation, substituting (3.11) into the maximum
likelihood objective function deﬁned in (2.11), we have
z ln y + (1 −z) ln(1 −y)
Similarly, for problems with multiple classes, substituting (3.13) into (2.11)
Collectively, the above objective functions are referred to as cross-entropy
error. See for more information on these and other
MLP objective functions.
Backward Pass
Since MLPs are, by construction, diﬀerentiable operators, they can be trained
to minimise any diﬀerentiable objective function using gradient descent. The
basic idea of gradient descent is to ﬁnd the derivative of the objective function with respect to each of the network weights, then adjust the weights in
the direction of the negative slope. Gradient descent methods for training
neural networks are discussed in more detail in Section 3.3.1.
Note that the cross-entropy error terms deﬁned above are a sum over the
input-target pairs in the training set. Therefore their derivatives are also a
sum of separate terms. From now on, when we refer to the derivatives of
an objective function, we implicitly mean the derivatives for one particular
input-target pair. The derivatives for the whole training set can then be
calculated by summing over the pairs. However, it is often advantageous to
consider the derivatives of the pairs separately, since doing so allows the use
of online gradient-descent (see Section 3.3.1).
To eﬃciently calculate the gradient, we use a technique known as backpropagation . This is often referred to as the backward pass of the network.
Backpropagation is simply a repeated application of chain rule for partial derivatives. The ﬁrst step is to calculate the derivatives of the objective
function with respect to the output units. For a binary classiﬁcation network, diﬀerentiating the objective function deﬁned in (3.14) with respect to
the network outputs gives
The chain rule informs us that
CHAPTER 3. NEURAL NETWORKS
and we can then substitute (3.6) and (3.16) into (3.17) to get
For a multiclass network, diﬀerentiating (3.15) gives
Bearing in mind that the activation of each unit in a softmax layer depends
on the network input to every unit in the layer, the chain rule gives us
Diﬀerentiating (3.12) we obtain
= ykδkk′ −ykyk′,
and we can then substitute (3.21) and (3.19) into (3.20) to get
where we have the used the fact that PK
k=1 zk = 1. Note the similarity to
(3.18). In the objective function is said to match the
output layer activation function when the output derivative has this form.
We now continue to apply the chain rule, working backwards through the
hidden layers. At this point it is helpful to introduce the following notation:
where j is any unit in the network. For the units in the last hidden layer,
where we have used the fact that any objective function O depends only on
unit h through its inﬂuence on the output units. Diﬀerentiating (3.9) and
(3.2) and substituting into (3.24) gives
δh = θ′(aj)
CHAPTER 3. NEURAL NETWORKS
The δ terms for each hidden layer l before the last one can then be calculated
recursively:
δh = θ′(ah)
Once we have the δ terms for all the hidden units, we can use (3.1) to
calculate the derivatives with respect to each of the network weights:
Numerical Gradient
When implementing backpropagation, it is strongly recommended to check
the weight derivatives numerically. This can be done by adding positive and
negative perturbations to each weight and calculating the changes in the
objective function:
= O(wij + ϵ) −O(wij −ϵ)
This technique is known as symmetrical ﬁnite diﬀerences. Note that setting
ϵ too small leads to numerical underﬂows and decreased accuracy.
optimal value therefore depends on the computer architecture and ﬂoating
point accuracy of a given implementation. For the systems we used, ϵ = 10−5
generally gave best results.
Recurrent Neural Networks
In the previous section we considered ANNs whose connections did not form
cycles. If we relax this condition, and allow cyclical connections as well, we
obtain recurrent neural networks (RNNs). As with FNNs, many varieties of
RNN have been proposed, such as Elman networks , Jordan
networks , time delay neural networks and
echo state networks . In this section, we focus on a simple
RNN containing a single, self connected hidden layer, as shown in Figure 3.3
While the extension from MLPs to RNNs may seem trivial, the implications for sequence learning are far-reaching. An MLP can only map from
input to output vectors, whereas an RNN can in principle map from the
entire history of previous inputs to each output. Indeed, the equivalent result to the universal approximation theory for MLPs is that an RNN with a
suﬃcient number of hidden units can approximate any measurable sequenceto-sequence mapping to arbitrary accuracy . The key point
is that the recurrent connections allow a ‘memory’ of previous inputs to
persist in the network’s internal state, which can then be used to inﬂuence
the network output.
CHAPTER 3. NEURAL NETWORKS
Figure 3.3: Recurrent neural network
Forward Pass
The forward pass of an RNN is the same as that of an MLP with a single
hidden layer, except that activations arrive at the hidden layer from both
the current external input and the hidden layer activations one step back
in time. Consider a length T input sequence x presented to an RNN with
I input units, H hidden units, and K output units. Let xt
i be the value of
input i at time t, and let at
j be respectively the network input to unit
j at time t and the activation of unit j at time t. For the hidden units we
Nonlinear, diﬀerentiable activation functions are then applied exactly as for
The complete sequence of hidden activations can be calculated by starting at
t = 1 and recursively applying (3.29) and (3.30), incrementing t at each step.
Note that this requires initial values b0
i to be chosen for the hidden units,
corresponding to the network’s state before it receives any information from
the data sequence. In this thesis, b0
i is always set to zero. However, other
researchers have found that in some cases, RNN stability and robustness to
noise can be improved by using nonzero initial values (Zimmermann et al.,
The network inputs to the output units can be calculated at the same
time as the hidden activations:
CHAPTER 3. NEURAL NETWORKS
For sequence classiﬁcation and segment classiﬁcation tasks (see Section 2.3) the same output activation functions can be used for RNNs as
for MLPs (i.e. softmax and logistic sigmoid), with the classiﬁcation targets
typically presented at the ends of the sequences or segments.
It follows
that the same objective functions can be used too. Chapter 7 introduces an
output layer speciﬁcally designed for temporal classiﬁcation with RNNs.
Backward Pass
Given the partial derivatives of the objective function with respect to the
network outputs, we now need the derivatives with respect to the weights.
Two well-known algorithms have been devised to eﬃciently calculate weight
derivatives for RNNs: real time recurrent learning and backpropagation through time . We focus on BPTT since it is both conceptually
simpler and more eﬃcient in computation time (though not in memory).
Like standard backpropagation, BPTT consists of a repeated application
of the chain rule. The subtlety is that, for recurrent networks, the objective
function depends on the activation of the hidden layer not only through its
inﬂuence on the output layer, but also through its inﬂuence on the hidden
layer at the next timestep, i.e.
The complete sequence of δ terms can be calculated by starting at t = T
and recursively applying (3.32), decrementing t at each step. (Note that
= 0 ∀j, since no error is received from beyond the end of the sequence).
Finally, bearing in mind that the weights to and from each unit in the hidden
layer are the same at every timestep, we sum over the whole sequence to get
the derivatives with respect to each of the network weights
Bidirectional RNNs
For many sequence labelling tasks, we would like to have access to future
as well as past context. For example, when classifying a particular written
letter, it is helpful to know the letters coming after it as well as those before.
However, since standard RNNs process sequences in temporal order, they
CHAPTER 3. NEURAL NETWORKS
Figure 3.4: Standard and bidirectional RNNs
ignore future context.
One obvious solution is to add a time-window of
future context to the network input.
However, as well as unnecessarily
increasing the number of input weights, this suﬀers from the same problems
as the time-window approaches discussed in Sections 2.3.2 and 2.3.3, namely
intolerance of distortions, and a ﬁxed range of context. Another possibility
is to introduce a delay between the inputs and the targets, thereby giving
the network a few timesteps of future context.
This method retains the
RNN’s robustness to distortions, but it still requires the range of future
context to be determined by hand. Furthermore it places an unnecessary
burden on the network by forcing it to ‘remember’ the original input, and
its previous context, throughout the delay. In any case, neither of these
approaches remove the asymmetry between past and future information.
Bidirectional recurrent neural networks oﬀer a more elegant solution. The
basic idea of BRNNs is to present each training sequence forwards and backwards to two separate recurrent hidden layers, both of which are connected
to the same output layer (Figure 3.4). This provides the network with complete, symmetrical, past and future context for every point in the input
sequence, without displacing the inputs from the relevant targets. BRNNs
have previously given improved results in various domains, notably protein
secondary structure prediction and speech processing . In this
thesis we ﬁnd that BRNNs consistently outperform unidirectional RNNs on
real-world sequence labelling tasks.
The forward pass for the BRNN hidden layers is the same as for a unidirectional RNN, except that the input sequence is presented in opposite
CHAPTER 3. NEURAL NETWORKS
directions to the two hidden layers, and the output layer is not updated
until both hidden layers have processed the entire input sequence:
for t = 1 to T do
Do forward pass for the forward hidden layer, storing activations at
each timestep
for t = T to 1 do
Do forward pass for the backward hidden layer, storing activations at
each timestep
for t = 1 to T do
Do forward pass for the output layer, using the stored activations from
both hidden layers
Algorithm 3.1: BRNN Forward Pass
Similarly, the backward pass proceeds as for a standard RNN trained
with BPTT, except that all the output layer δ terms are calculated ﬁrst,
then fed back to the two hidden layers in opposite directions:
for t = T to 1 do
Do BPTT backward pass for the output layer only, storing δ terms at
each timestep
for t = T to 1 do
Do BPTT backward pass for the forward hidden layer, using the stored
δ terms from the output layer
for t = 1 to T do
Do BPTT backward pass for the backward hidden layer, using the
stored δ terms from the output layer
Algorithm 3.2: BRNN Backward Pass
BRNNs and Causal Tasks
A common objection to BRNNs is that they violate causality. Clearly, for
tasks such as ﬁnancial prediction or robot navigation, an algorithm that
requires access to future inputs is unfeasible.
However, there are many
problems for which causality is unnecessary. Most obviously, for tasks where
the input sequences are spatial and not temporal (e.g. protein secondary
structure prediction), there should be no distinction between past and future
inputs. This is perhaps why bioinformatics is the domain where BRNNs
have been most widely adopted. However BRNNs can also be applied to
temporal tasks, as long as the network outputs are only needed at the end
of some input segment. For example, in speech and handwriting recognition,
the data is usually divided up into sentences, lines, or dialogue turns, each
of which is completely processed before the output labelling is required.
Furthermore, even for online temporal tasks, such as automatic dictation,
CHAPTER 3. NEURAL NETWORKS
bidirectional algorithms can be used as long as it is acceptable to wait for
some natural break in the input, e.g. a pause in speech, before processing a
section of the data.
Sequential Jacobian
It should be clear from the preceding discussions that the ability to make
use of contextual information is vitally important for sequence labelling. It
therefore seems desirable to have a way of analysing exactly where and how
an algorithm uses context during a particular data sequence. For RNNs, we
can take a big step towards this by measuring the sensitivity of the network
outputs to the network inputs.
For feedforward neural networks, the Jacobian matrix J is deﬁned as
having elements equal to the derivatives of the network outputs with respect
to the network inputs
These derivatives measure the relative sensitivity of the outputs to small
changes in the inputs, and can therefore be used, for example, to detect
irrelevant inputs. The Jacobian can be extended to recurrent neural networks by specifying the timesteps at which the input and output variables
are measured
We refer to the resulting four-dimensional matrix as the sequential Jacobian.
Figure 3.5 provides a sample plot of a slice through the sequential Jacobian. In general we are interested in observing the sensitivity of an output
at one time (for example, at the point when the network makes a label prediction) to the inputs at all times in the sequence. Note that the absolute
magnitude of the derivatives is not important. What matters is the relative
magnitudes of the derivatives to each other, since this determines the degree
to which the output is inﬂuenced by each input.
Slices like that shown in Figure 3.5 can be calculated with a simple
modiﬁcation of the RNN backward pass described in Section 3.2.2. First,
all output delta terms are set to zero except some δt
k, corresponding to the
time t and output k we are interested to.
This term is set equal to its
own activation during the forward pass, i.e. δt
k. The backward pass is
then carried out as usual, and the resulting delta terms at the input layer
correspond to the sensitivity of the output to the inputs over time. The
other derivatives calculated, e.g. for individual weights or the units in the
hidden layer, are also interesting, because they show the response of the
output to diﬀerent parts of the network over time.
We will use similar plots of the sequential Jacobian at various points
throughout the thesis as a means of analysing the use of context by RNNs.
CHAPTER 3. NEURAL NETWORKS
Figure 3.5: Sequential Jacobian for a bidirectional RNN during an
online handwriting recognition task. The derivatives of a single output
unit at time t = 300 are evaluated with respect to the two inputs (corresponding to the x and y coordinates of the pen) at all times throughout
the sequence. For bidirectional networks, the magnitude of the derivatives
typically forms an ‘envelope’ centred on t. In this case the derivatives remains large for about 100 timesteps before and after t. The magnitudes are
greater for the input corresponding to the x co-ordinate (blue line) because
this has a smaller normalised variance than the y input (x tends to increase
steadily as the pen moves from left to right, whereas y ﬂuctuates about a
ﬁxed baseline).
However it should be noted that sensitivity does not correspond directly
to contextual importance. For example, the sensitivity may be very large
towards an input that never changes, such as a corner pixel in a set of
images with a ﬁxed colour background, since the network does not ‘expect’
to see any change there. However, this pixel will not provide any context
information. Also, as shown in Figure 3.5, the sensitivity will be larger for
inputs with lower variance, since the network is tuned to smaller changes.
But this does not mean that these inputs are more important than those
with larger variance.
CHAPTER 3. NEURAL NETWORKS
Network Training
So far we have discussed how various neural networks can be diﬀerentiated
with respect to suitable objective functions, and thereby trained with gradient descent. However, to ensure that network training is both eﬀective and
tolerably fast, and that it generalises successfully to unseen data, several
issues must be considered.
Gradient Descent Algorithms
Most obviously, we need to decide which algorithm to use when following
the error gradient. The simplest such algorithm, generally known as steepest
descent or just gradient descent, is to repeatedly take a small, ﬁxed-size step
in the direction of the negative error gradient
∆w(n) = −α
where 0 ≤α ≤1 is the learning rate and w(n) represents the vector of
weights after the nth weight update.
A major problem with steepest descent is that it easily gets stuck in local
minima. This can be mitigated by the addition of a momentum term , which eﬀectively adds inertia to the motion of
the algorithm through weight space, thereby speeding up convergence and
helping to escape from local minima
∆w(n) = m∆w(n −1) −α
where 0 ≤m ≤1 is the momentum parameter.
A large number of more sophisticated gradient descent algorithms have
been developed that are generally observed to outperform steepest descent, if the gradient of the entire training set is calculated at once.
Such methods are known as batch learning algorithms. However, one advantage of steepest descent is that it lends itself naturally to online learning
(also known as sequential learning), where the weight updates are made after each pattern or sequences in the training set. Online steepest descent is
often referred to sequential gradient descent or stochastic gradient descent.
Online learning tends to be more eﬃcient than batch learning when large
datasets containing signiﬁcant redundancy or regularity are used .
In addition, the stochasticity of online learning can help
to escape from local minima , since the stationary
points of the objective function will be diﬀerent for each training example.
The stochasticity can be further increased by randomising the order of the
CHAPTER 3. NEURAL NETWORKS
sequences in the training set before each pass through the training set (often
referred to as a training epoch). The technique of training set randomisation
is used throughout this thesis
A recently proposed alternative for online learning is stochastic metadescent , which has been shown to give faster convergence and improved results for a variety of neural network tasks. However
our attempts to train RNNs with stochastic meta-descent were unsuccessful, and all experiments in this thesis were carried out using online steepest
descent with momentum.
Generalisation
Although the objective functions for network training are, of necessity, de-
ﬁned on the training set, the ﬁnal goal is to achieve the best performance on
some previously unseen test data. The issue of whether training set performance carries over to the test set is referred to as generalisation, and is of
fundamental importance to machine learning . Many methods for improved generalisation have been proposed over
the years, usually derived from statistical principles. In this thesis, however,
only two of the simplest, empirical methods for generalisation are used: early
stopping and training with noise.
Early Stopping
For early stopping, part of the training set is extracted for use as a validation
set. The objective function is evaluated at regular intervals during training
on the validation set, and the weight values giving the lowest validation
error are stored as the ‘best’ for that network.
For our experiments we
typically evaluate the validation error after every ﬁve training epochs, and
wait until ten such validations have passed without a new lowest value for
the validation error before stopping the network training. The network is
never trained on the validation set. In principle, the network should not be
evaluated on the test set at all until all training is complete.
During training, the error typically decreases at ﬁrst on both sets, but
after a certain point it begins to rise on the validation set. This behaviour is
referred to as overﬁtting on the training data, and is illustrated in Figure 3.6.
Early stopping is perhaps the simplest and most universally applicable
method for improved generalisation. However, one drawback of early stopping is that some of the training set has to be sacriﬁced for the validation
set, which can lead to reduced performance, especially if the training set
is small. Another problem is that there is no way of determining a priori
how big the validation set should be. For the experiments in this thesis,
we typically use ﬁve to ten percent of the training set for validation. Note
that the validation set does not have to be an accurate predictor of test set
CHAPTER 3. NEURAL NETWORKS
Figure 3.6: Overﬁtting on training data. Initially, network error decreases rapidly on all datasets.
Soon however it begins to level oﬀand
gradually rise on the validation and test sets. The dashed line indicates
the point of best performance on the validation set, which is close, but not
identical to the optimal point for the test set. These learning curves were
recorded during an oﬄine handwriting recognition task.
performance; it is only important that overﬁtting begins at approximately
the same time on it as on the test set.
Training with Noise
Adding Gaussian noise to the network inputs during training (also referred
to as training with jitter) is a well-established method for improved generalisation , closely
related to statistical regularisation techniques such as ridge regression. Its
eﬀect is to artiﬁcially enhance the size of the training set by generating new
input values suﬃciently close to the original ones that a mapping from inputs to targets can still be learned. This tends to decrease performance on
the training set, but increase it on the validation and test sets. However, if
the variance of the noise is too great, the network fails to learn at all.
Determining exactly how large the variance should be is a major diﬃculty
with training with noise, and appears to be heavily dependent on the dataset.
Although various rules of thumb exist, the most reliable method is to set the
variance empirically on the validation set. For some of our experiments on
speech recognition, we added noise with a variance of ﬁfty to sixty percent
of the variance of the original data. For other datasets, however, we found
that adding any amount of noise led to reduced performance.
CHAPTER 3. NEURAL NETWORKS
Input Representation
Choosing a suitable representation for the input data is a vital part of any
machine learning task. Indeed, in many cases the preprocessing needed to
get the data in the required form is more important to the ﬁnal performance
than the algorithm itself.
Neural networks tend to be relatively robust
to the choice of input representation: for example, in previous work on
speech recognition, RNNs were shown to perform almost equally well using
a wide range of preprocessing methods . Similarly,
in Chapter 7 we report excellent results in online handwriting recognition
using two very diﬀerent input representations.
Nonetheless, for acceptable results, the input data presented to an ANN
should be both complete (in the sense that it contains all the information
required to successfully predict the outputs) and reasonably compact. Although irrelevant inputs are not as much of a problem for neural networks as
they are for algorithms suﬀering from the so-called curse of dimensionality
 , having a very high dimensional input space, such as
raw images, leads to an excessive number of input weights and poor performance. Beyond that, the choice of input representation is something of a
black art. In particular, since neural networks do not make explicit assumptions about the form of the input data, there are no absolute requirements
for which representation to choose. However, best performance is generally
obtained when the relationship between the inputs and targets is as simple
and localised as possible.
One procedure that should be carried out for all input data used for
neural network training is to standardise the components of the input vectors
to have mean 0 and standard deviation 1 over the training set. That is, ﬁrst
calculate the mean
and standard deviation
(mi −xi)2,
of each component of the input vector, then calculate the standardised input
vectors ˆx as follows
ˆxi = xi −mi
This procedure does not alter the information in the training set, but it
improves performance by putting the input values in a range more suitable
for the standard activation functions . Note that test
and validation sets should be standardised with the mean and standard
deviation of the training set.
CHAPTER 3. NEURAL NETWORKS
Input standardisation can have a considerable eﬀect on network performance, and was carried out for all experiments in this thesis.
Weight Initialisation
All standard gradient descent algorithms require the network to have small,
random, initial weights. For the experiments in this thesis, we initialised
the weights with either a ﬂat random distribution in the range [−0.1, 0.1] or
a Gaussian distribution with mean 0, standard deviation 0.1. However, we
did not ﬁnd our results to be very sensitive to either the distribution or the
range. A consequence of having random initial conditions is that multiple
repetitions of each experiment must be carried out in order to determine
signiﬁcance.
Long Short-Term Memory
As discussed in the previous chapter, an important beneﬁt of recurrent networks is their ability to use contextual information when mapping between
input and output sequences.
Unfortunately, for standard RNN architectures, the range of context that can be accessed is limited. The problem
is that the inﬂuence of a given input on the hidden layer, and therefore
on the network output, either decays or blows up exponentially as it cycles
around the network’s recurrent connections. In practice this shortcoming
 makes it hard for an RNN
to learn tasks containing delays of more than about 10 timesteps between
relevant input and target events . The vanishing
gradient problem is illustrated schematically in Figure 4.1
Numerous attempts were made in the 1990s to address the problem of
vanishing gradients for RNNs. These included non-gradient based training
algorithms, such as simulated annealing and discrete error propagation , explicitly introduced time delays or time constants , and hierarchical
sequence compression . However, the most eﬀective solution so far is the Long Short Term Memory (LSTM) architecture .
This chapter reviews the background material for LSTM, which will be
the main RNN architecture used in this thesis. Section 4.1 describes the
basic structure of LSTM and explains how it overcomes the vanishing gradient problem. Section 4.3 discusses an approximate and an exact algorithm
for calculating the LSTM error gradient.
Section 4.4 describes some enhancements to the basic LSTM architecture. Section 4.2 discusses the eﬀect
of preprocessing on long range dependencies. Section 4.5 provides all the
equations required to train and activate LSTM networks.
CHAPTER 4. LONG SHORT-TERM MEMORY
Figure 4.1: Vanishing gradient problem for RNNs. The shading of the
nodes indicates the sensitivity over time of the network nodes to the input at
time one (the darker the shade, the greater the sensitivity). The sensitivity
decays exponentially over time as new inputs overwrite the activation of
hidden unit and the network ‘forgets’ the ﬁrst input.
The LSTM Architecture
The LSTM architecture consists of a set of recurrently connected subnets,
known as memory blocks. These blocks can be thought of as a diﬀerentiable
version of the memory chips in a digital computer.
Each block contains
one or more self-connected memory cells and three multiplicative units —
the input, output and forget gates — that provide continuous analogues of
write, read and reset operations for the cells.
Figure 4.2 provides an illustration of an LSTM memory block with a
single cell. An LSTM network is formed exactly like a simple RNN, except
that the nonlinear units in the hidden layer are replaced by memory blocks.
Indeed, LSTM blocks may be mixed with simple units if required — although
this is typically not necessary. Also, as with other RNNs, the hidden layer
can be attached to any type of diﬀerentiable output layer, depending on the
required task (regression, classiﬁcation etc.).
The multiplicative gates allow LSTM memory cells to store and access
information over long periods of time, thereby avoiding the vanishing gradient problem. For example, as long as the input gate remains closed (i.e.
has an activation close to 0), the activation of the cell will not be overwritten by the new inputs arriving in the network, and can therefore be made
available to the net much later in the sequence, by opening the output gate.
The preservation over time of gradient information by LSTM is illustrated
in Figure 4.3.
Over the past decade, LSTM has proved successful at a range of synthetic tasks requiring long range memory, including learning context free
languages , recalling high precision real num-
CHAPTER 4. LONG SHORT-TERM MEMORY
NET OUTPUT
FORGET GATE
INPUT GATE
OUTPUT GATE
Figure 4.2: LSTM memory block with one cell. The internal state of
the cell is maintained with a recurrent connection of ﬁxed weight 1.0. The
three gates collect activations from inside and outside the block, and control
the cell via multiplicative units (small circles). The input and output gates
scale the input and output of the cell while the forget gate scales the internal
state. The cell input and output activation functions (g and h) are applied
at the indicated places.
bers over extended noisy sequences and
various tasks requiring precise timing and counting . In
particular, it has solved several artiﬁcial problems that remain impossible
with any other RNN architecture.
Additionally, LSTM has been applied to various real-world problems,
such as protein secondary structure prediction , music generation , reinforcement learning and speech recognition and handwriting recognition . As would be expected, its advantages
are most pronounced for problems requiring the use of long range contextual
information.
CHAPTER 4. LONG SHORT-TERM MEMORY
Figure 4.3: Preservation of gradient information by LSTM. As in
Figure 4.1 the shading of the nodes indicates their sensitivity to the input
unit at time one. The state of the input, forget, and output gate states
are displayed below, to the left and above the hidden layer node, which
corresponds to a single memory cell. For simplicity, the gates are either
entirely open (‘O’) or closed (‘—’). The memory cell ‘remembers’ the ﬁrst
input as long as the forget gate is open and the input gate is closed, and
the sensitivity of the output layer can be switched on and oﬀby the output
gate without aﬀecting the cell.
Inﬂuence of Preprocessing
Th above discussion raises an important point about the inﬂuence of preprocessing. If we can ﬁnd a way to transform a task containing long range
contextual dependencies into one containing only short-range dependencies
before presenting it to a sequence learning algorithm, then architectures such
as LSTM become somewhat redundant. For example, a raw speech signal
typically consists of 16000 amplitude values per second. Clearly, a great
many timesteps would have to be spanned by a sequence learner attempting
to label or model an utterance presented in this form. However, by carrying
out a series of transforms, averages, rescalings and decorrelations, it is possible to turn such a sequence into one that can be reasonably with modelled
by decorrelated, localised function approximators such as the mixtures of
diagonal Gaussians used for standard HMMs.
Nonetheless, if such a transform is diﬃcult or unknown, or if we simply
wish to get a good result without having to design task-speciﬁc preprocessing
methods, algorithms capable of learning long time dependencies are useful.
In Chapter 7 we evaluate LSTM on an online handwriting task using both
raw data and data carefully preprocessed to reduce the need for long range
context. We ﬁnd that LSTM is only slightly more accurate with the hand
crafted data.
CHAPTER 4. LONG SHORT-TERM MEMORY
Gradient Calculation
Like the ANNs discussed in the last chapter, LSTM is a diﬀerentiable function approximator that is typically trained with gradient descent. used an approximate error gradient calculated with a combination of
Real Time Recurrent Learning and
Backpropagation Through Time . The
BPTT part was truncated after one timestep, because it was felt that long
time dependencies would be dealt with by the memory blocks, and not by the
(vanishing) ﬂow of activation around the recurrent connections. Truncating
the gradient has the beneﬁt of making the algorithm completely online,
in the sense that weight updates can be made after every timestep. This
is an important property for tasks such as continuous control or time-series
prediction. Additionally, it could only be proven with the truncated gradient
that the error ﬂow through the memory cells was constant .
However, it is also possible to calculate the exact LSTM gradient with
BPTT . As well as being more accurate
than the truncated gradient, the exact gradient has the advantage of being
easier to debug, since it can be checked numerically using the technique
described in Section 3.1.4. Only the exact gradient is used in this thesis,
and the equations for it are provided in Section 4.5.
Architectural Enhancements
In its original form, LSTM contained only input and output gates. The
forget gates , along with additional peephole weights connecting the gates to the memory cell were added later. The
purpose of the forget gates was to provide a way for the memory cells to reset
themselves, which proved important for tasks that required the network to
‘forget’ previous inputs. The peephole connections, meanwhile, improved
the LSTM’s ability to learn tasks that require precise timing and counting
of the internal states.
For the purposes of this thesis, we consider only the extended form of
LSTM with the forget gates and peepholes added.
Since LSTM is entirely composed of simple multiplication and summation units, and connections between them, it would be straightforward to
create further variations of the block architecture.
However the current
setup seems to be a good general purpose structure for sequence learning
CHAPTER 4. LONG SHORT-TERM MEMORY
A further enhancement introduced in this thesis is bidirectional LSTM
 , which consists of the standard bidirectional RNN architecture with LSTM used in the hidden layers.
BLSTM provides access to long range context in both input directions and,
as we will see, consistently outperforms other neural network architectures
on sequence labelling tasks. It has proved especially popular in the ﬁeld of
bioinformatics .
LSTM Equations
This section provides the equations for the activation (forward pass) and
gradient calculation (backward pass) of an LSTM hidden layer within a recurrent neural network. Only the exact error gradient , calculated with backpropagation through time, is presented
here. The approximate error gradient is not used in this thesis.
As before, wij is the weight of the connection from unit i to unit j, the
network input to some unit j at time t is denoted at
j and the value of the
same unit after the activation function has been applied is bt
j. The LSTM
equations are given for a single memory block only. For multiple blocks the
calculations are simply repeated for each block, in any order. The subscripts
ι, φ and ω refer to the input gate, forget gate and output gate respectively.
The subscripts c refers to one of the C memory cells. st
c is the state of cell
c at time t (i.e. the activation of the linear cell unit). f is the activation
function of the gates, and g and h are respectively the cell input and output
activation functions.
Let I be the number of inputs, K be the number of outputs and H be
the number of cells in the hidden layer. Note that only the cell outputs bt
are connected to the other blocks in the layer. The other LSTM activations,
such as the states, the cell inputs, or the gate activations, are only visible
within the block. We use the index h to refer to cell outputs from other
blocks in the hidden layer, exactly as for standard hidden units (indeed
normal units and LSTM blocks could be mixed in the same hidden layer, if
desired). As with standard RNNs the forward pass is calculated for a length
T input sequence x by starting at t = 1 and recursively applying the update
equations while incrementing t, and the BPTT backward pass is calculated
by starting at t = T, and recursively calculating the unit derivatives while
decrementing t (see Section 3.2 for details). The ﬁnal weight derivatives are
found by summing over the derivatives at each timestep, as expressed in
Eqn. (3.34). Recall that
where O is the objective function used for training.
CHAPTER 4. LONG SHORT-TERM MEMORY
The order in which the equations are calculated during the forward and
backward passes is important, and should proceed as speciﬁed below. As
with standard RNNs, all states and activations are set to zero at t = 0, and
all δ terms are zero at t = T + 1.
Forward Pass
Input Gates
Forget Gates
Output Gates
Cell Outputs
CHAPTER 4. LONG SHORT-TERM MEMORY
Backward Pass
Cell Outputs
Output Gates
Forget Gates
Input Gates
Framewise Phoneme
Classiﬁcation
This chapter presents an experimental comparison between various neural
network architectures on a framewise phoneme classiﬁcation task . Framewise phoneme classiﬁcation is an example
of a segment classiﬁcation task (see Section 2.3.3). It tests an algorithm’s
ability to segment and recognise the constituent parts of a speech signal, and
requires the use of contextual information. However, phoneme classiﬁcation
alone is not suﬃcient for continuous speech recognition. As such, the work
in this chapter should be regarded as a preliminary study for the work on
temporal classiﬁcation with RNNs presented in Chapter 7.
Context is of particular importance in speech recognition due to phenomena such as co-articulation, where the human articulatory system blurs
together adjacent sounds in order to produce them rapidly and smoothly. In
many cases it is diﬃcult to identify a particular phoneme without knowing
the phonemes that occur before and after it. The main result of this chapter
is that network architectures capable of accessing more context give better
performance in phoneme classiﬁcation, and are therefore more suitable for
speech recognition.
Section 5.1 describes the experimental data and task. Section 5.2 gives
an overview of the various neural network architectures and Section 5.3
describes how they are trained, while Section 5.4 presents the experimental
Experimental Setup
The data for the experiments came from the TIMIT corpus of prompted speech, collected by Texas Instruments. The utterances
in TIMIT were chosen to be phonetically rich, and the speakers represent a
wide variety of American dialects. The audio data is divided into sentences,
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
each of which is accompanied by a phonetic transcript.
The task was to classify every timestep, or frame in audio parlance,
according to the phoneme it belonged to. For consistency with the literature,
we used the complete set of 61 phonemes provided in the transcriptions. In
continuous speech recognition, it is common practice to use a reduced set of
phonemes , by merging those with similar sounds, and not
separating closures from stops.
The standard TIMIT corpus comes partitioned into training and test
sets, containing 3,696 and 1,344 utterances respectively. In total there were
1,124,823 frames in the training set, and 410,920 in the test set. No speakers
or sentences exist in both the training and test sets. We used 184 of the
training set utterances (chosen randomly, but kept constant for all experiments) as a validation set and trained on the rest. All results for the training
and test sets were recorded at the point of lowest error on the validation set.
The following preprocessing, which is standard in speech recognition was used for the audio data. The data was characterised as a sequence of vectors of 26 coeﬃcients, consisting of twelve Melfrequency cepstral coeﬃcients (MFCC) plus energy and ﬁrst derivatives of
these magnitudes. First the coeﬃcients were computed every 10ms over 25
ms-long windows. Then a Hamming window was applied, a Mel-frequency
ﬁlter bank of 26 channels was computed and, ﬁnally, the MFCC coeﬃcients
were calculated with a 0.97 pre-emphasis coeﬃcient.
Network Architectures
We used the following ﬁve neural network architectures in our experiments
(henceforth referred to by the abbreviations in brackets):
• Bidirectional LSTM, with two hidden LSTM layers (forwards and
backwards), both containing 93 memory blocks of one cell each (BLSTM)
• Unidirectional LSTM, with one hidden LSTM layer, containing 140
one-cell memory blocks, trained backwards with no target delay, and
forwards with delays from 0 to 10 frames (LSTM)
• Bidirectional RNN with two hidden layers containing 185 sigmoid units
each (BRNN)
• Unidirectional RNN with one hidden layer containing 275 sigmoid
units, trained with target delays from 0 to 10 frames (RNN)
• MLP with one hidden layer containing 250 sigmoid units, and symmetrical time-windows from 0 to 10 frames (MLP)
The hidden layer sizes were chosen to ensure that all networks had
roughly the same number of weights W (≈100, 000), thereby providing
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
a fair comparison. Note however that for the MLPs the network grew with
the time-window size, and W ranged from 22,061 to 152,061. All networks
contained an input layer of size 26 (one for each MFCC coeﬃcient), and an
output layer of size 61 (one for each phoneme). The input layers were fully
connected to the hidden layers and the hidden layers were fully connected
to the output layers. For the recurrent networks, the hidden layers were also
fully connected to themselves. The LSTM blocks had the following activation functions: logistic sigmoids in the range [−2, 2] for the input and output
activation functions of the cell (g and h in Figure 4.2), and in the range 
for the gates. The non-LSTM networks had logistic sigmoid activations in
the range in the hidden layers. All units were biased.
Figure 5.1 illustrates the behaviour of the diﬀerent architectures during
classiﬁcation.
Computational Complexity
For all networks, the computational complexity was dominated by the O(W)
feedforward and feedback operations.
This means that the bidirectional
networks and the LSTM networks did not take signiﬁcantly more time per
training epoch than the unidirectional or RNN or (equivalently sized) MLP
Range of Context
Only the bidirectional networks had access to the complete context of the
frame being classiﬁed (i.e. the whole input sequence). For MLPs, the amount
of context depended on the size of the time-window. The results for the
MLP with no time-window (presented only with the current frame) give
a baseline for performance without context information.
However, some
context is implicitly present in the window averaging and ﬁrst-derivatives
included in the preprocessor.
Similarly, for unidirectional LSTM and RNN, the amount of future context depended on the size of target delay. The results with no target delay
(trained forwards or backwards) give a baseline for performance with context
in one direction only.
Output Layers
For the output layers, we used the cross entropy error function and the
softmax activation function, as discussed in Sections 3.1.2 and 3.1.3. The
softmax function ensures that the network outputs are all between zero and
one, and that they sum to one on every timestep. This means they can be
interpreted as the posterior probabilities of the phonemes at a given frame,
given all the inputs up to the current one (with unidirectional networks) or
all the inputs in the whole sequence (with bidirectional networks).
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
MLP 10 Frame Time-Window
BLSTM Duration Weighted Error
Figure 5.1: Various networks classifying the excerpt “at a window”
from TIMIT. In general, the networks found the vowels more diﬃcult
than the consonants, which in English are more distinct. Adding duration
weighted error to BLSTM tends to give better results on short phonemes,
(e.g. the closure and stop ‘dcl’ and ‘d’), and worse results on longer ones
(‘ow’), as expected. Note the jagged trajectories for the MLP; this is presumably because it lacks recurrency to smooth the outputs.
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
Several alternative error functions have been studied for this task .
One modiﬁcation in particular has been shown to
have a positive eﬀect on continuous speech recognition. This is to weight
the error according to the duration of the current phoneme, ensuring that
short phonemes are as signiﬁcant to training as longer ones. We will return
to the issue of weighted errors in the next two chapters.
Network Training
For all architectures, we calculated the full error gradient using BPTT for
each utterance, and trained the weights using online steepest descent with
We kept the same training parameters for all experiments:
initial weights chosen from a ﬂat random distribution with range [−0.1, 0.1],
a learning rate of 10−5 and a momentum of 0.9. As usual, weight updates
were carried out at the end of each sequence, and the order of the training
set was randomised at the start of each training epoch.
Keeping the training algorithm and parameters constant allowed us to
concentrate on the eﬀect of varying the architecture. However it is possible
that diﬀerent training methods would be better suited to diﬀerent networks.
Retraining
For the experiments with varied time-windows or target delays, we iteratively retrained the networks, instead of starting again from scratch. For
example, for LSTM with a target delay of 2, we ﬁrst trained with delay 0,
then took the best network and retrained it (without resetting the weights)
with delay 1, then retrained again with delay 2. To ﬁnd the best networks,
we retrained the LSTM networks for 5 epochs at each iteration, the RNN
networks for 10, and the MLPs for 20. It is possible that longer retraining
times would have given improved results. For the retrained MLPs, we had
to add extra (randomised) weights from the input layers, since the input
size grew with the time-window.
Although primarily a means to reduce training time, we have also found
that retraining improves ﬁnal performance . Indeed, the best result in this paper was achieved by retraining (on
the BLSTM network trained with a weighted error function, then retrained
with normal cross-entropy error). The beneﬁts presumably come from escaping the local minima that gradient descent algorithms tend to get caught
The ability of neural networks to beneﬁt from this kind of retraining
touches on the more general issue of transferring knowledge between diﬀerent tasks (usually known as meta-learning, learning to learn, or inductive
transfer) which has been widely studied in the neural network and general
machine learning literature .
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
Table 5.1: Phoneme classiﬁcation error rate on TIMIT. BLSTM result
is a mean over seven runs ± std. err.
Training Set
MLP (no time-window)
MLP (10 frame time-window)
RNN (0 frame delay)
LSTM (0 frame delay)
LSTM (backwards, 0 frame delay)
RNN (3 frame delay)
LSTM (5 frame delay)
BLSTM (Weighted Error)
BLSTM (retrained)
Table 5.1 summarises the performance of the diﬀerent network architectures.
For the MLP, RNN and LSTM networks we give both the best results, and
those achieved with least contextual information (i.e. with no target delay
or time-window). The complete set of results is presented in Figure 5.2.
The most obvious diﬀerence between LSTM and the RNN and MLP
networks was the number of epochs required for training, as shown in Figure 5.3. In particular, BRNN took more than 8 times as long to converge
as BLSTM, despite having more or less equal computational complexity per
timestep (see Section 5.2.1). There was a similar time increase between the
unidirectional LSTM and RNN networks, and the MLPs were slower still
(990 epochs for the best MLP result). A possible explanation for this is that
the MLPs and RNNs require more ﬁne-tuning of their weights to access long
range contextual information.
As well as being faster, the LSTM networks were also slightly more
accurate. However, the ﬁnal diﬀerence in score between BLSTM and BRNN
on this task is quite small (0.8%). The fact that the diﬀerence is not larger
could mean that very long time dependencies are not required for this task.
It is interesting to note how much more prone to overﬁtting LSTM was
than standard RNNs. For LSTM, after only 15-20 epochs the performance
on the validation and test sets would begin to fall, while that on the training
set would continue to rise (the highest score we recorded on the training set
with BLSTM was 86.4%). With the RNNs on the other hand, we never
observed a large drop in test set score. This suggests a diﬀerence in the way
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
% Frames Correctly Classified
Target Delay / Window Size
Framewise Phoneme Classification Scores
BLSTM Retrained
BLSTM Weighted Error
Figure 5.2: Framewise phoneme classiﬁcation results on the TIMIT
test set. The number of frames of introduced context (time-window size for
MLPs, target delay size for unidirectional LSTM and RNNs) is plotted along
the x axis. Therefore the results for the bidirectional networks (clustered
around 70%) are plotted at x=0.
the two architectures learn. Given that in the TIMIT corpus no speakers
or sentences are shared by the training and test sets, it is possible that
LSTM’s overﬁtting was partly caused by its better adaptation to long range
regularities (such as phoneme ordering within words, or speaker speciﬁc
pronunciations) than normal RNNs. If this is true, we would expect a greater
distinction between the two architectures on tasks with more training data.
Comparison with Previous Work
Table 5.2 shows how BLSTM compares with the best neural network results previously recorded for this task. Note that Robinson did not quote
framewise classiﬁcation scores; the result for his network was recorded by
Schuster, using the original software.
Overall BLSTM outperformed all networks found in the literature, apart
from one result quoted by Chen and Jamieson for an RNN . However this result is questionable as Chen and Jamieson
quote a substantially lower error rate on the test set than on the training
set (25.8% and 30.1% respectively). Moreover we were unable to reproduce,
or even approach, their scores in our own experiments. For these reasons we
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
% Frames Correctly Classified
Training Epochs
Learning Curves for Three Architectures
BLSTM training set
BLSTM test set
BRNN training set
BRNN test set
MLP training set
MLP test set
Figure 5.3: Learning curves on TIMIT for BLSTM, BRNN and
MLP with no time-window. For all experiments, LSTM was much faster
to converge than either the RNN or MLP architectures.
have not included their result in the table.
In general, it is diﬃcult to compare network architectures to previous
work on TIMIT, owing to the many variations in network training (diﬀerent
gradient descent algorithms, error functions etc.) and in the task itself (different training and test sets, diﬀerent numbers of phoneme labels, diﬀerent
input preprocessing etc.). That is why we reimplemented all the architectures ourselves.
Eﬀect of Increased Context
As is clear from Figure 5.2 networks with access to more contextual information tended to get better results. In particular, the bidirectional networks
were substantially better than the unidirectional ones. For the unidirectional
networks, LSTM beneﬁted more from longer target delays than RNNs; this
could be due to LSTM’s greater facility with long time-lags, allowing it to
make use of the extra context without suﬀering as much from having to
remember previous inputs.
Interestingly, LSTM with no time delay returns almost identical results
whether trained forwards or backwards. This suggests that the context in
both directions is equally important. Figure 5.4 shows how the forward and
backward layers work together during classiﬁcation.
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
Table 5.2: Comparison of BLSTM with previous neural network
Training Set
BRNN 
RNN 
BLSTM (retrained)
For the MLPs, performance increased with time-window size, and it
appears that even larger windows would have been desirable. However, with
fully connected networks, the number of weights required for such large input
layers makes training prohibitively slow.
Weighted Error
The experiment with a weighted error function gave slightly inferior framewise performance for BLSTM (68.9%, compared to 69.7%). However, the
purpose of this weighting is to improve overall phoneme recognition, rather
than framewise classiﬁcation.
As a measure of its success, if we assume
a perfect knowledge of the test set segmentation (which in real-life situations we cannot), and integrate the network outputs over each phoneme,
then BLSTM with weighted errors gives a phoneme correctness of 74.4%,
compared to 71.2% with normal errors.
CHAPTER 5. FRAMEWISE PHONEME CLASSIFICATION
Reverse Net Only
Forward Net Only
Bidirectional Output
Figure 5.4: BLSTM network classifying the utterance “one oh ﬁve”.
The bidirectional output combines the predictions of the forward and backward hidden layers; it closely matches the target, indicating accurate classiﬁcation. To see how the layers work together, their contributions to the
output are plotted separately. As we might expect, the forward layer is more
accurate. However there are places where its substitutions (‘w’), insertions
(at the start of ‘ow’) and deletions (‘f’) are corrected by the reverse layer.
In addition, both are needed to accurately locate phoneme boundaries, with
the reverse layer tending to ﬁnd the starts and the forward layer tending to
ﬁnd the ends (e.g. ‘ay’).
Hidden Markov Model
In this chapter LSTM is combined with hidden Markov models to form a
hybrid sequence labelling system . HMM-ANN hybrids
have been extensively studied in the literature, usually with MLPs as the
neural network component. The basic idea is to use HMMs to model the
long range sequential structure of the data, and neural networks to provide
localised classiﬁcations. The HMM is able to automatically segment the input sequences during training, and it also provides a principled method for
transforming network classiﬁcations into label sequences. Unlike the networks described in previous chapters, HMM-ANN hybrids can therefore be
directly applied to temporal classiﬁcation tasks (see Section 2.3.4), such as
speech recognition. However, the use of hidden Markov models introduces
unnecessary assumptions about the data, and fails to exploit the full potential of RNNs for modelling sequential data. A more powerful technique for
temporal classiﬁcation with RNNs is introduced in the next chapter.
We evaluate the performance of a HMM-BLSTM hybrid on a phoneme
recognition experiment, and ﬁnd that it outperforms both a standard HMM
and a hybrid using unidirectional LSTM. This suggests that the advantages
of using network architectures with increased access to context carry over
to temporal classiﬁcation.
Section 6.1 reviews the previous work on hybrid HMM-ANN systems.
Section 6.2 presents experimental results on a phoneme recognition task.
Background
Hybrids of hidden Markov models (HMMs) and artiﬁcial neural networks
(ANNs) were proposed by several researchers in the 1990s as a way of overcoming the drawbacks of HMMs .
The introduction of
CHAPTER 6. HIDDEN MARKOV MODEL HYBRIDS
ANNs was intended to provide more discriminant training, improved modelling of phoneme duration, richer, nonlinear function approximation, and
perhaps most importantly, increased use of contextual information.
In their simplest form, hybrid methods used HMMs to align the segment classiﬁcations provided by the ANNs into a temporal classiﬁcation of
the entire label sequence . In other
cases ANNs were used to estimate transition or emission probabilities for
HMMs , to re-score the N-best HMM labellings
according to localised classiﬁcations , and to extract
observation features that can be more easily modelled by an HMM . In this chapter we focus on the simplest case, since
there is no compelling evidence that the more complex systems give better
performance.
Although most hybrid HMM-ANN research focused on speech recognition, the framework is equally applicable to other sequence labelling tasks,
such as online handwriting recognition .
The two components in a the hybrid can be trained independently, but
many authors have proposed methods for combined optimisation which typically yields improved results. In this chapter we follow an
iterative approach, where the alignment provided by the HMM is used to
successively retrain the neural network .
A similar, but more general, framework for combining neural networks
with other sequential algorithms is provided by graph transformer networks
 . The diﬀerent modules
of a graph transformer network perform distinct tasks, such as segmentation, recognition and imposing grammatical constaints. The modules are
connected by transducers, which provide diﬀerentiable sequence to sequence
maps, and allow for global, gradient based learning.
Most hybrid HMM-ANN systems use multilayer perceptrons, typically
with a time-window to provide context, for the neural network component.
However there has also been considerable interest in the use of RNNs . Given that the main purpose of the ANN is to introduce contextual
information, RNNs seem a natural choice. However, their advantages over
MLPs remained inconclusive in early work .
Despite a ﬂurry of initial interest, hybrid HMM-ANN approaches never
achieved a great improvement over conventional HMMs, especially considering the added complexity of designing and training them. As a consequence,
most current speech and handwriting recognition systems are based on conventional HMMs. We believe this is in part due to the limitations of the
standard neural network architectures. In what follows we evaluate the in-
ﬂuence on hybrid performance of using LSTM and bidirectional LSTM as
the network architectures.
CHAPTER 6. HIDDEN MARKOV MODEL HYBRIDS
Experiment: Phoneme Recognition
The experiment was carried out on the TIMIT speech corpus .
The data preparation and division into training, test and
validation sets was identical to that described in Section 5.1. However the
task was diﬀerent. Instead of classifying phonemes at every frame, the goal
was to output the complete phonetic transcription of the input sequences.
Correspondingly, the error measure was the phoneme error rate (label error
rate with phonemes as labels — see Section 2.3.4) instead of the frame error
We evaluate the performance of standard HMMs with and without context dependent phoneme models, and hybrid systems using BLSTM, LSTM
and BRNNs.
We also evaluate the eﬀect of using a weighted error signal , as described in Section 5.2.3.
Experimental Setup
Traditional HMMs were developed with the HTK Speech Recognition Toolkit
( Both context independent (mono-phone) and
context dependent (triphone) models were trained and tested. Both were
left-to-right models with three states. Models representing silence (h#, pau,
epi) included two extra transitions, from the ﬁrst to the ﬁnal state and viceversa, to make them more robust. Observation probabilities were modelled
by eight Gaussian mixtures.
Sixty-one context-independent models and 5491 tied context-dependent
models were used. Context-dependent models for which the left/right context coincide with the central phoneme were included since they appear in
the TIMIT transcription (e.g. “my eyes” is transcribed as /m ay ay z/). During recognition, only sequences of context-dependent models with matching
context were allowed.
In order to make a fair comparison of the acoustic modelling capabilities
of the traditional and hybrid systems, no linguistic information or probabilities of partial phoneme sequences were included in the system.
For the hybrid systems, the following networks were used: unidirectional
LSTM, BLSTM, and BLSTM trained with weighted error. 61 models of
one state each with a self-transition and an exit transition probability were
trained using Viterbi-based forced-alignment. The training set transcription
was used to provide initial estimates of transition and prior probabilities.
The training set transcription was also used to initially train the networks.
In fact the networks trained for the framewise classiﬁcation experiments
in Chapter 5 were simply re-used; therefore the network architectures and
parameters were identical to those described in Sections 5.2 and 5.3. The
HMM was trained using the network output probabilities divided by prior
probabilities to obtain observation likelihoods. The alignment provided by
CHAPTER 6. HIDDEN MARKOV MODEL HYBRIDS
Table 6.1: Phoneme error rate (PER) on TIMIT. Hybrid results are
means over 5 runs, ± standard error. All diﬀerences were signiﬁcant (p <
Parameters
Context-independent HMM
Context-dependent HMM
39.6 ± 0.08%
33.84 ± 0.06%
HMM-BLSTM (weighted error)
31.57 ± 0.06%
the trained HMM was then used to deﬁne a new framewise training signal for
the neural networks, and the whole process was repeated until convergence.
For both the traditional and hybrid system, an insertion penalty was
estimated on the validation set and applied during recognition.
From Table 6.1, we can see that HMM-BLSTM hybrids outperformed both
context-dependent and context-independent HMMs. We can also see that
BLSTM gave better performance than unidirectional LSTM, in agreement
with the results in Chapter 5. The best result was achieved with the HMM-
BLSTM hybrid using a weighted error signal. This is what we would expect,
since the eﬀect of error weighting is to make all phonemes equally signiﬁcant,
as they are to the phoneme error rate.
Note that the hybrid systems had considerably fewer free parameters
than the context-dependent HMM. This is a consequence of the high number
of states required for HMMs to model contextual dependencies.
Note also that the networks in the hybrid systems were initially trained
with hand segmented training data. Although the experiments could have
been carried out with a ﬂat segmentation, this would have probably led to
inferior results.
Connectionist Temporal
Classiﬁcation
This chapter introduces connectionist temporal classiﬁcation , a novel output layer for temporal classiﬁcation with RNNs. Unlike the approach described in the previous chapter, CTC models all aspects
of the sequence with a single RNN, and does not require the network to
be combined with hidden Markov models.
It also does not require presegmented training data, or external post-processing to extract the label
sequence from the network outputs. We carry out several experiments on
speech and handwriting recognition, and ﬁnd that CTC networks with the
BLSTM architecture outperform HMMs and HMM-RNN hybrids, as well as
more recent sequence learning algorithms such as large margin HMMs . For a widely studied phoneme recognition benchmark on
the TIMIT speech corpus, CTC’s performance is on a par with the best
previous systems, even though these were speciﬁcally tuned for the task.
Section 7.1 introduces CTC and motivates its use for temporal classiﬁcation tasks. Section 7.2 deﬁnes the mapping from CTC outputs onto label
sequences, Section 7.3 provides an algorithm for eﬃciently calculating the
probability of a given label sequence, and Section 7.4 combines these to
derive the CTC objective function. Section 7.5 describes methods for decoding with CTC. Experimental results are presented in Section 7.6, and a
discussion of the diﬀerences between CTC networks and HMMs is given in
Section 7.7.
Motivation
In 1994, Bourlard and Morgan identiﬁed the following reason for the failure of purely connectionist approaches
to continuous speech recognition:
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
There is at least one fundamental diﬃculty with supervised training of a connectionist network for continuous speech recognition:
a target function must be deﬁned, even though the training is
done for connected speech units where the segmentation is generally unknown.
In other words, neural networks, at least with the standard objective
functions, require separate training targets for every segment or timestep
in the input sequence. This has two important consequences. Firstly, it
means that the training data must be presegmented to provide the targets.
Secondly, since the network only outputs local classiﬁcations, the global
aspects of the sequence (e.g. the likelihood of two labels appearing together)
must be modelled externally. Indeed, without some form of post-processing
the ﬁnal label sequence cannot reliably be inferred at all.
In Chapter 6 we showed how RNNs could be used for temporal classi-
ﬁcation by combining them with HMMs in hybrid systems. However, as
well as inheriting the disadvantages of HMMs (non-discriminative training,
unrealistic dependency assumptions etc. — see Section 7.7 for an in-depth
discussion), hybrid systems do not exploit the potential of RNNs for global
sequence modelling. It therefore seems preferable to train RNNs directly for
temporal classiﬁcation tasks.
Connectionist temporal classiﬁcation (CTC) achieves this by allowing
the network to make label predictions at any point in the input sequence,
so long as the overall sequence of labels is correct. This removes the need
for presegmented data, since the alignment of the labels with the input is
no longer important. Moreover, CTC directly estimates the probabilities of
the complete label sequences, which means that no external post-processing
is required to use the network as a temporal classiﬁer.
Figure 7.1 illustrates the diﬀerence between CTC and framewise classi-
ﬁcation, when applied to a speech signal.
From Outputs to Labellings
For a sequence labelling task where the labels are drawn from an alphabet
L, CTC consists of a softmax output layer with one more unit
than there are labels in L. The activations of the ﬁrst |L| units are used to
estimate the probabilities of observing the corresponding labels at particular
times, conditioned on the training set and the current input sequence. The
activation of the extra unit estimates the probability of observing a ‘blank’,
or no label. Together, the outputs give the joint conditional probability of all
labels at all timesteps. The conditional probability of any one label sequence
can then be found by summing over the corresponding joint probabilities.
More formally, the activation yt
k of output unit k at time t is interpreted
as the probability of observing label k (or blank if k = |L| + 1) at time t,
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
label probability
Figure 7.1: CTC and framewise classiﬁcation networks applied to a
speech signal. The shaded lines are the output activations, corresponding
to the probabilities of observing phonemes at particular times. The CTC
network predicts only the sequence of phonemes (typically as a series of
spikes, separated by ‘blanks’, or null predictions), while the framewise network attempts to align them with the manual segmentation (vertical lines).
given the length T input sequence x and the training set S. Together, these
probabilities estimate the distribution over the elements π ∈L′T (where L′T
is the set of length T sequences over the alphabet L′ = L ∪{blank})
p(π|x, S) =
Implicit in the above formulation is the assumption that the probabilities
of the labels at each timestep are conditionally independent, given x and S.
Provided S is large enough this is in principle true, since the likelihood of
all label sequences and subsequences can be learned directly from the data.
However it can be advantageous to condition on inter-label probabilities as
well, especially for tasks involving labels with known or partially known
relationships (such as words in human language). We discuss this matter
further in Section 7.5.3, where we introduce a CTC decoding algorithm that
includes explicit word-word transition probabilities.
CTC can be used with any RNN architecture. However, because the
label probabilities are conditioned on the entire input sequence, and not
just on the part occurring before the label is emitted, bidirectional RNNs
are preferred.
From now on, we refer to the elements π ∈L′T as paths, and we drop the
explicit dependency of the output probabilities on S to simplify notation.
The next step is to deﬁne a many-to-one map B : L′T 7→L≤T , from the
set of paths onto the set L≤T of possible labellings (i.e. the set of sequences
of length less than or equal to T over the original label alphabet L). We
do this by removing ﬁrst the repeated labels and then the blanks from the
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
paths. For example B(a −ab−) = B(−aa −−abb) = aab. Intuitively, this
corresponds to outputting a new label when the network either switches
from predicting no label to predicting a label, or from predicting one label to
another. Since the paths are mutually exclusive, the conditional probability
of some labelling l ∈L≤T can be calculated by summing the probabilities
of all the paths mapped onto it by B:
This ‘collapsing together’ of diﬀerent paths onto the same labelling is what
allows CTC to use unsegmented data, because it removes the requirement
of knowing where in the input sequence the labels occur. In theory, it also
makes CTC unsuitable for tasks where the location of the labels must be
determined. However in practice CTC networks tend to output labels close
to where they occur in the input sequence. In Section 7.6.3 an experiment
is presented in which both the labels and their approximate positions are
successfully predicted by CTC.
Role of the Blank Labels
In our original formulation of CTC, there were no blank labels, and B(π) was
simply π with repeated labels removed. This led to two problems. Firstly,
the same label could not appear twice in a row, since transitions only occurred when π passed between diﬀerent labels. And secondly, the network
was required to continue predicting one label until the next began, which
is a burden in tasks where the input segments corresponding to consecutive labels are widely separated by unlabelled data (e.g. speech recognition,
where there are often pauses or non-speech noises between the words in an
utterance).
CTC Forward-Backward Algorithm
So far we have deﬁned the conditional probabilities p(l|x) of the possible
label sequences. Now we need an eﬃcient way of calculating them. At ﬁrst
sight Eqn. (7.2) suggests this will be problematic: the sum is over all paths
corresponding to a given labelling, and for a labelling of length s and an
input sequence of length T, there are 2T−s2+s(T−3)3(s−1)(T−s)−2 of these.
Fortunately the problem can be solved with a dynamic-programming
algorithm similar to the forward-backward algorithm for HMMs . The key idea is that the sum over paths corresponding to a labelling
l can be broken down into an iterative sum over paths corresponding to
preﬁxes of that labelling.
To allow for blanks in the output paths, we consider a modiﬁed label
sequence l′, with blanks added to the beginning and the end of l, and inserted
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
between every pair of consecutive labels. The length of l′ is therefore 2|l| +
1. In calculating the probabilities of preﬁxes of l′ we allow all transitions
between blank and non-blank labels, and also those between any pair of
distinct non-blank labels.
For a labelling l, we deﬁne the forward variable αt(s) as the summed
probability of all paths whose length t preﬁxes are mapped by B onto the
length s/2 preﬁx of l, i.e.
αt(s) = P(π1:t : B(π1:t) = l1:s/2, πt = l′
B(π1:t)=l1:s/2
where, for some sequence s, sa:b is the subsequence (sa, sa+1, ..., sb−1, sb),
and s/2 is rounded down to an integer value. As we will see, αt(s) can be
calculated recursively from αt−1(s), αt−1(s −1) and αt−1(s −2).
Given the above formulation, the probability of l can be expressed as
the sum of the forward variables with and without the ﬁnal blank at time T
p(l|x) = αT (|l′|) + αT (|l′| −1)
Allowing all paths to start with either a blank (b) or the ﬁrst symbol in l
(l1), we get the following rules for initialisation
α1(1) = y1
α1(2) = y1
α1(s) = 0, ∀s > 2
and recursion
αt(s) = yt
i=s−1 αt−1(i)
s = b or l′
i=s−2 αt−1(i)
otherwise,
αt(s) = 0 ∀s < |l′| −2(T −t) −1,
because these variables correspond to states for which there are not enough
timesteps left to complete the sequence (the unconnected circles in the top
right of Figure 7.2), and
αt(0) = 0 ∀t
The backward variables βt(s) are deﬁned as the summed probability of
all paths whose suﬃxes starting at t map onto the suﬃx of l starting at label
βt(s) = P(πt+1:T : B(πt:T ) = ls/2:|l|, πt = l′
B(πt:T )=ls/2:|l|
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.2: CTC forward-backward algorithm. Black circles represent
labels, and white circles represent blanks. Arrows signify allowed transitions.
Forward variables are updated in the direction of the arrows, and backward
variables are updated against them.
The rules for initialisation and recursion of the backward variables are as
βT (|l′|) = 1
βT (|l′| −1) = 1
βT (s) = 0, ∀s < |l′| −1
i=s βt+1(i)yt
s = b or l′
i=s βt+1(i)yt
βt(s) = 0 ∀s > 2t,
as shown by the unconnected circles in the bottom left of Figure 7.2, and
βt(|l′| + 1) = 0 ∀t
In practice, the above recursions will soon lead to underﬂows on any digital
computer. A good way to avoid this is to work in the log scale, and only
exponentiate to ﬁnd the true probabilities at the end of the calculation. A
useful equation in this context is
ln(a + b) = ln a + ln
1 + eln b−ln a
which allows the forward and backward variables to be summed while remaining in the log scale. Note that rescaling the variables at every timestep is less robust, and can fail for very long sequences.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
CTC Objective Function
So far we have described how an RNN with a CTC output layer can be used
for temporal classiﬁcation. In this section we derive an objective function
that allows CTC networks to be trained with gradient descent.
Like the standard neural network objective functions (see Section 3.1.3),
the CTC objective function is derived from the principle of maximum likelihood. Because the objective function is diﬀerentiable, its derivatives with
respect to the network weights can be calculated with backpropagation
through time (Section 3.2.2), and the network can then be trained with
any gradient-based nonlinear optimisation algorithm (Section 3.3.1).
As usual, The objective function O is deﬁned as the negative log probability of correctly labelling the entire training set:
To ﬁnd the gradient of (7.19), we ﬁrst diﬀerentiate with respect to the
network outputs yt
k during some training example (x, z)
= −∂ln p(z|x)
We now show how the algorithm of Section 7.3 can be used to calculate
The key point is that the product of the forward and backward variables
at a given s and t is the summed probability of all paths mapped onto z by
B that go through symbol s in z′ at time t. Setting l = z, (7.3) and (7.11)
αt(s)βt(s) =
Substituting from (7.1) we get
αt(s)βt(s) =
From (7.2) we can see that this is the portion of the total probability p(z|x)
due to those paths going through z′
s at time t, as claimed. For any t, we can
therefore sum over all s to get
αt(s)βt(s)
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
To diﬀerentiate this with respect to yt
k, we need only consider those paths
going through label k at time t, since the network outputs do not inﬂuence
each other. Noting that the same label (or blank) may occur several times
in a single labelling, we deﬁne the set of positions where label k occurs in z′
as lab(z, k) = {s : z′
s = k}, which may be empty. Observing from (7.21)
∂αt(s)βt(s)
(αt(s)βt(s)
if k occurs in z′
0 otherwise,
we can diﬀerentiate (7.23) to get
s∈lab(z,k)
αt(s)βt(s).
and substitute this into (7.20) to get
s∈lab(z,k)
αt(s)βt(s).
Finally, to backpropagate the gradient through the output layer, we need
the objective function derivatives with respect to the outputs at
k before the
activation function is applied
where k′ ranges over all the output units. Recalling that for softmax outputs
k′δkk′ −yt
we can substitute (7.28) and (7.26) into (7.27) to obtain
s∈lab(z,k)
αt(s)βt(s),
which is the ‘error signal’ received by the network during training, as illustrated in Figure 7.3. Note that, for numerical stability, it is advised to
recalculate the p(z|x) term for every t using (7.23).
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.3: Evolution of the CTC error signal during training. The
left column shows the output activations for the same sequence at various
stages of training (the dashed line is the ‘blank’ unit); the right column
shows the corresponding error signals.
Errors above the horizontal axis
act to increase the corresponding output activation and those below act to
decrease it. (a) Initially the network has small random weights, and the
error is determined by the target sequence only. (b) The network begins
to make predictions and the error localises around them. (c) The network
strongly predicts the correct labelling and the error virtually disappears.
Once the network is trained, we would ideally label some unknown input
sequence x by choosing the most probable labelling l∗:
l∗= arg max
Using the terminology of HMMs, we refer to the task of ﬁnding this labelling
as decoding. Unfortunately, we do not know of a general, tractable decoding
algorithm for CTC. However there are two approximate methods that give
good results in practice.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.4: Problem with best path decoding. The single most probable path contains no labels, and best path decoding therefore outputs the
labelling ‘blank’. However the combined probabilities of the paths corresponding to the labelling ‘A’ is greater.
Best Path Decoding
The ﬁrst method, which refer to as best path decoding, is based on the assumption that the most probable path corresponds to the most probable
where π∗= arg max
Best path decoding is trivial to compute, since π∗is just the concatenation
of the most active outputs at every timestep. However it can lead to errors,
particularly if a label is weakly predicted for several consecutive timesteps
(see Figure 7.4).
Preﬁx Search Decoding
The second method (preﬁx search decoding) relies on the fact that, by modifying the forward variables of Section 7.3, we can eﬃciently calculate the
probabilities of successive extensions of labelling preﬁxes.
Preﬁx search decoding is a best-ﬁrst search through the tree of labellings, where the children of a given
labelling are those that share it as a preﬁx. At each step the search extends
the labelling whose children have the largest cumulative probability (see
Figure 7.5).
Let γt(pn) be the probability of the network outputting preﬁx p by time
t such that a non-blank label is output at t. Similarly, let γt(pb) be the
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.5: Preﬁx search decoding on the alphabet {X,Y}. Each
node either ends (‘e’) or extends the preﬁx at its parent node. The number
above an extending node is the total probability of all labellings beginning
with that preﬁx. The number above an end node is the probability of the
single labelling ending at its parent. At every iteration the extensions of
the most probable remaining preﬁx are explored. Search ends when a single
labelling (here ‘XY’) is more probable than any remaining preﬁx.
probability of the network outputting preﬁx p by time t such that the blank
label is output at t. i.e.
γt(pn) = P(π1:t : B(π1:t) = p, πt = p|p||x)
γt(pb) = P(π1:t : B(π1:t) = p, πt = blank|x)
Then for a length T input sequence x, p(p|x) = γT (pn) + γT (pb). Also let
p(p . . . |x) be the cumulative probability of all labellings not equal to p of
which p is a preﬁx
p(p . . . |x) =
P(p + l|x),
where ∅is the empty sequence. With these deﬁnitions is mind, the pseudocode for preﬁx search decoding is given in Algorithm 7.1.
Given enough time, preﬁx search decoding always ﬁnds the most probable labelling. However, the maximum number of preﬁxes it must expand
grows exponentially with the input sequence length. If the output distribution is suﬃciently peaked around the mode, it will nonetheless ﬁnish in
reasonable time. For many tasks however, a further heuristic is required to
make its application feasible.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
1: Initialisation:
2: 1 ≤t ≤T
γt(∅n) = 0
γt(∅b) = Qt
3: p(∅|x) = γT (∅b)
4: p(∅. . . |x) = 1 −p(∅|x)
5: l∗= p∗= ∅
6: P = {∅}
8: Algorithm:
9: while p(p∗. . . |x) > p(l∗|x) do
probRemaining = p(p∗. . . |x)
for all labels k ∈L do
k if p∗= ∅
0 otherwise
γ1(pb) = 0
prefixProb = γ1(pn)
for t = 2 to T do
newLabelProb = γt−1(p∗
0 if p∗ends in k
n) otherwise
γt(pn) = yt
k (newLabelProb + γt−1(pn))
γt(pb) = yt
b (γt−1(pb) + γt−1(pn))
prefixProb += yt
k newLabelProb
p(p|x) = γT (pn) + γT (pb)
p(p . . . |x) = prefixProb −p(p|x)
probRemaining −= p(p . . . |x)
if p(p|x) > p(l∗|x) then
if p(p . . . |x) > p(l∗|x) then
add p to P
if probRemaining ≤p(l∗|x) then
remove p∗from P
set p∗= p ∈P that maximises p(p . . . |x)
33: Termination:
34: output l∗
Algorithm 7.1: Preﬁx Search Decoding Algorithm.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Observing that the outputs of a trained CTC network tend to form a
series of spikes separated by strongly predicted blanks (see Figure 7.1), we
can divide the output sequence into sections that are very likely to begin
and end with a blank. We do this by choosing boundary points where the
probability of observing a blank label is above a certain threshold. We then
apply Algorithm 7.1 to each section individually and concatenate these to
get the ﬁnal transcription.
In practice, preﬁx search works well with this heuristic, and generally
outperforms best path decoding. However it still makes mistakes in some
cases, e.g. if the same label is predicted weakly on both sides of a section
Constrained Decoding
For certain tasks we want to constrain the output labellings according to
some predeﬁned grammar. For example, in speech and handwriting recognition, the ﬁnal transcriptions are usually required to form sequences of dictionary words. In addition it is common practice to use a language model
to weight the probabilities of particular sequences of words.
We can express these constraints by altering the label sequence probabilities in (7.30) to be conditioned on some probabilistic grammar G, as well
as the input sequence x
l∗= arg max
p(l|x, G).
Absolute requirements, for example that l contains only dictionary words,
can be incorporated by setting the probability of all sequences that fail to
meet them to 0.
At ﬁrst sight, conditioning on G would seem to contradict a basic assumption of CTC: that the labels are conditionally independent given the
input sequences (see Section 7.2). Since the network attempts to model the
probability of the whole labelling at once, there is nothing to stop it from
learning inter-label transitions direct from the data, which would then be
skewed by the external grammar. Indeed, when we tried using a biphone
model to decode a CTC network trained for phoneme recognition, the error rate increased. However, CTC networks are typically only able to learn
local relationships such as commonly occurring pairs or triples of labels.
Therefore as long as G focuses on long range label dependencies (such as
the probability of one word following another when the outputs are letters)
it doesn’t interfere with the dependencies modelled internally by CTC. This
argument is supported by the results in Sections 7.6.4 and 7.6.5.
Applying the basic rules of probability we obtain
p(l|x, G) = p(l|x)p(l|G)p(x)
p(x|G)p(l)
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
where we have used the fact that x is conditionally independent of G given
l. If we assume that x is independent of G, (7.36) reduces to
p(l|x, G) = p(l|x)p(l|G)
This assumption is in general false, since both the input sequences and the
grammar depend on the underlying generator of the data, for example the
language being spoken. However it is a reasonable ﬁrst approximation, and
is particularly justiﬁable in cases where the grammar is created using data
other than that from which x was drawn (as is common practice in speech
and handwriting recognition, where separate textual corpora are used to
generate language models).
If we further assume that, prior to any knowledge about the input or the
grammar, all label sequences are equally probable, (7.35) reduces to
l∗= arg max
p(l|x)p(l|G).
Note that, since the number of possible label sequences is ﬁnite (because
both L and |l| are ﬁnite), assigning equal prior probabilities does not lead
to an improper prior.
CTC Token Passing Algorithm
We now describe an algorithm, based on the token passing algorithm for
HMMs , that allows us to ﬁnd an approximate solution
to (7.38) for a simple grammar.
Let G consist of a dictionary D containing W words, and a set of W 2
bigrams p(w| ˆw) that deﬁne the probability of making a transition from word
ˆw to word w. The probability of any label sequence that does not form a
sequence of dictionary words is 0.
For each word w, deﬁne the modiﬁed word w′ as w with blanks added
at the beginning and end and between each pair of labels. Therefore |w′| =
2|w| + 1. Deﬁne a token tok = (score, history) to be a pair consisting of a
real valued score and a history of previously visited words. In fact, each
token corresponds to a particular path through the network outputs, and
the token score is the log probability of that path. The basic idea of the
token passing algorithm is to pass along the highest scoring tokens at every
word state, then maximise over these to ﬁnd the highest scoring tokens at
the next state. The transition probabilities are used when a token is passed
from the last state in one word to the ﬁrst state in another. The output
word sequence is then given by the history of the highest scoring end-of-word
token at the ﬁnal timestep.
At every timestep t of the length T output sequence, each segment s of
each modiﬁed word w′ holds a single token tok(w, s, t). This is the highest
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
scoring token reaching that segment at that time. In addition we deﬁne the
input token tok(w, 0, t) to be the highest scoring token arriving at word w
at time t, and the output token tok(w, −1, t) to be the highest scoring token
leaving word w at time t.
Pseudocode is provided in Algorithm 7.2.
1: Initialisation:
2: for all words w ∈D do
tok(w, 1, 1) = (ln y1
tok(w, 2, 1) = (ln y1
if |w| = 1 then
tok(w, −1, 1) = tok(w, 2, 1)
tok(w, −1, 1) = (−∞, ())
tok(w, s, 1) = (−∞, ()) for all other s
11: Algorithm:
12: for t = 2 to T do
sort output tokens tok(w, −1, t −1) by ascending score
for all words w ∈D do
w∗= arg max ˆw∈D [tok( ˆw, −1, t −1).score + ln p(w| ˆw)]
tok(w, 0, t).score = tok(w∗, −1, t −1).score + ln p(w|w∗)
tok(w, 0, t).history = tok(w∗, −1, t −1).history + w
for segment s = 1 to |w′| do
P = {tok(w, s, t −1), tok(w, s −1, t −1)}
s ̸= blank and s > 2 and w′
add tok(w, s −2, t −1) to P
tok(w, s, t) = token in P with highest score
tok(w, s, t).score += ln yt
tok(w, −1, t) = highest scoring of {tok(w, |w′|, t), tok(w, |w′| −1, t)}
26: Termination:
27: w∗= arg maxw tok(w, −1, T).score
28: output tok(w∗, −1, T).history
Algorithm 7.2: CTC Token Passing Algorithm
Computational Complexity
The CTC token passing algorithm has a worst case complexity of O(TW 2),
since line 15 requires a potential search through all W words. However,
because the output tokens tok(w, −1, T) are sorted in order of score, the
search can be terminated when a token is reached whose score is less than
the current best score with the transition included. The typical complexity is
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
therefore considerably lower, with a lower bound of O(TWlogW) to account
for the sort. If no bigrams are used, lines 15-17 can be replaced by a simple
search for the highest scoring output token, and the complexity reduces to
Experiments
In this section we apply RNNs with CTC output layers to ﬁve temporal
classiﬁcation tasks.
The ﬁrst three tasks are on speech recognition, and
the remaining two are on handwriting recognition.
In all cases, we use
bidirectional LSTM for the network architecture.
For the handwriting tasks, a dictionary and language model were present,
and we recorded results both with and without the constrained decoding
algorithm of Section 7.5.3. For the speech experiments there was no dictionary or language model, and the output labels (whether phonemes or
whole words) were used directly for transcription. For the experiment in
Section 7.6.1, we compare preﬁx search and best path decoding (see Section 7.5).
As discussed in Chapter 3, the choice of input representation is crucial
to any machine learning algorithm. For most of the experiments here, we
used standard input representations that have been tried and tested with
other sequence learning algorithms, such as HMMs.
The experiment in
Section 7.6.4 was diﬀerent in that we explicitly compared the performance
of CTC using two diﬀerent input representations. As usual, all inputs were
standardised to have mean 0 and standard deviation 1 over the training set.
For all the experiments, the BLSTM hidden layers were fully connected
to themselves, and to the input and output layers.
Each memory block
contained a single LSTM cell, with hyperbolic tangent used for the activation
functions g and h and a logistic sigmoid used for the activation function of
the gates.
The sizes of the input and output layers were determined by
the numbers of inputs and labels in each task. The weights were randomly
initialised from a Gaussian distribution with mean 0 and standard deviation
0.1. Online steepest descent with momentum was used for training, with a
learning rate 10−4 and a momentum of 0.9. All experiments used separate
training, validation and testing sets. Training was stopped when 50 epochs
had passed with no reduction of error on the validation set.
The only network parameters manually adjusted for the diﬀerent tasks
were (1) the number of blocks in the LSTM layers and (2) the variance of the
input noise added during training. We specify these for each experiment.
For all experiments, the basic error measure used to evaluate performance was the label error rate deﬁned in Section 2.3.4, applied to the test
Note however that we rename the label error rate according to the
type of labels used: for example phoneme error rate was used for phoneme
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Table 7.1: Phoneme error rate (PER) on TIMIT with 61 phonemes.
CTC and hybrid results are means over 5 runs, ± standard error. All diﬀerences were signiﬁcant (p < 0.01), except that between HMM-BLSTM with
weighted errors and CTC with best path decoding.
HMM-BLSTM (weighted error)
31.57 ± 0.06%
CTC (best path decoding)
31.47 ± 0.21%
CTC (preﬁx search decoding)
30.51 ± 0.19%
recognition, and word error rate for keyword spotting. For the handwriting
recognition tasks, we evaluate both the character error rate for the labellings
provided by the CTC output layer, and the word error rate for the word
sequences obtained from the token passing algorithm. All the CTC experiments were repeated several times and the results are quoted as a mean ±
the standard error. We also recorded the mean and standard error in the
number of training epochs before the best results were achieved.
Phoneme Recognition
In this section we compare a CTC network with the best HMM and HMM-
BLSTM hybrid results given in Chapter 6 for phoneme recognition on the
TIMIT speech corpus . The task, data and preprocessing were identical to those described in Section 5.1.
Experimental Setup
The CTC network had 26 input units and 100 LSTM blocks in both the
forward and backward hidden layers. It had 62 output units, one for each
phoneme plus the ‘blank’, giving 114, 662 weights in total. Gaussian noise
with mean 0 and standard deviation 0.6 was added to the inputs during
training. When preﬁx search decoding was used, the probability threshold
for the boundary points was 99.99%.
Table 7.1 shows that, with preﬁx search decoding, CTC outperformed both
an HMM and an HMM-RNN hybrid with the same RNN architecture. It also
shows that preﬁx search gave a small improvement over best path decoding.
Note that the best hybrid results were achieved with a weighted error
signal. Such heuristics are unnecessary for CTC, as its objective function
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
ah, ax, ax-h
pcl, tcl, kcl, bcl, dcl, gcl, h#, pau, epi
Table 7.2:
Folding the 61 phonemes in TIMIT onto 39 categories . The phonemes in the right column are folded
onto the corresponding category in the left column (‘q’ is discarded). All
other TIMIT phonemes are left intact.
depends only on the sequence of labels, and not on their duration or segmentation.
Input noise had a greater impact on generalisation for CTC than the
hybrid system, and a slightly higher level of noise was found to be optimal
for CTC (σ = 0.6 instead of 0.5). The mean training time for the CTC
network was 60.0 ± 7 epochs.
Phoneme Recognition with Reduced Label Set
In this Section we consider a variant of the previous task, where the number
of distinct phoneme labels is reduced from 61 to 39. In addition, only the
so-called core test set of TIMIT is used for evaluation. The purpose of these
changes was to allow a direct comparison with other results in the literature.
Data and Preprocessing
In most previous studies, a set of 48 phonemes were selected for modelling
during training, and confusions between several of these were ignored during
testing, leaving an eﬀective set of 39 distinct labels .
Since CTC is a discriminative algorithm, using extra phonemes during training is unnecessary (and probably counterproductive), and the networks were
therefore trained directly with 39 labels.
The folding of the original 61
phoneme labels onto 39 categories is shown in table 7.2.
The TIMIT corpus was divided into a training set, a validation set and
a test set according to . As in our previous experiments,
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
the training set contained 3696 sentences from 462 speakers. However in
this case the test set was much smaller, containing only 192 sentences from
24 speakers, and the validation set, which contained 400 sentences from
50 speakers, was drawn from the unused test sentences rather than the
training set.
This left us with slightly more sentences for training than
before. However this advantage was oﬀset by the fact that the core test set
is somewhat harder than the full test set.
As before, the speech data was transformed into Mel frequency cepstral
coeﬃcients (MFCC) using the HTK software package .
Spectral analysis was carried out with a 40 channel Mel ﬁlter bank from
64 Hz to 8 kHz. A pre-emphasis coeﬃcient of 0.97 was used to correct spectral tilt. Twelve MFCC plus the 0th order coeﬃcient were computed on
Hamming windows 25 ms long, every 10 ms. In this case the second as well
as ﬁrst derivatives of the coeﬃcients were used, giving a vector of 39 inputs
Note that the best previous results on this task were achieved with more
complex preprocessing techniques than MFCCs. For example, in frequency-warped LPC cepstra were used as inputs, while Halberstadt and Glass tried a number of variations and combinations of MFCC,
perceptual linear prediction (PLP) cepstral coeﬃcients, energy and duration .
Experimental Setup
The CTC network had an input layer of size 39, the forward and backward
hidden layers had 128 blocks each, and the output layer was size 40 (39
phonemes plus blank). The total number of weights was 183,080. Gaussian
noise with a standard deviation of 0.6 was added to the inputs during training to improve generalisation. When preﬁx search was used, the probability
threshold was 99.99%.
Results are shown in table 7.3, along with the best results found in the
literature. The performance of CTC with preﬁx search decoding was not
signiﬁcantly diﬀerent from either of the best two results so far recorded (by
Yu et al and Glass).
However, unlike CTC, both of these systems were
heavily tuned to speech recognition, and to TIMIT in particular, and would
not be suitable for other sequence labelling tasks. Furthermore, some of the
adaptations used by Yu et al and Glass, such as improved preprocessing and
decoding, could also be applied to CTC networks, giving potentially better
Yu et al.’s hidden trajectory models (HTM) are a type of probabilistic
generative model aimed at modelling speech dynamics and adding long range
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Table 7.3: Phoneme error rate (PER) on TIMIT with 39 phonemes.
Results for CTC are the average ± standard error over 10 runs. On average,
the networks were trained for 112.5 epochs (± 6.4). CTC with preﬁx search
is not signiﬁcantly diﬀerent from either Yu et al or Glass’s results, but is
signiﬁcantly better than all other results (including CTC with best path).
Large margin HMM 
Baseline HMM 
Triphone continuous density HMM 
RNN-HMM hybrid 
Bayesian triphone HMM 
Near-miss, probabilistic segmentation 
CTC (best path decoding)
25.17 ± 0.2%
HTM 
CTC (preﬁx search decoding)
24.58 ± 0.2%
Committee-based classiﬁer 
context that is missing in standard HMMs.
Glass’s system is a segment-based speech recogniser (as opposed to a
frame-based recogniser) which attempts to detect landmarks in the speech
Acoustic features are computed over hypothesised segments and
at their boundaries.
The standard decoding framework is modiﬁed and
extended to deal with this.
Yu et al.’s best result was achieved with a lattice-constrained A* search
with weighted HTM, HMM, and language model scores. Glass’s best results
were achieved with many heterogeneous information sources and classiﬁer
combinations. It is likely that both Yu et al.’s HTM and CTC would achieve
improved performance when combined with other classiﬁers and/or more
sources of input information.
Keyword Spotting
The task in this section is keyword spotting, using the Verbmobil speech
corpus .
The aim of keyword spotting is to identify a
particular set of spoken words within (typically unconstrained) speech signals. In most cases, the keywords occupy only a small fraction of the total
data. Discriminative approaches are interesting for keyword spotting, because they are able to concentrate on identifying and distinguishing the
keywords, while ignoring the rest of the signal. However, the predominant
method is to use hidden Markov models, which are generative, and must
therefore model the unwanted speech, and even the non-speech noises, in
addition to the keywords.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
In many cases one seeks not only the identity of the keywords, but also
their approximate position. For example, this would be desirable if the goal
were to further examine those segments of a long telephone conversation in
which a keyword occurred. In principle, locating the keywords presents a
problem for CTC, since the network is only trained to ﬁnd the sequence of
labels, and not their position. However we have observed that in most cases
CTC predicts labels close to the relevant segments of the input sequence. In
the following experiments , we conﬁrm this observation by recording the word error rate both with and without the requirement
that the network ﬁnd the approximate location of the keywords.
Data and Preprocessing
Verbmobil consists of dialogues of noisy, spontaneous German speech, where
the purpose of each dialogue is to schedule a date for an appointment or
meeting. It comes divided into training, validation and testing sets, all of
which have a complete phonetic transcription. The training set includes 748
speakers and 23,975 dialogue turns, giving a total of 45.6 hours of speech.
The validation set includes 48 speakers, 1,222 dialogue turns and a total of
2.9 hours of speech. The test set includes 46 speakers, 1,223 dialogue turns
and a total of 2.5 hours of speech. Each speaker appears in only one of the
The twelve keywords we chose were: April, August, Donnerstag, Februar,
Frankfurt, Freitag, Hannover, Januar, Juli, Juni, Mittwoch, Montag. Since
the dialogues are concerned with dates and places, all of these occurred fairly
frequently in the data sets. One complication is that there are pronunciation variants of some of these keywords (e.g. “Montag” can end either with
a /g/ or with a /k/). Another is that several keywords appear as sub-words,
e.g. in plural form such as “Montags” or as part of another word such as
“Ostermontag” (Easter Monday). The start and end times of the keywords
were given by the automatic segmentation provided with the phonetic transcription.
In total there were 10,469 keywords on the training set with an average
of 1.7% keywords per non-empty utterance (73.6% of the utterances did not
have any keyword); 663 keywords on the validation set with an average of
1.7% keywords per non-empty utterance (68.7% of the utterances did not
have any keyword); and 620 keywords on the test set with an average of 1.8
keywords per non-empty utterance (71.1% of the utterances did not have
any keyword).
The audio preprocessing was identical to that described in Section 5.1,
except that the second order derivatives of the MFCC coeﬃcients were also
computed, giving a total of 39 inputs per frame.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Table 7.4: Keyword error rate (KER) on Verbmobil. Results are a
mean over 4 runs, ± standard error.
CTC (approx. location)
15.5 ± 1.2%
CTC (any location)
13.9 ± 0.7%
Experimental Setup
The CTC network contained 128 LSTM blocks in the forward and backward
hidden layers.
The output layer contained 13 units and the input layer
contained 39 units, giving 176,141 weights in total. Gaussian noise with a
mean of 0 and a standard deviation of 0.5 was added during training.
We deﬁned the CTC network as having found the approximate location
of a keyword if the corresponding label was output within 0.5 seconds of
the boundary of the keyword segment. The experiment was not repeated
for the approximate location results: the output of the network was simply
re-scored with location required.
Table 7.4 shows that the CTC network gave a mean error rate of 15.5%.
The CTC system performed slightly better without the constraint that it
ﬁnd the approximate location of the keywords. This shows in most cases it
aligned the keywords with the relevant portion of the input signal.
Although we don’t have a direct comparison for this result, a benchmark HMM system performing full speech recognition on the same dataset
achieved a word error rate of 35%. We attempted to train an HMM system
speciﬁcally for keyword spotting, with a single junk model for everything
apart from the keywords, but found that it did not converge. This is symptomatic of the diﬃculty of using a generative model for a task where so
much of the signal is irrelevant.
The mean training time for the CTC network was 91.3 ± 22.5 epochs.
Figure 7.6 shows the CTC outputs during a dialogue turn containing several
keywords. For a zoomed in section of the same dialogue turn, Figure 7.7
shows the sequential Jacobian for the output unit associated with the keyword “Donnerstag” at the time step indicated by an arrow at the top of
the ﬁgure. The extent (0.9 s) and location of the keyword in the speech
signal is shown at the top of the ﬁgure. As can be seen, the output is most
sensitive to the ﬁrst part of the keyword. This is unsurprising, since the
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Keyword probability
montag donnerstag freitag
Figure 7.6: CTC outputs for keyword spotting on Verbmobil
δ(output) / δ(input)
donnerstag (0.9 seconds)
Figure 7.7: Sequential Jacobian for keyword spotting on Verbmobil
ending, “tag”, is shared by many of the keywords and is therefore the least
discriminative part.
Online Handwriting Recognition
The task in this section is online handwriting recognition, using the IAM-
OnDB handwriting database 1. In online handwriting recognition, the state and position of the pen is recorded during
writing, and used as input to the learning algorithm.
For the CTC experiments ,
we record the the character error rate using standard best-path decoding,
and the word error rate using constrained decoding. For the HMM system,
only the word error rate is given.
We compare results using two diﬀerent input representations, one hand
1Available for public download at fki/iamondb/
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
crafted for HMMs, the other consisting of raw data direct from the pen
Data and Preprocessing
IAM-OnDB consists of pen trajectories collected from 221 diﬀerent writers
using a ‘smart whiteboard’ . The writers were
asked to write forms from the LOB text corpus , and
the position of their pen was tracked using an infra-red device in the corner
of the board. The original input data consists of the x and y pen coordinates,
the points in the sequence when individual strokes (i.e. periods when the pen
is pressed against the board) end, and the times when successive position
measurements were made. Recording errors in the x, y data was corrected
by interpolating to ﬁll in for missing readings, and removing steps whose
length exceeded a certain threshold.
The character level transcriptions contain 80 distinct target labels (capital letters, lower case letters, numbers, and punctuation).
A dictionary
consisting of the 20, 000 most frequently occurring words in the LOB corpus
was used for decoding, along with a bigram language model. 5.6% of the
words in the test set were not contained in the dictionary. The language
model was optimised on the training and validation sets only.
IAM-OnDB is divided into a training set, two validation sets, and a
test set, containing respectively 5364, 1,438, 1,518 and 3,859 written lines
taken from 775, 192, 216 and 544 forms. For our experiments, each line was
assumed to be an independent sequence (meaning that the dependencies
between successive lines, e.g. for a continued sentence, were ignored).
Two input representations were used for this task. The ﬁrst consisted
simply of the oﬀset of the x, y coordinates from the top left of the line, along
with the time from the beginning of the line, and an extra input to mark
the points when the pen was lifted oﬀthe whiteboard (see Figure 7.8). We
refer to this as the raw representation.
The second representation required a large amount of sophisticated preprocessing and feature extraction . We refer to this as
the preprocessed representation. Brieﬂy, in order to account for the variance
in writing styles, the pen trajectories were ﬁrst normalised with respect to
such properties as the slant, skew and width of the letters, and the slope of
the line as a whole. Two sets of input features were then extracted, one consisting of ‘online’ features, such as pen position, pen speed, line curvature
etc., and the other consisting of ‘oﬄine’ features derived from a two dimensional window of the image reconstructed from the pen trajectory. Delayed
strokes (such as the crossing of a ‘t’ or the dot of an ‘i’) are removed by the
preprocessing because they introduce diﬃcult long time dependencies
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Table 7.5: CTC Character error rate (CER) on IAM-OnDB. Results
are a mean over 4 runs, ± standard error.
13.9 ± 0.1%
Preprocessed
11.5 ± 0.05%
Table 7.6: Word error rate (WER) on IAM-OnDB. LM = language
model. CTC results are a mean over 4 runs, ± standard error. All diﬀerences
were signiﬁcant (p < 0.01)
Preprocessed
30.1 ± 0.5%
Preprocessed
26.0 ± 0.3%
22.8 ± 0.2%
Preprocessed
20.4 ± 0.3%
Experimental Setup
The CTC network contained 100 LSTM blocks in the forward and backward
hidden layers. The output layer contained 81 units. For the raw representation, there were 4 input units, giving 100,881 weights in total. For the preprocessed representation, there were 25 input units, giving 117,681 weights
in total. No noise was added during training.
The HMM setup contained a separate, linear HMM
with 8 states for each character (8 ∗81 = 648 states in total). Diagonal
mixtures of 32 Gaussians were used to estimate the observation probabilities.
All parameters, including the word insertion penalty and the grammar scale
factor, were optimised on the validation set.
From Table 7.6 we can see that, with a language model and the preprocessed input representation, CTC gives a mean word error rate of 20.4%,
compared to 35.5% with a benchmark HMM. This is an error reduction
of 42.5%. Moreover, even without the language model or the handcrafted
preprocessing, CTC well outperforms HMMs.
The mean training time for the CTC network was 41.3 ± 2.4 epochs
for the preprocessed data, and 233.8 ± 16.8 epochs for the raw data. This
disparity reﬂects the fact that the preprocessed data contains simpler corre-
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
lations and shorter time-dependencies, and is therefore easier for the network
to learn. It is interesting to note how large the variance in training epochs
was for the raw data, given how small the variance in ﬁnal performance was.
The CTC results with the raw inputs, where the information required to
identify each character is distributed over many timesteps, demonstrate the
ability of BLSTM to make use of long range contextual information. An
indication of the amount of context required is given by the fact that when
we attempted to train a CTC network with a standard BRNN architecture
on the same task, it did not converge.
Figures 7.9 and 7.10 show sequential Jacobians for BLSTM CTC networks using respectively the raw and preprocessed inputs for a phrase from
IAM-OnDB. As expected, the size of the region of high sensitivity is considerably larger for the raw representation, because the preprocessing creates
localised input features that do not require as much use of long range context.
Oﬄine Handwriting Recognition
This section describes an oﬄine handwriting recognition experiment, using
the IAM-DB oﬄine handwriting corpus 2. Oﬄine
handwriting diﬀers from online in that only the ﬁnal image created by the
pen is available to the algorithm.
This makes the extraction of relevant
input features more diﬃcult, and usually leads to lower recognition rates.
Data and Preprocessing
The IAM-DB training set contains 6,161 text lines written by 283 writers,
the validation set contains 920 text lines by 56 writers, and the test set
contains 2,781 text lines by 161 writers. No writer in the test set appears
in either the training or validation sets.
Substantial preprocessing was used for this task . Brieﬂy, to reduce the impact of diﬀerent writing styles, a handwritten text line image is normalised with respect to skew, slant, and baseline
position in the preprocessing phase. After these normalisation steps, a handwritten text line is converted into a sequence of feature vectors. For this
purpose a sliding window is used which is moved from left to right, one pixel
at each step. Nine geometrical features are extracted at each position of the
sliding window.
A dictionary and statistical language model, derived from the same three
textual corpora as used in Section 7.6.4, were used for decoding (Bertolami
2Available for public download at fki/iamDB
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Table 7.7: Word error rate (WER) on IAM-DB. LM = language model.
CTC results are a mean over 4 runs, ± standard error.
34.6 ± 1.1%
25.9 ± 0.8%
and Bunke, 2007). The integration of the language model was optimised
on a validation set. As before, the dictionary consisted of the 20,000 most
frequently occurring words in the corpora.
Experimental Setup
The HMM-based recogniser was identical to the one used in , with each character modelled by a linear HMM. The number
of states was chosen individually for each character , and twelve Gaussian mixture components were used to model the
output distribution in each state.
The CTC network contained 100 LSTM blocks in the forward and backward hidden layers. There were 9 inputs and 82 outputs, giving 105,082
weights in total. No noise was added during training.
From Table 7.7 it can be seen that CTC with a language model gave a mean
word error rate of 25.9%, compared to 35.5% with a benchmark HMM. This
is an error reduction of 27.0%. Without the language model however, CTC
did not signiﬁcantly outperform the HMM. The character error rate for the
CTC system was 18.2 ± 0.6%.
While these results are impressive, the advantage of CTC over HMMs
was smaller for this task than for the online recognition in Section 7.6.4. A
possible reason for this is that the two-dimensional nature of oﬄine handwriting is less well suited to RNNs than the one-dimensional time series
found in online handwriting.
The mean training time for the CTC network was 71.3 ± 7.5 epochs.
Discussion
For most of the experiments in this chapter, the performance gap between
CTC networks and HMMs is substantial. In what follows, we discuss the
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
key diﬀerences between the two systems, and suggest reasons for the RNN’s
superiority.
Firstly, HMMs are generative, while an RNN trained with CTC is discriminative. As discussed in Section 2.2.3, advantages of the generative approach include the possibility of adding extra models to an already trained
system, and being able to generate synthetic data. However, discriminative
methods tend to give better results for classiﬁcation tasks, because they focus entirely on ﬁnding the correct labels. Additionally, for tasks where the
prior data distribution is hard to determine, generative approaches can only
provide unnormalised likelihoods for the label sequences. Discriminative approaches, on the other hand, yield normalised label probabilities, which can
be used to assess prediction conﬁdence, or to combine the outputs of several
classiﬁers.
A second diﬀerence is that RNNs, and particularly LSTM, provide more
ﬂexible models of the input features than the mixtures of diagonal Gaussians
used in standard HMMs. In general, mixtures of Gaussians can model complex, multi-modal distributions; however, when the Gaussians have diagonal
covariance matrices (as is usually the case) they are limited to modelling distributions over independent variables. This assumes that the input features
are decorrelated, which can be diﬃcult to ensure for real-world tasks. RNNs,
on the other hand, do not assume that the features come from a particular
distribution, or that they are independent, and can model nonlinear relationships among features. However, RNNs generally perform better using
input features with simpler inter-dependencies.
A third diﬀerence is that the internal states of a standard HMM are
discrete and single valued, while those of an RNN are deﬁned by the vector
of activations of the hidden units, and are therefore continuous and multivariate. This means that for an HMM with N states, only O(logN) bits of
information about the past observation sequence are carried by the internal
state. For an RNN, on the other hand, the amount of internal information
grows linearly with the number of hidden units.
A fourth diﬀerence is that HMMs are constrained to segment the entire
input, in order to determine the sequence of hidden states. This is often
problematic for continuous input sequences, since the precise boundary between units, e.g. characters, can be ambiguous. It is also an unnecessary
burden in tasks, such as keyword spotting, where most of the inputs should
be ignored. A further problem with segmentation is that, at least with standard Markovian transition probabilities, the probability of remaining in a
particular state decreases exponentially with time. Exponential decay is in
general a poor model of state duration, and various measures have been
suggested to alleviate this . However, an RNN trained with
CTC does not need to segment the input sequence, and therefore avoids
both of these problems.
A ﬁnal, and perhaps most crucial, diﬀerence is that unlike RNNs, HMMs
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
assume that the probability of each observation depends only on the current
state. A consequence of this is that data consisting of continuous trajectories (such as the sequence of pen coordinates for online handwriting, and the
sequence of window positions in oﬄine handwriting) are diﬃcult to model
with standard HMMs, since each observation is heavily dependent on those
around it. Similarly, data with long range contextual dependencies is troublesome, because individual sequence elements (e.g., letters or phonemes)
are inﬂuenced by the elements surrounding them. The latter problem can
be mitigated by adding extra models to account for each sequence element
in all diﬀerent contexts (e.g., using triphones instead of phonemes for speech
recognition). However, increasing the number of models exponentially increases the number of parameters that must be inferred which, in turn,
increases the amount of data required to reliably train the system.
RNNs on the other hand, modelling continuous trajectories is natural, since
their own hidden state is itself a continuous trajectory. Furthermore, the
range of contextual information accessible to an RNN is limited only by the
choice of architecture, and in the case of BLSTM can in principle extend to
the entire input sequence.
To summarise, the observed advantages of CTC networks over HMMs
can be explained by the fact that, as researchers approach problems of
increasing complexity, the assumptions HMMs are based on lose validity.
For example, unconstrained handwriting or spontaneous speech are more
dynamic and show more pronounced contextual eﬀects than hand printed
scripts or read speech.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.8: CTC network labelling an excerpt from IAM-OnDB,
using raw inputs. The ‘:’ label in the outputs is an end-of-word marker.
The ‘Reconstructed Image’ was recreated from the pen positions stored by
the sensor. Successive strokes have been alternately coloured red and black
to highlight their boundaries. Note that strokes do not necessarily correspond to individual letters: this is no problem for CTC because it does not
require segmented data. This example demonstrates the robustness of CTC
to line slope, and illustrates the need for context when classifying letters
(the ‘ri’ in ‘bring’ is ambiguous on its own, and the ﬁnal ‘t’ could equally be
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.9: CTC Sequential Jacobian from IAM-OnDB, with raw
inputs. For ease of visualisation, only the derivative with highest absolute
value is plotted at each timestep. The Jacobian is plotted for the output
corresponding to the label ‘i’ at the point when ‘i’ is emitted (indicated
by the vertical dashed lines). Note that the network is mostly sensitive to
the end of the word: this is possibly because ‘ing’ is a common suﬃx, and
ﬁnding the ‘n’ and ‘g’ therefore increases the probability of identifying the
‘i’. Note also the spike in sensitivity at the very end of the sequence: this
corresponds to the delayed dot of the ‘i’.
CHAPTER 7. CONNECTIONIST TEMPORAL CLASSIFICATION
Figure 7.10: CTC Sequential Jacobian from IAM-OnDB, with preprocessed inputs.
As before, only the highest absolute derivatives are
shown, and the Jacobian is plotted at the point when ‘i’ is emitted. The
range of sensitivity is smaller and more symmetrical than for the raw inputs.
Multidimensional Recurrent
Neural Networks
As we have seen in previous chapters, recurrent networks are an eﬀective
architecture for sequence learning tasks, where the data is strongly correlated along a single axis. This axis typically corresponds to time, or in
some cases (such as protein secondary structure prediction) one-dimensional
space. Some of the properties that make RNNs suitable for sequence learning, for example robustness to input warping and the ability to incorporate
context, are also desirable in domains with more than one spatio-temporal
dimension. However, standard RNNs are inherently one dimensional, and
therefore poorly suited to multidimensional data. This chapter describes
multidimensional recurrent neural networks ,
a special case of directed acyclic graph RNNs . MDRNNs extend the potential applicability of RNNs to vision,
video processing, medical imaging and many other areas, while avoiding the
scaling problems that have plagued other multidimensional models. We also
introduce multidimensional Long Short-Term Memory, thereby bringing the
beneﬁts of long range contextual processing to multidimensional tasks.
Although we will focus on the application of MDRNNs to supervised
labelling and classiﬁcation, it should be noted that the same architecture
could be used for any task requiring the processing of multidimensional
Section 8.1 provides the background material and literature review for
multidimensional algorithms. Section 8.2 describes the MDRNN architecture in detail. Section 8.3 presents experimental results on two image classiﬁcation tasks.
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Background
Recurrent neural networks were originally developed as a way of extending
neural networks to sequential data. As discussed in previous chapters, the
addition of recurrent connections allows RNNs to make use of previous context, as well as making them more more robust to warping along the time
axis than non-recursive models. Access to contextual information and robustness to warping are also important when dealing with multidimensional
data. For example, a face recognition algorithm should use the entire face
as context, and should be robust to changes in perspective, distance etc. It
therefore seems desirable to apply RNNs to such tasks.
However, the standard RNN architectures are inherently one dimensional, meaning that in order to use them for multidimensional tasks, the
data must be preprocessed to one dimension, for example by presenting one
vertical line of an image at a time to the network. Perhaps the best known
use of neural networks for multidimensional data has been the application of
convolution networks to image processing tasks such
as digit recognition . One disadvantage of convolution
networks is that, because they are not recurrent, they rely on hand speciﬁed kernel sizes to introduce context. Another disadvantage is that they
do not scale well to large images. For example, sequences of handwritten
digits must be presegmented into individual characters before they can be
recognised by convolution networks .
Other neural network based approaches to two-dimensional data have been structured like
cellular automata. A network update is performed at every timestep for
every data-point, and contextual information propagates one step at a time
in all directions. This provides an intuitive solution to the problem of simultaneously assimilating context from all directions. However, one problem
is that the total computation time grows linearly with the range of context
required, up to the length of the longest diagonal in the sequence.
A more eﬃcient way of building multidimensional context into recurrent networks is provided by directed acyclic graph RNNs .
DAG-RNNs generalise
the forwards-backwards structure of bidirectional RNNs (see Section 3.2.3)
to networks whose pattern of recurrency is determined by an arbitrary directed acyclic graph. The network processes the entire sequence in one pass,
and the diﬀusion of context is as eﬃcient as for one dimensional RNNs. An
important special case of the DAG-RNN architecture occurs when the recurrency graph corresponds to an n-dimensional grid, with 2n distinct networks
used to process the data along all possible directions. Baldi refers to this
as the “canonical” generalisation of BRNNs. In two dimensions, canonical
DAG-RNNs have been successfully used to evaluate positions in the board
game Go . Furthermore the multidirectional version
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
of the MDRNNs discussed in this chapter are equivalent to n-dimensional
canonical DAG-RNNs, although the formulation is somewhat diﬀerent.1
Various statistical models have also been proposed for multidimensional
data, notably multidimensional hidden Markov models. However, multidimensional HMMs suﬀer from two serious drawbacks: (1) the time required
to run the Viterbi algorithm, and thereby calculate the optimal state sequences, grows exponentially with the size of the data exemplars, and (2) the
number of transition probabilities, and hence the required memory, grows
exponentially with the data dimensionality. Numerous approximate methods have been proposed to alleviate one or both of these problems, including
pseudo 2D and 3D HMMs , isolating elements , approximate Viterbi algorithms , and dependency
tree HMMs . However, none of these methods exploit the
full multidimensional structure of the data.
As we will see, MDRNNs bring the beneﬁts of RNNs to multidimensional
data, without suﬀering from the scaling problems described above.
The MDRNN architecture
The basic idea of MDRNNs is to replace the single recurrent connection
found in standard RNNs with as many recurrent connections as there are
dimensions in the data. During the forward pass, at each point in the data
sequence, the hidden layer of the network receives both an external input
and its own activations from one step back along all dimensions. Figure 8.1
illustrates the two dimensional case.
Note that, although the word sequence usually denotes one dimensional
data, we will use it to refer to independent data exemplars of any dimensionality. For example, an image is a two dimensional sequence, a video
is a three dimensional sequence, and a series of fMRI brain scans is a four
dimensional sequence.
Clearly, the data must be processed in such a way that when the network
reaches a point in an n-dimensional sequence, it has already passed through
all the points from which it will receive its previous activations. This can be
ensured by following a suitable ordering on the set of points {(p1, p2, ..., pn)}.
One example of such an ordering is (p1, . . . , pn) < (p′
1, . . . , p′
n) if ∃m ∈
(1, . . . , n) such that pm < p′
m and pd = p′
d ∀d ∈(1, . . . , m −1). Note that
this is not the only possible ordering, and that its realisation for a particular
sequence depends on an arbitrary choice of axes. We will return to this point
in Section 8.2.1. Figure 8.3 illustrates the above ordering for a 2 dimensional
1We only discovered the equivalence between MDRNNs and DAG-RNNs during the
review process of the thesis, after our ﬁrst publication on the subject Graves et al., 2007
had already appeared.
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Figure 8.1: 2D RNN forward
Figure 8.2: 2D RNN backward
Figure 8.3: Sequence ordering of 2D data. The MDRNN forward pass
starts at the origin and follows the direction of the arrows. The point (i,j)
is never reached before both (i-1,j) and (i,j-1).
The forward pass of an MDRNN can then be carried out by feeding forward the input and the n previous hidden layer activations at each point in
the ordered input sequence, and storing the resulting hidden layer activations. Care must be taken at the sequence boundaries not to feed forward
activations from points outside the sequence.
Note that the ‘points’ in the input sequence will in general be multivalued
vectors. For example, in a two dimensional colour image, the inputs could be
single pixels represented by RGB triples, or blocks of pixels, or the outputs
of a preprocessing method such as a discrete cosine transform.
The error gradient of an MDRNN (that is, the derivative of some objective function with respect to the network weights) can be calculated with an
n-dimensional extension of backpropagation through time (BPTT; see Section 3.1.4 for more details). As with one dimensional BPTT, the sequence
is processed in the reverse order of the forward pass.
At each timestep,
the hidden layer receives both the output error derivatives and its own n
‘future’ derivatives. Figure 8.2 illustrates the BPTT backward pass for two
dimensions. Again, care must be taken at the sequence boundaries.
j respectively as the network input to unit j and the
activation of unit j at point p = (p1, . . . , pn) in an n-dimensional sequence
ij be the weight of the recurrent connection from unit i to unit j
along dimension d. Consider an n-dimensional MDRNN with I input units,
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
K output units, and H hidden summation units. Let θh be the activation
function of hidden unit h. Then the forward pass up to the hidden layer for
a sequence with dimensions (D1, D2, . . . , Dn) is given in Algorithm 8.1.
for p1 = 0 to D1 −1 do
for p2 = 0 to D2 −1 do
for pn = 0 to Dn −1 do
for h = 1 to H do
for d = 1 to n do
if pd > 0 then
h′=1 b(p1,...,pd−1,...,pn)
Algorithm 8.1: MDRNN Forward Pass
Note that units are indexed starting at 1, while coordinates are indexed
starting at 0. For some unit j and some diﬀerentiable objective function O,
Then the backward pass for the hidden layer is given in Algorithm 8.2.
for p1 = D1 −1 to 0 do
for p2 = D2 −1 to 0 do
for pn = Dn −1 to 0 do
for h = 1 to H do
for d = 1 to n do
if pd < Dd −1 then
h′=1 δ(p1,...,pd+1,...,pn)
Algorithm 8.2: MDRNN Backward Pass
The ‘loop over loops’ in these algorithms can be naturally implemented
with recursive functions.
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Deﬁning p−
= (p1, . . . , pd −1, . . . , pn) and p+
= (p1, . . . , pd + 1, . . . , pn),
the above procedures can be compactly expressed as follows:
Forward Pass
Backward Pass
Since the forward and backward pass require one pass each through the data
sequence, the overall complexity of MDRNN training is linear in the number
of data points and the number of network weights.
In the special case where n = 1, the above equations reduce to those of
a standard RNN (see Section 3.2).
Multidirectional MDRNNs
At some point (p1, . . . , pn) in the input sequence, the network described
above has access to all points (p′
1, . . . , p′
n) such that p′
d ≤pd ∀d ∈(1, . . . , n).
This deﬁnes an n-dimensional ‘context region’ of the full sequence, as illustrated in Figure 8.4. For some tasks, such as object recognition, this would
in principle be suﬃcient. The network could process the image according to
the ordering, and output the object label at a point when the object to be
recognised is entirely contained in the context region.
Intuitively however, we would prefer the network to have access to the
surrounding context in all directions.
This is particularly true for tasks
where precise localisation is required, such as image segmentation. As discussed in Chapter 3, for one dimensional RNNs, the problem of multidirectional context was solved by the introduction of bidirectional recurrent
neural networks (BRNNs). BRNNs contain two separate hidden layers that
process the input sequence in the forward and reverse directions. The two
hidden layers are connected to a single output layer, thereby providing the
network with access to both past and future context.
BRNNs can be extended to n-dimensional data by using 2n separate hidden layers, each of which processes the sequence using the ordering deﬁned
above, but with a diﬀerent choice of axes. The axes are chosen so that each
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Figure 8.4: Context available at (i,j) to a 2D RNN with a single
hidden layer
Figure 8.5: Axes used by the 4 hidden layers in a multidirectional
2D RNN. The arrows inside the rectangle indicate the direction of propagation during the forward pass.
one has its origin on a distinct vertex of the sequence. The 2 dimensional
case is illustrated in Figure 8.5. As before, the hidden layers are connected
to a single output layer, which now has access to all surrounding context
(see Figure 8.6).
Clearly, if the size of the hidden layers is held constant, the complexity of
the multidirectional MDRNN architecture scales as O(2n) for n-dimensional
data. In practice however, the computing power of the network tends to be
governed by the overall number of weights, rather than the size of the hidden layers, because the data processing is shared between the layers. Since
the complexity of the algorithm is linear in the number of parameters, the
O(2n) scaling factor can be oﬀset by simply using smaller hidden layers for
higher dimensions. Furthermore, the complexity of a task, and therefore
the number of weights likely to be needed for it, does not necessarily increase with the dimensionality of the data. For example, both the networks
described in this chapter have less than half the weights than the one dimensional networks we applied to speech recognition in Chapters 5–7. For a
given task, we have also found that using a multidirectional MDRNN gives
better results than a unidirectional MDRNN with the same overall number
of weights, as demonstrated for one dimension in Chapter 5.
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Figure 8.6: Context available at (i,j) to a multidirectional 2D RNN
In fact, the main scaling concern for MDRNNs is that there tend to be
many more data points in higher dimensions (e.g. a video sequence contains
far more pixels than an image). However this can be alleviated by gathering
together the inputs into multidimensional ‘blocks’ — for example 8 by 8 by
8 pixels for a video.
For a multidirectional MDRNN, the forward and backward passes through
an n-dimensional sequence can be summarised as follows:
1: For each of the 2n hidden layers choose a distinct vertex of the sequence,
then deﬁne a set of axes such that the vertex is the origin and all sequence
coordinates are ≥0
2: Repeat Algorithm 8.1 for each hidden layer
3: At each point in the sequence, feed forward all hidden layers to the
output layer
Algorithm 8.3: Multidirectional MDRNN Forward Pass
1: At each point in the sequence, calculate the derivative of the objective
function with respect to the activations of output layer
2: With the same axes as above, repeat Algorithm 8.2 for each hidden layer
Algorithm 8.4: Multidirectional MDRNN Backward Pass
Multidimensional Long Short-Term Memory
The standard formulation of LSTM is explicitly one-dimensional, since the
cell contains a single self connection, whose activation is controlled by a
single forget gate. However we can extend this to n dimensions by using
instead n self connections (one for each of the cell’s previous states along
every dimension) with n forget gates. The suﬃx ι, d denotes the forget gate
corresponding to connection d. As before, peephole connections lead from
the cells to the gates. Note however that the input gates ι is connected to
previous cell c along all dimensions with the same weight (wcι) whereas each
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
forget gate d is only connected to cell c along dimension d, with a separate
weight wc(ι,d) for each d. The peephole to the output gate receives input
from the current state, and therefore requires only a single weight.
Combining the above notation with that of Sections 4.5 and 8.2, the
equations for training multidimensional LSTM can be written as follows:
Forward Pass
Input Gates
Forget Gates
i wi(φ,d) +
c=1 wc(φ,d)s
0 otherwise
φ,d = f(ap
Output Gates
Cell Outputs
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Backward Pass
Cell Outputs
Output Gates
φ,dwc(φ,d)
Forget Gates
s if pd > 0
0 otherwise
Input Gates
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Figure 8.7: Frame from the Air Freight database. The original image
is on the left and the colour-coded texture segmentation is on the right.
Experiments
Air Freight Data
The Air Freight database is a ray-traced
colour image sequence that comes with a ground truth segmentation into the
diﬀerent textures mapped onto the 3-d models (Figure 8.7). The sequence
is 455 frames long and contains 155 distinct textures. Each frame is 120
pixels high and 160 pixels wide.
The advantage of ray-traced data is the true segmentation can be deﬁned
directly from the 3D models. Although the images are not real, they are
realistic in the sense that they have signiﬁcant lighting, specular eﬀects etc.
We used the sequence to deﬁne a 2D image segmentation task, where
the aim was to assign each pixel in the input data to the correct texture
class. We divided the data at random into a 250 frame train set, a 150
frame test set and a 55 frame validation set. We could have instead deﬁned
a 3D task where the network processed segments of the video as independent sequences. However, this would have left us with fewer exemplars for
training and testing.
For this task we used a multidirectional 2D RNN with LSTM hidden
layers. Each of the 4 layers consisted of 25 memory blocks, each containing
1 cell, 2 forget gates, 1 input gate, 1 output gate and 5 peephole weights.
This gave a total 600 hidden units. tanh was used for the input and output
activation functions of the cells, and the activation function for the gates
was the logistic sigmoid. The input layer was size 3 (one each for the red,
green and blue components of the pixels) and the output layer was size 155
(one unit for each texture). The network contained 43,257 trainable weights
The softmax activation function was used at the output layer,
with the cross-entropy objective function (Section 3.1.3). The network was
trained using online gradient descent (weight updates after every training
sequence) with a learning rate of 10−6 and a momentum of 0.9.
The ﬁnal pixel classiﬁcation error rate, after 330 training epochs, was
7.1% on the test set.
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Figure 8.8: MNIST image before and after deformation
MNIST Data
The MNIST database of isolated handwritten digits
is a subset of a larger database available from NIST. It consists of sizenormalised, centred images, each of which is 28 pixels high and 28 pixels
wide and contains a single handwritten digit. The data comes divided into
a training set with 60,000 images and a test set with 10,000 images. We used
10,000 of the training images for validation, leaving 50,000 for training.
The task on MNIST is to label the images with the corresponding digits. This is a well-known benchmark for which many pattern classiﬁcation
algorithms have been evaluated.
We trained the network to perform a slightly modiﬁed task where each
pixel was classiﬁed according to the digit it belonged to, with an additional
class for background pixels. We then recovered the original task by choosing
for each sequence the digit whose corresponding output unit had the highest
cumulative activation over the entire sequence.
To test the network’s robustness to input warping, we also evaluated it
on an altered version of the MNIST test set, where elastic deformations had
been applied to every image (see Figure 8.8). The deformations were the
same as those used by Simard to augment the MNIST training set,
with parameters σ = 4.0 and α = 34.0, and using a diﬀerent initial random
ﬁeld for every sample image.
We compared our results with the convolution neural network that has
achieved the best results so far on MNIST .
that we re-implemented the convolution network ourselves, and we did not
augment the training set with elastic distortions, which gives a substantial
improvement in performance.
The MDRNN for this task was identical to that for the Air Freight task
with the following exceptions: the sizes of the input and output layers were
now 1 (for grayscale pixels) and 11 (one for each digit, plus background)
respectively, giving 27,511 weights in total, and the learning rate was 10−5.
Table 8.1 shows that the MDRNN matched the convolution network on
the clean test set, and was considerably better on the warped test set. This
suggests that MDRNNs are more robust to input warping than convolution
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Table 8.1: Image error rate on MNIST
Clean Test Set
Warped Test Set
Convolution
Figure 8.9: 2D RNN applied to an image from the Air Freight
database. The hidden layer activations display one unit from each of the
layers. A common behaviour is to ‘mask oﬀ’ parts of the image, exhibited
here by layers 2 and 3.
networks. The pixel classiﬁcation error rates for the MDRNN were 0.4% for
the clean test set and 3.8% for the warped test set.
However one area in which the convolution net greatly outperformed the
MDRNN was training time. The MDRNN required 95 training epochs to
converge, whereas the convolution network required 20. Furthermore, each
training epoch took approximately 3.7 hours for the MDRNN compared to
under ten minutes for the convolution network, using a similar processor.
The total training time for the MDRNN was over two weeks.
One beneﬁt of two dimensional tasks is that the operation of the network
can be easily visualised. Figure 8.9 shows the network activations during a
frames from the Air Freight database. As can be seen, the network segments
this image almost perfectly, in spite of diﬃcult, reﬂective surfaces such as
the glass and metal tube running from left to right.
Clearly, classifying
CHAPTER 8. MULTIDIMENSIONAL RECURRENT NETWORKS
Figure 8.10: Sequential Jacobian of a 2D RNN for an image from
MNIST. The white outputs correspond to the class ‘background’ and the
light grey ones to ‘8’. The black outputs represent misclassiﬁcations. The
output pixel for which the Jacobian is calculated is marked with a cross.
Absolute values are plotted for the Jacobian, and lighter colours are used
for higher values.
individual pixels in such a surface requires the use of contextual information.
Figure 8.10 shows the absolute value of the sequential Jacobian of an
output during classiﬁcation of an image from the MNIST database. It can
be seen that the network responds to context from across the entire image,
and seems particularly attuned to the outline of the digit.
Conclusions and Future
The aim of this thesis was to advance the state-of-the-art in supervised
sequence labelling with RNNs. In particular, it focused on applying and
extending the LSTM RNN architecture.
As well as presenting an exact
calculation of LSTM error gradient, we introduced bidirectional LSTM and
demonstrated its advantage over other neural network architectures in a realworld segment classiﬁcation task. We investigated the use of HMM-LSTM
hybrids on a real-world temporal classiﬁcation task. We introduced the connectionist temporal classiﬁcation (CTC) output layer, which allows RNNs to
be trained directly for sequence labelling tasks with unknown input-output
alignments. We presented an eﬃcient decoding algorithm for CTC in the
case where a dictionary and a bigram language model is used to constrain
the outputs.
We found that a CTC network outperformed both HMMs
and HMM-RNN hybrids on several experiments in speech and handwriting
recognition.
Lastly, we discussed the how RNNs can be extended to data with several
spatio-temporal dimensions using multidimensional RNNs (MDRNNs). We
showed how multidirectional MDRNNs are able to access to context along
all input directions, and we introduced multidimensional LSTM, thereby
bringing the beneﬁts of long range conext to MDRNNs. We successfully
applied 2-dimensional LSTM networks to two tasks in image segmentation,
and achieved better generalisation to distorted images than a state-of-the-art
image recognition algorithm.
In the future, we would to like to investigate the use of MDRNNs for
data with more than two spatio-temporal dimensions, such as videos and
sequences of fMRI brain scans. We would also like to combine MDRNNs
with advanced visual preprocessing techniques to see how they compare with
the state-of-the-art in computer vision.
Another direction we would like to pursue is the use of RNNs for hierar-
CHAPTER 9. CONCLUSIONS AND FUTURE WORK
chical sequence learning. In our experiments so far, higher level constraints
such as language models and dictionaries have been applied externally to
the CTC outputs, during decoding. Ideally, however, the network would be
able to learn both high and low level constraints directly from the data. One
way to do this would be to create a hierarchy of RNNs in which the detection of progressively more complex patterns is carried out by progressively
higher level networks. Our preliminary research in this direction has been
encouraging .
Lastly, we would like to investigate the use of RNNs for unsupervised
learning. One straightforward approach would be to use RNNs to predict future inputs for sequential data. This may overlap with hierarchical learning,
since a possible technique for ensuring that complex patterns are eﬃciently
decomposed into multiple levels would be for each level to attempt to predict
the behaviour of the levels above and below it.
Bibliography
G. An. The eﬀects of adding noise during backpropagation training on a
generalization performance. Neural Comput., 8(3):643–674, 1996. ISSN
0899-7667.
B. Bakker.
Reinforcement learning with Long Short-Term Memory.
Advances in Neural Information Processing Systems, 14, 2002.
P. Baldi and G. Pollastri. The principled design of large-scale recursive neural network architectures–dag-rnns and the protein structure prediction
problem. J. Mach. Learn. Res., 4:575–602, 2003. ISSN 1533-7928.
P. Baldi, S. Brunak, P. Frasconi, G. Soda, and G. Pollastri.
Exploiting the past and the future in protein secondary structure prediction. BIOINF: Bioinformatics, 15, 1999. URL citeseer.ist.psu.edu/
article/baldi99exploiting.html.
P. Baldi, S. Brunak, P. Frasconi, G. Pollastri, and G. Soda. Bidirectional
dynamics for protein secondary structure prediction.
Lecture Notes in
Computer Science, 1828:80–104, 2001.
URL citeseer.ist.psu.edu/
baldi99bidirectional.html.
Y. Bengio. A connectionist approach to speech recognition. International
Journal on Pattern Recognition and Artiﬁcial Intelligence, 7(4):647–
668, 1993.
URL 
ijprai93.ps.
Y. Bengio.
Markovian models for sequential data.
Neural Computing
Surveys, 2:129–162, 1999. URL 
pointeurs/hmms.ps.
Y. Bengio and Y. LeCun. Scaling learning algorithms towards ai. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large-Scale Kernel
Machines. MIT Press, 2007.
Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. Global optimization of
a neural network–Hidden Markov Model hybrid. IEEE Transactions on
BIBLIOGRAPHY
Neural Networks, 3(2):252–259, March 1992. URL citeseer.ist.psu.
edu/bengio91global.html.
Y. Bengio, P. Simard, and P. Frasconi.
Learning long-term dependencies with gradient descent is diﬃcult.
IEEE Transactions on Neural
Networks, 5(2):157–166, March 1994.
URL citeseer.ist.psu.edu/
bengio94learning.html.
Y. Bengio, Y. LeCun, C. Nohl, and C. Burges. LeRec: A NN/HMM hybrid
for on-line handwriting recognition. Neural Computation, 7(6):1289–1303,
1995. URL citeseer.ist.psu.edu/bengio95lerec.html.
N. Beringer. Human language acquisition in a machine learning task. Proc.
ICSLP, 2004.
R. Bertolami and H. Bunke. Multiple classiﬁer methods for oﬄine handwritten text line recognition. In 7th International Workshop on Multiple
Classiﬁer Systems, Prague, Czech Republic, 2007.
C. Bishop. Neural Networks for Pattern Recognition. Oxford University
Press, Inc., 1995. ISBN 0198538642.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
L. Bottou and Y. LeCun. Graph transformer networks for image recognition.
In Proceedings of ISI, 2005. (invited paper).
H. Bourlard and N. Morgan. Connnectionist Speech Recognition: A Hybrid
Approach. Kluwer Academic Publishers, 1994.
H. Bourlard, Y. Konig, N. Morgan, and C. Ris. A new training algorithm
for hybrid hmm/ann speech recognition systems. In 8th European Signal
Processing Conference, volume 1, pages 101–104, 1996.
J. S. Bridle. Probabilistic interpretation of feedforward classiﬁcation network outputs, with relationships to statistical pattern recognition.
F. Fogleman-Soulie and J.Herault, editors, Neurocomputing: Algorithms,
Architectures and Applications, pages 227–236. Springer-Verlag, 1990.
D. Broomhead and D. Lowe. Multivariate functional interpolation and adaptive networks. Complex Systems, 2:321–355, 1988.
R. H. Byrd, P. Lu, J. Nocedal, and C. Y. Zhu.
A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16(6):1190–1208, 1995. URL citeseer.ist.psu.edu/
byrd94limited.html.
BIBLIOGRAPHY
J. chang. Near-Miss Modeling: A Segment-Based Approach to Speech Recognition. PhD thesis, Department of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology, 1998.
J. Chen and N. Chaudhari.
Protein secondary structure prediction with
bidirectional lstm networks. In Post-Conference Workshop on Computational Intelligence Approaches for the Analysis of Bio-data (CI-BIO),
Montreal, Canada, August 2005.
J. Chen and N. S. Chaudhari. Capturing long-term dependencies for protein
secondary structure prediction. In F. Yin, J. Wang, and C. Guo, editors,
Advances in Neural Networks - ISNN 2004, International Symposiumon
Neural Networks, Part II, volume 3174 of Lecture Notes in Computer
Science, pages 494–500, Dalian, China, 2004. Springer. ISBN 3-540-22843-
R. Chen and L. Jamieson. Experiments on the implementation of recurrent
neural networks for speech phone recognition. In Proceedings of the Thirtieth Annual Asilomar Conference on Signals, Systems and Computers,
pages 779–782, 1996.
D. Decoste and B. Sch¨olkopf. Training invariant support vector machines.
Machine Learning, 46(1-3):161–190, 2002. URL citeseer.ist.psu.edu/
decoste02training.html.
R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. Wiley-
Interscience Publication, 2000. ISBN 0471056693.
D. Eck and J. Schmidhuber. Finding temporal structure in music: Blues
improvisation with LSTM recurrent networks.
In H. Bourlard, editor,
Neural Networks for Signal Processing XII, Proceedings of the 2002 IEEE
Workshop, pages 747–756, New York, 2002. IEEE.
J. L. Elman. Finding structure in time. Cognitive Science, 14:179–211, 1990.
S. Fahlman. Faster learning variations on back-propagation: An empirical
study. In D. Touretzky, G. Hinton, and T. Sejnowski, editors, Proceedings
of the 1988 connectionist models summer school, pages 38–51, San Mateo,
1989. Morgan Kaufmann.
S. Fern´andez, A. Graves, and J. Schmidhuber. An application of recurrent
neural networks to discriminative keyword spotting.
In Proceedings of
the 2007 International Conference on Artiﬁcial Neural Networks, Porto,
Portugal, September 2007a.
S. Fern´andez, A. Graves, and J. Schmidhuber. Sequence labelling in structured domains with hierarchical recurrent neural networks. In Proceedings
BIBLIOGRAPHY
of the 20th International Joint Conference on Artiﬁcial Intelligence, IJ-
CAI 2007, Hyderabad, India, 2007b.
T. Fukada, M. Schuster, and Y. Sagisaka. Phoneme boundary estimation
using bidirectional recurrent neural networks and its applications. Systems
and Computers in Japan, 30(4):20–30, 1999.
J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, , and
N. L. Dahlgren. Darpa timit acoustic phonetic continuous speech corpus
cdrom, 1993.
F. Gers. Long Short-Term Memory in Recurrent Neural Networks. PhD
thesis, Ecole Polytechnique F´ed´erale de Lausanne, 2001. URL citeseer.
nj.nec.com/article/gers01long.html.
F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing with
LSTM recurrent networks.
Journal of Machine Learning Research, 3:
115–143, 2002.
F. A. Gers and J. Schmidhuber. LSTM recurrent networks learn simple context free and context sensitive languages. IEEE Transactions on Neural
Networks, 12(6):1333–1340, 2001.
F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual
prediction with LSTM. Neural Computation, 12(10):2451–2471, 2000.
C. Giraud-Carrier, R. Vilalta, and P. Brazdil. Introduction to the special
issue on meta-learning. Mach. Learn., 54(3):187–193, 2004. ISSN 0885-
J. R. Glass. A probabilistic framework for segment-based speech recognition.
Computer Speech and Language, 17:137–152, 2003.
A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with bidirectional LSTM networks. In Proceedings of the 2005 International Joint
Conference on Neural Networks, Montreal, Canada, 2005a.
A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures. Neural Networks,
18(5-6):602–610, June/July 2005b.
A. Graves, N. Beringer, and J. Schmidhuber. Rapid retraining on speech
data with lstm recurrent networks. Technical Report IDSIA-09-05, IDSIA,
www.idsia.ch/techrep.html, 2005a.
A. Graves, S. Fern´andez, and J. Schmidhuber. Bidirectional LSTM networks
for improved phoneme classiﬁcation and recognition. In Proceedings of the
2005 International Conference on Artiﬁcial Neural Networks, Warsaw,
Poland, 2005b.
BIBLIOGRAPHY
A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber. Connectionist
temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the International Conference on
Machine Learning, ICML 2006, Pittsburgh, USA, 2006.
A. Graves, S. Fern´andez, and J. Schmidhuber. Multi-dimensional recurrent
neural networks. In Proceedings of the 2007 International Conference on
Artiﬁcial Neural Networks, Porto, Portugal, September 2007.
A. Graves, S. Fern´andez, M. Liwicki, H. Bunke, and J. Schmidhuber. Unconstrained online handwriting recognition with recurrent neural networks.
In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in
Neural Information Processing Systems 20. MIT Press, Cambridge, MA,
A. Graves, M. Liwicki, S. Fern´andez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 2008b. To appear.
A. K. Halberstadt. Heterogeneous acoustic measurements and multiple classiﬁers for speech recognition. PhD thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology,
B. Hammer. On the approximation capability of recurrent neural networks.
Neurocomputing, 31(1-4):107–123, 2000.
J. Hennebert, C. Ris, H. Bourlard, S. Renals, and N. Morgan.
Estimation of global posteriors and forward-backward training of hybrid HMM/ANN systems.
In Proc. of the European Conference on
Speech Communication and Technology (Eurospeech 97), pages 1951–1954,
Rhodes, 1997. URL 
eurosp97-remap.html.
M. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving
linear systems. Journal of Research of the National Bureau of Standards,
49(6):409–436, 1952.
G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep
belief nets. Neural Computation, 18(7):1527–1554, 2006. ISSN 0899-7667.
S. Hochreiter.
Untersuchungen zu dynamischen neuronalen Netzen. PhD
thesis, Institut f¨ur Informatik, Technische Universit¨at M¨unchen, 1991. See
 
S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–1780, 1997.
BIBLIOGRAPHY
S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in
recurrent nets: the diﬃculty of learning long-term dependencies. In S. C.
Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent
Neural Networks. IEEE Press, 2001.
S. Hochreiter, M. Heusel, and K. Obermayer. Fast Model-based Protein
Homology Detection without Alignment. Bioinformatics, 2007.
J. J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. PNAS, 79(8):2554–2558, April 1982.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks
are universal approximators.
Neural Netw., 2(5):359–366, 1989.
0893-6080. doi: 
F. H¨ulsken, F. Wallhoﬀ, and G. Rigoll. Facial expression recognition with
pseudo-3d hidden markov models.
In Proceedings of the 23rd DAGM-
Symposium on Pattern Recognition, pages 291–297, London, UK, 2001.
Springer-Verlag. ISBN 3-540-42596-9.
H. Jaeger. The ”echo state” approach to analysing and training recurrent
neural networks. Technical Report GMD Report 148, German National
Research Center for Information Technology, 2001. URL 
faculty.iu-bremen.de/hjaeger/pubs/EchoStatesTechRep.pdf.
J. Jiten, B. M´erialdo, and B. Huet.
Multi-dimensional dependency-tree
hidden Markov models. In ICASSP 2006, 31st IEEE International Conference on Acoustics, Speech, and Signal Processing, May 14-19, 2006,
Toulouse, France, May 2006.
 
opac?punumber=11024.
S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus
user’s manual; Norwegian Computing Centre for the Humanities, 1986.
M. T. Johnson. Capacity and complexity of HMM duration modeling techniques. IEEE Signal Processing Letters, 12(5):407–410, May 2005.
M. I. Jordan. Attractor dynamics and parallelism in a connectionist sequential machine, pages 112–127. IEEE Press, Piscataway, NJ, USA, 1990.
ISBN 0-8186-2015-3.
D. Joshi, J. Li, and J. Wang. Parameter estimation of multi-dimensional
hidden markov models: A scalable approach. In Proc. of the IEEE International Conference on Image Processing (ICIP05), pages III: 149–152,
BIBLIOGRAPHY
M. W. Kadous.
Temporal Classiﬁcation:
Extending the Classiﬁcation
Paradigm to Multivariate Time Series.
PhD thesis, School of Computer Science & Engineering, University of New South Wales, 2002. URL
 
D. Kershaw,
A. Robinson,
and M. Hochberg.
Context-dependent
classes in a hybrid recurrent network-HMM speech recognition system.
In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8,
pages 750–756. The MIT Press, 1996.
URL citeseer.ist.psu.edu/
kershaw95contextdependent.html.
T. Kohonen.
Self-organization and associative memory:
3rd edition.
Springer-Verlag New York, Inc., New York, NY, USA, 1989.
387-51387-6.
P. Koistinen and L. Holmstr¨om.
Kernel regression and backpropagation
training with noise. In J. E. Moody, S. J. Hanson, and R. Lippmann,
editors, Advances in Neural Information Processing Systems, 4, pages
1033–1039. Morgan Kaufmann, 1991.
URL 
de/db/conf/nips/nips1991.html#KoistinenH91.
L. Lamel and J. Gauvain. High performance speaker-independent phone
recognition using cdhmm. In Proc. Eurospeech, Berlin, Germany, September 1993.
K. J. Lang, A. H. Waibel, and G. E. Hinton. A time-delay neural network
architecture for isolated word recognition. Neural Netw., 3(1):23–43, 1990.
ISSN 0893-6080. doi: 
Y. LeCun, L. Bottou, and Y. Bengio. Reading checks with graph transformer
networks. In International Conference on Acoustics, Speech, and Signal
Processing, volume 1, pages 151–154, Munich, 1997. IEEE.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278–
2324, November 1998a.
Y. LeCun, L. Bottou, G. Orr, and K. Muller. Eﬃcient backprop. In G. Orr
and M. K., editors, Neural Networks: Tricks of the trade. Springer, 1998b.
K.-F. Lee and H.-W. Hon. Speaker-independent phone recognition using
hidden Markov models.
IEEE Transactions on Acoustics, Speech, and
Signal Processing, 37(11):1641–1648, November 1989.
J. Li, A. Najmi, and R. M. Gray. Image classiﬁcation by a two-dimensional
hidden markov model. IEEE Transactions on Signal Processing, 48(2):
517–533, 2000. URL citeseer.ist.psu.edu/article/li98image.html.
BIBLIOGRAPHY
T. Lin, B. G. Horne, P. Ti˜no, and C. L. Giles.
Learning long-term dependencies in NARX recurrent neural networks. IEEE Transactions on
Neural Networks, 7(6):1329–1338, November 1996. URL citeseer.ist.
psu.edu/lin96learning.html.
T. Lindblad and J. M. Kinser. Image Processing Using Pulse-Coupled Neural
Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.
ISBN 354024218X.
M. Liwicki and H. Bunke. Handwriting recognition of whiteboard notes. In
Proc. 12th Conf. of the International Graphonomics Society, pages 118–
122, 2005a.
M. Liwicki and H. Bunke. IAM-OnDB - an on-line English sentence database
acquired from handwritten text on a whiteboard. In Proc. 8th Int. Conf.
on Document Analysis and Recognition, volume 2, pages 956–961, 2005b.
M. Liwicki, A. Graves, S. Fern´andez, H. Bunke, and J. Schmidhuber. A
novel approach to on-line handwriting recognition based on bidirectional
long short-term memory networks.
In Proceedings of the 9th International Conference on Document Analysis and Recognition, ICDAR 2007,
Curitiba, Brazil, September 2007.
D. MacKay. Probable networks and plausible predictions - a review of practical bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6:469–505, 1995. URL citeseer.ist.psu.
edu/64617.html.
U.-V. Marti and H. Bunke. Using a statistical language model to improve the
performance of an HMM-based cursive handwriting recognition system.
Int. Journal of Pattern Recognition and Artiﬁcial Intelligence, 15:65–90,
U.-V. Marti and H. Bunke.
The IAM-database:
an English sentence
database for oﬄine handwriting recognition.
International Journal on
Document Analysis and Recognition, 5:39 – 46, 2002.
G. McCarter and A. Storkey. Air Freight Image Segmentation Database.
 2007.
W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in
nervous activity, pages 15–27. MIT Press, Cambridge, MA, USA, 1988.
ISBN 0-262-01097-6.
J. Ming and F. J. Smith.
Improved Phone Recognition Using Bayesian
Triphone Models. In ICASSP, volume 1, pages 409–412, 1998.
BIBLIOGRAPHY
M. C. Mozer.
Induction of multiscale temporal structure.
Moody, S. J. Hanson, and R. P. Lippmann, editors, Advances in Neural Information Processing Systems, volume 4, pages 275–282. Morgan Kaufmann Publishers, Inc., 1992.
URL citeseer.ist.psu.edu/
mozer92induction.html.
G. Navarro. A guided tour to approximate string matching. ACM Computing
Surveys, 33(1):31–88, 2001.
R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New
York, Inc., Secaucus, NJ, USA, 1996. ISBN 0387947248.
J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, and
A. Robinson. Speaker adaptation for hybrid hmm-ann continuous speech
recognition system. In Proceedings of Eurospeech 1995, volume 1, pages
2171–2174, 1995.
X. Pang and P. J. Werbos. Neural network design for J function approximation in dynamic programming. Mathematical Modeling and Scientiﬁc
Computing, 5(2/3), 1996. URL 
html. Also available at 
T. A. Plate. Holographic recurrent networks. In C. L. Giles, S. J. Hanson,
and J. D. Cowan, editors, Advances in Neural Information Processing
Systems 5: NIPS * 92, Denver, CO, November 1992, pages 34–41, San
Mateo, CA, 1993. Morgan Kaufmann.
URL citeseer.ist.psu.edu/
plate93holographic.html.
D. C. Plaut, S. J. Nowlan, and G. E. Hinton.
Experiments on learning
back propagation. Technical Report CMU–CS–86–126, Carnegie–Mellon
University, Pittsburgh, PA, 1986.
G. Pollastri, A. Vullo, P. Frasconi, and P. Baldi. Modular dag-rnn architectures for assembling coarse protein structures. J Comput Biol, 13(3):
631–650, April 2006.
ISSN 1066-5277.
doi: 10.1089/cmb.2006.13.631.
URL 
L. R. Rabiner. A tutorial on hidden markov models and selected applications
in speech recognition. Proc. IEEE, 77(2):257–286, 1989.
S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco. Connectionist probability estimators in HMM speech recognition. IEEE Transactions Speech and Audio Processing, 1993. URL citeseer.ist.psu.edu/
renals94connectionist.html.
M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Proc. of the IEEE Intl.
BIBLIOGRAPHY
Conf. on Neural Networks, pages 586–591, San Francisco, CA, 1993. URL
citeseer.ist.psu.edu/riedmiller93direct.html.
A. Robinson, J. Holdsworth, J. Patterson, and F. Fallside. A comparison
of preprocessors for the cambridge recurrent error propagation network
speech recognition system. In Proceedings of the First International Conference on Spoken Language Processing, ICSLP-1990, pages Kobe, Japan,
Vietri (Italy), 1990.
A. J. Robinson.
Several improvements to a recurrent error propagation network phone recognition system.
Technical Report CUED/F-
INFENG/TR82, University of Cambridge, September 1991.
A. J. Robinson. An application of recurrent nets to phone probability estimation. IEEE Transactions on Neural Networks, 5(2):298–305, March
1994. URL citeseer.nj.nec.com/robinson94application.html.
A. J. Robinson and F. Fallside. The utility driven dynamic error propagation network. Technical Report CUED/F-INFENG/TR.1, Cambridge
University Engineering Department, 1987.
A. J. Robinson, L. Almeida, J.-M. Boite, H. Bourlard, F. Fallside,
M. Hochberg, D. Kershaw, P. Kohn, Y. Konig, N. Morgan, J. P. Neto,
S. Renals, M. Saerens, and C. Wooters. A neural network based, speaker
independent, large vocabulary, continuous speech recognition system:
the Wernicke project.
In Proc. of the Third European Conference on
Speech Communication and Technology (Eurospeech 93), pages 1941–1944,
Berlin, 1993.
F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, 1963.
F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65:386–408, 1958.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation, pages 318–362. MIT Press, Cambridge,
MA, USA, 1986. ISBN 0-262-68053-X.
S. Russell and P. Norvig.
Artiﬁcial Intelligence:
A Modern Approach.
Prentice-Hall, Englewood Cliﬀs, NJ, 2nd edition edition, 2003.
J. Schmidhuber. Learning complex extended sequences using the principle
of history compression.
Neural Computing, 4(2):234–242, 1992.
citeseer.ist.psu.edu/article/schmidhuber92learning.html.
J. Schmidhuber, D. Wierstra, M. Gagliolo, and F. Gomez. Training recurrent
networks by evolino.
Neural Computation, 19(3):757–779, 2007.
0899-7667.
BIBLIOGRAPHY
N. Schraudolph.
Fast curvature matrix-vector products for second-order
gradient descent. Neural Computation, 14(7):1723–1738, 2002.
M. Schuster. On supervised learning from sequential data with applications
for speech recognition. PhD thesis, Nara Institute of Science and Technolog, Kyoto, Japan, 1999.
M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks.
IEEE Transactions on Signal Processing, 45:2673–2681, November 1997.
A. Senior and A. J. Robinson. Forward-backward retraining of recurrent
neural networks. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo,
editors, Advances in Neural Information Processing Systems, volume 8,
pages 743–749. The MIT Press, 1996.
URL citeseer.ist.psu.edu/
senior96forwardbackward.html.
F. Sha and L. K. Saul. Large margin hidden markov models for automatic
speech recognition. In NIPS, pages 1249–1256, 2006.
J. R. Shewchuk. An introduction to the conjugate gradient method without the agonizing pain.
Technical report, Carnegie Mellon University,
Pittsburgh, PA, USA, 1994.
P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR
’03: Proceedings of the Seventh International Conference on Document
Analysis and Recognition, page 958, Washington, DC, USA, 2003. IEEE
Computer Society. ISBN 0-7695-1960-1.
T. Thireou and M. Reczko.
Bidirectional long short-term memory networks for predicting the subcellular localization of eukaryotic proteins.
IEEE/ACM Trans. Comput. Biol. Bioinformatics, 4(3):441–446, 2007.
ISSN 1545-5963.
E. Trentin and M. Gori. Robust combination of neural networks and hidden
markov models for speech recognition. Neural Networks, IEEE Transactions onn, 14(6):1519–1531, 2003.
V. N. Vapnik. The nature of statistical learning theory. Springer-Verlag New
York, Inc., New York, NY, USA, 1995. ISBN 0-387-94559-8.
Verbmobil. Database version 2.3, March 2004. 
P. Werbos. Backpropagation through time: What it does and how to do it.
Proceedings of the IEEE, 78(10):1550 – 1560, 1990.
P. J. Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, 1, 1988.
BIBLIOGRAPHY
D. Wierstra, F. J. Gomez, and J. Schmidhuber.
Modeling systems with
internal state using evolino.
In GECCO ’05: Proceedings of the 2005
conference on Genetic and evolutionary computation, pages 1795–1802,
New York, NY, USA, 2005. ACM Press. ISBN 1-59593-010-8. doi: http:
//doi.acm.org/10.1145/1068009.1068315.
R. J. Williams and D. Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity.
In Y. Chauvin
and D. E. Rumelhart, editors, Back-propagation: Theory, Architectures
and Applications, pages 433–486. Lawrence Erlbaum Publishers, Hillsdale,
N.J., 1995.
URL citeseer.nj.nec.com/williams95gradientbased.
L. Wu and P. Baldi.
A scalable machine learning approach to go.
B. Schlkopf, J. Platt, and T. Hoﬀman, editors, NIPS, pages 1521–1528.
MIT Press, 2006. ISBN 0-262-19568-2. URL 
de/db/conf/nips/nips2006.html#WuB06.
S. Young and P. Woodland. HTK Version 3.2: User, Reference and Programmer Manual, 2002.
S. Young, N. Russell, and J. Thornton.
Token passing: A simple conceptual model for connected speech recognition systems. Technical Report CUED/F-INFENG/TR38, Cambridge University Engineering Dept.,
Cambridge, UK, 1989.
S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland. The HTK
Book. Cambridge University Engineering Department, HTK version 3.4
edition, December 2006.
D. Yu, L. Deng, and A. Acero.
A lattice search technique for a longcontextual-span hidden trajectory model of speech.
Speech Communication, 48(9):1214–1226, 2006.
URL 
journals/speech/speech48.html#YuDA06.
G. Zavaliagkos, S. Austin, J. Makhoul, and R. M. Schwartz.
continuous speech recognition system using segmental neural nets with
hidden markov models. IJPRAI, 7(4):949–963, 1993.
H. G. Zimmermann, R. Grothmann, A. M. Schaefer, and C. Tietz. Identiﬁcation and forecasting of large dynamical systems by dynamical consistent neural networks.
In S. Haykin, J. Principe, T. Sejnowski, and
J. McWhirter, editors, New Directions in Statistical Signal Processing:
From Systems to Brain, pages 203–242. MIT Press, 2006a.