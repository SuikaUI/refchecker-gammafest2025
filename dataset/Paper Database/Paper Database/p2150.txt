Cost function dependent barren plateaus in shallow
parametrized quantum circuits
1,2✉, Akira Sone1,2, Tyler Volkoff1, Lukasz Cincio1 & Patrick J. Coles1✉
Variational quantum algorithms (VQAs) optimize the parameters θ of a parametrized
quantum circuit V(θ) to minimize a cost function C. While VQAs may enable practical
applications of noisy quantum computers, they are nevertheless heuristic methods with
unproven scaling. Here, we rigorously prove two results, assuming V(θ) is an alternating
layered ansatz composed of blocks forming local 2-designs. Our ﬁrst result states that
deﬁning C in terms of global observables leads to exponentially vanishing gradients (i.e.,
barren plateaus) even when V(θ) is shallow. Hence, several VQAs in the literature must
revise their proposed costs. On the other hand, our second result states that deﬁning C with
local observables leads to at worst a polynomially vanishing gradient, so long as the depth
of V(θ) is Oðlog nÞ. Our results establish a connection between locality and trainability.
We illustrate these ideas with large-scale simulations, up to 100 qubits, of a quantum
autoencoder implementation.
 
1 Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM, USA. 2 Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos,
NM, USA. ✉email: ; 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
1234567890():,;
ne of the most important technological questions is
whether Noisy Intermediate-Scale Quantum (NISQ)
computers will have practical applications1. NISQ devices
are limited both in qubit count and in gate ﬁdelity, hence
preventing the use of quantum error correction.
The leading strategy to make use of these devices is variational
quantum algorithms (VQAs)2. VQAs employ a quantum computer to efﬁciently evaluate a cost function C, while a classical
optimizer trains the parameters θ of a Parametrized Quantum
Circuit (PQC) V(θ). The beneﬁts of VQAs are three-fold. First,
VQAs allow for task-oriented programming of quantum computers, which is important since designing quantum algorithms is
non-intuitive. Second, VQAs make up for small qubit counts by
leveraging classical computational power. Third, pushing complexity onto classical computers, while only running short-depth
quantum circuits, is an effective strategy for error mitigation on
NISQ devices.
There are very few rigorous scaling results for VQAs (with
exception of one-layer approximate optimization3–5). Ideally, in
order to reduce gate overhead that arises when implementing on
quantum hardware one would like to employ a hardware-efﬁcient
ansatz6 for V(θ). As recent large-scale implementations for
chemistry7 and optimization8 applications have shown, this
ansatz leads to smaller errors due to hardware noise. However,
one of the few known scaling results is that deep versions of
randomly initialized hardware-efﬁcient ansatzes lead to exponentially vanishing gradients9. Very little is known about the
scaling of the gradient in such ansatzes for shallow depths, and it
would be especially useful to have a converse bound that guarantees non-exponentially vanishing gradients for certain depths.
This motivates our work, where we rigorously investigate the
gradient scaling of VQAs as a function of the circuit depth.
The other motivation for our work is the recent explosion in
the number of proposed VQAs. The Variational Quantum
Eigensolver (VQE) is the most famous VQA. It aims to prepare
the ground state of a given Hamiltonian H = ∑αcασα, with H
expanded as a sum of local Pauli operators10. In VQE, the cost
function is obviously the energy C ¼ ψjHjψ
i of the trial state
j i. However, VQAs have been proposed for other applications,
compression11,
correction12, quantum metrology13, quantum compiling14–17,
quantum state diagonalization18,19, quantum simulation20–23,
ﬁdelity estimation24, unsampling25, consistent histories26, and
linear systems27–29. For these applications, the choice of C is
less obvious. Put another way, if one reformulates these VQAs
as ground-state problems (which can be done in many cases),
the choice of Hamiltonian H is less intuitive. This is because
many of these applications are abstract, rather than associated
with a physical Hamiltonian.
We remark that polynomially vanishing gradients imply that
the number of shots needed to estimate the gradient should grow
as Oðpoly ðnÞÞ. In contrast, exponentially vanishing gradients
(i.e., barren plateaus) imply that derivative-based optimization
will have exponential scaling30, and this scaling can also apply to
derivative-free optimization31. Assuming a polynomial number of
shots per optimization step, one will be able to resolve against
ﬁnite sampling noise and train the parameters if the gradients
vanish polynomially. Hence, we employ the term “trainable” for
polynomially vanishing gradients.
In this work, we connect the trainability of VQAs to the choice
of C. For the abstract applications in refs. 11–29, it is important for
C to be operational, so that small values of C imply that the task is
almost accomplished. Consider an example of state preparation,
where the goal is to ﬁnd a gate sequence that prepares a target
. A natural cost function is the square of the trace
j i ¼ VðθÞy 0j i,
CG ¼ DTð ψ0
2, which is equivalent to
CG ¼ Tr½OGVðθÞ ψ0
VðθÞy ;
with OG ¼ 1  0j i 0
h j. Note that
i  ψ0jMjψ0
has a nice operational meaning as a bound on the expectation value
difference for a POVM element M.
However, here we argue that this cost function and others like
it exhibit exponentially vanishing gradients. Namely, we consider
global cost functions, where one directly compares states or
operators living in exponentially large Hilbert spaces (e.g., ψ
). These are precisely the cost functions that have
operational meanings for tasks of interest, including all tasks in
refs. 11–29. Hence, our results imply that a non-trivial subset of
these references will need to revise their choice of C.
Interestingly, we demonstrate vanishing gradients for shallow
PQCs. This is in contrast to McClean et al.9, who showed vanishing gradients for deep PQCs. They noted that randomly
initializing θ for a V(θ) that forms a 2-design leads to a barren
plateau, i.e., with the gradient vanishing exponentially in the
number of qubits, n. Their work implied that researchers must
develop either clever parameter initialization strategies32,33 or
clever PQCs ansatzes4,34,35. Similarly, our work implies that
researchers must carefully weigh the balance between trainability
and operational relevance when choosing C.
While our work is for general VQAs, barren plateaus for global
cost functions were noted for speciﬁc VQAs and for a very speciﬁc tensor-product example by our research group14,18, and
more recently in29. This motivated the proposal of local cost
functions14,16,18,22,25–27, where one compares objects (states or
operators) with respect to each individual qubit, rather than in a
global sense, and therein it was shown that these local cost
functions have indirect operational meaning.
Our second result is that these local cost functions have
gradients that vanish polynomially rather than exponentially in n,
and hence have the potential to be trained. This holds for V(θ)
with depth Oðlog nÞ. Figure 1 summarizes our two main results.
Finally, we illustrate our main results for an important example: quantum autoencoders11. Our large-scale numerics show that
the global cost function proposed in11 has a barren plateau. On
the other hand, we propose a novel local cost function that is
trainable, hence making quantum autoencoders a scalable
application.
Warm-up example. To illustrate cost-function-dependent barren
plateaus, we ﬁrst consider a toy problem corresponding to the
state preparation problem in the Introduction with the target
state being 0j i. We assume a tensor-product ansatz of the form
j¼1eiθjσðjÞ
x =2, with the goal of ﬁnding the angles θj such
that VðθÞ 0j i ¼ 0j i. Employing the global cost of (1) results in
CG ¼ 1  Qn
j¼1 cos2 θj
2. The barren plateau can be detected via the
variance of its gradient: Var½∂CG
8Þn1, which is exponentially vanishing in n. Since the mean value is
gradient concentrates exponentially around zero.
On the other hand, consider a local cost function:
CL ¼ Tr OLVðθÞ 0j i 0
OL ¼ 1  1
j¼1 0j i 0
h jj  1j ;
where 1j is the identity on all qubits except qubit j. Note that CL
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
vanishes under the same conditions as CG14,16, CL = 0 ⇔CG = 0.
We ﬁnd CL ¼ 1  1
j¼1 cos2 θj
2, and the variance of its gradient is
8n2, which vanishes polynomially with n and hence
exhibits no barren plateau. Figure 2 depicts the cost landscapes of
CG and CL for two values of n and shows that the barren plateau
can be avoided here via a local cost function.
Moreover, this example allows us to delve deeper into the
cost landscape to see a phenomenon that we refer to as a
narrow gorge. While a barren plateau is associated with a ﬂat
landscape, a narrow gorge refers to the steepness of the valley
that contains the global minimum. This phenomenon is
illustrated in Fig. 2, where each dot corresponds to cost values
obtained from randomly selected parameters θ. For CG we see
that very few dots fall inside the narrow gorge, while for CL the
narrow gorge is not present. Note that the narrow gorge
makes it harder to train CG since the learning rate of descentbased optimization algorithms must be exponentially small in
order not to overstep the narrow gorge. The following
proposition (proved in the Supplementary Note 2) formalizes
characterizing the dependence on n of the probability C ⩽δ.
This probability is associated with the parameter space volume
that leads to C ⩽δ.
Proposition 1
Let θj be uniformly distributed on [−π, π] ∀j. For any δ ∈(0, 1),
the probability that CG ≤δ satisﬁes
PrfCG ≤δg ≤ð1  δÞ1 1
For any δ 2 ½1
2 ; 1, the probability that CL ≤δ satisﬁes
PrfCL ≤δg ≥
2n þ ð2δ  1Þ2 !
General framework. For our general results, we consider a family
of cost functions that can be expressed as the expectation value of
an operator O as follows
C ¼ Tr OVðθÞρVyðθÞ
where ρ is an arbitrary quantum state on n qubits. Note that this
framework includes the special case where ρ could be a pure state,
as well as the more special case where ρ ¼ 0j i 0
h j, which is the
input state for many VQAs such as VQE. Moreover, in VQE, one
chooses O = H, where H is the physical Hamiltonian. In general,
the choice of O and ρ essentially deﬁnes the application of interest
of the particular VQA.
It is typical to express O as a linear combination of the form
O ¼ c01 þ ∑N
i¼1 ciOi. Here Oi ≠1, ci 2 R, and we assume that at
least one ci ≠0. Note that CG and CL in (1) and (2) fall under this
framework. In our main results below, we will consider two
different choices of O that respectively capture our general
notions of global and local cost functions and also generalize the
aforementioned CG and CL.
As shown in Fig. 3a, V(θ) consists of L layers of m-qubit
unitaries Wkl(θkl), or blocks, acting on alternating groups of m
neighboring qubits. We refer to this as an Alternating Layered
Ansatz. We remark that the Alternating Layered Ansatz will be a
hardware-efﬁcient ansatz so long as the gates that compose each
block are taken from a set of gates native to a speciﬁc device. As
depicted in Fig. 3c, the one dimensional Alternating Layered
Ansatz can be readily implemented in devices with onedimensional connectivity, as well as in devices with twodimensional connectivity (such as that of IBM’s36 and Google’s37
quantum devices). That is, with both one- and two-dimensional
hardware connectivity one can group qubits to form an
Alternating Layered Ansatz as in Fig. 3a.
The index l = 1, …, L in Wkl(θkl) indicates the layer that
contains the block, while k = 1, …, ξ indicates the qubits it acts
upon. We assume n is a multiple of m, with n = mξ, and that m
does not scale with n. As depicted in Fig. 3a, we deﬁne Sk as the
m-qubit subsystem on which WkL acts, and we deﬁne S ¼ fSkg as
the set of all such subsystems. Let us now consider a block
Wkl(θkl) in the lth layer of the ansatz. For simplicity we
henceforth use W to refer to a given Wkl(θkl). As shown in the
Methods section, given a θν ∈θkl that parametrizes a rotation
eiθνσν=2 (with σν a Pauli operator) inside a given block W, one
can always express
∂θν :¼ ∂νW ¼ i
where WA and WB contain all remaining gates in W, and are
properly deﬁned in the Methods section.
The contribution to the gradient ∇C from a parameter θν in the
block W is given by the partial derivative ∂νC. While the value of
∂νC depends on the speciﬁc parameters θ, it is useful to compute
h∂νCiV, i.e., the average gradient over all possible unitaries V(θ)
within the ansatz. Such an average may not be representative near
Fig. 1 Summary of our main results. McClean et al.9 proved that a barren
plateau can occur when the depth D of a hardware-efﬁcient ansatz is
D 2 Oðpoly ðnÞÞ. Here we extend these results by providing bounds for the
variance of the gradient of global and local cost functions as a function of D.
In particular, we ﬁnd that the barren plateau phenomenon is cost-function
dependent. a For global cost functions (e.g., Eq. (1)), the landscape will
exhibit a barren plateau essentially for all depths D. b For local cost
functions (e.g., Eq. (2)), the gradient vanishes at worst polynomially and
hence is trainable when D 2 Oðlog ðnÞÞ, while barren plateaus occur for
D 2 Oðpoly ðnÞÞ, and between these two regions the gradient transitions
from polynomial to exponential decay.
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
the minimum of C, although it does provide a good estimate of
the expected gradient when randomly initializing the angles in V
(θ). In the Methods Section we explicitly show how to compute
averages of the form 〈…〉V, and in the Supplementary Note 3 we
provide a proof for the following Proposition.
Proposition 2
The average of the partial derivative of any cost function of the
form (6) with respect to a parameter θν in a block W of the ansatz
in Fig. 3 is
h∂νCiV ¼ 0 ;
provided that either WA or WB of (7) form a 1-design.
Here we recall that a t-design is an ensemble of unitaries, such
that sampling over their distribution yields the same properties as
sampling random unitaries from the unitary group with respect
to the Haar measure up to the ﬁrst t moments38. The Methods
section provides a formal deﬁnition of a t-design.
Proposition 2 states that the gradient is not biased in any
particular direction. To analyze the trainability of C, we consider
the second moment of its partial derivatives:
Var½∂νC ¼
where we used the fact that h∂νCiV ¼ 0. The magnitude of Var
[∂νC] quantiﬁes how much the partial derivative concentrates
around zero, and hence small values in (9) imply that the slope of
the landscape will typically be insufﬁcient to provide a costminimizing direction. Speciﬁcally, from Chebyshev’s inequality,
Var[∂νC] bounds the probability that the cost-function partial
derivative
Pr j∂νCj ≥c
Þ ≤Var½∂νC=c2 for all c > 0.
Main results. Here we present our main theorems and corollaries,
with the proofs sketched in the Methods and detailed in the
Supplementary Information. In addition, in the Methods section
we provide some intuition behind our main results by analyzing a
generalization of the warm-up example where V(θ) is composed
of a single layer of the ansatz in Fig. 3. This case bridges the gap
between the warm-up example and our main theorems and also
showcases the tools used to derive our main result.
The following theorem provides an upper bound on the
variance of the partial derivative of a global cost function which
can be expressed as the expectation value of an operator of the
O ¼ c01 þ ∑
i¼1 cibOi1  bOi2      bOiξ :
Speciﬁcally, we consider two cases of interest: (i) When N = 1 and
each bO1k is a non-trivial projector (bO
1k ¼ bO1k≠1) of rank rk
acting on subsystem Sk, or (ii) When N is arbitrary and bOik is
traceless with Tr½bO
ik ≤2m (for example, when bOik¼ Nm
tensor product of Pauli operators σμ
j 2 f1j; σx
j g, with at
least one σμ
j ≠1). Note that case (i) includes CG of (1) as a
special case.
Consider a trainable parameter θν in a block W of the ansatz in
Fig. 3. Let Var[∂νC] be the variance of the partial derivative of a
global cost function C (with O given by (10)) with respect to θν. If
Fig. 3 Alternating Layered Ansatz. a Each block Wkl acts on m qubits and
is parametrized via (27). As shown, we deﬁne Sk as the m-qubit subsystem
on which WkL acts, where L is the last layer of V(θ). Given some block W,
it is useful for our proofs (outlined in the Methods) to write
VðθÞ ¼ VRð1w  WÞVL, where VR contains all gates in the forward lightcone L of W. The forward light-cone L is deﬁned as all gates with at least
one input qubit causally connected to the output qubits of W. We deﬁne L
as the compliment of L, Sw as the m-qubit subsystem on which W acts, and
Sw as the n −m qubit subsystem on which W acts trivially. b The operator
Oi acts nontrivially only in subsystem Sk1 2 S, while Oi0 acts nontrivially on
the ﬁrst m/2 qubits of Sk+1, and on the second m/2 qubits of Sk. c Depiction
of the Alternating Layered Ansatz with one- and two-dimensional
connectivity. Each circle represents a physical qubit.
Fig. 2 Cost function landscapes. a Two-dimensional cross-section through the landscape of CG ¼ 1  Qn
j¼1 cos2ðθj=2Þ for n = 4 (blue) and n = 24
(orange). b The same cross-section through the landscape of CL ¼ 1  1
j¼1 cos2ðθj=2Þ is independent of n. In both cases, 200 Haar distributed points are
shown, with very few (most) of these points lying in the valley containing the global minimum of CG (CL).
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
WA, WB of (7), and each block in V(θ) form a local 2-design, then
Var[∂νC] is upper bounded by
Var½∂νC ⩽FnðL; lÞ :
For N = 1 and when each bO1k is a non-trivial projector,
then deﬁning R ¼ Qξ
k, we have
FnðL; lÞ ¼
22mþð2m1ÞðLlÞ
ð22m  1Þ  3
For arbitrary N and when each bOik satisﬁes Tr½bOik ¼ 0 and
ik ⩽2m, then
FnðL; lÞ ¼ 22mðLlþ1Þþ1
m  2 3 4
i;j¼1 cicj :
From Theorem 1 we derive the following corollary.
Corollary 1
Consider the function Fn(L, l).
Let N = 1 and let each bO1k be a non-trivial projector, as in
case (i) of Theorem 1. If c2
1R 2 Oð2nÞ and if the number of
layers L 2 Oðpoly ðlog ðnÞÞÞ, then
Þ 2 O 2 11
which implies that Var[∂νC] is exponentially vanishing in n
Let N be arbitrary, and let each bOik satisfy Tr½bOik ¼ 0 and
ik ⩽2m, as in case (ii) of Theorem 1. If N 2 Oð2nÞ,
ci 2 Oð1Þ,
L 2 Oðpoly ðlog ðnÞÞÞ, then
which implies that Var[∂νC] is exponentially vanishing in n
Let us now make several important remarks. First, note that
part (i) of Corollary 1 includes as a particular example the cost
function CG of (1). Second, part (ii) of this corollary also includes
as particular examples operators with N 2 Oð1Þ, as well as
N 2 Oðpoly ðnÞÞ. Finally, we remark that Fn(L, l) becomes trivial
when the number of layers L is Ω(poly(n)), however, as we
discuss below, we can still ﬁnd that Var[∂νCG] vanishes
exponentially in this case.
Our second main theorem shows that barren plateaus can be
avoided for shallow circuits by employing local cost functions.
Here we consider m-local cost functions where each bOi acts
nontrivially on at most m qubits and (on these qubits) can be
expressed as bOi ¼ bO
O ¼ c01 þ ∑
are operators acting on m/2 qubits which can be
written as a tensor product of Pauli operators. Here, we assume
the summation in Eq. (16) includes two possible cases as
schematically shown in Fig. 3b: First, when bO
i ) acts on the
ﬁrst (last) m/2 qubits of a given Sk, and second, when bO
acts on the last (ﬁrst) m/2 qubits of a given Sk (Sk+1). This type of
cost function includes any ultralocal cost function (i.e., where the
bOi are one-body) as in (2), and also VQE Hamiltonians with up to
m/2 neighbor interactions. Then, the following theorem holds.
Consider a trainable parameter θν in a block W of the ansatz in
Fig. 3. Let Var[∂νC] be the variance of the partial derivative of an
m-local cost function C (with O given by (16)) with respect to θν.
WA, WB of (7), and each block in V(θ) form a local 2-design, then
Var[∂νC] is lower bounded by
GnðL; lÞ ⩽Var½∂νC ;
GnðL; lÞ ¼
ð22m  1Þ2ð2m þ 1ÞLþl
ðk;k0Þ2kLB
i ϵðρk;k0ÞϵðbOiÞ ;
where iL is the set of i indices whose associated operators bOi
act on qubits in the forward light-cone L of W, and kLB is the
set of k indices whose associated subsystems Sk are in the backward light-cone LB of W. Here we deﬁned the function ϵðMÞ ¼
DHS M; TrðMÞ1=dM
Þ where DHS is the Hilbert–Schmidt distance
and dM is the dimension of the matrix M. In addition, ρk;k0 is the
partial trace of the input state ρ down to the subsystems
SkSkþ1:::Sk0.
Let us make a few remarks. First, note that the ϵðbOiÞ in the
lower bound indicates that training V(θ) is easier when bOi is far
from the identity. Second, the presence of ϵðρk;k0Þ in Gn(L, l)
implies that we have no guarantee on the trainability of a
parameter θν in W if ρ is maximally mixed on the qubits in the
backwards light-cone.
From Theorem 2 we derive the following corollary for m-local
cost functions, which guarantees the trainability of the ansatz for
shallow circuits.
Corollary 2
Consider the function Fn(L, l). Let O be an operator of the form
(16), as in Theorem 2. If at least one term c2
i ϵðρk;k0ÞϵðbOiÞ in the
sum in (18) vanishes no faster than Ω(1/poly(n)), and if the
number of layers L is Oðlog ðnÞÞ, then
GnðL; lÞ 2 Ω
On the other hand, if at least one term c2
i ϵðρk;k0ÞϵðbOiÞ in the sum
in (18) vanishes no faster than Ω 1=2poly ðlog ðnÞÞ
, and if the
number of layers is Oðpoly ðlog ðnÞÞÞ, then
GnðL; lÞ 2 Ω
2poly ðlog ðnÞÞ
Hence, when L is Oðpoly ðlog ðnÞÞÞ there is a transition region
where the lower bound vanishes faster than polynomially, but
slower than exponentially.
We ﬁnally justify the assumption of each block being a local
2-design from the fact that shallow circuit depths lead to such
local 2-designs. Namely, it has been shown that one-dimensional
2-designs have efﬁcient quantum circuit descriptions, requiring
Oðm2Þ gates to be exactly implemented38, or OðmÞ to be
approximately implemented39,40. Hence, an L-layered ansatz in
which each block forms a 2-design can be exactly implemented
with a depth D 2 Oðm2LÞ, and approximately implemented with
D 2 OðmLÞ. For the case of two-dimensional connectivity, it has
been shown that approximate 2-designs require a circuit depth of
p Þ to be implemented40. Therefore, in this case the depth of
the layered ansatz is D 2 Oð
p LÞ. The latter shows that
increasing the dimensionality of the circuit reduces the circuit
depth needed to make each block a 2-design.
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
Moreover, it has been shown that the Alternating Layered
Ansatz of Fig. 3 will form an approximate one-dimensional
2-design on n qubits if the number of layers is OðnÞ40. Hence, for
deep circuits, our ansatz behaves like a random circuit and we
recover the barren plateau result of9 for both local and global cost
functions.
Numerical simulations. As an important example to illustrate
the cost-function-dependent barren plateau phenomenon, we
consider quantum autoencoders11,41–44. In particular, the pioneering VQA proposed in ref. 11 has received signiﬁcant literature
attention, due to its importance to quantum machine learning
and quantum data compression. Let us brieﬂy explain the
algorithm in ref. 11.
Consider a bipartite quantum system AB composed of nA and
nB qubits, respectively, and let fpμ; jψμig be an ensemble of pure
states on AB. The goal of the quantum autoencoder is to train a
gate sequence V(θ) to compress this ensemble into the A
subsystem, such that one can recover each state jψμi with high
ﬁdelity from the information in subsystem A. One can think of B
as the “trash” since it is discarded after the action of V(θ).
To quantify the degree of data compression, ref. 11 proposed a
cost function of the form:
G ¼ 1  Tr½ 0j i 0
ABVðθÞy ;
AB ¼ ∑μpμjψμi hψμj is the ensemble-average input state,
¼ ∑μpμTrA½jψ0i hψ0j is the ensemble-average trash state,
i ¼ VðθÞjψμi. Equation (22) makes it clear that C0
form in (6), and O0
G ¼ 1AB  1A  0j i 0
h j is a global observable
of the form in (10). Hence, according to Corollary 1, C0
G exhibits a
barren plateau for large nB. (Speciﬁcally, Corollary 1 applies in
this context when nA < nB). As a result, large-scale data
compression, where one is interested in discarding large numbers
of qubits, will not be possible with C0
To address this issue, we propose the following local cost
ABVðθÞy ;
L ¼ 1AB  1
j¼1 1A  0j i 0
h jj  1j,
identity on all qubits in B except the jth qubit. As shown in the
Supplementary Note 9, C0
L satisﬁes C0
implies that C0
L is faithful (vanishing under the same conditions
G). Furthermore, note that O0
L has the form in (16). Hence
Corollary 2 implies that C0
L does not exhibit a barren plateau for
shallow ansatzes.
Here we simulate the autoencoder algorithm to solve a simple
problem where nA = 1, and where the input state ensemble
fpμ; jψμig is given by
¼ 0j iA  0; 0; 0; ¼ ; 0
p1 ¼ 2=3 ;
¼ 1j iA  1; 1; 0; ¼ ; 0
p2 ¼ 1=3 :
In order to analyze the cost-function-dependent barren plateau
phenomenon, the dimension of subsystem B is gradually
increased as nB = 10, 15, …, 100.
Numerical results. In our heuristics, the gate sequence V(θ) is
given by two layers of the ansatz in Fig. 4, so that the number of
gates and parameters in V(θ) increases linearly with nB. Note that
this ansatz is a simpliﬁed version of the ansatz in Fig. 3, as we can
only generate unitaries with real coefﬁcients. All parameters in V
(θ) were randomly initialized and as detailed in the Methods
section, we employ a gradient-free training algorithm that gradually increases the number of shots per cost-function evaluation.
Analysis of the n-dependence. Figure 5 shows representative
results of our numerical implementations of the quantum
autoencoder in ref. 11 obtained by training V(θ) with the global
and local cost functions respectively given by (22) and (23).
Speciﬁcally, while we train with ﬁnite sampling, in the ﬁgures we
show the exact cost-function values versus the number of
iterations. Here, the top (bottom) axis corresponds to the number
of iterations performed while training with C0
L). For nB = 10
and 15, Fig. 5 shows that we are able to train V(θ) for both cost
functions. For nB = 20, the global cost function initially presents a
plateau in which the optimizing algorithm is not able to
determine a minimizing direction. However, as the number of
shots per function evaluation increases, one can eventually
minimize C0
G. Such result indicates the presence of a barren
plateau where the gradient takes small values which can only be
detected when the number of shots becomes sufﬁciently large. In
this particular example, one is able to start training at around 140
iterations.
When nB > 20 we are unable to train the global cost function,
while always being able to train our proposed local cost function.
Note that the number of iterations is different for C0
for the global cost function case we reach the maximum number
of shots in fewer iterations. These results indicate that the global
cost function of (22) exhibits a barren plateau where the gradient
of the cost function vanishes exponentially with the number of
qubits, and which arises even for constant depth ansatzes. We
remark that in principle one can always ﬁnd a minimizing
direction when training C0
G, although this would require a
number of shots that increases exponentially with nB. Moreover,
Fig. 4 Alternating Layered Ansatz for V(θ) employed in our numerical
simulations. Each layer is composed of control-Z gates acting on
alternating pairs of neighboring qubits which are preceded and followed by
single qubit rotations around the y-axis, RyðθiÞ ¼ eiθiσy=2. Shown is the case
of two layers, nA = 1, and nB = 10 qubits. The number of variational
parameters and gates scales linearly with nB: for the case shown there are
71 gates and 51 parameters. While each block in this ansatz will not form an
exact local 2-design, and hence does not fall under our theorems, one can
still obtain a cost-function-dependent barren plateau.
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
one can see in Fig. 5 that randomly initializing the parameters
always leads to C0
G  1 due to the narrow gorge phenomenon (see
Proposition 1), i.e., where the probability of being near the global
minimum vanishes exponentially with nB.
On the other hand, Fig. 5 shows that the barren plateau is
avoided when employing a local cost function since we can train
L for all considered values of nB. Moreover, as seen in Fig. 5, C0
can be trained with a small number of shots per cost-function
evaluation (as small as 10 shots per evaluation).
Analysis of the L-dependence. The power of Theorem 2 is that
it gives the scaling in terms of L. While one can substitute a
function of n for L as we did in Corollary 2, one can also directly
study the scaling with L (for ﬁxed n). Figure 6 shows the
dependence on L when training C0
L for the autoencoder example
with nA = 1 and nB = 10. As one can see, the training becomes
more difﬁcult as L increases. Speciﬁcally, as shown in the inset it
appears to become exponentially more difﬁcult, as the number of
shots needed to achieve a ﬁxed cost value grows exponentially
with L. This is consistent with (and hence veriﬁes) our bound
on the variance in Theorem 2, which vanishes exponentially in
L, although we remark that this behavior can saturate for very
In summary, even though the ansatz employed in our
heuristics is beyond the scope of our theorems, we still ﬁnd
cost-function-dependent barren plateaus, indicating that the costfunction dependent barren plateau phenomenon might be more
general and go beyond our analytical results.
Discussion
While scaling results have been obtained for classical neural
networks45, very few such results exist for the trainability of
parametrized quantum circuits, and more generally for quantum
neural networks. Hence, rigorous scaling results are urgently
Fig. 5 Cost versus number of iterations for the quantum autoencoder problem deﬁned by Eqs. (25)–(26). In all cases we employed two layers of the
ansatz shown in Fig. 4, and we set nA = 1, while increasing nB = 10, 15, …, 100. The top (bottom) axis corresponds to the global cost function C0
G of Eq. (22)
(local cost function C0
L of (23)). As can be seen, C0
G can be trained up to nB = 20 qubits, while C0
L trained in all cases. These results indicate that global cost
function presents a barren plateau even for a shallow depth ansatz, and this can be avoided by employing a local cost function.
Fig. 6 Local cost C0
L versus number of iterations for the quantum
autoencoder problem in Eqs. (25–26) with nB = 10. Each curve
corresponds to a different number of layers L in the ansatz of Fig. 4 with L
= 2,…, 20. Curves were averaged over 9 instances of the autoencoder. As
the number of layers increases, the optimization becomes harder. Inset:
Number of shots needed to reach cost values of C0
L ¼ 0:02 and C0
versus number of layers L. As L increases the number of shots needed to
reach the indicated cost values appears to increase exponentially.
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
needed for VQAs, which many researchers believe will provide
the path to quantum advantage with near-term quantum computers. One of the few such results is the barren plateau theorem
of ref. 9, which holds for VQAs with deep, hardware-efﬁcient
In this work, we proved that the barren plateau phenomenon
extends to VQAs with randomly initialized shallow Alternating
Layered Ansatzes. The key to extending this phenomenon to
shallow circuits was to consider the locality of the operator O that
deﬁnes the cost function C. Theorem 1 presented a universal
upper bound on the variance of the gradient for global cost
functions, i.e., when O is a global operator. Corollary 1 stated the
asymptotic scaling of this upper bound for shallow ansatzes as
being exponentially decaying in n, indicating a barren plateau.
Conversely, Theorem 2 presented a universal lower bound on the
variance of the gradient for local cost functions, i.e., when O is a
sum of local operators. Corollary 2 notes that for shallow ansatzes
this lower bound decays polynomially in n. Taken together, these
two results show that barren plateaus are cost-function-dependent, and they establish a connection between locality and
trainability.
In the context of chemistry or materials science, our present
work can inform researchers about which transformation to use
when mapping a fermionic Hamiltonian to a spin Hamiltonian46,
Jordan-Wigner
Bravyi–Kitaev47.
Bravyi–Kitaev transformation often leads to more local Pauli
terms, and hence (from Corollary 2) to a more trainable cost
function. This fact was recently numerically conﬁrmed48.
Moreover, the fact that Corollary 2 is valid for arbitrary input
quantum states may be useful when constructing variational
ansatzes. For example, one could propose a growing ansatz
method where one appends log ðnÞ layers of the hardwareefﬁcient ansatz to a previously trained (hence ﬁxed) circuit. This
could then lead to a layer-by-layer training strategy where the
previously trained circuit can correspond to multiple layers of the
same hardware-efﬁcient ansatz.
We remark that our deﬁnition of a global operator (local
operator) is one that is both non-local (local) and many body (few
body). Therefore, the barren plateau phenomenon could be due
to the many-bodiness of the operator rather than the non-locality
of the operator; we leave the resolution of this question to future
work. On the other hand, our Theorem 1 rules out the possibility
that barren plateaus could be due to cardinality, i.e., the number
of terms in O when decomposed as a sum of Pauli products49.
Namely, case (ii) of this theorem implies barren plateaus for O of
essentially arbitrary cardinality, and hence cardinality is not the
key variable at work here.
We illustrated these ideas for two examples VQAs. In Fig. 2, we
considered a simple state-preparation example, which allowed us
to delve deeper into the cost landscape and uncover another
phenomenon that we called a narrow gorge, stated precisely in
Proposition 1. In Fig. 5, we studied the more important example
of quantum autoencoders, which have generated signiﬁcant
interest in the quantum machine learning community. Our
numerics showed the effects of barren plateaus: for more than 20
qubits we were unable to minimize the global cost function
introduced in11. To address this, we introduced a local cost
function for quantum autoencoders, which we were able to
minimize for system sizes of up to 100 qubits.
There are several directions in which our results could be
generalized in future work. Naturally, we hope to extend the
narrow gorge phenomenon in Proposition 1 to more general
VQAs. In addition, we hope in the future to unify our theorems 1
and 2 into a single result that bounds the variance as a function of
a parameter that quantiﬁes the locality of O. This would further
connection
trainability.
Moreover, our numerics suggest that our theorems (which are
stated for exact 2-designs) might be extendable in some form to
ansatzes composed of simpler blocks, like approximate 2designs39.
We emphasize that while our theorems are stated for a
hardware-efﬁcient ansatz and for costs that are of the form (6), it
remains an interesting open question as to whether other ansatzes,
cost function, and architectures exhibit similar scaling behavior as
that stated in our theorems. For instance, we have recently
shown50 that our results can be extended to a more general type of
Quantum Neural Network called dissipative quantum neural
networks51. Another potential example of interest could be the
unitary-coupled cluster (UCC) ansatz in chemistry52, which is
intended for use in the Oðpoly ðnÞÞ depth regime34. Therefore it is
important to study the key mathematical features of an ansatz that
might allow one to go from trainability for Oðlog nÞ depth (which
we guarantee here for local cost functions) to trainability for
Oðpoly nÞ depth.
Finally, we remark that some strategies have been developed to
mitigate the effects of barren plateaus32,33,53,54. While these
methods are promising and have been shown to work in certain
cases, they are still heuristic methods with no provable guarantees
that they can work in generic scenarios. Hence, we believe that
more work needs to be done to better understand how to prevent,
avoid, or mitigate the effects of barren plateaus.
In this section, we provide additional details for the results in the main text, as well
as a sketch of the proofs for our main theorems. We note that the proof of
Theorem 2 comes before that of Theorem 1 since the latter builds on the former.
More detailed proofs of our theorems are given in the Supplementary Information.
Variance of the cost function partial derivative. Let us ﬁrst discuss the formulas
we employed to compute Var[∂νC]. Let us ﬁrst note that without loss of generality,
any block Wkl(θkl) in the Alternating Layered Ansatz can be written as a product of
ζkl independent gates from a gate alphabet A ¼ fGμðθÞg as
WklðθklÞ ¼ Gζklðθζkl
kl Þ ¼ Gνðθν
klÞ ¼ G1ðθ1
where each θν
kl is a continuous parameter. Here, Gνðθν
klÞ ¼ Rνðθν
klÞQν where Qν is
an unparametrized gate and Rνðθν
klÞ ¼ eiθν
klσν=2 with σν a Pauli operator. Note that
WkL denotes a block in the last layer of V(θ).
For the proofs of our results, it is helpful to conceptually break up the ansatz as
follows. Consider a block Wkl(θkl) in the lth layer of the ansatz. For simplicity,
we henceforth use W to refer to a given Wkl(θkl). Let Sw denote the m-qubit
subsystem that contains the qubits W acts on, and let Sw be the (n −m) subsystem
on which W acts trivially. Similarly, let Hw and Hw denote the Hilbert spaces
associated with Sw and Sw, respectively. Then, as shown in Fig. 3a, V(θ) can be
expressed as
VðθÞ ¼ VRð1w  WÞVL :
Here, 1w is the identity on Hw, and VR contains the gates in the (forward) lightcone L of W, i.e., all gates with at least one input qubit causally connected to the
output qubits of W. The latter allows us to deﬁne SL as the subsystem of all qubits
Let us here recall that the Alternating Layered Ansatz can be implemented with
either a 1D or 2D square connectivity as schematically depicted in Fig. 3c. We
remark that the following results are valid for both cases as the light-cone structure
will be the same. Moreover, the notation employed in our proofs applies to both the
1D and 2D cases. Hence, there is no need to refer to the connectivity dimension in
what follows.
Let us now assume that θν is a parameter inside a given block W, we obtain
from (6), (27), and (28)
2 Tr ð1w  WBÞVLρVy
´ ½1w  σν; ð1w  Wy
ROVRð1w  WAÞ
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
Finally, from (29) we can derive a general formula for the variance:
Var½∂νC ¼ 2m1Tr½σ2
ð22m  1Þ2 ∑
qp iVRhΔΨp0q0
which holds if WA and WB form independent 2-designs. Here, the summation runs
over all bitstrings p, q, p0, q0 of length 2n−m. In addition, we deﬁned
qp ¼ Tr½ΩqpΩq0p0  Tr½ΩqpTr½Ωq0p0
pq ¼ Tr½ΨpqΨp0q0  Tr½ΨpqTr½Ψp0q0
where Trw indicates the trace over subsystem Sw, and Ωqp and Ψqp are operators on
Hw deﬁned as
Ωqp ¼ Trw ð pj i q
h j  1wÞVy
Ψpq ¼ Trw ð qj i p
h j  1wÞVLρVy
We derive Eq. (31) in the Supplementary Note 4.
Computing averages over V. Here we introduce the main tools employed to
compute quantities of the form 〈…〉V. These tools are used throughout the proofs
of our main results.
Let us ﬁrst remark that if the blocks in V(θ) are independent, then any average
over V can be computed by averaging over the individual blocks, i.e.,
h ¼ iV ¼ h ¼ iW11; ¼ ;Wkl; ¼ ¼ h ¼ iVL;W;VR. For simplicity let us ﬁrst consider the
expectation value over a single block W in the ansatz. In principle 〈…〉W can be
approximated by varying the parameters in W and sampling over the resulting
2m × 2m unitaries. However, if W forms a t-design, this procedure can be simpliﬁed
as it is known that sampling over its distribution yields the same properties as
sampling random unitaries from the unitary group with respect to the unique
normalized Haar measure.
Explicitly, the Haar measure is a uniquely deﬁned left and right-invariant
measure over the unitary group dμ(W), such that for any unitary matrix A ∈U(2m)
and for any function f(W) we have
dμðWÞf ðWÞ ¼
dμðWÞf ðAWÞ
dμðWÞf ðWAÞ ;
where the integration domain is assumed to be U(2m) throughout this work.
Consider a ﬁnite set fWygy2Y (of size ∣Y∣) of unitaries Wy, and let P(t, t)(W) be an
arbitrary polynomial of degree at most t in the matrix elements of W and at most t
in those of W†. Then, this ﬁnite set is a t-design if38
hPðt;tÞðWÞiw ¼ 1
y2Y Pðt;tÞðWyÞ
dμðWÞPðt;tÞðWÞ :
From the general form of C in Eq. (6) we can see the cost function is a
polynomial of degree at most 2 in the matrix elements of each block Wkl in V(θ),
and at most 2 in those of ðWklÞy. Then, if a given block W forms a 2-design, one
can employ the following elementwise formula of the Weingarten calculus55,56 to
explicitly evaluate averages over W up to the second moment:
dμðWÞwijw
dμðWÞwi1j1wi2j2w
22m1 Δ1  Δ2
where wij are the matrix elements of W, and
Δ1 ¼ δi1i0
Δ2 ¼ δi1i0
Intuition behind the main results. The goal of this section is to provide some
intuition for our main results. Speciﬁcally, we show here how the scaling of the cost
function variance can be related to the number of blocks we have to integrate to
compute h  iVR;VL, the locality of the cost functions, and with the number of layers
in the ansatz.
First, we recall from Eq. (38) that integrating over a block leads to a coefﬁcient
of the order 1/22m. Hence, we see that the more blocks one integrates over, the
worse the scaling can be.
We now generalize the warm-up example. Let V(θ) be a single layer of the
alternating ansatz of Fig. 3, i.e., V(θ) is a tensor product of m-qubit blocks Wk: =
Wk1, with k = 1, …, ξ (and with ξ = n/m), so that θν is in the block Wk0. In the
Supplementary Note 5 we generalize this scenario to the when the input state is not
0j i, but instead is an arbitrary state ρ.
From (31), the partial derivative of the global cost function in (1) can be
expressed as
Var½∂νCG ¼ υ
h jmWk 0j i 0
where υ ¼ ð2m1Þ2Tr½σ2
22mð2mþ11Þ2. From (40) we have that in order to compute (40) one needs
to integrate over ξ −1 blocks. Then, since each integration leads to a coefﬁcient 1/
22m the variance will scale as Oð1=ð22mÞξ1 ¼ Oð1=22nÞ. Hence, the scaling of the
variance gets worse for each block we integrate (such that the block acts on qubits
we are measuring).
On the other hand, for a local cost let us consider a single term in (3) where
j 2 S~k, so that
Var½∂νCL /
Tr ð 0j i 0
h jj  1jÞW~k 0j i 0
Here, in contrast to the global case, we only have to integrate over a single block
irrespective of the total number of qubits. Hence, we now ﬁnd that the variance
scales as Oð1=n2Þ, where we remark that the scaling is essentially given by the
prefactor 1/n2 in (3).
Let us now brieﬂy provide some intuition as to why the scaling of local cost
gradients becomes exponentially vanishing with the number of layers as in
Theorem 2. Consider the case when V(θ) contains L layers of the ansatz in Fig. 3.
Moreover, as shown in Fig. 7, let W be in the ﬁrst layer, and let Oi act on the m
topmost qubits of L. As schematically depicted in Fig. 7, we now have to
integrating over L −1 blocks. Then, as proved in the Supplementary Note 5,
integrating over a block leads to a coefﬁcient 2m/2/(2m + 1). Hence, after
integrating L −1 times, we obtain a coefﬁcient 2mðL1Þ=2=ð2m þ 1ÞL1 which
vanishes no faster than Ω 1=poly ðnÞ
Þ if mL 2 Oðlog ðnÞÞ.
As we discuss below, for more general scenarios the computation of Var[∂νC]
becomes more complex.
Sketch of the proof of the main theorems. Here we present a sketch of the proof
of Theorems 1 and 2. We refer the reader to the Supplementary Information for a
detailed version of the proofs.
As mentioned in the previous subsection, if each block in V(θ) forms a local 2design, then we can explicitly calculate expectation values 〈…〉W via (38). Hence, to
compute hΔΩp0q0
qp iVR, and hΔΨp0q0
pq iVL in (31), one needs to algorithmically integrate
over each block using the Weingarten calculus. In order to make such computation
tractable, we employ the tensor network representation of quantum circuits.
For the sake of clarity, we recall that any two-qubit gate can be expressed as
U ¼ ∑ijklUijkl ijj i kl
h j, where Uijkl is a 2 × 2 × 2 × 2 tensor. Similarly, any block in the
ansatz can be considered as a 2
2 tensor. As schematically shown in
Fig. 8a, one can use the circuit description of Ωi
qp and Ψpq to derive the tensor
Fig. 7 The block W is in the ﬁrst layer of V(θ), and the operator Oi acts on
the topmost m qubits in the forward light-cone L of W. Dashed thick lines
indicate the backward light-cone of Oi. All but L −1 blocks simplify to
identity in Ωqp of Eq. (34).
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
network representation of terms such as Tr½Ωi
q0p0. Here, Ωi
qp is obtained from
(34) by simply replacing O with Oi.
In Fig. 8b we depict an example where we employ the tensor network
representation of Ωi
qp to compute the average of Tr½Ωi
q0p0, and Tr½Ωi
As expected, after each integration one obtains a sum of four new tensor networks
according to Eq. (38).
Proof of Theorem 2. Let us ﬁrst consider an m-local cost function C where O is
given by (16), and where bOi acts nontrivially in a given subsystem Sk of S. In
particular, when bOi is of this form the proof is simpliﬁed, although the more
general proof is presented in the Supplementary Note 6. If Sk 6 SL we ﬁnd
qp / 1w, and hence
The latter implies that we only have to consider the operators bOi which act on
qubits inside of the forward light-cone L of W.
Then, as shown in the Supplementary Information
/ ϵðbOiÞ :
Here we remark that the proportionality factor contains terms of the form
δðp0;q0ÞSþ
w ¼ Sw), which arises from the
different tensor contractions of Ppq ¼ qj i p
h j in Fig. 8c. It is then straightforward to
δðp0;q0ÞSþ
DHS ~ρ; Trw½~ρ  1
where we deﬁne ~ρ as the reduced states of ~ρ ¼ VLρVy
L in the Hilbert spaces
associated with subsystems Sw ∪S
w. Here we recall that DHS is the Hilbert–Schmidt
distance DHS ρ; σ
Þ ¼ Tr½ðρ  σÞ2.
By employing properties of DHS one can show (see Supplementary Note 6)
DHS ~ρ; Trw½~ρ  1
≥DHS eρw; 1
2mðLlþ2Þ=2 ;
where ~ρw ¼ TrS
w ½~ρ. We can then leverage the tensor network representation of
quantum circuits to algorithmically integrate over each block in VL and compute
hDHS eρw; 1
iVL. One ﬁnds
DHS ~ρw; 1
ðk;k0Þ2kLB
tk;k0ϵðρk;k0Þ ;
with tk;k0⩾
ð2mþ1Þ2l8k; k0, and ϵðρk;k0Þ deﬁned in Theorem 2. Combining these results
leads to Theorem 2. Moreover, as detailed in the Supplementary information,
Theorem 2 is also valid when O is of the form (16).
Proof of Theorem 1. Let us now provide a sketch of the proof of Theorem 1, case
(i). Here we denote for simplicity bOk :¼ bO1k. We leave the proof of case (ii) for the
Fig. 8 Tensor-network representations of the terms relevant to Var[∂νC]. a Representation of Ωi
qp of Eq. (34) (left), where the superscript indicates that
O is replaced by Oi. In this illustration, we show the case of n = 2m qubits, and we denote Ppq ¼ qj i p
h j. We also show the representation of Tr½Ωi
(middle) and of Tr½Ωi
q0p0 (right). b By means of the Weingarten calculus we can algorithmically integrate over each block in the ansatz. After each
integration one obtains four new tensors according to Eq. (38). Here we show the tensor obtained after the integrations
dμðWÞTr½Ωi
R dμðWÞTr½Ωi
q0p0, which are needed to compute hΔΩq0p0
qp iVR as in Eq. (31). c As shown in the Supplementary Note 1, the result of the integration is a
sum of the form (49), where the deltas over p, q, p0, and q0 arise from the contractions between Ppq and Pp0q0.
NATURE COMMUNICATIONS | 
NATURE COMMUNICATIONS | 12:1791 | | www.nature.com/naturecommunications
Supplementary Note 7. In this case there are now operators Oi which act outside of
the forward light-cone L of W. Hence, it is convenient to include in VR not only all
the gates in L but also all the blocks in the ﬁnal layer of V(θ) (i.e., all blocks WkL,
with k = 1, …ξ). We can deﬁne SL as the compliment of SL, i.e., as the subsystem
of all qubits which are not in L (with associated Hilbert-space HL). Then, we
have VR ¼ VL  VL and q
L, where we deﬁne VL :¼
k2kLWkL, q
k0. Here, we
deﬁne kL :¼ fk : Sk  SLg and kL :¼ fk : Sk  SLg, which are the set of indices
whose associated qubits are inside and outside L, respectively. We also write
O ¼ c01 þ c1 ^OL  ^OL, where we deﬁne ^OL : ¼ N
k2kL bOk and ^OL : ¼ N
Using the fact that the blocks in V(θ) are independent we can now compute
qp iVR ¼ hΔΩq0p0
qp iVL;VL. Then, from the deﬁnition of Ωpq in Eq. (34) and the
fact that one can always express
qp ¼ TrL\w ð pj i q
h j  1wÞVLOLVL
Ωk ¼ Tr pj i q
and where TrL\w indicates the partial trace over the Hilbert-space associated with
the qubits in SL \ Sw. As detailed in the Supplementary Information we can use Eq.
(38) to show that
k δðp;qÞSk δðp0;q0ÞSk þ δðp;q0ÞSk δðp0;qÞSk
On the other hand, as shown in the Supplementary Note 7 (and as
schematically depicted in Fig. 8c), when computing the expectation value h ¼ iVL
in (47), one obtains
where we deﬁned δτ ¼ δðp;qÞSτ δðp0;q0ÞSτ δðp;q0ÞSτ δðp0;qÞSτ , tτ 2 R, Sτ ∪Sτ ¼ SL \ Sw
(with Sτ ≠;), and
τ ¼ Trxτyτ Trzτ Oi
Trzτ ½Oj
Trxτ Tryτzτ Oi
Tryτzτ ½Oj
Here we use the notation Trxτ to indicate the trace over the Hilbert space associated
with subsystem Sxτ , such that Sxτ ∪Syτ ∪Szτ ¼ SL. As shown in the Supplementary
Note 7, combining the deltas in Eqs. (48), and (49) with
Hilbert–Schmidt distances between two quantum states as in (44). One can then
use the following bounds DHS ρ1; ρ2
k, and ∑τtτ ≤2, along with
some additional simple algebra explained in the Supplementary Information to
obtain the upper bound in Theorem 1.
Ansatz and optimization method. Here we describe the gradient-free optimization method used in our heuristics. First, we note that all the parameters in the
ansatz are randomly initialized. Then, at each iteration, one solves the following
sub-space search problem: min
s2Rd Cðθ þ A  sÞ, where A is a randomly generated
isometry, and s = (s1, …, sd) is a vector of coefﬁcients to be optimized over. We
used d = 10 in our simulations. Moreover, the training algorithm gradually
increases the number of shots per cost-function evaluation. Initially, C is evaluated
with 10 shots, and once the optimization reaches a plateau, the number of shots is
increased by a factor of 3/2. This process is repeated until a termination condition
on the value of C is achieved, or until we reach the maximum value of 105 shots per
function evaluation. While this is a simple variable-shot approach, we remark that
a more advanced variable-shot optimizer can be found in ref. 57.
Finally, let us remark that while we employ a sub-space search algorithm, in the
presence of barren plateaus all optimization methods will (on average) fail unless
the algorithm has a precision (i.e., number of shots) that grows exponentially with
n. The latter is due to the fact that an exponentially vanishing gradient implies that
on average the cost function landscape will essentially be ﬂat, with the slope of the
order of Oð1=2nÞ. Hence, unless one has a precision that can detect such small
changes in the cost value, one will not be able to determine a cost minimization
direction with gradient-based, or even with black-box optimizers such as the
Nelder–Mead method58–61.
Data availability
Data generated and analyzed during the current study are available from the
corresponding author upon reasonable request.
Received: 5 February 2020; Accepted: 5 February 2021;