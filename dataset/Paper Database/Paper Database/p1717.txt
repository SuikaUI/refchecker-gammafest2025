Approximate k-NN Delta Test minimization method using genetic algorithms: Application
to time series$
Fernando Mateo∗,a, Duˇsan Soviljb, Rafael Gadeaa
aInstitute of Applications of Information Technologies and Advanced Communications, Universidad Polit´ecnica de Valencia, Valencia, Spain
bLaboratory of Information and Computer Science, Helsinki University of Technology, Espoo, Finland
In many real world problems, the existence of irrelevant input variables (features) hinders the predictive quality of the models used
to estimate the output variables. In particular, time series prediction often involves building large regressors of artiﬁcial variables
that can contain irrelevant or misleading information. Many techniques have arisen to confront the problem of accurate variable
selection, including both local and global search strategies. This paper presents a method based on genetic algorithms that intends
to ﬁnd a global optimum set of input variables that minimize the Delta Test criterion. The execution speed has been enhanced by
substituting the exact nearest neighbor computation by its approximate version. The problems of scaling and projection of variables
have been addressed. The developed method works in conjunction with MATLAB’s Genetic Algorithm and Direct Search Toolbox.
The goodness of the proposed methodology has been evaluated on several popular time series examples, and also generalized to
other non-time-series datasets.
Key words: Genetic algorithm, Delta test, Variable selection, Approximate k-nearest neighbors, Variable scaling, Variable
projection, Time series
1. Introduction
In many ﬁelds like science, industry and ﬁnance it is necessary to accurately predict future values of a time series. Some
examples of problems that would beneﬁt from an accurate prediction are: industrial processes, that can be modeled, predicted
and controlled based on sensory data; natural phenomena, such
as daily rainfall or seismic events; medical applications like the
modeling of biological signals such as EEG or ECG; and ﬁnancial problems like the prediction of stock market prices.
The correct estimation of future values of time series is usually aﬀected by complex processes like random ﬂuctuations,
sudden trend changes, volatility and noise. The horizon of prediction and the number of available samples to obtain one or
more future estimations are important issues too. When building a regressor, which can be understood as the number of past
events used to predict their next one, the number of inputs to
the model (which is translated as the size of the regressor) can
become very large, depending on the periodicity of the particular time series. With large regressors, the learning procedure of
the involved predictive models becomes slow and tedious.
Historically, the diﬀerent models employed to estimate time
series have been diﬀerentiated in two groups: linear and nonlinear methods. The most popular linear methods are based
$This work was supported by Spanish Ministry of Science and Innovation
(MICINN) projects AGL 2004-07549-C05-02/ALI and FPA 2007-65013-C02-
02, and a grant with reference BES-2005-9703.
∗Corresponding author.
Email addresses: (Fernando Mateo),
 (Duˇsan Sovilj), (Rafael Gadea)
on the Box-Jenkins methodology .
They include autoregressive (AR) models, integrated (I) models, and moving average (MA) models. Their combination has given rise to autoregressive moving average (ARMA) models, autoregressive
integrated moving average (ARIMA) models and their seasonal
generalization (SARIMA) . However, these models are too
limited and simplistic for the average complexity of a time series. In contrast, nonlinear methods are more suitable for complex series that contain irregularities and noise, such as chaotic
time series. There is abundant literature on nonlinear models
for time series forecasting . Among the existing
methods are neural networks , radial
basis function networks , support vector machines , self organizing maps and other
variants of these models . However, building
these models takes considerable computational time compared
to linear models. Recently, several hybrid methods (ARIMA +
fuzzy or neural networks) have been employed in the literature
 .
Both linear, nonlinear, and hybrid methods have the same
to gather enough information from past samples
to give a reliable prediction of the immediate future samples (short-term prediction) or give estimations about far-future
samples (long-term prediction). Long term prediction (i.e. predicting multiple steps ahead towards the future) is usually more
challenging because the accumulation of errors and inherent uncertainties of a multiple-step-ahead in time yields deteriorated
estimates of future samples.
Time series prediction can be considered a modeling problem
 . The inputs to the model are composed of a set of consec-
 
November 4, 2009
utive regressor instances, while the output is the next value or
values of the series that have to be predicted after each regressor
Normally, the size of the regressor is chosen according to
the periodicity components of the time series. Consequently,
time series with long periodic components may yield very large
regressors that can be troublesome to handle by predictive models. Most modeling techniques do not deal well with datasets
having a high number of input variables, due to the so called
curse of dimensionality . As the number of dimensions
grows, the number of input values required to sample the solution space increases exponentially. Many real life problems
present this drawback since they have a considerable amount of
variables to be selected in comparison to the small number of
observations. Therefore, eﬃcient variable selection procedures
are required to reduce the complexity while also improving the
interpretability of multidimensional problems.
Recently, it has been shown that Delta Test (DT) can be
a powerful tool to determine the quality of a subset of variables by estimating the variance of the noise at the output .
Several studies related to feature selection using the DT have
been developed using both local search strategies such as forward search (FS) and forward-backward selection (FBS)
 and also global search techniques like tabu search
 and Genetic Algorithm (GA) . Global search methods have the advantage of being able to escape from local minima, to which local methods are prone to converge .
This paper presents a global search technique based on GAs
that manages not only to select, but also scale and project the
input variables in order to minimize the DT criterion. The computation of the DT has been accelerated by using the approximate k-nearest neighbor approach . The designed method
makes use of MATLAB’s Genetic Algorithm and Direct Search
toolbox for the GA-based search. This methodology can be
generalized to all regression problems. In this study we are going to focus mainly on time series processing, but we have also
included non-time series datasets to show that the methodology
can be generalized to all regression problems regardless of their
nature. The predictive study of the time series is not going to be
addressed, as the methodology only provides reduced datasets
for their later modeling.
This paper is organized as follows: Section 2 explains the DT
criterion and its role in the present study. Section 3 presents the
approximate k-nearest neighbors algorithm that has been used
to speed up the DT calculation. Section 4 introduces the motivation for the GA and the ﬁtness function optimization methods, such as scaling, projection and their variations with a ﬁxed
number of variables. Section 5 describes the datasets tested,
preprocessing and hardware/software speciﬁcations for the performed experiments, and ﬁnally, Section 6 discusses the most
relevant results obtained.
2. Delta Test
Delta Test was introduced by Pi and Peterson for time series and recently further analyzed by Liiti¨ainen et al. .
However, its applicability to variable selection was proposed in
 . The DT is a nonparametric noise estimator, i.e. it aims
to estimate the variance of the noise at the output, or the mean
squared error (MSE) that can be achieved without overﬁtting.
Given N input-output pairs (⃗xi, yi) ∈Rd × R, the relationship
between ⃗xi and yi can be expressed as
yi = f(⃗xi) + ηi,
i = 1, ..., N ,
where f is the unknown function and η is the noise. The DT
estimates the variance of the noise η.
The DT is useful for evaluating the nonlinear correlation between input and output variables. According to the DT, the selected set of input variables is the one that represents the relationship between the input variables and the output variable in
the most deterministic way.
The DT is based on the hypothesis of continuity of the regression function. If two points ⃗x1 and ⃗x2 are close in the input
variable space, the continuity of regression function implies that
the outputs f(⃗x1) and f(⃗x2) will be close enough in the output
space. If this is not accomplished, it is due to the inﬂuence of
the noise.
The DT can be interpreted as a particularization of the
Gamma Test considering only the ﬁrst nearest neighbor.
This yields a fully nonparametric method as it removes the only
hyperparameter(number of neighbors) that had to be chosen for
the Gamma Test. Let us denote the nearest neighbor of a point
⃗xi ∈Rd as ⃗xNN(i). The nearest neighbor formulation of the DT
estimates Var[η] by
Var[η] ≈δ = 1
(yi −yNN(i))2 ,
determined from the
input-output pair
(⃗xNN(i), yNN(i)). For a proof of convergence the reader should
refer to .
3. Approximate k-nearest neighbors
Nearest neighbor search is an optimization technique for
ﬁnding closest points in metric spaces. Speciﬁcally, given a
set of n reference points R and query point q, both in the same
metric space V, we are interested in ﬁnding the closest or nearest point c ∈R to q. Usually, V is a d-dimensional space Rd,
where distances are measured using Minkowski metrics (e.g.
Euclidean distance, Manhattan distance, max distance).
The simplest solution to this neighbor search problem is to
compute the distance from the query point to every other point
in the reference set, while registering and updating the position
of the nearest or k-nearest neighbors of every point. This algorithm, sometimes referred to as the naive approach or bruteforce approach, works for small datasets, but quickly becomes
intractable as either the size or the dimensionality of the problem becomes large, because the running time is O(dn). In practice, computing exact nearest neighbors in dimensions much
higher than 8 seems to be a very diﬃcult task .
Few methods allow to ﬁnd the nearest neighbor in less time
than the brute-force computation of all distances does. In 1977,
Friedman et al. showed that O(n) space and O(log n) query
time are achievable through the use of kd-trees. However, even
these methods suﬀer as dimension increases. The constant factors hidden in the asymptotic running time grow at least as fast
as 2d (depending on the metric).
In some applications it may be acceptable to retrieve a “good
guess” of the nearest neighbor. In those cases one may use an
algorithm which does not guarantee to return the actual nearest
neighbor in every case, in return for improved speed or memory
saving. Such an algorithm will ﬁnd the nearest neighbor in the
majority of cases, but this depends strongly on the dataset being queried. It has been shown that by computing nearest
neighbors approximately, it is possible to achieve signiﬁcantly
faster running times (on the order of tens to hundreds) often
with relatively small actual errors.
The authors state that given any positive real ǫ, a data
point p is a (1 + ǫ)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + ǫ) of the distance to the
true nearest neighbor. It is possible to preprocess a set of n
points in Rd in O(dn log n) time and O(dn) space, so that given
a query point q ∈Rd, and ǫ > 0, a (1 + ǫ)-approximate nearest neighbor of q can be computed in O(cd,ǫ log n) time, where
cd,ǫ ≤d⌈1+6d/ǫ⌉d is a factor depending only on dimension and
ǫ. In general, it is shown that given an integer k ≥1, (1 + ǫ) approximations to the k-nearest neighbors of q can be computed
in additional O(kd log n) time.
This faster neighbor search has been applied to the computation of the DT as expressed in Eq. 2 with high computational
4. Using genetic algorithms for global search
To date, the GA has been successfully applied for variable selection in many publications . Their success stems from the fact that they manage to carry out a global
optimization of the selected set of variables. In such a setting,
convergence of GA will depend on the available time, but the
diversity of solutions produced in each generation allows the
search to reach good solutions.
The purpose of the GA in this work is the global optimization of the scaling weights and projection matrix that minimize
the DT when applied to datasets built from time series data, although this approach may apply to other regression problems.
This study intends to ﬁnd the optimal DT value in a ﬁxed number of generations. Pure selection (i.e. assigning only ‘0’ or ‘1’
scaling factors to each variable) would clearly outperform scaling in terms of speed, but the best DT found is often suboptimal.
Scaling or projection are necessary to get closer to the optimal
set of solutions. For that reason, a real-coded GA is proposed
to optimize a population of chromosomes that encode arrays of
potential solutions. The following subsections describe the different types of variable preprocessing: scaling, scaling + projection, and their corresponding versions with ﬁxed number of
variables.
4.1. Scaling (S)
The target of performing scaling is to optimize the value of
the DT beyond the minimum value that can be obtained with
pure selection.
When performing scaling, the variables are
weighted according to their inﬂuence on the output variable.
Let us consider f as the unknown function that determines the
relationship between the N input-output pairs of a regression
problem, y = f(⃗x) + η, with ⃗x ∈Rd, y ∈R and η ∈R is a
random variable that represents the noise. Thus, the estimate of
the output, ˆy ∈R, can be expressed as ˆy = g(⃗xs), where ⃗xs ∈Rd
is the modiﬁed sample with scaling weights ⃗s = [s1, s2, ..., sd]
and g is the model that best approximates the function f. The
objective is to ﬁnd a scaling vector ⃗s ∈Rd such that
ˆy = g (s1x1, s2x2, . . . , sdxd)
minimizes Var[η] for the given problem.
In the existing variable selection literature there are several
applications of scaling to minimize the DT, but often keeping a
discrete number of weights instead of using unconstrained real values like in this study. In each generation of the
GA, every input sample ⃗xi = [xi1, xi2, . . . xid] from the dataset
X[N×d] is multiplied element by element by an individual ⃗s (array of scaling factors), forming a new dataset Xs
i j = sjxi j,
i = 1, . . ., N, j = 1, . . ., d .
Thus, for a population of p individuals the same number p
new datasets will be created. The DT is calculated by obtaining the Euclidean distances among the weighted input samples
Xs. After composing the new dataset Xs, the ﬁrst approximate
nearest neighbor of each point is selected using the method described in Section 3 and the DT is obtained from the diﬀerence
between their corresponding outputs, according to Eq. 2. When
a predeﬁned number of generations has been evaluated, the GA
returns the ﬁttest individual and its corresponding DT value.
4.2. Scaling + projection to k dimensions (SP-k)
A projection can be used to reduce the number of variables by
applying a linear (idempotent) transformation, represented by a
matrix P[d×k], to the matrix of input samples X[N×d], resulting in
a lower dimensional matrix Xp
[N×k], k < d:
[N×k] = X[N×d] P[d×k] .
Although it might seem counterproductive, the idea of the
developed method that combines scaling and projection is to
add new variables to the input space (the projection of the input vectors on k dimensions). Eq. 6 describes how these new
variables are attached to the input matrix as new columns:
[N×(d+k)] = [Xs
where Xs is the scaled version of X as calculated in Eq. 4, Xp is
the projected version of X and Xsp is the new scaled/projected
input matrix. In this case, the length of the chromosome increases linearly with parameter k, indicating that this value
should be kept low to attain reasonable running times.
With a combination of both scaling and projection, the optimization problem should be able to reach a DT value that
is not larger than the value obtained for scaling or projection alone. Consider the following two special cases. In the
ﬁrst special case, projection columns are set to zero values
Xsp = Xs, 0[N×k]
. This special case is just a scaling problem
with additional zero columns that do not inﬂuence the search
process, but only increase computational time. The second special case is similar, with all elements of Xs set to zero, i.e.
Xsp = 0[N×d], Xp, leading to a pure projection problem with
extra computational cost. These two extreme cases suggest that
by allowing both Xs and Xp to have real values, it becomes possible to ﬁnd solutions that are at least as good as solutions for
either scaling or projection problem.
4.3. Scaling with a ﬁxed number of variables
In many real world datasets, the number of samples is sometimes so large (N > 10000) that optimizing scaling weights
takes a considerable amount of time. This is due to the high
computational cost of the inherent nearest neighbor search in
the DT formula. One approach to solve this would simply be to
randomly discard some portion of the samples in order to speed
up calculation time, but there is risk of losing valuable data and
there is no clear method to select important samples. Instead of
removing samples, a diﬀerent strategy involves drastically reducing the number of variables by forcing most of the scaling
weights to have zero value (si = 0). To achieve this goal, an additional constraint is added to the problem which requires that
at most d f scaling weights have non-zero values. Therefore, d f
variables are ﬁxed to be included in ﬁnal scaling vector ⃗s and
the remaining d−d f weights are forced to zero which eﬀectively
changes the dimensionality of the dataset. The computation of
nearest neighbor search is reduced to a lower d f-dimensional
space. Thus, the ﬁxed method enables a quick insight into the
d f most relevant variables of the regression problem.
The parameter d f should not be considered as an additional
hyperparameter to the problem, since the optimization with restricted number of scaling weights gives larger DT values compared to optimization without any restrictions. If we consider
the scaling without any additional constraints (optimization of
all d variables), we should be able to reach the global minimum, since it is included in the search space. Removing any of
the variables carries the risk of excluding the global minimum
from the search space (unless those variables have zero weights
in the global minimum), leading to solutions with larger DT values. Thus, to ﬁnd global minimum one should set d f = d. However, the search for nearest neighbors is computationally more
expensive in d-dimensional space than in d f-dimensional one.
Introducing d f parameter enables the control over the trade-oﬀ
between the DT values and computational time.
For easier notation and understanding, we refer to standard
scaling as scaling or pure scaling, while scaling with the ﬁxed
number of variables is referred to as ﬁxed scaling. The same
setup of the GA can be used for both scaling problems. For the
ﬁxed scaling problem, one can just take the d f most important
or largest weights, eﬀectively performing a ranking of scaling
weights. On the other hand, the chromosomes can be ﬁxed to
Objective 1: F1(⃗s)
Objective 2: F2(⃗s)
Pareto front (Santa Fe dataset)
Figure 1: Pareto front for Santa Fe dataset, using scaling with ﬁxed number
of variables and df = d/2. The desired solution is the one that, in ﬁrst place,
adopts the desired number of ﬁxed variables df (or as close as possible) and, in
second place, minimizes the DT with that constraint.
have at most d f non-zero values. With modiﬁed chromosomes,
the crossover and mutation operators have to be modiﬁed accordingly to further preserve the required constraint. However,
both approaches tend to converge extremely quickly to suboptimal solutions in just a couple of tens of generations. A diﬀerent
approach would be to consider this as a multi-objective (MO)
optimization problem , where one objective is the
minimization of the DT, the main goal, and the other objective
is the absolute diﬀerence between the number of non-zero scaling weights and the desired value d f, i.e.
 ⃗s = Var η on scaled dataset Xs
d f −|{si , 0 | i = 1, . . ., d}|
MO optimization tries to ﬁnd the Pareto Optimal Front 
(a set of non-dominated solutions) instead of a single solution.
This set contains solutions where the values of objective functions are in conﬂict, i.e. improving one objective leads to deterioration in the other objective(s). Therefore, the result to a MO
problem is a set of solutions on diﬀerent pareto fronts, after
which the user selects one (or more) based on his/her preference. In this study, when the solutions are returned, we look for
the one with the exact required number d f of non-zero scaling
weights and the smallest DT. If such a solution does not exist,
the one with the lowest F2 value is used, that is, we try to stay
close to d f variables. A pareto front of Santa Fe dataset (see
Table1 for details) is shown in Figure 1 to illustrate this.
The algorithm used for MO optimization is the Elitist Non-
Dominated Sorting Genetic Algorithm proposed in , denoted NSGA-II. It works by constructing the new population
layer by layer of non-dominated fronts. To ensure that the population has always the same size, the last layer to be added has
to be split up into two parts. The part that is included in the next
population contains solutions from the least crowded area of
that front. This crowded comparison is based on crowding distance, which is computed in the objective function space. For
details see . The overall complexity of NSGA-II is O(hp2),
where h is the number of objectives (in our case h = 2) and p is
the size of the population.
Fixed scaling is easily extended to include the projection
problem, in the same manner as explained in Section 4.2 for
scaling + projection. The combination of scaling with a ﬁxed
number of variables and projection will be referred to as ﬁxed
scaling + projection. The projection in this problem is not modiﬁed, only the scaling is replaced with the ﬁxed version.
5. Experiments
The experiments were carried out using MATLAB R2009a
(TheMathworks Inc., Natick, MA, USA) and its Genetic Algorithm and Direct Search Toolbox (GADS). Creation, crossover
and mutation operators are implemented outside of the toolbox.
The approximate nearest neighbor search uses a C++ library
available at . The hardware platform used was an Intel Core
i7TMTM 920 processor (CPU clock: 4.2 GHz, Cache size: 8
MB) with 6 GB of system memory running Windows 7 (64bit).
The populations are initially created using a speciﬁc function
that assigns a uniform initialization to a percentage of the population and the rest can be customized by the user, specifying
how many of the remaining individuals are initialized randomly
and how many of them are left as zeros. The function is ﬂexible
in the sense that the desired percentage of the initial population
can be further split into more subsets, each one with a customizable percentage of randomly initialized individuals.
The crossover and mutation operators have also been implemented as custom functions according to . The mutation operator is a pure random uniform function that operates
at a gene level. A relatively high mutation rate (0.1) is used
to enable an eﬀective exploration of the solution space. The
crossover operator is BLX-α , that is speciﬁcally developed for a real-coded GA. BLX-α consists in, given two individuals I1 = (i1
d) and I2 = (i2
d) with (i ∈R),
a new oﬀspring O = (o1, ..., o j, ..., od) can be generated where
o j, j = 1, ..., d is a random value chosen from a uniform distribution within the interval [imin −α · B, imax + α · B] where
imin = min(i1
j), imax = max(i1
j), B = imax −imin and α ∈R.
The selection operator is the binary tournament selection .
The binary tournament does not require the computation of any
probability for each individual, saving a considerable amount
of operations in each iteration. This can increase computation
time in problems that require large number of individuals in the
population.
The population size was ﬁxed to 150 individuals. This value
appears to be a good compromise between performance and
computational cost for GA-based search applied to similarly
sized datasets . After some preliminary tests, the number of generations was ﬁxed to 200 to ensure convergence in all
cases. An alternative to this setting could be allowing a dynamic
management of the number of generations to be evaluated before stopping the search. Thus, the search would be stopped if
no improvement in the DT has been found for n generations,
indicating that the algorithm has converged.
The ﬁtness function of the GA is the DT computed for diﬀerent types of problems. In the MO optimization, the DT is one of
two objective functions. In this paper, we denote with DTS the
optimization of scaling problem using DT, with DTFS the ﬁxed
scaling problem, with DTSP-k the problem of scaling + projection to k dimensions, with DTFS-d f the ﬁxed scaling with d f
variables and ﬁnally DTFSP-d f-k is the problem of ﬁxed scaling with d f variables plus projection to k dimensions. To emphasize the goodness of these methods, pure selection (DTSL)
has also been included in the comparison.
From previous analysis , the best crossover and mutation rates for a feature selection application using the GA are
0.85 and 0.1 respectively, and an elitism of 10% of the individuals was the best compromise.
To sum up, the GA parameters were set as follows:
• Number of averaged runs: 10
• Number of generations evaluated: 200
• Population size: 150
• Population initialization: 20% uniform / 80% custom. The
customized part is further divided into three parts:
– 1/3 with 90% zeros and 10% random genes
– 1/3 with 80% zeros and 20% random genes
– 1/3 with 70% zeros and 30% random genes
• Crossover operator: BLX-α (α = 0.5)
• Selection function: Binary tournament
• Crossover rate: 0.85
• Mutation rate: 0.11
• Elitism: 10%
• Mutation function: Random uniform
The parameter d f was set to ⌈d/2⌉for all datasets throughout
the experiments.
5.1. Datasets
The described methods have been evaluated on eight time series and two standard regression datasets, to show the applicability of this methodology to generic regression problems. For
the time series, the data matrices were composed using onestep-ahead direct prediction strategy . The size of the
built datasets are listed in Table 1. For the time series, the number of variables refers to the regressor size, which was chosen
according to the periodicity of each series.
Some of the series were preprocessed in order to make them
more stationary by removing the trend and seasonality. This is
particularly the case for all of the three series of ESTSP 2008
competition. The ﬁrst series is actually a set
of three series, with two exogenous and one target series. The
goal is to predict the next values of the target series. For our
1This is the same value used by Oh et al. and Guill´en et al. , also for
feature selection. The rate is higher than usual to enable a thorough exploration
of all regions of the solution space, rather than focus on the reﬁnement of a
small number of candidate solutions.
Table 1: Datasets tested
Mackey-Glass 17 
Mackey-Glass 30 
Poland electricity 
Santa Fe 
ESTSP 2007 
ESTSP 2008a 
ESTSP 2008b 
Darwin SLP 
Housing 
Tecator* 
* This dataset was normalized in a sample-wise way instead of the typical variable-wise way, because it has been
proved that better DT values are achieved with this variation .
experiments we only used the target series, as correlation of
the target series with the other two exogenous series is not signiﬁcant. The target series is then transformed by taking the
ﬁrst order diﬀerence. ESTSP 2008b series was transformed
by applying x′
t = log(xt/xt−1) to the original series deﬁned by
xt, t ∈[1, . . ., 1300], in order to remove the trend.
All datasets were normalized to zero mean and unit variance
to prevent variables with high variance from dominating over
those with a lower one. Therefore, all DT values shown in this
paper are normalized by the variance of its respective output
variable. No splitting of datasets was performed (i.e. training
and test sets) because the objective of the paper is the data preprocessing to minimize the DT value and not the ﬁtting of a
model to the data. It is important to note that in a real-world
prediction application, the methodology should be applied to
the training data available, but in these experiments the method
was applied to the full datasets to provide repeatability, independently of the particular choice of training and test subsets.
6. Results
6.1. Approximate k-nearest neighbor performance
A preliminary test was carried out to assess the speed of the
approximate method of nearest neighbor computation as a function of ǫ. Typically, ǫ controls the trade-oﬀbetween eﬃciency
and accuracy. When ǫ is set larger, the approximation is less
accurate, and the search completes faster.
The averaged results (10 runs) for Santa Fe, ESTSP 2007
and Mackey Glass 17 datasets, after 200 generations, are shown
in Figure 2 for values of ǫ between 0 and 2. Higher values
were not tested as the DT estimate quickly deteriorates. The
approximate method proves to be faster for larger values of ǫ,
as expected. The computational time improvement for ǫ = 1
with respect to exact k-NN (ǫ = 0) ranges from 15% (Santa Fe)
to 28% .
Additionally, a study on the DT performance has been done,
using several values of ǫ for approximate k-NN. The DT performance is plotted in Figure 3 using DTSP-1. A degradation
of the DT is expected when allowing higher error bound. Experimental results show a clear increase trend for ESTSP 2007
and Mackey Glass 17 datasets when ǫ > 1. From these observations, we select the value ǫ = 1 for the rest of the experiments as
it seems the best compromise between speed-up and goodness
of the DT estimate. The DT given by the approximate search
with ǫ = 1 stays close to the value given by the exact search
in the majority of cases. The greatest diﬀerence was registered
for Santa Fe, where there is an 8% DT reduction. Thus, we are
reducing the computational cost with no signiﬁcative performance loss. This result makes the approximate k-NN approach
very interesting, especially for large datasets.
6.2. DT performance
The average DT values computed by each method for each
dataset are shown in Figure 4. The scaling and projection factors assigned to each variable have been omitted because the
subject of interest are the DT values only.
From the inspection of the results, one can aﬃrm that all the
proposed methods outperform pure selection in the considered
scenarios, except from rare exceptions given by the ﬁxed methods. The best performing method is DTSP-1 in most cases, followed by DTFSP. However, the DTFSP method ties with DTSP
in some datasets, such as the Mackey Glass series. Overall,
the methods that include projections are the most advantageous.
This was an expected result because they include the beneﬁts of
scaling and also leverage the possibility of using newly created
variables to gain an advantage over scaling alone.
In general, the ﬁxed variations provide slightly worse DT
values than their standard counterparts (e.g. Santa Fe, Poland
electricity), meaning that learning models will be able to give
similar performance on the halved dataset. Since only half of
the variables are used, the training times of models will greatly
beneﬁt from this reduction. The ﬁxed version also gives an insight into the most relevant variables, and in these experiments
the ⌈d/2⌉most important dimensions for prediction. For ESTSP
datasets, values of DTFS are not on the same level as those of
DTS, suggesting that more than ⌈d/2⌉variables are required for
better prediction.
6.3. Computational time
A computational time comparison of the presented methods
is shown in Figure 5. The pure selection is obviously the fastest
method because the combination of solutions to try is very limited. It is noticeable that DTSP-1 requires similar computational time to perform the 200 generations to DTS, and in some
cases even improving times computed for pure selection. This might seem contradictory, as the
size of the individuals is twice the size of those used for DTS
in the GA setup. Although the computational time for GA doubles when moving from DTS to DTSP-1, the running time of
DT optimization is dominated by nearest neighbor search. The
faster calculation time for DTSP-1 could be attributed to the
Average run time (s)
Average run time (s)
ESTSP 2007
Average run time (s)
Mackey Glass 17
Figure 2: Computational time of the approximate k-NN search as a function of ǫ for three datasets. The results were obtained running DTSP-1 for 200 generations
and 10 runs were averaged.
Average DT
Average DT
ESTSP 2007
Average DT
Mackey Glass 17
Figure 3: DT performance comparison for approximate k-NN search and diﬀerent values of ǫ for three datasets. The results were obtained running DTSP-1 for 200
generations and 10 runs were averaged.
construction of the underlying data structure of approximate
nearest neighbors, which uses a hierarchial space decomposition tree called balanced-box decomposition (BBD) tree .
Additional dimensions might lead to favorable splitting of the
points/samples into leaves of the tree, eventually improving response time for query searches.
The ﬁxed versions yielded good results in terms of DT values
for some datasets, but their computational times are generally
higher than their non-ﬁxed versions. When using MO optimization there is an additional cost inherent in NSGA-II method,
which computes crowding distance to ensure that individuals
from least crowded areas are included in the next population.
The additional O(hp2) complexity of NSGA-II for p = 150
slightly increases the running time for most datasets. The only
exceptions is ESTSP 2008b, where the computational time for
ﬁxed methods is lower than for DTS/DTSP-1.
In the next subsection, the computational time and performance of the DTSP method is further analyzed for several values of k.
6.4. Projection to k > 1 dimensions
From the previous results one can extract the conclusion that
DTSP-1 has clearly outperformed the rest of the methods while
still keeping reasonably low computational times in many scenarios . The DTSP method also oﬀers the possibility of
projecting to more than one dimension. In this subsection, projections to k = {1, 2, 3, 4, 5} dimensions are tested for each time
series dataset. Figure 6 illustrates the DT results obtained and
Figure 7 represents the computational time evolution.
By looking at the results it is easy to observe that, for all
datasets, the value of DT has an optimum value after which it
starts to rise again when adding more projections. The question
that remains is how to automatically select a good value for k
that optimizes the DT. One possible heuristic is the following:
Start with k = 1 and compute the DT criterion. Then progressively increase k by 1 until the value of DT no longer improves.
Since the result of the GA depends on the initial population,
in the proposed heuristic, instead of taking only one run of the
GA, several runs should be performed and the minimum taken
as the result for a certain value of k. The downsides of this approach are: a) the huge computational cost, since for each value
of k, the GA is applied several times to produce reliable DT values, and b) the possibility of stopping prematurely due to local
The computational times show a general increasing tendency,
which is logical due to the complexity increase with k. The only
exception was ESTSP 2007, for which computational times
show an irregular descending behavior. For Darwin SLP, the
computational time registered for k = 1 is slightly higher than
expected, as compared to the time required to project to more
dimensions.
Table 2: Minimum DT values calculated for the ten datasets using FBS, tabu search and DTSP-k (denormalized values in brackets)
0.0164 (36.09)
0.0094 (20.71)
0.0164 (36.11)
0.0108 (23.82)
0.0050 (11.00)
ESTSP 2007
0.0133 (0.082)
0.0137 (0.084)
0.0135 (0.083)
0.0156 (0.096)
0.0092 (0.056)
ESTSP 2008a
0.4959 (4.978)
0.4553 (4.570)
0.5409 (5.430)
0.4028 (4.043)
0.2223 (2.238)
ESTSP 2008b
0.2907 (9.7E-3)
0.2775 (9.3E-3)
0.2907 (9.7E-3)
0.2782 (9.3E-3)
0.1878 (6.3E-3)
Darwin SLP
0.1062 (0.7214)
0.1019 (0.6923)
0.1071 (0.7277)
0.0982 (0.6671)
0.0725 (0.4925)
Mackey Glass 17
11.5E-4 (5.9E-5)
9.5E-4 (4.9E-5)
11.5E-4 (5.9E-5)
9.5E-4 (4.9E-5)
6.3E-4 (3.2E-5)
Mackey Glass 30
4.0E-3 (3.2E-4)
3.8E-3 (3.0E-4)
4.0E-3 (3.2E-4)
3.8E-3 (3.0E-4)
1.6E-3 (1.3E-4)
Poland electricity
0.0481 (0.0013)
0.0404 (0.0011)
0.0481 (0.0013)
0.0379 (0.0010)
0.024 (6.6E-4)
0.0710 (6.009)
0.05654 (4.783)
0.0720 (6.089)
0.0558 (4.720)
0.0385 (3.257)
0.0136 
0.0149 (2.412)
0.0112 (1.825)
0.0248 (4.023)
0.0028 (0.454)
Finally, we have compared the minimum DT values achieved
with DTSP-k to some popular search methods that can use DT
as performance criterion, such as forward-backward selection
and tabu search with the settings of , both for selection and
discrete scaling (with 10 equally spaced levels). For a fair comparison, these methods were executed in the same machine as
DTSP-k and the the same amount of solutions were evaluated
by each method. The results are listed in Table 2. The proposed methodology easily outperforms the classic techniques
by a large margin.
6.5. Discussion
After extensive testing, several conclusions can be drawn.
We have proﬁted from using the approximate version of the
nearest neighbor computation to evaluate the DT. Preliminary
tests on three datasets have shown that the approximate k-NN
algorithm produces computational time improvements of between 15% and 28% while keeping very similar DT values to
the ones obtained by the exact search. Better improvements can
be obtained at the expense of higher DT. This trade-oﬀis controlled by parameter ǫ, which provided acceptable DT results in
the range . The results obtained suggest that the approximate method is suitable for speed critical applications or large
With respect to the DT performance of the diﬀerent ﬁtness
functions, all the proposed variations provide better results than
selection. The improvement introduced depends on the method
used, but generally DTSP provides the best results, followed by
DTFSP. These improvements range from 24% (Darwin SLP) to
almost 70% (Santa Fe, Tecator). The importance of projection
stands out especially in Tecator dataset. Where scaling is hardly
able to obtain any improvement against selection, projection
manages to reduce the estimate by a great amount. In some
cases, ﬁxed methods do not perform well (e.g. most ESTSP
time series). This is probably because in complex problems,
the limited amount of variables (d/2) did not allow further DT
minimization.
We found that the DTSP method was not only the best overall
performer, but it also generally reached these good results in a
reasonable amount of time. It performs comparably to DTS in
terms of speed, despite having twice the number of genes to
optimize. In the particular case of ESTSP 2007 it outperformed
pure selection in terms of speed, which is a remarkable result.
The adjustable number of projections also aided in the obtention of lower DT values. The progression of the DT curves as
a function of k shows a minimum where the optimum DT has
been registered. As we only tested projections to k ≤5 dimensions, better values may be found for higher k’s, at the expense
of computational time. The increase in computational time as a
function of k generally follows an increase with k, save ESTSP
2007 dataset, which produced an irregular descending trend.
The second overall best performing method has been DTFSP.
The improvement provided by this method follows a similar
trend to DTSP, as both include projection capabilities. Likewise, DTFS was able to show similar performanceto DTS using
only half of the variables, which is a promising result. However,
the running times for DTFS and DTFSP were higher than for
DTS and DTSP, respectively, in 8 out of 10 datasets. The fact
of carrying out a double objective optimization without increasing the number of generations could have aﬀected the performance. Besides, we believe that the strong constraint imposed
by grounding half the regressor variables to zero can justify this
diﬀerence. Of course, the number of variables that are considered zero can be tweaked to match the needs of every particular
user (e.g. for applications limited by number of variables). To
sum up, the ﬁxed methods can be very beneﬁcial when building
a model because it will only be necessary to deal with a fraction
of the initial number of inputs, achieving similar results.
Casting the scaling problem with a ﬁxed number of variables
into MO setting increases the run time on the tested datasets.
In order to achieve lower running times, the number of individuals in the population has to be reduced, which inﬂuences
the exploration capabilities of the GA in a negative way. The
additional computational time of NSGA-II prevents it from being used in this type of problem. Therefore, faster and simpler
techniques should be employed to lower the running times of
the ﬁxed scaling (plus projection) problem. One such possibility is the island GA with migration policies that do not have
such high complexity.
7. Conclusion
This paper has presented a fast methodology for DT minimization based on a global search guided by a GA, which has
been successfully applied to a variety of time series datasets.
The methodology can be generalized to other regression problems with a single output, as it has also been shown. The most
important goals of the proposed methodology are to reduce the
datasets in order to simplify the complexity of the necessary
modeling schemes and to improve interpretability by pruning
unnecessary variables, only keeping those that are really important for the prediction.
The DT method requires the computation of the nearest
neighbor of each point. The time needed for this computation
has been greatly alleviated by using an approximate version of
the k-nearest neighbor algorithm. The DT was optimized using several methodologies: scaling of variables (DTS), scaling
+ projection to a number of dimensions (DTSP) and versions
of these with a ﬁxed number of variables (DTFS and DTFSP,
respectively).
These methods can minimize the DT beyond
the limits imposed by pure selection and help to increase interpretability of the datasets.
The results obtained are very promising, especially for the
methods that include projection. Projection has helped to bring
out underlying relationships between variables that were initially not apparent, thus allowing a signiﬁcant DT minimization
with respect to scaling alone. DTSP was the best performing
method in all scenarios and it reached a maximum DT minimization of 70% over pure selection. Moreover, the low computational time of this method makes it suitable for problems
that involve large datasets. The possibility of varying the number of dimensions to project to enables a ﬁne reﬁnement of the
result that proved useful in all tests.