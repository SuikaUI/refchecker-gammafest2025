Evaluating Explanation Methods
for Deep Learning in Security
Alexander Warnecke∗, Daniel Arp∗, Christian Wressnegger† and Konrad Rieck∗
∗Technische Universität Braunschweig, Germany
† Karlsruhe Institute of Technology, Germany
Abstract—Deep learning is increasingly used as a building
block of security systems. Unfortunately, neural networks
are hard to interpret and typically opaque to the practitioner.
The machine learning community has started to address this
problem by developing methods for explaining the predictions of neural networks. While several of these approaches
have been successfully applied in the area of computer vision,
their application in security has received little attention so
far. It is an open question which explanation methods are
appropriate for computer security and what requirements
they need to satisfy. In this paper, we introduce criteria
for comparing and evaluating explanation methods in the
context of computer security. These cover general properties,
such as the accuracy of explanations, as well as securityfocused aspects, such as the completeness, efﬁciency, and
robustness. Based on our criteria, we investigate six popular
explanation methods and assess their utility in security
systems for malware detection and vulnerability discovery.
We observe signiﬁcant differences between the methods and
build on these to derive general recommendations for selecting and applying explanation methods in computer security.
1. Introduction
Over the last years, deep learning has been increasingly
recognized as an effective tool for computer security.
Different types of neural networks have been integrated
into security systems, for example, for malware detection , binary analysis , and vulnerability discovery . Deep learning, however, suffers
from a severe drawback: Neural networks are hard to
interpret and their decisions are opaque to practitioners.
Even simple tasks, such as determining which features
of an input contribute to a prediction, are challenging to
solve on neural networks. This lack of transparency is a
considerable problem in security, as black-box learning
systems are hard to audit and protect from attacks .
The machine learning community has started to develop methods for interpreting deep learning in computer
vision [e.g., 5, 43, 54]. These methods enable tracing back
the predictions of neural networks to individual regions
in images and thereby help to understand the decision
process. These approaches have been further extended to
also explain predictions on text and sequences .
Surprisingly, this work has received little attention in
security and there exists only a single technique that has
been investigated so far .
In contrast to other application domains of deep
learning, computer security poses particular challenges
for the use of explanation methods. First, security tasks,
such as malware detection and binary code analysis,
require complex neural network architectures that are
challenging to investigate. Second, explanation methods in
security do not only need to be accurate but also satisfy
security-speciﬁc requirements, such as complete and robust
explanations. As a result of these challenges, it is an
unanswered question which of the available explanation
methods can be applied in security and what properties
they need to possess for providing reliable results.
In this paper, we address this problem and develop
evaluation criteria for assessing and comparing explanation
methods in security. Our work provides a bridge between
deep learning in security and explanation methods developed for other application domains of machine learning.
Consequently, our criteria for judging explanations cover
general properties of deep learning as well as aspects that
are especially relevant to the domain of security.
General evaluation criteria. As general criteria, we consider the descriptive accuracy and sparsity of explanations.
These properties reﬂect how accurate and concise an explanation method captures relevant features of a prediction.
While accuracy is an evident criterion for obtaining reliable
results, sparsity is another crucial constraint in security. In
contrast to computer vision, where an analyst can examine
an entire image, a security practitioner cannot investigate
large sets of features at once, and thus sparsity becomes
an essential property when non-graphic data is analyzed.
Security evaluation criteria. We deﬁne the completeness,
stability, robustness, and efﬁciency of explanations as
security criteria. These properties ensure that reliable
explanations are available to a practitioner in all cases and
in reasonable time—requirements that are less important
in other areas of deep learning. For example, an attacker
may expose pathologic inputs to a security system that
mislead, corrupt, or slow down the computation of
explanations. Note that the robustness of explanation
methods to adversarial examples is not well understood
yet, and thus we base our analysis on the recent work by
Zhang et al. and Dombrowski et al. .
With the help of these criteria, we analyze six recent
explanation methods and assess their performance in
different security tasks. To this end, we implement four
security systems from the literature that make use of deep
learning and enable detecting Android malware ,
malicious PDF ﬁles , and security vulnerabilities ,
 
respectively. When explaining the decisions of these
systems, we observe signiﬁcant differences between the
methods in all criteria. Some methods are not capable
of providing sparse results, whereas others struggle with
structured security data or suffer from unstable outputs.
While the importance of the individual criteria depends
on the particular task, we ﬁnd that the methods IG 
and LRP comply best with all criteria and resemble
general-purpose techniques for security systems.
To demonstrate the utility of explainable learning,
we also qualitatively examine the generated explanations.
As an example for this investigation, Figure 1 shows
three explanations for the system VulDeePecker that
identiﬁes vulnerabilities in source code. While the ﬁrst
explanation method provides a nuanced representation
of the relevant features, the second method generates an
unsharp explanation due to a lack of sparsity. The third
approach provides an explanation that even contradicts the
ﬁrst one. Note that the variables VAR2 and VAR3 receive a
positive relevance (blue) in the ﬁrst case and a negative
relevance (orange) in the third.
c = split(arg[i],"=",&n);
block_flgs = strcpy((xmalloc(strlen(c ) + 1)),c );
VAR0 = FUN0 ( VAR1 [ VAR2 ] , STR0 , & VAR3 ) ;
VAR0 = strcpy ( ( FUN0 ( strlen ( VAR1 [ INT0 ] ) +
INT0 ) ) , VAR1 [ INT0 ] ) ;
VAR0 = FUN0 ( VAR1 [ VAR2 ] , STR0 , & VAR3 ) ;
VAR0 = strcpy ( ( FUN0 ( strlen ( VAR1 [ INT0 ] ) +
INT0 ) ) , VAR1 [ INT0 ] ) ;
VAR0 = FUN0 ( VAR1 [ VAR2 ] , STR0 , & VAR3 ) ;
VAR0 = strcpy ( ( FUN0 ( strlen ( VAR1 [ INT0 ] ) +
INT0 ) ) , VAR1 [ INT0 ] ) ;
Figure 1: Explanations for the prediction of the security
system VulDeePecker on a code snippet from the original
dataset. From top to bottom: Original code, LRP, LEMNA,
Our evaluation highlights the need for comparing
explanation methods and determining the best ﬁt for a
given security task. Furthermore, it also unveils a notable
number of artifacts in the underlying datasets. For all of the
four security tasks, we identify features that are unrelated
to security but strongly contribute to the predictions. As a
consequence, we argue that explanation methods need
to become an integral part of learning-based security
systems—ﬁrst, for understanding the decision process of
deep learning and, second, for eliminating artifacts in the
training datasets.
The rest of this paper is organized as follows: We brieﬂy
review the technical background of explainable learning in
Section 2. The explanation methods and security systems
under test are described in Section 3. We introduce our
criteria for comparing explanation methods in Section 4
and evaluate them in Section 5. Our qualitative analysis is
presented in Section 6 and Section 7 concludes the paper.
2. Explainable Deep Learning
Neural networks have been used in artiﬁcial intelligence for over 50 years, yet concepts for explaining their
decisions have just recently started to be explored. This
development has been driven by the remarkable progress
of deep learning in several areas, such as image recognition and machine translation . To embed our work
in this context, we brieﬂy review two aspects of explainable
learning that are crucial for its application in security: the
type of neural network and the explanation strategy.
2.1. Neural Network Architectures
Different architectures can be used for constructing a
neural network, ranging from general-purpose networks to
highly speciﬁc architectures. In the area of security, three
of these architectures are prevalent: multilayer perceptrons,
convolutional neural networks, and recurrent neural networks (see Figure 2). Consequently, we focus our study
on these network types and refer the reader to the books
by Rojas and Goodfellow et al. for a detailed
description of network architectures in general.
(a) MLP layer
(b) CNN layer
(c) RNN layer
Figure 2: Overview of network architectures in security:
Multilayer perceptrons (MLP), convolutional neural networks (CNN), and recurrent neural networks (RNN).
Multilayer Perceptrons (MLPs). Multilayer perceptrons,
also referred to as feedforward networks, are a classic and
general-purpose network architecture . The network is
composed of multiple fully connected layers of neurons,
where the ﬁrst and last layer correspond to the input
and output of the network, respectively. MLPs have been
successfully applied to a variety of security problems,
such as intrusion and malware detection . While
MLP architectures are not necessarily complex, explaining
the contribution of individual features is still difﬁcult, as
several neurons impact the decision when passing through
the network layers.
Convolutional Neural Networks (CNNs). These networks share a similar architecture with MLPs, yet they
differ in the concept of convolution and pooling . The
neurons in convolutional layers receive input only from
a local neighborhood of the previous layer. These neighborhoods overlap and create receptive ﬁelds that provide
a powerful primitive for identifying spatial structure in
data. CNNs have thus been successfully used for detecting
malicious patterns in the bytecode of Android applications
 . Due to the convolution and pooling layers, however,
it is hard to explain the decisions of a CNN, as its output
needs to be “unfolded” and “unpooled” for analysis.
Recurrent Neural Networks (RNNs). Recurrent networks, such as LSTM and GRU networks , are
characterized by a recurrent structure, that is, some neurons
are connected in a loop. This structure enables memorizing
information and allows RNNs to operate on sequences
of data . As a result, RNNs have been successfully
applied in security tasks involving sequential data, such as
the recognition of functions in native code or the
discovery of vulnerabilities in software . Interpreting
the prediction of an RNN is also difﬁcult, as the relevance
of an input feature depends on the sequence of previously
processed features.
2.2. Explanation Strategies
Given the different architectures and the complexity
of many neural networks, decoding the entire decision
process is a challenging task that currently cannot be solved
adequately. However, there exist several recent methods
that enable explaining individual predictions of a neural
network instead of the complete decision process [e.g.,
5, 21, 36, 47, 54]. We focus on this form of explainable
learning that can be formally deﬁned as follows:
Deﬁnition 1. Given an input vector x = (x1, . . . , xd),
a neural network N, and a prediction fN(x) = y, an
explanation method determines why the label y has been
selected by N. This explanation is given by a vector r =
(r1, . . . , rd) that describes the relevance of the dimensions
of x for fN(x).
The computed relevance values r are typically real
numbers and can be overlayed with the input in form
of a heatmap, such that relevant features are visually
highlighted. An example of this visualization is depicted
in Figure 1. Positive relevance values are shown in blue
and indicate importance towards the prediction fN(x),
whereas negative values are given in orange and indicate
importance against the prediction. We will use this color
scheme throughout the paper1.
Despite the variety of approaches for computing a
relevance vector for a given neural network and an input, all
approaches can be broadly categorized into two explanation
strategies: black-box and white-box explanations.
Black-box Explanations. These methods operate under a
black-box setting that assumes no knowledge about the
neural network and its parameters. Black-box methods
are an effective tool if no access to the neural network
is available, for example, when a learning service is
audited remotely. Technically, black-box methods rest on an
approximation of the function fN, which enables them to
estimate how the dimensions of x contribute to a prediction.
Although black-box methods are a promising approach
for explaining deep learning, they can be impaired by the
black-box setting and omit valuable information provided
through the network architecture and parameters.
White-box Explanations. These approaches operate under
the assumption that all parameters of a neural network are
known and can be used for determining an explanation.
As a result, these methods do not rely on approximations
1. We use the blue-orange color scheme instead of the typical green-red
scheme to make our paper better accessible to color-blind readers.
and can directly compute explanations for the function fN
on the structure of the network. In practice, predictions
and explanations are often computed from within the same
system, such that the neural network is readily available
for generating explanations. This is usually the case for
stand-alone systems for malware detection, binary analysis,
and vulnerability discovery. However, several white-box
methods are designed for speciﬁc network layouts from
computer vision and not applicable to all considered
architectures [e.g., 43, 46, 54].
Black-box and white-box explanation methods often
share similarities with concepts of adversarial learning
and feature selection, as these also aim at identifying
features related to the prediction of a classiﬁer. However,
adversarial learning and feature selection pursue fundamentally different goals and cannot be directly applied for
explaining neural networks. We discuss the differences to
these approaches for the interested reader in Appendix A.
3. Methods and Systems under Test
Before presenting our criteria for evaluating explanation
methods, we ﬁrst introduce the methods and systems under
test. In particular, we cover six methods for explaining
predictions in Section 3.1 and present four security systems
based on deep learning in Section 3.2. For more information about explanation methods we do not evaluate in the
paper [e.g., 12, 17] we refer the reader to the Appendix B.
3.1. Explanation Methods
Table 1 provides an overview of popular explanation
methods along with their support for the different network
architectures. As we are interested in explaining predictions
of security systems, we select those methods for our
study that are applicable to all common architectures. In
the following, we brieﬂy sketch the main idea of these
approaches for computing relevance vectors, illustrating
the technical diversity of explanation methods.
Gradients and IG. One of the ﬁrst white-box methods
to compute explanations for neural networks has been
introduced by Simonyan et al. and is based on
simple gradients. The output of the method is given by
ri = ∂y/∂xi, which the authors call a saliency map.
Here ri measures how much y changes with respect to xi.
Sundararajan et al. extend this approach and propose
Integrated Gradients (IG) that use a baseline x′, for instance
a vector of zeros, and calculate the shortest path from x′
TABLE 1: Popular explanation methods. The support for
different neural network architectures is indicated by .
Methods evaluated in this paper are indicated by *.
Explanation methods
Gradients* , IG* 
LRP* , DeepLift 
PatternNet, PatternAttribution 
DeConvNet , GuidedBP 
CAM , GradCAM 
RTIS , MASK 
LIME* , SHAP* , QII 
LEMNA* 
to x, given by x −x′. To compute the relevance of xi, the
gradients with respect to xi are cumulated along this path
ri = (xi −x′
∂fN(x′ + α(x −x′))
Both gradient-based methods can be applied to all relevant
network architectures and thus are considered in our
comparative evaluation of explanation methods.
LRP and DeepLift. These popular white-box methods
determine the relevance of a prediction by performing a
backward pass through the neural network, starting at
the output layer and performing calculations until the
input layer is reached . The central idea of layer-wise
relevance propagation (LRP) is the use of a conservation
property that needs to hold true during the backward pass.
i is the relevance of the unit i in layer l of the neural
network then
i = · · · =
needs to hold true for all L layers. Similarly, DeepLift
performs a backward pass but takes a reference activation
y′ = fN(x′) of a reference input x′ into account. The
method enforces the conservation law,
ri = y −y′ = ∆y ,
that is, the relevance assigned to the features must sum
up to the difference between the outcome of x and x′.
Both approaches support explaining the decisions of feedforward, convolutional and recurrent neural networks [see
4]. However, as DeepLift and IG are closely related ,
we focus our study on the method ϵ-LRP.
LIME and SHAP. Ribeiro et al. introduce one of
the ﬁrst black-box methods for explaining neural networks
that is further extended by Lundberg and Lee . Both
methods aim at approximating the decision function fN
by creating a series of l perturbations of x, denoted as
˜x1, . . . , ˜xl by setting entries in the vector x to 0 randomly.
The methods then proceed by predicting a label fN(˜xi) =
˜yi for each ˜xi of the l perturbations. This sampling strategy
enables the methods to approximate the local neighborhood
of fN at the point fN(x). LIME approximates the
decision boundary by a weighted linear regression model,
 fN(˜xi) −g(˜xi)
where G is the set of all linear functions and πx is a
function indicating the difference between the input x and
a perturbation ˜x. SHAP follows the same approach
but uses the SHAP kernel as weighting function πx, which
is shown to create Shapley Values when solving the
regression. Shapley Values are a concept from game theory
where the features act as players under the objective of
ﬁnding a fair contribution of the features to the payout—in
this case the prediction of the model. As both approaches
can be applied to any learning model, we study them in
our empirical evaluation.
LEMNA. As last explanation method, we consider
LEMNA, a black-box method speciﬁcally designed for
security applications . It uses a mixture regression
model for approximation, that is, a weighted sum of K
linear models:
πj(βj · x + ϵj).
The parameter K speciﬁes the number of models, the
random variables ϵ = (ϵ1, . . . , ϵK) originate from a normal
distribution ϵi ∼N(0, σ) and π = (π1, . . . , πK) holds the
weights for each model. The variables β1, . . . , βK are the
regression coefﬁcients and can be interpreted as K linear
approximations of the decision boundary near fN(x).
3.2. Security Systems
As ﬁeld of application for the six explanation methods,
we consider four recent security systems that employ deep
learning (see Table 2). The systems cover the three major
architectures/types introduced in Section 2.1 and comprise
between 4 to 6 layers of different types.
TABLE 2: Overview of the considered security systems.
Publication
ESORICS’17 
CCS’18 
CODASPY’17 
VulDeePecker
NDSS’18 
Drebin+. The ﬁrst system uses an MLP for identifying
Android malware. The system has been proposed by Grosse
et al. and builds on features originally developed by
Arp et al. . The network consists of two hidden layers,
each comprising 200 neurons. The input features are statically extracted from Android applications and cover data
from the application’s manifest, such as hardware details
and requested permissions, as well as information based
on the application’s code, such as suspicious API calls
and network addresses. To verify the correctness of our
implementation, we train the system on the original Drebin
dataset , where we use 75 % of the 129,013 Android
application for training and 25 % for testing. Table 3 shows
the results of this experiment, which are in line with the
performance published by Grosse et al. .
Mimicus+. The second system also uses an MLP but
is designed to detect malicious PDF documents. The
system is re-implemented based on the work of Guo
et al. and builds on features originally introduced
by Smutz and Stavrou . Our implementation uses
two hidden layers with 200 nodes each and is trained
with 135 features extracted from PDF documents. These
features cover properties about the document structure,
such as the number of sections and fonts in the document,
and are mapped to binary values as described by Guo et al.
 . For a full list of features, we refer the reader to the
implementation by Šrndi´c and Laskov . For verifying
our implementation, we make use of the original dataset
that contains 5,000 benign and 5,000 malicious PDF ﬁles
and again split the dataset into 75 % for training and 25 %
TABLE 3: Performance of the re-implemented security
systems on the original datasets.
VulDeePecker
for testing. Our results are shown in Table 3 and come
close to a perfect detection.
DAMD. The third security system studied in our evaluation
uses a CNN for identifying malicious Android applications . The system processes the raw Dalvik bytecode
of Android applications and its neural network is comprised
of six layers for embedding, convolution, and max-pooling
of the extracted instructions. As the system processes
entire applications, the number of features depends on
the size of the applications. For a detailed description of
this process, we refer the reader to the publication by
McLaughlin et al. . To replicate the original results,
we apply the system to data from the Malware Genome
Project . This dataset consists of 2,123 applications in
total, with 863 benign and 1,260 malicious samples. We
again split the dataset into 75 % of training and 25 % of
testing data and obtain results similar to those presented
in the original publication.
VulDeePecker. The fourth system uses an RNN for
discovering vulnerabilities in source code . The RNN
consists of ﬁve layers, uses 300 LSTM cells , and
applies a word2vec embedding with 200 dimensions
for analyzing C/C++ code. As a preprocessing step, the
source code is sliced into code gadgets that comprise short
snippets of tokens. The gadgets are truncated or padded
to a length of 50 tokens. For verifying the correctness of
our implementation, we use the CWE-119 dataset, which
consists of 39,757 code gadgets, with 10,444 gadgets corresponding to vulnerabilities. In line with the original study,
we split the dataset into 80 % training and 20 % testing
data, and attain a comparable accuracy.
The four selected security systems provide a diverse
view on the current use of deep learning in security.
Drebin+ and Mimicus+ are examples of systems that make
use of MLPs for detecting malware. However, they differ in
the dimensionality of the input: While Mimicus+ works on
a small set of engineered features, Drebin+ analyzes inputs
with thousands of dimensions. DAMD is an example of a
system using a CNN in security and capable of learning
from large inputs, whereas VulDeePecker makes use of an
RNN, similar to other learning-based approaches analyzing
program code [e.g., 10, 41, 53].
4. Evaluation Criteria
In light of the broad range of available explanation
methods, the practitioner is in need of criteria for selecting
the best method for a security task at hand. In this section,
we develop these criteria and demonstrate their utility in
different examples. Before doing so, however, we address
another important question: Do the considered explanation
methods provide different results? If the methods generated
VulDeePecker
Figure 3: Comparison of the top-10 features for the
different explanation methods. An average value of 1
indicates identical top-10 features and a value of 0 indicates
no overlap.
similar explanations, criteria for their comparison would
be less important and any suitable method could be
chosen in practice.
To answer this question, we investigate the top-k
features of the six explanation methods when explaining
predictions of the security systems. That is, we compare
the set Ti of the k features with the highest relevance from
method i with the set Tj of the k features with the highest
relevance from method j. In particular, we compute the
intersection size
IS(i, j) = |Ti ∩Tj|
as a measure of similarity between the two methods. The
intersection size lies between 0 and 1, where 0 indicates
no overlap and 1 corresponds to identical top-k features.
A visualization of the intersection size averaged over
the samples of the four datasets is shown in Figure 3.
We choose k = 10 according to a typical use case of
explainable learning: An expert investigates the top-10
features to gain insights on a prediction. For DAMD,
we use k = 50, as the dataset is comprised of long
opcode sequences. We observe that the top features of the
explanation methods differ considerably. For example, in
the case of VulDeePecker, all methods determine different
top-10 features. While we notice some similarity between
the methods, it becomes clear that the methods cannot be
simply interchanged, and there is a need for measurable
evaluation criteria.
4.1. General Criteria: Descriptive Accuracy
As the ﬁrst evaluation criteria, we introduce the descriptive accuracy. This criterion reﬂects how accurate
an explanation method captures relevant features of a
prediction. As it is difﬁcult to assess the relation between
features and a prediction directly, we follow an indirect
data = NULL;
data = new wchar_t ;
data = L’\\0’;
wchar_t source ;
wmemset(source, L’C’, 100-1);
source = L’\\0’;
memmove(data, source, 100*sizeof(wchar_t));
(a) Original code
VAR0 [ INT0 ] = STR0 ;
wchar_t VAR0 [ INT0 ] ;
wmemset ( VAR0 , STR0 , INT0 - INT1 ) ;
VAR0 [ INT0 - INT1 ] = STR0 ;
memmove ( VAR0 , VAR1 , INT0 * sizeof ( wchar_t ) ) ;
(b) Integrated Gradients
VAR0 [ INT0 ] = STR0 ;
wchar_t VAR0 [ INT0 ] ;
wmemset ( VAR0 , STR0 , INT0 - INT1 ) ;
VAR0 [ INT0 - INT1 ] = STR0 ;
memmove ( VAR0 , VAR1 , INT0 * sizeof ( wchar_t ) ) ;
Figure 4: Explanations for a program slice from the
VulDeePecker dataset using (b) Integrated Gradients and
strategy and measure how removing the most relevant
features changes the prediction of the neural network.
Deﬁnition 2. Given a sample x, the descriptive accuracy
(DA) is calculated by removing the k most relevant features
x1, . . . , xk from the sample, computing the new prediction
using fN and measuring the score of the original prediction
class c without the k features,
 x | x1 = 0, . . . , xk = 0
If we remove relevant features from a sample, the
accuracy should decrease, as the neural network has less
information for making a correct prediction. The better
the explanation, the quicker the accuracy will drop, as the
removed features capture more context of the predictions.
Consequently, explanation methods with a steep decline of
the descriptive accuracy provide better explanations than
methods with a gradual decrease.
To demonstrate the utility of the descriptive accuracy,
we consider a sample from the VulDeePecker dataset,
which is shown in Figure 4(a). The sample corresponds
to a program slice and is passed to the neural network as
a sequence of tokens. Figures 4(b) and 4(c) depict these
tokens overlayed with the explanations of the methods
Integrated Gradients (IG) and LIME, respectively. Note
that the VulDeePecker system truncates all code snippets
to a length of 50 tokens before processing them through
the neural network .
The example shows a simple buffer overﬂow which
originates from an incorrect calculation of the buffer
size in line 7. The two explanation methods signiﬁcantly
differ when explaining the detection of this vulnerability.
While IG highlights the wmemset call as important, LIME
highlights the call to memmove and even marks wmemset
as speaking against the detection. Measuring the descriptive accuracy can help to determine which of the two
explanations reﬂects the prediction of the system better.
TABLE 4: Explanations of LRP and LEMNA for a sample
of the GoldDream family from the DAMD dataset.
invoke-virtual
invoke-virtual
move-result-object
move-result-object
const-string
const-string
invoke-virtual
invoke-virtual
move-result-object
move-result-object
check-cast
check-cast
array-length
array-length
array-length
array-length
aget-object
aget-object
4.2. General Criteria: Descriptive Sparsity
Assigning high relevance to features which impact a
prediction is a necessary prerequisite for good explanations.
However, a human analyst can only process a limited
number of these features, and thus we deﬁne the descriptive
sparsity as a further criterion for comparing explanations
methods as follows:
Deﬁnition 3. The descriptive sparsity is measured by
scaling the relevance values to the range [−1, 1], computing
a normalized histogram h of them and calculating the mass
around zero (MAZ) deﬁned by
h(x)dx for r ∈ .
The MAZ can be thought of as a window which starts
at 0 and grows uniformly into the positive and negative
direction of the x axis. For each window, the fraction
of relevance values that lies in the window is evaluated.
Sparse explanations have a steep rise in MAZ close to
0 and are ﬂat around 1, as most of the features are not
marked as relevant. By contrast, dense explanations have
a notable smaller slope close to 0, indicating a larger set
of relevant features. Consequently, explanation methods
with a MAZ distribution peaking at 0 should be preferred
over methods with less pronounced distributions.
As an example of a sparse and dense explanation,
we consider two explanations generated for a malicious
Android application of the DAMD dataset. Table 4 shows
a snapshot of these explanations, covering opcodes of the
onReceive method. LRP provides a crisp representation in
this setting, whereas LEMNA marks the entire snapshot as
relevant. If we normalize the relevance vectors to [−1, 1]
and focus on features above 0.2, LRP returns only 14
relevant features for investigation, whereas LEMNA returns
2,048 features, rendering a manual examination tedious.
It is important to note that the descriptive accuracy and
the descriptive sparsity are not correlated and must both
be satisﬁed by an effective explanation method. A method
marking all features as relevant while highlighting a few
ones can be accurate but is clearly not sparse. Vice versa, a
method assigning high relevance to very few meaningless
features is sparse but not accurate.
TABLE 5: Explanations for the Android malware FakeInstaller generated for Drebin+ using Gradients and SHAP.
feature::android.hardware.touchscreen
feature::android.hardware.touchscreen
intent::android.intent.category.LAUNCHER
intent::android.intent.category.LAUNCHER
real_permission::android.permission.INTERNET
real_permission::android.permission.INTERNET
api_call::android/webkit/WebView
api_call::android/webkit/WebView
intent::android.intent.action.MAIN
intent::android.intent.action.MAIN
url::translator.worldclockr.com
url::translator.worldclockr.com
url::translator.worldclockr.com/android.html
url::translator.worldclockr.com/android.html
permission::android.permission.INTERNET
permission::android.permission.INTERNET
activity::.Main
activity::.Main
4.3. Security Criteria: Completeness
After introducing two generic evaluation criteria, we
start focusing on aspects that are especially important for
the area of security. In a security system, an explanation
method must be capable of creating proper results in all
possible situations. If some inputs, such as pathological
data or corner cases, cannot be processed by an explanation method, an adversary may trick the method into
producing degenerated results. Consequently, we propose
completeness as the ﬁrst security-speciﬁc criterion.
Deﬁnition 4. An explanation method is complete, if it
can generate non-degenerated explanations for all possible
input vectors of the prediction function fN.
Several white-box methods are complete by deﬁnition,
as they calculate relevance vectors directly from the
weights of the neural network. For black-box methods, however, the situation is different: If a method approximates
the prediction function fN using random perturbations,
it may fail to derive a valid estimate of fN and return
degenerated explanations. We investigate this phenomenon
in more detail in Section 5.4.
As an example of this problem, Table 5 shows explanations generated by the methods Gradients and SHAP
for a benign Android application of the Drebin dataset.
The Gradients explanation ﬁnds the touchscreen feature in
combination with the launcher category and the internet
permission as an explanation for the benign classiﬁcation.
SHAP, however, creates an explanation of zeros which
provides no insights. The reason for this degenerated
explanation is rooted in the random perturbations used by
SHAP. By ﬂipping the value of features, these perturbations
aim at changing the class label of the input. As there exist
far more benign features than malicious ones in the case
of Drebin+, the perturbations can fail to switch the label
and prevent the linear regression to work resulting in a
degenerated explanation.
4.4. Security Criteria: Stability
In addition to complete results, the explanations generated in a security system need to be reliable. That is,
relevant features must not be affected by ﬂuctuations and
need to remain stable over time in order to be useful for
an expert. As a consequence, we deﬁne stability as another
security-speciﬁc evaluation criterion.
Deﬁnition 5. An explanation methods is stable, if the
generated explanations do not vary between multiple runs.
TABLE 6: Two explanations from LEMNA for the same
sample computed in different runs.
LEMNA (Run 1)
LEMNA (Run 2)
pos_page_min
pos_page_min
count_javascript
count_javascript
pos_acroform_min
pos_acroform_min
ratio_size_page
ratio_size_page
pos_image_min
pos_image_min
pos_image_max
pos_image_max
count_page
count_page
len_stream_avg
len_stream_avg
pos_page_avg
pos_page_avg
count_stream
count_stream
moddate_tz
moddate_tz
len_stream_max
len_stream_max
count_endstream
count_endstream
That is, for any run i and j of the method, the intersection
size of the top features Ti and Tj should be close to 1,
that is, IS(i, j) > 1 −ϵ for some small threshold ϵ.
The stability of an explanation method can be empirically determined by running the methods multiple times
and computing the average intersection size, as explained
in the beginning of this section. White-box methods are
deterministic by construction since they perform a ﬁxed
sequence of computations for generating an explanation.
Most black-box methods, however, require random perturbations to compute their output which can lead to different
results for the same input. Table 6, for instance, shows the
output of LEMNA for a PDF document from the Mimicus+
dataset over two runs. Some of the most relevant features
from the ﬁrst run receive very little relevance in the second
run and vice versa, rendering the explanations unstable.
We analyze these instabilities of the explanation methods
in Section 5.5.
4.5. Security Criteria: Efﬁciency
When operating a security system in practice, explanations need to be available in reasonable time. While
low run-time is not a strict requirement in general, time
differences between minutes and milliseconds are still
signiﬁcant. For example, when dealing with large amounts
of data, it might be desirable for the analyst to create
explanations for every sample of an entire class. We thus
deﬁne efﬁciency as a further criterion for explanation
methods in security applications.
Deﬁnition 6. We consider a method efﬁcient if it enables
providing explanations without delaying the typical work-
ﬂow of an expert.
As the workﬂow depends on the particular security
task, we do not deﬁne concrete run-time numbers, yet we
provide a negative example as an illustration. The runtime of the method LEMNA depends on the size of the
inputs. For the largest sample of the DAMD dataset with
530,000 features, it requires about one hour for computing
an explanation, which obstructs the workﬂow of inspecting
Android malware severely.
4.6. Security Criteria: Robustness
As the last criterion, we consider the robustness of explanation methods to attacks. Recently, several attacks [e.g.,
14, 44, 55] have shown that explanation methods may
suffer from adversarial perturbations and can be tricked
into returning incorrect relevance vectors, similarly to
adversarial examples . The objective of these attacks
is to disconnect the explanation from the underlying
prediction, such that arbitrary relevance values can be
generated that do not explain the behavior of the model.
Deﬁnition 7. An explanation method is robust if the
computed relevance vector cannot be decoupled from the
prediction by an adversarial perturbation.
Unfortunately, the robustness of explanation methods
is still not well understood and, similarly to adversarial
examples, guarantees and strong defenses have not been
established yet. To this end, we assess the robustness of
the explanation methods based on the existing literature.
5. Evaluation
Equipped with evaluation criteria for comparing explanation methods, we proceed to empirically investigate
these in different security tasks. To this end, we implement
a comparison framework that integrates the six selected
explanation methods and four security systems.
5.1. Experimental Setup
White-box Explanations. For our comparison framework, we make use of the iNNvestigate toolbox by
Alber et al. that provides efﬁcient implementations for
LRP, Gradients, and IG. For the security system VulDeePecker, we use our own LRP implementation based on
the publication by Arras et al. . In all experiments, we
set ϵ = 10−3 for LRP and use N = 64 steps for IG. Due
to the high dimensional embedding space of VulDeePecker,
we choose a step count of N = 256 in the corresponding
experiments.
Black-box Explanations. We re-implement LEMNA in
accordance to Guo et al. and use the Python package
cvxpy to solve the linear regression problem with
Fused Lasso restriction . We set the number of mixture
models to K = 3 and the number of perturbations to
l = 500. The parameter S is set to 104 for Drebin+ and
Mimicus+, as the underlying features are not sequential and
to 10−3 for the sequences of DAMD and VulDeePecker [see
21]. Furthermore, we implement LIME with l = 500 perturbations, use the cosine similarity as proximity measure,
and employ the regression solver from the scipy package
using L1 regularization. For SHAP we make use of the
open-source implementation by Lundberg and Lee 
including the KernelSHAP solver.
TABLE 7: Descriptive accuracy (DA) and sparsity (MAZ)
for the different explanation methods.
VulDeePecker
(a) Area under the DA curves from Figure 5.
VulDeePecker
(b) Area under MAZ curves from Figure 5.
5.2. Descriptive Accuracy
We start our evaluation by measuring the descriptive
accuracy (DA) of the explanation methods as deﬁned in
Section 4.1. In particular, we successively remove the
most relevant features from the samples of the datasets
and measure the decrease in the classiﬁcation score. For
Drebin+ and Mimicus+, we remove features by setting the
corresponding dimensions to 0. For DAMD, we replace
the most relevant instructions with the no-op opcode, and
for VulDeePecker we substitute the selected tokens with
an embedding-vector of zeros.
The top row in Figure 5 shows the results of this
experiment. As the ﬁrst observation, we ﬁnd that the
DA curves vary signiﬁcantly between the explanation
methods and security systems. However, the methods
IG and LRP consistently obtain strong results in all
settings and show steep declines of the descriptive accuracy.
Only on the VulDeePecker dataset, the black-box method
LIME can provide explanations with comparable accuracy.
Notably, for the DAMD dataset, IG and LRP are the only
methods to generate real impact on the outcome of the
classiﬁer. For Mimicus+, IG, LRP and Gradients achieve a
perfect accuracy decline after only 25 features and thus the
white-box explanation methods outperform the black-box
methods in this experiment.
Table 7(a) shows the area under curve (AUC) for the
descriptive accuracy curves from Figure 5. We observe
that IG is the best method over all datasets—lower values
indicate better explanations—followed by LRP. In comparison to other methods it is up to 48 % better on average.
# Removed features
# Removed features
# Removed features
VulDeePecker
# Removed features
Interval size
Interval size
Interval size
Interval size
KernelSHAP
Figure 5: Descriptive accuracy and sparsity for the considered explanation methods. Top row: Average descriptive
accuracy (ADA); bottom row: sparsity measured as mass around zero (MAZ).
Intuitively, this considerable difference between the whitebox and black-box methods makes sense, as white-box
approaches can utilize internal information of the neural
networks that are not available to black-box methods.
5.3. Descriptive Sparsity
We proceed by investigating the sparsity of the generated explanations with the MAZ score deﬁned in Section 4.2. The second row in Figure 5 shows the result of
this experiment for all datasets and methods. We observe
that the methods IG, LRP, and Gradients show the steepest
slopes and assign the majority of features little relevance,
which indicates a sparse distribution. By contrast, the other
explanation methods provide ﬂat slopes of the MAZ close
to 0, as they generate relevance values with a broader
range and thus are less sparse.
For Drebin+ and Mimicus+, we observe an almost
identical level of sparsity for LRP, IG and Gradients
supporting the ﬁndings from Figure 3. Interestingly, for
VulDeePecker, the MAZ curve of LEMNA shows a strong
increase close to 1, indicating that it assigns high relevance
to a lot of tokens. While this generally is undesirable,
in case of LEMNA, this is founded in the basic design
and the use of the Fused Lasso constraint. In case of
DAMD, we see a massive peak at 0 for IG, showing that
it marks almost all features as irrelevant. According to the
previous experiment, however, it simultaneously provides
a very good accuracy on this data. The resulting sparse
and accurate explanations are particularly advantageous for
a human analyst since the DAMD dataset contains samples
with up to 520,000 features. The explanations from IG
provide a compressed yet accurate representation of the
sequences which can be inspected easily.
We summarize the performance on the MAZ metric
by calculating the area under curve and report it in
Table 7(b). A high AUC indicates that more features have
been assigned a relevance close to 0, that is, the explanation
is more sparse. We ﬁnd that the best methods again are
white-box approaches, providing explanations that are up
to 50 % sparser compared to the other methods in this
experiment.
5.4. Completeness of Explanations
We further examine the completeness of the explanations. As shown in Section 4.3, some explanation methods
can not calculate meaningful relevance values for all inputs.
In particular, perturbation-based methods suffer from this
problem, since they determine a regression with labels
derived from random perturbations. To investigate this
problem, we monitor the creation of perturbations and
their labels for the different datasets.
When creating perturbations for some sample x it is
essential for black-box methods that a fraction p of them
is classiﬁed as belonging to the opposite class of x. In an
optimal case one can achieve p ≈0.5, however during our
experiments we ﬁnd that 5 % can be sufﬁcient to calculate
a non-degenerated explanation in some cases. Figure 6
shows for each value of p and all datasets the fraction
of samples remaining when enforcing a percentage p of
perturbations from the opposite class.
In general, we observe that creating malicious perturbations from benign samples is a hard problem, especially for
Drebin+ and DAMD. For example, in the Drebin+ dataset
only 31 % of the benign samples can obtain a p value
of 5 % which means that more than 65 % of the whole
dataset suffer from degenerated explanations. A detailed
calculation for all datasets with a p value of 5 % can be
found in Table 12 in the Appendix C.
The problem of incomplete explanations is rooted in the
imbalance of features characterizing malicious and benign
data in the datasets. While only few features make a sample
malicious, there exists a large variety of features turning
Perturbations from Class+
Samples remaining
Perturbations from Class-
Samples remaining
VulDeePecker
Figure 6: Perturbation label statistics of the datasets. For
each percentage of perturbations from the other class the
percentage of samples achieving this number is shown.
a sample benign. As a consequence, randomly setting
malicious features to zero leads to a benign classiﬁcation,
while setting benign features to zero usually does not
impact the prediction. As a consequence, it is often not
possible to explain predictions for benign applications and
the analyst is stuck with an empty explanation.
In summary, we argue that perturbation-based explanation methods should only be used in security settings
where incomplete explanations can be compensated by
other means. In all other cases, one should refrain from
using these black-box methods in the context of security.
5.5. Stability of Explanations
We proceed to evaluate the stability of the explanation
methods when processing inputs from the four security
systems. To this end, we apply the explanations to the
same samples over multiple runs and measure the average
intersection size between the runs.
Table 8 shows the average intersection size between
the top k features for three runs of the methods as deﬁned
in Equation 1. We use k = 10 for all datasets except
for DAMD where we use k = 50 due to the larger input
space. Since the outputs of Gradients, IG, and LRP are
deterministic, they reach the perfect score of 1.0 in all
settings and thus do not suffer from limitations concerning
stability.
For the perturbation-based methods, however, stability
poses a severe problem since none of those methods obtains
a intersection size of more than 0.5. This indicates that
on average half of the top features do not overlap when
computing explanations on the same input. Furthermore,
we see that the assumption of locality of the perturbationbased methods does not apply for all models under test,
since the output is highly dependent on the perturbations
used to approximate the decision boundary. Therefore, the
best methods for the stability criterion beat the perturbationbased methods by a factor of at least 2.5 on all datasets.
5.6. Efﬁciency of Explanations
We ﬁnally examine the efﬁciency of the different
explanation methods. Our experiments are performed on
a regular server system with an Intel Xeon E5 v3 CPU
at 2.6 GHz. It is noteworthy that the methods Gradients,
TABLE 8: Average intersection size between top features
for multiple runs. Values close to one indicate greater
stability.
VulDeePecker
IG and LRP can beneﬁt from computations on a graphical
processing unit (GPU), therefore we report both results
but use only the CPU results to achieve a fair comparison
with the black-box methods.
Table 9 shows the average run-time per input for all
explanations methods and security systems. We observe
that Gradients and LRP achieve the highest throughput in
general beating the other methods by orders of magnitude.
This advantage arises from the fact that data can be
processed batch-wise for methods like Gradients, IG, and
LRP, that is, explanations can be calculated for a set
of samples at the same time. The Mimicus+ dataset, for
example, can be processed in one batch resulting in a
speed-up factor of more than 16,000× over the fastest
black-box method. In general we note that the white-box
methods Gradients and LRP achieve the fastest run-time
since they require a single backwards-pass through the
network. Moreover, computing these methods on a GPU
results in additional speedups of a factor up to three.
TABLE 9: Run-time per sample in seconds. Note the range
of the different times from microseconds to minutes.
VulDeePecker
3.1 × 10−2
2.8 × 10−2
7.4 × 10−1
3.0 × 10−2
4.3 × 10−1
8.1 × 10−3
7.8 × 10−6
1.1 × 10−2
7.6 × 10−4
1.1 × 10−1
5.4 × 10−5
6.9 × 10−1
4.0 × 10−1
8.4 × 10−3
1.7 × 10−6
1.3 × 10−2
2.9 × 10−2
VulDeePecker
7.4 × 10−3
3.9 × 10−6
3.5 × 10−3
3.0 × 10−4
1.5 × 10−2
3.9 × 10−5
3.0 × 10−1
1.3 × 10−1
7.3 × 10−3
1.6 × 10−6
7.8 × 10−3
1.1 × 10−2
The run-time of the black-box methods increases for
high dimensional datasets, especially DAMD, since the
regression problems need to be solved in higher dimensions.
While the speed-up factors are already enormous, we have
not even included the creation of perturbations and their
classiﬁcation, which consume additional run-time as well.
5.7. Robustness of Explanations
Recently, multiple authors have shown that adversarial
perturbations are also applicable against explanation methods and can manipulate the generated relevance values.
Given a classiﬁcation function f, an input x and a target
class ct the goal of an adversarial perturbation is to ﬁnd
˜x = x + δ such that δ is minimal but at the same time
f(˜x) = ct ̸= f(x).
TABLE 10: Results of the evaluated explanation methods. The last column summarizes these metrics in a rating
comprising three levels: strong( ), medium ( ), and weak (#).
Explanation Method
Completeness
Robustness
Overall Rating
2.1 × 10−1 s
1.8 × 102 s
# # # # # #
5.0 × 10−3 s
3.0 × 10−1 s
5.0 × 10−2 s
For an explanation method gf(x) Zhang et al. 
propose to solve
 f(˜x), ct
 gf(˜x), gf(x)
where dp and de are distance measures for classes and
explanations of f. The crafted input ˜x is misclassiﬁed by
the network but keeps an explanation very close to the one
of x. Dombrowski et al. show that many white-box
methods can be tricked to produce an arbitrary explanation
et without changing the classiﬁcation by solving
 gf(˜x), et
 f(˜x), f(x)
While the aforementioned attacks are constructed for
white-box methods, Slack et al. have recently proposed an attack against LIME and SHAP. They show
that the perturbations, which have to be classiﬁed to
create explanations, deviate strongly from the original
data distribution and hence are easily distinguishable from
original data samples. With this knowledge an adversary
can use a different model ˜f to classify the perturbations
and create arbitrary explanations to hide potential biases
of the original model. Although LEMNA is not considered
by Slack et al. , it can be attacked likewise since it
relies on perturbation labels as well.
The white-box attacks by Zhang et al. and Dombrowski et al. require access to the model parameters
which is a technical hurdle in practice. Similarly, however,
the black-box attack by Slack et al. needs to bypass the
classiﬁcation process of the perturbations to create arbitrary
explanations which is equally difﬁcult. A further problem
of all attacks in the security domain are the discrete
input features: For images, an adversarial perturbation δ is
typically small and imperceptible, while binary features,
as in the Drebin+ dataset, require larger changes with
|δ| ≥1. Similarly, for VulDeePecker and DAMD, a
direct application of existing attacks will likely result in
broken code or invalid behavior. Adapting these attacks
seems possible but requires further research on adversarial
learning in structured domains.
Based on this analysis, we conclude that explanation
methods are not robust and vulnerable to different attacks.
Still, these attacks require access to speciﬁc parts of the
victim system as well as further extenions to work in
discrete domains. As a consequence, the robustness of the
methods is difﬁcult to assess and further work is needed
to establish a better understanding of this threat.
5.8. Summary
A strong explanation method is expected to achieve
good results for each criterion and on each dataset. For
example, we have seen that the Gradients method computes
sparse results in a decent amount of time. The features,
however, are not accurate on the DAMD and VulDeePecker
dataset. Equally, the relevance values of SHAP for the
Drebin+ dataset are sparser than those from LEMNA but
suffer from instability. To provide an overview, we average
the performance of all methods over the four datasets and
summarize the results in Table 10.
For each of the six evaluation criteria, we assign each
method one of the following three categories: , , and #.
The category is given to the best explanation method and
other methods with a similar performance. The # category
is assigned to the worst method and methods performing
equally bad. Finally, the category is given to methods
that lie between the best and worst methods.
Based on Table 10, we can see that white-box explanation methods achieve a better ranking than black-box
methods in all evaluation criteria. Due to the direct access
to the parameters of the neural network, these methods
can better analyze the prediction function and are able
to identify relevant features. In particular, IG and LRP
are the best methods overall regarding our evaluation
criteria. They compute results in less than 50 ms in our
benchmark, mark only few features as relevant, and the
selected features have great impact on the decision of
the classiﬁer. These methods also provide deterministic
results and do not suffer from incompleteness. As a result,
we recommend to use these methods for explaining deep
learning in security. However, if white-box access is not
available, we recommend the black-box method LIME as
it shows the best performance in our experiments or to
apply model stealing as shown in the following Section 5.9
to enable the use of white-box methods.
In general, whether white-box or black-box methods
are applicable also depends on who is generating the
explanations: If the developer of a security system wants
to investigate its prediction, direct access to all model
parameters is typically available and white-box methods
can be applied easily. Similarly, if the learning models
are shared between practitioners, white-box approaches
are also the method of choice. If the learning model,
however, is trained by a remote party, such as a machinelearning-as-a-service providers, only black-box methods
are applicable. Likewise, if an auditor or security tester
inspects a proprietary system, black-box methods also
become handy, as they do not require reverse-engineering
and extracting model parameters.
Top-10-Drebin+
Top-10-Mimicus+
Figure 7: Intersection size of the Top-10 features of
explanations obtained from models that were stolen from
the original model of the Drebin+ and Mimicus+ dataset.
5.9. Model Stealing for White-Box Explanations
Our experiments show that practitioners from the
security domain should favor white-box methods over
black-box methods when aiming to explain neural networks.
However, there are cases when access to the parameters of
the system is not available and white-box methods can not
be used. Instead of using black-box methods one could
also use model stealing to obtain an approximation of
the original network . This approach assumes that the
user can predict an unlimited number of samples with the
model to be explained. The obtained predictions can then
be used to train a surrogate model which might have a
different architecture but a similar behavior.
To evaluate the differences between the explanations
of surrogate models to the original ones we conduct an
experiment on the Drebin+ and Mimicus+ datasets as
follows: We use the predictions of the original model
from Grosse et al. which has two dense layers with
200 units each and use these predictions to train three
surrogate models. The number of layers is varied to be
 and the number of units in each layer is always
200 resulting in models with higher, lower and the original
complexity. For each model we calculate explanations via
LRP and compute the intersection size given by Equation 1
for k = 10.
The results in Figure 7 show that the models deliver similar explanations to the original model (IS≈0.7)
although having different architectures for the Drebin+
dataset. However, the similarity between the stolen models
is clearly higher (IS≈0.85). For the Mimicus+ dataset,
we observe a general stability of the learned features at
a lower level (IS≈0.55). These results indicate that the
explanations of the stolen models are better than those
obtained from black-box methods (see Figure 3) but still
deviate from the original model, i.e., there is no global
transferability between the explanations. At all, model
stealing can be considered a good alternative to the usage
of black-box explanation methods.
6. Insights on the Datasets
During the experiments for this paper, we have analyzed various explanations of security systems—not only
quantitatively as discussed in Section 5 but also qualitatively from the perspective of a security analyst. In
this section, we summarize our observations and discuss
insights related to the role of deep learning in security.
TABLE 11: Top-5 features for the Mimicus+ dataset
determined using IG. The right columns show the frequency
in benign and malicious PDF documents, respectively.
Top 5 Feature
count_font
producer_mismatch
pdfid1_num
pos_eof_min
count_javascript
count_trailer
pos_page_avg
count_endobj
createdate_tz
count_action
Moreover, we publish the generated explanations from all
datasets and methods on the project’s website2 in order to
foster future research.
6.1. Insights on Mimicus+
When inspecting explanations for the Mimicus+ system,
we observe that the features for detecting malware are
dominated by count_javascript and count_js, which
both stand for the number of JavaScript elements in
the document. The strong impact of these elements is
meaningful, as JavaScript is frequently used in malicious
PDF documents . However, we also identify features
in the explanations that are non-intuitive. For example,
features like count_trailer that measures the number of
trailer sections in the document or count_box_letter that
counts the number of US letter sized boxes can hardly
be related to security and rather constitute artifacts in the
dataset captured by the learning process.
To further investigate the impact of JavaScript features
on the neural network, we determine the distribution of
the top 5 features from the method IG for each class in
the entire dataset. It turns out that JavaScript appears in
88 % of the malicious documents, whereas only about 6 %
of the benign samples make use of it (see Table 11).
This makes JavaScript an extremely discriminating feature
for the dataset. From a security perspective, this is an
unsatisfying result, as the neural network of Mimicus+
relies on a few indicators for detecting the malicious code
in the documents. An attacker could potentially evade
Mimicus+ by not using JavaScript or obfuscating the
JavaScript elements in the document.
6.2. Insights on Drebin+
During the analysis of the Drebin+ dataset, we notice
that several benign applications are characterized by the
hardware feature touchscreen, the intent ﬁlter launcher,
and the permission INTERNET. These features frequently
occur in benign and malicious applications in the Drebin+
dataset and are not particularly descriptive for benignity.
Note that the interpretation of features speaking for benign
2. 
applications is challenging due to the broader scope and
the difﬁculty in deﬁning benignity. We conclude that the
three features together form an artifact in the dataset that
provides an indicator for detecting benign applications.
For malicious Android applications, the situation is
different: The explanation methods return highly relevant
features that can be linked to the functionality of the
malware. For instance, the requested permission SEND_SMS
or features related to accessing sensitive information, such
as the permission READ_PHONE_STATE and the API call
getSimCountryIso, receive consistently high scores in our
investigartion. These features are well in line with common
malware for Android, such as the FakeInstaller family ,
which is known to obtain money from victims by secretly
sending text messages (SMS) to premium services. Our
analysis shows that the MLP network employed in Drebin+
has captured indicative features directly related to the
underlying malicious activities.
6.3. Insights on VulDeePecker
In contrast to the datasets considered before, the features processed by VulDeePecker resemble lexical tokens
and are strongly interconnected on a syntactical level.
This becomes apparent in the explanations of the method
Integrated Gradients in Figure 4, where adjacent tokens
have mostly equal colors. Moreover, orange and blue
colored features in the explanation are often separated
by tokens with no color, indicating a gradual separation
of positive and negative relevance values.
During our analysis, we notice that it is still difﬁcult
for a human analyst to beneﬁt from the highlighted tokens.
First, an analyst interprets the source code rather than the
extracted tokens and thus maintains a different view on
the data. In Figure 4, for example, the interpretation of the
highlighted INT0 and INT1 tokens as buffer sizes of 50 and
100 wide characters is misleading, since the neural network is not aware of this relation. Second, VulDeePecker
truncates essential parts of the code. In Figure 4, during
the initialization of the destination buffer, for instance,
only the size remains as part of the input. Third, the large
amount of highlighted tokens like semicolons, brackets,
and equality signs seems to indicate that VulDeePecker
overﬁts to the training data at hand.
Given the truncated program slices and the seemingly
unrelated tokens marked as relevant, we conclude that
the VulDeePecker system might beneﬁt from extending
the learning strategy to longer sequences and cleansing
the training data to remove artifacts that are irrelevant for
vulnerability discovery.
6.4. Insights on DAMD
Finally, we consider Android applications from the
DAMD dataset. Due to the difﬁculty of analyzing raw
Dalvik bytecode, we guide our analysis of the dataset by inspecting malicious applications from three popular Android
malware families: GoldDream , DroidKungFu ,
and DroidDream . These families exﬁltrate sensitive
data and run exploits to take full control of the device.
In our analysis of the Dalvik bytecode, we beneﬁt
from the sparsity of the explanations from LRP and IG as
explained in Section 5.3. Analyzing all relevant features
becomes tractable with moderate effort using these methods
and we are able to investigate the opcodes with the highest
relevance in detail. We observe that the relevant opcode
sequences are linked to the malicious functionality.
As an example, Table 4 depicts the opcode sequence,
that is found in all samples of the GoldDream family.Taking
a closer look, this sequence occurs in the onReceive
method of the com.GoldDream.zj.zjReceiver class. In
this function, the malware intercepts incoming SMS and
phone calls and stores the information in local ﬁles before
sending them to an external server. Similarly, we can
interpret the explanations of the other two malware families, where functionality related to exploits and persistent
installation is highlighted in the Dalvik opcode sequences.
For all members of each malware family, the opcode
sequences identiﬁed using the explanation methods LRP
and IG are identical, which demonstrates that the CNN in
the DAMD system has learned an discriminative pattern
from the underlying opcode representation.
7. Conclusion
The increasing application of deep learning in security
renders means for explaining their decisions vitally important. While there exist a wide range of explanation methods
from the area of computer vision and machine learning,
it has been unclear which of these methods are suitable
for security systems. We have addressed this problem and
propose evaluation criteria that enable a practitioner to
compare and select explanation methods in the context of
security. While the importance of these criteria depends
on the particular security task, we ﬁnd that the methods
Integrated Gradients and LRP comply best with all requirements. Hence, we generally recommend these methods for
explaining predictions in security systems.
Aside from our evaluation of explanation methods,
we reveal problems in the general application of deep
learning in security. For all considered systems under test,
we identify artifacts that substantially contribute to the
overall prediction, but are unrelated to the security task.
Several of these artifacts are rooted in peculiarities of the
data. It is likely that the employed neural networks overﬁt
the data rather than solving the underlying task. We thus
conclude that explanations need to become an integral
part of any deep learning system to identify artifacts in
the training data and to keep the learning focused on the
targeted security problem.
Our study is a ﬁrst step for integrating explainable
learning in security systems. We hope to foster a series of
research that applies and extends explanation methods, such
that deep learning becomes more transparent in computer
security. To support this development, we make all our
implementations and datasets publicly available.
Acknowledgements
The authors gratefully acknowledge funding from the
German Federal Ministry of Education and Research
(BMBF) under the projects VAMOS (FKZ 16KIS0534)
and BIFOLD (FKZ 01IS18025B). Furthermore, the authors
acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy EXC 2092 CASA-390781972.