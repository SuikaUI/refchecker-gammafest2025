IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
Classiﬁcation of Hyperspectral Remote Sensing
Images With Support Vector Machines
Farid Melgani, Member, IEEE, and Lorenzo Bruzzone, Senior Member, IEEE
Abstract—This paper addresses the problem of the classiﬁcation of hyperspectral remote sensing images by support vector
machines (SVMs). First, we propose a theoretical discussion and
experimental analysis aimed at understanding and assessing the
potentialities of SVM classiﬁers in hyperdimensional feature
spaces. Then, we assess the effectiveness of SVMs with respect
to conventional feature-reduction-based approaches and their
performances in hypersubspaces of various dimensionalities. To
sustain such an analysis, the performances of SVMs are compared
with those of two other nonparametric classiﬁers (i.e., radial basis
function neural networks and the K-nearest neighbor classiﬁer).
Finally, we study the potentially critical issue of applying binary
SVMs to multiclass problems in hyperspectral data. In particular,
four different multiclass strategies are analyzed and compared:
the one-against-all, the one-against-one, and two hierarchical
tree-based strategies. Different performance indicators have
been used to support our experimental studies in a detailed and
accurate way, i.e., the classiﬁcation accuracy, the computational
time, the stability to parameter setting, and the complexity of
the multiclass architecture. The results obtained on a real Airborne Visible/Infrared Imaging Spectroradiometer hyperspectral
dataset allow to conclude that, whatever the multiclass strategy
adopted, SVMs are a valid and effective alternative to conventional
pattern recognition approaches (feature-reduction procedures
combined with a classiﬁcation method) for the classiﬁcation of
hyperspectral remote sensing data.
Index Terms—Classiﬁcation, feature reduction, Hughes phenomenon, hyperspectral images, multiclass problems, remote
sensing, support vector machines (SVMs).
I. INTRODUCTION
EMOTE sensing images acquired by multispectral sensors, such as the widely used Landsat Thematic Mapper
(TM) sensor, have shown their usefulness in numerous earth
observation (EO) applications. In general, the relatively small
number of acquisition channels that characterizes multispectral sensors may be sufﬁcient to discriminate among different
land-cover classes (e.g., forestry, water, crops, urban areas, etc.).
However, their discrimination capability is very limited when
different types (or conditions) of the same species (e.g., different
types of forest) are to be recognized. Hyperspectral sensors can
be used to deal with this problem. These sensors are characterized by a very high spectral resolution that usually results in
hundreds of observation channels. Thanks to these channels, it
Manuscript received November 4, 2003; revised May 16, 2004. This work
was supported by the Italian Ministry of Education, Research and University
The authors are with the Department of Information and Communication
Technologies, University of Trento, I-38050 Trento, Italy (e-mail: ; ).
Digital Object Identiﬁer 10.1109/TGRS.2004.831865
is possible to address various additional applications requiring
very high discrimination capabilities in the spectral domain (including material quantiﬁcation and target detection). From a
methodological viewpoint, the automatic analysis of hyperspectral data is not a trivial task. In particular, it is made complex by
many factors, such as: 1) the large spatial variability of the hyperspectral signature of each land-cover class; 2) atmospheric
effects; and 3) the curse of dimensionality. In the context of supervised classiﬁcation, one of the main difﬁculties is related to
the small ratio between the number of available training samples
and the number of features. This makes it impossible to obtain
reasonable estimates of the class-conditional hyperdimensional
probability density functions used in standard statistical classi-
ﬁers. As a consequence, on increasing the number of features
given as input to the classiﬁer over a given threshold (which
depends on the number of training samples and the kind of classiﬁer adopted), the classiﬁcation accuracy decreases (this behavior is known as the Hughes phenomenon ).
Much work has been carried out in the literature to overcome this methodological issue. Four main approaches can be
identiﬁed: 1) regularization of the sample covariance matrix; 2)
adaptive statistics estimation by the exploitation of the classi-
ﬁed (semilabeled) samples; 3) preprocessing techniques based
on feature selection/extraction, aimed at reducing/transforming
the original feature space into another space of a lower dimensionality; and 4) analysis of the spectral signatures to model the
The ﬁrst approach uses the multivariate normal (Gaussian)
probability density model, which is a widely accepted statistical
model for optically remotely sensed data. For each information
class, such a model requires the correct estimation of ﬁrst- and
second-order statistics. In the presence of an unfavorable ratio
between the number of available training samples and features,
the common way of estimating the covariance matrix may lead
to inaccurate estimations (that may make it impossible to invert
the covariance matrix in maximum-likelihood (ML) classiﬁers).
Several alternatives and improved covariance matrix estimators
have been proposed to reduce the variance of the estimate for
limited training samples , . The main problem involved by
improved covariance estimators is the risk that the estimated covariance matrices overﬁt the few available training samples and
lead to a poor approximation of statistics for the whole image
to be classiﬁed.
The second approach to overcome the Hughes phenomenon
proposes to use in an iterative way the semilabeled samples obtained after classiﬁcation in order to enhance statistics estimation and to improve classiﬁcation accuracy. Samples are initially
0196-2892/04$20.00 © 2004 IEEE
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
classiﬁed by using the available training samples. Then, the classiﬁed samples, together with the training ones, are exploited iteratively to update the class statistics and, accordingly, the results of the classiﬁcation up to convergence , . The process
of integration between these two typologies of samples (i.e.,
the training and the semilabeled samples) is carried out by the
expectation–maximization (EM) algorithm, which represents a
general and powerful solution to the problem of ML estimation
of statistics in the presence of incomplete data , . The main
advantage of this approach is that it ﬁts the true class distributions better, since a larger portion of the image (available with no
extra cost) contributes to the estimation process. The main problems related to this second approach are two: 1) it is demanding
from the computational point of view and 2) it requires that the
initial class model estimated from the training samples should
match well enough the unlabeled samples in order to avoid divergence of the estimation process and, accordingly, to improve
the accuracy of the model parameter estimation.
In order to overcome the problem of the curse of dimensionality, the third approach proposes to reduce the dimensionality
of the feature space by means of feature selection or extraction
techniques. Feature-selection techniques perform a reduction of
spectral channels by selecting a representative subset of original
features. This can be done following: 1) a selection criterion and
2) a search strategy. The former aims at assessing the discrimination capabilities of a given subset of features according to
statistical distance measures among classes (e.g., Bhattacharyya
distance, Jeffries–Matusita distance, and the transformed divergence measure , ). The latter plays a crucial role in hyperdimensional spaces, since it deﬁnes the optimization approach
necessary to identify the best (or a good) subset of features according to the used selection criterion. Since the identiﬁcation of
the optimal solution is computationally unfeasible, techniques
that lead to suboptimal solutions are normally used. Among
the search strategies proposed in the literature, it is worth mentioning the basic sequential forward selection (SFS) , the
more effective sequential forward ﬂoating selection , and
the steepest ascent (SA) techniques . The feature-extraction
approach addresses the problem of feature reduction by transforming the original feature space into a space of a lower dimensionality, which contains most of the original information.
In this context, the decision boundary feature extraction (DBFE)
method has proved to be a very effective method, capable
of providing a minimum number of transformed features that
achieve good classiﬁcation accuracy. However, this feature-extraction technique suffers from high computational complexity,
which makes it often unpractical. This problem can be overcome by coupling with the projection pursuit (PP) algorithm
 , which plays the role of a preprocessor to the DBFE by
applying a preliminary limited reduction of the feature space
with (hopefully) an almost negligible information loss. An alternative feature-extraction method, whose class-speciﬁc nature
makes it particularly attractive, was proposed by Kumar et al.
 . It is based on a combination of subsets of (highly correlated) adjacent bands into fewer features by means of top-down
and bottom-up algorithms. In general, it is evident that even if
feature-reduction techniques take care of limiting the loss of information, this loss is often unavoidable and may have a negative impact on classiﬁcation accuracy.
Finally, the approach inherited from spectroscopic methods
in analytical chemistry to deal with hyperspectral data is worth
mentioning. The idea behind this approach is that of looking
at the response from each pixel in the hyperspectral image as
a one-dimensional spectral signal (signature). Each information
class is modeled by some descriptors of the shape of its spectra
 , . The merit of this approach is that it signiﬁcantly simpliﬁes the formulation of the hyperspectral data classiﬁcation
problem. However, additional work is required to ﬁnd out appropriate shape descriptors capable of capturing the spectral shape
variability related to each information class accurately.
Other methods also exist that are not included in the group
of the four main approaches discussed above. In particular, it is
interesting to mention the method based on the combination of
different classiﬁers and that based on cluster-space representation .
Recently, particular attention has been dedicated to support
vector machines (SVMs) for the classiﬁcation of multispectral
remote sensing images – . SVMs have often been found
to provide higher classiﬁcation accuracies than other widely
used pattern recognition techniques, such as the maximum
likelihood and the multilayer perceptron neural network classi-
ﬁers. Furthermore, SVMs appear to be especially advantageous
in the presence of heterogeneous classes for which only few
training samples are available. In the context of hyperspectral
image classiﬁcation, some pioneering experimental investigations preliminarily pointed out the effectiveness of SVMs to
analyze hyperspectral data directly in the hyperdimensional
feature space, without the need of any feature-reduction procedure – . In particular, in , the authors found
that a signiﬁcant improvement of classiﬁcation accuracy can
be obtained by SVMs with respect to the results achieved
by the basic minimal-distance-to-means classiﬁer and those
reported in . In order to show its relatively low sensitivity
to the number of training samples, the accuracy of the SVM
classiﬁer was estimated on the basis of different proportions
between the number of training and test samples. As will be
explained in the following section, this mainly depends on the
fact that SVMs implement a classiﬁcation strategy that exploits
a margin-based “geometrical” criterion rather than a purely
“statistical” criterion. In other words, SVMs do not require
an estimation of the statistical distributions of classes to carry
out the classiﬁcation task, but they deﬁne the classiﬁcation
model by exploiting the concept of margin maximization. The
growing interest in SVMs – is conﬁrmed by their successful implementation in numerous other pattern recognition
applications such as biomedical imaging , image compression , and three-dimensional object recognition . Such
an interest is justiﬁed by three main general reasons: 1) their
intrinsic effectiveness with respect to traditional classiﬁers,
which results in high classiﬁcation accuracies and very good
generalization capabilities; 2) the limited effort required for
architecture design (i.e., they involve few control parameters);
and 3) the possibility of solving the learning problem according
to linearly constrained quadratic programming (QP) methods
(which have been studied intensely in the scientiﬁc literature).
However, a major drawback of SVMs is that, from a theoretical
point of view, they were originally developed to solve binary
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
classiﬁcation problems. This drawback becomes even more
evident when dealing with data acquired from hyperspectral
sensors, since they are intrinsically designed to discriminate
among a broad range of land-cover classes that may be very
similar from a spectral viewpoint. The implementation of
SVMs in multiclass classiﬁcation problems can be approached
in two ways , , , . The ﬁrst consists of deﬁning
an architecture made up of an ensemble of binary classiﬁers.
The decision is then taken by combining the partial decisions of
the single members of the ensemble. The second is represented
by SVMs formulated directly as a multiclass optimization
problem. Because of the number of classes that are to be
discriminated simultaneously, the number of parameters to
be estimated increases considerably in a multiclass optimization formulation. This renders the method less stable and,
accordingly, affects the classiﬁcation performances in terms
of accuracy. For this reason, multiclass optimization has not
been as successful as the approach based on the two-class
optimization.
In this paper, we present a theoretical discussion and an accurate experimental analysis that aim: 1) at assessing the properties of SVM classiﬁers in hyperdimensional feature spaces
and 2) at evaluating the impact of the multiclass problem involved by SVM classiﬁers when applied to hyperspectral data
by comparing different multiclass strategies. With regard to the
experimental part of the ﬁrst objective, assessment of SVM effectiveness is carried out through two different experiments. In
the ﬁrst, we propose to compare the performances of SVMs with
those of two other nonparametric classiﬁers applied directly to
the original hyperdimensional feature space: the radial basis
function neural network, which is another kernel-based classi-
ﬁcation method (like SVMs) that uses a different classiﬁcation
strategy based on a “statistical” rather than a “geometrical” criterion; and the K-nearest neighbors classiﬁer, which is widely
used in pattern recognition as a reference classiﬁcation method.
The second experiment consists of a comparison of SVMs with
the classical classiﬁcation approach adopted for hyperspectral
data, i.e., a conventional classiﬁer combined with a feature-reduction technique. This also allows to assess the performances
of SVMs in hypersubspaces of various dimensionalities. As regards the second objective of this work, four different multiclass
strategies are analyzed and compared. In particular, the widely
used one-against-all and one-against-one strategies are considered. In addition, two strategies based on the hierarchical tree
approach are investigated. The experimental studies were carried out on the basis of hyperspectral images acquired by the
Airborne Visible/Infrared Imaging Spectroradiometer (AVIRIS)
sensor in June 1992 on the Indian Pines area (Indiana) . Different performance indicators are used to support our experimental analysis, namely, the classiﬁcation accuracy, the computational time, the stability to parameter setting, and the complexity of the multiclass architecture adopted. Experimental results conﬁrm the signiﬁcant superiority of the SVM classiﬁers
in the context of hyperspectral data classiﬁcation over the conventional classiﬁcation methodologies, whatever the multiclass
strategy adopted to face the multiclass dilemma.
The rest of this paper is organized in four sections. Section II
recalls the mathematical formulation of SVMs and discusses
their potential properties in hyperspectral feature spaces. Section III describes different strategies that can be used to solve
multiclass problems with binary SVMs and that are adopted in
the experiments to assess the impact of the multiclass problem
in a hyperdimensional context. Section IV deals with the experimental phase of the work. Finally, Section V summarizes the
observations and concluding remarks to complete this paper.
II. SVM CLASSIFICATION APPROACH
A. SVM Mathematical Formulation
1) Linear SVM: Linearly Separable Case: Let us consider a
supervised binary classiﬁcation problem. Let us assume that the
training set consists of
vectors from the
-dimensional feature space
. A target
is associated to each vector
. Let us assume that the two classes
are linearly separable. This means that it is possible to ﬁnd at
least one hyperplane (linear surface) deﬁned by a vector
(normal to the hyperplane) and a bias
that can separate
the two classes without errors. The membership decision rule
can be based on the function sgn
is the discriminant function associated with the hyperplane and deﬁned
In order to ﬁnd such a hyperplane, one should estimate
The SVM approach consists in ﬁnding the optimal hyperplane
that maximizes the distance between the closest training sample
and the separating hyperplane. It is possible to express this distance as equal to
with a simple rescaling of the hyperplane parameters
The geometrical margin between the two classes is given by the
. The concept of margin is central in the SVM
approach, since it is a measure of its generalization capability.
The larger the margin, the higher the expected generalization
Accordingly, it turns out that the optimal hyperplane can be
determined as the solution of the following convex quadratic
programming problem:
subject to:
This classical linearly constrained optimization problem can be
translated (using a Lagrangian formulation) into the following
dual problem:
subject to:
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
Optimal separating hyperplane in SVMs for a linearly nonseparable case. White and black circles refer to the classes “+1” and “ 1,” respectively. Support
vectors are indicated by an extra circle.
The Lagrange multipliers
in (5) can be estimated using quadratic programming (QP)
methods . The discriminant function associated with the
optimal hyperplane becomes an equation depending both on
the Lagrange multipliers and on the training samples, i.e.,
is the subset of training samples corresponding to the
nonzero Lagrange multipliers
’s. It is worth noting that the
Lagrange multipliers effectively weight each training sample
according to its importance in determining the discriminant
function. The training samples associated to nonzero weights
are called support vectors. These lie at a distance exactly equal
from the optimal separating hyperplane.
2) Linear SVM: Linearly Nonseparable Case: The SVM
formulation described in the previous subsection holds only
if data are linearly separable. Such an optimistic condition is
difﬁcult to satisfy in the classiﬁcation of real data. In order to
handle nonseparable data, the concept of optimal separating
hyperplane has been generalized as the solution that minimizes
a cost function that expresses a combination of two criteria:
margin maximization (as in the case of linearly separable data)
and error minimization (to penalize the wrongly classiﬁed
samples). The new cost function is deﬁned as
’s are the so-called slack variables introduced to
account for the nonseparability of data, and the constant C
represents a regularization parameter that allows to control the
penalty assigned to errors. The larger the C value, the higher the
penalty associated to misclassiﬁed samples. The minimization
of the cost function expressed in (7) is subject to the following
constraints:
It is worth noting that, in the nonseparable case, two kinds of
support vectors coexist: 1) margin support vectors that lie on
the hyperplane margin and 2) nonmargin support vectors that
fall on the “wrong” side of this margin (Fig. 1).
3) Nonlinear SVM: Kernel Method: A natural way to improve further the separation between two information classes
consists in generalizing the above method to the category of
nonlinear discriminant functions. Accordingly, one may think
of mapping the data through a proper nonlinear transformation
into a higher dimensional feature space
, where a separation between the two classes can be looked
for following the method described in the previous subsections,
i.e., by means of an optimal hyperplane deﬁned by a normal
and a bias
. To identify the latter, one
should solve a dual problem such as the one deﬁned in (5) for
the linearly separable case by replacing the inner products in
the original space
with inner products in the transformed space
. At this point, the main problem
consists of the explicit computation of
, which can prove
expensive and at times unfeasible. The kernel method provides
an elegant and effective way of dealing with this problem. Let
us consider a kernel function that satisﬁes the condition stated
in Mercer’s theorem so as to correspond to some type of inner
product in the transformed (higher) dimensional feature space
[27, pp. 423–424], i.e.,
This kind of kernel function allows to simplify the solution of
the dual problem considerably, since it avoids the computation
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
of the inner products in the transformed space
i.e., as in
subject to:
The ﬁnal result is a discriminant function
conveniently expressed as a function of the data in the original (lower) dimensional feature space
The shape of the discriminant function depends on the kind of
kernel functions adopted. A common example of kernel type
that fulﬁlls Mercer’s condition is the Gaussian radial basis function
is a parameter inversely proportional to the width of the
Gaussian kernel. Another extensively used kernel is the polynomial function of order
expressed as
It is worth underlining that the kernel-based implementation of
SVMs involves the problem of the selection of multiple parameters, including the kernel parameters (e.g., the
parameters for the Gaussian and polynomial kernels, respectively) and
the regularization parameter C. Recently, two interesting automatic techniques have been developed to deal with this issue
 , . They are based on the idea of estimating the parameter values so that: 1) they maximize the margin; and 2)
they minimize the estimate of the expected generalization error.
The latter is expressed in analytical form by the well-known
leave-one-out (LOO) procedure. Optimization of the parameters is then carried out using a gradient descent search over the
space of the parameters.
Since a detailed analysis of the theory of SVMs is beyond the
scope of this paper, we refer the reader to – for greater
detail on SVMs.
B. SVMs in Hyperspectral Feature Spaces
Unlike traditional learning techniques, SVMs do not depend
explicitly on the dimensionality of input spaces. They solve
classical statistical problems such as pattern recognition, regression, and density estimation in high-dimensional spaces . In
greater detail, as stated in the previous subsection, the input feature space is mapped by a kernel transformation into a higher
dimensional space, where it is expected to ﬁnd a linear separation that maximizes the margin between the two classes. In
order to appreciate the potentialities of SVMs in high-dimensional spaces, it is useful to recall the statistical and geometrical
properties of the data in such spaces.
First, in a hyperspectral space, normally distributed samples
(a reasonable assumption for optically remotely sensed data)
tend to fall toward the tails of the density function with virtually no samples falling in the central region . This can be
illustrated by a simple geometric example . Let us consider
between the volume of a sphere of radius
one of a cube deﬁned in the interval
-dimensional space. It is equal to
represents the well-known gamma function. From
(15), it easy to show that the higher the dimensionality of the
space, the lower the volume ratio. Accordingly, the volume of a
hypercube is almost concentrated in its corners. In other words,
turning back to our classiﬁcation problem, the increase in dimensionality makes the space almost empty and results in a
“centrifuge” effect such that data have a tendency to concentrate
close to the tails of the distribution where they are very likely
to be in proximity of decision boundaries between the information classes. This statistical property is of interest potentially to
pattern recognition approaches, such as SVMs, that deﬁne discriminant functions on the basis of samples situated near the
decision boundaries, since the presence of a larger number of
samples in this region allows to generate more accurate and reliable discriminant functions.
In the second place, it is well-known that as the dimensionality of the data increases, the distances between the samples
(and consequently between the information classes) increase
 . In this situation, local neighborhoods are almost certainly
empty, requiring the bandwidth of estimation to be large and
producing the effect of losing accuracy in density estimation for
a statistical classiﬁer . On the contrary, the “geometrical”
nature of SVMs results in a methodology that is not aimed at
estimating the statistical distributions of classes over the entire
hyperdimensional space. Indeed, SVMs are inspired by the following idea:
If you possess a limited amount of information to solve
a problem, try solving it directly and never solve a more
general problem as an intermediate step. The available information may be sufﬁcient for a direct solution, though
insufﬁcient to solve a more general intermediate problem.
[27, p. 12]
In other words, SVMs do not involve a density estimation
problem that can lead to the Hughes effect, but they directly
exploit the geometrical behavior of data (space local emptiness) as they make it more likely to ﬁnd a decision boundary
between classes that results in a small classiﬁcation error. The
above-discussed properties (statistical and geometrical) render
SVMs potentially less sensitive to the curse of dimensionality.
Another important aspect to be pointed out is the intrinsic
good generalization capability of SVMs, which stems from
the selection of the hyperplane that maximizes the geometrical
margin between classes. In a hyperspectral context, the maximum margin solution allows to fully exploit the discrimination
capability of the relatively few training samples available.
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
Accordingly, this solution deals with some of the major problems, such as the large spatial variability of the hyperspectral
signature of each information class, in the best way in terms
of generalization capability, given the limited information
present in the training set. However, it is worth noting that to
solve the problem of the spatial variability of the hyperspectral
signature of classes effectively, good generalization properties
of the classiﬁers should be coupled with other data analysis
techniques.
III. SVMS: MULTICLASS STRATEGIES
As stated in the previous section, SVMs are intrinsically
binary classiﬁers. However, the classiﬁcation of hyperspectral remote sensing data usually involves the simultaneous
discrimination of numerous information classes. In this section, we describe four different strategies of combination of
SVMs considered to evaluate the impact of the multiclass
problem in the context of hyperspectral data classiﬁcation. Let
be the set of
possible labels (information classes) associated with the
-dimensional hyperspectral
of the study area. In the multiclass case, the problem is
to associate to each -dimensional sample
the label of the set
that optimizes a predeﬁned classiﬁcation criterion. In order
to carry out this task, the general approach adopted in strategies
based on binary classiﬁers consists of: 1) deﬁning an ensemble
of binary classiﬁers; and 2) combining them according to some
decision rules.
The deﬁnition of the ensemble of binary classiﬁers involves
the deﬁnition of a set of two-class problems, each modeled with
two groups
of classes (
Targets with values
are assigned to the samples of
, respectively, for each SVM. The selection of these
subsets depends on the kind of approach adopted to combine the
ensemble. Two main approaches can be identiﬁed: the “parallel”
and the “hierarchical tree-based” approaches. In the following,
we describe two multiclass strategies from each approach characterized by different classiﬁcation complexity and computational cost properties.
A. Parallel Approach
1) One-Against-All Strategy: The one-against-all (OAA)
strategy represents the earliest and most common multiclass
approach used for SVMs . It involves a parallel architecture
made up of
SVMs, one for each class (Fig. 2). Each SVM
solves a two-class problem deﬁned by one information class
) against all the others, i.e.,
The “winner-takes-all” rule is used for the ﬁnal decision, i.e.,
the winning class is the one corresponding to the SVM with the
highest output (discriminant function value).
2) One-Against-One Strategy: The main problem of the
OAA strategy is that the discrimination between an information
class and all the others often leads to the estimation of complex
discriminant functions. In addition, a problem with strongly
unbalanced prior probabilities should be solved by each SVM.
The idea behind the one-against-one (OAO) strategy is that
Block diagram of a parallel architecture for solving multiclass
problems with binary SVMs. In the OAA strategy, M is equal to T (i.e., the
number of information classes). By contrast, the OAO strategy involves a larger
number of SVMs and M is given by T(T   1)=2.
of a different reasoning, in which simple classiﬁcation tasks
are made possible thanks to a parallel architecture made up
of a large number of SVMs , . The OAO strategy
SVMs, which model all possible pairwise classiﬁcations. In this case, each SVM carries out a
binary classiﬁcation in which two information classes
are analyzed against each other
by means of a discriminant function
. Consequently, the
grouping becomes
Before the decision process, it is necessary to compute for each
a score function
, which sums the favorable
and unfavorable votes expressed for the considered class
The ﬁnal decision in the OAO strategy is taken on the basis of
the “winner-takes-all” rule, which corresponds to the following
maximization
Sometimes, conﬂict situations may occur between two different
classes characterized by the same score. Such ambiguities
can be solved by selecting the class with the highest prior
probability.
B. Hierarchical Tree-Based Approach
The idea of representing the data analysis process with a hierarchical tree is not new and has been under study in many pattern
recognition application areas. Tree-based classiﬁers have represented an interesting and effective way to structure and solve
complex classiﬁcation problems – . The organization of
information into a hierarchical tree allows to achieve a faster
processing capability and, at times, a higher accuracy of analysis. This is mainly explained by the fact that the nodes of the
tree carry out very focused tasks, meaningless when taken individually but meaningful when taken as a whole. Turning back
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
to our problem, the binary hierarchical tree (BHT) can be seen
as an alternative to the OAA and the OAO strategies, since it
allows to reach a good tradeoff between the number of SVMs
to be used and the complexity of the task assigned to each of
them. Furthermore, the BHT does not implement a global decision scheme after evaluating the local decisions as in the OAA
and OAO strategies. Indeed, the ﬁnal decision is implicitly made
after running through the tree and reaching one of its terminal
Many BHT strategies have been proposed in the literature. In
this paper, we investigate two different binary tree hierarchies
aimed at reducing the computational load required by the OAA
and OAO strategies, especially in the operational classiﬁcation
phase (the off-line training phase is less critical from the viewpoint of the computational time). This can become particularly
important when large hyperspectral images are considered. As
described in the following, both trees exploit the prior probabilities of the classes to deﬁne the hierarchy of binary SVMs.
It is worth noting that alternative strategies that also exploit the
underlying afﬁnities among the individual classes to deﬁne the
binary trees (like in ) could be considered.
1) BHT-Balanced Branches Strategy: In the BHT-balanced
branches (BHT-BB) strategy, the tree is deﬁned in such a way
that each node (SVM) discriminates between two groups of
with similar cumulative prior probabilities.
Fig. 3(a) shows an example of tree that can be found with the
BHT-BB strategy for a general
-class classiﬁcation problem.
The algorithm that implements the BHT-BB strategy is described as follows:
Step 0: Root Node
—Set level index
into two groups
Step 1:k-Level Branching
into two groups
into two groups
Step 2: Stop Condition
such that Card
Step 1. Otherwise, Stop.
2) BHT-One Against All Strategy: The second binary treebased hierarchy, called BHT-one against all (BHT-OAA), represents a simpliﬁcation of the OAA strategy obtained through its
implementation in a hierarchical context. To this end, we propose to deﬁne the tree in such a way that each node discriminates between two groups of classes
represents the information class with the highest prior probability among those belonging to
. This kind of hierarchy leads to a tree with only one single branch as depicted in
Examples of BHTs for a T-class classiﬁcation problem. (a) BHT-BB.
(b) BHT-OAA.
NUMBER OF TRAINING AND TEST SAMPLES USED IN THE EXPERIMENTS
Fig. 3(b). The algorithm of the BHT-OAA strategy is drawn up
in the following:
Step 0: Root Node
—Set level index
into two groups
Step 1: k-Level Branching
into two groups
Step 2: Stop Condition
, go to Step 1. Otherwise, Stop.
It is worth noting that both BHT strategies allow to reduce the
number of required SVMs from
, respectively,
for the OAA and OAO strategies, to
. Since the classiﬁcation time depends linearly on the number of SVMs and since
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
BEST OVERALL AND CLASS-BY-CLASS ACCURACIES, AND COMPUTATIONAL TIMES ACHIEVED ON THE TEST SET
BY THE DIFFERENT CLASSIFIERS IN THE ORIGINAL HYPERSPECTRAL SPACE
classiﬁcation tasks of medium complexity are assigned to the
SVMs of the tree, we expect a lower classiﬁcation time required
by the two BHT-based strategies with respect to both the OAO
and, especially, the standard OAA strategies.
IV. EXPERIMENTAL RESULTS
A. Dataset Description and Experiment Design
The hyperspectral dataset used in our experiments is a section of a scene taken over northwest Indiana’s Indian Pines by
the AVIRIS sensor in 1992 . From the 220 spectral channels
acquired by the AVIRIS sensor, 20 channels were discarded because affected by atmospheric problems. From the 16 different
land-cover classes available in the original ground truth, seven
were discarded, since only few training samples were available
for them (this makes the experimental analysis more signiﬁcant
from the statistical viewpoint). The remaining nine land-cover
classes were used to generate a set of 4757 training samples
(used for learning the classiﬁers) and a set of 4588 test samples (exploited for assessing their accuracies) (see Table I). The
experiments were run on a Sun Ultra 80 workstation.
The experimental analysis was organized into three main
experiments. The ﬁrst aims at analyzing the effectiveness
of SVMs in classifying hyperspectral images directly in the
original hyperdimensional feature space. A comparison with
two other nonparametric classiﬁers is provided as well as an
assessment of the stability of these three classiﬁcation methods
versus the setting of their parameters. In the second experiment,
SVMs are compared with the classical approach adopted for
hyperspectral data classiﬁcation, that is a conventional pattern recognition system made up of a classiﬁcation method
combined with a feature-reduction technique. In these two
experiments, we adopted the most popular multiclass strategy
used for SVMs, that is the OAA strategy. Finally, the third
experiment aims at analyzing and comparing the effectiveness
of the different multiclass strategies described in the previous
section, that is the OAA, OAO, BHT-BB, and BHT-OAA
strategies.
B. Results of Experiment 1: Classiﬁcation in the Original
Hyperdimensional Feature Space
SVMs were compared with two widely used nonparametric
classiﬁers: a radial basis functions (RBFs) neural network
trained with the technique described in and a conventional
K-nearest neighbors (K-nn) classiﬁer. The choice of the RBF
classiﬁer is motivated by the fact that it is a kernel-based
ANALYSIS OF THE STABILITY OF THE OVERALL CLASSIFICATION ACCURACY
AND OF THE COMPUTATIONAL TIME VERSUS THE SETTING OF THE
PARAMETERS OF THE DIFFERENT CLASSIFIERS
method (like SVMs), which adopts a different strategy based
on a “statistical” (rather than a “geometrical”) criterion for
deﬁning the discriminant hyperplane in the transformed kernel
space. The K-nn classiﬁer was considered in our experiments,
since it represents a reference classiﬁcation method in pattern
recognition. However, it is worth noting that we expect it to be
sensitive to the curse of dimensionality. For both classiﬁers,
different trials were carried out to determine empirically the
best related parameters, namely, the number of nodes in the
hidden layer and the variable
, respectively.
In the experiments, we considered two different kinds of
SVMs: a linear SVM (SVM-Linear) which corresponds to an
SVM without kernel transformation, and a nonlinear SVM
based on Gaussian radial basis kernel functions (SVM-RBF).
For both SVMs, the regularization parameter C must be estimated, since data are not ideally separable. In addition, the
nonlinear SVM requires the determination of the width parameter
of the Gaussian radial basis kernels, which tunes
the smoothing of the discriminant function. For the considered
dataset, the best values of the parameter C were 50 and 40
for the linear and nonlinear SVMs, respectively. The optimal
kernel width parameter
of the nonlinear SVM was found
equal to 0.25. These values were estimated empirically on the
basis of the available training samples.
The results in terms of classiﬁcation accuracy and computational time provided by the different classiﬁers are summarized
in Table II. The nonlinear SVM exhibited the best Overall
Accuracy (OA), i.e., the best percentage of correctly classiﬁed
pixels among all the test pixels considered, with a gain of
6.32%, 6.43%, and 9.48% over the linear SVM, the RBF, and
the K-nn classiﬁers, respectively. In terms of class accuracies,
the “corn-min till” class
was the most critical. For this
class, the nonlinear SVM still exhibited the best accuracy
(87.76%), whereas the worst accuracy (61.16%) was obtained
by the K-nn classiﬁer. It is worth noting that, since the K-nn
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
classiﬁer is based on counting the number of nearest neighboring training samples, it requires the feature space to be ﬁlled
in with a signiﬁcant number of training samples to obtain reliable local estimates of the conditional posterior probabilities of
classes. However, in the considered dataset, the small number
of training samples (4757) is not sufﬁcient to ﬁll in a proper
way the emptiness of the hyperdimensional feature space. This
explains the relatively poor classiﬁcation accuracies of the K-nn
classiﬁer. By contrast, SVMs exploit a discriminant model that
is deﬁned on the basis of a particular portion of the training
samples (support vectors). As explained in Section II and
conﬁrmed by the obtained results, the behavior of the class distributions in hyperdimensional spaces makes it more effective
to apply techniques that deﬁne discriminant functions on the
basis of training samples located near the decision boundaries.
Concerning computational cost, the nonlinear SVM exhibited
a reasonable total computational time (given by the sum of the
training and test times) compared to the other three classiﬁers.
It is worth noting that the long computational time required by
the linear SVM (40342 [s]) expresses the difﬁculties encountered by this kind of classiﬁer in the training phase to ﬁnd a
reasonable linear separation between information classes.
In order to assess the robustness of each classiﬁer to the parameter settings, we derived some statistics by looking at the
overall accuracy (OA) and at the total computational time as
random realizations obtained by varying the parameters in a predeﬁned range of values. The results reported in Table III conﬁrm
the superiority of the nonlinear SVM in terms of both mean
overall accuracy (92.64% and 92.51% by varying the parameters
and C, respectively) and in terms of stability (it provided the lowest variances). It is worth noting that the nonlinear
SVM is less sensitive to the choice of the kernel width value
than to the regularization parameter C. The linear SVM showed
the worst stability to the parameter C (overall-accuracy variance
equals 4.94). This is explained by the fact that a linear separation between classes involves a large number of error samples,
which lie on the wrong side of the separating hyperplane. This
makes it more difﬁcult to apply the regularization mechanism
implemented in the SVM formulation, resulting in signiﬁcant
sensitivity of the classiﬁcation accuracy to the value of the regularization parameter. Concerning the average total computational times, the obtained results conﬁrm the conclusions drawn
above on the basis of the total computational times obtained for
the best parameter values of the four considered classiﬁers.
C. Results of Experiment 2: Feature Reduction and
Classiﬁcation
As already discussed in Section I, the traditional approach
adopted to address the problem of the classiﬁcation of hyperspectral data consists of two main phases: 1) reducing the dimensionality of the feature space; and 2) applying the resulting
subset of features to a conventional classiﬁer. In this experiment,
we propose to assess the effectiveness of SVMs with respect
to a traditional feature-reduction-based approach and to evaluate their performances in hypersubspaces of various dimensionalities. To this end, we used the Jeffries–Matusita (JM) interclass distance measure and the steepest ascent (SA) search
strategy to reduce the original hyperdimensional space into
Overall accuracy versus the number of features obtained on the test set
by the four different classiﬁers considered in our investigation (i.e., linear and
nonlinear SVMs, RBF and K-nn classiﬁers).
FIRST- AND SECOND-ORDER STATISTICS OF THE OVERALL ACCURACIES
OBTAINED ON THE TEST SET BY THE DIFFERENT CLASSIFIERS COMBINED
WITH THE SA-BASED FEATURE-SELECTION PROCEDURE FOR A NUMBER
OF FEATURES VARYING FROM 20 TO 200 (WITH A STEP OF 10)
spaces of a lower dimensionality (the number of features was
varied from 20 to 200 with a step of 10). The SA technique
formulates the problem of deﬁning the subset of features that
maximizes the JM distance as a discrete optimization problem
in a -dimensional space, which is viewed as a space of binary
strings. It starts with a binary string randomly initialized, and
performs an iterative local optimization of the adopted criterion
function. At each iteration, the criterion is maximized over a
neighborhood of the current solution under a predeﬁned constraint. In our experiments, each subset of selected features was
given as input to all four considered classiﬁers (i.e., linear and
nonlinear SVMs, RBF neural networks, and the K-nn classiﬁer).
Fig. 4 plots the overall accuracy versus the number of selected
features for the four considered classiﬁers. As can be seen, the
obtained results still conﬁrm the strong superiority of nonlinear
SVMs over the other classiﬁers even in lower dimensional feature spaces, with a gain in overall accuracy (averaged over all the
subsets of features) of
% with respect to the linear SVM, the K-nn, and the RBF neural network
classiﬁers (see Table IV). In order to analyze the sensitivity of
each classiﬁer to the Hughes phenomenon, in the same table we
reported the variance of the overall accuracy exhibited by each
classiﬁcation method when varying the number of features from
20 to 200. The lowest sensitivity was again obtained by the nonlinear SVM classiﬁer with a sharp reduction of the variance with
respect to those achieved by the K-nn, the linear SVM, and the
RBF neural network classiﬁers.
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
CLASSIFICATION ACCURACIES YIELDED ON THE TEST SET BY THE DIFFERENT CLASSIFIERS WITH THE SUBSET OF THE BEST 30 FEATURES SELECTED ACCORDING
TO THE SA-BASED FEATURE-SELECTION PROCEDURE. THE DIFFERENCE IN OVERALL ACCURACY (DIFF-OA) FOR EACH CLASSIFIER WITH RESPECT TO THE
ACCURACY ACHIEVED IN THE ORIGINAL HYPERDIMENSIONAL SPACE IS ALSO GIVEN
Table V reports the overall and class-by-class accuracies obtained for the hypersubspace made up of the best 30 selected
features. The choice of this subspace is motivated by the fact that
it represents a good compromise between a low dimensionality
of the feature space and a high classiﬁcation accuracy achieved
on average by the four classiﬁers. In particular, one can see the
greater capacity of the nonlinear SVMs to recognize each information class, with a gain in the average of the class-by-class
accuracies of
% with respect to
the linear SVM, the K-nn, and the RBF neural network classi-
ﬁers. In addition, the same table reports the difference in overall
accuracy (DIFF-OA) for each classiﬁer with respect to the accuracy achieved in the original hyperdimensional space. It is interesting to note the lower difference (associated with the expected
lowest sensitivity to the problem of the curse of dimensionality)
achieved by the SVM-RBF classiﬁer (0.93%). The reduction in
the number of features involved a decrease in accuracy of 3.36%
for the linear SVM classiﬁer. By contrast, signiﬁcant increases
in accuracy of 2.96% and 3.46% were obtained by the conventional K-nn and RBF classiﬁers, respectively, conﬁrming a relatively high sensitivity to the curse of dimensionality.
In order to analyze the complexity of the decision boundaries produced by the nonlinear SVM classiﬁer, we computed
the number of SVs deﬁned in each binary SVM of the OAA
architecture in both the original hyperspace and the hypersubspace consisting of the best 30 selected features. These numbers
are represented graphically in Fig. 5. It can be observed in general that the numbers of SVs are relatively small, except for the
SVM associated with the class
. This suggests that decision
boundaries of moderate complexity were enough to discriminate accurately between the information classes. Furthermore,
as discussed in Section II-B, an important property related to
the “geometrical” nature of SVMs seems conﬁrmed, i.e., that
the classiﬁcation complexity does not depend on the dimension
of the feature space, since the number of SVs is almost similar
in both the original and the reduced spaces.
D. Results of Experiment 3: SVM and Multiclass Strategies
The third (and last) experiment addressed the application of
SVMs to the multiclass problem in the hyperdimensional space.
The different multiclass strategies described in Section III (i.e.,
the OAA, OAO, BHT-BB, and BHT-OAA strategies) were designed and trained using nonlinear SVMs based on the Gaussian
radial basis kernel functions. The trees of SVMs deﬁned for the
BHT-BB and the BHT-OAA strategies are illustrated in Fig. 6.
The class prior probabilities necessary to obtain such trees were
computed on the basis of the training set. After the training
Number of support vectors that characterize each binary SVM of
the multiclass nonlinear SVM classiﬁer (OAA strategy) in both the original
hyperspace (d = 200) and the hypersubspace made up of the best 30 selected
features (d = 30).
phase, the four strategies were analyzed and compared based on
three parameters: 1) classiﬁcation accuracy; 2) computational
time; and 3) architecture complexity. The obtained results are reported in Tables VI and VII. From the viewpoint of the accuracy,
all four strategies resulted in satisfactory results when compared
with the two other nonparametric classiﬁers (i.e., the RBF neural
networks and the K-nn classiﬁer). In greater detail, the OAO
strategy exhibited the best accuracy with a gain in overall accuracy of
% over the BHT-OAA,
the BHT-BB and the OAA strategies, respectively. This suggests that the decomposition of the multiclass problem into an
ensemble of two-class problems of very low-complexity represents an effective way of improving overall discrimination capability. The signiﬁcant reduction in the complexity of the classi-
ﬁcation problem assigned to each SVM of the OAO architecture
is shown by the very small average number of SVs that characterizes each SVM of the same architecture. Indeed, this number
is 130 against 333, 334, and 424 for the BHT-OAA, BHT-BB
and the OAA strategies, respectively (Table VII). These values
explain also why the time required to train the SVMs of the OAO
strategy is the shortest, despite the greater amount of SVMs required by the same strategy (212 [s] against 311 [s], 410 [s],
and 2361 [s] to train the BHT-BB, BHT-OAA, and OAA strategies, respectively). It is worth noting that the smallest number
of SVs was exhibited by the OAO strategy. Indeed, only nine
SVs were necessary to discriminate between the ﬁfth and ninth
classes (hay-windrowed and woods, respectively) with an accuracy of 100%. On the other hand, the larger number of SVMs
involved in the OAO strategy directly affects the computational
time demanded during the classiﬁcation of test samples the
BHT-BB strategy and (b) the BHT-OAA strategy.
BHT-OAA, and the OAA strategies, respectively). Thanks to
the small number of required SVMs and to the moderate complexity of the classiﬁcation tasks assigned to each of them, the
two BHT strategies seem particularly interesting in an operative phase involving the classiﬁcation of large scale images.
Table VIII shows the overall accuracies achieved by each SVM
COMPUTATIONAL TIME AND CLASSIFICATION COMPLEXITY ASSOCIATED TO
THE DIFFERENT SVM MULTICLASS STRATEGIES CONSIDERED
involved in both the BHT-BB and the BHT-OAA strategies. It is
worth noting that the relatively low accuracy (93.77%) obtained
by the ﬁrst SVM of the BHT-OAA architecture (SVM1) combined with a signiﬁcant depth of its associated tree (involving a
higher risk of error propagation) may explain why this strategy
was slightly less accurate than the BHT-BB strategy. In general, from a computational point of view, the two investigated
BHT-BB and BHT-OAA strategies proved effective, resulting
in a signiﬁcant decrease in computational time.
V. DISCUSSION AND CONCLUSION
In this paper, we addressed the problem of the classiﬁcation
of hyperspectral remote sensing data using support vector machines. In order to assess the effectiveness of this promising
classiﬁcation methodology, we considered two main objectives.
The ﬁrst was aimed at assessing the properties of SVMs in hyperdimensional spaces and hypersubspaces of various dimensionalities. In this context, the results obtained on the considered
dataset allow to identify the following three properties: 1) SVMs
are much more effective than other conventional nonparametric
classiﬁers (i.e., the RBF neural networks and the K-nn classiﬁer)
in terms of classiﬁcation accuracy, computational time, and stability to parameter setting; 2) SVMs seem more effective than
the traditional pattern recognition approach, which is based on
the combination of a feature extraction/selection procedure and
a conventional classiﬁer, as implemented in this paper; and 3)
SVMs exhibit low sensitivity to the Hughes phenomenon, resulting in an excellent approach to avoid the usually time-consuming phase required by any feature-reduction method. Indeed, as shown in the experiments, the improvement in accuracy
obtained on the considered dataset by combining SVMs with a
feature-reduction technique is deﬁnitely insufﬁcient to justify
the use of the latter.
The second objective of the work concerned the assessment
of the effectiveness of strategies based on ensembles of binary
SVMs used to solve multiclass problems in hyperspectral data.
In particular, four different multiclass strategies were investigated and compared. These four strategies differ basically in
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
TABLE VIII
OVERALL ACCURACY YIELDED ON THE TEST SET BY EACH SINGLE SVM OF THE BHT-BB AND BHT-OAA STRATEGIES
the manner in which the classiﬁcation problem complexity is
distributed over the single members (SVMs) of the architecture.
Compared with each other, the parallel architectures (OAA and
OAO) showed a better discrimination capability than the hierarchical tree-based architectures (BHT-BB and BHT-OAA). This
can be explained by the fact that the BHT strategies may involve
the risk of propagation of errors, since the ﬁnal decision is the
result of several hierarchical exchanges of partial decisions that
may accumulate errors. Accordingly, one may observe that the
design of a BHT strategy should favor a large number of ramiﬁcations at the expense of a lower ramiﬁcation depth, to attenuate
such a risk. Another reason that justiﬁes the lower discrimination capability of the two proposed BHT strategies can be found
in the kind of information used to construct the tree. Indeed,
the use of simple information, such as the class prior probabilities, cannot take into proper account the underlying afﬁnities
among individual classes (or metaclasses). However, from the
viewpoint of computational time, the BHT-BB and BHT-OAA
strategies proved the most effective. Consequently, depending
on the considered application, the multiclass strategy should be
selected according to a proper tradeoff between classiﬁcation
accuracy and computational time. As a ﬁnal remark, it is important to point out that the classiﬁcation accuracies exhibited by
all four strategies suggest that the multiclass problem does not
signiﬁcantly affect the performances of SVMs in the analysis of
hyperspectral data. Indeed, all the strategies exhibited accuracies sharply higher than those of the nonparametric classiﬁers
considered in our experimental analysis.
ACKNOWLEDGMENT
Support by the Italian Ministry of Education, Research and
University (MIUR) for this work is gratefully acknowledged.
The authors would like to thank D. Landgrebe for providing
the AVIRIS data and T. Joachims for supplying the software
( used in the context of
this work.