Bayesian inference for logistic models using
P´olya-Gamma latent variables
Nicholas G. Polson∗
University of Chicago
James G. Scott†
Jesse Windle‡
University of Texas at Austin
First Draft: August 2011
This Draft: July 2013
We propose a new data-augmentation strategy for fully Bayesian inference in models
with binomial likelihoods. The approach appeals to a new class of P´olya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show
the versatility of the method, including logistic regression, negative binomial regression,
nonlinear mixed-eﬀects models, and spatial models for count data. In each case, our
data-augmentation strategy leads to simple, eﬀective methods for posterior inference
that: (1) circumvent the need for analytic approximations, numerical integration, or
Metropolis–Hastings; and (2) outperform other known data-augmentation strategies,
both in ease of use and in computational eﬃciency. All methods, including an eﬃcient sampler for the P´olya-Gamma distribution, are implemented in the R package
BayesLogit.
In the technical supplement appended to the end of the paper, we provide further details regarding the generation of P´olya-Gamma random variables; the empirical
benchmarks reported in the main manuscript; and the extension of the basic dataaugmentation framework to contingency tables and multinomial outcomes.
Introduction
Bayesian inference for the logistic regression model has long been recognized as a hard
problem, due to the analytically inconvenient form of the model’s likelihood function. By
comparison, Bayesian inference for the probit model is much easier, owing to the simple
latent-variable method of Albert and Chib for posterior sampling.
In the two decades since the work of Albert and Chib on the probit model, there
have been many attempts to apply the same missing-data strategy to the logit model . The results have been mixed. Certainly many of these approaches have been used
successfully in applied work. Yet they all involve data-augmentation algorithms that are
either approximate, or are signiﬁcantly more complicated than the Albert/Chib method, as
they involve multiple layers of latent variables. Perhaps as a result, the Bayesian treatment
of the logit model has not seen widespread adoption by non-statisticians in the way that, for
example, the Bayesian probit model is used extensively in both political science and market
research . The lack of a standard computational
approach also makes it more diﬃcult to use the logit link in the kind of complex hierarchical
models that have become routine in Bayesian statistics.
In this paper, we present a new data-augmentation algorithm for Bayesian logistic regression. Although our method involves a diﬀerent missing-data mechanism from that of
Albert and Chib , it is nonetheless a direct analogue of their construction, in that
it is both exact and simple. Moreover, because our method works for any binomial likelihood parametrized by log odds, it leads to an equally painless Bayesian treatment of the
negative-binomial model for overdispersed count data.
This approach appeals to a new family of P´olya-Gamma distributions, described brieﬂy
here and constructed in detail in Section 2.
Deﬁnition 1. A random variable X has a P´olya-Gamma distribution with parameters
b > 0 and c ∈R, denoted X ∼PG(b, c), if
(k −1/2)2 + c2/(4π2) ,
where the gk ∼Ga(b, 1) are independent gamma random variables, and where D= indicates
equality in distribution.
Our main result (Theorem 1, below) is that binomial likelihoods parametrized by logodds can be represented as mixtures of Gaussians with respect to a P´olya-Gamma distribution. The fundamental integral identity at the heart of our approach is that, for b > 0,
(1 + eψ)b = 2−beκψ
e−ωψ2/2 p(ω) dω ,
where κ = a −b/2 and ω ∼PG(b, 0). When ψ = xT β is a linear function of predictors,
the integrand is the kernel of a Gaussian likelihood in β. Moreover, as we will show below,
the implied conditional distribution for ω, given ψ, is also a P´olya-Gamma distribution.
This suggests a simple strategy for Gibbs sampling across a wide class of binomial models:
Gaussian draws for the main parameters, and P´olya-Gamma draws for a single layer of
latent variables.
The success of this strategy depends upon the existence of a simple, eﬀective way to
simulate P´olya-Gamma random variables. The sum-of-gammas representation in Formula
(1) initially seems daunting, and suggests only a na¨ıve ﬁnite approximation. But we describe
a fast, exact P´olya-Gamma simulation method that avoids the diﬃculties that can result
from truncating an inﬁnite sum.
The method, which is implemented in the R package
BayesLogit , is an accept/reject sampler based on the alternatingseries method of Devroye . For the basic PG(1, c) case, the sampler is very eﬃcient:
it requires only exponential and inverse-Gaussian draws, and the probability of accepting a
proposed draw is uniformly bounded below at 0.99919. The method is also fully automatic,
with no tuning needed to get optimal performance.
It is therefore suﬃciently fast and
reliable to be used as a black-box sampling routine in complex hierarchical models involving
the logit link.
Many previous approaches have been proposed for estimating Bayesian logistic regression
This includes the Metropolis–Hastings method, along with many other latentvariable schemes that facilitate Gibbs sampling, all described below. Thus a major aim
of our paper is to demonstrate the eﬃciency of the P´olya-Gamma approach versus these
alternatives across a wide range of circumstances. We present evidence in support of two
1. In simple logit models with abundant data and no hierarchical structure, the P´olya-
Gamma method is a close second to the independence Metropolis-Hastings (MH)
sampler, as long as the MH proposal distribution is chosen carefully.
2. In virtually all other cases, the P´olya-Gamma method is most eﬃcient.
The one exception we have encountered to the second claim is the case of a negative-binomial
regression model with many counts per observation, and with no hierarchical structure in
the prior. Here, the eﬀective sample size of the P´olya-Gamma method remains the best,
but its eﬀective sampling rate suﬀers. As we describe below, this happens because our
present method for sampling PG(n, c) is to sum n independent draws from PG(1, c); with
large counts, this becomes a bottleneck. In such cases, the method of Fr¨uhwirth-Schnatter
et al. provides a fast approximation, at the cost of introducing a more complex
latent-variable structure.
This caveat notwithstanding, the P´olya-Gamma scheme oﬀers real advantages, both
in speed and simplicity, across a wide variety of structured Bayesian models for binary
and count data. In general, the more complex the model, and the more time that one
must spend sampling its main parameters, the larger will be the eﬃciency advantage of the
new method. The diﬀerence is especially large for the Gaussian-process spatial models we
consider below, which require expensive matrix operations. We have also made progress in
improving the speed of the P´olya-Gamma sampler for large shape parameters, beyond the
method described in Section 4. These modiﬁcations lead to better performance in negativebinomial models with large counts. They are detailed in Windle et al. , and have
been incorporated into the latest version of our R package .
Furthermore, in a recent paper based on an early technical report of our method, Choi
and Hobert have proven that the P´olya-Gamma Gibbs sampler for Bayesian logistic
regression is uniformly ergodic. This result has important practical consequences; most
notably, it guarantees the existence of a central limit theorem for Monte Carlo averages of
posterior draws. We are aware of no similar result for any other MCMC-based approach
to the Bayesian logit model. Together with the numerical evidence we present here, this
provides a strong reason to favor the routine use of the P´olya-Gamma method.
The paper proceeds as follows. The P´olya-Gamma distribution is constructed in Section
2, and used to derive a data-augmentation scheme for binomial likelihoods in Section 3.
Section 4 describes a method for simulating from the P´olya-Gamma distribution, which
we have implemented as a stand-alone sampler in the BayesLogit R package. Section 5
presents the results of an extensive benchmarking study comparing the eﬃciency of our
method to other data-augmentation schemes. Section 6 concludes with a discussion of some
open issues related to our proposal. Many further details of the sampling algorithm and
our empirical study of its eﬃciency are deferred to a technical supplement.
The P´olya-Gamma distribution
The case PG(b, 0)
The key step in our approach is the construction of the P´olya-Gamma distribution. We
now describe this new family, deferring our method for simulating PG random variates to
Section 4.
The P´olya-Gamma family of distributions, denoted PG(b, c), is a subset of the class of
inﬁnite convolutions of gamma distributions. We ﬁrst focus on the PG(1, 0) case, which is
a carefully chosen element of the class of inﬁnite convolutions of exponentials, also know as
P´olya distributions . The PG(1, 0) distribution has Laplace
transform E{exp(−ωt)} = cosh−1(
t/2). Using this as a starting point, one may deﬁne
the random variable ω ∼PG(b, 0), b > 0, as the inﬁnite convolution of gamma distributions
(hence the name P´olya-Gamma) that has Laplace transform
E{exp(−ωt)} =
2π2(k −1/2)2
The last equality is a consequence of the Weierstrass factorization theorem. By inverting
the Laplace transform, one ﬁnds that if ω ∼PG(b, 0), then it is equal in distribution to an
inﬁnite sum of gammas:
(k −1/2)2 ,
where the gk ∼Ga(b, 1) are mutually independent.
The PG(b, 0) class of distributions is closely related to a subset of distributions that are
surveyed by Biane et al. . This family of distributions, which we denote by J∗(b),
b > 0, has close connections with the Jacobi Theta and Riemann Zeta functions, and with
Brownian excursions. Its Laplace transform is
E{e−tJ∗(b)} = cosh−b(
implying that PG(b, 0) D= J∗(b)/4.
The general PG(b, c) class
The general PG(b, c) class arises through an exponential tilting of the PG(b, 0) density, much
in the same way that a Gaussian likelihood combines with a Gamma prior for a precision.
Speciﬁcally, a PG(b, c) random variable has the probability density function
p(ω | b, c) =
p(ω | b, 0)
where p(ω | b, 0) is the density of a PG(b, 0) random variable.
The expectation in the
denominator is taken with respect to the PG(b, 0) distribution; it is thus cosh−b(c/2) by
(3), ensuring that p(ω | b, c) is a valid density.
The Laplace transform of a PG(b, c) distribution may be calculated by appealing to the
Weierstrass factorization theorem again:
Eω {exp (−ωt)} =
2(k−1/2)2π2
2(k−1/2)2π2
where dk = 2
π2 + c2/2 .
Each term in the product is recognizable as the Laplace transform of a gamma distribution. We can therefore write a PG(b, c) as an inﬁnite convolution of gamma distributions,
2)2 + c2/(4π2) ,
which is the form given in Deﬁnition 1.
Further properties
The density of a P´olya-Gamma random variable can be expressed as an alternating-sign sum
of inverse-Gaussian densities. This fact plays a crucial role in our method for simulating
P´olya-Gamma draws.
From the characterization of J∗(b) density given by Biane et al.
 , we know that the PG(b, 0) distribution has density
f(x | b, 0) = 2b−1
(−1)n Γ(n + b)
2πx3 e−(2n+b)2
The density of PG(b, z) distribution is then computed by an exponential tilt and a renormalization:
f(x | b, c) = {coshb(c/2)}2b−1
(−1)n Γ(n + b)
2πx3 e−(2n+b)2
Notice that the normalizing constant is known directly from the Laplace transform of a
PG(b, 0) random variable.
A further useful fact is that all ﬁnite moments of a P´olya-Gamma random variable are
available in closed form. In particular, the expectation may be calculated directly. This
allows the P´olya-Gamma scheme to be used in EM algorithms, where the latent ω’s will
form a set of complete-data suﬃcient statistics for the main parameter. We arrive at this
result by appealing to the Laplace transform of ω ∼PG(b, c).
Diﬀerentiating (6) with
respect to t, negating, and evaluating at zero yields
2c tanh(c/2) = b
Lastly, the P´olya-Gamma class is closed under convolution for random variates with the
same scale (tilting) parameter. If ω1 ∼PG(b1, z) and ω2 ∼PG(b2, z) are independent, then
ω1 + ω2 ∼PG(b1 + b2, z). This follows from the Laplace transform. We will employ this
property later when constructing a P´olya-Gamma sampler.
The data-augmentation strategy
Main result
The P´olya-Gamma family has been carefully constructed to yield a simple Gibbs sampler
for the Bayesian logistic-regression model. The two diﬀerences from the Albert and Chib
 method for probit regression are that the posterior distribution is a scale mixture,
rather than location mixture, of Gaussians; and that Albert and Chib’s truncated normals
are replaced by P´olya-Gamma latent variables.
To ﬁx notation: let yi be the number of successes, ni the number of trials, and xi =
(xi1, . . . , xip) the vector of regressors for observation i ∈{1, . . . , N}. Let yi ∼Binom(ni, 1/{1+
e−ψi}), where ψi = xT
i β are the log odds of success. Finally, let β have a Gaussian prior,
β ∼N(b, B). To sample from the posterior distribution using the P´olya-Gamma method,
simply iterate two steps:
(β | y, ω)
N(mω, Vω) ,
(XT ΩX + B−1)−1
Vω(XT κ + B−1b) ,
where κ = (y1 −n1/2, . . . , yN −nN/2), and Ωis the diagonal matrix of ωi’s.
We now derive this sampler, beginning with a careful statement and proof of the integral
identity mentioned in the introduction.
Theorem 1. Let p(ω) denote the density of the random variable ω ∼PG(b, 0), b > 0.
Then the following integral identity holds for all a ∈R:
(1 + eψ)b = 2−beκψ
e−ωψ2/2 p(ω) dω ,
where κ = a −b/2.
Moreover, the conditional distribution
p(ω | ψ) =
e−ωψ2/2 p(ω)
0 e−ωψ2/2 p(ω) dω ,
which arises in treating the integrand in (7) as an unnormalized joint density in (ψ, ω), is
also in the P´olya-Gamma class: (ω | ψ) ∼PG(b, ψ).
Proof. Appealing to (3), we may write the lefthand side of (7) as
2−b exp{κψ}
coshb(ψ/2)
2−beκψ Eω{exp(−ωψ2/2} ,
where the expectation is taken with respect to ω ∼PG(b, 0), and where κ = a −b/2.
Turn now to the conditional distribution
p(ω | ψ) =
e−ωψ2/2 p(ω)
0 e−ωψ2/2 p(ω) dω ,
where p(ω) is the density of the prior, PG(b, 0). This is of the same form as (5), with ψ = c.
Therefore (ω | ψ) ∼PG(b, ψ).
To derive our Gibbs sampler, we appeal to Theorem 1 and write the likelihood contribution of observation i as
1 + exp(xT
exp{−ωi(xT
i β)2/2} p(ωi | ni, 0) ,
where κi = yi −ni/2, and where p(ωi | ni, 0) is the density of a P´olya-Gamma random
variable with parameters (ni, 0).
Combining the terms from all n data points gives the following expression for the con-
ditional posterior of β, given ω = (ω1, . . . , ωN):
p(β | ω, y) ∝p(β)
Li(β | ωi)
i β −ωi(xT
i β −κi/ωi)2o
2(z −Xβ)T Ω(z −Xβ)
where z = (κ1/ω1, . . . , κn/ωN), and where Ω= diag(ω1, . . . , ωN). This is a conditionally
Gaussian likelihood in β, with working responses z, design matrix X, and diagonal covariance matrix Ω−1. Since the prior p(β) is Gaussian, a simple linear-model calculation leads
to the Gibbs sampler deﬁned above.
Existing data-augmentation schemes
A comparison with the methods of Holmes and Held and Fr¨uhwirth-Schnatter and
Fr¨uhwirth clariﬁes how the P´olya-Gamma method diﬀers from previous attempts at
data augmentation. Both of these methods attempt to replicate the missing-data mechanism
of Albert and Chib , where the outcomes yi are assumed to be thresholded versions
of an underlying continuous quantity zi.
For simplicity, we assume that ni = 1 for all
observations, and that yi is either 0 or 1. Let
i β + ϵi ,
ϵi ∼Lo(1) ,
where ϵi ∼Lo(1) has a standard logistic distribution. Upon marginalizing over the zi, often
called the latent utilities, the original binomial likelihood is recovered.
Although (8) would initially seem to be a direct parallel with Albert and Chib ,
it does not lead to an easy method for sampling from the posterior distribution of β. This
creates additional complications compared to the probit case. The standard approach has
been to add another layer of auxiliary variables to handle the logistic error model on the
latent-utility scale. One strategy is to represent the logistic distribution as a normal-scale
mixture :
λi ∼KS(1) ,
where λi has a Kolmogorov–Smirnov distribution . Alternatively, one may approximate the logistic error term as a discrete mixture of normals
i = 1, . . . , n
i = 1, . . . , n
Directed acyclic graphs depicting two latent-variable constructions for the
logistic-regression model:
the diﬀerence of random-utility model of Holmes and Held
 and Fr¨uhwirth-Schnatter and Fr¨uhwirth , on the left; versus our direct dataaugmentation scheme, on the right.
 :
where δφ indicates a Dirac measure at φ. The weights wk and the points φ(k) in the discrete
mixture are ﬁxed for a given choice of K so that the Kullback–Leibler divergence from the
true distribution of the random utilities is minimized. Fr¨uhwirth-Schnatter and Fr¨uhwirth
 ﬁnd that the choice of K = 10 leads to a good approximation, and list the optimal
weights and variances for this choice.
In both cases, posterior sampling can be done in two blocks, sampling the complete
conditional of β in one block and sampling the joint complete conditional of both layers of
auxiliary variables in the second block. The discrete mixture of normals is an approximation,
but it outperforms the scale mixture of normals in terms of eﬀective sampling rate, as it is
much faster.
One may also arrive at the hierarchy above by manipulating the random utility-derivation
of McFadden ; this involves the diﬀerence of random utilities, or “dRUM,” using the
term of Fr¨uhwirth-Schnatter and Fr¨uhwirth .
The dRUM representation is superior to the random utility approach explored in Fr¨uhwirth-Schnatter and Fr¨uhwirth .
Further work by Fussl et al. improves the approach for binomial logistic models. In
this extension, one must use a table of diﬀerent weights and variances representing diﬀerent
normal mixtures, to approximate a ﬁnite collection of type-III logistic distributions, and
interpolate within this table to approximate the entire family.
Both Albert and Chib and O’Brien and Dunson suggest another approximation: namely, the use of a Student-t link function as a close substitute for the logistic
link. But this also introduces a second layer of latent variables, in that the Student-t error
model for zi is represented as a scale mixture of normals.
Our data-augmentation scheme diﬀers from each of these approaches in several ways.
First, it does not appeal directly to the random-utility interpretation of the logit model.
Instead, it represents the logistic CDF as a mixture with respect to an inﬁnite convolution
of gammas. Second, the method is exact, in the sense of making draws from the correct
joint posterior distribution, rather than an approximation to the posterior that arises out
of an approximation to the link function. Third, like the Albert and Chib method,
it requires only a single layer of latent variables.
A similar approach to ours is that of Gramacy and Polson , who propose a latentvariable representation of a powered-up version of the logit likelihood . This representation is useful for obtaining classical penalized-likelihood estimates via
simulation, but for the ordinary logit model it leads to an improper mixing distribution for
the latent variable. This requires modiﬁcations of the basic approach that make simulation
diﬃcult in the general logit case. As our experiments show, the method does not seem to
be competitive on speed grounds with the P´olya-Gamma representation, which results in a
proper mixing distribution for all common choices of ai, bi in (2).
For negative-binomial regression, Fr¨uhwirth-Schnatter et al. employ the discretemixture/table-interpolation approach, like that used by Fussl et al. , to produce
a tractable data augmentation scheme.
In some instances, the P´olya-Gamma approach
outperforms this method; in others, it does not.
The reasons for this discrepancy can
be explained by examining the inner workings of our P´olya-Gamma sampler, discussed in
Section 4.
Mixed model example
We have introduced the P´olya-Gamma method in the context of a binary logit model. We
do this with the understanding that, when data are abundant, the Metropolis–Hastings
algorithm with independent proposals will be eﬃcient, as asymptotic theory suggests that
a normal approximation to the posterior distribution will become very accurate as data accumulate. This is well understood among Bayesian practitioners .
But the real advantage of data augmentation, and the P´olya-Gamma technique in particular, is that it becomes easy to construct and ﬁt more complicated models. For instance,
the P´olya-Gamma method trivially accommodates mixed models, factor models, and models with a spatial or dynamic structure. For most problems in this class, good Metropolis–
Hastings samplers are diﬃcult to design, and at the very least will require ad-hoc tuning to
yield good performance.
Several relevant examples are considered in Section 5. But as an initial illustration of
the point, we ﬁt a binomial logistic mixed model using the data on contraceptive use among
Bangladeshi women provided by the R package mlmRev . The data comes
from a Bangladeshi survey whose predictors include a woman’s age, the number of children
at the time of the survey, whether the woman lives in an urban or rural area, and a more
speciﬁc geographic identiﬁer based upon the district in which the woman resides. Some
districts have few observations and district 54 has no observations; thus, a mixed model is
necessary if one wants to include this eﬀect. The response identiﬁes contraceptive use. We
Random Effect
Marginal posterior distribution of random intercepts for each district found
in a Bangladeshi contraception survey.
For 10,000 samples after 2,000 burn-in, median
ESS=8168 and median ESR=59.88 for the PG method. Grey/white bars: 90%/50% posterior credible intervals. Black dots: posterior means.
ﬁt the mixed model
Binom(1, pij) ,
1 + eψij ,
m + δj + x′
N(0, 1/φ),
N(0, κ2/φ),
where i and j correspond to the ith observation from the jth district. The ﬁxed eﬀect β is
given a N(0, 100I) prior while the precision parameter φ is given Ga(1, 1) prior. We take
κ →∞to recover an improper prior for the global intercept m. Figure 2 shows the box
plots of the posterior draws of the random intercepts m + δj. If one does not shrink these
random intercepts to a global mean using a mixed model, then several take on unrealistic
values due to the unbalanced design.
We emphasize that there are many ways to model this data, and that we do not intend
our analysis to be taken as deﬁnitive. It is merely a proof of concept, showing how various
aspects of Bayesian hierarchical modeling—in this case, models with both ﬁxed and random eﬀects—can be combined routinely with binomial likelihoods using the P´olya-Gamma
scheme. Together these changes require just a few lines of code and a few extra seconds of
runtime compared to the non-hierarchical logit model. A posterior draw of 2,000 samples
for this data set takes 26.1 seconds for a binomial logistic regression, versus 27.3 seconds
for a binomial logistic mixed model. As seen in the negative binomial examples below, one
may also painlessly incorporate a more complex prior structure using the P´olya-Gamma
technique. For instance, if given information about the geographic location of each district,
one could place spatial process prior upon the random oﬀsets {δj}.
Simulating P´olya-Gamma random variables
The PG(1,z) sampler
All our developments thus far require an eﬃcient method for sampling P´olya-Gamma random variates. In this section, we derive such a method, which is implemented in the R
package BayesLogit. We focus chieﬂy on simulating PG(1,z) eﬃciently, as this is most
relevant to the binary logit model.
First, observe that one may sample P´olya-Gamma random variables na¨ıvely (and approximately) using the sum-of-gammas representation in Equation (1). But this is slow,
and involves the potentially dangerous step of truncating an inﬁnite sum.
We therefore construct an alternate, exact method by extending the approach of Devroye
 for simulating J∗(1) from (4). The distribution J∗(1) is related to the Jacobi theta
function, so we call J∗(1) the Jacobi distribution. One may deﬁne an exponentially tilted
Jacobi distribution J∗(1, z) via the density
f(x | z) = cosh(z) e−xz2/2 f(x) ,
where f(x) is the density of J∗(1). The PG(1, z) distribution is related to J∗(1, z) through
the rescaling
PG(1, z) = 1
4J∗(1, z/2).
Devroye develops an eﬃcient J∗(1, 0) sampler. Following this work, we develop
an eﬃcient sampler for an exponentially tilted J∗random variate, J∗(1, z). In both cases,
the density of interest can be written as an inﬁnite, alternating sum that is amenable to the
series method described in Chapter IV.5 of Devroye . Recall that a random variable
with density f may be sampled by the accept/reject algorithm by: (1) proposing X from
a density g; (2) drawing U ∼U(0, cg(X)) where ∥f/g∥∞≤c; and (3) accepting X if
U ≤f(X) and rejecting X otherwise. When f(x) = P∞
n=0(−1)nan(x) and the coeﬃcients
an(x) are decreasing for all n ∈N0, for ﬁxed x in the support of f, then the partial sums,
Sn(x) = Pn
i=0(−1)iai(x), satisfy
S0(x) > S2(x) > · · · > f(x) > · · · > S3(x) > S1(x).
In that case, step (3) above is equivalent to accepting X if U ≤Si(X) for some odd i,
and rejecting X if U > Si(X) for some even i. Moreover, the partial sums Si(X) can be
calculated iteratively. Below we show that for the J∗(1, z) distribution the algorithm will
accept with high probability upon checking U ≤S1(X).
The Jacobi density has two alternating-sum representations, P∞
n=0(−1)naL
n(x) and P∞
n=0(−1)naR
neither of which satisfy (11) for all x in the support of f. However, each satisﬁes (11) on
an interval. These two intervals, respectively denoted IL and IR, satisfy IL ∪IR = (0, ∞)
and IL ∩IR ̸= ∅. Thus, one may pick t > 0 and deﬁne the piecewise coeﬃcients
π(n + 1/2)
−2(n + 1/2)2
π(n + 1/2) exp
−(n + 1/2)2π2
so that f(x) = P∞
n=0(−1)nan(x) satisﬁes the partial sum criterion (11) for x > 0. Devroye
shows that the best choice of t is near 0.64.
Employing (9), we now see that the J∗(1, z) density can be written as an inﬁnite,
alternating sum f(x|z) = P∞
n=0(−1)nan(x|z), where
an(x|z) = cosh(z) exp
This satisﬁes (11), as an+1(x|z)/an(x|z) = an+1(x)/an(x). Since a0(x|z) ≥f(x|z), the ﬁrst
term of the series provides a natural proposal:
c(z) g(x|z) = π
Examining these two kernels, one ﬁnds that X ∼g(x|z) may be sampled from a mixture of
an inverse-Gaussian and an exponential:
IG(|z|−1, 1)I(0,t]
with prob. p/(p + q)
Ex(−z2/2 + π2/8)I(t,∞)
with prob. q/(p + q)
where p(z) =
0 c(z) g(x|z)dx and q(z) =
c(z) g(x|z)dx. Note that we are implicitly
suppressing the dependence of p, q, c, and g upon t.
With this proposal in hand, sampling J∗(1, z) proceeds as follows:
1. Generate a proposal X ∼g(x|z).
2. Generate U ∼U(0, c(z)g(X|z)).
3. Iteratively calculate Sn(X|z), starting at S1(X|z), until U ≤Sn(X|z) for an odd n or
until U > Sn(X|z) for an even n.
4. Accept X if n is odd; return to step 1 if n is even.
To sample Y ∼PG(1, z), draw X ∼J∗(1, z/2) and then let Y = X/4. The details of the
implementation, along with pseudocode, can be found in the technical supplement.
Analysis of acceptance rate
This J∗(1, z) sampler is very eﬃcient. The parameter c = c(z, t) found in (14) characterizes
the average number of proposals we expect to make before accepting. Devroye shows that
in the case of z = 0, one can pick t so that c(0, t) is near unity. We extend this result to
non-zero tilting parameters and calculate that, on average, the J∗(1, z) sampler rejects no
more than 9 out of every 10,000 draws, regardless of z.
Proposition 2. Deﬁne
2 cosh(z) exp
2 cosh(z) exp
The following facts about the P´olya-Gamma rejection sampler hold.
1. The best truncation point t∗is independent of z ≥0.
2. For a ﬁxed truncation point t, p(z, t) and q(z, t) are continuous, p(z, t) decreases to
zero as z diverges, and q(z, t) converges to 1 as z diverges. Thus c(z, t) = p(z, t)+q(z, t)
is continuous and converges to 1 as z diverges.
3. For ﬁxed t, the average probability of accepting a draw, 1/c(z, t), is bounded below
for all z. For t∗, this bound to ﬁve digits is 0.99919, which is attained at z ≃1.378.
Proof. We consider each point in turn. Throughout, t is assumed to be in the interval of
valid truncation points, IL ∩IR.
1. We need to show that for ﬁxed z, c(z, t) = p(z, t) + q(z, t) has a maximum in t that is
independent of z. For ﬁxed z ≥0, p(z, t) and q(z, t) are both diﬀerentiable in t. Thus
any extrema of c will occur on the boundary of the interval IL ∩IR, or at the critical
points for which ∂c
∂t = 0; that is, t ∈IL ∩IR, for which
cosh(z) exp
0 (t)] = 0.
The exponential term is never zero, so an interior critical point must satisfy aL
0 (t) = 0, which is independent of z. Devroye shows there is one such critical point,
t∗≃0.64, and that it corresponds to a maximum.
2. Both p and q are integrals of recognizable kernels. Rewriting the expressions in terms
of the corresponding densities and integrating yields
p(z, t) = cosh(z)π
q(z, t) = (1 + e−2z)ΦIG(t|1/z, 1) ,
where ΦIG is the cumulative distribution function of an IG(1/z, 1) distribution.
One can see that p(z, t) is eventually decreasing in z for ﬁxed t by noting that the
sign of ∂p
∂z is determined by
which is eventually negative. (In fact, for the t∗calculated above it appears to be
negative for all z ≥0, which we do not prove here.) Further, p(z, t) is continuous in
z and converges to 0 as z diverges.
To see that q(z, t) converges to 1, consider a Brownian motion (Ws) deﬁned on the
probability space (Ω, F, P) and the subsequent Brownian motion with drift Xz
zs + Ws. The stopping time T z = inf{s > 0|Xz
s ≥1} is distributed as IG(1/z, 1) and
P(T z < t) = P(maxs∈[0,t] Xz
Hence P(T z < t) is increasing and limz→∞P(T z < t) = 1, ensuring that q(z, t) ∝
(1+e−2z)P(T z < t) converges to 1 as z →∞as well. Continuity follows by considering
the cumulative distribution P(T z < t) = Φ{(zt −1)/
t} −exp(2zt)Φ{(−1 −zt)/
which is a composition of continuous functions in z.
By the continuity and tail behavior of p and q, it follows that c(z, t) = p(z, t)+q(z, t),
for ﬁxed t, is continuous for all z and converges to 1 as z diverges. Further c(z, t) ≥1
since the target density and proposal density satisfy f(x|z) ≤c(z, t)g(x|z) for all
x ≥0. Thus, c takes on its maximum over z.
3. Since, for each t, c(z, t) is bounded above in z, we know that 1/c(z, t) is bounded
below above zero. For t∗, we numerically calculate that 1/c(z, t∗) attains its minimum
0.9991977 at z ≃1.378; thus, 1/c(z, t∗) > 0.99919 suggesting that no more than 9 of
every 10,000 draws are rejected on average.
Since t∗is the best truncation point regardless of z, we will assume that the truncation
point has been ﬁxed at t∗and suppress it from the notation.
Analysis of tail probabilities
Proposition 2 tells us that the sampler rarely rejects a proposal. One possible worry, however, is that the algorithm might calculate many terms in the sum before deciding to accept
or reject, and that the sampler would be slow despite rarely rejecting.
Happily, this is not the case, as we now prove. Suppose one samples X ∼J∗(1, z). Let
N denote the total number of proposals made before accepting, and let Ln be the number
of partial sums Si (i = 1, . . . , Ln) that are calculated before deciding to accept or reject
proposal n ≤N. A variant of theorem 5.1 from Devroye employs Wald’s equation
to show that that E[PN
n=1 Ln] = P∞
0 ai(x|z)dx. For the worst enclosing envelope,
z ≃1.378, E[N] = 1.0016; that is, on average, one rarely calculates anything beyond S1
of the ﬁrst proposal. A slight alteration of this theorem gives a more precise sense of how
many terms in the partial sum must be calculated.
Proposition 3. When sampling X ∼J∗(1, z), the probability of deciding to accept or
reject upon checking the nth partial sum Sn, n ≥1, is
{an−1(x|z) −an(x|z)} dx.
Proof. Let L denote the number of partials sums that are calculated before accepting or
rejecting the proposal. That is, a proposal, X, is generated; U is drawn from U(0, a0(X|z));
and L is the smallest natural number n ∈N for which U ≤Sn if n is odd or U > Sn if
n is even, where Sn denotes Sn(X|z). But since L is the smallest n for which this holds,
SL−2 < U ≤SL when L is odd and SL < U ≤SL−2 when L is even. Thus, the algorithm
accepts or rejects if and only if U ∈KL(X|z) where
(Sn−2(x|z), Sn(x|z)],
(Sn(x|z), Sn−2(x|z)],
In either case, |Kn(x|z)| = an−1(x|z) −an(x|z). Thus
P(L = n|X = x) = an−1(x|z) −an(x|z)
Marginalizing over x yields
P(L = n) =
{an−1(x|z) −an(x|z)} dx.
Since each coeﬃcient an is the piecewise composition of an inverse Gaussian kernel and
an exponential kernel, these integrals may be evaluated. In particular,
an(x|z) = cosh(z)
2e−(2n+1)z pIG(x|µn(z), λn),
yn(z) pE(x|yn(z)),
where µn(z) = 2n+1
, λn = (2n + 1)2, yn(z) = 0.5(z2 + (n + 1/2)2π2), and pIG and pE are
the corresponding densities. The table below shows the ﬁrst several probabilities for the
worst case envelope, z ≃1.378. Clearly P(L > n) decays rapidly with n.
8.023 × 10−4
1.728 × 10−9
8.213 × 10−18
8.066 × 10−29
Together with Proposition 2, this provides a strong guarantee of the eﬃciency of the PG(1,z)
The general PG(b, z) case
To sample from the entire family of PG(b, z) distributions, we exploit the additivity of the
P´olya-Gamma class. In particular, when b ∈N, one may sample PG(b, z) by taking b i.i.d.
draws from PG(1, z) and summing them. In binomial logistic regression, one will always
sample PG(b, z) using integral b. This will also be the case in negative-binomial regression
if one chooses an integer over-dispersion parameter. In the technical supplement, we discuss
the case of non-integral b.
The run-time of the latent-variable sampling step is therefore roughly linear in the
number of total counts in the data set. For example, to sample 1 million P´olya-Gamma(1,1)
random variables took 0.70 seconds on a dual-core Apple laptop, versus 0.17 seconds for
the same number of Gamma random variables. By contrast, to sample 1 million PG(10,1)
random variables required 6.43 seconds, and to sample 1 million PG(100,1) random variables
required 60.0 seconds.
We have had some initial success in developing a faster method to simulate from the
PG(n,z) distribution that does not require summing together n PG(1,z) draws, and that
works for non-integer values of n. This is an active subject of research, though somewhat
beyond the scope of the present paper, where we use the sum-of-PG(1,z)’s method on all
our benchmark examples. A full report on the alternative simulation method for PG(n,z)
may be found in Windle et al. .
Experiments
We benchmarked the P´olya-Gamma method against several alternatives for logit and negativebinomial models. Our purpose is to summarize the results presented in detail in our online
technical supplement, to which we refer the interested reader.
Our primary metrics of comparison are the eﬀective sample size and the eﬀective sampling rate, deﬁned as the eﬀective sample size per second of runtime. The eﬀective sampling
rate quantiﬁes how rapidly a Markov-chain sampler can produce independent draws from
the posterior distribution.
Following Holmes and Held , the eﬀective sample size
(ESS) for the ith parameter in the model is
ESSi = M/{1 + 2
where M is the number of post-burn-in samples, and ρi(j) is the jth autocorrelation of βi.
We use the coda package , which ﬁts an AR model to approximate the
spectral density at zero, to estimate each ESSi. All of the benchmarks are generated using
R so that timings are comparable. Some R code makes external calls to C. In particular,
the P´olya-Gamma method calls a C routine to sample the P´olya-Gamma random variates,
just as R routines for sampling common distributions use externally compiled code. Here
we report the median eﬀective sample size across all parameters in the model. Minimum
and maximum eﬀective sample sizes are reported in the technical supplement.
Our numerical experiments support several conclusions.
In binary logit models.
First, the P´olya-Gamma is more eﬃcient than all previously
proposed data-augmentation schemes. This is true both in terms of eﬀective sample size
Summary of experiments on real and simulated data for binary logistic regression. ESS: the
median eﬀective sample size for an MCMC run of 10,000 samples. ESR: the median eﬀective sample rate,
or median ESS divided by the runtime of the sampler in seconds. AC: Australian credit data set. GC1
and GC2: partial and full versions of the German credit data set. Sim1 and Sim2: simulated data with
orthogonal and correlated predictors, respectively. Best RU-DA: the result of the best random-utility dataaugmentation algorithm for that data set. Best Metropolis: the result of the Metropolis algorithm with the
most eﬃcient proposal distribution among those tested. See the technical supplement for full details.
P´olya-Gamma
Best RU-DA
Best Metropolis
P´olya-Gamma
Best RU-DA
Best Metropolis
and eﬀective sampling rate. Table 1 summarizes the evidence: across 6 real and 2 simulated data sets, the P´olya-Gamma method was always more eﬃcient than the next-best
data-augmentation scheme (typically by a factor of 200%–500%). This includes the approximate random-utility methods of O’Brien and Dunson and Fr¨uhwirth-Schnatter
and Fr¨uhwirth , and the exact method of Gramacy and Polson . Fr¨uhwirth-
Schnatter and Fr¨uhwirth ﬁnd that their own method beats several other competitors, including the method of Holmes and Held . We ﬁnd this as well, and omit these
timings from our comparison. Further details can be found in Section 3 of the technical
supplement.
Second, the P´olya-Gamma method always had a higher eﬀective sample size than the
two default Metropolis samplers we tried. The ﬁrst was a Gaussian proposal using Laplace’s
approximation. The second was a multivariate t6 proposal using Laplace’s approximation
to provide the centering and scale-matrix parameters, recommended by Rossi et al. 
and implemented in the R package bayesm .
On 5 of the 8 data sets, the best Metropolis algorithm did have a higher eﬀective
sampling rate than the P´olya-Gamma method, due to the diﬀerence in run times. But this
advantage depends crucially on the proposal distribution, where even small perturbations
can lead to surprisingly large declines in performance.
For example, on the Australian
credit data set (labeled AC in the table), the Gaussian proposal led to a median eﬀective
sampling rate of 122 samples per second. The very similar multivariate t6 proposal led to
far more rejected proposals, and gave an eﬀective sampling rate of only 2.6 samples per
second. Diagnosing such diﬀerences for a speciﬁc problem may cost the user more time
than is saved by a slightly faster sampler.
Finally, the P´olya-Gamma method truly shines when the model has a complex prior
structure. In general, it is diﬃcult to design good Metropolis samplers for these problems.
For example, consider a binary logit mixed model with grouped data and a random-eﬀects
structure, where the log-odds of success for observation j in group i are ψij = αi + xijβi,
Summary of experiments on real and simulated data for binary logistic mixed models. Metropolis:
the result of an independence Metropolis sampler based on the Laplace approximation. Using a t6 proposal
yielded equally poor results. See the technical supplement for full details.
P´olya-Gamma
Metropolis
P´olya-Gamma
Metropolis
Summary of experiments on simulated data for negative-binomial models. Metropolis: the result
of an independence Metropolis sampler based on a t6 proposal. FS09: the algorithm of Fr¨uhwirth-Schnatter
et al. . Sim1 and Sim2: simulated negative-binomial regression problems. GP1 and GP2: simulated
Gaussian-process spatial models. The independence Metropolis algorithm is not applicable in the spatial
models, where there as many parameters as observations.
Total Counts
P´olya-Gamma
Metropolis
P´olya-Gamma
Metropolis
and where either the αi, the βi, or both receive further hyperpriors. It is not clear that
a good default Metropolis sampler is easily constructed unless there are a large number
of observations per group.
Table 2 shows the results of na¨ıvely using an independence
Metropolis sampler based on the Laplace approximation to the full joint posterior. For a
synthetic data set with a balanced design of 100 observations per group, the P´olya-Gamma
method is slightly better. For the two real data sets with highly unbalanced designs, it is
much better.
Of course, it is certainly possible to design and tune better Metropolis–Hastings samplers
for mixed models; see, for example, Gamerman . We simply point out that what
works well in the simplest case need not work well in a slightly more complicated case.
The advantages of the P´olya-Gamma method are that it requires no tuning, is simple to
implement, is uniformly ergodic , and gives optimal or near-optimal
performance across a range of cases.
In negative-binomial models.
The P´olya-Gamma method consistently yields the best
eﬀective sample sizes in negative-binomial regression. However, its eﬀective sampling rate
suﬀers when working with a large counts or a non-integral over-dispersion parameter. Cur-
rently, our P´olya-Gamma sampler can draw from PG(b, ψ) quickly when b = 1, but not for
general, integral b: to sample from PG(b, ψ) when b ∈N, we take b independent samples
of PG(1, ψ) and sum them. Thus in negative-binomial models, one must sample at least
i=1 yi P´olya-Gamma random variates, where yi is the ith response, at every MCMC iteration. When the number of counts is relatively high, this becomes a burden. leads to better performance, but describing the
alternative method is beyond the subject of this paper.)
The columns labeled Sim1 and Sim2 of Table 3 show results for data simulated from
a negative-binomial model with 400 observations and 3 regressors. (See the technical supplement for details.) In the ﬁrst case (Sim1), the intercept is chosen so that the average
outcome is a count of 8 (3244 total counts). Given the small average count size, the P´olya-
Gamma method has a superior eﬀective sampling rate compared to the approximate method
of Fr¨uhwirth-Schnatter et al. , the next-best choice. In the second case (Sim2), the
average outcome is a count of 24 (9593 total counts). Here the Fr¨uhwirth-Schnatter et al.
algorithm ﬁnishes more quickly, and therefore has a better eﬀective sampling rate. In both
cases we restrict the sampler to integer over-dispersion parameters.
As before, the P´olya-Gamma method starts to shine when working with more complicated hierarchical models that devote proportionally less time to sampling the auxiliary
variables. For instance, consider a spatial model where we observe counts y1, . . . , yn at locations x1, . . . , xn, respectively. It is natural to model the log rate parameter as a Gaussian
yi ∼NB(n, 1/{1 + e−ψi}) ,
ψ ∼GP(0, K) ,
where ψ = (ψ1, . . . , ψn)T and K is constructed by evaluating a covariance kernel at the
locations xi. For example, under the squared-exponential kernel, we have
Kij = κ + exp
d(xi, xj)2
with characteristic length scale ℓ, nugget κ, and distance function d (in our examples,
Euclidean distance).
Using either the P´olya-Gamma or the Fr¨uhwirth-Schnatter et al. techniques,
one arrives at a multivariate Gaussian conditional for ψ whose covariance matrix involves
latent variables. Producing a random variate from this distribution is expensive, as one
must calculate the Cholesky decomposition of a relatively large matrix at each iteration.
Therefore, the overall sampler spends relatively less time drawing auxiliary variables. Since
the P´olya-Gamma method leads to a higher eﬀective sample size, it wastes fewer of the
expensive draws for the main parameter.
The columns labeled GP1 and GP2 of Table 3 show two such examples. In the ﬁrst
synthetic data set, 256 equally spaced x points were used to generate a draw for ψ from
a Gaussian process with length scale ℓ= 0.1 and nugget κ = 0.0. The average count was
¯y = 35.7, or 9137 total counts (roughly the same as in the second regression example,
Sim2). In the second synthetic data set, we simulated ψ from a Gaussian process over 1000
x points, with length scale ℓ= 0.1 and a nugget = 0.0001. This yielded 22,720 total counts.
In both cases, the P´olya-Gamma method led to a more eﬃcient sampler—by a factor of 3
for the smaller problem, and 5 for the larger.
Discussion
We have shown that Bayesian inference for logistic models can be implemented using a
data augmentation scheme based on the novel class of P´olya-Gamma distributions. This
leads to simple Gibbs-sampling algorithms for posterior computation that exploit standard
normal linear-model theory, and that are notably simpler than previous schemes. We have
also constructed an accept/reject sampler for the new family, with strong guarantees of
eﬃciency (Propositions 2 and 3).
The evidence suggests that our data-augmentation scheme is the best current method
for ﬁtting complex Bayesian hierarchical models with binomial likelihoods. It also opens
the door for exact Bayesian treatments of many modern-day machine-learning classiﬁcation
methods based on mixtures of logits . Applying the P´olya-Gamma mixture framework to such problems is currently an
active area of research.
Moreover, posterior updating via exponential tilting is a quite general situation that
arises in Bayesian inference incorporating latent variables. In our case, the posterior distribution of ω that arises under normal pseudo-data with precision ω and a PG(b, 0) prior
is precisely an exponentially titled PG(b, 0) random variable. This led to our characterization of the general PG(b, c) class. An interesting fact is that we were able to identify the
conditional posterior for the latent variable strictly using its moment-generating function,
without ever appealing to Bayes’ rule for density functions. This follows the L´evy-penalty
framework of Polson and Scott and relates to work by Ciesielski and Taylor 
on the sojourn times of Brownian motion. There may be many other situations where the
same idea is applicable.
Our benchmarks have relied upon serial computation. However, one may trivially parallelize a vectorized P´olya-Gamma draw on a multicore CPU. Devising such a sampler for a
graphical-processing unit (GPU) is less straightforward, but potentially more fruitful. The
massively parallel nature of GPUs oﬀer a solution to the sluggishness found when sampling
PG(n, z) variables for large, integral n, which was the largest source of ineﬃciency with the
negative-binomial results presented earlier.
Acknowledgements.
The authors wish to thank Hee Min Choi and Jim Hobert for
sharing an early draft of their paper on the uniform ergodicity of the P´olya-Gamma Gibbs
sampler. They also wish to thank two anonymous referees, the associate editor, and the
editor of the Journal of the American Statistical Association, whose many insights and
helpful suggestions have improved the paper. The second author acknowledges the support
of a CAREER grant from the U.S. National Science Foundation (DMS-1255187).