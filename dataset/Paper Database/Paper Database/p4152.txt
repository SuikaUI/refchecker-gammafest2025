HAL Id: hal-01977706
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution - NonCommercial 4.0 International License
Improving MMD-GAN Training with Repulsive Loss
Wei Wang, Yuan Sun, Saman Halgamuge
To cite this version:
Wei Wang, Yuan Sun, Saman Halgamuge.
Improving MMD-GAN Training with Repulsive Loss
Function. 2018. ￿hal-01977706￿
Published as a conference paper at ICLR 2019
IMPROVING MMD-GAN TRAINING WITH REPULSIVE
LOSS FUNCTION
University of Melbourne
RMIT University
Saman Halgamuge
University of Melbourne
Generative adversarial nets (GANs) are widely used to learn the data sampling
process and their performance may heavily depend on the loss functions, given a
limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two
contributions. First, we argue that the existing MMD loss function may discourage the learning of ﬁne details in data as it attempts to contract the discriminator
outputs of real data. To address this issue, we propose a repulsive loss function to
actively learn the difference among the real data by simply rearranging the terms
in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian
kernel to stabilize the training of MMD-GAN with the repulsive loss function.
The proposed methods are applied to the unsupervised image generation tasks on
CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the
repulsive loss function signiﬁcantly improves over the MMD loss at no additional
computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a
single DCGAN network and spectral normalization. 1
INTRODUCTION
Generative adversarial nets (GANs) ) are a branch of generative models that
learns to mimic the real data generating process. GANs have been intensively studied in recent years,
with a variety of successful applications ; Li et al. ; Lai et al. ;
Zhu et al. ; Ho & Ermon ). The idea of GANs is to jointly train a generator network
that attempts to produce artiﬁcial samples, and a discriminator network or critic that distinguishes
the generated samples from the real ones. Compared to maximum likelihood based methods, GANs
tend to produce samples with sharper and more vivid details but require more efforts to train.
Recent studies on improving GAN training have mainly focused on designing loss functions, network architectures and training procedures. The loss function, or simply loss, deﬁnes quantitatively
the difference of discriminator outputs between real and generated samples. The gradients of loss
functions are used to train the generator and discriminator. This study focuses on a loss function
called maximum mean discrepancy (MMD), which is well known as the distance metric between
two probability distributions and widely applied in kernel two-sample test ).
Theoretically, MMD reaches its global minimum zero if and only if the two distributions are equal.
Thus, MMD has been applied to compare the generated samples to real ones directly ) and extended as the loss function to the GAN framework recently ;
Li et al. ; Bi´nkowski et al. ).
In this paper, we interpret the optimization of MMD loss by the discriminator as a combination of
attraction and repulsion processes, similar to that of linear discriminant analysis. We argue that the
existing MMD loss may discourage the learning of ﬁne details in data, as the discriminator attempts
to minimize the within-group variance of its outputs for the real data. To address this issue, we
propose a repulsive loss for the discriminator that explicitly explores the differences among real
data. The proposed loss achieved signiﬁcant improvements over the MMD loss on image generation
∗Corresponding author: 
1The code is available at: 
Published as a conference paper at ICLR 2019
tasks of four benchmark datasets, without incurring any additional computational cost. Furthermore,
a bounded Gaussian kernel is proposed to stabilize the training of discriminator. As such, using a
single kernel in MMD-GAN is sufﬁcient, in contrast to a linear combination of kernels used in Li
et al. and Bi´nkowski et al. . By using a single kernel, the computational cost of the
MMD loss can potentially be reduced in a variety of applications.
The paper is organized as follows. Section 2 reviews the GANs trained using the MMD loss (MMD-
GAN). We propose the repulsive loss for discriminator in Section 3, introduce two practical techniques to stabilize the training process in Section 4, and present the results of extensive experiments
in Section 5. In the last section, we discuss the connections between our model and existing work.
In this section, we introduce the GAN model and MMD loss. Consider a random variable X ∈X
with an empirical data distribution PX to be learned. A typical GAN model consists of two neural
networks: a generator G and a discriminator D. The generator G maps a latent code z with a
ﬁxed distribution PZ (e.g., Gaussian) to the data space X: y = G(z) ∈X, where y represents the
generated samples with distribution PG. The discriminator D evaluates the scores D(a) ∈Rd of a
real or generated sample a. This study focuses on image generation tasks using convolutional neural
networks (CNN) for both G and D.
Several loss functions have been proposed to quantify the difference of the scores between real
and generated samples: {D(x)} and {D(y)}, including the minimax loss and non-saturating
loss ), hinge loss ), Wasserstein loss ; Gulrajani et al. ) and maximum mean discrepancy (MMD) ;
Bi´nkowski et al. ) (see Appendix B.1 for more details). Among them, MMD uses kernel embedding φ(a) = k(·, a) associated with a characteristic kernel k such that φ is inﬁnite-dimensional
and ⟨φ(a), φ(b)⟩H = k(a, b). The squared MMD distance between two distributions P and Q is
k(P, Q) = ∥µP −µQ∥2
H = Ea,a′∼P [k(a, a′)] + Eb,b′∼Q[k(b, b′)] −2Ea∼P,b∼Q[k(a, b)] (1)
The kernel k(a, b) measures the similarity between two samples a and b. Gretton et al. 
proved that, using a characteristic kernel k, M 2
k(P, Q) ≥0 with equality applies if and only if
In MMD-GAN, the discriminator D can be interpreted as forming a new kernel with k: k◦D(a, b) =
k(D(a), D(b)) = kD(a, b). If D is injective, k ◦D is characteristic and M 2
k◦D(PX, PG) reaches
its minimum if and only if PX = PG ). Thus, the objective functions for G and D
could be ; Bi´nkowski et al. ):
k◦D(PX, PG) = EPG[kD(y, y′)] −2EPX,PG[kD(x, y)] + EPX[kD(x, x′)]
k◦D(PX, PG) = 2EPX,PG[kD(x, y)] −EPX[kD(x, x′)] −EPG[kD(y, y′)]
MMD-GAN has been shown to be more effective than the model that directly uses MMD as the loss
function for the generator G ).
Liu et al. showed that MMD and Wasserstein metric are weaker objective functions for
GAN than the Jensen–Shannon (JS) divergence (related to minimax loss) and total variation (TV)
distance (related to hinge loss). The reason is that convergence of PG to PX in JS-divergence
and TV distance also implies convergence in MMD and Wasserstein metric. Weak metrics are
desirable as they provide more information on adjusting the model to ﬁt the data distribution ). Nagarajan & Kolter proved that the GAN trained using the minimax loss and
gradient updates on model parameters is locally exponentially stable near equilibrium, while the
GAN using Wasserstein loss is not. In Appendix A, we demonstrate that the MMD-GAN trained by
gradient descent is locally exponentially stable near equilibrium.
REPULSIVE LOSS FUNCTION
In this section, we interpret the training of MMD-GAN (using Latt
D and Lmmd
) as a combination of
attraction and repulsion processes, and propose a novel repulsive loss function for the discriminator
by rearranging the components in Latt
Published as a conference paper at ICLR 2019
paired with Lrep
Figure 1: Illustration of the gradient directions of each loss on the real sample scores {D(x)} (“r”
nodes) and generated sample scores {D(y)} (“g” nodes). The blue arrows stand for attraction and
the orange arrows for repulsion. When Lmmd
is paired with Latt
D, the gradient directions of Lmmd
{D(y)} can be obtained by reversing the arrows in (a), thus are omitted.
First, consider a linear discriminant analysis (LDA) model as the discriminator. The task is to ﬁnd a
projection w to maximize the between-group variance
wT µx −wT µy
and minimize the withingroup variance wT (Σx + Σy)w, where µ and Σ are group mean and covariance.
In MMD-GAN, the neural-network discriminator works in a similar way as LDA. By minimizing
D, the discriminator D tackles two tasks: 1) D reduces EPX,PG[kD(x, y)], i.e., causes the two
groups {D(x)} and {D(y)} to repel each other (see Fig. 1a orange arrows), or maximize between–
group variance; and 2) D increases EPX[kD(x, x′)] and EPG[k(y, y′)], i.e. contracts {D(x)} and
{D(y)} within each group (see Fig. 1a blue arrows), or minimize the within-group variance. We
refer to loss functions that contract real data scores as attractive losses.
We argue that the attractive loss Latt
D (Eq. 3) has two issues that may slow down the GAN training:
1. The discriminator D may focus more on the similarities among real samples (in order to contract
{D(x)}) than the ﬁne details that separate them. Initially, G produces low-quality samples and it
may be adequate for D to learn the common features of {x} in order to distinguish between {x}
and {y}. Only when {D(y)} is sufﬁciently close to {D(x)} will D learn the ﬁne details of {x}
to be able to separate {D(x)} from {D(y)}. Consequently, D may leave out some ﬁne details
in real samples, thus G has no access to them during training.
2. As shown in Fig. 1a, the gradients on D(y) from the attraction (blue arrows) and repulsion
(orange arrows) terms in Latt
D (and thus Lmmd
) may have opposite directions during training.
Their summation may be small in magnitude even when D(y) is far away from D(x), which
may cause G to stagnate locally.
Therefore, we propose a repulsive loss for D to encourage repulsion of the real data scores {D(x)}:
D = EPX[kD(x, x′)] −EPG[kD(y, y′)]
The generator G uses the same MMD loss Lmmd
as before (see Eq. 2). Thus, the adversary lies
in the fact that D contracts {D(y)} via maximizing EPG[kD(y, y′)] (see Fig. 1b) while G expands {D(y)} (see Fig. 1c). Additionally, D also learns to separate the real data by minimizing
EPX[kD(x, x′)], which actively explores the ﬁne details in real samples and may result in more
meaningful gradients for G. Note that in Eq. 4, D does not explicitly push the average score of
{D(y)} away from that of {D(x)} because it may have no effect on the pair-wise sample distances.
But G aims to match the average scores of both groups. Thus, we believe Lmmd
paired with Lrep
less likely to yield opposite gradients when compared against Lmmd
paired with Latt
D (see Fig. 1c). In
Appendix A, we demonstrate that GAN trained using gradient descent and the repulsive MMD loss
(LD,λ, Lmmd
) is locally exponentially stable near equilibrium.
At last, we identify a general form of loss function for the discriminator D:
LD,λ = λEPX[kD(x, x′)] −(λ −1)EPX,PG[kD(x, y)] −EPG[kD(y, y′)]
Published as a conference paper at ICLR 2019
Figure 2: (a) Gaussian kernels {krbf
σi (a, b)} and their mean as a function of e = ∥a −b∥, where
2, 4} were used in our experiments; (b) derivatives of {krbf
σi (a, b)} in (a); (c)
rational quadratic kernel {krq
αi(a, b)} and their mean, where αi ∈{0.2, 0.5, 1, 2, 5}; (d) derivatives
αi(a, b)} in (c).
where λ is a hyper-parameter2. When λ < 0, the discriminator loss LD,λ is attractive, with λ =
−1 corresponding to the original MMD loss Latt
D in Eq. 3; when λ > 0, LD,λ is repulsive and
λ = 1 corresponds to Lrep
D in Eq. 4. It is interesting that when λ > 1, the discriminator explicitly
contracts {D(x)} and {D(y)} via maximizing EPX,PG[kD(x, y)], which may work as a penalty
that prevents the pairwise distances of {D(x)} from increasing too fast. Note that LD,λ has the
same computational cost as Latt
D (Eq. 3) as we only rearranged the terms in Latt
REGULARIZATION ON MMD AND DISCRIMINATOR
In this section, we propose two approaches to stabilize the training of MMD-GAN: 1) a bounded
kernel to avoid the saturation issue caused by an over-conﬁdent discriminator; and 2) a generalized
power iteration method to estimate the spectral norm of a convolutional kernel, which was used in
spectral normalization on the discriminator in all experiments in this study unless speciﬁed otherwise.
KERNEL IN MMD
For MMD-GAN, the following two kernels have been used:
• Gaussian radial basis function (RBF), or Gaussian kernel ), krbf
σ (a, b) =
2σ2 ∥a −b∥2) where σ > 0 is the kernel scale or bandwidth.
• Rational quadratic kernel ), krq
α(a, b) = (1+ 1
2α ∥a −b∥2)−α, where the
kernel scale α > 0 corresponds to a mixture of Gaussian kernels with a Gamma(α, 1) prior on
the inverse kernel scales σ−1.
It is interesting that both studies used a linear combination of kernels with ﬁve different kernel scales,
i.e., krbf = P5
σi and krq = P5
αi, where σi ∈{1, 2, 4, 8, 16}, αi ∈{0.2, 0.5, 1, 2, 5} (see
Fig. 2a and 2c for illustration). We suspect the reason is that a single kernel k(a, b) is saturated when
the distance ∥a −b∥is either too large or too small compared to the kernel scale (see Fig. 2b and
2d), which may cause diminishing gradients during training. Both Li et al. and Bi´nkowski
et al. applied penalties on the discriminator parameters but not to the MMD loss itself. Thus
the saturation issue may still exist. Using a linear combination of kernels with different kernel scales
may alleviate this issue but not eradicate it.
Inspired by the hinge loss (see Appendix B.1), we propose a bounded RBF (RBF-B) kernel for
the discriminator. The idea is to prevent D from pushing {D(x)} too far away from {D(y)} and
causing saturation. For Latt
D in Eq. 3, the RBF-B kernel is:
2σ2 max(∥a −b∥2 , bl))
if a, b ∈{D(x)} or a, b ∈{D(y)}
2σ2 min(∥a −b∥2 , bu))
if a ∈{D(x)} and b ∈{D(y)}
2The weights for the three terms in LD,λ sum up to zero. This is to make sure the ∂LD,λ/∂θD is zero at
equilibrium PX = PG, where θD is the parameters of D.
Published as a conference paper at ICLR 2019
D in Eq. 4, the RBF-B kernel is:
2σ2 max(∥a −b∥2 , bl))
if a, b ∈{D(y)}
2σ2 min(∥a −b∥2 , bu))
if a, b ∈{D(x)}
where bl and bu are the lower and upper bounds. As such, a single kernel is sufﬁcient and we set
σ = 1, bl = 0.25 and bu = 4 in all experiments for simplicity and leave their tuning for future
work. It should be noted that, like the case of hinge loss, the RBF-B kernel is used only for the
discriminator to prevent it from being over-conﬁdent. The generator is always trained using the
original RBF kernel, thus we retain the interpretation of MMD loss Lmmd
as a metric.
RBF-B kernel is among many methods to address the saturation issue and stabilize MMD-GAN
training. We found random sampling kernel scale, instance noise ) and label
smoothing ; Salimans et al. ) may also improve the model performance
and stability. However, the computational cost of RBF-B kernel is relatively low.
SPECTRAL NORMALIZATION IN DISCRIMINATOR
Without any Lipschitz constraints, the discriminator D may simply increase the magnitude of its
outputs to minimize the discriminator loss, causing unstable training3. Spectral normalization divides the weight matrix of each layer by its spectral norm, which imposes an upper bound on the
magnitudes of outputs and gradients at each layer of D ). However, to estimate
the spectral norm of a convolution kernel, Miyato et al. reshaped the kernel into a matrix. We
propose a generalized power iteration method to directly estimate the spectral norm of a convolution
kernel (see Appendix C for details) and applied spectral normalization to the discriminator in all
experiments. In Appendix D.1, we explore using gradient penalty to impose the Lipschitz constraint
 ; Bi´nkowski et al. ; Arbel et al. ) for the proposed repulsive loss.
EXPERIMENTS
In this section, we empirically evaluate the proposed 1) repulsive loss Lrep
D (Eq. 4) on unsupervised training of GAN for image generation tasks; and 2) RBF-B kernel to stabilize MMD-GAN
The generalized power iteration method is evaluated in Appendix C.3.
To show the
efﬁcacy of Lrep
D , we compared the loss functions (Lrep
) using Gaussian kernel (MMD-rep)
with (Latt
) using Gaussian kernel (MMD-rbf) ) and rational quadratic kernel
(MMD-rq) ), as well as non-saturating loss ) and
hinge loss ). To show the efﬁcacy of RBF-B kernel, we applied it to both Latt
D , resulting in two methods MMD-rbf-b and MMD-rep-b. The Wasserstein loss was excluded for
comparison because it cannot be directly used with spectral normalization ).
EXPERIMENT SETUP
Dataset: The loss functions were evaluated on four datasets: 1) CIFAR-10 (50K images, 32 × 32
pixels) ); 2) STL-10 (100K images, 48 × 48 pixels) ); 3) CelebA (about 203K images, 64×64 pixels) ); and 4) LSUN bedrooms
(around 3 million images, 64×64 pixels) ). The images were scaled to range [−1, 1]
to avoid numeric issues.
Network architecture: The DCGAN ) architecture was used with hyperparameters from Miyato et al. (see Appendix B.2 for details). In all experiments, batch
normalization (BN) ) was used in the generator, and spectral normalization
with the generalized power iteration (see Appendix C) in the discriminator. For MMD related losses,
the dimension of discriminator output layer was set to 16; for non-saturating loss and hinge loss,
it was 1. In Appendix D.2, we investigate the impact of discriminator output dimension on the
performance of repulsive loss.
3Note that training stability is different from the local stability considered in Appendix A. Training stability
often refers to the ability of model converging to a desired state measured by some criterion. Local stability
means that if a model is initialized sufﬁciently close to an equilibrium, it will converge to the equilibrium.
Published as a conference paper at ICLR 2019
Table 1: Inception score (IS), Fr´echet Inception distance (FID) and multi-scale structural similarity
(MS-SSIM) on image generation tasks using different loss functions
LSUN-bedrom2
Non-saturating
1 The models here differ only by the loss functions and dimension of discriminator outputs. See Section 5.1.
2 For CelebA and LSUN-bedroom, IS is not meaningful ) and thus omitted.
3 On LSUN-bedroom, MMD-rbf and MMD-rq did not achieve reasonable results and thus are omitted.
Hyper-parameters: We used Adam optimizer ) with momentum parameters
β1 = 0.5, β2 = 0.999; two-timescale update rule (TTUR) ) with two learning
rates (ρD, ρG) chosen from {1e-4, 2e-4, 5e-4, 1e-3} (16 combinations in total); and batch size 64.
Fine-tuning on learning rates may improve the model performance, but constant learning rates were
used for simplicity. All models were trained for 100K iterations on CIFAR-10, STL-10, CelebA
and LSUN bedroom datasets, with ndis = 1, i.e., one discriminator update per generator update4.
For MMD-rbf, the kernel scales σi ∈{1,
2, 4} were used due to a better performance than
the original values used in Li et al. . For MMD-rq, αi ∈{0.2, 0.5, 1, 2, 5}. For MMD-rbf-b,
MMD-rep, MMD-rep-b, a single Gaussian kernel with σ = 1 was used.
Evaluation metrics: Inception score (IS) ), Fr´echet Inception distance (FID)
 ) and multi-scale structural similarity (MS-SSIM) ) were
used for quantitative evaluation. Both IS and FID are calculated using a pre-trained Inception model
 ). Higher IS and lower FID scores indicate better image quality. MS-SSIM
calculates the pair-wise image similarity and is used to detect mode collapses among images of
the same class ). Lower MS-SSIM values indicate perceptually more diverse
images. For each model, 50K randomly generated samples and 50K real samples were used to
calculate IS, FID and MS-SSIM.
QUANTITATIVE ANALYSIS
Table 1 shows the Inception score, FID and MS-SSIM of applying different loss functions on the
benchmark datasets with the optimal learning rate combinations tested experimentally. Note that
the same training setup (i.e., DCGAN + BN + SN + TTUR) was applied for each loss function.
We observed that: 1) MMD-rep and MMD-rep-b performed signiﬁcantly better than MMD-rbf and
MMD-rbf-b respectively, showing the proposed repulsive loss Lrep
D (Eq. 4) greatly improved over the
attractive loss Latt
D (Eq. 3); 2) Using a single kernel, MMD-rbf-b performed better than MMD-rbf and
MMD-rq which used a linear combination of ﬁve kernels, indicating that the kernel saturation may
be an issue that slows down MMD-GAN training; 3) MMD-rep-b performed comparable or better
than MMD-rep on benchmark datasets where we found the RBF-B kernel managed to stabilize
MMD-GAN training using repulsive loss. 4) MMD-rep and MMD-rep-b performed signiﬁcantly
better than the non-saturating and hinge losses, showing the efﬁcacy of the proposed repulsive loss.
Additionally, we trained MMD-GAN using the general loss LD,λ (Eq. 5) for discriminator and Lmmd
(Eq. 2) for generator on the CIFAR-10 dataset. Fig. 3 shows the inﬂuence of λ on the performance
4Increasing or decreasing ndis may improve the model performance in some cases, but it has signiﬁcant
impact on the computation cost. For simple and fair comparison, we set ndis = 1 in all cases.
Published as a conference paper at ICLR 2019
(a) MMD-GAN trained using a single RBF kernel in LD,λ
(b) MMD-GAN trained using and a single RBF-B kernel in LD,λ
Figure 3: FID scores of MMD-GAN using (a) RBF kernel and (b) RBF-B kernel in LD,λ on CIFAR-
10 dataset for 16 learning rate combinations. Each color bar represents the FID score using a learning
rate combination (ρD, ρG), in the order of (1e-4, 1e-4), (1e-4, 2e-4),...,(1e-3, 1e-3). The discriminator was trained using LD,λ (Eq. 5) with λ ∈{-1, -0.5, 0, 0.5, 1, 2}, and generator using Lmmd
(Eq. 2). We use the FID> 30 to indicate that the model diverged or produced poor results.
of MMD-GAN with RBF and RBF-B kernel5. Note that when λ = −1, the models are essentially
MMD-rbf (with a single Gaussian kernel) and MMD-rbf-b when RBF and RBF-B kernel are used
respectively. We observed that: 1) the model performed well using repulsive loss (i.e., λ ≥0),
with λ = 0.5, 1 slightly better than λ = −0.5, 0, 2; 2) the MMD-rbf model can be signiﬁcantly
improved by simply increasing λ from −1 to −0.5, which reduces the attraction of discriminator on
real sample scores; 3) larger λ may lead to more diverged models, possibly because the discriminator
focuses more on expanding the real sample scores over adversarial learning; note when λ ≫1, the
model would simply learn to expand all real sample scores and pull the generated sample scores to
real samples’, which is a divergent process; 4) the RBF-B kernel managed to stabilize MMD-rep for
most diverged cases but may occasionally cause the FID score to rise up.
The proposed methods were further evaluated in Appendix A, C and D. In Appendix A.2, we used a
simulation study to show the local stability of MMD-rep trained by gradient descent, while its global
stability is not guaranteed as bad initialization may lead to trivial solutions. The problem may be
alleviated by adjusting the learning rate for generator. In Appendix C.3, we showed the proposed
generalized power iteration (Section 4.2) imposes a stronger Lipschitz constraint than the method
in Miyato et al. , and beneﬁted MMD-GAN training using the repulsive loss. Moreover,
the RBF-B kernel managed to stabilize the MMD-GAN training for various conﬁgurations of the
spectral normalization method. In Appendix D.1, we showed the gradient penalty can also be used
with the repulsive loss. In Appendix D.2, we showed that it was better to use more than one neuron
at the discriminator output layer for the repulsive loss.
QUALITATIVE ANALYSIS
The discriminator outputs may be interpreted as a learned representation of the input samples. Fig. 4
visualizes the discriminator outputs learned by the MMD-rbf and proposed MMD-rep methods on
CIFAR-10 dataset using t-SNE ). MMD-rbf ignored the class structure in
data (see Fig. 4a) while MMD-rep learned to concentrate the data from the same class and separate
different classes to some extent (Fig. 4b). This is because the discriminator D has to actively learn
5For λ < 0, the RBF-B kernel in Eq. 6 was used in LD,λ.
Published as a conference paper at ICLR 2019
(a) MMD-rbf
(b) MMD-rep
Figure 4: t-SNE visualization of discriminator outputs {D(x)} learned by (a) MMD-rbf and (b)
MMD-rep for 2560 real samples from the CIFAR-10 dataset, colored by their class labels.
the data structure in order to expands the real sample scores {D(x)}. Thus, we speculate that
techniques reinforcing the learning of cluster structures in data may further improve the training of
In addition, the performance gain of proposed repulsive loss (Eq. 4) over the attractive loss (Eq. 3)
comes at no additional computational cost. In fact, by using a single kernel rather than a linear combination of kernels, MMD-rep and MMD-rep-b are simpler than MMD-rbf and MMD-rq. Besides,
given a typically small batch size and a small number of discriminator output neurons (64 and 16 in
our experiments), the cost of MMD over the non-saturating and hinge loss is marginal compared to
the convolution operations.
In Appendix D.3, we provide some random samples generated by the methods in our study.
DISCUSSION
This study extends the previous work on MMD-GAN ) with two contributions.
First, we interpreted the optimization of MMD loss as a combination of attraction and repulsion
processes, and proposed a repulsive loss for the discriminator that actively learns the difference
among real data. Second, we proposed a bounded Gaussian RBF (RBF-B) kernel to address the
saturation issue. Empirically, we observed that the repulsive loss may result in unstable training, due
to factors including initialization (Appendix A.2), learning rate (Fig. 3b) and Lipschitz constraints on
the discriminator (Appendix C.3). The RBF-B kernel managed to stabilize the MMD-GAN training
in many cases. Tuning the hyper-parameters in RBF-B kernel or using other regularization methods
may further improve our results.
The theoretical advantages of MMD-GAN require the discriminator to be injective. The proposed
repulsive loss (Eq. 4) attempts to realize this by explicitly maximizing the pair-wise distances among
the real samples. Li et al. achieved the injection property by using the discriminator as the
encoder and an auxiliary network as the decoder to reconstruct the real and generated samples, which
is more computationally extensive than our proposed approach. On the other hand, Bi´nkowski et al.
 ; Arbel et al. imposed a Lipschitz constraint on the discriminator in MMD-GAN via
gradient penalty, which may not necessarily promote an injective discriminator.
The idea of repulsion on real sample scores is in line with existing studies. It has been widely
accepted that the quality of generated samples can be signiﬁcantly improved by integrating labels
 ; Miyato & Koyama ; Zhou et al. ) or even pseudo-labels generated
by k-means method ) in the training of discriminator. The reason may be that
the labels help concentrate the data from the same class and separate those from different classes.
Using a pre-trained classiﬁer may also help produce vivid image samples ) as
the learned representations of the real samples in the hidden layers of the classiﬁer tend to be well
separated/organized and may produce more meaningful gradients to the generator.
Published as a conference paper at ICLR 2019
At last, we note that the proposed repulsive loss is orthogonal to the GAN studies on designing
network structures and training procedures, and thus may be combined with a variety of novel techniques. For example, the ResNet architecture ) has been reported to outperform the
plain DCGAN used in our experiments on image generation tasks ; Gulrajani
et al. ) and self-attention module may further improve the results ). On
the other hand, Karras et al. proposed to progressively grows the size of both discriminator
and generator and achieved the state-of-the-art performance on unsupervised training of GANs on
the CIFAR-10 dataset. Future work may explore these directions.
ACKNOWLEDGMENTS
Wei Wang is fully supported by the Ph.D. scholarships of The University of Melbourne. This work
is partially funded by Australian Research Council grant DP150103512 and undertaken using the
LIEF HPC-GPGPU Facility hosted at the University of Melbourne. The Facility was established
with the assistance of LIEF Grant LE170100200.