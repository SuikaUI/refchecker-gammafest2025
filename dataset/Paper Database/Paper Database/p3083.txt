ExFuse: Enhancing Feature Fusion for Semantic
Segmentation
Zhenli Zhang1, Xiangyu Zhang2, Chao Peng2, Dazhi Cheng3, Jian Sun2
1Fudan University, 2Megvii Inc. (Face++), 3Beijing Institute of Technology
 , {zhangxiangyu, pengchao}@megvii.com,
 , 
Abstract. Modern semantic segmentation frameworks usually combine
low-level and high-level features from pre-trained backbone convolutional
models to boost performance. In this paper, we ﬁrst point out that a
simple fusion of low-level and high-level features could be less eﬀective
because of the gap in semantic levels and spatial resolution. We ﬁnd
that introducing semantic information into low-level features and highresolution details into high-level features is more eﬀective for the later
fusion. Based on this observation, we propose a new framework, named
ExFuse, to bridge the gap between low-level and high-level features thus
signiﬁcantly improve the segmentation quality by 4.0% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012
segmentation benchmark and achieve 87.9% mean IoU, which outperforms the previous state-of-the-art results.
Keywords: Semantic Segmentation, Convolutional Neural Networks
Introduction
Most state-of-the-art semantic segmentation frameworks 
follow the design of Fully Convolutional Network (FCN) . FCN has a typical encoder-decoder structure – semantic information is ﬁrstly embedded into
the feature maps via encoder then the decoder takes responsibility for generating
segmentation results. Usually the encoder is the pre-trained convolutional model
to extract image features and the decoder contains multiple upsampling components to recover resolution. Although the top-most feature maps of the encoder
could be highly semantic, its ability to reconstruct precise details in segmentation
maps is limited due to insuﬃcient resolution, which is very common in modern
backbone models such as . To address this, an “U-Net” architecture is proposed and adopted in many recent work . The
core idea of U-Net is to gradually fuse high-level low-resolution features from
top layers with low-level but high-resolution features from bottom layers, which
is expected to be helpful for the decoder to generate high-resolution semantic
Though the great success of U-Net, the working mechanism is still unknown
and worth further investigating. Low-level and high-level features are complementary by nature, where low-level features are rich in spatial details but lack
 
Zhang et al.
Fig. 1. Fusion of low-level and high-level features. a) “Pure” low-level high-resolution
and “pure” high-level low-resolution features are diﬃcult to be fused because of the
signiﬁcant semantic and resolution gaps. b) Introducing semantic information into lowlevel features or spatial information into high-level features beneﬁts the feature fusion.
“dn” and “up” blocks represent abstract up/down-sampling feature embedding.
semantic information and vice versa. Consider the extreme case that “pure”
low-level features only encode low-level concepts such as points, lines or edges.
Intuitively, the fusion of high-level features with such “pure” low-level features
helps little, because low-level features are too noisy to provide suﬃcient highresolution semantic guidance. In contrast, if low-level features include more semantic information, for example, encode relatively clearer semantic boundaries,
then the fusion becomes easy – ﬁne segmentation results could be obtained by
aligning high-level feature maps to the boundary. Similarly, “pure” high-level
features with little spatial information cannot take full advantage of low-level
features; however, with additional high-resolution features embedded, high-level
features may have chance to reﬁne itself by aligning to the nearest low-level
boundary. Fig 1 illustrates the above concepts. Empirically, the semantic and
resolution overlap between low-level and high-level features plays an important
role in the eﬀectiveness of feature fusion. In other words, feature fusion could be
enhanced by introducing more semantic concepts into low-level features or by
embedding more spatial information into high-level features.
Motivated by the above observation, we propose to boost the feature fusion
by bridging the semantic and resolution gap between low-level and high-level
feature maps. We propose a framework named ExFuse, which addresses the gap
from the following two aspects: 1) to introduce more semantic information into
low-level features, we suggest three solutions – layer rearrangement, semantic
supervision and semantic embedding branch; 2) to embed more spatial infor-
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
mation into high-level features, we propose two novel methods: explicit channel
resolution embedding and densely adjacent prediction. Signiﬁcant improvements
are obtained by either approach and a total increase of 4% is obtained by the
combination. Furthermore, we evaluate our method on the challenging PASCAL
VOC 2012 semantic segmentation task. In the test dataset, we achieve the
score of 87.9% mean IoU, surpassing the previous state-of-the-art methods.
Our contributions can be summerized as follows:
– We suggest a new perspective to boost semantic segmentation performance,
i.e. bridging the semantic and resolution gap between low-level and high-level
features by more eﬀective feature fusion.
– We propose a novel framework named ExFuse, which introduces more semantic information into low-level features and more spatial high-resolution
information into high-level features. Signiﬁcant improvements are obtained
from the enhanced feature fusion.
– Our fully-equipped model achieves the new state-of-the-art result on the test
set of PASCAL VOC 2012 segmentation benchmark.
Related Work
Feature fusion in semantic segmentation. Feature fusion is frequently employed
in semantic segmentation for diﬀerent purposes and concepts. A lot of methods fuse low-level but high-resolution features and high-level low-resolution features together . Besides, ASPP module is proposed in DeepLab
 to fuse multi-scale features to tackle objects of diﬀerent size. Pyramid
pooling module in PSPNet serves the same purpose through diﬀerent implementation. BoxSup empirically fuses feature maps of bounding boxes and
segmentation maps to further enhance segmentation.
Deeply supervised learning. To the best of our knowledge, deeply supervised
training is initially proposed in , which aims to ease the training process of
very deep neural networks since depth is the key limitation for training modern neural networks until batch normalization and residual networks 
are proposed. Extra losses are utilized in GoogleNet for the same purpose.
Recently, PSPNet also employs this method to ease the optimization when
training deeper networks.
Upsampling. There are mainly three approaches to upsample a feature map. The
ﬁrst one is bilinear interpolation, which is widely used in . The second
method is deconvolution, which is initially proposed in FCN and utilized in
later work such as . The third one is called “sub-pixel convolution”,
which derives from in super resolution task and is widely broadcast to
other tasks such as semantic segmentation. For instance, employs it to replace
the traditional deconvolution operation.
Zhang et al.
Fig. 2. Overall architecture of our approach. Components with solid boxes belong to
the backbone GCN framework , while others with dashed lines are proposed in this
work. Similar to , Boundary Reﬁnement blocks are actually used but omitted in the
ﬁgure. Numbers (H×W ×C) in blocks specify the output dimension of each component.
SS – semantic supervision. ECRE – explicit channel resolution embedding. SEB –
semantic embedding branch. DAP – densely adjacent prediction.
In this work we mainly focus on the feature fusion problem in “U-Net” segmentation frameworks . In general, U-Net have an encoder-decoder
structure as shown in Fig 1. Usually the encoder part is based on a convolutional
model pretrained on large-scale classiﬁcation dataset (e.g. ImageNet ), which
generates low-level but high-resolution features from the bottom layers and highlevel low-resolution features from the top layers. Then the decoder part mixes
up the features to predict segmentation results. A common way of feature fusion
 is to formulate as a residual form:
yl = Upsample(yl+1) + F(xl)
where yl is the fused feature at l-th level; xl stands for the l-th feature generated
by the encoder. Features with larger l have higher semantic level but lower spatial
resolution and vice versa (see Fig 2).
In Sec 1 we argue that feature fusion could become less eﬀective if there
is a large semantic or resolution gap between low-level and high-level features.
To study and verify the impact, we choose one of the start-of-the-art “U-Net”
frameworks – Global Convolutional Network (GCN) – as our backbone segmentation architecture (see Fig 2 for details). In GCN, 4 diﬀerent semantic levels
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
of feature maps are extracted from the encoder network, whose spatial resolutions, given the 512 × 512 input, are {128, 64, 32, 16} respectively. To examine
the eﬀectiveness of feature fusion, we select several subsets of feature levels and
use them to retrain the whole system. Results are shown in Table 1. It is clear
that even though the segmentation quality increases with the fusion of more
feature levels, the performance tends to saturate quickly. Especially, the lowest
two feature levels (1 and 2) only contribute marginal improvements (0.24% for
ResNet 50 and 0.05% for ResNeXt 101), which implies the fusion of low-level
and high-level features is rather ineﬀective in this framework.
In the following subsections we will introduce our solutions to bridge the
gap between low-level and high-level features – embedding more semantic information into low-level features and more spatial resolution clues into high-level
features. First of all, we introduce our baseline settings:
Feature Levels ResNet 50 (%) ResNeXt 101 (%)
{1, 2, 3, 4}
Table 1. GCN segmentation results using given feature levels. Performances are
evaluated by standard mean IoU(%) on PASCAL VOC 2012 validation set. Lower
feature level involves less semantic but higher-resolution features and vice versa (see
Fig 2). The feature extractor is based on pretrained ResNet50 and ResNeXt101 
model. Performance is evaluated in mIoU.
Baseline Settings. The overall semantic segmentation framework follows the
fully-equipped GCN architecture, as shown in Fig 2. For the backbone encoder network, we use ResNeXt 101 model pretrained on ImageNet by default1 unless otherwise mentioned. We use two public-available semantic segmentation benchmarks – PASCAL VOC 2012 and Semantic Boundaries Dataset
 – for training and evaluate performances on PASCAL VOC 2012 validation
set, which is consistent with many previous work .
The performance is measured by standard mean intersection-over-union (mean
IoU). Other training and test details or hyper-parameters are exactly the same
as . Our reproduced GCN baseline score is 76.0%, shown in Table 3 (#1).
Introducing More Semantic Information into Low-level Features
Our solutions are inspired by the fact: for convolutional neural networks, feature
maps close to semantic supervisions (e.g. classiﬁcation loss) tend to encode more
1 Though ResNeXt 101 performs much better than ResNet 101 on ImageNet classiﬁcation task (21.2% vs. 23.6% in top-1 error), we ﬁnd there are no signiﬁcant
diﬀerences on the semantic segmentation results (both are 76.0% mIoU).
Zhang et al.
semantic information, which has been conﬁrmed by some visualization work .
We propose three methods as follows:
Layer Rearrangement In our framework, features are extracted from the
tail of each stage in the encoder part (res-2 to res-5 in Fig 2). To make lowlevel features (res-2 or res-3) ’closer’ to the supervisions, one straight-forward
approach is to arrange more layers in the early stages rather than the latter. For
example, ResNeXt 101 model has {3, 4, 23, 3} building blocks for Stage 2-5
respectively; we rearrange the assignment into {8, 8, 9, 8} and adjust the number
of channels to ensure the same overall computational complexity. Experiment
shows that even though the ImageNet classiﬁcation score of the newly designed
model is almost unchanged, its segmentation performance increases by 0.8%
(Table 3, compare #2 with #3), which implies the quality of low-level feature
might be improved.
Semantic Supervision We come up with another way to improve low-level
features, named Semantic Supervision (SS), by assigning auxiliary supervisions
directly to the early stages of the encoder network (see Fig 2). To generate semantic outputs in the auxiliary branches, low-level features are forced to encode
more semantic concepts, which is expected to be helpful for later feature fusion.
Such methodology is inspired by Deeply Supervised Learning used in some old
classiﬁcation networks to ease the training of deep networks. However,
more sophisticated classiﬁcation models suggest end-to-end
training without auxiliary losses, which is proved to have no convergence issue
even for models over 100 layers. Our experiment also shows that for ResNet or
ResNeXt models deeply supervised training is useless or even harms the classi-
ﬁcation accuracy (see Table 2). Therefore, our Semantic Supervision approach
mainly focuses on improving the quality of low-level features, rather than boosting the backbone model itself.
Cls err (top-1, %) Seg mIoU (%)
Table 2. Eﬀects of Semantic Supervision (SS). Classiﬁcation scores are evaluated on
ImageNet 2012 validation set.
Fig 3 shows the detailed structure of our Semantic Supervision block. When
pretraining the backbone encoder network, the components are attached to the
tail of each stage as auxiliary supervisions (see Fig 2). The overall classiﬁcation
loss equals to a weighted summation of all auxiliary branches. Then after pretraining, we remove these branches and use the remaining part for ﬁne tuning.
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
Experiment shows the method boosts the segmentation result by 1.1%. Moreover, we ﬁnd that if features are extracted from the second convolutional layer
in the auxiliary module for ﬁne tuning (Fig 3), more improvement (1.5%) is
obtained (see Table 3, compare #1 with #2), which supports our intuition that
feature maps closer to the supervision tend to encode more semantic information.
Fig. 3. Details of Semantic Supervision (SS) component in our pipeline.
It is worth noting that the recent semantic segmentation work PSPNet 
also employs deeply supervised learning and reports the improvements. Diﬀerent
from ours, the architecture of do not extract feature maps supervised by the
auxiliary explicitly; and their main purpose is to ease the optimization during
training. However, in our framework we ﬁnd the improvements may result from
diﬀerent reasons. For instance, we choose a relatively shallower network ResNet
50 and pretrain with or without semantic supervision. From Table 2, we ﬁnd
the auxiliary losses do not improve the classiﬁcation score, which implies ResNet
50 is unlikely to suﬀer from optimization diﬃculty. However, it still boosts the
segmentation result by 1.1%, which is comparable to the deeper case of ResNeXt
101 (1.0%). We believe the enhancement in our framework mainly results from
more “semantic” low-level features.
Semantic Embedding Branch As mentioned above, many “U-Net” structures involve low-level feature as the residue to the upsampled high-level feature.
In Equ 1 the residual term F(xl) is a function of low-level but high-resolution
feature, which is used to ﬁll the spatial details. However, if the low-level feature
contains little semantic information, it is insuﬃcient to recover the semantic
resolution. To address the drawback, we generalize the fusion as follows:
yl = Upsample (yl+1) + F(xl, xl+1, . . . , xL)
Zhang et al.
where L is the number of feature levels. Our insight is to involve more semantic
information from high-level features to guide the resolution fusion.
The detailed design of function F (·) is illustrated in Fig 4, named Semantic
Embedding Branch, (SEB). We use the component for features of Level 1-3 (see
Fig 2). In our experiment SEB improves the performance by 0.7% (Table 3,
compare #3 with #5).
Fig. 4. Design of the Semantic Embedding Branch in Fig 2. The “×” sign means
element-wise multiplication. If there are more than one groups of high-level features,
the component outputs the production of each feature map after upsampling.
Embedding More Spatial Resolution into High-level Features
For most backbone feature extractor networks, high-level features have very limited spatial resolution. For example, the spatial size of top-most feature map in
ResNet or ResNeXt is 7 × 7 for 224 × 224 input size. To encode more spatial
details, a widely used approach is dilated strategy , which is able
to enlarge feature resolution without retraining the backbone network. However,
since high-level feature maps involve a lot of channels, larger spatial size signiﬁcantly increases the computational cost. So in this work we mainly consider
another direction – we do not try to increase the “physical” resolution of the
feature maps; instead, we expect more resolution information encoded
within channels. We propose the following two methods:
Explicit Channel Resolution Embedding In our overall framework, segmentation loss is only connected to the output of decoder network (see Fig 2),
which is considered to have less impact on the spatial information of high-level
features by intuition. One straight-forward solution is to borrow the idea of Semantic Supervision (Sec 3.1) – we could add an auxiliary supervision branch
to the high-level feature map, upsample and force it to learn ﬁne segmentation
map. Following the insight, ﬁrstly we try adding an extra segmentation loss to
the ﬁrst deconvolution module (the light-blue component in Fig 2), however, no
improvements are obtained (Table 4, #2).
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
Fig. 5. Illustration of the design of Explicit Channel Resolution Embedding (ECRE)
module in Fig 2.
Why does the auxiliary loss fail to work? Note that the purpose of the supervision is to embed high resolution information “explicitly” into feature map
channels. However, since deconvolution layer includes weights, the embedding
becomes implicit. To overcome this issue, we adopt a parameter-free upsampling
method – Sub-pixel Upsample – to replace the original deconvolution.
Since sub-pixel upsample enlarge the feature map just by reshaping the spatial
and channel dimensions, the auxiliary supervision is able to explicitly impact the
features. Details of the component are shown in Fig 5. Experiment shows that
it enhances the performance by 0.5% (see Table 4 and Table 3).
Moreover, to demonstrate that the improvement is brought by explicit resolution embedding rather than sub-pixel upsampling itself, we also try to replace
the deconvolution layer only without auxiliary supervision. Table 4 (#3) shows
the result, which is even worse than the baseline.
Index Baseline SS LR ECRE SEB DAP mIoU (%)
Table 3. Ablation experiments of the methods in Sec 3. Performances are evaluated
by standard mean IoU(%) on PASCAL VOC 2012 validation set. The baseline model is
 (our impl.) SS – semantic supervision. LR – layer rearrangement. ECRE – explicit
channel resolution embedding. SEB – semantic embedding branch. DAP – densely
adjacent prediction.
Zhang et al.
Deconv + Supervised
Sub-pixel Upsample Only
ECRE (Fig 5)
Table 4. Ablation study on the design of Explicit Channel Resolution Embedding,
(ECRE). The baseline model is in Table 3 (#3)
Densely Adjacent Prediction In the decoder upstream of the original architecture (Fig 2), feature point at the spatial location (i, j) mainly takes responsibility for the semantic information at the same place. To encode as much spatial
information into channels, we propose a novel mechanism named Densely Adjacent Prediction (DAP), which allows to predict results at the adjacent position,
e.g. (i −1, j + 1). Then to get the ﬁnal segmentation map, result at the position
(i, j) can be generated by averaging the associated scores. Formally, given the
window size k × k, we divide the feature channels into k × k groups, then DAP
works as follows:
i+l−⌊k/2⌋,j+m−⌊k/2⌋
where ri,j denotes the result at the position (i, j) and x(c)
i,j stands for the
features at the position (i, j) belonging to channel group c. In Fig 6 we illustrate
the concept of DAP.
Fig. 6. Illustration of Densely Adjacent Prediction (DAP) component in Fig 2.
We use DAP on the output of our decoder (see Fig 2). In our experiment we
set k = 3. Note that DAP requires the number of feature channels increased by
k × k times, so we increase the output channels of each deconvolution block to
189 (21 × 3 × 3). For fair comparison, we also evaluate the baseline model with
the same number of channels. Results are shown in Table 5. It is clear that DAP
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
improves the performance by 0.6% while the counterpart model without DAP
only obtains marginal gain, which implies DAP may be helpful for feature maps
to embed more spatial information.
Baseline (more channels)
DAP (Fig 6)
Table 5. Ablation study on the eﬀect of Densely Adjacent Prediction (DAP). The
baseline model is in Table 3 (#5)
Discussions
Is Feature Fusion Enhanced? At the beginning of Sec 3 we demonstrate
that feature fusion in our baseline architecture (GCN ) is ineﬀective. Only
marginal improvements are obtained by fusing low-level features (Level 1 and
2), as shown in Table 1. We attribute the issue to the semantic and resolution gap
between low-level and high-level features. In Sec 3.1 and Sec 3.2, we propose a
series of solutions to introduce more semantic information into low-level features
and more spatial details into high-level features.
Despite the improved performance, a question raises: is feature fusion in the
framework really improved? To justify this, similar to Table 1 we compare several
subsets of diﬀerent feature levels and use them to train original baseline (GCN)
and our proposed model (ExFuse) respectively. For the ExFuse model, all the 5
approaches in Sec 3.1 and Sec 3.2 are used. Table 6 shows the results. We ﬁnd
that combined with low-level feature maps (Level 1 and 2) the proposed ExFuse
still achieves considerable performance gain (∼1.3%), while the baseline model
cannot beneﬁt from them. The comparison implies our insights and methodology
enhance the feature fusion indeed.
Table 6 also shows that the proposed model is much better than the baseline
in the case that only top-most feature maps (Level 4) are used, which implies
the superior high-level feature quality to the original model. Our further study
shows that methods in Sec 3.2 contribute most of the improvement. Empirically
we conclude that boosting high-level features not only beneﬁts feature fusion,
but also contributes directly to the segmentation performance.
Do techniques work in a vanilla U-Net? Previously we would like to demonstrate that the proposed perspective and techniques are able to improve one of
the state-of-the-art U-Net structure – GCN. To prove the good generalization
of this paper, we apply techniques illustrated above to a vanilla U-Net without
GCN module. Performance on PASCAL VOC 2012 is boosted from 72.7 to 79.6
Zhang et al.
Feature Levels Original GCN (%) ExFuse (%)
{1, 2, 3, 4}
Table 6. Comparison of Original GCN and ExFuse on segmentation results using
given feature levels. The backbone feature extractor networks are both ResNeXt 101.
in mIoU. We see that the gap is even bigger (6.9 instead of 4.0), which shows
that techniques illustrated above generalize well.
Could the perspective and techniques generalize to other computer
vision tasks? Since U-Net structure is widely applied to other vision tasks
such as low-level vision and detection , a question raises naturally: could
the proposed perspective and techniques generalize to other tasks? We carefully
conducted ablation experiments and observe positive results. We leave detailed
discussion for future work.
PASCAL VOC 2012 Experiment
In the last section we introduce our methodology and evaluate their eﬀectiveness
via ablation experiments. In this section we investigate the fully-equipped system
and report benchmark results on PASCAL VOC 2012 test set.
To further improve the feature quality, we use deeper ResNeXt 131 as our
backbone feature extractor, in which Squeeze-and-excitation modules are
also involved. The number of building blocks for Stage 2-5 is {8, 8, 19, 8} respectively, which follows the idea of Sec 3.1. With ResNeXt 131, we get 0.8%
performance gain and achieve 80.8% mIoU when training with 10582 images
from PASCAL VOC 2012 and Semantic Boundaries Dataset (SBD) ,
which is 2.3% better than DeepLabv3 at the same settings.
Following the same procedure as , we employ Microsoft
COCO dataset to pretrain our model. COCO has 80 classes and we only
retain images including the same 20 classes in PASCAL VOC 2012 and all other
Index ResNeXt 131 COCO Flip mIoU (%)
(ResNeXt 101)
Table 7. Strategies and results on PASCAL VOC 2012 validation set
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
classes are regarded as background. Training process has 3 stages. In stage-
1, we mix up all images in COCO, SBD and standard PASCAL VOC 2012
images, resulting in 109892 images for training in total. In stage-2, we utilize SBD
and PASCAL VOC 2012 training images. Finally for stage-3, we only employ
standard PASCAL VOC 2012 training set. We keep image crop size unchanged
during the whole training procedure and all other settings are exactly the same
as . COCO pretraining brings about another 4.6% increase in performance,
as shown in Table 7 (#2 and #3).
We further average the score map of an image with its horizontal ﬂipped
version and eventually get a 85.8% mIoU on PASCAL VOC 2012 validation set,
which is 2.3% better than DeepLabv3+ (Table 7 #4).
Resembling , we then freeze the batch normalization parameters and ﬁne
tune our model on oﬃcial PASCAL VOC 2012 trainval set. In particular, we duplicate the images that contain hard classes (namely bicycle, chair, dining table,
potted plant and sofa). Finally, our ExFuse framework achieves 87.9% mIoU on
PASCAL VOC 2012 test set without any DenseCRF post-processing, which
surpasses previous state-of-the-art results, as shown in Table 8. For fair comparison, we also evaluate our model using a standard ResNet101 and it achieves
86.2% mIoU, which is better than DeepLabv3 at the same setting.
Tusimple 
Large Kernel Matters 
Multipath ReﬁneNet 
ResNet 38 MS COCO 
PSPNet 
DeepLabv3 
DeepLabv3+ (Xception) 
ExFuse ResNet101 (ours)
ExFuse ResNeXt131 (ours) 87.9
Table 8. Performance on PASCAL VOC 2012 test set
Fig 7 visualizes some representative results of the GCN baseline and our
proposed ExFuse framework. It is clear that the visualization quality of our
method is much better than the baseline. For example, the boundary in ExFuse
is more precise than GCN.
Conclusions
In this work, we ﬁrst point out the ineﬀective feature fusion problem in current U-Net structure. Then, we propose our ExFuse framework to tackle this
problem via bridging the gap between high-level low-resolution and low-level
Zhang et al.
Fig. 7. Examples of semantic segmentation results on PASCAL VOC 2012 validation
set. (b) is our GCN baseline which achieves 81.0% mIoU on val set. (c) is our method
which achieves 85.4% on val set, as shown in Table 7 #3.
ExFuse: Enhancing Feature Fusion for Semantic Segmentation
high-resolution features. Eventually, better feature fusion is demonstrated by the
performance boost when fusing with original low-level features and the overall
segmentation performance is improved by a large margin. Our ExFuse framework also achieves new state-of-the-art performance on PASCAL VOC 2012
benchmark.