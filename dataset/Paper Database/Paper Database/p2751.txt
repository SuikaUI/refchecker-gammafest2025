Less is More: Exploiting the Standard Compiler
Optimization Levels for Better Performance and
Energy Consumption
Kyriakos Georgiou1, Craig Blackmore1, Samuel Xavier-de-Souza2, Kerstin
1 University of Bristol, UK
2 Universidade Federal do Rio Grande do Norte, Brazil
Abstract. This paper presents the interesting observation that by performing fewer of the optimizations available in a standard compiler optimization level such as -O2, while preserving their original ordering, signiﬁcant savings can be achieved in both execution time and energy consumption. This observation has been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM Cortex-M3, using two
diﬀerent versions of the LLVM compilation framework; v3.8 and v5.0.
Experimental evaluation with 71 embedded benchmarks demonstrated
performance gains for at least half of the benchmarks for both processors. An average execution time reduction of 2.4% and 5.3% was achieved
across all the benchmarks for the Cortex-M0 and Cortex-M3 processors,
respectively, with execution time improvements ranging from 1% up to
90% over the -O2. The savings that can be achieved are in the same range
as what can be achieved by the state-of-the-art compilation approaches
that use iterative compilation or machine learning to select ﬂags or to determine phase orderings that result in more eﬃcient code. In contrast to
these time consuming and expensive to apply techniques, our approach
only needs to test a limited number of optimization conﬁgurations, less
than 64, to obtain similar or even better savings. Furthermore, our approach can support multi-criteria optimization as it targets execution
time, energy consumption and code size at the same time.
Introduction
Compilers were introduced to abstract away the ever-increasing complexity of
hardware and improve software development productivity. At the same time,
compiler developers face a hard challenge: producing optimized code. A modern
compiler supports a large number of architectures and programming languages
and it is used for a vast diversity of applications. Thus, tuning the compiler
optimizations to perform well across all possible applications is impractical. The
task is even harder as compilers need to adapt to rapid advancements in hardware
and programming languages.
Modern compilers adopted two main practices to mitigate the problem and
ﬁnd a good balance between the eﬀort needed to develop compilers and their
 
K. Georgiou et al.
eﬀectiveness in optimizing code. The ﬁrst approach is the splitting of the compilation process into distinct phases. Modern compilers such as those based on the
LLVM compilation framework [Lat02], allow for a common optimizer that can
be used by any architecture and programming language. This is made possible
by the use of an Intermediate Representation (IR) language on which optimizations are applied. Then a front-end framework is provided to allow programming
languages to be translated into the IR, and a back-end framework exists that
allows the IR to be translated into speciﬁc instruction set architectures (ISA).
Therefore, to take advantage of the common optimizer one only needs to create a new front-end for a programming language and a new back-end for an
architecture.
The second practice is the use of standard optimization levels, typically -O0,
-O1, -O2, -O3 and -Os. Most modern compilers have a large number of transformations exposed to software developers via compiler ﬂags; for example, the
LLVM’s optimizer has 56 documented transformations [LLV18]. There are two
major challenges a software developer faces while using compilers. First, to select
the right set of transformations, and second to order the chosen transformations
in a meaningful way, also called the compiler phase-ordering problem. The common objective is to achieve the best resource usage based on the application’s requirements. To address this, each standard optimization level oﬀers a predeﬁned
sequence of optimizations, which are proven to perform well based on a number
of micro-benchmarks and a range of architectures. For example, for the LLVM
compilation framework, starting from the -O0 level, which has no optimizations
enabled, and moving to -O3, each level oﬀers more aggressive optimizations with
the main focus being performance, while -Os is focused on optimizing code size.
Code size is critical for embedded applications with a limited amount of memory
available. Furthermore, the optimization sequences deﬁned for each level encapsulate the accumulated empirical knowledge of compiler engineers over the years.
For example, some optimizations depend on other code transformations being
applied ﬁrst, and some optimizations oﬀer more opportunities for other optimizations. Note that a code transformation is not necessarily an optimization,
but instead, it can facilitate an IR structure which enables the application of
other optimizations. Thus, a code transformation does not always lead to better
performance.
Although standard optimization levels are a good starting point, they are
far from optimal in many cases, depending on the application and architecture
used. An optimization conﬁguration is a sequence of ordered ﬂags. Due to the
huge number of possible ﬂag combinations and their possible orderings, it is
impractical to explore the whole optimization-conﬁguration space. Thus, ﬁnding
optimal optimization conﬁgurations is still an open challenge. To tackle this
issue, iterative compilation and machine-learning techniques have been used to
ﬁnd good optimization sequences by exploiting only a fraction of the optimization
space [AKC+18]. Techniques involving iterative compilation are expensive since
typically a large amount of optimization conﬁgurations, in the order of hundreds
to thousands, need to be exercised before reaching any performance gains over
Less is More: Exploiting the Standard Compiler Optimization Levels
standard optimization levels. On the other hand, machine learning approaches
require a large training phase and are hardly portable across compilers and
architectures.
This paper takes a diﬀerent approach. Instead of trying to explore a fraction of the whole optimization space, we are focusing on exploiting the existing
optimization levels. For example, using the optimization ﬂags included in the
-O2 optimization level as a starting point, a new optimization conﬁguration is
generated each time by removing the last transformation ﬂag of the current optimization conﬁguration. In this way, each new conﬁguration is a subsequence of
the -O2 conﬁguration, that preserves the ordering of ﬂags in the original optimization level. Thus, each new optimization conﬁguration stops the optimization
earlier than the previously generated conﬁguration did. This approach aims to
preserve the empirical knowledge built into the ordering of ﬂags for the standard
optimization levels. The advantages of using this technique are:
– The architecture and the compiler are treated as a black box, and thus, this
technique is easy to port across diﬀerent compilers or versions of the same
compiler, and diﬀerent architectures. To demonstrate this we applied our
approach to two embedded architectures (Arm Cortex-M0 and Cortex-M3)
and two versions of the LLVM compilation framework (v3.8 and v5.0);
– An expensive training phase similar to the ones needed by the machine
learning approaches is not required;
– The empirical knowledge built into the existing optimization levels by the
compiler engineers is being preserved;
– In contrast to machine-learning approaches and random iterative compilation [BKK+98], which permit reordering transformation passes, our technique retains the original order of the transformation passes. Reordering can
break the compilation or create a malfunctioning executable;
– In contrast to the majority of machine-learning approaches, which are often
opaque, our technique provides valuable insights to the software engineer on
how each optimization ﬂag aﬀects the resource of interest;
– Because energy consumption, execution time and code size of each optimization conﬁguration are being monitored during compilation, multi-criteria
optimizations are possible without needing to train a new model for each
Our experimental evaluation demonstrates an average of 2.4% and 5.3% execution time improvement for the Cortex-M0 and Cortex-M3 processors, respectively. Similar savings were achieved for energy consumption. These results are
in the range of what existing complicated machine learning or time consuming iterative compilation approaches can oﬀer on the same embedded processors [BRE15, PHB15].
The rest of the paper is organized as follows. Section 2 gives an overview
of the compilation and analysis methodology used. Our experimental evaluation
methodology, benchmarks and results are presented and discussed in Section 3.
Section 4 critically reviews previous work related to ours. Finally, Section 5
concludes the paper and outlines opportunities for future work.
K. Georgiou et al.
Optimizaion Config.
Resource Usage
Measurement
LLVM Back-End
LLVM Optimizer
Clang Front-End
Executable
Fig. 1: Compilation and evaluation process.
Compilation and Analysis
As the primary focus of this work is deeply embedded systems, we demonstrate
the portability of our technique across diﬀerent architectures by exploring two
of the most popular embedded processors: the Arm Cortex-M0 [ARM18a] and
the Arm Cortex-M3 [ARM18b]. Although the two architectures belong to the
same family, they have signiﬁcant diﬀerences in terms of performance and power
consumption characteristics [ARM18c]. The technique treats an architecture
as a black box as no resource models are required e.g. energy-consumption or
execution-time models. Instead, execution time and energy consumption physical measurements are used to assess the eﬀectiveness of a new optimization
conﬁguration on a program.
For demonstrating the portability of the technique across diﬀerent compiler
versions, the analysis for the Cortex-M0 processor was performed using the
LLVM compilation framework v3.8., and for the Cortex-M3 using the LLVM
compilation framework v5.0. The technique treats the compiler as a black box
since it only uses the compilation framework to exercise the diﬀerent optimizationconﬁguration scenarios, extracted from a predeﬁned optimization level, on a particular program. In contrast, machine-learning-based techniques typically require
a heavy training phase for each new compiler version or when a new optimization
ﬂag is introduced [ABP+17, BRE15].
Figure 1 demonstrates the process used to evaluate the eﬀectiveness of the
diﬀerent optimization conﬁgurations explored. Each conﬁguration is a set of ordered ﬂags used to drive the analysis and transformation passes by the LLVM
optimizer. An analysis pass can identify properties and expose optimization opportunities that can later be used by transformation passes to perform optimizations. A standard optimization level (-O1, -O2, -O3, -Os, -Oz) can be selected
as the starting point. Each optimization level represents a list of optimization
Less is More: Exploiting the Standard Compiler Optimization Levels
ﬂags which have a predeﬁned order. Their order inﬂuences the order in which
the transformation / optimization and analysis passes will be applied to the code
under compilation. A new ﬂag conﬁguration is obtained by excluding the last
transformation ﬂag from the current list of ﬂags. Then the new optimization con-
ﬁguration is being applied to the unoptimized intermediate representation (IR)
of the program, obtained from the Clang front-end. Note that the program’s unoptimized IR only needs to be generated once by the Clang front-end; it can then
be used throughout the exploration process thus saving compilation time. The
optimized IR is then passed to the LLVM back-end and linker to generate the
executable for the architecture under consideration. Note that both the back-end
and linker are always called using the optimization level selected for exploration;
in our case -O2. The executable’s energy consumption, execution time and code
size are measured and stored. The exploration process ﬁnishes when the current
list of transformation ﬂags is empty. This is equivalent to optimization level -O0,
where no optimizations are applied by the optimizer. Then, depending on the
resource requirements, the best ﬂag conﬁguration is selected.
There are two kinds of pass dependencies for the LLVM optimizer; explicit
and implicit dependencies. An explicit dependency exists when a transformation
pass requires an other analysis pass to execute ﬁrst. In this case, the optimizer
will automatically schedule the analysis pass if only the transformation pass was
requested by the user. An implicit dependency exists when a transformation
or analysis pass is designed to work after another transformation instead of an
analysis pass. In this case, the optimizer will not schedule the pass automatically,
instead the user must manually add the passes in the correct order to be executed
either using the opt tool or the pass manager. The pass manager is the LLVM
built-in mechanism for scheduling passes and handling their dependencies. If a
pass is requested but its dependencies have not been requested in the correct
order, then the speciﬁed pass will be automatically skipped by the optimizer.
For the predeﬁned optimization levels, the implicit dependencies are predeﬁned
in the pass manager.
To extract the list of transformation and analysis passes, their ordering, and
their dependencies for a predeﬁned level of optimization, we use the argument
”-debug-pass=Structure” with the opt tool (the LLVM optimizer). This information is passed to our ﬂag-selection process, which, to extract a new conﬁguration,
simply eliminates the last optimization ﬂag applied. This ensures that all the implicit dependencies for the remaining passes in the new conﬁguration are still in
place. Thus, the knowledge built into the predeﬁned optimization levels about
eﬀective pass orderings is preserved in the newly generated optimization conﬁgurations. What we are actually questioning is whether the pass scheduling in the
predeﬁned-optimization levels is a good choice. In other words, can stopping the
optimizations at an earlier point yield more optimal code for a speciﬁc program
and architecture?
The BEEBS benchmark suite [PHB13] was used for evaluation. BEEBS is
design for assessing the energy consumption of embedded processors. The resource usage estimation process retrieves the execution time, energy consump-
K. Georgiou et al.
tion and code size for each executable generated. The code size can be retrieved
by examining the size of the executable. The execution time and energy consumption is being measured using the MAGEEC board [Hol13] together with
the pyenergy [Pal15] ﬁrmware and host-side software. The BEEBS benchmark
suite utilizes this energy measurement framework and allows for triggering the
begin and the end of the execution of a benchmark. Thus, energy measurements
are reported only during a benchmark’s execution. Energy consumption, execution time and average power dissipation are reported back to the host. The
MAGEEC board supports a sampling rate of up to six million samples per second. A calibration process was needed prior to measurement to determine the
number of times a benchmark should be executed in a loop while measuring
to obtain an adequate number of measurements. This ensured the collection of
reliable energy values for each benchmark. Finally, the BEEBS benchmark suite
has a built-in self-test mechanism that ﬂags up when a generated executable
is invalid, i.e. it does not provide the expected results. Standard optimization
levels shipped with each new version of a compiler are typically heavily tested
to ensure the production of functionally correct executables. In our case, using
optimization conﬁgurations that are subsequences of the standard optimization
levels increases the chance of generating valid executables. In fact, all the executables we tested passed the BEEBS validation.
Results and Discussion
For the evaluation of our approach, the same 71 benchmarks from the BEEBS
[PHB13] benchmark suite were used for both the Cortex-M0 and the Cortex-M3
processors. For each benchmark, Figure 2 (Figure 2a for the Cortex-M0 and the
LLVM v3.8 and Figure 2b for the Cortex-M3 and the LLVM v5.0) demonstrates
the biggest performance gains achieved by the proposed technique compared to
the standard optimization level under investigation, -O2. In other words, this
ﬁgure represents the resource usage results obtained by using the optimization
conﬁguration, among the conﬁgurations exercised by our technique, that achieves
the best performance gains compared to -O2 for each benchmark. A negative
percentage represents an improvement on a resource, e.g. a result of -20% for
execution time represents a 20% reduction in the execution time obtained by
the selected optimization conﬁguration when compared to the execution time
retrieved by -O2. The energy-consumption and code-size improvements are also
given for the selected conﬁgurations. If two optimization conﬁgurations have the
same performance gains, then energy consumption improvement is used as a
second criterion and code size improvement as a third criterion to select the best
optimization conﬁguration. The selection criteria can be modiﬁed according to
the resource requirements for a speciﬁc application. Moreover, a function can
be introduced to further formalize the selection process when complex multiobjective optimization is required.
For the Cortex-M0 processor, we observed an average reduction in execution
time of 2.5%, with 29 out of the 71 benchmarks seeing execution time improve-
Less is More: Exploiting the Standard Compiler Optimization Levels
arraybinsearch
bubblesort
radix4Division
montecarlo
listinsertsort
janne_complex
compress_test
stb_perlin
dijkstra_small
levenshtein
m0-matmult
Benchmarks
Percentage time, energy,
code size vs -O2
Improvements over -O2
Execution Time
Energy Usage
(a) Results for the Cortex-M0 processor and the LLVM v3.8 compilation framework.
janne_complex
levenshtein
bubblesort
listinsertsort
dijkstra_small
radix4Division
m0-matmult
arraybinsearch
montecarlo
stb_perlin
compress_test
Benchmarks
Percentage time, energy,
code size vs -O2
Improvements over -O2
Execution Time
Energy Usage
(b) Results for the Cortex-M3 processor and the LLVM v5.0 compilation framework.
Fig. 2: Best achieved execution-time improvements over the standard optimization level -O2. For the best execution-time optimization conﬁguration, energy
consumption and code size improvements are also given. A negative percentage
represents a reduction of resource usage compared to -O2.
K. Georgiou et al.
-simplifycfg 10
-ipsccp 23
-globalopt 24
-deadargelim 25
-instcombine 27
-simplifycfg 28
-prune-eh 30
-inline 32
-functionattrs 33
-argpromotion 34
-jump-threading 39
-simplifycfg 41
-instcombine 43
-tailcallelim 44
-simplifycfg 45
-reassociate 46
-loop-simplify 49
-loop-rotate 51
-loop-unswitch 53
-instcombine 54
-loop-simplify 56
-indvars 58
-loop-deletion 60
-loop-unroll 62
-memcpyopt 69
-instcombine 72
-jump-threading 74
-simplifycfg 80
-instcombine 82
-loop-simplify 86
-instcombine 92
-simplifycfg 95
-instcombine 97
-loop-simplify 99
-lcssa 100
-loop-unroll 103
-strip-dead-prt 105
-globaldce 106
-constmerge 107
Compilation Configuration
Percentage time, energy,
code size vs -O2
Mergesort Benchmark (Improvements over -O2)
Execution Time
Energy Usage
-simplifycfg 10
-ipsccp 23
-globalopt 24
-deadargelim 25
-instcombine 27
-simplifycfg 28
-prune-eh 30
-inline 32
-functionattrs 33
-argpromotion 34
-jump-threading 39
-simplifycfg 41
-instcombine 43
-tailcallelim 44
-simplifycfg 45
-reassociate 46
-loop-simplify 49
-loop-rotate 51
-loop-unswitch 53
-instcombine 54
-loop-simplify 56
-indvars 58
-loop-deletion 60
-loop-unroll 62
-memcpyopt 69
-instcombine 72
-jump-threading 74
-simplifycfg 80
-instcombine 82
-loop-simplify 86
-instcombine 92
-simplifycfg 95
-instcombine 97
-loop-simplify 99
-lcssa 100
-loop-unroll 103
-strip-dead-prt 105
-globaldce 106
-constmerge 107
Compilation Configuration
Percentage time, energy,
code size vs -O2
Montecarlo Benchmark (Improvements over -O2)
Execution Time
Energy Usage
(a) Compilation proﬁles for two of the benchmarks, using the Cortex-M0 processor
and the LLVM v3.8 compilation framework.
-simplifycfg 6
-ipsccp 19
-globalopt 21
-mem2reg 23
-deadargelim 24
-instcombine 32
-simplifycfg 33
-prune-eh 36
-inline 37
-functionattrs 38
-jump-threading 49
-simplifycfg 52
-instcombine 60
-tailcallelim 62
-simplifycfg 63
-reassociate 64
-loop-simplify 67
-loop-rotate 73
-loop-unswitch 75
-simplifycfg 76
-instcombine 84
-loop-simplify 85
-indvars 89
-loop-deletion 91
-loop-unroll 92
-memcpyopt 103
-instcombine 114
-jump-threading 116
-loop-simplify 125
-lcssa 127
-simplifycfg 133
-instcombine 141
-globalopt 146
-globaldce 147
-loop-simplify 153
-lcssa 155
-loop-rotate 159
-loop-simplify 176
-instcombine 186
-simplifycfg 191
-instcombine 199
-loop-simplify 200
-lcssa 202
-loop-unroll 204
-instcombine 208
-loop-simplify 209
-lcssa 211
-strip-dead-prt 215
-globaldce 216
-constmerge 217
-loop-simplify 222
-lcssa 224
-simplifycfg 236
Compilation Configuration
Percentage time, energy,
code size vs -O2
Levenshtein Benchmark (Improvements over -O2)
Execution Time
Energy Usage
-simplifycfg 6
-ipsccp 19
-globalopt 21
-mem2reg 23
-deadargelim 24
-instcombine 32
-simplifycfg 33
-prune-eh 36
-inline 37
-functionattrs 38
-jump-threading 49
-simplifycfg 52
-instcombine 60
-tailcallelim 62
-simplifycfg 63
-reassociate 64
-loop-simplify 67
-loop-rotate 73
-loop-unswitch 75
-simplifycfg 76
-instcombine 84
-loop-simplify 85
-indvars 89
-loop-deletion 91
-loop-unroll 92
-memcpyopt 103
-instcombine 114
-jump-threading 116
-loop-simplify 125
-lcssa 127
-simplifycfg 133
-instcombine 141
-globalopt 146
-globaldce 147
-loop-simplify 153
-lcssa 155
-loop-rotate 159
-loop-simplify 176
-instcombine 186
-simplifycfg 191
-instcombine 199
-loop-simplify 200
-lcssa 202
-loop-unroll 204
-instcombine 208
-loop-simplify 209
-lcssa 211
-strip-dead-prt 215
-globaldce 216
-constmerge 217
-loop-simplify 222
-lcssa 224
-simplifycfg 236
Compilation Configuration
Percentage time, energy,
code size vs -O2
Ns Benchmark (Improvements over -O2)
Execution Time
Energy Usage
(b) Compilation proﬁles for two of the benchmarks, using the Cortex-M3 processor
and the LLVM v5.0 compilation framework
Fig. 3: For each optimization conﬁguration tested by the proposed technique the
execution-time, energy-consumption and code-size improvements over -O2 are
given. A negative percentage represents a reduction of resource usage compared
to -O2. Each element of the horizontal axis has the name of the last ﬂag applied
and the total number of ﬂags used. The conﬁgurations are incremental subsequences of the -O2, starting from -O0 and adding optimization ﬂags till reaching
the complete -O2 set of ﬂags.
Less is More: Exploiting the Standard Compiler Optimization Levels
ments over -O2 ranging from around 1% to around 23%. For the Cortex-M3
processor, we observed an average reduction in execution time of 5.3%, with 38
out of the 71 benchmarks seeing execution time improvements over -O2 ranging
from around 1% to around 90%. The energy consumption improvements were
always closely related to the execution time improvements for both of the processors. This is expected due to the predictable nature of these deeply embedded
processors. In contrast, there were no signiﬁcant ﬂuctuations in the code size
between diﬀerent optimization conﬁgurations. We anticipate that, if the -Os or
-Oz optimization levels, which both aim to achieve smaller code size, had been
used as a starting point for our exploration, then more variation would have
been observed for code size.
As it can be seen from Figures 2a and 2b, our optimization strategy performed signiﬁcantly diﬀerent for the two processors per benchmark. This can
be caused by the diﬀerent performance and power consumption characteristics
of the two processors and/or the use of diﬀerent compiler versions in each case.
Furthermore, the technique performed better on the Cortex-M3 with the LLVM
v5.0 compilation framework. This could be due to the compilation framework
improvements from version 3.8 to version 5.0. Another possible reason might
be that the -O2 optimization level for LLVM v5.0 includes more optimization
ﬂags than the LLVM v.3.8. The more ﬂags in an optimization level, the more
optimization conﬁgurations will be generated and exercised by our exploitation
technique, and thus, more opportunities for execution-time, energy-consumption
and code-size savings can be exposed.
Figures 3a and 3b demonstrate the eﬀect of each optimization conﬁguration,
exercised by our exploitation technique, on the three resources (execution time,
energy consumption and code size), for two of the benchmarks for the Cortex-
M0 and Cortex-M3 processors, respectively. Similar ﬁgures were obtained for
all the 71 benchmarks and for both of the processors. Similarly to Figure 2, a
negative percentage represents an improvement on the resource compared to the
one achieved by -O2. The horizontal axis of the ﬁgures shows the ﬂag at which
compilation stopped together with the total number of ﬂags included up to that
point. This represents an optimization conﬁguration that is a subsequence of
the -O2. For example, the best optimization conﬁguration for all three resources
for the Levenstein benchmark (see top part of Figure 3b) is achieved when
the compilation stops at ﬂag number 91, -loop-deletion. This means that the
optimization conﬁguration includes the ﬁrst 91 ﬂags of the -O2 conﬁguration
with their original ordering preserved. The optimization conﬁgurations include
both transformation and analysis passes.
The number of optimization conﬁgurations exercised in each case depends
on the number of transformation ﬂags included in the -O2 level of the version
of the LLVM optimizer used. Note that we are only considering the documented
transformation passes [LLV18]. For example, 50 and 64 diﬀerent conﬁgurations
are being tested in the case of the Cortex-M0 processor with the LLVM compilation framework v3.8, and the case of Cortex-M3 with the LLVM framework v5.0,
respectively. Many of the transformation passes are repeated multiple times in
K. Georgiou et al.
a standard optimization level, but because of their diﬀerent ordering, they have
a diﬀerent eﬀect. Thus, we consider each repetition as an opportunity to create
a new optimization conﬁguration. Furthermore, note that more transformation
passes exist in the LLVM optimizer, but typically, these are passes that have
implicit dependencies on the documented passes. The methodology of creating a
new optimization conﬁguration explained in Section 2 ensures the preservation
of all the implicit dependencies for each conﬁguration. This is part of preserving the empirical knowledge of good interactions between transformations built
into the predeﬁned optimization levels and reusing it in the new conﬁgurations
generated.
Typically, optimization approaches based on iterative compilation are extremely time consuming [ABP+17], since thousands of iterations are needed to
reach levels of resource savings similar to the ones achieved by our approach.
In our case the maximum number of iterations we had to apply were the 64
iterations for the Cortex-M3 processor. This makes our simple and inexpensive
approach an attractive alternative, before moving to the more expensive approaches, such as iterative-compilation-based and machine-learning-based compilation techniques [AKC+18, Ash16].
By manually observing the compilation proﬁles obtained for all the benchmarks, similar to the ones demonstrated in Figure 3, no common behavior patterns were detected, except that typically there is a signiﬁcant improvement on
the execution time and the energy consumption at the third optimization con-
ﬁguration. Future work will use clustering to see if programs can be grouped
together based on their compilation proﬁles. This can be useful to identify optimization sequences that perform well for a particular type of program. Furthermore, the retrieved optimization proﬁles can also give valuable insights to
compiler engineers and software developers on the eﬀect of each optimization
ﬂag on a speciﬁc program and architecture. It is beyond the scope of this work
to investigate these eﬀects.
Related Work
Iterative compilation has been proved an eﬀective technique for tackling both
the problems of choosing the right set of transformations and for ordering them
to maximize their eﬀectiveness [ABP+17]. The technique is typically used to
iterate over diﬀerent sets of optimizations with the aim of satisfying an objective function. Usually, each iteration involves some feedback, such as proﬁling
information, to evaluate the eﬀectiveness of the tested conﬁguration. In random
iterative compilation [BKK+98], random optimization sequences are generated,
ranging from hundreds to thousands, and then used to optimize a program.
Random iterative compilation has been proved to provide signiﬁcant performance gains over standard optimization levels. Thus, it has become a standard
baseline metric for evaluating the eﬀectiveness of machine-guided compilation
approaches [FKM+11, ABP+17, BRE15], where the goal is to achieve better
performance gains with less exploration time. Due to the huge number of pos-
Less is More: Exploiting the Standard Compiler Optimization Levels
sible ﬂag combinations and their possible orderings, it is impossible to explore
a large fraction of the optimization space. To mitigate this problem, machine
learning is used to drive iterative compilation [ABC+06, OPWL17, CFA+07].
Based on either static code features [FKM+11] or proﬁling data [CFA+07],
such as performance counters, machine learning algorithms try to predict the
best set of ﬂags to apply to satisfy the objective function with as few iterations
as possible. The techniques have proven to be eﬀective in optimizing the resource
usage, mainly execution-time, of programs on a speciﬁc architecture but generally suﬀer from a number of drawbacks. Typically, these techniques require a
large training phase [OPWL17] to create their predictive models. Furthermore,
they are hardly portable across diﬀerent compilers or versions of the same compiler and diﬀerent architectures. Even if a single ﬂag is introduced to the set of a
compiler’s existing ﬂags the whole training phase has to be repeated. Moreover,
extracting some of the metrics that these techniques depend on, such as static
code features, might require a signiﬁcant amount of engineering.
A recent work that is focused on mitigating the phase-ordering problem,
[ABP+17], divided the -O3 standard optimization ﬂags of the LLVM compilation framework v3.8, into ﬁve subgroups using clustering. Then they used iterative compilation and machine learning techniques to select optimization con-
ﬁgurations by reordering the subgroups. The approach demonstrated average
performance speedup of 1.31. An interesting observation is that 79% of the -O3
optimization ﬂags were part of a single subgroup with a ﬁxed ordering that is
similar to that used in the -O3 conﬁguration. This suggests that the ordering of
ﬂags in a predeﬁned optimization level is a good starting point for further performance gains. Our results actually conﬁrm this hypothesis for the processors
under consideration.
Embedded applications typically have to meet strict timing, energy consumption, and code-size constraints [GdSE17]. Hand-written optimized code is a complex task and requires extensive knowledge of architectures. Therefore, utilizing
the compilers optimizations to achieve optimal resource usage is critical.
In an attempt to ﬁnd better optimization conﬁgurations than the ones oﬀered
by the standard optimization levels, the authors in [BRE15] applied inductive
logic programming (ILP) to predict compiler ﬂags that minimize the execution
time of software running on embedded systems. This was done by using ILP to
learn logical rules that relate eﬀective compiler ﬂags to speciﬁc program features.
For their experimental evaluation they used the GCC compiler, [GCC18], and
the Arm Cortex-M3 architecture; the same architecture used by this paper. Their
method was evaluated on 60 benchmarks selected from the BEEBS benchmark
suite; the same used in this work. They were able to achieve an average reduction
in execution time of 8%, with about half of the benchmarks seeing performance
improvements. The main drawback of their approach was the large training phase
of their predictive model. For each benchmark, they needed to create and test
1000 optimization conﬁgurations. This resulted in about a week of training time.
Furthermore, for their approach to be transferred to a new architecture, compiler
or compiler version, or even to add a new optimization ﬂag, the whole training
K. Georgiou et al.
phase has to be repeated from scratch. The same applies for applying their
approach to resources other than execution time, such as energy consumption
or code size. In contrast, our approach, for the same architecture and more
benchmarks of the same benchmark suite, was able to achieve similar savings
in execution time (average 5.3%) by only testing 65 optimization conﬁgurations
for each program. At the same time, our approach does not suﬀer from the
portability issues faced by their technique.
In [PHB15], the authors used fractional factorial design (FFD) to explore
the large optimization space (282 possible combinations for the GCC compiler
used) and determine the eﬀects of optimizations and optimization combinations.
The resources under investigation were execution time and energy consumption.
They tested their approach on ﬁve diﬀerent embedded platforms including the
Cortex-M0 and Cortex-M3, which are also used in this work. For their results
to be statistically signiﬁcant, they needed to exercise 2048 optimization conﬁgurations for each benchmark. Although they claimed that FFD was able to ﬁnd
optimization conﬁgurations that perform better than the standard optimization
levels, they demonstrated this only on a couple of benchmarks. Again, this approach suﬀers from the same portability issues as [BRE15].
In our work, to maximize the accuracy of our results, hardware measurements
were used for both the execution time and energy consumption. Although, high
accuracy is desirable, in many cases physical hardware measurements are diﬃcult to deploy and use. Existing works demonstrated that energy modeling and
estimation techniques could accurately estimate both execution time and energy
consumption for embedded architectures similar to the ones used in this paper [GKCE17, GGP+15]. Such estimation techniques can replace the physicalhardware measurements used in our approach in order to make the proposed
technique accessible to more software developers.
Conclusion
Finding optimal optimization conﬁgurations for a speciﬁc compiler, architecture,
and program is an open challenge since the introduction of compilers. Standard
optimization levels that are built-in to modern compilers, on average perform
well on a range of architectures and programs and provide convenience to the
software developer. Over the past years, iterative compilation and complex machine learning approaches have been exploited to yield optimization conﬁgurations that outperform these standard optimization levels. These techniques are
typically expensive either due to their large training phases or the large number
of conﬁgurations that they need to test. Moreover, they are hardly portable to
new architectures and compilers.
In contrast, in this work an inexpensive and easily portable approach that
generates and tests less than 64 optimization conﬁgurations proved able to
achieve execution-time and energy-consumption savings in the same range of
the ones achieved by state of the art machine learning and iterative compilation
techniques [BRE15, PHB15, AKC+18]. The eﬀectiveness of this simple approach
Less is More: Exploiting the Standard Compiler Optimization Levels
is attributed to the fact that we used subsequences of the optimization passes
deﬁned in the standard optimization levels, but stopped the optimizations at
an earlier point than the standard optimization level under exploitation. This
indicates that the accumulated empirical knowledge built into the standard optimization levels is a good starting point for creating optimization conﬁgurations
that will perform better than the standard ones.
The approach is compiler and target independent. Thus, for its validation,
two processors and two versions of the LLVM compiler framework were used;
namely, the Arm Cortex-M0 with the LLVM v3.8 and the Arm Cortex-M3 with
the LLVM v5.0. An average execution time reduction of 2.4% and 5.3% was
achieved across all the benchmarks for the Cortex-M0 and Cortex-M3 processors,
respectively, with at least half of the 71 benchmarks tested seeing performance
and energy consumption improvements. Finally, our approach can support multicriteria optimization as it targets execution time, energy consumption and code
size at the same time.
In future work, clustering and other machine learning techniques can be
applied on the compilation proﬁles retrieved by our exploitation approach (Figure 3) to ﬁne-tune the standard optimization levels of a compiler to perform
better for a speciﬁc architecture. Furthermore, the technique is currently being
evaluated on more complex architectures, such as Intel’s X-86.