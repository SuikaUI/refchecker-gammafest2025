Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 169–174
Brussels, Belgium, October 31–November 4, 2018. c⃝2018 Association for Computational Linguistics
Universal Sentence Encoder for English
Daniel Cera†, Yinfei Yanga†, Sheng-yi Konga, Nan Huaa, Nicole Limtiacob,
Rhomni St. Johna, Noah Constanta, Mario Guajardo-C´espedesa, Steve Yuanc,
Chris Tara, Yun-Hsuan Sunga, Brian Stropea, Ray Kurzweila
aGoogle AI
Mountain View, CA
bGoogle AI
New York, NY
Cambridge, MA
We present easy-to-use TensorFlow Hub
sentence embedding models having good
task transfer performance.
Model variants allow for trade-offs between accuracy
and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that
incorporate word-level transfer. Transfer
learning using sentence-level embeddings
is shown to outperform models without
transfer learning and often those that use
only word-level transfer. We show good
transfer task performance with minimal
training data and obtain encouraging results on word embedding association tests
(WEAT) of model bias.
Introduction
We present easy-to-use sentence-level embedding models with good transfer task performance
even when using remarkably little training data.1
Model engineering characteristics allow for tradeoffs between accuracy versus memory and compute resource consumption.
Model Toolkit
Models are implemented in TensorFlow and are made publicly available on
TensorFlow Hub.2 Listing 1 provides an example
† Corresponding authors:
{cer, yinfeiy}@google.com
1We describe our publicly released models.
et al. and Henderson et al. for additional architectural details of models similar to those presented here.
2 Apache
2.0 license, with models available as saved TF graphs.
import tensorflow_hub as hub
embed = hub.Module(" "
"universal-sentence-encoder/2")
embedding = embed(["Hello World!"])
Listing 1: Python sentence embedding code.
code snippet to compute a sentence-level embedding from a raw untokenized input string.3 The resulting embedding can be used directly or incorporated into a downstream model for a speciﬁc task.4
Two sentence encoding models are provided: (i)
transformer , which achieves
high accuracy at the cost of greater resource consumption; (ii) deep averaging network (DAN)
 , which performs efﬁcient inference but with reduced accuracy.
Transformer
The transformer sentence encoding model constructs sentence embeddings using the encoding sub-graph of the transformer architecture
 . The encoder uses attention to compute context aware representations of
words in a sentence that take into account both the
ordering and identity of other words. The context
aware word representations are averaged together
to obtain a sentence-level embedding.
We train for broad coverage using multi-task
learning, with the same encoding model supporting multiple downstream tasks.
The task types
include: a Skip-Thought like task ;5 conversational response prediction ; and a select supervised classi-
ﬁcation task that improves sentence embeddings.6
The transformer encoder achieves the best transfer
performance. However, this comes at the cost of
compute time and memory usage scaling dramatically with sentence length.
Deep Averaging Network (DAN)
The DAN sentence encoding model begins by
averaging together word and bi-gram level embeddings. Sentence embeddings are then obtain
by passing the averaged representation through
a feedforward deep neural network (DNN). The
DAN encoder is trained similar to the transformer
encoder. Multitask learning trains a single DAN
encoder to support multiple downstream tasks. An
advantage of the DAN encoder is that compute
time is linear in the length of the input sequence.
Similar to Iyyer et al. , our results demonstrate that DANs achieve strong baseline performance on text classiﬁcation tasks.
Encoder Training Data
Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia,
web news, web question-answer pages and discussion forums. We augment unsupervised learning
with training on supervised data from the Stanford
Natural Language Inference (SNLI) corpus in order to further improve our
representations . Since the
only supervised training data is SNLI, the models
can be used for a wide range of downstream supervised tasks that do not overlap with this dataset.7
Transfer Tasks
This section presents the data used for the transfer
learning experiments and word embedding association tests (WEAT): (MR) Movie review sentiment on a ﬁve star scale ;
(CR) Sentiment of customer reviews ; (SUBJ) Subjectivity of movie reviews and plot summaries ;
5The Skip-Thought like task replaces the LSTM in the original formulation with
a transformer model.
6SNLI 
7For questions on downstream evaluations possibly overlapping with the encoder training data, visit the TF Hub
discussion board, 
tensorflow.org/d/forum/hub, or e-mail the corresponding authors.
Table 1: Transfer task evaluation sets.
(MPQA) Phrase opinion polarity from news data
 ; (TREC) Fine grained question classiﬁcation sourced from TREC ; (SST) Binary phrase sentiment classiﬁcation ; (STS Benchmark)
Semantic textual similarity (STS) between sentence pairs scored by Pearson r with human judgments ; (WEAT) Word pairs from
the psychology literature on implicit association
tests (IAT) that are used to characterize model bias
 .8 Table 1 gives the number
of samples for each transfer task.
Transfer Learning Models
For sentence classiﬁcation transfer tasks, the output of the sentence encoders are provided to a task
speciﬁc DNN. For the pairwise semantic similarity task, the similarity of sentence embeddings u
and v is assessed using −arccos
||u|| ||v||
For each transfer task, we include baselines that
only make use of word-level transfer and baselines
that make use of no transfer learning at all. For
word-level transfer, we incorporate word embeddings from a word2vec skip-gram model trained
on a corpus of news data .
The pretrained word embeddings are included as
input to two model types: a convolutional neural
network model (CNN) ; a DAN. The
baselines that use pretrained word embeddings allow us to contrast word- vs. sentence-level transfer. Additional baseline CNN and DAN models
are trained without using any pretrained word or
sentence embeddings.
For reference, we compare with InferSent and
8For MR, CR, SUBJ, SST, and TREC we use the preparation of the data provided by Conneau et al. .
9arccos converts cosine similarity into an angular distance
that obeys the triangle inequality. We ﬁnd that angular distance performs better on STS than cosine similarity.
Skip-Thought with layer normalization on sentence-classiﬁcation tasks. On the STS
Benchmark, we compare with InferSent and the
state-of-the-art neural STS systems CNN (HCTI)
 and gConv .
Combined Transfer Models
We explore combining the sentence and wordlevel transfer models by concatenating their representations prior to the classiﬁcation layers. For
completeness, we report results providing the classiﬁcation layers with the concatenating of the
sentence-level embeddings and the representations
produced by baseline models that do not make use
of word-level transfer learning.
Experiments
Experiments use our most recent transformer and
DAN encoding models.10 Transfer task model hyperparamaters are tuned using a combination of
Vizier and light manual tuning. When available, model hyperparameters are
tuned using task dev sets. Otherwise, hyperparameters are tuned by cross-validation on task training data or the evaluation test data when neither
training nor dev data are provided. Training repeats ten times for each task with randomly initialized weights and we report results by averaging
across runs. Transfer learning is important when
training data is limited. We explore using varying amounts of training data for SST. Contrasting
the transformer and DAN encoders demonstrates
trade-offs in model complexity and the training
data required to reach a desired level of task accuracy. Finally, to assess bias in our encoders, we
evaluate the strength of biased model associations
on WEAT. We compare to Caliskan et al. 
who discovered that word embeddings reproduce
human-like biases on implicit association tasks.
Table 2 presents results on classiﬁcation tasks. Using transformer sentence-level embeddings alone
outperforms InferSent on MR, SUBJ, and TREC.
The transformer sentence encoder also strictly outperforms the DAN encoder. Models that make use
of just the transformer sentence-level embeddings
tend to outperform all models that only use wordlevel transfer, with the exception of TREC and
10universal-sentence-encoder/2
universalsentence-encoder-large/3 (Transformer).
SUBJ MPQA TREC
Sentence Embedding Transfer Learning
Word Embedding Transfer Learning
Sentence Embedding Transfer Learning
+ DNN/CNN with word-level transfer
UT +CNNw2v
UT +DANw2v
Sentence Embedding Transfer Learning
+ DNN/CNN without word-level transfer
UT +CNNrnd
UT +DANrnd
Baselines with No Transfer Learning
Prior Work
Skip Thght
Table 2: Classiﬁcation tasks. UT uses the transformer encoder for transfer learning, while UD
uses the DAN encoder. DAN/CNNw2v use pretrained w2v emb. DAN/CNNrnd train rand. init.
word emb. on the ﬁnal classiﬁcation task.
SST, on which CNNw2v performs better. Transfer learning with DAN sentence embeddings tends
to outperform a DAN with word-level transfer, except on MR and SST. Models with sentence- and
word-level transfer often outperform similar models with sentence-level transfer alone.
Transformer Encoder
DAN Encoder
Prior Work
gConv 
CNN (HCTI) 
InferSent 
Table 3: STS Benchmark Pearson’s r. Our prior
gConv model is a variant of
our TF Hub transformer model tuned to STS.
Table 3 compares our models to strong baselines on the STS Benchmark. Our transformer embeddings outperform the sentence representations
produced by InferSent. Moreover, computing similarity scores by directly comparing the representations produced by our encoders approaches
the performance of state-of-the-art neural models
whose representations are ﬁt to the STS task.
Table 4 illustrates transfer task performance for
varying amounts of training data.
With small
quantities of training data, sentence-level transfer achieves surprisingly good performance. Using only 1k labeled examples and the transformer
embeddings for sentence-level transfer surpasses
the performance of transfer learning using InferSent on the full training set of 67.3k examples. Training with 1k labeled examples and the
transformer sentence embeddings surpasses wordlevel transfer using the full training set, CNNw2v,
and approaches the performance of the best model
without transfer learning trained on the complete
dataset, . Transfer learning is not
always helpful when there is enough task training
data. However, we observe that our best performing model still makes use of transformer sentencelevel transfer but combined with a CNN with no
word-level transfer, UT +CNNrnd.
Table 5 contrasts Caliskan et al. ’s ﬁndings on bias within GloVe embeddings with results
from the transformer and DAN encoders. Similar
to GloVe, our models reproduce human associations between ﬂowers vs. insects and pleasantness
vs. unpleasantness. However, our models demonstrate weaker associations than GloVe for probes
targeted at revealing ageism, racism and sexism.11
Differences in word association patterns can be attributed to training data composition and the mixture of tasks used to train the representations.
Resource Usage
This section describes memory and compute resource usage for the transformer and DAN sentence encoding models over different batch sizes
and sentence lengths.
Figure 1 plots model resource consumption against sentence length.12
Compute Usage
The transformer model time
complexity is O(n2) in sentence length, while the
11The development of our models did not target reducing
bias. Researchers and developers are strongly encouraged to
independently verify whether biases in their overall model
or model components impacts their use case. For resources
on ML fairness visit 
12 All benchmark values are averaged over 25 runs that
follow 5 priming runs. CPU and mem. benchmarks are performed on a machine with an Intel(R) Xeon(R) Platinum
P-8136 CPU @ 2.00GHz CPU. GPU benchmarks use an
Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz CPU and
NVIDIA Tesla P100 GPU.
SST 1K SST 4K SST 16K SST 67.3K
Sentence Embedding Transfer Learning
Word Embedding Transfer Learning
Sentence Embedding Transfer Learning
+ DNN/CNN with word-level transfer
Sentence Embedding Transfer Learning
+ DNN/CNN without word-level transfer
Baselines with No Transfer Learning
Prior Work
Table 4: SST performance varying the amount of
training data. Model types are the same as Table
2. Using 1k examples, UT transfer learning rivals
models trained on the full training set, 67.3k.
DAN model is O(n).
As seen in Figure 1 (ab), for short sentences, the transformer encoding
model is only moderately slower than the much
simpler DAN model.
However, compute time
for transformer increases noticeably with sentence
In contrast, the compute time for the
DAN model stays nearly constant across different
lengths. When running on GPU, even for large
batches and longer sentence lengths, the transformer model still achieves performance that can
be used within an interactive systems.
Memory Usage
The transformer model space
complexity also scales quadratically, O(n2), in
sentence length, while the DAN is linear, O(n).
Similar to compute usage, memory for the transformer model increases quickly with sentence
length, while the memory for the DAN model remains nearly constant. For the DAN model, memory is dominated by the parameters used to store
the model unigram and bigram embeddings. Since
the transformer model only stores unigrams, for
(a) CPU Time vs. Sentence Length
(b) GPU Time vs. Sentence Length
(c) Memory vs. Sentence Length
Figure 1: Resource usage for the Universal Sentence Encoder DAN (USE-D) and Transformer (USE-T)
models for different batch sizes and sentence lengths.
Target words
Attrib. words
U. Enc. DAN
U. Enc. Trans.
Eur.- vs. Afr.-American names
Pleasant vs. Unpleasant
Eur.- vs. Afr.-American names
Pleasant vs.Unpleasant from (a)
Eur.- vs. Afr.-American names
Pleasant vs. Unpleasant from (c)
Male vs. female names
Career vs. family
Math vs. arts
Male vs. female terms
Science vs. arts
Male vs. female terms
Mental vs. physical disease
Temporary vs. permanent
Young vs old peoples names
Pleasant vs unpleasant
Flowers vs. Insects
Pleasant vs. Unpleasant
Instruments vs. Weapons
Pleasant vs Unpleasant
Table 5: WEAT for GloVe vs. our DAN and transformer encoding models. Effect size is reported as
Cohen’s d over the mean cosine similarity scores across grouped attribute words. Statistical signiﬁcance
uses one-tailed p-scores. The Ref column indicates the source of the IAT word lists: (a) Greenwald
et al. (b) Bertrand and Mullainathan (c) Nosek et al. (d) Nosek et al. (e)
Monteith and Pettit .
very short sequences transformer requires almost
half as much memory as the DAN model.
Conclusion
Our encoding models provide sentence-level embeddings that demonstrate strong transfer performance on a number of NLP tasks. The encoding
models make different trade-offs regarding accuracy and model complexity that should be considered when choosing the best one for a particular
application. Overall, our sentence-level embeddings tend to surpass the performance of transfer using word-level embeddings alone. Models
that make use of sentence- and word-level transfer often achieve the best performance. Sentencelevel transfer using our models can be exceptionally helpful when limited training data is available. The pre-trained encoding models are publicly available for research and use in industry
applications that can beneﬁt from a better understanding of natural language.
Acknowledgments
We thank our teammates from Descartes, Ai.h and
other Google groups for their feedback and suggestions. Special thanks goes to Ben Packer and
Yoni Halpern for implementing the WEAT assessments and discussions on model bias.