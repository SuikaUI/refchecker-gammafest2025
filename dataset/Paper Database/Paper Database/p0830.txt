A Test for Superior Predictive Ability
Peter Reinhard HANSEN
Stanford University, Department of Economics, 579 Serra Mall, Stanford, CA 94305 ( )
We propose a new test for superior predictive ability. The new test compares favorably to the reality check
(RC) for data snooping, because it is more powerful and less sensitive to poor and irrelevant alternatives.
The improvements are achieved by two modiﬁcations of the RC. We use a studentized test statistic that
reduces the inﬂuence of erratic forecasts and invoke a sample-dependent null distribution. The advantages
of the new test are conﬁrmed by Monte Carlo experiments and an empirical exercise in which we compare
a large number of regression-based forecasts of annual U.S. inﬂation to a simple random-walk forecast.
The random-walk forecast is found to be inferior to regression-based forecasts and, interestingly, the best
sample performance is achieved by models that have a Phillips curve structure.
KEY WORDS:
Forecast evaluation; Forecasting; Inequality testing; Multiple comparison; Testing for
superior predictive ability.
INTRODUCTION
Testing whether a particular forecasting procedure is outperformed by alternative forecasts represents a test of superior predictive ability (SPA). White developed a framework for
comparing multiple forecasting models and proposed a test for
SPA that is known as the reality check (RC) for data snooping.
Here the term “model” is used in a broad sense that includes
forecasting rules/methods, which need not involve modeling
data. In White’s framework, m alternative forecasts (where
m is a ﬁxed number) are compared with a benchmark forecast,
where the predictive abilities are deﬁned by expected loss. The
complexity of this inference problem arises from the need to
control for the full set of alternatives.
In this article, we propose a new test for SPA. Our framework
is identical to that of White , but we take a different path
in our construction of the test. To be speciﬁc, we use a different
test statistic and invoke a sample-dependent distribution under
the null hypothesis. Compared with the RC, the new test is more
powerful and less sensitive to the inclusion of poor and irrelevant alternatives.
We make three contributions in this article. First, we provide a theoretical analysis of the testing problem and expose
some of its important aspects. Our theoretical results reveal that
the RC can be manipulated by the including poor and irrelevant forecasts in the set of alternative forecasts. This problem
is alleviated by studentizing the test statistic and by invoking
a sample-dependent null distribution. The latter is based on a
novel procedure that incorporates additional sample information to identify the “relevant” alternatives. Second, we provide
a detailed explanation of a bootstrap implementation of our
test for SPA. Third, we apply the tests in an empirical analysis of U.S. inﬂation. Our benchmark is a simple random-walk
forecast that uses current inﬂation as the prediction of future
inﬂation. The benchmark is compared with a large number of
regression-based forecasts, and our empirical results show that
the benchmark is signiﬁcantly outperformed. Interestingly, the
strongest evidence is provided by regression models that have a
Phillips curve structure.
When testing for SPA, the question of interest is whether
any alternative forecast is better than the benchmark forecast
or, equivalently, whether the best alternative forecasting model
is better than the benchmark. This question can be addressed by
testing the null hypothesis that “the benchmark is not inferior to
any alternative forecast.” This testing problem is relevant for applied econometrics, because several ideas and speciﬁcations are
often used before a model is selected. This mining over alternative forecasts may be exacerbated if more than one researcher is
searching for a good forecasting model. Testing for SPA is useful for a
forecaster who wants to explore whether a better forecasting
model than the model currently being used to make predictions
is available. After a search over several alternative forecasts, the
relevant question is whether one of the alternative forecasts is
signiﬁcantly more accurate than the benchmark. 
Tests for equal predictive ability (EPA) in a general setting were proposed by Diebold and Mariano and West
 , where the framework of West can accommodate the situation where forecasts involve estimated parameters. Harvey,
Leybourne, and Newbold suggested a modiﬁcation of
the Diebold–Mariano test that leads to better small-sample
properties. A test for comparing multiple-nested models was
given by Harvey and Newbold , and McCracken 
derived results for the case with estimated parameters and nondifferentiable loss functions, such as the mean absolute deviation loss function. West and McCracken developed
regression-based tests, and other extensions were made by
Harvey et al. , Chao, Corradi, and Swanson , Clark
and McCracken , West , and Corradi and Swanson
 , who considered tests for forecast encompassing, and by
Corradi, Swanson, and Olivetti , who compared forecasting models that include cointegrated variables. 
Whereas the frameworks of Diebold and Mariano 
and West involve tests for EPA, the testing problem in
White’s framework is a test for SPA. The distinction is important because the former leads to a simple null hypothesis,
whereas the latter leads to a composite hypothesis. One of the
© 2005 American Statistical Association
Journal of Business & Economic Statistics
October 2005, Vol. 23, No. 4
DOI 10.1198/073500105000000063
main complications in composite hypotheses testing is that (asymptotic) distributions typically depend on nuisance parameters, such that the null distribution is not unique. The usual way
to handle this ambiguity is to use the least favorable conﬁguration (LFC), which is sometimes referred to as “the point
least favorable to the alternative.” Our analysis shows that the
LFC-based approach leads to some rather unfortunate properties when testing for SPA. The following situation delivers
key insight to the advantages of using a sample-dependent null
distribution. Let pmin denote the smallest p value of the m pairwise comparisons (comparing each alternative with the benchmark); then the Bonferroni bound test (at level α) rejects the
null hypothesis if pmin < α/m. It is now evident that the power
of this test can be driven to 0 by adding poor and irrelevant alternatives to the comparison, because this increases m but does
not affect pmin. However, sample information will (at least asymptotically) identify the poor and irrelevant alternative, which
allows us to use a smaller denominator when deﬁning the critical value, for example, α/m0 for some m0 ≤m. Although our
testing procedure is quite different from the conservative Bonferroni bound test, our sample-dependent null distribution is
similar to this improvement of the Bonferroni bound test, although the (presumed) poor alternatives are not discarded entirely in our framework.
In relation to the existing literature on forecast evaluation and
comparison, it is important to acknowledge a limitation of the
speciﬁc test that we propose in this article. A comparison of
models with parameters that are estimated recursively is not accommodated by our framework, because this situation violates
our stationarity assumption. 
However, our framework does permit parameters that are estimated once (ﬁxed scheme) or with a moving window (rolling
schemes), as we discuss in Section 2. The advantages of the
studentized test statistic and our sample-dependent null distribution do not rely on stationarity, so these modiﬁcations are
expected to be useful in a more general context. A related issue
concerns the optimality of our test. Although the new test dominates the RC, we do not claim that it is optimal. The lack of an
optimality result is not surprising, because such results are rare
in composite hypothesis testing. It is also worth observing that
leading statisticians continue to quarrel about what constitutes
a suitable test in this context , where ξt is a random variable
that represents the aspects of the decision problem that are unknown at the time that the decision is made. We evaluate forecasts in terms of their expected loss, E[L(ξt,δk,t−h)]. Thus we
need not assume that any of the forecasts are constructed from
a correctly speciﬁed model. Whenever δk,t−h = δk,t−h( ˆθk,t−h)
is based on estimated parameters, ˆθk,t−h, these are likely to in-
ﬂuence the expected loss—typically by increasing the expected
loss. We make assumptions that do not permit parameters that
are estimated with the recursive scheme. However, the rolling
scheme is accommodated by our framework, and so is the ﬁxed
scheme when the comparison of forecasts is interpreted as being conditional on the estimated parameters. An overview of
our notation is given in Table 1. This provides a general framework for comparing forecasts and decision rules. Our leading
example is the comparison of forecasts, so we often refer to
δk,t−h as the kth forecasting model. The ﬁrst model, k = 0, has
a special role and is referred to as the benchmark. The decision
rule, δk,t−h, can represent a point forecast, an interval forecast,
a density forecasts, or a trading rule for an investor, as we illustrate next with some examples.
Example 1 (Point forecast). Let δk,t−h, k = 0,1,...,m, be
different point forecasts of a real random variable ξt. The mean
squared error loss function, L(ξt,δk,t−h) = (ξt −δk,t−h)2, is an
example of a loss function that could be used to compare the
different forecasts.
Example 2 (Conditional distribution and value-at-risk forecasts). Let ξt be a conditional density on R, and let δk,t−h
be a forecast of ξt. Then we might evaluate the precision
of δk by the Kolmogorov–Smirnov statistic, L(ξt,δk,t−h) =
−∞[ξt(y) −δk,t−h(y)]dy|, or a Kullback–Leibler
L(ξt,δk,t−h)
−∞log[δk,t−h(x)/ξt(x)]ξt(x)dx.
Alternatively, δk,t−h could be a value-at-risk measure (at
quantile α) that may be evaluated with L(ξt,δk,t−h) =
ξt(x)dx −α|.
In Example 2, ξt will often be unobserved, which creates
additional complications for empirical evaluation and comparison. When a proxy is substituted for ξt it can cause the empirical
ranking of alternatives to be inconsistent for the intended (true)
ranking . Corradi and Swanson recently derived an RC-type test for comparing
conditional density forecasts, which is closely related to the
problem of Example 2. Their test is similar to that of White
 , because their test statistic is also the maximum of multiple nonstudentized quantities. So it would be interesting to
analyze whether our two modiﬁcations can be implemented in
their framework.
Example 3 (Trading rules). Let δk,t−1 be a binary variable
that instructs a trader to take either a short (δ = −1) or a long
(δ = 1) position in an asset at time t −1. The kth trading rule
yields the proﬁt πk,t = δk,t−1rt, where rt is the return on the
asset in period t. A trader who is currently using the rule, δ0,
might be interested to know whether an alternative rule has a
larger expected proﬁt than δ0. This can be formulated in our
framework by setting ξt = rt and L(ξt,δk,t−1) = −δk,t−1ξt.
Table 1. Overview of Notation and Deﬁnitions
t = 1, . . . , n
Sample period for the model comparison
k = 0, 1, . . . , m
Model index (k = 0 is the benchmark)
Object (variable) of interest
The kth decision rule (e.g., h-step-ahead forecast of ξt)
Lk,t ≡L(ξt, δk,t−h)
Observed loss of the kth decision rule/forecast
dk,t ≡L0,t −Lk,t
Performance of model k relative to the benchmark
¯dk ≡n−1 n
Average relative performance of model k
dt ≡(d1,t , . . . , dm,t)′
Vector of relative performances at time t
¯d ≡n−1 n
Vector of average relative performance
µk ≡E(dk,t)
Expected excess performance of model k
µ ≡(µ1, . . . , µm)′
Vector of expected excess performances
 ≡avar(n1/2 ¯d)
Asymptotic m × m covariance matrix
The benchmark in Example 3 could be δ0,t = 1, which is the
rule that is always “long in the market.” This was the benchmark used by Sullivan et al. who evaluated the
signiﬁcance of technical trading rules and calendar effects in
stock returns.
Hypothesis of Interest
We are interested to know whether any of the models,
k = 1,...,m, are better than the benchmark in terms of expected loss. So we seek a test of the null hypothesis that the
benchmark is not inferior to any of the alternatives. The variables that are key for our analysis are the relative performance
variables, which are deﬁned by
dk,t ≡L(ξt,δ0,t−h) −L(ξt,δk,t−h),
k = 1,...,m.
So dk,t denotes the performance of model k relative to the
benchmark at time t, and we stack these variables into the vector
of relative performances, dt = (d1,t,...,dm,t)′. Provided that
µ ≡E(dt) is well deﬁned, we can now formulate the null hypothesis of interest as
and our maintained hypothesis is µ ∈Rm.
We work under the assumption that model k is better than the
benchmark if and only if E(dk,t) > 0. So we focus exclusively
on the properties of dt and abstract entirely from all aspects
that relate to the construction of the δ-variables. Thus dt, t =
1,...,n, is de facto viewed as our data, and we therefore state
all assumptions in terms dt. Speciﬁcally we make the following
assumption.
Assumption 1. The vector of relative loss variables, {dt},
is (strictly) stationary and α-mixing of size −(2 + δ)(r + δ)/
(r −2), for some r > 2 and δ > 0, where E|dt|r+δ < ∞and
var(dk,t) > 0 for all k = 1,...,m.
Assumption 1 is made for two reasons: ﬁrst, to ensure that
certain population moments such as µ are well deﬁned, and
second, to justify the use of bootstrap techniques that we describe in detail in Section 3. Note that Assumption 1 does not
require that the individual loss variables, L(ξt,δk,t−h), be stationary. An immediate consequence of Assumption 1 is that a
central limit theorem applies, such that
n1/2(¯d −µ)
d→Nm(0,),
where ¯d ≡n−1 n
t=1 dt and  ≡avar(n1/2(¯d −µ)) .
Diebold and Mariano and West provided suf-
ﬁcient conditions that also lead to the asymptotic normality
in (2). Giacomini and White established this property
for a related testing problem. However, the asymptotic normality does not hold in general. An important exception is the situation where the benchmark is nested in all alternative models
(under the null hypothesis) and the parameters are estimated
recursively. In this situation the limiting distribution will typically be given as a function of Brownian motions . When comparing nested models,
the null hypothesis simpliﬁes to the simple hypothesis, µ = 0.
So in this case it seems more appropriate to apply a test for EPA,
such as that of Harvey and Newbold , which can be used
to compare multiple-nested models.
At this point, all essential aspects of our framework are identical to those of White . White proceeded by constructing
the RC from the test statistic,
n1/2¯d1,...,n1/2¯dm
and an asymptotic null distribution based on n1/2 ¯d ∼Nm(0, ˆ),
where ˆ is a consistent estimator of . Here it is worth noting
that the RC relies on an asymptotic null distribution that assumes µk = 0 for all k, even though all negative values of µk
also conform with the null hypothesis. This aspect is the underlying topic of Sections 2.3 and 2.4, but ﬁrst we discuss a
studentization of the test statistic.
Given the asymptotic normality of ¯d, it may seem natural to
use a quadratic-form test statistic to test H0, such as the likelihood ratio test used by Wolak . However, the situation
that we have in mind is one in which m is too large to obtain a sensible estimate of all elements of . Instead we consider simpler statistics, such as TSPA
(deﬁned later) that requires
only that the diagonal elements of  be estimated. It is not
surprising that nonquadratic statistics will be nonpivotal—even
asymptotically—because their asymptotic distribution will depend on (some elements of ) the covariance matrix, which
makes  a nuisance parameter. To handle this problem, we follow White and use a bootstrap method that implicitly
takes care of this nuisance parameter problem. So our motivation for using the bootstrap is not driven by higher-order reﬁnements, but is merely to handle this nuisance parameter problem.
We analyze this testing problem in the remainder of this section, and our ﬁndings motivate the following two recommendations that spell out the differences between the RC and our new
test for SPA:
1. Use the studentized test statistic,
k is some consistent estimator of ω2
k ≡var(n1/2¯dk).
2. Invoke a null distribution that is based on Nm( ˆµc, ˆ),
where ˆµc is a carefully chosen estimator for µ that conforms with the null hypothesis. Speciﬁcally, we suggest
the estimator
k = ¯dk1{n1/2 ¯dk/ ˆωk≤−√2loglogn},
k = 1,...,m,
where 1{·} denotes the indicator function.
We explain our reasons for this choice of µ-estimator in Section 2.4, but it is important to understand that using a consistent
estimator of µ need not produce a valid test.
Choice of Test Statistic
When the benchmark has the best sample performance
(¯d ≤0), the test statistic is normalized to 0. In this case there
is no evidence against the null hypothesis, and consequently
the null should not be rejected. The normalization is convenient
for theoretical reasons, because we avoid a divergence problem
(to −∞) that would otherwise occur when µ < 0.
As discussed in Section 1, there are few optimality results in
the context of composite hypothesis testing. This is particularly
the case for the present problem of testing multiple inequalities.
However, some arguments that justify our choice of test statistic TSPA
(instead of TRC
n ) are called on. Although we argue
is preferable to TRC
n , it cannot be shown that the former uniformly dominates the latter in terms of power. In fact,
there are situations where TRC
leads to a more powerful test
(such as the case where ω2
k ∀j,k = 1,...,m). However,
such exceptions are unlikely to be of much empirical relevance,
as we discuss later. So we are comfortable recommending the
use of TSPA
in practice, and it is worth pointing out that studentization of the individual statistics is the conventional approach to multiple comparisons .
This studentization is also embedded in the related approach
where the individual statistics are converted into “p values,”
with the smallest p value used as the test statistic . In the present context, Romano and
Wolf also adopted the studentized test statistic .
Our main argument for studentization is that it typically will
improve the power. This can be understood from the following
simple example.
Example 4. Consider the case where m = 2 and suppose that
n1/2(¯d −µ) ∼N2
where the covariance is 0 (a simpliﬁcation that is not necessary for our argument). Now consider the particular local
alternative where µ2 = 2n−1/2 > 0. Here ¯d2 is expected to
yield a fair amount of evidence against H0 :µ ≤0, because
the t-statistic, n1/2¯d2/ ˆωk, will be centered about 2. It follows that the null distributions (using µ = 0) are given by
∼F0(x) ≡(x/2)(x) and TSPA
a∼G0(x) ≡(x)(x),
whereas TRC
∼F1(x) ≡(x/2)(x + 2) and TSPA
(x)(x+2) under the local alternative. Here (·) denotes the
standard Gaussian distribution and
a∼means “asymptotically
distributed as.” Figure 1 shows the upper tails of the null distributions, 1 −F0(x) and 1 −G0(x) (thick lines) and the upper tails
of 1 −F1(x) and 1 −G1(x) (thin lines) that represent the distributions of the test statistics under the local alternative. Dotted
Figure 1. (One minus) The cdfs for the Test Statistics T RC and TSPA Under the Null Hypothesis, µ1 = µ2 = 0, and the Local Alternative,
µ2 = 2/√n > 0. [
1−G1(x).] Studentization improves the power from about 15% to about 53%.
lines represent for the distributions of TRC
n , and solid lines represent for the distributions of TSPA
. The power for a given level
of either of the two tests can be read off the ﬁgure, and we have
singled out the powers of the 5%-level tests. These reveal that
studentization more than triples the power, from about 15% to
about 53%. So the RC is much less likely to detect the false null,
because the noisy ¯d1 conceals the evidence against H0 that ¯d2
The preceding example highlights the advantages of studentizing the individual statistics, because it avoids a comparison
of objects measured in different “units of standard deviation”
(avoiding a comparison of apples and bananas). There is one
exception where studentization may reduce the power, when
the best performing model is associated with the largest variance [i.e., if var(¯d2) ≥var(¯d1) in the previous example]. We
consider this case to be of little empirical relevance, because
poorly performing models also tend to have the most erratic
performances in practice. Moreover, the loss in power from estimating ω2
k, k = 1,...,m, is quite modest when these are estimated precisely, as is the case when n is large.
In the remainder of this section we formulate our theoretical
results that motivate our data-dependent choice of null distribution. We derive our results for a broad class of test statistics
to emphasize that our results are not speciﬁc to the two statistics, TRC
. This is also convenient because other statistics (from this class of statistics) may be used in future applied
Theoretical Results for a Class of Test Statistics
We consider a class of test statistics, where each of the statistics satisﬁes the following conditions.
Assumption 2. The test statistic has the form Tn = ϕ(Un,Vn),
where Un ≡n1/2 ¯d and Vn
p→v0 ∈Rq (a constant). The mapping, ϕ(u,v), is continuous in u on Rm and continuous in v in
a neighborhood of v0. Further, ϕ has the following properties:
(a) ϕ(u,v) ≥0 and ϕ(0,v) = 0.
(b) ϕ(u,v) = ϕ(u+,v), where u+
= max(0,uk), k =
(c) ϕ(u,v) →∞, if uk →∞for some k = 1,...,m.
Thus, in addition to the sample average, ¯d, the test statistic
may depend on the data through Vn ≡v(d1,...,dn), as long
as Vn converges in probability to a constant (or vector of constants). Assumption 2(a) is a normalization (if ¯d = 0, then there
is no evidence against H0), Assumption 2(b) states that only the
positive elements of u matter for the value of the test statistic,
and Assumption 2(c) requires that the test statistic diverges to
inﬁnity as the evidence against the null hypothesis increases (to
The mapping (µ,)
→0, given by
ij ≡ ij1{µi=µj=0},
i,j = 1,...,m,
deﬁnes an m × m covariance matrix, 0, that plays a role in
our asymptotic results. So 0 is similar to , except that the
elements of certain rows and columns have been set to 0. An
example of how µ and  translate into 0 is as follows:
11 ω12 ω13
ω31 ω32 ω2
and 0 has at most rank m0, where m0 is the number of elements in µ that equal 0.
The following theorem provides the asymptotic null distribution for all test statistics that satisfy Assumption 2.
Theorem 1. Suppose that Assumptions 1 and 2 hold and let
F0 be the cumulative distribution function (cdf ) of ϕ(Z,v0),
where Z ∼Nm(0,0). Under the null hypothesis, µ ≤0, we
have that ϕ(n1/2 ¯d,Vn)
d→F0, where v0 = plimVn. Under the
alternative, µ ≰0, we have that ϕ(n1/2 ¯d,Vn)
The test statistic TSPA
satisﬁes Assumption 2, whereas that
of the RC does not. It is nevertheless possible to obtain critical
values for TRC
from Theorem 1. This is done by applying Theorem 1 to the test statistic TRC+
n ,0) that satisﬁes Assumption 2 and noting that the distributions of TRC+
coincide on the positive axis, which is the relevant support
for the critical value. Alternatively, the asymptotic distribution
can be obtained directly, as we do in the following corollary.
Corollary 1. Let m0 ≤m be the number of models with
µk = 0, deﬁne  to be the m0 × m0 submatrix of  that contains the (i,j)th element of  if µi = µj = 0, and let ζ denote the distribution of Zmax ≡maxj=1,...,m0 Z0
j , where Z0 =
m0)′ ∼Nm0(0,). Then TRC
d→ζ if maxk µk = 0,
whereas TRC
p→−∞if µk < 0 for all k = 1,...,m. Under the
alternative where µk > 0 for some k, it holds that TRC
Theorem 1 and Corollary 1 demonstrate that it is only the
binding constraints (i.e., those with µk = 0) that matter for
the asymptotic distribution. Naturally, the number of binding
constraints can be small relative to the number of inequalities, m, being tested. This result is known from the problem
of testing linear inequalities in linear (regression) models . The testing problem is also related
to that of Gouriéroux, Holly, and Monfort , King and
Smith , and Andrews , where the alternative is
constrained by inequalities. 
An immediate consequence of Corollary 1 is that the RC
is easy to manipulate by including irrelevant alternative models. This follows because the RC’s p value, which is based
on max(Z1,...,Zm), can be increased in an artiﬁcial way by
adding poor forecasts to the set of alternative forecasts (i.e., by
increasing m while m0 remains constant). In other words, it is
possible to erode the power of the RC to 0 by including poor
Figure 2. A Situation Where the RC Fails to Reject a False Null Hypothesis. The true parameter value is µ = (µ1, µ2)′, the sample estimate is ¯d = ( ¯d1, ¯d2)′, and CRC represents the critical value derived
from a null distribution that tacitly assumes that µ = (0, 0) ′.
alternatives in the analysis. Naturally, we would want to avoid
such properties to the extent possible.
Because the test statistics have asymptotic distributions that
depend on µ and , these are nuisance parameters. The traditional way to proceed in this case is to substitute a consistent
estimator for  and use the LFC over the values of µ that satisfy the null hypothesis. In the present situation, the point least
favorable to the alternative is µ = 0, which presumes that all alternatives are as good as the benchmark. In the next section we
explore an alternative way to handle the nuisance dependence
on µ, where we use a data-dependent choice for µ rather than
µ = 0 as dictated by the LFC.
Figure 2 illustrates a situation for m = 2, where the twodimensional plane represents the sampling space for ¯d =
(¯d1, ¯d2)′. We have plotted a realization of ¯d, that is in the neighborhood of its true expected value, µ = (µ1,µ2)′, and the ellipse around µ is meant to illustrate the covariance structure
of ¯d. The shaded area represents the values of µ that conform
with the null hypothesis. Because we have placed µ outside
this shaded area, the situation in Figure 2 is one where the null
hypothesis is false. The RC is an LFC-based test, so it derives
critical values as if µ = 0 [the origin, o = (0,0)′, of the ﬁgure].
The critical value, CRC, is represented by the dashed line, such
that the area above and to the right of the dashed line deﬁnes
the critical region of the RC. The shape of the critical region
follows from the deﬁnition of TRC
n . Because ¯d is outside the
critical region in this example, the RC fails to reject the false
null hypothesis in this case.
The Distribution Under the Null Hypothesis
Hansen proposed an alternative to the LFC approach
that leads to more powerful tests of composite hypotheses. The
LFC is based on a supremum taken over the null hypothesis,
whereas the idea of Hansen is to take the supremum
over a smaller (conﬁdence) set chosen such that it contains the
true parameter with a probability that converges to 1. In this
article, we use a closely related procedure based directly on the
asymptotic distributions of Theorem 1 and Corollary 1.
In the preceding section, we saw that the poor alternatives
are irrelevant for the asymptotic distribution. So a proper test
should reduce the inﬂuence of these models while preserving
the inﬂuence of the models with µk = 0. It may be tempting to
simply exclude the alternatives with ¯dk < 0 from the analysis.
But this approach does not lead to valid inference in general, because the models that are (or appear to be) a little worse than the
benchmark can have a substantial inﬂuence on the distribution
of the test statistic in ﬁnite samples (and even asymptotically
if µk = 0). So we construct our test in a way that incorporates
all models, while reducing the inﬂuence of alternatives that the
data suggest are poor.
Our choice of estimator, ˆµc, is motivated by the law of the
iterated logarithm stating that
n1/2(¯dk −µk)
n1/2(¯dk −µk)
The ﬁrst equality shows that ˆµc
k effectively captures all of the
elements of µ that are 0, such that µk = 0 ⇒ˆµc
k = 0 almost
surely. Similarly, if µk < 0, then the second equality states that
¯dk is very close to µk; in fact, n1/2¯dk is smaller than −n1/2−ϵ
for any ϵ > 0 and n sufﬁciently large. Thus n1/2¯dk/ωk is, in
particular, smaller than the threshold rate, −√2loglogn, for
n sufﬁciently large, demonstrating that ¯dk eventually will stay
below the implicit threshold in our deﬁnition of ˆµc
k, such that
µk < 0 ⇒ˆµc
k ≪0 almost surely. So ˆµc meets the necessary
asymptotic requirements that we identiﬁed in Theorem 1 and
Corollary 1.
Although the poor alternatives should be discarded asymptotically, this is not the case in ﬁnite samples, as we discussed
earlier. Our estimator, ˆµc, explicitly accounts for this by keeping all alternatives in the analysis. A poor alternative, µk < 0,
has an impact on the critical value whenever µk/(ωkn1/2) is
only moderately negative, say between −1 and 0. This is the
reason that the poorly performing alternatives cannot simply
be omitted from the analysis. We emphasize this point because
an earlier version of this article has been incorrectly quoted for
“discarding the poor models.”
Although ˆµc leads to a correct separation of good and poor
alternatives, other threshold rates also produce valid tests. The
rate √2loglogn is the slowest rate that captures all alternatives
with µk = 0, whereas the faster rate, n1/2−ϵ for any ϵ > 0, guarantees that all of the poor models are discarded asymptotically.
So a range of rates can be used to asymptotically discriminate
between good and poor alternatives. One example is 1
which was used in a previous version of this article. Because
different threshold rates will lead to different p values in ﬁnite
samples, it is convenient to determine an upper and lower bound
for the p values in which different threshold rates can result.
These are easily obtained using the “estimators,” ˆµl and ˆµu,
given by ˆµl
k ≡min(¯dk,0) and ˆµu
k = 0, k = 1,...,m, where
the latter yields the LFC-based test. It is simple to verify that
ˆµl ≤ˆµc ≤ˆµu, which in part motivates the superscripts, and we
have the following result, where F0 is the cdf of ϕ(Z,v0) that
we deﬁned in Theorem 1.
Theorem 2. Let Fi
n be the cdf of ϕ(n1/2Zi
n,Vn), for i = l, c,
or u, where n1/2(Zi
d→Nm(0,). Suppose that Assumptions 1 and 2 hold; then Fc
n →F0 as n →∞, for all continuity
points of F0 and Fl
n(x) for all n and all x ∈R.
Theorem 2 demonstrates that ˆµc leads to a consistent estimate of the asymptotic distribution of our test statistic. The theorem also demonstrates that ˆµl and ˆµu provide upper and lower
bound for the distribution Fc
n that can be useful in practice; for
example, a substantial difference between these bounds is indicative of the presence of poor alternatives, in which case the
sample-dependent null distribution is useful.
Given a value for the test statistic t = Tn(d1,...,dn), it is natural to deﬁne the true asymptotic p value as p0(t) ≡1 −F0(t).
The empirical p value is deduced from an estimate of Fi
i = l,c,u, and the following corollary demonstrates that ˆµc
yields a consistent p value.
Corollary 2. Consider the studentized test statistic, t =
(d1,...,dn). Let the empirical p value, ˆpc
n(t), be inferred from ˆFc
n, where ˆFc
n(t) = o(1) for all t. Then
p→p0(t) for any t > 0.
The two other choices, ˆµl and ˆµu, do not produce consistent p values in general. It follows directly from Theorem 1
that ˆµu will not produce a consistent p value unless µ = 0.
That the p value from using ˆµl is inconsistent is easily understood by noting that a critical value based on Nm(0,) will
be greater than one based on the mixed Gaussian distribution,
Nm(n1/2 ˆµl,). So a p value based on ˆµl is (asymptotically)
smaller than the correct p value, which makes this a liberal test
despite the fact that ˆµl
p→µ under the null hypothesis. This
problem is closely related to the inconsistency of the bootstrap,
when a parameter is on the boundary of the parameter space,
as analyzed by Andrews . In our situation the inconsistency arises because µ is on the boundary of the null hypothesis,
which leads to a violation of a similarity on the boundary condition . 
Figure 3 shows how the consistent estimate of the null distribution can improve the power. Recall the situation from Figure 2, where the null hypothesis is false. The data-dependent
null distribution is deﬁned from a projection of ¯d = (¯d1, ¯d2)′
onto the set of parameter values that conform with the null hypothesis. This yields the point a, which represents ˆµl = ˆµc (assuming that ¯d2 is below the relevant 2loglogn-threshold). The
critical region of the SPA test (induced by ¯d) is the area above
and to the right of the dotted line marked by CSPA. Because ¯d is
in the critical region, the SPA test (correctly) rejects the null
hypothesis in this case.
Sample-Dependent Null Distribution. This distribution is centered about
ˆµc = a, which leads to the critical value CSPA. In contrast, the RC fails to
reject the null hypothesis, because the LFC-based null distribution leads
to the larger critical value CRC.
BOOTSTRAP IMPLEMENTATION OF THE TEST
FOR SUPERIOR PREDICTIVE ABILITY
In this section we describe a bootstrap implementation of the
SPA tests in detail. The implementation is based on the stationary bootstrap of Politis and Romano , but it is straightforward to modify the implementation to the block bootstrap
of Künsch . Although there are arguments that favor the
block bootstrap over the stationary bootstrap ,
these advantages require the use of an optimal block length that
is difﬁcult to determine when m is large relative to n, as will
often be the case when testing for SPA.
The stationary bootstrap of Politis and Romano is
based on pseudo-time series of the original data. The pseudotime series {d∗
b,t} ≡{dτb,t}, b = 1,...,B, are resamples of dt,
where {τb,1,...,τb,n} is constructed by combining blocks of
{1,...,n} with random lengths. The leading case is that where
the block length is chosen to be geometrically distributed with
parameter q ∈ . The
number of bootstrap resamples, B, should be chosen to be sufﬁciently large such that the results are not affected by the actual
draws of τb,t. This can be achieved by increasing B until the results are robust to increments, or more formal methods, such as
the three-step method of Andrews and Buchinsky , can
be applied. Here we follow the conventional setup of the stationary bootstrap and generate B resamples from two random
B × n matrices, U and V, where the elements, ub,t and vb,t,
are independent and uniformly distributed on (0,1]. The ﬁrst
element of each resample is deﬁned by τb,1 = ⌈nub,1⌉, where
⌈x⌉is the smallest integer that is larger than or equal to x. For
t = 2,...,n, the elements are given recursively by
if vb,t < q
1{τb,t−1<n}τb,t−1 + 1
if vb,t ≥q.
So with probability q, the tth element is chosen uniformly on
{1,...,n} and with probability 1 −q, the tth element is chosen
to be the integer that follows τb,t−1, unless τb,t−1 = n in which
case τb,t ≡1. The block bootstrap is very similar to the stationary bootstrap, but instead of using blocks with random length,
the block bootstrap combines blocks of equal length.
From the pseudo-time series, we calculate their sample averages, ¯d∗
b,t, b = 1,...,B, that can be viewed as
(asymptotically) independent draws from the distribution of ¯d,
under the bootstrap distribution. So this provides an intermediate step to estimate the distribution of our test statistic.
Lemma 1. Let Assumption 1 hold and suppose that the bootstrap parameter, q = qn, satisﬁes qn →0 and nq2
n →∞. Then
n1/2(¯d −µ) ≤z
where P∗denotes the bootstrap probability measure.
This lemma demonstrates that the empirical distribution of
the pseudo-time series can be used to approximate the distribution of n1/2(¯d−µ). This result follows directly from Goncalves
and de Jong who derived the result under slightly
weaker assumptions than we have stated. (Their assumptions
are formulated for near–epoch-dependent processes.) The test
statistic TSPA
requires estimates of ω2
k, k = 1,...,m. An earlier
version of this article was based on the estimator
k,b −n1/2¯dk
k,b = n−1 n
t=1 dk,τb,t. By the law of large numbers, this
estimator is consistent for the bootstrap population value of the
variance, which in turn is consistent for the true variance, ω2
 . However, it is our
experience that B needs to be quite large to sufﬁciently reduce
the additional layer of randomness introduced by the resampling scheme. So our recommendation is to use the bootstrap
population value directly, which is given by
k ≡ˆγ0,k + 2
κ(n,i) ˆγi,k,
ˆγi,k ≡n−1
(dk,j −¯dk)(dk,j+i −¯dk),
i = 0,1,...,n −1,
are the usual empirical covariances and the kernel weights (under the stationary bootstrap) are given by
κ(n,i) ≡n −i
(1 −q)i + i
n(1 −q)n−i
 .
We seek the distribution of the test statistics under the null
hypothesis, so we impose the null by recentering the bootstrap
variables about ˆµl, ˆµc, or ˆµu. This is done by deﬁning
k,b,t −gi(¯dk),
i = l,c,u,b = 1,...,B,t = 1,...,n,
where gl(x) = max(0,x), gc(x) = x · 1{x≥−
k/n)2loglogn}, and
gu(x) = x. It is simple to verify that the expected values of
k,b,t, i = l,c,u (conditional on d1,...,dn), are given by ˆµl,
ˆµc, and ˆµu.
Corollary 3. Let Assumption 1 hold and let Z∗
b,t be centered
about ˆµ, for ˆµ = ˆµl, ˆµc, or ˆµu. Then
n1/2(¯d −µ) ≤z
k,b = n−1 n
k,b,t, k = 1,...,m.
Given our assumptions about the test statistic, Corollary 3
demonstrates that we can approximate the distribution of our
test statistics under the null hypothesis by the empirical distribution we obtain from the bootstrap resamples Z∗
b,t, t = 1,...,n.
The p values of the three tests for SPA are now simple to obtain. We calculate TSPA∗
= max{0,maxk=1,...,m[n1/2 ¯Z∗
k,b/ ˆωk]}
for b = 1,...,B, and the bootstrap p value is given by
where the null hypothesis should be rejected for small p values.
Thus we obtain three p values, one for each of the estimators
ˆµl, ˆµc, and ˆµu. The p values based on the test statistic TRC
be derived similarly.
Note that we are using the same estimate of ω2
k to calculate
, b = 1,...,B. A nice robustness property of the
SPA test is that it is valid even if ˆω2
k is inconsistent for ω2
is easy to understand by recalling that ˆω2
k = 1 for all k leads
to the RC (and 1 is generally inconsistent for ω2
k). Although
this robustness is convenient, it is desirable that ( ˆω2
1,..., ˆω2
be close to (ω2
m), such that the individual statistics,
n1/2¯dk/ ˆωk, have approximately the same scale, due to the power
issues that we discussed in Section 2.
SIZE AND POWER COMPARISON BY
MONTE CARLO SIMULATIONS
The two test statistics TRC
and the three null distributions centered about ˆµl, ˆµc, and ˆµu result in six different
tests. In this section we study the size and power properties of
these tests in a Monte Carlo experiment.
We generate Lk,t ∼iidN(λk/√n,σ 2
k ) for k = 0,1,...,m and
t = 1,...,n, where the benchmark model has λ0 = 0. So positive values (λk > 0) correspond to alternatives that are worse
than the benchmark, whereas negative values (λk < 0) correspond to alternatives that are better than the benchmark.
In our experiment we have λ1 ≤0 and λk ≥0 for k =
2,...,m, such that the ﬁrst alternative (k = 1) deﬁnes whether
the rejection probability corresponds to a type I error (λ1 = 0)
or a power (λ1 < 0). The performances of the “poor” models
are such that their mean values are spread evenly between 0
and λm = 0 (the worst model). So the vectors of the λk’s are




In our experiments we use 0 = 0,1,2,5,10 to control the
extent to which the inequalities are binding with (0 = 0 corresponding to the case where all inequalities are binding). The
ﬁrst alternative model has 1 = 0, −1, −2, −3, −4, −5. So
λ1 = 1 deﬁnes the local alternative that is being analyzed
(unless 1 = 0, which conforms with the null hypothesis). To
make the experiment more realistic, we tie the variance, σ 2
the “quality” of the model. Speciﬁcally, we set
2 exp(arctan(λk)),
such that a good model has a smaller variance than poor model.
Note that this implies that
√n¯dk ∼N(µk,ω2
where the expression for ω2
k now follows from var(dk,t) =
var(L0,t −Lk,t) = 1
2 + var(Lk,t) and the Taylor expansion
2 exp(arctan(x)) = 1
24x4 + O(x5)
Simulation Results
First, we consider the case with m = 100 and the two sample sizes n = 200 and n = 1,000. Then we consider the case
with m = 1,000 using the sample size n = 200. The rejection
frequencies that we report are based on 10,000 independent
samples, where we used q = 1 in accordance with the lack of
time dependence in dt, t = 1,...,n. All of our simulations were
made using Ox 3.30 . The rejection frequencies of the tests at the 5% and 10% levels are reported in Tables 2–4. The numbers in italic type are used when the null
hypothesis is true (1 = 0), so these frequencies correspond to
type I errors. The numbers in regular type represent powers for
the various local alternatives (1 < 0).
Table 2. Rejection Frequencies Under the Null and Alternative (m = 100 and n = 200)
Level: α = .05
Level: α = .10
Panel A: 0 = 0
Panel B: 0 = 1
Panel C: 0 = 2
Panel D: 0 = 5
Panel E: 0 = 10
Estimated rejection frequencies for the six tests for SPA under the null hypothesis (1 = 0) and local alternatives (1 < 0). The rejection frequencies in italic type correspond to type I
errors, and those in regular type correspond to local powers. The reality check of White is denoted by RCu, and the test advocated in this article is denoted by SPAc.
Table 3. Rejection Frequencies Under the Null and Alternative (m = 100 and n = 1,000)
Level: α = .05
Level: α = .10
Panel A: 0 = 0
Panel B: 0 = 1
Panel B: 0 = 2
Panel B: 0 = 5
Panel B: 0 = 10
Estimated rejection frequencies for the six tests for SPA under the null hypothesis (1 = 0) and local alternatives (1 < 0). The rejection frequencies in italic type correspond to type I
errors, and those in regular type correspond to local powers. The reality check of White is denoted by RCu, and the test advocated in this article is denoted by SPAc.
Table 2 presents the results for the case where m = 100 and
n = 200. In the situation where all 100 inequalities are binding (0 = 1 = 0), we see that the rejection probabilities are
close to the nominal levels for all the tests. The SPAc test has an
overrejection by 1%. This overrejection appears to be a smallsample problem, because it disappears when the sample size is
increased to n = 1,000 (see Table 3). The fact that the liberal
null distribution does not lead to a larger overrejection is interesting. This ﬁnding may be due to the positive correlation
across alternatives, cov(di,t,dj,t) = var(L0,t) > 0, which creates
a positive correlation between the test statistic and ˆµl. Thus the
critical value will tend to be (too) small, when the test statistic
is small and this correlation will reduce the overrejection of the
tests based on ˆµl. This suggests that our test may be improved
if there is a reliable way to incorporate information about the
off-diagonal elements of . We do not pursue this aspect in this
Panel A corresponds to the case where µ = 0, that is, the best
possible situation for LFC-based tests. This is the only situation
where the LFC-based tests apply the correct asymptotic distribution, so it is not surprising that the tests based on ˆµu = 0 do
well. Fortunately, our new test, SPAc, also performs well in this
case. Turning to the conﬁgurations where 0 > 0, we immediately see the advantages of using the sample-dependent null
distribution. A somewhat extreme situation is observed in Table 2, panel E for (0,1) = (10,−3), whereas the RC almost
never rejects the null hypothesis, while the new SPAc-test has a
power close to 84%.
Table 4 is quite interesting, because this is a situation where
m = 1,000 exceeds the sample size n = 200. So in this case it is
impossible to estimate  in a sensible manner without imposing a restrictive structure on its coefﬁcients. Thus using standard ﬁrst-order asymptotics is not a viable alternative to the
bootstrap implementation in this situation. Because the bootstrap invokes an implicit estimate of , one might worry about
its properties in this situation, where an explicit estimate is unavailable. Nevertheless, the bootstrap does surprisingly well,
and we notice only a slight overrejection when all inequalities are binding (0 = 1 = 0). The power properties are quite
good despite the fact that 1,000 alternatives are being compared
with the benchmark.
The power curves for the tests that use ˆµc and ˆµu are shown
in Figure 4 for the case where m = 100, n = 200, and 0 = 20.
These power curves are based on tests that aim at a 5% signiﬁcance level, and we have plotted their rejection frequencies
against a range of local alternatives. These rejection frequencies have not been adjusted for their underrejection at 1 = 0.
This is a fair comparison, because it would not be possible to
Table 4. Rejection Frequencies Under the Null and Alternative (m = 1,000 and n = 200)
Level: α = .05
Level: α = .10
Panel A: 0 = 0
Panel B: 0 = 1
Panel C: 0 = 2
Panel D: 0 = 5
Panel E: 0 = 10
Estimated rejection frequencies for the six tests for SPA under the null hypothesis (1 = 0) and local alternatives (1 < 0). The rejection frequencies in italic type correspond to type I
errors, and those in regular type correspond to local powers. The reality check of White is denoted by RCu, and the test advocated in this article is denoted by SPAc.
Figure 4. Local Power Curves of the Four Tests, SPAc, SPAu, RCc, and RCu, for the Simulation Experiment Where m = 100, Λ0 = 20, and
µ1/√n ( = −Λ1) Ranges From 0 to 8 (the x-axis). The power curves quantify the power improvements that are achieved by the two modiﬁcations
of the reality check. Both the studentization and the data-dependent null distribution lead to substantial power gains in this design.
make such an adjustment in practice without exceeding the intended level of the test for other conﬁgurations—particularly
the case where 0 = 1 = 0. From the
power curves in Figure 4, it is clear that the RC is dominated by the three other tests. There is a substantial increase
in power from using the consistent distribution, and a similar
improvement is achieved by using the standardized test statistic, TSPA
. For example, the local alternative 1 = −4 is rejected
by the RC in about 5.5%. Using the data-dependent null distribution (RCc) or the studentization (SPAu) improves the power
to about 73.6% and 96.4%. Invoking both modiﬁcations (as we
advocate) improves the power to 99.7% in this simulation experiment. So both modiﬁcations are very useful, and the combination of the two yields a substantial improvement in power.
Comparing the sample sizes that would result in the same
power is an effective way to convey the relative efﬁciency of the
tests. For the conﬁguration used in Figure 4, we see that the four
tests have 50% power at the local alternatives, µ1/√n ≃2.13,
2.60, 3.63, and 5.28. These numbers demonstrate that we would
need a sample size that is (2.60/2.13)2 = 1.49 times larger to
regain the power that is lost by using the LFC instead of the
sample-dependent null distribution. In other words, using the
LFC is equivalent to tossing away about 33% of the data. Similarly, dropping the studentization is equivalent to tossing away
about 65% of the data, and dropping both modiﬁcations (i.e.,
using the RC instead of SPAc) is equivalent to tossing away
about 84% of the data in this simulation design.
FORECASTING U.S. INFLATION USING LINEAR
REGRESSION MODELS
In an attempt to forecast annual U.S. inﬂation, we estimate
a large number of linear regression models used to construct
competing forecasts. The annual U.S. inﬂation rate is deﬁned
by Yt ≡log[Pt/Pt−4], where Pt is the GDP price deﬂator for
the tth quarter. Inﬂation and most of the variables are not observed instantaneously. For this reason, we let the set of potential regressors consist of variables that are lagged ﬁve quarters
or more relative to the end of the 12-month period for which
we attempt to predict inﬂation. This leaves time (one quarter)
for observing most of our regressors at the beginning of the
12-month period.
The linear regression models include 1, 2, or 3 regressors out
of the pool of 27 regressors, X1,t,...,X27,t, which leads to a
total of 3,303 regression models. Descriptions and deﬁnitions
of the regressors are given in Table 5.
The sequence of forecasts produced by the kth regression
model is given by
ˆYk,τ+5 ≡ˆβ′
(k),τX(k),τ,
τ = 0,...,n −1,
where X(k),τ contains the regressors included in model k
(k),τ is the least squares estimator based on the 32
most recent observations (a rolling window). Thus ˆβ(k),τ ≡
k,τXk,τ)−1X′
k,τYτ, where the rows of Xk,τ are given by
(k),t−5, t = τ −32+1,...,τ, and similarly the elements of Yτ
are given by Yτ , t = τ −32 + 1,...,τ. Using a rolling-window
estimation scheme ensures that stationarity of (Xt,Yt) is carried over to L(Yt+h, ˆβ′
(k),tX(k),t), whereby a violation of Assumption 1 is avoided. For example, it is difﬁcult to reconcile
Assumption 1 with the case where β(k) is estimated recursively
(i.e., using an expanding window of observation as n →∞).
The ﬁrst forecast of annual inﬂation is made at time 1959:Q4
 . So the
evaluation period includes n = 160 quarters:
t = 1952:Q1,...,1959:Q4
initial estimation period
,1961:Q1,...,2000:Q4
evaluation period
The models are evaluated using a mean absolute error criterion (MAE) given by L(Yt, ˆYk,t) = |Yt −ˆYk,t|, and the bestperforming models turn out to have a Phillips curve structure. In
fact, the best forecasts are produced by regressors that measure
(changes in) inﬂation, interest rates, employment, and GDP, and
the very best sample performance is achieved by the three regressors X1,t, X8,t, and X13,t, which represent annual inﬂation,
employment relative to the previous year’s employment, and
the change in GDP (see Table 5). We also include the average forecast (i.e., average across all regression-based forecasts),
because this simple combination of forecasts is often found to
dominate the individual forecasts . In addition to the average forecast, the 27 regressors lead
to 3,303 regression-based forecasts when we consider all possible subset regressions with 1, 2, or 3 regressors. So we are to
compare m = 3,304 forecasts to the random-walk benchmark,
and we refer to this set of competing forecasts as the large universe.
Panel A of Table 6 contains the output produced by the tests
for SPA for the large universe. Because the SPAc p value is .741,
there is no statistical evidence that any of the regression-based
forecasts (including the average of them) are better than the
random-walk forecast. Note the discrepancy between the p values based on ˆµl and ˆµu. This difference suggests that some
of the alternatives are poor forecasts, and a closer inspection of
the large universe veriﬁes that several models have substantially
worse performance than the benchmark.
The ability to construct better forecasts using models with
additional regressors is made difﬁcult by the need to estimate
additional parameters. In a forecasting exercise there is a tradeoff between estimating a parameter and imposing it to have a
particular value (typically 0, which is implicitly imposed on the
coefﬁcient of an omitted regressor). Imposing a particular value
will (most likely) introduce a “bias,” but if this bias is small, it
may be less severe for out-of-sample predictions than the prediction error introduced by estimation error . Exploiting this bias–variance trade-off is
particularly useful whenever the estimator is based on a moderate number of observations, as is the case in our application.
For this reason, we also consider a small universe of regressionbased forecasts, where all models include lagged inﬂation, X1,t,
as a predictor (regressor) with a coefﬁcient set to unity. The
remaining parameters are estimated by ridge regressions that
shrink these parameters toward 0.
Thus the regression models have the form
Yτ+5 −Yτ ≡β′
(k)X(k),τ + ε(k),t,
τ = 0,...,n −1,
where X(k),τ is a vector that includes either one or two regressors. As before, we use a rolling-window scheme (32 quar-
Table 5. Deﬁnitions of Variables
Panel A: Description of variables
Annual inﬂation
X1, t, X2, t
Annual inﬂation (lags of Yt)
X3, t, X4, t
Quarterly inﬂation
Quarterly inﬂation relative to previous year’s inﬂation
X6, t, X7, t
Changes in employment in manufacturing sector
Quarterly employment relative to average of previous year
Quarterly employment relative to average of previous 2 years
X10, t, X11, t
Quarterly changes in real inventory
X12, t, X13, t
Quarterly changes in quarterly GDP
Interest paid on 3-month T-bill
X15, t, X16, t
Changes in 3-month T-bill
X17, t, X18, t
Changes in 3-month T-bill relative to level of T-bill
X19, t, X20, t
Changes in prices of fuel and energy
X21, t, X22, t
Changes in prices of food
X23, t–X26, t
Quarterly dummies: ﬁrst, second, third, and fourth quarters
Panel B: Deﬁnitions of variables
= log(GDPCTPIt) −log(GDPCTPIt −4),
X1, t = Y t −5,
X2, t = Y t −8
= 4[log(GDPCTPIt) −log(GDPCTPIt −1],
X4, t = X3, t −1
= log(1 + X3, t) −log(1 + X1, t −1)
= log(MANEMPt) −log(MANEMPt −1),
X7, t = X6, t −1
= log(MANEMPt) −log( 1
i=1 MANEMPt −i)
= log(MANEMPt) −log( 1
i=1 MANEMPt −i)
X10, t = log(CBIt) −log(GDPt),
X11, t = X10, t −1
X12, t = log(GDPt) −log(GDPt −1),
X13, t = X12, t −1
X14, t = TB3MSt,
X15, t = X14, t,
X16, t = X15, t −1,
X17, t = X14, t/X14, t,
X18, t = X17, t −1
X19, t = log(PPIENGt) −log(PPIENGt −1),
X20, t = X19, t −1
X21, t = log(PPIFCFt) −log(PPIFCFt −1),
X22, t = X21, t −1
X23, t = 1,
X24, t = X23, t −1,
X25, t = X23, t −2,
X26, t = X23, t −3,
X27, t = 1
Raw data: GDPCTPI = gross domestic product: chain-type price index; CBI = change in private inventories; GDP = gross domestic product; TB3MS = 3-month Treasury bill rate, secondary market∗; PPIENG = producer price index: fuels and related products
and power∗∗; PPIFCF = producer price index: ﬁnished consumer foods∗∗; MANEMP = employees on nonfarm payrolls: manufacturing.
∗Quarterly data are deﬁned as the average of the monthly observations over the quarter.
∗∗Quarterly data are deﬁned as be the last monthly observation of the quarter.
Table 6. Tests for Superior Predictive Ability
t-statistic
Panel A: Results for the large universe of forecasts
Evaluated by MAE
Benchmark:
m = 3,304 (number of models)
Best performing:
n = 160 (sample size)
Most signiﬁcant:
B = 10,000 (resamples)
q = .25 (dependence)
SPA p values
Panel B: Results for the small universe of forecasts
Evaluated by MAE
Benchmark:
m = 352 (number of models)
Best performing:
n = 160 (sample size)
Most signiﬁcant:
B = 10,000 (resamples)
q = .25 (dependence)
SPA p values
Panel C: Results for the full universe of forecasts
Evaluated by MAE
Benchmark:
m = 3,656 (number of models)
Best performing:
n = 160 (sample size)
Most signiﬁcant:
B = 10,000 (resamples)
q = .25 (dependence)
SPA p value
The table reports SPA p values for three sets of regression-based forecasts that are compared to a random-walk forecast. The p value of the new test, SPAc, is in bold type. Panel A
contains the results for the large universe, panel B contains the results for the small universe, and panel C contains the results for the full universe. For each “universe of forecasts” we also
report the sample loss for the benchmark and the four alternative forecasts that had the smallest sample loss, the largest t-statistic for relative sample loss ( ¯dk ), the median sample loss (across
alternatives), and the worst sample loss. The corresponding t-statistic (of their sample loss relative to the benchmark) is given in the second last column. We also report the “p values” from the
pairwise comparisons of “best” and “largest t-statistic” forecasts to the benchmark. These p values (unlike the SPA p values) do not account for the entire universe of forecasts.
ters), but with the estimator for β(k) now given by ˆβ(k),τ ≡
k,τXk,τ + λI)−1X′
k,τ ˜Yτ , where λ = .1 is the shrinkage parameter and the elements of ˜Yτ are given by Yt −Yt−5 for
t = τ −32+1,...,τ. This results in 351 regression-based forecasts plus the average forecast, such that the total number of alternative forecasts in the small universe is m = 352. The most
accurate forecast in the small universe is produced by the regression model with the regressors X8,t and X9,t, which are
two measures of (relative) employment. The largest t-statistic
is produced by the regressions X6,t and X10,t, which represent
changes in employment and inventories. So our ﬁndings support a conclusion reached by Stock and Watson that
forecasts based on Phillips curve speciﬁcations are useful for
forecasting inﬂation.
The empirical results for this universe are presented in
panel B of Table 6. The SPAc p value for this universe is
.048, which suggests that the benchmark is outperformed by
the regression-based forecasts. For each of the test statistics,
we note that the p values are quite similar. This agreement is
not surprising, because the worst forecast is only slightly worse
than the benchmark, such that ˆµl and ˆµu are similar. The difference in p values across the two test statistics is fairly modest but
do suggest some variation in the variances, ω2
k, k = 1,...,352.
Reporting the results in panel B is not fully consistent with
the spirit of this article, because the results in panel B do not
control for the 3,304 forecasting models that were compared to
the benchmark in the initial analysis of the large universe. So
the signiﬁcant p values in panel B are subject to the criticisms
of data mining. To address this concern, we perform the tests
for SPA over the union of the large universe and the small universe. We refer to this set of forecasts as the full universe, and
present the results for this set of alternatives in panel C. Adding
the large number of insigniﬁcant alternatives to the comparison
reduces the signiﬁcance, although the excess performance continues to be borderline signiﬁcant with an SPAc p value of 10%.
Note that the RC’s p value increases from 10.6% to 96.3% by
“adding” the large universe to the small universe. This jump in
the p value is most likely due to the RC’s sensitivity to poor and
irrelevant alternatives. The SPAc test’s p value increases from
4.8% to 10%. Although this increment is more moderate, it reveals that the new test is not entirely immune to the inclusion
of (a large number of ) poor forecasts. This reminds us that excessive data mining can be costly in terms of the conclusions
that can be drawn from an empirical analysis, because it may
prevent the researcher from concluding that a particular ﬁnding
is signiﬁcant. Given the scarcity of macroeconomic data, it will
often be useful to conﬁne the set of alternatives to those motivated by theoretical considerations, instead of undertaking a
blind search over a large number of alternatives.
SUMMARY AND CONCLUDING REMARKS
We have analyzed the problem of comparing multiple forecasts to a given benchmark through tests for superior predictive ability. We have shown that the power can be improved
(often substantially) by using a studentized test statistic and incorporating additional sample information by means of a datadependent null distribution. The latter serves to identify the
irrelevant alternatives and reduce their inﬂuence on the test
The power improvements were quantiﬁed in simulation experiments and an empirical forecasting exercise of U.S. inﬂation. These also highlighted that the RC is sensitive to poor
and irrelevant alternatives. Two researchers are therefore more
likely to arrive at the same conclusion when they use the
SPAc test than they would when using the RC—even if they
do not fully agree on the set of forecasts that is relevant for the
Interestingly, we found that the best (and most signiﬁcant)
predictions of U.S. inﬂation were produced by regression-based
forecasts that had a Phillips curve structure. In our full universe of alternatives, we found that the (random-walk) benchmark forecast is outperformed by the regression-based forecasts
if a moderate (10%) signiﬁcance level is used. Whereas the
SPAc test yields a p value of 10%, the RC yields a p value of
about 96%, such that the two tests arrive at opposite conclusions
(weak evidence against H0 vs. no evidence). This is caused by
the poor alternatives that conceal the evidence against the null
hypothesis when the RC is used. This phenomenon was also
discussed Hansen and Lunde , who compared a large
number of volatility models using the methods of this article.
Although there are several advantages of our new test, some
important issues need to be addressed in future research. In this
article we have proposed two modiﬁcations and adopted these
in a stationary framework. This framework does not permit the
comparison of parameterized models when a recursive scheme
is used to estimate the parameters. So it would be interesting to
construct a suitable test that can accommodate this situation and
analyze the need for our two modiﬁcations in this framework.
Despite its many pitfalls, data mining is a constructive device
for the discovery of true phenomena and has become a popular tool in many applied areas, such as genetics, e-commerce,
and ﬁnancial services. However, it is necessary to account for
the full data exploration before making a legitimate statement
about signiﬁcance. Increasing the number of comparisons raises
the bar at which alternatives are classiﬁed as being signiﬁcantly
better than the benchmark. This aspect is particularly problematic for economic applications where data are scarce. In this
context it is particularly useful to conﬁne the exploration to alternatives motivated by theoretical considerations. Our empirical application provides a good illustration of this issue. Within
the small universe we found fairly compelling evidence against
the null hypothesis, and ex post it is easy to produce arguments
that motivate the use of shrinkage methods, which led to the
small universe of regression-based forecasts. However, because
the large universe was explored in the initial analysis, we cannot exclude the possibility that the largest t-statistic would have
been found in this universe. The weaker evidence against the
null hypothesis found in the full universe is the price that we
have to pay for the data exploration that preceded our analysis
of the small universe.
ACKNOWLEDGMENTS
I thank Albert Chun, Jinyong Hahn, James D. Hamilton,
Søren Johansen, Tony Lancaster, Asger Lunde, Michael
McCracken, Barbara Rossi, and seminar participants at Princeton, Harvard/MIT, University of Montreal, UBC, New York
Fed, Stanford, NBER/NSF Summer Institute 2001, three anonymous referees, and Torben G. Andersen (editor) for many valuable comments and suggestions. I am also grateful for ﬁnancial
support from the Danish Research Agency (grant 24-00-0363).
APPENDIX: PROOFS
Proof of Theorem 1
We deﬁne the vectors, Wn,Zn ∈Rm, whose elements are
given by Wn,k = n1/2¯dk1{µk<0} and Zn,k = n1/2¯dk1{µk=0}, k =
1,...,m. Thus Un = Wn + Zn under the null hypothesis.
The mappings (coordinate selectors) that transform Un into
Wn and Zn are continuous, so that (Wn −n1/2µ)
0) and Zn
d→Nm(0,0) by the continuous mapping theorem.
This implies that
ϕ(Un,Vn) = ϕ(Wn + Zn,Vn)
= ϕ(Zn,Vn) + op(1)
d→ϕ(Z,v0),
where the second equality uses Assumption 2(b) and the fact
that the elements of Wn are either 0 (µk = 0) or diverges to
minus inﬁnity in probability (µk < 0). Under the alternative
hypothesis, there will be an element of n1/2 ¯d that diverges to
inﬁnity. So the last result of the theorem follows by Assumption 2(c).
Proof of Corollary 1
The results follow from n1/2(¯d −µ)
d→Nm(0,) and
the continuous mapping theorem, applied to the mapping
Proof of Theorem 2
Without loss of generality, suppose that µk = 0 for k =
1,...,m0 and that µk < 0 for k = m0 + 1,...,m. Given our
deﬁnition of ˆµc, it holds that P( ˆµc
1 = ··· = ˆµc
m0 = 0, ˆµc
ϵ,..., ˆµc
m < ϵ) almost surely as n →0, for some ϵ < 0. So
for n sufﬁciently large, the last m −m0 elements of Zi
bounded below 0 in probability, which demonstrates that ˆµc
leads to the correct limiting distribution and Fc
n →F0. That
n(x) follows from ˆµl ≤ˆµc ≤ˆµu.
Proof of Corollary 2
The test statistic TSPA
leads to a continuous asymptotic distribution, F0(t), for all t > 0. Because ˆFc
n(t) −F0(t) = [ ˆFc
n(t)] + [Fc
n(t) −F0(t)], the result now follows, because the
ﬁrst term is o(1) by assumption and the second term is o(1) by
Theorem 2.
Proof of Lemma 1
This follows from work of Goncalves and de Jong ) −(¯dk −gi(¯dk)) =
k,b,t −¯dk for all k = 1,...,m, the corollary follows trivially
from Lemma 1.
[Received February 2005. Revised April 2005.]