Hyper-Local, Directions-Based Ranking of Places
Petros Venetis
Hector Gonzalez
Christian S. Jensen
Alon Halevy
Stanford University
Google Inc.
Aarhus University
Google Inc.
 
 
 
 
Studies ﬁnd that at least 20% of web queries have local intent; and
the fraction of queries with local intent that originate from mobile properties may be twice as high. The emergence of standardized support for location providers in web browsers, as well as of
providers of accurate locations, enables so-called hyper-local web
querying where the location of a user is accurate at a much ﬁner
granularity than with IP-based positioning.
This paper addresses the problem of determining the importance
of points of interest, or places, in local-search results. In doing
so, the paper proposes techniques that exploit logged directions
queries. A query that asks for directions from a location a to a location b is taken to suggest that a user is interested in traveling to b
and thus is a vote that location b is interesting. Such user-generated
directions queries are particularly interesting because they are numerous and contain precise locations.
Speciﬁcally, the paper proposes a framework that takes a user location and a collection of near-by places as arguments, producing
a ranking of the places. The framework enables a range of aspects
of directions queries to be exploited for the ranking of places, including the frequency with which places have been referred to in
directions queries. Next, the paper proposes an algorithm and accompanying data structures capable of ranking places in response
to hyper-local web queries. Finally, an empirical study with very
large directions query logs offers insight into the potential of directions queries for the ranking of places and suggests that the proposed algorithm is suitable for use in real web search engines.
INTRODUCTION
With the proliferation of high-end mobile devices and improvement to the wireless infrastructure, accessing the Internet from mobile phones is becoming commonplace. Current projections are
that in the near future, the sales of high-end mobile devices with
capable browsers will outnumber the sales of desktop computers,
and the mobile Internet is slated to become bigger than the conventional, wired Internet. Recent studies ﬁnd that at least 20% of
web queries have local intent, and the fraction of queries with local
intent that originate from mobile properties may be twice as high.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee. Articles from this volume were invited to present
their results at The 37th International Conference on Very Large Data Bases,
August 29th - September 3rd 2011, Seattle, Washington.
Proceedings of the VLDB Endowment, Vol. 4, No. 5
Copyright 2011 VLDB Endowment 2150-8097/11/02... $ 10.00.
The emergence of standardized support for location providers in
web browsers, as well as of providers of accurate locations, enables
so-called hyper-local web querying where the location of a user is
accurate at a much ﬁner granularity than with IP-based positioning.
This paper addresses a fundamental problem that arises in mobile
search: determining the importance of points of interest in localsearch results. We consider a setting in which a mobile user queries
the web for popular near-by places. The user may optionally also
specify a category of places of interest (e.g., restaurants, attractions, shopping). We assume that a collection of geo-referenced,
near-by places is available, but we are agnostic as to how they were
collected (e.g., from a yellow pages service or a web crawl). The
challenge is to produce a good ranking of the places and to do so
in a scalable fashion so that the places can be ranked for any query
location. While previous work on local-web querying (e.g., )
assumes IP-based positioning and therefore knows the location of
the user within a ZIP code, we assume that the user’s location is
known within tens to hundreds of meters, leading to a fundamentally different problem.
Our ﬁrst contribution is to show that logs of directions queries
are a promising source of user-generated content that may enable
the desired ranking. A directions-query log entry contains an origin, a destination, and the time at which the query was issued. With
the rise in queries to map-based services, such logs are voluminous.
In particular, they are much larger and much more current than online reviews of points of interest, which would be another source
for determining user interest in places. Although one can imagine many reasons for issuing directions queries (e.g., to determine
mileages for expense reports) a query suggests that there is an interest of some kind in the destination. Also, aggregate ﬁgures across
large numbers of queries serve to eliminate noise (as conﬁrmed by
our studies).
Intuitively, we interpret a query for directions to a point b as a
vote by a user that point b is of interest. However, we also allow
our scoring functions to take several other aspects into consideration. First, not all votes are equal: a vote from an origin farther
away from b could be considered as more important than a vote
from nearby b, as it suggests that the user was willing to travel
a longer distance to reach b. Second, when candidate places are
considered in response to a point-of-interest query from a user location l, the distance to a should be a factor in the ranking. Third,
as directions queries have a temporal component, it is relevant to
consider whether temporal patterns in the directions queries can be
exploited. For example, users may have different intentions in the
morning than in the evening, or on weekdays versus weekend days.
We believe that this is the ﬁrst research that explores the potential
of directions logs for the ranking of places in local search. Studies
have been reported that aim to identify places that individuals visit
from GPS logs and related data sources . Some
of these studies also study link-based ranking of places. This paper does not consider the identiﬁcation of places; and it considers
ranking based on directions queries, not link-based techniques.
Our second contribution is a scalable query processing technique
for a general hyper-local location-ranking architecture. The architecture, shown in Figure 1, operates in two phases. In the ofﬂine
phase, we compute a score for each place in our directory. In the
online phase, we rank places relative to a user’s query location and
the distance the user is willing to travel .
Hyper−local
Top−k places
hcell−rankings
Figure 1: Architecture for hyper-local ranking
The ofﬂine score can combine scores from several scorers, thus
leveraging different information sources. In addition to driving directions, we can consider the rankings of web pages (or just PageRank) associated with places, or reviews that places have received in
Google or yelp.com. Combining multiple scorers has been considered in previous work and is not considered here.
Our query processing technique superimposes a grid on the space.
For each grid cell, called an hCell, we store a list of the places that
fall into the cell, sorted by their (ofﬂine) score. At query time, we
retrieve the set of hCells relevant to the query and then compute the
top-k places, adjusting the score of each candidate place according
to its distance to the user’s query location.
We report on experiments that show that real (driving) directions
logs are a viable source for the scoring of places and that the proposed framework scales to large data sources and is capable of fast
query response, enabling integration with an existing search engine
infrastructure.
Roadmap: Section 2 presents the problem setting and proposes
techniques that use driving directions for place ranking. Section 3
then discusses the spatial indexing required to follow our work and
presents a baseline algorithm and a more efﬁcient algorithm for
place ranking. Section 4 considers the suitability of a real driving directions log for ranking of places. Section 5 presents performance experiments for the ranking algorithms we propose. We
cover related work in Section 6 and conclude in Section 7. An appendix contains a detailed description of our data sources, some
examples of ranking functions that take into account the directions
query logs, pseudocode for the baseline algorithm, proofs, and additional experimental results.
RANKING OF PLACES
Problem Setting
A query takes four arguments: (i) the user’s location, (ii) the time
that the query is issued, (iii) the maximum distance the user is willing to travel, and (iv) a query string. The query string can be one
of a predeﬁned set of categories, e.g., museums, restaurants, and
shopping. This query is capable of supporting a class of popular
services. The result of a query is a ranked list of nearby places
that match the query string, where the ranking aims to reﬂect the
interestingness of the places.
To formalize the key concepts of the problem setting, we assume
that a set of places P is given and has signature L × C, where L is
the set of locations in Euclidean space and C is a set of categories
(e.g., restaurant, cafe). Places thus model points of interest that a
user can visit, and they are categorized. We use Google’s business
directory as P.
We also assume that a set D of directions query log entries is
given, and we use the Google Maps query log. An entry d ∈D is
a record ⟨t, a, b, ||a, b||⟩with signature T × L × L × R+. Here, T
is the domain of query times and t is the time when the query was
issued; L was deﬁned earlier, and a is the “from” and b is the “to”
of the query; ||a, b|| is the distance between a and b.
A user query q ∈Q is a quadruple ⟨l, t, D, ¯q⟩with signature
L × T × R+ × Qq. Here, Qq is the set of all possible strings, i.e.,
a set of predetermined categories C, with the empty string denoting
all categories. Also, q.D is the distance that the user is willing to
travel. This parameter may be speciﬁed by the user, or it may be
derived from the travel pattern of the user (e.g., walking, driving)
using the query logs.
Scoring Functions
We use a scoring function S : Q × P × D to rank an argument
place according to a set of log entries and a user query. Since we
use scoring functions for ranking, the absolute values returned by a
scoring function do not matter—only the relative values matter.
Speciﬁcally, our framework supports the following kind of scoring function:
S(q, p, D) = S(p, D) × weightq.D(||q.l, p.l||).
This function separates the ofﬂine scoring from the online scoring, as discussed in Section 1. Thus, S(p, D) represents the ofﬂine
part of the scoring, while the weight weightq.D(·) is computed in
the online part. This arrangement enables maximum precomputation for the problem considered. At query time, the ofﬂine score
needs only be adjusted by a simple multiplication with a score that
depends on the distance between a query and a place. This is important to achieve low query latency.
We restrict the function weightq.D(·) to be non-increasing, as
we discuss in Section 3.3. This simply means that a user is assumed to prefer to travel a shorter rather than a longer distance to
achieve the same reward. This assumption also underlies k nearest
neighbor querying. Appendix A presents a range of possible scoring and weight functions and an experimental comparison of these
functions.
Time-Aware Scoring
Time-aware scoring may be applied to any scoring function we
just described. To account for the intuition that users’ behavior
vary across the day and between weekdays and weekend days, we
assign different weights to different directions queries according to
the temporal match between their time and the time in the user’s
Thus, we can obtain a temporal counterpart of each instance of
the previous kind of scoring function as follows:
S′(q, p, D)
S(q, p, D) + α × Stod(q.t)(q, p, D) +
+β × Sdow(q.t)(q, p, D).
Here, α and β are positive constants. Function Stod(·) restricts the
scoring function to the query log tuples that occur during a particular time of the day. We divide a day into disjoint intervals for morning (06.00–10.00), lunch (10.00–14.00), afternoon (14.00–17.00),
dinner (17.00–20.00), evening (20.00–23.00), and night (23.00–
06.00). Similarly, function Sdow(·) restricts the scoring function
to the query log tuples that occur during particular sets of days of
the week: either weekends or weekdays.
We note that the directions query logs can be used in this timeaware setting in contrast to other methods used until now (e.g.,
number of reviews, sentiment of reviews).
RANKING ALGORITHMS
We assume that a set of places has been retrieved and scored in
an ofﬂine phase. We describe two algorithms that then compute the
top-k most relevant places for a query issued by a user at a known
We ﬁrst provide background information on our spatial indexing
approach. Because major search engines (e.g., Google) use space-
ﬁlling curves to index their geo content, our methods rely on these
techniques for better integration.
Spatial Indexing
To efﬁciently retrieve places located in a given geographic region, we use a space-ﬁlling curve that maps locations on Earth to a
one dimensional curve.
5.1.1 5.1.2
Figure 2: Example of a Hilbert curve at two levels
We project the Earth’s surface onto the six faces of a unit cube
(applying standard transformations to account for distortion). On
each face of the cube we use the Hilbert curve to map points
from 2-D to 1-D. The Hilbert curve is deﬁned recursively, as exempliﬁed in Figure 2 that considers part of the 5th face of the Earth.
Each cell is subdivided into four smaller cells at each subsequent
level (identiﬁed by the numbers 0, 1, 2, 3 every time). The process
stops when the desired granularity has been achieved. We use cells
from level 1 (a face in the unit cube) to level 23 (approximately a
1 m×1 m cell). We refer to cells in a Hilbert Curve as hCells in the
following. Also, we denote as hCellIDl the ID of a hCell at level
l. Example hCellIDs can be seen in Figure 2 for l = 3 (left) and
l = 4 (right)1. Advise Appendix B for additional details.
Each (latitude, longitude) location is mapped to an hCell at level
23. Addresses are mapped into hCells by using a standard geocoder.
A geocoder is a piece of software that maps various geographic data
(e.g., address, ZIP code) to (latitude, longitude) pairs.
Table-Based Algorithm (Baseline)
We ﬁrst describe a table-based ranking algorithm. The algorithm uses a table BusinessListing, which contains informa-
1We have that hCellID1 ∈{0, . . . , 5} (face of the Earth) and
hCellIDl+1 = hCellIDl.s where s ∈{0, 1, 2, 3} tells us the order by which the Hilbert curve crosses the four smaller squares of
the hCellIDl.
tion about P. The schema of BusinessListing is (PlaceID,
Location, Category, Score), where PlaceID is an ID
of a place, Location is its location (expressed as an hCell23 identiﬁer), Category is the category to which it belongs (e.g., restaurant), and Score is the ofﬂine score assigned to this place (see
Figure 1), which can be the result of a combination of scores from
multiple scorers.
The table-based algorithm takes as input a subset of the tuples in
BusinessListing, namely the places that are within the range
the user is willing to travel. We use standard spatial indexes to
ﬁnd such tuples/places. From this subset of tuples, the algorithm
computes a place ranking by sorting the result tuples according to
their distance-weighted scores (online part of Figure 1). The pseudocode can be found as Algorithm 2 in Appendix C. The following
example illustrates the ranking.
EXAMPLE 1. Assume that we have a database with the businesses
in Table 1 and that Table 2 contains the information necessary for
the algorithm, computed from the query log.
Table 1: Example business directory
Petros’ place
Greek restaurant
Restaurant
Christian’s place
Danish restaurant
Restaurant
Hector’s place
Colombian restaurant
Restaurant
Alon’s place
Coffee shop
Restaurant
Jack’s place
American restaurant
Restaurant
Table 2: Statistics of interest for Example 1
Offline score
Distance (km)
Petros’ place
Christian’s place
Hector’s place
Alon’s place
Jack’s place
We assume that the query q is ⟨5.2. . . , 2009/Jun/15 14:23:24.412
PST, 2 km, restaurant⟩. The relevant set of places then excludes
Petros’ place, which is too far away (further than the 2 km that the
user is willing to travel). Assuming a simple linear weighting function (weightq.D(d) = 1 −d/D), the algorithm ranks the results
as follows: Christian’s place (score = (1−1.2/2.0)×700 = 280),
Alon’s place (score = (1 −1.0/2.0) × 500 = 250), Jack’s place
(score = (1 −1.2/2.0) × 550 = 220), Hector’s place (score =
(1 −1.5/2.0) × 300 = 75).
Although this naive algorithm is fast when the set of relevant
places is small (i.e., the distance the user is willing to travel is small
or the density of places is low), it rapidly degrades as we need to
examine more places. One reason is that the distance function call
is expensive.
Threshold-Based Algorithm
The algorithm described next adopts the approach of the threshold algorithm and is capable of handling a very large number
of places. This algorithm integrates well with current search engines that use space ﬁlling curves to index their geo content.
Supported scoring functions. The algorithm accepts any scoring
of places that is adjusted with a weighting function weightq.D(d)
that decreases monotonically with d, i.e., for di < dj we always
have weightq.D(di) ≥weightq.D(dj).
A simple example in our case would be a scoring function of
the form: S(p, D) = |{d| d ∈D ∧d.b = p.l}|, where d.b is the
destination for the directions query d and p.l is the location of the
The set of possible scorers is very general, and arbitrary scorers
can be combined to compute a query-independent score for each
place. At query time, the (ofﬂine) scores are adjusted according to
the query location.
Place ranking in hCells. We assume that the space has been partitioned into hCells at a given level. For each hCell, we precompute
a sorted list of places that map to it. This sorting is done ofﬂine according to S(p, D) as it is independent of the query location. Given
a preselected set of categories, we carry out this procedure for each
category (i.e., having places of the same category in each sorted
When receiving a user query that contains a location q.l and the
distance q.D the user is willing to travel, we select the hCells that
intersect the circle with center q.l and radius q.D. Assume that we
obtain n hCells and thus n ranked lists Li, i ∈{1, 2, . . . , n}, of
places. Algorithm 1 computes the top-k places from these lists and
returns them in priority queue L.
The algorithm maintains a priority queue PQ of the lists Li,
where the priority of a list is the best possible score that an unseen
place in the list can have. This number is easily computed from the
shortest distance from the user to the hCell of the list and the top
scoring unexamined place in the list. In each iteration, we pick the
most promising place from the list that is the head of PQ that we
(according to its ofﬂine score) have not yet examined and add it to
the results (L). The algorithm terminates early if the score of the
k-th element of the results L is higher than the maximum possible
(online) score of the ﬁrst element in the list at the head of the queue
This algorithm is an adaptation of the threshold algorithm (TA;
or more speciﬁcally NRA) . First, it does not rank objects for
which each list contains a separate dimension; rather, each list contains completely different objects. Second, the score for each place
in each list is adjusted dynamically according to its distance to the
The following theorems establish the main properties of our algorithm: the algorithm correctly ﬁnds the top-k list of places; and
for a particular level of hCells, it examines the minimum number
of places possible.
THEOREM 1. Algorithm 1 ﬁnds the correct top-k list if function
weightq.D(·) is non-increasing, i.e., if for every di ≥dj we have
weightq.D(di) ≤weightq.D(dj).
THEOREM 2. Algorithm 1 ﬁnds the top-k list in the minimum
number of steps for a given partition of space (and only sorted
access to our lists Li).
The proofs, in Appendix D, adapt proofs of similar properties by
Fagin et al. , to our setting and assumptions.
In the experimental section, we show that it is possible to keep
the ranked lists according to the ofﬂine score for only one particular
level of the hCells; this yields good results in all possible scenarios.
EXAMPLE 2. We continue with Example 1. Assume that the businesses are located as shown in Figure 3. We use level 2 hCells, and
aim to ﬁnd the top-3 results.
Initially PQ and L are empty. We have three lists as input to the
algorithm: L0, L1, and L3. We know that the minimum distance
between the user and the hCells are: m0 = 0.5 km, m1 = 0.4 km,
and m3 = 0.3 km.
2In our case, the scoring function in the right part of the equation
depends on the query logs; thus, it is S(p, D). In general, any
function that does not depend on the user’s query q can be used.
Algorithm 1: Threshold-based algorithm
Input: A query q = ⟨l, t, D, ¯q⟩, lists (L1, . . . , Ln), minimum
distances for every list’s hCell (m1, . . . , mn), k
Output: List L with all the k highest ranked places according to user
activeLists ←{1, 2, . . . , n};
/* Priority queues (PQ and L) have 2
operations:
getNext() returns the next
element from the queue and pops the element;
insert(element, priority) inserts an element into
the queue with a specific priority
/* lists Li have 3 operations:
returns the next element in the list and
removes the element; pollNext() returns the
next element without removing the element
from the list; hasMore() returns true iff
the list is non-empty
for i = 1 to n do
/* The prioritization is over the maximum
possible score achieved for that list
p = Li.pollNext();
PQ.insert(i, S(p, D) × weightq.D(mi));
while activeLists.size() > 0 do
nextListToCheck ←PQ.getNext();
if nextListToCheck /∈activeLists then
if LnextListT oCheck.hasMore() then
p = LnextListT oCheck.getNext();
/* notice that we are using the exact
if ||p.l, q.l|| ≤q.D then
L.insert(p, S(p, D) × weight(||p.l, q.l||));
if LnextListT oCheck.hasMore() then
p = LnextListT oCheck.pollNext();
PQ.insert(nextListToCheck, S(p, D) ×
weight(mnextListT oCheck));
activeLists.remove(nextListToCheck);
if the k-th element in L has a score greater than the ﬁrst list in
PQ maximum expected score then
The algorithm ﬁrst initializes priority queue PQ. Taking into account the minimum distances, the lists, and the simple linear weight
function from Example 1, we have:
⟨(L1, (1 −0.5
2.0) · 1000), (L3, (1 −0.3
2.0) · 550)
(L0, (1 −0.4
2.0) · 500)⟩
⟨(L1, 750), (L3, 467.5), (L0, 375)⟩.
Since the hCell with ID 1 has the highest priority in PQ, it is
examined ﬁrst. List L1 contains Petros’ place ﬁrst, which has a distance from user’s location that exceeds q.D. Thus it is ignored, but
the priority queue is updated. Since Christian’s place is next in L1,
after this step, PQ will be ⟨(L1, 560), (L3, 467.5), (L0, 375)⟩.
Again, list L1 is examined as it has the highest priority in PQ.
Christian’s place is added to priority queue L, and L = ⟨(C, 280)⟩.
Since we extracted list L1 from PQ, we have PQ = ⟨(L3, 467.5),
(L0, 375)⟩.
We then examine list L0, since it is next in PQ. After one iteration of the main while loop, we get PQ = ⟨(L0, 375)⟩and
L = ⟨(C, 280), (A, 220)⟩.
L0 = ⟨(J, 500), (H, 300)⟩
L1 = ⟨(P, 1000), (C, 700)⟩
L3 = ⟨(A, 550)⟩
Figure 3: Threshold-based algorithm example
We now examine list L0 since it is next in PQ. After the iteration, we get L = ⟨(C, 280), (J, 250), (A, 220)⟩and PQ =
⟨(L0, 150)⟩.
We now have three elements in priority queue L.
Since the last element has an actual score that is greater than the
maximum possible score of any remaining element in list L0, we
have found the top-3 elements and are done.
DIRECTIONS LOGS EVALUATION
We now describe key characteristics of our data sources (more
details can be found in Appendix E) and show that the directions
log can be a very effective signal for place ranking. Additional
experiments can be found in Appendix F.
Key Data Source Statistics
We use a sample of queries from a travel directions log that contains all the queries issued at Google Maps during July 2009. We
focus on queries in a sub-region of the United States and thus consider the business listings located in this sub-region. We limit the
set of business listings to a subset of categories including museums,
hotels, restaurants, night clubs, and landmarks.
The query log that we use (D) contains queries for 18,968,123
different locations. For the categories of interest we have 151,721
business listings (P). It is clear that most of the destinations of
driving directions queries are not businesses in the directory.
We perform a join between the destination query log and the
business directory, on the log destination and the business location attributes. There are 128,159 businesses for which there is at
least one query with a destination matching the business location.
That is, around 84.47% of all business listings in the directory are
queried in the driving directions queries; 0.676% of driving directions queries in D have a business in P as their destination. Although, 0.676% is a small percentage, the actual number of queries
issued for the 128,159 businesses is 49,533,223; this number is almost two orders of magnitude greater than the number of reviews
for the same places (which was around 550,000). Interestingly,
22.5% of the places for which we have directions queries do not
have any reviews, showing that the abundance of directions queries
offers additional coverage.
In the business directory available, there were multiple businesses located in the same location (hCell23). This happens because the business directory merges several data sources and then
performs an imperfect Entity Resolution (ER). Furthermore, the
information extracted from web pages is not always completely accurate. The problem of ER in the business listing is orthogonal to
our study (see Appendix E.3 for additional details).
Figure 4 shows the distribution of the queries across locations
for the locations that “survived” the join between D and P, i.e.,
locations in which at least one business is located and for which at
least one directions query has the location as its destination. For
example, in the lower right corner of the ﬁgure, we can observe
that there is a location that has received ∼8 × 106 queries. In the
top left corner of the ﬁgure, we can see that there are around 8,000
locations that have received 1 query each.
Number of locations (log axis)
Number of queries (log axis)
Figure 4: Distribution of queries across locations
Utility of Directions Logs
This experiment investigates the utility of the directions logs signal. We compare the sets of the top-k results produced and the
rankings of the results with the corresponding sets and ranking produced by other methods.
We have created a system where a user is able to select a location the user knows very well and then select a category among the
following three: restaurant, point of interest, and hotel. Finally, the
user selects a maximum travel distance. The system then computes
the top-5 places that three methods produce:
1. DL: number of directions log entries that have the place as
their destination,
2. NR: number of reviews for the place under consideration, and
3. AR: the average score (sentiment) that reviews have assigned
to the place under consideration.
Finally, the resulting 15 places (top-5 from each method) are
shufﬂed and shown to the user, without the user knowing the origin of each place3. We did not incorporate distance between the
user and a place since we wanted to compare the usefulness of our
(ofﬂine) signal against two other standard signals. The user is then
asked to evaluate these 15 places with a score between 0 and 4:
Speciﬁcation
I have no idea of what this is
Not interesting to most people in general and not recommended
Neutral to most people in general
An OK location to most people in general
Very interesting to most people in general and recommended
Since users are experts in the areas they selected (e.g., around their
houses or work), their assessments are considered to be of high
We had 10 users select locations, issue queries, and evaluate 15
places for each query. A total of 45 queries and 675 places were
evaluated during this process.
We compared the average score that the evaluators assigned to
the top-5 places of each method: our proposed signal (DL) had an
average score of 1.960, while AR had 1.498, and NR had 1.453.
This means that our signal is better at reporting the best places in
its top-5 list than the other two signals.
We also evaluated the rankings produced by each method using
the nDCG metric . This metric can be deﬁned for any list of
evaluated objects; it compares the ranking a method has done (list
of objects) to an optimal list of objects according to the evaluation
scores. We evaluated nDCG for the top-5 places of each of the
methods we examined. The metric takes values between 0 and 1: 1
means that the ranking is as expected according to the evaluations,
3If there are duplicates among the top-5 lists, we continue deeper
into the lists so that we always show 15 places.
and 0 means that the ranking is very bad. DL had nDCG equal to
0.787, AR had 0.845, and NR had 0.827.
This study shows that driving directions logs can serve as a strong
signal, on par with reviews, for place ranking. This is an important
ﬁnding because log data offer a number of advantages over reviews,
as mentioned in Section 4.3. The fact that the directions-based signal is comparable to the review-based signals is surprising. It is up
to a scorer that takes into account multiple signals to decide how
the signals should be combined based on their characteristics.
Correlation with Number of Reviews
In this experiment, we evaluate the feasibility of using driving
directions logs as a proxy for the popularity/importance of a place.
We compare the correlation of the driving directions based signal
with the number of reviews for a place, which is a commonly accepted measure of popularity. The number of reviews was extracted
via Google’s business directory, and it is the total number of reviews found in various data sources on the Web.
For this experiment, we choose 100 random user locations. We
then issue the category query “food” and set the distance the user
is willing to drive to 2 km. Each of the 100 queries has at least 100
ranked results (otherwise, we choose a new random user location).
We had to consider 514 distinct places to ﬁnd 100 user locations
with at least 100 food-related places within a radius of 2 km (∼20%
of the randomly selected places had at least 100 results).
We then ﬁnd the number of reviews of each ranked place for
all the queries. We partition the ranked results into batches of ten
results (the top-10, the top-11–20, etc.). For each batch, we sum the
number of reviews for the places ranked in those places for the 100
rankings that we have; the result can be seen in Figure 5. There
is a clear correlation between the rankings of the results and the
number of reviews that a place has, which shows that the directions
logs are indicative of the popularity of a place.
These ﬁndings are important because driving directions logs are
cheap to collect and are orders of magnitude more frequent than
user reviews, which are expensive to obtain. Further, the logs provide near real-time evidence of changing sentiment, an aspect that
is usually hard to capture with other signals (e.g., the reviews that
a place has received; even a newly added web page of a restaurant
will need time to increase its PageRank), and they are available for
broader types of locations.
The scoring function we used for this experiment is S(p, D) =
|{d = ⟨t, a, p.l, ||a, p.l||⟩| d ∈D}| and weightq.D(d) = 1 −
However, similar results are obtained when using other scoring and
weighting functions (like the ones described in Appendix A).
PERFORMANCE EVALUATION
We now proceed to report on the evaluation of the runtime performance of the proposed table-based and threshold-based ranking
algorithms in the presence of very large log and place databases.
In the experiments, we assume that the user issues an empty
query string (retrieve all possible places around me) in order to
have more results to rank. Also, the user is located in a region with
high business density.
We use the same datasets as described in Section 4.1, and we run
all the experiments on a single machine with two AMD dual-core
Opteron CPUs (we only use one of the cores) at 2.2 GHz and with
8 GB of RAM.
For the table-based algorithm, we use MySQL as the DBMS with
indexing for efﬁciently determining the places near a user. For the
threshold-based algorithm, we load the ranked lists from the ofﬂine
computation into memory.
0 10 20 30 40 50 60 70 80 90 100
Number of reviews for the batch
Midpoint of the batch
Figure 5: Number of reviews for our ranked list of results
Table- vs. Threshold-Based Algorithm
For the threshold-based algorithm we use hCells at level 7 (for
reasons that will be clear later on) and compute the top-100,000
Figure 6 presents the ranking time vs. the distance that the user is
willing to travel. It can be observed that the table-based algorithm
Elapsed time (msecs)
Distance (kms)
Threshold method
Figure 6: Performance comparison for our two algorithms
(baseline) takes up to 9.5 secs for large distances (which translates
into a lot of places), while the threshold-based (efﬁcient) algorithm
takes around 350 msecs in the worst case.
Varying the hCell Level
In this experiment, we vary the hCell level from 3 to 23. We
start at 3 because the areas the query logs cover are at this level.
We measure the time required to ﬁnd the top-1,000 results. The
measurements can be seen in Figure 7 for varying distances (q.D)
that the user is willing to travel and for all hCell levels.
Elapsed time (msecs)
hCell Level
q.D = 1 kms
q.D = 2 kms
q.D = 4 kms
q.D = 8 kms
q.D = 16 kms
q.D = 32 kms
q.D = 64 kms
q.D = 128 kms
q.D = 256 kms
q.D = 512 kms
Figure 7: Elapsed time vs. hCell level
We observe that the higher the hCell level (i.e., the smaller our
hCells are), the more time is required for our computation for all
distances d. This is so because we have to handle more ranked
lists of results, which makes the handling of the priority queue PQ
We also see that for small distances (like q.D = 1 km and
q.D = 2 km), the elapsed time for ranking increases signiﬁcantly
if one uses a very low hCell level. This happens because we have
to process large lists in order to ﬁnd results that belong to the small
region of interest to the user. Put differently, we have to consider
places that lie in relatively large regions, and that will eventually be
ﬁltered given the user’s low willingness to travel.
It seems that a very good trade-off for all possible distances is to
use hCell levels 6 or 7. Thus, a search engine can keep statistics for
only one of these two hCell levels and not for every possible level,
thus achieving important savings in storage and computation.
Varying the Value of k
Next, we evaluate the time required to ﬁnd the top-k places for a
ﬁxed maximum travel distance and different values of k. We chose
the distance q.D = 512 km, for a user located in a region with high
business density. This setting maximizes the number of places to
rank, thus imposing a greater burden on the ranking algorithm. We
experiment with hCell level 7, since the previous experiment shows
that the threshold-based algorithm performs very well at this level.
Figure 8 presents the time required to compute the top-k lists
for several values of k. As expected, the total time increases with
k. We note that the process is incremental, meaning that we can
ﬁrst ﬁnd the top-10 results and then, using the same data structures,
continue running the algorithm up to the desired level of k.
Elapsed time (msecs)
hCell Level
k = 100000
Figure 8: Time required to ﬁnd the top-k results for varying
values for k
Performance of Ofﬂine Procedures
For this experiment, we measure the storage and time requirements of the ofﬂine part of our two algorithms. The MySQL table
BusinessListing is around 89 MB for the table-based algorithm (it contains some metadata about the places); the total memory required for the creation of all the ranked lists (based on the
ofﬂine score) is around 100.7 MB for all possible levels we have
considered.
Both algorithms need to compute the ofﬂine score; the additional
cost that the threshold-based algorithm imposes is that one must
create ranked lists for each hCell beforehand, which is not required
in the table-based algorithm. This additional cost (when everything
run on one machine) is around 25–26 secs for hCell levels 3–8 and
then steadily increases to 28.3 secs for level 9, 31.7 secs for level
10, 38.8 secs for level 11, ..., 329.5 secs for level 23.
We can see that for low levels, the additional ofﬂine time is insigniﬁcant; as the level of detail increases, the overhead also increases. However, the cost is not prohibitive for our method, especially when we are at levels 6 and 7 where, as we saw earlier,
we achieve the best performance. Also, this computation is done
ofﬂine and its cost will be amortized across millions of queries.
RELATED WORK
Finding interesting locations and/or destinations: A number of
studies have aimed to identify important locations using primarily
GPS data. Our setting assumes that we know the important locations and that they are classiﬁed (e.g., restaurants, hotels); this
information is found in business directories.
In the location identiﬁcation setting, several studies aim to identify places visited by individuals from GPS logs. Ashbrook and
Starner present techniques capable of learning signiﬁcant
user locations and predicting user movement based on GPS data.
Brilingaite et al. capture routes and (start and end) destinations from GPS trajectories and use these along with temporal usage information for predicting the routes and destinations of users.
Experiments with data from users demonstrate that the techniques
are effective. Krumm and Horvitz also propose techniques
for destination prediction based on GPS data.
Liu et al. and Cao et al. extract so-called stay points
from the trajectories of users; they propose clustering and reverse
geocoding techniques that aim to derive semantically meaningful
locations from the stay points. Zheng and Xie also identify
stays from GPS logs, propose a hierarchical containment ordering
for the geographical extents of stays, and then mine so-called life
patterns of individuals from the resulting location histories. Along
the same lines Zheng et al. , mine locations from GPS trajectories. Here, focus is on identifying public locations, which are then
ranked using link-based techniques. Cao et al. offer improved
link-based ranking techniques.
Our approach can use the results from this line of research as
another source of places (instead of a business directory).
approach does not use GPS data and link-based techniques for its
ranking, but considers a different alternative: that of using driving
directions ranking. GPS datasets typically contain many positions
for few users; in contrast, directions queries are derived from larger
user populations, with each user issuing few queries.
Ranking functions in a Spatial Context: Other related work deals
with the problem of place ranking , where the ranking of objects (e.g., houses) is deﬁned with respect to other qualities around
them (e.g., restaurants, schools, hospitals) within a distance range.
Also, a framework for the efﬁcient computations of this ranking is
provided. These methods do not try to determine the inherent value
of a place; they determine the value of a place given the fact that
other places are around it. Thus, one could apply our algorithms
and then, as a next step, the algorithms described in .
In a more general context, substantial research has considered
local search where the results that a user sees for a query depend
on the user’s location; the results though are documents and not
ranked places/businesses (for example, one result could be a blog
post about a particular restaurant), which makes the setting very
different. Two recent works in this ﬁeld propose indexes that are
evaluated in terms of efﬁciently in their settings .
In other work , NLP techniques are used for geo-searches on
the web. The idea is to ﬁnd phrases like “next to Eiffel tower” in
a hotel’s web document and to thus be able to answer queries like
“hotel in Paris” more accurately. Again the output is ranked documents, not ranked places. Our work differs from other techniques
in that no geocoding is used, but only NLP techniques. Means of
detecting and extracting geographical information (e.g., addresses)
from web documents has also been studied . The idea is to
extract addresses, phones, ZIP codes and other useful geographical
information from a web document and then to augment the location
information with keywords from the content of the web document.
Query log analysis: Backstrom et al. aim to determine the region of importance for a query, using a probabilistic model. This
work does not use positioning, but rather depends on IP-based (10×
10 km2) positioning. We try to determine good places, which is different from trying to determine the region to which a query is relevant. Other work in this line of research aims to determine the
so-called dominant location for each query (the location most important for a particular query). The techniques used include query
tokenization, query log analysis, and exploration of the snippets of
search results for the top-k results.
Work on mobile search log analysis aims to determine how
users search using mobile devices. Some of the explored user behaviors are the number of keywords in queries, number of queries
per user per day, and search topics.
Finally, there is some work on categorization of queries depending on their scope (local, neighborhood, etc.) . Other work
in this area explores the distances that one is willing to travel
in order to get to a particular place. This work can be used in our
framework to automatically ﬁnd a maximum threshold for travel
distance for a given query.
TA/NRA comparison: The threshold-based algorithm proposed
in Section 3.3 is an adaptation of the threshold algorithm (TA) and
more speciﬁcally, a variation of TA called “no random accesses”
(NRA) . Both TA and NRA rank objects. Each object has m
scores for m different attributes. The overall score of an object
is determined by a combination of the scores for the m attributes,
using some monotone aggregation function. For each of the m attributes there is a ranked list of the objects according to their score
for that attribute. TA and NRA ﬁnd the top-k objects according to
the overall score. TA and NRA differ in that NRA assumes sorted
access to the ranked lists, i.e., to see the object in the lth position
of a sorted list, one must have seen all l −1 objects before it. Our
algorithm differs from NRA in that each list contains different objects in our setting, not a ranking of the same objects for a different
attribute. In addition, in our setting, the score for each place in
each list is adjusted dynamically according to its distance to the
user, which we believe has not been explored in any other work.
CONCLUSIONS AND DIRECTIONS
Given the ability to accurately determine the location of a mobile user and the obvious revenue possibilities involved, it is likely
that answering hyper-local queries will receive increasing attention. Our ﬁrst contribution is to present a new signal, directions
logs, that can be used for scoring places in response to hyper-local
queries. We present key statistics about these logs and show that direction queries have a good correlation with the number of reviews
for places. But unlike reviews and ratings, they are inexpensive to
collect and readily available.
We present an architecture for answering hyper-local queries that
is the ﬁrst to take into account the distance between a user and a
place and to adjust the score of the place accordingly. To support
this framework, we propose a threshold-based algorithm that scales
very well with the number of places under consideration. This algorithm returns exact rankings under assumptions that are reasonable
in practice.
In terms of future research, we see interesting issues in exploring additional scoring functions and methods for combining these
functions. In particular, some scoring functions may be more appropriate than others for speciﬁc types of user queries. Another
interesting aspect of our work is the personalization of the ranking
depending on a user’s speciﬁc search history.
Acknowledgments. Christian S. Jensen is an Adjunct Professor
at University of Agder, Norway.