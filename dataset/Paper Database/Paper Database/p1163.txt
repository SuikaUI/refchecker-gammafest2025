Hypercolumns for Object Segmentation and Fine-grained Localization
Bharath Hariharan
University of California
 
Pablo Arbel´aez
Universidad de los Andes
 
Ross Girshick
Microsoft Research
 
Jitendra Malik
University of California
 
Recognition algorithms based on convolutional networks
(CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer
may be too coarse spatially to allow precise localization.
On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both
worlds, we deﬁne the hypercolumn at a pixel as the vector
of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three
ﬁne-grained localization tasks: simultaneous detection and
segmentation , where we improve state-of-the-art from
49.7 mean APr to 60.0, keypoint localization, where
we get a 3.3 point boost over , and part labeling, where
we show a 6.6 point gain over a strong baseline.
1. Introduction
Features based on convolutional networks (CNNs) 
have now led to the best results on a range of vision tasks:
image classiﬁcation , object segmentation and detection , action classiﬁcation , pose estimation and ﬁne-grained category recognition . We
have thus moved from the era of HOG and SIFT to the era
of convolutional network features. Therefore, understanding these features and how best to exploit them is of wide
applicability.
Typically, recognition algorithms use the output of the
last layer of the CNN. This makes sense when the task is
assigning category labels to images or bounding boxes: the
last layer is the most sensitive to category-level semantic
information and the most invariant to “nuisance” variables
such as pose, illumination, articulation, precise location and
so on. However, when the task we are interested in is ﬁnergrained, such as one of segmenting the detected object or
estimating its pose, these nuisance variables are precisely
what we are interested in. For such applications, the top
layer is thus not the optimal representation.
The information that is generalized over in the top layer
is present in intermediate layers, but intermediate layers are
also much less sensitive to semantics. For instance, bar detectors in early layers might localize bars precisely, but cannot discriminate between bars that are horse legs and bars
that are tree trunks. This observation suggests that reasoning at multiple levels of abstraction and scale is necessary,
mirroring other problems in computer vision where reasoning across multiple levels has proven beneﬁcial. For example, in optical ﬂow, coarse levels of the image pyramid are
good for correspondence, but ﬁner levels are needed for accurate measurement, and a multiscale strategy is used to get
the best of both worlds .
In this paper, we think of the layers of a convolutional
network as a non-linear counterpart of the image pyramids
used in optical ﬂow and other vision tasks. Our hypothesis is that the information of interest is distributed over
all levels of the CNN and should be exploited in this way.
We deﬁne the “hypercolumn” at a given input location as
the outputs of all units above that location at all layers of
the CNN, stacked into one vector. (Because adjacent layers are strongly correlated, in practice we need not consider
all layers but can simply sample a few.) Figure 1 shows a
visualization of the idea. We borrow the term “hypercolumn” from neuroscience, where it is used to describe a set
of V1 neurons sensitive to edges at multiple orientations and
multiple frequencies arranged in a columnar structure .
However, our hypercolumn includes not just edge detectors
but also more semantic units and is thus a more general notion.
We show the utility of the hypercolumn representation
 
Convolutional
Hypercolumn
Figure 1. The hypercolumn representation. The bottom image is
the input, and above it are the feature maps of different layers in
the CNN. The hypercolumn at a pixel is the vector of activations
of all units that lie above that pixel.
on two kinds of problems that require precise localization.
The ﬁrst problem is simultaneous detection and segmentation (SDS) , where the aim is to both detect and segment every instance of an object category in the image. The
second problem deals with detecting an object and localizing its parts. We consider two variants of this: one, locating the keypoints , and two, segmenting out each
part .
We present a general framework for tackling these and
other ﬁne-grained localization tasks by framing them as
pixel classiﬁcation and using hypercolumns as pixel descriptors. We formulate our entire system as a neural network, allowing end-to-end training for particular tasks simply by changing the target labels. Our empirical results are:
1. On SDS, the previous state-of-the-art was 49.7 mean
APr . Substituting hypercolumns into the pipeline
of improves this to 52.8. We also propose a more
efﬁcient pipeline that allows us to use a larger network,
pushing up the performance to 60.0.
2. On keypoint prediction, we show that a simple keypoint prediction scheme using hypercolumns achieves
a 3.3 point gain in the APK metric over prior approaches working with only the top layer features .
While there isn’t much prior work on labeling parts of
objects, we show that the hypercolumn framework is
signiﬁcantly better (by 6.6 points on average) than a
strong baseline based on the top layer features.
2. Related work
Combining features across multiple levels:
Adelson introduced Laplacian pyramids , a representation that is widely used in computer vision. Koenderink
and van Doorn used “jets”, which are sets of partial
derivatives of intensity up to a particular order, to estimate
edge orientation, curvature, etc. Malik and Perona used
the output of a bank of ﬁlters as a representation for texture
discrimination. This representation also proved useful for
optical ﬂow and stereo . While the ﬁlter banks in
these works cover multiple scales, they are still restricted
to simple linear ﬁlters, whereas many of the features in the
hypercolumn representation are highly non-linear functions
of the image.
There has also been work in convolutional networks that
combines multiple levels of abstraction and scale. Farabet
et al. combine CNN outputs from multiple scales of
an image to do semantic segmentation. Tompson et al. 
use a similar idea for detecting parts and estimating pose.
However, the features being combined still come from the
same level of the CNN and hence have similar invariance.
Sermanet et al. combine subsampled intermediate layers with the top layer for pedestrian detection. In contrast,
since we aim for precise localization, we maintain the high
resolution of the lower layers and upsample the higher layers instead. In contemporary work, Long et al. also use
multiple layers for their fully convolutional semantic segmentation system.
Detection and segmentation:
The task of simultaneous
detection and segmentation task, introduced in , requires one to detect and segment every instance of a category in the image. SDS differs from classical bounding
box detection in its requirement of a segmentation and from
classical semantic segmentation in its requirement of separate instances. There has been other prior work on segmenting out instances of a category, mostly starting from
bounding box detections. Borenstein and Ullman ﬁrst
suggested the idea of using class-speciﬁc knowledge for
segmentation. Yang et al. use ﬁgure ground masks associated with DPM detectors to segment out detected
objects and reason about depth orderings. Parkhi et al. 
use color models extracted from the detected cat and dog
heads to segment them out. Dai and Hoiem generalize this reasoning to all categories. Fidler et al. and
Dong et al. combine object detections from DPM 
with semantic segmentation outputs from O2P to improve both systems. Current leading methods use CNNs
to score bottom-up object proposals, both for object detection and for SDS .
Pose estimation and part labeling: Current best performers for pose estimation are based on CNNs. Toshev and
Szegedy use a CNN to regress to keypoint locations.
Tompson et al. show large improvements over stateof-the-art by predicting a heatmap for each keypoint, where
the value of the heatmap at a location is the probability of
the keypoint at that location. These algorithms show results in the setting where the rough location of the person
is known. Yang and Ramanan propose a more realistic
setting where the location of the person is not known and
one has to both detect the person and identify his/her keypoints. Gkioxari et al. show some results in this setting using HOG-based detectors, but in their later work 
show large gains using CNNs.
Related to pose estimation is the task of segmenting out
the different parts of a person, a task typically called “object parsing”. Yamaguchi et al. parse fashion photographs into clothing items. There has also been work on
parsing pedestrians . Ionescu et al. jointly infer
part segmentations and pose. However, the setting is typically tightly cropped bounding boxes of pedestrians, while
we are interested in the completely unconstrained case.
3. Pixel classiﬁcation using hypercolumns
Problem setting: We assume an object detection system
that gives us a set of detections. Each detection comes with
a bounding box, a category label and a score (and sometimes an initial segmentation hypothesis). The detections
have already been subjected to non-maximum suppression.
For every detection, we want to segment out the object, segment its parts or predict its keypoints.
For each task, we expand the bounding box of the detection slightly and predict a heatmap on this expanded box.
The type of information encoded by this heatmap depends
on the particular task. For segmentation, the heatmap encodes the probability that a particular location is inside the
object. For part labeling, we predict a separate heatmap
for each part, where each heatmap is the probability a location belongs to that part. For keypoint prediction, again
we output a separate heatmap for each keypoint, with each
heatmap encoding the probability that the keypoint is at a
particular location.
In each case, we predict a 50×50 heatmap that we resize
to the size of the expanded bounding box and splat onto the
image. Thus, in our framework, these diverse ﬁne-grained
localization problems are addressed as the uniﬁed task of
assigning a probability to each of the 50 × 50 locations
or, in other words, of classifying each location. We solve
this classiﬁcation problem using the hypercolumn representation as described in detail below.
Computing the hypercolumn representation: We take
the cropped bounding box, resize it to a ﬁxed size and feed
it into a CNN as in . For each location, we extract features from a set of layers by taking the outputs of the units
that are “above” the location (as shown in Figure 1). All the
intermediate outputs in a CNN are feature maps (the output
of a fully connected layer can be seen as a 1 × 1 feature
map). However, because of subsampling and pooling operations in the CNN, these feature maps need not be at the
same resolution as the input or the target output size. So
which unit lies above a particular location is ambiguous.
We get around this by simply resizing each feature map to
the size we want with bilinear interpolation. If we denote
the feature map by F and the upsampled feature map by f,
then the feature vector for the ith location has the form:
αik depends on the position of i and k in the box and feature
map respectively.
We concatenate features from some or all of the feature
maps in the network into one long vector for every location which we call the hypercolumn at that location. As an
example, using pool2 (256 channels), conv4 (384 channels)
and fc7 (4096 channels) from the architecture of would
lead to a 4736 dimensional vector.
Interpolating into a grid of classiﬁers: Because these feature maps are the result of convolutions and poolings, they
do not encode any information about where in the bounding
box a given pixel lies. However, location can be an important feature. For instance, in a person bounding box, the
head is more likely to be at the top of the bounding box than
at the bottom. Thus a pixel that looks like a nose should be
considered as part of the person if it occurs at the top of the
box and should be classiﬁed as background otherwise. The
reasoning should be the opposite for a foot-like pixel. This
is a highly non-linear effect of location, and such reasoning cannot be achieved simply by a location-speciﬁc bias.
(Indeed, our classiﬁers include (x, y) as features but assign
negligible weight to them). Such reasoning requires different classiﬁers for each location.
Location is also needed to make better use of the features from the fully connected layers at the top. Since these
features are shared by all the locations in the bounding box,
they can at best contribute a global instance-speciﬁc bias.
However, with a different classiﬁer at each location, we
can have a separate instance-speciﬁc bias for each location. Thus location-speciﬁc classiﬁers in conjunction with
the global, instance-level features from the fully connected
layer produce an instance-speciﬁc prior.
The simplest way to get a location-speciﬁc classiﬁer is to
train separate classiﬁers for each of the 50 × 50 locations.
However, doing so has three problems. One, it dramatically
reduces the amount of data each classiﬁer sees during training. In our training sets, some categories may have only a
few hundred instances, while the dimensionality of the feature vector is of the order of several thousand. Thus, having
fewer parameters and more sharing of data is necessary to
prevent overﬁtting. Two, training this many classiﬁers is
computationally expensive, since we will have to train 2500
classiﬁers for 20 categories. Three, while we do want the
classiﬁer to vary with location, the classiﬁer should change
slowly: two adjacent pixels that are similar to each other in
appearance should also be classiﬁed similarly.
Our solution is to train a coarse K × K grid of classi-
ﬁers and interpolate between them. In our experiments we
use K = 5 or 10. For the interpolation, we use an extension of bilinear interpolation where we interpolate a grid of
functions instead of a grid of values. Concretely, each classiﬁer in the grid is a function gk(·) that takes in a feature
vector and outputs a probability between 0 and 1. We use
this coarse grid of functions to deﬁne the function hi at each
pixel i as a linear combination of the nearby grid functions,
analogous to Equation 1:
If the feature vector at the ith pixel is fi, then the score
of the ith pixel is:
αikgk(fi) =
where pik is the probability output by the kth classiﬁer for
the ith pixel. Thus, at test time we run all our K2 classiﬁers
on all the pixels. Then, at each pixel, we linearly combine
the outputs of all classiﬁers at that pixel using the above
equation to produce the ﬁnal prediction. Note that the coef-
ﬁcients of the linear combination depend on the location.
Training this interpolated classiﬁer is a hard optimization
problem. We use a simple heuristic and ignore the interpolation at train time, using it only at test time.We divide each
training bounding box into a K × K grid. The training
data for the kth classiﬁer consists only of pixels from the
kth grid cell across all training instances. Each classiﬁer is
trained using logistic regression. This training methodology does not directly optimize the loss we would encounter
at test time, but allows us to use off-the-shelf code such as
liblinear to train the logistic regressor.
Efﬁcient classiﬁcation using convolutions and upsampling: Our system requires us to resize every feature map
to 50 × 50 and then classify each location. But resizing
feature maps with hundreds of channels can be expensive.
However, we know we are going to run several linear classiﬁers on top of the hypercolumn features and we can use
this knowledge to save computation as follows: each feature
map with c channels will give rise to a c-dimensional block
of features in the hypercolumn representation of a location,
and this block will have a corresponding block of weights
in the classiﬁers. Thus if fi is the feature vector at location
i, then fi will be composed of blocks f (j)
corresponding to
the jth feature map. A linear classiﬁer w will decompose
similarly. The dot product between w and fi can then be
written as:
w(j)T f (j)
The jth term in the decomposition corresponds to a linear classiﬁer on top of the upsampled jth feature map. However, since the upsampling is a linear operation, we can ﬁrst
apply the classiﬁer and then upsample using Equation 1:
w(j)T f (j)
ik w(j)T F(j)
We note that this insight was also used by Barron et al. 
in their volumetric semantic segmentation system.
Observe that applying a classiﬁer to each location in a
feature map is the same as a 1×1 convolution. Thus, to run
a linear classiﬁer on top of hypercolumn features, we break
it into blocks corresponding to each feature map, run 1 × 1
convolutions on each feature map to produce score maps,
upsample all score maps to the target resolution, and sum.
We consider a further modiﬁcation to this pipeline where
we replace the 1 × 1 convolution with a general n × n convolution. This corresponds to looking not only at the unit
directly above a pixel but also the neighborhood of the unit.
This captures the pattern of activations of a whole neighborhood, which can be more informative than a single unit,
especially in the lower layers of the network.
Representation as a neural network:
We can write our
ﬁnal hypercolumn classiﬁer using additional layers grafted
onto the original CNN as shown in Figure 2. For each feature map, we stack on an additional convolutional layer.
Each such convolutional layer has K2 channels, corresponding to the K2 classiﬁers we want to train. We can
choose any kernel size for the convolutions as described
above, although for fully connected layers that produce 1×1
feature maps, we are restricted to 1 × 1 convolutions. We
take the outputs of all these layers, upsample them using
bilinear interpolation and sum them. Finally, we pass these
outputs through a sigmoid, and combine the K2 heatmaps
using equation 3 to give our ﬁnal output. Each operation is
differentiable and can be back-propagated over.
Representing our pipeline as a neural network allows us
to train the whole network (including the CNN from which
we extract features) for this task. For such training, we feed
in the target 50 × 50 heatmap as a label. The loss is the
sum of logistic losses (or equivalently, the sum of the negative log likelihoods) over all the 50 × 50 locations. We
found that treating the sigmoids, the linear combination and
the log likelihood as a single composite function and computing the gradient with respect to that led to simpler, more
numerically stable expressions. Instead of training the network from scratch, we use a pretrained network and ﬁnetune, i.e., do backpropagation with a small learning rate.
Finally, this representation as a neural network also allows
us to train the grid classiﬁers together and use classiﬁer inconv
classifier
interpolation
Figure 2. Representing our hypercolumn classiﬁers as a neural network. Layers of the original classiﬁcation CNN are shown in red,
and layers that we add are in blue.
terpolation during training, instead of training separate grid
classiﬁers independent of each other.
Training classiﬁers for segmentation and part localization: For each category we take bottom-up MCG candidates that overlap a ground truth instance by 70% or
more. For each such candidate, we ﬁnd the ground truth
instance it overlaps most with, and crop that ground truth
instance to the expanded bounding box of the candidate.
Depending on the task we are interested in (SDS, keypoint
prediction or part labeling), we then use the labeling of the
cropped ground truth instance to label locations in the expanded bounding box as positive or negative. For SDS, locations inside the instance are considered positive, while locations outside are considered negative. For part labeling,
locations inside a part are positive and all other locations
are negative. For keypoint prediction, the true keypoint location is positive and locations outside a certain radius (we
use 10% of the bounding box diagonal) of the true location
are labeled negative.
4. Experiments on SDS
Our ﬁrst testbed is the SDS task. Our baseline for this
task is the algorithm presented in . This pipeline scores
bottom-up region proposals from using CNN features
computed on both the cropped bounding box of the region
and the cropped region foreground. The regions are subjected to non-max suppression. Finally, the surviving candidates are reﬁned using ﬁgure-ground predictions based on
the top layer features.
As our ﬁrst system for SDS, we use the same pipeline
as above, but replace the reﬁnement step with one based
on hypercolumns. (We also add a bounding box regression
step so as to start from the best available bounding
box). We present results with this pipeline in section 4.1,
where we show that hypercolumn-based reﬁnement is signiﬁcantly better than the reﬁnement in , and is especially accurate when it comes to capturing ﬁne details of
the segmentation. We also evaluate several ablations of our
system to unpack this performance gain. For ease of reference, we call this System 1.
One issue with this system is its computational cost. Extracting features from region foregrounds is expensive and
doubles the time taken. Further, while CNN-based bounding box detection can be speeded up dramatically using approaches such as , no such speedups exist for region classiﬁcation. To address these drawbacks, we propose as our second system the pipeline shown in Figure 3.
This pipeline starts with bounding box detections after nonmaximum suppression. We expand this set of detections
by adding nearby high-scoring boxes that were removed by
non-maximum suppression but may be better localized (explained in detail below). This expanded set is only twice
as large as the original set, and about two orders of magnitude smaller than the full set of bottom-up proposals. For
each candidate in this set, we predict a segmentation, and
score this candidate using CNN features computed on the
segmentation. Because region-based features are computed
only on a small set, the pipeline is much more efﬁcient. We
call this system System 2.
This pipeline relies crucially on our ability to predict a
good segmentation from just bounding boxes. We use hypercolumns to make this prediction. In section 4.2, we show
that these predictions are accurate, and signiﬁcantly better
than predictions based on the top layer of the CNN.
Finally, the efﬁciency of this pipeline also allows us to
experiment with larger but more expressive architectures.
While used the architecture proposed by Krizhevsky et
al. (referred to as “T-Net” henceforth, following )
for both the box features and the region features, we show in
section 4.2 that the architecture proposed by Simonyan and
Zisserman (referred to as “O-Net” henceforth ) is
signiﬁcantly better.
4.1. System 1: Reﬁnement using hypercolumns
experiments,
hypercolumn-based reﬁnement to that proposed in . We
use the ranked hypotheses produced by and reﬁne each
hypothesis using hypercolumns. For the CNN, we use the
same network that was used for the region classiﬁcation (described as C in ). This network consists of two pathways, each based on T-Net. It takes in both the cropped
bounding box as well as the cropped foreground. For the hypercolumn representation we use the top-level fc7 features,
the conv4 features from both pathways using a 1 × 1 neighborhood, and the pool2 features from the box pathway with
a 3 × 3 neighborhood. We choose these layers because they
are spread out evenly in the network and capture a diverse
Figure 3. An alternative pipeline for SDS starting from bounding
box detections (Section 4)
set of features. In addition, for each location, we add as features a 0 or 1 encoding if the location was inside the original
region candidate, and a coarse 10 × 10 discretization of the
original candidate ﬂattened into a 100-dimensional vector.
This is to be commensurate with where these features
were used in the reﬁnement step. We use a 10 × 10 grid of
classiﬁers. As a last step, we project our predictions to superpixels by averaging the prediction over each superpixel.
We train on VOC2012 Train and evaluate on VOC2012 Val.
Table 1 shows the results of our experiments. The ﬁrst
two columns show the performance reported in with
and without the reﬁnement step. “Hyp” is the result we get
using hypercolumns, without bounding box regression or
ﬁnetuning. Our mean APr at 0.5 is 1.5 points higher, and
at 0.7 is 6.3 points higher, indicating that our reﬁnement is
much better than that of and is a large improvement
over the original candidate. Bounding box regression and
ﬁnetuning the network both provide signiﬁcant gains, and
with both of these, our mean APr at 0.5 is 3.1 points higher
and at 0.7 is 8.4 points higher than .
Table 1 also shows the results of several ablations of our
model (all without bounding box regression or ﬁnetuning):
1. Only fc7 uses only fc7 features and is thus similar to
the reﬁnement step in . We include this baseline to
conﬁrm that we can replicate those results.
2. fc7+pool2, fc7+conv4 and pool2+conv4 are reﬁnement systems that use hypercolumns but leave out features from conv4, pool2 and fc7 respectively. Each of
these baselines performs worse than our full system.
In each case the difference is statistically signiﬁcant at
a conﬁdence threshold of 0.05, computed using paired
sample permutation tests.
3. The 1 × 1, 2 × 2 and 5 × 5 models use different grid
resolutions, with the 1 × 1 grid amounting to a single
classiﬁer. There is a signiﬁcant loss in performance
(2.4 points at 0.7 overlap) when using a 1 × 1 grid.
However this baseline still outperforms indicating
that even without our grid classiﬁers (and without fc7,
since the global fc7 features are ineffectual without the
grid), the hypercolumn representation by itself is quite
powerful. A 5 × 5 grid is enough to recover full performance.
Finally, following , we take our Hyp+FT+bbox-reg
system and use the pasting scheme of to obtain a semantic segmentation. We get a mean IU of 54.6 on VOC2012
Segmentation Test, 3 points higher than (51.6 mean
4.2. System 2: SDS from bounding box detections
For our experiments with System 2, we use the detections of R-CNN as the starting point. R-CNN uses
CNNs to classify bounding box proposals from selective
We use the ﬁnal output after non-max suppression and bounding box regression. However, to allow direct
comparison with our previous experiments, we retrained R-
CNN to work with box proposals from MCG . We do all
training on VOC2012 Train.
We ﬁrst evaluate our segmentation predictions. As before, we use the same network as the detector to compute
the hypercolumn transform features. We ﬁrst experiment
with the T-Net architecture. We use the layers fc7, conv4
with a neighborhood of 1, and pool2 with a neighborhood
of 3. For computational reasons we do not do any ﬁnetuning. We use superpixel projection as before.
We show results in Table 2. Since we use only one network operating on bounding boxes instead of two working
on both the box and the region, we expect a drop in performance. We ﬁnd that this is the case, but the loss is small:
we get a mean APr of 49.1 at 0.5 and 29.1 at 0.7, compared
to 51.9 and 32.4 when we have the region features. In fact,
our performance is nearly as good as at 0.5 and about
4 points better at 0.7, and we get this accuracy starting from
just the bounding box.
To see how much of this performance is coming from the
hypercolumn representation, we also run a baseline using
just fc7 features. As expected, this baseline is only able to
output a fuzzy segmentation, compared to the sharp delineation we get using hypercolumns. It performs considerably
worse, losing 5 points at 0.5 overlap and almost 13 points at
0.7 overlap. Figure 4 shows example segmentations.
We now replace the T-Net architecture for the O-Net architecture. This architecture is signiﬁcantly larger, but provides an 8 point gain in detection AP . We again retrain
the R-CNN system using this architecture on MCG bounding box proposals. Again, for the hypercolumn representation we use the same network as the detector. We use the
layers fc7, conv4 with a neighborhood of 1 and pool3 with a
neighborhood of 3. (We use pool3 instead of pool2 because
the pool3 feature map has about half the resolution and is
thus easier to work with.)
We observe that the O-Net architecture is signiﬁcantly
better than the T-Net: we get a boost of 7.5 points at the
0.5 overlap threshold and 8 points at the 0.7 threshold. We
also ﬁnd that this architecture gives us the best performance
on the SDS task so far: with simple bounding box detection followed by our hypercolumn-based mask prediction,
we achieve a mean APr of 56.5 at an overlap threshold of
0.5 and a mean APr of 37.0 at an overlap threshold of 0.7.
These numbers are about 6.8 and 11.7 points better than the
results of . Last but not the least, we observe that the
large gap between our hypercolumn system and the only-fc7
baseline persists, and is equally large for the O-Net architecture. This implies that the gain provided by hypercolumns
is not speciﬁc to a particular network architecture. Figure 4
visualizes our O-Net results.
We now implement the full pipeline proposed in Figure 3. For this, we expand the initial pool of detections as
follows. We pick boxes with score higher than a threshold
that were suppressed by NMS but that overlap the detections by less than 0.7. We then do a non-max suppression
with a lenient threshold of 0.7 to get a pool of candidates
to rescore. Starting from 20K initial detections per category
across the dataset, our expanded pool is typically less than
50K per category, and less than 600K in total.
Next we segment each candidate using hypercolumns
and score it using a CNN trained to classify regions. This
network has the same architecture as O-Net. However, instead of a bounding box, this network takes as input the
bounding box with the region background masked out. This
network is trained as described in . We use features
from the topmost layer of this network and concatenate
them with the features from the top layer of the detection
network, and feed these into an SVM. For training data,
we use our expanded pool of candidates on the training set,
and take all candidates for which segmentation predictions
overlap groundtruth by more than 70% as positive and those
with overlap less than 50% as negative. After rescoring, we
do a non-max suppression using region overlap to get the
ﬁnal set of detections (we use an overlap threshold of 0.3).
We get 60.0 mean APr at 0.5, and 40.4 mean APr at
0.7. These numbers are state-of-the-art on the SDS benchmark (in contemporary work, get slightly higher performance at 0.5 but do not report the performance at 0.7;
our gains are orthogonal to theirs). Finally, on the semantic
segmentation benchmark, we get a mean IU of 62.6, which
is comparable to state-of-the-art.
5. Experiments on part localization
We evaluate part localization in the unconstrained detection setting, where the task is to both detect the object and
Figure 4. Figure ground segmentations starting from bounding box
detections. Top row: baseline using fc7, bottom row: Ours.
mAPr at 0.5
mAPr at 0.7
Table 2. Results on SDS on VOC 2012 val using System 2. Our
ﬁnal pipeline is state-of-the-art on SDS. (Section 4.2)
label its keypoints/segment its parts. This is different from
most prior work on these problems ,
which operates on the immediate vicinity of ground-truth
instances. We start from the detections of . We use the
same features and network as in Section 4.1. As before, we
do all training on VOC2012 Train.
Keypoint prediction
We evaluate keypoint prediction on
the “person” category using the protocol described in .
The test set for evaluating keypoints is the person images
in the second half of VOC2009 val. We use the APK metric , which evaluates keypoint predictions in a detection
setting. Each detection comes with a keypoint prediction
and a score. A predicted keypoint within a threshold distance (0.2 of the torso height) of the ground-truth keypoint
is a true positive, and is a false positive otherwise. The area
under the PR curve gives the APK for that keypoint.
We start from the person detections of .
bounding box regression to start from a better bounding
box. As described in Section 3 we train a separate system for each keypoint using the hypercolumn representation. We use keypoint annotations collected by . We produce a heatmap for each keypoint and then take the highest
scoring location of the heatmap as the keypoint prediction.
The APK metric requires us to attach a score with each
keypoint prediction. This score must combine the conﬁdence in the person detection and the conﬁdence in the keypoint prediction, since predicting a keypoint when the keypoint is invisible counts as a false positive. For this score we
multiply the value of the keypoint heatmap at the predicted
location with the score output by the person detector (which
we pass through a sigmoid).
Results are shown in Table 3. We compare our perfor-
pool2+ 1 × 1 2 × 2 5 × 5
+bbox-reg +bbox-reg
pool2 conv4
mean APr at 0.5 47.7
mean APr at 0.7 22.8
Table 1. Results on SDS on VOC2012 val using System 1. Our system (Hyp+FT+bbox-reg) is signiﬁcantly better than (Section 4.1).
Table 3. Results on keypoint prediction (APK on the Person subset of VOC2009 val). Our system is 3.3 points better than (Section 5).
Figure 5. Keypoint prediction (left wrist). Top row: baseline using fc7, bottom row: ours (hypercolumns without ﬁnetuning). In
black is the bounding box and the predicted heatmap is in red. We
normalize each heatmap so that the maximum value is 1.
part at 0.5
Table 4. Results on part labeling. Our approach (Hyp) is almost
uniformly better than using top level features (Section 5).
mance to , the previous best on this dataset. Gkioxari et
al. ﬁnetuned a network for pose, person detection and
action classiﬁcation, and then trained an SVM to assign a
score to the keypoint predictions. Without any ﬁnetuning
for pose, our system achieves a 1.8 point boost. A baseline
system trained using our pipeline but with just the fc7 features performs signiﬁcantly worse than our system, and is
even worse than a HOG-based method . This conﬁrms
that the gains we get are from the hypercolumn representation. Figure 5 shows some example predictions.
Finetuning the network as described in Section 3 gives
an additional 1.5 point gain, raising mean APK to 18.5.
Part labeling
We evaluate part labeling on the articulated
object categories in PASCAL VOC: person, horse, cow,
sheep, cat, dog, bird. We use the part annotations provided
by . We group the parts into top-level parts: head, torso,
Figure 6. Part labeling. Top: baseline using fc7, bottom: ours (hypercolumns). Both rows use the same ﬁgure-ground segmentation.
Red: head, green: torso, blue: legs, magenta: arms.
arms and legs for person, head, torso, legs and tail for the
four-legged animals and head, torso, legs, wings, tail for the
bird. We train separate classiﬁers for each part. At test time,
we use the Hyp+bbox-reg+FT system from Section 4.1 to
predict a ﬁgure-ground mask for each detection, and to every pixel in the ﬁgure-ground mask, we assign the part with
the highest score at that pixel.
For evaluation, we modify the deﬁnition of intersectionover-union in the APr metric : we count in the intersection only those pixels for which we also get the part label
correct. We call this metric APr
part. As before, we evaluate
both our system and a baseline that uses only fc7 features.
Table 4 shows our results. We get a large gain in almost all
categories by using hypercolumns. Note that this gain is entirely due to improvements in the part labeling, since both
methods use the same ﬁgure-ground mask. Figure 6 shows
some example part labelings.
6. Conclusion
We have shown that the hypercolumn representation provides large gains in three different tasks. We also believe
that this representation might prove useful for other ﬁnegrained tasks such as attribute or action classiﬁcation. We
leave an investigation of this to future work.
Acknowledgments.
This work was supported by ONR
MURI N000141010933, a Google Research Grant and a
Microsoft Research fellowship. We thank NVIDIA for providing GPUs through their academic program.