A Convex Optimization Framework for Active Learning
Ehsan Elhamifar
University of California, Berkeley
Guillermo Sapiro
Duke University
Allen Yang, S. Shankar Sastry
University of California, Berkeley
In many image/video/web classiﬁcation problems, we
have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain
labels for the samples. Active learning is the problem of
progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classi-
ﬁcation performance. Most existing active learning algorithms select only one sample at a time prior to retraining
the classiﬁer. Hence, they are computationally expensive
and cannot take advantage of parallel labeling systems such
as Mechanical Turk. On the other hand, algorithms that
allow the selection of multiple samples prior to retraining
the classiﬁer, may select samples that have signiﬁcant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classiﬁer type
such as SVM. In this paper, we develop an efﬁcient active
learning framework based on convex programming, which
can select multiple samples at a time for annotation. Unlike
the state of the art, our algorithm can be used in conjunction with any type of classiﬁers, including those of the family of the recently proposed Sparse Representation-based
Classiﬁcation (SRC). We use the two principles of classi-
ﬁer uncertainty and sample diversity in order to guide the
optimization program towards selecting the most informative unlabeled samples, which have the least information
overlap. Our method can incorporate the data distribution
in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness
of our framework in person detection, scene categorization
and face recognition on real-world datasets.
1. Introduction
The goal of recognition algorithms is to obtain the highest level of classiﬁcation accuracy on the data, which can
be images, videos, web documents, etc. The common ﬁrst
step of building a recognition system is to provide the machine learner with labeled training samples. Thus, in supervised and semi-supervised frameworks, the classiﬁer’s
performance highly depends on the quality of the provided
labeled training samples. In many problems in computer
vision, pattern recognition and information retrieval, it is
fairly easy to obtain a large number of unlabeled training
samples, e.g., by downloading images, videos or web documents from the Internet. However, it is, in general, dif-
ﬁcult to obtain labels for the unlabeled samples, since the
labeling process is typically complex, expensive and time
consuming. Active learning is the problem of progressively
selecting and annotating the most informative data points
from the pool of unlabeled samples, in order to obtain a
high classiﬁcation performance.
Prior Work.
Active learning has been well studied in
the literature with a variety of applications in image/video
categorization , text/web classiﬁcation , relevance feedback , etc. The
majority of the literature consider the single mode active
learning , where the algorithm selects and annotates only one unlabeled sample at a time
prior to retraining the classiﬁer. While this approach is effective in some applications, it has several drawbacks. First,
there is a need to retrain the classiﬁer after adding each new
labeled sample to the training set, which can be computationally expensive and time consuming. Second, such methods cannot take advantage of parallel labeling systems such
as Mechanical Turk or LabelMe , since they request annotation for only one sample at a time. Third, single mode active learning schemes might select and annotate
an outlier instead of an informative sample for classiﬁcation . Fourth, these methods are often developed for a
certain type of a classiﬁer such as SVM or Naive Bayes and
cannot be easily modiﬁed to work with other classiﬁer types
 .
To address some of the above issues, more recent methods have focused on the batch mode active learning, where
they select and annotate multiple unlabeled samples at a
time prior to retraining the classiﬁer . Notice that one can run a single mode active learning method
multiple times without retraining the classiﬁer in order to
select multiple unlabeled samples. However, the drawback
of this approach is that the selected samples can have signiﬁcant information overlap, hence, they do not improve
Figure 1. We demonstrate the effectiveness of our proposed active learning framework on three problems of person detection, scene categorization and
face recognition. Top: sample images from the INRIA Person dataset . The dataset contains images from 2 classes, either containing people or not.
Middle: sample images from the Fifteen Scene Categories dataset . The dataset contains images from 15 different categories, such as street, building,
mountain, etc. Bottom: sample images from the Extended YaleB Face dataset . The dataset contains images from 38 classes, corresponding to 38
different individuals, captured under a ﬁxed pose and varying illumination.
the classiﬁcation performance compared to the single mode
active learning scheme. Other approaches try to decrease
the information overlap among the selected unlabeled samples . However, such methods are often
ad-hoc or involve a non-convex optimization, which cannot be solved efﬁciently , hence approximate solutions are sought. Moreover, similar to the single mode active learning, most batch mode active learning algorithms
are developed for a certain type of a classiﬁer and cannot be easily modiﬁed to work with other classiﬁer types
 .
Paper Contributions.
In this paper, we develop an efﬁcient active learning framework based on convex programming that can be used in conjunction with any type of classiﬁers. We use the two principles of classiﬁer uncertainty
and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples. More speciﬁcally, for each unlabeled sample, we de-
ﬁne a conﬁdence score that reﬂects how uncertain the sample’s predicted label is according to the current classiﬁer
and how dissimilar the sample is with respect to the labeled
training samples.
A large value of the conﬁdence score
for an unlabeled sample means that the current classiﬁer is
more certain about the predicted label of the sample and
also the sample is more similar to the labeled training samples. Hence, annotating it does not provide signiﬁcant additional information to improve the classiﬁer’s performance.
On the other hand, an unlabeled sample with a small con-
ﬁdence score is more informative and should be labeled.
Since we can have many unlabeled samples with low con-
ﬁdence scores and they may have information overlap with
each other, i.e., can be similar to each other, we need to select a few representatives of the unlabeled samples with low
conﬁdence scores. We perform this task by employing and
modifying a recently proposed algorithm for ﬁnding data
representatives based on simultaneous sparse recovery .
The algorithm that we develop has the following advantages
with respect to the state of the art:
– It addresses the batch mode active leaning problem,
hence, it can take advantage of parallel annotation systems
such as Mechanical Turk and LabelMe.
– It can be used in conjunction with any type of classi-
ﬁers. The choice of the classiﬁer affects selection of unlabeled samples through the conﬁdence scores, but the proposed framework is generic. In fact, in our experiments, we
consider the problem of active learning using the recently
proposed Sparse Representation-based Classiﬁcation (SRC)
method . To the best of our knowledge, this is the ﬁrst
active learning framework for the SRC algorithm.
– It is based on convex programming, hence can be solved
efﬁciently. Unlike the state of the art, it incorporates both
the classiﬁer uncertainty and sample diversity in a convex
optimization to select multiple informative samples that are
diverse with respect to each other and the labeled samples.
– It can incorporate the distribution of the data by using an
appropriate dissimilarity matrix in the convex optimization
program. The dissimilarity between pairs of points can be
Euclidean distances (when the data come from a mixture
of Gaussians), geodesic distances (when data lie on a manifold) or other types of content/application-dependent dissimilarity, which we do not restrict to come from a metric.
Paper Organization.
The organization of the paper is as
follows. In Section 2, we review the Dissimilarity-based
Sparse Representative Selection (DSMRS) algorithm that
we leverage upon in this paper. In Section 3, we propose
our framework of active learning. We demonstrate experimental results on multiple real-world problems in Section
4. Finally, Section 5 concludes the paper.
2. Dissimilarity-based Sparse Modeling Representative Selection (DSMRS)
In this section, we review the Dissimilarity-based Sparse
Modeling Representative Selection (DSMRS) algorithm
 that ﬁnds representative points of a dataset. Assume
we have a dataset with N points and we are given dissimilarities {dij}i,j=1,...,N between every pair of points. dij
denotes how well i represents j. The smaller the value of
dij is, the better point i is a representative of point j. We assume that the dissimilarities are nonnegative and djj ≤dij
for every i and j. We can collect the dissimilarities in a
dN1 dN2 · · · dNN
Given the dissimilarities, the goal is to ﬁnd a few points that
well represent the dataset. To do so, proposes a convex
optimization framework by introducing variables zij associated to dij. zij ∈ indicates the probability that i is
a representative of j. We can collect the optimization variables in a matrix as
zN1 zN2 · · · zNN
In order to select a few representatives that well encode the
collation of points in the dataset, two objective functions
should be optimized. The ﬁrst objective function is the encoding cost of the N data points via the representatives. The
encoding cost of j via i is set to dijzij ∈[0, dij], hence the
total encoding cost for all points is
dijzij = tr(D⊤Z).
The second objective function corresponds to penalizing the
number of selected representatives.
Notice that if i is a
representative of some points in the dataset, then zi ̸= 0
and if i does not represent any point in the dataset, then
zi = 0. Hence, the number of representatives corresponds
to the number of nonzero rows of Z. A convex surrogate for
the cost associated to the number of selected representative
is given by
∥zi∥q ≜∥Z∥1,q,
where q ∈{2, ∞}. Putting the two objectives together, the
DSMRS algorithm solves
min λ ∥Z∥q,1 + tr(D⊤Z)
Z ≥0, 1⊤Z = 1⊤,
Figure 2. Separating data in two different classes. Class 1 consists of
data in {G(1)
} and class 2 consists of data in {G(2)
Left: a max-margin linear SVM learned using two training samples (green
crosses). Data in G(1)
are misclassiﬁed as belonging to class 1. Note that
labeling samples from G(1)
does not change the decision boundary
much and G(1)
will be still misclassiﬁed. Right: labeling a sample that the
classiﬁer is more uncertain about its predicted class, helps to improve the
classiﬁcation performance. In this case, labeling a sample from G(1)
is close to the decision boundary, results in changing the decision boundary
and correct classiﬁcation of all samples.
where the constraints ensure that each column of Z is a
probability vector, denoting the association probability of j
to each one of the data points. Thus, the nonzero rows of
the solution Z indicate the indices of the representatives.
Notice that λ > 0 balances the two costs of the encoding
and the number of representatives. A smaller value of λ
puts more emphasis on better encoding, hence results in obtaining more representatives, while a larger value of λ puts
more emphasis on penalizing the number of representatives,
hence results in obtaining less representatives.
3. Active Learning via Convex Programming
In this section, we propose an efﬁcient algorithm for active learning that takes advantage of convex programming
in order to ﬁnd the most informative points. Unlike the
state of the art, our algorithm can be used in conjunction
with any classiﬁer type. To do so, we use the two principles
of classiﬁer uncertainty and sample diversity to deﬁne con-
ﬁdence scores for unlabeled samples. A lower conﬁdence
score for an unlabeled sample indicates that we can obtain
more information by annotating that sample. However, the
number of unlabeled samples with low conﬁdence scores
can be large and, more importantly, the samples can have
information overlap with each other or they can be outliers.
Thus, we integrate the conﬁdence scores in the DSMRS
framework in order to ﬁnd a few representative unlabeled
samples that have low conﬁdence scores. In the subsequent
sections, we deﬁne the conﬁdence scores and show how to
use them in the DSMRS framework in order to ﬁnd the most
informative samples. We assume that we have a total of N
samples, where U and L denote sets of indices of unlabeled
and labeled samples, respectively.
3.1. Classiﬁer Uncertainty
First, we use the classiﬁer uncertainty in order to select
informative points for improving the classiﬁer performance.
The uncertainty sampling principle states that the informative samples for classiﬁcation are the ones that the classiﬁer is most uncertain about.
To illustrate this, consider the example shown in the left
plot of Figure 2, where the data belong to two different
classes. G(i)
denotes the j-th cluster of samples that belong to class i. Assume that we already have two labeled
samples, shown by green crosses, one from each class. For
this speciﬁc example, we consider the linear SVM classiﬁer
but the argument is general and applies to other classiﬁer
types. A max-margin hyperplane learned via SVM for the
two training samples is shown in the ﬁgure. Notice that the
classiﬁer is more conﬁdent about the labels of samples in
as they are farther from the decision boundary,
while it is less conﬁdent about the labels of samples in G(1)
since they are closer to the hyperplane boundary. In this
case, labeling any of the samples in G(1)
change the decision boundary, hence, samples in G(1)
still be misclassiﬁed. On the other hand, labeling a sample
changes the decision boundary so that points in
the two classes will be correctly classiﬁed, as shown in the
right plot of Figure 2.
Now, for a generic classiﬁer, we deﬁne its conﬁdence
about the predicted label of an unlabeled sample.
Consider data in L different classes. For an unlabeled sample i,
we consider the probability vector pi =
where pij denotes the probability that sample i belongs to
class j. We deﬁne the classiﬁer conﬁdence score of point i
cclassiﬁer(i) ≜σ −(σ −1) E(pi)
log2(L) ∈[1, σ],
where σ > 1 and E(·) denotes the entropy function. Note
that when the classiﬁer is most certain about the label of a
sample i, i.e., only one element of pi is nonzero and equal
to one, then the entropy is zero and the conﬁdence score
is maximum, i.e., is equal to σ. On the other hand, when
the classiﬁer is most uncertain about the label of a sample
i, i.e., when all the elements of pi are equal to 1/L, then
the entropy is equal to log2(L) and the conﬁdence score is
minimum, i.e., is equal to one.
Remark 1 For probabilistic classiﬁers such as Naive
Bayes, the probability vectors, pi, are directly given by
the output of the algorithms.
For SVM, we use the result of to estimate pi.
For SRC, we can compute
the multi-class probability vectors as follows. Let xi =
⊤be the sparse representation of an unlabeled sample i, where xij denotes the representation coefﬁcients using labeled samples from class j. We set pij ≜
Figure 3. Separating data in two different classes. Class 1 consists of data
} and class 2 consists of data in {G(2)
}. Left: a maxmargin linear SVM learned using two training samples (green crosses).
Data in G(1)
are misclassiﬁed as belonging to class 2 and 1, respectively. Note that the most uncertain samples according to the classiﬁer
are samples from G(1)
, which are close to the decision boundary.
However, labeling such samples does not change the decision boundary
much and samples in G(1)
will still be misclassiﬁed. Right:
labeling samples that are sufﬁciently dissimilar from the labeled training
samples helps to improve the classiﬁcation performance. In this case, labeling a sample from G(1)
and a sample from G(2)
results in changing the
decision boundary and correct classiﬁcation of all samples.
∥xij∥1/∥xi∥1.
3.2. Sample Diversity
We also use the sample diversity criterion in order to ﬁnd
the most informative points for improving the classiﬁer performance. More speciﬁcally, sample diversity states that informative points for classiﬁcation are the ones that are suf-
ﬁciently dissimilar from the labeled training samples (and
from themselves in the batch mode setting).
To illustrate this, consider the example of Figure 3,
where the data belong to two different classes. G(i)
the j-th cluster of samples that belong to class i. Assume
that we already have two labeled samples, shown by green
crosses, one from each class. For this example, we consider the linear SVM classiﬁer but the argument applies to
other classiﬁer types. The max-margin hyperplane learned
via SVM for the two training samples is shown in the the
left plot of Figure 3. Notice that samples in G(1)
are similar to the labeled samples (have small Euclidean distances to the labeled samples in this example). In fact, labeling any of the samples in G(1)
does not change the
decision boundary much, and the points in G(1)
will be still
misclassiﬁed as belonging to class 2. On the other hand,
samples in G(1)
are more dissimilar from the labeled training samples. In fact, labeling a sample from G(1)
changes the decision boundary so that points in the
two classes will be correctly classiﬁed, as shown in the right
Figure 4. Separating data in two different classes. Class 1 consists of data in {G(1)
} and class 2 consists of data in {G(2)
}. Left: a max-margin
linear SVM learned using two training samples (green crosses). All samples in G(2)
as well as some samples in G(1)
are misclassiﬁed. Middle: two
samples with lowest conﬁdence scores correspond to two samples from G(1)
that are close to the decision boundary. A retrained classiﬁer using these two
samples, which have information overlap, still misclassiﬁes samples in G(2)
. Right: two representatives of samples with low conﬁdence scores correspond
to a sample from G(1)
and a sample from G(2)
. A retrained classiﬁer using these two samples correctly classiﬁes all the samples in the dataset.
plot of Figure 3.
To incorporate diversity with respect to the labeled training set, L, for a point i in the unlabeled set, U, we deﬁne
the diversity conﬁdence score as
cdiversity(i) ≜σ −(σ −1)
minj∈L dji
maxk∈U minj∈L djk
where σ > 1. When the closest labeled sample to an unlabeled sample i is very similar to it, i.e., minj∈L dji is close
to zero, then the diversity conﬁdence score is large, i.e., is
close to σ. This means that sample i does not promote diversity. On the other hand, when all labeled samples are
very dissimilar from an unlabeled sample i, i.e., the fraction
in (7) is close to one, then the diversity conﬁdence score is
small, i.e., is close to one. This means that selecting and
annotating sample i promotes diversity with respect to the
labeled samples.
3.3. Selecting Informative Samples
Recall that our goal is to have a batch mode active learning framework that selects multiple informative and diverse
unlabeled samples, with respect to the labeled samples as
well as each other, for annotation. One can think of a simple algorithm that selects samples that have the lowest con-
ﬁdence scores. The drawback of this approach is that while
the selected unlabeled samples are diverse with respect to
the labeled training samples, they can still have signiﬁcant
information overlap with each other. This comes from the
fact that the conﬁdence scores only reﬂect the relationship
of each unlabeled sample with respect to the classiﬁer and
the labeled training samples and do not capture the relationships among the unlabeled samples.
To illustrate this, consider the example of Figure 4,
where the data belong to two different classes.
that we already have two labeled samples, shown by green
crosses, one from each class. A max-margin hyperplane
learned via SVM for the two training samples is shown in
the the left plot of Figure 4. In this case, all samples in
as well as some samples in G(1)
are misclassiﬁed. Notice that samples in G(1)
have small classiﬁer and diversity
conﬁdence scores and samples in G(2)
have small diversity
conﬁdence scores. Now, if we select two samples with lowest conﬁdence scores, we will select two samples from G(1)
as they are very close to the decision boundary. However,
these two samples have information overlap, since they belong to the same cluster. In fact, after adding these two
samples to the labeled training set, the retrained classiﬁer,
shown in the middle plot of Figure 4, still misclassiﬁes samples in G(2)
2 . On the other hand, two representatives of samples with low conﬁdence scores, i.e., two samples that capture the distribution of samples with low conﬁdence scores,
correspond to one sample from G(1)
and one sample from
2 . As shown in the right plot of Figure 4, the retrained
classiﬁer using these two points correctly classiﬁes all of
the samples.
To select a few diverse representatives of unlabeled samples that have low conﬁdence scores, we take advantage
of the DSMRS algorithm. Let D ∈R|U|×|U| be the dissimilarity matrix for samples in the unlabeled set U =
{i1, · · · , i|U|}. We propose to solve the convex program
min λ ∥CZ∥1,q+tr(D⊤Z)
Z ≥0, 1⊤Z = 1⊤,
over the optimization matrix Z ∈R|U|×|U|. The matrix
C = diag(c(i1), . . . , c(i|U|)) is the conﬁdence matrix with
the active learning conﬁdence scores, c(i), deﬁned as
c(ik) ≜min{cclassiﬁer(ik), cdiversity(ik)} ∈[1, σ].
More speciﬁcally, for an unlabeled sample ik that has a
small conﬁdence score c(ik), the optimization program puts
less penalty on the k-th row of Z being nonzero. On the
other hand, for a sample ik that has a large conﬁdence score
c(ik), the optimization program puts more penalty on the
k-th row of Z being nonzero. Hence, the optimization promotes selecting a few unlabeled samples with low conﬁdence scores that are, at the same time, representatives of
Labeled training set size
Test set accuracy
Figure 5. Classiﬁcation accuracy of different active learning algorithms
on the INRIA Person dataset as a function of the total number of labeled
training samples selected by each algorithm.
the distribution of the samples. This therefore addresses
the main problems with previous active learning algorithms,
which we discussed in Section 1.
Remark 2 We should note that other combinations of the
classiﬁer and diversity scores can be used, such as c(i) ≜
cclassiﬁer(i) · cdiversity(i) ∈[1, σ]. However, (9) is very intuitive and works best in our experiments.
Remark 3 In the supplementary materials, we discuss the
appropriate choice for the range of the regularization parameter, λ, in (8). We also explain how our algorithm is robust to outliers and can reject them for annotation. Moreover, we present an alternative convex formulation to (8),
where we can impose the number of selected samples and
can also address the active learning with a given budget
 .
4. Experiments
In this section, we examine the performance of our proposed active learning framework on several real-world applications.
We consider person detection, scene categorization and face recognition from real images (see Figure
1). We refer to our approach, formulated in (8), as Convex Programming-based Active Learning (CPAL) and implement it using an Alternating Direction Method of Multipliers method , which has quadratic complexity in the
number of unlabeled samples. For all the experiments, we
ﬁx σ = 20 in (6) and (7), however, the performances do
not change much for σ ∈ . As the experimental results show, our algorithm works well with different types of
classiﬁers.
To illustrate the effect of conﬁdence scores and representativeness of samples in the performance of our proposed
framework, we consider several methods for comparison.
Iteration number
Labeled training set size
Figure 6. Total number of samples from each class of INRIA Person
dataset selected by our proposed algorithm (CPAL) at different active
learning iterations.
Assume that our algorithm selects Kt samples at iteration t,
i.e., prior to training the classiﬁer for the t-th time.
– We select Kt samples uniformly at random from the pool
of unlabeled samples. We refer to this method as RAND.
– We select Kt samples that have the smallest classiﬁer con-
ﬁdence scores. For an SVM classiﬁer, this method corresponds to the algorithm proposed in . We refer to this
algorithm as Classiﬁer Conﬁdence-based Active Learning
4.1. Person Detection
In this section, we consider the problem of detecting humans in images. To do so, we use the INRIA Person dataset
 that consists of a set of positive training/test images,
which contain people, and a set of negative train/test images, which do not contain a person (see Figure 1). For each
image in the dataset, we compute the Histogram of Oriented
Gradients (HOG), which has been shown to be an effective
descriptor for the task of person detection . We use
the positive/negative training images in the dataset to form
the pool of unlabeled samples (2, 416 positive and 2, 736
negative samples) and use the the positive/negative test images for testing (1, 126 positive and 900 negative samples).
For this binary classiﬁcation problem (L = 2), we use the
linear SVM classiﬁer, which has been shown to work well
with HOG features for the person detection task . We
use the χ2-distance to compute the dissimilarity between
the histograms, as it works better than other dissimilarity
types, such as the ℓ1-distance and KL-divergence, in our
experiments.
Figure 5 shows the classiﬁcation accuracy of different
active learning methods on the test set as a function of the
total number of labeled samples. From the results, we make
the following conclusions:
– Our proposed active learning algorithm, consistently out-
Labeled training set size
Test set accuracy
Figure 7. Classiﬁcation accuracy of different active learning algorithms
on the Fifteen Scene Categories dataset as a function of the total number
of labeled training samples selected by each algorithm.
performs other algorithms. In fact, with 316 labeled samples, CPAL obtains 96% accuracy while other methods obtain less than 84% accuracy on the test set.
– CCAL and RAND perform worse than our proposed algorithm. This comes from the fact that the selected samples by
CCAL can have information overlap and are not necessarily representing the distribution of unlabeled samples with
low conﬁdence scores. Also, RAND ignores all conﬁdence
scores and obtains, in general, lower classiﬁcation accuracy
than CCAL.
Figure 6 shows the total number of samples selected by
our method from each class. Although our active learning
algorithm is unaware of the separation of unlabeled samples
into classes, it consistently selects about the same number
of samples from each class. Notice also that our method
selects a bit more samples from the nonperson class, since,
as expected, the negative images have more variation than
the positive ones.
4.2. Scene Categorization
In this section, we consider the problem of scene categorization in images. We use the Fifteen Scene Categories
dataset that consists of images from L = 15 different classes, such as coasts, forests, highways, mountains,
stores, etc (see Figure 1). There are between 210 and 410
images in each class, making a total of 4, 485 images in the
dataset. We randomly select 80% of images in each class to
form the pool of unlabeled samples and use the rest of the
20% of images in each class for testing. We use the kernel
SVM classiﬁer (one-versus-rest) with the Spatial Pyramid
Match (SPM) kernel, which has been shown to be effective
for scene categorization . More speciﬁcally, the SPM
kernel between a pair of images is given by the weighted
intersection of the multi-resolution histograms of the images. We use 3 pyramid levels and 200 bins to compute the
histograms and the kernel. As the SPM is itself a similar-
Labeled training set size
Test set accuracy
Figure 8. Classiﬁcation accuracy of different active learning algorithms
on the Extended YaleB Face dataset as a function of the total number of
labeled training samples selected by each algorithm.
ity between pairs of images, we also use it to compute the
dissimilarities by negating the similarity matrix and shifting
the elements to become non-negative.
Figure 7 shows the accuracy of different active learning
methods on the test set as a function of the total number of
selected samples. Our method consistently performs better
than other approaches. Unlike the experiment in the previous section, here the RAND method, in general, has a
better performance than CCAL method that selects multiple samples with low conﬁdence scores. A careful look into
the selected samples by different methods shows that, this
is due to the fact that CCAL may repeatedly select similar
samples from a ﬁxed class while a random strategy, in general, does not get stuck to repeatedly select similar samples
from a ﬁxed class.
4.3. Face Recognition
Finally, we consider the problem of active learning for
face recognition. We use the Extended YaleB Face dataset
 , that consists of face images of L = 38 individuals
(classes). Each class consists of 64 images captured under the same pose and varying illumination. We randomly
select 80% of images in each class to form the pool of unlabeled samples and use the rest of the 20% of images in each
class for testing. We use the Sparse Representation-based
Classiﬁcation (SRC), which has been shown to be effective
for the classiﬁcation of human faces . To the best of our
knowledge, our work is the ﬁrst one addressing the active
learning problem in conjunction with SRC. We downsample the images and use the 504-dimensional vectorized images as the feature vectors. We use the Euclidean distance
to compute dissimilarities between pairs of samples.
Figure 8 shows the classiﬁcation accuracy of different
active learning methods as a function of the total number of
labeled training samples selected by each algorithm. One
can see that our proposed algorithm performs better than
other methods. With a total of 790 labeled samples (average of 21 samples per class), we obtain the same accuracy
(about 97%) as reported in for 32 random samples per
class. It is important to note that the performances of RAND
and CCAL are very close. This comes from the fact that the
space of images from each class are not densely sampled.
Hence, samples are typically dissimilar from each other. As
a result, samples with low conﬁdence scores are generally
dissimilar from each other.
5. Conclusions
We proposed a batch mode active learning algorithm
based on simultaneous sparse recovery that can be used
in conjunction with any classiﬁer type. The advantage of
our algorithm with respect to the state of the art is that
it incorporates classiﬁer uncertainty and sample diversity
principles via conﬁdence scores in a convex programming
scheme. Thus, it selects the most informative unlabeled
samples for classiﬁcation that are sufﬁciently dissimilar
from each other as well as the labeled samples and represent the distribution of the unlabeled samples. We demonstrated the effectiveness of our approach by experiments on
person detection, scene categorization and face recognition
on real-world images.
Acknowledgment
E. Elhamifar, A. Yang and S. Sastry are supported
by grants ONR-N000141310341, ONR-N000141210609,
BAE Systems (94316), DOD Advanced Research Project
(30989), Army Research Laboratory (31633), University of
Pennsylvania (96045). G. Sapiro acknowledges the support
by ONR, NGA, NSF, ARO and AFOSR grants.