HAL Id: hal-01216730
 
Submitted on 16 Oct 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
What, Where & How Many? Combining Object
Detectors and CRFs
Lubor Ladicky, Paul Sturgess, Karteek Alahari, Chris Russell, Philip H. S.
To cite this version:
Lubor Ladicky, Paul Sturgess, Karteek Alahari, Chris Russell, Philip H. S. Torr. What, Where & How
Many? Combining Object Detectors and CRFs. ECCV - European Conference on Computer Vision,
Sep 2010, Heraklion, Crete, Greece. ￿10.1007/978-3-642-15561-1_31￿. ￿hal-01216730￿
What, Where & How Many?
Combining Object Detectors and CRFs
L’ubor Ladický, Paul Sturgess, Karteek Alahari, Chris Russell, and Philip H.S. Torr ⋆
Oxford Brookes University
 
Abstract. Computer vision algorithms for individual tasks such as object recognition, detection and segmentation have shown impressive results in the recent
past. The next challenge is to integrate all these algorithms and address the problem of scene understanding. This paper is a step towards this goal. We present a
probabilistic framework for reasoning about regions, objects, and their attributes
such as object class, location, and spatial extent. Our model is a Conditional Random Field deﬁned on pixels, segments and objects. We deﬁne a global energy
function for the model, which combines results from sliding window detectors,
and low-level pixel-based unary and pairwise relations. One of our primary contributions is to show that this energy function can be solved efﬁciently. Experimental results show that our model achieves signiﬁcant improvement over the
baseline methods on CamVid and PASCAL VOC datasets.
Introduction
Scene understanding has been one of the central goals in computer vision for many
decades . It involves various individual tasks, such as object recognition, image segmentation, object detection, and 3D scene recovery. Substantial progress has been made
in each of these tasks in the past few years . In light of these successes, the challenging problem now is to put these individual elements together to achieve the grand
goal — scene understanding, a problem which has received increasing attention recently . The problem of scene understanding involves explaining the whole image by recognizing all the objects of interest within an image and their spatial extent
or shape. This paper is a step towards this goal. We address the problems of what,
where, and how many: we recognize objects, ﬁnd their location and spatial extent, segment them, and also provide the number of instances of objects. This work can be
viewed as an integration of object class segmentation methods , which fail to distinguish between adjacent instances of objects of the same class, and object detection
approaches , which do not provide information about background classes, such as
grass, sky and road.
The problem of scene understanding is particularly challenging in scenes composed
of a large variety of classes, such as road scenes and images in the PASCAL VOC
⋆This work is supported by EPSRC research grants, HMGCC, the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886. P. H. S.
Torr is in receipt of Royal Society Wolfson Research Merit Award.
L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr
Fig. 1. A conceptual view of our method. (a) An example input image. (b) Object class segmentation result of a typical CRF approach. (c) Object detection result with foreground/background
estimate within each bounding box. (d) Result of our proposed method, which jointly infers about
objects and pixels. Standard CRF methods applied to complex scenes as in (a) underperform on
the “things” classes, e.g. inaccurate segmentation of the bicyclist and persons, and misses a pole
and a sign, as seen in (b). However, object detectors tend to perform well on such classes. By
incorporating these detection hypotheses (§2.2), shown in (c), into our framework, we aim to
achieve an accurate overall segmentation result as in (d) (§3.3). (Best viewed in colour)
dataset . For instance, road scene datasets contain classes with speciﬁc shapes such
as person, car, bicycle, as well as background classes such as road, sky, grass, which
lack a distinctive shape (Figure 1). The distinction between these two sets of classes
— referred to as things and stuff respectively — is well known . Adelson 
emphasized the importance of studying the properties of stuff in early vision tasks.
Recently, these ideas are being revisited in the context of the new vision challenges, and
have been implemented in many forms . In our work, we follow the deﬁnition
by Forsyth et al. , where stuff is a homogeneous or reoccurring pattern of ﬁnescale properties, but has no speciﬁc spatial extent or shape, and a thing has a distinct
size and shape. The distinction between these classes can also be interpreted in terms
of localization. Things, such as cars, pedestrians, bicycles, can be easily localized by
bounding boxes unlike stuff, such as road, sky1.
Complete scene understanding requires not only the pixel-wise segmentation of an
image, but also an identiﬁcation of object instances of a particular class. Consider an
image of a road scene taken from one side of the street. It typically contains many cars
parked in a row. Object class segmentation methods such as would label all
the cars adjacent to each other as belonging to a large car segment or blob, as illustrated
in Figure 2. Thus, we would not have information about the number of instances of
a particular object—car in this case. On the other hand, object detection methods can
identify the number of objects , but cannot be used for background (stuff) classes.
In this paper, we propose a method to jointly estimate the class category, location,
and segmentation of objects/regions in a visual scene. We deﬁne a global energy function for the Conditional Random Field (CRF) model, which combines results from detectors (Figure 1(c)), pairwise relationships between mid-level cues such as superpixels,
and low-level pixel-based unary and pairwise relations (Figure 1(b)). We also show that,
unlike , our formulation can be solved efﬁciently using graph cut based move
1 Naturally what is classiﬁed as things or stuff might depend on either the application or viewing
scale, e.g. ﬂowers or trees might be things or stuff.
What, Where & How Many? Combining Object Detectors and CRFs
Fig. 2. (a) Object class segmentation results (without detection), (b) The detection result, (c)
Combined segmentation and detection. Object class segmentation algorithms, such as , label
all the cars adjacent to each other as belonging to one large blob. Detection methods localize
objects and provide information about the number of objects, but do not give a segmentation. Our
method jointly infers the number of object instances and the object class segmentation. See §2.3
for details. (Best viewed in colour)
making algorithms. We evaluate our approach extensively on two widely used datasets,
namely Cambridge-driving Labeled Video Database (CamVid) and PASCAL VOC
2009 , and show a signiﬁcant improvement over the baseline methods.
Outline of the paper. Section 1.1 discusses the most related work. Standard CRF approaches for the object segmentation task are reviewed in Section 2.1. Section 2.2 describes the details of the detector-based potential, and its incorporation into the CRF
framework. We also show that this novel CRF model can be efﬁciently solved using
graph cut based algorithms in Section 2.3. Implementation details and the experimental
evaluation are presented in Section 3. Section 4 discusses concluding remarks.
Related Work
Our method is inspired by the works on object class segmentation , foreground (thing) object detection , and relating things and stuff . Whilst the
segmentation methods provide impressive results on certain classes, they typically underperform on things, due to not explicitly capturing the global shape information of
object class instances. On the other hand, detection methods are geared towards capturing this information, but tend to fail on stuff, which is amorphous.
A few object detection methods have attempted to combine object detection and
segmentation sub-tasks, however they suffer from certain drawbacks. Larlus and Jurie obtained an initial object detection result in the form of a bounding box, and
then reﬁned this rectangular region using a CRF. A similar approach has been followed
by entries based on object detection algorithms in the PASCAL VOC 2009 segmentation challenge. This approach is not formulated as one energy cost function and
cannot be applied to either cluttered scenes or stuff classes. Furthermore, there is no
principled way of handling multiple overlapping bounding boxes. Tu et al. also
presented an effective approach for identifying text and faces, but leave much of the
L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr
image unlabelled. Gu et al. used regions for object detection instead of bounding boxes, but were restricted to using a single over-segmentation of the image. Thus,
their approach cannot recover from any errors in this initial segmentation step. In comparison, our method does not make such a priori decisions, and jointly reasons about
segments and objects.
The work of layout CRF also provides a principled way to integrate things
and stuff. However, their approach requires that things must conform to a predeﬁned
structured layout of parts, and does not allow for the integration of arbitrary detector
responses. To our knowledge, the only other existing approaches that attempt to jointly
estimate segmentation and detection in one optimization framework are the works of . However, the minimization of their cost functions is intractable and their inference
methods can get easily stuck in local optima. Thus, their incorporation of detector potentials does not result in a signiﬁcant improvement of performance. Also, focussed
only on two classes (cars and pedestrians), while we handle many types of objects (e.g.
20 classes in the PASCAL VOC dataset). A direct comparison with this method was not
possible as neither their code nor their dataset because ground truth annotations are not
publicly available at the time of publication.
CRFs and Detectors
We deﬁne the problem of jointly estimating segmentation and detection in terms of minimizing a global energy function on a CRF model. Our approach combines the results
from detectors, pairwise relationships between superpixels, and other low-level cues.
Note that our framework allows us to incorporate any object detection approach into
any pixel or segment based CRF.
CRFs for labelling problems
In the standard CRF formulation for image labelling problems we represent each
pixel as random variable. Each of these random variables takes a label from the set
L = {l1, l2, . . . , lk}, which may represent objects such car, airplane, bicycle. Let
X = {X1, X2, . . . , XN} denote the set of random variables corresponding to the image
pixels i ∈V = {1, 2, . . ., N}. A clique c is a set of random variables Xc which are
conditionally dependent on each other. A labelling x refers to any possible assignment
of labels to the random variables and takes values from the set L = LN.
The posterior distribution Pr(x|D) over the labellings of the CRF can be written
as: Pr(x|D) = 1
c∈C ψc(xc)), where Z is a normalizing constant called the
partition function, C is the set of all cliques, and D the given data. The term ψc(xc)
is known as the potential function of the clique c ⊆V, where xc = {xi : i ∈c}.
The corresponding Gibbs energy is given by: E(x) = −log Pr(x|D) −log Z =
c∈C ψc(xc). The most probable or Maximum a Posteriori (MAP) labelling x∗of the
random ﬁeld is deﬁned as: x∗= arg maxx∈L Pr(x|D) = arg minx∈L E(x).
In computer vision labelling problems such as segmentation or object recognition,
the energy E(x) is typically modelled as a sum of unary, pairwise , and higher
order potentials. The unary potentials are based on local feature responses and
What, Where & How Many? Combining Object Detectors and CRFs
Fig. 3. (a) Segmentation without object detectors, (b) Object detections for car and pedestrian
shown as bounding boxes, (c) Segmentation using our method. These detector potentials act as a
soft constraint. Some false positive detections (such as the large green box representing person)
do not affect the ﬁnal segmentation result in (c), as it does not agree with other strong hypotheses
based on pixels and segments. On the other hand, a strong detector response (such as the purple
bounding box around the car) correctly relabels the road and pedestrian region as car in (c)
resulting in a more accurate object class segmentation. (Best viewed in colour)
capture the likelihood of a pixel taking a certain label. Pairwise potentials encourage
neighbouring pixels in the image to take the same label. Similarly, a CRF can be deﬁned
over segments obtained by unsupervised segmentation of the image.
Recently, these models have been generalized to include pixels and segments in a single
CRF framework by introducing higher order potentials . All these models successfully reason about pixels and/or segments. However, they fail to incorporate the notion
of object instances, their location, and spatial extent (which are important cues used by
humans to understand a scene) into the recognition framework. Thus, these models are
insufﬁcient to address the problem of scene understanding. We aim to overcome these
issues by introducing novel object detector based potentials into the CRF framework.
Detectors in CRF framework
MAP estimation can be understood as a soft competition among different hypotheses
(deﬁned over pixel or segment random variables), in which the ﬁnal solution maximizes
the weighted agreement between them. These weighted hypotheses can be interpreted
as potentials in the CRF model. In object class recognition, these hypotheses encourage: (i) variables to take particular labels (unary potentials), and (ii) agreement between
variables (pairwise). Existing methods are limited to such hypotheses provided by pixels and/or segments only. We introduce an additional set of hypotheses
representing object detections for the recognition framework2.
Some object detection approaches have used their results to perform a segmentation within the detected areas3. These approaches include both the true and false
2 Note that our model chooses from a set of given detection hypotheses, and does not propose
any new detections.
3 As evident in some of the PASCAL VOC 2009 segmentation challenge entries.
L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr
positive detections, and segment them assuming they all contain the objects of interest. There is no way of recovering from these erroneous segmentations. Our approach
overcomes this issue by using the detection results as hypotheses that can be rejected
in the global CRF energy. In other words, all detections act as soft constraints in our
framework, and must agree with other cues from pixels and segments before affecting
the object class segmentation result. We illustrate this with one of our results shown
in Figure 3. Here, the false positive detection for “person” class (shown as the large
green box on the right) does not affect the segmentation result in (c). Although, the true
positive detection for “car” class (shown as the purple box) reﬁnes the segmentation
because it agrees with other hypotheses. This is achieved by using the object detector
responses4 to deﬁne a clique potential over the pixels, as described below.
Let D denote the set of object detections, which are represented by bounding boxes
enclosing objects, and corresponding scores that indicate the strength of the detections.
We deﬁne a novel clique potential ψd over the set of pixels xd belonging to the d-th
detection (e.g. pixels within the bounding box), with a score Hd and detected label ld.
Figure 4 shows the inclusion of this potential graphically on a pixel-based CRF. The
new energy function is given by:
E(x) = Epix(x) +
ψd(xd, Hd, ld),
where Epix(x) is any standard pixel-based energy. The minimization procedure should
be able to reject false detection hypotheses on the basis of other potentials (pixels and/or
segments). We introduce an auxiliary variable yd ∈{0, 1}, which takes value 1 to
indicate the acceptance of d-th detection hypothesis. Let φd be a function of this variable
and the detector response. Thus the detector potential ψd(.) is the minimum of the
energy values provided by including (yd = 1) and excluding (yd = 0) the detector
hypothesis, as given below:
ψd(xd, Hd, ld) =
yd∈{0,1} φd(yd, xd, Hd, ld).
We now discuss the form of this function φd(·). If the detector hypothesis is included
(yd = 1), it should: (a) Encourage consistency by ensuring that labellings where all the
pixels in xd take the label ld should be more probable, i.e. the associated energy of such
labellings should be lower; (b) Be robust to partial inconsistencies, i.e. pixels taking a
label other than ld in the detection window. Such inconsistencies should be assigned
a cost rather than completely disregarding the detection hypothesis. The absence of
the partial inconsistency cost will lead to a hard constraint where either all or none of
the pixels in the window take the label ld. This allows objects partially occluded to be
correctly detected and labelled.
To enable a compact representation, we choose the potential ψd such that the associated cost for partial inconsistency depends only on the number of pixels Nd =
i∈xd δ(xi ̸= ld) disagreeing with the detection hypothesis. Let f(xd, Hd) deﬁne the
strength of the hypothesis and g(Nd, Hd) the cost taken for partial inconsistency. The
detector potential then takes the form:
4 This includes sliding window detectors as a special case.
What, Where & How Many? Combining Object Detectors and CRFs
Fig. 4. Inclusion of object detector potentials into a CRF model. We show a pixel-based CRF as
an example here. The set of pixels in a detection d1 (corresponding to the bicyclist in the scene)
is denoted by xd1. A higher order clique is deﬁned over this detection window by connecting the
object pixels xd1 to an auxiliary variable yd1 ∈{0, 1}. This variable allows the inclusion of
detector responses as soft constraints. (Best viewed in colour)
ψd(xd, Hd, ld) =
yd∈{0,1}(−f(xd, Hd)yd + g(Nd, Hd)yd).
A stronger classiﬁer response Hd indicates an increased likelihood of the presence
of an object at a location. This is reﬂected in the function f(·), which should be monotonically increasing with respect to the classiﬁer response Hd. As we also wish to penalize inconsistency, the function g(·) should be monotonically increasing with respect
to Nd. The number of detections used in the CRF framework is determined by a threshold Ht. The hypothesis function f(·) is chosen to be a linear truncated function using
f(xd, Hd) = wd|xd| max(0, Hd −Ht),
where wd is the detector potential weight. This ensures that f(·) = 0 for all detections
with a response Hd ≤Ht. We choose the inconsistency penalizing function g(·) to be
a linear function of the number of inconsistent pixels Nd of the form:
g(Nd, Hd) = kdNd,
kd = f(xd, Hd)
where the slope kd was chosen such that the inconsistency cost equals f(·) when the
percentage of inconsistent pixels is pd.
Detectors may be applied directly, especially if they estimate foreground pixels
themselves. However, in this work, we use sliding window detectors, which provide
a bounding box around objects. To obtain a more accurate set of pixels xd that belong
to the object, we use a local colour model to estimate foreground and background
within the box. This is similar to the approach used by submissions in the PASCAL
VOC 2009 segmentation challenge. Any other foreground estimation techniques may
be used. See §3 for more details on the detectors used. Note that equation (1) could be
deﬁned in a similar fashion over superpixels.
L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr
Inference for detector potentials
One of the main advantages of our framework is that the associated energy function
can be solved efﬁciently using graph cut based move making algorithms (which
outperform message passing algorithms for many vision problems). We now
show that our detector potential in equation (3) can be converted into a form solvable
using αβ-swap and α-expansion algorithms . In contrast, the related work in 
suffers from a difﬁcult to optimize energy. Using equations (3), (4), (5), and Nd =
i∈xd δ(xi ̸= ld), the detector potential ψd(·) can be rewritten as follows:
ψd(xd, Hd, ld) = min(0, −f(xd, Hd) + kd
δ(xi ̸= ld))
= −f(xd, Hd) + min(f(xd, Hd), kd
δ(xi ̸= ld)).
This potential takes the form of a Robust P N potential , which is deﬁned as:
ψh(x) = min(γmax, min
l (γl + kl
δ(xi ̸= l))),
where γmax = f(·), γl = f(·), ∀l ̸= d, and γd = 0. Thus it can be solved efﬁciently
using αβ-swap and α-expansion algorithms as shown in . The detection instance
variables yd can be recovered from the ﬁnal labelling by computing yd as:
d∈{0,1}(−f(xd, Hd)y′
d + g(Nd, Hd)y′
Experimental Evaluation
We evaluated our framework on the CamVid and PASCAL VOC 2009 datasets.
CamVid. The Cambridge-driving Labeled Video Database (CamVid) consists of over
10 minutes of high quality 30 Hz footage. The videos are captured at 960 × 720 resolution with a camera mounted inside a car. Three of the four sequences were shot in
daylight, and the fourth sequence was captured at dusk. Sample frames from the day
and dusk sequences are shown in Figures 1 and 3. Only a selection of frames from the
video sequences are manually annotated. Each pixel in these frames was labelled as one
of the 32 candidate classes. We used the same subset of 11 class categories as 
for experimental analysis. We have detector responses for the 5 thing classes, namely
Car, Sign-Symbol, Pedestrian, Column-Pole, and Bicyclist. A small number of pixels
were labelled as void, which do not belong to one of these classes and are ignored. The
dataset is split into 367 training and 233 test images. To make our experimental setup
the same as , we scaled all the images by a factor of 3.
PASCAL VOC 2009. This dataset was used for the PASCAL Visual Object Category segmentation contest 2009. It contains 14,743 images in all, with 20 foreground (things)
What, Where & How Many? Combining Object Detectors and CRFs
classes and 1 background (stuff) class. We have detector responses for all foreground
classes. Each image has an associated annotation ﬁle with the bounding boxes and the
object class label for each object in the image. A subset of these images are also annotated with pixel-wise segmentation of each object present. We used only these images
for training our framework. It contains 749 training, 750 validation, and 750 test images.
CRF Framework
We now describe the baseline CRF formulation used in our experiments. Note that any
CRF formulation based on pixels or segments could have been used. We use the Associative Hierarchical CRF model , which combines features at different quantization
levels of the image, such as pixels, segments, and is a generalization of commonly
used pixel and segment-based CRFs. We have a base layer of variables corresponding
to pixels, and a hierarchy of auxiliary variables, which encode mid-level cues from and
between segments. Furthermore, it assumes that pixels in the same segment obtained
using unsupervised segmentation methods, are highly correlated, but are not required to
take the same label. This allows us to incorporate multiple segmentations in a principled
In our experiments we used a two level hierarchy based on pixels and segments.
Three segmentations are used for the CamVid dataset and six for the PASCAL VOC 2009
dataset; these were obtained by varying parameters of the MeanShift algorithm ,
similar to .
Pixel-based potentials. The pixel-based unary potential is identical to that used in , and is derived from TextonBoost . It estimates the probability of a pixel taking a
certain label by boosting weak classiﬁers based on a set of shape ﬁlter responses. Shape
ﬁlters are deﬁned by triplets of feature type, feature cluster, and rectangular region and
their response for a given pixel is the number of features belonging to the given cluster in the region placed relative to the given pixel. The most discriminative ﬁlters are
found using the Joint Boosting algorithm . Details of the learning procedure are
given in . To enforce local consistency between neighbouring pixels we use the
standard contrast sensitive Potts model as the pairwise potential on the pixel level.
Segment-based potentials. We also learn unary potentials for variables in higher layers (i.e. layers other than the base layer), which represent segments or super-segments
(groups of segments). The segment unary potential is also learnt using the Joint Boosting algorithm . The pairwise potentials in higher layers (e.g. pairwise potentials
between segments) are deﬁned using a contrast sensitive (based on distance between
colour histogram features) Potts model. We refer the reader to for more details on
these potentials and the learning procedure.
Detection-based potentials
The object detections are included in the form of a higher order potential over pixels
based on detector responses, as detailed in §2.2. The implementation details of this
potential are described below. In order to jointly estimate the class category, location,
L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr
Fig. 5. (a) Segmentation without object detectors, (b) Object detection results on this image showing pedestrian and sign/symbol detections, (c) Segmentation using all the detection results. Note
that one of the persons (on the left side of the image) is originally labelled as bicyclist (shown
in cyan) in (a). This false labelling is corrected in (c) using the detection result. We also show
that unary potentials on segments (trafﬁc light on the right), and object detector potentials (traf-
ﬁc light on the left) provide complementary information, thus leading to both the objects being
correctly labelled in (c). Some of the regions are labelled incorrectly (the person furthest on the
left) perhaps due to a weak detection response. (Best viewed in colour)
and segmentation of objects, we augment the standard CRF using responses of two of the
most successful detectors5: (i) histogram-based detector proposed in ; and (ii) partsbased detector proposed in . Other detector methods could similarly be incorporated
into our framework.
In , histograms of multiple features (such as bag of visual words, self-similarity
descriptors, SIFT descriptors, oriented edges) were used to train a cascaded classiﬁer
composed of Support Vector Machines (SVM). The ﬁrst stage of the cascade is a linear
SVM, which proposes candidate object windows and discards all the windows that do
not contain an object. The second and third stages are more powerful classiﬁers using
quasi-linear and non-linear SVMs respectively. All the SVMs are trained with ground
truth object instances . The negative samples (which are prohibitively large in number) are obtained by bootstrapping each classiﬁer, as follows. Potential object regions
are detected in the training images using the classiﬁer. These potential object regions
are compared with the ground truth, and a few of the incorrect detections are added to
the training data as negative samples. The SVM is then retrained using these negative
and the positive ground truth samples.
In each object is composed of a set of deformable parts and a global template.
Both the global template and the parts are represented by HOG descriptors , but
computed at a coarse and ﬁne level respectively. The task of learning the parts and
the global template is posed as a latent SVM problem, which is solved by an iterative
method. The negative samples are obtained by bootstrapping the classiﬁer, as described
Both these methods produce results as bounding boxes around the detected objects
along with a score, which represents the likelihood of a box containing an object. A
more accurate set of pixels belonging to the detected object is obtained using local
5 We thank the authors of for providing their detections on the PASCAL VOC 2009 dataset.
What, Where & How Many? Combining Object Detectors and CRFs
Table 1. We show quantitative results on the CamVid test set on both recall and intersection
vs union measures. ‘Global’ refers to the overall percentage of pixels correctly classiﬁed, and
‘Average’ is the average of the per class measures. Numbers in bold show the best performance
for the respective class under each measure. Our method includes detectors trained on the 5
“thing” classes, namely Car, Sign-Symbol, Pedestrian, Column-Pole, Bicyclist. We clearly see
how the inclusion of our detector potentials (‘Our method’) improves over a baseline CRF method
(‘Without detectors’), which is based on . For the recall measure, we perform better on 8 out
of 11 classes, and for the intersection vs measure, we achieve better results on 9 classes. Note
that our method was optimized for intersection vs union measure. Results, where available, of
previous methods are also shown for reference.
Sign-Symbol
Pedestrian
Column-Pole
Without detectors
Our method
Intersection vs Union7
Without detectors
Our method
foreground and background colour models . In our experiments we observed that
the model is robust to change in detector potential parameters. The parameter pd (from
equation (5)) can be set anywhere in the range 10% −40%. The parameter Ht (which
deﬁnes the detector threshold, equation (4)) can be set to 0 for most of the SVM-based
classiﬁers. To compensate the bias towards foreground classes the unary potentials of
background class(es) were weighted by factor wb. This bias weight and the detector
potential weight wd were learnt along with the other potential weights on the validation
set using the greedy approach presented in . The CRF was solved efﬁciently using
the graph cut based α-expansion algorithm .
Figures 2, 3 and 5 show qualitative results on the CamVid dataset. Object segmentation
approaches do not identify the number of instances of objects, but this information is
recovered using our combined segmentation and detection model (from yd variables,
as discussed in §2.3), and is shown in Figure 2. Figure 3 shows the advantage of our
soft constraint approach to include detection results. The false positive detection here
(shown as the large green box) does not affect the ﬁnal segmentation, as the other hypotheses based on pixels and segments are stronger. However, a strong detector hypothesis (shown as the purple box) reﬁnes the segmentation accurately. Figure 5 highlights
the complementary information provided by the object detectors and segment-based
6 Deﬁned as
True Positive
True Positive + False Negative.
7 Deﬁned as
True Positive
True Positive + False Negative + False Positive; also used in PASCAL VOC challenges.
L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. S. Torr
Table 2. Quantitative analysis of VOC 2009 test dataset results using the intersection vs
union performance measure. Our method is ranked third when compared the 6 best submissions in the 2009 challenge. The method UOCTTI_LSVM-MDPM is based on an object detection algorithm and reﬁnes the bounding boxes with a GrabCut style approach. The method
BROOKESMSRC_AHCRF is the CRF model used as an example in our work. We perform better
than both these baseline methods by 3.1% and 7.3% respectively. Underlined numbers in bold
denote the best performance for each class.
Background
Dining table
Motor bike
Potted plant
TV/monitor
BONN_SVM-SEGM
UOCTTI_LSVM-MDPM
NECUIUC_CLS-DTCT
LEAR_SEGDET
BROOKESMSRC_AHCRF
Our method
Fig. 6. (a) Original test image from PASCAL VOC 2009 dataset , (b) The labelling obtained
by without object detectors, (c) The labelling provided by our method which includes detector based potentials. Note that no groundtruth is publicly available for test images in this dataset.
Examples shown in the ﬁrst ﬁve rows illustrate how detector potentials not only correctly identify
the object, but also provide very precise object boundaries, e.g. bird (second row), car (third row).
Some failure cases are shown in the last row. This was caused by a missed detection or incorrect
detections that are very strong and dominate all other potentials. (Best viewed in colour)
potentials. An object falsely missed by the detector (trafﬁc light on the right) is recognized based on the segment potentials, while another object (trafﬁc light on the left)
overlooked by the segment potentials is captured by the detector. More details are provided in the ﬁgure captions. Quantitative results on the CamVid dataset are shown in
What, Where & How Many? Combining Object Detectors and CRFs
Table 1. For the recall measure, our method performs the best on 5 of the classes, and
shows near-best (< 1% difference in accuracy) results on 3 other classes. Accuracy
of “things” classes improved by 7% on average. This measure does not consider false
positives, and creates a bias towards smaller classes. Therefore, we also provide results
with the intersection vs union measure in Table 1. We observe that our method shows
improved results on almost all the classes in this case.
Qualitative results on PASCAL VOC 2009 test set are shown in Figure 6. Our approach provides very precise object boundaries and recovers from many failure cases.
For example, bird (second row), car (third row), potted plant (fourth row) are not only
correctly identiﬁed, but also segmented with accurate object boundaries. Quantitative
results on this dataset are provided in Table 2. We compare our results with the 6 best
submissions from the 2009 challenge, and achieve the third best average accuracy. Our
method shows the best performance in 3 categories, and a close 2nd/3rd in 10 others.
Note that using the detector based work (UOCTTI_LSVM-MDPM: 29.0%) and pixelbased method (BROOKESMSRC_AHCRF: 24.8%) as examples in our framework, we
improve the accuracy to 32.1%. Both the BONN and CVC methods can be
directly placed in our work, and should lead to an increase in performance.
We have presented a novel framework for a principled integration of detectors with
CRFs. Unlike many existing methods, our approach supports the robust handling of occluded objects and false detections in an efﬁcient and tractable manner. We believe the
techniques described in this paper are of interest to many working in the problem of object class segmentation, as they allow the efﬁcient integration of any detector response
with any CRF. The beneﬁts of this approach can be seen in the results; our approach consistently demonstrated improvement over the baseline methods, under the intersection
vs union measure.
This work increases the expressibility of CRFs and shows how they can be used
to identify object instances, and answer the questions: “What object instance is this?”,
“Where is it?”, and “How many of them?”, bringing us one step closer to complete scene
understanding.