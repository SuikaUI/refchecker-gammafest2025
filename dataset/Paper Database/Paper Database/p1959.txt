Carnegie Mellon University
Research Showcase @ CMU
Computer Science Department
School of Computer Science
Learning state space trajectories in recurrent neural
Barak Pearlmutter
Carnegie Mellon University
Follow this and additional works at: 
This Technical Report is brought to you for free and open access by the School of Computer Science at Research Showcase @ CMU. It has been
accepted for inclusion in Computer Science Department by an authorized administrator of Research Showcase @ CMU. For more information, please
contact .
 
NOTICE WARNING CONCERNING COPYRIGHT RESTRICTIONS:
The copyright law of the United States (title 17, U.S. Code) governs the making
of photocopies or other reproductions of copyrighted material. Any copying of this
document without permission of its author may be prohibited by law.
Learning State Space Trajectories
in Recurrent Neural Networks
Barak A. Pearlmutter
December 31, 1988
CMU-CS-88-191-
We describe a number of procedures for finding dE/dwij where E is an error functional of the
temporal trajectory of the states of a continuous recurrent network and w,y are the weights of that
network. Computing these quantities allows one to perform gradient descent in the weights to
minimize £, so these procedures form the kernels of connectionist learning algorithms. Simulations
in which networks are taught to move through limit cycles are shown. We also describe a number of
elaborations of the basic idea, such as mutable time delays and teacher forcing, and conclude with
a complexity analysis. This type of network seems particularly suited for temporally continuous
domains, such as signal processing, control, and speech.
This research was sponsored in part by National Science Foundation grant EET-8716324 and by the Office of Naval
Research under contract number N00014-86-K-0678. Barak Pearlmutter is a Fannie and John Hertz Foundation fellow.
The views and conclusions contained in this document are those of the author and should not be interpreted as representing
the official policies, either expressed or implied, of NSF, ONR, the Fannie and John Hertz Foundation or the U.S.
Government.
Introduction
Note: this is an expanded version of an earlier paper of the
same title .
Pineda has shown how to train the fixpoints of a recurrent temporally continuous generalization of backpropagation networks . Such networks are governed
by the coupled differential equations
1 dt = -yi
+ <r(Xi) + Ii
is the total input to unit z, y, is the state of unit /, Tt is
the time constant of unit z, a is an arbitrary differentiate
1, wfy are the weights, and the initial conditions
yi(t0) and driving functions It(t) are the inputs to the system.
Consider minimizing £(y), some functional of the trajectory taken by y between to and t\.
For instance,
= f£(yo(0 ~f(0)
measures the deviation of yo from
the function/, and minimizing this £ would teach the network to have yo imitate/. Below, we develop a technique
for computing dE(y)/dwij and dE(y)/dTiy thus allowing us
to do gradient descent in the weights and time constants so
as to minimize £. The computation of dE/dwij seems to
require a phase in which the network is run backwards in
time, and tricks for avoiding this are also described.
A Forward/Backward Technique
We can approximate the derivative in (1) with
which yields a first order difference approximation to (1),
We use tildes to indicate temporally discretized versions of
continuous functions. The notation y,(0 is being used as
shorthand for the particular variable representing the descrete version of yt(fo + nAt), where n is an integer and
t = to + nAt.
Let us define
In the usual case £ is of the form f£f(y(t),
df(y(t)tt) /dyi(t).
Intuitively,
measures how much a
typically <r(0 = (1 + e~*y-
in which case *'(0 = <r(0(l - *(0).
Figure 1: The infinitesimal changes to y considered in e\(t)
(left) and z{(t) (right).
small change to yi at time t affects £ if everything else is
left unchanged.
Let us define
Ut) = -rryr
where the d+ denotes an ordered derivative , with variables ordered by time. Intuitively, zt(t) measures how much
a small change to yt at time t affects £ when this change
is propagated forward through time and influences the remainder of the trajectory, as in figure 1. Of course, z; is
the limit of z, as At —* 0.
We can use the chain rule for ordered derivatives to calculate zi(t) in terms of the Zj(t+At). According to the chain
rule, we add all the separate influences that varying yi(t)
has on £. It has a direct contribution of Atei(t)t which
comprises the first term of our equation for zt(r). Varying
yi(t) by e has an effect on y,(z+ At) of e(l - At/Ti), giving
us a second term, namely (1 — At/Ti)z{t + At).
Each weight vv,y allows y t(r) to influence yj(t+At). Let us
compute this influence in stages; varying yKO by e varies
Xj(t) by ewij,
which varies c(xj(t)) by €Wija'(Xj(t)),
varies $j(t + At) by ewija
f(xj(t))At/Tj.
This gives us our
third and final term, ^jWij<rXxj(t))AtZj{t + At)/Tj.
Combining these,
2,-(0 = Atei(t) + ^ 1 - — J ~Zl(t + At)
If we put this in the form of (3) and take the limit as
0 we obtain the differential equation
For boundary conditions note that by (5) and (6) zt(fO =
Atei(t\), so in the limit as At —• 0 we have zi(t\) = 0.
Consider making an infinitesimal change dwij to
a period At starting at r. This will cause a corresponding
infinitesimal change in £ of
yMcrXxjitV—ZjiOdwij.
Unlwrsity Libraries
Carnegie Mellon University
Pittsburgh, Pennsylvania 152W
Figure 2: A lattice representation of (4).
Since we wish to know the effect of making this infinitesimal change to wxy throughout time, we integrate over the
entire interval yielding
If we substitute p, = 7f
1 into (4), find dE/dpi by proceeding analogously, and substitute 7, back in we get
One can also derive (8), (9) and (10) using the calculus
of variations and Lagrange multipliers (William Skaggs,
personal communication), or from the continuous form of
dynamic programming .
Simulation Results
Using first order finite difference approximations, we integrated the system y forward from to to t\9 set the boundary
conditions z;(fi) = 0, and integrated the system z backwards from t\ to to while numerically integrating z; cr
and Zidyi/du
thus computing dE/dwij and dE/dTi. Since
computing dzjdt
requires knowing <7'(jcj), we stored it and
replayed it backwards as well. We also stored and replayed
y, as it is used in expressions being numerically integrated.
We used the error functional
£ - 5 £ ^ \ < * - 4 )
where dt(t) is the desired state of unit / at time t and si(t)
is the importance of unit i achieving that state at that time.
Throughout, we used <r(£) = (1 + e"*)"
1. Time constants
were initialized to 1, weights were initialized to uniformly
distributed random values between 1 and - 1 , and the initial
values yt(to) were set to /,(*<)) + cr(0). The simulator used
the approximations (4) and (7) with At = 0.1.
All of these networks have an extra unit which has no
incoming connections, an external input of 0.5, and outgoing connections to all other units. This unit provides a
bias, which is equivalent to the negative of a threshold.
This detail is suppressed below.
Exclusive Or
The network of figure 3 was trained to solve the xor problem. Aside from the addition of time constants, the network topology was that used by Pineda in . We defined
E - Y^k j fify^
2dt where k ranges over the four
cases, d is the correct output, and y0 is the state of the
output unit. The inputs to the net
range over
the four possible boolean combinations in the four different cases. With suitable choice of step size and momentum
training time was comparable to standard backpropagation,
averaging about one hundred epochs.
For this task it is to the network's benefit for units to
attain their final values as quickly as possible, so there
was a tendency to lower the time constants towards 0. In
an effort to avoid small time constants, which degrade the
numerical accuracy of the simulation, we introduced a term
to decay the time constants towards 1. This decay factor
was not used in the other simulations described below, and
was not really necessary in this task if a suitably small At
was used in the simulation.
It is interesting that even for this binary task, the network
made use of dynamical behavior. After extensive training
the network behaved as expected, saturating the output unit
to the correct value. Earlier in training, however, we occasionally (about one out of every ten training sessions)
observed the output unit at nearly the correct value between t = 2 and t = 3, but then saw it move in the wrong
direction at t = 3 and end up stabilizing at a wildly incorrect value. Another dynamic effect, which was present in
almost every run, is shown in figure 4. Here, the output
unit heads in the wrong direction initially and then corrects
itself before the error window. A very minor case of diving
towards the correct value and then moving away is seen in
the lower left hand corner of figure 4.
Figure 3: The XOR network.
Figure 4: The states of the output unit in the four input cases plotted from t = 0 to t = 5 after 200 epochs of learning
The error was computed only between t = 2 and t = 3.
^ i ^ S S S
; * " * « * * »
A Circular Trajectory
We trained a network with no input units, four hidden units,
and two output units, all fully connected, to follow the
circular trajectory of figure 5. It was required to be at the
leftmost point on the circle at t = 5 and to go around the
circle twice, with each circuit taking 16 units of time. The
environment does not include desired outputs between t = 0
and t = 5, and during this period the network moves from
its initial position at (0.5,0.5) to the correct location at
the leftmost point on the circular trajectory. Although the
network was run for ten circuits of its cycle, these overlap
so closely that the separate circuits are not visible.
Upon examining the network's internals, we found that it
devoted three of its hidden units to maintaining and shaping
a limit cycle, while the fourth hidden unit decayed away
quickly. Before it decayed, it pulled the other units to the
appropriate starting point of the limit cycle, and after it
decayed it ceased to affect the rest of the network. The
network used different units for the limit behavior and the
initial behavior, an appropriate modularization.
A Figure Eight
We were unable to train a network with four hidden units to
follow the figure eight shape shown in figure 6, so we used
a network with ten hidden units. Since the trajectory of the
output units crosses itself, and the units are governed by
first order differential equations, hidden units are necessary
for this task regardless of the a- function. Training was
more difficult than for the circular trajectory, and shaping
the network's behavior by gradually extending the length
of time of the simulation proved useful.
Figure 6: Desired states d\ and d2 plotted against each other (left); actual states yi and y 2 plotted against each other at
epoch 3182 (center) and 20000 (right).
From t = 0 to t = 5 the network moves in a short loop
from its initial position at (0.5,0.5) to where it ought to
be at t = 5, namely (0.5,0.5). Following this, it goes
through the figure eight shaped cycle every 16 units of
time. Although the network was run for ten circuits of its
cycle to produce this graph, these overlap so closely that
the separate circuits are not visible.
Perturbation Experiments
In an attempt to judge the stability of the limit cycles exhibited above, we modified the simulator to introduce random
perturbations and observed the effects of these perturbations upon the cycles. It is interesting to note that the two
output units in the figure eight task appear to be phase
locked, as their phase relationship remains invariant even
in the face of major perturbations. This phase locking is
unlike the solution that a human would wire up by hand.
The limit cycle on the right in figure 6 is symmetric,
but when perturbations are introduced, as in the right of
figure 7, symmetry is broken. The portion of the limit cycle moving from the upper left hand corner towards the
lower right hand corner has diverging lines, but we do not
believe that they indicate high eigenvalues and instability.
The lines converge rapidly in the upward stroke on the
right hand side of the figure, and analogous unstable behavior is not present in the symmetric downward stroke
from the upper right hand corner towards the lower left.
Analysis shows that the instability is caused by the initialization circuitry being inappropriately activated; since the
initialization circuitry is adapted for controlling just the initial behavior of the network, when the net must delay at
(0.5,0.5) for a time before beginning the cycle by moving
lowards the lower left corner, this circuitry is explicidy not
symmetric. The diverging lines seem to be caused by this
circuitry being activated and exerting a strong influence on
the output units while the circuitry itself deactivates.
Embellishments
Time Delays
Consider a network of this sort in which signals take finite
time to travel over each link, so that (2) is modified to
Tji being the time delay along the connection from unit; to
unit i. Surprisingly, such time delays merely add analogous
time delays to (8) and (9),
+ T^ZJO + ri})dt,
while (10) remains unchanged. If we set rty = At, these
modified equations alleviate concern over time skew when
simulating networks of this sort, obviating the need for
predictor/corrector methods.
Instead of regarding the time delays as a fixed part of the
architecture, we can imagine modifiable time delays. Given
modifiable time delays, we would like to be able to learn
appropriate values for them, which can be accomplished
using gradient descent by
Zj(t)<T\Xj{t))»At
We have not yet simulated networks with modifyable time
Figure 7: The output states y{ and y2 plotted against each other for a 1000 time unit run, with all the units in the
network perturbed by a random amount about every 40 units of time. The perturbations in the circle network (left) were
of magnitude less than 0.1, and in the eight network (right) of magnitude less than 0.05.
An interesting class of architectures would have the state
of one unit modulate the time delay along some arbitrary
link in the network or the time constant of some other
unit. Such architectures seem appropriate for tasks in which
time warping is an issue, such as speech recognition, and
such architectures can certainly be accommodated by our
In the presence of time delays, it is reasonable to have
more than one connection between a single pair of units,
with different time delays along the different connections.
Such "time delay neural networks" have proven useful in
the domain of speech recognition , Having more
than one connection from one unit to another requires us
to modify our notation somewhat; weights and time delays
are modified to take a single index, and we introduce some
external apparatus to specity the source and destination of
each connection. Thus w; is the weight on a connection
between unit C(f) and unit 72(0, and r,- is the time delay
along that connection. Using this notation we write (12) as
Our equations would be more general if written in this
notation, but readability would suffer, and the translation
is quite mechanical.
Avoiding the Backwards Pass
As mentioned in section 3, the obvious way to simulate
these networks is to start at to, simulate y forward to t\
while storing it, set z(t\) = 0 and simulate z backwards
from t\ to to while replaying y. While simulating backwards, we numerically integrate according to equations (9)
and (10), thereby computing the partials of E. However,
this requires simulating backwards in time, which is not
pleasing, and it requires remembering the trajectory of y,
which takes storage linear in t\ - fo. One way to avoid
storing the trajectory of y is to simulate it backwards as we
simulate z backwards, but note that simulating y backwards
is typically numerically unstable.
Here, we consider the alternative of guessing z(fo) such
that z(fi) = 0 and doing all of our simulations forward
through time. This is not attractive on serial machines with
plentiful memory, but might be more attractive on parallel
machines with limited storage. These complexity issues
are discussed in section 5.2.
Let us find a way to compute dzi{t\)/dzj(to). We define
and take the partial of (8) with respect to z ;(r 0), substituting in Cy where appropriate. This results in a differential
equation for Qljy
and for boundary conditions we note that
s { 0 otherwise.
Given guesses for the correct value of z(r 0), we will
simulate y, z and C forward from / 0 to t\ and then update
the guesses in order to minimize B where
with a shooting method by making use of the fact that
For notational convenience, let b{ = dB/dzfa).
use a Newton-Raphson method with the appropriate modification for the fact that B has a minimum of zero, resulting
in the simple update rule
— Zi(t0) -
Zrrr-rrrbi.
During our simulation we accumulate the appropriate integrals, so if our guesses for zt(f0) were nearly correct we
will have computed nearly correct values for dE/dwij and
If the w,y change slowly the correct values for
z,(fo) will change slowly, so tolerable accuracy can be obtained by using the dE/dwij computed from the slightly
incorrect values for z,(fo) while simultaneously updating
the zt(/o) for future use, eliminating the need for an inner
loop which iterates to find the correct values for the z,(/o).
This argument assumes that the quadratic convergence of
the Newton-Raphson method dominates the linear divergence of the changes to the w,y, which can be guaranteed
by choosing suitably low learning parameters. Regrettably,
it also assumes that the forward simulation of z is numerically stable enough for our purposes, which is typically not
the case for long trajectories.
An Online Variation
We can use the technique of Williams and Zipser to
create an online version of our algorithm. Let us define
and note that
If we begin with (1), substitute k for i, take the partial with
respect to w,,, and substitute in p where possible, we have
a differential equation for p ,
which is stable in the forward direction. To construct an
online algorithm we simulate the systems y and p forward
through time and continuously update the weights to do
gradient descent using (23), spreading the weight update
across time using the continuous update rule
We can derive analogous equations for the time constants; define
take the partial of (1) with respect to 7y, and substitute in
q. This yields
which can be used to update the time constants using the
continuous update rule
Similarly, let us derive equations for modifying the time
delays of section 4.1. Define
and take the partial of (1) with respect to r y, arriving at a
differential equations for r,
Tk-jj- = -r* + <r\xk)(wy-£(t
- ri}) - ] T w^t
included if j = k
The time delays can be updated online using the continuous
update equation
Teacher Forcing
Williams and Zipser report that their teacher forcing technique radically improves learning time in recurrent networks . Teacher forcing involves using the training
signal to modify the states of units to desired values as
the network is run. Williams and Zipser's application of
teacher forcing to their networks is deeply dependent on
discrete time steps, so applying teacher forcing to temporally continuous networks requires a different approach.
The essential idea is that we will add some knobs that can
be used to control the states of the output units, we will use
them to keep the output units locked at some desired states,
and we will minimize an error functional which measures
the amount of control we have exerted.
r.- = ^ ( - y i + *(*.0 + /.-)
so that (1) is just dyi/dt = F,-, and add a new forcing term
MO to (1),
Let the set of forced units be
For each i e <P let dL be
the trajectory that we will force y, to follow, so we set
and yi(t0) = di(t0) for i € # and / = 0 for i g <£, with
the consequence that y, = 4 for i e $. Now let the error
functional be of the form
Jto L(fu...,fn,t)dt,
where typically L =
We can modify the derivation in section 2 for this
"teacher forced" system. For i € # a change to y, will
be canceled immediately, so taking the limit as At —* 0
yields z,- = 0. Because of this, it doesn't matter what e, is
We can apply (5) to calculate a for i £ $. The chain rule
is used to calculate how a change in y» effects E through
the/1, yielding
For i £ $ (8) and (10) are unchanged, and for j & $ and
any i (9) also remains unchanged. The only equations still
required are dE/dwij for ; € # and dE/dTL for i e
derive the first, consider the instantaneous effect of a small
change to wijy
Analogously, for / 6 0
We are left with a system with a number of special cases
depending on whether units are in <P or not. Interestingly,
an equivalent system results if we leave (8), (9), and (10)
unchanged except for setting zt = dL/dfi for i € <P and
setting all the ev = 0. It is an open question as to whether
there is some other way of defining z, and e< that results in
this simplification.
Computational Power
It would be useful to have some characterization of the
class of trajectories that a network can learn as a function of
the number of hidden units. We are investigating this area,
and have some preliminary results. These networks have at
least the representational power of Fourier decompositions,
as one can use a pair of nodes to build an oscillator of
arbitrary frequency by making use of the local linearity
of the a function, so one can take the first n terms of a
function's Fourier decomposition and analytically find a set
of weights for a network with In + 1 nodes that generates
this approximation to the function (Merrick Furst, personal
communication).
We can also derive some fairly straightforward bounds
on the possible ranges of the states and their derivatives.
We use n for the number of units in the network, and the
notation max |/| is used to delimit the maximum absolute
value attainable by any
max|y| < max|<r| +max|/|
max|j«| (max
(1 + n max|cr'| max|w|)
This bounds the rate at which the network's state can
change, and the rate at which its velocity vector can change,
thus limiting the class of trajectories that may be learned.
But it does not limit the complexity (number of squiggles)
of a trajectory, provided it is sufficiendy slow moving. A
stronger notion of trajectory complexity would be desirable.
Complexity
Consider a network with n units and m weights which is
run for s time steps
2 where s = (h — to)/At. Additionally,
assume that the computation of each e4(r) is 0(1) and that
the network is not partitioned.
Under these conditions, simulating the y system takes
0(m + n) = 0{m) time for each time step, as does simulating the z system. This means that using the technique described in section 3, the en are simulation takes 0(m) time
per time step, the best that could be hoped for. Storing
the activations and weights takes 0(n + m) = 0(m) space,
and storing y during the forward simulation to replay while
simulating z backwards takes 0(sn) space, so if we use this
technique the entire computation takes 0(sn + m) space. If
we simulate y backwards during the backwards simulation
of z, the simulation requires 0(n + m) space, again the best
that could be hoped for. This later technique, however, is
susceptible to numeric stability problems.
Maintaining the Cy terms of section 4.2 takes 0(nm) time
each time step, and 0(n
2) space. These are the dominant
factors in the calculation of the partials of B. The technique
of Williams and Zipser described in section 4.3 requires
2m) time each time step, and 0(nm) space.
These time complexity results are for sequential machines, and are summarized in table 1. All these algorithms
are embarrassingly parallel and eminently suitable for implementation on both vector processors and highly parallel
We can analytically determine the stability of the network
by measuring the eigenvalues At- of Df where / is the
function that maps the state of the network at one point in
time to its state at a later time. For instance, for a network
exhibiting a limit cycle one would typically use the function
that maps the network's state at some time in the cycle to
its state at the corresponding time in the next cycle. It
is tempting to introduce a term to be minimized which
rewards the network for being stable, for instance £ , ^?
where A, is an eigenvalue of Df. Regrettably, computing
Df is costiy, so we are investigating ways to add terms to
2Variable grid methods can reduce s by dynamically varying At.
Table 1: A summary of the complexity of some learning
procedures for recurrent networks. In the "store y" technique we store y as time is run forwards and replay it as we
run time backwards computing z. In "back y" we do not
store y, instead recomputing it as time is run backwards.
W&Z is the technique of Williams m d Zipser.
E which measure weaker but more economically computed
criteria of stability than max, |A,| < 1, such as
Vl+DeKD/); '
We conjecture that the apparent noise tolerance shown in
the simulations of section 3.4 is caused by the learning algorithm running in the presence of noise introduced by the
conversion from differential equations to difference equations and perhaps floating point roundoff errors. This leads
to the thought of enhancing the stability of the solutions
that the learning algorithm derives by deliberately injecting noise into the system during training, thus punishing
the algorithm for even short stretches of instability.
Future Work
Our next experiments will involve using inputs to specify a
member of a class of continuous tasks, and testing generalization to novel inputs. We will make a network with two
inputs and two outputs, where the inputs are used to specify
the radius and cycle time of a circle to be traced out on the
two output units. After that, we would like to experiment
with more complex error functional, involving dyi/dt and
correspondences between states at different points in time.
We also plan on simulating networks with adjustable time
delays, something which we have not experimented with
at all to date.
In the longer term, there are obvious applications to identification and control, some of which will be explored in
the author's thesis research. Signal processing and speech
generation and recognition (using generative techniques)
are also domains to which this type of network can be
naturally applied. Such domains may lead us to complex
architectures like those discussed in section 4.1. For control domains, it seems important to have ways to force
the learning towards solutions that are stable in the control sense of the term, so we are attempting to develop the
ideas hinted at in section 5.3 into workable additions to the
learning algorithm.
On the other hand, we can turn the logic of section 5.3
around. Consider a difficult contraint satisfaction task of
the sort that neural networks are sometimes applied to, such
as the traveling salesman problem . Two competing
techniques for such problems are simulated annealing 
and mean field theory . By providing a network with
a noise source which can be modulated (by second order
connections, say) we could see if the learning algorithm
constructs a network that makes use of the noise to generate networks that do simulated annealing, or if pure gradient descent techniques are evolved. If a hybrid network
evolves, its structure may give us insight into the relative
advantages of these two different optimization techniques.
Relation to Other Work
We use the same class of networks used by Pineda , but
he is concerned only with the limit behavior of these networks, and completely suppresses all other temporal behavior. His learning technique is applicable only when the network has a simple fixpoint; limit cycles or other non-point
attractors violate a mathematical assumption upon which
his technique is based.
We can derive Pineda's equations from ours. Let /, be
held constant, assume that the network settles to a fixpoint,
let the initial conditions be this fixpoint, i.e., yt('o) = yt(oo),
and let E measure Pineda's error integrated over a short interval after to, with an appropriate normalization constant.
As t\ tends to infinity, (8) and (9) reduce to Pineda's equations, so in a sense our equations are a generalization of
Pineda's; but these assumptions strain the analogy.
Jordan uses a conventional backpropagation network
with the outputs clocked back to the inputs to generate temporal sequences. The treatment of time is the major difference between Jordan's networks and those in this work.
The heart of Jordan's network is atemporal, taking inputs to
outputs without reference to time, while an external mechanism is used to clock the network through a sequence of
states in much the same way that hardware designers use
a clock to drive a piece of combinatorial logic though a
sequence of states. In our work, the network is not externally clocked; instead, it evolves continuously though time
according to a set of coupled differential equations.
Williams and Zipser have discovered an online
learning procedure for networks of this sort; a derivation
of their technique is given in section 4.3 above.
Acknowledgments
We thank Richard Szeliski for helpful comments and David
Touretzky for unflagging support.