International Journal of Electrical and Computer Engineering (IJECE)
Vol. 11, No. 5, October 2021, pp. 4392~4402
ISSN: 2088-8708, DOI: 10.11591/ijece.v11i5.pp4392-4402
Journal homepage: 
Artificial neural network technique for improving prediction of
credit card default: A stacked sparse autoencoder approach
Sarah. A. Ebiaredoh-Mienye, E. Esenogho, Theo G. Swart
Centre for Telecommunication, Department of Electrical and Electronic Engineering Science,
University of Johannesburg, Johannesburg, South Africa
Article Info
Article history:
Received Jun 21, 2020
Revised Mar 16, 2021
Accepted Mar 26, 2021
Presently, the use of a credit card has become an integral part of
contemporary banking and financial system. Predicting potential credit card
defaulters or debtors is a crucial business opportunity for financial
institutions. For now, some machine learning methods have been applied to
achieve this task. However, with the dynamic and imbalanced nature of
credit card default data, it is challenging for classical machine learning
algorithms to proffer robust models with optimal performance. Research has
shown that the performance of machine learning algorithms can be
significantly improved when provided with optimal features. In this paper,
we propose an unsupervised feature learning method to improve the
performance of various classifiers using a stacked sparse autoencoder
(SSAE). The SSAE was optimized to achieve improved performance. The
proposed SSAE learned excellent feature representations that were used to
train the classifiers. The performance of the proposed approach is compared
with an instance where the classifiers were trained using the raw data. Also, a
comparison is made with previous scholarly works, and the proposed
approach showed superior performance over other methods.
Artificial neural network
Credit card default
Deep learning
Feature learning
Machine learning
Sparse autoencoder
Unsupervised learning
This is an open access article under the CC BY-SA license.
Corresponding Author:
Ebenezer Esenegho
Centre for Telecommunication
Department of Electrical and Electronic Engineering Science
University of Johannesburg, Kingsway Avenue and University Road, Auckland Park
Johannesburg 2006, South Africa
Email: 
INTRODUCTION
In artificial intelligence and machine learning, tasks such as classification and clustering, the input
data tends to influence the performance of the algorithms. Optimal performance is obtained when algorithms
are given suitable data. To this end, some machine learning methods focus on processing high dimensional
data, including linear dimensionality reduction methods such as linear discriminant analysis, principal
component analysis, and multiple dimensional scaling and nonlinear dimensionality reduction methods such
as isometric feature mapping, locally linear embedding, and Laplacian Eigenmap. Meanwhile, feature
engineering and representation learning are the two main methods used to achieve representation from raw
data. Recent research has focused on the latter since feature engineering methods are usually dependent on
domain knowledge, are labour-intensive, and time-consuming . Furthermore, representation learning
methods tend to learn a representation from data automatically, which can then be used for classification. An
autoencoder (AE) is a type of unsupervised representation learning.
Int J Elec & Comp Eng
ISSN: 2088-8708
Artificial neural network technique for improving prediction of credit cardâ€¦ (Sarah. A. Ebiaredoh-Mienye)
Autoencoders are unsupervised neural networks with three layers, including an input layer, a hidden
layer, and an output layer. The output layer can be considered as the reconstruction layer . The structure of
a basic autoencoder is shown in Figure 1. Autoencoders tend to learn a representation of the input data,
usually for dimensionality reduction, by training the network to ignore noise. Together with the reduction
side, a reconstructing side is learned, where the AE attempts to create a representation of the original input
 , . There are different types of autoencoders, including sparse, denoising, contractive, variational, and
convolutional autoencoders .
Figure 1. Structure of an autoencoder
Credit card default/fraud detection is a crucial problem that has gotten the attention of machine
learning researchers, and a significant number of approaches have been proposed - . However, the
problem is still challenging since most credit card data seem to suffer from class imbalance as non-fraud
transactions overwhelmingly supersede fraud transactions, making it difficult for many machine learning
algorithms to achieve good performance. Meanwhile, a good feature representation can be obtained from the
dataset, which can enhance the classification performance of the algorithms. Representation learning is a
possible solution to the challenge of credit card default and fraud prediction because of its remarkable feature
learning ability in large and unbalanced datasets. While basic autoencoders aim at learning a representation
or encode data by training the network to ignore noise and reconstructing the data as close as possible to the
input data, however, training the autoencoder network in such a way that encourages sparsity can result in
optimal feature learning. Sparsity induced neural networks have been extensively applied in image
recognition and several other applications resulting in state-of-the-art performance - .
In this paper, an approach is proposed to improve the classification performance of classifiers by
using the unsupervised feature learning capability of autoencoders. During the training of the autoencoder,
sparsity is encouraged, and the model is optimized using the AdaMax algorithm instead of the
conventional stochastic gradient descent. To ensure accurate feature representation, we stack two sparse
autoencoders to get the final model. Also, to further prevent overfitting and enhance the performance, speed,
and stability of the network, we introduced the batch normalization technique to the network. The lowdimensional features are then used to train various classifiers, including logistic regression (LR),
classification and regression tree (CART), k-nearest neighbor (KNN), support vector machine (SVM), and
linear discriminant analysis (LDA). The performance of the proposed method is compared with an instance
where the classifiers were trained with the raw data. Further comparison is made with other scholarly works,
and our proposed method shows better performance. The main contributions of this study can be summarized
is being as:
ï€­ To construct an effective artificial neural network for feature learning using multiple layers of sparse
autoencoder.
ï€­ To improve the classification performance of various classifiers using the proposed stacked sparse
autoencoder.
ISSN: 2088-8708
Int J Elec & Comp Eng, Vol. 11, No. 5, October 2021: 4392 - 4402
ï€­ To demonstrate the effectiveness of the proposed method by applying it to a popular credit card dataset.
The rest of the paper is organized is being as. In section 2, we briefly review previous related works
that utilized different types of autoencoders. Section 3 presents the proposed method and section 4 provides a
brief case study of credit card defaulting prediction models. The obtained results are presented and discussed
in section 5. Lastly, section 6 concludes the paper and highlights some future research directions.
RELATED WORKS
Recently, autoencoders have been applied to several tasks, and they achieved state-of-the-art
performance. In this section, we discuss some previous works that utilized various autoencoders and lay the
foundation for the proposed stacked sparse autoencoder network. Sun et al. proposed a method for fault
diagnosis by applying a sparse stacked denoising autoencoder for feature extraction due to its robustness and
data reconstruction capability, which improved the diagnostic accuracy. The autoencoder was used together
with an optimized transfer learning algorithm. Similarly, Zhu et al. proposed a novel stacked pruning
sparse denoising autoencoder for intelligent fault diagnosis of rolling bearings. The method comprised of a
fully connected autoencoder network, connecting the optimal features extracted from previous layers to
subsequent layers. To effectively train the autoencoder, a pruning operation was added to the model to
restrict non-superior units from participating in all subsequent layers. When compared with other fault
diagnostic models, their approach showed superior performance.
Furthermore, Sankaran et al. proposed a feature extraction method using an autoencoder
network, and â„“2,1-norm based regularization was used to achieve sparsity. The authors identified that due to
the presence of many training parameters, several feature learning models are susceptible to overfitting, and
different regularization approaches have been studied in literature to mitigate overfitting in deep learning
models. The performance of their model was studied on publicly available latent fingerprint datasets, and it
gave an improved performance. Chen et al. proposed a method to address the challenge of learning
efficiency and computational complexity in deep neural networks. The technique used a deep sparse
autoencoder network to learn facial features and softmax regression applied to classify expression features.
The softmax regression aimed at handling extensive data in the output of the autoencoder network. Also, to
overcome local extrema and the challenge of gradient diffusion during training, the network weights were
finetuned, and this improved the performance of the architecture.
Most approaches used to implement autoencoders depend on the single autoencoder model, and this
presents a problem when learning different characteristics of data. Yang et al. proposed a method to
solve the problem by implementing a feature learning framework using serial autoencoders. The technique
achieved superior representation learning by serially connecting two different types of autoencoders. The
approach incorporated two encoding stages using a marginalized denoising autoencoder and a stacked robust
autoencoder via graph regularization. When compared to baseline methods, the proposed approach showed
significant improvement. Meanwhile, Al-Hmouz et al. , introduced a logic-driven autoencoder, whereby
the network structure was achieved using some fuzzy logic operations. The autoencoder was also optimized
using gradient-based learning. Lastly, sparse autoencoder networks have achieved remarkable performance in
representation learning , . However, better representation learning can be gotten when multiple
sparse autoencoders are stacked and optimized effectively, which is the focus of this research.
PROPOSED METHOD
This section considers the method applied to developing the proposed autoencoder. An autoencoder
consists of two functions, i.e., an encoder and decoder, the former maps the d-dimensional input data to get a
hidden representation, and the latter maps the hidden representation back to a d-dimensional vector that is as
close as possible to the encoder input . Assuming the original input is ğ‘¥, the autoencoder encodes it into a
hidden layer â„ to reduce the input dimension, which is then decoded at the output. The input vector is
encoded according to:
â„= ğœ(ğ‘Šğ‘¥+ ğ‘)
where ğœ represents the activation function; in this case, the sigmoid activation function, ğ‘Š is the weight
matrix, and ğ‘ is a bias vector. The hidden representation is decoded to get the data as close as possible to the
input ğ‘¥ using:
ğ‘¥Ì‚ = ğœ(ğ‘Šâ€²â„+ ğ‘â€²)
Int J Elec & Comp Eng
ISSN: 2088-8708
Artificial neural network technique for improving prediction of credit cardâ€¦ (Sarah. A. Ebiaredoh-Mienye)
where ğ‘Šâ€² is weight matrix and ğ‘â€² represents the bias vector . The sigmoid activation function is described
The disparity between the original input ğ‘¥ and the reconstructed input ğ‘¥Ì‚ is called the reconstruction
error. To optimize the parameters W, ğ‘Šâ€², ğ‘, ğ‘â€², the mean squared error (MSE) function is used as the
reconstruction error function:
The average activation of neurons in the hidden layer is represented as:
To induce sparsity in the autoencoder, we limit ğœŒğ‘—Ì‚ = ğœŒ, where ğœŒ is the sparsity proportion, and it is
usually a small positive number near 0. Therefore, we try to minimize the kullback-leibler (KL) divergence
between ğœŒğ‘—Ì‚ and ğœŒ according to:
ğ¾ğ¿(ğœŒ||ğœŒÌ‚) = ğœŒlog (
Ì‚) + (1 âˆ’ğœŒ)log (
Also, to ensure better feature representation and, by extension, enhance the performance of the
classifiers, multiple sparse autoencoders are stacked. A stacked sparse autoencoder (SSAE) can comprise of
numerous sparse autoencoders whereby the outputs of each layer are connected to the inputs of the next layer
 . The SSAE is based on research conducted by Hinton and Salakhutdinov , where they proposed a
deep neural network with layer by layer initialization. The error function of the SSAE is expressed as:
2 â€–ğœ(ğ‘Šğ‘¥(ğ‘–) + ğ‘) âˆ’ğ‘¦(ğ‘–)â€–
where ğ‘ and ğ‘› represents the number of samples and the number of layers, respectively, the original input is
ğ‘¥, and ğ‘¦ denotes the corresponding label. The regularization coefficient is represented by ğœ†. ğ‘ ğ‘™
the rows and columns of the matrix ğ‘Šğ‘—ğ‘–
(ğ‘™) . By adding the sparsity term to (7), the overall cost function of
the SSAE becomes:
ğ¿ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’(ğ‘Š, ğ‘) = ğ½(ğ‘Š, ğ‘) + ğ›½âˆ‘
where S represents the total number of neurons in a layer and ğ›½ is the sparsity regularization parameter, and it
sets the sparsity penalty term. We now have three optimization parameters, including ğ›½, ğœ†, and ğœŒ, and we set
their values as 3, 0.0001, and 0.05, respectively. In the sparse autoencoder network, a neuron is said to be
active if its output is a value close to 1, while it is inactive if its output is a value closer to 0 . Algorithm 1
shows the proposed sparse autoencoder procedure. Figure 2 shows the structure of the proposed stacked
sparse autoencoder (SSAE). For simplicity, the decoder parts of the SAE are not shown. The output of the
SSAE is then used to train the various classifiers.
Algorithm 1. Proposed method of the SSAE
train set x
Initialize ğœ, ğ‘Š, ğ‘Šâ€², ğ‘, ğ‘â€²
Obtain the cost function according to (4)
Apply weight penalty to the cost function according to (7)
Add the sparsity regularizer to the cost function according to (8)
Train network until convergence
Reconstructed representation of the input
ISSN: 2088-8708
Int J Elec & Comp Eng, Vol. 11, No. 5, October 2021: 4392 - 4402
The greedy layer-wise training strategy proposed by Bengio et al. is employed to successively
train every layer of the SSAE in order to obtain access to the weights and bias parameters of the network.
Also, the network is finetuned using the backpropagation algorithm to obtain the best parameter settings. The
AdaMax algorithm , a variant of the adaptive moment estimation (Adam) algorithm that uses the infinity
norm, was applied to optimize the autoencoder network. Lastly, we introduced the batch normalization
technique to prevent overfitting and enhance the performance, speed, and stability of the network.
Figure 2. Structure of the proposed SSAE model
CASE STUDY OF CREDIT CARD DEFAULTING PREDICTION MODELS
Credit risk plays a crucial role in the financial industry. Most financial institutions grant loans,
mortgage, and credit cards, among many other services. Due to the rising number of credit card clients, these
institutions have faced an increasing default rate. They are thereby resorting to the use of machine learning
methods to automate the application process and predict the probability of a clientâ€™s future default. However,
several machine learning methods have been developed in various literature with varying performance. A
major limitation to achieving optimal performance in the credit card default prediction is that the datasets are
highly imbalanced, i.e., the instances where clients do not default are more than the defaulting cases.
Certain studies have used the default of credit card clients dataset and achieved good
performance. For example, Prusti and Rath used various algorithms such as decision tree, KNN, SVM,
and multilayer perceptron to make predictions on the dataset. Additionally, they proposed a method that
hybridized decision tree, SVM, and KNN, which gave improved performance compared to the stand-alone
algorithms. Sayjadah et al. conducted a performance evaluation of credit card default prediction using
logistic regression, random forest, and decision tree. The experimental results showed that random forest
achieved superior performance with an accuracy of about 82%.
Furthermore, because the dataset is imbalanced, a method is proposed to tackle the problem using
synthetic minority over-sampling technique (SMOTE) . Using the SMOTE method together with seven
other algorithms, the random forest algorithm achieved the best performance with an accuracy of 89.01% and
F1-score of 89%. Lastly, Hsu et al. and Chishti and Awan also proposed models to predict the
defaulting of credit card clients and achieved comparable performance. However, we are aiming to improve on
what has been done by applying our proposed method on the same dataset.
RESULTS AND DISCUSSION
In this work, the defaulting of the credit card client dataset is used. The dataset was obtained
from the University of California Irvine (UCI) machine learning repository, and it contains 30,000 instances
and 25 attributes, including demographic and financial records. The dataset was established to predict
customers who are likely to default on payments in Taiwan. Out of the 30,000 instances 23,364 are non-
Int J Elec & Comp Eng
ISSN: 2088-8708
Artificial neural network technique for improving prediction of credit cardâ€¦ (Sarah. A. Ebiaredoh-Mienye)
default and 6,636 are default cases. The rationale behind the dataset is for financial institutions to be able to
identify possible customers who will default on their credit card payments, thereby declining such
applications. We use the 70-30% train-test split. The SSAE is trained with the training set in an unsupervised
fashion, while the test set is input with the learned SSAE model to obtain the low-dimensional data. The
classifiers are then trained using the low-dimensional train set, and the performance tested using the lowdimensional test set. The number of neurons in the first and second hidden layers was set at 100 and 85,
respectively.
To efficiently evaluate the performance of our approach, we utilize performance metrics such as
accuracy, sensitivity, precision, and F1 score. Accuracy is the ratio of the number of correct predictions to the
total number of predictions made, sensitivity is the ratio of the number of correct positive predictions to the
total actual positives, precision is the number of correct positive predictions divided by the number of positive
results predicted, and F1 score is the harmonic mean between precision and sensitivity. Mathematically, the
performance metrics can be represented as:
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘
See appendix for the complete derivation of F1 score:
ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦=
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=
2Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›Ã—ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
Therefore, it can be represented as (13):
ğ¹1 ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’= 2 [
For simplicity, F1 score in (13) can be derived is being as:
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ ) (
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ ) + (
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ ) + (
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)] Ã· [(
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ ) + (
Finding the lowest common factor
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)] Ã· [ğ‘‡ğ‘ƒ(ğ‘‡ğ‘ƒ+ ğ¹ğ‘) + ğ‘‡ğ‘ƒ(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)
Open the bracket
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)] Ã· [
(ğ‘‡ğ‘ƒ )2 + ğ‘‡ğ‘ƒğ¹ğ‘+ (ğ‘‡ğ‘ƒ )2 + ğ‘‡ğ‘ƒğ¹ğ‘ƒ
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)
Bring like terms together and factorize.
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘) Ã· 2(ğ‘‡ğ‘ƒ )2 + ğ‘‡ğ‘ƒ(ğ¹ğ‘+ ğ¹ğ‘ƒ)
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)
Invert and cancel out like terms
ISSN: 2088-8708
Int J Elec & Comp Eng, Vol. 11, No. 5, October 2021: 4392 - 4402
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘) Ã—
(ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ)(ğ‘‡ğ‘ƒ+ ğ¹ğ‘)
2(ğ‘‡ğ‘ƒ )2 + ğ‘‡ğ‘ƒ(ğ¹ğ‘+ ğ¹ğ‘ƒ)
2(ğ‘‡ğ‘ƒ )2 + ğ‘‡ğ‘ƒ(ğ¹ğ‘+ ğ¹ğ‘ƒ)
Factorized TP out of the denominator
ğ‘‡ğ‘ƒ[(2ğ‘‡ğ‘ƒ) + (ğ¹ğ‘+ ğ¹ğ‘ƒ)]
For simplicity we have,
ğ¹1 ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’= [
2ğ‘‡ğ‘ƒ+(ğ¹ğ‘ƒ+ğ¹ğ‘)]
where ğ‘‡ğ‘ƒ, ğ‘‡ğ‘, ğ¹ğ‘ƒ, ğ¹ğ‘ stands for the number of true positives, the number of true negatives, the number of
false positives, and the number of false negatives, respectively. Meanwhile, all the experiments were carried
out using a computer with the following specifications: Intel Core i5-6300U, 2.40 GHz, with 16 GB RAM.
And Python programming language was used for the computations. To show the effectiveness of the
proposed approach, we conduct a comparative study with five base classifiers. Therefore, we first show the
performance of these classifiers on the raw dataset. The classifiers include CART, LR, KNN, SVM, and LDA,
and the results are shown in Table 1.
Table 1. Performance of the base classifiers on the dataset
Accuracy (%)
Precision (%)
Sensitivity (%)
F1 score (%)
Table 2 summarizes the results obtained when the classifiers are trained using the features learned
from the stacked sparse autoencoder. It can be seen that the learned features significantly improve the
performance of the classifiers. Furthermore, the results show the ability of the proposed SSAE to learn a good
representation of the data. To further show the effectiveness of the proposed method, the best performing
model from our experiments, which is the LDA, is used to compare with other well-performing methods
proposed in recent studies that have been discussed in section 4. To give a fair comparison, we focused on
studies that used similar datasets. This comparison is shown in Table 3, and it can be seen that our method
outperforms those in the stated literature. Also, the receiver operating characteristic (ROC) curve is employed
to show the improved performance of the SSAE based LDA compared to the LDA that was trained with the
raw dataset. The ROC curve is a graphical plot which shows the prediction performance of binary classifiers.
From the ROC curve shown in Figure 3, it can be seen that the proposed method performed better than the
conventional LDA.
Table 2. Impact of the features learned by the SSAE on the base classifiers
Accuracy (%)
Precision (%)
Sensitivity (%)
F1 score (%)
From the above results, we can see that our proposed approach achieved better performance
compared to the other methods. The improved performance can be attributed to the proposed SSAE that was
able to learn a good representation of the original input data. Also, the results have shown the capability of
Int J Elec & Comp Eng
ISSN: 2088-8708
Artificial neural network technique for improving prediction of credit cardâ€¦ (Sarah. A. Ebiaredoh-Mienye)
deep learning in achieving exceptional performance in different tasks, including feature representation. Lastly,
this study has demonstrated the importance of training machine learning algorithms with suitable data, and that
improved performance can be obtained not only by hyper-parameter tuning but also and more efficiently by
effective feature learning.
Table 3. Comparison with other methods
Literature
Accuracy (%)
Precision (%)
Sensitivity (%)
F1 score (%)
Prusti and Rath 
Sayjadah et al. 
Subasi and Cankurt 
Hsu et al. 
Chishti and Awan 
Proposed SSAE+LDA
Figure 3. ROC Curve showing improved performance
CONCLUSION
Conventional machine learning algorithms are often ineffective in performing classification on large
datasets such as most credit card datasets. Hence, in this paper, a stacked sparse autoencoder is proposed to
achieve optimal feature learning. In the proposed autoencoder network, we introduced a batch normalization
technique to enhance the performance, speed, and stability of the model and to prevent overfitting further.
Also, the model was optimized using the AdaMax algorithm. The learned data was then used to train five
shallow machine learning algorithms, and the performance tested. When compared with a case where the
algorithms were trained with the raw data, our proposed method showed superior performance. Furthermore,
the results were compared with methods in some literature that used a similar dataset, and the proposed
approach also showed significant improvement. Future research will focus on studying the effect of different
optimizers and stacking diverse autoencoders and observing the resultant impact on the feature learning
process. Also, future research will consider comparing the feature leaning capability of the stacked sparse
autoencoder with other feature learning and feature engineering methods.
ACKNOWLEDGEMENTS
This work is supported partially by the Center of Telecommunications, University of Johannesburg,
South Africa. This research received no external funding. The APC will be paid from our research center