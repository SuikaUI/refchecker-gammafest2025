EXPOSING DEEP FAKES USING INCONSISTENT HEAD POSES
Xin Yang⋆, Yuezun Li⋆and Siwei Lyu
University at Albany, State University of New York, USA
In this paper, we propose a new method to expose AIgenerated fake face images or videos (commonly known
as the Deep Fakes). Our method is based on the observations
that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing
errors that can be revealed when 3D head poses are estimated
from the face images. We perform experiments to demonstrate this phenomenon and further develop a classiﬁcation
method based on this cue. Using features based on this cue,
an SVM classiﬁer is evaluated using a set of real face images
and Deep Fakes.
Index Terms— Media Forensics, DeepFake Detection,
Head Pose Estimation
1. INTRODUCTION
Thanks to the recent developments of machine learning, the
technologies for manipulating and fabricating images and
videos have reached a new level of sophistication. The cutting edge of this trend are the so-called Deep Fakes, which are
created by inserting faces synthesized using deep neural networks into original images/videos. Together with other forms
of misinformation shared through the digital social network,
Deep Fakes created digital impersonations have become a serious problem with negative social impact . Accordingly,
there is an urgent need for effective methods to expose Deep
To date, detection methods of Deep Fakes have relied
on artifacts or inconsistencies intrinsic to the synthesis algorithms, for instance, the lack of realistic eye blinking and
mismatched color proﬁles . Neural network based classi-
ﬁcation approach has also been used to directly discern real
imagery from Deep Fakes . In this work, we propose a new
approach to detect Deep Fakes. Our method is based on an
intrinsic limitations in the deep neural network face synthesis
models, which is the core component of the Deep Fake production pipeline. Speciﬁcally, these algorithms create faces
of a different person but keeping the facial expression of the
original person. However, the two faces have mismatched
facial landmarks, which are locations on human faces corresponding to important structures such as eye and mouth tips,
⋆The authors contribute equally.
as the neural network synthesis algorithm does not guarantee
the original face and the synthesized face to have consistent
facial landmarks, as shown in Fig. 1.
The errors in landmark locations may not be visible directly to human eyes, but can be revealed from head poses
(i.e, head orientation and position) estimated from 2D landmarks in the real and faked parts of the face. Speciﬁcally, we
compare head poses estimated using all facial landmarks and
those estimated using only the central region, as shown in Fig.
1. The rationale is that the two estimated head poses will be
close for the original face, Fig. 1(k). But for a Deep Fake, as
the central face region is from the synthesized face, the errors
due to the mismatch of landmark locations from original and
generated images aforementioned will lead to a larger difference between the two estimated head poses, Fig. 1(n). We
experimentally conﬁrm the signiﬁcant difference in the estimated head pose in Deep Fakes. Then we use the difference in
estimated head pose as a feature vector to train a simple SVM
based classiﬁer to differentiate original and Deep Fakes. Experiments on realistic Deep Fake videos demonstrate the effectiveness of our algorithm.
2. DEEP FAKE PRODUCTION PIPELINE
The overall process of making Deep Fakes is illustrated in
Fig. 1(a)-(h). To generate a Deep Fake, we feed the algorithm
an image (or a frame from a video) that contains the source
face to be replaced. Bounding box of faces are obtained with
a face detector, followed by the detection of facial landmarks.
The face area is warped into a standard conﬁguration through
afﬁne transformation M, by minimizing the alignment errors
of central facial landmarks (red dots in Fig. 1(c)) to a set of
standard landmark locations, a process known as face alignment. This image is then cropped into 64 × 64 pixels, and
fed into the deep generative neural network to create the synthesized face. The synthesized face is transformed back with
M −1 to match the original face. Finally, with post-processing
such as boundary smoothing, a Deep Fake image/video frame
is created.
3. 3D HEAD POSE ESTIMATION
The 3D head pose corresponds to the rotation and translation
of the world coordinates to the corresponding camera coordiarXiv:1811.00661v2 [cs.CV] 13 Nov 2018
Fig. 1. Overview of Deep Fake work-ﬂow (Left) and our method (Right). In (Deep Fake work-ﬂow): (a) is the original image. (b) Detected
face in the image. (c) Detected 2D facial landmarks. (d) Cropped face in (a) is warped to a standardized face using an afﬁne transformation
M. (e) Deep Fake face synthesized by the deep neural network. (f) Deep Fake face is transformed back using M −1. (g) The mask of
transformed face is reﬁned based on landmarks. (g) Synthesized face is merged into the original image. (h) The ﬁnal fake image. For (our
method): The top row corresponds to a real image and the bottom corresponds to a Deep Fake. We compare head poses estimated using
facial landmarks from the whole face (j),(m) or only the central face region (i), (l). The alignment error is revealed as differences in the head
poses shown as their projections on the image plane. The difference of the head poses is then fed to an SVM classiﬁer to differentiate the
original image (k) from the Deep Fake (n).
68 facial landmarks. Red dots are used as central face
region. Blue and red landmarks are used as whole face. The landmarks represented as empty circles are not used in head pose estimation.
nates. Speciﬁcally, denote [U, V, W]T as the world coordinate
of one facial landmark, [X, Y, Z]T be its camera coordinates,
and (x, y)T be its image coordinates. The transformation between the world and the camera coordinate system can be formulated as
where R is the 3 × 3 rotation matrix, ⃗t is 3 × 1 translation
vector. The transformation between camera and image coordinate system is deﬁned as
where fx and fy are the focal lengths in the x- and ydirections and (cx, cy) is the optical center, and s is an
unknown scaling factor.
In 3D head pose estimation, we need to solve the reverse
problem, i.e, estimating s, R and ⃗t using the 2D image coordinates and 3D world coordinates of the same set of facial
landmarks obtained from a standard model, e.g, a 3D avearge
face model, assuming we know the camera parameter. Specifically, for a set of n facial landmark points, this can be formulated as an optimization problem, as
that can be solved efﬁciently using the Levenberg-Marquardt
algorithm . The estimated R is the camera pose which is
the rotation of the camera with regards to the world coordinate, and the head pose is obtained by reversing it as RT (as
R is an orthornormal matrix).
4. INCONSISTENT HEAD POSES IN DEEP FAKES
As a result of swapping faces in the central face region in the
Deep Fake process in Fig. 1, the landmark locations of fake
faces often deviate from those of the original faces. As shown
in Fig. 1(c), a landmark in the central face region P0 is ﬁrstly
afﬁne-transformed into P0 in = MP0. After the generative
neural network, its corresponding landmark on the faked face
is Q0 out.
As the conﬁguration of the generative neural network in
Deep Fake does not guarantee landmark matching, and people have different facial structures, this landmark Q0 out on
generated face could have different locations to P0 in. Based
on the comparison 51 central region landmarks of 795 pairs
of images in 64 × 64 pixels, the mean shifting of a landmark from the input (Fig. 1(d)) to the output (Fig. 1(e)) of
the generative neural network is 1.540 pixels, and its standard deviation is 0.921 pixels. After an in versed transformation Q0 = M −1Q0 out, the landmark locations Q0 in the
faked faces will differ from the corresponding landmarks P0
in the original face. However, due to the fact that Deep Fake
only swap faces in the central face region, the locations of
the landmarks on the outer contour of the face (blue landmarks P1 in Fig. 1(c) and (f)) will remain the same. This
mismatch between the landmarks at center and outer contour
of faked faces is revealed as inconsistent 3D head poses estimated from central and whole facial landmarks. Particularly,
the head pose difference between central and whole face region will be small in real images, but large in fake images.
We conduct experiments to conﬁrm our hypothesis. For
simplicity, we look at the head orientation vector only. Denote
a as the rotation matrix estimated using facial landmarks
from the whole face (red and blue landmarks in Fig. 2) using
method described in Section 3, and RT
c as the one estimated
using only landmarks in the central region (red landmarks
in Fig. 2). We obtain the 3D unit vectors ⃗va and ⃗vc corresponding to the orientations of the head estimated this way, as
a ⃗w and ⃗vc = RT
c ⃗w, respectively, with ⃗w = T
being the direction of the w-axis in the world coordinate. We
then compare the cosine distance between the two unit vectors ⃗vc and ⃗va, 1 −⃗va · ⃗vc/(∥⃗va∥∥⃗vc∥), which takes value in
 with 0 meaning the two vectors agree with each other.
The smaller this value is, the closer the two vectors are to
each other. Shown in Fig. 3 are histograms of the cosine distances between ⃗vc and ⃗va for a set of original and Deep Fake
generated images. As these results show, the cosine distances
of the two estimated head pose vectors for the real images
concentrates on a signiﬁcantly smaller range of values up to
0.02, while for Deep Fakes the majority of the values are in
the range between 0.02 and 0.08. The difference in the distribution of the cosine distances of the two head orientation
vectors for real and Deep Fakes suggest that they can be differentiated based on this cue.
Fig. 3. Distribution of the cosine distance between ⃗vc and ⃗va for
fake and real face images.
5. CLASSIFICATION BASED ON HEAD POSES
We further trained SVM classiﬁers based on the differences
between head poses estimated using the full set of facial landmarks and those in the central face regions to differentiate
Deep Fakes from real images or videos. The features are extracted in following procedures: (1) For each image or video
frame, we run a face detector and extract 68 facial landmarks
using software package DLib . (2) Then, with the standard 3D facial landmark models of the same 68 points from
OpenFace2 , the head poses from central face region (Rc
and tc) and whole face (Ra and ta) are estimated with landmarks 18 −36, 49, 55 (red in Fig. 2) and 1 −36, 49, 55 (red
and blue in Fig. 2), respectively. Here, we approximate the
camera focal length as the image width, camera center as image center, and ignore the effect of lens distortion. (3) The
differences between the obtained rotation matrices (Ra −Rc)
and translation vectors (⃗ta −⃗tc) are ﬂattened into a vector,
which is standardized by subtracting its mean and divided by
its standard deviation for classiﬁcation.
The training and testing data for the SVM classiﬁer are
based on two datasets of real and Deep Fake images and
videos. The ﬁrst, UADFV, is a set of Deep Fake videos and
their corresponding real videos that are used in our previous
work . This dataset contains 49 real videos, which were
used to create 49 Deep Fake videos. The average length of
these videos is approximately 11.14 seconds, with a typical
resolution of 294×500 pixels. The second data set is a subset
from the DARPA MediFor GAN Image/Video Challenge ,
which has 241 real images and 252 Deep Fake images. For
the training of the SVM classiﬁer, we use frames from 35 real
and 35 Deep Fake videos in the UADFV dataset, with a total
number of 21, 694 images. Frames (a total 11, 058 frames)
from the remaining 14 real and 14 Deep Fake videos from
the UADFV dataset and all images in the DARPA GAN set
are used to test the SVM classiﬁers. We train SVM classiﬁer
Table 1. AUROC based on videos and frames
(⃗va −⃗vc) & (⃗ta −⃗tc)
(⃗ra −⃗rc) & (⃗ta −⃗tc)
(Ra −Rc) & (⃗ta −⃗tc)
with RBF kernels on the training data, with a grid search on
the hyperparameters using 5 fold cross validation.
The performance, evaluated using individual frames as
unit of analysis with Area Under ROC (AUROC) as the performance metric, is shown for the two datasets in Fig. 4. As
these results show, on the UADFV dataset, the SVM classiﬁer
achieves an AUROC of 0.89. This indicates that the difference between head poses estimated from central region and
whole face is a good feature to identify Deep Fake generated
images. Similarly, on the DARPA GAN Challenge dataset,
the AUROC of the SVM classiﬁer is 0.843. This results from
the fact that the synthesized faces in the DARPA GAN challenges are often blurry, leading to difﬁculties to accurately
predict facial landmark locations, and consequently the head
pose estimations. We also estimate the performance using individual videos as unit of analysis for the UADFV dataset.
This is achieved by averaging the classiﬁcation prediction on
frames over individual videos. The performance is shown in
the last row of Table 1.
We also perform an ablation study to compare the performance of different types of features used in the SVM classi-
ﬁer. Speciﬁcally, we compare five different types of features
based on the rotation and translation of estimated 3D head
pose in camera coordinates are also examined as in Table 1.
(1) As in Section 4, we simpliﬁed head poses as head orientations, ⃗va and ⃗vc. Classiﬁcation using ⃗va −⃗vc as features
achieves 0.738 AUROC on Deep Fake Dataset. This is expected, as this simpliﬁcation neglects the translation and rotation on other axes. (2) As there are 3 degrees of freedom in
rotation, representing head pose rotation matrix as Rodrigues’
rotation vector (⃗ra −⃗rc) could increase the AUROC to 0.798.
(3) Instead of Rodrigues’ vector ⃗r ∈R3, ﬂatten the difference of 3 by 3 rotation matrices Ra −Rc as features further
improve the AUROC to 0.840. (4) Introducing the difference
of translation vectors ⃗ta −⃗tc to (1) and (2) results in AUROCs
as 0.866 and 0.890, due to the increase of head poses in translation.
6. CONCLUSION
In this paper, we propose a new method to expose AIgenerated fake face images or videos (commonly known
as the Deep Fakes). Our method is based on observations
Fig. 4. ROC curves of the SVM classiﬁcation results, see texts for
that such Deep Fakes are created by splicing synthesized face
region into the original image, and in doing so, introducing
errors that can be revealed when 3D head poses are estimated
from the face images. We perform experiments to demonstrate this phenomenon and further develop a classiﬁcation
method based on this cue. We also report experimental evaluations of our methods on a set of real face images and Deep
7. REFERENCES
 Robert Chesney and Danielle Keats Citron, “Deep Fakes: A
Looming Challenge for Privacy, Democracy, and National Security,” 107 California Law Review ; U of
Texas Law, Public Law Research Paper No. 692; U of Maryland
Legal Studies Research Paper No. 2018-21.
 Yuezun Li, Ming-Ching Chang, and Siwei Lyu, “In ictu oculi:
Exposing ai generated fake face videos by detecting eye blinking,” in IEEE International Workshop on Information Forensics
and Security (WIFS), 2018.
 Haodong Li, Bin Li, Shunquan Tan, and Jiwu Huang, “Detection of deep network generated images using disparities in color
components,” arXiv preprint arXiv:1808.07276, 2018.
 Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao
“Mesonet: a compact facial video forgery detection network,” in IEEE International Workshop on Information
Forensics and Security (WIFS), 2018.
 G. Bradski,
“The OpenCV Library,”
Dr. Dobb’s Journal of
Software Tools, 2000.
 Davis E. King, “Dlib-ml: A machine learning toolkit,” Journal
of Machine Learning Research, vol. 10, pp. 1755–1758, 2009.
 Tadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and Louis-
Philippe Morency,
“Openface 2.0: Facial behavior analysis
toolkit,” in Automatic Face & Gesture Recognition ,
2018 13th IEEE International Conference on. IEEE, 2018, pp.