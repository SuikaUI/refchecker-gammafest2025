A Multidisciplinary Survey and Framework for Design and
Evaluation of Explainable AI Systems
SINA MOHSENI and NILOOFAR ZAREI, Texas A&M University
ERIC D. RAGAN, University of Florida
The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial
intelligence applications used in everyday life. Explainable AI systems are intended to self-explain the reasoning
behind system decisions and predictions. Researchers from different disciplines work together to define, design,
and evaluate explainable systems. However, scholars from different disciplines focus on different objectives
and fairly independent topics of Explainable AI research, which poses challenges for identifying appropriate
design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper
presents a survey and framework intended to share knowledge and experiences of Explainable AI design
and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation
methods in XAI research, after a thorough review of Explainable AI related papers in the fields of machine
learning, visualization, and human-computer interaction, we present a categorization of Explainable AI design
goals and evaluation methods. Our categorization presents the mapping between design goals for different
Explainable AI user groups and their evaluation methods. From our findings, we develop a framework with
step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation
cycles in multidisciplinary Explainable AI teams. Further, we provide summarized ready-to-use tables of
evaluation methods and recommendations for different goals in Explainable AI research.
Additional Key Words and Phrases: Explainable artificial intelligence (XAI); human-computer interaction
(HCI); machine learning; explanation; transparency;
ACM Reference Format:
Sina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design
and Evaluation of Explainable AI Systems. ACM Trans. Interact. Intell. Syst. 1, 1, Article 1 ,
46 pages. 
INTRODUCTION
Impressive applications of Artificial Intelligence (AI) and machine learning have become prevalent
in our time. Tech giants like Google, Facebook, and Amazon have collected and analyzed enough
personal data through smartphones, personal assistant devices, and social media that can model
individuals better than other people. Recent negative interference of social media bots in political
elections were yet another sign of how susceptible our lives are to the misuse of artificial
intelligence and big data . In these circumstances, despite tech giants and the thirst for more
advanced systems, others suggest holding off on fully unleashing AI for critical applications until
they can be better understood by those who will rely on them. The demand for predictable and
Authors’ addresses: Sina Mohseni, ; Niloofar Zarei, , Texas A&M University,
College Station, Texas; Eric D. Ragan, , University of Florida, Gainesville, Florida.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from .
© 2020 Association for Computing Machinery.
2160-6455/2020/1-ART1 $15.00
 
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
 
Mohseni et al.
accountable AI grows as tasks with higher sensitivity and social impact are more commonly entrusted to AI services. Hence, algorithm transparency is an essential factor in holding organizations
responsible and accountable for their products, services, and communication of information.
Explainable Artificial Intelligence (XAI) systems are a possible solution towards accountable
AI, making it possible by explaining AI decision-making processes and logic for end users .
Specifically, explainable algorithms can enable control and oversight in case of adverse or unwanted
effects, such as biased decision-making or social discrimination. An XAI system can be defined as a
self-explanatory intelligent system that describes the reasoning behind its decisions and predictions.
The AI explanations (either on-demand explanations or in the form of model description) could
benefit users in many ways such as improving safety and fairness when relying on AI decisions.
While the increasing impact of advanced black-box machine learning systems in the big-data era
has attracted much attention from different communities, interpretability of intelligent systems has
also been studied in numerous contexts . The study of personalized agents, recommendation
systems, and critical decision-making tasks (e.g., medical analysis, powergrid control) has added to
the importance of machine-learning explanation and AI transparency for end-users. For instance, as
a step towards this goal, the legal right to explanations has been established in the European Union
General Data Protection Regulation (GDPR) commission. While the current state of regulations
is mainly focused on user data protection and privacy, it is expected to cover more algorithmic
transparency and explanations requirements from AI systems .
Clearly, addressing such a broad array of definitions and expectations for XAI requires multidisciplinary research efforts, as existing communities have different requirements and often have
drastically different priorities and areas of specialization. For instance, research in the domain of
machine learning seeks to design new interpretable models and explain black -box models with
ad-hoc explainers. Along the same line but with different approaches, researchers in visual analytics
design and study tools and methods for data and domain experts to visualize complex black-box
models and study interactions to manipulate machine learning models. In contrast, research in
human-computer interaction (HCI) focuses on end-user needs such as user trust and understanding
of machine generated explanations. Psychology research also studies the fundamentals of human
understanding, interpretability, and the structure of explanations.
Looking at the broad spectrum of research on XAI, it is evident that scholars from different
disciplines have different goals in mind. Even though different aspects of XAI research are following
the general goals of AI interpretability, researchers in each discipline use different measures and
metrics to evaluate the XAI goals. For example, numerical analytic methods are employed in
machine learning fields to evaluate computational interpretability, while human interpretability
and human-subjects evaluations are more commonly the primary goals in HCI and visualization
communities. In this regard, although there seems to be a mismatch in specific objectives for
designing and evaluating explainability and interpretability, a convergence in goals is beneficial
for achieving the full potential of XAI. To this end, this paper presents a survey and framework
intended to share knowledge and experiences of XAI design and evaluation methods across multiple
disciplines. To support the diverse design goals and evaluation methods in XAI research, after a
thorough review of XAI related papers in the fields of machine learning, visualization, and HCI, we
present a categorization of interpretable machine learning design goals and evaluation methods
and show a mapping between design goals for different XAI user groups and their evaluation
methods. From our findings, we develop a framework with step-by-step design guidelines paired
with evaluation methods to close the iterative design and evaluation loops in multidisciplinary
teams. Further, we provide summarized ready-to-use evaluation methods for different goals in
XAI research. Lastly, we review recommendations for XAI design and evaluation drawn from our
literature review.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
BACKGROUND
Nowadays, algorithms analyze user data and affect decision-making processes for millions of people
on matters like employment, insurance rates, loan rates, and even criminal justice . However,
these algorithms that serve critical roles in many industries have their own disadvantages that
can result in discrimination , and unfair decision-making . For instance, recently,
news feed and targeted advertising algorithms in social media have attracted much attention for
aggravating the lack of information diversity in social media . A significant part of the trouble
could be because algorithmic decision-making systems—unlike recommender systems—do not
allow their users to choose between the recommended items, but instead, present the most relevant
content or option themselves. To address this, Heer suggests the use of shared representations
of tasks that are augmented with both machine learning models and user knowledge to reduce
negative effects of immature AI autonomous systems. They present case studies of interactive
systems that integrate proactive computational support into interactive systems.
Bellotti and Edwards argue that intelligent context-aware systems should not act on our
behalf. They suggest user control over the system as a principle to support the accountability of a
system and its users. Transparency can provide essential information for decision-making that is
hidden to the end-users and prevents blind faith . The key benefits of algorithmic transparency
and interpretability include: user awareness ; bias and discrimination detection ; interpretable behavior of intelligent systems ; and accountability for users . Furthermore,
considering the growing body of examples of discrimination and other legal aspects of algorithmic
decision making, researchers are demanding and investigating transparency and accountability of
AI under the law to mitigate adverse effects of algorithmic decision making . In this
section, we review research background related to XAI systems from a broad and multidisciplinary
perspective. At the end, we relate the summaries and positions derived through our survey to other
work in the field.
Auditing Inexplicable AI
Researchers audit algorithms to study bias and discrimination in algorithmic decision making 
and study the users’ awareness of the effects of these algorithms . Auditing of algorithms is a
mechanism for investigating algorithms’ functionality to detect bias and other unwanted algorithm
behaviors without the need to know about its specific design details. Auditing methods focus on
problematic effects on the results of algorithmic decision-making systems. To audit an algorithm,
researchers feed new inputs to the algorithm and review system output and behavior. Researchers
generate new data and user accounts with the help of scripts, bots , and crowdsourcing 
to emulate real data and real users in the auditing process. For bias detection among multiple
algorithms, cross-platform auditing can detect if an algorithm behaves differently from another
algorithm. A recent example of cross-platform auditing is a work by Eslami et al. , in which
they analyzed user reviews in three hotel booking websites to study user awareness of bias in
online rating algorithms. These examples demonstrate that auditing is a valuable yet time-intensive
process that could not be scaled easily to large numbers of algorithms. This calls for new research
for more effective solutions toward algorithmic transparency.
Explainable AI
Along with the methods mentioned above for supporting transparency, machine learning explanations have also become a common approach to achieve transparency in many applications such as
social media, e-commerce, and data-driven management of human workers . The
XAI system, as illustrated in Figure 1, is able to generate explanations and describe the reasoning
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Interpretable
Machine Learning
Explainable
Explainable AI System
Fig. 1. The user interacts with the explainable interface to send queries to the interpretable machine learning
and receive model prediction and explanations. The interpretable model interacts with the data to generate
explanation or new new prediction for the user query.
behind machine-learning decisions and predictions. Machine-learning explanations enable users to
understand how the data is processed. They aim to bring awareness to possible bias and system malfunctions. For example, to measure user perception of justice in intelligent decision making, Binns
et al. studied explanations in systems for everyday tasks such as determining car insurance
rates and loan application approvals. Their results highlight the importance of machine learning
explanations in users’ comprehension and trust in algorithmic decision-making systems. In a
similar work studying knowledge of social media algorithms, Radar et al. ran a crowdsourced
study to see how different types of explanations affect users’ beliefs on news feed algorithmic
transparency in a social media platform. In their study, they measured users’ awareness, correctness,
and accountability to evaluate algorithmic transparency. They found that all explanations caused
users to become more aware of the system’s behavior. Stumpf et al. designed experiments to
investigate meaningful explanations and interactions to hold users accountable by machine learning
algorithms. They show explanations as a potential method for supporting richer human-computer
collaboration to share intelligence.
The recent advancements and trends for explainable AI research demand a wide range of goals
for algorithmic transparency which calls for research across varied application areas. To this end,
our review encourages a cross-discipline perspective of intelligibility and transparency goals.
Related Surveys and Guidelines
In recent years, there have been surveys and position papers suggesting research directions and
highlighting challenges in interpretable machine learning research . Although our
review is limited to computer science literature, here we summarize several of the most relevant
peer-reviewed surveys related to the topic of XAI across active disciplines including social science.
While all surveys, models, and guidelines in this section add value to the XAI research, to the
best of our knowledge, there is no existing comprehensive survey and framework for evaluation
methods of explainable machine learning systems.
Social Science Surveys. Research in the social sciences is particularly important for XAI
systems to understand how people generate, communicate, and understand explanations by taking
into account each others’ thinking, cognitive biases, and social expectations in the process of
explaining. Hoffman, Mueller, and Klein reviewed the key concepts of explanations for intelligent
systems in a series of essays to identify how people formulate and accept explanations, ways to
generate self-explanations, and identified purposes and patterns for causal reasoning .
They lastly focus on deep neural networks (DNN) to examine their theoretical and empirical
findings on a machine learning algorithm . In other work, they presented a conceptual model
of the process of explaining in the XAI context . Their framework includes specific steps and
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
measures for the goodness of explanations, user satisfaction and understanding of explanations,
users’ trust and reliance on XAI systems, effects of curiosity on the search for explanations, and
human-XAI system performance.
Miller suggests a close collaboration between machine learning researchers in the space of
XAI with social science would further refine the explainability of AI for people. He discusses how
understanding and replicating how people generate, select, and present explanations could improve
human-XAI interactions. For instance, Miller reviews how people generate and select explanations
that are involved with cognitive biases and social expectations. Other papers reviewing social science
aspects of XAI systems include studies on the role of algorithmic transparency and explanation in
lawful AI and of fair and accountable algorithmic decision-making processes .
Human Computer Interactions Surveys. Many HCI surveys discuss the limitations and challenges in AI transparency and interactive machine learning . Others suggest a set of
theoretical and design principles to support intelligibility of intelligent system and accountability
of human users (e.g., ). In a recent survey, Abdul et al. presented a thorough literature
analysis to find XAI-related topics and relationships among these topics. They used visualization of
keywords, topic models, and citation networks to present a holistic view of research efforts in a wide
range of XAI related domains; from privacy and fairness to intelligent agents and context-aware
systems. In another work, Wang et al. explored theoretical underpinnings of human decisionmaking and proposed a conceptual framework for building human-centered decision-theory-driven
XAI systems. Their framework helps to choose better explanations to present, backed by reasoning
theories, and human cognitive biases. Focused on XAI interface design, Eiband et al. present a
stage-based participatory process for integration of transparency in existing intelligent systems
using explanations. Another design framework is XAID from Zhu et al. , which presents
a human-centered approach for facilitating game designers to co-create with machine learning
techniques. Their study investigates the usability of XAI algorithms in terms of how well they
support game designers.
Visual Analytics Surveys. XAI-related surveys in the visualization domain follow visual
analytics goals such as understanding and interacting with machine learning systems in different
visual analytics applications . Choo and Liu reviewed challenges and opportunities for
Visual Analytics for explainable deep learning design. In a recent paper, Hohman et al. provide
an excellent review and categorization of visual analytics tools for deep learning applications.
They cover various data and visualization techniques that are being used in deep visual analytics
applications. Also, Spinner et al. proposed a XAI pipeline which maps the XAI process
to an iterative workflow in three stages: model understanding, diagnosis, and refinement. To
operationalize their framework, they designed explAIner, a visual analytics system for interactive
and interpretable machine learning that instantiates all steps of their pipeline.
Machine Learning Surveys. In the machine learning area, Guidotti et al. present a comprehensive review and classification of machine learning interpretability techniques. Also, Montavon
et al. focus on interpretability techniques for DNN models. On Convolutional Neural Network (CNN), Zhang et al. reviews research on interpretability techniques in six directions
including visualization of CNN representations, diagnosing techniques for CNNs, approaches for
transforming CNN representations into interpretable graphs, building explainable models, and
semantic-level learning based on model interpretability. In another work, Gilpin et al. reviews
interpretability techniques in machine learning algorithms and categorizes evaluation approaches
to bridge the gap between machine learning and HCI communities.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Multidisciplinary paper
selection with upward and
downward literature
investigation using Google
Scholar search.
Paper selection from
related conference and
journals to balance
different attributes in
the reference table.
Creating a reference
table with 10 research
attributes to maintain
literature investigation
breadth and depth.
Fig. 2. A diagram summarizing our iterative and multi-pass literature selection and review process to achieve
desired literature investigation breadth and depth. We started with 40 papers to create the reference table.
Then we added 80 papers by upward and downward literature investigation to improve review breath and
depth. Finally, we added another 80 papers from related conferences proceedings and journals to balance the
reference table.
In complementing the existing work, our survey provides a multidisciplinary categorization of
design goals and evaluation methods for XAI systems. As a result of surveyed papers, we propose a
framework that provides a step-by-step design and evaluation plan for a multidisciplinary team
of designers for building real-world XAI systems. Unlike Eiband et al. , we do not make
the assumption of adding transparency to an existing intelligent interface and do not limit the
evaluation of XAI systems to the users’ mental model. We instead characterize both design goals
and evaluation methods and compile all in a unified framework for multidisciplinary teamwork.
Our design framework has similarities to Wang et al’s theoretical framework which supports
our design goals (see Section 9.6). Our multidisciplinary work extends their conceptual framework
by 1) including the design of interpretability algorithms as part of the framework and 2) pairing
evaluation methods with each design step to close the iterative design and evaluation loops.
SURVEY METHOD
We conducted a survey of the existing research literature to capture and organize the breadth of
designs and goals for XAI evaluation. We used a structured and iterative methodology to find XAIrelevant research and categorize the evaluation methods presented in research articles (summarized
in Figure 2). In our iterative paper selection process, we started by selecting existing work from top
computer science conferences and journals across the fields of HCI, visualization, and machine
learning. However, since XAI is a quite fast growing topic, we also wanted to include arXiv preprints
and useful discussions in workshop papers. We started with 40 papers related to XAI topics across
three research fields including but not limited to research on interpretable machine learning
techniques, deep learning visualization, interactive model visualization, machine explanations in
intelligent agents and context-aware systems, explainable user interfaces, explanatory debugging,
and algorithmic transparency and fairness.
We then used selective coding to identify 10 main research attributes in those papers. The main
attributes we identified include: research discipline (social science, HCI, visualization, or machine
learning), paper type (interface design, algorithm design, or evaluation paper), application domain
(machine learning interpretability, algorithmic fairness, recommendation systems, transparency of
intelligent systems, intelligent interactive systems and agents, explainable intelligent systems and
agents, human explanations, or human trust), machine learning model (e.g., deep learning, decision
trees, SVM), data modality (image, text, tabular data), explanation type (e.g., graphical, textual, data
visualization), design goal (e.g., model debugging, user reliance, bias mitigation), evaluation type
(e.g., qualitative, computational, quantitative with human-subjects), targeted user (AI novices, data
experts, AI experts), and evaluation measure (e.g., user trust, task performance, user mental model).
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
In the second round of collecting XAI literature, we conducted an upward and downward
literature investigation using the Google Scholar search engine to add 80 more papers to our
reference table. We narrowed down our search by XAI related topics and keywords including but
not limited to: interpretability, explainability, intelligibility, transparency, algorithmic decisionmaking, fairness, trust, mental model, and debugging in machine learning and intelligent systems.
With this information, we performed axial coding to organize the literature and started discussions
on our proposed design and evaluation categorization.
Finally, to maintain reasonable literature coverage and balance the number of papers for each
of our categories of design goals and evaluation measures, we added another 80 papers to our
reference table. The conferences from which we selected XAI related paper were including but not
limited to: CHI, IUI, HCOMP, SIGDIAL, UbiComp, AIES, VIS, ICWSM, IJCAI, KDD, AAAI, CVPR,
and NeurIPS conferences. The journals included: Trends in cognitive science, Transactions on
Cognitive and Developmental Systems, Cognition Journal, Transactions on Interactive Intelligent
Systems, International Journal of Human-Computer Studies, Transactions on Visualization and
Computer Graphics, and Transactions on Neural Networks and Learning Systems.
Following a review of 226 papers, our categorization of XAI design goals and evaluation methods
is supported by references from papers performing design or evaluation of XAI systems. Our
reference table1 is available online to the research community to provide further insight beyond
our discussions in this document. Table 2 shows a digest of our surveyed papers that contains 42
papers with both design and evaluation of XAI system. Later in the Section 7, we provide a series
of tables to organize different evaluation methods from research papers with example references
for each, documenting our in-depth analysis of 69 papers in total.
XAI TERMINOLOGY
To familiarize the readers with common XAI concepts and terminologies that are repeatedly
referenced in this review, the following four subsections summarize high-level characterizations of
model explanations. Many related surveys (e.g., ) and reports (e.g., ) also provide
useful compilations of terminology and concepts in comprehensive reports. For instance, Abdul et
al. present a citation graph from diverse domains related to explanations, including intelligible
intelligent systems, context-aware systems, and software learnability. Later, Arrieta et al. 
present a thorough review of XAI concepts and taxonomies and arrives at the concept of Responsible
AI as a manifold of multiple AI principles including model fairness, explainability, and privacy.
Similarly, the concept of Safe AI has been reviewed by Amodei et al. , which is an interest
in safety-critical intelligent applications such as autonomous vehicles . Table 1 presents
descriptions for 14 common terms related to this survey’s topic and organizes their relation to
Intelligible Systems and Transparent AI topics. We consider Transparent AI systems as the AI-based
class of Intelligible Systems. Therefore, properties and goals previously established for Intelligible
Systems are ideally transferable to Transparent AI systems. However, challenges and limitations
for achieving transparency in complex machine learning algorithms raise issues (e.g., ensuring the
fairness of an algorithm) that were not necessarily problematic in intelligible rule-based systems
but now require closer attention from research communities.
The descriptions presented in Table 1 are meant to be an introduction to these terms, though exact
definitions and interpretations can depend on usage context and research discipline. Consequently,
researchers from different disciplines often use these terms interchangeably, disregarding differences
in meaning . Perhaps the two generic terms of Black-box Model and Transparent Model are in
the center of XAI terminology ambiguity. The black-box term refers to complex machine learning
1 
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Table 1. Table of common terminology related to Intelligible Systems and Transparent AI. Higher-level main
concepts are shown in gray, while related terms for the main concepts are listed below and categorized as
a desired outcome, property, or practical approach. Explainable AI is one particular practical approach for
intelligible systems to enable improve transparency. Note that definitions and interpretations can vary across
the literature, and this table is meant to serve as a quick reference.
Description
Intelligible System
Main Concept
A system that is understandable and predictable for its users
through transparency or explanations .
Understandability
(Intelligibility)
Intelligible systems support user understanding
of system’s underlying functions .
Predictability
Properties
Intelligibility supports building a mental model of the system
that enables user to predict system behavior .
Trustworthiness
Enabling positive user attitude toward the system that
emerges from knowledge, experience, and emotion .
Reliability
Supporting user trust to rely and follow
system’s advice for higher performance .
Improving safety by reducing user unintended
misuse due to misperception and unawareness .
Transparent AI
Main Concept
An AI-based system that provides information
about its decision-making processes .
Interpretable AI
Inherently human-interpretable models due to
their low complexity of machine learning algorithms .
Explainable AI
Approaches
Supporting user understanding of complex models
by providing explanations for predictions .
Interpretability
The ability to support user understanding and comprehension
of the model decision making process and predictions .
Explainability
Properties
The ability to explain underlying model and its reasoning
with accurate and user comprehensible explanations .
Accountable AI
Allowing for auditing and documentation to hold organizations
accountable for their AI-based products and services .
Enabling ethical and fairness analysis of models
and data used in decision-making processes .
models that are not human-interpretable as opposed to transparent models which are simple
enough to be human-interpretable . We find it more accurate and consistent to separate the
transparency of an XAI system (as described in Figure 1) from the interpretability of its internal
machine learning models. Specifically, Table 1 shows that Transparent AI could be achieved by
either Interpretable AI or Explainable AI approaches. Other examples of terminology ambiguity
include the terms Interpretability and Explainability that are often used as synonyms in the field
of machine learning. For example the phrase “interpretable machine learning technique” often
refers to ad-hoc techniques for generating explanations for non-interpretable models such as
DNNs . Another example is the occasional case of using the terms Transparent System and
Explainable System interchangeably in HCI research , while others clarify that explainability is
not equivalent to transparency because it does not require knowing the flow of the bits in the AI
decision-making process .
Global and Local Explanations
One way to classify explanations is by their interpretation scale. For instance, an explanation
could be as thorough as describing the entire machine learning model. Alternatively, it could only
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
partially explain the model, or it could be limited to explaining an individual input instance. Global
Explanation (or Model Explanation) is an explanation type that describes how the overall machine
learning model works. Model visualization and decision rules are examples of
explanations falling in this category. In other cases, interpretable approximations of complex models
serve as the model explanation. Tree regularization is a recent example of regularized complex
model to learn tree-like decision boundaries. Model complexity and explanation design are the
main factors used to choose between different types of global explanations.
In contrast, Local Explanations (or Instance Explanations) aim to explain the relationship between
specific input-output pairs or the reasoning behind the results for an individual user query. This
type of explanation is thought to be less overwhelming for novices, and it can be suited for
investigating edge cases for the model or debugging data. Local explanations often make use of
saliency methods or local approximation of the main model . Saliency methods
(also as known as attribution maps or sensitivity maps) use different approaches (e.g., perturbationbased methods, gradient-based methods) to show what features in the input strongly influence the
model’s prediction. Local approximation of the model, on the other hand, trains an interpretable
model (learned from the main model) to locally represent the complex model’s behavior.
Interpretable Models vs. Ad-hoc Explainers
The human interpretability of a machine learning model is inversely proportional to the model’s size
and complexity. Complex models (e.g., deep neural networks) with high performance and robustness
in real-world applications are not interpretable by human users due to their large variable space.
Linear regression models or decision trees offer better interpretability but have limited performance
on high-dimensional data, whereas a random forest model (ensemble of hundreds of decision
trees) can have much higher performance but is less understandable. This trade-off between model
interpretability and performance led researchers to design ad-hoc methods to explain any black-box
machine learning algorithm such as deep neural networks. Ad-hoc explainers (e.g., ) are
independent algorithms that can describe model predictions by explaining “why” a certain decision
has been made instead of describing the whole model. However, there are limitations in explaining
black-box models with ad-hoc explainers, such as the uncertainty of the fidelity of the explainer
itself. We will discuss more about the fidelity of explanations in Section 7.5. Furthermore, although
ad-hoc explainers generally describe “why” a prediction is made, these methods lack in explaining
“how” the decision is made.
What to Explain
When users face a complex intelligent system, they may demand different types of explanatory
information and each explanation type may require its own design. Here we review six common
types of explanations used in XAI system designs.
How Explanations demonstrate a holistic representation of the machine learning algorithm
to explain How the model works. For visual representations, model graphs and decision
boundaries are common design examples for How explanations. However, research shows
users may also be able to develop a mental model of the algorithm based on a collection of
explanations from multiple individual instances .
Why Explanations describe Why a prediction is made for a particular input. Such explanations
aim to communicate what features in the input data or what logic in the model 
has led to a given prediction by the algorithm. This type of explanation can have either model
agnostic or model dependent solutions.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Why-not Explanations help users to understand the reasons why a specific output was not in the
output of the system . Why-not explanations (also called Contrastive Explanations) characterize
the reasons for differences between a model prediction and the user’s expected outcome. Feature
importance (or feature attribution) is commonly used as an interpretability technique for Why and
Why-not explanations.
What-If Explanations involve demonstration of how different algorithmic and data changes
affect model output given new inputs , manipulation of inputs , or changing model
parameters . Different what-if scenarios may be automatically recommended by the system or
can be chosen for exploration through interactive user control. Domains with high-dimensional data
(e.g., image and text) and complex machine learning models (e.g., DNNs) have fewer parameters for
users to directly tune and examine trained model compared to simpler data (e.g., low-dimensional
tabular data) and models.
How-to Explanations spell out hypothetical adjustments to the input or model that would result in
a different output , such as a user-specified output of interest. Techniques to generate Howto (or Counterfactual) explanations are ad-hoc and model-agnostic considering that model structure
and internal values are not a part of the explanation . Such methods can work interactively
with the user’s curiosity and partial conception of the system to allow an evolving mental model of
the system through iterative testing.
What-else Explanations present users with similar instances of input that generate the same
or similar outputs from the model. Also called Explanation by Example, What-else explanations
pick samples from the model’s training dataset that are similar to the original input in the model
representation space . Although very popular and easy to achieve, research shows examplebased explanations could be misleading when training datasets lack uniform distribution of the
data .
How to Explain
In all types of machine learning explanations, the goal is to reveal new information about the
underlying system. In this survey, we mainly focus on human-understandable explanations, though
we note that research on interpretable machine learning has also studied other purposes such as
knowledge transfer, object localization, and error detection .
Explanations can be designed using a variety of formats for different user groups . Visual
Explanations use visual elements to describe the reasoning behind the machine learning models.
Attention maps and visual saliency in the form of saliency heatmaps are examples of
visual explanations that are widely used in machine learning literature. Verbal Explanations describe
the machine’s model or reasoning with words, phrases, or natural language. Verbal explanations
are popular in applications like question-answering explanations and decision lists . This form
of explanation has also been implemented in recommendation systems and robotics .
Explainable interfaces commonly make use of multiple modalities (e.g., visual, verbal, and numerical
elements) for explanations to support user understanding . Analytic Explanation is another approach to view and explore the data and the machine learning models representations . Analytic
explanations commonly rely on numerical metrics and data visualizations. Visual analytics tools
also allow researchers to review model structures, relations, and their parameters in complex deep
models. Heatmap visualizations , graphs and networks , and hierarchical (decision trees)
visualizations are commonly used to visualize analytic explanations for interpretable algorithms.
Recently, Hohman et al. implemented a combination of visualization and verbalization to
communicate or summarize key aspects of a model.
From a different perspective, Chromik et al. extends the idea of “dark patterns” from
interactive user interface design into machine learning explanations. They review possible
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
Data Experts
AI Experts
AI Novices
Design Goals
Evaluation
Algorithmic Transparency
User Trust and Reliance
Bias Mitigation
Privacy Awareness
User Mental Model
Usefulness and Satisfaction
User Trust and Reliance
Human-AI Performance
Model Interpretability
Model Debugging
Explainer Fidelity
Model Trustworthiness
Model Visualization
and Inspection
Model Tuning and Selection
Task Performance
Model Performance
Design Goals
Evaluation
Design Goals
Evaluation
XAI Target Users
Fig. 3. A summary of our categorization of XAI design goals and evaluation measures between user groups.
Top: Different system design goals for each user group. Bottom: Common evaluation measures used in each
user group. Notice that similar XAI goals for different user groups require different research objectives, design
methods, and implementation paths.
ways that phrasing of explanations and their implementation in the interface could deceive users
for the benefit of other parties. They review negative effects such as lack of user attention to
explanations, formation of an incorrect mental model, and even algorithmic anxiety could be
among the consequences of such deceptive presentations and interactions of machine learning
explanations.
CATEGORIZATION OF XAI DESIGN GOALS AND EVALUATION METHODS
While an ideal XAI system should be able to answer all user queries and meet all XAI concept
goals , individual research efforts focus on designing and studying XAI systems with respect
to specific interpretability goals and specific users. Similarly, evaluating the explanations can
demonstrate and verify the effectiveness of the explainable systems for their intended goals.
After careful review and analysis of XAI goals and their evaluation methods in the literature, we
recognized the following two attributes to be most significant for our purposes of interdisciplinary
organization of XAI design and evaluation methods:
• Design Goals. The first attribute in our categorization is the design goal for interpretable
algorithms and explainable interfaces in XAI research. We obtain XAI design goals from
multiple research disciplines: machine learning, data visualization, and HCI. To better understand the differences between various goals for XAI, we organize XAI design goals with their
three user groups: AI novices (i.e., general AI product end-user), data experts (experts in data
analytics and domain experts), and AI experts (machine learning model designers).
• Evaluation Measures. We review evaluation methods and discuss measures used to evaluate
machine learning explanations. The measures include user mental model, user trust and
reliance, explanation usefulness and satisfaction, human-machine task performance, and
computational measures. In our review, we will pay more attention to evaluation measures
of XAI as the authors believe that this category is relatively less explored.
Figure 3 presents the pairing between XAI design goals and their evaluation measures. Note that
user groups is used as an auxiliary dimension to emphasize on the importance of end users for
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
system goals. The overlap between XAI user groups shows similarities in the design and evaluation
methods between different targeted user groups. However, the similar XAI goals in different user
groups require different research objectives, design methods, and implementation paths. To help
summarize our characterization along with example literature, Table 2 presents a cross-reference
table of XAI evaluation literature to emphasize the importance of design goals, evaluation measures,
and user types. We first review details of research focusing on XAI design goals in Section 6
including eight goals organized by their user groups. We then review evaluation measures and
methods in Section 7 including six main measures and their methods collected from the surveyed
literature.
XAI DESIGN GOALS
Research efforts have explored many goals for XAI systems. Doshi-Velez and Kim reviewed
multiple high-level priorities for XAI systems with examples including safety, ethics, user reliance,
and scientific understanding. Later, Arrieta et al. presented a thorough review of XAI opportunities in different application domains. Accordingly, different design choices such as explanation
type, scope, and level of detail will be affected by the application domain, design goal, and user
type. For example, while machine learning experts might prefer highly-detailed visualizations of
deep models to help them optimize and diagnose algorithms, end-users of daily-used AI products
do not expect fully detailed explanations for every query from a personalized agent. Therefore,
XAI systems are expected to provide the right type of explanations for the right group of users,
meaning it will be most efficient to design an XAI system according to the user’s needs and levels
of expertise.
To this end, we distinguish XAI design goals based on the designated end-user and evaluation
subjects, which we categorize into three general groups of AI experts, data experts, and AI novices.
We emphasize that this separation of groups is presented primarily for organizational convenience,
as goals are not mutually exclusive across groups, and specific priorities are case dependent for
any particular project. The XAI design goals also extend to the broader goal of Responsible AI by
improving transparency and explainability of intelligent systems. Note that although there are
overlaps in the methods used to achieve these goals, the research objectives and design approaches
are substantially different among distinct research fields and their user groups. For instance, even
though leveraging interpretable models to reduce machine learning model bias is a research goal for
AI experts, bias mitigation is also a design goal for AI novices to avoid adverse effects of algorithmic
decision-making in their respective domain settings. However, interpretability techniques for AI
experts and bias mitigation tools for AI novice require different design methods and elements. In
the following subsections, we review eight design goals for XAI systems organized by their user
AI Novices
AI novices refer to end-users who use AI products in daily life but have no (or very little) expertise
on machine learning systems. These include end-users of intelligent applications like personalized
agents (e.g., home assistant devices), social media, and e-commerce websites. In most smart systems,
machine learning algorithms serve as internal functions and APIs to enable specific features
embedded in intelligent and context-aware interfaces. Previous research shows intuitive interface
and interaction design can enhance users’ experience with the system through improving endusers’ comprehension and reliance on the intelligent systems . In this regard, creating humanunderstandable and yet accurate representations of complicated machine learning explanations for
novice end-users is a challenging design trade-off in XAI systems. Note that although there are
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
Table 2. Tabular summary of our XAI evaluation dimensions of measures and targeted user types. The table
includes 42 papers that represent a subset of the surveyed literature organized by the two dimensions.
Design Goals
Novice Users
Data Experts
AI Experts
Evaluation Measures
G1: Algorithmic Transparency
G2: User Trust and Reliance
G3: Bias Mitigation
G4: Privacy Awareness
G5: Model Visualization
and Inspection
G6: Model Tuning
and Selection
G7: Model Interpretability
G8: Model Debugging
M1: Mental Model
M2: Usefulness and
Satisfaction
M3: User Trust
and Reliance
M4: Human-AI
Task Performance
M5: Computational Measures
Herlocker et al. 2000 
Kulesza et al. 2012 
Lim et al. 2009 
Stumpf et al. 2018 
Bilgic et al. 2005 
Bunt et al. 2012 
Gedikli et al. 2014 
Kulesza et al. 2013 
Lim et al. 2009 
Lage et al. 2019 
Schmid et al. 2016 
Berkovsky et al. 2017 
Glass et al. 2008 
Haynes et al. 2009 
Holliday et al. 2016 
Nothdurft et al. 2014 
Pu and Chen et al. 2006 
Bussone et al. 2015 
Groce et al. 2014 
Myers et al. 2006 
Binns et al. 2018 
Lee et al. 2019 
Rader et al. 2018 
Datta et al. 2015 
Kulesza et al. 2015 
Kulesza et al. 2010 
Krause et al. 2016 
Krause et al. 2017 
Liu et al. 2014 
Ribeiro et al. 2016 
Ribeiro et al. 2018 
Ross et al. 2017 
Adebayo et al. 2018 
Samek et al. 2017 
Zeiler et al. 2014 
Lakkaraju et al. 2016 
Kahng et al. 2018 
Liu et al. 2018 
Liu 2017 et al. 2009 
Ming et al. 2017 
Pezzotti et al. 2018 
Strobelt et al. 2018 
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
overlaps among goals for AI Novices and AI experts who build interpretable algorithms, each user
group requires a different set of design methods and objectives that are being studied in different
research communities.
The main design goals for AI novice end-users of XAI system can be itemized as the following:
G1: Algorithmic Transparency: An immediate goal for a XAI system – in comparison to an
inexplicable intelligent system – is to help end-users understand how the intelligent system works.
Machine learning explanations improve users’ mental model of the underlying intelligent algorithms by providing comprehensible transparency for the complex intelligent algorithms .
Further, transparency of a XAI system can improve user experience through better user understanding of model output , thus improving user interactions with the system .
G2: User Trust and Reliance: XAI system can improve end-users trust in the intelligent algorithm
by providing explanations. A XAI system lets users assess system reliability and calibrate their
perception of system accuracy. As a result, users’ trust in the algorithm leads to their reliance on
the system. Example applications where XAI aims to improve user reliance through its transparent
design include recommendation systems , autonomous systems , and critical decision
making systems .
G3: Bias Mitigation: Unfair and biased algorithmic decision-making is a critical side effect of
intelligent systems. Bias in machine learning has many sources, including biased training data
and feature learning that could result in discrimination in algorithmic decision-making .
Machine learning explanations can help end-users to inspect if the intelligent systems are biased
in their decision-making. Examples of cases in which XAI is used for bias mitigation and fairness
assessment are criminal risk assessment and loan and insurance rate prediction . It
is worth mentioning that there is overlap between the biased decision-making mitigation goal
for AI novices and the goal of dataset bias for AI experts (Section 6.2), which results in shared
implementation techniques. However, the two distinct user groups require their own sets of XAI
design goals and processes.
G4: Privacy Awareness: Another goal in designing XAI systems is to provide a means for endusers to assess their data privacy. Machine learning explanations can disclose to end-users what
user data is being used in algorithmic decision-making. Examples of AI application examples in
which privacy awareness is primarily important include personalized advertisements using users’
online advertisement and personalized news feed in social media .
In addition to the major XAI goals, interactive visualization tools have also been developed to
help AI novices to learn machine learning concepts and models by interacting with simplified data
and model representations. Examples of these educative tools include TensorFlow PlayGround 
for teaching elementary neural networks concepts and Adversarial Playground for learning
concept of adversarial examples in DNNs. These minor goals cover XAI system objectives that
have limited extent compared to main goals.
Data Experts
Data experts include data scientists and domain experts who use machine learning for analysis,
decision-making, or research. Visual analysis tools can support interpretable machine learning in
many ways, such as visualizing the network architecture of a trained model and training process
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
of machine learning models. Researchers have implemented various visualization designs and
interaction techniques to understand better and improve machine learning models.
Data experts analyze data in specialized forms and domains, such as cybersecurity ,
medicine , text , and satellite image analysis . These users might be experts
of certain domain areas or experts in general areas of data science, but in our categorization, we
consider users in the data experts category to generally lack expertise in the technical specifics of
the machine learning algorithms. Instead, this group of users often utilize intelligent data analysis
tools or visual analytics systems to obtain insights from the data. Notice that there are overlaps
between XAI goals in different disciplines and visual analytics tools designed for Data Experts could
be used by both model designers and data analysts. However, design needs and approaches for
these XAI systems may be different across research communities. The main design goals for data
experts users of a XAI system are as follows:
G5: Model Visualization and Inspection: Similar to AI novices, data experts also benefit from
machine learning interpretability to inspect model uncertainty and trustworthiness . For
instance, machine-learning explanations help data experts to visualize models and inspect for
problems like bias . Another important aspect of model visualization and inspection for domain
experts is to identify and analyze failure cases of machine learning models and systems .
Therefore, the main challenge for data-analysis and decision-support systems is to improve model
transparency via visualization and interaction techniques for domain experts .
G6: Model Tuning and Selection: Visual analytics approaches can help data experts to tune
machine learning parameters for their specific data in an interactive visual fashion . The interpretability element in XAI visual analytic systems increase data experts’ ability to compare multiple
models and select the right model for the targeted data. As an example, Du et al. present
EventAction, an event sequence recommendation approach that allows the users to interactively
select records that share their desired attribute values. In the case of tuning DNN networks, visual
analytics tools enhance designers’ ability to modify networks , improve training , and to
compare different networks .
AI Experts
In our categorization, AI experts are machine learning scientists and engineers who design machine learning algorithms and interpretability techniques for XAI systems. Machine learning
interpretability techniques either provide model interpretation or instance explanations. Examples
of model interpretation techniques include inherently interpretable models , deep model
simplification , and visualization of model internals . Instance explanations techniques,
however, present feature importance for individual instances such as saliency map in image data
and attention in textual data . AI engineers also benefit from visualization and visual analytics
tools to interactively inspect model internal variables to detect architecture and training flaws
or monitor and control the training process , which indicates possible overlaps among design
goals. We list main design goals for AI Experts into two following items:
G7: Model Interpretability: Model interpretability is often a primary XAI Goal for AI experts.
Model interpretability allows getting new insights into how deep models learn patterns from
data . In this regard, various interpretability techniques for different domains have been
proposed to satisfy the need for explanation . For example, Yosinski et al. created an
interactive toolbox to explore CNN’s activation layers in real-time that gives an intuition about
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Table 3. Evaluation measures and methods used in studying user mental models in XAI systems
Mental Model Measures
Evaluation Methods
User Understanding of Model
Interview ( ) and Self-explanation ( )
Likert-scale Questionnaire ( )
Model Output Prediction
User Prediction of Model Output ( )
Model Failure Prediction
User Prediction of Model Failure ( )
“how the CNN works” to the user.
G8: Model Debugging: AI researchers use interpretability techniques in different ways to improve
model architecture and training process. For example, Zeiler and Fergus present a use case in
which visualization of filters and feature maps in CNN leads to revising training hyper-parameters
and, therefore, model performance improvement. In another work, Ribeiro et al. used model
instance explanations and human review of explanations to improve model performance through
feature engineering.
Other than main XAI goals for AI experts, machine learning explanations are used for other
purposes including detecting dataset bias , adversarial attack detection , and model failure
prediction . Also, visual saliency maps and attention mechanisms have been used as weakly
supervised object localization , multiple object recognition , and knowledge transfer 
techniques.
XAI EVALUATION MEASURES
Evaluation measures for XAI systems is another important factor in the design process of XAI
systems. Explanations are designed to answer different interpretability goals, and hence different
measures are needed to verify explanation validity for the intended purpose. For example, experimental design with human-subject studies is a common approach to perform evaluations with AI
novice end-users. Various controlled in-lab and online crowdsourced studies have been used for
XAI evaluation. Also, case studies aim to collect domain expert users’ feedback while performing
high-level cognitive tasks with analytics tools. By contrast, computational measures are designed
to evaluate the accuracy and completeness of explanations from interpretable algorithms.
In this section, we review and categorize the main evaluation measures for XAI systems and
algorithms. Table 2 shows a list of five evaluation measures associated with their design goals.
Additionally, we provide summarized and ready-to-use XAI evaluation measures and methods
extracted from the literature in Tables 3-7.
M1: Mental Model
Following cognitive psychology theories, a mental model is a representation of how users understands a system. Researchers in HCI study users’ mental models to determine their understanding
of intelligent systems in various applications. For example, Costanza et al. studied how users
understand a smart grid system, and Kay et al. studied how users understand and adapt to
uncertainty in machine learning prediction of bus arrival times.
In the context of XAI, explanations help users to create a mental model of how the AI works.
Machine learning explanation is a way to help the users in building a more accurate mental
model. Studying users’ mental models of XAI systems can help verify explanation effectiveness
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
in describing an algorithm’s decision-making process. Table 3 summarizes different evaluation
methods used to measure users’ mental model of machine learning models.
Psychology research in human-AI interactions has also explored structure, types, and functions
of explanations to find essential ingredients of ideal explanation for better user understanding and
more accurate mental models . For instance, Lombrozo studied how different types of
explanations can help structure conceptual representation. In order to find out how an intelligent
system should explain its behavior for non-experts, research on machine learning explanations has
studied how users interpret intelligent agents and algorithms to find out what users
expect from machine explanations. Related to this, Lim and Dey elicit types of explanations
that users might expect in four real-world applications. They specifically study what types of
explanations users demand in different scenarios such as system recommendation, critical events,
and unexpected system behavior. In measuring user mental model through model failure prediction,
Bansal et al. designed a game in which participants receive monetary incentives based on
their final performance score. Although experiments were done on a simple three-dimensional
task, their results indicate a decrease in users’ ability to predict model failure as data and model get
more complicated.
A useful way of studying users comprehension of intelligent systems is to directly ask them
about the intelligent system’s decision-making process. Analyzing users’ interviews, think-alouds,
and self-explanations provides valuable information about the users’ thought processes and mental
models . On studying user comprehension, Kulesza et al. studied the impact of explanation
soundness and completeness on fidelity of end-users mental model in a music recommendation
interface. Their results found that explanation completeness (broadness) had a more significant
effect on user understanding of the agent compared to explanation soundness. In another example,
Binns et al. studied the relation between machine explanations and users’ perception of
justice in algorithmic decision-making with different sets of explanation styles. User attention and
expectations may also be considered during the interpretable interface design cycles for intelligent
systems .
Interest in developing and evaluating human-understandable explanations has also led to interpretable models and ad-hoc explainers to measure mental models. For example, Ribeiro et al. 
evaluated users’ understanding of the machine learning algorithm with visual explanations. They
showed how explanations mitigate human overestimation of the accuracy of an image classifier and
help users choose a better classifier based on the explanations. In a follow-up work, they compared
the global explanations of a classifier model with the instance explanations of the same model and
found global explanations were more effective solutions for finding the model weaknesses .
In another paper, Kim et al. conducted a crowdsourced study to evaluate feature-based explanation understandability for end-users. Addressing understanding of model representations,
Lakkaraju et al. presented interpretable decision sets, an interpretable classification model,
and measured users’ mental models with different metrics such as user accuracy on predicting
machine output and length of users’ self-explanations.
M2: Explanation Usefulness and Satisfaction
End-user satisfaction and usefulness of machine explanation are also of importance when evaluating explanations in intelligent systems . Researchers use different subjective and objective
measures for understandability, usefulness, and sufficiency of details to assess explanatory value
for users . Although there are implicit methods to measure user satisfaction , a considerable part of the literature follows qualitative evaluation of satisfaction in explanations, such as
questionnaires and interviews. For example, Gedikli et al. evaluated ten different explanation
types with user ratings of explanation satisfaction and transparency. Their results showed a strong
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Table 4. User satisfaction measures and study methods used in measuring user satisfaction and usefulness of
explanations in XAI studies.
Satisfaction Measures
Evaluation Methods
User Satisfaction
Interview and Self-report ( )
Likert-scale Questionnaire ( )
Expert Case Study ( )
Explanation Usefulness
Engagement with Explanations ( )
Task Duration and Cognitive Load ( )
relationship between user satisfaction and perceived transparency. Similarly, Lim et al. explore
explanation usefulness and efficiency in their interpretable context-aware system by presenting
different types of explanations such as “why”, “why not” and “what if” explanation types and
measuring users response time.
Another line of research studies whether intelligible systems are always appreciated by the users
or it has a conditional value. An early work from Lim and Dey studied user understanding and
satisfaction of different explanation types in four real-world context-aware applications. Their findings show that, when considering scenarios involved with criticality, users want more information
explaining the decision making process and experience higher levels of satisfaction after receiving
these explanations. Similarly, Bunt et al. considered whether explanations are always necessary
for users in every intelligent system. Their results show that, in some cases, the cost of viewing
explanations in diary entries like Amazon and YouTube recommendations could outweigh their
benefits. To study the impact of explanation complexity on users’ comprehension, Lage et al. 
studied how explanation length and complexity affect users’ response time, accuracy, and subjective satisfaction. They also observed that increasing explanation complexity resulted in lowered
subjective user satisfaction. In a recent study, Coppers et al. also show that adding intelligibility
does not necessarily improve user experience in a study with expert translators. Their experiment
suggests that an intelligible system is preferred by experts when the additional explanations are not
part of the translators readily available knowledge. In another work, Curran et al. measured
users’ understanding and preference of explanations in an image recognition task by ranking and
coding user transcripts. They provide three types of instance explanations for participants and
show that although all explanations were coming from the same model, participants had different
levels of trust in explanations’ correctness, according to explanations clarity and understandability.
Table 4 summarizes the study methods used to measure user satisfaction and usefulness of
machine learning explanations. Note that the primary goal of XAI system evaluations for domain
and AI experts is through direct evaluation of user satisfaction of explanation design during the
design cycle. For example, case studies and participatory design are common approaches for directly
including expert users as part of the system design and evaluation processes.
M3: User Trust and Reliance
User trust in an intelligent system is an affective and cognitive factor that influences positive or
negative perceptions of a system . Initial user trust and the development of trust over time
have been studied and presented with different terms such as swift trust , default trust 
and suspicious trust . Prior knowledge and beliefs are important in shaping the initial state
of trust; however, trust and confidence can change in response to exploring and challenging the
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
Table 5. Evaluation measures and methods used in measuring user trust in XAI studies.
Trust Measures
Evaluation Methods
Subjective Measures
Self-explanation and Interview ( )
Likert-scale Questionnaire ( )
Objective Measures
User Perceived System Competence ( )
User Compliance with System ( )
User Perceived Understandability ( )
system with edge cases . Therefore, the user may have different feelings of trust and mistrust
during different stages of experience with any given system.
Researchers define and measure trust in different ways. User knowledge, technical competence,
familiarity, confidence, beliefs, faith, emotions, and personal attachments are common terms used to
analyze and investigate trust . For these outcomes, user trust and reliance can be measured
by explicitly asking about user opinions during and after working with a system, which can be
done through interviews and questionnaires. For example, Ming et al. studied the importance
of model accuracy on user trust. Their findings show that user trust in the system was affected by
both the system’s stated accuracy and users’ perceived accuracy over time. Similarly, Nourani et
al. explored how explanation inclusion and level of meaningfulness would affect the user’s
perception of accuracy. Their controlled experiment results show that whether explanations are
human-meaningful can significantly affect perception of system accuracy independent of the actual
accuracy observed from system usage. Additionally, trust assessment scales could be specific to the
systems application context and XAI design purposes. For instance, multiple scales would assess
user opinion on systems reliability, predictability, and safety separately. Related to this, a detailed
trust measurement setup is presentation in the paper by Cahour and Forzy , which measures
user trust with multiple trust scales (constructs of trust), video recording, and self-confrontation
interviews to evaluate three modes of system presentation. Also, to better understand factors that
influence trust in adaptive agents, Glass et al. studied which types of questions users would like
to be able to ask an adaptive assistant. Others have looked at changes to user awareness over time
by displaying system confidence and uncertainty of the machine learning outputs in applications
with different degrees of criticality .
Multiple efforts have studied the impact of XAI on developing justified trust in users in different
domains. For instance, Pu and Chen proposed an organizational framework for generating
explanations and measured perceived competence and user’s intention to return as the measures
for user trust. Another example compared user trust with explanations for different goals like
transparency and justification explanation . They considered perceived understandability to
measure user trust and show that transparent explanations can help reduce the negative effects of
trust loss in unexpected situations.
Studying user trust in real-world applications, Berkovsky et al. evaluated trust with various
recommendation interfaces and content selection strategies. They measured user reliance on a
movie recommender system with six distinct constructs of trust. Also on recommender algorithms,
Eiband et al. repeats the Langer et al.’s experiment on the role of “placebic” explanations
(i.e., explanations that convey no information) in mindlessness of user behavior. They studied if
providing placebic explanations would increase user reliance on the recommender system. Their
results suggest that future work on explanations for intelligent systems may consider using placebic
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
explanations as a baseline for comparison with machine learning generated explanations. Also
concerned with expert user’s trust, Bussone et al. measured trust by Likert-scale and thinkalouds and found that explanations of facts lead to higher user trust and reliance in a clinical
decision-support system. Table 5 summarizes a list of subjective and objective evaluation methods
to measure user trust in the machine learning systems and their explanations.
Many studies evaluate user trust as a static property. However, it is essential to take user’s
experience and learning over time into account when working with complex AI systems. Collecting
repeated measures over time can help in understanding and analyzing the trend of users’ developing
trust with the progression of experience. For instance, in their study, Holliday et al. evaluated
trust and reliance in multiple stages of working with an explainable text-mining system. They
showed that the level of user trust in the system varied over time as the user gained more experience
and familiarity with the system.
We note that although our literature review did not find a direct measurement of trust to be
commonly prioritized in analysis tools for data and machine learning experts, users’ reliance on
tools and the tendency to continue using tools are often considered as a part of the evaluation
pipeline during case studies. In other words, our summarization is not meant to claim that data
experts do not consider trust, but rather we did not find it to be a core outcome explicitly measured
in the literature for this user group.
M4: Human-AI Task Performance
A key goal of XAI is to help end-users to be more successful in their tasks involving machine
learning systems . Thus, human-AI task performance is a measure relevant to all three groups
of user types. For example, Lim et al. measured users’ performance in terms of success rate
and task completion time to evaluate the impact of different types of explanations. They use a
generic interface that can be applied to various types of sensor-based context-aware systems, such
as weather prediction. Further, explanations can assist users in adjusting the intelligent system
to their needs. Kulesza et al. study of explanations for a music recommender agent found a
positive effect of explanations on users’ satisfaction with the agent’s output, as well as on users’
confidence in the system and their overall experience.
Another use case for machine learning explanations is to help users judge the correctness of
system output . Explanations also assist users in debugging interactive machine
learning programs for their needs . In a study of end-users interacting with an email
classifier system, Kulesza et al. measured classifier performance to show that explanatory
debugging benefits user and machine performance. Similarly, Ribeiro et al. found users could
detect and remove wrong explanations in text classification, resulting in training better classifiers
with higher performance and explanations quality. To support these goals, Myers et al. 
designed a framework that users can ask why and why not questions and expect explanations
from the intelligent interfaces. Table 6 summarizes a list of evaluation methods to measure task
performance in human-AI collaboration and model tuning scenarios.
Visual analytics tools also help domain experts to better perform their tasks by providing
model interpretations. Visualizing model structure, details, and uncertainty in machine outputs
can allow domain experts to diagnose models and adjust hyper-parameters to their specific data
for better analysis. Visual analytics research has explored the need for model interpretation in
text and multimedia analysis tasks. This body of work demonstrates the
importance of integrating user feedback to improve model results. An example of a visual analytics
tool for text analysis is TopicPanaroma , which models a textual corpus as a topic graph
and incorporates machine learning and feature selection to allow users to modify the graph
interactively. In their evaluation procedure, they ran case studies with two domain experts: a public
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
Table 6. Evaluation measures and methods used in measuring human-machine task performance in XAI
Performance Measures
Evaluation Methods
User Performance
Task Performance ( )
Task Throughput( )
Model Failure Prediction ( )
Model Performance
Model Accuracy ( )
Model Tuning and Selection ( )
relations manager used the tool to find a set of tech-related patterns in news media, and a professor
analyzed the impact of news media on the public during a health crisis. In analysis of streaming
data, automated approaches are error-prone and require expert users to review model details and
uncertainty for better decision making . For example, Goodall et al. presented Situ,
a visual analytics system for discovering suspicious behavior in cyber network data. The goal
was to make anomaly detection results understandable for analysts, so they performed multiple
case studies with cybersecurity experts to evaluate how the system could help users to improve
their task performance. Ahn and Lin present a framework and visual analytic design to aid fair
data-driven decision making. They proposed FairSight, a visual analytic system to achieve different
notions of fairness in ranking decisions through visualizing, measuring, diagnosing, and mitigating
Other than domain experts using visual analytics tools, machine learning experts also use visual
analytics to find shortcomings in the model architecture or training flaws in DNNs to improve
the classification and prediction performance . For instance, Kahng et al. designed a
system to visualize instance-level and subset-level of neuron activation in a long-term investigation
and development with machine learning engineers. In their case studies, they interviewed three
machine learning engineers and data scientists who used the tool and reported the key observations.
Similarly, Hohman et al. present an interactive system that scalably summarizes and visualizes
what features a DNN model has learned and how those features interact in instance predictions.
Their visual analytic system presents activation aggregation to discover important neurons and
neuron-influence aggregation to identify interactions between important neurons. In the case
of recurrent neural networks (RNN), LSTMVis and RNNVis are tools to interpret
RNN models for natural language processing tasks. In another recent paper, Wang et al. 
presented DNN Genealogy, an interactive visualization tool that offers a visual summary of DNN
representations.
Another critical role of visual analytics for machine learning experts is to visualize model training
processes . An example of a visual analytics tool for diagnosing the training process of a deep
generative model is DGMTracker , which helps experts understand the training process by
visually representing training dynamics. An evaluation of DGMTracker was conducted in two case
studies with experts to validate efficiency of the tool in supporting understanding of the training
process and diagnosing a failed training process.
M5: Computational Measures
Computational measures are common in the field of machine learning to evaluate interpretability
techniques’ correctness and completeness in terms of explaining what the model has learned.
Herman notes that reliance on human evaluation of explanations may lead to persuasive
explanations rather than transparent systems due to user preference for simplified explanations.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Table 7. Evaluation measures and methods used for evaluating fidelity of interpretability techniques and
reliability of trained models. This set of evaluation methods is used by machine learning and data experts to
eighter evaluate the correctness of interpretability methods or evaluate the training quality trained models
beyond standard performance metrics.
Computational Measures
Evaluation Methods
Explainer Fidelity
Simulated Experiments ( )
Sanity Check ( 
Comparative Evaluation ( )
Model Trustworthiness
Debugging Model and Training ( )
Human-Grounded Evaluation ( )
Therefore, this problem leads to the argument that explanations’ fidelity to the black-box model
should be evaluated by computational methods instead of human subject studies. Fidelity of an adhoc explainer refers to the correctness of the ad-hoc technique in generating the true explanations
(e.g., correctness of a saliency map) for model predictions. This leads to a series of computational
methods to evaluate correctness of generated explanations, consistency of explanation results, and
fidelity of ad-hoc interpretability techniques to the original black-box model .
In many cases, machine learning researchers often consider consistency in explanation results,
computational interpretability, and qualitative self-interpretation of results as evidence for explanation correctness . For example, Zeiler and Fergus discuss fidelity
of the visualization for CNN network by its validity in finding model weaknesses resulted in
improved prediction results. In other cases, comparing a new explanation technique with existing
state-of-the-art explanation techniques is is used to verify explanation quality . For
instance, Ross et al. designed a set of empirical evaluations and compared their explanations’
consistency and computational cost with the LIME technique . In a comprehensive setup,
Samek et al. proposed a framework for evaluating saliency explanations for image data that
quantify the feature importance with respect to the classifier prediction. They compared three different saliency explanation techniques for image data (sensitivity-based , deconvolution ,
and layer-wise relevance propagation ) and investigated the correlation between saliency
map quality and network performance on different image datasets under input perturbation. On
the contrary, Kindermans et al. show interpretability techniques have inconsistencies on
simple image transformations, hence their saliency maps can be misleading. They define an input
invariance property for reliability of explanations from saliency methods. To extend a similar idea,
Adebayo et al. propose three tests to measure adequacy of interpretability techniques for tasks
that are sensitive to either data or the model itself.
Other evaluation methods include assessing explanation’s fidelity in comparison to inherently
interpretable models (e.g., linear regression and decision trees). For example, Ribeiro et al. 
compared explanations generated by the LIME ad-hoc explainer to explanations from an interpretable model. They created gold standard explanations directly from the interpretable models
(sparse logistic regression and decision trees) and used these for comparisons in their study. A
downside of this approach is that the evaluation is limited to generating a gold standard by an
interpretable model. User simulated evaluation is another method to perform computational evaluation of model explanations. Ribeiro et al. simulated user trust in explanations and models by
defining “untrustworthy” explanations and models. They tested a hypothesis on how real users
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
would prefer more reliable explanations and choose better models. The authors later repeated
similar user simulated evaluations in the Anchors explanation approach to report simulated
users’ precision and coverage in finding the better classifier by only looking at explanations.
A different approach in quantifying explanations quality with human intuition has been taken
by Schmidt and Biessmann by defining an explanation quality metric based on user task
completion time and agreement of predictions. Another example is the work of Lundberg and
Lee , who compared the SHAP ad-hoc explainer model with LIME and DeepLIFT with
the assumption that good model explanations should be consistent with the explanations from
humans who understand the model. Lertvittayakumjorn and Toni also present three user tasks
to evaluate local explanation techniques for text classification through revealing model behavior
to human users, justifying the predictions, and helping humans investigate uncertain predictions.
A similar idea has been implemented in by feature-wise comparison of a ground-truth
and model explanation. They provide a user annotated benchmark to evaluate machine learning
instance explanations. Later, Poerner et al. use this benchmark as human annotated ground
truth in comparison to small-context (word level) and large-context (sentence level) explanation
evaluation. User annotated benchmarks can be valuable when considering human meaningfulness
of explanations, though the discussion by Das et al. implies that machine learning models
(visual question answering attention models in their case) do not seem to look at the same regions
as humans. They introduce a human-attention dataset (collection of mouse-tracking data) and
evaluate attention maps generated by state-of-the-art models against human.
Interpretability techniques also enable quantitative measures for evaluating model trustworthiness (e.g., model fairness, reliability, and safety) through its explanations. Trustworthiness of a
model represents a set of domain specific goals such as fairness (by fair feature learning), reliability,
and safety (by robust feature learning). For example, Zhang et al. present a case of using machine learning explanations to find representation learning flaws caused by potential biases in the
training dataset. Their technique mines the relationships between pairs of attributes according to
their inference patterns. Further, Kim et al. presented quantitative testing of machine learning
models by their explanations. In their concept activation vector technique, the model can be tested
for specific concepts (e.g., image patterns) and a vector score shows if the model is biased toward
that concept. They later extended their concept-based global explanation of model representation
learning for systematic discovery of concepts that are human meaningful and important for the
model prediction . They used human subject experiments to evaluate learned concepts. Table 7
summarizes a list of evaluation methods to measure fidelity of interpretability techniques and
model trustworthiness with computational techniques.
XAI DESIGN AND EVALUATION FRAMEWORK
The variety of different XAI design goals (Section 6) and evaluation methods (Section 7) from
our review suggests the need for diverse sets of techniques to build end-to-end XAI systems.
However, it is generally insufficient to take design practices and evaluation methods separately. A
holistic and more actionable vantage will require consideration of dependencies between design
goals and evaluation methods and will inform when to choose between them during the design
cycles. Previously, various models and guidelines for the design and evaluation of AI-infused
interactive user interfaces and visual analytics systems have been proposed to help
designers through the design process. However, challenges in generating useful machine learning
explanations and presenting them through an appropriate interface that aligns with target outcomes
call for a multidisciplinary workflow framework.
Thus, based on our analysis of prior work, we propose a design and evaluation framework for
XAI systems. The impetus for this framework is the desire to organize and relate the diverse set of
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
XAI System Goals
XAI System Outcomes
Explainable
Interface Design
Explainable Interface
Evaluation
Interpretable
Algorithm Design
Interpretable Model
Evaluation
Design Pole
Evaluation Pole
Fig. 4. XAI Design and Evaluation Framework: our nested model for design and evaluation of explainable
machine learning systems. The outer layer demonstrates system-level design goals which are paired with
evaluation of high-level XAI outcomes. The middle layer shows explainable user interface and visualization design step paired with appropriate user understandability and satisfaction evaluation measures. The innermost
layer presents design and evaluation of trustworthy interpretable machine learning algorithms.
existing design goals and evaluation methods in a unified model. The framework is intended to
give guidance on what evaluation measures are appropriate to use at which design stage of the
XAI system. Figure 4 summarizes the framework as a nested model for end-to-end XAI system
design and evaluation. The formulation of the model as layers relates to the core design goals
and evaluation interests from the different research communities (as identified from the literature
review) to help promote interdisciplinary progress in XAI research. The model is structured to
support system design steps by starting from the outer layer (XAI System Goals), then addressing
end-user needs in the middle layer (Explainable Interface), and finally focusing on underlying
interpretable algorithms in the innermost layer (Interpretable Algorithms). The nested model is
organized with a Design Pole focusing on design goals and choices, and an Evaluation Pole presenting appropriate evaluation methods and measures for each layer. Our framework suggests
iterative cycles of design and evaluation to cover both algorithmic and human-related aspects of
XAI systems. In this section, we elaborate on details of the nested framework and provide guidelines
on using it for multidisciplinary XAI system design.
Case Study Example: To showcase a practical example of using the framework, we also include
a case study of a collaborative design and development effort for an XAI system. In the scenario
of the case study, a multidisciplinary team of researchers designed a XAI system for fake news
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
detection for non-expert (not AI experts or news analysts) daily newsreaders. The design team
planned to add a XAI Assistant feature to a news reading and sharing website to perform fake news
detection. The system design consisted of a news reading interface equipped with the XAI news
assistant (news assistant) to help the user identify fake news while reviewing news stories and
articles. The presented case is the result of an ongoing research done over a one-year period by
a team of eight university researchers with HCI, Visualization, and AI backgrounds. During the
following subsections, each framework guideline is followed by an example of its application in
our case study.
XAI System Goals Layer
As designers in a multidisciplinary team have different roles and priorities in building a XAI system,
we suggest beginning the system design cycle from the XAI Goal layer (the outer layer of Figure 4)
to characterize design goal and system expectations. Specifically, this step involves identifying
the purpose for explanation and choosing what to explain for the targeted end-user and dedicated
application. The iterative refinements between XAI goal (top pole) and system evaluation (bottom
pole) present how the paired evaluation measures help to improve system design. We organize the
following guidelines for the XAI goal layer.
At the beginning of the system design process, the team will need to specify explainability requirements for each framework layer based on the evaluation metrics. The explainability requirements
are intended to satisfy overall system goals defined by user (or customer) needs, and sometimes
regulations, laws, and safety standards. Later, the evaluation step in each design cycle will have the
team revisit the initial XAI system requirements. The sufficiency of the evaluation results in comparison to the initial design requirements serves as a key indicator of whether to stop or continue
design iteration. However, since many subjective measures are used in the process, it is important
to choose an appropriate evaluation baseline (see Section 9.4) to track progress during design cycles.
Guideline 1: Determine XAI System Goals: Identifying and establishing clear goals and expectations from XAI system is the first step in the design process. XAI Design goals could be driven
by many motivations like improving user experience on an existing system, advancing scientific
findings , or adhering to new regulations . In Section 6 we reviewed eight main goals
(G1-G8) for XAI systems. Also, ordering the priority of goals in cases with multiple design goals
can be beneficial in the next steps of the process (see Guideline 2). Given the fact that different
XAI user types and applications are interested in various design goals, it is important to establish
these goals early in the design process to identify and align with appropriate design principles.
A pitfall in this stage is to pick XAI goals without considering the end-user group, algorithmic
limitations, and user preferences in the context of the application. Overshooting XAI goals could
hurt evaluation results moving forward in the design process.
Application in Case Study: In the first step of our case study with a news curation application, the team started with identifying the main goals and expectations for the XAI
news assistant. The design focused on novice end-users without any particular expertise.
The XAI design goal was to improve user reliance and mental model of news predictions
through explainable design. The team hypothesized that end-users would trust and rely
on the fake news detection assistant, given that the new XAI is capable of providing
explanations for each news story. Also, the team hoped that users would be able to use
the explanations to learn model weaknesses and strengths to provide feedback to the
developer team.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Guideline 2: Decide What to Explain: The second step in the XAI system design is to identify
“what to explain” to the user in order to achieve the initial XAI goals (see Guideline 1) of the
system. We reviewed multiple machine learning interpretability techniques and explanation types
in Sections 4.1, 4.2, and 4.3 which can provide different types of information to the user. Although
theory-driven design frameworks discuss explanation mechanisms driven by human reasoning
semantics , user-centered methods to identify useful explanations such as online surveys,
interviews, and user observations (e.g., ) to understand when and what needs to be explained
for the users to understand better and trust intelligent systems. Preliminary experiments are valuable
in the early steps of the design cycle to identify and narrow down explanation options for the user
in order to satisfy design goals. A typical approach for evaluating the effectiveness and usefulness
of explanation choice in user-centric experiments is to compare the user’s mental model of the
system with and without explanation components. On this subject, Lim and Dey conducted
experiments to discover what type of information users are interested in different real-world
context-aware application scenarios. Stumpf et al. also performed end-user interviews to
identify user perceptions and expectations from an interpretable interface as well as finding main
decision points where users may need explanations. In another work, Haynes et al. provide a
review and studies incorporating different explanations (operational, ontological, mechanistic, and
design rationale explanations) in intelligent systems. Similarly, visual analytics design involves
expert interviews and focus groups in the design path to identify design goals .
The design process in this step involves algorithmic implementation constraints like “what can
be explained” to the user. For example, global explanations from a DNN may not feasible and comprehensible due to the large number of variables in the graph. Additionally, research shows instance
explanations from a DNN lack completeness and may fail to present salient features in cases .
Such constraints and decision points could be solved through focused groups, brainstorming, and
interviews between model designers and interface designers in the team. Therefore, a design pitfall
for explanation choices is not to take limitations of interpretability techniques into account.
Application in Case Study: In our scenario, efficient news curation required fake news
detection with the help of our XAI assistant. In the analysis of what the system should
explain, the design team decided to identify candidate useful and impactful explanation
options. We started with reviewing machine learning research on false information
(e.g., rumor, hoax, fake news, clickbait) detection as well as HCI research on news feeds
and news search systems to identify key attributes for news veracity checking .
Given the non-expert target end-users, explanatory information needed to limit technical
details. Next, the user interface designers and machine learning designers in the team
discussed candidate explanation choices and algorithmic constraints in interpretability
techniques. That is, some options for what to explain may not be entirely possible given the
interpretability of existing models, and the team needed to consider whether alternative
learning techniques could provide better explanations or if the design team would need
to figure out meaningful ways to explain the information that was available from the
Guideline 3: Evaluate System Outcomes: Evaluation of XAI system outcomes is the final step
in the evaluation process. Figure 4 shows how the final system outcome evaluation is paired
with the initial design goals in the outer layer of our framework. The main goal of this stage
is to quantitatively and qualitatively assess the effectiveness of the XAI system for the initially
established system-level XAI goals. Clearly, evaluation of final system outcomes could be influenced
by the design of the explainable user interface (intermediate layer) and the design of interpretable
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
algorithms (innermost layer). For example, evaluating a newborn interpretable machine learning
algorithm’s output using human subjects through a weak in-lab or crowdsourced user study may
not be meaningful or productive for XAI system outcomes if core computational changes are still
in progress and could ultimately change the entire model interpretability and explanation format
later. Also, changes in the targeted user could affect evaluation results at this stage. For example,
a system designed for novices may not satisfy the needs of an expert user and hence would not
improve performance as expected. Evaluation measures in this layer depend on the design goals,
application domain, and targeted users. Example evaluation measures for final system outcomes
include user trust and reliance on the system , human-machine task performance ,
user awareness , and user understanding of their personal data . An effective process
for evaluation of high-level XAI outcomes is to break down the evaluation goal into multiple
well-defined measures and metrics. This way, the team can perform evaluation studies on different
steps using valid methods in controlled setup. For example, in the evaluation of XAI systems
for trustworthiness, several factors of human trust could be measured during and after a period
of user experience with the XAI system. In addition, computational measures (Section 7.5) are
used to examine the fidelity of interpretability methods and trustworthiness of the model with
objective metrics. A possible pitfall in evaluation of the XAI system outcomes is performing the
evaluation without considering the model trustworthiness and explanations’ correctness from the
interpretable model layer (see Guideline 7) and explanation understandability and usefulness from
the user interface layer (see Guideline 5).
Application in Case Study: In our case study with news review and curation, we needed to
evaluate our XAI news assistant with non-expert users who would gather news stories
while flagging fake news articles. In the evaluation step, the team ran multiple large-scale
human-subject studies with novice participants recruited through Amazon Mechanical
Turk to work with our news reading system. Note that both the explainable interface
and interpretable algorithm passed multiple design and testing iterations before this
evaluation step. Major decisions for this evaluation was how to structure the duration
and complexity of the user task while appropriately testing the system’s full range of
functionality. The task was designed with questions built in to help collect subjective
data in addition to the objective user performance data. Multiple evaluation measures
were chosen for system outcomes, including: subjective user trust in the news assistant,
user agreement rate with the news assistant, veracity of user-shared news stories, and
user accuracy in guessing the news assistant output. Both qualitative and quantitative
analysis of user feedback and interaction data were valuable to the evaluation of system
outcomes. The results and analysis from these evaluations helped the team to understand
the effectiveness of the XAI elements (in both the algorithm and the interface) for the
initial system goals.
User Interface Design Layer
The middle layer of our framework is concerned with designing and evaluating an explainable
interface or visualization for the user to interact with XAI system. Interface design for explanations
consists of presenting model explanations from interpretable algorithms to end-users in terms of
their explanation format and interaction design. The importance of this layer is to satisfy design
requirements and needs to be determined in the XAI system design layer (see Guideline 2). An
elegant translation of machine-generated explanations (e.g., verbal, numeric, or visual explanation)
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
needs carefully designed human-understandable and satisfying explanations in the user interface.
In Section 4.4, we reviewed multiple types of explanation formats for integrating XAI elements
into the user interface. The iterative movement between Design pole and Evaluation pole in this
layer presents design refinement in pursuit a desired goal state.
Guideline 4: Decide How to Explain: Identifying candidate explanation formats for the targeted
system and user group is the first step to deliver machine learning explanations to end-users. The
design process can account for different levels of complexity, length, presentation state (e.g.,
permanent or on-demand), and interactivity options depending on the application and user type.
The explanations format in the interface is particularly important to improve user understanding
of underlying algorithms. Studies show that while detailed and complex interactive representations
may aim to communicate the explanations to the expert users, AI-novice users of XAI system prefer
more simplified explanation and representation interfaces . User satisfaction of interface design
is also another critical factor in user engagement of the interface components . Additionally,
interaction design for explainable interfaces can allow a user to communicate with the system
to adjust explanations and could better support user inspection of the system . Research of
intelligent interface design presents multiple design methods such as wireframing and low-fidelity
prototyping (e.g., ) that could also be adapted to the explainable interface design. Also,
existing design guidelines and best-practice knowledge for AI-infused interfaces (e.g., ) and
visualizations (e.g., ) could be used in this stage to leverage similar systems for explainable
interface design. Aside from model explanations, providing prediction uncertainty also has been
identified as an important factor for both general end-users and data expert users . For example,
Kay et al. presented the full design cycle for an uncertainty visualization interface in a bus
arrival time application. Their design process included surveying to identify usage requirements,
developing alternative layouts, running user testing, and final evaluation of user understanding of
machine learning output.
Application in Case Study: To determine how to explain news classification results to nonexpert end users, the user interface design team started the process by reviewing the initial
system goals and explanation types. The team then continued with multiple interface
sketches that matched the intended application and user tasks. During the initial design
steps, the team tried to keep a balance between interface complexity and explanation
usefulness by choosing among available explanation types from our interpretable machine
learning algorithms. Next, mock-ups from the top three designs were implemented for
testing with a small number of participants. Each mock-up had a different arrangement
of data, user task flow, and explanation format for the news assistant interface. Our
human-subject experiments in this stage were based on user observations and postusage interviews to collect qualitative feedback regarding participant understanding and
subjective satisfaction of explanation components and interface arrangements. Interviews
resulted in the selection of the most comprehensible and conclusive design among the
available options to continue with (see Guidelines 5).
Guideline 5: Evaluate Explanation Usefulness: This mid-layer evaluation step can be used
along with various measures to help assess user understanding of the XAI underlying intelligent
algorithms. A series of user-centered evaluations of explainable interface with multiple goals and
granularity levels could be performed to measure:
(1) User understanding of explanation.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
(2) User satisfaction of explanation.
(3) User mental model of the intelligent system.
Evaluations in the middle layer are particularly important due to the impact on XAI system
outcomes (outer layer) and being affected by interpretable model outputs (inner-most layer). Specifically, evaluation measures in this stage can inform how well users understand the interpretable
system, however, the design validity at this step also may be reflected by higher-level XAI outcomes
(i.e., outer-layer evaluation) such as user trust and task performance. Note that user understanding
of an XAI system could be limited to parts of the system rather than the entire system; similarly,
understanding may be limited to a subspace of scenarios rather than all possible scenarios.
The three evaluation measures introduced for this step could be used on multiple iterative
cycles to improve overall explainable interface design. For example, Saket et al. studies users
understanding of visualization encoding and effectiveness of interactive graphical encoding for
end-user. On the other hand, user satisfaction of explanation type and format depends on factors
such as targeted application criticality and user-preferred cognitive load . Evaluating user
mental model is also an effective way to measure usefulness of explainable interfaces. Tables 3 and 4
present a list of measures for evaluating explainable interfaces in this step. The choice of baseline is
another important factor in evaluating explainable interfaces. Typically, a combination of qualitative
and quantitative analysis are used to measure effects of explanation components (in comparison
to non-explainable system) or to compare multiple explanations types. However, the choice of
placebic explanations has been proposed as the evaluation baseline for more accurate measurement
of explanation content . In the case of expert review, evaluation of a domain expert’s mental
model commonly involves comparison with the AI expert’s mental model and description of
“how the model works”. Section 9.4 reviews common choices of ground-truth baselines in XAI
evaluation studies. With all approaches, updates in explanation components of the interface require
assessment of their impact on user experience and understandability. However, the metrics and
depth of evaluation vary during the evaluation cycles as the team narrows down specific needs.
Finally, a possible evaluation pitfall for explainable interfaces is going after broad measures of XAI
outcomes (See Guideline 3) rather than focusing on a narrower scope of explanation components
and interactions.
Application in Case Study: In our case study, interface designers started evaluation of
candidate explanation components by a series of small studies with a repeated-measures
design so that the same study participant could experience different explanation designs
in one session. Next, we analyzed quantitative and qualitative data collected from the
end-users to choose candidate designs and routes to further improve the interface for
explainable components. Discussions with the machine learning team also helped to
find sources of limitations in the interpretability technique that could possibly affect
user satisfaction. After the initial cycles of revision, we collected a round of external
and internal expert reviews to update the study methodology and data collection details
according to project progress.
Interpretable Algorithm Design Layer
The innermost layer of our framework involves designing interpretable algorithms that are able
to generate explanations for the users. The last design step in our XAI system framework is the
choice of interpretability technique (design pole) to generate the outlined explanation types. However, evaluating the generated explanation (evaluation pole) is the first evaluation step before
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
human-subject evaluations in the explainable interface. Ideally, the interpretability techniques
should generate explanations in accordance with the requirements in the explainable interface
design step (see Guideline 4); however, the choice of interpretability technique depends on domain
and carries implementation limitations. For example, while shallow models are desired for their
high interpretability, these models typically do not perform well in cases of complex and high
dimensional data like image and text. On the other hand, highly accurate predictions in black-box
models (e.g., deep neural networks and random forest models) require post-processing and ad-hoc
algorithms to generate explanations. The ad-hoc approach also has limitations on both choice
of explanation type and need for completeness and fidelity validation compared to the
original model. This shows not only machine learning designers should consider the trade-off
between model interpretability and performance but also should consider the fidelity of the ad-hoc
explainer to black-box model. We suggest two following design and evaluation guidelines for this
Guideline 6: Design Interpretability Technique: Designing interpretable decision-making
algorithms starts with the choice of machine learning model. Shallow machine learning models
(e.g., linear models and decision trees) have intrinsic interpretability due to low number of variables
and model simplicity. For more complex models (e.g., random forest and DNN), ad-hoc explainer
technique (see Section 4.2) are needed to generate explanations. However, the choice of machine
learning model (i.e., shallow vs. deep) is bounded by model’s performance on data domain. Secondly,
ad-hoc explainer techniques have certain limitations in their explanation type. The importance of
choosing the right combination of model and explainer is in their impact on providing useful (See
Guideline 4) and trustworthy explanations for end-users.
Machine learning research has proposed various ad-hoc explainers to generate “Why” explanations (e.g., feature attribution ), “How” explanations (e.g., rules list ), “What else”
explanation (e.g., similar training instance ), and “What if” (e.g., sensitivity analysis )
explanation types. However, despite substantial research in interpretable machine learning techniques, a core issue in model explanations is the difference between machine learning model’s
decision-making logic and human sense-making as the receiver .
Application in Case Study: In our fake news detection case study, the explainable interface
design team had previously discussed candidate explanation choices with the machine
learning design team (see Guidelines 2 and 4). Therefore, a final review of model-generated
explanations and an assessment of implementation limitations were performed in this step.
For example, removing noise-like features from saliency maps, normalizing attributions
scores, and resolving contradicting explanations between an ensemble of models were
primary implementation bottlenecks that were resolved in this step. Specifically, as a
decision point for trade-offs between clarity and faithfulness of explanations, the team
decided on using heuristic filters to eliminate features with a very low attribution score
for the sake of presentation simplicity.
Guideline 7: Evaluate Model Trustworthiness: Evaluating the interpretable machine learning is the first evaluation step in our framework due to its impact on outer layer evaluation measure.
The high significance of this evaluation step stems from the possibility that any unreliability of
interpretability at this inner layer will propagate to all other outer layers. Such unintended error
propagation may lead to problematic outer-layer design decisions as well as misleading evaluation
results. We discuss two main evaluation goals for the innermost layer:
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
(1) Evaluating model trustworthiness.
(2) Evaluating ad-hoc explainer fidelity.
The first evaluation goal aims to utilize interpretability techniques as a debugging tool to analyze
the model’s trustworthiness on learning concepts beyond general performance measures .
Examples of model trustworthiness validation include evaluating model reliability in financial risk
assessment , model fairness in social influencing applications , and model safety for its
intended functionality . Researchers have also proposed various regularization techniques for
enhancing trustworthy feature learning in machine learning models . Next, the second
evaluation goal targets fidelity of ad-hoc explainer techniques to the black-box model. Research
shows that different ad-hoc interpretability techniques have inconsistencies and can be misleading . Evaluating explanation trustworthiness can verify explainer fidelity in terms of how well it
represents the black-box model (see Section 7.5).
Application in Case Study: In our case study, we paid careful attention to qualitative
reviewing of the model explanations after each design iteration. Our initial qualitative
review of model explanations led to dataset cleaning through a heuristic search aimed
at the removal of mislabeled examples and unrelated news articles. An improvement
to model performance was achieved after dataset cleaning. Then, after the first round
of human-subject evaluation of the explainable interface (see Guideline 5), the team
identified negative effects of keyword explanations with low attention scores from endusers. The team decided on using a lower threshold for visualizing attention maps to
reduce clutter and “noisy explanations” for end-users. Finally, after one round of XAI
outcome evaluation (see Guideline 3), analysis of users’ mental models revealed that a
dataset imbalance between the “fake news” and “true news” was causing a bias for the
model in that the model was usually more confident in predicting fake news over true
DISCUSSION
In our review, we discussed multiple XAI design goals and evaluation measures appropriate for
various targeted user types. Table 2 presents the categorization of selected existing design and
evaluation methods that organizes literature along three perspectives: design goals, evaluation
methods, and the targeted users of the XAI system. Our categorization revealed the necessity of
an interdisciplinary effort for designing and evaluating XAI systems. To address these issues, we
proposed a design and evaluation framework that connects design goals and evaluation methods
for end-to-end XAI systems design, as presented through a model (Figure 4) and guidelines. In
this section, we discuss further considerations for XAI designers to benefit from the body of
knowledge of XAI system design and evaluation. The following recommendations support and
promote different layers of the proposed design and evaluation framework as well.
Pairing Design Goals with Evaluation Methods
It is essential to use appropriate measures for evaluating the effectiveness of design elements.
A common pitfall in choosing evaluation measures in XAI systems is that the same evaluation
measure is sometimes used for multiple design goals. A simple solution to address this issue is to
distinguish between measurements by using multiple scales to capture different attributes in each
evaluation target. For example, the concept of user trust consists of multiple constructs that
could be measured with separate scales in questionnaires and interviews (see Section 7.3). User
satisfaction measurements could also be designed for various attributes such as understandability of
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
explanations, usefulness of explanations, and sufficiency of details to target specific explanation
qualities (see Section 7.2).
An efficient way to pair design goals with appropriate evaluation measures is to balance different
design methods and evaluation types in iterative cycles of design. Managing the trade-offs between
qualitative and quantitative methods in the design process can allow designers to take advantage
of different approaches, as needed. For example, while focus groups and interviews provide more
detailed and in-depth feedback on the users’ mental model , remote measurements are highly
valuable due to the scalability of the collected data even though they provide less detail for drawing conclusions . Thus, one successful approach could be to start with multiple small-scale
prototyping and formative studies collecting qualitative measures at the earlier stages of the design
(e.g., for XAI system goals layer in the framework) and continue with larger-scale studies and
quantitative measures in the later stages (e.g., for interpretable model and interface evaluations in
the framework).
Overlap Among Design Goals
In our categorization of XAI systems, we chose two main dimensions to organize XAI systems by
their Design Goals and Evaluation Measures in Section 5. The XAI design goals (G1–G8) were based
on the goals extracted from the surveyed papers, and since the XAI design goals are primarily derived
from their targeted user groups, we note that overlaps among goals do exist across disciplines.
For instance, there is overlap of the goals of G1: Algorithmic Transparency for novice users in
HCI research, G5: Model Visualization for data experts in visual analytics, and G7: Interpretability
Techniques for AI experts in machine learning research. While overlapping, these similar goals
are studied with different objectives across the three research disciplines leading to diverse sets of
design requirements and implementation paths. For example, designing XAI systems for AI novices
requires processes and steps to build human-centered explainable interfaces to communicate model
explanations to the end-users, whereas designing new interpretability techniques for AI experts has
a different set of computational requirements. Another example of overlap in XAI goals is between
the goal for G6: Model Visualization and Inspection for data experts and G8: Model Debugging for AI
experts, in which different sets of tools and requirements are used to address different research
objectives.
To address the overlap between XAI goal among research disciplines, we used the XAI User
Groups as an auxiliary dimension to organize XAI goals in this cross-disciplinary topic (Section 6)
and emphasize the diversity of diverse research objectives. The three user groups were chosen
to organize research objectives and efforts into HCI (for AI novices), visual analytics (for data
experts), and machine learning (for AI experts) research fields. Additionally, as described in the
framework, the three user groups prioritize design objectives in the design process for the XAI
system rather than absolute separation of design goals. For example, the objectives and priorities
in XAI system design for algorithmic bias mitigation for domain experts in a law firm are certainly
different from those of model training and tuning tools for AI experts. However, by following the
multidisciplinary design framework, a design team can translate XAI system goals into design
objectives for explainable interface and machine learning techniques to improve the design process
in different layers. Therefore, in the above example, the design team can focus on diverse interface
design and interpretability technique objectives to achieve the primary XAI goal of bias mitigation
for the domain experts. Note that the specifics of any particular system will determine the priorities
of different objectives.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
System Evaluation Over Time
An important aspect in evaluating complex AI and XAI systems is to take the user learning into
account. Learnability is even more critical when measuring mental models and user trust in the
system. A user learns and gets more familiar with the system over time with continued interaction
with the system. This brings the importance of repeated temporal data capture (in contrast to
static measurements) for XAI evaluations. Holliday et al. present an example of multiple
trust assessments during the user study. They measured user trust at regular intervals during the
study to capture changes in user trust as the user interacts more with the system. Their results
indicates an XAI system outperformed a non-XAI counterpart in maintaining user trust over time.
Time-based measurements, also referred to as dynamic measurements, allows designers to monitor
explanation usability and effectiveness in various contexts and situations . For instance,
Zhang et al. explore the effect of intelligent system explanations in user trust calibration. In
their experiments, they observe significant effect on calibration of trust when model prediction
confidence score was shown to participants. In another example, a study by Nourani et al. 
controlled whether users’ early experiences with an explainable activity recognition system had
better or worse model outputs, and the first impressions significantly affected both task performance
and user confidence in understanding how the system works. In a study with a news review task,
Mohseni et al. identified different user profiles for changes in trust over time (trust dynamics)
while working with the assistance of an explainable fake news detector. Their analysis of results
revealed a significant effect of machine learning explanations on user trust dynamics.
Long-term evaluation of XAI systems can also allow designers to estimate valuable user experience factors such as over-trust and under-trust on the system. User-perceived system accuracy 
and transparency are examples of long-term measures for explanation usability that depend
on building user trust in the system’s interpretability. As more information is provided by explanations over time, reasoning and mental strategies may change as users create new hypotheses about
system functionality. Therefore, it is essential to also consider users’ mental models and trust in
extended studies to evaluate all aspects of the XAI system.
Another use case of long-term measurements is to evaluate the effects of intelligent system’s
non-uniform behaviors in real-world scenarios. This means, although in a controlled study setup, a
balanced set of input examples will present the system to the user, in real-world scenarios, users
may face alterations in system performance in long-term interaction with the system. Long-term
measurements will identify user’s unjust trust in the system due to a limited or biased set of
interactions with the system. For example, in the context of autonomous vehicles, Kraus et al. 
presented a model of trust calibration and presented studies on trust dynamics in the early phases
of user interaction with the system. Their results indicate the effects of error-free automation in
steady increase of user trust as well as the effects of user a priori information in eliminating the
decrease of trust in case of system malfunction.
Evaluation Ground Truth
Research on XAI systems study various goals with different measures across multiple domains.
The breadth of XAI research makes it challenging to interpret and transfer findings from one
task and domain to another. Knowing key factors for interpreting implications of evaluation
results is essential to aggregate findings across domains and disciplines. An important factor in
understanding XAI evaluation results and comparing results among multiple studies is the choice of
ground truth. In the following, we review common choices of ground truth for both human-subject
and computational evaluation methods.
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
Human-subject experiments often take the form of controlled studies to examine the effects of
machine learning explanations on a control group in comparison to a baseline group. In these setups,
the choice of the baseline could affect results implications and significance. Our review of papers in
the space of XAI evaluation shows the majority of study designs use a no explanation condition as
the baseline condition to measure the effectiveness of model explanations in an explanation group.
Examples for the baseline include approaches that remove model explanations related components
and features form the interface in the baseline condition . In other work, Poursabzi et
al. also included a no AI baseline to measure participants’ performance without the help
of model predictions. Another way is to compare the effects of explanation type or complexity
between study conditions without the no explanation baseline. For instance, Lage et al. 
present a study to evaluate the effects of explanation complexity on participants’ comprehension
and performance. They used linear and logistic regression to estimate the effects of explanation
complexity on participants’ normalized response time, response accuracy, and subjective task
difficulty rating.
Though the above mentioned studies are controlled experiments, there may still be unaccounted
human behavioral implications due to differences in the complex process of explaining worthy of
consideration. Langer et al. present an experiment on “placebic” explanations that shows
people’s mindless behavior when facing explanations for actions. In a simple setup, their study
showed that when asking a request, inclusion of explanations and justifications increased user’s
willingness to comply even if the explanations convey no meaningful information. Recently, Eiband
et al. proposed using placebic explanations instead of a no explanation condition as the baseline
for XAI human subject studies. Therefore, using non-informative or even randomly generated
explanations as the baseline condition could potentially counteract a participant’s positive tendency
toward explanations and improve study results.
Considering other approaches, a commonly accepted computational technique for quantitatively
evaluating instance explanations is to create a ground truth based on the input features that
semantically contribute to the target class. For example, image segmentation maps (annotations of
objects in images) are used to evaluate model generated saliency maps in weakly supervised object
localization tasks . Mohseni et al. proposed a multi-layer Human-Attention baseline
for feature-level evaluation of machine learning explanations. Their Human-Attention baseline
provides a human-grounded feature attribution map with a higher level of granularity compared to
object segmentation maps. Similarly, feature-level annotations have been used as the explanation
ground truth in the text classification domain . Other less accurate means of feature attribution
like bounding box in images datasets have been used for quantitative evaluation of saliency maps.
For instance, Du et al. evaluated saliency maps generated from a CNN model by calculating
pixel-wise IOU (intersection over union) of model-explanation bounding boxes and ground truth
bounding boxes.
Role of User Interactions in XAI
Another important consideration in designing XAI systems is how to leverage user interactions
to better support system understandability. The benefits of interactive system design have been
previously explored in the topic of interactive machine learning for novice end-users. AI and
data experts also benefit from interactive visual tools to improve model and task performance .
In this section, we discuss multiple examples of interaction design that support user understanding
of the underlying black-box model.
Focusing on interactive design for AI-based systems for AI novices, Amershi et al. reviewed
multiple case studies that demonstrate the effectiveness of interactivity with a tight coupling
between the algorithm and the user. They emphasize how interactive machine learning processes
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
A Multidisciplinary Survey and Framework for Explainable AI
allow the users to instantly examine the impact of their actions and adapt their next queries to
improve outcomes. Such interactions allow users to test various inputs and learn about the model
by creating What-If explanations . Particularly, user-led cyles of trial and error help novices
to understand how the machine learning model works and how to steer the model to improve
results. In the context of XAI, Jongejan and Holbrook present a study in which users draw
images to see whether an image recognition algorithm can correctly recognize the intended sketch.
Their system and study allows for interactive trial-and-error to explore how the algorithm works.
In addition, their system provides example-based explanations in cases where the algorithm fails
to correctly classify drawings. Another approach is to allow users to control or tune algorithmic
parameters to achieve better results. For example, Kocielnik et al. present a study in which
users were able to freely control detection sensitivity in an AI assistant. Their results showed a
significant effect on user perception of control and acceptance.
Visual analytics tools also support model understanding for expert users through interaction with
algorithms. Examples including allowing data scientists and model experts to interactively explore
model representations , analyze model training processes , and detect learning biases .
Also, embedded interaction techniques can support the exploration of very large deep learning
networks. For instance, Hohman et al. present multiple interactive features to select and filter
of neurons and zoom and pan in feature representations to support AI experts in interpreting and
reviewing trained models.
Generalization and Extension of the Framework
Our framework is extendable and compatible with existing AI-infused interface and interaction
design guidelines. For example, Amershi et al. propose 18 design guidelines for human-AI
interaction design. Their guidelines are based on a review of a large number of AI-related design
recommendation sources. They systematically validated guidelines through multiple rounds of
evaluations with 49 design practitioners in 20 AI-infused products. Their design guidelines provide
further details within the user interface design layer of our framework (Section 8.2) to guide
the development of appropriate user interactions with model output and interactions. In other
work, Dudley and Kristensson present a review and characterization of user interface design
principles for interactive machine learning systems. They propose a structural breakdown of
interactive machine learning systems and present six principles to support system design. This
work also benefits our framework by contributing practices of interactive machine learning design
to the XAI system goals layer (Section 8.1) and the user interface design layer (Section 8.2) From
the standpoint of evaluation methods, Mueller and Klein discuss how common usability tests
cannot address intelligent tools where software replicates human intelligence. They suggest new
solutions are needed to allow the users to experience an AI-based tool’s strengths and weaknesses.
Likewise, our nested framework points out the potential for error propagation from the inner layers
(e.g., interpretable algorithms design) to the outer layers (e.g., system outcomes) in the XAI system
evaluation pole. The iterative back-and-forth between layers in the nested model encourages expert
review of system outcomes, user-centered evaluation of the explainable interface, and computational
evaluation of machine learning algorithms.
Limitations of the Framework
Our framework provides a basis for XAI system design in interdisciplinary teamwork and we have
described our case study example to validate and improve the framework. The presented case
study serves as a practical example of using our framework in a multidisciplinary collaborative
XAI design and development effort. Our use case is the result of a year-long (and ongoing) research
done by a team of eight university researchers with diverse backgrounds. The lessons learned and
ACM Trans. Interact. Intell. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2020.
Mohseni et al.
pitfalls in our end-to-end implementation case study are incorporated in the presented design
guidelines. However, no framework is perfect or entirely comprehensive. We acknowledge that the
validity and usefulness of a framework are to be proven in practice with further case studies. In our
future work, we plan to run multiple validation case studies to examine practicality and usefulness
of this framework.
Moreover, this framework has a common limitation of many multidisciplinary design frameworks
of being light on specific details at each step. Rather than contributing detailed guidelines for
each framework layer, the framework is intended to pave the path for efficient collaboration
among and within different teams, which is essential for XAI system design given the inherently
interdisciplinary nature of this field. This higher level of freedom allows for extendability with other
design guidelines (see the discussion in Section 9.6) to integrate with more tailored approaches for
specific domains. Additionally, the diversity of design goals and evaluation methods at each layer
can help maintain the balance of attention from the design team to different aspects of XAI system.
CONCLUSION
We reviewed XAI-related research to organize multiple XAI design goals and evaluation measures.
Table 2 presents our categorization of selected existing design and evaluation methods that organizes
literature along three perspectives: design goals, evaluation methods, and the targeted users of the XAI
system. We provide summarized ready-to-use tables of evaluation methods and recommendations
for different goals in XAI research. Our categorization revealed the necessity of an interdisciplinary
effort for designing and evaluating XAI systems. We want to draw attention to related resources
in the social sciences that can facilitate the extent of social and cognitive aspects of explanations.
To address these issues, we proposed a design and evaluation framework that connects design
goals and evaluation methods for end-to-end XAI systems design, as presented through a model
and a series of guidelines. We hope our framework drives further discussion about the interplay
between design and evaluation of explainable artificial intelligent systems. Although the presented
framework is organized to provide a high-level guideline for a multidisciplinary effort to build XAI
systems, it is not meant to offer all aspects of interface and interaction design and development of
interpretable machine learning techniques. Lastly, we briefly discussed additional considerations
for XAI designers to benefit from the body of knowledge of XAI system design and evaluation.
ACKNOWLEDGMENTS
The authors would like to thank anonymous reviewers for their helpful comments on earlier
versions of this manuscript. The work in this paper is supported by the DARPA XAI program under
N66001-17-2-4031 and by NSF award 1900767. The views and conclusions in this paper are those of
the authors and should not be interpreted as representing any funding agencies.