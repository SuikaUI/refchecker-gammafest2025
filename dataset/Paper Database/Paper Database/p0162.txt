International Journal of Computer Vision
 
Deep Learning for Generic Object Detection: A Survey
· Wanli Ouyang3 · Xiaogang Wang4 · Paul Fieguth5 · Jie Chen2 · Xinwang Liu1 · Matti Pietikäinen2
Received: 6 September 2018 / Accepted: 26 September 2019
© The Author(s) 2019
Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances
from a large number of predeﬁned categories in natural images. Deep learning techniques have emerged as a powerful
strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the ﬁeld of
generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of
the recent achievements in this ﬁeld brought about by deep learning techniques. More than 300 research contributions are
included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation,
object proposal generation, context modeling, training strategies, and evaluation metrics. We ﬁnish the survey by identifying
promising directions for future research.
Keywords Object detection · Deep learning · Convolutional neural networks · Object recognition
1 Introduction
As a longstanding, fundamental and challenging problem
in computer vision, object detection (illustrated in Fig. 1)
has been an active area of research for several decades . The goal of object detection is
to determine whether there are any instances of objects from
given categories (such as humans, cars, bicycles, dogs or
cats) in an image and, if present, to return the spatial location and extent of each object instance . As
the cornerstone of image understanding and computer vision,
object detection forms the basis for solving complex or high
level vision tasks such as segmentation, scene understanding, object tracking, image captioning, event detection, and
activity recognition. Object detection supports a wide range
of applications, including robot vision, consumer electronics,
security, autonomous driving, human computer interaction,
content based image retrieval, intelligent video surveillance,
and augmented reality.
Recently, deep learning techniques have emerged as powerful
methods for learning feature representations automatically
from data. In particular, these techniques have provided
major improvements in object detection, as illustrated in
As illustrated in Fig. 2, object detection can be grouped
into one of two types : detection of speciﬁc instances versus the detection of
broad categories. The ﬁrst type aims to detect instances of
a particular object (such as Donald Trump’s face, the Eiffel
Tower, or a neighbor’s dog), essentially a matching problem.
International Journal of Computer Vision
Fig. 1 Most frequent keywords in ICCV and CVPR conference papers
from 2016 to 2018. The size of each word is proportional to the frequency of that keyword. We can see that object detection has received
signiﬁcant attention in recent years
Fig. 2 Object detection includes localizing instances of a particular
object (top), as well as generalizing to detecting object categories in
general (bottom). This survey focuses on recent advances for the latter
problem of generic object detection
The goal of the second type is to detect (usually previously unseen) instances of some predeﬁned object categories
(for example humans, cars, bicycles, and dogs). Historically,
much of the effort in the ﬁeld of object detection has focused
on the detection of a single category (typically faces and
pedestrians) or a few speciﬁc categories. In contrast, over
the past several years, the research community has started
moving towards the more challenging goal of building general purpose object detection systems where the breadth of
object detection ability rivals that of humans.
Krizhevsky et al. proposed a Deep Convolutional Neural Network (DCNN) called AlexNet which
achieved record breaking image classiﬁcation accuracy in the
Large Scale Visual Recognition Challenge (ILSVRC) . Since that time, the research focus in
most aspects of computer visionhas beenspeciﬁcallyondeep
learning methods, indeed including the domain of generic
object detection . Although
tremendous progress has been achieved, illustrated in Fig. 3,
we are unaware of comprehensive surveys of this subject
Fig. 3 An overview of recent object detection performance: we can
observe a signiﬁcant improvement in performance (measured as mean
average precision) since the arrival of deep learning in 2012. a Detection
results of winning entries in the VOC2007-2012 competitions, and b
top object detection competition results in ILSVRC2013-2017 (results
in both panels use only the provided training data)
over the past 5 years. Given the exceptionally rapid rate of
progress, this article attempts to track recent advances and
summarize their achievements in order to gain a clearer picture of the current panorama in generic object detection.
1.1 Comparison with Previous Reviews
Many notable object detection surveys have been published,
as summarized in Table 1. These include many excellent surveys on the problem of speciﬁc object detection, such as
pedestrian detection , face detection , vehicle detection and text detection . There are
comparatively few recent surveys focusing directly on the
problem of generic object detection, except for the work by
Zhang et al. who conducted a survey on the topic
of object class detection. However, the research reviewed
in Grauman and Leibe , Andreopoulos and Tsotsos
 and Zhang et al. is mostly pre-2012, and therefore prior to the recent striking success and dominance of
deep learning and related methods.
Deep learning allows computational models to learn
fantastically complex, subtle, and abstract representations,
driving signiﬁcant progress in a broad range of problems such
as visual recognition, object detection, speech recognition,
natural language processing, medical image analysis, drug
discovery and genomics. Among different types of deep neural networks, DCNNs have brought about breakthroughs in processing
images, video, speech and audio. To be sure, there have been
many published surveys on deep learning, including that of
Bengio et al. , LeCun et al. , Litjens et al. ,
Gu et al. , and more recently in tutorials at ICCV and
In contrast, although many deep learning based methods
have been proposed for object detection, we are unaware of
International Journal of Computer Vision
Table 1 Summary of related object detection surveys since 2000
Survey title
References
Monocular pedestrian detection:
survey and experiments
Enzweiler and Gavrila 
An evaluation of three pedestrian
Survey of pedestrian detection for
advanced driver assistance
Geronimo et al. 
A survey of pedestrian detection
for advanced driver assistance
Pedestrian detection: an evaluation
of the state of the art
Dollar et al. 
A thorough and detailed evaluation
of detectors in monocular images
Detecting faces in images: a survey
Yang et al. 
First survey of face detection from
a single image
A survey on face detection in the
wild: past, present and future
Zafeiriou et al. 
A survey of face detection in the
wild since 2000
On road vehicle detection: a review
Sun et al. 
A review of vision based on-road
vehicle detection systems
Text detection and recognition in
imagery: a survey
Ye and Doermann 
A survey of text detection and
recognition in color imagery
Toward category level object
recognition
Ponce et al. 
Representative papers on object
categorization, detection, and
segmentation
The evolution of object
categorization and the challenge
of image abstraction
Dickinson et al. 
A trace of the evolution of object
categorization over 4 decades
Context based object
categorization: a critical survey
Galleguillos and Belongie 
A review of contextual information
for object categorization
50 years of object recognition:
directions forward
Andreopoulos and Tsotsos 
A review of the evolution of object
recognition systems over
Visual object recognition
Grauman and Leibe 
Instance and category object
recognition techniques
Object class detection: a survey
Zhang et al. 
Survey of generic object detection
methods before 2011
Feature representation for
statistical learning based object
detection: a review
Li et al. 
Feature representation methods in
statistical learning based object
detection, including handcrafted
and deep learning based features
Salient object detection: a survey
Borji et al. 
A survey for salient object
Representation learning: a review
and new perspectives
Bengio et al. 
Unsupervised feature learning and
deep learning, probabilistic
models, autoencoders, manifold
learning, and deep networks
Deep learning
LeCun et al. 
An introduction to deep learning
and applications
A survey on deep learning in
medical image analysis
Litjens et al. 
A survey of deep learning for
image classiﬁcation, object
detection, segmentation and
registration in medical image
Recent advances in convolutional
neural networks
Gu et al. 
A broad survey of the recent
advances in CNN and its
applications in computer vision,
speech and natural language
processing
Tutorial: tools for efﬁcient object
A short course for object detection
only covering recent milestones
International Journal of Computer Vision
Table 1 continued
Survey title
References
Tutorial: deep learning for objects
and scenes
A high level summary of recent
work on deep learning for visual
recognition of objects and scenes
Tutorial: instance level recognition
A short course of recent advances
on instance level recognition,
including object detection,
instance segmentation and
human pose prediction
Tutorial: visual recognition and
A tutorial on methods and
principles behind image
classiﬁcation, object detection,
instance segmentation, and
semantic segmentation
Deep learning for generic object
A comprehensive survey of deep
learning for generic object
any comprehensive recent survey. A thorough review and
summary of existing work is essential for further progress in
object detection, particularly for researchers wishing to enter
the ﬁeld. Since our focus is on generic object detection, the
extensive work on DCNNs for speciﬁc object detection, such
as face detection , pedestrian detection , vehicle detection and trafﬁc sign
detection will not be considered.
The number of papers on generic object detection based on
deep learning is breathtaking. There are so many, in fact, that
compiling any comprehensive review of the state of the art is
beyond the scope of any reasonable length paper. As a result,
it is necessary to establish selection criteria, in such a way
that we have limited our focus to top journal and conference
papers. Due to these limitations, we sincerely apologize to
those authors whose works are not included in this paper. For
surveys of work on related topics, readers are referred to the
articles in Table 1. This survey focuses on major progress of
the last 5 years, and we restrict our attention to still pictures,
leaving the important subject of video object detection as a
topic for separate consideration in the future.
The main goal of this paper is to offer a comprehensive
survey of deep learning based generic object detection techniques, and to present some degree of taxonomy, a high
level perspective and organization, primarily on the basis
of popular datasets, evaluation metrics, context modeling,
and detection proposal methods. The intention is that our
categorization be helpful for readers to have an accessible understanding of similarities and differences between
a wide variety of strategies. The proposed taxonomy gives
researchers a framework to understand current research and
to identify open challenges for future research.
The remainder of this paper is organized as follows.
Related background and the progress made during the last
2 decades are summarized in Sect. 2. A brief introduction
to deep learning is given in Sect. 3. Popular datasets and
evaluation criteria are summarized in Sect. 4. We describe
the milestone object detection frameworks in Sect. 5. From
Sects. 6 to 9, fundamental sub-problems and the relevant
issues involved in designing object detectors are discussed.
Finally, in Sect. 10, we conclude the paper with an overall
discussion of object detection, state-of-the- art performance,
and future research directions.
2 Generic Object Detection
2.1 The Problem
Generic object detection, also called generic object category
detection, object class detection, or object category detection , is deﬁned as follows. Given an
image,determinewhetherornotthereareinstancesofobjects
from predeﬁned categories (usually many categories, e.g.,
200 categories in the ILSVRC object detection challenge)
and, if present, to return the spatial location and extent of
each instance. A greater emphasis is placed on detecting
a broad range of natural categories, as opposed to speciﬁc
object category detection where only a narrower predeﬁned
category of interest (e.g., faces, pedestrians, or cars) may
be present. Although thousands of objects occupy the visual
world in which we live, currently the research community is
primarily interested in the localization of highly structured
objects (e.g., cars, faces, bicycles and airplanes) and artic-
International Journal of Computer Vision
Fig. 4 Recognition problems related to generic object detection: a
image level object classiﬁcation, b bounding box level generic object
detection,cpixel-wisesemanticsegmentation,dinstancelevelsemantic
segmentation
ulated objects (e.g., humans, cows and horses) rather than
unstructured scenes (such as sky, grass and cloud).
The spatial location and extent of an object can be deﬁned
coarsely using a bounding box (an axis-aligned rectangle
tightly bounding the object) , a precise pixelwise segmentation mask
 , or a closed boundary , as illustrated in Fig. 4. To the best of
our knowledge, for the evaluation of generic object detection algorithms, it is bounding boxes which are most widely
used in the current literature , and therefore this is also the approach
we adopt in this survey. However, as the research community
movestowardsdeepersceneunderstanding(fromimagelevel
object classiﬁcation to single object localization, to generic
object detection, and to pixelwise object segmentation), it is
anticipated that future challenges will be at the pixel level
 .
There are many problems closely related to that of generic
object detection1. The goal of object classiﬁcation or object
categorization (Fig. 4a) is to assess the presence of objects
from a given set of object classes in an image; i.e., assigning
one or more object class labels to a given image, determining the presence without the need of location. The additional
requirement to locate the instances in an image makes detection a more challenging task than classiﬁcation. The object
recognition problem denotes the more general problem of
identifying/localizing all the objects present in an image,
subsuming the problems of object detection and classiﬁcation .
Fig. 5 Taxonomy of challenges in generic object detection
et al. 2006; Andreopoulos and Tsotsos 2013). Generic object
detection is closely related to semantic image segmentation
(Fig. 4c), which aims to assign each pixel in an image to a
semantic class label. Object instance segmentation (Fig. 4d)
aims to distinguish different instances of the same object
class, as opposed to semantic segmentation which does not.
2.2 Main Challenges
The ideal of generic object detection is to develop a generalpurpose algorithm that achieves two competing goals of high
quality/accuracy and high efﬁciency (Fig. 5). As illustrated
in Fig. 6, high quality detection must accurately localize and recognize objects in images or video frames, such
that the large variety of object categories in the real world
can be distinguished (i.e., high distinctiveness), and that
object instances from the same category, subject to intraclass appearance variations, can be localized and recognized
(i.e., high robustness). High efﬁciency requires that the entire
detection task runs in real time with acceptable memory and
storage demands.
2.2.1 Accuracy Related Challenges
Challenges in detection accuracy stem from (1) the vast range
of intra-class variations and (2) the huge number of object
categories.
Intra-class variations can be divided into two types: intrinsic factors and imaging conditions. In terms of intrinsic
factors, each object category can have many different object
instances, possibly varying in one or more of color, texture, material, shape, and size, such as the “chair” category
shown in Fig. 6i. Even in a more narrowly deﬁned class, such
as human or horse, object instances can appear in different
poses, subject to nonrigid deformations or with the addition
of clothing.
International Journal of Computer Vision
Fig. 6 Changes in appearance of the same class with variations in imaging conditions (a–h). There is an astonishing variation in what is meant
to be a single object class (i). In contrast, the four images in j appear
very similar, but in fact are from four different object classes. Most
images are from ImageNet and MS COCO
 
Imaging condition variations are caused by the dramatic impacts unconstrained environments can have on
object appearance, such as lighting (dawn, day, dusk,
indoors), physical location, weather conditions, cameras,
backgrounds, illuminations, occlusion, and viewing distances. All of these conditions produce signiﬁcant variations
in object appearance, such as illumination, pose, scale,
occlusion, clutter, shading, blur and motion, with examples
illustrated in Fig. 6a–h. Further challenges may be added by
digitization artifacts, noise corruption, poor resolution, and
ﬁltering distortions.
In addition to intraclass variations, the large number of
objectcategories,ontheorderof104–105,demandsgreatdiscrimination power from the detector to distinguish between
subtly different interclass variations, as illustrated in Fig. 6j.
In practice, current detectors focus mainly on structured
object categories, such as the 20, 200 and 91 object classes
in PASCAL VOC , ILSVRC and MS COCO 
respectively. Clearly, the number of object categories under
consideration in existing benchmark datasets is much smaller
than can be recognized by humans.
2.2.2 Efﬁciency and Scalability Related Challenges
Theprevalenceofsocialmedianetworksandmobile/wearable
devices has led to increasing demands for analyzing visual
data. However, mobile/wearable devices have limited computational capabilities and storage space, making efﬁcient
object detection critical.
The efﬁciency challenges stem from the need to localize
and recognize, computational complexity growing with the
(possibly large) number of object categories, and with the
(possibly very large) number of locations and scales within
a single image, such as the examples in Fig. 6c, d.
A further challenge is that of scalability: A detector should
be able to handle previously unseen objects, unknown situ-
Fig.7 Milestones of object detection and recognition, including feature
representations ,
detection frameworks , and
datasets . The time period up to 2012 is dominated by handcrafted features, a transition took place in 2012 with the development of DCNNs
for image classiﬁcation by Krizhevsky et al. , with methods after
2012 dominated by related deep networks. Most of the listed methods
are highly cited and won a major ICCV or CVPR prize. See Sect. 2.3
for details
International Journal of Computer Vision
ations, and high data rates. As the number of images and
the number of categories continue to grow, it may become
impossible to annotate them manually, forcing a reliance on
weakly supervised strategies.
2.3 Progress in the Past 2 Decades
Early research on object recognition was based on template
matching techniques and simple part-based models , focusing on speciﬁc objects whose
spatial layouts are roughly rigid, such as faces. Before 1990
the leading paradigm of object recognition was based on geometric representations , with
the focus later moving away from geometry and prior models towards the use of statistical classiﬁers [such as Neural
Networks , SVM and
Adaboost ] based on
appearance features . This successful family of object detectors set
the stage for most subsequent research in this ﬁeld.
The milestones of object detection in more recent years are
presented in Fig. 7, in which two main eras (SIFT vs. DCNN)
are highlighted. The appearance features moved from global
representations to local representations
that are designed to be invariant to changes in translation,
scale, rotation, illumination, viewpoint and occlusion. Handcraftedlocal invariant features gainedtremendous popularity,
starting from the Scale Invariant Feature Transform (SIFT)
feature , and the progress on various visual
recognition tasks was based substantially on the use of local
descriptors such as Haarlike features , SIFT ,
Shape Contexts , Histogram of Gradients (HOG) Local Binary Patterns
(LBP) , and region covariances . These local features are usually aggregated by simple
concatenation or feature pooling encoders such as the Bag of
Visual Words approach, introduced by Sivic and Zisserman
 and Csurka et al. , Spatial Pyramid Matching
(SPM) of BoW models , and Fisher
Vectors .
For years, the multistage hand tuned pipelines of handcrafted local descriptors and discriminative classiﬁers dominated a variety of domains in computer vision, including
object detection, until the signiﬁcant turning point in 2012
when DCNNs achieved their
record-breaking results in image classiﬁcation.
The use of CNNs for detection and localization can be traced back to the 1990s, with a
modest number of hidden layers used for object detection
 , successful in restricted domains such as face detection. However, more recently, deeper CNNs have led to
record-breaking improvements in the detection of more general object categories, a shift which came about when the
successful application of DCNNs in image classiﬁcation
 was transferred to object detection, resulting in the milestone Region-based CNN (RCNN)
detector of Girshick et al. .
The successes of deep detectors rely heavily on vast training data and large networks with millions or even billions of
parameters. The availability of GPUs with very high computational capability and large-scale detection datasets [such as
ImageNet and
MS COCO ] play a key role in their success. Large datasets have allowed researchers to target more
realistic and complex problems from images with large intraclass variations and inter-class similarities . However, accurate annotations are
labor intensive to obtain, so detectors must consider methods that can relieve annotation difﬁculties or can learn with
smaller training datasets.
The research community has started moving towards the
challenging goal of building general purpose object detection systems whose ability to detect many object categories
matches that of humans. This is a major challenge: according to cognitive scientists, human beings can identify around
3000 entry level categories and 30,000 visual categories overall, and the number of categories distinguishable with domain
expertise may be to the order of 105 .
Despite the remarkable progress of the past years, designing
an accurate, robust, efﬁcient detection and recognition system that approaches human-level performance on 104–105
categories is undoubtedly an unresolved problem.
3 A Brief Introduction to Deep Learning
Deep learning has revolutionized a wide range of machine
learning tasks, from image classiﬁcation and video processing to speech recognition and natural language understanding. Given this tremendously rapid evolution, there exist
many recent survey papers on deep learning . These surveys have reviewed deep
learning techniques from different perspectives , or with applications to medical image analysis , natural language processing , speech recognition systems , and
remote sensing .
International Journal of Computer Vision
Fig. 8 a Illustration of three operations that are repeatedly applied by a
typical CNN: convolution with a number of linear ﬁlters; Nonlinearities
(e.g. ReLU); and local pooling (e.g. max pooling). The M feature maps
from a previous layer are convolved with N different ﬁlters (here shown
as size 3 × 3 × M), using a stride of 1. The resulting N feature maps
are then passed through a nonlinear function (e.g. ReLU), and pooled
(e.g. taking a maximum over 2 × 2 regions) to give N feature maps
at a reduced resolution. b Illustration of the architecture of VGGNet
 , a typical CNN with 11 weight layers.
An image with 3 color channels is presented as the input. The network
has 8 convolutional layers, 3 fully connected layers, 5 max pooling layers and a softmax classiﬁcation layer. The last three fully connected
layers take features from the top convolutional layer as input in vector
form. The ﬁnal layer is a C-way softmax function, C being the number
of classes. The whole network can be learned from labeled training data
by optimizing an objective function (e.g. mean squared error or cross
entropy loss) via stochastic gradient descent (Color ﬁgure online)
Convolutional Neural Networks (CNNs), the most representative models of deep learning, are able to exploit the basic
properties underlying natural signals: translation invariance,
local connectivity, and compositional hierarchies . A typical CNN, illustrated in Fig. 8, has a hierarchical structure and is composed of a number of layers to
learn representations of data with multiple levels of abstraction . We begin with a convolution
between an input feature map xl−1 at a feature map from
previouslayerl−1,convolvedwitha2Dconvolutionalkernel
(or ﬁlter or weights) wl. This convolution appears over a
sequence of layers, subject to a nonlinear operation σ, such
with a convolution now between the Nl−1 input feature maps
and the corresponding kernel wl
i, j, plus a bias term bl
The elementwise nonlinear function σ(·) is typically a recti-
ﬁed linear unit (ReLU) for each element,
σ(x) = max{x, 0}.
Finally, pooling corresponds to the downsampling/upsampling of feature maps. These three operations (convolution,
nonlinearity, pooling) are illustrated in Fig. 8a; CNNs having
a large number of layers, a “deep” network, are referred to
as Deep CNNs (DCNNs), with a typical DCNN architecture
illustrated in Fig. 8b.
Most layers of a CNN consist of a number of feature maps,
within which each pixel acts like a neuron. Each neuron in a
convolutional layer is connected to feature maps of the previous layer through a set of weights wi, j (essentially a set of
2D ﬁlters). As can be seen in Fig. 8b, where the early CNN
layers are typically composed of convolutional and pooling
layers, the later layers are normally fully connected. From
earlier to later layers, the input image is repeatedly convolved, and with each layer, the receptive ﬁeld or region of
support increases. In general, the initial CNN layers extract
low-level features (e.g., edges), with later layers extracting
more general features of increasing complexity .
DCNNs have a number of outstanding advantages: a
hierarchical structure to learn representations of data with
multiple levels of abstraction, the capacity to learn very complex functions, and learning feature representations directly
and automatically from data with minimal domain knowledge. What has particularly made DCNNs successful has
International Journal of Computer Vision
been the availability of large scale labeled datasets and of
GPUs with very high computational capability.
Despite the great successes, known deﬁciencies remain. In
particular, there is an extreme need for labeled training data
and a requirement of expensive computing resources, and
considerable skill and experience are still needed to select
appropriate learning parameters and network architectures.
Trained networks are poorly interpretable, there is a lack of
robustness to degradations, and many DCNNs have shown
serious vulnerability to attacks , all
of which currently limit the use of DCNNs in real-world
applications.
4 Datasets and Performance Evaluation
4.1 Datasets
Datasets have played a key role throughout the history of
object recognition research, not only as a common ground
for measuring and comparing the performance of competing
algorithms, but also pushing the ﬁeld towards increasingly
complex and challenging problems. In particular, recently,
deeplearningtechniques havebrought tremendous success to
many visual recognition problems, and it is the large amounts
of annotated data which play a key role in their success.
Access to large numbers of images on the Internet makes it
possible to build comprehensive datasets in order to capture
a vast richness and diversity of objects, enabling unprecedented performance in object recognition.
For generic object detection, there are four famous
datasets: PASCAL VOC ,
ImageNet , MS COCO 
and Open Images . The attributes
of these datasets are summarized in Table 2, and selected
sample images are shown in Fig. 9. There are three steps to
creatinglarge-scaleannotateddatasets:determiningthesetof
target object categories, collecting a diverse set of candidate
images to represent the selected categories on the Internet,
and annotating the collected images, typically by designing
crowdsourcing strategies. Recognizing space limitations, we
refer interested readers to the original papers for detailed descriptions of these
datasets in terms of construction and properties.
The four datasets form the backbone of their respective
detection challenges. Each challenge consists of a publicly
available dataset of images together with ground truth annotation and standardized evaluation software, and an annual
competition and corresponding workshop. Statistics for the
number of images and object instances in the training, validation and testing datasets2 for the detection challenges are
given in Table 3. The most frequent object classes in VOC,
COCO, ILSVRC and Open Images detection datasets are
visualized in Table 4.
PASCAL VOC Everingham et al. is a multiyear effort devoted to the creation and maintenance of a series
of benchmark datasets for classiﬁcation and object detection,
creating the precedent for standardized evaluation of recognition algorithms in the form of annual competitions. Starting
from only four categories in 2005, the dataset has increased to
20 categories that are common in everyday life. Since 2009,
the number of images has grown every year, but with all previous images retained to allow test results to be compared
from year to year. Due the availability of larger datasets like
ImageNet, MS COCO and Open Images, PASCAL VOC has
gradually fallen out of fashion.
ILSVRC, the ImageNet Large Scale Visual Recognition
Challenge , is derived from ImageNet , scaling up PASCAL VOC’s goal of
standardized training and evaluation of detection algorithms
by more than an order of magnitude in the number of object
classes and images. ImageNet1000, a subset of ImageNet
images with 1000 different object categories and a total of
1.2 million images, has been ﬁxed to provide a standardized
benchmark for the ILSVRC image classiﬁcation challenge.
MS COCO is a response to the criticism of ImageNet that
objects in its dataset tend to be large and well centered, making the ImageNet dataset atypical of real-world scenarios.
To push for richer image understanding, researchers created
the MS COCO database containing complex everyday scenes with common objects in their natural
context, closer to real life, where objects are labeled using
fully-segmented instances to provide more accurate detector evaluation. The COCO object detection challenge features two object detection tasks: using either
bounding box output or object instance segmentation output.
COCO introduced three new challenges:
1. It contains objects at a wide range of scales, including a
high percentage of small objects ;
2. Objects are less iconic and amid clutter or heavy occlusion;
3. The evaluation metric (see Table 5) encourages more
accurate object localization.
Just like ImageNet in its time, MS COCO has become the
standard for object detection today.
OICOD (the Open Image Challenge Object Detection) is
derived from Open Images V4 , currently the largest publicly available object
2 The annotations on the test set are not publicly released, except for
PASCAL VOC2007.
International Journal of Computer Vision
Table 2 Popular databases for object recognition
Total images
Categories Images per category Objects per image Image size
Started year Highlights
VOC 
 
14 millions+
Large number of object categories;
More instances and more
categories of objects per image;
More challenging than PASCAL
VOC; Backbone of the ILSVRC
challenge; Images are
object-centric
 
9 millions+
Annotated with image level labels,
object bounding boxes and visual
relationships; Open Images V5
supports large scale object
detection, object instance
segmentation and visual
relationship detection
Example images from PASCAL VOC, ImageNet, MS COCO and Open Images are shown in Fig. 9
Fig. 9 Some example images with object annotations from PASCAL VOC, ILSVRC, MS COCO and Open Images. See Table 2 for a summary of
these datasets
detection dataset. OICOD is different from previous large
scale object detection datasets like ILSVRC and MS COCO,
not merely in terms of the signiﬁcantly increased number
of classes, images, bounding box annotations and instance
segmentation mask annotations, but also regarding the annotation process. In ILSVRC and MS COCO, instances of all
International Journal of Computer Vision
Table 3 Statistics of commonly used object detection datasets
Object classes
Number of images
Number of annotated objects
Summary (Train+Val)
Boxes/Image
PASCAL VOC object detection challenge
6301(7844)
6307(7818)
5082(6337)
5281(6347)
8505(9760)
8713(9779)
11,577(13,339)
11,797(13,352)
13,609(15,774)
13,841(15,787)
13,609(15,774)
13,841(15,787)
ILSVRC object detection challenge
MS COCO object detection challenge
Open images challenge object detection (OICOD) 
11,498,734
12,195,144
Object statistics for VOC challenges list the non-difﬁcult objects used in the evaluation (all annotated objects). For the COCO challenge, prior to
2017, the test set had four splits (Dev, Standard, Reserve, and Challenge), with each having about 20K images. Starting in 2017, the test set has
only the Dev and Challenge splits, with the other two splits removed. Starting in 2017, the train and val sets are arranged differently, and the test set
is divided into two roughly equally sized splits of about 20,000 images each: Test Dev and Test Challenge. Note that the 2017 Test Dev/Challenge
splits contain the same images as the 2015 Test Dev/Challenge splits, so results across the years are directly comparable
classes in the dataset are exhaustively annotated, whereas
for Open Images V4 a classiﬁer was applied to each image
and only those labels with sufﬁciently high scores were sent
for human veriﬁcation. Therefore in OICOD only the object
instances of human-conﬁrmed positive labels are annotated.
4.2 Evaluation Criteria
There are three criteria for evaluating the performance of
detection algorithms: detection speed in Frames Per Second
(FPS), precision, and recall. The most commonly used metric is Average Precision (AP), derived from precision and
recall. AP is usually evaluated in a category speciﬁc manner,
i.e., computed for each object category separately. To compare performance over all object categories, the mean AP
(mAP) averaged over all object categories is adopted as the
ﬁnal measure of performance3. More details on these metrics
3 In object detection challenges, such as PASCAL VOC and ILSVRC,
the winning entry of each object category is that with the highest AP
score, and the winner of the challenge is the team that wins on the most
object categories. The mAP is also used as the measure of a team’s
can be found in Everingham et al. , Everingham et al.
 , Russakovsky et al. , Hoiem et al. .
The standard outputs of a detector applied to a testing
image I are the predicted detections {(b j, c j, p j)} j, indexed
by object j, of Bounding Box (BB) b j, predicted category c j,
andconﬁdence p j.Apredicteddetection(b, c, p)isregarded
as a True Positive (TP) if
• The predicted category c equals the ground truth label
• The overlap ratio IOU (Intersection Over Union) 
IOU(b, bg) = area (b ∩bg)
area (b ∪bg),
between the predicted BB b and the ground truth bg is
not smaller than a predeﬁned threshold ε, where ∩and
Footnote 3 continued
performance, and is justiﬁed since the ranking of teams by mAP was
always the same as the ranking by the number of object categories won
 .
International Journal of Computer Vision
Table 4 Most frequent object classes for each detection challenge
The size of each word is proportional to the frequency of that class in
the training dataset
cup denote intersection and union, respectively. A typical
value of ε is 0.5.
Otherwise, it is considered as a False Positive (FP). The con-
ﬁdence level p is usually compared with some threshold β
to determine whether the predicted class label c is accepted.
AP is computed separately for each of the object classes,
based on Precision and Recall. For a given object class c and
a testing image Ii, let {(bi j, pi j)}M
j=1 denote the detections
returnedbyadetector,rankedbyconﬁdence pi j indecreasing
order. Each detection (bi j, pi j) is either a TP or an FP, which
can be determined via the algorithm4 in Fig. 10. Based on
the TP and FP detections, the precision P(β) and recall R(β)
 can be computed as a function of
the conﬁdence threshold β, so by varying the conﬁdence
4 It is worth noting that for a given threshold β, multiple detections of
the same object in an image are not considered as all correct detections,
and only the detection with the highest conﬁdence level is considered
as a TP and the rest as FPs.
threshold different pairs (P, R) can be obtained, in principle
allowing precision to be regarded as a function of recall, i.e.
P(R), from which the Average Precision (AP) can be found.
Since the introduction of MS COCO, more attention has
been placed on the accuracy of the bounding box location.
Instead of using a ﬁxed IOU threshold, MS COCO introduces
a few metrics (summarized in Table 5) for characterizing the
performance of an object detector. For instance, in contrast to
the traditional mAP computed at a single IoU of 0.5, APcoco
is averaged across all object categories and multiple IOU values from 0.5 to 0.95 in steps of 0.05. Because 41% of the
objects in MS COCO are small and 24% are large, metrics
coco , APmedium
and APlarge
coco are also introduced. Finally,
Table 5 summarizes the main metrics used in the PASCAL,
ILSVRC and MS COCO object detection challenges, with
metric modiﬁcations for the Open Images challenges proposed in Kuznetsova et al. .
5 Detection Frameworks
There has been steady progress in object feature representations and classiﬁers for recognition, as evidenced by the
dramatic change from handcrafted features to learned DCNN
features . In contrast, in terms
of localization, the basic “sliding window” strategy remains
mainstream, although with some efforts to avoid exhaustive
search . However,
the number of windows is large and grows quadratically
with the number of image pixels, and the need to search
over multiple scales and aspect ratios further increases the
search space. Therefore, the design of efﬁcient and effective detection frameworks plays a key role in reducing this
computational cost. Commonly adopted strategies include
cascading, sharing feature computation, and reducing perwindow computation.
This section reviews detection frameworks, listed in
Fig. 11 and Table 11, the milestone approaches appearing
since deep learning entered the ﬁeld, organized into two main
categories:
(a) Two stage detection frameworks, which include a preprocessing step for generating object proposals;
(b) One stage detection frameworks, or region proposal free
frameworks, having a single proposed method which
does not separate the process of the detection proposal.
International Journal of Computer Vision
Table 5 Summary of commonly used metrics for evaluating object detectors
Deﬁnition and description
True positive
A true positive detection, per Fig. 10
False positive
A false positive detection, per Fig. 10
Conﬁdence threshold
A conﬁdence threshold for computing P(β) and R(β)
IOU threshold
Typically around 0.5
(w+10)(h+10)); w × h is the size of a GT box
Ten IOU thresholds ε ∈{0.5 : 0.05 : 0.95}
The fraction of correct detections out of the total detections returned by the detector with conﬁdence of at
The fraction of all Nc objects detected by the detector having a conﬁdence of at least β
Average Precision
Computed over the different levels of recall achieved by varying the conﬁdence β
mean Average Precision
AP at a single IOU and averaged over all classes
AP at a modiﬁed IOU and averaged over all classes
APcoco: mAP averaged over ten IOUs: {0.5 : 0.05 : 0.95};
: mAP at IOU = 0.50 (PASCAL VOC metric);
APIOU=0.75
: mAP at IOU = 0.75 (strict metric);
coco : mAP for small objects of area smaller than 322;
: mAP for objects of area between 322 and 962;
coco : mAP for large objects of area bigger than 962;
Average Recall
The maximum recall given a ﬁxed number of detections per image, averaged over all categories and IOU
thresholds
Average Recall
: AR given 1 detection per image;
: AR given 10 detection per image;
: AR given 100 detection per image;
coco : AR for small objects of area smaller than 322;
: AR for objects of area between 322 and 962;
coco : AR for large objects of area bigger than 962;
Fig. 10 The algorithm for determining TPs and FPs by greedily matching object detection results to ground truth boxes
Sections6–9willdiscussfundamentalsub-problemsinvolved
in detection frameworks in greater detail, including DCNN
features, detection proposals, and context modeling.
5.1 Region Based (Two Stage) Frameworks
In a region-based framework, category-independent region
proposals5 are generated from an image, CNN features are extracted from these regions, and
then category-speciﬁc classiﬁers are used to determine the
category labels of the proposals. As can be observed from
Fig. 11, DetectorNet , OverFeat , MultiBox and RCNN
 independently and almost simultaneously proposed using CNNs for generic object detection.
RCNN : Inspired by the breakthrough image classiﬁcation results obtained by CNNs and
thesuccessoftheselectivesearchinregionproposalforhandcrafted features , Girshick et al. were among the ﬁrst to explore CNNs for generic
object detection and developed RCNN, which integrates
5 Object proposals, also called region proposals or detection proposals,
are a set of candidate regions or bounding boxes in an image that may
potentially contain an object .
International Journal of Computer Vision
(Girshick et al.)
DetectorNet
(Szegedy et al.)
Faster RCNN
(Ren et al.)
(Dai et al.)
(Redmon et al.)
(He et al.)
(Erhan et al.)
(Sermanet et al.)
MSC Multibox
(Szegedy et al.)
(Girshick)
(Liu et al.)
(Lin et al.)
(Simonyan and Zisserman)
(Szegedy et al.)
(He et al.)
(He et al.)
(Redmon and Farhadi)
(Lin et al.)
(Huang et al.)
Feature Pyramid Network
(FPN) (Lin et al.)
(Law and Deng)
Fig. 11 Milestones in generic object detection
Fig. 12 Illustration of the RCNN detection framework 
AlexNet with a region proposal
selective search . As illustrated in detail
in Fig. 12, training an RCNN framework consists of multistage pipelines:
1. Region proposal computation Class agnostic region proposals, which are candidate regions that might contain
objects, are obtained via a selective search :
1. Training is a multistage pipeline, slow and hard to optimize because each individual stage must be trained
separately.
2. For SVM classiﬁer and bounding box regressor training,
it is expensive in both disk space and time, because CNN
features need to be extracted from each object proposal
in each image, posing great challenges for large scale
detection, particularly with very deep networks, such as
VGG16 .
3. Testing is slow, since CNN features are extracted per
object proposal in each test image, without shared computation.
All of these drawbacks have motivated successive innovations, leading to a number of improved detection frameworks
such as SPPNet, Fast RCNN, Faster RCNN etc., as follows.
6 Please refer to Sect. 4.2 for the deﬁnition of IOU.
International Journal of Computer Vision
SPPNet During testing, CNN feature
extraction is the main bottleneck of the RCNN detection
pipeline, which requires the extraction of CNN features
from thousands of warped region proposals per image. As a
result, He et al. introduced traditional spatial pyramid
pooling (SPP) into CNN architectures. Since convolutional layers
accept inputs of arbitrary sizes, the requirement of ﬁxedsized images in CNNs is due only to the Fully Connected
(FC) layers, therefore He et al. added an SPP layer on top
of the last convolutional (CONV) layer to obtain features
of ﬁxed length for the FC layers. With this SPPNet, RCNN
obtains a signiﬁcant speedup without sacriﬁcing any detection quality, because it only needs to run the convolutional
layers once on the entire test image to generate ﬁxed-length
features for region proposals of arbitrary size. While SPPNet
accelerates RCNN evaluation by orders of magnitude, it does
not result in a comparable speedup of the detector training.
Moreover, ﬁne-tuning in SPPNet is unable to
update the convolutional layers before the SPP layer, which
limits the accuracy of very deep networks.
Fast RCNN Girshick proposed Fast
RCNN that addresses some of the disadvantages of RCNN and SPPNet, while improving on
their detection speed and quality. As illustrated in Fig. 13,
Fast RCNN enables end-to-end detector training by developing a streamlined training process that simultaneously
learns a softmax classiﬁer and class-speciﬁc bounding box
regression, rather than separately training a softmax classiﬁer, SVMs, and Bounding Box Regressors (BBRs) as in
RCNN/SPPNet. Fast RCNN employs the idea of sharing
the computation of convolution across region proposals,
and adds a Region of Interest (RoI) pooling layer between
the last CONV layer and the ﬁrst FC layer to extract a
ﬁxed-length feature for each region proposal. Essentially,
RoI pooling uses warping at the feature level to approximate warping at the image level. The features after the
RoI pooling layer are fed into a sequence of FC layers that
ﬁnally branch into two sibling output layers: softmax probabilities for object category prediction, and class-speciﬁc
bounding box regression offsets for proposal reﬁnement.
Compared to RCNN/SPPNet, Fast RCNN improves the efﬁciency considerably—typically 3 times faster in training and
10 times faster in testing. Thus there is higher detection quality, a single training process that updates all network layers,
and no storage required for feature caching.
Faster RCNN Although Fast
RCNN signiﬁcantly sped up the detection process, it still
relies on external region proposals, whose computation is
exposed as the new speed bottleneck in Fast RCNN. Recent
work has shown that CNNs have a remarkable ability to localize objects in CONV layers , an
Fig. 13 High level diagrams of the leading frameworks for generic
object detection. The properties of these methods are summarized in
International Journal of Computer Vision
ability which is weakened in the FC layers. Therefore, the
selective search can be replaced by a CNN in producing
region proposals. The Faster RCNN framework proposed
by Ren et al. offered an efﬁcient and accurate Region Proposal Network (RPN) for generating region
proposals. They utilize the same backbone network, using
features from the last shared convolutional layer to accomplish the task of RPN for region proposal and Fast RCNN for
region classiﬁcation, as shown in Fig. 13.
RPN ﬁrst initializes k reference boxes (i.e. the so called
anchors) of different scales and aspect ratios at each CONV
feature map location. The anchor positions are image content
independent, but the feature vectors themselves, extracted
from anchors, are image content dependent. Each anchor is
mapped to a lower dimensional vector, which is fed into two
sibling FC layers—an object category classiﬁcation layer and
a box regression layer. In contrast to detection in Fast RCNN,
the features used for regression in RPN are of the same shape
as the anchor box, thus k anchors lead to k regressors. RPN
shares CONV features with Fast RCNN, thus enabling highly
efﬁcient region proposal computation. RPN is, in fact, a kind
of Fully Convolutional Network (FCN) ; Faster RCNN is thus a purely CNN
based framework without using handcrafted features.
For the VGG16 model ,
Faster RCNN can test at 5 FPS (including all stages) on a
GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 using 300 proposals per image.
The initial Faster RCNN in Ren et al. contains several alternating training stages, later simpliﬁed in Ren et al.
Concurrent with the development of Faster RCNN, Lenc
and Vedaldi challenged the role of region proposal
generation methods such as selective search, studied the role
of region proposal generation in CNN based detectors, and
found that CNNs contain sufﬁcient geometric information
for accurate object detection in the CONV rather than FC
layers. They showed the possibility of building integrated,
simpler, and faster object detectors that rely exclusively on
CNNs, removing region proposal generation methods such
as selective search.
RFCN (Region based Fully Convolutional Network)
While Faster RCNN is an order of magnitude faster than
Fast RCNN, the fact that the region-wise sub-network still
needs to be applied per RoI (several hundred RoIs per image)
led Dai et al. to propose the RFCN detector which is
fully convolutional (no hidden FC layers) with almost all
computations shared over the entire image. As shown in
Fig. 13, RFCN differs from Faster RCNN only in the RoI
sub-network. In Faster RCNN, the computation after the RoI
pooling layer cannot be shared, so Dai et al. proposed
usingallCONVlayerstoconstructasharedRoIsub-network,
and RoI crops are taken from the last layer of CONV features
prior to prediction. However, Dai et al. found that this
naive design turns out to have considerably inferior detection
accuracy, conjectured to be that deeper CONV layers are
more sensitive to category semantics, and less sensitive to
translation, whereas object detection needs localization representations that respect translation invariance. Based on this
observation, Dai et al. constructed a set of positionsensitive score maps by using a bank of specialized CONV
layers as the FCN output, on top of which a position-sensitive
RoI pooling layer is added. They showed that RFCN with
ResNet101 could achieve comparable accuracy to Faster RCNN, often at faster running times.
Mask RCNN He et al. proposed Mask RCNN to
tackle pixelwise object instance segmentation by extending Faster RCNN. Mask RCNN adopts the same two stage
pipeline, with an identical ﬁrst stage (RPN), but in the second stage, in parallel to predicting the class and box offset,
Mask RCNN adds a branch which outputs a binary mask for
each RoI. The new branch is a Fully Convolutional Network
(FCN) on top of a
CNN feature map. In order to avoid the misalignments caused
by the original RoI pooling (RoIPool) layer, a RoIAlign
layer was proposed to preserve the pixel level spatial correspondence. With a backbone network ResNeXt101-FPN
 , Mask RCNN achieved
top results for the COCO object instance segmentation and
bounding box object detection. It is simple to train, generalizes well, and adds only a small overhead to Faster RCNN,
running at 5 FPS .
Chained Cascade Network and Cascade RCNN The
essence of cascade is to learn more discriminative classiﬁers by using multistage classiﬁers, such
that early stages discard a large number of easy negative
samples so that later stages can focus on handling more difﬁcult examples. Two-stage object detection can be considered
as a cascade, the ﬁrst detector removing large amounts of
background, and the second stage classifying the remaining
regions. Recently, end-to-end learning of more than two cascaded classiﬁers and DCNNs for generic object detection
were proposed in the Chained Cascade Network , extended in Cascade RCNN , and more recently applied for simultaneous object
detectionandinstancesegmentation , winning the COCO 2018 Detection Challenge.
Light Head RCNN In order to further increase the detection speed of RFCN , Li et al. proposed Light Head RCNN, making the head of the detection
network as light as possible to reduce the RoI computation.
In particular, Li et al. applied a convolution to produce thin feature maps with small channel numbers (e.g.,
490 channels for COCO) and a cheap RCNN sub-network,
leading to an excellent trade-off of speed and accuracy.
International Journal of Computer Vision
5.2 Unified (One Stage) Frameworks
The region-based pipeline strategies of Sect. 5.1 have dominated since RCNN , such that the
leading results on popular benchmark datasets are all based
on Faster RCNN . Nevertheless, regionbased approaches are computationally expensive for current
mobile/wearable devices, which have limited storage and
computational capability, therefore instead of trying to optimize the individual components of a complex region-based
pipeline, researchers have begun to develop uniﬁed detection
strategies.
Uniﬁed pipelines refer to architectures that directly predict class probabilities and bounding box offsets from full
images with a single feed-forward CNN in a monolithic setting that does not involve region proposal generation or post
classiﬁcation / feature resampling, encapsulating all computation in a single network. Since the whole pipeline is a single
network, it can be optimized end-to-end directly on detection
performance.
DetectorNet were among the ﬁrst to
explore CNNs for object detection. DetectorNet formulated
object detection a regression problem to object bounding
box masks. They use AlexNet 
and replace the ﬁnal softmax classiﬁer layer with a regression layer. Given an image window, they use one network
to predict foreground pixels over a coarse grid, as well as
four additional networks to predict the object’s top, bottom,
left and right halves. A grouping process then converts the
predicted masks into detected bounding boxes. The network
needs to be trained per object type and mask type, and does
not scale to multiple classes. DetectorNet must take many
crops of the image, and run multiple networks for each part
on every crop, thus making it slow.
OverFeat, proposed by Sermanet et al. and illustrated in Fig. 14, can be considered as one of the ﬁrst
single-stage object detectors based on fully convolutional
Fig. 14 Illustration of the OverFeat detection
deep networks. It is one of the most inﬂuential object detection frameworks, winning the ILSVRC2013 localization and
detection competition. OverFeat performs object detection
via a single forward pass through the fully convolutional
layers in the network (i.e. the “Feature Extractor”, shown
in Fig. 14a). The key steps of object detection at test time
can be summarized as follows:
1. Generate object candidates by performing object classiﬁcation via a sliding window fashion on multiscale
images OverFeat uses a CNN like AlexNet , which would require input images ofa ﬁxed
size due to its fully connected layers, in order to make
the sliding window approach computationally efﬁcient,
OverFeat casts the network (as shown in Fig. 14a) into
a fully convolutional network, taking inputs of any size,
by viewing fully connected layers as convolutions with
kernels of size 1 × 1. OverFeat leverages multiscale features to improve the overall performance by passing up to
six enlarged scales of the original image through the network (as shown in Fig. 14b), resulting in a signiﬁcantly
increased number of evaluated context views. For each
of the multiscale inputs, the classiﬁer outputs a grid of
predictions (class and conﬁdence).
2. Increase the number of predictions by offset max pooling
In order to increase resolution, OverFeat applies offset
max pooling after the last CONV layer, i.e. performing a subsampling operation at every offset, yielding
many more views for voting, increasing robustness while
remaining efﬁcient.
3. Bounding box regression Once an object is identiﬁed,
a single bounding box regressor is applied. The classi-
ﬁer and the regressor share the same feature extraction
(CONV)layers,onlytheFClayersneedtoberecomputed
after computing the classiﬁcation network.
4. Combine predictions OverFeat uses a greedy merge strategy to combine the individual bounding box predictions
across all locations and scales.
OverFeat has a signiﬁcant speed advantage, but is less accuratethanRCNN ,becauseitwasdifﬁcult
to train fully convolutional networks at the time. The speed
advantage derives from sharing the computation of convolution between overlapping windows in the fully convolutional
network. OverFeat is similar to later frameworks such as
YOLO and SSD ,
except that the classiﬁer and the regressors in OverFeat are
trained sequentially.
YOLO Redmon et al. proposed YOLO (You Only
Look Once), a uniﬁed detector casting object detection as
a regression problem from image pixels to spatially separated bounding boxes and associated class probabilities,
illustrated in Fig. 13. Since the region proposal generation
International Journal of Computer Vision
stage is completely dropped, YOLO directly predicts detections using a small set of candidate regions7. Unlike region
based approaches (e.g. Faster RCNN) that predict detections
based on features from a local region, YOLO uses features
from an entire image globally. In particular, YOLO divides
an image into an S × S grid, each predicting C class probabilities, B bounding box locations, and conﬁdence scores.
By throwing out the region proposal generation step entirely,
YOLO is fast by design, running in real time at 45 FPS and
Fast YOLO at 155 FPS. Since YOLO
sees the entire image when making predictions, it implicitly
encodes contextual information about object classes, and is
less likely to predict false positives in the background. YOLO
makes more localization errors than Fast RCNN, resulting
from the coarse division of bounding box location, scale and
aspect ratio. As discussed in Redmon et al. , YOLO
may fail to localize some objects, especially small ones, possibly because of the coarse grid division, and because each
grid cell can only contain one object. It is unclear to what
extent YOLO can translate to good performance on datasets
with many objects per image, such as MS COCO.
YOLOv2 and YOLO9000 Redmon and Farhadi 
proposed YOLOv2, an improved version of YOLO, in which
the custom GoogLeNet network is
replaced with the simpler DarkNet19, plus batch normalization , removing the fully connected layers,
and using good anchor boxes8 learned via kmeans and multiscale training. YOLOv2 achieved state-of-the-art on standard
detection tasks. Redmon and Farhadi also introduced
YOLO9000, which can detect over 9000 object categories in
real time by proposing a joint optimization method to train
simultaneously on an ImageNet classiﬁcation dataset and
a COCO detection dataset with WordTree to combine data
from multiple sources. Such joint training allows YOLO9000
to perform weakly supervised detection, i.e. detecting object
classes that do not have bounding box annotations.
SSD In order to preserve real-time speed without sacriﬁcing too much detection accuracy, Liu et al. proposed
SSD (Single Shot Detector), faster than YOLO and with an accuracy competitive with regionbased detectors such as Faster RCNN . SSD
effectively combines ideas from RPN in Faster RCNN , YOLO and multiscale
CONV features to achieve fast detection speed, while still retaining high detection quality. Like
YOLO, SSD predicts a ﬁxed number of bounding boxes
and scores, followed by an NMS step to produce the ﬁnal
detection. The CNN network in SSD is fully convolutional,
whose early layers are based on a standard architecture, such
7 YOLO uses far fewer bounding boxes, only 98 per image, compared
to about 2000 from Selective Search.
8 Boxesofvarioussizesandaspectratiosthatserveasobjectcandidates.
as VGG , followed by several auxiliary CONV layers, progressively decreasing in size.
The information in the last layer may be too coarse spatiallytoallowpreciselocalization,soSSDperformsdetection
over multiple scales by operating on multiple CONV feature
maps, each of which predicts category scores and box offsets for bounding boxes of appropriate sizes. For a 300×300
input, SSD achieves 74.3% mAP on the VOC2007 test at 59
FPS versus Faster RCNN 7 FPS / mAP 73.2% or YOLO 45
FPS / mAP 63.4%.
CornerNet Recently, Law and Deng questioned the
dominant role that anchor boxes have come to play in SoA
object detection frameworks . Law and Deng 
argue that the use of anchor boxes, especially in one stage
detectors , has drawbacks such as causing a huge imbalance between
positive and negative examples, slowing down training and
introducingextrahyperparameters.Borrowingideasfromthe
work on Associative Embedding in multiperson pose estimation , Law and Deng proposed
CornerNet by formulating bounding box object detection
as detecting paired top-left and bottom-right keypoints9. In
CornerNet, the backbone network consists of two stacked
Hourglass networks , with a simple corner pooling approach to better localize corners. CornerNet
achieved a 42.1% AP on MS COCO, outperforming all previous one stage detectors; however, the average inference
time is about 4FPS on a Titan X GPU, signiﬁcantly slower
than SSD and YOLO .
CornerNet generates incorrect bounding boxes because it is
challenging to decide which pairs of keypoints should be
grouped into the same objects. To further improve on CornerNet, Duan et al. proposed CenterNet to detect each
object as a triplet of keypoints, by introducing one extra keypoint at the centre of a proposal, raising the MS COCO AP to
47.0%, but with an inference speed slower than CornerNet.
6 Object Representation
As one of the main components in any detector, good feature
representations are of primary importance in object detection
 . In the past, a great deal
of effort was devoted to designing local descriptors [e.g.,
SIFT and HOG ] and to
explore approaches [e.g., Bag of Words and Fisher Vector ] to group and
9 The idea of using keypoints for object detection appeared previously
in DeNet .
International Journal of Computer Vision
Fig. 15 Performance of winning entries in the ILSVRC competitions
from 2011 to 2017 in the image classiﬁcation task
abstract descriptors into higher level representations in order
to allow the discriminative parts to emerge; however, these
feature representation methods required careful engineering
and considerable domain expertise.
In contrast, deep learning methods (especially deep
CNNs) can learn powerful feature representations with multiple levels of abstraction directly from raw images . As the learning procedure
reduces the dependency of speciﬁc domain knowledge and
complex procedures needed in traditional feature engineering , the burden for
feature representation has been transferred to the design of
better network architectures and training procedures.
The leading frameworks reviewed in Sect. 5 [RCNN , Fast RCNN , Faster RCNN
 ,YOLO ,SSD ] have persistently promoted detection accuracy and
speed, in which it is generally accepted that the CNN architecture (Sect. 6.1 and Fig. 15) plays a crucial role. As a result,
most of the recent improvements in detection accuracy have
been via research into the development of novel networks.
Therefore we begin by reviewing popular CNN architectures
used in Generic Object Detection, followed by a review of
the effort devoted to improving object feature representations, such as developing invariant features to accommodate
geometric variations in object scale, pose, viewpoint, part
deformation and performing multiscale analysis to improve
object detection over a wide range of scales.
6.1 Popular CNN Architectures
CNN architectures (Sect. 3) serve as network backbones used
in the detection frameworks of Sect. 5. Representative frameworks include AlexNet , ZFNet
 VGGNet , GoogLeNet , Inception series
 , ResNet
 , DenseNet and SENet
 , summarized in Table 6, and where the
improvement over time is seen in Fig. 15. A further review
of recent CNN advances can be found in Gu et al. .
The trend in architecture evolution is for greater depth:
AlexNet has 8 layers, VGGNet 16 layers, more recently
ResNet and DenseNet both surpassed the 100 layer mark,
and it was VGGNet and
GoogLeNet which showed that increasing depth can improve the representational power. As can be
observed from Table 6, networks such as AlexNet, OverFeat,
ZFNet and VGGNet have an enormous number of parameters, despite being only a few layers deep, since a large
fraction of the parameters come from the FC layers. Newer
networks like Inception, ResNet, and DenseNet, although
having a great depth, actually have far fewer parameters by
avoiding the use of FC layers.
With the use of Inception modules in
carefully designed topologies, the number of parameters of
GoogLeNet is dramatically reduced, compared to AlexNet,
ZFNet or VGGNet. Similarly, ResNet demonstrated the
effectiveness of skip connections for learning extremely deep
networks with hundreds of layers, winning the ILSVRC
2015 classiﬁcation task. Inspired by ResNet ,
InceptionResNets combined the Inception networks with shortcut connections, on the basis that
shortcut connections can signiﬁcantly accelerate network
training. Extending ResNets, Huang et al. proposed
DenseNets, which are built from dense blocksconnecting
each layer to every other layer in a feedforward fashion, leading to compelling advantages such as parameter efﬁciency,
implicit deep supervision10, and feature reuse. Recently, He
et al. proposed Squeeze and Excitation (SE) blocks,
which can be combined with existing deep architectures to
boost their performance at minimal additional computational
cost, adaptively recalibrating channel-wise feature responses
by explicitly modeling the interdependencies between convolutional feature channels, and which led to winning the
ILSVRC 2017 classiﬁcation task. Research on CNN architectures remains active, with emerging networks such as
Hourglass , Dilated Residual Networks
 , Xception , DetNet ,DualPathNetworks(DPN) ,Fish-
Net , and GLoRe .
10 DenseNets perform deep supervision in an implicit way, i.e. individual layers receive additional supervision from other layers through the
shorter connections. The beneﬁts of deep supervision have previously
been demonstrated in Deeply Supervised Nets (DSN) .
International Journal of Computer Vision
Table 6 DCNN architectures that were commonly used for generic object detection
DCNN architecture
#Paras (×106)
#Layers (CONV+FC)
Test error (Top 5)
First used in
Highlights
AlexNet 
Girshick et al. 
The ﬁrst DCNN found effective for
ImageNet classiﬁcation; the
historical turning point from
hand-crafted features to CNN;
Winning the ILSVRC2012
Image classiﬁcation competition
ZFNet (fast) 
He et al. 
Similar to AlexNet, different in
stride for convolution, ﬁlter size,
and number of ﬁlters for some
OverFeat 
Sermanet et al. 
Similar to AlexNet, different in
stride for convolution, ﬁlter size,
and number of ﬁlters for some
VGGNet 
Girshick 
Increasing network depth
signiﬁcantly by stacking 3 × 3
convolution ﬁlters and increasing
the network depth step by step
GoogLeNet 
Szegedy et al. 
Use Inception module, which uses
multiple branches of
convolutional layers with
different ﬁlter sizes and then
concatenates feature maps
produced by these branches. The
ﬁrst inclusion of bottleneck
structure and global average
Inception v2 
Howard et al. 
Faster training with the introduce
of batch normalization
Inception v3 
Inclusion of separable convolution
and spatial resolution reduction
YOLONet 
Redmon et al. 
A network inspired by GoogLeNet
used in YOLO detector
ResNet50 
3.6% (ResNets)
He et al. 
With identity mapping,
substantially deeper networks
can be learned
International Journal of Computer Vision
Table 6 continued
DCNN architecture
#Paras (×106)
#Layers (CONV+FC)
Test error (Top 5)
First used in
Highlights
ResNet101 
He et al. 
Requires fewer parameters than
VGG by using the global average
pooling and bottleneck
introduced in GoogLeNet
InceptionResNet v1 
3.1% (Ensemble)
Combination of identity mapping
and Inception module, with
similar computational cost of
Inception v3, but faster training
InceptionResNet v2 Szegedy et al. 
 
A costlier residual version of
Inception, with signiﬁcantly
improved recognition
performance
Inception v4 Szegedy et al. 
An Inception variant without
residual connections, with
roughly the same recognition
performance as InceptionResNet
v2, but signiﬁcantly slower
ResNeXt 
Xie et al. 
Repeating a building block that
aggregates a set of
transformations with the same
DenseNet201 
Zhou et al. 
Concatenate each layer with every
other layer in a feed forward
fashion. Alleviate the vanishing
gradient problem, encourage
feature reuse, reduction in
number of parameters
DarkNet 
Redmon and Farhadi 
Similar to VGGNet, but with
signiﬁcantly fewer parameters
MobileNet 
Howard et al. 
Light weight deep CNNs using
depth-wise separable
convolutions
SE ResNet 
2.3% (SENets)
Hu et al. 
Channel-wise attention by a novel
block called Squeeze and
Excitation. Complementary to
existing backbone CNNs
Regarding the statistics for “#Paras” and “#Layers”, the ﬁnal FC prediction layer is not taken into consideration. “Test Error” column indicates the Top 5 classiﬁcation test error on ImageNet1000.
When ambiguous, the “#Paras”, “#Layers”, and “Test Error” refer to: OverFeat (accurate model), VGGNet16, ResNet101 DenseNet201 (Growth Rate 32, DenseNet-BC), ResNeXt50 (32*4d), and
SE ResNet50
International Journal of Computer Vision
The training of a CNN requires a large-scale labeled
dataset with intraclass diversity. Unlike image classiﬁcation,
detection requires localizing (possibly many) objects from an
image. It has been shown that pretraining a deep model with a large scale dataset having object level
annotations (such as ImageNet), instead of only the image
level annotations, improves the detection performance. However, collecting bounding box labels is expensive, especially
for hundreds of thousands of categories. A common scenario
is for a CNN to be pretrained on a large dataset (usually with
a large number of visual categories) with image-level labels;
the pretrained CNN can then be applied to a small dataset,
directly, as a generic feature extractor , which can support a wider range of visual recognition tasks. For detection, the pre-trained network is typically
ﬁne-tuned11 on a given detection dataset . Several large scale image
classiﬁcation datasets are used for CNN pre-training, among
them ImageNet1000 with 1.2 million images of 1000 object categories,
Places , which is much larger than ImageNet1000 but with fewer classes, a recent Places-Imagenet
hybrid , or JFT300M .
Pretrained CNNs without ﬁne-tuning were explored for
object classiﬁcation and detection in Donahue et al. ,
Girshick et al. , Agrawal et al. , where it was
shown that detection accuracies are different for features
extracted from different layers; for example, for AlexNet pretrained on ImageNet, FC6 / FC7 / Pool5 are in descending
order of detection accuracy . Fine-tuning a pre-trained network can increase
detection performance signiﬁcantly ,althoughinthecaseofAlexNet,theﬁne-tuningperformance boost was shown to be much larger for FC6 / FC7 than
for Pool5, suggesting that Pool5 features are more general.
Furthermore, the relationship between the source and target
datasetsplaysacriticalrole,forexamplethatImageNetbased
CNN features show better performance for object detection
than for human action , Fast RCNN , Faster RCNN andYOLO ,typicallyusethedeep
CNN architectures listed in Table 6 as the backbone network
11 Fine-tuning is done by initializing a network with weights optimized
for a large labeled dataset like ImageNet. and then updating the network’s weights using the target-task training set.
and use features from the top layer of the CNN as object representations; however, detecting objects across a large range
of scales is a fundamental challenge. A classical strategy to
address this issue is to run the detector over a number of
scaled input images (e.g., an image pyramid) , which
typically produces more accurate detection, with, however,
obvious limitations of inference time and memory.
6.2.1 Handling of Object Scale Variations
Since a CNN computes its feature hierarchy layer by layer,
the sub-sampling layers in the feature hierarchy already lead
to an inherent multiscale pyramid, producing feature maps at
different spatial resolutions, but subject to challenges .
In particular, the higher layers have a large receptive ﬁeld and
strong semantics, and are the most robust to variations such
as object pose, illumination and part deformation, but the resolution is low and the geometric details are lost. In contrast,
lower layers have a small receptive ﬁeld and rich geometric details, but the resolution is high and much less sensitive
to semantics. Intuitively, semantic concepts of objects can
emerge in different layers, depending on the size of the
objects. So if a target object is small it requires ﬁne detail
information in earlier layers and may very well disappear at
later layers, in principle making small object detection very
challenging, for which tricks such as dilated or “atrous” convolution have been proposed, increasing feature resolution,
but increasing computational complexity. On the other hand,
if the target object is large, then the semantic concept will
emerge in much later layers. A number of methods have been proposed to improve detection accuracy by exploiting multiple CNN layers, broadly falling into
three types of multiscale object detection:
1. Detecting with combined features of multiple layers;
2. Detecting at multiple layers;
3. Combinations of the above two methods.
(1) Detecting with combined features of multiple CNN layers Many approaches, including Hypercolumns , HyperNet , and ION , combine features from multiple layers before
making a prediction. Such feature combination is commonly
accomplished via concatenation, a classic neural network
idea that concatenates features from different layers, architectures which have recently become popular for semantic
segmentation . As shown in Fig. 16a, ION uses RoI pooling to extract RoI features from multiple
International Journal of Computer Vision
Fig. 16 Comparison of HyperNet and ION. LRN is local response normalization, which performs a kind of “lateral inhibition” by normalizing
over local input regions 
layers, and then the object proposals generated by selective
searchandedgeboxesareclassiﬁedbyusingtheconcatenated
features. HyperNet , shown in Fig. 16b,
follows a similar idea, and integrates deep, intermediate and
shallow features to generate object proposals and to predict
objects via an end to end joint training strategy. The combined feature is more descriptive, and is more beneﬁcial for
localizationandclassiﬁcation,butatincreasedcomputational
complexity.
(2) Detecting at multiple CNN layers A number of recent
approaches improve detection by predicting objects of different resolutions at different layers and then combining these
predictions: SSD and MSCNN , RBFNet , and DSOD . SSD spreads out default boxes of
different scales to multiple layers within a CNN, and forces
each layer to focus on predicting objects of a certain scale.
RFBNet replaces the later convolution layers of SSD with a Receptive Field Block (RFB) to enhance
the discriminability and robustness of features. The RFB is
a multibranch convolutional block, similar to the Inception
block , but combining multiple branches
with different kernels and convolution layers . MSCNN applies deconvolution on
multiple layers of a CNN to increase feature map resolution
before using the layers to learn region proposals and pool features. Similar to RFBNet , TridentNet constructs a parallel multibranch architecture
where each branch shares the same transformation parameters but with different receptive ﬁelds; dilated convolution
with different dilation rates are used to adapt the receptive
ﬁelds for objects of different scales.
(3) Combinations of the above two methods Features from
different layers are complementary to each other and can
improve detection accuracy, as shown by Hypercolumns
 , HyperNet and
ION . On the other hand, however, it is
natural to detect objects of different scales using features
of approximately the same size, which can be achieved by
detecting large objects from downscaled feature maps while
detecting small objects from upscaled feature maps. Therefore, in order to combine the best of both worlds, some recent
works propose to detect objects at multiple layers, and the
resulting features obtained by combining features from different layers. This approach has been found to be effective
for segmentation 
and human pose estimation , has been
widely exploited by both one-stage and two-stage detectors to alleviate problems of scale variation across object
instances. Representative methods include SharpMask , Deconvolutional Single Shot Detector
(DSSD) , Feature Pyramid Network (FPN)
 , TopDownModulation(TDM) , Reverse connection with Objectness prior Network (RON) , ZIP , Scale
Transfer Detection Network (STDN) ,
ReﬁneDet , StairNet ,
Path Aggregation Network (PANet) , Feature Pyramid Reconﬁguration (FPR) ,
DetNet , Scale Aware Network (SAN) , Multiscale Location aware Kernel Representation (MLKP) and M2Det , as shown in Table 7 and contrasted in Fig. 17.
Early works like FPN , DSSD , TDM , ZIP ,
RON and ReﬁneDet 
construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbone, and achieved
encouraging results. As can be observed from Fig. 17a1–
f1, these methods have very similar detection architectures
which incorporate a top-down network with lateral connections to supplement the standard bottom-up, feed-forward
network. Speciﬁcally, after a bottom-up pass the ﬁnal high
level semantic features are transmitted back by the top-down
network to combine with the bottom-up features from intermediate layers after lateral processing, and the combined
features are then used for detection. As can be seen from
Fig. 17a2–e2, the main differences lie in the design of the
simple Feature Fusion Block (FFB), which handles the selection of features from different layers and the combination of
multilayer features.
FPN shows signiﬁcant improvement as
a generic feature extractor in several applications including
object detection and instance segmentation . Using FPN in a basic Faster RCNN
system achieved state-of-the-art results on the COCO detection dataset. STDN used DenseNet
 to combine features of different layers
and designed a scale transfer module to obtain feature maps
International Journal of Computer Vision
Table 7 Summary of properties of representative methods in improving DCNN feature representations for generic object detection
Detector name
mAP@IoU=0.5
 
Highlights
(1) Single
detection with
multilayer
ION (Bell et al.
79.4 (07+12)
76.4 (07+12)
Use features from
multiple layers; use
spatial recurrent
neural networks for
modeling contextual
information; the Best
Student Entry and the
3rd overall in the
COCO detection
challenge 2015
HyperNet 
Faster RCNN
76.3 (07+12)
71.4 (07T+12)
Use features from
multiple layers for
both region proposal
and region
classiﬁcation
PVANet 
Faster RCNN
(07+12+CO)
84.2 (07T+12+CO)
Deep but lightweight;
Combine ideas from
concatenated ReLU
 ,
Inception , and
HyperNet (Kong et al.
International Journal of Computer Vision
Table 7 continued
Detector name
mAP@IoU=0.5
 
Highlights
(2) Detection at
multiple layers
SDP+CRC 
Use features in multiple
layers to reject easy
negatives via CRC,
and then classify
remaining proposals
MSCNN 
Faster RCNN
Only Tested on KITTI
Region proposal and
classiﬁcation are
performed at multiple
layers; includes
feature upsampling;
end to end learning
 
et al. 2016)
Concatenate features
from different
convolutional layers
and features of
different contextual
regions; loss function
for multiple overlap
thresholds; ranked 2nd
in both the COCO15
detection and
segmentation
challenges
DSOD 
77.7 (07+12)
72.2 (07T+12)
Concatenate feature
sequentially, like
DenseNet. Train from
scratch on the target
dataset without
pre-training
RFBNet 
82.2 (07+12)
81.2 (07T+12)
Propose a multi-branch
convolutional block
similar to Inception
 ,
but using dilated
convolution
International Journal of Computer Vision
Table 7 continued
Detector name
mAP@IoU=0.5
 
Highlights
(3) Combination
of (1) and (2)
DSSD (Fu et al.
81.5 (07+12)
80.0 (07T+12)
Use Conv-Deconv, as
shown in Fig. 17c1, c2
FPN 
Faster RCNN
Use Conv-Deconv, as
shown in Fig. 17b2
RON (Kong et al.
Faster RCNN
(07+12+CO)
80.7 (07T+12+CO)
Use Conv-deconv, as
shown in Fig. 17d2;
Add the objectness
prior to signiﬁcantly
reduce object search
ZIP (Li et al.
Inceptionv2
Faster RCNN
79.8 (07+12)
Use Conv-Deconv, as
shown in Fig. 17f1.
Propose a map
attention decision
(MAD) unit for
features from different
International Journal of Computer Vision
Table 7 continued
Detector name
mAP@IoU=0.5
 
Highlights
STDN 
DenseNet169
80.9 (07+12)
A new scale transfer
module, which resizes
features of different
scales to the same
scale in parallel
(Zhang et al.
Faster RCNN
83.8 (07+12)
83.5 (07T+12)
Use cascade to obtain
better and less
anchors. Use
Conv-deconv, as
shown in Fig. 17e2 to
improve features
PANet 
81.1 (07T+12)
Fuse task oriented
features across
different spatial
locations and scales,
globally and locally;
Shown in Fig. 17h
M2Det 
VGG16 ResNet101
Shown in Fig. 17j,
newly designed top
down path to learn a
set of multilevel
features, recombined
to construct a feature
pyramid for object
International Journal of Computer Vision
Table 7 continued
Detector name
mAP@IoU=0.5
 
Highlights
transforms
(Ouyang et al.
AlexNet ZFNet
Introduce a deformation
constrained pooling
layer, jointly learned
with convolutional
layers in existing
DCNNs. Utilize the
following modules
that are not trained end
to end: cascade,
context modeling,
model averaging, and
bounding box location
reﬁnement in the
multistage detection
DCN (Dai et al.
82.6 (07+12)
Design deformable
convolution and
deformable RoI
pooling modules that
can replace plain
convolution in existing
DPFCN 
AttractioNet
(Gidaris and
83.3 (07+12)
81.2 (07T+12)
Design a deformable
part based RoI pooling
layer to explicitly
select discriminative
regions around object
Details for Groups (1), (2), and (3) are provided in Sect. 6.2. Abbreviations: Selective Search (SS), EdgeBoxes (EB), InceptionResNet (IRN). Conv-Deconv denotes the use of upsampling and
convolutional layers with lateral connections to supplement the standard backbone network. Detection results on VOC07, VOC12 and COCO were reported with mAP@IoU=0.5, and the additional
COCO results are computed as the average of mAP for IoU thresholds from 0.5 to 0.95. Training data: “07”←VOC2007 trainval; “07T”←VOC2007 trainval and test; “12”←VOC2012 trainval;
CO←COCO trainval. The COCO detection results were reported with COCO2015 Test-Dev, except for MPN which reported with COCO2015 Test-Standard
International Journal of Computer Vision
Fig. 17 Hourglass architectures: Conv1 to Conv5 are the main Conv
blocks in backbone networks such as VGG or ResNet. The ﬁgure compares a number of feature fusion blocks (FFB) commonly used in recent
approaches: FPN , TDM ,
DSSD , RON , ReﬁneDet , ZIP , PANet , FPR
 , DetNet and M2Det . FFM feature fusion module, TUM thinned U-shaped module
International Journal of Computer Vision
with different resolutions. The scale transfer module can be
directly embedded into DenseNet with little additional cost.
More recent work, such as PANet , FPR
 , DetNet , and M2Det
 , as shown in Fig. 17g–j, propose to further
improve on the pyramid architectures like FPN in different
ways. Based on FPN, Liu et al. designed PANet (Fig. 17g1) by adding another bottom-up path with
clean lateral connections from low to top levels, in order
to shorten the information path and to enhance the feature
pyramid. Then, an adaptive feature pooling was proposed to
aggregate features from all feature levels for each proposal.
In addition, in the proposal sub-network, a complementary
branch capturing different views for each proposal is created to further improve mask prediction. These additional
steps bring only slightly extra computational overhead, but
are effective and allowed PANet to reach 1st place in the
COCO 2017 Challenge Instance Segmentation task and 2nd
place in the Object Detection task. Kong et al. proposed FPR
 by explicitly reformulating the feature
pyramid construction process [e.g. FPN ]
as feature reconﬁguration functions in a highly nonlinear but
efﬁcient way. As shown in Fig. 17h1, instead of using a topdown path to propagate strong semantic features from the
topmost layer down as in FPN, FPR ﬁrst extracts features
from multiple layers in the backbone network by adaptive
concatenation, and then designs a more complex FFB module
(Fig. 17h2) to spread strong semantics to all scales. Li et al.
 proposed DetNet (Fig. 17i1) by introducing dilated
convolutions to the later layers of the backbone network in
order to maintain high spatial resolution in deeper layers.
Zhao et al. proposed a MultiLevel Feature Pyramid
Network (MLFPN) to build more effective feature pyramids
for detecting objects of different scales. As can be seen from
Fig. 17j1, features from two different layers of the backbone
are ﬁrst fused as the base feature, after which a top-down
path with lateral connections from the base feature is created
to build the feature pyramid. As shown in Fig. 17j2, j5, the
FFB module is much more complex than those like FPN, in
that FFB involves a Thinned U-shaped Module (TUM) to
generate a second pyramid structure, after which the feature
maps with equivalent sizes from multiple TUMs are combined for object detection. The authors proposed M2Det by
integrating MLFPN into SSD, and achieved better detection
performance than other one-stage detectors.
6.3 Handling of Other Intraclass Variations
Powerful object representations should combine distinctiveness and robustness. A large amount of recent work has been
devoted to handling changes in object scale, as reviewed in
Sect. 6.2.1. As discussed in Sect. 2.2 and summarized in
Fig. 5, object detection still requires robustness to real-world
variations other than just scale, which we group into three
categories:
• Geometric transformations,
• Occlusions, and
• Image degradations.
To handle these intra-class variations, the most straightforward approach is to augment the training datasets with a
sufﬁcient amount of variations; for example, robustness to
rotation could be achieved by adding rotated objects at many
orientations to the training data. Robustness can frequently
be learned this way, but usually at the cost of expensive training and complex model parameters. Therefore, researchers
have proposed alternative solutions to these problems.
Handling of geometric transformations DCNNs are inherently limited by the lack of ability to be spatially invariant
to geometric transformations of the input data . The introduction of local max pooling layers has allowed DCNNs to
enjoy some translation invariance, however the intermediate
feature maps are not actually invariant to large geometric
transformations of the input data .
Therefore, many approaches have been presented to enhance
robustness, aiming at learning invariant CNN representations
with respect to different types of transformations such as
scale , rotation
 , or both . One
representative work is Spatial Transformer Network (STN)
 , which introduces a new learnable
module to handle scaling, cropping, rotations, as well as nonrigid deformations via a global parametric transformation.
STN has now been used in rotated text detection , rotated face detection and generic object detection .
Although rotation invariance may be attractive in certain
applications, such as scene text detection , face detection , and aerial
imagery , there is limited
generic object detection work focusing on rotation invariance
because popular benchmark detection datasets (e.g. PAS-
CALVOC,ImageNet,COCO)donotactuallypresentrotated
Before deep learning, Deformable Part based Models
(DPMs) were successful for
generic object detection, representing objects by component parts arranged in a deformable conﬁguration. Although
DPMs have been signiﬁcantly outperformed by more recent
object detectors, their spirit still deeply inﬂuences many
recent detectors. DPM modeling is less sensitive to transformations in object pose, viewpoint and nonrigid deformations,
motivating researchers to
explicitly model object composition to improve CNN based
detection. The ﬁrst attempts combined DPMs with CNNs by using deep features
learned by AlexNet in DPM based detection, but without
region proposals. To enable a CNN to beneﬁt from the builtin capability of modeling the deformations of object parts, a
number of approaches were proposed, including DeepIDNet
 , DCN and DPFCN
 (shown in Table 7). Although similar in spirit, deformations are computed in different ways:
DeepIDNet designed a deformation
constrained pooling layer to replace regular max pooling, to
learn the shared visual patterns and their deformation properties across different object classes; DCN 
designed a deformable convolution layer and a deformable
RoI pooling layer, both of which are based on the idea of
augmenting regular grid sampling locations in feature maps;
and DPFCN proposed a deformable
part-based RoI pooling layer which selects discriminative
parts of objects around object proposals by simultaneously
optimizing latent displacements of all parts.
Handling of occlusions In real-world images, occlusions are common, resulting in information loss from object
instances. A deformable parts idea can be useful for occlusion handling, so deformable RoI Pooling and deformable
convolution have been proposed to alleviate occlusion by giving more ﬂexibility to the typically ﬁxed
geometric structures. Wang et al. propose to learn an
adversarial network that generates examples with occlusions
and deformations, and context may be helpful in dealing with
occlusions . Despite these efforts, the
occlusion problem is far from being solved; applying GANs
to this problem may be a promising research direction.
Handling of image degradations Image noise is a commonprobleminmanyreal-worldapplications.Itisfrequently
caused by insufﬁcient lighting, low quality cameras, image
compression, or the intentional low-cost sensors on edge
devices and wearable devices. While low image quality may
be expected to degrade the performance of visual recognition, most current methods are evaluated in a degradation free
and clean environment, evidenced by the fact that PASCAL
VOC, ImageNet, MS COCO and Open Images all focus on
relatively high quality images. To the best of our knowledge,
there is so far very limited work to address this problem.
7 Context Modeling
In the physical world, visual objects occur in particular environments and usually coexist with other related objects.
There is strong psychological evidence that context plays an essential role in human
object recognition, and it is recognized that a proper modeling of context helps object detection and recognition
 ,
especially when object appearance features are insufﬁcient
because of small object size, object occlusion, or poor image
quality. Many different types of context have been discussed
 , and
can broadly be grouped into one of three categories:
1. Semantic context: The likelihood of an object to be found
in some scenes, but not in others;
2. Spatial context: The likelihood of ﬁnding an object in
some position and not others with respect to other objects
in the scene;
3. Scale context: Objects have a limited set of sizes relative
to other objects in the scene.
A great deal of work preceded the prevalence of deep learning,
and much of this work has yet to be explored in DCNN-based
object detectors .
The current state of the art in object detection detects objects without explicitly exploiting any contextual information. It is
broadly agreed that DCNNs make use of contextual information implicitly 
since they learn hierarchical representations with multiple
levels of abstraction. Nevertheless, there is value in exploring
contextual information explicitly in DCNN based detectors
 , so
the following reviews recent work in exploiting contextual
cues in DCNN- based object detectors, organized into categories of global and local contexts, motivated by earlier work
in Zhang et al. , Galleguillos and Belongie .
Representative approaches are summarized in Table 8.
7.1 Global Context
Global context refers to image or scene level contexts, which can serve
as cues for object detection (e.g., a bedroom will predict the
presence of a bed). In DeepIDNet , the
image classiﬁcation scores were used as contextual features,
and concatenated with the object detection scores to improve
detection results. In ION , Bell et al. proposed to use spatial Recurrent Neural Networks (RNNs) to
explore contextual information across the entire image. In
SegDeepM , Zhu et al. proposed a Markov
random ﬁeld model that scores appearance as well as context
International Journal of Computer Vision
Table 8 Summary of detectors that exploit context information, with labelling details as in Table 7
Detector name
Region proposal
Backbone DCNN
Pipelined Used
mAP@IoU=0.5
 
Highlights
Global context
SegDeepM 
Additional features
extracted from an
enlarged object
proposal as context
information
(Ouyang et al.
AlexNet ZFNet
Use image classiﬁcation
scores as global
contextual information
to reﬁne the detection
scores of each object
ION (Bell et al.
The contextual
information outside
the region of interest is
integrated using
spatial recurrent
neural networks
CPF (Shrivastava
Faster RCNN
76.4 (07+12)
72.6 (07T+12)
Use semantic
segmentation to
provide top-down
Local context
(Gidaris and
78.2 (07+12)
73.9 (07+12)
Extract features from
multiple regions
surrounding or inside
the object proposals.
Integrate the semantic
segmentation-aware
GBDNet 
Inception v2
(Zhang et al.
77.2 (07+12)
ECCV16 TPAMI18
A GBDNet module to
learn the relations of
multiscale
contextualized regions
surrounding an object
proposal; GBDNet
passes messages
among features from
different context
regions through
convolution between
neighboring support
regions in two
directions
International Journal of Computer Vision
Table 8 continued
Detector name
Region proposal
Backbone DCNN
Pipelined Used
mAP@IoU=0.5
 
Highlights
et al. 2017b)
72.0 (07+12)
70.6 (07T+12)
Use LSTM to capture
global context.
Concatenate features
from multi-scale
contextual regions
surrounding an object
proposal. The global
and local context
features are
concatenated for
recognition
CoupleNet 
82.7 (07+12)
80.4 (07T+12)
Concatenate features
from multiscale
contextual regions
surrounding an object
proposal. Features of
different contextual
regions are then
combined by
convolution and
element-wise sum
SMN 
Faster RCNN
Model object-object
relationships
efﬁciently through a
spatial memory
network. Learn the
functionality of NMS
automatically
ORN (Hu et al.
Faster RCNN
Model the relations of a
set of object proposals
through the
interactions between
their appearance
features and geometry.
Learn the functionality
of NMS automatically
SIN (Liu et al.
Faster RCNN
76.0 (07+12)
73.1 (07T+12)
Formulate object
detection as
graph-structured
inference, where
objects are graph
relationships the edges
International Journal of Computer Vision
for each detection, and allows each candidate box to select a
segment out of a large pool of object segmentation proposals
and score the agreement between them. In Shrivastava and
Gupta , semantic segmentation was used as a form of
contextual priming.
7.2 Local Context
Local context considers the relationship
among locally nearby objects, as well as the interactions
between an object and its surrounding area. In general, modeling object relations is challenging, requiring reasoning
about bounding boxes of different classes, locations, scales
etc. Deep learning research that explicitly models object relations is quite limited, with representative ones being Spatial
Memory Network (SMN) , Object
Relation Network , and Structure Inference
Network (SIN) . In SMN, spatial memory
essentially assembles object instances back into a pseudo
image representation that is easy to be fed into another CNN
for object relations reasoning, leading to a new sequential
reasoning architecture where image and memory are processed in parallel to obtain detections which further update
memory. Inspired by the recent success of attention modules in natural language processing ,
ORN processes a set of objects simultaneously through the
interaction between their appearance feature and geometry.
It does not require additional supervision, and it is easy to
embed into existing networks, effective in improving object
recognition and duplicate removal steps in modern object
detection pipelines, giving rise to the ﬁrst fully end-to-end
object detector. SIN considered two kinds
of context: scene contextual information and object relationships within a single image. It formulates object detection as
a problem of graph inference, where the objects are treated
as nodes in a graph and relationships between objects are
modeled as edges.
A wider range of methods has approached the context challenge with a simpler idea: enlarging the detection window size to extract some form of local context.
Representative approaches include MRCNN , Gated BiDirectional CNN (GBDNet)
Zeng et al. , Zeng et al. , Attention to Context CNN (ACCNN) , CoupleNet , and Sermanet et al. . In MRCNN (Fig. 18a), in addition to the features
extracted from the original object proposal at the last CONV
layer of the backbone, Gidaris and Komodakis proposed to
extract features from a number of different regions of an
object proposal (half regions, border regions, central regions,
contextual region and semantically segmented regions), in
order to obtain a richer and more robust object representation. All of these features are combined by concatenation.
Quiteanumberofmethods,allcloselyrelatedtoMRCNN,
have been proposed since then. The method in Zagoruyko
et al. used only four contextual regions, organized in
a foveal structure, where the classiﬁers along multiple paths
are trained jointly end-to-end. Zeng et al. , Zeng et al.
 proposed GBDNet (Fig. 18b) to extract features from
multiscale contextualized regions surrounding an object proposal to improve detection performance. In contrast to the
somewhat naive approach of learning CNN features for each
region separately and then concatenating them, GBDNet
passes messages among features from different contextual
regions. Noting that message passing is not always helpful,
but dependent on individual samples, Zeng et al. used
gated functions to control message transmission. Li et al.
 presented ACCNN (Fig. 18c) to utilize both global
and local contextual information: the global context was
captured using a Multiscale Local Contextualized (MLC)
subnetwork, which recurrently generates an attention map for
an input image to highlight promising contextual locations;
local context adopted a method similar to that of MRCNN
 . As shown in Fig. 18d, CoupleNet is conceptually similar to ACCNN
 , but built upon RFCN ,
which captures object information with position sensitive
RoI pooling, CoupleNet added a branch to encode the global
context with RoI pooling.
8 Detection Proposal Methods
An object can be located at any position and scale in an
image. During the heyday of handcrafted feature descriptors [SIFT , HOG and
LBP ], the most successful methods for
object detection [e.g. DPM ] used
sliding window techniques . However, the number of windows is
huge, growing with the number of pixels in an image, and
the need to search at multiple scales and aspect ratios further
increases the search space12. Therefore, it is computationally
too expensive to apply sophisticated classiﬁers.
Around 2011, researchers proposed to relieve the tension
between computational tractability and high detection qual-
12 Sliding window based detection requires classifying around 104–
105 windows per image. The number of windows grows signiﬁcantly
to 106–107 windows per image when considering multiple scales and
aspect ratios.
International Journal of Computer Vision
Fig. 18 Representative approaches that explore local surrounding contextual features: MRCNN , GBDNet , ACCNN and CoupleNet ; also see Table 8
ity by using detection proposals13 . Originating in the idea of objectness
proposed by Alexe et al. , object proposals are a set
of candidate regions in an image that are likely to contain
objects, and if high object recall can be achieved with a modest number of object proposals (like one hundred), signiﬁcant
speed-ups over the sliding window approach can be gained,
allowing the use of more sophisticated classiﬁers. Detection
proposals are usually used as a pre-processing step, limiting the number of regions that need to be evaluated by the
detector, and should have the following characteristics:
1. High recall, which can be achieved with only a few proposals;
2. Accurate localization, such that the proposals match the
object bounding boxes as accurately as possible; and
3. Low computational cost.
The success of object detection based on detection proposals
 has attracted
broad interest . A comprehensive review
of object proposal algorithms is beyond the scope of this
13 We use the terminology detection proposals, object proposals and
region proposals interchangeably.
paper, because object proposals have applications beyond
object detection . We refer interested readers to the
recent surveys which
provide in-depth analysis of many classical object proposal
algorithms and their impact on detection performance. Our
interest here is to review object proposal methods that are
based on DCNNs, output class agnostic proposals, and are
related to generic object detection.
In 2014, the integration of object proposals and DCNN features
 led to the milestone RCNN in generic object detection. Since then,
detection proposal has quickly become a standard preprocessing step, based on the fact that all winning entries in
the PASCAL VOC , ILSVRC and MS COCO object
detection challenges since 2014 used detection proposals
 .
Among object proposal approaches based on traditional
low-level cues (e.g., color, texture, edge and gradients),
Selective Search , MCG and EdgeBoxes are among
the more popular. As the domain rapidly progressed, traditional object proposal approaches , which were
adopted as external modules independent of the detectors,
International Journal of Computer Vision
became the speed bottleneck of the detection pipeline . An emerging class of object proposal algorithms
 using
DCNNs has attracted broad attention.
Recent DCNN based object proposal methods generally
fall into two categories: bounding box based and object
segment based, with representative methods summarized in
Bounding Box Proposal Methods are best exempliﬁed by
the RPC method of Ren et al. , illustrated in Fig. 19.
RPN predicts object proposals by sliding a small network
over the feature map of the last shared CONV layer. At each
sliding window location, k proposals are predicted by using
k anchor boxes, where each anchor box14 is centered at some
location in the image, and is associated with a particular scale
and aspect ratio. Ren et al. proposed integrating RPN
and Fast RCNN into a single network by sharing their convolutional layers, leading to Faster RCNN, the ﬁrst end-to-end
detection pipeline. RPN has been broadly selected as the
proposal method by many state-of-the-art object detectors,
as can be observed from Tables 7 and 8.
Instead of ﬁxing a priori a set of anchors as MultiBox
 and RPN , Lu et al. proposed generating anchor locations
by using a recursive search strategy which can adaptively
guide computational resources to focus on sub-regions likely
to contain objects. Starting with the whole image, all regions
visited during the search process serve as anchors. For any
anchor region encountered during the search procedure, a
scalar zoom indicator is used to decide whether to further partition the region, and a set of bounding boxes with objectness
scores are computed by an Adjacency and Zoom Network
(AZNet), which extends RPN by adding a branch to compute the scalar zoom indicator in parallel with the existing
Further work attempts to generate object proposals by
exploiting multilayer convolutional features. Concurrent
with RPN , Ghodrati et al. proposed DeepProposal, which generates object proposals by
using a cascade of multiple convolutional features, building
an inverse cascade to select the most promising object locations and to reﬁne their boxes in a coarse-to-ﬁne manner.
An improved variant of RPN, HyperNet 
designs Hyper Features which aggregate multilayer convolutional features and shares them both in generating proposals
and detecting objects via an end-to-end joint training strategy. Yang et al. proposed CRAFT which also used
a cascade strategy, ﬁrst training an RPN network to generate
object proposals and then using them to train another binary
Fast RCNN network to further distinguish objects from back-
14 The concept of “anchor” ﬁrst appeared in Ren et al. .
ground. Li et al. proposed ZIP to improve RPN by
predicting object proposals with multiple convolutional feature maps at different network depths to integrate both low
level details and high level semantics. The backbone used in
ZIP is a “zoom out and in” network inspired by the conv and
deconv structure .
Finally, recent work which deserves mention includes
Deepbox , which proposed a lightweight
CNN to learn to rerank proposals generated by EdgeBox, and
DeNet which introduces
bounding box corner estimation to predict object proposals
efﬁciently to replace RPN in a Faster RCNN style detector.
Object Segment Proposal Methods Pinheiro et al. ,
Pinheiro et al. aim to generate segment proposals that
are likely to correspond to objects. Segment proposals are
more informative than bounding box proposals, and take a
step further towards object instance segmentation . In addition,
usinginstancesegmentationsupervisioncanimprovetheperformance of bounding box object detection. The pioneering
work of DeepMask, proposed by Pinheiro et al. , segments proposals learnt directly from raw image data with a
deep network. Similarly to RPN, after a number of shared
convolutional layers DeepMask splits the network into two
branches in order to predict a class agnostic mask and an
associated objectness score. Also similar to the efﬁcient sliding window strategy in OverFeat ,
the trained DeepMask network is applied in a sliding window manner to an image (and its rescaled versions) during
inference. More recently, Pinheiro et al. proposed
SharpMask by augmenting the DeepMask architecture with
a reﬁnement module, similar to the architectures shown in
Fig. 17 (b1) and (b2), augmenting the feed-forward network with a top-down reﬁnement process. SharpMask can
efﬁciently integrate spatially rich information from early featureswithstrongsemanticinformationencodedinlaterlayers
to generate high ﬁdelity object masks.
Motivated by Fully Convolutional Networks (FCN) for
semantic segmentation and DeepMask
 proposed Instance-
FCN to generate instance segment proposals. Similar to
DeepMask, the InstanceFCN network is split into two fully
convolutional branches, one to generate instance sensitive
score maps, the other to predict the objectness score. Hu et al.
 proposed FastMask to efﬁciently generate instance
segment proposals in a one-shot manner, similar to SSD , in order to make use of multiscale convolutional
features. Sliding windows extracted densely from multiscale
convolutional feature maps were input to a scale-tolerant
attentional head module in order to predict segmentation
masks and objectness scores. FastMask is claimed to run
at 13 FPS on 800 × 600 images.
International Journal of Computer Vision
Table 9 Summary of object proposal methods using DCNN. Bold values indicates the number of object proposals
Proposer name
Detector tested
Recall@IoU (VOC07)
Detection results (mAP)
 
Highlights
Bounding box object proposal methods
 
0.96 (1000)
0.84 (1000)
0.15 (1000)
37.8 (500)
Use a lightweight CNN
to learn to rerank
proposals generated
by EdgeBox. Can run
at 0.26s per image. Do
not share features for
RPN 
Faster RCNN
0.97 (300)
0.98 (1000)
0.79 (300)
0.84 (1000)
0.04 (300)
0.04 (1000)
73.2 (300)
70.4 (300)
21.9 (300)
The ﬁrst to generate
object proposals by
sharing full image
convolutional features
with detection. Most
widely used object
proposal method.
Signiﬁcant
improvements in
detection speed
DeepProposal
(Ghodrati et al.
0.74 (100)
0.92 (1000)
0.58 (100)
0.80 (1000)
0.12 (100)
0.16 (1000)
53.2 (100)
Generate proposals
inside a DCNN in a
multiscale manner.
Share features with the
detection network
CRAFT 
Faster RCNN
0.98 (300)
0.90 (300)
0.13 (300)
Introduced a
classiﬁcation network
(i.e. two class Fast
RCNN) cascade that
comes after the RPN.
Not sharing features
extracted for detection
International Journal of Computer Vision
Table 9 continued
Proposer name
Detector tested
Recall@IoU (VOC07)
Detection results (mAP)
 
Highlights
AZNet (Lu et al.
0.91 (300)
0.71 (300)
0.11 (300)
Use coarse-to-ﬁne
search: start from large
regions, then
recursively search for
subregions that may
contain objects.
Adaptively guide
computational
resources to focus on
likely subregions
ZIP (Li et al.
Inception v2
Faster RCNN
Generate proposals
using conv-deconv
network with
multilayers; Proposed
a map attention
decision (MAD) unit
to assign the weights
for features from
different layers
(TychsenSmith
and Petersson
0.82 (300)
0.74 (300)
0.48 (300)
73.9 (07++12)
A lot faster than Faster
RCNN; Introduces a
bounding box corner
estimation for
predicting object
proposals efﬁciently to
replace RPN; Does not
require predeﬁned
International Journal of Computer Vision
Table 9 continued
Proposer name
Detector tested
Box proposals (AR, COCO)
Segment proposals (AR, COCO)
 
Highlights
Segment proposal methods
(Pinheiro et al.
0.33 (100), 0.48 (1000)
0.26 (100), 0.37 (1000)
First to generate object
mask proposals with
DCNN; Slow
inference time; Need
segmentation
annotations for
training; Not sharing
features with detection
network; Achieved
mAP of 69.9% (500)
with Fast RCNN
InstanceFCN
(Dai et al.
0.32 (100), 0.39 (1000)
Combines ideas of FCN
 and
DeepMask .
Introduces instance
sensitive score maps.
Needs segmentation
annotations to train the
 
0.39 (100), 0.53 (1000)
0.30 (100), 0.39 (1000)
Leverages features at
multiple convolutional
layers by introducing a
top-down reﬁnement
module. Does not
share features with
detection network.
Needs segmentation
annotations for
FastMask 
0.43 (100), 0.57 (1000)
0.32 (100), 0.41 (1000)
Generates instance
segment proposals
efﬁciently in one-shot
manner similar to SSD
 . Uses
multiscale
convolutional features.
Uses segmentation
annotations for
The detection results on COCO are based on mAP@IoU[0.5, 0.95], unless stated otherwise
International Journal of Computer Vision
Fig. 19 Illustration of the region proposal network (RPN) introduced
in Ren et al. 
9 Other Issues
Data Augmentation Performing data augmentation for learning DCNNs is generally recognized to be important for visual
recognition. Trivial data augmentation refers to perturbing
an image by transformations that leave the underlying category unchanged, such as cropping, ﬂipping, rotating, scaling,
translating, color perturbations, and adding noise. By artiﬁcially enlarging the number of samples, data augmentation
helps in reducing overﬁtting and improving generalization.
It can be used at training time, at test time, or both. Nevertheless, it has the obvious limitation that the time required
for training increases signiﬁcantly. Data augmentation may
synthesize completely new training images , however it is hard to guarantee that the synthetic images generalize well to real ones. Some researchers
 proposed augmenting datasets by pasting real segmented objects into natural
images; indeed, Dvornik et al. showed that appropriately modeling the visual context surrounding objects is
crucial to place them in the right environment, and proposed
a context model to automatically ﬁnd appropriate locations
on images to place new objects for data augmentation.
Novel Training Strategies Detecting objects under a wide
range of scale variations, especially the detection of very
small objects, stands out as a key challenge. It has been shown
 that image resolution
has a considerable impact on detection accuracy, therefore
scaling is particularly commonly used in data augmentation,
since higher resolutions increase the possibility of detecting
small objects . Recently, Singh et al.
proposed advanced and efﬁcient data argumentation methods SNIP and SNIPER to 1 illustrate the scale invariance problem, as summarized in Table 10. Motivated by the intuitive understanding
that small and large objects are difﬁcult to detect at smaller
and larger scales, respectively, SNIP introduces a novel training scheme that can reduce scale variations during training,
but without reducing training samples; SNIPER allows for
efﬁcient multiscale training, only processing context regions
around ground truth objects at the appropriate scale, instead
of processing a whole image pyramid. Peng et al. 
studied a key factor in training, the minibatch size, and
proposed MegDet, a Large MiniBatch Object Detector, to
enable the training with a much larger minibatch size than
before (from 16 to 256). To avoid the failure of convergence
and signiﬁcantly speed up the training process, Peng et al.
 proposed a learning rate policy and Cross GPU Batch
Normalization, and effectively utilized 128 GPUs, allowing
MegDet to ﬁnish COCO training in 4 hours on 128 GPUs,
and winning the COCO 2017 Detection Challenge.
Reducing Localization Error In object detection, the Intersection Over Union15 (IOU) between a detected bounding
box and its ground truth box is the most popular evaluation metric, and an IOU threshold (e.g. typical value of 0.5)
is required to deﬁne positives and negatives. From Fig. 13,
in most state of the art detectors 
object detection is formulated as a multitask learning problem,i.e.,jointlyoptimizingasoftmaxclassiﬁerwhichassigns
object proposals with class labels and bounding box regressors, localizing objects by maximizing IOU or other metrics
between detection results and ground truth. Bounding boxes
are only a crude approximation for articulated objects, consequently background pixels are almost invariably included
in a bounding box, which affects the accuracy of classiﬁcation and localization. The study in Hoiem et al. 
shows that object localization error is one of the most inﬂuential forms of error, in addition to confusion between similar
objects. Localization error could stem from insufﬁcient overlap (smaller than the required IOU threshold, such as the
green box in Fig. 20) or duplicate detections (i.e., multiple
overlapping detections for an object instance). Usually, some
post-processing step like NonMaximum Suppression (NMS)
 is used for eliminating duplicate detections. However, due to misalignments the
bounding box with better localization could be suppressed
during NMS, leading to poorer localization quality (such as
the purple box shown in Fig. 20). Therefore, there are quite
a few methods aiming at improving detection performance
by reducing localization error.
MRCNN introduces iterative bounding box regression, where an RCNN is applied
several times. CRAFT and AttractioNet
 use a multi-stage detection
sub-network to generate accurate proposals, to forward to
Fast RCNN. Cai and Vasconcelos proposed Cascade RCNN, a multistage extension of RCNN, in which a
sequence of detectors is trained sequentially with increasing
15 Please refer to Sect. 4.2 for more details on the deﬁnition of IOU.
International Journal of Computer Vision
Table 10 Representative methods for training strategies and class imbalance handling
Detector name
Region proposal
Backbone DCNN
Pipelined used
VOC07 results
VOC12 results
COCO results
 
Highlights
MegDet 
ResNet50+FPN
FasterRCNN
Allow training with
much larger minibatch
size than before by
introducing cross GPU
batch normalization;
Can ﬁnish the COCO
training in 4 hours on
128 GPUs and
achieved improved
accuracy; Won
COCO2017 detection
SNIP 
DPN +DCN
 
ResNet101+DCN
Faster RCNN
An efﬁcient multiscale
training strategy.
Process context
regions around
ground-truth instances
at the appropriate scale
 
78.9 (07+12)
76.3 (07++12)
A simple and effective
Online Hard Example
Mining algorithm to
improve training of
region based detectors
International Journal of Computer Vision
Table 10 continued
Detector name
Region proposal
Backbone DCNN
Pipelined used
VOC07 results
VOC12 results
COCO results
 
Highlights
(Ouyang et al.
Identify the imbalance
in the number of
samples for different
object categories;
divide-and-conquer
feature learning
Chained Cascade
Vasconcelos
Inceptionv2
Fast RCNN,
Faster RCNN
80.4 (07+12)
Jointly learn DCNN and
multiple stages of
cascaded classiﬁers.
Boost detection
accuracy on PASCAL
VOC 2007 and
ImageNet for both fast
RCNN and Faster
RCNN using different
region proposal
Cascade RCNN
Vasconcelos
VGG ResNet101
Faster RCNN
Jointly learn DCNN and
multiple stages of
cascaded classiﬁers,
which are learned
using different
localization accuracy
for selecting positive
samples. Stack
bounding box
regression at multiple
RetinaNet 
ResNet101 +FPN
Propose a novel Focal
Loss which focuses
training on hard
examples. Handles
well the problem of
imbalance of positive
and negative samples
when training a
one-stage detector
Results on COCO are reported with Test Dev. The detection results on COCO are based on mAP@IoU[0.5, 0.95]
International Journal of Computer Vision
Fig. 20 Localization error could stem from insufﬁcient overlap or
duplicate detections. Localization error is a frequent cause of false positives (Color ﬁgure online)
IOU thresholds, based on the observation that the output of a
detector trained with a certain IOU is a good distribution to
train the detector of the next higher IOU threshold, in order to
be sequentially more selective against close false positives.
This approach can be built with any RCNN-based detector,
and is demonstrated to achieve consistent gains (about 2 to
4 points) independent of the baseline detector strength, at a
marginal increase in computation. There is also recent work
 
formulating IOU directly as the optimization objective, and
in proposing improved NMS results , such as Soft NMS and learning
NMS .
Class Imbalance Handling Unlike image classiﬁcation,
object detection has another unique problem: the serious
imbalance between the number of labeled object instances
and the number of background examples (image regions
not belonging to any object class of interest). Most background examples are easy negatives, however this imbalance
can make the training very inefﬁcient, and the large number of easy negatives tends to overwhelm the training. In
the past, this issue has typically been addressed via techniques such as bootstrapping . More
recently, this problem has also seen some attention . Because
the region proposal stage rapidly ﬁlters out most background
regions and proposes a small number of object candidates,
this class imbalance issue is mitigated to some extent in
two-stage detectors , although example mining
approaches, such as Online Hard Example Mining (OHEM)
 , may be used to maintain a reasonable balance between foreground and background. In the
case of one-stage object detectors , this imbalance is extremely serious (e.g. 100,000
background examples to every object). Lin et al. 
proposed Focal Loss to address this by rectifying the Cross
Entropy loss, such that it down-weights the loss assigned
to correctly classiﬁed examples. Li et al. studied
this issue from the perspective of gradient norm distribution,
and proposed a Gradient Harmonizing Mechanism (GHM) to
handle it.
10 Discussion and Conclusion
Generic object detection is an important and challenging
problem in computer vision and has received considerable
attention. Thanks to remarkable developments in deep learning techniques, the ﬁeld of object detection has dramatically
evolved. As a comprehensive survey on deep learning for
generic object detection, this paper has highlighted the recent
achievements, provided a structural taxonomy for methods
according to their roles in detection, summarized existing
populardatasetsandevaluationcriteria,anddiscussedperformance for the most representative methods. We conclude this
review with a discussion of the state of the art in Sect. 10.1,
an overall discussion of key issues in Sect. 10.2, and ﬁnally
suggested future research directions in Sect. 10.3.
10.1 State of the Art Performance
A large variety of detectors has appeared in the last few
years, and the introduction of standard benchmarks, such as
PASCAL VOC , ImageNet
 and COCO , has
made it easier to compare detectors. As can be seen from
our earlier discussion in Sects. 5–9, it may be misleading
to compare detectors in terms of their originally reported
performance (e.g. accuracy, speed), as they can differ in
fundamental / contextual respects, including the following
• Meta detection frameworks, such as RCNN , Fast RCNN , Faster RCNN
 , RFCN , Mask RCNN
 , YOLO and SSD
 ;
• Backbone networks such as VGG , Inception , ResNet , ResNeXt , and Xception etc. listed in Table 6;
• Innovations such as multilayer feature combination ,
deformable convolutional networks ,
deformable RoI pooling , heavier heads ,
and lighter heads ;
International Journal of Computer Vision
• PretrainingwithdatasetssuchasImageNet , COCO , Places , JFT and Open Images
 ;
• Different detection proposal methods and different numbers of object proposals;
• Train/test data augmentation, novel multiscale training
strategies 
etc, and model ensembling.
Although it may be impractical to compare every recently
proposed detector, it is nevertheless valuable to integrate
representative and publicly available detectors into a common platform and to compare them in a uniﬁed manner.
There has been very limited work in this regard, except for
Huang’s study of the three main families of detectors [Faster RCNN , RFCN
 and SSD ] by varying the
backbone network, image resolution, and the number of box
proposals.
As can be seen from Tables 7, 8, 9, 10, 11, we have summarized the best reported performance of many methods on
three widely used standard benchmarks. The results of these
methods were reported on the same test benchmark, despite
their differing in one or more of the aspects listed above.
Figures 3 and 21 present a very brief overview of the state
of the art, summarizing the best detection results of the PAS-
CAL VOC, ILSVRC and MSCOCO challenges; more results
can be found at detection challenge websites . The competition
winner of the open image challenge object detection task
achieved 61.71% mAP in the public leader board and 58.66%
mAP on the private leader board, obtained by combining the
detection results of several two-stage detectors including Fast
RCNN , Faster RCNN , FPN
 , Deformable RCNN , and
CascadeRCNN(CaiandVasconcelos2018).Insummary,the
backbone network, the detection framework, and the availability of large scale datasets are the three most important
factors in detection accuracy. Ensembles of multiple models,
the incorporation of context features, and data augmentation
all help to achieve better accuracy.
In less than 5 years, since AlexNet was proposed, the Top5 error on ImageNet classiﬁcation with 1000 classes has dropped
from 16% to 2%, as shown in Fig. 15. However, the mAP of
the best performing detector on COCO
 , trained to detect only 80 classes, is only
at 73%, even at 0.5 IoU, illustrating how object detection
is much harder than image classiﬁcation. The accuracy and
robustness achieved by the state-of-the-art detectors far from
satisﬁes the requirements of real world applications, so there
remains signiﬁcant room for future improvement.
10.2 Summary and Discussion
With hundreds of references and many dozens of methods
discussed throughout this paper, we would now like to focus
on the key factors which have emerged in generic object
detection based on deep learning.
(1) Detection frameworks: two stage versus one stage
In Sect. 5 we identiﬁed two major categories of detection
frameworks:regionbased(twostage)anduniﬁed(onestage):
• When large computational cost is allowed, two-stage
detectors generally produce higher detection accuracies
than one-stage, evidenced by the fact that most winning
approaches used in famous detection challenges like are
predominantly based on two-stage frameworks, because
their structure is more ﬂexible and better suited for region
based classiﬁcation. The most widely used frameworks
are Faster RCNN , RFCN and Mask RCNN .
• It has been shown in Huang et al. that the detection accuracy of one-stage SSD is less
sensitive to the quality of the backbone network than representative two-stage frameworks.
• One-stage detectors like YOLO and
SSD are generally faster than two-stage
ones, because of avoiding preprocessing algorithms,
using lightweight backbone networks, performing prediction with fewer candidate regions, and making the
classiﬁcation subnetwork fully convolutional. However,
two-stage detectors can run in real time with the introduction of similar techniques. In any event, whether one
stage or two, the most time consuming step is the feature
extractor (backbone network) .
• It has been shown that one-stage frameworks like
YOLO and SSD typically have much poorer performance
when detecting small objects than two-stage architectures like Faster RCNN and RFCN, but are competitive
in detecting large objects.
There have been many attempts to build better (faster, more
accurate, or more robust) detectors by attacking each stage
of the detection framework. No matter whether one, two or
multiple stages, the design of the detection framework has
converged towards a number of crucial design choices:
• A fully convolutional pipeline
• Exploring complementary information from other correlated tasks, e.g., Mask RCNN 
• Sliding windows 
International Journal of Computer Vision
Table 11 Summary of properties and performance of milestone detection frameworks for generic object detection
Detector name
Backbone DCNN
Input ImgSize
VOC07 results
VOC12 results
Speed (FPS)
 
Source code
Highlights and Disadvantages
Region based (Sect. 5.1)
RCNN 
Caffe Matlab
Highlights: First to integrate CNN
with RP methods; Dramatic
performance improvement over
previous state of the artP
Disadvantages: Multistage
pipeline of sequentially-trained
(External RP computation, CNN
ﬁnetuning, each warped RP
passing through CNN, SVM and
BBR training); Training is
expensive in space and time;
Testing is slow
SPPNet 
AlexNet VGGM
70.0 (VGG)
68.4 (VGG)
Caffe Python
Highlights: First to enable
end-to-end detector training
(ignoring RP generation); Design
a RoI pooling layer; Much faster
and more accurate than SPPNet;
No disk storage required for
feature caching
Disadvantages: External RP
computation is exposed as the
new bottleneck; Still too slow for
real time applications
International Journal of Computer Vision
Table 11 continued
Detector name
Backbone DCNN
Input ImgSize
VOC07 results
VOC12 results
Speed (FPS)
 
Source code
Highlights and Disadvantages
Faster RCNN
(Ren et al.
73.2 (VGG)
70.4 (VGG)
Caffe Matlab
Highlights: Propose RPN for
generating nearly cost-free and
high quality RPs instead of
selective search; Introduce
translation invariant and
multiscale anchor boxes as
references in RPN; Unify RPN
and Fast RCNN into a single
network by sharing CONV
layers; An order of magnitude
faster than Fast RCNN without
performance loss; Can run
testing at 5 FPS with VGG16
Disadvantages: Training is
complex, not a streamlined
process; Still falls short of real
RCNN⊖R (Lenc
and Vedaldi
ZFNet +SPP
Highlights: Replace selective
search with static RPs; Prove the
possibility of building integrated,
simpler and faster detectors that
rely exclusively on CNN
Disadvantages: Falls short of real
time; Decreased accuracy from
RFCN (Dai et al.
80.5 (07+12)
(07+12+CO)
77.6 (07++12)
(07++12+CO)
Caffe Matlab
Highlights: Fully convolutional
detection network; Design a set
of position sensitive score maps
using a bank of specialized
CONV layers; Faster than Faster
RCNN without sacriﬁcing much
Disadvantages: Training is not a
streamlined process; Still falls
short of real time
International Journal of Computer Vision
Table 11 continued
Detector name
Backbone DCNN
Input ImgSize
VOC07 results
VOC12 results
Speed (FPS)
 
Source code
Highlights and Disadvantages
Mask RCNN 
ResNeXt101
50.3 (ResNeXt101) (COCO Result)
Caffe Matlab
Highlights: A simple, ﬂexible, and
effective framework for object
instance segmentation; Extends
Faster RCNN by adding another
branch for predicting an object
mask in parallel with the existing
branch for BB prediction;
Feature Pyramid Network (FPN)
is utilized; Outstanding
performance
Disadvantages: Falls short of real
time applications
Uniﬁed (Sect. 5.2)
 
GoogLeNet like
66.4 (07+12)
57.9 (07++12)
< 25 (VGG)
Highlights: First efﬁcient uniﬁed
detector; Drop RP process
completely; Elegant and efﬁcient
detection framework;
Signiﬁcantly faster than previous
detectors; YOLO runs at 45 FPS,
Fast YOLO at 155 FPS;
Disadvantages: Accuracy falls far
behind state of the art detectors;
Struggle to localize small objects
International Journal of Computer Vision
Table 11 continued
Detector name
Backbone DCNN
Input ImgSize
VOC07 results
VOC12 results
Speed (FPS)
 
Source code
Highlights and Disadvantages
 
78.6 (07+12)
73.5 (07++12)
Highlights: Propose a faster
DarkNet19; Use a number of
existing strategies to improve
both speed and accuracy;
Achieve high accuracy and high
speed; YOLO9000 can detect
over 9000 object categories in
Disadvantages: Not good at
detecting small objects
SSD (Liu et al.
76.8 (07+12)
(07+12+CO)
74.9 (07++12)
(07++12+CO)
Caffe Python
Highlights: First accurate and
efﬁcient uniﬁed detector;
Effectively combine ideas from
RPN and YOLO to perform
detection at multi-scale CONV
layers; Faster and signiﬁcantly
more accurate than YOLO; Can
run at 59 FPS;
Disadvantages: Not good at
detecting small objects
See Sect. 5 for a detailed discussion. Some architectures are illustrated in Fig. 13. The properties of the backbone DCNNs can be found in Table 6
Training data: “07”←VOC2007 trainval; “07T”←VOC2007 trainval and test; “12”←VOC2012 trainval; “CO”←COCO trainval. The “Speed” column roughly estimates the detection speed with
a single Nvidia Titan X GPU
RP region proposal; SS selective search; RPN region proposal network; RC N N ⊖R RCNN minus R and used a trivial RP method
International Journal of Computer Vision
• Fusing information from different layers of the backbone.
The evidence from recent success of cascade for object detection and
instance segmentation on COCO and
other challenges has shown that multistage object detection
could be a future framework for a speed-accuracy trade-off.
A teaser investigation is being done in the 2019 WIDER
Challenge .
(2) Backbone networks
As discussed in Sect. 6.1, backbone networks are one of
the main driving forces behind the rapid improvement of
detection performance, because of the key role played by discriminative object feature representation. Generally, deeper
backbones such as ResNet , ResNeXt , InceptionResNet perform
better; however, they are computationally more expensive
and require much more data and massive computing for training.Somebackbones were proposed for focusing on speed
instead, such as MobileNet which has
been shown to achieve VGGNet16 accuracy on ImageNet
with only 1
30 the computational cost and model size. Backbone training from scratch may become possible as more
training data and better training strategies are available .
(3) Improving the robustness of object representation
The variation of real world images is a key challenge in object
recognition. The variations include lighting, pose, deformations, background clutter, occlusions, blur, resolution, noise,
and camera distortions.
(3.1) Object scale and small object size
Large variations of object scale, particularly those of small
objects, pose a great challenge. Here a summary and discussion on the main strategies identiﬁed in Sect. 6.2:
• Using image pyramids: They are simple and effective,
helping to enlarge small objects and to shrink large ones.
They are computationally expensive, but are nevertheless
commonly used during inference for better accuracy.
• Using features from convolutional layers of different
resolutions: In early work like SSD ,
predictions are performed independently, and no information from other layers is combined or merged. Now
it is quite standard to combine features from different
layers, e.g. in FPN .
Fig.21 Evolution of object detection performance on COCO (Test-Dev
results). Results are quoted from . The backbone network, the design of detection framework
and the availability of good and large scale datasets are the three most
important factors in detection accuracy
• Using dilated convolutions : A
simple and effective method to incorporate broader context and maintain high resolution feature maps.
• Using anchor boxes of different scales and aspect ratios:
Drawbacks of having many parameters, and scales and
aspect ratios of anchor boxes are usually heuristically
determined.
• Up-scaling:Particularlyforthedetectionofsmallobjects,
high-resolution networks can be
developed. It remains unclear whether super-resolution
techniques improve detection accuracy or not.
Despite recent advances, the detection accuracy for small
objects is still much lower than that of larger ones. Therefore, the detection of small objects remains one of the key
challenges in object detection. Perhaps localization requirements need to be generalized as a function of scale, since
certain applications, e.g. autonomous driving, only require
the identiﬁcation of the existence of small objects within a
larger region, and exact localization is not necessary.
(3.2) Deformation, occlusion, and other factors
As discussed in Sect. 2.2, there are approaches to handling geometric transformation, occlusions, and deformation
mainly based on two paradigms. The ﬁrst is a spatial
transformer network, which uses regression to obtain a
deformation ﬁeld and then warp features according to the
deformation ﬁeld . The second is based on
a deformable part-based model ,
International Journal of Computer Vision
which ﬁnds the maximum response to a part ﬁlter with spatial constraints taken into consideration .
Rotation invariance may be attractive in certain applications, but there are limited generic object detection work
focusing on rotation invariance, because popular benchmark
detection datasets (PASCAL VOC, ImageNet, COCO) do not
have large variations in rotation. Occlusion handling is intensively studied in face detection and pedestrian detection, but
very little work has been devoted to occlusion handling for
generic object detection. In general, despite recent advances,
deep networks are still limited by the lack of robustness to
a number of variations, which signiﬁcantly constrains their
real-world applications.
(4) Context reasoning
As introduced in Sect. 7, objects in the wild typically coexist
with other objects and environments. It has been recognized that contextual information (object relations, global
scene statistics) helps object detection and recognition , especially for small objects, occluded
objects, and with poor image quality. There was extensive
work preceding deep learning , and also quite a few
works in the era of deep learning . How to efﬁciently and effectively incorporate contextual information remains to be explored, possibly guided
by how human vision uses context, based on scene graphs
 , or via the full segmentation of objects and
scenes using panoptic segmentation .
(5) Detection proposals
Detection proposals signiﬁcantly reduce search spaces. As
recommended in Hosang et al. , future detection proposals will surely have to improve in repeatability, recall,
localization accuracy, and speed. Since the success of RPN
 , which integrated proposal generation and
detection into a common framework, CNN based detection
proposal generation methods have dominated region proposal. It is recommended that new detection proposals should
be assessed for object detection, instead of evaluating detection proposals alone.
(6) Other factors
As discussed in Sect. 9, there are many other factors affecting
object detection quality: data augmentation, novel training strategies, combinations of backbone models, multiple
detection frameworks, incorporating information from other
related tasks, methods for reducing localization error, handling the huge imbalance between positive and negative
samples, mining of hard negative samples, and improving
loss functions.
10.3 Research Directions
Despite the recent tremendous progress in the ﬁeld of object
detection, the technology remains signiﬁcantly more primitive than human vision and cannot yet satisfactorily address
real-world challenges like those of Sect. 2.2. We see a number
of long-standing challenges:
• Working in an open world: being robust to any number
of environmental changes, being able to evolve or adapt.
• Object detection under constrained conditions: learning
from weakly labeled data or few bounding box annotations, wearable devices, unseen object categories etc.
• Object detection in other modalities: video, RGBD
images, 3D point clouds, lidar, remotely sensed imagery
Based on these challenges, we see the following directions
of future research:
(1) Open World Learning The ultimate goal is to develop
object detection capable of accurately and efﬁciently recognizing and localizing instances in thousands or more object
categories in open-world scenes, at a level competitive with
the human visual system. Object detection algorithms are
unable, in general, to recognize object categories outside of
their training dataset, although ideally there should be the
ability to recognize novel object categories . Current detection datasets
 contain only a few dozen to hundreds of categories,
signiﬁcantly fewer than those which can be recognized by
humans. New larger-scale datasets with signiﬁcantly more categories will need to be developed.
(2) Better and More Efﬁcient Detection Frameworks One
of the reasons for the success in generic object detection has
been the development of superior detection frameworks, both
region-based[RCNN ,FastRCNN , Faster RCNN , Mask RCNN
 ] and one-stage detectors [YOLO , SSD ]. Region-based detectors
have higher accuracy, one-stage detectors are generally faster
and simpler. Object detectors depend heavily on the underlying backbone networks, which have been optimized for
image classiﬁcation, possibly causing a learning bias; learning object detectors from scratch could be helpful for new
detection frameworks.
International Journal of Computer Vision
(3) Compact and Efﬁcient CNN Features CNNs have
increased remarkably in depth, from several layers [AlexNet
 ] to hundreds of layers [ResNet
 , DenseNet ]. These
networks have millions to hundreds of millions of parameters, requiring massive data and GPUs for training. In order
reduce or remove network redundancy, there has been growing research interest in designing compact and lightweight
networks and network acceleration .
(4) Automatic Neural Architecture Search Deep learning
bypasses manual feature engineering which requires human
experts with strong domain knowledge, however DCNNs
require similarly signiﬁcant expertise. It is natural to consider automated design of detection backbone architectures,
such as the recent Automated Machine Learning (AutoML)
 , which has been applied to image
classiﬁcation and object detection .
(5) Object Instance Segmentation For a richer and more
detailed understanding of image content, there is a need to
tackle pixel-level object instance segmentation , which can play an
important role in potential applications that require the precise boundaries of individual objects.
(6) Weakly Supervised Detection Current state-of-theart detectors employ fully supervised models learned from
labeled data with object bounding boxes or segmentation
masks . However, fully supervised learning has serious limitations, particularly where the collection
of bounding box annotations is labor intensive and where the
number of images is large. Fully supervised learning is not
scalable in the absence of fully labeled training data, so it
is essential to understand how the power of CNNs can be
leveraged where only weakly / partially annotated data are
provided Few / Zero Shot Object Detection The success of deep
detectors relies heavily on gargantuan amounts of annotated
training data. When the labeled data are scarce, the performance of deep detectors frequently deteriorates and fails
to generalize well. In contrast, humans (even children) can
learn a visual concept quickly from very few given examples and can often generalize well . Therefore, the ability to learn
from only few examples, few shot detection, is very appealing
 . Even more constrained, zero shot object detection
localizes and recognizes object classes that have never been
seen16 before , essential for life-long learning machines
that need to intelligently and incrementally discover new
object categories.
(8) Object Detection in Other Modalities Most detectors
are based on still 2D images; object detection in other modalities can be highly relevant in domains such as autonomous
vehicles, unmanned aerial vehicles, and robotics. These
modalities raise new challenges in effectively using depth
 , video , and point clouds .
(9) Universal Object Detection: Recently, there has been
increasing effort in learning universal representations, those
which are effective in multiple image domains, such as natural images, videos, aerial images, and medical CT images
 . Most such research focuses on
image classiﬁcation, rarely targeting object detection , and developed detectors are usually domain speciﬁc. Object detection independent of image domain and
cross-domain object detection represent important future
directions.
The research ﬁeld of generic object detection is still far
from complete. However given the breakthroughs over the
past 5 years we are optimistic of future developments and
opportunities.
Acknowledgements Open access funding provided by University of
Oulu including Oulu University Hospital. The authors would like to
thank the pioneering researchers in generic object detection and other
related ﬁelds. The authors would also like to express their sincere appreciation to Professor Jiˇrí Matas, the associate editor and the anonymous
reviewers for their comments and suggestions. This work has been supported by the Center for Machine Vision and Signal Analysis at the
University of Oulu (Finland) and the National Natural Science Foundation of China under Grant 61872379.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material
in this article are included in the article’s Creative Commons licence,
unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the
permitteduse,youwillneedtoobtainpermissiondirectlyfromthecopyright holder. To view a copy of this licence, visit 
ons.org/licenses/by/4.0/.
16 Although side information may be provided, such as a wikipedia
page or an attributes vector.
International Journal of Computer Vision