IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
Cost-Effective Active Learning for Deep Image
Classiﬁcation
Keze Wang, Dongyu Zhang*, Ya Li, Ruimao Zhang, and Liang Lin, Senior Member, IEEE
Abstract—Recent successes in learning-based image classiﬁcation, however, heavily rely on the large number of annotated
training samples, which may require considerable human efforts.
In this paper, we propose a novel active learning framework,
which is capable of building a competitive classiﬁer with optimal
feature representation via a limited amount of labeled training
instances in an incremental learning manner. Our approach advances the existing active learning methods in two aspects. First,
we incorporate deep convolutional neural networks into active
learning. Through the properly designed framework, the feature
representation and the classiﬁer can be simultaneously updated
with progressively annotated informative samples. Second, we
present a cost-effective sample selection strategy to improve the
classiﬁcation performance with less manual annotations. Unlike
traditional methods focusing on only the uncertain samples of low
prediction conﬁdence, we especially discover the large amount of
high conﬁdence samples from the unlabeled set for feature learning. Speciﬁcally, these high conﬁdence samples are automatically
selected and iteratively assigned pseudo-labels. We thus call our
framework “ Cost-Effective Active Learning” (CEAL) standing
for the two advantages. Extensive experiments demonstrate that
the proposed CEAL framework can achieve promising results on
two challenging image classiﬁcation datasets, i.e., face recognition
on CACD database and object categorization on Caltech-256
Index Terms—Incremental learning, Active learning, Deep
neural nets, Image classiﬁcation.
I. INTRODUCTION
Aiming at improving the existing models by incrementally selecting and annotating the most informative unlabeled
samples, Active Learning (AL) has been well studied in
the past few decades , , , , , , , ,
 , , and applied to various kind of vision tasks, such
as image/video categorization , , , , ,
text/web classiﬁcation , , , image/video retrieval
 , , etc. In the AL methods , , , classiﬁer/model
is ﬁrst initialized with a relatively small set of labeled training
This work was supported in part by Guangdong Natural Science Foundation
under Grant S2013050014548 and 2014A030313201, in part by State Key
Development Program under Grant No 2016YFB1001000, and in part by the
Fundamental Research Funds for the Central Universities. This work was also
supported by Special Program for Applied Research on Super Computation
of the NSFC-Guangdong Joint Fund (the second phase). We would like to
thank Depeng Liang and Jin Xu for their preliminary contributions on this
project. We gratefully acknowledge the support of NVIDIA Corporation with
the donation of the Tesla K40 GPU used for this research.
Keze Wang, Dongyu Zhang, Ruimao Zhang and Liang Lin are with the
School of Data and Computer Science, Sun Yat-sen University, Guang Zhou.
E-mail: . (Corresponding author is Dongyu Zhang.)
Ya Li is with Guangzhou University, Guang Zhou.
Copyright (c) 2016 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to .
samples. Then it is continuously boosted by selecting and
pushing some of the most informative samples to user for annotation. Although the existing AL approaches , have
demonstrated impressive results on image classiﬁcation, their
classiﬁers/models are trained with hand-craft features (e.g.,
HoG, SIFT) on small scale visual datasets. The effectiveness
of AL on more challenging image classiﬁcation tasks has not
been well studied.
Recently, incredible progress on visual recognition tasks
has been made by deep learning approaches , .
With sufﬁcient labeled data , deep convolutional neural
networks(CNNs) , are trained to directly learn features from raw pixels which has achieved the state-of-the-art
performance for image classiﬁcation. However in many real
applications of large-scale image classiﬁcation, the labeled
data is not enough, since the tedious manual labeling process
requires a lot of time and labor. Thus it has great practical
signiﬁcance to develop a framework by combining CNNs and
active learning, which can jointly learn features and classi-
ﬁers/models from unlabeled training data with minimal human
annotations. But incorporating CNNs into active learning
framework is not straightforward for real image classiﬁcation
tasks. This is due to the following two issues.
• The labeled training samples given by current AL approaches are insufﬁcient for CNNs, as the majority unlabeled samples are usually ignored in active learning. AL
usually selects only a few of the most informative samples
(e.g., samples with quite low prediction conﬁdence) in
each learning step and frequently solicit user labeling.
Thus it is difﬁcult to obtain proper feature representations
by ﬁne-tuning CNNs with these minority informative
• The process pipelines of AL and CNNs are inconsistent
with each other. Most of AL methods pay close attention
to model/classiﬁer training. Their strategies to select
the most informative samples are heavily dependent on
the assumption that the feature representation is ﬁxed.
However, the feature learning and classiﬁer training are
jointly optimized in CNNs. Because of this inconsistency,
simply ﬁne-tuning CNNs in the traditional AL framework
may face the divergence problem.
Inspired by the insights and lessons from a signiﬁcant
amount of previous works as well as the recently proposed
technique, i.e., self-paced learning , , , , we
address above mentioned issues by cost-effectively combining
the CNN and AL via a complementary sample selection.
In particular, we propose a novel active learning framework
 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
Unlabeled Dataset
Auto Pseudo-Labeling
Human Labeling
Majority & Clearly Classified Samples
Minority & Most Informative Samples
Progressively
Illustration of our proposed CEAL framework. Our proposed CEAL progressively feeds the samples from the unlabeled dataset into the CNN. Then
both of the clearly classiﬁed samples and most informative samples selection criteria are applied on the classiﬁer output of the CNN. After adding user
annotated minority uncertain samples into the labeled set and pseudo-labeling majority certain samples, the model (feature representation and classiﬁer of the
CNN) are further updated.
called “Cost-Effective Active Learning” (CEAL), which is
enabled to ﬁne-tune the CNN with sufﬁcient unlabeled training
data and overcomes the inconsistency between the AL and
Different from the existing AL approaches that only consider the most informative and representative samples, our
CEAL proposes to automatically select and pseudo-annotate
unlabeled samples. As Fig. 1 illustrates, our proposed CEAL
progressively feeds the samples from the unlabeled dataset into
the CNN, and selects two kinds of samples for ﬁne-tuning
according to the output of CNN’s classiﬁers. One kind is the
minority samples with low prediction conﬁdence, called most
informative/uncertain samples. The predicted labels of samples
are most uncertainty. For the selection of these uncertain
samples, the proposed CEAL considers three common active
learning methods: Least conﬁdence , margin sampling
 and entropy . The selected samples are added into
the labeled set after active user labeling. The other kind is
the majority samples with high prediction conﬁdence, called
high conﬁdence samples. The predicted labels of samples are
most certainty. For this certain kind of samples, the proposed
CEAL automatically assigns pseudo-labels with no human
labor cost. As one can see that, these two kinds of samples are
complementary to each other for representing different conﬁdence levels of the current model on the unlabeled dataset.
In the model updating stage, all the samples in the labeled
set and currently pseudo-labeled high conﬁdence samples are
exploited to ﬁne-tuning the CNN.
The proposed CEAL advances in employing these two
complementary kinds of samples to incrementally improve the
model’s classiﬁer training and feature learning: the minority
informative kind contributes to train more powerful classiﬁers,
while the majority high conﬁdence kind conduces to learn
more discriminative feature representations. On one hand,
although the number is small, the most uncertainty unlabeled
samples usually have great potential impact on the classiﬁers.
Selecting and annotating them into training can lead to a
better decision boundary of the classiﬁers. On the other hand,
though unable to signiﬁcantly improve the performance of
classiﬁers, the high conﬁdence unlabeled samples are close
to the labeled samples in the CNN’s feature space. Thus
pseudo-labeling these majority high conﬁdence samples for
training is a reasonable data augmentation way for the CNN
to learn robust features. In particular, the number of the
high conﬁdence samples is actually much larger than that
of most uncertainty ones. With the obtained robust feature
representation, the inconsistency between the AL and CNN
can be overcome.
For the problem of keep the model stable in the training
stage, many works , are proposed in recent years inspired by the learning process of humans that gradually include
samples into training from easy to complex. Through this way,
the training samples for the further iterations are gradually
determined by the model itself based on what it has already
learned . In other words, the model can gradually select
the high conﬁdence samples as pseudo-labeled ones along
with the training process. The advantages of these related
studies motivate us to incrementally select unlabeled samples
in a easy-to-hard manner to make pseudo-labeling process
reliable. Speciﬁcally, considering the classiﬁcation model is
usually not reliable enough in the initial iterations, we employ
high conﬁdence threshold to deﬁne clearly classiﬁed samples
and assign them pseudo-labels. When the performance of the
classiﬁcation model improves, the threshold correspondingly
decreases.
The main contribution of this work
is threefold. First, to
the best of our knowledge, our work is the ﬁrst one addressing
the deep image classiﬁcation problems in conjunction with
active learning framework and convolutional neural networks
training. Our framework can be easily extended to other
similar visual recognition tasks. Second, this work also advances the active learning development, by introducing a costeffective strategy to automatically select and annotate the high
conﬁdence samples, which improves the traditional samples
selection strategies. Third, experiments on challenging CACD
 and Caltech 256 datasets show that our approach outperforms other methods not only in the classiﬁcation accuracy
but also in the reduction of human annotation.
The rest of the paper is organized as follows. Section II
presents a brief review of related work. Section III discusses
the component of our framework and the corresponding learning algorithm. Section IV presents the experiments results with
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
deep empirical analysis. Section V concludes the paper.
II. RELATED WORK
The key idea of the active learning is that a learning
algorithm should achieve greater accuracy with fewer labeled
training samples, if it is allowed to choose the ones from
which it learns . In this way, the instance selection scheme
is becoming extremely important. One of the most common
strategy is the uncertainty-based selection , , which
measures the uncertainties of novel unlabeled samples from
the predictions of previous classiﬁers. In , Lewis et al.
proposed to extract the sample, which has the largest entropy
on the conditional distribution over predicted labels, as the
most uncertain instance. The SVM-based method determined the uncertain samples based on the relative distance
between the candidate samples and the decision boundary.
Some earlier works , also determined the sample
uncertainty referring to a committee of classiﬁers (i.e. examining the disagreement among class labels assigned by a set of
classiﬁers). Such theoretically-motivated framework is called
query-by-committee in literature . All of above mentioned
uncertainty-based methods usually ignore the majority certain
unlabeled samples, and thus are sensitive to outliers. The
later methods have taken the information density measure
into account and exploited the information of unlabeled data
when selecting samples. These approaches usually sequentially
select the informative samples relying on the probability
estimation , or prior information to minimize the
generalization error of the trained classiﬁer over the unlabeled
data. For example, Joshi et al. considered the uncertainty
sampling method based on the probability estimation of class
membership for all the instance in the selection pool, and such
method can be effective to handle the multi-class case. In ,
some context constraints are introduced as the priori to guide
users to tag the face images more efﬁciently. At the same time,
a series of works , is proposed to take the samples
to maximize the increase of mutual information between the
candidate instance and the remaining unlabeled instances under the Gaussian Process framework. Li et al. presented
a novel adaptive active learning approach that combines an
information density measure and a most uncertainty measure
together to label critical instances for image classiﬁcations.
Moreover, the diversity of the selected instance over the certain
category has been taken into consideration in as well.
Such work is also the pioneer study expanding the SVM-based
active learning from the single mode to batch mode. Recently,
Elhamifar et al. further integrated the uncertainty and
diversity measurement into a uniﬁed batch mode framework
via convex programming for unlabeled sample selection. Such
approach is more feasible to conjunction with any type of
classiﬁers, but not limited in max-margin based ones. It
is obvious that all of the above mentioned active learning
methods only consider those low-conﬁdence samples (e.g.,
uncertain and diverse samples), but losing the sight of large
majority of high conﬁdence samples. We hold that due to
the majority and consistency, these high conﬁdence samples
will also be beneﬁcial to improve the accuracy and keep the
stability of classiﬁers. Even more, we shall demonstrate that
considering these high conﬁdence samples can also reduce the
user effort of annotation effectively.
III. COST-EFFECTIVE ACTIVE LEARNING
In this section, we develop an efﬁcient algorithm for the
cost-effective active learning (CEAL) framework.
Our objective is to apply our CEAL framework to deep image
classiﬁcation tasks by progressively selecting complementary
samples for model updating. Suppose we have a dataset of
m categories and n samples denoted as D = {xi}n
denote the currently annotated samples of D as DL while
the unlabeled ones as DU. The label of xi is denoted as
yi = j, j ∈{1, ..., m}, i.e., xi belongs to the jth category. We
should give two necessary remarks on our problem settings.
One is that in our investigated image classiﬁcation problems,
almost all data are unlabeled, i.e., most of {yi} of D is
unknown and needed to be completed in the learning process.
The other remark is that DU might possibly been inputted into
the system in an incremental way. This means that data scale
might be consistently growing.
Thanks to handling both manually annotated and automatically pseudo-labeled samples together, our proposed CEAL
model can progressively ﬁt the consistently growing unlabeled
data in such a holistic manner. The CEAL for deep image
classiﬁcation is formulated as follows:
{W,yi,i∈DU} −1
1{yi = j} log p(yi = j|xi; W), (1)
where 1{·} is the indicator function, so that 1{a true statement} = 1, and 1{a false statement} = 0, W denotes the
network parameters of the CNN. p(yi = j|xi; W) denotes
the softmax output of the CNN for the jth category, which
represents the probability of the sample xi belonging to the
jth classiﬁers.
The alternative search strategy is readily employed to optimize the above Eq. (1). Speciﬁcally, the algorithm is designed
by alternatively updating the pseudo-labeled sample yi ∈DU
and the network parameters W. In the following, we introduce
the details of the optimization steps, and give their physical
interpretations. The practical implementation of the CEAL will
also be discussed in the end.
A. Initialization.
Before the experiment starts, the labeled samples DL is
empty. For each class we randomly select few training samples
from DU and manually annotate them as the starting point to
initialize the CNN parameters W.
B. Complementary sample selection.
Fixing the CNN parameters W, we ﬁrst rank all unlabeled
samples according to the common active learning criteria, then
manually annotate those most uncertain samples and add them
into DL. For those most certain ones, we assign pseudo-labels
and denote them as DH.
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
Informative Sample Annotating: Our CEAL can use
in conjunction with any type of common actively learning
criteria, e.g., least conﬁdence , margin sampling and
entropy to select K most informative/uncertain samples
left in DU. The selection criteria are based on p(yi = j|xi; W)
which denotes the probability of xi belonging to the jth class.
Speciﬁcally, the three selection criteria are deﬁned as follows:
1) Least conﬁdence: Rank all the unlabeled samples in an
ascending order according to the lci value. lci is deﬁned
p(yi = j|xi; W).
If the probability of the most probable class for a sample
is low then the classiﬁer is uncertain about the sample.
2) Margin sampling: Rank all the unlabeled samples in
an ascending order according to the msi value. msi is
deﬁned as:
msi = p(yi = j1|xi; W) −p(yi = j2|xi; W),
where j1 and j2 represent the ﬁrst and second most
probable class labels predicted by the classiﬁers. The
smaller of the margin means the classiﬁer is more
uncertain about the sample.
3) Entropy: Rank all the unlabeled samples in an descending order according to their eni value. eni is deﬁned
p(yi = j|xi; W) log p(yi = j|xi; W). (4)
This method takes all class label probabilities into consideration to measure the uncertainty. The higher entropy
value, the more uncertain is the sample.
High Conﬁdence Sample Pseudo-labeling: We select the
high conﬁdence samples from DU, whose entropy is smaller
than the threshold δ. Then we assign clearly predicted pseudolabels to them. The pseudo-label yi is deﬁned as:
j∗= arg max
p(yi = j|xi; W),
otherwise.
where yi = 1 denotes that xi is regarded as high conﬁdence
sample. The selected samples are denoted as DH. Note that
compared with classiﬁcation probability p(yi = j∗|xi; W)
for the j∗th category, the employed entropy eni holistically
considers the classiﬁcation probability of the other categories,
i.e., the selected sample should be clearly classiﬁed with high
conﬁdence. The threshold δ is set to a large value to guarantee
a high reliability of assigning a pseudo-label.
C. CNN ﬁne-tuning
Fixing the labels of self-labeled high conﬁdence samples
DH and manually annotated ones DL by active user, the
Eq. (1) can be simpliﬁed as:
1{yi = j} log p(yi = j|xi; W),
Algorithm 1 Learning Algorithm of CEAL
Unlabeled samples DU, initially labeled samples DL, uncertain samples selection size K, high conﬁdence samples
selection threshold δ, threshold decay rate dr, maximum
iteration number T, ﬁne-tuning interval t.
CNN parameters W.
1: Initialize W with DL.
not reach maximum iteration T do
Add K uncertainty samples into DL based on Eq. (2)
or (3) or (4),
Obtain high conﬁdence samples DH based on Eq. (5)
In every t iterations:
Update W via ﬁne-tuning according to Eq. (6) with
Update δ according to Eq. (9)
6: end while
7: return W
where N denotes the number of samples in DH ∪DL. We
employ the standard back propagation to update the CNN’s
parameters W. Speciﬁcally, let L denote the loss function of
Eq. (6), then the partial derive of the network parameter W
according to Eq. (6) is:
j=1 1{yi = j} log p(yi = j|xi; W)
1{yi = j}∂log p(yi = j|xi; W)
(1{yi = j} −p(yi = j|xi; W))∂zj(xi; W)
where {zj(xi; W)}m
j=1 denotes the activation for the ith sample of the last layer of CNN model before feeding into the
softmax classiﬁer, which is deﬁned as:
p(yi = j|xi; W) =
t=1 ezt(xi;W)
After ﬁne-tuning we put the high conﬁdence samples DH
back to DU and erase their pseudo-label.
D. Threshold updating
As the incremental learning process goes on, the classiﬁcation capability of classiﬁer improves and more high conﬁdence
samples are selected, which may result in the decrease of
incorrect auto annotation. In order to guarantee the reliability
of high conﬁdence sample selection, at the end of each
iteration t, we update the high conﬁdence sample selection
threshold by setting
where δ0 is the initial threshold, dr controls the threshold
decay rate.
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
We demonstrate the effectiveness of our proposed heuristic deep active learning framework on face recognition and object categorization. The ﬁrst
and second line: sample images from the Caltech-256 dataset. The last line: samples images from the Cross-Age Celebrity Dataset .
The entire algorithm can be then summarized into Algorithm
1. It is easy to see that this alternative optimizing
strategy ﬁnely accords with the pipeline of the proposed CEAL
framework.
IV. EXPERIMENTS
A. Datasets and Experiment settings
1) Dataset Description: In this section, we evaluate our
cost-effective active learning framework on two public challenging benchmarks, i.e., the Cross-Age Celebrity face recognition Dataset (CACD) and the Caltech-256 object categorization Dataset (see Figure 2). CACD is a large-scale
and challenging dataset for face identiﬁcation and retrieval
problems. It contains more than 160, 000 images of 2, 000
celebrities, which are varying in age, pose, illumination, and
occlusion. Since not all of the images are annotated, we adopt
a subset of 580 individuals from the whole dataset in our
experiments, in which, 200 individuals are originally annotated
and 380 persons are extra annotated by us. Especially, 6, 336
images of 80 individuals are utilized for pre-training the
network and the remaining 500 persons are used to perform
the experiments. Caltech-256 is a challenging object categories
dataset. It contains a total of 30, 607 images of 256 categories
collected from the Internet.
2) Experiment setting: For CACD, we utilize the method
proposed in to detect the facial points and align the faces
based on the eye locations. We resize all the faces into 200 ×
150, then we set the parameters: δ0 = 0.05, dr = 0.0033
and K = 2000. For Caltech-256, we resize all the images to
256 × 256 and we set δ0 = 0.005, dr = 0.00033 and K =
1000. Following the settings in the existing active learning
method , we randomly select 80% images of each class to
form the unlabeled training set, and the rest are as the testing
set in our experiments. Among the unlabeled training set, we
randomly select 10% samples of each class to initialize the
network and the rest are for incremental learning process. To
get rid of the inﬂuence of randomness, we average 5 times
execution results as the ﬁnal result.
We use different network architectures for CACD and
Caltech256 datasets because the difference between face
and object is relatively large. Table I shows the overall network
architecture for CACD experiments and table II shows the
THE DETAILED CONFIGURATION OF THE CNN ARCHITECTURE USED IN
CACD . IT TAKES THE 200 × 150 × 3 IMAGES AS INPUT AND
GENERATES THE 500-WAY SOFTMAX OUTPUT FOR CLASSES PREDICTION.
THE RELU ACTIVATION FUNCTION IS NOT SHOWN FOR BREVITY.
layer type
kernel size/stride
output size
convolution
98 × 73 × 32
48 × 36 × 32
48 × 36 × 32
convolution(padding2)
48 × 36 × 64
23 × 17 × 64
23 × 17 × 64
convolution(padding1)
23 × 17 × 96
fc(dropout50%)
1 × 1 × 1536
fc(dropout50%)
1 × 1 × 1536
1 × 1 × 500
THE DETAILED CONFIGURATION OF THE CNN ARCHITECTURE USED IN
CALTECH-256 . IT TAKES THE 256 × 256 × 3 IMAGES AS INPUT WHICH
WILL BE RANDOMLY CROPPED INTO 227 × 227 DURING THE TRAINING
AND GENERATES THE 256-WAY SOFTMAX OUTPUT FOR CLASSES
PREDICTION. THE RELU ACTIVATION FUNCTION IS NOT SHOWN FOR
layer type
kernel size/stride
output size
convolution
55 × 55 × 96
27 × 27 × 96
27 × 27 × 96
convolution(padding2)
27 × 27 × 256
13 × 13 × 256
13 × 13 × 256
convolution(padding1)
13 × 13 × 384
convolution(padding1)
13 × 13 × 384
convolution(padding1)
13 × 13 × 256
6 × 6 × 256
fc(dropout50%)
1 × 1 × 4096
fc(dropout50%)
1 × 1 × 4096
1 × 1 × 256
overall network architecture for Caltech-256 experiments. We
use the Alexnet as the network architecture for Caltech-
256 and using the ImageNet ILSVRC dataset pre-trained
model as the starting point following the setting of . Then
we keep all layers ﬁxed and just modify the last layer to
be the 256-way softmax classiﬁer to perform the Caltech-256
experiments. We employ Caffe for CNN implementation.
For CACD, we set the learning rates of all the layers as 0.01.
For Caltech-256, we set the learning rates of all the layers as
0.001 except for the softmax layer which is set to 0.01. All
the experiments are conducted on a common desktop PC with
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
percentage of labeled samples
percentage of labeled samples
Classiﬁcation accuracy under different percentages of annotated samples of the whole training set on (a) CACD and (b) Caltech-256 datasets. Our
proposed method CEAL MS performs consistently better than the compared TCAL and AL RAND.
CLASS ACCURACY PER SOME SPECIFIC AL ITERATIONS ON THE CACD AND CALTECH-256 DATASET.
Training iteration
Percentage of labeled samples
(b) Caltech-256
Training iteration
Percentage of labeled samples
an intel 3.8GHz CPU and a Titan X GPU. Average 17 hours
are needed to ﬁnish training on the CACD dataset with 44708
3) Comparison Methods: To demonstrate that our proposed
CEAL framework can improve the classiﬁcation performance
with less labeled data, we compare CEAL with a new
state-of-the-art active learning (TCAL) and baseline methods
(AL ALL and AL RAND):
• AL ALL: We manually label all the training samples and
use them to train the CNN. This method can be regarded
as the upper bound (best performance that CNN can reach
with all labeled training samples).
• AL RAND: During the training process, we randomly
select samples to be annotated to ﬁne-tune the CNN. This
method discards all active learning techniques and can be
considered as the lower bound.
• Triple Criteria Active Learning (TCAL) : TCAL is
a comprehensive active learning approach and is well
designed to jointly evaluate sample selection criteria
(uncertainty, diversity and density), and has overcome
the state-of-the-art methods with much less annotations.
TCAL represents those methods who intend to mine minority informative samples to improve the performance.
Thus, we regard it as a relevant competitor.
Implementation Details. The compared methods share the
same CNN architecture with our CEAL on the both datasets.
The only difference in the sample selection criteria. For the
BaseLine method, we select all training samples to ﬁne-tune
the CNN, i.e. all labels are used. For TCAL, we follow the
pipeline of by training a SVM classiﬁer and then applying
the uncertainty, diversity and density criteria to select the most
informative samples. Speciﬁcally, the uncertainty of samples
is assessed according to the margin sampling strategy. The
diversity is calculated by clustering the most uncertain samples
via k-means with histogram intersection kernel. The density
of one sample is measured by calculating the average distance
with other samples within a cluster it belonged to. For each
cluster, the highest density (i.e., the smallest average distance)
sample is selected as the most informative sample. For CACD,
we cluster 2000 most uncertain samples and select 500 most
informative samples according to above mentioned diversity
and density. For Caltech-256, we select 250 most informative
samples from 1000 most uncertain samples. To make a fair
comparison, samples selected in each iteration by the TCAL
are also used to ﬁne-tune the CNN to learn the optimal feature
representation as AL RAND. Once optimal feature learned,
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
percentage of labeled samples
percentage of labeled samples
percentage of labeled samples
percentage of labeled samples
percentage of labeled samples
percentage of labeled samples
Extensive study for different informative sample selection criteria on CACD (the ﬁrst row) and Caltech-256 (the second row) datasets. These criteria
includes least conﬁdence (LC, the ﬁrst column), margin sampling (MS, the second column) and entropy (EN, the third column). One can observe that our
CEAL framework works consistently well on the common information sample selection criteria.
percentage of labeled samples
CEAL_FUSION
percentage of labeled samples
CEAL_FUSION
Comparison between different informative sample selection criteria and their fusion (CEAL FUSION) on CACD (left) and Caltech-256 (right)
the SVM classiﬁer of TCAL is further updated.
B. Comparison Results and Empirical Analysis
1) Comparison Results:
To demonstrate the effectiveness of our proposed framework, we also apply margin sampling criterion to measure the uncertainty of samples and denote this method as CEAL MS. Fig. 3 illustrates the accuracy-percentage of annotated samples curve of
AL RAND, AL ALL, TCAL and the proposed CEAL MS
on both CACD and Caltech-256 datasets. This curve demonstrates the classiﬁcation accuracy under different percentages
of annotated samples of the whole training set.
As illustrated in Fig. 3, Tab. III(a) and Tab. III(b), our
proposed CEAL framework overcomes the compared method
from the aspects of the recognition accuracy and user annotation amount. From the aspect of recognition accuracy, given
the same percentage of annotated samples, our CEAL MS
outperforms the compared method in a clear margin, especially when the percentage of annotated samples is low.
From the aspect of the user annotation amount, to achieve
91.5% recognition accuracy on the CACD dataset, AL RAND
and TCAL require 99% and 81% labeled training samples,
respectively. CEAL MS needs only 63% labeled samples and
reduces around 36% and 18% user annotations, compared to
AL RAND and TCAL. To achieve the 73.8% accuracy on
the caltech-256 dataset, AL RAND and TCAL require 97%
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
and 93% labeled samples, respectively. CEAL MS needs only
78% labeled samples and reduces around 19% and 15% user
annotations, compared to AL RAND and TCAL. This justiﬁes
that our proposed CEAL framework can effectively reduce the
need of labeled samples.
From above results, one can see that our proposed frame
CEAL performs consistently better than the state-of-the-art
method TCAL in both recognition accuracy and user annotation amount through fair comparisons. This is due to that
TCAL only mines minority informative samples and is not
able to provide sufﬁcient training data for feature learning
under deep image classiﬁcation scenario. Hence, our CEAL
has a competitive advantage in deep image classiﬁcation task.
To clearly analyze our CEAL and justify the effectiveness of
its component, we have conducted the several experiments and
discussed in the following subsections.
2) Component Analysis: To justify that the proposed CEAL
can works consistently well on the common informative
sample selection criteria, we implement three variants of
CEAL according to least conﬁdence (LC), margin sampling
(MS) and entropy (EN) to assess uncertain samples. These
three variants are denoted as CEAL LC, CEAL MS and
CEAL EN. Meanwhile, to show the raw performance of
these criteria, we discard the cost-effective high conﬁdence
sample selection of above mentioned variants and denoted the
discarded versions as AL LC, AL MS and AL EN. To clarify
the contribution of our proposed pseudo-labeling majority high
conﬁdence sample strategy, we further introduce this strategy
into the AL RAND and denote this variant as CEAL RAND.
Since AL RAND means randomly select samples to be annotated, CEAL RAND reﬂects the original contribution of the
pseudo-labeled majority high conﬁdence sample strategy, i.e.,
CEAL RAND denotes the method who only uses pseudolabeled majority samples.
Fig. 4 illustrates the results of these variants on dataset
CACD (the ﬁrst row) and Caltech-256 (the second row). The
results demonstrate that giving the same percentage of labeled
samples and compared with AL RAND, CEAL RAND, simply exploiting pseudo-labeled majority samples, obtains similar performance gain as AL LC, AL MS and AL EN, which
employs informative sample selection criterion. This justiﬁes
that our proposed pseudo-labeling majority sample strategy
is effective as some common informative sample selection
criteria. Moreover, as one can see that in Fig. 4, CEAL LC,
CEAL MS and CEAL EN all consistently outperform the
pure pseudo-labeling samples version CEAL RAND and their
excluding pseudo-labeled samples versions AL LC, AL MS
and AL EN in a clear margin on both the CACD and Caltech-
256 datasets, respectively. This validates that our proposed
pseudo-labeling majority sample strategy is complementary
to the common informative sample selection criteria and can
further signiﬁcantly improve the recognition performance.
To analyze the choice of informative sample selection criteria, we have made a comparison among three above mentioned
criteria. We also make an attempt to simply combine them
together. Speciﬁcally, in each iteration, we select top K/2
samples according to each criterion respectively. Then we
remove repeated ones (i.e., some samples may be selected
#iteration
average error rate
CACD error rate
Caltech-256 error rate
The average error rate of the pseudo-labels of high conﬁdence
samples assigned by the heuristic strategy on CACD and Caltech-256 datasets
experiments. The vertical axes represent the average error rate and the horizontal axes represent the learning iteration. Our proposed CEAL framework
can assign reliable pseudo-labels to the unlabeled samples under acceptable
average error rate
by the three criteria at the same time) from obtained 3K/2
samples. After removing the repeated samples, we randomly
select K samples from them to require user annotations. We
denote this method as CEAL FUSION.
Fig. 5 illustrates that CEAL LC, CEAL MS and CEAL EN
have similar performance while CEAL FUSION performs
better. This demonstrates that informative sample selection criterion still plays an important role in improving the recognition
accuracy. Though being a minority, the informative samples
have great potential impact on the classiﬁer.
C. Reliability of CEAL
From the above experiments, we know that the performance
of our framework is better from other methods, which shows
the superiority of introducing the majority pseudo-labeled
samples. But how does the accuracy of assigning the pseudolabel to those high conﬁdence samples? In order to demonstrate the reliability of our proposed CEAL framework, we
also evaluate the average error in selecting high conﬁdence
samples. Fig. 6 shows the error rate of assigning pseudo-label
along with the learning iteration. As one can see that, the
average error rate is quite low (say less than 3% on the CACD
dataset and less than 5.5% on the Caltech-256 dataset) even
at early iterations. Hence, our proposed CEAL framework can
assign reliable pseudo-labels to the unlabeled samples under
acceptable average error rate along with the learning iteration.
D. Sensitivity of High Conﬁdence Threshold
Since the training phase of deep convolutional neural
networks is time-consuming, it is not affordable to employ
a try and error approach to set the threshold for deﬁning high
conﬁdence samples. We further analyze the sensitivity of the
threshold parameters δ (threshold) and dr (threshold decay
rate) on our system performance on CACD dataset using the
CEAL EN. While analyzing the sensitivity of parameter δ on
our system, we ﬁx the decrease rate dr to 0.0033. We ﬁx
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
percentage of annotated samples
percentage of annotated samples
The sensitivity analysis of heuristic threshold δ (the ﬁrst row) and
decay rate dr (the second row). One can observe that these parameters do not
substantially affect the overall system performance.
the threshold δ to 0.05 when analyzing the sensitivity of dr.
Results of the sensitivity analysis of δ (range 0.045 to 0.1) is
shown in the ﬁrst row of Fig. 7, while the sensitivity analysis
of dr (range 0.001 to 0.0035) is shown in the second row of
Fig. 7. Note that the test range of δ and dr is set to ensure the
majority high conﬁdence assumption of this paper. Though
the range of {δ, dr} seems to be narrow from the value, it
leads to the signiﬁcant difference: about 10% 60% samples
are pseudo-labeled in high conﬁdence sample selection. Lower
standard deviation of the accuracy in Fig. 7 proves that the
choice of these parameters does not signiﬁcantly affect the
overall system performance.
V. CONCLUSIONS
In this paper, we propose a cost-effective active learning
framework for deep image classiﬁcation tasks, which employs
a complementary sample selection strategy: Progressively
select minority most informative samples and pseudo-label
majority high conﬁdence samples for model updating. In
such a holistic manner, the minority labeled samples beneﬁt
the decision boundary of classiﬁer and the majority pseudolabeled samples provide sufﬁcient training data for robust
feature learning. Extensive experiment results on two public challenging benchmarks justify the effectiveness of our
proposed CEAL framework. In future works, we plan to
apply our framework on more challenging large-scale object
recognition tasks (e.g., 1000 categories in ImageNet). And we
plan to incorporate more persons from the CACD dataset to
evaluate our framework. Moreover, we plan to generalize our
framework into other multi-label object recognition tasks (e.g.,
20 categories in PASCAL VOC).