Proceedings of The 11th International Natural Language Generation Conference, pages 463–471,
Tilburg, The Netherlands, November 5-8, 2018. c⃝2018 Association for Computational Linguistics
E2E NLG Challenge: Neural Models vs. Templates
Yevgeniy Puzikov and Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
Research Training Group AIPHES
www.ukp.tu-darmstadt.de
E2E NLG Challenge is a shared task on
generating restaurant descriptions from sets
of key-value pairs. This paper describes the
results of our participation in the challenge.
We develop a simple, yet effective neural
encoder-decoder model 1 which produces
ﬂuent restaurant descriptions and outperforms a strong baseline. We further analyze
the data provided by the organizers and conclude that the task can also be approached
with a template-based model developed in
just a few hours.
Introduction
Natural Language Generation (NLG) is the task of
generating natural language utterances from structured data representations. The E2E NLG Challenge2 is a shared task which focuses on end-toend data-driven NLG methods. These approaches
attract a lot of attention, because they perform joint
learning of textual structure and surface realization
patterns from non-aligned data, which allows for a
signiﬁcant reduction of the amount of human annotation effort needed for NLG corpus creation .
The contribution of our submission to the challenge can be summarized as follows: (1) we show
how exploiting data properties allows us to design
more accurate neural architectures; (2) we develop
a simple template-based system which achieves
performance comparable to neural approaches.
1 
2 
InteractionLab/E2E
name[The Eagle]
eatType[coffee shop]
food[French]
priceRange[moderate]
customerRating[3/5]
area[riverside]
kidsFriendly[yes]
near[Burger King]
Human Natural Language Reference:
“The three star coffee shop, The Eagle, gives families a midpriced dining experience featuring a variety of wines and
cheeses. Find The Eagle near Burger King.”
Figure 1: E2E NLG Challenge data speciﬁcation.
Task Deﬁnition
The organizers of the shared task provided a crowdsourced data set of 50k instances in the restaurant
domain . Each training
instance consists of a dialogue act-based meaning
representation (MR) and up to 16 references in
natural language (Figure 1).
The data was collected using pictorial representations as stimuli, with the intention of creating
more natural, informative and diverse human references compared to the ones one might generate
from textual inputs.
The task is to generate an utterance from a given
MR, which is both similar to human-generated reference texts and highly rated by humans. Similarity is assessed using standard evaluation metrics: BLEU , NIST , METEOR ,
ROUGE-L , CIDEr . However, the ﬁnal assessment is done via
human ratings obtained using a mixture of crowdsourcing and expert annotations.
To facilitate a better assessment of the proposed
approaches, the organizing team used TGen , one of the recent E2E datadriven systems, as a baseline. It is a sequence-tosequence neural system with attention . TGen uses beam search for decod-
ing, incorporates a reranker over the top k outputs,
penalizing the candidates that do not verbalize all
attributes from the input MR. TGen also includes a
delexicalization module which deals with sparsely
occurring MR attributes (name, near) by mapping
such values to placeholder tokens when preprocessing the input data, and substituting the placeholders
with actual values as a post-processing step.
Our Approach
This section describes two different approaches we
developed for the shared task.
The ﬁrst one (Model-D, for “data-driven”) is an
encoder-decoder neural system which is similar to
TGen, but uses a more efﬁcient encoder module.
The second approach is a simple template-based
model (Model-T, for “template-based”) which we
developed based on the results of the data analysis.
Model-D was motivated by two important properties of the E2E NLG Challenge data:
• ﬁxed number of unique MR attributes
• low diversity of the lexical instantiations of
the MR attribute values
Each input MR contains a ﬁxed number of
unique attributes (between three and eight), which
allows us to associate a positional id with each attribute and omit the corresponding attribute names
(or keys) from the encoding procedure. This shortens the encoded sequence, presumably making the
learning procedure easier for the encoder. This also
uniﬁes the lengths of input MRs and thus allows
us to use simpler and more efﬁcient neural networks which are not sequential and process input
sequences in one step (e.g. multilayer perceptron
(MLP) networks).
One might argue that using an MLP would be
complicated by the fact that neither the number
of active (non-null value) input MR keys nor the
number of tokens constituting the corresponding
values is ﬁxed. For example, an MR key price
may have a one-token value of “low” or a more
lengthy “less than £10”. However, realizations of
the MR attribute values exhibit low variability: six
out of eight keys have less than seven unique values,
while the remaining two keys (name, near) denote
named entities and thus are easy to delexicalize.
This allows us to treat each value as a single token,
customerRating
familyFriendly
priceRange
Table 1: Input representation of the running example using positional ids.
even if it consists of multiple words (e.g. “more
than £30”, “Fast food”).
Each predicted output is a textual description of a
restaurant. As reported by Novikova et al. ,
the average number of words per reference is 20.1.
We used the value of 50 as a cut-off threshold,
ﬁltering out training instances with long restaurant
descriptions.
The overall architecture of our model is shown
in Figure 2. The system is an encoder-decoder
model 
consisting of three main modules: an embedding
matrix, one dense hidden layer as an encoder and
a RNN-based decoder with gated recurrent units
(GRU) .
Let us ﬁrst describe the input speciﬁcations of
the model. We will use the following MR instance
as a running example:
name[Wrestlers]
customerRating[high]
familyFriendly[yes]
Considering the alphabetic ordering of the MR
key names, we can assign positional ids to the keys
as shown in Table 1. The remaining ﬁve keys are
assigned dummy PAD values.
Given an instance of a (MR, text) pair, we decompose the MR into eight components (mrj in Figure 2), each corresponding to a value for a unique
MR key, and add an end-of-sentence symbol (EOS)
to denote the end of the encoded sequence. Each
component is represented as a high-dimensional
embedding vector. Each embedding vector is further mapped to a dense hidden representation via
an afﬁne transformation followed by a ReLu function. These hidden representations are further used by the decoder network,
which in our case is a unidirectional GRU-based
RNN with an attention module . The decoder is initialized with an average
of the encoder outputs.
The decoder generates a sequence of tokens, one
name[The Bakers], food[English], . . .
Embed Lookup
Context vector
Input padded
Embeddings
Figure 2: Schematic view of the neural network architecture (Model-D).
token at a time, until it predicts the EOS token. Our
model employs the greedy search decoding strategy
and does not use any reranker module.
Taking into consideration low lexical variation of
the MR attribute values, one might be interested
in whether it is possible to design a deterministic
NLG system to tackle the task. We examined the
ways MR attribute keys and values are verbalized
in the training data and discovered that the majority
of textual descriptions follow a similar ordering of
MR attribute verbalizations:
[name] is a [familyFriendly] [eatType]
which serves [food] food in the [price]
price range. It has a [customerRating]
customer rating. It is located in the
[area] area, near [near].
Here [X] denotes the value of the MR key X.
This pattern became a central template of Model-T.
Not all MR attribute verbalizations ﬁt into this
schema. For example, a key-value pair customer-
Rating[3 out of 5] would be verbalized as “...has
a 3 out of 5 customer rating”, which is not the best
phrasing one can come up with. A better way to
describe it is “. ..has a customer rating of 3 out of
5”. We incorporate such variations into Model-T
with a set of simple rules which modify the general
template depending on a speciﬁc value of an MR
attribute.
As mentioned in Section 2.1, each instance’s input can have up to eight MR attributes. In order
to account for this fact, we decomposed the general template into smaller components, each corresponding to a speciﬁc MR attribute mentioned in
the input. We further developed a set of rules which
activate each component depending on whether an
MR attribute is part of the input. For example, if
0.7128 ± 0.013
8.5020 ± 0.092
2.4432 ± 0.088
0.7378 ± 0.015
0.4770 ± 0.012
Table 2: Evaluation results according to automatic
metrics (development set).
price is not in the set of input MR attributes, then
the general template becomes:
[name] is a [familyFriendly] [eatType]
which serves [food] food. It has a
[customerRating] customer rating.
It is located in the [area] area,
near [near].
Finally, we also add a simple post-processing
step to handle speciﬁc punctuation and article
Metric Evaluation
Table 2 shows the results of metric evaluation of
the systems. Since we were provided with only
one TGen prediction ﬁle and a single performance
score, comparing score distributions is not possible
and statistical signiﬁcance tests are not meaningful due to the non-deterministic nature of the approaches based on neural networks and randomized
training procedures .
In order to facilitate a fair comparison with other
competing systems, we report the mean development score of Model-D (averaged across twenty
runs with different random seeds) and performance
variance for each automatic metric. Model-T is a
deterministic system, so it is sufﬁcient to report the
results of a single run.
The results show that Model-D outperforms
Error type
dropped contents
punctuation errors
modiﬁed contents
bad grammar
Table 3: Common errors made by the compared
models (100 randomly sampled development instances).
TGen as measured by all ﬁve metrics, albeit the performance variance is quite large. Model-T clearly
scores below both TGen and Model-D. This is
expected, since Model-T is not data-driven, and
hence the texts it generates might be different from
the reference outputs.
Previous studies have shown that widely used
automatic metrics (including the ones used in our
competition) lack strong correlation with human
judgments . We decided to examine the predictions made by the compared systems
on one hundred randomly sampled input instances,
focusing on generic errors, which make sense to
look out for in many NLG scenarios. Table 3 shows
the error types and the number of mistakes found in
each of the prediction ﬁles. The error types should
be self-explanatory (sample predictions are given
in Appendix A.2).
As far as the (subjective) manual analysis goes,
Model-T outputs descriptions with the best linguistic quality. Table 3 shows that the predictions of
the template-based system contain no errors – this
is because we incorporated our notion of grammaticality into the templates’ deﬁnition, which allowed
Model-T to avoid the errors found in predictions of
the other two approaches.
The majority of errors made by Model-D are
either wrong verbalizations of the input MR values or punctuation mistakes. The latter ones are
limited to the cases of missing a comma between
clauses or not ﬁnishing a sentence with a full stop.
An easy solution to this problem is adding a postprocessing step which ﬁxes punctuation mistakes
before outputting the text.
Crucially, Model-D often drops or modiﬁes
some MR attribute values. According to the organizers, 40% of the data by design contain either
additional or omitted information on the output
side : crowd workers were
allowed to not lexicalize attribute values which they
deemed unimportant. We decided to examine the
training data and ﬁnd out if the discrepancies of
Model-D were learned from the data.
Training Data Analysis
The E2E NLG Challenge is based on noisy data,
but the organizers provided multiple instances to
account for this noise. In order to better understand
the behaviour of Model-D and determine if it took
advantage of having multiple references per training instance, we have randomly sampled a hundred
training instances and manually checked their linguistic quality. Table 4 shows the most common
errors we encountered.
Most mistakes come from ungrammatical constructions, e.g.
incorrect phrase attachment
decisions (“The price of the food is high and
is located ...”),
incorrect usage of articles
(“located in riverside”), repetitive constructions
(“Cotto, an Indian coffee shop located in ..., is
an Indian coffee shop ...”). Some restaurant descriptions follow a tweet-style narration pattern
which is understandable, but ungrammatical (“The
Golden Palace Italian riverside coffee shop price
range moderate and customer rating 1 out of 5”).
A considerable number of instances have restaurant descriptions which contain information that
does not entirely follow from the given input MR.
These are cases in which input content elements
are modiﬁed or dropped, which goes in line with
what we observed in the outputs of Model-D.
Some instances (10%) contained descriptions
which we marked as questionable due to pragmatic
and/or stylistic considerations. For example, restaurants which have familyFriendly[no] as part of the
input MR are often described by crowd workers as
“adults-only” establishments, which has an undesirable connotation. Finally, it is necessary to mention that some crowd workers followed inconsistent
spelling and punctuation rules when hyphenating
compound modiﬁers (“family friendly restaurant”,
“the restaurant is family friendly”) or capitalizing
MR attributes (“Riverside”, “Fast food”). Punctuation errors were mainly restricted to missing a full
stop at the end of a restaurant description or failing
to delimit sentence clauses with commas.
The results of manual data analysis show that
Model-D indeed generates texts that are similar to
the restaurant descriptions in the provided data set.
Unfortunately, our data-driven approach is not ﬂexible enough to make use of multiple references; it
cannot cancel out the noise present in some train-
Error type
bad grammar
“it’s French food falls within a high price range”
modiﬁed contents
area[riverside] →“city centre”
dropped contents
priceRange[high] →∅
questionable lexicalization
“Adult-only Chinese restaurant, The Waterman, offers top-rated
food in the city centre”
punctuation errors
“X is a coffee shop and also a Japanese restaurant great for
family and close to Crowne Plaza Hotel”
Table 4: Data annotation discrepancies (100 randomly sampled training instances).
Best result
Metric evaluation
Human evaluation
0.228/(2.0, 4.0)/2
0.300/(1.0, 1.0)/1
Naturalness
0.077/(5.0, 10.0)/2
0.211/(1.0, 1.0)/1
Table 5: Final evaluation results on the test set. Human evaluation results have the following format:
score/(range)/cluster.
ing instances. One way of alleviating this problem
could be reformulating the loss function to inform
the system about the existence of multiple ways of
generating a good restaurant description. Given a
training instance, Model-D would generate a corresponding candidate text which could be compared
to all human references. Each comparison results
in computing a certain cost; the gradients could
be then computed on the minimal cost among all
comparisons.
Final Evaluation
For the ﬁnal submission we have chosen Model-T’s
predictions – despite lower metric scores, they contained most grammatical outputs and kept all input
information in the generated text.
The results of the ﬁnal evaluation on the test data
are presented in Table 5. They were produced by
the TrueSkill algorithm ,
which performs pairwise system comparisons and
clusters them into groups. For completeness, we include the highest reported scores among all the participants (rightmost column). Note, however, that
the numerical scores are not directly interpretable,
but the relative ranking of a system in terms of its
range and cluster is important – systems within one
cluster are considered tied.
Model-T was assigned to the second best cluster
both in terms of quality and naturalness, despite
the much lower metric scores. Retrospectively, this
justiﬁes our decision to choose Model-T instead
of Model-D for the ﬁnal submission. The E2E
NLG Challenge focuses on end-to-end data-driven
NLG methods, which is why systems like Model-T
might not exactly ﬁt into the task setup. Nevertheless, we view such a system as a necessary candidate for comparison, since the E2E NLG Challenge
data was designed to learn models that produce
“more natural, varied and less template-like system
utterances” .
Conclusion
In this paper we have presented the results of our
participation in the E2E NLG Challenge. We have
developed two conceptually different approaches
and analyzed their performance, both in quantity
and in quality. We have shown that sometimes the
costs of developing complex data-driven models
are not justiﬁed and one is better off approaching
the problem with simpler techniques. We hope that
our observations and conclusions shed some light
on the limitations of modern NLG approaches and
possible ways of overcoming them.
Acknowledgments
This work was supported by the German Research
Foundation (DFG) under grant No. GU 798/17-1
and the DFG-funded research training group “Adaptive Preparation of Information form Heterogeneous Sources” . The ﬁrst
author of the paper is supported by the FAZIT Foundation scholarship. We thank the anonymous reviewers and our colleagues Michael Bugert, Tristan
Miller, Maxime Peyrard, Nils Reimers and Markus
Zopf who provided insightful comments and suggestions that greatly assisted our research.