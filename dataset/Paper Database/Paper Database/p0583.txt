Deep Active Learning for Efﬁcient Training of a LiDAR 3D Object Detector
Di Feng1,2, Xiao Wei1,3, Lars Rosenbaum1, Atsuto Maki3, Klaus Dietmayer2
Abstract— Training a deep object detector for autonomous
driving requires a huge amount of labeled data. While recording
data via on-board sensors such as camera or LiDAR is relatively
easy, annotating data is very tedious and time-consuming, especially when dealing with 3D LiDAR points or radar data. Active
learning has the potential to minimize human annotation efforts
while maximizing the object detector’s performance. In this
work, we propose an active learning method to train a LiDAR
3D object detector with the least amount of labeled training data
necessary. The detector leverages 2D region proposals generated
from the RGB images to reduce the search space of objects
and speed up the learning process. Experiments show that our
proposed method works under different uncertainty estimations
and query functions, and can save up to 60% of the labeling
efforts while reaching the same network performance.
Keywords— Deep neural network, active learning, uncertainty estimation, object detection, autonomous driving
©2019 IEEE, to appear in the 30th IEEE Intelligent Vehicles Symposium.
I. INTRODUCTION
In recent years deep learning has set the benchmark for
object detection tasks on many open datasets (e.g. KITTI ,
Cityscapes ), and has become the de-facto method for
perception in autonomous driving. Despite its high performance, training a deep object detector usually requires a
huge amount of labeled samples. The annotation process is
tedious and time-consuming work, especially for 3D LiDAR
points (as discussed in ), necessitating the development of
methods to reduce labeling efforts. Furthermore, a common
way to optimize a deep object detector is to feed all training
samples into the network with random shufﬂing. However,
the informativeness of each training sample differs, i.e. some
are more informative and contribute more to the performance
gain, while some others are less informative. A more efﬁcient training strategy is to optimize the network with only
the most informative samples. This is speciﬁcally helpful
when adapting an object detector to new driving scenarios
which are different from the previous training set, e.g. from
highway to urban scenarios.
Active learning is a training strategy to reduce human
annotation efforts while maximizing the performance of a
machine learning model (usually in a supervised-learning
1 Robert Bosch GmbH, Corporate Research, Driver Assistance Systems
and Automated Driving, 71272 Renningen, Germany.
2 Institute of Measurement, Control and Microtechnology, Ulm University, 89081 Ulm, Germany.
3 School of Electrical Engineering and Computer Science, KTH Royal
Institute of Technology, 100 44 Stockholm, Sweden.
Estimating
uncertainty
Querying data
LiDAR points
Object detector
Unlabeled data pool
Human annotator
LiDAR Frustum
Fig. 1: Our proposed active learning method to efﬁciently
train a LiDAR 3D object detector. The detector is based
on 2D proposals from images, which serve as seeds to
locate objects as frustums in LiDAR 3D space. We assume
that there exists a large unlabeled data pool of LiDAR
point clouds. The object detector iteratively uses predictive
uncertainty to quantify the informativeness of each sample
in the unlabeled data pool, queries the human annotator for
the class label and 3D geometrical information of objects,
and updates the training set with the newly-labeled data. We
validate our method both with “perfect” image proposals
provided by human annotators, or by an on-the-shelf pretrained image detector with high recall rate.
fashion) . In active learning, a model iteratively evaluates the informativeness of unlabeled data, selects the most
informative samples to be labeled by human annotators,
and updates the training set with the newly-labeled data.
Active learning has long been applied to Support Vector
Machines (SVM) or Gaussian Processes (GP) – , and
has only recently been used in deep learning for classiﬁcation
of medical images or hyperspectral images in remote
sensing , 2D image detection , , road-scene image
segmentation , and natural language processing .
In this work, we propose an active learning method to
efﬁciently train a LiDAR 3D object detector for autonomous
driving, as in Fig. 1. We assume that there exists a large
unlabeled data pool of LiDAR point clouds, because it is
relatively easy to collect and prepare LiDAR data using a
test vehicle. We use the network’s predictive uncertainty to
 
quantify the informativeness of each sample in the unlabeled
data pool, and assume that the network can iteratively query
the human annotator for the class label and 3D geometrical
information of objects. Furthermore, as it is much easier
to do human labeling with 2D RGB images than 3D point
clouds and a lot of pre-trained image detectors with high
recall rate exist (e.g. Detectron ), we propose to leverage
an image detector to provide 2D object proposals which serve
as seeds to locate objects, so that the human annotator only
needs to label LiDAR points within frustums (see Fig. 1).
In this way, the 3D labeling efforts can be further reduced
and the speed of learning process can be increased. In the
experiments, we evaluate our method either by assuming
a “perfect” image detector which provides accurate object
proposals, or by an on-the-shelf pre-trained image detector.
Results show our method outperforms the baseline method
in both experimental settings.
Our contributions can be summarized as follows: (1) We
propose a deep active learning method to signiﬁcantly reduce
the labeling efforts for training a 3D object detector using
LiDAR points. To our knowledge, ours is the ﬁrst attempt
to introduce deep active learning for the 3D environment
perception on autonomous driving. (2) Our method leverages
the 2D object proposals from RGB images, which reduces
the search space of objects of interests and speeds up the
learning process. (3) We compare several approaches for
quantifying uncertainties in the neural network, and study
their efﬁciencies to query informative unlabeled data.
II. RELATED WORKS
In this section, we brieﬂy summarize existing works on
deep object detection for autonomous driving using LiDAR
points as well as deep active learning.
A. Object Detection for Autonomous Driving using LiDAR
Most driverless cars are equipped with multiple sensors,
such as cameras and LiDARs. Therefore, many methods
have been proposed for object detection using camera images – , LiDAR point clouds – , or the fusion
of both to exploit their complementary properties – .
State-of-the-art deep object detection networks follow two
pipelines: the two-stage and the one-stage object detections.
In the former pipeline, several object candidates called regions of interest (ROI) or region proposals (RP) are extracted
from a scene. Then, these candidates are veriﬁed and reﬁned
in terms of classiﬁcation scores and locations. For example,
Asvadi et al. cluster LiDAR points for on-ground
obstacles using DBSCAN. These clusters are then fed into
a ConvNet for 2D detection. Chen et al. propose to
generate 3D ROIs from the birds eye view LiDAR feature
maps by a Region Proposal Network (RPN), and combine
the regional features from the front view LiDAR feature
maps and RGB camera images for 3D vehicle detection.
In the latter pipeline, single-stage and uniﬁed CNN models
are used to directly map the input features to the detection
outputs. Li et al. and Yang et al. employ the Fully
Convolutional Network (FCN) on LiDAR point clouds to
produce an objectness map and several bounding box maps.
Caltagirone et al. use a FCN for road detection. In
this work, we follow the 2-stage object detection pipeline:
2D region proposals are provided by camera images, and
the network detects objects using the LiDAR points in the
corresponding frustum (Fig. 1), similar to .
B. Deep Active Learning
Active learning has a long history in the machine learning
community (a comprehensive survey is provided by ), and
has been introduced in deep neural networks in 2015 .
While many works exist in image classiﬁcation , , 
and segmentation problems , , , little attention
has been paid to 2D image detection , . Compared
to these works, ours is the ﬁrst attempt for active learning in
3D object detection problem.
There are numerous approaches to querying unlabeled
data, such as variance reduction, query-by-committee, and
expected model change . Among them, the uncertaintybased approach suggests to use the predictive uncertainty
to represent the data informativeness, and to query samples
with the highest uncertainty. The effectiveness of this strategy
is naturally dependent on obtaining reliable uncertainty estimates. Many recent methods have focused on obtaining such
estimates in an efﬁcient manner in deep neural networks.
For example, Lakshminarayanan et al. propose to use
an ensemble of networks to predict uncertainty. Kendall et
al. decompose predictive uncertainty into model dependent (epistemic) and data dependent (aleatoric) uncertainties
in Bayesian Neural Network. The former is obtained by
Monte-Carlo dropout sampling , while the latter by
predicting the noise in the input data. Guo et al. use
a simple calibration technique to improve the network’s
probability output. Application of these uncertainties has
also featured in many recent works. For example, Miller et
al. employ epistemic uncertainty for object detection in
open-set scenarios. Feng et al. evaluate the uncertainty
estimation in a LiDAR 3D object detection network, and
leverage the aleatoric uncertainty to signiﬁcantly improve the
network’s robustness against noisy data . Ilg et al. 
compare several uncertainty estimation methods in optical
ﬂow. In this work, we use Monte-Carlo dropout and
Deep Ensembles to estimate uncertainties, and compare
their efﬁciencies in query functions.
III. METHODOLOGY
In this work, we propose an active learning method to
iteratively train a 3D LiDAR detector using the fewest
Algorithm 1 Active Learning for 3D LiDAR Object Detector
Input : Du,Dl,A
1 Initialization: M ←trainDetector(Dl)
while notStopCondition() do
U(Du) ←uncertaintyEstimate(Du)
u ←dataQuery
▷A subset of unlabeled
Y ∗←dataLabel(X∗u ,A)
▷Class label and object location
Dl ←Dl ∪{X∗u ,Y ∗}
▷Add data to the training dataset
Du ←Du\X∗u
▷Delete data from the unlabeled dataset
M ←trainDetector(Dl)
▷Update the network
regression
classification
LiDAR depth map
LiDAR intensity map
Fig. 2: Network architecture. The detector takes the LiDAR
depth and intensity maps as input and outputs the objectness
score and object location information (width, length, height,
and depth).
number of labeled training samples, given a large unlabeled
data pool.
Denote the large unlabeled data pool as Du, which consists
of Nu i.i.d data samples Du = {xu
n=1, and the labeled
training dataset as Dl, with Nl samples (Nu ≫Nl) and their
labels Dl = {xl
n=1. Also denote the human annotator
as A and the object detection model as M. Our method is
summarized in Alg. 1. The keys to the approach are the
uncertainty estimation in neural networks, and the data query
functions.
A. Process
To start the active learning process, the network is initialized with a small labeled dataset Dl and trained in loop. After
each training, the detector evaluates the informativeness of
each sample in the unlabeled dataset Du by predicting the
uncertainty (Step 2 in Alg. 1), selects the most informative
samples via a query function (Step 3), and asks the human
annotator for their class labels and object positions (Step 4).
Afterwards, these samples are added to the labeled dataset,
and the detector is re-trained. The process iterates until a
stop condition is satisﬁed, e.g. the network’s performance
converges for several iterations, or a desired performance is
B. 3D Object Detector
1) Inputs and Outputs: As mentioned in the Introduction
(Sec. I), our detector leverages the 2D region proposals in
RGB images, which build frustums in the 3D space. We
project those LiDAR points in frustums onto the front-view
camera plane and build sparse LiDAR depth and intensity
maps. Since these maps bring complementary LiDAR information (see Fig. 2), we concatenate them to build the
network inputs. Note that we do not perform interpolation
(e.g. Delaunay Triangulation and Bilateral Filtering
 ) in order to avoid interpolation artifacts. The network
outputs softmax classiﬁcation scores s, and object locations
t. We encode an object’s 3D position as the relative width
wmax , length ˆl =
lmax , and height ˆh =
hmax of the bounding
box, as well as the euclidean distance between our egovehicle and the object centroid ˆd =
dmax , i.e. t = { ˆw, ˆl, ˆh, ˆd}.
We select wmax,lmax,hmax,dmax based on the heuristics from
the dataset.
2) Network Architecture: Our object detector is built on
the ConvNet depicted in Fig. 2. It is composed of four
convolutional layers (each with 32 3 × 3 kernels and relu
activation), a pooling layer, three fully connected layers (each
with 256 hidden units), and three dropout layers. The dropout
layers are used for stochastic regularization during training
and uncertainty estimation during testing.
C. Uncertainty Estimation and Query Functions
1) Uncertainty Estimation: In this work, we use the
predictive probability p(y|x) in classiﬁcation to estimate uncertainty in our object detection network. For simpliﬁcation,
we denote x as a data point and y classiﬁcation labels. A
direct way to obtain predictive probability is by softmax
output, i.e. p(y|x) = softmax(x). However, as discussed in
several works (e.g. , ), the softmax output may assign
high probability to unseen data, resulting in over-conﬁdent
predictions. Therefore, we also use two recent methods to
obtain uncertainty estimates, namely, Monte-Carlo dropout
(MC-dropout ) and Deep Ensembles ( ).
MC-dropout regards dropout regularization as approximate variational inference in the Bayesian Neural Network
framework, and extracts predictive uncertainty by performing
multiple feed-forward passes with dropout active during test
time. More speciﬁcally, given a test point x, the network
performs T inferences with the same dropout rate as training, and averages the outputs to approximate the predictive
probability:
p(y|x,Wt) = 1
softmax(Wt)(x),
with Wt being network’s weights for the tth inference.
Compared to MC-dropout, Deep Ensembles estimates
predictive uncertainty in an non-Bayesian way. It suggests
to train several networks with the same architecture but
with random initialization, and average the networks’ outputs
during testing. Let E be the number of ensembles and Me a
single network in the ensemble, similar to Eq. 1, we have:
p(y|x,Me) = 1
softmax(Me)(x).
2) Query Functions:
Based on the above-mentioned
methods to obtain the predictive probability, we can calculate
the informativeness (or uncertainty) for each sample in the
unlabeled data pool Du and use acquisition functions to query
the most uncertain samples. A common way is to measure
the Shannon Entropy (SE) with:
H[y|x] = −
p(y = c|x)log p(y = c|x),
and query unlabeled samples with the highest Entropy values.
y refers to a classiﬁcation label, and C the number of classes.
Additionally, since both MC-dropout and Deep Ensembles
provide samples from the predictive probability distribution
(i.e. p(y|x,Wt) or p(y|x,Me)), we can use them to measure
the Mutual Information (MI) between the model weights
and the class labels, and query unlabeled data with the
highest MI. The mutual information using MC-dropout is
calculated by:
I[y;W] = H[y|x]−Ep(W|Dl)H[y|x,W]
≈H[y|x]+ 1
p(y = c|x,Wt)log p(y = c|x,Wt),
where p(W|Dl) indicates the posterior distribution of network weights W given the training dataset calDl. For Deep
Ensembles, we only need to replace W with M. As discussed
in our previous work , SE and MI capture different
aspects of uncertainty: SE measures the output uncertainty
(predictive uncertainty), whereas MI measures the model’s
conﬁdence in the data (epistemic uncertainty).
IV. EXPERIMENTAL RESULTS
A. Experimental Design
We evaluate our proposed method based on two experimental settings. In the ﬁrst experiment, we study the active
learning performance with different uncertainty estimation
approaches and query functions. To avoid the inﬂuence from
the RGB image detector, we assume a “perfect” image
detector that provides only accurate object proposals. This
is achieved by extracting objects using their ground-truth
labels. Thus, we simplify the object detection problem in this
setup to a classiﬁcation and a location regression problem. In
the second experiment, we use a pre-trained image detector
to predict region proposals, which contain either object or
background images.
Both experiments are conducted on the KITTI dataset .
The LiDAR depth and intensity maps are generated by
projecting LiDAR points onto the image plane and then
warped to 100×100 pixels, with additional 5 pixels padding
to include context information, similar to . To start the
active learning process, the network is trained with some
samples randomly selected from the training data and balanced over all classes (200 samples per class). The network
is trained with Adam optimizer, together with dropout and l2
regularization to prevent over-ﬁtting. We set dropout rate to
be 0.5, and the weight decay to be 10−4. At each query step,
200 samples are selected from the remaining “unlabeled”
training dataset, and the network is re-trained from scratch.
At each query step, we calculate both the classiﬁcation
accuracy and the mean squared error (MSE) on the test
data set, to track classiﬁcation and localization performance,
respectively. To have a fair comparison among different
uncertainty estimation approaches, the MC-dropout methods
are evaluated by performing the single forward pass on the
test dataset without dropout. We also use the predictions from
only one network in the ensemble for evaluation. For MC-
Dropout, 20 forward passes are used during inference, while
the ensemble consists of 5 classiﬁers. We ﬁx the number of
query steps to be 60, and repeat each experiment 3 times.
Besides, we report the performance of a full-trained network.
B. Evaluation with a “Perfect” RGB Image Detector
1) Setting: We divide objects into ﬁve classes, namely,
“Small Vehicle” (including “Car” and “Van” categories in
KITTI), “Human” (including “Pedestrian”, “Person sitting”,
and “Cyclist”), “Truck”, “Tram”, and “Misc”. We randomly
divide the dataset into a training set with 31500 samples, a
test set with 6000 samples and a val set with 3000 samples.
We compare the active learning strategy with the baseline method which randomly queries the unlabeled data
points following the uniform distribution. We also study
the behavior of active learning using ﬁve different query
functions. “Softmax + Entropy”, “MC-dropout + Entropy”
and “Ensembles + Entropy” use the same query function
that maximizes Shannon Entropy. “MC-dropout + MI” and
“Ensembles + MI” query the data by maximizing the mutual
information.
2) Active Learning Performance: Results are shown in
Fig. 3. We have three observations: (1) All active learning
methods signiﬁcantly outperform the baseline method for
classiﬁcation and localization tasks. They achieve higher
recognition accuracy or lower mean squared error with
the same number of training data as the baseline method.
(2) With a relatively small amount of data (e.g. < 7000
samples), MI-based query functions (“MC-dropout + MI”
and “Ensembles + MI”) consistently perform better than
their Entropy counterparts (“MC-dropout + Entropy” and
“Ensembles + Entropy”) in the localization task (Fig. 3(b)),
whereas Entropy-based methods are more suitable for the
classiﬁcation task (Fig. 3(a)). (3) Using MC-dropout and
Deep Ensembles to estimate uncertainty results in slightly
better active learning results compared to using a single
softmax output (see “Softmax + Entropy”, “MC-dropout +
Entropy” and “Ensembles + Entropy”).
Tab. I compares the number of labeled training samples
required to train the detector so that it can reach a certain error level relative to the full-trained network. Denote accufull
and accum as the classiﬁcation accuracy from the full-trained
detector and the classiﬁcation accuracy from a detector in the
active learning process, respectively. Also denote msefull and
msem as the mean squared error for localization. We deﬁne
the relative error for classiﬁcation as |accufull −accum|
and for localization as |msem −mse full|/mse full. We also
calculate the percentage of labeling efforts saved by the
active learning methods compared to the Baseline. As can be
seen in the Table, our methods reach the relative error with
signiﬁcantly fewer training points, saving up to 60% training
samples. In addition, using MC-dropout or Deep Ensembles
to evaluate predictive uncertainty produces similar or better
results than the single softmax approach, indicating that they
can better represent the informativeness of unlabeled data.
3) Understanding How Active Learning Works: Our proposed method is based on a good uncertainty estimation. A
better uncertainty score can better represent the data informativeness, leading to a better active learning performance.
In this regard, we use the calibration plot and error curve to
evaluate the quality of predictive uncertainty, similar to 
and . Furthermore, we investigate the class distribution
of the queried samples.
Calibration Plot: A calibration plot is a quantile-quantile
plot (QQ-plot) between the predictive probability of a model
and the observed frequency of correct predictions. A well
calibrated uncertainty estimation should match the frequency
of correct prediction, showing as a diagonal line. As an
example, 60% of samples should be correctly classiﬁed as
class c, when a well-calibrated network predicts them with
probability output p(y = c|x) = 60%. We compare the uncertainty estimations using single softmax (“Softmax + Entropy”), MC-dropout (“MC-dropout + Entropy”), and Deep
Ensembles (“Ensemble + Entropy”) on the test dataset. The
calibration plots at query steps 5, 15, 30, 55 are illustrated in
the left four ﬁgures in Fig. 4, and the evolution of calibration
error with query step in the right ﬁgure. The calibration error
is calculated as the mean absolute deviation between the
frequency and the diagonal line. The ﬁgures show that at
the ﬁrst few query steps all methods are “under-conﬁdent”
with predictions, as their calibration plots are above the
diagonal line. This indicates that networks are under-ﬁtted
with only a small number of training samples. As the query
step increases, the predictive uncertainties from MC-dropout
and Deep Ensembles become well-calibrated. However, the
network using single softmax turns out to be over-ﬁtted with
data and produces over-conﬁdent predictions, resulting in a
calibration plot under the diagonal line. The experiment show
that MC-dropout and Deep Ensembles produce more reliable
uncertainty estimation and improve the learning performance
more than the single softmax.
Error Curve: Another way to evaluate the quality of predictive uncertainty is via the error curve (or “sparsiﬁcation
plot” proposed in ). It is assumed that a well-estimated
predictive uncertainty should correlate with the true error,
and by gradually removing the predictions with high uncertainty, the average errors over the rest of the predictions
will decrease. In our problem, we use the cross entropy loss
to represent error. An exemplary error curve for “Ensemble
+ Entropy” at a speciﬁc query step is shown by Fig. 6(a).
The benchmark is obtained by thresholding the predictions
by their cross entropy losses (true errors). Note that for
each uncertainty estimation method, we obtain a different
benchmark. Threrefore, we calculate the mean absolute deviation between the error curve and its benchmark, denoted
as “Error sum”, to have a fair comparison on the quality of
uncertainty estimation. Fig. 6(b) illustrates the evolution of
the “Error sum” over the query steps for single softmax,
MC-dropout and Deep Ensembles. Both MC-dropout and
Deep Ensembles consistently outperform the single softmax
method with smaller error sums.
Distribution of Sampled Objects To further understand how
active learning outperforms the baseline method, we compare
the class distribution of queried samples between “Ensemble
+ Entropy” and Baseline. This is calculated by taking the
difference between the two at each step, normalized by the
total number of samples from the corresponding classes in
the unlabeled data pool. The results are shown in Fig. 5.
The unlabeled data is highly class-imbalanced with ratios
“Small Vehicle”= 78%, “Human”= 15.6%, “Truck”= 2.7%,
“Tram”= 1.3%, “Misc”= 2.4%. However, our method naturally alleviates such problem by querying fewer samples from
“Small Vehicle” and more from other classes. Note that this
effect of balancing samples over classes is due to using the
uncertainty estimation in the query function rather than an
ad-hoc solution that explicitly over/undersample examples.
C. Evaluation with a Pre-trained RGB Image Detector
1) Setting: In this experiment, we evaluate the active
learning method based on region proposals provided by a
RGB image detector. To this end, we follow to divide
the KITTI dataset into a train set and a val set, and use the
RGB image detector proposed by . The train set is used
to ﬁne-tune the image detector which has been trained on
COCO dataset , and the val set is used to evaluate our
method. We consider to detect objects with the classes “Small
vehicle” and “Human” (same to the previous experiment).
A proposal is assigned positive when its 2D Intersection
over Union (IoU) with the ground truth is larger than 0.5.
Proposals with IoU smaller than 0.5 or from other object
classes are marked as “Background”. The recall scores of
the image detector is shown by Tab. II.
Full-train network
Full-train network
Fig. 3: Detection performance for the baseline and several active learning methods with different uncertainty estimations
and query functions. The horizontal axis represents the increasing number of labeled training data samples. All networks
are initialized with 1000 samples balanced over all classes. At each query step, 200 samples are queried from the unlabeled
data pool. The vertical axis represents the detection performance for (a) classiﬁcation and (b) object localization.
Probability
Probability
Probability
Probability
Query step
Calibration error
Softmax+Entropy
MC-dropout+Entropy
Ensemble+Entropy
Fig. 4: A comparison of the calibration quality of predictive uncertainty from “Softmax + Entropy”, “MC-dropout + Entropy”,
and “Ensemble + Entropy” averaged over three runs. To this end, we divide the probability values into several bins and
calculate the frequency of correct predictions in each bin. The calibration plots at query step 5, 15, 30, and 55 are shown in
the ﬁrst four subplots. The diagonal line indicates perfect calibration, where the predictive uncertainty matches the observed
frequency of correct predictions. The evolution of calibration error w.r.t. query step is shown in the right plot. A smaller
error value indicates better uncertainty estimation.
Query step
Percentage [%]
Query step
Percentage [%]
Small Vehicle
Query step
Percentage [%]
Query step
Percentage [%]
Query step
Percentage [%]
Fig. 5: Class distribution of queried samples for “Ensemble + Entropy” compared to the baseline method. The vertical
axis represents the accumulative queried samples relative to the baseline, normalized by the number of samples of the
corresponding classes in the unlabeled data pool. Compared to the baseline method, active learning queries fewer samples
from “Small Vehicle” and more from the other classes. Note that since there are much more objects in “Small vehicle” than
the other classes, a small percentage drop in “Small vehicle” means a much more percentage rise in other classes.
Recognition accuracy (Classiﬁcation)
Mean Squared Error (Localization)
Relative error to the full-trained network
Softmax + Entropy
3200(+36%)
3800(+44%)
4400(+51%)
6000(+47%)
3800(+17%)
8800(+12%)
10600(+13%)
MC-dropout + MI
3600(+28%)
3800(+44%)
4600(+49%)
6000(+47%)
2000(+57%)
5400(+21%)
8600(+14%)
10000(+18%)
MC-dropout + Entropy
2600(+48%)
3600(+47%)
4400(+51%)
5600(+51%)
3800(+17%)
8000(+20%)
10200(+16%)
Ensemble + MI
3400(+32%)
3800(+44%)
4400(+51%)
6000(+47%)
2200(+52%)
4400(+35%)
9000(+10%)
10200(+16%)
Ensemble + Entropy
3000(+40%)
3400(+50%)
3800(+58%)
4400(+61%)
3200(+30%)
4600(+32%)
7600(+24%)
10600(+13%)
TABLE I: The number of labeled training samples required to train the detector in order to reach a certain relative error
to the full-trained network. The table also shows the percentage of labeling efforts saved by the active learning methods
compared to the Baseline.
Percentage of samples removed [%]
Averaged cross entropy loss
Ensemble + Entropy
Query step
Softmax + Entropy
MC-dropout+ Entropy
Ensemble + Entropy
Fig. 6: (a): Error curve for “Ensemble + Entropy” at
a speciﬁc query step. We use the cross entropy loss to
represent the error. (b): The evolution of error sum w.r.t.
query step. A smaller error sum indicates better uncertainty
estimation. The plots are averaged over three runs.
Small Vehicle
TABLE II: Recall rate for the RGB image detector.
Based on image proposals, we build a training data pool
with 17221 samples to train our active learning method.
These samples are selected with their IoU being either larger
than 0.5 (Positive) or below 0.2 (Background). The test
dataset contains 6000 samples, including those with IoU
ranging between 0.2 and 0.5. Note that ignoring samples
with some IoU ranges is a common procedure when training
object detectors, as discussed in , .
2) Active Learning Performance: We compare the active
learning based on “Ensemble + Entropy” strategy with the
baseline method. Results are shown in Fig. 7(a) and Fig. 7(b).
Compared to the ﬁrst experiment (Fig. 3), the network in
this experiment results in a lower recognition accuracy, as
this experiment is more challenging than the previous one.
Despite of this, active learning consistently outperforms the
baseline methods by reaching the same recognition accuracy
or mean squared error with fewer labeled training samples.
V. DISCUSSION AND CONCLUSION
We presented a method that leverages active learning to
efﬁciently train a LiDAR 3D object detector. The network
predicts the object classiﬁcation score and 3D geometric
information based on 2D proposals on camera images. We
conducted experiments using a “perfect” image detector
to compare several ways of uncertainty estimation and
Number of labeled samples
Recognition Accuracy
Ensemble + Entropy
Full-train network
Number of labeled samples
Mean Square Error (MSE)
Ensemble + Entropy
Full-train network
Fig. 7: A comparison of learning learning between active
learning and baseline methods for (a) classiﬁcation and (b)
localization tasks. The active learning is built by “Ensemble
+ Entropy” strategy.
query functions. Results show that MC-dropout and Deep
Ensembles provide more reliable predictive uncertainties
compared to the single softmax output, and achieve better
active learning performance. We also used a pre-trained
image detector to predict image region proposals. In both
experimental settings, our active learning method reaches the
same detection performance with signiﬁcantly fewer training
samples compared to the baseline method, saving up to 60%
labeling efforts.
We show that building query functions based on predictive
uncertainty in classiﬁcation is effective not only in improving
recognition accuracy, but also in reducing the mean squared
error for the localization task at the same time (e.g. Tab. I
and Fig. 3). This indicates that by sharing weights in the
hidden layers, the classiﬁcation and localization are related
to each other in the object detection network. It is an
interesting future work to introduce location uncertainty into
our active learning method. Furthermore, in this work the
network is retrained from scratch after each query step. In
applications where we want to adapt an object detector to
new driving scenarios (as discussed in Sec. I), it is preferable
to ﬁne-tune the network with newly-labeled data. Employing
our proposed active learning method in such a “life-long
learning” scenario is an interesting future work.
One limitation of our method is that the performance of
the LiDAR detector is highly dependent on region proposals from the image detection. Despite on-the-shelf image
detectors already achieve high detection performance, the
LiDAR detector can not handle false negatives in images.
In order to guarantee highly qualiﬁed region proposals, we
can incorporate the image detector into the active learning
loop, i.e. the region proposals are ﬁrst provided by the image
detector and then corrected by human annotators. We leave
this as an interesting future work.
ACKNOWLEDGMENT
We thank William H. Beluch, Radek Mackowiak, and
Christian H. Schuetz for the suggestions and fruitful discussions.