ORIGINAL RESEARCH
published: 13 May 2020
doi: 10.3389/fnins.2020.00259
Edited by:
Yu-Chien Wu,
Indiana University Bloomington,
United States
Reviewed by:
Giuseppe Jurman,
Fondazione Bruno Kessler, Italy
Han Zhang,
University of North Carolina at
Chapel Hill, United States
*Correspondence:
 
†Alzheimer’s Disease Neuroimaging
Initiative – Data used in preparation of
this article were obtained from the
Alzheimer’s Disease Neuroimaging
Initiative (ADNI) database
(adni.loni.usc.edu). As such, the
investigators within the ADNI
contributed to the design and
implementation of ADNI and/or
provided data but did not participate
in analysis or writing of this report.
A complete listing of ADNI
investigators can be found at:
 
uploads/how_to_apply/ADNI_
Acknowledgement_List.pdf
Specialty section:
This article was submitted to
Brain Imaging Methods,
a section of the journal
Frontiers in Neuroscience
Received: 27 September 2019
Accepted: 09 March 2020
Published: 13 May 2020
Pan D, Zeng A, Jia L, Huang Y,
Frizzell T and Song X Early
Detection of Alzheimer’s Disease
Using Magnetic Resonance Imaging:
A Novel Approach Combining
Convolutional Neural Networks
and Ensemble Learning.
Front. Neurosci. 14:259.
doi: 10.3389/fnins.2020.00259
Early Detection of Alzheimer’s
Disease Using Magnetic Resonance
Imaging: A Novel Approach
Combining Convolutional Neural
Networks and Ensemble Learning
Dan Pan1, An Zeng1,2*, Longfei Jia1, Yin Huang1, Tory Frizzell3 and
Xiaowei Song3 for the Alzheimer’s Disease Neuroimaging Initiative (ADNI)†
1 School of Computers, Guangdong University of Technology, Guangzhou, China, 2 Guangdong Key Laboratory of Big Data
Analysis and Processing, Guangzhou, China, 3 SFU ImageTech Lab, Surrey Memorial Hospital, Fraser Health, Surrey, BC,
Early detection is critical for effective management of Alzheimer’s disease (AD) and
screening for mild cognitive impairment (MCI) is common practice. Among several deeplearning techniques that have been applied to assessing structural brain changes on
magnetic resonance imaging (MRI), convolutional neural network (CNN) has gained
popularity due to its superb efﬁciency in automated feature learning with the use of
a variety of multilayer perceptrons. Meanwhile, ensemble learning (EL) has shown to
be beneﬁcial in the robustness of learning-system performance via integrating multiple
models. Here, we proposed a classiﬁer ensemble developed by combining CNN and
EL, i.e., the CNN-EL approach, to identify subjects with MCI or AD using MRI: i.e.,
classiﬁcation between (1) AD and healthy cognition (HC), (2) MCIc (MCI patients who
will convert to AD) and HC, and (3) MCIc and MCInc (MCI patients who will not convert
to AD). For each binary classiﬁcation task, a large number of CNN models were trained
applying a set of sagittal, coronal, or transverse MRI slices; these CNN models were then
integrated into a single ensemble. Performance of the ensemble was evaluated using
stratiﬁed ﬁvefold cross-validation method for 10 times. The number of the intersection
points determined by the most discriminable slices separating two classes in a binary
classiﬁcation task among the sagittal, coronal, and transverse slice sets, transformed
into the standard Montreal Neurological Institute (MNI) space, acted as an indicator
to assess the ability of a brain region in which the points were located to classify
AD. Thus, the brain regions with most intersection points were considered as those
mostly contributing to the early diagnosis of AD. The result revealed an accuracy rate of
0.84 ± 0.05, 0.79 ± 0.04, and 0.62 ± 0.06, respectively, for classifying AD vs. HC, MCIc
vs. HC, and MCIc vs. MCInc, comparable to previous reports and a 3D deep learning
approach (3D-SENet) based on a more state-of-the-art and popular Squeeze-and-
Excitation Networks model using channel attention mechanism. Notably, the intersection
points accurately located the medial temporal lobe and several other structures of the
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
limbic system, i.e., brain regions known to be struck early in AD. More interestingly,
the classiﬁers disclosed multiple patterned MRI changes in the brain in AD and MCIc,
involving these key regions. These results suggest that as a data-driven method, the
combined CNN and EL approach can locate the most discriminable brain regions
indicated by the trained ensemble model while the generalization ability of the ensemble
model was maximized to successfully capture AD-related brain variations early in the
disease process; it can also provide new insights into understanding the complex
heterogeneity of whole-brain MRI changes in AD. Further research is needed to examine
the clinical implication of the ﬁnding, capability of the advocated CNN-EL approach to
help understand and evaluate an individual subject’s disease status, symptom burden
and progress, and the generalizability of the advocated CNN-EL approach to locate
the most discriminable brain regions in the detection of other brain disorders such as
schizophrenia, autism, and severe depression, in a data-driven way.
Keywords: Alzheimer’s disease, mild cognitive impairment, convolutional neural networks, ensemble learning,
magnetic resonance imaging, MRI biomarkers, MCI-to-AD conversion, Alzheimer’s Disease Neuroimaging
Initiative
INTRODUCTION
Alzheimer’s
progressive,
irreversible neurodegenerative disease clinically manifested by
amnesia, cognitive dysfunction, and gradual loss of multiple
other brain functions and daily living independency . The number of patients with AD worldwide is expected
to increase from the current 47 million to 152 million by 2050,
causing serious economic, medical, and societal consequences
 . The pathogenesis of AD remains not fully
elucidated and no available therapy can cure AD or completely
stop disease progression. Amnestic mild cognitive impairment
(MCI) is a transitional stage between cognitively normal aging
and AD, and patients with MCI are more likely to develop AD
than age-matched healthy cognition (HC) . Early
detection of AD by screening MCI is crucial both for eﬀective
management and care strategies and for developing new drugs
and measures to prevent further deterioration of the disease.
Brain magnetic resonance imaging (MRI) has enabled noninvasive in vivo investigations of AD-related changes in the brain.
A large number of promising machine learning applications have
used MRI for AD prediction , which
include random forests (RF) , support
vector machine (SVM) , and boosting
algorithms . Even so, existing machine
learning approaches typically involve manual selection of predeﬁned brain regions of interest (ROIs) based on known MRI
features of AD. Given the limited understanding of deﬁnitive
MRI biomarkers for AD, it is likely that pre-selected ROIs
cannot include all the information potentially useful to uncover
the complexity of AD. Manual selection can also be prone to
subjective errors and be time-consuming and labor-intensive
 .
Deep learning represents a more advanced approach; methods
such as stacked auto-encoder (SAE) , deep
belief networks (DBNs) , and convolutional neural
networks (CNNs) can automatically build a more
abstract high-level representation of the learning system by
integrating low-level features embedded in the data . The CNN model has been widely used for classiﬁcation
 , segmentation , and
object detection , due to several advantages:
CNNs can directly accept images data as input, utilize spatial
information embedded in adjacent pixels, and eﬀectively reduce
the number of model parameters by using local receptive ﬁelds,
weights sharing, and subsampling. When a CNN model is trained
with MRI slices, image features can be automatically retrieved,
eliminating the need of manual selection of features for the
learning process . Meanwhile, ensemble learning
(EL) has shown beneﬁcial in the performance and robustness via
integrating multiple learning systems ,
which has also been applied to MRI .
So far, some researchers have combined deep learning and
EL on MRI data for AD. A method for AD and early AD
diagnosis by fusing functional and structural imaging data based
on the use of the Deep Learning paradigm, and more speciﬁcally,
deep belief networks (DBN) has been advocated . Gray matter (GM) images from each brain area have been
split into 3D patches according to the regions deﬁned by the
Automated Anatomical Labeling (AAL) atlas, and these patches
were used to train a set of DBNs. The DBNs were then ensembled
where the ﬁnal prediction was determined by a voting scheme.
Two deep learning based structures and four diﬀerent voting
schemes were implemented and compared, giving as a result a
potent classiﬁcation architecture where discriminative features
were computed in an unsupervised fashion .
Islam and Zhang proposed an ensemble of three deep
CNNs with slightly diﬀerent conﬁgurations for Alzheimer’s
disease diagnosis using brain MRI data analysis. In addition,
sparse regression models were combined with deep neural
networks for AD diagnosis . Here, sparse
regression models with diﬀerent regularization control values
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
outputted their own prediction values. To obtain the ﬁnal
prediction values, CNNs discovered the optimal weights to
ensemble multiple sparse regression models in a hierarchical and
non-linear way . In 2019, 20 white matter and
GM slices with signiﬁcant brain structures from MR images were
selected to train an ensemble of ConvNet networks . In Li et al. , a whole MR brain image was partitioned
into diﬀerent local regions and a number of 3D patches were
extracted from each region. Subsequently, the authors grouped
the patches from each region into diﬀerent clusters with the
K-Means clustering method. Next, a DenseNet was constructed
to learn the patch features for each cluster and the features
acquired from the discriminative clusters of each region were
ensembled for classiﬁcation. At the end, the authors combined
the classiﬁcation results from diﬀerent local regions to improve
ﬁnal image classiﬁcation.
In the present study, we proposed a novel CNN–EL approach
based on an established eight-layer CNN network structure
 , to automatically retrieve features from brain
MRI data that can be used to diﬀerentiate subjects with clinical
diagnosed AD and MCI from HC, and those with MCIc and
MCInc. We are also interested in identifying patterns of MRI
brain changes that characterize AD and MCIc. To achieve the
study objectives, we ﬁrst derived a CNN model using each
set of the sagittal, coronal, or transverse MRI slices; then, we
developed a classiﬁer ensemble based on three-axis slices using
EL. A number of sophisticated techniques were employed in our
approach, which included six ways of data augmentation (DA)
to facilitate an equal and relatively large number of instances of
each class in the training dataset, top-performance enforcing to
achieve a high classiﬁcation accuracy and robustness of the model
training, and parallel processing to improve the time eﬃciency of
the system function.
In the CNN-EL, a data-driven, homogeneous ensemble
learning approach was employed. A base classiﬁer based on
2D CNN model was trained using each set of the sagittal,
coronal, or transverse MRI slices; that is, a trained base classiﬁer
corresponds to a slice dataset, which is composed of slices in
a speciﬁc position in brain from the subjects in the training
dataset. The preparations of training datasets didn’t depend on
prior experience or domain knowledge. In order to reduce the
loss of information as much as possible during the process of
slicing the 3D volume into 2D slices, we have utilized as many
and meaningful 2D-sagittal, -coronal, or -transverse slices from
all over the brain as we can at the same time to train the
base classiﬁers. Among them, the trained base classiﬁers with
the best generalization performance on the validation datasets
were selected and combined to generate a reﬁned ﬁnal classiﬁer
ensemble based on three-axis slices. In this data-driven way, the
slices corresponding to the selected trained base classiﬁers were
considered as those with the strongest capabilities to classify
AD. The number of the intersection points determined by the
most discriminable slices separating two classes in a binary
classiﬁcation task among the sagittal, coronal, and transverse
slice-sets, transformed into the standard Montreal Neurological
Institute (MNI) space, acted as an indicator to assess the ability
of a brain region in which the points were located to classify
AD. Thus, we located the most discriminable brain regions
indicated by the trained CNN-EL model while its generalization
abilities were maximized and superior to those of the compared
methods. That is, we can understand the predictions made by the
trained CNN-EL model to some extent. However, the compared
methods, i.e., PCA + SVM and a 3D deep
learning approach (3D-SENet) based on a more state-of-theart and popular Squeeze-and-Excitation Networks model using
channel attention mechanism, which was derived from the paper
 , were unable to do the same thing as the
above-mentioned and failed to provide meaningful explanations
for predictions since the models achieved with those compared
methods were still like a “black-box”. To our knowledge, this is
the ﬁrst attempt to do the above way with both CNN and EL,
and at the same time, the promising experimental results have
been achieved.
In detail, the CNN-EL was diﬀerent from the abovementioned methods which combined the deep learning with
the ensemble learning to analyze MRI data for detecting AD
in the base classiﬁers , the ensemble methods , the model
interpretability , or the preparation of training datasets .
Furthermore, in the paper , the authors
ﬁrstly systematically and critically reviewed the state-of-the-art
on classiﬁcation of Alzheimer’s disease based on convolutional
neural networks and T1-weighted MRI. Next, they proposed
an open-source framework for reproducible evaluation of
classiﬁcation approaches. In this study, the ﬁvefold cross
validation procedure was strictly followed and repeated ten times
for each binary experiment, i.e., AD vs. HC, MCIc vs. HC,
and MCIc vs. MCInc. The potential data leakage among binary
classiﬁcation tasks was avoided and therefore the experimental
results were unbiased and reproducible.
MATERIALS AND METHODS
Participants and Datasets
Data used in the study were obtained from the Alzheimer’s
Disease Neuroimaging Initiative (ADNI) database.1 The ADNI
was launched in 2003 as a public–private partnership, led by
Principal Investigator, Michael W. Weiner, MD. The primary goal
of ADNI has been to test whether serial MRI, positron emission
tomography (PET), other biological markers, and clinical and
neuropsychological assessment can be combined to measure the
progression of MCI and early AD.
To facilitate comparison of our results with those reported
previously, we used the same MRI dataset from the ADNI
database as utilized by Christian et al. in building the
eight-layer CNN networks to train the
base classiﬁers, as well as to test the performance of the ﬁnal
classiﬁer ensemble based on three-axis slices the training and testing dataset
(upper panel) and (B) the validation dataset (lower panel).
(male:female)
Age (year;
mean, std)
Weight (kg;
mean, std)
70.9, 14.0
72.7, 14.3
76.2, 12.9
73.8, 13.6
(mean, std)
26.47, 1.84
27.19, 1.71
29.18, 0.96
CDR (mean,
0.75, 0.25
0.50, 0.00
0.50, 0.00
0.00, 0.00
GDS (mean,
1.59, 1.32
1.38, 1.14
1.52, 1.37
0.80, 1.08
(male:female)
Age (years;
mean, std)
74.24, 7.82
74.15, 7.10
76.02, 7.00
73.36, 5.70
Weight (kg;
mean, std)
76.04, 15.83
73.59, 14.14
78.35, 12.99
76.16, 15.66
(mean, std)
23.84, 2.08
27.05, 1.59
27.56, 1.83
28.92, 1.25
CDR (mean,
0.82, 0.24
0.50, 0.00
0.50, 0.00
0.00, 0.00
GDS (mean,
1.81, 1.56
1.92, 1.35
1.79, 1.45
0.83, 1.34
AD, Alzheimer’s disease patients; MCIc, mild cognitive impairment patients who
will convert to AD; MCInc, mild cognitive impairment patients who will not convert
to AD; HC, healthy controls; MMSE, Mini Mental State Examination; CDR, Clinical
Dementia Rating; GDS, Global Deterioration Scale.
AD = 137, 18 months MCIc = 76 and MCInc = 134, and
HC = 162; Table 1A). We enrolled 162 cognitively normal elderly
controls (HC), 137 patients with diagnosis of AD, 76 patients
with diagnosis of MCI who converted to AD within 18 months
(MCIc), and 134 patients with diagnosis of MCI who did not
convert to AD within 18 months (MCInc). MCI patients who
had been followed less than 18 months were not considered
 . A total of 509 subjects from 41 diﬀerent
radiology centers were considered. Inclusion criteria for HC
were as follows: Mini Mental State Examination (MMSE) scores
between 24 and 30; Clinical Dementia Rating (CDR) of zero; and absence of depression, MCI, and dementia.
Inclusion criteria for MCI were as follows: MMSE scores between
24 and 30; CDR of 0.5; objective memory loss, measured by
education adjusted scores on Wechsler Memory Scale Logical
Memory II ; absence of signiﬁcant levels of
impairment in other cognitive domains; and absence of dementia.
Inclusion criteria for AD were as follows: MMSE scores between
20 and 26; CDR of 0.5 or 1.0; and NINCDS/ADRDA criteria for
probable AD .
To facilitate the development of the EL process, an additional
validation dataset of 278 subjects (AD = 100, 36 months
MCIc = 39 and MCInc = 39, and HC = 100; Table 1B) was
also retrieved from the ADNI database and used to identify
the base classiﬁers showing the best generalization performance.
The validation data of 278 subjects had no overlapping with
the aforementioned data of 509 subjects, i.e., the validation data
were used for neither training the base classiﬁers nor testing
the acquired ﬁnal classiﬁer ensemble based on three-axis slices
(Table 1B). Here, among 164 patients with diagnosis of pMCI
(progressive MCI) used by Moradi et al. , i.e., if diagnosis
was MCI at baseline but conversion to AD was reported after
baseline within 1, 2, or 3 years, and without reversion to MCI
or HC at any available follow-up (0–96 months), 39 patients
who were not in the 509 subjects were selected as MCIc subjects
in the validation dataset. Meanwhile, among 100 patients with
diagnosis of sMCI (stable MCI) used by Moradi et al. , i.e.,
if diagnosis was MCI at all available time points (0–96 months)
but at least for 36 months, 39 patients who were not in the
aforementioned 509 subjects were chosen as MCInc subjects in
the validation dataset. In order to keep the validation dataset
relatively balanced, we enrolled 100 cognitively normal elderly
controls (HC) and 100 patients with diagnosis of AD who were
not in the aforementioned 509 subjects as well.
MRI Preprocessing
Upon downloading, the T1-weighted MRI data in.nii format were
processed using the CAT12 toolkit2 with default value setting. The
preprocessing pipeline included skull extraction, registration to
the MNI space, and image smoothing, so that after processing, all
the images had a dimension of 121 × 145 × 121 (X × Y × Z) with
a spatial resolution of 1.5 × 1.5 × 1.5 mm3 per voxel. Voxel-based
MRI signal intensity normalization was then performed for each
image; i.e., the value of each voxel was normalized as the original
value divided by the original maximal value of the image, yielding
a value between 0 and 1. The complete preprocessing pipeline is
summarized in Figure 1.
To facilitate the CNN training, veriﬁcation, and testing, a
3D image set of each subject was re-sliced into three 2D image
sets, each of the sagittal, coronal, or transverse orientation (with
X, Y, and Z axes perpendicular to the sagittal, coronal, and
transverse planes, respectively). A preprocessed 3D MRI image
(of 121 × 145 × 121) was thus re-sliced into 121 sagittal, 145
coronal, and 121 transverse slices; the values on the X, Y, and Z
axis were {−90, −88, −87, . . . 90}, {−126, −125, −123, . . . 90},
and {−72, −71, −69, . . . 108}, respectively. For example, X(i),
i∈{−90, −88, −87, . . . 90} is the sagittal slice through the point
[i, 0, 0]. Here, the numbers within the brackets were the MNI
coordinates. To reduce the number of base classiﬁers without
compromising the eﬀectiveness of the classiﬁcation, every other
slice was used (given the relatively small diﬀerence between
two adjacent slices) and slices near either end of an axis were
discarded (given the relatively less amount of information useful
for classiﬁcation), which lay outside the blue rectangle shown
in Figure 2. The CNN model training, testing, and veriﬁcation
involved use of only 40 sagittal slices {X(−61), X(−58), X(56)},
50 coronal slices {Y(−91), Y(−88), Y(56)}, and 33 transverse
2 
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
FIGURE 1 | Preprocessing pipeline—an example showing the formation of a transverse slice used in the learning. (A) Original image. (B) Skull-stripping and spatial
normalization. (C) Smoothing. (D) Gray normalization. (E) Slicing and resizing.
slices {Z(−28), Z(−25), Z(68)}, i.e., in total, 123 slices of a
subject’s 3D brain image.
Given the dimension of the 3D MRI (121 × 145 × 121),
the sizes of the sagittal, coronal, and transverse slices obtained
through re-slicing were 145 × 121, 121 × 121, and 121 × 145,
respectively. Each of the 2D slices was reformatted to 145 × 145
using edge padding and zero ﬁlling, so that the 2D slice is squared,
while the center and the spatial resolution of the resized image
remained unchanged.
Convolutional Neural Network
As an automated image recognition method, the CNN has
widespread
tremendous
success in recent years. Hubel and Wiesel ﬁrst described
receptive ﬁelds, binocular interactions, and the functional
architecture of cat primary visual cortex about 55 years
 .
proposed a neural network model nicknamed “Neocognitron”
 that is structurally similar to the hierarchy
model of the visual nervous system proposed by Hubel
and Wiesel. This unique network structure can eﬀectively
reduce the complexity of feedback neural networks, which
FIGURE 2 | The cropping range (inside the blue rectangle) of the slices used
to train the model on (A) a sagittal plane and (B) a coronal plane, respectively.
(A) Sagittal Plane. (B) Coronal plane.
characterizes the CNN model. With the CNN, each input
image is passed through a series of convolution layers: ﬁltering
layers (kernels), pooling layers, and fully connected layers
(FCs). A softmax function is then applied to classify an
image with probabilistic values between 0 and 1, making the
CNN suitable for learning representations of image features
 .
A convolution layer in the CNN model is typically composed
of two segments: feature extraction and feature mapping
 . In the feature-extraction segment, each
neuron is connected to the local receptive ﬁeld of the upper
layer to extract local features. Once the local feature is extracted,
its spatial relationship with other features is also determined.
In the feature-mapping segment, convolution is performed on
the input data using a learnable ﬁlter or kernel to produce a
feature map. Feature mapping computes the outputs of neurons
connected to receptive ﬁelds in the input, with each neuron
computing a dot product between its weight (i.e., ﬁlter) and
a local receptive ﬁeld (equivalent to ﬁlter size) to which it
is connected (the input volume). Multiple feature maps can
be calculated with a set of learnable ﬁlters. In this way, the
number of parameters to be tuned in the CNN is eﬀectively
reduced. A convolutional layer is followed by a pooling layer,
e.g., max-pooling layer , which performs a
down-sampling operation along the spatial dimensions (e.g., X,
Y for a transverse slice). This unique dual-feature extraction
method can eﬀectively reduce the feature resolution . The basic structures of the convolutional layer
and the pooling layer of the CNN model are shown in
In this study, the CNN was utilized mainly to recognize
2D images with displacement, scaling, and other non-deformed
distortions. Data were reconstructed, so that an image was
inputted into the CNN model as a vector for easy feature
extraction and classiﬁcation. The eﬀectiveness of the CNN
was improved as the pooling layer learned the features
from training data without manual extraction. Applying the
learnable kernels and convolution operation, the CNN was
trained in parallel, while the local weight-sharing eﬀectively
reduced its complexity.
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
FIGURE 3 | Basic structures of the CNN convolutional layer and pooling layer. (A) Convolutional layer. (B) Pooling layer.
Ensemble Learning
EL algorithms including Bagging , Boosting
 , and Random Forest have been typically used to construct a set of base
classiﬁers in solving a given problem. Using a training dataset,
EL discriminates features to produce a weighted vote for
classes, which is then applied in classifying more cases in new
datasets. Based on the methods with which a base learner
is generated, each of the EL algorithms can be divided into
two general approaches: the heterogeneous approach, which
applies diﬀerent learning algorithms in the same training
data, and the homogeneous approach, which applies the
same learning algorithm in diﬀerent training data . Both approaches have been shown to
signiﬁcantly improve the generalizability and robustness of a
learning system.
In the present study, the homogeneous EL approach was
adopted from the stratiﬁed Bagging method. The same CNN
algorithm was employed to train diﬀerent base classiﬁers
using diﬀerent 2D MRI slices. The outputs from the multiple
trained base classiﬁers with the best generalization performance
on the validation dataset were then combined to generate
a reﬁned ﬁnal classiﬁer ensemble based on three-axis slices
that was used to predict classiﬁcation results for new cases,
i.e., 3D MRI data.
Classiﬁcation Experiment
A total of 787 subjects’ 3D MR images from the ADNI
database were partitioned into three datasets: training and
testing datasets to build the base classiﬁers and examine the
performance of the ﬁnal classiﬁer ensemble based on threeaxis slices (n = 509; Table 1A) and a veriﬁcation dataset to
evaluate and select the best base classiﬁers (n = 278; Table 1B).
For training and testing, a stratiﬁed ﬁvefold cross-validation
method was employed, such that each binary classiﬁcation task
(e.g., MCIc vs. MCInc) was conducted ﬁve times. No images
in the training/testing datasets were used to select the best
base classiﬁers, and thus potential data leakage among binary
classiﬁcation tasks was avoided.
In each binary classiﬁcation task, a total of 123 2D sagittal,
coronal, and transverse slices extracted from each 3D MRI
were employed to generate 123 trained base classiﬁers. Using
classiﬁcation of AD (n = 137) vs. HC (n = 162) as an example, 299
labeled 3D MRI (Table 1A), were partitioned into 80% training
and 20% testing cases with stratiﬁed random sampling. The 299
2D slices of X(i) [or Y(j), or Z(k)] were compiled as a 2D dataset,
where i∈{−61, −58, . . . 56}, j∈{−91, −88, . . . 56}, and k∈{−28,
−25, . . . 68}; 239 (or 80%) of stratiﬁed randomly selected cases
were employed to train the X(i) [or Y(j), or Z(k)] base classiﬁer,
while the remaining slices of 60 (or 20%) cases were used to
test the trained classiﬁer ensemble based on three-axis slices. In
this way, all 123 trained base classiﬁers to classify AD vs.HC
were acquired.
Then, the 123 labeled 2D MR images from each of AD
(n = 100) and HC (n = 100) cases were altogether used
as the validation dataset (Table 1B): they were employed
to select the ﬁve base classiﬁers (i.e., in total 15) with the
best generalization performance, as determined by classiﬁcation
accuracy, among the sagittal, coronal, and transverse slicebased base classiﬁers, respectively. The number of ﬁve was
determined by the experiments. Finally, after building three
classiﬁer ensembles based on single-axis slices (i.e., sagittal,
coronal, and transverse), a classiﬁer ensemble based on three-axis
slices, which was composed of all the three classiﬁer ensembles
based on single-axis slices, was ﬁnally built using these 15 base
classiﬁers, following a simple majority voting scheme . The 2D slices that were extracted from the 3D MR
images of the remaining 60 (or 20%) cases in the training and
testing dataset and were corresponding to the 15 base classiﬁers
were used to test the performance of the built classiﬁer ensemble
based on three-axis slices.
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
TABLE 2 | Numbers of augmented images in the MCIc and MCInc datasets.
Augmentation methods
Original slices
Translation
Gamma correction
Random noise
Random afﬁne transformation
Total number of images in the augmented dataset
Data Augmentation
To overcome the possible over-ﬁtting problem in training robust
CNN models and to incorporate possible image discrepancy,
augmented images were generated from the original slices by
six operations: rotation, translation, gamma correction, random
noise addition, scaling, and random aﬃne transformation. The
augmented data were added to the original training dataset
to allow a suﬃciently large sample size (Table 2). Data
augmentation was also used to mitigate the originally imbalanced
dataset (e.g., there were more subjects with MCInc than those
with MCIc), for which the preset number of augmented slices to
be generated varied from class to class. For example, to classify
MCIc vs. MCInc, there were 76 MCIc and 134 MCInc cases.
Using six data augmentation operations, 10 new slices were
generated from an MCInc case and 18 from an MCIc case with
each operation. In this way, slice ratios of MCInc:MCIc became
∼1:1 after data augmentation from the original ∼1.8:1.
Base Classiﬁers
To address the objective of the study, i.e., binary classiﬁcation of
AD or MCIc vs. HC, and MCIc vs. MCInc, three corresponding
classiﬁer ensembles based on the three slice orientation groups
(sagittal, coronal, and transverse), i.e., classiﬁer ensembles based
on three-axis slices, were trained. The overall architecture of
the proposed classiﬁer ensemble based on three-axis slices is
shown in Figure 4 and the ﬂow chart of the experiment is
shown in Figure 5.
Each base classiﬁer consisted of six convolution layers
(conv) and two fully connected layers (FCs). The last FC layer
had only two nodes, and the softmax function was used to
implement the binary classiﬁcation. The network architecture
and corresponding hyper-parameters are shown in Figure 6 and
Table 3, respectively. Each base classiﬁer was trained for 30
epochs, as 30 epochs proved suﬃcient for a base classiﬁer to
converge. That is, after 30 epochs, a trained base classiﬁer could
achieve 100% classiﬁcation accuracy on the original slices (rather
than the augmented slices) in the training dataset. Activation
functions in all convolutional layers were of the leaky rectiﬁer
linear activation (LReLU) type , while the
Adam optimization algorithm was used
FIGURE 4 | The architecture of the classiﬁer ensemble based on the three sets of 2D slices (from left to right: sagittal, coronal, and transverse).
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
FIGURE 5 | Experimental ﬂow chart. (A) Training phase. (B) Testing phase.
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
FIGURE 6 | Base-classiﬁer architecture used in the CNN-EL approach proposed here.
to update network weights. The learning rate and the batch size
were set to 0.0001 and 200, respectively.
Ensemble Learning
The proposed model employed a two-stage EL scheme. Phase
1 involved building three classiﬁer ensembles based on singleaxis slices (i.e., sagittal, coronal, and transverse) and Phase 2
involved constructing a classiﬁer ensemble based on three-axis
slices, which was composed of all the three classiﬁer ensembles
based on single-axis slices acquired in Phase 1. In total, 40 sagittal,
50 coronal, and 33 transverse base classiﬁers were acquired. Then,
the ﬁve base classiﬁers with the best generalization performance
for each slice orientation were selected using the veriﬁcation
dataset, yielding three classiﬁer ensembles based on single-axis
slices, each with the 5 best base classiﬁers. The output of a
classiﬁer ensemble based on single-axis slices was generated by
combining the outputs of the ﬁve best base classiﬁers. Finally,
a simple majority voting scheme was used to combine the
predictions of these three classiﬁer ensemble based on singleaxis slices to yield the output of the classiﬁer ensemble based on
three-axis slices. Experimental results demonstrated that this EL
method greatly improved the generalizability and robustness of
early stage AD detection.
TABLE 3 | Detailed hyper-parameters of base classiﬁers of the CNN-EL
approach advocated here.
Kernel size
Classiﬁcation Performance
Using the stratiﬁed ﬁvefold cross-validation procedure and
repeating it 10 times, the average classiﬁcation accuracies were
84% for AD vs. HC, 79% for MCIc vs. HC, and 62% for MCIc
vs. MCInc. The average classiﬁcation accuracies for AD vs. HC
and MCIc vs. HC were statistically signiﬁcantly higher than those
achieved using principal component analysis (PCA) plus the
SVM method described in a previous study , while the average classiﬁcation accuracy for MCIc vs.
MCInc was not statistically signiﬁcantly lower . As for the reason why the classiﬁcation accuracy for MCIc
vs. MCInc task was relatively low, we suppose the performance
of the proposed CNN-EL method, as a deep learning approach,
which usually demands more training data, was a little bit more
negatively aﬀected by the insuﬃcient training samples in the
MCIc vs. MCInc classiﬁcation task. Plus, one additional possible
reason might be the cutoﬀthreshold of follow-up duration
to deﬁne MCIc and MCInc, and the cohorts of MCIc and
MCInc subjects might be highly heterogeneous regardless of the
threshold used .
More importantly, the standard deviations of the classiﬁcation
accuracies were only 0.05 for AD vs. HC, 0.04 for MCIc vs. HC,
and 0.06 for MCIc vs. MCInc, all of which were about one-third
of those reported previously .
In this study, all of the experiments were run on one node in
a GPU cluster with ﬁve nodes, each of which had two NVIDIA
Tesla P100-PCIe-16GB 250W cards. For a 1 × 5-fold crossvalidation process, the computing time of the CNN-EL proposed
here in AD vs. HC, MCIc vs. HC, and MCIc vs. MCInc task was
about 21, 19, and 15 h, respectively.
At the same time, the proposed approach here was compared
with the 3D-SENet. As the central building block of CNNs,
the convolution operator could enable networks to acquire
informative features by fusing both spatial and channel-wise
information within local receptive ﬁelds at each layer. To achieve
better generalization performance, the SENet automatically
learned the weight of each feature channel to enhance the
useful features and suppress the useless features for the task
to be tackled, by introducing “Squeeze-and-Excitation” block
as a self-attention function on channels .
Here, the architecture of the compared 3D-SENet model and
corresponding detailed hyper-parameters are shown in Figure 7
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
FIGURE 7 | The architecture of the 3D-SENet model. (A) Convolution block (Conv), (B) Squeeze-and-Excitation block (Se_block), (C) 3D-SENet model.
TABLE 4 | Detailed hyper-parameters of 3D-SENet model.
Layer name
Sub-layer name
Kernel size
Output size
MRI_images
121 × 145 × 121
121 × 145 × 121
Maxpooling
60 × 72 × 60
30 × 36 × 30
30 × 36 × 30
30 × 36 × 30
15 × 18 × 15
15 × 18 × 15
15 × 18 × 15
1 × 1 × 1024
and Table 4, respectively. With 10 × 5-fold cross-validation
processes, the accuracy rates of 0.80 ± 0.05, 0.75 ± 0.07, and
0.57 ± 0.11 were obtained, respectively, for classifying AD vs.
HC, MCIc vs. HC, and MCIc vs. MCInc. For a 1 × 5-fold crossvalidation process, the computing time of the 3D-SENet in AD
vs. HC, MCIc vs. HC, and MCIc vs. MCInc task was about 11.5,
10.9, and 10.6 h, respectively.
In order to evaluate the classiﬁcation performance more
comprehensively, the Area Under the Curve (AUC) and
Matthews Correlation Coeﬃcient (MCC) have
been used as the performance metrics in this study as well.
To verify whether or not our performance is diﬀerent from
those of two methods, i.e., Christian et al. and the
3D-SENet model, we have further run six hypothesis tests (pvalue approach) for three binary experiments, i.e., AD vs. HC,
MCIc vs. HC, and MCIc vs. MCInc. After the homogeneity
of variance test was performed, the Student’s t-test with the
Cox-Cochran correction for unequal variances was applied if
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
TABLE 5 | Comparison of experimental results with PCA + SVM and 3D-SENet.
Experiment model
MCIc vs. HC
MCIc vs. MCInc
0.76 ± 0.11
0.72 ± 0.12
0.66 ± 0.16
0.80 ± 0.05
0.88 ± 0.04
0.62 ± 0.09
0.75 ± 0.07
0.79 ± 0.07
0.42 ± 0.16
0.57 ± 0.11
0.57 ± 0.08
0.11 ± 0.15
CNN + EL proposed here
0.84 ± 0.05
0.92 ± 0.03
0.68 ± 0.10
0.79 ± 0.04
0.83 ± 0.06
0.49 ± 0.12
0.62 ± 0.06
0.59 ± 0.07
0.10 ± 0.15
the homogeneity of variance test failed. Experimental results
and corresponding statistical performance comparisons with pvalues are summarized in Tables 5, 6, respectively. For all three
binary classiﬁcation tasks, the average classiﬁcation accuracies
of the CNN-EL were statistically signiﬁcantly higher than those
achieved using the 3D-SENet, while the standard deviations of
the CNN-EL were lower than or equal to those of the 3D-SENet.
It can be seen that the proposed early detection model for
Alzheimer’s disease based on CNN and EL was more accurate and
robust than the PCA plus SVM method 
and the 3D-SENet model.
Discriminable Brain Regions
In the ﬁrst phase of EL, the validation set was employed to
examine each base classiﬁer and subsequently to acquire three
classiﬁer ensembles based on each of the three single-axis slice
datasets, each comprising of the best ﬁve sagittal, coronal, and
transverse base classiﬁers in generalization capabilities. As a base
classiﬁer corresponds to a slice dataset, all 15 best base classiﬁers
correspond to 15 slices in the X–Y–Z coordinate system, which
can deﬁne 5 × 5 × 5 points in the X–Y–Z coordinate system.
As an example, the sagittal, coronal, and transverse slice numbers
corresponding to the 15 best base classiﬁers for the ﬁrst time to
run the stratiﬁed ﬁvefold cross-validation procedure are shown
in Table 7.
Take the AD vs. HC classiﬁcation task for the ﬁrst time
to run the stratiﬁed ﬁvefold cross-validation procedure as
an example. One hundred twenty-ﬁve points in the X–Y–Z
TABLE 6 | Statistical comparisons with p-values about accuracy mean of the
three methods for (A) AD vs. HC task (upper panel), (B) MCIc vs. HC task (middle
panel), and (C) MCIc vs. MCInc task (lower panel).
proposed here
CNN + EL proposed here
CNN + EL proposed here
CNN + EL proposed here
coordinate system were determined by the top 5 sagittal, coronal,
and transverse slices, respectively, e.g., (22, −5, −23), (20,
−17, −25). . . (28, −7, −11). These 125 points were mapped
onto various brain regions using the Brainnetome Atlas , which can facilitate investigation of structurefunction relationships and comparative neuroanatomical studies.
The Brainnetome Atlas currently contains 246 regions of the
bilateral hemispheres. Moreover, the atlas connectivity-based
parcellation-yielded regions are functionally deﬁned according to
behavioral domain and paradigm class meta-data labels of the
BrainMap database3 using forward and reverse inferences. The
brain regions corresponding to the 125 points in the standard
MNI space were located with the help of the Brainnetome
Atlas. In this way, the brain regions with particularly signiﬁcant
contributions to the classiﬁcation were identiﬁed according to
the number of intersection points located in those regions.
Here, the number of the intersection points determined by the
most discriminable slices separating two classes in a binary
classiﬁcation task among the sagittal, coronal, and transverse
slice sets, transformed into the standard MNI space, acted as an
indicator to assess the contributions of a brain region in which
the points were located to classifying AD. Given that the brain
regions in a discriminable slice contribute to the classiﬁcation
3 
TABLE 7 | Sagittal, coronal, and transverse slice numbers corresponding to the
15 best base classiﬁers for the ﬁrst time to run the stratiﬁed ﬁvefold
cross-validation procedure.
Experiment
Sagittal slice #
Transverse
MCIc vs. HC
MCIc vs. MCInc
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
of AD, we cannot deny the fact that a brain region at which
an intersection point formed by three discriminable sagittal,
coronal, and transverse slices is located contributes most to the
classiﬁcation of AD among all the brain regions that existed in the
sagittal, coronal or transverse discriminable slice since the brain
region exists in the three slices at the same time.
In this way, for all the 10 × 5-fold cross-validation processes,
the number of all the intersection points located in the same
brain region is summed to measure the ability of the brain
region to classify AD. The brain regions identiﬁed with the
most intersection points might be the most discriminable for a
binary classiﬁcation task. Thus, the details of the identiﬁed brain
regions with the classiﬁcation capacity are shown in Figure 8 and
Tables 8a–c. It is notable that the sum of the last column (i.e.,
the number of points located in a brain region) in each of the
three tables was less than 1250 since some intersection points
were located in the unlabeled brain regions. In Figure 8, values
on the vertical and the horizontal axes represent the brain region
labels and the number of intersection points located in each brain
region, respectively. The preﬁx capital letters R and L of a brain
region label (e.g., R.rHipp) refer to the right and left cerebral
hemispheres, respectively.
From the above ﬁgures and tables, the most discriminable
brain regions in the AD vs. HC classiﬁcation task were the
rostral hippocampus , medial amygdala
 , globus pallidus , lateral
amygdala , area 28/34 (EC, entorhinal cortex),
and caudal area 35/36, i.e., parahippocampal gyrus , while those in the MCIc vs. HC classiﬁcation
task were rostral hippocampus , medial
FIGURE 8 | The list of brain regions with the classiﬁcation capacity in each classiﬁcation task. (A) Discriminable brain regions in the AD vs. HC. (B) Discriminable
brain regions in the MCIc vs. HC. (C) Discriminable brain regions in the MCIc vs. MCInc.
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
TABLE 8a | Details of the discriminable brain regions in the AD vs. HC task.
Label of a
brain region
Name of a brain region
# of points located
in a brain region
Rostral hippocampus
Rostral hippocampus
Medial amygdala
Medial amygdala
Caudal area 35/36
Globus pallidus
Globus pallidus
Lateral amygdala
Area 28/34 (EC, entorhinal cortex)
Caudal area 35/36
Caudal hippocampus
Caudal hippocampus
Area TH (medial PPHC)
Area 28/34 (EC, entorhinal cortex)
Dorsolateral putamen
Dorsolateral putamen
Lateral amygdala
Area TL (lateral PPHC, posterior
parahippocampal gyrus)
Nucleus accumbens
Nucleus accumbens
Area TL (lateral PPHC, posterior
parahippocampal gyrus)
amygdala , caudal hippocampus , lateral amygdala , dorsolateral
putamen , rostroventral area 20, i.e., Fusiform
gyrus , globus pallidus , area 28/34 (EC, entorhinal cortex) , and area TL (lateral
PPHC, posterior parahippocampal gyrus) . Finally, the most discriminable brain regions in the
MCIc vs. MCInc classiﬁcation task were rostral area 21 and
anterior superior temporal sulcus, i.e., middle temporal gyrus
 ; rostral area 22 and lateral area 38, i.e.,
superior temporal gyrus ; lateroventral area
37, i.e., fusiform gyrus ; and caudoventral
of area 20 and intermediate lateral area 20 and caudolateral
of area 20, i.e., inferior temporal gyrus 
and caudal hippocampus . The top 10
most discriminable brain regions are mapped onto brain images
in Figure 9.
In the paper , the results showed that
the patients with aMCI (elderly patients with amnestic MCI)
merely had slight atrophy in the inferior parietal lobe of the
left hemisphere but a signiﬁcant diﬀerence was NOT found
in comparison with the NC (normal controls). The results
are consistent with the highly lateralized MCIc vs. MCIncrelated features acquired in this study, to some degree. Plus,
the most discriminable brain regions identiﬁed in the MCIc
vs. MCInc classiﬁcation task in our study were in agreement
with the conclusion of the paper that
TABLE 8b | Details of the discriminable brain regions in the MCIc vs. HC task.
Label of a
brain region
Name of a brain region
# of points located
in a brain region
Rostral hippocampus
Rostral hippocampus
Medial amygdala
Caudal hippocampus
Lateral amygdala
Dorsolateral putamen
Rostroventral area 20
Medial amygdala
Globus pallidus
Area 28/34 (EC, entorhinal cortex)
Globus pallidus
Area TL (lateral PPHC, posterior
parahippocampal gyrus)
TE1.0 and TE1.2
Rostral area 22
Caudal hippocampus
Area TH (medial PPHC)
Caudal area 35/36
Caudal area 35/36
Area 28/34 (EC, entorhinal cortex)
Nucleus accumbens
Rostroventral area 20
Dorsolateral putamen
Ventromedial putamen
Area TL (lateral PPHC, posterior
parahippocampal gyrus)
Rostral area 21
Lateral amygdala
Intermediate ventral area 20
Rostral area 35/36
Nucleus accumbens
Caudoventral of area 20
Anterior superior temporal sulcus
Ventral dysgranular and granular
Lateroventral area 37
Lateral area 38
Intermediate lateral area 20
Occipital thalamus
Ventromedial putamen
the atrophy of cortical thickness and surface area in aMCI
began in the temporal lobe but the range of atrophy gradually
expanded with the progression of disease, to a great extent.
Furthermore, in the paper , the obtained
results were that MCI converters (patients with MCI who
will progress to AD) had more left lateral temporal lobe
atrophy (superior and middle temporal gyrus) and left parietal
atrophy (angular gyrus and inferior parietal lobule) than MCI
non-converters, i.e., stable patients with MCI, and the drawn
conclusion was that by studying two MCI converter vs. nonconverter populations, atrophy beyond the medial temporal
lobe was found to be characteristic of converters and atrophy
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
TABLE 8c | Discriminable brain regions in the MCIc vs. MCInc classiﬁcation task.
Label of a
brain region
Name of a brain region
# of points located
in a brain region
Rostral area 21
Rostral area 22
Lateral area 38
Lateroventral area 37
Caudoventral of area 20
Anterior superior temporal sulcus
Intermediate lateral area 20
Caudal hippocampus
Caudolateral of area 20
Rostral hippocampus
Medial amygdala
Ventral dysgranular and granular
Rostroventral area 20
Medial amygdala
Medial area 38
Rostroventral area 20
Rostral hippocampus
Lateral area 38
Intermediate ventral area 20
Lateroventral area 37
Medial area 38
Ventral agranular insula
Area 1/2/3 (upper limb, head and
face region)
Dorsal dysgranular insula
Intermediate ventral area 20
Area 28/34 (EC, entorhinal cortex)
Nucleus accumbens
Area TI (temporal agranular insular
Caudal ventrolateral area 6
Area 4 (tongue and larynx region)
Dorsal dysgranular insula
TE1.0 and TE1.2
Caudoventral of area 20
Caudal dorsolateral area 6
Area 1/2/3 (tongue and larynx
Medioventral area 37
Area TL (lateral PPHC, posterior
parahippocampal gyrus)
Caudal hippocampus
Rostroventral area 40 (pfop)
Rostral area 22
Rostral area 21
Area 4 (head and face region)
Ventrolateral area 6
Area 28/34 (EC, entorhinal cortex)
Ventral agranular insula
Ventral area 23
Ventral area 44
(Continued)
TABLE 8c | Continued
Label of a
brain region
Name of a brain region
# of points located
in a brain region
TE1.0 and TE1.2
Dorsal granular insula
Hypergranular insula
Opercular area 44
Hypergranular insula
Lateral pre-frontal thalamus
Lateral area 12/47
Nucleus accumbens
of structures such as the left parietal cortex and left lateral
temporal lobe might independently predict conversion. The
results and conclusion were consistent with most of our
results to some extent.
corresponding
behavioral
domains to every identiﬁed brain region were obtained from
FIGURE 9 | Top 10 most discriminable brain regions in each binary
classiﬁcation task: (A) AD vs. HC; (B) MCIc vs. HC; (C) MCIc vs. MCInc.
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
FIGURE 10 | Distributions of the identiﬁed brain regions on the relevant behavioral domains in each binary classiﬁcation task: (A) AD vs. HC; (B) MCIc vs. HC;
(C) MCIc vs. MCInc.
the Brainnetome Atlas oﬃcial website,4 and the functions of
these identiﬁed brain regions were analyzed. Then, the number
of identiﬁed brain regions corresponding to each AD-related
behavioral domain was calculated for each task (Figure 10)
to reveal the distribution of structures showing the largest
diﬀerences between classes and thus most informative for
classiﬁcation (e.g., emotion-related structures for AD vs. HC).
In the ﬁgure, the vertical and horizontal axes show the relevant
behavioral domains and the number of identiﬁed brain regions
associated with these relevant behavioral domains, respectively.
From Figure 10, it can be seen that the functions related
to these identiﬁed brain regions with the discriminability were
mainly involved with the behavioral domains of emotion,
memory, language, perception, internal feelings, and activity.
The most common symptoms of AD, especially in the early
4www.Brainnetome.org
stage, include memory loss that disrupts daily life, challenges in
planning or problem solving, diﬃculty completing familiar tasks
at home, at work, or at leisure, confusion with time or place,
trouble understanding visual images and spatial relationships,
new problems with words in speaking or writing, misplacing
things and losing the ability to retrace steps, decreased or poor
judgment, and changes in mood and personality . Thus, the behavioral domains relevant to
the identiﬁed brain regions were generally consistent with the
common symptoms of AD.
DISCUSSION
In this study, we developed a novel deep learning approach that
combined CNN and EL and applied it to the most commonly
acquired anatomical MRI of the brain, i.e., T1WI. We aimed to
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
achieve two objectives: i.e., classiﬁcation of AD or MCIc vs. HC,
and MCIc vs. MCInc and identiﬁcation of the complex change
patterns associated with AD.
In comparison with a previous PCA plus SVM method
 , the current method does not require
manual selection of ROIs, but automatically extracts the
discriminable features from the MR images using a CNNbased adaptive representation learning method in a datadriven way. The proposed method employs a two-stage EL
scheme to improve generalization and robustness. The model
achieved average classiﬁcation accuracies (± standard deviation)
of 0.84 ± 0.05 for AD vs. HC, 0.79 ± 0.04 for MCIc vs. HC, and
0.62 ± 0.06 for MCIc vs. MCInc. Compared to the PCA plus SVM
method, the proposed method showed statistically substantially
improved accuracy and robustness for distinguishing among the
AD, MCIc, and HC groups, while model accuracy was NOT
statistically lower than that achieved by the PCA plus SVM
method for distinguishing MCIc from MCInc. At the same time,
compared to the 3D-SENet model, the CNN-EL method achieved
statistically higher accuracy and robustness for all the three
binary classiﬁcation tasks.
For a 1 × 5-fold cross-validation processes, we also identiﬁed
the 15 slices and resultant 125 (i.e., 5 × 5 × 5) intersection points
in the standard MNI space based on the ﬁve best base classiﬁers
trained respectively with sagittal, coronal, or transverse slice data.
These points were then mapped onto the Brainnetome Atlas to
identify the corresponding brain regions with the discriminability
in the three binary classiﬁcation tasks. For all the 10 × 5-fold
cross-validation processes, the number of all the intersection
points located in the same brain region was summed to evaluate
the capability of the brain region to help diagnose AD. The
identiﬁed brain regions included hippocampus, amygdala, and
temporal lobe, which are known to be aﬀected by AD and
involved in neurological processes impaired in AD . Also, we acquired the corresponding behavioral
domains based on all identiﬁed brain regions, which were
generally consistent with the common symptoms of AD.
In two-dimensional convolutional neural network (2D-
CNN)-based models for early detection of AD, only sagittal,
coronal, or transverse slices of 3D MR images are usually used
as the training dataset. A speciﬁc slice, such as a transverse
slice through the hippocampus, was often selected based on
experience or prior domain knowledge .
Using only the data from a single 2D slice of a 3D MR image
removes potentially valuable information. In comparison, the
novel CNN-EL approach that we proposed here has the following
signiﬁcant features:
(1) Six data augmentation (DA) methods are used to deal
with the imbalanced data problem by disproportionately
increasing the number of image slices in classes with fewer
samples. As a result, each class can have approximately
an equal increased number of training instances in the
augmented dataset.
identiﬁed from the sagittal, coronal, and transverse slices
of a 3D MRI dataset together, to improve classiﬁcation
accuracy and model adaptability. Each of the base 2D
CNN classiﬁer was trained with the data from a single
slice orientation. Then, the top “N” trained base classiﬁers
were selected according to the generalization performance
on the veriﬁcation dataset to build the ﬁnal ensemble. In
this way, the method eﬀectively improved classiﬁcation
accuracy and robustness. The slices used as training
data to construct base classiﬁers were not necessarily
speciﬁed based on prior domain knowledge; rather, each
available and valid slice (sagittal, coronal, or transverse)
in the dataset was used to train the corresponding
base classiﬁer.
(3) Compared to the length of time spent on building a model
with data from only a single slice orientation, it may take
more time to build the proposed model since many more
base classiﬁers need to be trained. To eﬀectively solve this
problem, the parallel processing method was adopted to
train the base classiﬁers used to build the ensemble model.
This greatly improved the training eﬃciency and made the
proposed model scalable.
(4) According to the classiﬁcation performances of all trained
base classiﬁers on the veriﬁcation dataset, the three
sets of top “N” base classiﬁers trained using data from
sagittal, coronal, and transverse slices, respectively, were
determined. Since a base classiﬁer was trained with the
data from only a speciﬁc slice orientation, the most
important sagittal, coronal, or transverse slice for a binary
classiﬁcation task (e.g., AD vs. HC) could be located
according to the three sets of top “N” base classiﬁers
in a data-driven way. Furthermore, the brain regions
corresponding to the intersection points determined by the
top “N” sagittal, coronal, and transverse slices could be
located with the help of the Brainnetome Atlas. The brain
regions identiﬁed with the most intersection points might
be the most discriminable for a binary classiﬁcation task,
given that the number of the intersection points could be an
indicator to measure the ability of a brain region in which
the points were located to classify AD.
(5) The performance of the proposed classiﬁer ensemble was
compared to that of other machine learning models using
the same dataset. The experimental results showed that
the proposed model achieved better classiﬁcation accuracy
and robustness.
The relatively low classiﬁcation accuracy for MCIc vs. MCInc
warrants further investigation and the classiﬁcation performance
needs to be improved with the optimization methods and/or
other deep learning models to identify the brain regions with
stronger discriminability.
For an individual subject to be diagnosed, the votes of base
classiﬁers in the trained classiﬁer ensemble based on the threeaxis slices and the number of resulting intersection points located
in each brain region might be employed to disclose the extent
to which AD impaired each brain region and each behavioral
domain, which could help understand and evaluate the subject’s
disease status, symptom burden and, more importantly, progress.
Plus, with the advancements of brain atlases and advanced
Frontiers in Neuroscience | www.frontiersin.org
May 2020 | Volume 14 | Article 259
Pan et al.
A Novel Approach Combining CNN and EL
ultra-high-ﬁeld scanners, chances are that the positions and the
number of the intersection points determined by the proposed
CNN-EL methods might provide more details on and insights
into the progress of AD pathology.
Furthermore, the advocated method may be useful for
identifying additional candidate neuroimaging biomarkers for
AD as well as for other brain diseases such as Parkinson’s
disease, autism, schizophrenia and severe depression, especially
for identifying candidate neuroimaging biomarkers for other
little-known brain disorders, in a data-driven way.
The above-mentioned discussions, the clinical implication of
the ﬁnding applying other samples, and the generalizability of
the advocated CNN-EL approach need to be examined in the
future research.
DATA AVAILABILITY STATEMENT
The datasets analyzed for this study can be found in the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) database
(adni.loni.usc.edu).
AUTHOR CONTRIBUTIONS
DP and AZ designed and coordinated the study. LJ, DP,
YH, and AZ carried out experiment and data process. XS
reviewed the study design and data processing, and edited
results interpretation and presentation. All authors drafted and
revised the manuscript, and approved the ﬁnal version of the
submitted manuscript.
ACKNOWLEDGMENTS
We thank the reviewers for their constructive comments
on this article. This study was supported by NSF of China
(Grant Nos. 61976058 and 61772143), Science and Technology
Planning Project of Guangdong (Grant No. 2019A050510041),
2019B010109001), and Science and Technology Planning Project
of Guangzhou (Grant Nos. 202002020090 and 201804010278).
Additional funding for neurological research was from Surrey
Hospital & Outpatient Centre Foundation (FHG2017-001).
Data collection and sharing for this project were funded by the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National
Institutes of Health Grant U01 AG024904) and DOD ADNI
(Department of Defense Award No. W81XWH-12-2-0012).
ADNI was funded by the National Institute on Aging, the
National Institute of Biomedical Imaging and Bioengineering,
contributions
following:
AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery
Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-
Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan
Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F.
Hoﬀmann-La Roche Ltd and its aﬃliated company Genentech,
Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer
Immunotherapy Research & Development, LLC.; Johnson
& Johnson Pharmaceutical Research & Development LLC.;
Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics,
LLC.; NeuroRx Research; Neurotrack Technologies; Novartis
Pharmaceuticals Corporation; Pﬁzer Inc.; Piramal Imaging;
Servier; Takeda Pharmaceutical Company; and Transition
Therapeutics. The Canadian Institutes of Health Research was
providing funds to support ADNI clinical sites in Canada.
Private sector contributions are facilitated by the Foundation
for the National Institutes of Health (www.fnih.org). The grantee
organization is the Northern California Institute for Research
and Education, and the study is coordinated by the Alzheimer’s
Therapeutic Research Institute at the University of Southern
California. ADNI data are disseminated by the Laboratory
for Neuro Imaging at the University of Southern California.
Additional funding support for the manuscript preparation was
from the Surrey Hospital & Outpatient Centre Foundation of
Fraser Health, Canada.