Machine Learning, 31, 55–85 
c⃝1998 Kluwer Academic Publishers. Manufactured in The Netherlands.
Module-Based Reinforcement Learning:
Experiments with a Real Robot *
ZSOLT KALM´AR
 
Department of Informatics, “J´ozsef Attila” University of Szeged,
Szeged, Aradi vrt. tere 1, Hungary H-6720
CSABA SZEPESV´ARI
 
Research Group on Artiﬁcial Intelligence, “J´ozsef Attila” University of Szeged
Szeged, Aradi vrt. tere 1, Hungary H-6720
ANDR´AS L˝ORINCZ
 
Department of Adaptive Systems, “J´ozsef Attila” University of Szeged
Szeged, Aradi vrt. tere 1, Hungary H-6720
Editors: Henry Hexmoor and Maja Matari´c
Abstract. The behavior of reinforcement learning (RL) algorithms is best understood in completely observable,
discrete-time controlled Markov chains with ﬁnite state and action spaces. In contrast, robot-learning domains are
inherently continuous both in time and space, and moreover are partially observable. Here we suggest a systematic
approach to solve such problems in which the available qualitative and quantitative knowledge is used to reduce
the complexity of learning task. The steps of the design process are to: i) decompose the task into subtasks using
the qualitative knowledge at hand; ii) design local controllers to solve the subtasks using the available quantitative
knowledge and iii) learn a coordination of these controllers by means of reinforcement learning. It is argued
that the approach enables fast, semi-automatic, but still high quality robot-control as no ﬁne-tuning of the local
controllers is needed. The approach was veriﬁed on a non-trivial real-life robot task. Several RL algorithms
were compared by ANOVA and it was found that the model-based approach worked signiﬁcantly better than the
model-free approach. The learnt switching strategy performed comparably to a handcrafted version. Moreover,
the learnt strategy seemed to exploit certain properties of the environment which were not foreseen in advance,
thus supporting the view that adaptive algorithms are advantageous to non-adaptive ones in complex environments.
Keywords: reinforcement learning, module-based RL, robot learning, problem decomposition, Markovian Decision Problems, feature space, subgoals, local control, switching control
Introduction
Reinforcement learning (RL) is the process of learning the coordination of concurrent behaviors and their timing so as to optimize some performance cost, where the cost is a function
of the reinforcement signals communicated to the learner in each time step. A few years ago
Markovian Decision Problems (MDPs) were proposed as the model for the analysis of RL
 and since then, a mathematically well-founded theory has
been constructed for a large class of RL algorithms. These algorithms are based on modi-
ﬁcations of the two basic dynamic-programming algorithms used to solve MDPs, namely
the value- and policy-iteration algorithms .
The RL algo-
Present address of all authors: Associative Computing Ltd.. Budapest 1121, Konkoly Thege M. ´ut 29–33
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
rithms learn via experience, gradually building an estimate of the optimal value function,
which is known to encompass all the knowledge needed to behave in an optimal way according to a ﬁxed criterion, usually the expected total discounted-cost criterion. The basic
limitations of all of the early theoretical results of these algorithms was that these assumed
ﬁnite state- and action-spaces, and discrete-time models in which the state was assumed to
be available for measurement. In a real-life problem, however, the state- and action-spaces
are inﬁnite, usually non-discrete, time is continuous and the system’s state is not measurable (i.e., with the latter property, the process is only partially observable as opposed to
being completely observable) . Recognizing the serious drawbacks
of the simple theoretical case, researchers have begun looking at the more interesting yet
theoretically more difﬁcult cases . To date, however, no complete and
theoretically sound solution has been found to deal with such involved problems. In fact
the above-mentioned learning problem is indeed intractable owing to partial observability.
This result follows from a theorem of Littman’s .
One of the most promising approaches, originally suggested to deal with large, but observable problems, is based on the idea of decomposing the task into smaller subtasks. This very
basic idea can be traced back at least to the idea of using abstractions, macro-operators and
subgoals which was studied by Newell and Simon and Korf 
in planning domains. The attractive property of this decomposition is that if it is done in a
hierarchical manner then it can reduce exponential complexity to linear . In the
framework of planning with MDPs the plan acceleration aspect of using macro-operators
has recently received some attention . On the other hand, the
learning of macro-operators, which can be interpreted as learning control modules, has a
longer history . The third aspect
is the learning of the switching of the particular controllers, which has been studied among
others by Singh and more recently by Parr and Russell in hierarchical models; by Matari´c who used a modiﬁed RL algorithm; and by Koza and Rice 
and Dorigo and Colombetti who made use of a genetic algorithms, to mention just
a few examples. Inventing subgoals, macro-operators and hierarchies turns to be a more
difﬁcult problem. Some results in this direction include those of Schwartz and
T´oth et al., whose algorithms learn skills useful in multiple tasks, or Wiering and
Schmidhuber who deal with partial observability and follow a divide and conquer
approach. Switching controls have also received considerable attention among traditional
control theorists under the name hybrid control ,
but they usually focused on more basic properties like existence or stability of solutions
 and the references therein), or the existence of
optimal controls . Recently, the existence of optimal switching strategies
was proved by Branicky for a fairly broad class of systems.
In this article, we propose a systematic approach to solve real-world robotic tasks which
builds on the above mentioned works. In order to keep the problems tractable we suggest
to incorporate a priori knowledge when available and use learning only when it is really
Namely, we suggest that high-level, abstract qualitative knowledge, which is
often available, can and should be used in the planning phase to identify subgoals and
macro-operators; quantitative, but still rough knowledge can and should be used to design
MODULE-BASED REINFORCEMENT LEARNING
controllers that safely implement the macro-operators under some well deﬁned conditions
which should be made measurable from the observation process; and learning should be
used to resolve conﬂicts when the operating condition of more than one controller is met.
Since we know in advance that learning will be used to ﬁnd the appropriate switching
function we may bravely incorporate alternative solutions to the same subtask. Another
goal to be taken into account in the design phase is that the task to be solved by the learning
algorithm should have a ﬁnite (small) state & action space, and be a completely observable
In the ﬁrst part of the article (Section 2), in addition to discussing the above design principles in detail, some theoretical tools are put forth which can be used to determine if a
given subtask decomposition is proper, i.e., if it results in a solution to the original problem.
Also convergence issues of the learning algorithms are touched upon. In the second part
(Section 3), the approach is demonstrated via a real-life example. We provide a detailed statistical comparison of several RL methods combined with different exploration strategies,
such as Adaptive Dynamic Programming (ADP), Adaptive Real-Time Dynamic Programming (ARTDP) and Q-learning, with Boltzmann exploration started from different initial
“temperatures”. The relationship of our work to that of others is described in more detail
in Section 4, and then ﬁnally our conclusions and possible directions for further research
are given in Section 5.
Module-based reinforcement learning
First of all, we will brieﬂy overview Markovian Decision Problems (MDPs), a valuefunction-approximation-based RL algorithm to learn solutions for MDPs and their associated theory. Next, the concept of recursive features and time-discretization based on these
features are elaborated upon. This is then followed by a sensible deﬁnition and principles
of module-design, together with a brief explanation of why the modular approach can prove
successful in practice.
Markovian decision problems
RL is the process by which an agent improves its behavior from observing its own interactions with the environment. One particularly well-studied RL scenario is that of a
single agent minimizing the expected discounted total cost in a discrete-time ﬁnite-state,
ﬁnite-action environment, in which the theory of MDPs can be used as the underlying
mathematical model. A ﬁnite MDP is deﬁned by the 4-tuple ⟨S, A, p, c⟩, where S is a
ﬁnite set of states, A is a ﬁnite set of actions, p is a matrix of transition probabilities, and
c is the so-called immediate cost function. The ultimate objective of learning is identifying an optimal policy. A policy is some function that tells the agent which set of actions
should be chosen under which circumstances. A policy π is optimal under the expected
discounted total-cost criterion if, with respect to the space of all possible policies, π results
in a minimum expected discounted total cost for all states. The optimal policy can be found
by identifying the optimal value function, deﬁned recursively by
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
v∗(s) = min
c (s, a) + γ
p (s, a, s′) v∗(s′)
for all states s ∈S, where c (s, a) is the immediate cost for taking action a from state s, γ is
the discount factor, and p (s, a, s′) is the probability that state s′ is reached from state s when
action a is chosen. U (s) is the set of admissible actions in state s. The policy which for each
state selects the action that minimizes the right-hand side of the above ﬁxed-point equation
constitutes an optimal policy. This yields the result that to identify an optimal policy it is
sufﬁcient just to ﬁnd the optimal value function v∗. The above simultaneous non-linear
equations (non-linear because of the presence of the minimization operator), also known as
the Bellman equations , can be solved by various dynamic-programming
methods such as the value- or policy-iteration methods .
RL algorithms are generalizations of the DP methods to the case when the transition
probabilities and immediate costs are unknown. The class of RL algorithms of interest
here can be viewed as variants of the value-iteration method: these algorithms gradually
improve an estimate of the optimal value-function via learning from the interactions with
the environment. There are two possible ways to learn the optimal value function. One is
to estimate the model (i.e., the transition probabilities and immediate costs) while the other
is to estimate the optimal action-values directly. The optimal action-value of an action a
given a state s is deﬁned as the total expected discounted cost of executing the action from
the given state and proceeding in an optimal fashion afterwards:
Q∗(s, a) = c(s, a) + γ
p(s, a, s′)v∗(s′).
The general structure of value-function-approximation based RL algorithms is given in
In the RL algorithms, various models are utilized along with an update rule Ft and actionselection rule St. In the case of the Adaptive Real-Time Dynamic Programming (ARTDP)
algorithm the model consists (Mt) of the estimates of the transition probabilities and costs,
the update-rule Ft being implemented, e.g., through the equations
pt+1(st, at, s) =
nt(st, at)
pt(st, at, s) +
nt(st, at) δ(st+1, s),
ct+1(st, at) =
nt(st, at)
ct(st, at) +
nt(st, at) ct,
where δ(i, j) is the Kronecker-function, nt(s, a) is the number of times the state-action pair
(s, a) was visited by the process {(st,at)}t before time t plus one, and values not shown are
left unchanged. Instead of the optimal Q-function, the optimal value function is estimated
and stored to spare storage space, and the Q-values are then computed by replacing the true
transition probabilities, costs and the optimal value function in Eq. 1 by their estimates. An
update of the estimate for the optimal value function is implemented by an asynchronous
dynamic-programming algorithm using an inner loop in Step 2 of the algorithm. In each
step of this loop, a subset of the states, F t
j , is selected and the value estimates of the states
j are updated via
MODULE-BASED REINFORCEMENT LEARNING
Table 1. The structure of value-function-approximation-based RL algorithms. Speciﬁc forms for the model
update operators Ft and the action selection operator St are deﬁned in the examples presented below.
Let t = 0, and initialize the utilized model (M0) and the Q-function (Q0)
Repeat forever
(A) Observe the next state st+1 and reinforcement signal ct.
(B) Incorporate the new experience (st, at, st+1, ct) into the model and into the estimate of the optimal Q-function:
(Mt+1, Qt+1) = Ft(Mt, Qt, (st, at, st+1, ct)),
where Ft is the model update operator.
(C) Choose the next action to be executed based on (Mt+1, Qt+1):
at+1 = St(Mt+1, Qt+1, st+1)
and execute the selected action, where St is the action selection operator.
(D) t := t + 1.
v(s) := min
ct+1(s, a) + γ
pt+1(s, a, s′)v(s′)
v being initialized to vt at the beginning of the loop and letting vt+1 = v at the end of
the loop. Algorithms where the value of the actual state is updated are called “real time”
 . If, in each step, all the states are updated (F t
j = S), and the inner
loop is run until convergence is reached, the resulting algorithm will be called Adaptive
Dynamic Programming (ADP). Another popular RL algorithm is Q-learning, which does
not employ a model but instead the Q-values are updated directly according to the iteration
procedure 
Qt+1(st, at) = (1 −αt(st, at))Qt(st, at) + αt(st, at)
ct + γ min
a Qt(st+1, a)
where αt(st, at) ≥0,
αt(s, a)δ(s, st)δ(a, at) = ∞
t(s, a)δ(s, st)δ(a, at) < ∞.
For example, one might set αt(s, a) =
nt(s,a) but often in practice αt(s, a) = const is
employed, which, while yielding increased adaptivity, no longer ensures convergence.
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
Both algorithms mentioned previously are guaranteed to converge to the optimal value/Q
function if each state-action pair is updated inﬁnitely often . The action-selection procedure St should be carefully chosen so that it ﬁts the
dynamics of the controlled process in a way that the condition is met. For example, the
execution of random actions meets this “sufﬁcient-exploration” condition when the MDP
is communicating. However, if on-line performance is important, then more sophisticated
explorationisneeded, which, inadditiontoensuringsufﬁcientexploratorybehavior, exploits
accumulated knowledge. If the Q-values were already exact, then, according to what was
explained above, the optimal action in state s would be argmina Qt(s, a); the choice of
such actions corresponding to pure exploitation. Unfortunately, pure exploitation applied
from the beginning of learning will not work in general ). Typical suggestions to overcome
these difﬁculties include choosing random actions occasionally and exploiting actions at
other times, or to select actions that minimize some kind of artiﬁcially biased Q-values,
where the bias is such that the biased Q-values of less often visited state-action pairs become
smaller ). The most popular of these
is randomization when the exploiting action is chosen with a probability smaller than one,
while having this probability converge slowly to one with time. Recently, it has been
shown that if the probability of selecting non-exploiting actions summed up in time equals
inﬁnity (for example, when this probability is proportional to 1/nt(s), where nt(s) is the
number of times the state s was visited before time t increased by 1), then, on the one
hand, sufﬁcient exploration is ensured while on the other, the whole process eventually
converges to optimality . The most common form
of randomized action selection is called “Boltzmann exploration”, where the probability of
choosing action a in state s equals
eQt(s,a)/T (s,t)
a∈U(s) eQt(s,a)/T (s,t) ,
where T(s, t) is a “temperature” parameter whose rate of decrease with t should be
bounded from below by o(1/ ln(nt(s))) if one wants to ensure sufﬁcient exploration
 .
Recursive features and feature-based time discretization
In the case of a real-life robot-learning task, the dynamics cannot be formulated exactly as a
ﬁnite MDP, nor is the state information available for measurement. This latter restriction is
modeled by Partially Observable MDPs (POMDPs) where (in the simplest case) one extends
an MDP with an observation function h, which maps the set of states S into a set X, called
the observation set (which is usually non-countable, just like S). The deﬁning assumption
of a POMDP is that the full state s can be observed only through the observation function,
i.e., only h(s) is available as input and this information alone is usually insufﬁcient for
efﬁcient control since h is usually a non-injection (i.e., h may map different states to the
same observations). Features which mathematically are just well-designed observation
functions, are known to be efﬁcient in dealing with the problem of inﬁnite state spaces.
MODULE-BASED REINFORCEMENT LEARNING
Moreover, when their deﬁnitions are extended in a sensible way, they become efﬁcient in
dealing with partial observability.
It is well known that in the partial observable case optimal policies can depend on their
whole past histories. This leads us to a generalization of features, such that the feature’s
values can depend on all past observations, i.e., mathematically a feature becomes an inﬁnite
sequence of mappings (f 0, f 1, . . . ft, . . .), with f t : (X × A)t × X →F, where F and
X are the feature- and observation-spaces. Since RL is supposed to work on the output
of features, and RL requires ﬁnite spaces it means that F should be ﬁnite. Features that
require inﬁnite memory are clearly impossible to implement, so features used in practice
should be restricted in such a way that they require a ﬁnite “memory”. For example, besides
stationary features, which take the form (f 0, f 0, . . . , f0, . . .) (i.e., f t = f 0 for all t ≥1)
and are called sensor-based features, recursive features (in control theory these are called
ﬁlters or state-estimators) are those that can be implemented using a ﬁnite memory.1 For
example, in the case of a one-depth recursive feature the value at the tth step is given by
ft = R(xt, at−1, ft−1), where R : X ×A×F →F deﬁnes the recursion and f0 = f 0(x0)
for some function f 0 : X →F. Features whose values depend on the past observations of
a ﬁnite window form a special class of recursive ﬁlters.2
A very simple one-step recursive feature is the switching feature (of Boolean type) whose
value depends on two disjoint sets of observation-action pairs, those of “on-pairs” and “offpairs”. The feature’s value is one (or ‘on’), if the last observed observation-action pair
which was either an on-pair or off-pair is an on-pair, otherwise it is zero (‘off’). In other
words, the feature’s value does not change as long as the observation is outside the union
of the sets of “on-pairs” and “off-pairs” and the feature’s value is reset to the label of these
sets when the observation data gets into either of them. In Section 3.2.2 we will give an
example of such a feature.
Instead of relying on a single feature, it is usually more convenient to deﬁne and employ
a set of features, each of which indicates a certain event of interest. Note that a ﬁnite set
of features can always be replaced by a single feature whose output space is the Cartesian
product of the output spaces of the individual features and whose values can be computed
componentwise by the individual single features. That is to say, the new feature’s values
are the ‘concatenated’ values of the individual features.
Since the feature space is ﬁnite, a natural discretization of time can be obtained. The new
time counter clicks only when the feature value jumps in the feature space.3 This makes it
useful to think of such features as event-indicators which represent the actuality of certain
conditions of interest. This interpretation gives us an idea of how to deﬁne features in such
a way that the dynamics at the level of the new counter be simpliﬁed.
So far, we have realized that the new “state space” (the feature space) and “time” can be made
discrete. However, the action space may still be inﬁnite. Uniform discretization which lacks
a priori knowledge would again be impractical in most of the cases (especially when the
action space is unbounded), so we would rather consider an idea motivated by a technique
that is often applied in artiﬁcial intelligence (AI) to solve large search problems efﬁciently.
The method in question follows a kind of “divide-and-conquer” approach which divides the
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
problem into smaller subproblems that in turn are divided into even smaller subproblems,
etc., then at the end routines are provided that deal with the resulting mini-problems. The
solution of the entire problem is then obtained by working backwards: the routines that
solve mini-problems are combined to get larger routines, then these are combined again
to get even larger routines, and this is repeated until the root of the hierarchy is reached.
In planning domains the combined routines are called macro-actions. The actual solution
of the original problem can be obtained if the macro-action corresponding to the actual
state of the search problem is applied . To put
it another way, the problem solver deﬁnes a set of sub-goals, sub-sub-subgoals, etc. in
such a way that if one of the sub-goals is satisﬁed then the resolution of the main goal
will be easier to achieve. It turns out that hierarchical subgoal decomposition can reduce
the problem complexity from exponential to linear if the decomposition scheme is optimal
 . Also in robotic domains sometimes it is convinent to deﬁne a subgoals by
means of specifying a ‘desired beavior’ pattern.
In control tasks the same decomposition can usually be done with respect to the main
control objective without any difﬁculty provided that a qualitatively correct model of the
plant is available. A model is said to be qualitatively correct if it leads to designs which
when extended with learning in later stages result in a proper solution of the task, i.e., we do
not require that a proper solution of the task could be obtained on the basis of a qualitatively
correct model. So qualitatively correct models should be much easier to obtain than ones
which are actually used for control. Human are usually good at obtaining a decomposition
if they have sufﬁcient knowledge about the controls and sensors. Note that this model
should ideally be low level, i.e., it should describe the dynamics of sensors and the plant,
but at the same time it should hide details which are unimportant from the point of view of
planning. A naive physics description of the problem seems to be suitable if one wanted to
automate this step using a planner. Qualitative modelling has a long tradition in artiﬁcial
intelligence .
Nevertheless, regardless of what representation and method is used, we end up with a set of
macro-actions and their associated subgoals. In the next step the designer should implement
the macro-actions as closed-loop local controllers which achieve the associated subgoal.
In order to be successful at this task, in general more detailed, quantitative knowledge of
the plant is needed (see Figure 1). A quantitatively correct model should be suitable for
designing local controllers which work as intended under well deﬁned (and observable)
conditions. Note that having a quantitatively correct model still does not mean that we can
solve the control task without any further reﬁnements. If there is considerable uncertainty
in the quantitative model then these local controllers may also have to be learnt by an
adaptive method, such as e.g., an adaptive control technique, iterative learning or even by
reinforcement learning. Even when using some kind of adaption or robustiﬁcation, one
cannot expect that the resulting controller will work reliably under all possible conditions
that can occur. The other reason to restrict the operating conditions of the controllers is that,
for example, in the case of serializable subgoals in order the application of the
macro-action to make sense the previous subgoal must be met. The operating conditions
should be given as measurable quantities. The controllers together with their operating
conditions, which may also serve as a basic set of features, will be called modules. The
process of breaking up the problem into small subtasks should be repeated several times
MODULE-BASED REINFORCEMENT LEARNING
recursivelybeforetheactualcontrollersaredesigned, sothatthecomplexityoftheindividual
controllers can be kept low.
In complex problems, it may happen that a particular controller proves to be useful in
accomplishing several subtasks. For example, in a mobile-robot task such a general-purpose
controller could be that which ’rescues’ the robot when it becomes stuck.
In principle, a consistent transfer of the AI decomposition yields the result that the
operating conditions of the situation are exclusive and cover every situation. However,
such a solution would be very sensitive to perturbations and unmodeled dynamics and is
hard to achieve due to the ambiguities in the plant models. A more robust solution can
be obtained by considering broader operating conditions or designing alternate modes to
solve the problem. This, however, yields that more than one controller can be applied at the
same time, under the same conditions. This calls for the introduction of a mechanism, the
switching function, that determines which controller has to be activated if there are more
than one available. This decision should, however, be made solely on the basis of the state
of operating conditions and some some possible additional auxiliary ﬁlters. These together
compose the feature vector available for the switching mechanism.
Quantitative World Model
Qualitative World Model
Controller A
Op. Cond. A
Controller B
Op. Cond. B
Controller C
Op. Cond. C
Robot Programming
Reinforcement
State Space
Op. Cond. A
Op. Cond. B
Op. Cond. C
Figure 1. Illustration of the proposed robot programming method. The main task is divided into subtasks
to reduce complexity of design and learning at lower levels. Each subtask has a controller associated to it and
the controllers have operating conditions under which the controller can safely accomplish the given subtask.
Controllers and their operating conditions, which are referred together as modules, can be designed by hand on
the basis of available quantitative knowledge. Operating conditions of different controllers may overlap in which
case more than one controller may be applied at the same time. Reinforcement learning is applied to remove this
ambiguity in an optimal way.
So the switching function S maps feature vectors to the index of the module that should
be activated when the actual feature vector under consideration is observed. Of course,
only those modules can be activated whose operating conditions are satisﬁed.4 The operation of the whole mechanism is then the following. A controller remains active until the
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
switching function switches to another controller. Since the switching function depends on
the observed feature values, the controllers will certainly remain active until a change in the
feature vector is observed. We further allow the controllers to manipulate the observation
process. In this the controllers may inhibit an observation from occurring and thus may
hold up their activity for a while, i.e., the controller works then in an open-loop mode. This
yields a rougher time-discretization, which reduces the problem complexity again (since
less number of decisions is needed). The working mechanism is illustrated in Figure 2.
Controller A
Auxiliary Features
Feature Vector
Reinforcement
Op. Cond. A
Environment
Controller B
Op. Cond. B
Reinforcement Learning
Controller C
Op. Cond. C
Figure 2. The control and learning mechanisms. At any time one and only one controller controls the plant. This
controller is selected by the switching function learnt by RL. Switching can occur only when the feature vector
changes which is the concatenation of the operating conditions of the controllers and some auxiliary features.
The reinforcement signal is also a function of this feature vector. Controllers can only be activated when their
associated operating conditions are observed.
Accessibility decision problems
The goal of the design procedure is to set up the modules and additional features in such
a way that there exists a switching controller S : F →{1, 2, . . . , n}, which for any given
history results in a closed-loop behavior that fulﬁlls the “goal” of control in time. It can
be extremely hard to prove even the existence of such a valid switching controller. One
approach is to use a so-called accessibility decision problem for this purpose, which is a 5tuple ⟨X, A, r, T , A⟩, where X is the state-space, A is the action space, r : X ×A×X →R
gives the immediate reward associated with transitions, T : X×A →P(X) is the transition
mapping determining the states which are accessible from any given state-action pair, and
A : X →A gives us the admissible actions for any state x.
In our case the state space is F (the possible values of the composed feature-vector), the
action space is {1, 2, . . . , n}, the indices of the controllers, and the transition mapping is
deﬁned in the following way. The states that are accessible from feature-state f when using
action i are those elements f ′ of F for which there exists a history compatible with the
feature f, such that when assuming the given history and using the controller indexed by i
the next observed feature different from f will be f ′. This means that the utilized controllers
should be designed such that their operating conditions should not remain satisﬁed forever.5
MODULE-BASED REINFORCEMENT LEARNING
However, this is quite a natural condition since when translated back to the level of design
it just means that each subtask should be completed in ﬁnite time.6
There are two ways of meeting this ﬁnite-ﬁnishing-time requirement. First, design each
controller with special attention to the problem, or second employ special features such as
operating conditions, which, once “activated”, terminate in some ﬁnite time. Continuing
with the deﬁnition of the accessibility decision problem, the set of admissible “actions”
corresponding to a feature-vector f consists of the index of those modules whose operating
conditions are satisﬁed in f. Assume that the ultimate goal of control is to reach a certain
subset of F (for this, goal-indicator features should be provided). Let us now set up a reward
functionr asafunctionofthefeaturessuchthatr(f) = 1onlyiff isinthegoalset, otherwise
r(f) = 0. In the context of hybrid control, Sastry et al., noted the following: If there
exists a controller for the above accessibility decision problem where the cumulated worstcase reward is non-zero ) then there exists a switching controller that can solve the original problem
 . Such a controller will be called proper in the worst-case sense.
The reverse of the above implication is not necessarily true and this makes the analysis
somewhat limited.
Besides this, the exact accessibility decision problem can be very
difﬁcult to construct – usually it is easier to construct a slightly broader one, which has
additional transitions in it. Another problem is that the exact transitions may well already
revoke the existence of a controller that is proper in the above worst-case sense.
A weaker condition to the above, which may seem somewhat artiﬁcial at a ﬁrst glance,
is that the total probability of reaching the goal for all possible accessibility-compatible
transition probabilities should equal one. The idea behind it is that under certain conditions,
such as ergodicity, we may assume that transitions can be modeled probabilistically. A
transition-probability matrix is called accessibility-compatible if the transition-probability
associated with a given transition (f, i, f ′) is non-zero if and only if f ′ is accessible from
f using i in the sense of the deﬁnition given in the previous paragraph. Clearly, if there is a
path in the graph underlying the accessibility transitions from each non-goal state f to one of
the goal states, then there exists a switching policy under which the probability of reaching
the goal states is one for each accessibility-compatible transition-probability matrix. Such
a switching strategy will be called an almost surely proper switching. Notice that due to
the ﬁniteness of the problem the expected number of steps to reach the goal will almost
certainly be ﬁnite under any proper switching. If we knew that transitions take bounded
physical time (this is not immediately obvious since the same transition may happen under
an inﬁnite number of different conditions because the process has an inﬁnite state space
and also transitions can depend on the history of the process – another inﬁnite set), then the
expected total physical time will also be bounded.
Of course, since the deﬁnitions of the modules and features depend on the designer, it
is reasonable to assume that by clever design a satisfactory decomposition and controllers
could be found even if only qualitative properties of the controlled object were known. RL
could then be used for two purposes: either to ﬁnd the best switching function assuming
that at least two proper switching functions exist, or to decide empirically whether a valid
switching controller exists at all. The ﬁrst kind of application of RL arises as result of the
desire to guarantee the existence of a proper switching function through the introduction
of more modules and features than is minimally needed. But then good switching that
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
exploits the capabilities of all the available modules could well become too complicated to
ﬁnd manually.
RL and ε-stationary decision problems
If the accessibility decision problem were extendible with transition-probabilities to turn
it to an MDP7 then RL could be rightly applied to ﬁnd the best switching function. For
example if one uses a ﬁxed (maybe stochastic) stationary switching policy and provided
that the system dynamics can be formulated as an MDP, then there is a theoretically wellfounded way of introducing transition-probabilities . Unfortunately,
the resulting probabilities may well depend on the switching policy which can prevent the
convergence of the RL algorithms.
However, the following “stability” theorem shows that the difference of the cost of optimal
policies corresponding to different transition probabilities is proportional to the extent
the transition probabilities differ, so we may expect that a slight change in the transition
probabilities does not result in completely different optimal switching policies and hence,
as will be explained shortly after the theorem, we may expect RL to work properly, after
Theorem 1 Assume that two MDPs differ only in their transition-probability matrices,
and let these two matrices be denoted by p1 and p2. Let the corresponding optimal cost
functions be v∗
2|| ≤γ nC||p1 −p2||
where C = ||c|| is the maximum of the immediate costs, || · || denotes the supremum norm
and n is the size of the state space.
Let Ti be the optimal cost operator corresponding to the transition-probability
matrix pi, i.e.,
(Tiv)(s) = min
c(s, a) + γ
pi(s, a, s′)v(s′)
, v : S →ℜ, i = 1, 2.
Proceeding with standard ﬁxed point and contraction arguments ) we get that ∥v∗
2∥and since T1
is a contraction with index γ, and the inequality ∥T1v−T2v∥≤γ∥p1−p2∥P
y∈X |v(y)| we
obtain δ = ∥v∗
2∥≤γδ+γ∥p1−p2∥|X|C/(1 −γ), where ∥v∗
2∥≤C/(1−γ) has been
employed . Rearranging the inequality in terms of δ then yields Theorem 1.
Motivated by the previous theorem, we deﬁne ε-stationary MDPs as the 4-tuple
⟨S, A, p, c⟩, where S, A and c are as before but p, the transition probability matrix, may vary
in time but with ||pt −p∗|| ≤ε holding for all t > 0. Our expectations are that although
the transitions cannot be modeled with a ﬁxed transition probability matrix (i.e., stationary
MODULE-BASED REINFORCEMENT LEARNING
MDP), they can be modeled by an ε-stationary one even if the switching functions are
arbitrarily varied.
By still assuming that a stationary MDP corresponds to a ﬁxed switching, we obtain,
if a set (maybe stochastic) switching policy is followed during learning, that the values
learnt by RL will converge somewhere. However, on-line RL will change the explorationpolicy continuously, which may result in oscillating transition-probabilities. We conjecture
that the method developed by Szepesv´ari and Littman can
still be applied, but now with a reduced goal to show that if during learning the transition
probabilities were oscillating slightly (say remaining within a set of diameter ε) then RL
methods would result in oscillating estimates of the optimal value function, but with the
oscillation being asymptotically proportional to ε. Again this is an incomplete result as
it leaves open the question of whether the oscillations in the transition probabilities are
asymptotically small, which can be hard to answer since the actual policy executed during
learning usually depends on the estimated values of the optimal cost function, and the
transition probabilities may depend on the learnt values, which all go to close the circle. A
possible way out of this vicious circle is to make use of an assumption like, say, that the
transition probabilities corresponding to different policies should not differ too much at all,
i.e., to assume that the MDP is ε-stationary. This property was clearly observed in our
experiments, which we will now describe.
Experiments
The validity of the proposed method was checked with actual experiments carried out
using a Khepera robot. After the speciﬁcation of the task the modules were designed and
then several RL algorithms were tried and compared. The description of the robot, the
experimental setup, general speciﬁcations of the modules, and the results are all presented
in this section. We would expect similar results for other robots, too.
The robot and its environment
The mobile robot employed in the experiments is shown in Figure 3.
It is a Khepera8 robot equipped with eight IR sensors, six in the front and two at the
back, the IR sensors measuring the proximity of objects in the range 0-5 cm. The robot
has two wheels driven by two independent DC motors and a gripper that has two degrees
of freedom and is equipped with a resistivity sensor and an object-presence sensor. The
vision turret is mounted on the top of the robot as shown. It is an image sensor giving a
linear image of the horizontal view of the environment with a resolution of 64 pixels and
256 levels of gray. The horizontal viewing angle is limited to about 36 degrees. This sensor
is designed to detect objects in front of the robot situated at a distance spanning 5 to 50 cm.
The image sensor has no tilt angle, so the robot observes only those things whose height is
exceeds 5 cm.
The learning task was deﬁned as follows: ﬁnd a ball in an arena, bring it to one of the
corners marked by a stick and hit the stick with the ball. The robot’s environment is shown
in Figure 3. The size of the arena was 50 cm x 50 cm with a black colored ﬂoor and
white colored walls. The stick was black and 7 cm long, while three white-colored balls
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
Figure 3. The Khepera and the experimental environment. The top-left sub-ﬁgure shows a close-up on the
Khepera robot. The robot has two independent wheels, a gripper, a vision turret on the top of it (this can be better
observed in the third sub-ﬁgure) and proximity sensors. The task was to grasp a ball and hit the stick with it. The
top-right sub-ﬁgure shows a phase when the robot is searching for a ball, while the third sub-ﬁgure shows a case
when the robot is just about to hit the stick by the ball. The umbilical cord can also be seen in the ﬁgures.
with diameter 3.5 cm were scattered about in the arena. The environment is highly chaotic
because the balls move in an unpredictable manner and so the outcome of certain actions is
not completely predictable, e.g., a grasped ball may easily slip out from the gripper. Note
also that the task is quite complex compared to the tasks considered in the mobile learning
literature .
The modules
Subtask decomposition
Firstly, according to the principles laid down in Section
2, the task was decomposed into subtasks. The following subtasks emerged naturally (see
Figure 4): (T1) to ﬁnd a ball, (T2) grasp it, (T3) bring it to the stick, and (T4) hit the stick
with the grasped ball. Subtask (T3) was further broken into two subtasks, that of (T3.1)
MODULE-BASED REINFORCEMENT LEARNING
‘safe wandering’ and (T3.2) ‘go to the stick’, since the robot cannot see the stick from every
position and direction. Similarly, because of the robot’s limited sensing capabilities, subtask
(T1) was replaced by safe wandering and subtask (T2) was reﬁned to ‘when an object nearby
is sensed examine it and grasp it if it is a ball’. Notice that subtask ‘safe wandering’ is used
for two purposes (to ﬁnd a ball or the stick). The operating conditions of the corresponding
controllers arose naturally as (T2) – an object should be nearby, (T3.2) – the stick should
be detected, (T4) – the stick should be in front of the robot, and (T1,T3.1) – no condition.
Since the behavior of the robot must differ before and after locating a ball, an additional
feature indicating when a ball was held was supplied. As the robot’s gripper is equipped
with an ‘object-presence’ sensor the ‘the ball is held’ feature was easy to implement. If
there had not been such a sensor then this feature still could have been implemented as
a switching feature: the value of the feature would be ‘on’ from the time instant when
the robot used the grasping behavior until it uses the hitting behavior. An ‘rescue’ subtask
and corresponding controller were also included since the robot sometimes got stuck. Of
course yet another feature is included for the detection of “goal states”. The corresponding
feature indicates when the stick was hit by the ball. This feature’s value is ‘on’ iff the
gripper is half-closed but the object presence sensor does not give a signal. Because of
the implementation of the grasping module (the gripper was closed only after the grasping
module was executed) this implementation of the “stick has been hit by the ball” feature
was satisfactory for our purposes, although sometimes the ball slipped out from the gripper
in which case the feature turned ‘on’ even though the robot did not actually reach the goal.
Fortunately, this situation did not happen too often, and, thus did not affect learning.
can not see
a ball and
does not hold
hit the stick with
has a ball and
hit the stick
find the stick
has a ball and
does not see
get a ball
hit the stick
with the ball
does not have
have a ball
hit the stick
find a ball
have a ball
and an object
grasp a ball
go to the stick
has a ball and
go to the stick
Figure 4. Subtask decomposition example. The main task of hitting the stick with a ball is ﬁrst broken up into
two subproblems: get a ball and then hit the stick with the ball. Then these are again decomposed into smaller
subtasks which can already be accomplished by simple controllers. The bubbles show the subtask, the rectangles
with rounded corners show the conditions under which the solution of the given subtask is sensible and the shaded
rectangles show the macro-actions. The arrows show the ideal ﬂow of working, in practice other transitions are
also possible and do exist. Note that the “explore” macro-action is used twice in the ideal ﬂow of working. One
particular “maintenance subtask” is not shown to preserve the clarity of the ﬁgure. This is the “rescue” subtask.
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
Featuresandcontrollers
Theresultinglistofmodulesandfeatures, eachofwhich
we now elaborate upon, is shown in Table 2. The dynamics of the controller associated with
Module 1 was based on the maximization of a function that depended on the proximity
of objects and the speed of both motors.9 If there were no obstacles near the robot, this
module made the robot go forward. This controller could thus serve as one for exploring
the environment. Module 2 was applicable only if the stick was in the viewing angle of
the robot, which could be detected in an unambiguous way because the only black thing that
could get into the view of the robot was the stick. The range of allowed behavior associated
with this module was implemented as a proportional controller that drove the robot in such
a way that the angle difference between the direction of motion and line of sight to the stick
was reduced. The behavior associated with Module 3 was applicable only if there was
an object next to the robot, which was deﬁned as a function of the immediate values of IR
sensors. The associated behavior was the following: the robot turned to a direction that
brought it to point directly at the object, then the gripper was lowered. If the object-presence
sensor attached to the gripper gave a signal, then the robot judged that the object sensed
was a ball and not the fence, so the robot closed the gripper and picked the object up. If
the object-presence sensor did not signal the robot lifted up the gripper, then turned to a
random direction and started going forward. The observation process was switched off
until the whole procedure was ﬁnished. Module 4 was the “hitting” module. Its feature
function was ‘on’ when the stick was near, i.e., the total activity of the linear-eye sensors
exceeded a given constant10, otherwise it was ‘off’. The associated behavior was to let
the gripper down and then raise it anew. This module was independent of whether there
was a ball in the gripper or not, which entailed the robot having to learn that this module
wasn’t needed unless the ball was grasped. Module 5, as noted earlier, was created to
handle stuck situations. This module makes the robot going backward and is applicable if
the robot has not been able to move the wheels into the desired position for a while. This
condition is a typical time-window-based feature. The sixth feature indicated the presence
of the ball, which was ‘on’ if a ball was held, while Feature 7 was the goal-detection
feature described earlier.
The operating conditions of controllers were not exclusive; on the contrary, there were
many “states” when more than one behavior was simultaneously applicable. On the other
hand, some features were totally independent of each other. For example, if the robot
was stuck then there must have been an object nearby, i.e., if Feature 2 = ‘on’ then
Feature 4 = ‘on’ as well. These dependencies mean that the “actual” state space was
much smaller than 27 (= 128), the total number of possible states.
Simple case-analysis shows that there is no switching controller that can reach the goal
with complete certainty within ﬁnite time (in the worst case, the robot could return accidentally to state “10000000” from any state when the goal feature was ‘off’), but this
argument also shows that an almost-sure switching strategy, and therefore one which attains
the goal in ﬁnite expected time, should always exist. This is simply because the goal state
can be reached from the state “10000000” with positive probability under a simple action
MODULE-BASED REINFORCEMENT LEARNING
Table 2. Description of the features and the modules. ‘FNo.’ means feature number.
Note that features 1-5 are the operating conditions of their associated controllers. In
the column labeled by ‘on’ the conditions under which the respective feature’s value is
‘on’ are listed, while the last column lists the controllers associated with the respective
feature (if any).
explore while avoiding obstacles
if the stick is in the viewing angle
go to the stick
if an object is near
examine the object grasp it if it is a ball
if the stick is near
hit the stick
if the robot is stuck
go backward
if the ball is grasped
if the stick is hit with the ball
The cost structure
In order to promote fast learning using RL one must design the immediate costs in a careful
manner. A dense cost structure was applied: the cost of using each behavior was one
except when the goal was reached, which had a cost of zero. Costs were discounted at
a rate of γ = 0.99. Note that from time to time the robot by chance became stuck (the
robot’s ‘stuck feature’ was ‘on’), and the robot tried to execute a module which could
not change the value of the feature vector. This meant that the robot did not have a second
option to try another module since by deﬁnition the robot could only make decisions if
the feature-representation changed. As a result the robot could sometimes get stuck in a
“perpetual” or so-called “jammed” state. To prevent this from happening, we built in an
additional rule which was to stop and reinitialize the robot when it got stuck and could not
unjam itself after 50 sensory measurements. A cost equivalent to the cost of never reaching
the goal, i.e., a cost of
1−γ (= 100) was then communicated to the robot, which mimicked
in effect that such actions virtually last forever.
The details of learning
Experiments were fully automated and organized in trials.
Each trial run lasted until
the robot reached the goal or the number of decisions exceeded 150 (a number that was
determined experimentally), or until the robot became jammed. The ‘stick was hit’ event
was registered by checking the state of the gripper (see also the description of Feature 7).
Duringlearning, theBoltzmann-explorationstrategywasemployedwherethetemperature
was reduced by Tt+1 = 0.999 Tt uniformly for all states .11 During the
experiments, the cumulative number of successful trials were measured and compared to
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
the total number of trials done so far, together with the average number of decisions made
in a trial.
Two sets of experiments were conducted. The ﬁrst set was performed to check the validity
of the module-based approach, while the second was carried out to compare different RL
algorithms. In the ﬁrst set, the starting exploration parameter T0 was set to 100 and the
experiment lasted for 100 trials. These values were chosen in such a way that the robot could
learn a good switching policy, the results of these experiments being shown in Figure 5. One
Perc. of Succ. Trials
Number of Trials
Num. of Decisions
Number of Trials
Figure 5. Learning curves. In the ﬁrst graph, the percentage of successful trials out of ten are shown as a function
of the number of trials. In the second graph, the number of decisions taken by the robot and averaged over ten
trials is shown, also as a function of the number of learning trials. Results are shown for both the rules obtained
by ADP and handcraft.
might conclude from the left subgraph, which shows the percentage of task completions
in different stages of learning, that the robot could solve the task after 50 trials fairly well.
Late ﬂuctuations were attributable to unsuccessful ball searches: as the robot could not see
the balls if they were far from it, the robot had to explore to ﬁnd one and the exploration
MODULE-BASED REINFORCEMENT LEARNING
sometimes took more than 150 decisions, yielding trials which were categorized as being
failures. At the very beginning of learning, the robot tried out the appropriate types of
behavior almost completely randomly, resulting in a large number of decisions per trial.
The evaluation of behavior coordination is also observed in the second subgraph, which
showsthenumberofdecisionspertrialasafunctionoftime. Thereasonforlaterﬂuctuations
is again due to a ticklish ball search. The performance of a handcrafted switching policy
is shown on the graphs as well. As can be seen the differences between the respective
performances of the handcrafted and learnt switching functions are negligible. In order to
get a more precise evaluation of the differences the average number of steps to reach the goal
was computed for both switchings over 300 trials, together with their standard deviations.
The averages were 46.61 and 48.37 for the learnt and the handcrafted switching functions
respectively, with nearly equal standard deviations of 34.78 and 34.82, respectively.
One part of the learned policy is shown in Table 3, where 10 states were selected from the
25 explored ones together with their learned associated behaviors. Theoretically, the total
Table 3. One part of the learned policy. The six
middle entries of the rows are the feature values corresponding to those features listed and numbered in
Table 2. (The values of Feature 7 are not shown
as these are always zero here). The column marked
by the label ‘Mid’ denotes the number of the module
that was chosen by a pure exploitation policy under
the conditions described by the respective feature
values. A handcrafted policy is shown in the column labeled by ‘Hand’. For example, in State 3
the robot would use the module “examine object”
(Controller 3) under the pure exploitation strategy
and the controller “go backward” under the handcrafted policy.
number of states is 27 = 128, but as learning concentrates on feature conﬁgurations that
really occur, this number happened to be just 25 here. The expected difference between
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
the robot’s behavior before and after ﬁnding a ball can readily be seen. For example, in
State 5 the robot moves towards the stick (compare Tables 2 and 3), because it has realized
that this behavior leads to an object with high conﬁdence that usually happens to be a ball.
In the dual state, State 10, which differs from State 5 only in that ball is now held, the
robot properly chose the hitting action. Note that the learned policy was always consistent
with the handcrafted rules, but in certain cases the learned rules are more reﬁned than their
handcrafted counterparts. One part of the handcrafted rules are shown in the Table 3. For
example, as the above example shows the robot learned to exploit the fact that the arena
was not completely level and as a result balls were biased towards the stick. The learned
actions in States 3 and 4 reveal another unexpected result: when the robot was stuck, the
presence of the stick made a difference. If the stick was in the viewing angle the robot
chose the object-examination behavior, otherwise it went backwards. This difference is
again due to the fact that the balls are frequently situated around the stick and consequently
the object-examination behavior in a stuck state, and when the robot was close to the stick,
would often lead to the gripping of a ball while simultaneously freeing the robot with high
probability – so it was worth trying the object-examination behavior in states like State 3.
In the rest of the experiments, we compared two versions of ARTDP and three versions
of real-time Q-learning (RTQL). The two variants of ARTDP were ADP, and ARTDP
with the single case when Ft := {st} and only one iteration in the inner cycle of the
algorithm was performed. Note that due to the small number of states and module-based
time, discretization even ADP could be run in real time. But variants of RTQL differ in the
choice of the learning rate’s time dependence. RTQL:SC refers to the choice of the so-called
search-then-converge method, where αk (s, a) =
100+nk(s,a), nk (s, a) being the number
of times the event (s, a) = (st, at) happened before time k plus one (the parameters 50
and 100 were determined experimentally as being the best choices). In the other two cases
(the corresponding algorithms were denoted by RTQL:0,1 and RTQL:0,25 respectively),
constant learning rates (0.1 and 0.25, respectively) were utilized.
The online performances of the algorithms were measured as the cumulative number of
successful trials. An example of the time-dependence of these values are depicted in Figure
Cum. Num. of Succ. Trials
Number of Trials
Figure 6. The cumulative number of successful trials in the case of learning with ADP and RTQL:0,25, and
Boltzmann exploration with initial temperature T0 = 50. The results for the handcrafted rule are also shown.
MODULE-BASED REINFORCEMENT LEARNING
The table shows the statistics used in ANOVA. In each cell the sample mean and the
sums of squares (SS) are shown for a given initial temperature and an algorithm type. The initial
temperatures are shown in the ﬁrst column, while the acronyms for algorithms are shown in the ﬁrst
row. The numbers in the last column give the sample mean and the sums of squares for a given initial
temperature and, similarly, the numbers in the last row give the same statistics for a given algorithm
Total Rows
60.4;449.3
76.8;245.2
75.08;314.33
53.8;306.7
83.8;391.7
80.2;194.7
69.84;367.89
44.8;154.7
68.6;191.3
71.8;368.7
68.6;460.8
83.2;150.2
54.8; 702.7
45.6;277.3
68.8;152.7
82.8;383.7
66.2;506.17
Total Columns
53.45;372.58
61.3;373.06
71.2;188.17
79.85;318.03
82.35;164.03
6 during the learning procedure, and for the learning cases with ADP and RTQL:0,25.
The starting exploration constant was set to T0 = 50, a slope of 45◦meaning that all of
the trials were successful. Again, late drops in the graphs can be ascribed to unlucky ball
searches. The bigger the curve slope, the faster was the rate of learning, i.e., the smaller
is the regret of learning. By deﬁnition, the regret Rt at time t is the difference between
the performance of an optimal agent (robot) and that of the learning agent accumulated
up to trial t, i.e., it is the price of learning up to time t. If s (t) denotes the number of
successful trials out of the ﬁrst t trials and the robot learns to behave “optimally” after
trial number t0 (i.e., it is able to hit the stick with the ball in every trial after time t0) then
R = Rt = t −s(t) = t0 −s (t0) is the total regret assuming that the optimal agent can
reach the goal in every trial (this assumption is relevant since the handcrafted agent could
almost achieve this performance). All algorithms were examined with all the four different
exploration parameters (T0 = 100, T0 = 50, T0 = 25, T0 = 0) since the same exploration
rate may well result in different regrets for different algorithms, as was also conﬁrmed
in the experiments. For each algorithm and temperature 5 independent experiments were
performed. (Altogether 5 × 4 × 5 experiments were conducted which took a total of 40
days and nights.) The results are evaluated by the analysis of variance (ANOVA).
Since ANOVA requires the normality of the data and that the within group variances are
equal we performed the following tests: Denote the data obtained from the kth experiment
( 1 ≤k ≤5 = n) for algorithm index i and temperature index j by ξijk, denote the
sample average of ξijk for ﬁxed i, j and variable k by ξij·, the empirical variance of the
same data by s2
ij·. If ξijk are independent and for ﬁxed (i, j) and variable k they are
normal from the same distribution, then √n −1
ξijk −ξij·
/sij·, i, j, 1 ≤k ≤4 are
independent and t-distributed with parameter 4. The Kolmogorov test was performed to
check this. The obtained statistic was 0.1067, which corresponds to a p-level of 0.32
since n = 5 × 4 × 4 = 80, which means that only if we allow a larger than 32% errorprobability on the test can the null-hypothesis (i.e., that the data is normal) be rejected.
Second, we computed the Bartlett-statistics to check the equality of within group variances
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
and obtained K2 = 13, 2786 which corresponds to a probability of 0, 82 with f = 19
being the degrees of freedom, thus the hypothesis on the equality of variances can be
After this ANOVA was performed.
The variance table is shown in Table
The results of the ANOVA are shown in Table 5.
The main conclusions of this
analysis are that with a 90% conﬁdence i) there is no interaction in between the temperature
and the algorithms, i.e., the effects of these are can be decoupled (p = 0.74); ii) the
regret is not effected by the temperature (p = 0.27), but iii) the choice of algorithms
inﬂuences the regret (p = 0). Considering the average regrets for the different algorithms
one gets the ordering (ADP,ARTDP,RTQL:SC,RTQL:0,25,RTQL:0,1) in terms of
increasing regret. Further analysis showed that the p-levels for the differences between
the means are ADP-ARTDP:0,44, ARTDP-RTQL:SC: 0,37, RTQL:SC-RTQL:0,1:
0,41, RTQL:0,1-RTQL:0,25: 0,77, i.e., the model-based algorithms have a signiﬁcant
advantage over the model-free ones, among which the best algorithm which uses the searchand-then-converge learning rate schedule performs signiﬁcantly better than the others with
constant learning rates.
We have also tested another exploration strategy which Thrun found the best among
several undirected methods12 . These runs reinforced our previous ﬁndings
that estimating a model (i.e., running ADP or ARTDP instead of Q-learning) could reduce
the regret rate by as much as 50%.
The table shows the results of ANOVA at the conﬁdence level
The rows give the same statistics for different groups of data. SS is
the sum of squares, df is the degrees of freedom for the F-statistics, MS is the
empirical variance, F is the obtained F-statistics and Fcrit. is the critical F-value
at the conﬁdence level 90%. If F ≥Fcrit.then the null-hypothesis that the given
factor does not inﬂuence the variances must be rejected at the given level.
Factors\Statistics
Temperature
Between groups
Within group
Related work
There are two main research tracks that inﬂuenced our work.
The ﬁrst was the introduction of features in RL. Learning while using features was studied by Tsitsiklis
and Van Roy to deal with large ﬁnite state spaces, and also to deal with inﬁnite state
spaces . Working on the output of features can well make
the problem partially observable, so one should not expect that RL algorithms that involve optimization will work in general (the theoretical results of Tsitsiklis and Van Roy
concern only estimation types of algorithms, such as TD(λ), or non-adaptive algorithms
MODULE-BASED REINFORCEMENT LEARNING
 ). Issues of learning in partially observable environments have
been discussed by Singh et al., .
The second track is related to the use of local controllers together with a switching function
that selects the controller to be activated at any arbitrary time. In connection with this topic,
very recently and independently of us, Sastry proposed the use of a hybrid-control approach
to solve complex problems like highway-trafﬁc control . He proposed the
design of several controllers, such as a car-following controller and an overtaking-controller,
among others, which are imagined to work under different and well-speciﬁed conditions.
He assumed that the system dynamics were known and so the main concern of his approach
was to ﬁnd a switching controller that switched between the different controllers and that
met certain requirements such as safety, maximal comfort (of the passengers) and maximal
throughput (of the highway), the criteria importance having been ordered in this way. He
used analytical tools to derive the accessibility decision problem and suggested worst-case
analysis to prove feasibility. Clearly, in contrast with his, we assume here only a qualitative
knowledge of the system-dynamics, which nevertheless enables us to design the modules
and perform a preliminary feasibility analysis. Further, we let let the system itself ﬁnd a
good switching strategy by adapting to the actual environment.
A different approach was taken by Connell and Mahadevan whose work complements
ours in that they set up subtasks to be learned by RL and ﬁxed the switching controller
 . The main aim of their work was to prove that RL could be
applied to learn good controllers at the noisy and unreliable sensor-actuator level, where
also the state and action spaces were inﬁnite. They found that statistics-based clustering
combined with RL could solve this problem. Dorigo and Colombetti also considered
the learning controllers using genetic algorithms. In future, we plan to extend our work
in this direction, i.e., only the subtask decomposition and the features will be designed
by hand, and RL will then be employed to learn both the low-level controllers and the
switching controller, possibly simultaneously and the references therein).
Asada et al., considered many aspects of mobile-robot learning. They applied a visionbased state-estimation approach and deﬁned “macro-actions” similar to our controllers
 . In one of their papers, they describe a goal-shooting problem in which
a mobile robot shot a goal while avoiding another robot . First, the
robot learned two behaviors separately: the “shoot” and “avoid” behaviors. Then, the two
behaviors were synthesized by a handcrafted rule and later this rule was reﬁned via RL.
The learned action values of the two behaviors were reused in the learning process while
the combination of rules took place at the level of state variables.
Matari´c considered a multi-robot learning task where each robot had the same set of
behaviors and features . Although the features of Matari´c could clearly
be interpreted as operating conditions of behaviors she did not restrict the applicability of
the behaviors to the appropriate subspaces, which increased the complexity of the decision
problem unnecessarily. Just as in our case, her goal was to learn a good switching function
by RL. She considered the case when each of the robots learned separately and the ultimate
goal was that learning should lead to a good collective behavior, i.e., she concentrated mainly
on the more involved multi-agent perspective of learning. She found that Q-learning worked
badly compared to her “shaped reinforcement” approach, which she found to be comparable
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
to a handcrafted rule. In her approach, time averages of summed and appropriately deﬁned
immediate reinforcements served as the basis of decisions, i.e., the handcrafted immediate
reinforcements balanced the different aspects of the task and helped encode the structure
of the switching function. This design required a lot of a priori knowledge and experience,
and seemed to be costly compared to the design of a good switching function. In her
experiments with Q-learning, Matari´c used a sparse reward structure, a unit reward was
communicated to the learner when it reached the goal, otherwise a reward of zero was
given. However, it is well known that dense rewards facilitate clever exploration rather than
sparse ones. ). In contrast to
her work, we followed a more engineering-oriented approach when we suggested designing
the modules based on well-articulated and simple principles. Contrary to her ﬁndings, we
discovered that RL (with a dense-reward structure) can indeed work well at the modular
In the AI community, there is an interesting approach to mobile-robot control called
Behavior-Based Artiﬁcial Intelligence in which “competence” modules or behaviors have
been proposed as the building blocks of “creatures” . Each
of these modules has a list of preconditions similar to our own operating conditions cited
here. The decision-making procedure is, on the other hand, usually quite different from
ours. Maes, for example, proposed what she called a local-computation scheme. The
modules were linked together through different channels and may inhibit or excite each
other. Activation was spread along the channels and accumulated at the most relevant
behavior nodes. The behavior whose activation ﬁrst went above threshold was selected and
executed. After ﬁnishing the behavior, the activation level of the behavior was reset to zero
and the whole process repeated. Like every ad hoc method, this method required careful
tuning. Tyrrell found that, in a complex decision task aimed to simulate the task faced by
a zebra living in the African Savannah, the decision-making mechanism by Maes could
not work well compared to other action-selection mechanisms. Moreover, he added that
there might be theoretical reasons behind this failure . Maes later pointed
out that Tyrrell’s ﬁndings could be debated . In another work of hers, she
also proposed the learning of links between the modules and she also tried
out this on a real-robot .
Yet another main direction of automatic robot programming research uses genetic algorithms to ﬁnd good robotic programs in a
space of possible programs. Alternatively basic behaviors, including their coordination,
can be learned by using a classiﬁer systems’ approach and genetic algorithm. Like us, Dorigo also emphasized that design and learning should be well balanced
and outlined a general “methodology for behavior engineering” .
Here we have gone further as we suggested speciﬁc tools which link the design issues
to theoretically well based disciplines such as planning in AI systems, classical control
designs and reinforcement learning. In this way a consistent view of the design issues
has been developed and so the role of different components (models, planning, subgoals,
behaviors, operating conditions, features, ﬁlters, modules, reinforcement, learning, etc.)
becomes clear. Nevertheless, Dorigo touched some issues which are outside the scope
MODULE-BASED REINFORCEMENT LEARNING
of our work. For example, he considered some further complex relationships between
behaviors, such as the combination, ‘independent sum’ and sequences of controllers, i.e.,
respectively: the superposition of control signals coming from different local controllers;
different local controllers operating simultaneously but affecting a disjoint set of actuators;
and the operation where controllers are only used sequentially, each controller waiting for
the preceding controller to ’ﬁnish’ before acting. Sequencing can be viewed as a tool to
resolve problems related to partial observability. It is a form of implicit memory usage and
could be easily incorporated in our framework by adding a feature which becomes activated
only when a controller ﬁnishes working. The other two relationships can be viewed as tools
to exploit different properties of the task, such as a superposition property of control in the
case of combination and subgoal independence in the case of ‘independent
A more involved problem related to subgoal independence has recently been explored by
Singh and Cohn who considered the problem of ﬁnding an optimal policy in the direct
product of a ﬁnite set of MDPs with a single action set (the state space of the product MDP
is the direct product of the state spaces of the individual MDPs, the transition probabilities
are also multiplied, but the rewards are summed up). True subgoal independence could be
modeled in this way if the action sets were independent.13 As noted in , the
importance of independent subgoals should not be underestimated: i) independent subgoals
reduce the branching factor by allowing the problem solver to focus on only a subset of
actions at any given time and ii) most goals that we try to satisfy in our everyday life
are almost independent. In the case of ii), think of the independence of actions needed to
accomplish chores, job-related tasks, and recreational or social objectives related tasks. The
only dependence between these tasks is the limited resources (time or money) available.
Note that this dependence is quite weak until we reach the limit of these resources. As we
do not have any restrictions on the kind of subgoals to be used, independent subgoals can
indeed be utilized in our design, however the notion of composite actions are not currently
supported.
Sometimes dependence of subgoals is explicit and clear. For example, Dorigo and Colombetti implicitly use dependent subgoals to deﬁne useful behaviors, e.g., in their definition of Chase/Feed/Escape behavior when they declare that in this behavior the subgoal
of escaping from a predator has precedence over the subgoal of feeding which again has
precedence over chasing moving objects. In MDPs such precedence relations can be captured by certain vector-valued evaluation functions and also RL algorithms
can be derived which take into account the predeﬁned precedences .
Our module concept (operating conditions together with controllers) ﬁts well with the
skill concept of Thrun and Schwartz who derived an algorithm that learns “skills”
useful to complete a set of tasks. The algorithm minimizes the sum of the loss due to
the use of skills (instead of the low level actions) and the storage size needed to represent
the policies using the acquired skills. They note that skills are also needed in certain
single tasks. Here we would like to note that multiple task problems usually correspond to
a subgoal decomposition of a single task (with e.g., independent, or serializable, or block
serializable subgoals). We ﬁnd this view fascinating since this is why it becomes meaningful
to speak about the interplay of the subtasks! The algorithm of Thrun and Schwartz 
saves memory since a skill has a single associated value for each (sub)task independently
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
of the actual state of the system. Skills also have domains which can be identiﬁed with
our operating conditions – only those skills can be activated whose domain indicator are
triggered by the actual state. In Thrun & Schwartz the probability of choosing an
available skill is proportional to its squared value. The learned domains of skills could
provide a very useful way of doing state-space abstraction when a true RL algorithm would
work on the top of the feature space induced by these domains, as in our case where the RL
algorithm worked on the feature space induced by the operating conditions. Restricting the
decisions to be only dependant on the features’ values introduces deviations from optimality
but enables the introduction of hierarchies of modules (more precisely, a lattice structure
over the modules): every module can activate any other module at a lower level in the
hierarchy (the hierarchy prevents inﬁnite cycles) or a low level action. A method similar to
that of Thrun and Schwartz could then be used to invent operating conditions and/or
the relationships among the modules, i.e., their coordination. The operating conditions
of a controller could also be learned by relying on their deﬁnitions. Another application
could be to directly use the algorithm of in the planning phase
provided that the qualitative plant model is given as a MDP. This is the subject of our future
research. Notethat, incontrasttothemacro-actionutilizationof , where
the macro actions are used in a transparent manner (i.e., the computed policy can eventually
be given purely by simple actions) to facilitate planning, we suggest to transfer information
about subgoal decompositions to the actual control level also. This serves the purpose of
task-oriented abstracting space, time and action. The idea of ) could be utilized in the planning phase of our method
to initialize the coordination of behavior modules.
It is very important to note that the qualitative knowledge of the plant can be represented by
any method, such as dynamical equation, symbolic rules, and is not restricted to MDPs. This
maymeanaverycompactrepresentationandmayenabledifferentkindofalgorithmstowork
at the planning phase. Earlier, we suggested a method to learn a rule based representation
on the top of an MDP representation .
algorithms relies on a ‘triplet’ representation of MDPs when transitions are represented and evaluated instead of state-action
pairs or states. Transitions are then interpreted as rules that apply to speciﬁc situations and
are combined to get new, more general rules which apply to a larger set of situations. We
argued that the algorithm works well in deterministic problems . Such
an algorithm, when only the most probable transitions are kept, could well be used to derive
the qualitative representation needed in the planning phase of the method presented here.
Summary and conclusions
Following the traditions of RL based robot programming, an approach to module-based
reinforcement learning was proposed to solve the coordination of multiple “behaviors” or
controllers. Extended features (ﬁlters) served as the basis of time and space discretization
as well as specifying the operating conditions of the modules. The construction principles
of the modules were to: i) decompose the problem into subtasks using a qualitative model
of the plant; ii) for each subtask, design controllers and specify the controllers’ operating
conditions using a more detailed model of the plant; iii) check if the problem could be
MODULE-BASED REINFORCEMENT LEARNING
solved by the controllers under the operating and observability conditions, add additional
features or modules if necessary; iv) set up the reinforcement function and learn a switching
function from experience. Although individual elements of our methods existed previously
in the literature, we have combined them into a single, coherent, framework.
One particularly important motivation behind our approach was that a partially observable
decision problem can usually be transformed into a completely observable one if appropriate
features and local controllers are employed. Of course, some a priori knowledge of the task
and robot is required to ﬁnd those features and controllers. However, it is important to note
that because of the adaptive part, the controllers and the interactions among them need not
to be ﬁne-tuned which allows quick and easy development of robot programming. It was
argued that RL could work well even if the resulting problem was only almost stationary.
The design principles were applied to a fairly complex real-life robot learning problem and
several RL-algorithms were compared in practice using the Analysis of Variance. We found
that estimating the model and solving the optimality equation at each step (which could be
done owing to the economic, feature-based time-discretization) yielded signiﬁcantly better
results than other approaches. The robot learned the task after 700 decisions, which usually
took less than 15 minutes in real-time. We conjecture that using a rough initial model good
initial solutions could be computed off-line that could further decrease the time required to
learn the optimal solution for the task.
The main difference between earlier works and our approach here is that we have established principles for the design modules and found that our subsequent design and
simple RL worked splendidly. Plans for future research include extending the method via
automatic subtask decomposition mechanisms, the learning of modules and operating conditions, and even by the learning of qualitatively correct world models which can be used
in the planning phase to invent the subtask decomposition. These would reduce the amount
of a priori human knowledge which is important when human knowledge is unavailable
such as in industrial process control. Also the analysis of almost stationarity in decision
problems would be important to consider since this notation may provide a bridge between
the theory, when we consider probabilistic models and practice, which corresponds to a
deterministic, but chaotic world.
Acknowledgments
The authors would like to thank Zolt´an G´abor for his efforts of building the experimental
environment. This work was supported by the CSEM Centre Suisse d’Electronique et de
Microtechnique, Switzerland, OTKA Grants No. F20132 and T017110, and the Hungarian
Ministry of Education Grant No. FKFP 1354/1997.
If the state space is inﬁnite then not all sensor-based features can be realized in practice.
2. If time is continuous, then recursive features should be replaced by features that admit continuous-time
dynamics such as ˙f = R(x, a, f). In order to have a ﬁnite-space output, such features should be used
together with discretization mappings, i.e., the output of the feature is given by hR(f) instead of f, where
hR : F0 →F is a discretization mapping, F0 being a suitable subset of a vector space (such as a connected
subset of ℜ) and F the ﬁnite feature space.
ZS. KALM´AR, CS. SZEPESV´ARI AND A. L˝ORINCZ
3. For continuous-time systems some additional care is needed since arbitrarily fast transitions cannot be observed
in practice. This places restrictions on the dynamics of the system and its features, but we do not concern
ourselves with such structural questions here. In digital control, time is already discretized, but the introduction
of feature-jump based clocks is still worth the effort since the complexity of the feature-level “dynamics” might
be a great deal simpler than the original one.
4. This is a very important restriction: It reduces the problem complexity hugely, by a factor of 1/22n(1+o(1)),
where n is the number of operating conditions.
5. One exception may be a controller whose purpose is just to maintain the “goal state”.
6. Again, the goal state may be retained for an inﬁnitely long period of time.
7. Note that as the original control problem is deterministic it is not immediate when the introduction of probabilities can be justiﬁed. One idea is to refer to the ergodicity of the control problem.
8. The Khepera was designed and built at Laboratory of Microcomputing, Swiss Federal Institute of Technology,
Lausanne, Switzerland.
9. Modules are numbered by the identiﬁcation number of their features.
10. All the feature functions of the modules could be implemented as a threshold sum of the measurements of one
sensor modality. The thresholds were chosen in such a way that the task remained solvable, but no additional
ﬁne-tuning of these parameters was performed. Nevertheless, the values of these parameters could inﬂuence
the resulting performance.
11. The theoretically-funded inverse logarithmic decrease was tried as well, but it was found to yield worse on-line
performances than the faster geometric decrease, which, in this particular speciﬁc case, seemed to guarantee
the sufﬁciency of exploration.
12. An exploration strategy is called undirected when the exploration does not depend on the number of visits to
the state-action pairs.
13. Korf’s deﬁnition would also require that only one action is chosen at each time step from the union of the
disjoint action sets , but this is unnecessarily restricting in our case.