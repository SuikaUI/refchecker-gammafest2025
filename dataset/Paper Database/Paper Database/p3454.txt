Self-supervised Learning: Generative or Contrastive
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, Jie Tang*, IEEE Fellow
Abstract—Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on
manual labels and vulnerability to attacks have driven people to ﬁnd other paradigms. As an alternative, self-supervised learning (SSL)
attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation
learning leverages input data itself as supervision and beneﬁts almost all types of downstream tasks. In this survey, we take a look into
new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We
comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives:
generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised
learning to provide deeper thoughts on why self-supervised learning works. Finally, we brieﬂy discuss open problems and future
directions for self-supervised learning. An outline slide for the survey is provided1.
Index Terms—Self-supervised Learning, Generative Model, Contrastive Learning, Deep Learning
Introduction
Motivation of Self-supervised Learning
Generative Self-supervised Learning
Auto-regressive (AR) Model . . . . . . .
Flow-based Model
. . . . . . . . . . . .
Auto-encoding (AE) Model
. . . . . . .
Basic AE Model . . . . . . . .
Prediction
(CPM) . . . . . . . . . . . . . .
Denoising AE Model
Variational AE Model . . . . .
Hybrid Generative Models . . . . . . . .
Combining AR and AE Model.
Combining AE and Flowbased Models
. . . . . . . . .
Pros and Cons . . . . . . . . . . . . . . .
Contrastive Self-supervised Learning
Context-Instance Contrast . . . . . . . . .
Predict Relative Position
Maximize Mutual Information
Instance-Instance Contrast . . . . . . . . .
Cluster Discrimination
Xiao Liu, Fanjin Zhang, and Zhenyu Hou are with the Department of
Computer Science and Technology, Tsinghua University, Beijing, China.
E-mail: , ,
 
Jie Tang is with the Department of Computer Science and Technology,
Tsinghua University, and Tsinghua National Laboratory for Information
Science and Technology (TNList), Beijing, China, 100084.
E-mail: , corresponding author
Li Mian is with the Beijing Institute of Technonlogy, Beijing, China.
Email: 
Zhaoyu Wang is with the Anhui University, Anhui, China.
Email: 
Jing Zhang is with the Renmin University of China, Beijing, China.
Email: 
Instance Discrimination . . . .
Self-supervised Contrastive Pre-training
for Semi-supervised Self-training . . . .
Pros and Cons . . . . . . . . . . . . . . .
Generative-Contrastive
(Adversarial)
Selfsupervised Learning
Generate with Complete Input
Recover with Partial Input . . . . . . . .
Pre-trained Language Model . . . . . . .
Graph Learning . . . . . . . . . . . . . .
Domain Adaptation and Multi-modality
Representation . . . . . . . . . . . . . . .
Pros and Cons . . . . . . . . . . . . . . .
Theory behind Self-supervised Learning
GAN . . . . . . . . . . . . . . . . . . . .
Divergence Matching . . . . .
Disentangled Representation .
Maximizing Lower Bound . . . . . . . .
Evidence Lower Bound . . . .
Mutual Information . . . . . .
Contrastive Self-supervised Representation Learning
. . . . . . . . . . . . . . .
Relationship with Supervised
Learning . . . . . . . . . . . .
Understand Contrastive Loss .
Generalization . . . . . . . . .
Discussions and Future Directions
Conclusion
References
 
Fig. 1: An illustration to distinguish the supervised, unsupervised and self-supervised learning framework. In selfsupervised learning, the “related information” could be
another modality, parts of inputs, or another form of the
inputs. Repainted from .
INTRODUCTION
eep neural networks have shown outstanding
performance on various machine learning tasks, especially on supervised learning in computer vision (image
classiﬁcation , , , semantic segmentation ,
 ), natural language processing (pre-trained language
models , , , , sentiment analysis , question
answering , , , etc.) and graph learning (node
classiﬁcation , , , , graph classiﬁcation ,
 , etc.). Generally, the supervised learning is trained
on a speciﬁc task with a large labeled dataset, which is
randomly divided for training, validation and test.
However, supervised learning is meeting its bottleneck. It
relies heavily on expensive manual labeling and suffers from
generalization error, spurious correlations, and adversarial
attacks. We expect the neural networks to learn more with
fewer labels, fewer samples, and fewer trials. As a promising
alternative, self-supervised learning has drawn massive
attention for its data efﬁciency and generalization ability,
and many state-of-the-art models have been following this
paradigm. This survey will take a comprehensive look at
the recent developing self-supervised learning models and
discuss their theoretical soundness, including frameworks
such as Pre-trained Language Models (PTM), Generative
Adversarial Networks (GAN), autoencoders and their extensions, Deep Infomax, and Contrastive Coding. An outline
slide is also provided.1
The term “self-supervised learning” is ﬁrst introduced
in robotics, where training data is automatically labeled
by leveraging the relations between different input sensor
signals. Afterwards, machine learning community further
develops the idea. In the invited speech on AAAI 2020, The
Turing award winner Yann LeCun described self-supervised
learning as ”the machine predicts any parts of its input for
any observed part”. 2 Combining self-supervised learning’s
traditional deﬁnition and LeCun’s deﬁnition, we can further
summarize its features as:
• Obtain “labels” from the data itself by using a “semiautomatic” process.
• Predict part of the data from other parts.
1. Slides at 
2. 
Fig. 2: Number of publications and citations on selfsupervised learning during 2012-2020, from Microsoft Academic , . Self-supervised learning is drawing
tremendous attention in recent years.
Speciﬁcally, the “other part” could be incomplete, transformed, distorted, or corrupted (i.e., data augmentation
technique). In other words, the machine learns to ’recover’
whole, or parts of, or merely some features of its original
People are often confused by the concepts of unsupervised learning and self-supervised learning. Self-supervised
learning can be viewed as a branch of unsupervised learning
since there is no manual label involved. However, narrowly
speaking, unsupervised learning concentrates on detecting
speciﬁc data patterns, such as clustering, community discovery, or anomaly detection, while self-supervised learning
aims at recovering, which is still in the paradigm of supervised settings. Figure 1 provides a vivid explanation of the
differences between them.
There exist several comprehensive reviews related to
Pre-trained Language Models , Generative Adversarial
Networks , autoencoders, and contrastive learning for visual representation . However, none of them concentrates
on the inspiring idea of self-supervised learning itself. In this
work, we collect studies from natural language processing,
computer vision, and graph learning in recent years to
present an up-to-date and comprehensive retrospective on
the frontier of self-supervised learning. To sum up, our
contributions are:
• We provide a detailed and up-to-date review of selfsupervised learning for representation. We introduce
the background knowledge, models with variants, and
important frameworks. One can easily grasp the frontier
ideas of self-supervised learning.
• We categorize self-supervised learning models into
generative, contrastive, and generative-contrastive (adversarial), with particular genres inner each one. We
demonstrate the pros and cons of each category.
• We identify several open problems in this ﬁeld, analyze
the limitations and boundaries, and discuss the future
direction for self-supervised representation learning.
We organize the survey as follows. In Section 2, we
introduce the motivation of self-supervised learning. We also
present our categorization of self-supervised learning and a
conceptual comparison between them. From Section 3 to Section 5, we will introduce the empirical self-supervised learning methods utilizing generative, contrastive and generative-
Fig. 3: Categorization of Self-supervised learning (SSL):
Generative, Contrastive and Generative-Contrastive (Adversarial).
contrastive objectives. In Section 6, we introduce some recent
theoretical attempts to understand the hidden mechanism of
self-supervised learning’s success. Finally, in Section 7 and
8, we discuss the open problems, future directions and our
conclusions.
MOTIVATION OF SELF-SUPERVISED LEARNING
It is universally acknowledged that deep learning algorithms
are data-hungry. Compared to traditional feature-based
methods, deep learning usually follows the so-called “endto-end” fashion (raw-data in, prediction out). It makes very
few prior assumptions, which leads to over-ﬁtting and
biases in scenarios with little supervised data. Literature has
shown that simple multi-layer perceptrons have a very poor
generalization ability (always assume a linear relationship
for out-of-distribution (OOD) samples) , which results
in over-conﬁdent (and wrong) predictions.
To conquer the fundamental OOD and generalization
problem, while numerous works focus on designing new
architectures for neural networks, another simple yet effective solution is to enlarge the training dataset to make as
many samples “in-distribution”. However, the fact is, despite
massive available unlabeled web data in this big data era,
high-quality data with human labeling could be costly. For
example, Scale.ai3, a data labeling company, charges $6.4
per image for the image segmentation labeling. An image
segmentation dataset containing 10k+ high-quality samples
could cost up to a million-dollar.
The most crucial point for self-supervised learning’s
success is that it ﬁgures out a way to leverage the tremendous
amounts of unlabeled data that becomes available in the
big data era. It a time for deep learning algorithms to
get rid of human supervision and turn back to data’s selfsupervision. The intuition of self-supervised learning is to
leverage the data’s inherent co-occurrence relationships as
the self-supervision, which could be versatile. For example,
in the incomplete sentence “I like
apples”, a well-trained
language model would predict “eating” for the blank (i.e., the
famous Cloze Test ) because it frequently co-occurs with
the context in the corpora. We can summarize the mainstream
3. 
Fig. 4: Conceptual comparison between Generative, Contrastive, and Generative-Contrastive methods.
self-supervision into three general categories (see Fig. 3) and
detailed subsidiaries:
• Generative: train an encoder to encode input x into an
explicit vector z and a decoder to reconstruct x from z
(e.g., the cloze test, graph generation)
• Contrastive: train an encoder to encode input x into
an explicit vector z to measure similarity (e.g., mutual
information maximization, instance discrimination)
• Generative-Contrastive (Adversarial): train an encoderdecoder to generate fake samples and a discriminator to
distinguish them from real samples (e.g., GAN)
Their main difference lies in model architectures and
objectives. A detailed conceptual comparison is shown in
Fig. 4. Their architectures can be uniﬁed into two general
components: the generator and the discriminator, and the
generator can be further decomposed into an encoder and a
decoder. Different things are:
1) For latent distribution z: in generative and contrastive
methods, z is explicit and is often leveraged by downstream tasks; while in GAN, z is implicitly modeled.
2) For discriminator: the generative method does not
have a discriminator while GAN and contrastive have.
Contrastive discriminator has comparatively fewer parameters (e.g., a multi-layer perceptron with 2-3 layers)
than GAN (e.g., a standard ResNet ).
3) For objectives: the generative methods use a reconstruction loss, the contrastive ones use a contrastive similarity
metric (e.g., InfoNCE), and the generative-contrastive
ones leverage distributional divergence as the loss (e.g.,
JS-divergence, Wasserstein Distance).
A properly designed training objective related to downstream tasks could turn our randomly initialized models
into excellent pre-trained feature extractors. For example,
contrastive learning is found to be useful for almost all
visual classiﬁcation tasks. This is probably because the
contrastive object is modeling the class-invariance between
different image instances. The contrastive loss makes images
containing the same object class more similar. It makes
those containing different classes less similar, essentially
accords with the downstream image classiﬁcation, object
detection, and other classiﬁcation-based tasks. The art of
self-supervised learning primarily lies in deﬁning proper
objectives for unlabeled data.
Self-supervision
Pretext Task
NS strategy
GPT/GPT-2 , 
Following words
Next word prediction
PixelCNN , 
Following pixels
Next pixel prediction
Whole image
Image reconstruction
RealNVP 
word2vec , 
Context words
CBOW & SkipGram
End-to-end
FastText 
End-to-end
DeepWalk-based
 , , 
Graph edges
Link prediction
End-to-end
End-to-end
Masked words
Sentence topic
Masked language model,
Next senetence prediction
SpanBERT 
Masked words
Masked language model
ALBERT 
Masked words
Sentence order
Masked language model,
Sentence order prediction
ERNIE , 
Masked words
Sentence topic
Masked language model,
Next senetence prediction
GPT-GNN 
Attribute & Edge
Masked graph generation
VQ-VAE 2 
Whole image
Image reconstruction
XLNet 
Masked words
Permutation language model
GraphAF 
Attribute & Edge
Sequential graph generation
RelativePosition 
Spatial relations
(Context-Instance)
Relative postion prediction
Jigsaw + Inpainting
+ Colorization
End-to-end
Memory bank
RotNet 
Rotation Prediction
Deep InfoMax 
(Context-Instance)
MI Maximization
End-to-end
End-to-end
End-to-end
InfoWord 
End-to-end
End-to-end
InfoGraph 
End-to-end
(batch-wise)
CMC-Graph 
End-to-end
S2GRL 
End-to-end
Pre-trained GNN 
Node attributes
MI maximization,
Masked attribute prediction
End-to-end
DeepCluster 
Similarity
(Instance-Instance)
Cluster discrimination
Local Aggregation 
ClusterFit 
End-to-end
End-to-end
InstDisc 
(Instance-Instance)
Instance discrimination
Memory bank
End-to-end
MoCo v2 
SimCLR 
End-to-end
InfoMin 
End-to-end
End-to-end
ReLIC 
End-to-end
SimSiam 
End-to-end
SimCLR v2 (semi) 
End-to-end
GraphCL 
End-to-end
Whole image
Image reconstruction
Adversarial AE 
BiGAN/ALI , 
BigBiGAN 
Colorization 
Image color
Colorization
Inpainting 
Parts of images
Inpainting
Super-resolution 
Details of images
Super-resolution
ELECTRA 
Masked words
Replaced token detection
End-to-end
WKLM 
Masked entities
Replaced entity detection
End-to-end
Graph edges
Link prediction
GraphGAN 
GraphSGAN 
Graph nodes
Node classiﬁcation
TABLE 1: An overview of recent self-supervised representation learning. For acronyms used, “FOS” refers to ﬁelds of study;
“NS” refers to negative samples; “PS” refers to positive samples; “MI” refers to mutual information. For alphabets in “Type”:
G Generative ; C Contrastive; G-C Generative-Contrastive (Adversarial). For symbols in “Hard NS” and “Hard PS”, “-”
means not applicable, “×” means not adopted, “✓”’ means adopted; “no NS” particularly means not using negative samples
in instance-instance contrast.
GENERATIVE SELF-SUPERVISED LEARNING
This section will introduce important self-supervised learning methods based on generative models, including autoregressive (AR) models, ﬂow-based models, auto-encoding
(AE) models, and hybrid generative models.
Auto-regressive (AR) Model
Auto-regressive (AR) models can be viewed as “Bayes net
structure” (directed graph model). The joint distribution can
be factorized as a product of conditionals
log pθ(xt|x1:t−1)
where the probability of each variable is dependent on the
previous variables.
In NLP, the objective of auto-regressive language modeling is usually maximizing the likelihood under the forward
autoregressive factorization . GPT and GPT-2 
use Transformer decoder architecture for language
model. Different from GPT, GPT-2 removes the ﬁne-tuning
processes of different tasks. To learn uniﬁed representations that generalize across different tasks, GPT-2 models
p(output|input, task), which means given different tasks,
the same inputs can have different outputs.
The auto-regressive models have also been employed
in computer vision, such as PixelRNN and Pixel-
CNN . The general idea is to use auto-regressive
methods to model images pixel by pixel. For example, the
lower (right) pixels are generated by conditioning on the
upper (left) pixels. The pixel distributions of PixelRNN and
PixelCNN are modeled by RNN and CNN, respectively.
For 2D images, auto-regressive models can only factorize
probabilities according to speciﬁc directions (such as right
and down). Therefore, masked ﬁlters are employed in CNN
architecture. Furthermore, two convolutional networks are
combined to remove the blind spot in images. Based on Pixel-
CNN, WaveNet – a generative model for raw audio was
proposed. To deal with long-range temporal dependencies,
the authors develop dilated causal convolutions to improve
the receptive ﬁeld. Moreover, Gated Residual blocks and skip
connections are employed to empower better expressivity.
The auto-regressive models can also be applied to graph
domain problems, such as graph generation. You et al. 
propose GraphRNN to generate realistic graphs with deep
auto-regressive models. They decompose the graph generation process into a sequence generation of nodes and edges
conditioned on the graph generated so far. The objective
of GraphRNN is deﬁned as the likelihood of the observed
graph generation sequences. GraphRNN can be viewed as a
hierarchical model, where a graph-level RNN maintains the
state of the graph and generates new nodes, while an edgelevel RNN generates new edges based on the current graph
state. After that, MRNN and GCPN are proposed
as auto-regressive approaches. MRNN and GCPN both use
a reinforcement learning framework to generate molecule
graphs through optimizing domain-speciﬁc rewards. However, MRNN mainly uses RNN-based networks for state
representations, but GCPN employs GCN-based encoder
The advantage of auto-regressive models is that they
can model the context dependency well. However, one
shortcoming of the AR model is that the token at each
position can only access its context from one direction.
Flow-based Model
The goal of ﬂow-based models is to estimate complex highdimensional densities p(x) from data. Intuitively, directly
formalizing the densities is difﬁcult. To obtain a complicated densities, we hope to generate it “step by step” by
stacking a series of transforming functions that describing
different data characteristics respectively. Generally, ﬂowbased models ﬁrst deﬁne a latent variable z which follows
a known distribution pZ(z). Then deﬁne z = fθ(x), where
fθ is an invertible and differentiable function. The goal is
to learn the transformation between x and z so that the
density of x can be depicted. According to the integral
rule, pθ(x)dx = p(z)dz. Therefore, the densities of x and
z satisﬁes:
pθ(x) = p(fθ(x))
and the objective is to maximize the likelihood:
log pθ(x(i)) = max
log pZ(fθ(x(i)))
The advantage of ﬂow-based models is that the mapping
between x and z is invertible. However, it also requires
that x and z must have the same dimension. fθ needs to
be carefully designed since it should be invertible and the
Jacobian determinant in Eq. (2) should also be calculated
easily. NICE and RealNVP design afﬁne coupling
layer to parameterize fθ. The core idea is to split x into two
blocks (x1, x2) and apply a transformation from (x1, x2) to
(z1, z2) in an auto-regressive manner, that is z1 = x1 and
z2 = x2 + m(x1). More recently, Glow was proposed
and it introduces invertible 1 × 1 convolutions and simpliﬁes
Auto-encoding (AE) Model
The auto-encoding model’s goal is to reconstruct (part of)
inputs from (corrupted) inputs. Due to its ﬂexibility, the AE
model is probably the most popular generative model with
many variants.
Basic AE Model
Autoencoder (AE) was ﬁrst introduced in for pre-training
artiﬁcial neural networks. Before autoencoder, Restricted
Boltzmann Machine (RBM) can also be viewed as
a special “autoencoder”. RBM is an undirected graphical
model, and it only contains two layers: the visible layer
and the hidden layer. The objective of RBM is to minimize
the difference between the marginal distribution of models
and data distributions. In contrast, an autoencoder can
be regarded as a directed graphical model, and it can
be trained more efﬁciently. Autoencoder is typically for
dimensionality reduction. Generally, the autoencoder is a
feed-forward neural network trained to produce its input
at the output layer. The AE is comprised of an encoder
network h = fenc(x) and a decoder network x
′ = fdec(h).
The objective of AE is to make x and x
′ as similar as possible
(such as through mean-square error). It can be proved that
the linear autoencoder corresponds to the PCA method.
Sometimes the number of hidden units is greater than the
number of input units, and some interesting structures can
be discovered by imposing sparsity constraints on the hidden
units .
Context Prediction Model (CPM)
The idea of the Context Prediction Model (CPM) is to predict
contextual information based on inputs.
In NLP, when it comes to self-supervised learning on
word embedding, CBOW and Skip-Gram are pioneering
works. CBOW aims to predict the input tokens based on
context tokens. In contrast, Skip-Gram aims to predict context
tokens based on input tokens. Usually, negative sampling is
employed to ensure computational efﬁciency and scalability.
Following CBOW architecture, FastText is proposed by
utilizing subword information.
Inspired by the progress of word embedding models in
NLP, many network embedding models are proposed based
on a similar context prediction objective. Deepwalk 
samples truncated random walks to learn latent node embedding based on the Skip-Gram model. It treats random walks
as the equivalent of sentences. However, another network
embedding approach LINE aims to generate neighbors
rather than nodes on a path based on current nodes:
wij log p(vj|vi)
where E denotes edge set, v denotes the node, wij represents
the weight of edge (vi, vj). LINE also uses negative sampling
to sample multiple negative edges to approximate the
objective.
Denoising AE Model
The intuition of denoising autoencoder models is that representation should be robust to the introduction of noise. The
masked language model (MLM), one of the most successful
architectures in natural language processing, can be regarded
as a denoising AE model. To model text sequence, the masked
language model (MLM) randomly masks some of the tokens
from the input and then predicts them based on their context
information, which is similar to the Cloze task . BERT 
is the most representative work in this ﬁeld. Speciﬁcally, in
BERT, a unique token [MASK] is introduced in the training
process to mask some tokens. However, one shortcoming
of this method is that there are no input [MASK] tokens
for down-stream tasks. To mitigate this, the authors do not
always replace the predicted tokens with [MASK] in training.
Instead, they replace them with original words or random
words with a small probability.
Following BERT, many extensions of MLM emerge.
SpanBERT chooses to mask continuous random spans
rather than random tokens adopted by BERT. Moreover,
it trains the span boundary representations to predict the
masked spans, inspired by ideas in coreference resolution.
ERNIE (Baidu) masks entities or phrases to learn
entity-level and phrase-level knowledge, which obtains good
results in Chinese natural language processing tasks. ERNIE
(Tsinghua) further integrates knowledge (entities and
relations) in knowledge graphs into language models.
Compared with the AR model, in denoising AE for
language modeling, the predicted tokens have access to
contextual information from both sides. However, the fact
that MLM assumes the predicted tokens are independent
if the unmasked tokens are given (which does not hold in
reality) has long been considered as its inherent drawback.
In graph learning, Hu et al. proposes GPT-GNN, a
generative pre-training method for graph neural networks.
It also leverages the graph masking techniques and then
asks the graph neural network to generate masked edges
and attributes. GPT-GNN’s wide range of experiments on
OAG , , , the largest public, academic graph
with 100 million nodes and 2 billion edges, shows impressive
improvements on various graph learning tasks.
Variational AE Model
The variational auto-encoding model assumes that data are
generated from underlying latent (unobserved) representation. The posterior distribution over a set of unobserved
variables Z = {z1, z2, ..., zn} given some data X is approximated by a variational distribution q(z|x) ≈p(z|x).
In variational inference, the evidence lower bound (ELBO)
on the log-likelihood of data is maximized during training.
log p(x) ≥−DKL(q(z|x)||p(z)) + E∼q(z|x)[log p(x|z)] (5)
where p(x) is evidence probability, p(z) is prior and p(x|z)
is likelihood probability. The right-hand side of the above
equation is called ELBO. From the auto-encoding perspective,
the ﬁrst term of ELBO is a regularizer forcing the posterior
to approximate the prior. The second term is the likelihood
of reconstructing the original input data based on latent
variables.
Variational Autoencoders (VAE) is one important
example where variational inference is utilized. VAE assumes
the prior p(z) and the approximate posterior q(z|x) both
follow Gaussian distributions. Speciﬁcally, let p(z) ∼N(0, 1).
Furthermore, reparameterization trick is utilized for modeling approximate posterior q(z|x). Assume z ∼N(µ, σ2),
z = µ + σϵ where ϵ ∼N(0, 1). Both µ and σ are parameterized by neural networks. Based on calculated latent variable
z, decoder network is utilized to reconstruct the input data.
Recently, a novel and powerful variational AE model
called VQ-VAE was proposed. VQ-VAE aims to learn
discrete latent variables motivated by the fact that many
modalities are inherently discrete, such as language, speech,
and images. VQ-VAE relies on vector quantization (VQ) to
learn the posterior distribution of discrete latent variables.
The discrete latent variables are calculated by the nearest
neighbor lookup using a shared, learnable embedding table.
In training, the gradients are approximated through straightthrough estimator as
L(x, D(e)) = ∥x−D(e)∥2
2+∥sg[E(x)]−e∥2
2+β∥sg[e]−E(x)∥2
where e refers to the codebook, the operator sg refers to a
stop-gradient operation that blocks gradients from ﬂowing
Fig. 5: Architecture of VQ-VAE . Compared to VAE, the
orginal hidden distribution is replaced with a quantized vector dictionary. In addition, the prior distribution is replaced
with a pre-trained PixelCNN that models the hierarchical
features of images. Taken from 
into its argument, and β is a hyperparameter which controls
the reluctance to change the code corresponding to the
encoder output.
More recently, researchers propose VQ-VAE-2 ,
which can generate versatile high-ﬁdelity images that rival
BigGAN on ImageNet , the state-of-the-art GAN
model. First, the authors enlarge the scale and enhance the
autoregressive priors by a powerful PixelCNN prior.
Additionally, they adopt a multi-scale hierarchical organization of VQ-VAE, which enables learning local information
and global information of images separately. Nowadays,
VAE and its variants have been widely used in the computer
vision area, such as image representation learning, image
generation, video generation.
Variational auto-encoding models have also been employed in node representation learning on graphs. For
example, Variational graph auto-encoder (VGAE) uses
the same variational inference technique as VAE with graph
convolutional networks (GCN) as the encoder. Due to
the uniqueness of graph-structured data, the objective of
VGAE is to reconstruct the adjacency matrix of the graph by
measuring node proximity. Zhu et al. propose DVNE, a
deep variational network embedding model in Wasserstein
space. It learns Gaussian node embedding to model the
uncertainty of nodes. 2-Wasserstein distance is used to
measure the similarity between the distributions for its
effectiveness in preserving network transitivity. vGraph 
can perform node representation learning and community
detection collaboratively through a generative variational
inference framework. It assumes that each node can be generated from a mixture of communities, and each community
is deﬁned as a multinomial distribution over nodes.
Hybrid Generative Models
Combining AR and AE Model.
Some researchers propose to combine the advantages of
both AR and AE. MADE makes a simple modiﬁcation
to autoencoder. It masks the autoencoder’s parameters
to respect auto-regressive constraints. Speciﬁcally, for the
original autoencoder, neurons between two adjacent layers
are fully-connected through MLPs. However, in MADE, some
connections between adjacent layers are masked to ensure
that each input dimension is reconstructed solely from its
dimensions. MADE can be easily parallelized on conditional
computations, and it can get direct and cheap estimates of
Fig. 6: Illustration for permutation language modeling 
objective for predicting x3 given the same input sequence x
but with different factorization orders. Adapted from 
high-dimensional joint probabilities by combining AE and
AR models.
In NLP, Permutation Language Model (PLM) is a
representative model that combines the advantage of autoregressive model and auto-encoding model. XLNet ,
which introduces PLM, is a generalized auto-regressive
pretraining method. XLNet enables learning bidirectional
contexts by maximizing the expected likelihood over all
permutations of the factorization order. To formalize the idea,
let ZT denotes the set of all possible permutations of the
length-T index sequence [1, 2, ..., T], the objective of PLM
can be expressed as follows:
log pθ(xzt|xz<t)]
Actually, for each text sequence, different factorization orders are sampled. Therefore, each token can see its contextual
information from both sides. Based on the permuted order,
XLNet also conducts reparameterization with positions to
let the model know which position is needed to predict.
Then a special two-stream self-attention is introduced for
target-aware prediction.
Furthermore, different from BERT, inspired by the latest
advancements in the AR model, XLNet integrates the segment recurrence mechanism and relative encoding scheme
of Transformer-XL into pre-training, which can model
long-range dependency better than Transformer .
Combining AE and Flow-based Models
In the graph domain, GraphAF is a ﬂow-based autoregressive model for molecule graph generation. It can
generate molecules in an iterative process and also calculate
the exact likelihood in parallel. GraphAF formalizes molecule
generation as a sequential decision process. It incorporates
detailed domain knowledge into the reward design, such
as valency check. Inspired by the recent progress of ﬂowbased models, it deﬁnes an invertible transformation from a
base distribution (e.g., multivariate Gaussian) to a molecular
graph structure. Additionally, Dequantization technique 
is utilized to convert discrete data (including node types and
edge types) into continuous data.
Pros and Cons
A reason for the generative self-supervised learning’s success
in self-supervised learning is its ability to recover the original
data distribution without assumptions for downstream tasks,
which enables generative models’ wide applications in
both classiﬁcation and generation. Notably, all the existing
generation tasks (including text, image, and audio) rely
heavily on generative self-supervised learning. Nevertheless,
two shortcomings restrict its performance.
First, despite its central status in generation tasks, generative self-supervised learning is recently found far less
competitive than contrastive self-supervised learning in
some classiﬁcation scenarios because contrastive learning’s
goal naturally conforms the classiﬁcation objective. Works
including MoCo , SimCLR , BYOL and SwAV 
have presented overwhelming performances on various CV
benchmarks. Nevertheless, in the NLP domain, researchers
still depend on generative language models to conduct text
classiﬁcation.
Second, the point-wise nature of the generative objective has some inherent defects. This objective is usually
formulated as a maximum likelihood function LMLE =
x log p(x|c) where x is all the samples we hope to model,
and c is a conditional constraint such as context information.
Considering its form, MLE has two fatal problems:
1) Sensitive
Conservative
Distribution.
p(x|c) →0, LMLE becomes super large, making generative model extremely sensitive to rare samples. It
directly leads to a conservative distribution, which has
a low performance.
2) Low-level Abstraction Objective. In MLE, the representation distribution is modeled at x’s level (i.e., point-wise
level), such as pixels in images, words in texts, and nodes
in graphs. However, most of the classiﬁcation tasks target at high-level abstraction, such as object detection, long
paragraph understanding, and molecule classiﬁcation.
and as an opposite approach, generative-contrastive selfsupervised learning abandons the point-wise objective. It
turns to distributional matching objectives that are more
robust and better handle the high-level abstraction challenge
in the data manifold.
CONTRASTIVE SELF-SUPERVISED LEARNING
From a statistical perspective, machine learning models are
categorized into generative and discriminative models. Given
the joint distribution P(X, Y ) of the input X and target
Y , the generative model calculates the p(X|Y = y) while
the discriminative model tries to model the P(Y |X = x).
Because most of the representation learning tasks hope to
model relationships between x, for a long time, people
believe that the generative model is the only choice for
representation learning.
However, recent breakthroughs in contrastive learning,
such as Deep InfoMax, MoCo and SimCLR, shed light on
the potential of discriminative models for representation.
Contrastive learning aims at ”learn to compare” through a
Noise Contrastive Estimation (NCE) objective formatted
L = Ex,x+,x−[−log(
ef(x)T f(x+)
ef(x)T f(x+) + ef(x)T f(x−) ]
where x+ is similar to x, x−is dissimilar to x and f is an
encoder (representation function). The similarity measure
and encoder may vary from task to task, but the framework
Fig. 7: Self-supervised representation learning performance
on ImageNet top-1 accuracy in March, 2021, under linear
classiﬁcation protocol. The self-supervised learning’s ability
on feature extraction is rapidly approaching the supervised
method (ResNet50). Except for BigBiGAN, all the models
above are contrastive self-supervised learning methods.
remains the same. With more dissimilar pairs involved, we
have the InfoNCE formulated as:
L = Ex,x+,xk[−log(
ef(x)T f(x+)
ef(x)T f(x+) + PK
k=1 ef(x)T f(xk) ]
Here we divide recent contrastive learning frameworks
into 2 types: context-instance contrast and instance-instance
contrast. Both of them achieve amazing performance in
downstream tasks, especially on classiﬁcation problems
under the linear protocol.
Context-Instance Contrast
The context-instance contrast, or so-called global-local contrast,
focuses on modeling the belonging relationship between the
local feature of a sample and its global context representation.
When we learn the representation for a local feature, we hope
it is associative to the representation of the global content,
such as stripes to tigers, sentences to its paragraph, and
nodes to their neighborhoods.
There are two main types of Context-Instance Contrast:
Predict Relative Position (PRP) and Maximize Mutual Information (MI). The differences between them are:
• PRP focuses on learning relative positions between local
components. The global context serves as an implicit
requirement for predicting these relations (such as
understanding what an elephant looks like is critical
for predicting relative position between its head and
• MI focuses on learning the direct belonging relationships
between local parts and global context. The relative
positions between local parts are ignored.
Predict Relative Position
Many data contain rich spatial or sequential relations between parts of it. For example, in image data such as Fig. 8,
the elephant’s head is on the right of its tail. In text data, a
sentence like ”Nice to meet you.” would probably be ahead of
”Nice to meet you, too.”. Various models regard recognizing
relative positions between parts of it as the pretext task .
It could be to predict relative positions of two patches from
a sample , or to recover positions of shufﬂed segments
of an image (solve jigsaw) , , , or to infer the
rotation angle’s degree of an image . PRP may also serve
as tools to create hard positive samples. For instance, the
jigsaw technique is applied in PIRL to augment the
positive sample, but PIRL does not regard solving jigsaw
and recovering spatial relation as its objective.
Fig. 8: Three typical methods for spatial relation contrast: predict relative position , rotation and solve jigsaw ,
 , , .
In the pre-trained language model, similar ideas such as
Next Sentence Prediction (NSP) are also adopted. NSP loss is
initially introduced by BERT , where for a sentence, the
model is asked to distinguish the following and a randomly
sampled one. However, some later work empirically proves
that NSP helps little, even harm the performance. So in
RoBERTa , the NSP loss is removed.
To replace NSP, ALBERT proposes Sentence Order
Prediction (SOP) task. That is because, in NSP, the negative
next sentence is sampled from other passages that may have
different topics from the current one, turning the NSP into a
far easier topic model problem. In SOP, two sentences that
exchange their position are regarded as a negative sample,
making the model concentrate on the semantic meaning’s
coherence.
Maximize Mutual Information
This kind of method derives from mutual information (MI)
– a fundamental concept in statistics. Mutual information
targets modeling the association between two variables, and
our objective is to maximize it. Generally, this kind of models
g1∈G1,g2∈G1 I(g1(x1), g2(x2))
where gi is the representation encoder, Gi is a class of
encoders with some constraints, and I(·, ·) is a samplebased estimator for the accurate mutual information. In
applications, MI is notorious for its complex computation.
A common practice is to alternatively maximize I’s lower
bound with an NCE objective.
Deep InfoMax is the ﬁrst one to explicitly model
mutual information through a contrastive learning task,
which maximize the MI between a local patch and its global
context. For real practices, take image classiﬁcation as an
example, we can encode a cat image x into f(x) ∈RM×M×d,
and take out a local feature vector v ∈Rd. To conduct
contrast between instance and context, we need two other
Fig. 9: Two representatives for mutual information’s application in contrastive learning. Deep InfoMax (DIM) ﬁrst
encodes an image into feature maps, and leverage a readout function (or so-called summary function) to produce a
summary vector. AMDIM enhances the DIM through
randomly choosing another view of the image to produce
the summary vector.
• a summary function g:RM×M×d →Rd to generate the
context vector s = g(f(x)) ∈Rd
• another cat image x−and its context vector s−=
and the contrastive objective is then formulated as
L = Ev,x[−log(
evT ·s + evT ·s−)]
Deep InfoMax provides us with a new paradigm and
boosts the development of self-supervised learning. The
ﬁrst inﬂuential follower is Contrastive Predictive Coding
(CPC) for speech recognition. CPC maximizes the
association between a segment of audio and its context audio.
To improve data efﬁciency, it takes several negative context
vectors at the same time. Later on, CPC has also been applied
in image classiﬁcation.
AMDIM enhances the positive association between
a local feature and its context. It randomly samples two
different views of an image (truncated, discolored, and so
forth) to generate the local feature vector and context vector,
respectively. CMC extends it into several different views
for one image and samples another irrelevant image as the
negative. However, CMC is fundamentally different from
Deep InfoMax and AMDIM because it proposes to measure
the instance-instance similarity rather than context-instance
similarity. We will discuss it in the following subsection.
In language pre-training, InfoWord proposes to maximize the mutual information between a global representation
of a sentence and n-grams in it. The context is induced from
the sentence with selected n-grams being masked, and the
negative contexts are randomly picked out from the corpus.
In graph learning, Deep Graph InfoMax (DGI) 
regards a node’s representation as the local feature and
the average of randomly samples 2-hop neighbors as the
context. However, it is hard to generate negative contexts
on a single graph. To solve this problem, DGI proposes
Fig. 10: Deep Graph InfoMax uses a readout function to
generate summary vector s1, and puts it into a discriminator
with node 1’s embedding x1 and corrupted embedding
ex1 respectively to identify which embedding is the real
embedding. The corruption is to shufﬂe the positions of
to corrupt the original context by keeping the sub-graph
structure and permuting the node features. DGI is followed
by many works, such as InfoGraph , which targets
learning graph-level representation rather than node level,
maximizing the mutual information between graph-level
representation and substructures at different levels. As what
CMC has done to improve Deep InfoMax, in authors
propose a contrastive multi-view representation learning
method for the graph. They also discover that graph diffusion
is the most effective way to yield augmented positive sample
pairs in graph learning.
As an attempt to unify graph pre-training, in ,
the authors systematically analysis the pre-training strategies for graph neural networks from two dimensions: attribute/structural and node-level/graph-level. For structural
prediction at node-level, they propose Context Prediction
to maximize the MI between the k-hop neighborhood’s
representations and its context graph. For attributes in the
chemical domain, they propose Attribute Mask to predict a
node’s attribute according to its neighborhood, which is a
generative objective similar to token masks in BERT.
S2GRL further separates nodes in the context graph
into k-hop context subgraphs and maximizes their MI with
target node, respectively. However, a fundamental problem
of graph pre-training is about learning inductive biases
across graphs, and existing graph pre-training work is only
applicable for a speciﬁc domain.
Instance-Instance Contrast
Though MI-based contrastive learning achieves great success,
some recent studies , , , cast doubt on the
actual improvement brought by MI.
The provides empirical evidence that the success
of the models mentioned above is only loosely connected
to MI by showing that an upper bound MI estimator leads
to ill-conditioned and lower performance representations.
Instead, more should be attributed to encoder architecture
and a negative sampling strategy related to metric learning.
A signiﬁcant focus in metric learning is to perform hard
positive sampling while increasing the negative sampling
efﬁciency. They probably play a more critical role in MI-based
models’ success.
As an alternative, instance-instance contrastive learning
discards MI and directly studies the relationships between
different samples’ instance-level local representations as what
Fig. 11: Cluster-based instance-instance contrastive emthods:
DeepCluster and Local Aggregation . In the embedding space, DeepCluster uses clustering to yield pseudo
labels for discrimination to draw near similar samples.
However, Local Aggregation shows that a egocentric softclustering objective would be more effective.
metric learning does. Instance-level representation, rather
than context-level, is more crucial for a wide range of classi-
ﬁcation tasks. For example, in an image classiﬁed as “dog”,
while there must be dog instances, some other irrelevant
context objects such as grass might appear. But what matters
for the image classiﬁcation is the dog rather than the context.
Another example would be sentence emotional classiﬁcation,
which primarily relies on few but important keywords.
In the early stage of instance-instance contrastive learning’s development, researchers borrow ideas from semisupervised learning to produce pseudo labels via clusterbased discrimination and achieve rather good performance
on representations. More recently, CMC , MoCo ,
SimCLR , and BYOL further support the above
conclusion by outperforming the context-instance contrastive
methods and achieve a competitive result to supervised
methods under the linear classiﬁcation protocol. We will
start with cluster-based discrimination proposed earlier and
then turn to instance-based discrimination.
Cluster Discrimination
Instance-instance contrast is ﬁrst studied in clustering-based
methods , , , , especially the DeepCluster 
which ﬁrst achieves competitive performance to the supervised model AlexNet .
Image classiﬁcation asks the model to categorize images
correctly, and the representation of images in the same category should be similar. Therefore, the motivation is to pull
similar images near in the embedding space. In supervised
learning, this pulling-near process is accomplished via label
supervision; in self-supervised learning, however, we do
not have such labels. To solve the label problem, Deep
Cluster proposes to leverage clustering to yield pseudo
labels and asks a discriminator to predict images’ labels. The
training could be formulated in two steps. In the ﬁrst step,
DeepCluster uses K-means to cluster encoded representation
and produces pseudo labels for each sample. Then in the
second step, the discriminator predicts whether two samples
are from the same cluster and back-propagates to the encoder.
These two steps are performed iteratively.
Recently, Local Aggregation (LA) has pushed forward the cluster-based method’s boundary. It points out
several drawbacks of DeepCluster and makes the corresponding optimization. First, in DeepCluster, samples are
assigned to mutual-exclusive clusters, but LA identiﬁes
neighbors separately for each example. Second, DeepCluster
optimizes a cross-entropy discriminative loss, while LA
employs an objective function that directly optimizes a local
soft-clustering metric. These two changes substantially boost
the performance of LA representation on downstream tasks.
A similar work to LA would be VQ-VAE , 
that we introduce in Section 3. To conquer the traditional
deﬁciency for VAE to generate high-ﬁdelity images, VQ-VAE
proposes quantizing vectors. For the feature matrix encoded
from an image, VQ-VAE substitutes each 1-dimensional
vector in the matrix to the nearest one in an embedding
dictionary. This process is somehow the same as what LA is
Clustering-based discrimination may also help in the
generalization of other pre-trained models, transferring
models from pretext objectives to downstream tasks better.
Traditional representation learning models have only two
stages: one for pre-training and the other for evaluation. ClusterFit introduces a cluster prediction ﬁne-tuning stage
similar to DeepCluster between the above two stages, which
improves the representation’s performance on downstream
classiﬁcation evaluation.
Despite the previous success of cluster discriminationbased contrastive learning, the two-stage training paradigm
is time-consuming and poor performing compared to later
instance discrimination-based methods, including CMC ,
MoCo and SimCLR . These instance discriminationbased methods have got rid of the slow clustering stage
and introduced efﬁcient data augmentation (i.e., multiview) strategies to boost the performance. In light of these
problems, authors in SwAV bring online clustering
ideas and multi-view data augmentation strategies into the
cluster discrimination approach. SwAV proposes a swapped
prediction contrastive objectives to deal with multi-view
augmentation. The intuition is that, given some (clustered)
prototypes, different views of the same images should
be assigned into the same prototypes. SwAV names this
“assignment” as “codes”. To accelerate code computing, the
authors of SwAV design an online computing strategy. SwAV
outperforms instance discrimination-based methods when
model size is small and is more computationally efﬁcient.
Based on SwAV, a 1.3-billion-parameter SEER is trained
on 1 billion web images collected from Instagram.
In graph learning, M3S adopts a similar idea to
perform DeepCluster-style self-supervised pre-training for
better semi-supervised prediction. Given little labeled data
and many unlabeled data, for every stage, M3S ﬁrst pretrain itself to produce pseudo labels on unlabeled data as
DeepCluster does and then compares these pseudo labels
with those predicted by the model being supervised trained
on labeled data. Only top-k conﬁdent labels are added into
a labeled set for the next stage of semi-supervised training.
In , this idea is further developed into three pre-training
tasks: topology partitioning (similar to spectral clustering),
node feature clustering, and graph completion.
Instance Discrimination
The prototype of leveraging instance discrimination as a
pretext task is InstDisc . Based on InstDisc, CMC 
proposes to adopt multiple different views of an image as
positive samples and take another one as the negative. CMC
draws near multiple views of an image in the embedding
space and pulls away from other samples. However, it is
somehow constrained by the idea of Deep InfoMax, which
only samples one negative sample for each positive one.
In MoCo , researchers further develop the idea of
leveraging instance discrimination via momentum contrast,
which substantially increases the amount of negative samples.
For example, given an input image x, our intuition is to learn
a instinct representation q = fq(x) by a query encoder fq(·)
that can distinguish x from any other images. Therefore,
for a set of other images xi, we employ an asynchronously
updated key encoder fk(·) to yield k+ = fk(x) and ki =
fk(xi), and optimize the following objective
exp(q · k+/τ)
i=0 exp(q · ki/τ)
where K is the number of negative samples. This formula is
in the form of InfoNCE.
Besides, MoCo presents two other critical ideas in dealing
with negative sampling efﬁciency.
• First, it abandons the traditional end-to-end training
framework. It designs the momentum contrast learning
with two encoders (query and key), which prevents the
ﬂuctuation of loss convergence in the beginning period.
• Second, to enlarge negative samples’ capacity, MoCo
employs a queue (with K as large as 65536) to save
the recently encoded batches as negative samples. This
signiﬁcantly improves the negative sampling efﬁciency.
Fig. 12: Conceptual comparison of three contrastive loss
mechanisms. Taken from MoCo .
There are some other auxiliary techniques to ensure the
training convergence, such as batch shufﬂing to avoid trivial
solutions and temperature hyper-parameter τ to adjust the
However, MoCo adopts a too simple positive sample
strategy: a pair of positive representations come from the
same sample without any transformation or augmentation,
making the positive pair far too easy to distinguish. PIRL 
adds jigsaw augmentation as described in Section 4.1.1. PIRL
asks the encoder to regard an image and its jigsawed one as
similar pairs to produce a pretext-invariant representation.
In SimCLR , the authors further illustrate the importance of a hard positive sample strategy by introducing
data augmentation in 10 forms. This data augmentation
is similar to CMC , which leverages several different
views to augment the positive pairs. SimCLR follows the endto-end training framework instead of momentum contrast
Fig. 13: Ten different views adopted by SIMCLR . The
enhancement of positive samples substantially improves the
self-supervised learning performance. Taken from 
from MoCo, and to handle the large-scale negative samples
problem, SimCLR chooses a batch size of N as large as 8196.
The details are as follows. A minibatch of N samples
is augmented to be 2N samples ˆxj(j = 1, 2, ..., 2N). For a
pair of a positive sample ˆxi and ˆxj (derive from one original
sample), other 2(N −1) are treated as negative ones. A
pairwise contrastive loss NT-Xent loss is deﬁned as
li,j = −log
exp(sim(ˆxi, ˆxj)/τ)
k=1 I[k̸=i]exp(sim(ˆxi, ˆxk)/τ)
noted that li,j is asymmetrical, and the sim(·, ·) function
here is a cosine similarity function that can normalize the
representations. The summed up loss is
[l2i−1,2i + l2i,2i−1]
SimCLR also provides some other practical techniques,
including a learnable nonlinear transformation between the
representation and the contrastive loss, more training steps,
and deeper neural networks. conducts ablation studies
to show that techniques in SimCLR can also further improve
MoCo’s performance.
More investigation into augmenting positive samples is
made in InfoMin . The authors claim that we should
select those views with less mutual information for betteraugmented views in contrastive learning. In the optimal
situation, the views should only share the label information.
To produce such optimal views, the authors ﬁrst propose
an unsupervised method to minimize mutual information
between views. However, this may result in a loss of
information for predicting labels (such as a pure blank view).
Therefore, a semi-supervised method is then proposed to
ﬁnd views sharing only label information. This technique
leads to an improve about 2% over MoCo v2.
A more radical step is made by BYOL , which discards
negative sampling in self-supervised learning but achieves
an even better result over InfoMin. For contrastive learning
methods we mentioned above, they learn representations
by predicting different views of the same image and cast
the prediction problem directly in representation space.
However, predicting directly in representation space can
lead to collapsed representations because multi-views are
generally too predictive for each other. Without negative
samples, it would be too easy for the neural networks to
distinguish those positive views.
In BYOL, researchers argue that negative samples may
not be necessary in this process. They show that, if we
Fig. 14: The architecture of BYOL . Noted that the online
encoder has an additional layer qθ compared to the target
one, which gives the representations some ﬂexibility to be
improved during the training. Taken from 
use a ﬁxed randomly initialized network (which would
not collapse because it is not trained) to serve as the key
encoder, the representation produced by query encoder
would still be improved during training. If then we set
the target encoder to be the trained query encoder and
iterate this procedure, we would progressively achieve better
performance. Therefore, BYOL proposes an architecture
(Figure 14) with an exponential moving average strategy
to update the target encoder just as MoCo does. Additionally,
instead of using cross-entropy loss, they follow the regression
paradigm in which mean square error is used as:
≜|| ¯qθ(zθ) −¯zξ
2 = 2 −2 ·
⟨qθ(zθ), z′
||qθ(zθ)||2 · ||z′
This not only makes the model better-performed in
downstream tasks, but also more robust to smaller batch
size. In MoCo and SimCLR, a drop in batch size results
in a signiﬁcant decline in performance. However, in BYOL,
although batch size still matters, it is far less critical. The
ablation study shows that a batch size of 512 only causes
a drop of 0.3% compared to a standard batch size of 4096,
while SimCLR shows a drop of 1.4%.
In SimSiam , researchers further study how necessary
is negative sampling, and even batch normalization in
contrastive representation learning. They show that the most
critical component in BYOL is the stop gradient operation,
which makes the target representation stable. SimSiam is
proved to converge faster than MoCo, SimCLR, and BYOL
with even smaller batch sizes, while the performance only
slightly decreases.
Some other works are inspired by theoretical analysis into
the contrastive objective. ReLIC argues that contrastive
pre-training teaches the encoder to causally disentangle
the invariant content (i.e., main objects) and style (i.e.,
environments) in an image. To better enforce this observation
in the data augmentation, they propose to add an extra
KL-divergence regularizer between prediction logits of an
image’s different views. The results show that this can
enhance the models’ generalization ability and robustness
and improve the performance.
In graph learning, Graph Contrastive Coding (GCC) 
is a pioneer to leverage instance discrimination as the pretext
task for structural information pre-training. For each node,
we sample two subgraphs independently by random walks
with restart and use top eigenvectors from their normalized
graph Laplacian matrices as nodes’ initial representations.
Then we use GNN to encode them and calculate the InfoNCE
loss as what MoCo and SimCLR do, where the node
embeddings from the same node (in different subgraphs)
are viewed as similar. Results show that GCC learns better transferable structural knowledge than previous work
such as struc2vec , GraphWave and ProNE .
GraphCL studies the data augmentation strategies in
graph learning. They propose four different augmentation
methods based on edge perturbation and node dropping. It
further demonstrates that the appropriate combination of
these strategies can yield even better performance.
Self-supervised Contrastive Pre-training for Semisupervised Self-training
While contrastive learning-based self-supervised learning
continues to push the boundaries on various benchmarks,
labels are still important because there is a gap between
training objectives of self-supervised learning and supervised
learning. In other words, no matter how self-supervised
learning models improve, they are still the only powerful
feature extractor, and to transfer to the downstream task, we
still need labels more or less. As a result, to bridge the gap
between self-supervised pre-training and downstream tasks,
semi-supervised learning is what we are looking for.
Recall the MoCo that have topped the ImageNet
leader-board. Although it is proved beneﬁcial for many other
downstream vision tasks, it fails to improve the COCO object
detection task. Some following work , investigates
this problem and attributes it to the gap between the instance
discrimination and object detection. In such a situation,
while pure self-supervised pre-training fails to help, semisupervised-based self-training can contribute a lot to it.
First, we will clarify the deﬁnitions of semi-supervised
learning and self-training. Semi-supervised learning is an
approach to machine learning that combines a small amount
of labeled data with many unlabeled data during training.
Various methods derive from several different assumptions
made on the data distribution, with self-training (or selflabeling) being the oldest. In self-training, a model is trained
on the small amount of labeled data and then yield labels
on unlabeled data. Only those data with highly conﬁdent
labels are combined with original labeled data to train a new
model. We iterate this procedure to ﬁnd the best model.
The current state-of-the-art supervised model on
ImageNet follows the self-training paradigm, where we ﬁrst
train an EfﬁcientNet model on labeled ImageNet images
and use it as a teacher to generate pseudo labels on 300M
unlabeled images. We then train a larger EfﬁcientNet as a
student model based on labeled and pseudo labeled images.
We iterate this process by putting back the student as the
teacher. During the pseudo labels generation, the teacher
is not noised so that the pseudo labels are as accurate
as possible. However, during the student’s learning, we
inject noise such as dropout, stochastic depth, and data
augmentation via RandAugment to the student to generalize
better than the teacher.
In light of semi-supervised self-training’s success, it is
natural to rethink its relationship with the self-supervised
methods, especially with the successful contrastive pretrained methods. In Section 4.2.1, we have introduced
M3S that attempts to combine cluster-based contrastive
pre-training and downstream semi-supervised learning. For
computer vision tasks, Zoph et al. study the MoCo
pre-training and a self-training method in which a teacher
is ﬁrst trained on a downstream dataset (e.g., COCO) and
then yield pseudo labels on unlabeled data (e.g., ImageNet),
and ﬁnally a student learns jointly over real labels on
the downstream dataset and pseudo labels on unlabeled
data. They surprisingly ﬁnd that pre-training’s performance
hurts while self-training still beneﬁts from strong data
augmentation. Besides, more labeled data diminishes the
value of pre-training, while semi-supervised self-training
always improves. They also discover that the improvements
from pre-training and self-training are orthogonal to each
other, i.e., contributing to the performance from different
perspectives. The model with joint pre-training and selftraining is the best.
Chen et al. ’s SimCLR v2 supports the conclusion
mentioned above by showing that with only 10% of the
original ImageNet labels, the ResNet-50 can surpass the
supervised one with joint pre-training and self-training. They
propose a 3-step framework:
1) Do self-supervised pre-training as SimCLR v1, with
some minor architecture modiﬁcation and a deeper
2) Fine-tune the last few layers with only 1% or 10% of
original ImageNet labels.
3) Use the ﬁne-tuned network as teacher to yield labels on
unlabeled data to train a smaller student ResNet-50.
The success in combining self-supervised contrastive pretraining and semi-supervised self-training opens up our eyes
for a future data-efﬁcient deep learning paradigm. More
work is expected for investigating their latent mechanisms.
Pros and Cons
Because contrastive learning has assumed the downstream
applications to be classiﬁcations, it only employs the encoder
and discards the decoder in the architecture compared
to generative models. Therefore, contrastive models are
usually light-weighted and perform better in discriminative
downstream applications.
Contrastive learning is closely related to metric learning,
a discipline that has been long studied. However, selfsupervised contrastive learning is still an emerging ﬁeld,
and many problems remain to be solved, including:
1) Scale to natural language pre-training. Despite its
success in computer vision, contrastive pre-training
does not present a convincing result in the NLP benchmarks. Most contrastive learning in NLP now lies
in BERT’s supervised ﬁne-tuning, such as improving
BERT’s sentence-level representation , information
retrieval . Few algorithms have been proposed to
apply contrastive learning in the pre-training stage. As
most language understanding tasks are classiﬁcations,
a contrastive language pre-training approach should be
better than the current generative language models.
2) Sampling efﬁciency. Negative sampling is a must for
most contrastive learning, but this process is often
tricky, biased, and time-consuming. BYOL and
SimSiam are the pioneers to get contrastive learning
rid of negative samples, but it can be improved. It is also
not clear enough that what role negative sampling plays
in contrastive learning.
3) Data augmentation. Researchers have proved that data
augmentation can boost contrastive learning’s performance, but the theory for why and how it helps is still
quite ambiguous. This hinders its application into other
domains, such as NLP and graph learning, where the
data is discrete and abstract.
GENERATIVE-CONTRASTIVE
(ADVERSARIAL)
SELF-SUPERVISED LEARNING
Generative-contrastive representation learning, or in a more
familiar name adversarial representation learning, leverage
discriminative loss function as the objective. Yann Lecun
comments on adversarial learning as ”the most interesting
idea in the last ten years in machine learning.”. Its application
in learning representation is also booming.
The idea of adversarial learning derives from generative
learning, where researchers have observed some inherent
shortcomings of point-wise generative reconstruction (See
Section 3.5). As an alternative, adversarial learning learns
to reconstruct the original data distribution rather than the
samples by minimizing the distributional divergence.
In terms of contrastive learning, adversarial methods still
preserve the generator structure consisting of an encoder and
a decoder. In contrast, the contrastive abandons the decoder
component (as shown in Fig. 4). It is critical because, on the
one hand, the generator endows adversarial learning with
strong expressiveness that is peculiar to generative models;
on the other hand, it also makes the objective of adversarial
methods far more challenging to learn than that of contrastive
methods, leading to unstable convergence. In the adversarial
setting, the decoder’s existence asks the representation to be
”reconstructive,” in other words, it contains all the necessary
information for constructing the inputs. However, in the
contrastive setting, we only need to learn ”distinguishable”
information to discriminate different samples.
To sum up, the adversarial methods absorb merits from
both generative and contrastive methods together with some
drawbacks. In a situation where we need to ﬁt on an implicit
distribution, it is a better choice. In the following several
subsections, we will discuss its various applications on
representation learning.
Generate with Complete Input
This section introduces GAN and its variants for representation learning, focusing on capturing the sample’s complete
information.
The inception of adversarial representation learning
should be attributed to Generative Adversarial Networks
(GAN) , which proposes the adversarial training framework. Follow GAN, many variants , , , , ,
 emerge and reshape people’s understanding of deep
learning’s potential. GAN’s training process could be viewed
as two players play a game; one generates fake samples
while another tries to distinguish them from real ones. To
formulate this problem, we deﬁne G as the generator, D as
the discriminator, pdata(x) as the real sample distribution,
pz(z) as the learned latent sample distribution, we want to
optimize this min-max game
Ex∼pdata(x)[logD(x)]+Ez∼pz(z)[log(1−D(G(z)))]
Before VQ-VAE2, GAN maintains dominating performance on image generation tasks over purely generative
models, such as autoregressive PixelCNN and autoencoder
VAE. It is natural to think about how this framework could
beneﬁt representation learning.
However, there is a gap between generation and representation. Compared to autoencoder’s explicit latent sample
distribution pz(z), GAN’s latent distribution pz(z) is implicitly modeled. We need to extract this implicit distribution
out. To bridge this gap, AAE ﬁrst proposes a solution to
follow the autoencoder’s natural idea. The generator in GAN
could be viewed as an implicit autoencoder. We can replace
the generator with an explicit variational autoencoder (VAE)
to extract the representation out. Recall the objective of VAE
LVAE = −Eq(z|x)(−log(p(x|z)) + KL(q(z|x)∥p(z))
As we mentioned before, compared to l2 loss of autoencoder,
discriminative loss in GAN better models the high-level
abstraction. To alleviate the problem, AAE substitutes the KL
divergence function for a discriminative loss
LDisc = CrossEntropy(q(z), p(z))
that asks the discriminator to distinguish representation from
the encoder and a prior distribution.
However, AAE still preserves the reconstruction error,
which contradicts GAN’s core idea. Based on AAE, Bi-
GAN and ALI argue to embrace adversarial learning
without reservation and put forward a new framework.
Given an actual sample x
• Generator G: the generator here virtually acts as the
decoder, generates fake samples x′ = G(z) by z from a
prior latent distribution (e.g. [uniform(-1,1)]d, d refers to
dimension).
• Encoder E: a newly added component, mapping real
sample x to representation z′ = E(x). This is also exactly
what we want to train.
• Discriminator D: given two inputs [z, G(z)] and [E(x),
x], decide which one is from the real sample distribution.
It is easy to see that their training goal is E = G−1. In
other words, encoder E should learn to ”convert” generator
G. This goal could be rewritten as a l0 loss for autoencoder , but it is not the same as a traditional autoencoder
because the distribution does not make any assumption
about the data itself. The distribution is shaped by the
discriminator, which captures the semantic-level difference.
Based on BiGAN and ALI, later studies , discover
that GAN with deeper and larger networks and modiﬁed
architectures can produce even better results on downstream
Recover with Partial Input
As we mentioned above, GAN’s architecture is not born
for representation learning, and modiﬁcation is needed to
apply its framework. While BiGAN and ALI choose to extract
the implicit distribution directly, some other methods such
Fig. 15: Illustration of typical ”recovering with partial input” methods: colorization, inpainting and super-resolution.
Given the original input on the left, models are asked to
recover it with different partial inputs given on the right.
as colorization , , , , inpainting , 
and super-resolution apply the adversarial learning via
in a different way. Instead of asking models to reconstruct
the whole input, they provide models with partial input
and ask them to recover the rest parts. This is similar to
denoising autoencoder (DAE) such as BERT’s family in
natural language processing but conducted in an adversarial
Colorization is ﬁrst proposed by
 . The problem
can be described as given one color channel L in an image
and predicting the value of two other channels A, B. The
encoder and decoder networks can be set to any form of
convolutional neural network. Interestingly, to avoid the
uncertainty brought by traditional generative methods such
as VAE, the author transforms the generation task into a
classiﬁcation one. The ﬁrst ﬁgure out the common locating
area of (A, B) and then split it into 313 categories. The
classiﬁcation is performed through a softmax layer with
hyper-parameter T as an adjustment. Based on
range of colorization-based representation methods , ,
 are proposed to beneﬁt downstream tasks.
Inpainting , is more straight forward. We will ask
the model to predict an arbitrary part of an image given the
rest of it. Then a discriminator is employed to distinguish
the inpainted image from the original one. Super-resolution
method SRGAN follows the same idea to recover highresolution images from blurred low-resolution ones in the
adversarial setting.
Pre-trained Language Model
For a long time, the pre-trained language model (PTM)
focuses on maximum likelihood estimation based pretext task
because discriminative objectives are thought to be helpless
due to languages’ vibrant patterns. However, recently some
work shows excellent performance and sheds light on
contrastive objectives’ potential in PTM.
Fig. 16: The architecture of ELECTRA . It follows GAN’s
framework but uses a two-stage training paradigm to avoid
using policy gradient. The MLM is Masked Language Model.
The pioneering work is ELECTRA , surpassing BERT
given at the same computation budget. ELECTRA proposes
Replaced Token Detection (RTD) and leverages GAN’s
structure to pre-train a language model. In this setting, the
generator G is a small Masked Language Model (MLM),
which replaces masked tokens in a sentence to words.
The discriminator D is asked to predict which words are
replaced. Notice that replaced means not the same with
original unmasked inputs. The training is conducted in two
1) Warm-up the generator: train the G with MLM pretext
task LMLM(x, θG) for some steps to warm up the
parameters.
2) Trained with the discriminator: D’s parameters is initialized with G’s and then trained with the discriminative
objective LDisc(x, θD) (a cross-entropy loss). During this
period, G’s parameter is frozen.
The ﬁnal objective could be written as
LMLM(x, θG) + λLDisc(x, θD)
Though ELECTRA is structured as GAN, it is not trained
in the GAN setting. Compared to image data, which is continuous, word tokens are discrete, which stops the gradient
backpropagation. A possible substitution is to leverage policy
gradient, but ELECTRA experiments show that performance
is slightly lower. Theoretically speaking, LDisc(x, θD) is actually turning the conventional k-class softmax classiﬁcation
into a binary classiﬁcation. This substantially saves the computation effort but may somehow harm the representation
quality due to the early degeneration of embedding space.
In summary, ELECTRA is still an inspiring pioneer work in
leveraging discriminative objectives.
At the same time, WKLM proposes to perform
RTD at the entity-level. For entities in Wikipedia paragraphs,
WKLM replaced them with similar entities and trained the
language model to distinguish them in a similar discriminative objective as ELECTRA, performing exceptionally well
in downstream tasks like question answering. Similar work
is REALM , which conducts higher article-level retrieval
augmentation to the language model. However, REALM is
not using the discriminative objective.
Graph Learning
There are also attempts to utilize adversarial learning ( ,
 , ). Interestingly, their ideas are quite different from
each other.
The most natural idea is to follow BiGAN and
ALI ’s a practice that asks discriminator to distinguish
representation from generated and prior distribution. Adversarial Network Embedding (ANE) designs a generator
G that is updated in two stages: 1) G encodes sampled graph
into target embedding and computes traditional NCE with
a context encoder F like Skip-gram, 2) discriminator D is
asked to distinguish embedding from G and a sampled one
from a prior distribution. The optimized objective is a sum
of the above two objectives, and the generator G could yield
better node representation for the classiﬁcation task.
GraphGAN considers to model the link prediction
task and follow the original GAN style discriminative
objective to distinguish directly at node-level rather than
representation-level. The model ﬁrst selects nodes from the
target node’s subgraph vc according to embedding encoded
by the generator G. Then some neighbor nodes to vc selected
from the subgraph, together with those selected by G, are
put into a binary classiﬁer D to decide whether they are
linked to vc. Because this framework involves a discrete
selection procedure, while gradient descents could update
the discriminator D, the generator G is updated via policy
gradients.
Fig. 17: The architecture of GraphSGAN , which investigates density gaps in embedding space for classiﬁcation
problems. Taken from 
GraphSGAN applies the adversarial method in semisupervised graph learning with the motivation that marginal
nodes cause most classiﬁcation errors in the graph. Consider
samples in the same category; they are usually clustered in
the embedding space. Between clusters, there are density
gaps where few samples exist. The author provides rigorous
mathematical proof that we can perform complete classi-
ﬁcation theoretically if we generate enough fake samples
in density gaps. GraphSGAN leverages a generator G to
generate fake nodes in density gaps during the training and
asks the discriminator D to classify nodes into their original
categories and a category for those fake ones. In the test
period, fake samples are removed, and classiﬁcation results
on original categories could be improved substantially.
Domain Adaptation and Multi-modality Representation
Essentially, the discriminator in adversarial learning serves
to match the discrepancy between latent representation
distribution and data distribution. This function naturally
relates to domain adaptation and multi-modality representation problems, aiming to align different representation
distribution. , , , studies how GAN can help on
domain adaptation. , leverage adversarial sampling
to improve the negative samples’ quality. For multi-modality
representation, ’s image to image translation, ’s
text style transfer, ’s word to word translation and 
image to text translation show great power of adversarial
representation learning.
Pros and Cons
Generative-contrastive (adversarial) self-supervised learning
is particularly successful in image generation, transformation
and manipulation, but there are also some challenges for its
future development:
• Limited applications in NLP and graph. Due to the
discrete nature of languages and graphs, the adversarial
methods do not perform as well as they do in computer
vision. Furthermore, GAN-based language generation
has been found to be much worse than unidirectional
language models such as GPTs.
• Easy to collapse. It is also notorious that adversarial
models are prone to collapse during the training, with
numerous techniques developed to stabilize its training,
such as spectral normalization , W-GAN and so
• Not for feature extraction. Although works such as
BiGAN and BigBiGAN have explored some
ways to leverage GAN’s learned latent representation
and achieve good performance, contrastive learning has
soon outperformed them with fewer parameters.
Despite the challenges, however, it is still promising
because it overcomes some inherent deﬁcits of the pointwise generative objective. Maybe we still need to wait for a
better future implementation of this idea.
THEORY BEHIND SELF-SUPERVISED LEARNING
In last three sections, we introduces a number of empirical
works for self-supervised learning. However, we are also
curious about their theoretical foundations. In this part, we
will provide some theoretical insights on self-supervised
learning’s success from different perspectives.
Divergence Matching
As generative models, GANs pays attention to the difference between real data distribution Pdata(x) and generated
data distribution PG(x; θ):
θ∗= arg max
i=1PG(xi; θ)
KL(Pdata(x)||PG(x; θ))
f-GAN shows that the generative-adversarial approach is a special case of an exsiting more general variational
divergence estimation problem, and uses f-divergence to
train the generative models. f-divergence reﬂects the difference of two distributions P and Q:
Df(P||Q) = Eq(x)[f(p(x)
T (Ex∼p(x)[T(x)] −Ex∼q(x)[g(T(x))])
Replace KL-divergence in (20) with Jensen-Shannon(JS)
divergence JS = 1
2[Ep(x) log
p(x)+q(x) + Eq(x) log
p(x)+q(x)]
and calculate the replaced one with (21), the optimization
target of the minmax GAN is achieved.
D (EPdata(x)[log D(x)] + EPG(x;θ)[log(1 −D(x))])
Different divergence functions leads to different GAN variants. also discusses the effects of various choices of
divergence functions.
Fig. 18: GAN is able to learn disentangled features and
encode them in its modular structure. In , researchers
show that in GAN the features of cock is disentangled with
the features of background. Repainted from 
Disentangled Representation
An important drawback of supervised learning is that it
easily get trapped into spurious information. A famous
example is that supervised neural networks learn to distinguish dogs and wolves by whether they are in the grass
or snow , which means the supervised models do not
learn the disentangled representations of the grass and the
dog, which should be mutual independent.
As an alternative, GAN show its superior potential in
learning disentangled features empirically and theoretically.
InfoGAN ﬁrst proposes to learn disentangled representation with DCGAN. Conventionally, we sample white
noise from a uniform or Gaussian distribution as input to
generator of GAN. However, this white noise does not make
any sense to the characteristics of the image we generated. We
assume that there should be a latent code c whose dimensions
represent different characteristics of the image respectively
(such as rotation degree and width). We will learn this c
jointly in the discrimination period by the discriminator D,
and maximize c’s mutual information I(c; x) with the image
x = G(z, c), where G refers to the generator (actually the
Since mutual information is notoriously hard to compute,
the authors leverage the variational inference approcach to
estimates its lower bound LI(c, x), and the ﬁnal objective for
InfoGAN is modiﬁed as:
D VI(D, G) = V (D, G) −λLI(c; G(z, c))
Experiments show that InfoGAN surely learns a good
disentangled representation on MNIST. This further encourage researchers to identify whether the modular structures
for generation inner the GAN could be disentangled and
independent with each others. GAN dissection is a
pioneer work in applying causal analysis into understading
GAN. They identify the correlations between channels in the
convolutional layers and objects in the generated images, and
examine whether they are causally-related with the output.
 takes another step to examine these channels’ conditional
independence via rigorous counterfactual interventions over
them. Results indicate that in BigGAN researchers are able to
disentangle backgrounds and objects, such as replacing the
background of a cock from the bare soil with the grassland.
These work indicates the ability of GAN to learn disentangled features and other self-supervised learning methods
are likely to be capable too.
Maximizing Lower Bound
Evidence Lower Bound
VAE (variational auto-encoder) learns the representation
through learning a distribution qφ(z|x) to approximate the
posteriori distribution pθ(z|x),
KL(qφ(z|x)||pθ(z|x)) = −ELBO(θ; φ; x) + log pθ(x)
ELBO = Eqφ(z|x)[log qφ(z|x)] −Epθ[log pθ(z, x)]
where ELBO (Evidence Lower Bound Objective) is the lower
bound of the optimization target KL(qφ(z|x)||pθ(z|x)). VAE
maximizes the ELBO to minimize the difference between
qφ(z|x) and pθ(z|x).
ELBO(θ; φ; x) = −KL(qφ(z|x)||pθ(z)) + Eqφ[log pθ(x|z)]
where KL(qφ(z|x)||pθ(z)) is the regularization loss to approximate the Gaussian Distribution and Eqφ[log pθ(x|z)] is the
reconstruction loss.
Mutual Information
Most of current contrastive learning methods aim to maximize the MI(Mutual Information) of the input and its representation with joint density p(x, y) and marginal densities
p(x) and p(y):
I(X, Y ) = Ep(x,y)[log p(x, y)
= KL(p(x, y)|p(x)p(y))
Deep Infomax w maximizes the MI of local and global
features and replaces KL-divergence with JS-divergence,
which is similar to GAN mentioned above. Therefore the
optimization target of Deep Infomax becomes:
T (Ep(x,y)[log(T(x, y)]+Ep(x)p(y)[log(1−T(x, y))]) (28)
The form of the objective optimization function is similar
to (22), except that the data distribution becomes the global
and local feature distributions. From a probability point of
view, GAN and DeepInfoMax are derived from the same
process but for a different learning target. The encoder
in GAN, to an extent, works the same as the encoder in
representation learning models. The idea of generativecontrastive learning deserves to be used in self-learning
Instance Discrimination directly optimizes the
proportion of gap of positive pairs and negative pairs. One
of the commonly used estimators is InfoNCE :
L = EX[−log
exp(x · y/τ)
i=0 exp(x · x−/τ) + exp(x · y/τ)]
p(x|y)/p(x)
p(x|y)/p(x) + Σx−∈X¬p(x−|y)/p(x−)]
≈EX log[1 + p(x)
p(x|y)(N −1)Ex−p(x−|y)
≥EX log[ p(x)
= −I(y, x) + log(N)
Therefore the MI I(x, y) ≥log(N) −L. The approximation becomes increasingly accurate, and I(x, y) also
increases as N grows. This implies that it is useful to
use large negative samples(large values of N). But has
demonstrated that increasing the number of negative samples
does not necessarily help. Negative sampling remains a key
challenge to study.
Though maximizing ELBO and MI has been achieved to
obtain the state-of-art result in self-supervised representation
learning, it is demonstrated that MI and ELBO are loosely
connected with the downstream task performance .
Maximizing the lower bound(MI and ELBO) is not sufﬁcient
to learn useful representations. On the one hand, looser
bounds often yield better test accuracy in downstream tasks.
On the other hand, achieving the same lower bound value
can lead to vastly different representations and performance
on downstream tasks, which indicates that it does not
necessarily capture useful information of data .
There is a non-trivial interaction between the representation
encoder, critic, and loss function .
MI maximization can also be analyzed from the metric
learning view. provides some insight by connecting InfoNCE to the triplet (k-plet) loss in deep learning community.
The InfoNCE (29) can be rewriten as follows:
INCE = E[1
j=1ef(xi,yj) ]
= log k −E[1
i=1 log(1 + Σk
j̸=ief(xi,yj)−f(xi,yi))]
In particular f is contrained to the form f(x, y) =
φ(x)T φ(y) for a certain function φ. Then the InfoNCE is
corresponding to the expectation of the multi-class k-pair loss:
Lk−pair(φ) = 1
i=1 log(1 + Σj̸=ieφ(xi)T φ(yj)−φ(xi)T φ(yi))
In metric learning, the encoder is shared across views(φ(x)
and φ(y)) and the critic function f(x, y) = φ(x)T φ(y) is
symmetric, while the MI maximization is not constrained by
these conditions. (31) can be viewed as learning encoders
with a parameter-less inner product.
Contrastive Self-supervised Representation Learning
Relationship with Supervised Learning
Self-supervised learning, as is indicated literally, follows the
supervised learning pattern. Empirical evidence shows that
contrastive learning for pre-training is especially effective for
downstream classiﬁcation tasks (while this improvement is
not obvious on many generation tasks). We want to know
how contrastive pre-training beneﬁts supervised learning,
especially on whether self-supervised learning could learn
more, at least for accuracy, than supervised learning does.
Newell et al. examine the three possible assumptions in Figure 19 that pre-training: (a) always provides
an improvement, (b) reaches higher accuracy with fewer
labels but plateaus to the same accuracy as baseline, (c)
converges to baseline performance before accuracy plateaus.
They conducts experiment on synthetic COCO by rendering
which can provide as many labels as possible and discover
that self-supervised pre-training follows the patterns in (c),
Fig. 19: Three possible assumptions about supervised pretraining v.s. supervised training from scratch. Taken from 
indicating that self-supervised learning cannot learn more
than supervised learning, but can make it with few labels.
Although self-supervised learning cannot help on improving accuracy, there are many other aspects it can learn more,
such as model robustness and stability. Hendrycks et al. 
discovers that self-supervised trained neural networks are
much robust to adversarial examples, label corruption, and
common input corruptions. What’ more, it greatly beneﬁts
out-of-distribution detection on difﬁcult, near-distribution
outliers, so much so that it exceeds the performance of fully
supervised methods.
Understand Contrastive Loss
In , Wang et al. conduct interesting theoretical analysis
on functions of contrastive loss, and split it into two terms:
Lcontrast = E[−log
x fy/τ + P
+ E[log(ef T
uniformity
where the ﬁrst term aims at “alignment” and the second
aims at “uniformity” of sample vectors on a sphere given
the normalization condition. Experiments show that these
two terms have a large agreement with downstream tasks. In
addition, the authors explore to directly optimize alignment
and uniformity loss as:
Lalign(f; α) ≜E(x,y)∼ppos[||f(x) −f(y)||α
Luniform(f; t) ≜log Ex,y∼pdata[e−t||f(x)−f(y)||2
in a joint additive form. They conduct experiments in wide
range of scenarios including using CNN or RNN in computer
vision or natural language processing tasks, and discover
that direct optimization is consistently better than contrastive
loss. Besides, both alignment and uniformity are necessary
for a good representation. When one of the weights for these
two losses is too large, the representation would collapse.
However, it is doubtful that whether alignment and
uniformity are necessarily in the form of upper two losses,
because in BYOL , the authors display a framework
without direct negative sampling but outperform all previous contrastive learning pre-training. This illustrates us
that we may still achieve uniformity via other techniques
such as exponential moving average, batch normalization,
regularization and random initialization.
Generalization
It seems intuitive that minimizing the aforementioned loss
functions should lead the representations better to capture
the ”similarity” between different entities, but it is unclear
why the learned representations should also lead to better
performance on downstream tasks, such as linear classi-
ﬁcation tasks. Intuitively, a self-supervised representation
learning framework must capture the feature in unlabelled
data and the similarity with semantic information that is
implicitly present in downstream tasks. proposed a
conceptual framework to analyze contrastive learning on
average classiﬁcation tasks.
Contrastive learning assumes that similar data pair
(x, x+) comes from a distribution Dsim and negative sample
1 , ..., x−
k ) from a dstribution Dneg that is presumably
unrelated to x. Under the hypothesis that semantically
similar points are sampled from the same latent class, the
unsupervised loss can be expressed as:
Lun(f) = Ex+∼Dsim
[l({f(x)T (f(x+) −f(x−))})]
The self-supervised learning is to ﬁnd a funtion ˆf ∈
arg minf ˆLun(f) that minimizes the empirical unsupervised
loss within the capacity of the used encoder. As negative
points are sampled independently identically from the
datasets, Lun can be decomposed into τL=
un and (1 −τ)L̸=
according to the latent class the negative sample drawed
from. The intraclass deviation s(f) ≥c′(Lun(f)−1) controls
the Lun(f) and implies the unexpected loss contradictive
to our optimization target, which is caused by the negative
sampling strategies. Under the context of only 1 negative
sample, it is proved that optimizing unsupervised loss
beneﬁts the downstream classiﬁcation tasks:
Lsup( ˆf) ≤Lµ
sup( ˆf) ≤L̸=
un(f) + βs(f) + ηGenM
With probability at least 1 −δ, f is the feature mapping
function the encoder can capture, GenM is the generalization
error. When the sampled pair M →inf and the numebr of
latent class |C| →inf, GenM and δ →0. If the encoder is
powerful enough and trained using sufﬁently large number
of samples, the learned function f with low L̸=
un as well as
low βs(f) will have good performance on supervised tasks
(low Lsup( ˆf)).
Contrastive learning also has limitations. In fact, contrastive learning does not always pick the best supervised
representation function f. Minimizing the unsupervised
loss to get low Lsup( ˆf) does not mean that ˆf ≈
arg minf Lsup because high L̸=
un and high s(f) does not
imply high Lsup, resulting the failure of the algorithm.
The relationship between Lsup(f) and Lsup( ˆf) are further explored on the condition of mean classiﬁer loss Lµ
where µ indicates that a label c only corresponds to a
embedding vector µc := Ex∼Dc[f(x)]. If there exists a
functoin f that has intraclass concentration in strong sense
and can separate latent classes with high margin(on average)
with mean classiﬁer, then Lµ
sup( ˆf) will be low. If f(X) is
σ2 −sub −Gaussian in every direction for every class and
has maximum norm R = maxx inX ||f(x)||, then Lµ
can be controlled by Lµ
sup( ˆf) ≤γ(f)Lµ
γ(f),sup(f) + βs(f) + ηGenM + ϵ
For all ϵ > 0 and with the probability at least 1 −δ, γ = 1 +
ϵ . Under the assumption and context, optimizing
the unsupervised loss indeed helps pick the best downstream
task supervised loss.
As in the aformentioned models , (36) can also
be extended to more than one negative samples for every
similar pair. Then average loss is
Lsup( ˆf) := EΥ∼D[Lsup(Υ, ˆf)]
Besides, the general belief is that increasing the number
of negative samples always helps, at the cost of increased
computational costs. Noise Contrastive Estimation(NCE) 
explains that increasing the number of negative samples
can provably improve the variance of learning parameters.
However, argues that this does not hold for contrastive
learning and shows that it can hurt performance when the
negative samples exceed a threshold.
Under the assumptions, contradictive representation
learning is theoretically proved to beneﬁt the downstream
classiﬁcation tasks. More detailed proofs can be found in
 . This connects the ”similarity” in unlabelled data with
the semantic information in downstream tasks. Though the
connection temporarily is only in a restricted context, more
generalized research deserves exploration.
DISCUSSIONS AND FUTURE DIRECTIONS
In this section, we will discuss several open problems and future directions in self-supervised learning for representation.
Theoretical Foundation Though self-supervised learning
has achieved great success, few works investigate the mechanisms behind it. In this survey, we have listed several recent
works on this topic and show that theoretical analysis is
signiﬁcant to avoid misleading empirical conclusions.
In , researchers present a conceptual framework to
analyze the contrastive objective’s function in generalization
ability. empirically proves that mutual information
is only loosely related to the success of several MI-based
methods, in which the sampling strategies and architecture
design may count more. This type of works is crucial for selfsupervised learning to form a solid foundation, and more
work related to theory analysis is urgently needed.
Transferring to downstream tasks There is an essential gap
between pre-training and downstream tasks. Researchers
design elaborate pretext tasks to help models learn some
critical features of the dataset that can transfer to other jobs,
but sometimes this may fail to realize. Besides, the process
of selecting pretext tasks seems to be too heuristic and tricky
without patterns to follow.
A typical example is the selection of pre-training tasks
in BERT and ALBERT. BERT uses Next Sentence Prediction
(NSP) to enhance its ability for sentence-level understanding.
However, ALBERT shows that NSP equals a naive topic
model, which is far too easy for language model pre-training
and even decreases BERT’s performance.
For the pre-training task selection problem, a probably
exciting direction would be to design pre-training tasks
for a speciﬁc downstream task automatically, just as what
Neural Architecture Search does for neural network
architecture.
Transferring across datasets This problem is also known
as how to learn inductive biases or inductive learning.
Traditionally, we split a dataset into the training used for
learning the model parameters and the testing part for evaluation. An essential prerequisite for this learning paradigm
is that data in the real world conform to our dataset’s
distribution. Nevertheless, this assumption frequently fails
in experiments.
Self-supervised representation learning solves part of
this problem, especially in the ﬁeld of natural language
processing. Vast amounts of corpora used in the language
model pre-training help cover most language patterns and,
therefore, contribute to the success of PTMs in various
language tasks. However, this is based on the fact that text
in the same language shares the same embedding space. For
other tasks like machine translation and ﬁelds like graph
learning where embedding spaces are different for different
datasets, learning the transferable inductive biases efﬁciently
is still an open problem.
Exploring potential of sampling strategies In , the
authors attribute one of the reasons for the success of mutual
information-based methods to better sampling strategies.
MoCo , SimCLR , and a series of other contrastive
methods may also support this conclusion. They propose
to leverage super large amounts of negative samples and
augmented positive samples, whose effects are studied in
deep metric learning. How to further release the power of
sampling is still an unsolved and attractive problem.
Early Degeneration for Contrastive Learning Contrastive
learning methods such as MoCo and SimCLR are
rapidly approaching the performance of supervised learning
for computer vision. However, their incredible performances
are generally limited to the classiﬁcation problem. Meanwhile, the generative-contrastive method ELETRA for
language model pre-training is also outperforming other
generative methods on several standard NLP benchmarks
with fewer model parameters. However, some remarks
indicate that ELETRA’s performance on language generation
and neural entity extraction is not up to expectations.
Problems above are probably because the contrastive
objectives often get trapped into embedding spaces’ early
degeneration problem, which means that the model over-ﬁts
to the discriminative pretext task too early, and therefore
lost the ability to generalize. We expect that there would be
techniques or new paradigms to solve the early degeneration
problem while preserving contrastive learning’s advantages.
CONCLUSION
This survey comprehensively reviews the existing selfsupervised representation learning approaches in natural
language processing (NLP), computer vision (CV), graph
learning, and beyond. Self-supervised learning is the present
and future of deep learning due to its supreme ability to
utilize Web-scale unlabeled data to train feature extractors
and context generators efﬁciently. Despite the diversity
of algorithms, we categorize all self-supervised methods
into three classes: generative, contrastive, and generative
contrastive according to their essential training objectives.
We introduce typical and representative methods in each
category and sub-categories. Moreover, we discuss the pros
and cons of each category and their unique application scenarios. Finally, fundamental problems and future directions
of self-supervised learning are listed.
ACKNOWLEDGMENTS
The work is supported by the National Key R&D Program
of China (2018YFB1402600), NSFC for Distinguished Young
Scholar (61825602), and NSFC (61836013).