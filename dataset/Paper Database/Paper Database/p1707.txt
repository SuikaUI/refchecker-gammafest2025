Deep Multiple Kernel Learning
Eric V. Strobl & Shyam Visweswaran
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, USA
 ; 
Abstract‚ÄîDeep learning methods have predominantly been
applied to large artificial neural networks. Despite their state-ofthe-art performance, these large networks typically do not
generalize well to datasets with limited sample sizes. In this
paper, we take a different approach by learning multiple layers
of kernels. We combine kernels at each layer and then optimize
over an estimate of the support vector machine leave-one-out
error rather than the dual objective function. Our experiments
on a variety of datasets show that each layer successively
increases performance with only a few base kernels.
Keywords‚ÄîDeep Learning; Multiple Kernel Learning;
Kernels; Support Vector Machine.
INTRODUCTION
Deep learning methods construct new features by
transforming the input data through multiple layers of
processing.
conventionally
accomplished by training a large artificial neural network with
several hidden layers. However, the method has been limited
to datasets with very large sample sizes such as the MNIST
dataset which contains 60,000 training samples. More
recently, there has been a drive to apply deep learning to
datasets with more limited sample sizes as typical in many
real-world situations.
Kernel methods have been particularly successful on a
variety of sample sizes because they can enable a classifier to
learn a complex decision boundary with only a few parameters
by projecting the data onto a high-dimensional reproducing
kernel Hilbert space. As a result, several researchers have
investigated whether kernel learning can be modified for deep
learning. Cho et al. described the first approach by
optimizing an arc-cosine kernel, a function that mimics the
massive random projections of an infinite neural network, and
successfully integrated the kernel in a deep architecture.
However, the method did not allow easily tunable parameters
beyond the first layer. Subsequently, Zhuang et al. 
proposed to tune a combination of kernels but had trouble
optimizing the network beyond two layers. Moreover, the
second layer only consisted of a single Gaussian radial basis
function (RBF) kernel.
In this paper, we improve on the previous methods by
contributing to several key issues in deep kernel learning. The
rest of the paper is structured as follows. First, we describe
related work and provide some background on how kernels
can be constructed from other kernels. Next, we show that a
deep architecture that incorporates multiple kernels can
substantially increase the ‚Äúrichness‚Äù of representations
compared to a shallow architecture. Then, we prove that the
upper bound of the generalization error for deep multiple
kernels can be significantly less than the upper bound for deep
feed-forward networks under some conditions. We then
modify the optimization method by tuning over an estimate of
the leave-one-out error rather than the dual objective function.
We finally show that the proposed method increases test
accuracy on datasets with sample sizes as low as the upper
RELATED WORK
Several investigators have tried to extend kernels to deep
learning. Cho et al. described the first approach by
developing an arc-cosine kernel that mimics the projections of
a randomly initialized neural network. The kernel admits a
normalized kernel and can thus be stacked in multiple layers.
Successively combining these kernels can lead to increased
performance in some datasets. Nonetheless, arc-cosine kernels
do not easily admit hyper-parameters beyond the first layer,
since the kernel projects the data to an infinite-dimensional
reproducing kernel Hilbert space.
Zhuang et al. attempted to introduce tunable hyperparameters by borrowing ideas from multiple kernel learning.
The authors proposed to successively combine multiple
kernels in multiple layers, where each kernel has an associated
weight value. However, the authors had trouble optimizing the
network beyond a second layer which only consisted of a
single Gaussian RBF kernel. In this paper, we improve on the
multiple kernel learning approach by successfully optimizing
multiple layers each with multiple kernels.
BACKGROUND
Kernels compute a similarity function between two vector
inputs . A kernel can be described by the dot product
of its two basis functions.
where represents a first layer kernel. One way to
view a kernel within a kernel is the respective basis functions
within the basis functions for an number of layers:
Note that the linear kernel does not change throughout the
In the case of the polynomial kernel, we observe a polynomial
of higher order:
where , and denote the free parameters of the polynomial
kernel. From , the Gaussian RBF kernel can be
approximated as:
COMPLEXITY ANALYSIS
Kernels are designed to create different representations of
the data using basis functions. If we stack two kernels of
different types, we can often develop a representation that is
different from either alone. Moreover, we can obtain ‚Äúricher‚Äù
representations that cannot be well-approximated by a single
kernel, when we combine multiple kernels within a kernel
such as by taking their sum.
richness/complexity of a kernel via its pseudo-dimension and
then more specifically by the upper bound of the second-order
Rademacher chaos complexity ÃÇ as defined in . We also
introduce the following new definition:
Definition 1. A deep multiple kernel architecture is an -level
multiple kernel architecture with sets of kernels at each
represents the th kernel in set at layer with an
associated weight parameter
, and represents the
single combined kernel at layer . The term is used as
short-hand to denote all kernels in layer . The architecture is
depicted in Figure 1.
Theorem 1. Let be a finite number of base kernels,
the single layer kernel functions, and the multi-layer
kernel functions. Then:
ÃÇ ( ) ÃÇ ( ).
Proof. The tunable weights of the first and last layer can be
represented as two second-order tensors of non-negative
. Assuming the same architecture for each layer
excluding the first and last, the number of weights can be
represented as a fourth-order tensor of non-negative
. The total number of free weights in is
thus . The pseudo-dimension of
can now be stated as . On the other
hand, the pseudo-dimension of for the single layer
kernels can be stated as (Lemma 7, ). We can now
derive the upper bound of the Rademacher chaos complexity
for the single and multi-layer cases from Theorem 3, :
where is a natural constant, and ‚àö .
Thus, ÃÇ ( ) ÃÇ ( ).
Remark. The looser upper bound with a deep compared to a
shallow multiple kernel architecture suggests that multiple
layers can increase the richness of the kernel representations.
COMPARISON TO FEED-FORWARD NETWORKS
The increased richness of the kernels can increase the risk
of over-fitting. However, we can prove that the upper bound
of the generalization error for deep multiple kernels is
significantly less than the upper bound for deep feedforward
networks under some conditions.
Definition 2. We define a large margin feed-forward network
in which a large margin classifier is applied to the last hidden
layer of the network. We can thus equivalently represent this
feed-forward network in kernel form. We define the large
margin feed-forward network for an instance as and its
kernel as:
Theorem 2. The ÃÇ upper bound of the deep multiple kernel is
proportional to with the ÃÇ upper bound of the large margin
feed-forward network kernel when:
where represents the dimensionality of the data and the
number of hidden nodes at each layer.
Proof. Assuming we adopt the same number of hidden nodes
as the dimensionality of the data, the weights of the large
Fig 1. Depiction of a deep multiple kernel architecture. Lines represent the
weights for each set, ùúÉ
margin feed-forward network can be represented as a thirdorder tensor, where the number of free parameters is
. We equate the number of free parameters from the feedforward network kernel to the number of free parameters of a
deep multiple kernel as derived in Theorem 1 assuming the
same number of layers.
In this case, both the large margin feed-forward network kernel
and the deep multiple kernel have the same pseudo-dimension
upper bound. Hence, it follows that both
Rademacher chaos complexity upper bound proportional to
from Theorem 1, .
Remark. Theorem 2 implies that a deep multiple kernel can
have a lower generalization bound than a large margin feedforward network kernel, if we select a small number of base
kernels and sets of base kernels at each layer. This is in
contrast to the large feed-forward networks traditionally used
in deep learning.
OPTIMIZATION METHOD
classifier
. Ideally, we would like to
choose the coefficients to minimize an estimate of the true
risk of the SVM. Traditionally, this has been solved by
maximizing the margin through the gradient of the dual
objective function with respect to the kernel hyper-parameters.
However, deep learning schemes present a risk of over-fitting
with increased richness of the representations. Thus, it is
particularly important to seek a tight bound of the leave-oneout error. In this paper, we decided to use the span bound,
since it has shown promising results in single layer multiple
kernel learning . Assuming that the set of support vectors
remains the same throughout the leave-one-out procedure, the
span bound can be stated as:
where is the leave-one-out error, and is the distance
We now modify the arguments presented in Liu et al.
 for deep multiple kernel learning. The estimate of the
span bound requires a step function that is not differentiable.
Therefore, we can smooth the step function instead by using a
contracting function , where
and are non-negative weights. Similar to , we chose
and . Chapelle et al. showed that ÃÖ
can then be
smoothed by adding a regularization term:
Now, denote the set of support vectors
these new notations, we can rewrite ÃÖ
where is a diagonal matrix with elements [ ]
Theorem 3. Let be a diagonal matrix with elements [ ]
and [ ] . We also define ÃÖ as the
inverse of ÃÉ with the last row and column removed. Then,
. The proof can be found in
We calculate
by performing the standard chain rule,
where each set is normalized to a unit hypersphere:
Normalization is critical to prevent kernel values from growing
out of control. We can now create an algorithm with the
derivative of
by alternating between (1) fixing
and solving for , and (2) fixing and solving for .
Algorithm: Adaptive Span Deep Multiple Kernel Learning
and [ ] for every kernel
2. for = 1,2,‚Ä¶ do
3. solve the SVM problem with
4. for = 1,2,‚Ä¶ do
6. end for
7. if stopping criterion then break
8. end for
VII. EXPERIMENTS
Multiple kernel learning algorithms have traditionally used
RBF and polynomial kernels. However, we chose not to use
these kernels, since our objective based on the proposed
theorems suggests that we should try to maximize the upper
bound of the pseudo-dimension of the final kernel to increase
its richness with each successive layer. In fact, it can be shown
that the sum of RBF kernels has a pseudo-dimension of 1 from
Lemma 2, . Hence, we use four unique base kernels: a linear
kernel, an RBF kernel with , a sigmoid kernel with
and , and a polynomial kernel with
, , . We used one set of kernels for each
layer making the 3-layer Radamacher upper bound of the
architecture proportional to a large margin feed-forward
network kernel with ‚àö according to Theorem 2. We
initialize all
. Moreover, we use gradient descent on
the span bound for 500 iterations for both shallow and deep
multiple kernel architectures with fixed to 10 on 22
standardized UCI datasets. Datasets were randomized and split
in half, while instances with missing values were excluded.
We show increased accuracy with the incorporation of
each successive layer by optimizing over the dual objective
and span bound. There was a larger increase in accuracy with
the addition of the second layer than with the addition of the
third. However, the third layer did result in small increases in
accuracy such as the 2% increase seen in the Ionosphere
dataset with the span bound.
The proposed method increases accuracy on a range of
sample sizes. The experimental results are thus consistent with
the theorems proposed in section V. Namely, we can avoid
over-fitting by choosing a small number of base kernels and
sets of kernels at each layer. Thus, similar to single layer
kernels, the key to increased accuracy may be to choose a few
appropriate kernel representations. At the very least, we can
choose a set of appropriate single layer kernels and then the
deep architecture can help boost accuracy beyond the single
The method of optimizing over the span bound generally
performs better than optimizing over the dual objective
function. The performance difference is significant as the 2layer optimized over the span outperforms the 3-layer
optimized over the dual. These results are consistent with the
conclusions in Section VI that using a tighter upper bound on
the generalization performance can help offset the increased
kernel complexity with each subsequent layer.
VIII. CONCLUSION
We have developed a new method to successfully optimize
multiple, complete layers of kernels while increasing
generalization performance on a variety of datasets. The
method works by combining multiple kernels within each
layer to increase the richness of representations and then by
optimizing over a tight upper bound of the leave-one-out error.
ACKNOWLEDGEMENTS
This research was funded by the NLM/NIH grant T15
LM007059-24 to the University of Pittsburgh Biomedical
Informatics Training Program and the NIGMS/NIH grant T32
GM008208 to the University of Pittsburgh Medical Scientist
Training Program.