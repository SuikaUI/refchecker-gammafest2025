Large scale deep learning for computer aided detection of mammographic lesions
Kooi, T.; Litjens, G.J.; Ginneken, B. van; Gubern Merida, A.; Sanchez, C.I.; Mann, R.M.; Heeten, A.
den; Karssemeijer, N.
2017, Article / Letter to editor , pp. 303-312)
Doi link to publisher: 
Version of the following full text: Publisher’s version
Published under the terms of article 25fa of the Dutch copyright act. Please follow this link for the
Terms of Use: 
Downloaded from: 
Download date: 2025-03-26
To cite this publication please use the final published version (if applicable).
Article 25fa pilot End User Agreement
This publication is distributed under the terms of Article 25fa of the Dutch Copyright Act (Auteurswet)
with explicit consent by the author. Dutch law entitles the maker of a short scientific work funded either
wholly or partially by Dutch public funds to make that work publicly available for no consideration
following a reasonable period of time after the work was first published, provided that clear reference is
made to the source of the first publication of the work.
This publication is distributed under The Association of Universities in the Netherlands (VSNU) ‘Article
25fa implementation’ pilot project. In this pilot research outputs of researchers employed by Dutch
Universities that comply with the legal requirements of Article 25fa of the Dutch Copyright Act are
distributed online and free of cost or other barriers in institutional repositories. Research outputs are
distributed six months after their first online publication in the original published version and with
proper attribution to the source of the original publication.
You are permitted to download and use the publication for personal purposes. All rights remain with the
author(s) and/or copyrights owner(s) of this work. Any use of the publication other than authorised
under this licence or copyright law is prohibited.
If you believe that digital publication of certain material infringes any of your rights or (privacy)
interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library
will make the material inaccessible and/or remove it from the website. Please contact the Library
through email: , or send a letter to:
University Library
Radboud University
Copyright Information Point
PO Box 9100
6500 HA Nijmegen
You will be contacted as soon as possible.
Medical Image Analysis 35 303–312
Contents lists available at ScienceDirect
Medical Image Analysis
journal homepage: www.elsevier.com/locate/media
Large scale deep learning for computer aided detection of
mammographic lesions
Thijs Kooi a , ∗, Geert Litjens a , Bram van Ginneken a , Albert Gubern-Mérida a ,
Clara I. Sánchez a , Ritse Mann a , Ard den Heeten b , Nico Karssemeijer a
a Diagnostic Image Analysis Group, Department of Radiology, Radboud University Medical Center, Nijmegen, The Netherlands
b Department of Radiology, University Medical Centre Amsterdam, Amsterdam, The Netherlands
a r t i c l e
Article history:
Received 11 February 2016
Revised 12 July 2016
Accepted 20 July 2016
Available online 2 August 2016
Computer aided detection
Mammography
Deep learning
Machine learning
Breast cancer
Convolutional neural networks
a b s t r a c t
Recent advances in machine learning yielded new techniques to train deep neural networks, which re-
sulted in highly successful applications in many pattern recognition tasks such as object detection and
speech recognition. In this paper we provide a head-to-head comparison between a state-of-the art in
mammography CAD system, relying on a manually designed feature set and a Convolutional Neural Net-
work (CNN), aiming for a system that can ultimately read mammograms independently. Both systems are
trained on a large data set of around 45,0 0 0 images and results show the CNN outperforms the traditional
CAD system at low sensitivity and performs comparable at high sensitivity. We subsequently investigate
to what extent features such as location and patient information and commonly used manual features
can still complement the network and see improvements at high speciﬁcity over the CNN especially with
location and context features, which contain information not available to the CNN. Additionally, a reader
study was performed, where the network was compared to certiﬁed screening radiologists on a patch
level and we found no signiﬁcant difference between the network and the readers.
© 2016 Elsevier B.V. All rights reserved.
1. Introduction
Nearly 40 million mammographic exams are performed in the
US alone on a yearly basis, arising predominantly from screening
programs implemented to detect breast cancer at an early stage,
which has been shown to increase chances of survival . Similar programs have been imple-
mented in many western countries. All this data has to be in-
spected for signs of cancer by one or more experienced readers
which is a time consuming, costly and most importantly error
prone endeavor. Striving for optimal health care, Computer Aided
Detection and Diagnosis (CAD) systems are being developed and are
currently widely employed as a second reader , with numbers from the US going up to 70% of
all screening studies in hospital facilities and 85% in private insti-
tutions . Computers do not suffer from drops in
concentration, are consistent when presented with the same input
data and can potentially be trained with an incredible amount of
∗Corresponding author.
E-mail address: , (T. Kooi).
training samples, vastly more than any radiologist will experience
in his lifetime.
Until recently, the effectiveness of CAD systems and many other
pattern recognition applications depended on meticulously hand-
crafted features, topped off with a learning algorithm to map it to
a decision variable. Radiologists are often consulted in the process
of feature design and features such as the contrast of the lesion,
spiculation patterns and the sharpness of the border are used, in
the case of mammography. These feature transformations provide
a platform to instill task-speciﬁc, a-priori knowledge, but cause a
large bias towards how we humans think the task is performed.
Since the inception of Artiﬁcial Intelligence (AI) as a scientiﬁc dis-
cipline, research has seen a shift from rule-based, problem spe-
ciﬁc solutions to increasingly generic, problem agnostic methods
based on learning, of which deep learning is its most
recent manifestation. Directly distilling information from training
samples, rather than the domain expert, deep learning allows us to
optimally exploit the ever increasing amounts of data and reduce
human bias. For many pattern recognition tasks, this has proven to
be successful to such an extent that systems are now reaching hu-
man or even superhuman performance .
 
1361-8415/© 2016 Elsevier B.V. All rights reserved.
T. Kooi et al. / Medical Image Analysis 35 303–312
The term deep typically refers to the layered non-linearities
in the learning systems, which enables the model to represent
a function with far less parameters and facilitates more eﬃcient
learning . These models are not
new and work has been done since the late seventies . In 2006, however, two papers showing deep networks can be
trained in a greedy, layer-wise fashion sparked new interest in the
topic. Restricted Boltzmann Machines (RBM’s), probabilistic gener-
ative models, and autoencoders (AE), one layer neural networks,
were shown to be expedient pattern recognizers when stacked
to form Deep Belief Networks (DBN) and Stacked Autoencoders, respectively. Currently,
fully supervised, Convolutional Neural Networks (CNN) dominate
the leader boards . Their performance increase with respect to the pre-
vious decades can largely be attributed to more eﬃcient training
methods, advances in hardware such as the employment of many
core computing and most importantly, sheer
amounts of annotated training data .
To the best of our knowledge, Sahiner et al. were the
ﬁrst to attempt a CNN setup for mammography. Instead of raw im-
ages, texture maps were fed to a simple network with two hidden
layers, producing two and three feature images respectively. The
method gave acceptable, but not spectacular results. Many things
have changed since this publication, however, not only with regard
to statistical learning, but also in the context of acquisition tech-
niques. Screen Film Mammography (SFM) has made way for Dig-
ital Mammography (DM), enabling higher quality, raw images in
which pixel values have a well-deﬁned physical meaning and eas-
ier spread of large amounts of training data. Given the advances in
learning and data, we feel a revisit of CNNs for mammography is
more than worthy of exploration.
Work on CAD for mammography has been done since
the early nineties but unfortunately, progress has mostly stag-
nated in the past decade. Methods are being developed on small
data sets which
are not always shared and algorithms are diﬃcult to compare
 . Breast cancer has two main manifesta-
tions in mammography, ﬁrstly the presence of malignant soft tis-
sue or masses and secondly the presence of microcalciﬁcations
 and separate systems are being devel-
oped for each. Microcalciﬁcations are often small and can easily
be missed by oversight. Some studies suggest CAD for microcal-
ciﬁcations is highly effective in reducing oversight with acceptable numbers of false positives. However, the
merit of CAD for masses is less clear, with research suggesting hu-
man errors do not stem from oversight but rather misinterpreta-
tion . Some studies show no increase in sen-
sitivity or speciﬁcity with CAD for masses or
even a decreased speciﬁcity without an improvement in detection
rate or characterization of invasive cancers . We therefore feel motivated to improve upon
the state-of-the art.
In previous work in our group we showed
that a sophisticated CAD system taking into account not only local
information, but also context, symmetry and the relation between
the two views of the same breast can operate at the performance
of a resident radiologist and of a certiﬁed radiologist at high speci-
ﬁcity. In a different study it was shown
that when combining the judgment of up to twelve radiologists,
reading performance improved, providing a lower bound on the
maximum amount of information in the medium and suggesting
ample room for improvement of the current system.
In this paper, we provide a head-to-head comparison between
a CNN and a CAD system relying on an exhaustive set of manu-
ally designed features and show the CNN outperforms a state-of-
the-art mammography CAD system, trained on a large dataset of
around 45,0 0 0 images. We will focus on the detection of solid,
malignant lesions including architectural distortions, treating be-
nign abnormalities such as cysts or ﬁbroadenomae as false posi-
tives. The goal of this paper is not to give an optimally concise
set of features, but to use a complete set where all descriptors
commonly applied in mammography are represented and provide
a fair comparison with the deep learning method. As mentioned
by Szegedy et al. , success in the past two years in the con-
text of object recognition can in part be attributed to judiciously
combining CNNs with classical computational vision techniques. In
this spirit, we employ a candidate detector to obtain a set of sus-
picious locations, which are subjected to further scrutiny, either by
the classical system or the CNN. We subsequently investigate to
what extent the CNN is still complementary to traditional descrip-
tors by combining the learned representation with features such
as location, contrast and patient information, part of which are not
explicitly represented in the patch fed to the network. Lastly, a
reader study is performed, where we compare the scores of the
CNN to experienced radiologists on a patch level.
The rest of this paper is organized as follows. In the next sec-
tion, we will give details regarding the candidate detection system,
shared by both methods. In Section 3 , the CNN will be introduced
followed by a description of the reference system in Section 4 . In
Section 5 , we will describe the experiments performed and present
results, followed by a discussion in Section 6 and conclusion in
Section 7 .
2. Candidate detection
Before gathering evidence, every pixel is a possible center of
a lesion. This approach yields few positives and an overwhelming
amount of predominantly obvious negatives. The actual diﬃcult
examples could be assumed to be outliers and generalized away,
hindering training. Sliding window methods, previously popular in
image analysis are recently losing ground in favor of candidate de-
tection such as selective search to reduce the search space . We therefore follow a two-stage classiﬁcation pro-
cedure where in the ﬁrst stage, candidates are detected and sub-
jected to further scrutiny in a second stage, similar to the pipeline
described in Hupse et al. . Rather than class agnostic and
potentially less accurate candidate detection methods, we use an
algorithm designed for mammographic lesions . It operates by extracting ﬁve features based on ﬁrst
and second order Gaussian kernels, two designed to spot the cen-
ter of a focal mass and two looking for spiculation patterns, char-
acteristic of malignant lesions. A ﬁnal feature indicates the size of
optimal response in scale-space.
To generate the pixel based training set, we extracted positive
samples from a disk of constant size inside each annotated malig-
nant lesion in the training set, to sample the same amount from
every lesion size and prevent bias for larger areas. To obtain nor-
mal pixels for training, we randomly sampled 1 in 300 pixels from
normal tissue in normal images, resulting in approximately 130
negative samples per normal image. The resulting samples were
used to train a random forest (RF) classiﬁer. RFs
can be parallelized easily and are therefore fast to train, are less
susceptible to overﬁtting and easily adjustable for class-imbalance
and therefore suitable for this task.
To obtain lesion candidates, the RF is applied to all pixel loca-
tions in each image, both in the train and test set, generating a
likelihood image, where each pixel indicates the estimated suspi-
T. Kooi et al. / Medical Image Analysis 35 303–312
Fig. 1. Illustration of the candidate detection pipeline. A candidate detector is
trained using ﬁve pixel features and applied to all pixesl in all images, generating a
likelihood image. Local optima in the likelihood image are used as seed points for
both the reference system and the CNN. (See Fig. 2 ).
Fig. 2. Two systems are compared. A candidate detector (see Fig. 1 ) generates a set
of candidate locations. A traditional CAD system (left) uses these locations as seed
points for a segmentation algorithm. The segmentations are used to compute region
based features. The second system based on a CNN (right) uses the same locations
as the center of a region of interest.
ciousness. Non-maximum suppression was performed on this im-
age and all optima in the likelihood image are treated as candi-
dates and fed as input to both the reference feature system and
the CNN. For the reference system, the local optima in the likeli-
hood image are used as seed points for a segmentation algorithm.
For the CNN, a patch centered around the location is extracted. An
overview of the ﬁrst stage pipeline is provided in Fig. 1 . Fig. 2 il-
lustrates the generated candidates for both systems.
3. Deep convolutional neural network
In part inspired by human visual processing faculties, CNNs
learn hierarchies of ﬁlter kernels, in each layer creating a more ab-
stract representation of the data. The term deep generally refers
to the nesting of non-linear functions . Multi Lay-
ered Perceptrons (MLPs) have been shown to be universal function
approximators, under some very mild assumptions, and therefore,
there is no theoretical limit that prevents them from learning the
same mapping as a deep architecture would. Training, however,
has been shown, mostly empirically, to be far more eﬃcient in a
deep setting and the same function can be represented with fewer
parameters. Deep CNN’s are currently the most proﬁcient for vi-
sion and in spite of the simple mathematics, have been shown to
be extremely powerful.
Contemporary architectures roughly comprise convolutional,
pooling and fully connected layers. Every convolution results in
a feature map, which is downsampled in the pooling layer. The
most common form of pooling is max-pooling, in which the max-
imum of a neighborhood in the feature map is taken. Pooling in-
duces some translation invariance and downscales the image to re-
duce the amount of weights with each layer. It also reduces loca-
tion precision, however, rendering it less suitable for segmentation
tasks. The exact merit of fully connected layers is still an open
research question, but many studies report an increase in perfor-
mance with these in the architectures.
If we let Y k
L denote the k th feature map of layer L , generated by
convolution with kernel W k , it is computed according to:
L ∗Y L −1 + b k
with ∗the convolution operator and f ( ·) a non-linear activation
function and b k
L a bias term. Traditional MLPs use sigmoidal func-
tions to facilitate learning of non-linearly separable problems.
However, Rectiﬁed Linear Units (ReLU):
f(a ) = max (0 , a )
of activation a , have been shown to be easier to train, since the
activation is not squashed by asymptote in the logistic functions
 . The parameters  are typically ﬁt to the
data using maximum likelihood estimation:
L (, D) = arg max
where h ( X | ) produces the posterior probability of sample X . Tak-
ing the logarithm and negating it to put it into a minimization
framework for convenience, will yield the cross-entropy loss:
−ln [ P (D| )] = −
yh (X ;) + (1 −y )(1 −h (X ;))]
where y indicates the class label. This can be optimized using gra-
dient descent. For large datasets that do not ﬁt in memory and
data with many redundant samples, minibatch Stochastic Gradient
Descent (SGD) is typically used. Rather than computing the gra-
dient on the entire set, it is computed in small batches. Standard
back propagation is subsequently used to adjust weights in all lay-
Although powerful, contemporary architectures are not fully in-
variant to geometric transformations, such as rotation and scale.
Data augmentation is typically performed to account for this.
3.1. Data augmentation
Data augmentation is a technique often used in the context of
deep learning and refers to the process of generating new samples
from data we already have, hoping to ameliorate data scarcity and
prevent overﬁtting. In object recognition tasks in natural images,
simple horizontal ﬂipping is usually only performed, but for tasks
such as optical character recognition it has been shown that elas-
tic deformations can greatly improve performance . The main sources of variation in mammography at a lesion
level are rotation, scale, translation and the amount of occluding
We augmented all positive examples with scale and transla-
tion transformations. Full scale or translation invariance is not de-
sired nor required since the candidate detector is expected to ﬁnd
a patch centered around the actual focal point of the lesion. The
problem is not completely scale-invariant either: large lesions in
a later stage of growth are not simply scaled-up versions of re-
cently emerged abnormalities. The key is therefore to perform the
right amount of translation and scaling in order to generate re-
alistic lesion candidates. To this end, we translate each patch in
the training set containing an annotated malignant lesion 16 times
by adding values sampled uniformly from the interval [ −25 , 25]
(0.5 cm) to the lesion center and scale it 16 times by adding val-
ues from the interval [ −30 , 30] (0.6 cm) to the top left and bot-
tom right of the bounding box. After this, all patches including the
normals were rotated using simple ﬂipping actions, which can be
computed on the ﬂy to generate three more samples. This results
T. Kooi et al. / Medical Image Analysis 35 303–312
Fig. 3. Examples of scaling and translation of the patches. The top left image is the
original patch, the second and third image of the top row examples of the smallest
and largest scaling employed. The bottom row indicates the extrema in the range
of translation used.
in (1 + 16 + 16)4 = 132 patches per positive lesions and 4 per neg-
ative. Examples of the range of scaling and translation augmenta-
tion are given in Fig. 3 .
4. Reference system
The large majority of CAD systems rely on some form of seg-
mentation of the candidates on which region based features are
computed. To this end, we employ the mass segmentation method
proposed by Timp and Karssemeijer , which was shown
to be superior to other methods and active contour segmentation ) on their particular feature set. The image is trans-
formed to a polar domain around the center of the candidate and
dynamic programming is used to ﬁnd an optimal contour, subject
to the constraint that the path must start and end in the same
column in order to generate a closed contour in the Cartesian do-
main. A cost function incorporating a deviation from the expected
Grey level, edge strength and size terms is used to ﬁnd an optimal
segmentation. One of the problems with this method and many
knowledge driven segmentation methods for that matter, is that it
is conditioned on a false prior : the size constraint is based on data
from malignant lesions. When segmenting a candidate, we there-
fore implicitly assume that this is a malignant region, inadvertently
driving the segmentation into a biased result. Many of the manual
features described below rely on a precise segmentation but in the
end, it is an intermediate problem. For a stand-alone application,
we are interested to provide the patient with an accurate diagno-
sis, not the exact delineation. A huge advantage of CNNs is that no
segmentation is required and patches are fed without any interme-
diate processing.
After segmentation, we extract a set of 74 features. These can
broadly be categorized into pixel level features , used by the can-
didate detector, contrast features , capturing the relation between
the attenuation coeﬃcients inside and outside the region, texture
features describing relations between pixels within the segmented
region, geometry features summarizing shape and border informa-
tion location features, indicating where the lesion is with respect to
some landmarks in the breast, context features, capturing informa-
tion about the rest of the breast and other candidates and patient
features, conveying some of the subjects background information.
4.1. Candidate detector features
As a ﬁrst set of descriptors, we re-use the ﬁve features em-
ployed by the candidate detector, which has been shown to be
beneﬁcial in previous work in our group. On top of this, we com-
pute the mean of the four texture features within the segmented
Fig. 4. A lesion (a), its segmentation (b), areas used for computing contrast features
(c) and areas used for computing margin contrast (d).
boundary and add the output of the candidate detector at the
found optimum. This gives us a set of nine outputs we call can-
didate detector features.
4.2. Contrast features
When talking to a radiologist, a feature that is often mentioned
is how well a lesion is separated from the background. Contrast
features are designed to capture this. To compute these, we apply
a distance transform to the segmented region and compare the in-
side of the segmentation with a border around it. The distance d
to the border of the segmentation is determined according to:
with A the area of the segmented lesion. An illustration is provided
in Fig. 4 . An important nuisance in this setting is the tissue sur-
rounding the lesion. In previous work, we have derived two model
based features, designed to be invariant to this factor , which were also normalized for size of the le-
sion. The sharpness of the border of the lesion is also often men-
tioned by clinicians. To capture this, we add two features: the acu-
tance and margin contrast, the different
between the inside and outside of the segmentation, using a small
margin. Illustrations of contrast features are provided in Fig. 4 .
Other contrast features described in te Brake et al. (20 0 0) were
added to give a set of 12 features.
4.3. Texture features
The presence of holes in the candidate lesion often decrease
their suspiciousness, since tumours are solid, with possibly the ex-
ception of lobular carcinoma. To detect this, we added the two iso-
density features proposed by te Brake et al. (20 0 0) . Linear struc-
tures within a lesion can indicate an unfortunate projection rather
then cancer, for which we used four linear texture features as de-
scribed by the same authors ( te Brake et al., 20 0 0 ). On top of this
we added two features based on the second order gradient im-
age of the segmented lesion. The image was convolved with sec-
ond order Gaussian derivative ﬁlters and the optimal location in
scale space was selected for each pixel. We subsequently took the
ﬁrst and second moment of the segmented lesion of the maximum
magnitude, which is expected to be high for lesions with much line
structure. Secondly, we computed gradient coocurence, by count-
ing the number of times adjacent pixels have the same orienta-
tion. Ten less biophysical features in the form of Haralick features
T. Kooi et al. / Medical Image Analysis 35 303–312
 at two different scales ( entropy, contrast, cor-
relation, energy and homogeneity ) were added to give a set of 21
texture descriptors.
4.4. Geometrical features
Regularity of the border of a lesion is often used to classify le-
sions. Again, expedient computation relies heavily on proper seg-
mentations. Nevertheless, we have incorporated ﬁve simple topol-
ogy descriptors as proposed by Peura and Iivarinen in the
system. These are eccentricity, convexity, compactness, circular vari-
ance and elliptic variance . In order to capture more of the 3D shape,
we extended these descriptors to also work with 3 dimensions.
The lesion was smoothed with a Gaussian kernel ﬁrst and 3D ec-
centricity : the ratio between the largest and smallest eigenvalue of
the point cloud, 3D compactness : the ratio of the surface area to
the volume, spherical deviance , the average deviation of each point
from a sphere and elliptical deviance : the average deviation of each
point to an ellipse ﬁtted to the point cloud were computed. Since
convex hull algorithms in 3D suffer from relatively high computa-
tional complexity, this was not extended. Op top of this, we added
a feature measuring reﬂective symmetry. The region is divided into
radial and angular bins and average difference pixel intensity be-
tween opposing bins is summed and normalized by the size of the
region. Lastly the area of the segmented region is added, giving us
a set of 10 geometric features.
4.5. Location features
Lesions are more likely to occur in certain parts of the breast
than others and other structures such as lymph nodes are more
common in the pectoralis than in other parts of the breast. To cap-
ture this, we use a simple coordinate system. The area of the breast
and pectoral muscle are segmented using thresholding and a poly-
nomial ﬁt. We subsequently estimate the nipple location by taking
the largest distance to the chest wall and a central landmark in
the chest wall is taken as the row location of the center of gravity.
From this, we extract: (1) the distance to the nipple (2) the same,
but normalized for the size of the breast, (3) the distance to the
chest wall and (4) the fraction of the lesion that lies in the pec-
toral muscle.
4.6. Context features
To add more information about the surroundings of the le-
sion, we added three context features as described by Hupse and
Karssemeijer . The features again make use of the candidate
detector and assume the posterior of pixels in the rest of the breast
convey some information about the nature of the lesion in ques-
tion. The ﬁrst feature averages the output around the lesion, the
second in a band at a ﬁxed distance from the nipple and a third
takes the whole segmented breast into account. On top of this, we
added the posterior of the candidate detector, normalized by the
sum of the top three and top ﬁve lesions in the breast, to give us
ﬁve context features in total.
4.7. Patient features
Lastly, we added the age of the patient, which is an important
risk factor. From the age, we also estimate the screening round by
subtracting 50 (the age at which screening starts in The Nether-
lands) and dividing by 2 (the step size of the screening). This gives
us two features.
Note that the last three sets of features provide information
outside of the patch fed to the CNN. Even if the network is able
to exploit all information in the training set, these could still sup-
ply complementary information regarding the nature of the lesion.
Overview of the data. Pos refers to the amount of malignant lesions and neg to
the amount of normals.
Candidates
5. Experiments
The mammograms used were collected from a large scale
screening program in The Netherlands ( bevolkingsonderzoek
midden-west ) and recorded using a Hologic Selenia digital mam-
mography system. All tumours are biopsy proven malignancies
and annotated by an experienced reader. Before presentation
to a radiologist, the manufacturer applies some processing to
optimize it for viewing by a human. To prevent information loss
and bias, we used the raw images instead and only applied a log
transform which results in pixel values being linearly related to
the attenuation coeﬃcient. Images were scaled from 70 micron to
200 for faster processing. Structure important for detecting lesions
occur at larger scales and therefore this does not cause any loss of
information.
An overview of the data is provided in Table 1 . With the term
case, we refer to all screening images recorded from a single pa-
tient. Each case consists of several exams taken at typically a two
year interval and each exam typically comprises four views, two
of each breast, although these numbers vary: some patients skip
a screening and for some exams only one view of each breast is
recorded. For training and testing, we selected all regions found
by the candidate detector. The train, validation and test set were
all split on a patient level to prevent any bias. The train and vali-
dation set comprise 44,090 mammographic views, from which we
used 39,872 for training and 4218 for validation. The test set con-
sisted of 18,182 images of 2064 patients with 271 malignant an-
notated lesions. A total of 30 views from 20 exams in the test set
contained an interval cancer that was visible in the mammogram
or were taken prior to a screen detected cancer, with the abnor-
mality already visible.
Before patch extraction in the CNN system, we segmented all
lesions in the training set in order to get the largest possible le-
sion and choose the patch size with an extra margin resulting in
patches of size 250 × 250 (5 × 5 cm). The pixel values in the
patches were scaled using simple min-max scaling, with values
calculated over the whole training set. We experimented with scal-
ing the patches locally, but this seemed to perform slightly though
not signiﬁcantly worse on the validation set. All interpolation pro-
cesses were done with bilinear interpolation. Since some candi-
dates occur at the border of the imaged breast, we pad the im-
age with zeros. Negative examples were only taken from normal
images. Annotated benign samples such as cysts and ﬁbroadeno-
mae were removed from the training set. However, not all be-
nign lesions in our data are annotated and therefore some may
have ended in the train or validation set as negatives. After aug-
mentation, the train set consisted of 334 , 752 positive patches and
853 , 800 negatives. When combining the train and validation set,
this amounts to 379 , 632 positive and 931 , 640 negative patches.
5.2. Training and classiﬁcation details
For the second stage classiﬁcation, we have experimented with
several classiﬁers 303–312
Fig. 5. Illustration of the network architecture, The numbers indicate the amount
of kernels used. We employ a scaled down version of the VGG model. To see the
extent to which conventional features can still help, the network is trained fully
supervised and the learned features are subsequently extracted from the ﬁnal layer
and concatenated with the manual features and retrained using a second classiﬁer.
Boosted Trees, MLPs) on a validation set, but found in nearly all
circumstances the random forest performed similar or better than
others. To counteract the effect of class imbalance, trees in the RF
were grown using the balance corrected Gini criterion for split-
ting and in all situations we used 20 0 0 estimators and the square
root heuristic for the maximum number of features. The maxi-
mum depth was cross-validated using 8 folds. We employed class
weights inversely proportional to the distribution in the particu-
lar bootstrap sample. The posterior probability output by the RF
was calculated as a mean of the estimated classes. The systems are
trained using at most the ten most suspicious lesions per image
found by the candidate detector, during testing no such threshold
is applied to obtain highest sensitivity.
We implemented the network in Theano 
and pointers provided by Bengio were followed and very
helpful. We used OxfordNet-like architectures with 6 convolutional layers of {16, 32, 64, 128, 128}
with 3 × 3 kernels and 2 × 2 max-pooling on all but the fourth
convolutional layer. A stride of 1 was used in all convolutions. Two
fully connected layers of 300 each were added. An illustration of
the network is provided in Fig. 5 .
We employed Stochastic Gradient Descent (SGD) with RMSProp
 , an adaption of R-Prop for SGD with Nesterov
momentum . Drop-out was used on the fully connected layers with p = 0 . 5 . We
used the MSRA weight ﬁller, a learning rate of
5 × 10 −5 with a weight decay of 5 × 10 −5 . To battle the strong
class imbalance, positive samples were presented multiple times
during an epoch, keeping a 50/50 positive/negative ratio in each
minibatch. Alternatively, the loss function could be weighted, but
we found this to perform worse, we suspect this is because re-
balancing maintains a certain diversity in the minibatch. All hy-
perparameters were optimized on a validation set and the CNN
was subsequently retrained on the full training set using the found
parameters. All test patches were also augmented using the same
augmentation scheme. On the validation set, this gave a small im-
provement. The best validation AUC was 0.90.
5.3. ROC analysis
To ﬁrst get an understanding of how well each feature set per-
forms individually, we trained different RFs for each feature set and
applied them separately to the test set. In all cases, the training
procedure as described above was used. AUC values along with a
95% conﬁdence interval, acquired using bootstrapping with 5000 bootstrap samples are
shown in Table 2 .
The CNN was compared to the reference system with equal
amount of information (i.e., excluding location, context and patient
information) to get a fair performance comparison. Fig. 6 shows a
plot of the mean curves along with the 95% conﬁdence interval
obtained after bootstrapping. Results were not found to be signif-
icantly different p = 0 . 2 on the full ROC. Fig. 7 shows a plot com-
Overview of results of individual feature sets
along the 95% conﬁdence interval (CI) obtained
using 50 0 0 bootstraps.
Feature group
Candidate detector
[0 .827, 0.887]
[0 .752, 0.817]
[0 .681, 0.753]
[0 .721, 0.784]
[0 .651, 0.719]
[0 .781, 0.850]
[0 .612, 0.688]
Equal information
[0 .864, 0.918]
[0 .881, 0.929]
Fig. 6. Comparison of the CNN with the reference system using equal information,
i.e., only information represented in the patch used by the CNN, excluding context,
location and patient information.
Fig. 7. Comparison of the CNN without any augmentation, with augmentation and
with added manual features.
paring the CNN with data augmentation to the network without
data augmentation and with data augmentation and added man-
ual features. Again bootstrapping was used to obtain signiﬁcance. It
is clear that the proposed data augmentation methods contributes
greatly to the performance, which was also found to be signiﬁcant
( p ≪0.05).
To combine the CNN with other descriptors, we extracted the
features from the last fully connected layer and appended the
other set (see Fig. 5 ). For each augmented patch, the additional
features were simply duplicated. Table 3 shows results of the CNN
T. Kooi et al. / Medical Image Analysis 35 303–312
Overview of results of the CNN combined with individual
feature sets.
Feature group added to CNN
[0 .897, 0.938]
Candidate detector
[0 .919, 0.955]
[0 .91, 0.949]
[0 .912, 0.950]
[0 .907, 0.946]
[0 .913, 0.950]
[0 .914, 0.952]
[0 .908, 0.947]
[0 .922, 0.958]
AUC values obtained when training the
network on subsets of malignant lesions
in the training set, keeping the same
amount of normals.
Data Augmentation
combined with different feature sets, again with conﬁdence inter-
val acquired by bootstrapping with 50 0 0 samples.
To investigate the degree to which a large data set is really
needed, we trained several networks on subsets, removing 40% of
the malignant lesions. Results are provided in Table 4 . Since the
differences are rather large, we did not perform signiﬁcance test-
ing. For all settings, we optimized the learning rate but kept all
other hyperparameters equal to the ones found to be optimal for
the full training set.
5.4. FROC analysis
In practice, a CAD system should ideally be operating at a refer-
ral rate similar to that of a radiologists. To get a better understand-
ing of the system’s performance around this operating point, we
compute the Partial Area Under the Curve (PAUC) on a log scale:
ln −ln [0 . 01]
and generate Free Receiver Operator Characteristic (FROC) curves,
to illustrate the numbers of false positives per image.
Plots of the FROCs of the full reference system (last line in
Table 2 ), the CNN only and the CNN plus manual features are
shown in Figs. 8 and 9 . To further investigate which features are
helpful at high speciﬁcity, we compute PAUC for each feature set
individually. Results are shown in Table 5 . We see a signiﬁcant dif-
ference comparing the CNN with additional features to the refer-
ence system P = 0 . 015 on a lesion level and P = 0 . 0 0 02 on a case
5.5. Human performance
In previous work in our group, performance of the CAD sys-
tem was compared to the performance of a radiologists at an exam
level, a collection of four images, which contains more information
than only a patch, such as context in the mammogram, symmetri-
cal difference between two breast, the relation between the CC and
MLO views. To get a better understanding of how close the CNN
is to human performance on a patch level and how much more
room there is for improvement in this sub part of the pipeline, we
performed a study where we measured the performance of experi-
enced readers on a patch level, providing the reader with the same
Fig. 8. Lesion based FROC of the three systems. Please note that this concerns the
full reference system, where context, location and patient features are incorporated.
Fig. 9. Case based FROC of the three systems. In areas of high speciﬁcity, the CNN
and the addition of manual features is particularly useful. Please note that this con-
cerns the full reference system, where context, location and patient features are
incorporated.
information as the CNN. The group of readers consisted of one ex-
perienced reader (non-radiologist) and two experienced certiﬁed
radiologists. To get an idea of the performance that can at least be
obtained on this set, the mean of the three readers was also com-
puted by simply averaging the scores that each of the three readers
assigned to each patch.
Patches were extracted from the mammogram processed by the
manufacturer for optimal viewing and were shown at a normal
computer screen at a resolution of 200 micron. Microcalciﬁcations
are diﬃcult to see in this setting, but all structures relevant for soft
tissue lesions are intact and readers did not report diﬃculties. The
readers were provided with a slider and instructed to score the
patch between zero and one hundred based on their assessment
of the suspiciousness of the patch.
As a test set, we used all masses that were used in
Hupse et al. and selected an equal amount of negatives, that
were considered the most diﬃcult by the candidate detector, re-
sulting in 398 patches. This gives a representative set of diﬃcult
samples and allows for larger differences between readers and the
CNN, but is biased towards a set diﬃcult for the reference system,
which was therefore left out of the comparison (obtained AUC was
0.64 on this set). Fig. 12 shows the ROC curves resulting from the
T. Kooi et al. / Medical Image Analysis 35 303–312
Partial Area under the FROC of different systems. P-values are refer-
ring to the comparison between the CNN with additional features
and the CNN without the speciﬁc feature group. In this case, the
reference system is the full system, including context, location and
patient information.
Reference system
CNN + candidate det.
< 0 .0 0 01
CNN + contrast
CNN + texture
CNN + topology
CNN + location
CNN + context
CNN + patient
CNN + all features
Fig. 10. Top misclassiﬁed negatives by the CNN. The second sample in the ﬁrst row
is simply the nipple and the third sample in the second row displays fat necrosis.
Both are obviously normal patches and are ﬁltered out using additional feature sets.
Fig. 11. Top misclassiﬁed positives by the CNN, most samples are very large lesion
unlikely to be found in the screening population and therefore under represented
in the training set.
reader study. Again, to test signiﬁcance we used bootstrapping and
two sided testing to get a signiﬁcance score. We found no signiﬁ-
cant difference between the CNN and any of the readers: CNN vs
reader 1: p = 0 . 1008 , CNN vs reader 2: p = 0 . 6136 , CNN vs reader
3: p = 0 . 64 , but found a signiﬁcant difference between the CNN
and the mean of the human readers ( p = 0 . 001 ).
6. Discussion
To get more insight into the performance of the network, ex-
amples of the top misclassiﬁed positives and negatives are shown
in Fig. 11 and 10 respectively. A large part of the patches deter-
mined as suspicious by the network are benign abnormalities such
as cysts and ﬁbroadenomae or normal structures such as lymph
nodes or fat necrosis. Cysts and lymph nodes can look relatively
similar to masses. These strong false positives occur due to the ab-
sence of benign lesions in our training set. In the future we plan
Fig. 12. Comparison between the CNN and three experienced readers on a patch
to add these to the training set and perform three-class classiﬁ-
cation or train a separate network to discriminate these lesions
The majority of ‘misclassiﬁed’ positives are lesions ill-
represented in the training data, either very subtle or extremely
large. When using CAD as a second reader, these will not inﬂuence
the referral decision much, as they are clearly visible to a human,
but when using the computer as an independent reader, these is-
sues need to be solved. In preliminary experiments, we have seen
that many of these misclassiﬁcations can be prevented by con-
sidering the contralateral breast and plan to work on this in the
From the results in Tables 3 and 2 we can see that individually,
apart from the candidate detector, contrast and context are useful
features. Although age and screening round are some of the most
important risk factors, we do not see clear improvements when
added as features, which is slightly disappointing. To get training
data, we took negative patches only from normal images, but not
only from normal exams, to get as many data points as possible.
A possible explanation for the disappointing performance may be
that the relation between age and cancer is more diﬃcult to learn
in the setting, since it is a relation that exist on an exam level.
To add features, we have used a second classiﬁcation stage. This
has the advantage it is easy to evaluate which features add infor-
mation, without retraining a network and re-optimizing the pa-
rameters, which can take several weeks to do properly. On top of
this, the learned feature representation of the CNN is the same in
all situations, rendering comparison more reliable. A major disad-
vantage, however, is that the training procedure is rather compli-
cated. Other more elegant methods such as coding features as a
second channel, as done by Maddison et al. or adding the
features in one of the fully connected layers of the network during
training could be better strategies and we plan to explore this in
future work.
We have made use of a more shallow and scaled down ver-
sion of the networks proposed by Simonyan and Zisserman ,
who obtain best performance on ImageNet with a 19 layer ar-
chitecture with four times the amount of kernels in each layer.
In initial experiments, we have worked with Alexnet-like archi-
tectures, which performed worse on our problem, obtaining and
AUC of around 0.85 on the validation set. We have also experi-
mented with deeper networks and increasing the amount of ker-
nels, but found no signiﬁcant improvement on the validation set
(0.896 vs 0.897 of the network with larger capacity and 0.90 of
9 layer network). We suspect that with more data, larger capacity
T. Kooi et al. / Medical Image Analysis 35 303–312
networks can become beneﬁcial. The problem could be less com-
plex than classifying natural images since it concerns a two-class
classiﬁcation in the current setting and we are dealing with gray
scale images, contrary to the thousands of classes and RGB data in
ImageNet . Therefore, more shallow and
lower capacity networks than the one found optimal for natural
images could suﬃce for this particular problem.
In our work, we made extensive use of data augmentation in
the form of simply geometric transformations. We have also ex-
perimented with full rotation, but this creates lesions not expected
during testing, due to the zero padding. This could be prevented
using test time augmentation, but when used in a sliding win-
dow fashion this is not convenient. The ROC curves in Fig. 7 show
a clear increase in performance for the full data set. The results
in Table 4 show the current data augmentation scheme improves
performance for large amounts of data but not for small amounts
of data. We suspect in the latter setting, the network overﬁts
and more regularization is needed. These results may be differ-
ent when fully optimizing the architecture and augmentation pro-
cedure for each setting individually. More research is needed to
draw clear conclusions. However effective, data augmentation is a
rather computationally costly procedure. A more elegant approach
would be to add the invariance properties in the network architec-
ture, which is currently being investigated in several papers . On top of the geomet-
ric transforms, occluding tissue is an important source of variance,
which is more challenging to explicitly code in the network archi-
tecture. In future work, we plan to explore simulation methods for
In this work, we have employed a previously developed can-
didate detector. This has two main advantages: (1) it is fast and
accurate (2) the comparison with the traditional CAD system is
straightforward and fair, since exactly the same candidate locations
are trained with and evaluated on. The main disadvantage is that
the sensitivity is not hundred percent, which causes lesions to be
missed, although the case-based performance is close to optimal.
In future work, we plan to explore other methods, such as the
strategy put forth by Cire ¸s an et al. , to train the system end-
to-end. This will make training and classiﬁcation less cumbersome
and has the potential to increase the sensitivity of the system.
In this work we have compared the CNN to a state-of-the art
CAD system , which was combined with sev-
eral other features commonly used in the mammography CAD lit-
erature. A random forest was subsequently used, that performs fea-
ture selection during its training stage. We think the feature set we
used is suﬃciently exhaustive to include most features commonly
used in literature and therefore think similar conclusions hold for
other state-of-the art CAD systems. To the best of our knowl-
edge, the Digital Database of Screening Mammography (DDSM) is
the only publicly available data set, which comprises of digitized
screen ﬁlm mammograms. Since almost all screening centers have
migrated to digital mammography, we have elected not to run our
system on this data set, because we think the clinical relevance
is arguable. On top of this, since this entails a transfer learning
problem, the system may require retraining to adapt to the older
The reader study illustrates the network is not far from the ra-
diologists performance, but still substantially below the mean of
the readers, suggesting a large performance increase is still pos-
sible. We suspect that some other augmentation methods as dis-
cussed above could push the network a bit further, but expect
more training data, when it becomes available will be the most im-
portant factor. Also, we feel still employing some handcrafted fea-
tures that speciﬁcally target weaknesses of the CNN may be a good
strategy and may be more pragmatic and effective than adding
thousands of extra samples to the training set.
7. Conclusion
In this paper we have shown that a deep learning model in
the form of a Convolutional Neural Network (CNN) trained on a
large data set of mammographic lesions outperforms a state-of-
the art system in Computer Aided Detection (CAD) and therefore
has great potential to advance the ﬁeld of research. A major ad-
vantage is that the CNN learns from data and does not rely on
domain experts, making development easier and faster. We have
shown that the addition of location information and context can
easily be added to the network and that several manually designed
features can give some small improvements, mostly in the form of
‘common sense’: obviously false negatives will no longer be con-
sidered as such. On top of this, we have compared the CNN to a
group of three experienced readers on a patch level, two of which
were certiﬁed radiologist and have show that the human readers
and CNN have similar performance.
Acknowledgements
This research was funded by grant KUN 2012-557 of the Dutch
Cancer Society and supported by the Foundation of Population
Screening Mid West.