UC San Diego
UC San Diego Previously Published Works
Modeling, clustering, and segmenting video with mixtures of dynamic textures
 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(5)
Chan, Antoni B.
Vasconcelos, Nuno
Publication Date
2008-05-01
Peer reviewed
eScholarship.org
Powered by the California Digital Library
University of California
Modeling, Clustering, and Segmenting
Video with Mixtures of Dynamic Textures
Antoni B. Chan, Student Member, IEEE, and Nuno Vasconcelos, Member, IEEE
Abstract—A dynamic texture is a spatio-temporal generative model for video, which represents video sequences as observations from a
linear dynamical system. This work studies the mixture of dynamic textures, a statistical model for an ensemble of video sequences that is
sampled from a finite collection of visual processes, each of which is a dynamic texture. An expectation-maximization (EM) algorithm is
derived for learning the parameters of the model, and the model is related to previous works in linear systems, machine learning, timeseries clustering, control theory, and computer vision. Through experimentation, it is shown that the mixture of dynamic textures is a
suitable representation for both the appearance and dynamics of a variety of visual processes that have traditionally been challenging for
computer vision (for example, fire, steam, water, vehicle and pedestrian traffic, and so forth). When compared with state-of-the-art
methods in motion segmentation, including both temporal texture methods and traditional representations (for example, optical flow or
other localized motion representations), the mixture of dynamic textures achieves superior performance in the problems of clustering and
segmenting video of such processes.
Index Terms—Dynamic texture, temporal textures, video modeling, video clustering, motion segmentation, mixture models, linear
dynamical systems, time-series clustering, Kalman filter, probabilistic models, expectation-maximization.
INTRODUCTION
NE family of visual processes that has relevance for
various applications of computer vision is that of, what
could be loosely described as, visual processes composed of
ensembles of particles subject to stochastic motion. The particles
can be microscopic (for example, plumes of smoke),
macroscopic (for example, leaves and vegetation blowing
in the wind), or even objects (for example, a human crowd,
a flock of birds, a traffic jam, or a bee hive). The applications
range from remote monitoring for the prevention of natural
disasters (for example, forest fires), to background subtraction in challenging environments (for example, outdoors
scenes with vegetation), and various type of surveillance
(for example, traffic monitoring, homeland security applications, or scientific studies of animal behavior).
Despite their practical significance and the ease with
which they are perceived by biological vision systems, the
visual processes in this family still pose tremendous
challenges for computer vision. In particular, the stochastic
nature of the associated motion fields tends to be highly
challenging for traditional motion representations such as
optical flow , , , , which requires some degree
of motion smoothness, parametric motion models , ,
 , which assume a piecewise planar world , or object
tracking , , , which tends to be impractical when
the number of subjects to track is large, and these objects
interact in a complex manner.
The main limitation of all these representations is that they
are inherently local, aiming to achieve understanding of the
whole by modeling the motion of the individual particles.
This is contrary to how these visual processes are perceived by
biological vision: smoke is usually perceived as a whole, a tree
is normally perceived as a single object, and the detection of
traffic jams rarely requires tracking individual vehicles.
Recently, there has been an effort to advance toward this
type of holistic modeling by viewing video sequences derived
from these processes as dynamic textures or, more precisely,
samples from stochastic processes defined over space and
time , , , , , , , . In fact, the
dynamic texture framework has been shown to have great
potentialfor videosynthesis , ,image registration ,
motion segmentation , , , , , , and video
classification , . This is, in significant part, due to the
fact that the underlying generative probabilistic framework is
capable of 1) abstracting a wide variety of complex motion
patterns into a simple spatio-temporal process and 2) synthesizing samples of the associated time-varying texture.
One significant limitation of the original dynamic texture
model is, however, its inability to provide a perceptual
decomposition into multiple regions, each of which belongs
to a semantically different visual process: for example, a
flock of birds flying in front of a water fountain, highway
traffic moving in opposite directions, video containing both
smoke and fire, and so forth. One possibility to address this
problem is to apply the dynamic texture model locally 
by splitting the video into a collection of localized spatiotemporal patches, fitting the dynamic texture to each patch,
and clustering the resulting models. However, this method,
along with other recent proposals , , lacks some of
the attractive properties of the original dynamic texture
model: a clear interpretation as a probabilistic generative
model for video and the necessary robustness to operate
without manual initialization.
To address these limitations, we note that while the
holistic dynamic texture model in is not suitable for
such scenes, the underlying generative framework is. In
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
. The authors are with the Department of Electrical and Computer
Engineering, University of California, San Diego, 9500 Gilman Drive,
Mail Code 0409, La Jolla, CA 92093-0409.
E-mail: , .
Manuscript received 8 Sept. 2006; revised 6 Mar. 2007; accepted 16 May
2007; published online 5 July 2007.
Recommended for acceptance by S. Stefano.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number TPAMI-0656-0906.
Digital Object Identifier no. 10.1109/TPAMI.2007.70738.
0162-8828/08/$25.00  2008 IEEE
Published by the IEEE Computer Society
fact, co-occurring textures can be accounted for by
augmenting the probabilistic generative model with a
discrete hidden variable, which has a number of states equal
to the number of textures, and encodes which of them is
responsible for a given piece of the spatio-temporal video
volume. Conditioned on the state of this hidden variable,
the video volume is then modeled as a simple dynamic
texture. This leads to a natural extension of the dynamic
texture model, a mixture of dynamic textures (or mixture
of linear dynamical systems (LDSs)), which we study in this
work. The mixture of dynamic textures is a generative
model, where a collection of video sequences (or video
patches) are modeled as samples from a set of underlying
dynamic textures. It provides a natural probabilistic framework for clustering video and for video segmentation
through the clustering of spatio-temporal patches.
In addition to introducing the dynamic texture mixture as
a generative model for video, we report on three main
contributions. First, an expectation-maximization (EM) algorithm is derived for maximum-likelihood estimation of the
parameters of a dynamic texture mixture. Second, the
relationships between the mixture model and various other
models previously proposed, including mixtures of factor
analyzers, LDSs, and switched linear dynamic models, are
analyzed. Finally, we demonstrate the applicability of the
model to the solution of traditionally difficult vision
problems that range from clustering traffic video sequences
to segmentation of sequences containing multiple dynamic
textures. The paper is organized as follows. In Section 2, we
formalize the dynamic texture mixture model. In Section 3,
we present the EM algorithm for learning its parameters from
training data. In Sections 4 and 5, we relate it to previous
models and discuss its application to video clustering and
segmentation. Finally, in Section 6, we present an experimental evaluation in the context of these applications.
MIXTURES OF DYNAMIC TEXTURES
In this section, we describe the dynamic texture mixture
model. For completeness, we start with a brief review of the
dynamic texture model.
Dynamic Texture
A dynamic texture , is a generative model for both the
appearance and the dynamics of video sequences. It consists
of a random process containing an observed variable yt, which
encodes the appearance component (video frame at time t),
and a hidden state variable xt, which encodes the dynamics
(evolution of the video over time). The state and observed
variables are related through the LDS defined by
xtþ1 ¼ Axt þ vt
yt ¼ Cxt þ wt;
where xt 2 IRn and yt 2 IRm (typically n  m). The parameter A 2 IRnn is a state-transition matrix, and C 2 IRmn is
an observation matrix (for example, containing the principal
components of the video sequence when learned with that in
 ). The driving noise process vt is normally distributed with
zero mean and covariance Q, that is, vt  N ð0; QÞ, where Q 2
þ is a positive-definite n  n matrix. The observation noise wt
is also zero mean and Gaussian, with covariance R, that is,
wt  N ð0; RÞ, where R 2 SSm
þ. Note that the model adopted
throughout this work, which supports an initial state x1 of
arbitrary mean and covariance, that is, x1  N ð; SÞ, is a
slight extension of the model originally proposed in .1
Thisextensionproducesarichervideomodelthatcancapture
variability in the initial frame and is necessary for learning a
dynamic texture from multiple video samples with different
initial frames (as is the case in clustering and segmentation
problems). The dynamic texture is specified by the parameters  ¼ fA; Q; C; R; ; Sg and can be represented by the
graphical model in Fig. 1a.
It can be shown , from this definition that the
distributions of the initial state, the conditional state, and
the conditional observation are
pðx1Þ ¼ Gðx1; ; SÞ;
pðxtjxt1Þ ¼ Gðxt; Axt1; QÞ;
pðytjxtÞ ¼ Gðyt; Cxt; RÞ;
where Gðx; ; Þ ¼ ð2Þn=2jj1=2e1
 is the n-dimensional multivariate Gaussian distribution, and kxk2
xT1x is the Mahalanobis distance with respect to the
covariance matrix . Letting x
1 ¼ ðx1;    ; xÞ, and y
ðy1;    ; yÞ be a sequence of states and observations, the joint
distribution is
1Þ ¼ pðx1Þ
pðxtjxt1Þ
A number of methods are available to learn the parameters of
the dynamic texture from a training video sequence,
including maximum-likelihood methods (for example, expectation-maximization ), noniterative subspace methods (for example, N4SID ) or a suboptimal, but
computationally efficient, procedure .
Mixture of Dynamic Textures
Under the dynamic texture mixture model, the observed
videosequencey
1 issampledfromoneofK dynamictextures,
each having some nonzero probability of occurrence. This is a
useful extension for two classes of applications. The first class
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
1. The initial condition in is specified by a fixed initial vector x0 2
IRn or, equivalently, x1  N ðAx0; QÞ.
Fig. 1. (a) Dynamic texture. (b) Dynamic texture mixture. The hidden
variable z selects the parameters of the remaining nodes.
involves a video that is homogeneous at each time instant but
has varying statistics over time. For example, the problem of
clustering a set of video sequences taken from a stationary
highway traffic camera. Although each video will depict
traffic moving at homogeneous speed, the exact appearance
of each sequence is controlled by the amount of traffic
congestion. Different levels of traffic congestion can be
represented by K dynamic textures. The second involves
inhomogeneous video, that is, video composed of multiple
processthatcanbeindividuallymodeled asdynamictextures
of different parameters. For example, in a video scene
containing fire and smoke, a random video patch taken from
the video will contain either fire or smoke, and a collection of
video patches can be represented as a sample from a mixture
of two dynamic textures.
 ¼ f1; . . . ; Kg
j¼1 j ¼ 1 and dynamic texture components of
parameters f1; . . . ; Kg, a video sequence is drawn by
sampling a component index z from the multinomial
distribution parameterized by .
Sampling an observation y
1 from the dynamic
texture component of parameters z.
The probability of a sequence y
1 under this model is
where pðy
1jz ¼ jÞ is the class conditional probability of the
jth dynamic texture, that is, the dynamic texture component
parameterized by j ¼ fAj; Qj; Cj; Rj; j; Sjg. The system of
equations that define the mixture of dynamic textures is
xtþ1 ¼ Azxt þ vt
yt ¼ Czxt þ wt;
where the random variable z  multinomialð1;    ; KÞ
signals the mixture component from which the observations
are drawn, the initial condition is given by x1  N ðz; SzÞ
and the noise processes by vt  N ð0; QzÞ and wt  N ð0; RzÞ.
The conditional distributions of the states and observations,
given the component index z are
pðx1jzÞ ¼ Gðx1; z; SzÞ;
pðxtjxt1; zÞ ¼ Gðxt; Azxt1; QzÞ;
pðytjxt; zÞ ¼ Gðyt; Czxt; RzÞ;
and the overall joint distribution is
1; zÞ ¼ pðzÞpðx1jzÞ
pðxtjxt1; zÞ
pðytjxt; zÞ:
The graphical model for the dynamic texture mixture is
presented in Fig. 1b. Note that, although the addition of the
random variable z introduces loops in the graph, exact
inference is still tractable because z is connected to all other
nodes. Hence, the graph is already moralized and triangulated , and the junction tree of Fig. 1b is equivalent to that
of thebasic dynamictexture, with thevariable z added toeach
clique. This makes the complexity of exact inference for a
mixture of K dynamic textures K times that of the underlying
dynamic texture.
PARAMETER ESTIMATION USING EM
Given a set of independent and identically distributed
(i.i.d.) video sequences fyðiÞgN
i¼1, we would like to learn the
parameters  of a mixture of dynamic textures that best fits
the data in the maximum-likelihood sense , that is,
 ¼ argmax
log pðyðiÞ; Þ:
When the probability distribution depends on hidden
variables (that is, the output of the system is observed, but
its state is unknown), the maximum-likelihood solution can
be found with the EM algorithm . For the dynamic texture
mixture, the observed information is a set of video sequences
i¼1,andthemissingdataconsistsof1)theassignmentzðiÞ
of each sequence to a mixture component and 2) the hidden
state sequence xðiÞ that produces yðiÞ. The EM algorithm is
an iterative procedure that alternates between estimating
the missing information with the current parameters and
computing new parameters given the estimate of the missing
information. In particular, each iteration consists of
E-Step : Qð; ^Þ ¼ EX;ZjY ;^ðlog pðX; Y ; Z; ÞÞ;
M-Step : ^ ¼ argmax
where pðX; Y ; Z; Þ is the complete-data likelihood of the
observations, hidden states, and hidden assignment variables, parameterized by . To maximize clarity, we only
present here the equations of the E and M steps for the
estimation of dynamic texture mixture parameters. Their
detailed derivation is given in Appendix A. The only
assumptions are that the observations are drawn independently and have zero-mean, but the algorithm could be
trivially extended to the case of nonzero means. All equations
follow the notation in Table 1.
EM Algorithm for Mixture of Dynamic Textures
Observations are denoted by fyðiÞgN
i¼1, the hidden state
variables by fxðiÞgN
i¼1, and the hidden assignment variables
by fzðiÞgN
i¼1. As is usual in the EM literature , we
introduce a vector zi 2 f0; 1gK, such that zi;j ¼ 1 if and
only if zðiÞ ¼ j. The complete-data log-likelihood is (up to
a constant) given by (15), where P ðiÞ
t;t ¼ xðiÞ
t;t1 ¼xðiÞ
t1ÞT. Applying the expectation of (13) to (15)
yields the Q function (16), where
‘ðX; Y ; ZÞ ¼
zi;j log j  1
zi;j log jSjj
zi;jtr S1
1;1  xðiÞ
j  jxðiÞ
zi;j log jRjj  1
j  CjxðiÞ
T þ CjP ðiÞ
zi;j log jQjj  1
j  AjP ðiÞ
T þ AjP ðiÞ
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
Qð; ^Þ ¼
j þ CjjCT
j þ AjjAT
Nj log j  
2 log jRjj    1
log jQjj  1
2 log jSjj
t¼1 ^P ðiÞ
i ^zi;j^xðiÞ
t¼2 ^P ðiÞ
i ^zi;j ^P ðiÞ
t¼2 ^P ðiÞ
t1;t1jj;
t¼2 ^P ðiÞ
are the aggregates of the expectations
tjj ¼ ExðiÞjyðiÞ;zðiÞ¼j xðiÞ
t;tjj ¼ ExðiÞjyðiÞ;zðiÞ¼j P ðiÞ
t;t1jj ¼ ExðiÞjyðiÞ;zðiÞ¼j P ðiÞ
and the posterior assignment probability is
^zi;j ¼ pðzðiÞ ¼ jjyðiÞÞ ¼
jpðyðiÞjzðiÞ ¼ jÞ
k¼1 kpðyðiÞjzðiÞ ¼ kÞ
Hence, the E-step consists of computing the conditional
expectations (18)-(21) and can be implemented efficiently
with the Kalman smoothing filter (see Appendix B),
which estimates the mean and covariance of the state xðiÞ
conditioned on the observation yðiÞ and zðiÞ ¼ j
tjj ¼ ExðiÞjyðiÞ;zðiÞ¼j
t;tjj ¼ covxðiÞjyðiÞ;zðiÞ¼j
t;t1jj ¼ covxðiÞjyðiÞ;zðiÞ¼j
The second-order moments of (19) and (20) are then
calculated as ^P ðiÞ
t;tjj ¼ ^V ðiÞ
t;tjj þ ^xðiÞ
tjjÞT and ^P ðiÞ
t;t1jj ¼ ^V ðiÞ
t1jjÞT. Finally, the data likelihood pðyðiÞjzðiÞ ¼ jÞ is
computed using the “innovations” form of the log-likelihood
(again, see or Appendix B).
In the M-step, the dynamic texture parameters are
updated according to (14), resulting in the following update
step for each mixture component j:
j ¼ jðjÞ1;
j ¼ jðjÞ1;
A summary of EM for the mixture of dynamic textures is
presented in Algorithm 1. The E-step relies on the Kalman
smoothing filter to compute 1) the expectations of the hidden
state variables xt, given the observed sequence yðiÞ and the
component assignment zðiÞ, and 2) the likelihood of observation yðiÞ given the assignment zðiÞ. The M-step then computes
the maximum-likelihood parameter values for each dynamic
texturecomponentj byaveraging overallsequencesfyðiÞgN
weighted by the posterior probability of assigning zðiÞ ¼ j.
Algorithm 1 EM for a mixture of dynamic textures.
Input: N sequences fyðiÞgN
i¼1, number of components K.
Initialize fj; jg for j ¼ 1 to K.
{Expectation Step}
for i ¼ f1; . . . ; Ng and j ¼ f1; . . . ; Kg do
Compute the expectations (18)-(21) with the Kalman
smoothing filter (Appendix B) on yi and j.
{Maximization Step}
for j ¼ 1 to K do
Compute aggregate expectations (17).
Compute new parameters fj; jg with (25).
until convergence
Output: fj; jgK
Initialization Strategies
It is known that the accuracy of parameter estimates
produced by EM is dependent on how the algorithm is
initialized. In the remainder of this section, we present three
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
Notation for EM for Mixture of Dynamic Textures
initialization strategies that we have empirically found to be
effective for learning mixtures of dynamic textures.
3.2.1 Initial Seeding
If an initial clustering of the data is available (for example,
by specification of initial contours for segmented regions),
then each mixture component is learned by applying the
method in to each of the initial clusters.
3.2.2 Random Trials
Several trials of EM are run with different random
initializations, and the parameters that best fit the data, in
the maximum-likelihood sense, are selected. For each EM
trial, each mixture component is initialized by application of
the method of to a randomly selected example from the
3.2.3 Component Splitting
The EM algorithm is run with an increasing number of
mixture components, a common strategy in the speechrecognition community :
components.
Duplicate a mixture component and perturb the new
component’s parameters.
Run the EM algorithm, using the new mixture model
as initialization.
Repeat Steps 2 and 3 until the desired number of
components is reached.
For the mixture of dynamic textures, we use the following
selection and perturbation rules: 1) the mixture component
with the largest misfit in the state-space (that is, the mixture
component with the largest eigenvalue of Q) is selected for
duplication and 2) the principal component whose coefficients have the largest initial variance (that is, the column of
C associated with the largest variance in S) is scaled by 1.01.
CONNECTIONS TO THE LITERATURE
Although novel as a tool for modeling video, with application
to problems such as clustering and segmentation, the mixture
of dynamic textures and the proposed EM algorithm are
related to various previous works in adaptive filtering,
statistics and machine learning, time-series clustering, and
video segmentation.
Adaptive Filtering and Control
In the control-theory literature, Magill proposes an
adaptive filter based on banks of Kalman filters running in
parallel, where each Kalman filter models a mode of a
physical process. The output of the adaptive filter is the
average of the outputs of the individual Kalman filters,
weighted by the posterior probability that the observation
was drawn from the filter. This is an inference procedure for
the hidden state of a mixture of dynamic textures conditioned
on the observation. The key difference with respect to this
work is that, even though Magill focuses on inference on
the mixture model with known parameters, this work focuses
on learning the parameters of the mixture model. The work of
Magill is the forefather of other multiple-model-based
methods in adaptive estimation and control , , ,
but none address the learning problem.
Models Proposed in Statistics and Machine
For a single component ðK ¼ 1Þ and a single observation
ðN ¼ 1Þ, the EM algorithm for the mixture of dynamic
textures reduces to the classical EM algorithm for learning
an LDS , , . The LDS is a generalization of the factor
analysis model , a statistical model that explains an
observed vector as a combination of measurements that are
driven by independent factors. Similarly, the mixture of
dynamic textures is a generalization of the mixture of factor
analyzers,2 and the EM algorithm for a dynamic texture
mixture reduces to the EM algorithm for learning a mixture of
factor analyzers .
The dynamic texture mixture is also related to “switching”
linear dynamical models, where the parameters of an LDS are
selected via a separate Markovian switching variable as time
progresses. Variations of these models include , ,
where only the observation matrix C switches, , , ,
where the state parameters switch (A and Q), and , ,
where the observation and state parameters switch (C, R, A,
and Q). These models have one state variable that evolves
according to the active system parameters at each time step.
Thismakestheswitchingmodelamixtureofanexponentially
increasing number of LDSs with time-varying parameters.
In contrast to switching modelswith a single statevariable,
theswitching state-spacemodel proposedin switches the
observed variable between the output of different LDSs at
each time step. Each LDS has its own observation matrix and
state variable, which evolves according to its own system
parameters.Thedifference betweentheswitching state-space
model and the mixture of dynamic textures is that the
switching state-spacemodelcan switch betweenLDS outputs
at each time step, whereas the mixture of dynamic textures
selects an LDS only once at time t ¼ 1 and never switches from
it. Hence, the mixture of dynamic textures is similar to a
special case of the switching state-space model, where the
initial probabilities of the switching variable are the mixture
component probabilities j, and the Markovian transition
matrixoftheswitchingvariableisequaltotheidentitymatrix.
The ability of the switching state-space model to switch at
each time step results in a posterior distribution that is a
Gaussian mixture with a number of terms that increases
exponentially with time .
Although the mixture of dynamic textures is closely
related to both switching LDS models and the model in ,
the fact that it selects only one LDS per observed sequence
makes the posterior distribution a mixture of a constant
number of Gaussians. This key difference has consequences
of significant practical importance. First, when the number of
components increases exponentially (as is the case for models
that involve switching), exact inference becomes intractable,
and the EM-style of learning requires approximate methods
(for example, variational approximations) which are, by
definition, suboptimal. Second, because the exponential
increase in the number of degrees of freedom is not
accompanied by an exponential increase in the amount of
available data (which only grows linearly with time), the
difficulty of the learning problem increases with sequence
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
2. Restricting the LDS parameters S ¼ Q ¼ In,  ¼ 0, A ¼ 0, and R as a
diagonal matrix yields the factor analysis model. Similarly, for the mixture
of dynamic textures, setting Sj ¼ Qj ¼ In and Aj ¼ 0 for each factor analysis
component, and  ¼ 1 (since there are no temporal dynamics) yields the
mixture of factor analyzers.
length. None of these problems affect the dynamic texture
mixture, for which exact inference is tractable, allowing the
derivation of the exact EM algorithm presented above.
Time-Series Clustering
Although we apply the mixture of dynamic textures to video,
the model is general and can be used to cluster any type of
time series. When compared to the literature in this field ,
the mixture of dynamic textures can be categorized as a
model-based method for clustering multivariate continuousvalued time-series data. Two alternative methods are available for clustering this type of data, both based on the
K-Means algorithm with distance (or similarity) measures
suitable for time series: 1) Kakizawa et al. measures the
distance between two time-series with the KL divergence or
the Chernoff measure, which are estimated nonparametrically in the spectral domain, and 2) Singhal and Seborg 
measuressimilaritybycomparingthePCAsubspacesandthe
means of the time-series but disregards the dynamics of the
time series. The well-known connection between EM and
K-Means makes these algorithms somewhat related to the
dynamic texture mixture, but they lack a precise probabilistic
interpretation. Finally, the dynamic texture mixture is related
to the ARMA mixture proposed in . The main differences
are that the ARMA mixture 1) only models univariate data
and 2) does not utilize a hidden state model. On the other
hand, the ARMA mixture supports higher-order Markov
models, whereas the dynamic texture mixture is based on a
first-order Markovian assumption.
Video Segmentation
The idea of applying dynamic texture representations to the
segmentation of the video has previously appeared in the
video literature. In fact, some of the inspiration for our work
was the promise shown for temporal texture segmentation
(for example, smoke and fire) by the dynamic texture model
in . For example, Doretto et al. segments a video by
clustering patches of dynamic texture using the level-sets
framework and the Martin distance. More recently, Ghoreyshi and Vidal clusters pixel intensities (or local texture
features) using autoregressive (AR) processes and level sets,
and Vidal and Ravichandran segments a video by
clustering pixels with similar trajectories in time using
generalized PCA (GPCA). Although these methods have
shown promise, they do not exploit the probabilistic nature of
the dynamic texture representation for the segmentation
itself. On the other hand, the segmentation algorithms
proposed in the following section are statistical procedures
that leverage on the mixture of dynamic texture to perform
optimal inference. This results in greater robustness to
variations due to the stochasticity of the video appearance
and dynamics, leading to superior segmentation results, as
will be demonstrated in Sections 5 and 6.
Finally, it is worth mentioning that we have previously
proposed a graphical model that represents a video as a
collection of layers, where each layer is modeled as a dynamic
texture , and a Markov random field (MRF) prior is
included to guarantee spatial coherence of the segmentation.
When compared to the dynamic texture mixture, this model
has significantly larger computational requirements. We
have not, so far, been able to apply it in the context of
experiments of the scale discussed in the following sections.
APPLICATIONS
Like any other probabilistic model, the dynamic texture has a
large number of potential application domains, many of
which extend well beyond the field of computer vision (for
example, modeling of high-dimensional time series for
financial applications, weather forecasting, and so forth). In
this work, we concentrate on vision applications, where
mixture models arefrequently used tosolve problems suchas
clustering , , background modeling , image
segmentation and layering , , , , , , or
retrieval , . The dynamic texture mixture extends this
class of methods to problems involving video of particle
ensembles subject to stochastic motion. We consider, in
particular, the problems of clustering and segmentation.
Video Clustering
Video clustering can be a useful tool to uncover high-level
patterns of structure in a video stream, for example, recurring
events, events of high and low probability, outlying events,
and so forth. These operations are of great practical interest
for some classes of particle-ensemble video such as those that
involve understanding video acquired in crowded environments. In this context, video clustering has application to
problems such as surveillance, novelty detection, event
summarization, or remote monitoring of various types of
environments. It can also be applied to the entries of a video
database in order to automatically create a taxonomy of video
classes that can then be used for database organization or
video retrieval. Under the mixture of dynamic textures
representation, a set of video sequences is clustered by first
learning the mixture that best fits the entire collection of
sequences and then assigning each sequence to the mixture
component with the largest posterior probability of having
generated it, that is, by labeling sequence yðiÞ with
‘i ¼ argmax
log pðyðiÞjzðiÞ ¼ jÞ þ log j:
Motion Segmentation
Video segmentation addresses the problem of decomposing a
video sequence into a collection of homogeneous regions.
Although this has long been known to be solvable with
mixture models and the EM algorithm , , , , ,
the success of the segmentation operation depends on the
ability of the mixture model to capture the dimensions along
which the video is statistically homogeneous. For spatiotemporal textures (for example, video of smoke and fire),
traditional mixture-based motion models are not capable of
capturing these dimensions due to their inability to account
for the stochastic nature of the underlying motion. The
mixture of dynamic textures extends the application of
mixture-based segmentation algorithms to video composed
of spatio-temporal textures. As in most previous mixturebased approaches to video segmentation, the process consists
of two steps. The mixture model is first learned, and the video
is then segmented by assigning video locations to mixture
components.
In the learning stage, the video is first represented as a bag
of patches. For spatio-temporal texture segmentation, a patch
of dimensions p  p  q is extracted from each location in the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
video sequence (or along a regularly spaced grid),3 where p
and q should be large enough to capture the distinguishing
characteristics of the various components of the local motion
field. Note that this is unlike methods that model the changes
of appearance of single pixels (for example, and the AR
model in ) and, therefore, have no ability to capture the
spatial coherence of the local motion field. If the segmentation
boundaries are not expected to change over time, q can be set
to the length of the video sequence. The set of spatio-temporal
patches is then clustered with recourse to the EM algorithm of
Section 3. The second step, segmentation, scans the video
locations sequentially. At each location, a patch is extracted
and assigned to one of the mixture components, according to
(26). The location is then declared to belong to the segmentation region associated with that cluster.
It is interesting to note that the mixture of dynamic
textures can be used to efficiently segment very long video
sequences by first learning the mixture model on a short
training sequence (for example, a clip from the long
sequence) and then segmenting the long sequence with
the learned mixture. The segmentation step only requires
computing the patch log-likelihoods under each mixture
component, that is, (73). Since the conditional covariances
t and the Kalman filter gains Kt do not depend
on the observation yt, they can be precomputed. The
computational steps required for the data-likelihood of a
single patch therefore reduce to computing, 8t ¼ f1; . . . ; g
þ Kt yt  C^xt1
2 log jMj  m
2 ðyt  C^xt1
ÞTM1ðyt  C^xt1
where M ¼ C ^V t1
CT þ R. Hence, computing the datalikelihood under one mixture component of a single patch
requires Oð5Þ matrix-vector multiplications. For a mixture
of K dynamic textures and N patches, the computation is
Oð5KNÞ matrix-vector multiplications.
EXPERIMENTAL EVALUATION
The performance of the mixture of dynamic textures was
evaluated with respect to various applications: 1) clustering
of time-series data, 2) clustering of highway traffic video,
and 3) motion segmentation of both synthetic and real video
sequences. In all cases, performance was compared to a
representative of the state-of-the-art for these application
domains. The initialization strategies from Section 3.2 were
used. The observation noise was assumed to be independent and identically distributed (that is, R ¼ 2Im), the
initial state covariance S was assumed to be diagonal, and
the covariance matrices Q, S, and R were regularized by
enforcing a lower bound on their eigenvalues. Videos of the
results from all experiments are available from a companion
Web site, accessible in .
Time-Series Clustering
We start by presenting results of a comparison of
the dynamic texture mixture with several multivariate
time-series clustering algorithms. To enable an evaluation
based on clustering ground truth, this comparison was
performed on a synthetic time-series data set, generated as
follows. First, the parameters of K LDSs, with state-space
dimension n ¼ 2 and observation-space dimension m ¼ 10,
were randomly generated according to
  Unð5; 5Þ;
S  WðIn; nÞ;
C  N m;nð0; 1Þ;
Q  WðIn; nÞ;
0  U1ð0:1; 1Þ;
A0  N n;nð0; 1Þ;
2  Wð1; 2Þ;
where N m;nð; 2Þ is a distribution on IRmn matrices with
each entry distributed as N ð; 2Þ, Wð; dÞ is a Wishart
distribution with covariance  and d degrees of freedom,
Udða; bÞ is a distribution on IRd vectors with each coordinate
distributed uniformly between a and b, and
maxðA0Þ is the
magnitude of the largest eigenvalue of A0. Note that A is a
random scaling of A0 such that the system is stable (that is, the
poles of A are within the unit circle). A time-series data set
was generated by sampling 20 time-series of length 50 from
each of the K LDSs. Finally, each time-series sample was
normalized to have zero temporal mean.
The data was clustered using the mixture of dynamic
textures (DytexMix), and three multivariate-series clustering algorithms from the time-series literature. The latter are
based on variations of K-means for various similarity
measures: PCA subspace similarity (Singhal) , the KL
divergence (KakKL) , and the Chernoff measure
(KakCh) . As a baseline, the data was also clustered
with K-means using the euclidean distance (K-Means)
and the cosine similarity (K-means-c) on feature vectors
formed by concatenating each time series. The correctness
of a clustering is measured quantitatively using the Rand
index between the clustering and the ground truth.
Intuitively, the Rand index corresponds to the probability of
pairwise agreement between the clustering and the ground
truth, that is, the probability that the assignment of any two
items will be correct with respect to each other (in the same
cluster or in different clusters). For each algorithm, the
average Rand index was computed for each value of K ¼
f2; 3; . . . ; 8g by averaging over 100 random trials of the
clustering experiment. We refer to this synthetic experiment
setup as “SyntheticA.” The clustering algorithms were also
tested on two variations of the experiment based on timeseries that were more difficult to cluster. In the first
(SyntheticB), the K random LDSs were forced to share the
same observation matrix ðCÞ, therefore forcing all the time
series to be defined in similar subspaces. In the second
(SyntheticC), the LDSs had large observation noise, that is,
2  Wð16; 2Þ. Note that these variations are typically
expected in video clustering problems. For example, in
applications where the appearance component does not
change significantly between clusters (for example, highway video with varying levels of traffic), all the video will
span similar image subspaces.
Fig. 2 presents the results obtained with the six clustering
algorithms on the three experiments, and Table 2 shows the
overall Rand index, computed by averaging over K. In
SyntheticA (Fig. 2a), Singhal, KakCh, and DytexMix achieved
comparable performance (overall Rand of 0.995, 0.991, and
0.991) with Singhal performing slightly better. On the other
hand, it is clear that the two baseline algorithms are not
suitable for clustering time-series data, albeit there is
significant improvement when using K-Means-c (0.831)
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
3. Although overlapping patches violate the independence assumption,
they work well in practice and are commonly adopted in computer vision.
rather than K-Means (0.585). In SyntheticB (Fig. 2b), the
results were different: the DytexMix performed best (0.995),
closely followed by KakCh and KakKL (0.985 and 0.977). On
the other hand, Singhal did not perform well (0.865) because
all the time series have similar PCA subspaces. Finally, in
SyntheticC (Fig. 2c), the DytexMix repeated the best
performance (0.993), followed by KakKL and KakCh (0.960
and 0.958), with Singhal performing the worst again (0.858).
In this case, the difference between the performance of
DytexMix and those of KakKL and KakCh was significant.
This can be explained by the robustness of the former to
observation noise, a property not shared by the latter due to
the fragility of nonparametric estimation of spectral matrices.
In summary, these results show that the mixture of
dynamic textures performs similarly to state-of-the-art
methods such as and , when clusters are well
separated. It is, however, more robust against occurrences
that increase the amount of cluster overlap, which proved
difficult for the other methods. Such occurrences include
1) time series defined in similar subspaces and 2) time series
with significant observation noise and are common in videoclustering applications.
Video Clustering
To evaluate its performance in problems of practical
significance, the dynamic texture mixture was used to cluster
video of vehicle highway traffic. Clustering was performed
on 253 video sequences collected by the Washington
Department of Transportation (WSDOT) on interstate I-5 in
Seattle, Washington . Each video clip is 5 seconds long,
and the collection spanned about 20 hours over two days. The
videosequenceswereconvertedtograyscaleandnormalized
to have size 48  48 pixels, zero mean, and unit variance.
The mixture of dynamic textures was used to organize this
data set into five clusters. Fig. 3a shows six typical sequences
for each of the five clusters. These examples, and further
analysis of the sequences in each cluster, reveal that the
clusters are in agreement with classes frequently used in the
perceptual categorization of traffic: light traffic (spanning
two clusters), medium traffic, slow traffic, and stopped traffic
(“traffic jam”). Figs. 3b, 3c, and 3d show a comparison
between the temporal evolution of the cluster index and the
averagetrafficspeed.ThelatterwasmeasuredbytheWSDOT
with an electromagnetic sensor (commonly known as a loop
sensor) embedded in the highway asphalt, near the camera.
The speed measurements are shown in Fig. 3b, and the
temporal evolution of the cluster index is shown for K ¼ 2
(Fig. 3c) and K ¼ 5 (Fig. 3d). Unfortunately, a precise
comparison between the speed measurements and the video
is not possible because the data originate from two separate
systems, and the video data is not time stamped with fine
enough precision. Nonetheless, it is still possible to examine
the correspondence between the speed data and the video
clustering. For K ¼ 2, the algorithm forms two clusters that
correspond to fast-moving and slow-moving traffic. Similarly, for K ¼ 5, the algorithm creates two clusters for fastmoving traffic and three clusters for slow-moving traffic
(which correspond to medium, slow, and stopped traffic).
Motion Segmentation
Several experiments were conducted to evaluate the usefulness of dynamic texture mixtures for video segmentation. In
all cases, two initialization methods were considered: The
manual specification of a rough initial segmentation contour
(referred to as DytexMixIC) and the component splitting
strategy of Section 3.2 (DytexMixCS). The segmentation
results are compared with those produced by several
segmentation procedures previously proposed in the literature: the level-sets method in using Ising models (Ising),
generalized PCA (GPCA) , and two algorithms representative of the state-of-the-art for traditional optical-flow-based
motion segmentation. The first method (NormCuts) is based
on normalized cuts and the “motion profile” representation proposed in and .4 The second (OpFlow)
represents each pixel as a feature-vector containing the
average optical flow over a 5  5 window and clusters the
feature-vectors using the mean-shift algorithm .
Preliminary evaluation revealed various limitations of
the different techniques. For example, the optical flow
methods cannot deal with the stochasticity of microscopic
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
Fig. 2. Time-series clustering results on three synthetic problems: (a) SyntheticA, (b) SyntheticB, and (c) SyntheticC. Plots show the Rand index
versus the number of clusters ðKÞ for six time-series clustering algorithms.
Overall Rand Index on the Three Synthetic Experiments
for Six Clustering Algorithms
4. For this representation, we used a patch of size 15  15 and a motion
profile neighborhood of 5  5.
textures (for example, water and smoke), performing much
better for textures composed of nonmicroscopic primitives
(for example, video of crowded scenes composed of objects,
such as cars or pedestrians, moving at a distance). On the
other hand, level sets and GPCA perform best for
microscopic textures. Due to these limitation, and for the
sake of brevity, we limit the comparisons that follow the
methods that performed best for each type of video under
consideration. In some cases, the experiments were also
restricted by implementation constraints. For example, the
available implementations of the level-sets method can only
be applied to video composed of two textures.
In all experiments, the video are gray scale, and the video
patches are either 5  5 or 7  7 pixels, depending on the
image size (see Table 3 for more details). The patches are
normalized to have zero temporal mean and unit variance.
Unless otherwise specified, the dimension of the state-space
is n ¼ 10. Finally, all segmentations are postprocessed with
a 5  5 “majority” smoothing filter.
6.3.1 Segmentation of Synthetic Video
We start with the four synthetic sequences studied in :
steam over an ocean background (ocean-steam),
ocean with two regions rotated by 90 degrees, that is,
regions of identical dynamics but different appearance (ocean-appearance),
ocean with two regions of identical appearance but
different dynamics (ocean-dynamics), and
superimposed
background
(ocean-fire).
Thesegmentations ofthe firstthreevideos areshownin Fig. 4.
Qualitatively, the segmentations produced by DytexMixIC
and Ising ðn ¼ 2Þ are similar, with Ising performing slightly
better with respect to the localization of the segmentation
borders. Although both of these methods improve on the
segmentations in , GPCA can only segment ocean-steam,
failing on the other two sequences. We next consider the
segmentation of a sequence containing ocean and fire
(Fig. 5a). This sequence is more challenging because the
boundary of the fire region is not stationary (that is, the region
changes over time as the flame evolves). Once again,
DytexMixIC ðn ¼ 2Þ and Ising ðn ¼ 2Þ produce comparable
segmentations (Figs. 5b and 5c) and are capable of tracking
the flame over time. We were not able to produce any
meaningful segmentation with GPCA on this sequence.
In summary, both DytexMixIC and Ising perform qualitatively well on the sequences in but GPCA does not.
Section 6.3.2 will compare these algorithms quantitatively
using a much larger database of synthetic video.
6.3.2 Segmentation of Synthetic Video Database
The segmentation algorithms were evaluated quantitatively
on a database of 300 synthetic sequences. These consist of
three groups of 100 videos, with each group generated from
a common ground-truth template, for K ¼ f2; 3; 4g. The
segments were randomly selected from a set of 12 textures,
which included grass, sea, boiling water, moving escalator,
fire, steam, water, and plants. Examples from the database
can be seen in Figs. 7a and 8a, along with the initial
contours provided to the segmentation algorithms.
All sequences were segmented with DytexMixIC, Dytex-
MixCS, and GPCA . Due to implementation limitations,
the algorithms in , Ising, AR, and AR0 (AR without mean)
were applied only for K ¼ 2. All methods were run with
different orders of the motion models ðnÞ, whereas the
remainingparametersforeachalgorithmwerefixedthroughout the experiment. Performance was evaluated with the
Rand index between segmentation and ground truth. In
addition,twobaselinesegmentationswereincluded 1)“Baseline Random,” which randomly assigns pixels and 2) “Baseline Init,” which is the initial segmentation (that is, initial
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
Fig. 3. Clustering of traffic video: (a) six typical sequences from the five clusters, (b) speed measurements from the highway loop sensor over time,
and the clustering index over time for (c) two clusters, and (d) five clusters.
Videos in Motion Segmentation Experiment
K is the number of clusters.
contour) provided to the algorithms. The database and
segmentation results are available in .
Fig. 6a shows the average Rand index resulting from the
segmentation algorithms for different orders n, and Table 4
presents the best results for each algorithm. For all K,
DytexMixIC achieved the best overall performance, that is,
the largest average Rand index. For K ¼ 2, Ising also
performs well (0.879) but is inferior to DytexMixIC (0.916).
The remaining algorithms performed poorly, with GPCA
performing close to random pixel assignment.
Although the average Rand index quantifies the overall
performance on the database, it does not provide insight
on the characteristics of the segmentation failures, that
is, whether there are many small errors or a few gross
ones. To overcome this limitation, we have also examined
the segmentation precision of all algorithms. Given a
, segmentation precision is defined as the
percentage of segmentations deemed to be correct with
respect to the threshold, that is, the percentage with Rand
index larger than
. Fig. 6b plots the precision of the
segmentation algorithms for different threshold levels.
One interesting observation can be made when K ¼ 2. In
this case, for a Rand threshold of 0.95 (corresponding to
tolerance of about 1 pixel error around the border), the
precisions of DytexMixIC and Ising are, respectively,
73 percent and 51 percent. On the other hand, for very
high thresholds (for example, 0.98), Ising has a larger
precision (31 percent versus 14 percent of DytexMixIC).
This suggests that Ising is very good at finding the exact
boundary when nearly perfect segmentations are possible
but is more prone to dramatic segmentation failures. On
the other hand, DytexMixIC is more robust but not as
precise near borders.
An example of these properties is shown in Fig. 7. The
first column presents a sequence for which both methods
work well, whereas the second column shows an example
where Ising is more accurate near the border and where
DytexMixIC still finds a good segmentation. The third and
fourth columns present examples where DytexMixIC
succeeds and Ising fails (for example, in the fourth column,
Ising confuses the bright escalator edges with the wave
ripples). Finally, both methods fail in the fifth column, due
to the similarity of the background and foreground water.
With respect to initialization, it is clear in Fig. 6 and Table 4
that, even in the absence of an initial contour, the mixture of
dynamic textures (DytexMixCS) outperforms all other
methods considered, including those that require an initial
contour (for example, Ising) and those that do not (for
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
Fig. 4. Segmentation of synthetic video in : (a) ocean-steam, (b) ocean-appearance, and (c) ocean-dynamics. The first column shows a video
frame and the initial contour, and the remaining columns show the segmentations from DytexMixIC, Ising , GPCA , and Martin distance (from
that in ).
Fig. 5. Segmentation of ocean-fire : (a) video frames and the segmentation using (b) DytexMixIC and (c) Ising .
example, GPCA). Again, this indicates that video segmentation with the dynamic texture mixture is quite robust.
Comparing DytexMixIC and DytexMixCS, the two methods
achieve equivalent performance for K ¼ 2 with DytexMixIC
performing slightly better for K ¼ 3 and K ¼ 4. With multiple textures, manual specification of the initial contour
reduces possible ambiguities due to similarities between
pairs of textures. An example is given in the third column in
Fig.8,wherethetwoforegroundtexturesaresimilar,whereas
the background texture could perceptually be split into two
regions (particles moving upward and to the right, and those
moving upward and to the left). In the absence of initial
contour, DytexMixCS prefers this alternative interpretation.
Finally, the first two columns in Fig. 8 show examples where
both methods perform well, whereas in the fourth and fifth
columns both fail (for example, when two water textures are
almost identical).
In summary, the quantitative results on a large database
of synthetic video textures indicate that the mixture of
dynamic textures (both with or without initial contour
specification) is superior to all other video texture segmentation algorithms considered. Although Ising also achieves
acceptable performance, it has two limitations: 1) it requires
an initial contour and 2) it can only segment video composed
of two regions. Finally, AR and GPCA do not perform well
on this database.
6.3.3 Segmentation of Real Video
We finish with segmentation results on five real video
sequences, depicting a water fountain, highway traffic, and
pedestrian crowds. Although a precise evaluation is not
possible, because there is no segmentation ground truth, the
results are sufficiently different to support a qualitative
ranking of the different approaches. Fig. 9 illustrates the
performance on the first sequence, which depicts a water
fountain with three regions of different dynamics: water
flowing down a wall, falling water, and turbulent water in a
pool. Although DytexMixCS separates the three regions,
GPCA does not produce a sensible segmentation.5 This result
confirms the observations obtained with the synthetic
database of the previous section.
We next consider macroscopic dynamic textures, in which
case segmentation performance is compared against that of
traditionalmotion-basedsolutions,thatis,thecombinationof
normalized-cuts with motion profiles (NormCuts) or optical
flow with mean-shift clustering (OpFlow). Fig. 10a shows a
scene of highway traffic, for which DytexMixCS is the only
method that correctly segments the video into regions of
traffic that move away from the camera (the two large regions
on the right) and traffic that move toward the camera (the
regions on the left). The only error is the split of the incoming
traffic into two regions and can be explained by the strong
perspective effects inherent to car motion toward the camera.
This could be avoided by applying an inverse perspective
mapping to the scene, but we have not considered any such
geometric transformations. Fig. 10b shows another example
of segmentation of vehicle traffic on a bridge. Traffic lanes of
opposite direction are correctly segmented near the camera
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
The Best Average Rand Index for Each Segmentation
Algorithm on the Synthetic Database
The order of the model ðnÞ is shown in parenthesis.
Fig. 6. Results on the synthetic database: (a) average Rand index versus the order of the motion model ðnÞ and (b) segmentation precision for the
best n for each algorithm. Each column presents the results for 2, 3, or 4 segments in the database.
5. The other methods could not be applied to this sequence because it
contains three regions.
but merged further down the bridge. The algorithm also
segments the water in the bottom right of the image but
assigns it to the same cluster as the distant traffic. Although
not perfect, these segmentations are significantly better than
those produced by the traditional representations. For both
NormCuts and OpFlow, segmented regions tend to extend
over multiple lanes, incoming and outgoing traffic are
merged, and the same lane is frequently broken into a
number of subregions.
The final two videos are of pedestrian scenes. The first
scene, shown in Fig. 11a, contains sparse pedestrian traffic,
that is, with large gaps between pedestrians. DytexMixCS
(Fig. 11b) segments people moving up the walkway from
people moving down the walkway. The second scene
(Fig. 11c) contains a large crowd moving up the walkway
with only a few people moving in the opposite direction.
Again, DytexMixCS (Fig. 11d) segmented the groups
moving in different directions, even in instances where
only one person is surrounded by the crowd and moving in
the opposite direction. Figs. 11e and 11f show the
segmentations produced by the traditional methods. The
segmentation produced by NormCuts contains gross errors,
for example, frame 2 at the far end of the walkway.
Although the segmentation achieved with OpFlow is more
comparable to that of DytexMixCS, it tends to oversegment
the people moving down the walkway.
Finally, we illustrate the point that the mixture of dynamic
textures can be used to efficiently segment very long video
sequences. Using the procedure discussed in Section 5.2, a
continuous hour of pedestrian video was segmented with the
mixture model learned from that in Fig. 11c and is available
for visualization in the companion Web site . It should be
noted that this segmentation required no reinitialization at
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
Fig. 7. Examples of segmentation of synthetic database ðK ¼ 2Þ: (a) a video frame and the initial contour, segmentation with (b) DytexMixIC, and
(c) Ising . The Rand index ðrÞ of each segmentation is shown above the image.
Fig. 8. Examples of segmentation of synthetic database (K ¼ 3 and K ¼ 4): (a) a video frame and the initial contour, (b) segmentation using
DytexMixIC, and (c) DytexMixCS. The Rand index ðrÞ of each segmentation is shown above the image.
Fig. 9. Segmentation of a fountain scene using DytexMixCS and GPCA.
any point or any other type of manual supervision. We
consider this a significant result, given that this sequence
contains a fair variability of traffic density, various outlying
events (for example, bicycles, skateboarders, or even small
vehicles that pass through, pedestrians that change course or
stop to chat, and so forth), and variable environmental
conditions (such as varying clouds shadows). None of these
factors appear to influence the performance of the Dytex-
MixCS algorithm. To the best of our knowledge, this is the
firsttimethatacoherentvideosegmentation,overatimespan
of this scale, has been reported for a crowded scene.
CONCLUSIONS
In this work, we have studied the mixture of dynamic
textures, a principled probabilistic extension of the dynamic
texture model. Although a dynamic texture models a single
video sequence as a sample from a linear dynamic system, a
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
Fig. 10. Segmentation of traffic scenes: (a) highway traffic and (b) vehicle traffic on bridge. The left column shows a frame from the original videos,
whereas the remaining columns show the segmentations.
Fig. 11. Segmentation of two pedestrian scenes: (a) pedestrian scene with sparse traffic, (b) the segmentation by DytexMixCS, (c) pedestrian scene
with heavy traffic and the segmentations by (d) DytexMixCS, (e) NormCuts, and (f) OpFlow.
mixture of dynamic textures models a collection of
sequences as samples from a set of linear dynamic systems.
We derived an exact EM algorithm for learning the
parameters of the model from a set of training video and
explored the connections between the model and other
linear system models, such as factor analysis, mixtures of
factor analyzers, and switching linear systems.
Through extensive video clustering and segmentation
experiments, we have also demonstrated the efficacy of the
mixture of dynamic textures for modeling video, both
holistically and locally (patch-based representations). In
particular, it has been shown that the mixture of dynamic
textures is a suitable model for simultaneously representing
the localized motion and appearance of a variety of visual
processes (for example, fire, water, steam, cars, and people),
and that the model provides a natural framework for
clustering such processes. In the application of motion
segmentation, the experimental results indicate that the
mixture of dynamic textures provides better segmentations
than other state-of-the-art methods, based on either dynamic
textures or on traditional representations. Some of the results,
namely, the segmentation of pedestrian scenes, suggest that
the dynamic texture mixture could be the basis for the design
of computer vision systems capable of tackling problems,
such as the monitoring and surveillance of crowded environments, which currently have great societal interest.
There are also some issues that we leave open for future
work. One example is how to incorporate, in the dynamic
texture mixture framework, some recent developments in
asymptotically efficient estimators based on noniterative
subspace methods . By using such estimators in the
M-step of the EM algorithm, it may be possible to reduce the
number of hidden variables required in the E-step and
consequently improve the convergence properties of EM. It is
currently unclear if the optimality of these estimators is
compromised when the initial state has arbitrary covariance
or when the LDS is learned from multiple sample paths, as is
the case for dynamic texture mixtures.
Another open question is that of the identifiability of an
LDS, when the initial state has arbitrary covariance. It is well
known, in the system identification literature, that the
parameters of an LDS can only be identified from a single
spatio-temporal sample if the covariance of the initial
condition satisfies a Lyapunov condition. In the absence of
identifiability, the solution may not be unique, may not exist,
learning may not converge, or the estimates may not be
consistent. It is important to note that none of these problems
are of great concern for the methods discussed in this paper.
For example, it is well known that EM is guaranteed to
converge to a local maximum of the likelihood function and
produces asymptotically consistent parameter estimates.
These properties are not contingent on the identifiability of
the LDS components. Although the local maxima of the
likelihood could be ridges (that is, not limited to points but
supported by manifolds), in which case, the optimal
component parameters would not be unique, there would
benoconsequenceforsegmentationorclusteringaslongasall
computations are based on likelihoods (or other probabilistic
distance measures such as the Kullback-Leibler divergence).
Nonuniqueness could, nevertheless, be problematic for
procedures that rely on direct comparison of the component
parameters (for example, based on their euclidean distances),
which we do not advise. In any case, it would be interesting to
investigate more thoroughly the identifiability question. The
fact that we have not experienced convergence speed
problems, in the extensive experiments discussed above,
indicates that likelihood ridges are unlikely. In future work,
we will attempt to understand this question more formally.
APPENDIX A
EM ALGORITHM FOR THE MIXTURE OF DYNAMIC
This appendix presents the derivation of the EM algorithm
for the mixture of dynamic textures. In particular, the
complete-data log-likelihood function, the E-step, and the
M-step are derived in the remainder of the appendix.
A.1 Log-Likelihood Functions
We start by obtaining the log-likelihood of the completedata (see Table 1 for notation). Using (11) and the indicator
variables zi;j, the complete-data log-likelihood is
‘ðX; Y ; ZÞ ¼
log p xðiÞ; yðiÞ; zðiÞ
p xðiÞ; yðiÞ; zðiÞ ¼ j
1 jzðiÞ ¼ j
t1; zðiÞ ¼j
t ; zðiÞ ¼j
zi;j log j þ log p xðiÞ
1 jzðiÞ ¼ j
log p xðiÞ
t1; zðiÞ ¼ j
log p yðiÞ
t ; zðiÞ ¼ j
Note that, from (8)-(10), the sums of the log-conditional
probability terms are of the form
log Gðbt; cj;t; MjÞ ¼
2 ðt1  t0 þ 1Þ log 2
kbt  cj;tk2
 t1  t0 þ 1
ai;j log jMjj:
Since the first term on the right-hand side of this equation
does not depend on the parameters of the dynamic texture
mixture, it does not affect the maximization performed in
the M-step and can, therefore, be dropped. Substituting the
appropriate parameters for bt, cj;t, and Mj, we have
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
‘ðX; Y ; ZÞ ¼
zi;j log j1
zi;j log jSjj
Sj    1
zi;j log jQjj
zi;j log jRjj:
Defining the random variables P ðiÞ
t;t ¼ xðiÞ
t ÞT and P ðiÞ
t1ÞT and expanding the Mahalanobis distance terms,
the log-likelihood becomes (15).
A.2 E-Step
The E-step of the EM algorithm is to take the expectation of
(15) conditioned on the observed data and the current
parameter estimates ^, as in (13). We note that each term of
‘ðX; Y ; ZÞ is of the form zi;jfðxðiÞ; yðiÞÞ, for some functions f
of xðiÞ and yðiÞ, and its expectation is
zi;jfðxðiÞ; yðiÞÞ
¼ EZjY EXjY ;Z zi;jfðxðiÞ; yðiÞÞ
¼ EzðiÞjyðiÞ ExðiÞjyðiÞ;zðiÞ zi;jfðxðiÞ; yðiÞÞ
¼ pðzi;j ¼ 1jyðiÞÞExðiÞjyðiÞ;zðiÞ¼j fðxðiÞ; yðiÞÞ
where (38) follows from the assumption that the observations
are independent. For the first term of (39), pðzi;j ¼ 1jyðiÞÞ is the
posterior probability of zðiÞ ¼ j given the observation yðiÞ
^zi;j ¼ p zi;j ¼ 1jyðiÞ
¼ p zðiÞ ¼ jjyðiÞ
jpðyðiÞjzðiÞ ¼ jÞ
k¼1 kpðyðiÞjzðiÞ ¼ kÞ
The functions fðxðiÞ; yðiÞÞ are at most quadratic in xðiÞ
t . Hence,
the second term of (39) only depends on the first and second
moments of the states conditioned on yðiÞ and component j
(18)-(20) and are computed, as described in Section 3.1.
Finally, the Q function (16) is obtained by first replacing the
random variables zi;j, ðzi;jxðiÞ
t Þ, ðzi;jP ðiÞ
t;t Þ, and ðzi;jP ðiÞ
t;t1Þ in the
complete-data log-likelihood (15) with the corresponding
expectations ^zi;j, ð^zi;j^xðiÞ
tjjÞ, ð^zi;j ^P ðiÞ
t;tjjÞ, and ð^zi;j ^P ðiÞ
t;t1jjÞ, and then
defining the aggregated expectations (17).
A.3 M-Step
In the M-step of the EM algorithm (14), the reparameterization of the model is obtained by maximizing the Q function by
taking the partial derivative with respect to each parameter
and setting it to zero. The maximization problem with respect
to each parameter appears in two common forms. The first is
a maximization with respect to a square matrix X
X ¼ argmax
2 log jXj:
Maximizing by taking the derivative and setting to zero
yields the following solution:
2 log jXj ¼ 0;
2 XTATXT  b
2 XT ¼ 0;
AT  bXT ¼ 0;
The second form is a maximization problem with respect to
a matrix X of the form
X ¼ argmax
2 tr DðBXT  XBT þ XCXTÞ
where D and C are symmetric and invertible matrices. The
maximum is given by
2 tr DðBXT  XBT þ XCXTÞ
2 ðDB  DTB þ DTXCT þ DXCÞ ¼ 0;
DB  DXC ¼ 0;
X ¼ BC1:
The optimal parameters are found by collecting the relevant
terms in (16) and maximizing.
A.3.1 Observation Matrix
j ¼ argmax
j þ CjjCT
This is of the form in (47), hence, the solution is given by
j ¼ jðjÞ1.
A.3.2 Observation Noise Covariance
j ¼ argmax
2 log jRjj
j þ CjjCT
This is of the form in (42), hence, the solution is
j þ CjjCT
where (54) follows from substituting for the optimal value C
A.3.3 State-Transition Matrix
j ¼ argmax
j þ AjjAT
This is of the form in (47); hence, A
j ¼ jðjÞ1.
CHAN AND VASCONCELOS: MODELING, CLUSTERING, AND SEGMENTING VIDEO WITH MIXTURES OF DYNAMIC TEXTURES
A.3.4 State Noise Covariance
j ¼ argmax
 ð  1Þ ^
j þ AjjAT
This is of the form in (42), hence, the solution can be
computed as
ð  1Þ ^Nj
j þ AjjAT
ð  1Þ ^Nj
where (56) follows from substituting for the optimal value A
A.3.5 Initial State Mean
j ¼ argmax
This is of the form in (47), hence, the solution is given by
A.3.6 Initial State Covariance
j ¼ argmax
2 log jSjj
This is of the form in (42), hence, the solution is given by
where (59) follows from substituting for the optimal value 
A.3.7 Class Probabilities
A Lagrangian multiplier is used to enforce that fjg sum to 1
 ¼ argmax
Nj log j þ
where  ¼ f1;    ; Kg. The optimal value is 
APPENDIX B
KALMAN SMOOTHING FILTER
The Kalman smoothing filter , estimates the mean
and covariance of the state xt of an LDS, conditioned on the
entire observed sequence fy1; . . . ; yg. It can also be used to
efficiently compute the log-likelihood of the observed
sequence. Define the expectations conditioned on the
observed sequence from time t ¼ 1 to t ¼ s
t ¼ Exjy1;...;ysðxtÞ;
t ¼ Exjy1;...;ys ðxt  ^xs
tÞðxt  ^xs
t;t1 ¼ Exjy1;...;ys ðxt  ^xs
tÞðxt1  ^xs
then the mean and covariances conditioned on the entire
observed sequence are ^x
t , and ^V 
t;t1. The estimates are
calculated using a set of recursive equations: for t ¼ 1; . . . ; 
¼ A ^V t1
t1 AT þ Q;
Kt ¼ ^V t1
CT C ^V t1
t ¼ ^V t1
 KtC ^V t1
þ Kt yt  C^xt1
where the initial conditions are ^x0
1 ¼  and ^V 0
1 ¼ S. The
estimates ^x
are obtained with the backward
recursions. For t ¼ ; . . . ; 1
Jt1 ¼ ^V t1
t1 ATð ^V t1
t1 ¼ ^xt1
t1 þ Jt1ð^x
t  A^xt1
t1 ¼ ^V t1
t1 þ Jt1ð ^V 
t  ^V t1
The covariance ^V 
t;t1 is computed recursively, for t ¼ ; . . . ; 2
t1;t2 ¼ ^V t1
t2 þ Jt1ð ^V 
t;t1  A ^V t1
with initial condition ^V 
;1 ¼ ðI  KCÞA ^V 1
1 . Finally, the
data log-likelihood can also be computed efficiently using
the “innovations” form 
log pðytjyt1
log Gðyt; C^xt1
; C ^V t1
If R is an i.i.d. or diagonal covariance matrix (for example,
R ¼ rIm), then the filter can be computed efficiently using
the matrix inversion lemma.
ACKNOWLEDGMENTS
The authors thank Gianfranco Doretto and Stefano Soatto
for the synthetic sequences in , , the Washington
State DOT for the videos of highway traffic, Daniel
Dailey for the loop-sensor data, Rene Vidal, Dheeraj
Singaraju, and Atiyeh Ghoreyshi for code in , ,
Pedro Moreno for helpful discussions, and the anonymous
reviewers for insightful comments. This work was funded
by the US National Science Foundation Award IIS-0534985
and NSF IGERT Award DGE-0333451.