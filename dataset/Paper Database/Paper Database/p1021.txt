Multi-Scale Orderless Pooling of
Deep Convolutional Activation Features
Yunchao Gong1, Liwei Wang2, Ruiqi Guo2, and Svetlana Lazebnik2
1University of North Carolina at Chapel Hill
 
2University of Illinois at Urbana-Champaign
{lwang97,guo29,slazebni}@illinois.edu
Abstract. Deep convolutional neural networks (CNN) have shown their
promise as a universal representation for recognition. However, global
CNN activations lack geometric invariance, which limits their robustness
for classiﬁcation and matching of highly variable scenes. To improve the
invariance of CNN activations without degrading their discriminative
power, this paper presents a simple but eﬀective scheme called multiscale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD
pooling of these activations at each level separately, and concatenates the
result. The resulting MOP-CNN representation can be used as a generic
feature for either supervised or unsupervised recognition tasks, from image classiﬁcation to instance-level retrieval; it consistently outperforms
global CNN activations without requiring any joint training of prediction
layers for a particular target dataset. In absolute terms, it achieves stateof-the-art results on the challenging SUN397 and MIT Indoor Scenes classiﬁcation datasets, and competitive results on ILSVRC2012/2013 classi-
ﬁcation and INRIA Holidays retrieval datasets.
Introduction
Recently, deep convolutional neural networks (CNN) have demonstrated
breakthrough accuracies for image classiﬁcation . This has spurred a ﬂurry
of activity on further improving CNN architectures and training algorithms
 , as well as on using CNN features as a universal representation for
recognition. A number of recent works show that CNN features
trained on suﬃciently large and diverse datasets such as ImageNet can be
successfully transferred to other visual recognition tasks, e.g., scene classiﬁcation and object localization, with a only limited amount of task-speciﬁc training
data. Our work also relies on reusing CNN activations as oﬀ-the-shelf features for
whole-image tasks like scene classiﬁcation and retrieval. But, instead of simply
computing the CNN activation vector over the entire image, we ask whether we
can get improved performance by combining activations extracted at multiple
local image windows. Inspired by previous work on spatial and feature space
pooling of local descriptors , we propose a novel and simple pooling
 
Y. Gong et al.
scheme that signiﬁcantly outperforms global CNN activations for both supervised tasks like image classiﬁcation and unsupervised tasks like retrieval, even
without any ﬁne-tuning on the target datasets.
Image representation has been a driving motivation for research in computer
vision for many years. For much of the past decade, orderless bag-of-features
(BoF) methods were considered to be the state of the art. Especially when built on top of locally invariant features like SIFT , BoF can
be, to some extent, robust to image scaling, translation, occlusion, and so on.
However, they do not encode global spatial information, motivating the incorporation of loose spatial information in the BoF vectors through spatial pyramid
matching (SPM) . Deep CNN, as exempliﬁed by the system of Krizhevsky
et al. , is a completely diﬀerent architecture. Raw image pixels are ﬁrst sent
through ﬁve convolutional layers, each of which ﬁlters the feature maps and then
max-pools the output within local neighborhoods. At this point, the representation still preserves a great deal of global spatial information. For example,
as shown by Zeiler and Fergus , the activations from the ﬁfth max-pooling
layer can be reconstructed to form an image that looks similar to the original
one. Though max-pooling within each feature map helps to improve invariance
to small-scale deformations , invariance to larger-scale, more global deformations might be undermined by the preserved spatial information. After the
ﬁltering and max-pooling layers follow several fully connected layers, ﬁnally producing an activation of 4096 dimensions. While it becomes more diﬃcult to
reason about the invariance properties of the output of the fully connected layers, we will present an empirical analysis in Section 3 indicating that the ﬁnal
CNN representation is still fairly sensitive to global translation, rotation, and
scaling. Even if one does not care about this lack of invariance for its own sake,
we show that it directly translates into a loss of accuracy for classiﬁcation tasks.
Intuitively, bags of features and deep CNN activations lie towards opposite
ends of the “orderless” to “globally ordered” spectrum for visual representations.
SPM is based on realizing that BoF has insuﬃcient spatial information for
many recognition tasks and adding just enough such information. Inspired by
this, we observe that CNN activations preserve too much spatial information,
and study the question of whether we can build a more orderless representation
on top of CNN activations to improve recognition performance. We present a
simple but eﬀective framework for doing this, which we refer to as multi-scale
orderless pooling (MOP-CNN). The idea is summarized in Figure 1. Brieﬂy, we
begin by extracting deep activation features from local patches at multiple scales.
Our coarsest scale is the whole image, so global spatial layout is still preserved,
and our ﬁner scales allow us to capture more local, ﬁne-grained details of the
image. Then we aggregate local patch responses at the ﬁner scales via VLAD
encoding . The orderless nature of VLAD helps to build a more invariant
representation. Finally, we concatenatenate the original global deep activations
with the VLAD features for the ﬁner scales to form our new image representation.
Section 2 will introduce our multi-scale orderless pooling approach. Section
3 will present a small-scale study suggesting that CNN activations extracted
Multi-scale Orderless Pooling of Deep Convolutional Activation Features
(a) level1: global activation
(b) level2: pooled features
(c) level3: pooled features
4096-D activations
4096-D activations
4096-D activations
Kmeans + VLAD pooling
Kmeans + VLAD pooling
Fig. 1. Overview of multi-scale orderless pooling for CNN activations (MOP-CNN).
Our proposed feature is a concatenation of the feature vectors from three levels: (a)
Level 1, corresponding to the 4096-dimensional CNN activation for the entire 256×256
image; (b) Level 2, formed by extracting activations from 128×128 patches and VLAD
pooling them with a codebook of 100 centers; (c) Level 3, formed in the same way as
level 2 but with 64 × 64 patches.
at sub-image windows can provide more robust and discriminative information
than whole-image activations, and conﬁrming that MOP-CNN is more robust
in the presence of geometric deformations than global CNN. Next, Section 4
will report comprehensive experiments results for classiﬁcation on three image
datasets and retrieval
on the Holidays dataset. A sizable boost in performance across these popular
benchmarks conﬁrms the promise of our method. Section 5 will conclude with a
discussion of future work directions.
The Proposed Method
Inspired by SPM , which extracts local patches at a single scale but then
pools them over regions of increasing scale, ending with the whole image, we
propose a kind of “reverse SPM” idea, where we extract patches at multiple
scales, starting with the whole image, and then pool each scale without regard
to spatial information. The basic idea is illustrated in Figure 1.
Our representation has three scale levels, corresponding to CNN activations
of the global 256 × 256 image and 128 × 128 and 64 × 64 patches, respectively.
To extract these activations, we use the Caﬀe CPU implementation pretrained on ImageNet . Given an input image or a patch, we resample it
to 256 × 256 pixels, subtract the mean of the pixel values, and feed the patch
through the network. Then we take the 4096-dimensional output of the seventh
(fully connected) layer, after the rectiﬁed linear unit (ReLU) transformation, so
that all the values are non-negative (we have also tested the activations before
ReLU and found worse performance).
Y. Gong et al.
For the ﬁrst level, we simply take the 4096-dimensional CNN activation for
the whole 256 × 256 image. For the remaining two levels, we extract activations
for all 128 × 128 and 64 × 64 patches sampled with a stride of 32 pixels. Next,
we need to pool the activations of these multiple patches to summarize the
second and third levels by single feature vectors of reasonable dimensionality.
For this, we adopt Vectors of Locally Aggregated Descriptors (VLAD) ,
which are a simpliﬁed version of Fisher Vectors (FV) . At each level, we
extract the 4096-dimensional activations for respective patches and, to make
computation more eﬃcient, use PCA to reduce them to 500 dimensions. We
also learn a separate k-means codebook for each level with k = 100 centers.
Given a collection of patches from an input image and a codebook of centers
ci, i = 1, . . . , k, the VLAD descriptor (soft assignment version from ) is
constructed by assigning each patch pj to its r nearest cluster centers rNN(pj)
and aggregating the residuals of the patches minus the center:
j: c1∈rNN(pj)
wj1(pj −c1),
j: ck∈rNN(pj)
wjk(pj −ck)
where wjk is the Gaussian kernel similarity between pj and ck. For each patch,
we additionally normalize its weights to its nearest r centers to have sum one. For
the results reported in the paper, we use r = 51 and kernel standard deviation
of 10. Following , we power- and L2-normalize the pooled vectors. However,
the resulting vectors still have quite high dimensionality: given 500-dimensional
patch activations pj (after PCA) and 100 k-means centers, we end up with
50,000 dimensions. This is too high for many large-scale applications, so we
further perform PCA on the pooled vectors and reduce them to 4096 dimensions.
Note that applying PCA after the two stages (local patch activation and global
pooled vector) is a standard practice in previous works . Finally, given
the original 4096-dimensional feature vector from level one and the two 4096dimensional pooled PCA-reduced vectors from levels two and three, we rescale
them to unit norm and concatenate them to form our ﬁnal image representation.
Analysis of Invariance
We ﬁrst examine the invariance properties of global CNN activations vs. MOP-
CNN. As part of their paper on visualizing deep features, Zeiler and Fergus 
analyze the transformation invariance of their model on ﬁve individual images
by displaying the distance between the feature vectors of the original and transformed images, as well as the change in the probability of the correct label for
the transformed version of the image (Figure 5 of ). These plots show very
1 In the camera-ready version of the paper, we incorrectly reported using r = 1, which
is equivalent to the hard assignment VLAD in . However, we have experimented
with diﬀerent r and their accuracy on our datasets is within 1% of each other.
Multi-scale Orderless Pooling of Deep Convolutional Activation Features
scaling ratio=10/9
v-translation = -40
horizontal ﬂipping
rotation degree
=scaling ratio=10/8
scaling ratio=10/7
scaling ratio=10/6
scaling ratio=10/5
scaling ratio=10/4
vertical ﬂipping
rotation degree
=rotation degree
=rotation degree 5
rotation degree 10
rotation degree 20
v-translation = -20
v-translation = 20
v-translation = 40
h-translation = 40
h-translation = 20
h-translation = -20
h-translation = -40
Fig. 2. Illustration of image transformations considered in our invariance study. For
scaling by a factor of ρ, we take crops around the image center of (1/ρ) times original
size. For translation, we take crops of 0.7 times the original size and translate them by
up to 40 pixels in either direction horizontally or vertically (the translation amount is
relative to the normalized image size of 256 × 256). For rotation, we take crops from
the middle of the image (so as to avoid corner artifacts) and rotate them from -20 to 20
degrees about the center. The corresponding scaling ratio, translation distance (pixels)
and rotation degrees are listed below each instance.
diﬀerent patterns for diﬀerent images, making it diﬃcult to draw general conclusions. We would like to conduct a more comprehensive analysis with an emphasis
on prediction accuracy for entire categories, not just individual images. To this
end, we train one-vs-all linear SVMs on the original training images for all 397
categories from the SUN dataset using both global 4096-dimensional CNN
activations and our proposed MOP-CNN features. At test time, we consider four
possible transformations: translation, scaling, ﬂipping and rotation (see Figure
2 for illustration and detailed explanation of transformation parameters). We
apply a given transformation to all the test images, extract features from the
transformed images, and perform 397-way classiﬁcation using the trained SVMs.
Figure 3 shows classiﬁcation accuracies as a function of transformation type and
parameters for four randomly selected classes: arrival gate, ﬂorist shop, volleyball
court, and ice skating. In the case of CNN features, for almost all transformations, as the degree of transformation becomes more extreme, the classiﬁcation
accuracies keep dropping for all classes. The only exception is horizontal ﬂipping,
which does not seem to aﬀect the classiﬁcation accuracy. This may be due to the
fact that the Caﬀe implementation adds horizontal ﬂips of all training images
to the training set (on the other hand, the Caﬀe training protocol also involves
taking random crops of training images, yet this does not seem suﬃcient for
building in invariance to such transformations, as our results indicate). By contrast with global CNN, our MOP-CNN features are more robust to the degree
of translation, rotation, and scaling, and their absolute classiﬁcation accuracies
are consistently higher as well.
Figure 4 further illustrates the lack of robustness of global CNN activations
by showing the predictions for a few ILSVRC2012/2013 images based on different image sub-windows. Even for sub-windows that are small translations of
each other, the predicted labels can be drastically diﬀerent. For example, in (f),
Y. Gong et al.
CNN: scaling (ratio)
Classification Accuracy
CNN: vertical translation (pixels)
Classification Accuracy
CNN: horizontal translation (pixels)
Classification Accuracy
MOP−CNN: scaling (ratio)
Classification Accuracy
(a) scaling
MOP−CNN: vertical translation (pixels)
Classification Accuracy
(b) v-translation
MOP−CNN: horizontal translation (pixels)
Classification Accuracy
(c) h-translation
horizontal
CNN: flipping
Classification Accuracy
CNN: rotation (degrees)
Classification Accuracy
horizontal
MOP−CNN: flipping
Classification Accuracy
(d) ﬂipping
MOP−CNN: rotation (degrees)
Classification Accuracy
(e) rotation
Fig. 3. Accuracies for 397-way classiﬁcation on four classes from the SUN dataset as a
function of diﬀerent transformations of the test images. For each transformation type
(a-e), the upper (resp. lower) plot shows the classiﬁcation accuracy using the global
CNN representation (resp. MOP-CNN).
the red rectangle is correctly labeled “alp,” while the overlapping rectangle is incorrectly labeled “garﬁsh.” But, while picking the wrong window can give a bad
prediction, picking the “right” one can give a good prediction: in (d), the whole
image is wrongly labeled, but one of its sub-windows can get the correct label –
“schooner.” This immediately suggests a sliding window protocol at test time:
given a test image, extract windows at multiple scales and locations, compute
their CNN activations and prediction scores, and look for the window that gives
the maximum score for a given class. Figure 5 illustrates such a “scene detection”
approach on a few SUN images. In fact, it is already common for CNN
implementations to sample multiple windows at test time: the systems of 
can take ﬁve sub-image windows corresponding to the center and four corners,
together with their ﬂipped versions, and average the prediction scores over these
ten windows. As will be shown in Table 4, for Caﬀe, this “center+corner+ﬂip”
Multi-scale Orderless Pooling of Deep Convolutional Activation Features
bighorn sheep
wood rabbit
bighorn sheep
(b) bighorn sheep
hand blower
(c) pitcher
(d) schooner
(e) bee eater
Fig. 4. Classiﬁcation of CNN activations of local patches in an image. The ground
truth labels are listed below each image. Labels predicted by whole-image CNN are
listed in the bottom right corner.
Fig. 5. Highest-response windows (in red) for (a) basilica, (b) control tower, (c) boardwalk, and (d) tower. For each test image resampled to 256×256, we search over windows
with widths 224, 192, 160, and 128 and a stride of 16 pixels and display the window
that gives the highest prediction score for the ground truth category. The detected
windows contain similar structures: in (a), (b) and (d), the top parts of towers have
been selected; in (c), the windows are all centered on the narrow walkway.
strategy gets 56.30% classiﬁcation accuracy on ILSVRC2012/2013 vs. 54.34%
for simply classifying global image windows. An even more recent system, Over-
Feat , incorporates a more comprehensive multi-scale voting scheme for classiﬁcation, where eﬃcient computations are used to extract class-level activations
at a denser sampling of locations and scales, and the average or maximum of
these activations is taken to produce the ﬁnal classiﬁcation results. With this
scheme, OverFeat can achieve as high as 64.26% accuracy on ILSVRC2012/2013,
albeit starting from a better baseline CNN with 60.72% accuracy.
While the above window sampling schemes do improve the robustness of prediction over single global CNN activations, they all combine activations (classiﬁer
Y. Gong et al.
Table 1. A summary of baselines and their relationship to the MOP-CNN method.
pooling method / scale
multi-scale
concatenation
Average pooling
Avg (multi-scale)
Avg (concatenation)
Max pooling
Max (multi-scale) Max (concatenation)
VLAD pooling
VLAD (multi-scale)
responses) from the ﬁnal prediction layer, which means that they can only be
used following training (or ﬁne-tuning) for a particular prediction task, and do
not naturally produce feature vectors for other datasets or tasks. By contrast,
MOP-CNN combines activations of the last fully connected layer, so it is a more
generic representation that can even work for tasks like image retrieval, which
may be done in an unsupervised fashion and for which labeled training data may
not be available.
Large-Scale Evaluation
To validate MOP-CNN, we need to demonstrate that a simpler patch sampling
and pooling scheme cannot achieve the same performance. As simpler alternatives to VLAD pooling, we consider average pooling, which involves computing
the mean of the 4096-dimensional activations at each scale level, and maximum
pooling, which involves computing their element-wise maximum. We did not
consider standard BoF pooling because it has been demonstrated to be less accurate than VLAD ; to get competitive performance, we would need a codebook
size much larger than 100, which would make the quantization step prohibitively
expensive. As additional baselines, we need to examine alternative strategies with
regards to pooling across scale levels. The multi-scale strategy corresponds to
taking the union of all the patches from an image, regardless of scale, and pooling them together. The concatenation strategy refers to pooling patches from
three levels separately and then concatenating the result. Finally, we separately
examine the performance of individual scale levels as well as concatenations of
just pairs of them. In particular, level1 is simply the 4096-dimensional global
descriptor of the entire image, which was suggested in as a generic image
descriptor. These baselines and their relationship to our full MOP-CNN scheme
are summarized in Table 1.
We test our approach on four well-known benchmark datasets:
SUN397 is the largest dataset to date for scene recognition. It contains
397 scene categories and each has at least 100 images. The evaluation protocol
involves training and testing on ten diﬀerent splits and reporting the average
classiﬁcation accuracy. The splits are ﬁxed and publicly available from ; each
has 50 training and 50 test images.
Multi-scale Orderless Pooling of Deep Convolutional Activation Features
MIT Indoor contains 67 categories. While outdoor scenes, which comprise
more than half of SUN (220 out of 397), can often be characterized by global
scene statistics, indoor scenes tend to be much more variable in terms of composition and better characterized by the objects they contain. This makes the MIT
Indoor dataset an interesting test case for our representation, which is designed
to focus more on appearance of sub-image windows and have more invariance to
global transformations. The standard training/test split for the Indoor dataset
consists of 80 training and 20 test images per class.
ILSVRC2012/2013 , or ImageNet Large-Scale Visual Recognition Challenge, is the most prominent benchmark for comparing large-scale image classiﬁcation methods and is the dataset on which the Caﬀe representation we use 
is pre-trained. ILSVRC diﬀers from the previous two datasets in that most of its
categories focus on objects, not scenes, and the objects tend to be highly salient
and centered in images. It contains 1000 classes corresponding to leaf nodes in
ImageNet. Each class has more than 1000 unique training images, and there is
a separate validation set with 50,000 images. The 2012 and 2013 versions of the
ILSVRC competition have the same training and validation data. Classiﬁcation
accuracy on the validation set is used to evaluate diﬀerent methods.
INRIA Holidays is a standard benchmark for image retrieval. It contains
1491 images corresponding to 500 image instances. Each instance has 2-3 images
describing the same object or location. A set of 500 images are used as queries,
and the rest are used as the database. Mean average precision (mAP) is the
evaluation metric.
Image Classiﬁcation Results
In all of the following experiments, we train classiﬁers using the linear SVM
implementation from the INRIA JSGD package . We ﬁx the regularization
parameter to 10−5 and the learning rate to 0.2, and train for 100 epochs.
Table 2 reports our results on the SUN397 dataset. From the results for baseline pooling methods in (a), we can see that VLAD works better than average
and max pooling and that pooling scale levels separately works better than pooling them together (which is not altogether surprising, since the latter strategy
raises the feature dimensionality by a factor of three). From (b), we can see
that concatenating all three scale levels gives a signiﬁcant improvement over any
subset. For reference, Part (c) of Table 2 gives published state-of-the-art results
from the literature. Xiao et al. , who have collected the SUN dataset, have
also published a baseline accuracy of 38% using a combination of standard features like GIST, color histograms, and BoF. This baseline is slightly exceeded by
the level1 method, i.e., global 4096-dimensional Caﬀe activations pre-trained on
ImageNet. The Caﬀe accuracy of 39.57% is also comparable to the 40.94% with
an analogous setup for DeCAF .2 However, these numbers are still worse than
2 DeCAF is an earlier implementation from the same research group and Caﬀe is its
“little brother.” The two implementations are similar, but Caﬀe is faster, includes
support for both CPU and GPU, and is easier to modify.
Y. Gong et al.
Table 2. Scene recognition on SUN397. (a) Alternative pooling baselines (see Section
4.1 and Table 1); (b) Diﬀerent combinations of scale levels – in particular, “level1” corresponds to the global CNN representation and “level1+level2+level3” corresponds to
the proposed MOP-CNN method. (c) Published numbers for state-of-the-art methods.
feature dimension
Avg (Multi-Scale)
Avg (Concatenation)
Max (Multi-Scale)
Max (Concatenation)
VLAD (Multi-Scale)
level1 + level2
level1 + level3
level2 + level3
level1 + level2 + level3 (MOP-CNN)
Xiao et al. 
FV (SIFT + Local Color Statistic) 
the 47.2% achieved by high-dimensional Fisher Vectors – to our knowledge,
the state of the art on this dataset to date. With our MOP-CNN pooling scheme,
we are able to achieve 51.98% accuracy with feature dimensionality that is an
order of magnitude lower than that of . Figure 6 shows six classes on which
MOP-CNN gives the biggest improvement over level1, and six on which it has
the biggest drop. For classes having an object in the center, MOP-CNN usually
cannot improve too much, or might hurt performance. However, for classes that
have high spatial variability, or do not have a clear focal object, it can give a
substantial improvement.
Table 3 reports results on the MIT Indoor dataset. Overall, the trends are
consistent with those on SUN, in that VLAD pooling outperforms average and
max pooling and combining all three levels yields the best performance. There is
one interesting diﬀerence from Table 2, though: namely, level2 and level3 features
work much better than level1 on the Indoor dataset, whereas the diﬀerence
was much less pronounced on SUN. This is probably because indoor scenes are
better described by local patches that have highly distinctive appearance but
can vary greatly in terms of location. In fact, several recent methods achieving
state-of-the-art results on this dataset are based on the idea of ﬁnding such
patches . Our MOP-CNN scheme outperforms all of them – 68.88% vs.
64.03% for the method of Doersch et al. .
Table 4 reports results on ILSVRC2012/2013. The trends for alternative
pooling methods in (a) are the same as before. Interestingly, in (b) we can see
that, unlike on SUN and MIT Indoor, level2 and level3 features do not work as
well as level1. This is likely because the level1 feature was speciﬁcally trained
on ILSVRC, and this dataset has limited geometric variability. Nevertheless, by
Multi-scale Orderless Pooling of Deep Convolutional Activation Features
Playroom (+48%) Cottage Garden (+50%) Florist shop (+56%)
Poolroom (-30%) Utility room (-26%) Shed (-22%)
football stadium (+46%) Van interior (+46%) Ice Skating Rink (+48%)
Volleyball court (-18%) Industrial area (-18%) Arrival gate (-18%)
Fig. 6. SUN classes on which MOP-CNN gives the biggest decrease over the level1
global features (top), and classes on which it gives the biggest increase (bottom).
combining the three levels, we still get a signiﬁcant improvement. Note that directly running the full pre-trained Caﬀe network on the global features from the
validation set gives 54.34% accuracy (part (c) of Table 4, ﬁrst line), which is
higher than our level1 accuracy of 51.46%. The only diﬀerence between these
two setups, “Caﬀe (Global)” and “level1,” are the parameters of the last classiﬁer layer – i.e., softmax and SVM, respectively. For Caﬀe, the softmax layer
is jointly trained with all the previous network layers using multiple random
windows cropped from training images, while our SVMs are trained separately
using only the global image features. Nevertheless, the accuracy of our ﬁnal
MOP-CNN representation, at 57.93%, is higher than that of the full pre-trained
Caﬀe CNN tested either on the global features (“Global”) or on ten sub-windows
(“Center+Corner+Flip”).
It is important to note that in absolute terms, we do not achieve state-of-theart results on ILSVRC. For the 2012 version of the contest, the highest results
were achieved by Krizhevsky et al. , who have reported a top-1 classiﬁcation
accuracy of 59.93%. Subsequently, Zeiler and Fergus have obtained 64% by
reﬁning the Krizhevsky architecture and combining six diﬀerent models. For the
2013 competition, the highest reported top-1 accuracies are those of Sermanet
et al. : they obtained 64.26% by aggregating CNN predictions over multiple sub-window locations and scales (as discussed in Section 3), and 66.04% by
combining seven such models. While our numbers are clearly lower, it is mainly
because our representation is built on Caﬀe, whose baseline accuracy is below
that of . We believe that MOP-CNN can obtain much better performance when combined with these better CNN models, or by combining multiple
independently trained CNNs as in .
Y. Gong et al.
Table 3. Classiﬁcation results on MIT Indoor Scenes. (a) Alternative pooling baselines
(see Section 4.1 and Table 1); (b) Diﬀerent combinations of scale levels; (c) Published
numbers for state-of-the-art methods.
feature dimension
Avg (Multi-Scale)
Avg (Concatenation)
Max (Multi-Scale)
Max (Concatenation)
VLAD (Multi-Scale)
level1 + level2
level1 + level3
level2 + level3
level1 + level2 + level3 (MOP-CNN)
Discriminative patches 
Disc. patches+GIST+DPM+SPM 
FV + Bag of parts 
Mid-level elements 
Image Retrieval Results
As our last experiment, we demonstrate the usefulness of our approach for an unsupervised image retrieval scenario on the Holidays dataset. Table 5 reports the
mAP results for nearest neighbor retrieval of feature vectors using the Euclidean
distance. On this dataset, level1 is the weakest of all three levels because images
of the same instance may be related by large rotations, viewpoint changes, etc.,
and global CNN activations do not have strong enough invariance to handle
these transformations. As before, combining all three levels achieves the best
performance of 78.82%. Using aggressive dimensionality reduction with PCA
and whitening as suggested in , we can raise the mAP even further to 80.8%
with only a 2048-dimensional feature vector. The state of the art performance on
this dataset with a compact descriptor is obtained by Gordo et al. by using
FV/VLAD and discriminative dimensionality reduction, while our method still
achieves comparable or better performance. Note that it is possible to obtain
even higher results on Holidays with methods based on inverted ﬁles with very
large vocabularies. In particular, Tolias et al. report 88% but their representation would take more than 4 million dimensions per image if expanded into
an explicit feature vector, and is not scalable to large datasets. Yet further improvements may be possible by adding techniques such as query expansion and
geometric veriﬁcation, but they are not applicable for generic image representation, which is our main focus. Finally, we show retrieval examples in Figure 7.
We can clearly see that MOP-CNN has improved robustness to shifts, scaling,
and viewpoint changes over global CNN activations.
Multi-scale Orderless Pooling of Deep Convolutional Activation Features
Table 4. Classiﬁcation results on ILSVRC2012/2013. (a) Alternative pooling baselines
(see Section 4.1 and Table 1); (b) Diﬀerent combinations of scale levels; (c) Numbers
for state-of-the-art CNN implementations. All the numbers come from the respective
papers, except the Caﬀe numbers, which were obtained by us by directly testing their
full network pre-trained on ImageNet. “Global” corresponds to testing on global image
features, and “Center+Corner+Flip” corresponds to averaging the prediction scores
over ten crops taken from the test image (see Section 3 for details).
feature dimension
Avg (Multi-Scale)
Avg (Concatenation)
Max (Multi-Scale)
Max (Concatenation)
VLAD (Multi-Scale)
level1 + level2
level1 + level3
level2 + level3
level1 + level2 + level3 (MOP-CNN)
Caﬀe (Global) 
Caﬀe (Center+Corner+Flip) 
Krizhevsky et al. 
Zeiler and Fergus (6 CNN models) 
OverFeat (1 CNN model) 
OverFeat (7 CNN models) 
(b) level1
(c) MOP-CNN
Fig. 7. Image retrieval examples on the Holiday dataset. Red border indicates a ground
truth image (i.e., a positive retrieval result). We only show three retrieved examples
per query because each query only has one to two ground truth images.
Discussion
This paper has presented a multi-scale orderless pooling scheme that is built on
top of deep activation features of local image patches. On four very challenging
Y. Gong et al.
Table 5. Image retrieval results on the Holidays dataset. (a) Alternative pooling baselines (see Section 4.1 and Table 1); (b) Diﬀerent combinations of scale levels; (c) Full
MOP-CNN descriptor vector compressed by PCA and followed by whitening , for
two diﬀerent output dimensionalities; (c) Published state-of-the-art results with a compact global descriptor (see text for discussion).
feature dimension
Avg (Multi-Scale)
Avg (Concatenation)
Max (Multi-Scale)
Max (Concatenation)
VLAD (Multi-Scale)
level1 + level2
level1 + level3
level2 + level3
level1 + level2 + level3 (MOP-CNN)
MOP-CNN + PCA + Whitening
MOP-CNN + PCA + Whitening
FV + PCA 
Gordo et al. 
datasets, we have achieved a substantial improvement over global CNN activations, in some cases outperforming the state of the art. These results are achieved
with the same set of parameters (i.e., patch sizes and sampling, codebook size,
PCA dimension, etc.), which clearly shows the good generalization ability of the
proposed approach. As a generic low-dimensional image representation, it is not
restricted to supervised tasks like image classiﬁcation, but can also be used for
unsupervised tasks such as retrieval.
Our work opens several promising avenues for future research. First, it remains interesting to investigate more sophisticated ways to incorporate orderless
information in CNN. One possible way is to change the architecture of current
deep networks fundamentally to improve their holistic invariance. Second, the
feature extraction stage of our current pipeline is somewhat slow, and it is interesting to exploit the convolutional network structure to speed it up. Fortunately, there is fast ongoing progress in optimizing this step. One example is
the multi-scale scheme of Sermanet et al. mentioned earlier, and another is
DenseNet . In the future, we would like to reimplement MOP-CNN to beneﬁt
from such architectures.
Acknowledgments. Lazebnik’s research was partially supported by NSF grants
1228082 and 1302438, the DARPA Computer Science Study Group, Xerox UAC,
Microsoft Research, and the Sloan Foundation. Gong was supported by the 2013
Google Ph.D. Fellowship in Machine Perception.
Multi-scale Orderless Pooling of Deep Convolutional Activation Features