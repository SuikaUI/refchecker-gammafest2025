Psychological Methods
1996. Vol. 1. No. 2, 170-177
Copyright 19% by the American Psychological Association, Inc
1082-989X/96/$3.W
Meta-Analysis of Experiments With Matched Groups or
Repeated Measures Designs
William P. Dunlap, Jose M. Cortina, Joel B. Vaslow, and Michael J. Burke
Tulane University
Tests for experiments with matched groups or repeated measures designs use
error terms that involve the correlation between the measures as well as the
variance of the data. The larger the correlation between the measures, the
smaller the error and the larger the test statistic. If an effect size is computed
from the test statistic without taking the correlation between the measures
into account, effect size will be overestimated. Procedures for computing
effect size appropriately from matched groups or repeated measures designs
are discussed.
The purpose of this article is to address issues
that arise when meta-analyses are conducted on
experiments with matched groups or repeated
measures designs. It should be made clear at the
outset that although this article pertains to metaanalyses of experiments with correlated measures,
it does not pertain to meta-analyses of correlations. Such experimental designs, often called
matched groups designs or designs with repeated
measures, we call correlated designs (CDs), and
their analysis is decidedly different from that of the
independent groups designs (IGDs). The matched
groups design in its simplest form occurs when
subjects are matched on some variable and then
randomly assigned by matched pairs to experimental and control conditions. The correlation for this
type of CD is the correlation between experimental and control scores across matched pairs. The
second type of CD is the repeated measures design, which in its simplest form tests the same subject under both experimental and control conditions, usually in random or counterbalanced orders
to minimize carryover. The correlation of importance here is the correlation that commonly occurs
between the repeated measures, and this correlation is often quite high in human research.
William P. Dunlap, Jose M. Cortina, Joel B. Vaslow,
and Michael J. Burke, Department of Psychology,
Tulane University.
Correspondence concerning this article should be addessed to William P. Dunlap, Department of Psychology, Tulane University, New Orleans, Louisiana 70118.
CDs are common in many areas of psychology
including motivation , social perception , employment discrimination ,
and training and development . When the correlation between
measures is large in such experiments, the correlated designs have a power advantage in comparison with that of IGDs; the advantage appears in
the form of larger values for the computed test
statistic. The problem is that if the test statistic is
converted directly to an effect size measure for
purposes of meta-analysis, the estimates obtained
from the correlated designs will be too large, unless an equation that eliminates the influence of
the correlation is used.
These CDs stand in marked contrast to IGDs
in which subjects are assigned at random to either
the experimental or control condition. Because of
random assignment, the order of scores within a
group is entirely arbitrary; therefore the correlation between scores would be meaningless. Hence
the term independent groups design.
To make the argument more concrete, consider
the artificial data provided in Table 1. For purposes of meta-analysis, an effect size must first be
computed. An effect size is simply an index of the
relationship between treatment and outcome that
can be compared across studies. The effect size
for data such as those in Table 1 is defined by
Cohen's "d statistic" to be
d = (ME - MC)/SD,
META-ANALYSIS WITH CORRELATED DESIGNS
Artificial Data to Illustrate the Difference
Independent Groups Designs and Correlated Designs
in Computing Effect Size
Experimental
Difference (E - C)
Note. Effect size: d = (Aft - MJ/SD = (30 - 26)13.162 =
1.265; t for independent groups: tt(14) = 2.530; d calculated from
t,: d = (,(2/«)"! = 2.530(2/8)"- = 1.265. Correlation between
measures, r - .6857; t for correlated measures: f((7) = 4.513;
d calculated from r(, incorrectly, d = ((-(2/«)l: = 4.513
(2/8)"' = 2.256, and correctly, d = tc[2(1 - r)/n]"- = 4.513
[2(1 - ,6857)/8]"; = 1.265.
where ME and Mc are the means of the experimental and control groups, respectively, and SD is the
common standard deviation. For the data in Table
1 the difference between means is 4.0, which when
divided by the common standard deviation gives
an effect size of 1.265. Note that this is the appropriate standardized level of effect regardless of
whether the design was IGD or CD.
If these data were obtained from 16 subjects
randomly assigned to two groups of 8 each, this
would constitute an IGD, and the t statistic for
independent groups (r,) would equal 2.530. The
effect size d can be directly determined from t\ by
 , where n is the sample size per group.
Alternatively, if these data were obtained in an
experiment in which the subjects were first
matched on some variable prior to assignment to
groups (or in which each subject was tested under
both conditions), then the appropriate significance
test would be a t for correlated observations, tc,
which is calculated from the difference scores between matched pairs. The statistic tc equals 4.513,
which is clearly larger than t\ above. The tc is larger
than ?, because the existence of correlation between measures reduces the standard error of the
difference between the means, making the difference across conditions more identifiable. If, however, Equation 2 is used to compute effect size
from tc, the result will be 2.256, nearly twice the
size of the d = 1.256, as defined by Equation 1.
What makes tc larger than t} is the correlation
between these measures, which is .6857 for the
data in Table 1. This correlation does not change
the size of the effect; it simply makes the effect
more noticeable by reducing the standard error.
Table 2 shows what will happen to d if it is estimated by inserting tc in place of r, in Equation 2.
Therefore, the effect of ignoring the correlation
between measures when computing an effect size
from tc computed from a CD can be seen in Table
2. When the correlation equals .75, the effect size
computed using Equation 2 will be double the
effect size as defined by Equation 1. Correlations
in the neighborhood of .75 are to be expected
as the test-retest reliabilities of psychometrically
measurements
Pearlman, Schmidt, & Hunter, 1980); thus it is not
unreasonable to expect this level of overestimation
if the effect size is not estimated properly.
The error term for fc is the standard error of
the mean difference, which is inversely related to
the correlation between measures. The correct formula for estimating effect size from fc is
= tc[2(l -r)/n]m,
where r is the correlation across pairs of measures.
Clearly, Equation 3 applied to these data correctly
estimates d. Because we were unable to find Equation 3 in textbooks on meta-analysis , it is derived in Appendix A. The
derivations are entirely consistent with those of
McGaw and Glass , although these authors did not show how to
go directly from rc to d. The problem, however, is
that the correlation between measures is almost
never reported when CDs are used, so Equation
3 can almost never be used. Instead, for such designs, d must ordinarily be estimated directly from
the means and standard deviations.
To examine the relative accuracy of estimates
of d from Equations 1 and 3, we performed a
Monte Carlo study simulating a matched groups
DUNLAP, CORTINA, VASLOW, AND BURKE
Overestimates of Effect Size if Correlation Between Measures in Correlated Designs Is Ignored in Computation of d
Correlation between measures
" When the correlation between measures is zero, the effect size is not overestimated.
correlated design, the results of which are presented in Table 3. For the simulation, two random
normal samples, n(0,1), were generated by a FOR-
TRAN program using the RNNOF random normal generator from the International Mathematics
Subroutine Library run on an IBM AIX RISC
System/6000 computer. The two samples were intercorrelated by the method of Knapp and Swoyer
Monte Carlo Simulation (10,000 Iterations) of
Estimated Effect Size (d) and Effect Size Variance
(Var) and Becker's Variance
Estimate as Functions of Sample Size per Group (n)
and Population Correlation Between Measures (p)
Where Population Effect Size Equals 1.0
Equation 1
Equation 3
Becker 
Equation 6a
' Presented in the text as Equation 4.
 to have a population correlation of the desired amount. A population effect size of 1 was
produced by adding 1 to the second sample. For
each pair of samples the means, standard deviations, correlation, and /c were computed, from
which d was estimated from the means and standard deviations as in Equation 1 or was estimated
from tc using Equation 3. For each combination
of sample size and correlation the simulation was
iterated 10,000 times.
As can be seen in Table 3, the estimated d values
slightly overestimate the true population value,
which was 1.0, as is predicted by Hedges and Olkin
 . Of the two estimators,
Equation 3 is consistently slightly more accurate
than is Equation 1, although the differences are
quite small and are trivial for the sample size of
50. The variance of these estimates, however, is
clearly a function of the population correlation
between measures and appears to be fairly well
estimated by an equation derived by Becker , which after some change in
notation to conform to that used here is
Var (d) = [2(1 - r)ln] + [d2/(2n - 2)] (4)
(see the last column of Table 3). Becker 
recommended computing effect sizes directly from
the means and standard deviations with CDs, as
in our Equation 1, and therefore does not provide
Equation 3, which the Monte Carlo results show
is very slightly more accurate.
Past Treatment of Problem
One would expect that the problem of estimating effect size from CDs would be carefully and
META-ANALYSIS WITH CORRELATED DESIGNS
correctly treated in most textbooks on metaanalytic procedures, but this is decidedly not the
case. Some sources ignore the difference between
designs completely ,
whereas others provide incorrect suggestions for
dealing with CDs. For example, Rosenthal claimed that if we use effect size expressed
as a correlation "we need not make any special
adjustment in moving from t tests for independent to those for correlated observations." Thus,
he recommended converting any t first to an r by
r = [t2l(t2 + df)]1'
where df is the degrees of freedom on which the
t is based. The r value is then converted to d by
d = [4r2/(l - r2)]"2.
These equations provide the basis for the programs for meta-analysis described by Mullen and
Rosenthal . Applying Rosenthal's 
procedure to tc from Table 1, we would get an
r = .8627 and a d = 3.412, even a worse overestimation of effect size than that of the incorrect procedure described earlier. If Rosenthal's procedure
is applied to t}, the resulting r = .5601 and d =
1.352. The slight overestimation of effect size seen
here is a result of inaccuracies in Equation 6. The
r estimated by Equation 5 is not the r between
measures used in Equation 3, but instead it is the
point-biserial correlation between the dependent
variable under both experimental and control conditions with a binomial dummy predictor indicating the condition, which is .5601 for the data in
Table 1. The inaccuracy of Equation 6 is pointed
out in Appendix B.
Therefore, if Rosenthal's procedure was
applied to an independent-groups t computed
from a CD, even though it is not the appropriate
statistic for that design, it would yield a better
estimate of effect size. Of course, studies that use
CDs would not report t\, so this possible solution
is essentially moot. Failure to carefully distinguish
among design types has led to incorrect determinations of effect size by users of Mullen and Rosenthal's software for meta-analysis . To illustrate the extent of overestimation of effect size
from the application of Mullen and Rosenthal's
software, we describe three studies used in the
Driskell et al. meta-analysis, which reported a total of 16 effects from correlated designs,
in Table 4. As Table 4 shows, in every case the
reported effect size was larger than the effect size
computed by Equation 1. The correctness of the
latter effect sizes can be confirmed by applying
Equation 1 to the means and pooled standard deviation provided. In many cases the overestimation
is striking, with the reported effect size often being
more than double the more consistently defined
effect size. Furthermore, there were at least six
studies used by Driskell et al. in which CDs
were used but for which either the means or standard deviations were not reported. In such cases
effect size cannot be correctly estimated. Such
studies should not have been used in the metaanalysis.
Glass et al. discussed the difference between matched pairs designs in comparison with
IGDs but provided an incorrect equation for the
solution of d from tc; in this case,
d = fc{2/Kl - r2)]}"
 . Application of this incorrect equation to the data in Table
1 results in d - 3.100, an overestimation of effect
size similar in magnitude to the use of Mullen
and Rosenthal's procedure. As one can see
from Equation 6, larger correlations between measures will lead to even greater overestimation of
effect size. Rather than correcting for the correlation between measures, Equation 7 will only make
the estimate more inaccurate.
Finally, Hunter and Schmidt did not discuss the matter of designs with correlated measures in general, although they did include a discussion of the analysis of gain scores. While the
analysis of gain scores is related to the issues discussed in this article and is worthwhile in its own
right, it does not speak directly to the general
problem of the proper meta-analysis of studies
with correlated designs.
Discussion
Again the reader is reminded that there are
other types of meta-analyses, such as meta-analyses of correlations, in which the questions ad-
DUNLAP, CORTINA, VASLOW, AND BURKE
Effect Sizes Reported by Driskell, Copper, and Moran Incorrectly Calculated
From t Statistic From Correlated Designs Together With Correct Effect Size
Computed Directly From Means and Standard Deviations
Experimental
1 P refers to physical practice, M to mental practice.
dressed above are not germane. Also there are
target-independent variables, such as effects of
gender, for which it is unlikely that correlated designs will be used; certainly gender cannot be a
repeated measures variable, and it is unlikely that
male-female pairs would be matched prior to data
collection. However, there are many meta-analyses in which designs with correlated measures are
common, such as a recent study by Driskell et al.
 that combined the findings of both CDs
and IGDs. The Driskell et al. meta-analytic
study of mental practice, which used the Mullen
and Rosenthal procedure with Equations
5 and 6 above, is a case in which the distinction
between CDs and IGDs is ignored, and for that
reason, the conclusions reached in that article must
be questioned.
Because effect sizes with CDs are inflated if the
correct formula is not used, any overall assessment
of the effect size will be too large and will exaggerate the apparent strength of the manipulation. Furthermore, meta-analyses often search for moderators of effect size . To the extent that the likelihood of CDs as opposed to IGDs changes as
a function of the moderator variable, incorrect
estimation of effect size from the former designs
may create an apparent moderator effect where
none in fact actually exists.
Another thought is to separate CDs from IGDs
and analyze the two design types in separate metaanalyses that use different metrics for the effect
size estimates. This idea is fine for the IGDs; however, for the CDs there is no reason to expect that
the correlation from study to study would be the
same. Therefore, unless the correlation for each
META-ANALYSIS WITH CORRELATED DESIGNS
CD was known, they would each most likely be
on their own metric, and of course, if the correlation was known, they could be integrated with the
findings of IGDs using Equation 3. Another option
is to estimate the size of the correlation between
measures from previous findings, as is sometimes
done with reliabilities in meta-analysis; such a procedure would depend on the comfort that the researcher has in the accuracy of the estimated correlation. If the means and standard deviations are
not provided, and if the correlation between measures is not reported nor can be estimated appropriately, then it is best to exclude the study from
the meta-analysis rather than risk incorrectly estimating the effect size.
It should be noted that if one were to compute
a weighted mean effect size or weighted sampling
variance of the mean effect size with weights that
are proportional to the inverse of the sampling
variance of the individual effects , then the appropriate
sampling variances for IGDs and CDs should be
used. Although weights of this type are used in
some meta-analyses, often weights based on sample size alone are used. Researchers might consider alternative sample size-based weights such
as suggested by Hedges and Olkin 
or by Finkelstein, Burke, and Raju .
Hunter and Schmidt pointed out that
meta-analyses provide the empirical building
blocks for theory development and social policy
decisions. If, however, the building blocks are not
constructed correctly, the theory or social policy
they are used to support will be unstable. Moreover, meta-analytic tests of theoretical expectations based on incorrect estimates of effects may
lead to erroneous conclusions about the tenability
of a theory and thus misguide future research and
practice. Only if care is taken to examine the experimental design and to compute effect size correctly can the scientific progress promised by metaanalysis be realized in the behavioral and social
The conclusions to be drawn from this article are
rather clear: (a) In a meta-analysis of experimental
findings, one cannot affort to ignore type of experimental design; instead, CDs must be treated differently from IGDs. (b) With CDs, most authors do
not report the correlation across matched pairs
or across repeated measures; therefore for most
published CD experiments, effect size cannot be
correctly estimated from the test statistic using
Equation 3. (c) Instead, with CDs the meta-analyst
must use the means and standard deviations to
estimate effect size directly, when the correlation
between measures is not provided.