Growing Interpretable Part Graphs on
ConvNets via Multi-Shot Learning
Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu
University of California, Los Angeles
This paper proposes a learning strategy that extracts objectpart concepts from a pre-trained convolutional neural network
(CNN), in an attempt to 1) explore explicit semantics hidden in CNN units and 2) gradually grow a semantically interpretable graphical model on the pre-trained CNN for hierarchical object understanding. Given part annotations on very
few (e.g. 3–12) objects, our method mines certain latent patterns from the pre-trained CNN and associates them with different semantic parts. We use a four-layer And-Or graph to
organize the mined latent patterns, so as to clarify their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior performance
(about 13%–107% improvement) in part center prediction on
the PASCAL VOC and ImageNet datasets1.
Introduction
Convolutional
Krizhevsky, Sutskever, and Hinton 2012; He et al. 2016)
(CNNs) have achieved near human-level performance in object classiﬁcation on some datasets. However, in real-world
applications, we are still facing the following two important
Firstly, given a CNN that is pre-trained for object classiﬁcation, it is desirable to derive an interpretable graphical model to explain explicit semantics hidden inside the
CNN. Based on the interpretable model, we can go beyond
the detection of object bounding boxes, and discover an object’s latent structures with different part components from
the pre-trained CNN representations.
Secondly, it is also desirable to learn from very few annotations. Unlike data-rich applications (e.g. pedestrian and
vehicle detection), many visual tasks demand for modeling
certain objects or certain object parts on the ﬂy. For example, when people teach a robot to grasp the handle of a cup,
they may not have enough time to annotate sufﬁcient training samples of cup handles before the task. It is better to
mine common knowledge of cup handles from a few examples on the ﬂy.
Motivated by the above observations, in this paper, given
a pre-trained CNN, we use very few (3–12) annotations to
Copyright c⃝2017, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
1Codes here: 
Part 1 Part 2
Part 1 Part 2
Limited annotations for
multi-shot learning
t t t 
Figure 1: Comparison of three learning strategies. (a) Individually learning/ﬁne-tuning each part without sharing patterns between parts has large information redundancy in
model representation. (b) Jointly learning/ﬁne-tuning parts
requires all parts to be simultaneously learned. (c) Given
a small number (e.g. 3–12) of part annotations based on
demands on the ﬂy, we incrementally grow new semantic
graphs on a pre-trained CNN, which associate certain CNN
units with new parts.
model a semantic part for the task of part localization. When
a CNN is pre-trained using object-level annotations, we believe that its conv-layers have contained implicit representations of the objects. We call the implicit representations latent patterns, each corresponding to a component of the semantic part (namely a sub-part) or a contextual region w.r.t.
the semantic part. For each semantic part, our goal is to
mine latent patterns from the conv-layers related to this part.
We use an And-Or graph (AOG) to organize the mined latent
patterns to represent the semantic hierarchy of the part.
Input and output: Given a pre-trained CNN and a number of images for a certain category, we only annotate the semantic parts on a few images as input. We develop a method
to grow a semantic And-or Graph (AOG) on the pre-trained
CNN, which associates certain CNN units with the semantic
part. Our method does not require massive annotations for
learning, and can work with even a single part annotation.
We can use the learned AOG to parse/localize object parts
and their sub-parts for hierarchical object parsing.
Fig. 2 shows that the AOG has four layers. In the AOG,
each OR node encodes its alternative representations as
children, and each AND node is decomposed into its constituents.
• Layer 1: the top OR node for semantic part describes the
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)
Layer 2 (AND)
part templates
Layer 4 (terminals)
Layer 1 (OR)
Layer 3 (OR)
latent patterns
Heat map of the selected CNN
units under a part template
A conv-slice in
a conv-layer
Deformation range
for a latent pattern
Figure 2: Semantic And-Or graph grown on the pre-trained CNN. The AOG associates CNN units with certain semantic parts
(head, here). Red lines in the AOG indicate a parse graph for concept association. To visualize these latent patterns, we show
the heat map (left) at the 5-th conv-layers in the VGG-16 network, which sums up the associated units (red squares) throughout
all conv-slices. In Fig. 4, we reconstructed the dog head using the learned AOG to show its interpretability.
head of a sheep in Fig. 2. It lists a number of part templates as children.
• Layer 2: AND nodes for part templates correspond to different poses or local appearances for the part, e.g. a black
sheep head from a front view and a white sheep head from
side view.
• Layer 3: OR nodes for latent patterns describe sub-parts
of the sheep head (e.g. a corner of the nose) or a contextual
region (e.g. the neck region).
• Layer 4: terminal nodes are CNN units. A latent pattern
naturally corresponds to a certain range of units within
a conv-slice. It selects a CNN unit within this range to
account for local shape deformation of this pattern.
Learning method and key beneﬁts: The basic idea for
growing AOG is to deﬁne a metric to distinguish reliable
latent patterns from noisy neural activations in the convlayers. We expect latent patterns with high reliability to 1)
consistently represent certain sub-parts on the annotated object samples, 2) frequently appear in unannotated objects,
and 3) keep stable spatial relationship with other latent patterns. We mine reliable latent patterns to construct the AOG.
This learning method is related to previous studies of pursuing AOGs, which mined hierarchical object structures from
Gabor wavelets on edges and HOG features . We extend such ideas to
feature maps of neural networks.
Our method has the following three key beneﬁts:
• CNN semanticization: We semanticize the pre-trained
CNN by connecting its units to an interpretable AOG. In recent years, people have shown a special interest in opening
the black-box representation of the CNN. In this paper, we
retrieve “implicit” patterns from the CNN, and use the AOG
to associate each pattern with a certain “explicit” semantic
part. We can regard the AOG as an interpretable representation of CNN patterns, which may contribute to the understanding of black-box knowledge organization in the CNN.
• Multi-shot learning: The idea of pattern mining also enables multi-shot learning from small data. Conventional endto-end learning usually requires a large number of annotations to learn/ﬁnetune networks. In contrast, in our learning
scenario, all patterns in the CNN have been well pre-trained
using object-level annotations. We only use very few (3–12)
part annotations to retrieve certain latent patterns, instead of
ﬁnetuning CNN parameters. For example, we use the annotation of a speciﬁc tiger head to mine latent patterns. The
mined patterns are not over-ﬁtted to the head annotation, but
represent common head appearance among different tigers.
Therefore, we can greatly reduce the number of part annotations for training.
• Incremental learning: we can incrementally enrich the
knowledge of semantic parts. Given a pre-trained CNN, we
can incrementally grow new neural connections from CNN
units to a new AOG, in order to represent a new semantic
part. It is important to maintain the generality of the pretrained CNN during the learning procedure. I.e. we do not
change/ﬁne-tune the original convolutional weights within
the CNN, when we grow new AOGs. This allows us to continuously add new semantic parts to the same CNN, without
worrying about the model drift problem.
Contributions of this paper are summarized as follows.
1) From the perspective of model learning, given a few part
annotations, we propose a method to incrementally grow interpretable AOGs on a pre-trained CNN to gradually model
semantic parts of the object.
2) From the perspective of knowledge transferring, our
method semanticizes a CNN by mining reliable latent patterns from noisy neural responses of the CNN and associating the implicit patterns with explicit semantic parts.
3) To the best of our knowledge, we can regard our method
as the ﬁrst study to achieve weakly supervised (e.g. 3–
12 annotations) learning for part localization. Our method
exhibits superior localization performance in experiments
(about 13%–107% improvement in part center prediction).
Related work
Long-term learning & short-term learning: As reported
in , there are
“two learning systems instantiated in mammalians:” 1) the
neocortex gradually acquires sophisticated knowledge representation, and 2) the hippocampus quickly learns speciﬁcs
of individual experiences. CNNs are typically trained using
big data, and contain rich appearance patterns of objects. If
one compares CNNs to the neocortex, then the fast retrieval
of latent patterns related to a semantic part can be compared
to the short-term learning in hippocampus.
Semantics in the CNN: In order to explore the hidden semantics in the CNN, many studies have focused on the visualization of CNN units and
analyzed their statistical features . Liu et
al. extracted and visualized a subspace of CNN features.
Going beyond “passive” visualization, some studies “actively” extracted CNN units with certain semantics for different applications. Zhou et al. discovered latent “scene” semantics from CNN feature maps.
Simon et al. discovered objects 
in an unsupervised manner from CNN feature maps, and
learned semantic parts in a supervised fashion . In our study, given very few part
annotations, we mine CNN patterns that are related to the
semantic part. Obtaining clear semantics makes it easier to
transfer CNN patterns to other part-based tasks.
AOG for knowledge transfer: Transferring hidden patterns in the CNN to other tasks is important for neural
networks. Typical research includes end-to-end ﬁne-tuning
and transferring CNN knowledge between different categories 
and/or datasets . In contrast,
we believe that a good explanation and transparent representation of part knowledge will creates a new possibility of transferring part knowledge. As in , the AOG is suitable to represent the semantic hierarchy, which enables semantic-level interactions
between human and neural networks.
Modeling “objects” vs. modeling “parts” in un-
/weakly-supervised learning: Generally speaking, in terms
of un-/weakly-supervised learning, modeling parts is usually more challenging than modeling entire objects. Given
image-level labels (without object bounding boxes), object
discovery can be achieved by
identifying common foreground patterns from noisy background. Closed boundaries and common object structure are
also strong prior knowledge for object discovery.
In contrast to objects, semantic parts are hardly distinguishable from other common foreground patterns in an
unsupervised manner. Some parts (e.g. the abdomen) do
not have shape boundaries to determine their shape extent. Inspired by graph mining , we mine common patterns from
CNN activation maps in conv-layers to explain the part.
Part localization/detection vs. semanticizing CNN patterns: Part localization/detection is an important task in
computer vision .
There are two key points to differentiate our study from conventional part-detection approaches. First, most methods for
detection, such as the CNN and the DPM , limit their attention to the classiﬁcation problem.
In contrast, our effort is to clarify semantic meanings of implicit CNN patterns. Second, instead of summarizing knowledge from massive annotations, our method mines CNN semantics with very limited supervision.
And-Or graph for part parsing
In this section, we introduce the structure of the AOG and
part parsing/localization based on the AOG. The AOG structure is suitable for clearly representing semantic hierarchy
of a part. The method for mining latent patterns and building the AOG will be introduced in the next section. An AOG
represents the semantic structure of a part at four layers.
semantic part
part template
V tmp ∈Ωtmp
latent pattern
V lat ∈Ωlat
Terminal node
V unt ∈Ωunt
Each OR node in the AOG represents a list of alternative
appearance (or deformation) candidates. Each AND node is
composed of a number of latent patterns to describe its subregions.
In Fig. 2, given CNN activation maps on an image I2,
we can use the AOG for part parsing. From a top-down perspective, the parsing procedure 1) identiﬁes a part template
for the semantic part; 2) parses an image region for the selected part template; 3) for each latent pattern under the part
template, it selects a CNN unit within a certain deformation
range to represent this pattern.
In this way, we select certain AOG nodes in a parse graph
to explain sub-parts of the object (shown as red lines in
Fig. 2). For each node V in the parse graph, we parse an
image region ΛV within image I3. We use SI(V ) to denote
an inference/parsing score, which measures the ﬁtness between the parsed region ΛV and V (as well as the sub-AOG
under V ).
Given an image I2 and an AOG, the actual parsing procedure is solved by dynamic programming in a bottom-up
manner, as follows.
Terminal nodes (CNN units): We ﬁrst focus on parsing
conﬁgurations of terminal nodes. Terminal nodes under a latent pattern are displaced in location candidates of this latent
pattern. Each terminal node V unt has a ﬁxed image region
ΛV unt: we propagate V unt’s receptive ﬁeld back to the image
plane as ΛV unt. We compute V unt’s inference score SI(V unt)
2Because the CNN has demonstrated its superior performance
in object detection, we assume that the target object can be well
detected by the pre-trained CNN. Thus, to simplify the learning
scenario, we crop I to only contain the object, resize it to the image
size for CNN inputs, and only focus on the part localization task.
3Image regions of OR nodes are propagated from their children. Each terminal node has a ﬁxed image region, and each part
template (AND node) has a ﬁxed region scale (will be introduced
later). Thus, we only need infer the center position of each part
template in (2) during part parsing.
based on both its neural response value and its displacement
w.r.t. its parent (see appendix for details4).
Latent patterns: Then, we propagate parsing conﬁgurations from terminal nodes to latent patterns. Each latent
pattern V lat is an OR node. V lat naturally corresponds to
a square within a certain conv-slice in the output of a certain CNN conv-layer as its deformation range5. V lat connects all the CNN units within the deformation range as
children, which represent different deformation candidates.
Given parsing conﬁgurations of its children CNN units as
input, V lat selects the child ˆV unt with the highest score as
the true deformation conﬁguration:
SI(V lat) =
V unt∈Child(V lat) SI(V unt),
ˆΛV lat = Λ ˆV unt
Part templates: Each part template V tmp is an AND
node, which uses its children (latent patterns) to represent
its sub-part/contextual regions. Based on the relationship between V tmp and its children, V tmp uses its children’s parsing
conﬁgurations to parse its own image region ΛV tmp. Given
parsing scores of children, V tmp computes the image region
ˆΛV tmp that maximizes its inference score.
SI(V tmp) = max
V lat∈Child(V tmp)
SI(V lat) + Sinf(ΛV tmp|ˆΛV lat)
Just like typical part models (e.g. DPMs), the AND node
uses each child’s region V lat to infer its own region. SI(V lat)
measures the score of each child, and Sinf(ΛV tmp|ˆΛV lat) measures spatial compatibility between V tmp and each child V lat
in region parsing (see the appendix for formulations).
Semantic part: Finally, we propagate parsing conﬁgurations to the top node V sem. V sem is an OR node. It contains a
list of alternative templates for the part. Just like OR nodes
of latent patterns, V sem selects the child ˆV tmp with the highest score as the true parsing conﬁguration:
SI(V sem)=
V tmp∈Child(V sem) SI(V tmp),
ˆΛV sem = ˆΛ ˆV tmp (3)
Learning: growing an And-Or graph
The basic idea of AOG growing is to distinguish reliable
latent patterns from noisy neural responses in conv-layers
and use reliable latent patterns to construct the AOG.
Training data:
Let I denote an image set for a target category. Among all objects in I, we label bounding
boxes of the semantic part in a small number of images,
Iant ={I1, I2, . . . , IM} ⊂I. In addition, we manually deﬁne
a number of templates for the part. Thus, for each I ∈Iant,
we annotate (Λ∗
V sem, V tmp∗), where Λ∗
V sem denotes the groundtruth bounding box of the part in I, and V tmp∗speciﬁes the
ground-truth template ID for the part.
Which AOG parameters to learn:
We can use human
annotations to deﬁne the ﬁrst two layers of the AOG. If people specify a total of m different part templates during the
4Please see the section of appendix for details.
5We set a constant deformation range for each latent pattern,
which potentially covers 75×75 pxls on the image. Deformation
ranges of different patterns in the same conv-slice may overlap.
annotation process, correspondingly, we can directly connect the top node with m part templates {V tmp∗} as children.
For each part template V tmp, we ﬁx a constant scale for its region ΛV tmp. I.e. if there are n ground-truth part boxes that are
labeled for V tmp, we compute the average scale among the n
part boxes as the constant scale for ΛV tmp.
Thus, the key to AOG construction is to mine children latent patterns for each part template. We need to mine latent
patterns from a total of K conv-layers. We select nk latent
patterns from the k-th (k = 1, 2, . . . , K) conv-layer, where
K and {nk} are hyper-parameters. Let each latent pattern
V lat in the k-th conv-layer correspond to a square deformation range5, which is located in the DV lat-th conv-slice of the
conv-layer. PV lat denotes the center of the range. As analyzed in the appendix, we only need to estimate the parameters of DV lat, PV lat for V lat.
How to learn:
We mine the latent patterns by estimating their best locations DV lat, PV lat ∈θ that maximize the
following objective function.
SI(V sem) −λV tmp∗∥ˆPV sem −P∗
annotated images
unannotated images
where θ is the set of AOG parameters. First, let us focus
on the ﬁrst half of the equation, which learns from part annotations. Given annotations (Λ∗
V sem, V tmp∗) on I, SI(V sem)
denotes the parsing score of the part. ∥ˆPV sem −P∗
V sem∥measures localization error between the parsed part region ˆPV sem
and the ground truth P∗
V sem. We ignore the small probability
of the AOG assigning an annotated image with an incorrect
part template to simplify the computation of parsing scores,
i.e. SI(V sem) ≈SI(V tmp∗).
The second half of (4) learns from objects without part
annotations. We formulate Sunsup
(V lat) = λunsup
I′ ( ˆV unt) +
I′(V unt)−λclose∥ΔPV lat∥2
, where latent pattern V lat selects
CNN unit ˆV unt as its deformation conﬁguration on I′. The
ﬁrst term Srsp
I′ ( ˆV unt) denotes the neural response of the CNN
unit ˆV unt. The second term Sloc
I′ (V unt) = −λloc∥ˆPV unt −PV lat∥2
measures the deformation level of the latent pattern. The
third term measures the spatial closeness between the latent
pattern and its parent V tmp. We assume that 1) latent patterns
that frequently appear among unannotated objects may potentially represent stable sub-parts and should have higher
priorities; and that 2) latent patterns spatially closer to V tmp
are usually more reliable. Please see the appendix for details
I′ ( ˆV unt) and scalar weights of λunsup, λclose, and λloc.
When we set λV tmp∗to a constant λinf K
k=1 nk, we can
transform the learning objective in (4) as follows.
∀V tmp ∈Ωtmp,
V lat∈Child(V tmp)
Score(V lat)
where Score(V lat)=meanI∈IV tmp [SI(V lat) + Sinf(Λ∗
V sem|ˆΛV lat)]
+meanI′∈ISunsup
(V lat). θV tmp ⊂θ denotes the parameters for
Table 1: Average number of children
#1 semantic #2 part #3 latent
Children number
the sub-AOG of V tmp. We use IV tmp ⊂Iant to denote the subset of images that are annotated with V tmp as the groundtruth part template.
Learning the sub-AOG for each part template:
on (5), we can mine the sub-AOG for each part template
V tmp, which uses this template’s own annotations on images
I ∈IV tmp ⊂Iant, as follows.
1) We ﬁrst enumerate all possible latent patterns corresponding to the k-th CNN conv-layer (k = 1, . . . , K), by sampling
all pattern locations w.r.t. DV lat and PV lat.
2) Then, we sequentially compute ˆΛV lat and Score(V lat) for
each latent pattern.
3) Finally, we sequentially select a total of nk latent patterns.
In each step, we select ˆV lat =argmaxV latΔL. I.e. we select latent patterns with top-ranked values of Score(V lat) as V tmp’s
Experiments
Implementation details
We chose the 16-layer VGG network (VGG-16) that was pre-trained using the 1.3M
images in the ImageNet ILSVRC 2012 dataset for object classiﬁcation. Then, given a target category,
we used images in this category to ﬁne-tune the original
VGG-16 (based on the loss for classifying target objects and
background). VGG-16 has 13 conv-layers and 3 fully connected layers. We chose the last 9 (from the 5-th to the 13th) conv-layers as valid conv-layers, from which we selected
units to build the AOG.
Note that during the learning process, we applied the following two techniques to further reﬁne the AOG model.
First, multiple latent patterns in the same conv-slice may
have similar positions PV lat, and their deformation ranges
may highly overlap with each other. Thus, we selected the
latent pattern with the highest Score(V lat) within each small
range of ϵ × ϵ in this conv-slice, and removed other nearby
patterns to obtain a spare AOG structure. Second, for each
V tmp, we estimated nk, i.e. the best number of latent patterns in conv-layer k. We assumed that scores of all the latent patterns in the k-th conv-layer follow the distribution
of Score(V lat) ∼α exp[−(βrank)0.5] + γ, where rank denotes the score rank of V lat. We found that when we set
nk = ⌈0.5/β⌉, the AOG usually had reliable performance.
We tested our method on three benchmark datasets: the PAS-
CAL VOC Part Dataset , the CUB200-
2011 dataset , and the ILSVRC 2013
DET dataset . Just like in most partlocalization studies , we also selected six
animal categories—bird, cat, cow, dog, horse, and sheep—
from the PASCAL Part Dataset for evaluation, which prevalently contain non-rigid shape deformation. The CUB200-
2011 dataset contains 11.8K images of 200 bird species. As
in , we regarded these images as a single
bird category by ignoring the species labels. All the above
seven categories have ground-truth annotations of the head
 and
torso/back. Thus, for each category, we learned two AOGs
to model its head and torso/back, respectively.
In order to provide a more comprehensive evaluation of
part localization, we built a larger object-part dataset based
on the off-the-shelf ILSVRC 2013 DET dataset. We used
30 animal categories among all the 200 categories in the
ILSVRC 2013 DET dataset. We annotated bounding boxes
for the heads and front legs/feet in these animals as two common semantic parts for evaluation. In Experiments, we annotated 3–12 boxes for each part to build the AOG, and we
used the rest images in the dataset as testing images.
Two experiments on multi-shot learning
We applied our method to all animal categories in the above
three benchmark datasets. We designed two experiments to
test our method in the scenarios of (1×3)-shot learning and
(4 × 3)-shot learning, respectively. We applied the learned
AOGs to part localization for evaluation.
Exp. 1, three-shot AOG construction:
For each semantic part of an object category, we learn three different
part templates. We annotated a single bounding box for each
part template. Thus, we used a total of three annotations to
build the AOG for this part.
construction
annotations:
We continuously added more part annotations to
check the performance changes. Just as in Experiment 1,
each part contains the same three part templates. For each
part template, we annotated four parts in four different object images to build the corresponding AOG.
We compared our method with the following nine baselines. The ﬁrst baseline was the fast-RCNN .
We directly used the fast-RCNN to detect the target parts
on objects. To enable a fair comparison, we learned the
fast-RCNN by ﬁrst ﬁne-tuning the VGG-16 network of the
fast-RCNN using all object images in the target category
and then training the fast-RCNN using the part annotations.
The second baseline was the strongly supervised DPM (SS-
DPM) , which was trained with
part annotations for part localization. The third baseline was
proposed in , which trained a DPM component for each object pose to localize object parts (namely,
PL-DPM). We used the graphical model proposed in as the fourth baseline for part localization (PL-
Graph). The ﬁfth baseline, namely CNN-PDD, was proposed by , which selected certain conv-slices (channels) of the CNN to represent the target object part. The sixth baseline (VGG-PDD-
ﬁnetuned) was an extension of CNN-PDD, which was con-
Table 2: Part localization performance
Exp. 1: 3-shot learning
Exp. 2: 12-shot learning
Semantic part
Head Torso
F-legs Head Torso
Because object parts may not have clear boundaries, many
studies did not consider part scales in evaluation. Similarly,
our method mainly localizes part center, and does not discriminatively learn a model for the regression of part bounding boxes, which is different from fast-RCNN methods. Instead, we simply ﬁx a constant bounding-box scale for each
part template, i.e. the average scale of part annotations for
this part template. Nevertheless, our method still exhibits superior performance.
Table 3: Normalized distance of part localization. The performance was evaluated using the CUB200-2011 dataset.
Exp. 1: 3-shot learning
Exp. 2: 12-shot learning
Semantic part
fc7+linearSVM
fc7+RBF-SVM
fc7+NearestNeighbor
VGG-PDD-ﬁnetune
ducted based the VGG-16 network that was pre-ﬁne-tuned
using object images in the target category. Because in the
scope of weakly supervised learning, “simple” methods are
usually insensitive to the over-ﬁtting problem, we designed
the last three baselines as follows. Given the pre-trained
VGG-16 network that was used in our method, we directly
used this network to extract fc7 features from image patches
of the annotated parts, and learned a linear SVM and a RBF
SVM to classify target parts and background. Then, given a
testing image, the three baselines brutely searched part candidates from the image, and used the linear SVM, the RBF
SVM, and the nearest-neighbor strategy, respectively, to detect the best part. All the baselines were conducted using the
same set of annotations for a fair comparison.
Evaluation metric
As mentioned in , a fair evaluation of part
localization requires to remove the factors of object detection. Therefore, we used object bounding boxes to crop objects from the original images as the testing samples. Note
that detection-based baselines (e.g. fast-RCNN, PL-Graph)
may produce several bounding boxes for the part. Just as
in , we took the most
conﬁdent bounding box per image as the localization re-
Table 4: Part center prediction accuracy on the PASCAL
VOC Part Dataset
3-shot learning
12-shot learning
EƵŵďĞƌŽĨĂŶŶŽƚĂƚŝŽŶƐ
ĚĞƚĞĐƚŝŽŶ
Figure 3: Performance with different numbers of annotations. We annotate 1–4 parts for each of the 3 part templates.
sult. Given localization results of a part in a certain category, we used three evaluation metrics. 1) Part detection: a
true part detection was identiﬁed based on the widely used
“IOU ≥0.5” criterion ; the part detection
rate of this category was computed. 2) Center prediction: as
in , if the predicted part center was localized inside the true part bounding box, we considered it a
correct center prediction; otherwise not. The average center
prediction rate was computed among all objects in the category for evaluation. 3) The normalized distance in is a standard metric to evaluate
localization accuracy on the CUB200-2011 dataset. Because
object parts may not have clear boundaries (e.g. the forehead
of the bird), center prediction and normalized distance are
more often used for evaluation of part localization.
Results and quantitative analysis
Table 1 lists the average children number of an AOG node at
different layers. Fig. 4 shows the positions of the extracted
latent pattern nodes, and part-localization results based on
the AOGs. Given an image, we also used latent patterns
in the AOG to reconstruct the corresponding semantic part
based on the technique of , in
order to show the interpretability of the AOG.
In Tables 2, 3, 4, 5, and 6, we compared the performance
Table 5: Part center prediction accuracy of 3-shot learning on the ILSVRC 2013 DET Animal-Part dataset.
liza. koala lobs.
hams. squi. horse
SS-DPM 
24.7 20.7 54.4
PL-DPM 
13.6 14.8 46.0
PL-Graph 
18.8 16.3 56.7
Fast-RCNN 
57.8 84.4 95.8
zebra swine hippo catt. sheep ante. camel otter arma. monk. elep. red pa. gia.pa. gold.
SS-DPM 
64.4 50.8 52.1
PL-DPM 
PL-Graph 
42.6 38.9 47.2
Fast-RCNN 
58.5 56.8 59.5
79.3 80.0 88.3
Front legs
liza. koala lobs.
hams. squi. horse
SS-DPM 
51.9 33.0 51.1
PL-DPM 
PL-Graph 
54.4 14.7 38.0
Fast-RCNN 
67.1 37.6 82.6
zebra swine hippo catt. sheep ante. camel otter arma. monk. elep. red pa. gia.pa. gold.
SS-DPM 
42.8 42.6 47.9
PL-DPM 
PL-Graph 
23.2 16.5 54.2
Fast-RCNN 
18.1 39.1 17.7
59.4 76.5 77.1
Table 6: Part center prediction accuracy of 12-shot learning on the ILSVRC 2013 DET Animal-Part dataset.
liza. koala lobs.
hams. squi. horse
SS-DPM 
26.0 28.9 72.6
PL-DPM 
24.0 17.8 55.3
PL-Graph 
10.4 21.5 55.8
Fast-RCNN 
58.4 63.7 85.1
59.1 80.0 94.9
zebra swine hippo catt. sheep ante. camel otter arma. monk. elep. red pa. gia.pa. gold.
SS-DPM 
61.7 52.4 66.3
PL-DPM 
54.3 42.7 35.6
PL-Graph 
72.3 36.8 50.3
Fast-RCNN 
88.8 76.2 76.1
82.4 82.4 87.1
of different baselines. Our method exhibited much better
performance than other baselines that suffered from over-
ﬁtting problems. In Fig. 3, we showed the performance curve
when we increased the annotation number from 3 to 12. Note
that the 12-shot learning only improved about 0.9%–2.9%
of center prediction over the 3-shot learning. This demonstrated that our method was efﬁcient in mining CNN semantics, and the CNN units related to each part template had
been roughly mined using just three annotations. In fact, we
can further improve the performance by deﬁning more part
templates, rather than by annotating more part boxes for existing part templates.
Conclusions and discussion
In this paper, we have presented a method for incrementally
growing new neural connections on a pre-trained CNN to
encode new semantic parts in a four-layer AOG. Given an
demand for modeling a semantic part on the ﬂy, our method
can be conducted with a small number of part annotations
(even a single box annotation for each part template). In addition, our method semanticizes CNN units by associating
them with certain semantic parts, and builds an AOG as a interpretable model to explain the semantic hierarchy of CNN
Because we reduce high-dimensional CNN activations
to low-dimensional representation of parts/sub-parts, our
method has high robustness and efﬁciency in multi-shot
learning, and has exhibited superior performance to other
baselines.
Acknowledgement
This study is supported by MURI project N00014-16-1-200
and DARPA SIMPLEX project N66001-15-C-4035.
/RFDOL]DWLRQRIKHDGV
/RFDOL]DWLRQRIIURQWOHJV
Figure 4: Image reconstruction based on the AOG (top left), heat maps corresponding to latent patterns at the 5-th conv-layers
in the VGG-16 network (top right), and part localization performance to demonstrate the AOG interpretability (bottom).
Parameters for latent patterns
In general, we use the notation of PV to denote the central
position of an image region ΛV as follows.
center of ΛV tmp
center of ˆΛV lat obtained during part parsing
a constant center position of ΛV unt
an AOG parameter, the center of the square
deformation range of V lat, i.e. V lat’s “ideal”
position without any deformation.
an AOG parameter, average displacement
from V lat to the parent V tmp
Each latent pattern V lat is deﬁned by its location parameters {LV lat, DV lat, PV lat, ΔPV lat} ⊂θ, where θ is the set of
AOG parameters. It means that a latent pattern V lat uses a
square5 within the DV lat-th conv-slice/channel in the output of the LV lat-th CNN conv-layer as its deformation range.
Each V lat in the k-th conv-layer has a ﬁxed value of LV lat =
k. ΔPV lat is used to compute Sinf(ΛV tmp|ˆΛV lat). Given parameter PV lat, the displacement ΔPV lat can be estimated as
ΔPV lat =P∗
V tmp−PV lat, where P
V tmp denotes the average position among all ground-truth parts that are annotated for V tmp.
As a result, for each latent pattern V lat, we only need to learn
its conv-slice DV lat ∈θ and central position PV lat ∈θ.
Scores of terminal nodes
The inference score for each terminal node V unt under a latent pattern V lat is formulated as
SI(V unt) = Srsp
I (V unt) + Sloc
I (V unt) + Spair
I (V unt) =
λrspX(V unt),
X(V unt) > 0
λrspSnone,
X(V unt) ≤0
I (V unt) = −λpair mean
upper∈Neighbor(V lat)
∥[PV unt −ˆPV lat
upper]−[PV lat
upper−PV lat]∥
The score of SI(V unt) consists of the following three terms:
I (V unt) denotes the response value of the unit V unt, when
we input image I into the CNN. X(V unt) denotes the normalized response value of V unt; Snone = −3 is set for nonactivated units. 2) When the parent V lat selects V unt as its location inference (i.e. ˆΛV lat ←ΛV unt), Sloc
I (V unt) measures the
deformation level between V unt’s location PV unt and V lat’s
ideal location PV lat. 3) Spair
I (V unt) indicates the spatial compatibility between neighboring latent patterns: we model the
pairwise spatial relationship between latent patterns in the
upper conv-layer and those in the current conv-layer. For
each V unt (with its parent V lat) in conv-layer LV lat, we select 15 nearest latent patterns in conv-layer LV lat + 1, w.r.t.
∥PV lat −PV lat
upper∥, as the neighboring latent patterns. We set
constant weights λrsp = 1.5, λloc = 1/3, λpair = 10.0,
λunsup = 5.0, and λclose = 0.4 for all categories. Based on
the above design, we ﬁrst infer latent patterns corresponding
to high conv-layers, and use the inference results to select
units in low conv-layers.
Scores of AND nodes
Sinf(ΛV tmp|ˆΛV lat)=−λinf min{∥ˆPV lat + ΔPV lat −PV tmp∥2, d2}
where we set d=37 pxls and λinf = 5.0.