An Exploration of Parameter Redundancy in Deep Networks with
Circulant Projections
Yu Cheng1,3∗
Felix X. Yu2,4∗
Rogerio S. Feris1
Sanjiv Kumar2
Alok Choudhary3
Shih-Fu Chang4
1IBM Research
2Google Research
3Northwestern University
4Columbia University
We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory
footprint and enables the use of the Fast Fourier Transform to speed up the computation.
Considering a fullyconnected neural network layer with d input nodes, and
d output nodes, this method improves the time complexity
from O(d2) to O(d log d) and space complexity from O(d2)
to O(d). The space savings are particularly important for
modern deep convolutional neural network architectures,
where fully-connected layers typically contain more than
90% of the network parameters. We further show that the
gradient computation and optimization of the circulant projections can be performed very efﬁciently. Our experiments
on three standard datasets show that the proposed approach
achieves this signiﬁcant gain in storage and efﬁciency with
minimal increase in error rate compared to neural networks
with unstructured projections.
1. Introduction
Deep neural network-based methods have recently
achieved dramatic accuracy improvements in many areas of
computer vision, including image classiﬁcation ,
object detection , face recognition , and text
recognition .
These high-performing methods rely on deep networks
containing millions or even billions of parameters. For example, the work by Krizhevsky et al. achieved breakthrough results on the 2012 ImageNet challenge using a network containing 60 million parameters with ﬁve convolutional layers and three fully-connected layers. The top face
veriﬁcation results on the Labeled Faces in the Wild (LFW)
dataset were obtained with networks containing hundreds
of millions of parameters, using a mix of convolutional,
∗indicates equal contributions.
locally-connected, and fully-connected layers . In
architectures that rely only on fully-connected layers, the
number of parameters can grow to billions .
As larger neural networks are considered, with more layers and also more nodes in each layer, reducing their storage and computational costs becomes critical to meeting
the requirements of practical applications. Current efforts
towards this goal focus mostly on the optimization of convolutional layers , which consume the bulk of
computational processing in modern convolutional architectures. We instead explore the redundancy of parameters
in fully-connected layers, which are often the bottleneck in
terms of memory consumption. Our solution relies on a
simple and efﬁcient approach based on circulant projections
to signiﬁcantly reduce the storage and computational costs
of fully-connected neural network layers, while mantaining
competitive error rates. Our work brings the following advantages:
• Fully-connected layers in modern convolutional architectures typically contain over 90% of the network parameters. Our approach signiﬁcantly reduces the cost to store
these networks in memory, which is crucial for GPUs or
embedded systems with tight memory constraints.
• The proposed method enables FFT-based computation,
which speeds up evaluation of fully connected layers.
This is especially useful for neural networks with many
fully connected layers, or consisting exclusively of fully
connected layers .
• With much fewer parameters, our method is empirically
shown to require less training data.
1.1. Overview of the proposed approach
A basic computation in a fully-connected neural network
h(x) = φ(Rx),
where R ∈Rk×d, and φ(·) is a element-wise nonlinear
activation function. The above operation connects a layer
 
with d nodes, and a layer with k nodes. In convolutional
neural networks, the fully connected layers are often used
before the ﬁnal softmax output layer, in order to capture
global properties of the image. The computational complexity and space complexity of this linear projection are
O(dk). In practice, k is usually comparable or even larger
than d. This leads to computation and space complexity at
least O(d2), creating a bottleneck for many neural network
architectures.
In this work, we propose to impose a circulant structure
on the projection matrix R in (1)1.
h(x) = φ(Rx),
R is a circulant matrix.
This special structure dramatically reduces the number of
parameters. It also allows us to use the Fast Fourier Transform (FFT) to speed up the computation. Considering a
neural network layer with d input nodes, and d output
nodes, the proposed method reduces the space complexity from O(d2) to O(d), and the time complexity from
O(d2) to O(d log d). Table 1 compares the time and space
complexity of the proposed approach with the conventional
Surprisingly, although the circulant matrix is highly
structured with a very small number of parameters (O(d)),
it captures the global information well, and does not impact the ﬁnal performance much. We show empirically that
our method can provide signiﬁcant reduction of storage and
computational costs while achieving very competitive error
1.2. Organization
Our work is organized as follows. We propose imposing the circulant structure on the linear projection matrix
of fully-connected layers of neural networks to speed up
computations and reduce storage costs in Section 3. We
show a method which can efﬁciently optimize the neural
network while keeping the circulant structure in Section 4.
We demonstrate with experiments on visual data that the
proposed method can speed up the computation and reduce
memory needs while maintaining competitive error rates in
Section 5. We begin by reviewing related work in the following section.
2. Related Work
2.1. Deep Learning
In the past few years, deep neural network methods have
achieved impressive results in many visual recognition tasks
 . Recent advances on learning these models include the use of drop-out to prevent overﬁtting,
1A sign ﬂipping operation is applied before the circulant projection matrix. We will present the formal framework in Section 3.1
Time (Learning)
Conventional NN
Circulant NN
O(d log d)
O(ntd log d)
Table 1. Comparison of the proposed method with neural networks
based on unstructured projections. We assume a fully-connected
layer, and the number of input nodes and number of output nodes
are both d. t is the number of gradient steps in optimizing the
neural network.
more effective non-linear activation functions such as rectiﬁed linear units or max-out , and richer modeling through Network in Network (NiN) . In particular,
training high-dimensional networks with large quantities of
training data is key to obtaining good results, but at the same
time incurs increased computation and storage costs.
2.2. Compressing Neural Networks
The work of Collins and Kohli addresses the problem of memory usage in deep networks by applying
sparsity-inducing regularizers during training to encourage zero-weight connections in the convolutional and fullyconnected layers. Memory consumption is reduced only at
test time, whereas our method cuts down storage costs at
both training and testing times. Other approaches exploit
low-rank matrix factorization to reduce the number
of neural network parameters. In contrast, our approach exploits the redundancy in the parametrization of deep architectures by imposing a circulant structure on the projection
matrix, reducing its storage to a single column vector, while
allowing the use of FFT for faster computation.
Techniques based on knowledge distillation aim to
compress the knowledge of a network with a large set of parameters into a compact and fast-to-execute network model.
This can be achieved by training a compact model to imitate the soft outputs of a larger model. Romero et al 
further show that the intermediate representations learned
by the large model serve as hints to improve the training
process and ﬁnal performance of the compact model. In
contrast, our work does not require the training of an auxiliary model.
Network in Network has been recently proposed as
a tool for richer local patch modeling in convolutional networks, where linear convolutions in each layer are replaced
by convolving the input with a micro-network ﬁlter deﬁned,
for example, by a multi-layer perceptron. The inception
architecture extends this work by using these micronetworks as dimensionality reduction modules to remove
computational bottlenecks and reduce storage costs. A key
differentiating aspect is that we focus on modeling global
dependencies and reducing the cost of fully connected layers, which usually contain the large majority of parameters
in standard conﬁgurations. Therefore, our work is complementary to these methods. Although suggests that
fully-connected layers could be replaced by average pooling without hurting performance for general image classi-
ﬁcation, other works in computer vision and speech
recognition highlight the importance of these layers to
capture global dependencies and achieve state-of-the-art results.
2.3. Speeding up Neural Networks
Several recent methods have been proposed to speed-up
the computation of neural networks, with focus on convolutional architectures . Related to our work,
Mathieu et al. use the Fast Fourier Transform to accelerate the computation of convolutional layers, through
the well-known convolution theorem. In contrast, our work
is focused on the optimization of fully-connected layers by
imposing circulant structure on the weight matrix to speed
up the computation in both training and testing stages.
In the context of object detection, many techniques such
as detector cascades or segmentation-based selective search
 have been proposed to reduce the number of candidate object locations in which a deep neural network is
applied. Our proposed approach is complementary to these
techniques.
Other approaches for speeding up neural networks rely
on hardware-speciﬁc optimizations. For example, fast neural network implementations have been proposed for GPUs
 , CPUs , FPGAs , and on-chip implementations
Our method is also related to the recent efforts
around“shallow” neural networks, which show that sometimes shallow structures can match the performance of deep
structures .
2.4. Linear Projection with Structured Matrices
Structured matrices have been used in improving the
space and computational complexities for different learning paradigms. For example, circulant matrices have been
used in dimensionality reduction , binary embedding
 and kernel approximation . It has been shown that
circulant structure can be used to save space and computation costs without performance degradation. The properties
of circulant matrices have also been exploited to avoid expensive rounds of hard negative mining in training of object
detectors and for real-time tracking .
One could in principle use other structured matrices such
as Hadamard matrices along with a sparse random Gaussian matrix to achieve fast projection as was done in the fast
Johnson-Lindenstrauss transform , but they are slower
than the circulant projection and need more space. We note
that very recently, this idea has been studied in .
3. Circulant Neural Network Model
In this section, we present the general framework of the
circulant neural network model, showing the advantages of
this model in achieving more efﬁcient computational processing and storage cost savings.
3.1. Framework
A circulant matrix R ∈Rd×d is a matrix deﬁned by a
vector r = (r0, r1, · · · , rd−1):
R = circ(r) :=
Let D be a diagonal matrix with each diagonal entry
being a Bernoulli variable (±1 with probability 1/2). For
x ∈Rd, its d-dimensional output is:
h(x) = φ(RDx),
R = circ(r).
The projection with the matrix D corresponds to a random sign ﬂipping step on the data.
3.2. Motivation
The idea of replacing unstructured projection with circulant projection is motivated by using circulant projections
in dimensionality reduction , binary embedding ,
and kernel approximation . From the efﬁciency point of
view, this structure creates great advantages in both space
and computation (detailed in Section 3.4), and enables efﬁcient optimization procedures (Section 4).
It is shown in previous works that when
the parameters of the circulant projection matrix are generated iid. from the standard normal distribution, the circulant
projection (with the random sign ﬂipping matrix) mimics an
unstructured randomized projection. In other words, randomized circulant projections can also be used in different
frameworks to preserve pairwise ℓ2 distance , angle
 , and shift-invariant kernels . It is then reasonable
to conjecture that randomized circulant projections can also
achieve good performance in neural networks (compared to
using unstructured randomized matrices). This is indeed
true, as we will demonstrate empirically. And similar to
binary embedding and kernel approximation, by optimizing
the parameters of the projection matrix, we can signiﬁcantly
improve the performance.
3.3. The Need for Matrix D
This matrix is required in prior work .
By adding the random sign ﬂipping matrix, the resulting
projections are less correlated . In practice, the performance of a circulant neural network drops when the random sign ﬂipping is not performed, which we demonstrate
in Section 5.5. To simplify the notation, we omit the matrix
D in the following sections.
3.4. Space and Time Efﬁciency
The two main advantages of the circulant binary embedding are superior space and time efﬁciency.
Space Efﬁciency. Typically, over 90% of the storage cost of
convolutional neural networks is due to the fully connected
layers. So “compressing” such layers is a very important
task. As the proposed circulant projection contains only
2d parameters (d ﬂoats for R, and d booleans for D)2, the
space complexity is O(d). This is a signiﬁcant advantage
compared to the conventional fully connected layer which
requires O(d2) parameters. For example, when d = 4096,
as in the “AlexNet” , the proposed method can decrease
memory requirements by a factor of thousands, making
the most space consuming component (the fully connected
layer) negligible in memory cost. We will show in Section 5
that surprisingly, the circulant neural network can have very
competitive performance with such dramatic space savings.
The small number of parameters also makes the neural network perform better with limited amount of training data.
In addition, we can further improve the performance (yet
maintain signiﬁcant space savings) by adding more nodes
(“fatter” layers) and more layers.
Time Efﬁciency.
The structure enables the use of Fast
Fourier Transform (FFT) to speed up the computation. For
d-dimensional data, the 1-layer circulant neural network
has time complexity O(d log d). Next we explain how we
achieve this time complexity. Given a data point x, h(x)
can be efﬁciently computed as follows. Denote by ⊛the
operator of a circulant convolution. Based on the deﬁnition
of a circulant matrix,
Rx = r ⊛x.
The convolution above can be computed more efﬁciently in
the Fourier domain, using the Discrete Fourier Transform
(DFT), for which a fast algorithm (FFT) is available.
 F−1(F(r) ◦F(x))
where F(·) is the DFT operator, and F−1(·) is the inverse
DFT (IDFT) operator. As DFT and IDFT can be efﬁciently
computed in O(d log d) with FFT , the proposed approach has time complexity O(d log d).
3.5. When k ̸= d
We have so far assumed the number of nodes in the input
layer d to be equal to the number of nodes in the output
2Note that the circulant matrix is never explicitly computed or stored.
layer.3 In this section, we provide extensions to handle the
case where k ̸= d.
When k < d, the fully connected layer performs a “compression” of the signal. In this case, we still use the circulant
matrix R ∈Rd×d with d parameters, but the output is set
to be the ﬁrst k elements in (4). The circulant neural network is not computationally more efﬁcient in this situation
compared to when k = d.
When k > d, the fully connected layer performs an “expansion” of the signal. In this case, the simplest solution is
to use multiple circulant projections, and concatenate their
output. This has computational complexity O(k log d), and
space complexity O(k). Note that the DFT of the feature
vector can be reused in this case. An alternative approach
is to extend every feature vector to a k-dimensional vector,
by appending k −d zeros. This returns the problem to the
previous setting described in section 3.1 with d replaced by
k. This gives space complexity O(k), and computational
complexity O(k log k). In practice, k is usually at most a
few times larger than d. Empirically the two approaches
take similar computational time. Our experimental results
are based on the second approach.
4. Training Circulant Neural Networks
In this section, we propose a highly efﬁcient way of training circulant neural networks. We also discuss a special
type of circulant neural network, where the parameters of
the circulant matrix are randomized instead of optimized.
4.1. Gradient Computation
The most critical step for optimizing a neural network
given a training set is to compute the gradient of the error function with respect to the network weights. Let us
consider the conventional neural network with two layers,
where the ﬁrst layer computes a linear transformation followed by a nonlinear activation function:
h(x) = φ(Rx),
where R is an unstructured matrix. We assume the second
layer is a linear classiﬁer with weights w. Therefore the
output of the two-layer neural network is
J(x) = wT φ(Rx).
When training the neural network, computing the gradient of the error function involves computing the gradient of
J(x) with respect to each entry of R. It is easy to show that
= wiφ′(Ri·x)xj,
i = 0, · · · , d −1.
j = 0, · · · , d −1.
3Such a setting is commonly used in fully connected layers of recent
convolutional neural network architectures.
where φ′(·) is the derivative of φ(·).
Note that (9) sufﬁces for the gradient-based optimization of neural networks, as the gradient w.r.t. networks with
more layers can simply be computed with the chain rule,
leading to the well-known “back-propagation” scheme.
In the circulant case, we need to compute the gradient of
the following objective function:
J(x) = wT φ(Rx) =
wiφ (Ri·x) ,
R = circ(r).
It is easy to show that
= wT (φ′(Rx) ◦s→i(x))
= s→i(x)T (w ◦φ′(Rx)),
where s→i(·) : Rd →Rd, right (downwards for a column
vector) circularly shifts the vector by one element. Therefore,
=[s→0(x), s→1(x), · · · , s→(d−1)(x)]T (w ◦φ′(Rx))
= circ(s→1(rev(x)))(w ◦φ′(Rx))
=s→1(rev(x)) ⊛(w ◦φ′(r ⊛x)),
rev(x) = (xd−1, xd−2, . . . , x0),
s→1(rev(x)) = (x0, xd−1, xd−2, · · · , x1).
The above uses the same trick of converting the circulant
matrix multiplication to circulant convolution. Therefore,
computing the gradient takes only O(d log d) time with
FFT. Training a multi-layer neural network is nothing more
than applying (13) in each layer with the chain rule.
Note that when k < d, we can simply set the last d −k
entries of w in (8) to be zero. And when k > d, the above
derivations can be applied with minimal changes.
4.2. Randomized Circulant Neural Networks
We also consider the case where the elements of r in (3)
are generated independently from a standard normal distribution N(0, 1). We refer to these models as randomized circulant neural networks. In this case, the parameters of the
circulant projections are deﬁned by random weights, without optimization. In other words, in the optimization process, only the parameters of convolutional layers and the
softmax classiﬁcation layer are optimized. This setting is
interesting to study as it provides insight into the “capacity”
of the model, independent of speciﬁc optimization mechanisms.
We will show empirically that compared to unstructured
randomized neural networks, the circulant neural network
is faster with the same number of nodes, while achieving
similar performance. This surprising result is in line with
recent theoretical/empirical discoveries around using circulant projections on dimensionality reduction , and binary embedding . It has been shown that the circulant
projection behaves similarly to fully randomized projections in terms of the distance preserving properties. In other
words, the randomized circulant projection can be seen as a
simulation of the unstructured randomized projection, both
of which can capture global properties of the data.
In addition, we will show that with the optimizations
described in Section 4.1, the error rate of the neural networks improves signiﬁcantly over the randomized version,
meaning that the circulant structure is ﬂexible and powerful
enough to be used in a data-dependent fashion.
5. Experiments
We apply our model to three standard datasets in our experiments: MNIST, CIFAR-10, and ImageNet. We note
that it is not our goal to obtain state-of-the-art results on
these datasets, but rather to provide a fair analysis of the
effectiveness of circulant projections in the context of deep
neural networks, compared to unstructured projections.
Next we describe our implementation and analysis of accuracy and storage costs on these three datasets, followed by
an experiment on reduced training set size.
5.1. Experiments on MNIST
The MNIST digit dataset contains 60,000 training and
10,000 test images of ten handwritten digits (0 to 9), with
28 × 28 pixels.
We use the LeNet network as our
basic CNN model, which is known to work well on digit
classiﬁcation tasks. LeNet consists of a convolutional layer
followed by a pooling layer, another convolution layer followed by a pooling layer, and then two fully connected layers similar to conventional multilayer perceptrons. We used
a slightly different version from the original LeNet implementation, where the sigmoid activations are replaced by
Rectiﬁed Linear Unit (ReLU) activations for the neurons.
Our implementation extends Caffe , by replacing the
weight matrix of the proposed circulant projections with the
same dimensionality. The results are compared and shown
in Table 2. Our fast circulant neural network achieves an
error rate of 0.95% on the full MNIST test set, which is very
competitive with the 0.92% error rate from the conventional
neural network. At the same time, the circulant LeNet is
5.7x more space efﬁcient and 1.43x more time efﬁcient than
5.2. Experiments on CIFAR
CIFAR-10 is a dataset of natural 32x32 RGB images
covering 10-classes with 50,000 images for training and
Train Error
Test Error
Memory (MB)
Testing Time (sec.)
Circulant LeNet
Table 2. Experimental results on MNIST.
Train Error
Test Error
Memory (MB)
Testing Time (sec.)
CIFAR-10 CNN
Circulant CIFAR-10 CNN
Table 3. Experimental results on CIFAR-10.
10,000 for testing. Images in CIFAR-10 vary signiﬁcantly
not only in object position and object scale within each
class, but also in object colors and textures.
The CIFAR10-CNN network used in our test consists of 3 convolutional layers, 1 fully-connected layer and
1 softmax layer. Rectiﬁed linear units (ReLU) are used as
the activation units. The circulant CIFAR10-CNN is implemented by using a circulant weight matrix to replace the
fully connected layer. Images are cropped to 24x24 and
augmented with horizontal ﬂips, rotation, and scaling transformations. We use an initial learning rate of 0.001 and train
for 700-300-50 epochs with their default weight decay.
A comparison of the error rates obtained by circulant and
unstructured projections is shown in Table 3. Our efﬁcient
approach based on circulant networks obtains test error of
16.71% on this dataset, compared to 15.60% obtained by
the conventional model. At the same time, the circulant network is 4x more space efﬁcient and 1.2x more time efﬁcient
than the conventional CNN.
5.3. Experiments on ImageNet 
ImageNet is a dataset containing over 15 million labeled
high-resolution images belonging to roughly 22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet
Large-Scale Visual Recognition Challenge (ILSVRC) has
been held. A subset of ImageNet with roughly 1000 images
in each of 1000 considered categories is used in this challenge. Our experiments were performed on ILSVRC-2010.
We use a standard CNN network – “AlexNet” as
the building block.
The AlexNet consists of 5 convolutional layers, 2 fully-connected layers and 1 ﬁnal softmax
layer. Rectiﬁed linear units (ReLU) are used as the activation units. Pooling layers and response normalization layers
are also used between convolutional layers. Our circulant
network version involves three components: 1) a feature
extractor, 2) fully circulant layers, and 3) a softmax classiﬁcation layer. For 1 and 3 we utilize the Caffe package
 . For 2, we implement it with Cuda FFT.
All models are trained using mini-batch stochastic gradient descent (SGD) with momentum on batches of 128
images with the momentum parameter ﬁxed at 0.9.
set the initial learning rate to 0.01, and manually decrease
the learning rate if the network stops improving as in 
according to a schedule determined on a validation set.
Dataset augmentation is also exploited.
Table 4 shows the error rate of various models.
have used two types of structures for the proposed method.
Circulant CNN 1 replaces the fully connected layers of
AlexNet with circulant layers. Circulant CNN 2 uses “fatter” circulant layers compared to Circulant CNN 1: d of
Circulant CNN 2 is set to be 214. In “Reduced AlexNet”,
we reduce the parameter size on the fully-connected layer
of the original AlexNet to a size similar to our Circulant
CNN by cutting d. We have the following observations.
• The performance of Randomized Circulant CNN 14 is
very competitive with Randomized AlexNet. This is expected as the circulant projection closely simulates a fully
randomized projection (Section 3.1).
• Optimization signiﬁcantly improves the performance for
both unstructured projections and circulant projections.
The performance of Circulant CNN 1 is very competitive
with AlexNet, yet with fraction of the space cost.
• By tweaking the structure to include more parameters,
Circulant CNN 2 further drops the error rate to 17.8%,
yet it takes only a marginally larger amount of space compared to Circulant CNN 1, an 18x space saving compared
to AlexNet.
• With the same memory cost, the Reduced AlexNet performs much worse than Circulant CNN 1.
In addition, one interesting ﬁnding is that “dropout”,
which is widely used in training CNNs, does not improve
the performance of circulant neural networks.
In fact it
increases the error rate from 19.4% (without dropout) to
20.3% (not shown in the ﬁgure). This indicates that the proposed method is more immune to over-ﬁtting. We also show
the training time (per image) on the standard and circulant
4This is Circulant CNN 1 with randomized circulant projections. In
other words, only the convolutional layer is optimized.
Top-5 Error
Top-1 Error
Memory (MB)
Randomized AlexNet
Randomized Circulant CNN 1
Circulant CNN 1
Circulant CNN 2
Reduced-AlexNet
Table 4. Classiﬁcation error rate and memory cost on ILSVRC-2010.
Full projection
Circulant projection
Space Saving (in a fully connected layer)
Table 5. Comparison of training time (ms per image) and space of full projection and circulant projection. Speedup is deﬁned as the time
of circulant projection divided by the time of unstructured projection. Space saving is deﬁned as the space of storing the circulant model
by the space of storing the unstructured matrix. The unstructured projection matrix in conventional neural networks takes more than 90%
of the space cost. In AlexNet, d is 212.
versions of AlexNet. We vary the number of hidden nodes d
in the fully connected layers and compare the training time
until the model converges (ms/per image). Table 5 shows
the result. Our method provides dramatic space savings and
signiﬁcant speedup compared to the conventional approach.
5.4. Reduced Training Set Size
Compared to the neural network model with unstructured projections, the circulant neural network has fewer
parameters. Intuitively, this may bring the beneﬁt of better model generalization. In other words, the circulant neural network might be less data hungry compared to conventional neural networks. To verify our assumption, we report
the performance of each model when trained with different
training set sizes on the MINST, CIFAR-10, and ILSVRC-
2010 datasets. Figure 1 shows test error rate when training
on a random subset of the training data. On MNIST and
ILSVRC-2010, to achieve a ﬁxed error rate, the circulant
models need less data. On CIFAR-10, this improvement is
limited as the circulant layer only occupies a small part of
the model.
5.5. Results Without D
As noted in Section 3.3, the sign ﬂipping matrix D of (4)
is important in our formulation. We provide some empirical
results in this section. On the MNIST dataset, the test error
rate increases from 0.95% to 2.45% by dropping D. On the
CIFAR-10 dataset, the test error rate increases from 16.71%
to 21.33% by dropping D.
6. Discussion
6.1. Fully Connected Layer vs. Convolution Layer
The goal of the method developed in this paper is to improve the efﬁciency of the fully connected layers of neural networks. In convolutional architectures, the fully connected layers are often the bottleneck in terms of the space
cost. For example, in “Alexnet”, the fully connected layers
take 95% of the storage. Remarkably, the proposed method
enables dramatic space savings in the fully connected layer
(4000x as shown in Table 5), making it negligible in memory cost compared to the convolutional layers. Our discovery resonates with recent work showing that the fully connected layers can be compressed, or even completely removed .
In addition, the fully connected layer costs roughly 20%
– 30% of the computation time based on our implementation. The FFT-based implementation can further improve
the time cost, though not to the same degree as the realized
space savings, if the majority of layers are convolutional.
Our method is complementary to the work on improving the
time and space cost of convolutional layers .
6.2. Circulant Projection vs. 2D Convolution
One may notice that, although our approach leverages
convolutions for speeding-up computations, it is fundamentally different from the convolutions performed in CNNs.
The convolution ﬁlters in CNNs are all small 2D ﬁlters aiming at capturing local information of the images, whereas
the proposed method is used to replace the fully connected
layers, which are often “big” layers capturing global information.
The operation involved is large 1D convolution
rather than small 2D convolution. The circulant projection
(b) CIFAR-10
(c) ILSVRC-2010
Figure 1. Test error when training with reduced dataset sizes of circulant CNN and conventional CNN.
can be understood as “simulating” an unstructured projection, with much less cost. Note that one can also apply FFT
to compute the convolutions on the 2D convolutional layers,
but due to the computational overhead, the speed improvement is generally limited on small-scale problems. In contrast, our method can be used to dramatically speed up and
scale the processing in fully connected layers. For instance,
when the number of input nodes and output nodes are both
1 million, conventional linear projection is essentially impossible, as it requires TBs of memory. On the other hand,
doing a convolution of two 1 million dimensional vectors
requires only MBs of memory and tens of milliseconds.
6.3. Towards Larger Neural Networks
Currently, deep neural network models usually contain
hundreds of millions of parameters. In real-world applications, there exist problems which involve an increasing
amount of data. We may need larger and deeper networks
to learn better representations from large amounts of data.
Compared to unstructured projections, the circulant projection signiﬁcantly reduces computation and storage costs.
Therefore, with the same amount of resources, circulant
neural networks can use deeper as well as larger fullyconnected networks. We have conducted preliminary experiments showing that the circulant model can be extended at
least 10x deeper than conventional neural networks with the
same scale of computational resources.
7. Conclusions
We proposed to use circulant projections to replace
the unstructured projections in order to optimize fully
connected layers of neural networks.
This dramatically
improves the computational complexity from O(d2) to
O(d log d) and space complexity from O(d2) to O(d). An
efﬁcient approach was proposed for optimizing the parameters of the circulant projections. We demonstrated empirically that this optimization can lead to much faster convergence and training time compared to conventional neural networks. Our experimental analysis was carried out
on three standard datasets, showing the effectiveness of the
proposed approach. We also reported experiments on randomized circulant projections, achieving performance similar to that of unstructured randomized projections. Our ongoing work is to explore different matrix structures to compress and speed up neural networks.
Acknowledgement.
Felix X. Yu was supported in part
by the IBM PhD Fellowship Award when working on this
project at Columbia University. We would like to thank Dan
Holtmann-Rice for proofreading the paper.