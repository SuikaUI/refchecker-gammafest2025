Deep Neural Network Compression
with Single and Multiple Level Quantization
Yuhui Xu,1∗Yongzhuang Wang,1 Aojun Zhou,2 Weiyao Lin,1 Hongkai Xiong1
1 School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China
2 University of Chinese Academy of Sciences, China
Network quantization is an effective solution to compress
deep neural networks for practical usage. Existing network
quantization methods cannot sufﬁciently exploit the depth information to generate low-bit compressed network. In this paper, we propose two novel network quantization approaches,
single-level network quantization (SLQ) for high-bit quantization and multi-level network quantization (MLQ) for extremely low-bit quantization (ternary). We are the ﬁrst to consider the network quantization from both width and depth
level. In the width level, parameters are divided into two
parts: one for quantization and the other for re-training to
eliminate the quantization loss. SLQ leverages the distribution of the parameters to improve the width level. In the
depth level, we introduce incremental layer compensation to
quantize layers iteratively which decreases the quantization
loss in each iteration. The proposed approaches are validated
with extensive experiments based on the state-of-the-art neural networks including AlexNet, VGG-16, GoogleNet and
ResNet-18. Both SLQ and MLQ achieve impressive results.
Introduction
Recent years, deep convolutional neural networks (DNNs)
are playing an important role in a variety of computer vision
tasks including image classiﬁcation , object detection , semantic segmentation and face recognition
 . The
promising results of DNNs are contributed by many factors.
Regardless of more training resources and powerful computational hardware, the large number of learnable parameters is the most important one. To achieve high accuracy,
deeper and wider networks are designed which in turn poses
heavy burden on storage and computational resources. It becomes more difﬁcult to deploy a typical DNN model on resource constrained mobile devices such as mobile phones
and drones. Thus, network compression is critical and has
become an effective solution to reduce the storage and computation costs for DNN models.
∗Corresponding authors are Weiyao Lin and Hongkai Xiong:
{wylin, xionghongkai}@sjtu.edu.cn.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
(a) Single-level
(b) Multi-level
Figure 1: Comparison of single-level quantization and multilevel quantization. The blue parts indicate the full-precision
weights and layers of the network. The orange parts are
quantized weights and layers.
One major challenge for network compression is the
tradeoff between complexity and accuracy. However, most
of the recent network compression methods degrade the accuracy of the network more or less . Recently, propose incremental network quantization which
re-trains the un-quantized parameters to compensate for the
quantization loss can achieve high compression rate while
maintaining performance. However, they pay no attention to
the distribution of parameters and treat all layers equally (as
shown in Figure 1a).
In this paper, we argue that both of the width level (partitioning parameters) and the depth level (partitioning layers)
are important in network quantization (shown in Figure 1b).
In the width level, quantization should ﬁt the distribution of
the weights which directly affects the accuracy of the network. Vector quantization is a quantization method that fully
considers the distribution and makes quantization loss easy
to be controlled. Furthermore, weights with special type may
have special use (e.g. weights with type powers of two may
accelerate computations in FPGA devices). We extend our
approach by using L1 norm to constrain the clustering process. In the depth level, layers are important elements of networks. They are interacted and make joint contributions to
the networks. Thus, the quantization loss of one layer can
be eliminated by re-training other layers. For ternary quantization, the huge quantization loss can not be compensated
by re-training if only considering the width level. Thus, we
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
introduce incremental layer compensation that quantize the
layers partially and retrain other layers to compensate for the
quantization loss. Considering both width level and depth
level, the accuracy can be recovered after iteratively ternary
quantization.
In summary, our contributions to network compression
are two folds: (1) We propose single-level quantization approach for high-bit quantization. (2) For extremely lowbit quantization (ternary), we propose multi-level network
quantization.
In the rest of the paper, we ﬁrst introduce some related
works and propose the single-level quantization approach.
Next, we introduce the multi-level approach. Finally, we
give the experiment results and the conclusion of the paper.
Related Work
Compression by Low-rank Decomposition. Reducing parameter dimensions using techniques like Singular Value
Decomposition (SVD) works well on
fully-connected layers and can achieve 3× compression rate.
 introduce this idea to convolutional layers by
noting that weight ﬁlters usually share smooth components
in a low-rank subspace and also remember some important
information represented by weights that are sparsely scattered outside the low-rank subspace. Although this kind of
method can achieve relatively good compression rate, the
accuracy of some neural network models can be hurt.
Compression by Pruning. Pruning is a straightforward
method to compress the networks by removing the unimportant parameters or convolutional ﬁlters. 
present an effective unstructured method to prune the parameters with values under a threshold and they reduce the
model size by 9× on AlexNet and 16× on VGG-16. Filter
level pruning can greatly reduce the computation cost. prune ﬁlters with small effect on the accuracy of
the model and reduce the computation cost for VGG-16 by
up to 34% and ResNet-110 by up to 38%.
Compression by Quantization. Quantization is a manyto-few mapping between a large set of input and a smaller
set of output. It groups weights with similar values to reduce the number of free parameters. Hash-net constrains weights hashed into different groups before training. Within each group the weights are shared and
only the shared weights and hash indices need to be stored.
 compress the network with vector quantization techniques. present deep
compression which combines the pruning ,
vector quantization and Huffman coding, and reduces the
model size by 35× on AlexNet and 49× on VGG-16. However, these quantization methods takes time and will more or
less hurt the performance of the network. Recently, present incremental network quantization (INQ)
method. This method partitions the weights into two different parts: one part is used to quantize and another part is used
to retrain to compensate for quantization loss. The weights
of the network are quantized incrementally and ﬁnally the
accuracy of the quantized model is even higher the the original one. This method basically solves the problem of accuracy loss during network compression. However, in this
paper, the values in the codebook are pre-determined and
quantization group is handcrafted. Thus, this kind of quantization is not data based and the quantization loss can not be
controlled. Besides, they only partition weights which we
refer to the width level and can not achieve great result in
extremely low-bit quantization.
Compression by other strategies. Some other people are
trying to design DNNs with low precision weights, gradients
and activations. propose Xnor-Net
which is a network with binary weights and even binary inputs. discuss the basic elements
of training a high accuracy binary network. design ternary weight network. 
propose HWQN with low bit activations. Knowledge transfer is another method to train a small network. Knowledge
Distilling is proposed to
distill the knowledge from an ensemble of models to a single model by imitate the soft output of them. Neuron selectivity transfer method explores
a new kind of knowledge neuron selectivity to transfer the
knowledge from the teacher model to the student model and
achieves better performance. Speciﬁc DNN architectures are
designed for mobile devices. propose
MobileNets which apply depth-wise separable convolution
to factorize a standard convolution into a depthwise convolution and a 1 × 1 convolution and show the effectiveness
of such architecture across a wide range of applications.
 present ShufﬂeNet. They apply group
convolutions to pointwise convolutions and introduce shuf-
ﬂe operations to maintain the connections between groups
which achieves 13× speed up in ALexNet.
The framework of our approach is shown in Figure 2. Either
single-level quantization or multi-level quantization is composed of four steps: clustering, loss based partition, weightsharing and re-training. Clustering uses k-means clustering
to cluster the weights into k clusters layer-wise. Loss based
partition divides the k clusters of each layer into two disjoint
groups based on their quantization loss. The weights in one
group are quantized into the centroids of their corresponding
clusters by the weight-sharing step. The weights the other
group are re-trained. Furthermore, all of the four steps are
iteratively conducted until all the weights are quantized. The
mainly difference for SLQ and MLQ is the loss based partition step. For SLQ, we only partition clusters. While for
MLQ, we partition clusters and layers. Actually, SLQ is a
particular case of MLQ. Technique details are discussed in
the next sections.
Figure 2: Framework of the proposed approach.
Figure 3: (a) shows the quantization of two different clusters
generated by k-means in AlexNet. Q/C means the value Q
that weights to be quantized into is divided by the centroid of
the cluster C.The accuracy of the network changes with the
change of Q. (b) shows the test accuracy when 10 clusters of
AlexNet are quantized respectively. The clusters are sorted
in the descending order.
Single-level Quantization
Clustering
Other than using handcrafted mapping rules , we adopt k-means clustering which is more datadriven and can easily control quantization loss. We choose
two clusters generated by k-means and quantize the weights
into different values including the centroids of them. Figure
3a shows that the quantization loss is low if we quantize the
weights into the centroids of the clusters.
Loss based Partition
After the layer-wise clustering, each layer holds a code
2 . . . ci
k}, i = 1, 2 . . . L, where ci
k denotes the
kth centroid in the code book of ith layer. We partition the
weights into two groups: the weights in one group are quantized and the weights in the other are re-trained. use the pruning inspired strategy 
that weights with bigger values are more important and need
to be quantized prior. However, this strategy is not suitable
for our approach because the accuracy of the network can
be affected by many factors during quantization including
the value to be quantized into (as shown in Figure 3a) and
the number of weights to be quantized. We test the quantization loss of 10 different clusters of AlexNet that generated
by k-means. The result is shown in Figure 3b. There exist
some clusters that do not ﬁt the pruning inspired strategy
 . Beneﬁt from clustering, the weights are
roughly partitioned and we only need to further partition the
clusters. Besides, for the fact that the number of the clusters is relatively small, we propose loss based partition. We
test the quantization loss of each cluster and sort the clusters
by quantization loss. Cluster with bigger quantization loss is
quantized prior.
For the ith layer, the loss based partition can be deﬁned
(1) is the group containing the clusters to be quantized, while Φi
(2) is the group containing the clusters to be
re-trained. Wi is the set that covers all of the weights in the
ith layer. EQ is quantization loss of the cluster. The minimum EQ of the clusters in Φi
(1) is bigger than the maximum
EQ of clusters in Φi
The clusters are partitioned into two groups, meanwhile
the code book is also divided into two parts: one part is ﬁxed
while the other is updated.
Weight-sharing
We quantize the weights in the group Φi
(1) by weightsharing. The weights in this group are quantized into the
centroids of the corresponding clusters. The weight-sharing
of ith layer is described in Equation 2.
ω(p, q) = ci
ω(p, q) ∈Ψi
j is the cluster in the quantization group Φi
(1), while
j is the centroid of Ψi
Re-training
As weight-sharing brings error to the network, we need to retrain the model to recover accuracy. Thus, we ﬁx the quantized weights and re-train the weights in the other group.
After re-training, as shown in Figure 4, we will come back
to beginning of our approach (clustering) to quantize the left
weights iteratively until all the weights are quantized.
Taking the lth layer as an example, we use Ql to denote
the set of quantized weights in the lth layer. To simplify the
problem, we deﬁne a mask matrix Ml(p, q), which has the
same size as weight matrix ωl(p, q) and acts as an indicator
function to indicate that if the weights has been quantized.
Ml(p, q) can be deﬁned as:
Ml(p, q) =
ωl(p, q) ∈Ql
, otherwise
During re-training, our quantization approach can also be
treated as an optimization problem:
E(ωl) = L(ωl) +
ωl(p, q) ∈Bl, if Ml(p, q) = 0
where L(ωl) is the loss of the network, Rm(ωl) is the regulation term of the mth iteration that constrains the weights
to be quantized into the centroids within Blm. λm is a positive scalar. Bl is the codebook of the centroids after m iterations.
To solve the optimization problem, we re-train the network using stochastic gradient decent(SGD) to update the
un-quantized weights. To ﬁx the quantized weights, we use
the indicator function Ml(p, q) as a mask on the gradient of
the weights to control the gradient propagation:
ωl(p, q) ←ωl(p, q) −γ
∂(ωl(p, q))Ml(p, q)
The whole quantization process is shown in Algorithm 1.
Figure 4: An schematic diagram of our single-level quantization approach: The small rectangle is the codebook. Blue, green and
orange points indicates the full precision, re-trained and quantized weights. Clustering is conducted on pre-trained full-precision
network; Performing loss based partition on the clusters; One group of clusters with weights are quantized into the centroids of
clusters; Fixing quantized weights, the other clusters are re-trained; The re-trained weights are clustered by k-means clustering;
After several iterations, all weights are quantized into the centroids.
Extended Approach
Later, we extend single-level quantization (SLQ) approach.
In the SLQ approach, we quantize the weights layer-wise
into the centroids of clusters. However, sometimes we need
the weights to be some special type. For instance, if all the
weights are power of two, the model will be convenient to
be deployed in FPGA devices.
The main difference of our extended single-level quantization (ESLQ) with original SLQ is that we extend traditional clustering to constrain the cluster centroid to close or
equal to the number with oriented type (t-centroid). Thus,
after weight-sharing, we can quantize the weights into values with oriented type. For instance, we want to constrain
centroid ci
1 to close to or equal to a speciﬁc type: t-centroid
1. We incorporate the L1 norm regulation into the traditional k-means loss function as:
|ω(p, q) −ci
j|2 + β1|ci
ω(p, q), i = 1, 2 . . . L
1 is the t-centroid of ci
1, |ωi| denotes the total number
of weights in the ith layer. We weighted the original k-means
|ωi| to strengthen the impact of the regularization
In ESLQ, we ﬁrst conduct traditional clustering and loss
based partition. Then we determine the t-centroids of the
cluster to be quantized. Subsequently, we re-cluster the
weights by our extended clustering. The weight-sharing and
re-training steps are the same as SLQ. After several iterations, the network can be quantized into oriented type.
Algorithm 1 Single-Level Quantization
1: Input: {ωl : 1 ≤l ≤L}: the pre-trained full-precision DNN
2: output:
l : 1 ≤l ≤L
: the ﬁnal low-precision model with
the weights quantized into the centroids in code book Bl
3: for m = 1, 2, . . . , N do
Reset the base learning rate and the learning policy
Apply k-means clustering layer-wise
Perform loss based partition layer-wise by Equation 1
Quantize the weights in one group by Equation 2
Re-train the network as described in the Re-training section
9: end for
Multi-Level Quantization
The proposed SLQ approach is not suitable for low-bit quantization (e.g. 2-bit quantization into ternary networks) because the number of clusters is small and the quantization
loss in each iteration step is too huge to be eliminated. We
introduce incremental layer compensation (ILC) to partition
the layers of the network which is the depth level of the network. The ILC is motivated by the intuition that different
layers have different impact on the performance of the network during quantization, e.g. convolutional layers and fully
connected layers. The layers L of the network are partitioned
into two groups: one group Lq containing layers with more
quantization loss is quantized prior and another group Lr
containing the remaining layers is re-trained:
Lq ∪Lr = L, and
Lq ∩Lr = 0
We introduce ILC into SLQ which is multi-level quantization (shown in Figure 5). The MLQ partitions both the layers and the parameters within layers, which lowers the huge
quantization loss in low-bit quantization (e.g. 2-bit quantization). Taking the ith layer as an example (ternary quantization), each layer is clustered into 3 clusters and we ob-
Figure 5: Quantization process of multi-level ternary quantization. Blue, green and orange parts indicates the full precision, re-trained and quantized layers. We ﬁrst quantize the
Boundaries and then quantize the Hearts of the network.
Algorithm 2 Multi-Level Quantization
1: Input: {ωl : 1 ≤l ≤L}: the pre-trained full-precision CNN
2: output:
l : 1 ≤l ≤L
: the ternary network
3: Apply k-means clustering layer-wise (cluster number is 3)
4: Perform loss based partition layer-wise to generate Boundaries
and Hearts of the network
5: Quantize the Boundaries iteratively by ILC (partition, weightsharing and re-training)
6: Quantize the Hearts iteratively by ILC (partition, weightsharing and re-training)
tain three centroids: ai, bi and ci. ai and ci affect the performance of the networks more. We call them Boundaries.
bi holding smaller effect is called Heart. We ﬁrst quantize
Boundaries of the network. Different from SLQ that quantizes all the Boundaries at the same time, the MLQ quantizes the boundaries iteratively by ILC. The Boundaries in
different layers are partitioned into two groups, one group is
quantized and the remaining weights in the network are all
re-trained. After all the boundaries are quantized, we then
quantize the Hearts iteratively by ILC too. After several iterations, the Boundaries and the Hearts are all quantized
(shown in Algorithm 2).
Experiments
To analyze the performance of SLQ and MLQ, we conduct
extensive experiments on two datasets: CIFAR-10 and ImageNert.
The bit-width parameter b represents the space we used to
store each quantized weight. To fairly compared with other
methods, we use b bits to code the centroids: one bit to
store zero and the other (b-1) bits to code non-zero centroids
which means that for bit-width b, the centroid number of
each layer is 2b−1 + 1.
CIFAR-10: This dataset consists of 60,000 32×32 colour
images in 10 classes, with 6000 images per class. There are
50,000 training images and 10,000 test images.
ImageNet: This dataset contains as much as 1000 classes
of objects with nearly 1.2 million training images and 50
thousand validation images.
(a) Light CNN training
(b) Light CNN quantization
Figure 6: (a) is the training curves of the light CNN. (b) is
the training curves in 5 iterations of SLQ quantization on
light CNN.
Light CNN ref
Light CNN SLQ
ResNet20 ref
ResNet20 SLQ
Table 1: Experiment results of 5-bit SLQ on CIFAR-10.
Results for SLQ
SLQ Results on CIFAR-10
We use the light CNN (three
convolutional layers and three fully connected layers) offered in Caffe and ResNet20 
to conduct the classiﬁcation on CIFAR-10. The light CNN is
trained from scratch (as shown in Figure 6a). After 5 iterations the trained full-precision light CNN model is quantized
into 5-bit low-precision model (shown in Figure 6b). The
quantization loss of each iteration is decreasing. The quantization results of two networks are shown in Table 1. Both of
the two networks enjoy accuracy increase after quantization
SLQ Results on ImageNet
We apply the proposed SLQ
approach to various popular models on ImageNet including:
AlexNet , VGG-
16 , GoogleNet and Resinet-18 . All these fullprecision networks are quantized into 5-bit low precision
ones. The setting of the parameters is shown in Table 2.
The cluster partition ways of the four networks are the same
which means that our approach is easier to implement and is
robust on different DNN architectures. The results are shown
in Table 3. The 5-bit CNN models quantized by SLQ have
better performance in the ImageNet large scale classiﬁcation
task both in Top1 and Top5 accuracy than full-precision references. We also compare our SLQ results with INQ . Our approach achieves improvement in all of
the Top1 accuracy and most of the Top5 accuracy. It shows
that considering the distribution of weights during quantization is very important and the loss based partition also contributes to the increase.
Batch size
Weight decay
Cluster number
Table 2: Parameter settings of networks.
Cluster number
Top1 accuracy
Top5 accuracy
Increase in top-1/top-5 error
AlexNet ref
AlexNet INQ
ALexNet SLQ
0.46%/0.30%
3.69%/1.35%
Googlenet ref
Googlenet INQ
Googlenet SLQ
0.21%/0.16%
Resinet18 ref
Resinet18 INQ
Resinet18 SLQ
0.82%/0.46%
Table 3: Experiment results of SLQ method on ImageNet.
Centroid number
Top-1 accuracy
Top-5 accuracy
Increase in top-1/top-5 error
3.69%/1.35%
2.64%/0.60%
-0.16%/-1.10%
Table 4: Experiment results of bit-width change on ImageNet.
Cluster number
Top-1 accuracy
Top-5 accuracy
Increase in top-1/top-5 error
VGG16 non-linear
3.69%/1.35%
VGG16 linear
3.31%/1.22%
Table 5: Experiment results of centroid initialization of SLQ.
Cluster number
Top-1 accuracy
Top-5 accuracy
Increase in top-1/top-5 error
AlexNet ref
AlexNet ESLQ1
0.16%/0.08%
AlexNet ESLQ2
0.32%/0.05%
VGG16 ESLQ1
2.63%/0.85%
VGG16 ESLQ2
3.41%/1.21%
Table 6: Experiment results of ESLQ method on ImageNet.
Results for SLQ with Low-bit Setting
In this experiment, we test our SLQ approach in different bitwidth settings. We use VGG-16 as our test model. Except
for the original 5-bit quantization result, we present 4-bit
and 3-bit results which is shown in Table 4. As 5-bit compressed model, our 4-bit compressed model can also have
good performance in both Top-1 and Top-5. However, for
bit-width as low as 3 which means that the centroid number
is 5, the accuracy of the compressed model drops a little.
The loss based partition step in SLQ is related to the number
of centroids. If the centroid number is big enough(for instance 17 and 9), we can have more iterations during quantization. While if the centroid number is small(for instance 5),
we will have less iterative quantization steps and the quantization loss in the last quantization step is big. That is why
the accuracy of the 3-bit compressed model is slightly lower
than reference full-precision VGG-16 model. Thus, we have
to try other ways (e.g. our proposed MLQ) to conduct extremely low-bit quantization. The partition ways of the experiments are described bellow:
5-bit VGG-16 cluster partitions are {5, 4, 4, 2, 2};
4-bit VGG-16 cluster partitions are {3, 2, 2, 2};
3-bit VGG-16 cluster partitions are {2, 2, 1}.
Results for Centroid Initialization
We conduct experiments to show the effect of centroid initialization on our SLQ approach. We choose two kinds of
centroid initialization ways. One is linear (linear decaying)
and the other is non-linear (exponential decaying).
We choose VGG-16 as our test model. The results are
shown in Table 5. The accuracy of the model quantized by
SLQ with non-linear initialization is higher than the accuracy of SLQ with linear initialization. The centroid of the
clusters to be quantized in the last iteration is smaller, so the
number of weights is also smaller. This leads to the smaller
quantization loss in the last iteration. Thus, we adopt nonlinear initialization in all of our experiments.
Results for ESLQ
In this experiment, we test our ESLQ approach. The highlights of our ESLQ approach is to quantize the weights to
oriented type: t-centroids. To test it, we choose two types:
one is scientiﬁc notation with two signiﬁcant ﬁgures and
the other is either power of two or zero. The experiment
results are shown in Table 6. In Table 6, ESLQ1 indicates
the scientiﬁc notation and ESLQ2 indicates the power of 2.
The model quantized with ESLQ in both of the two situations have accuracy increase which shows the effectiveness
Results for MLQ
We quantize light CNN and ResNet20 into ternary networks
on CIFAR-10. In our experiments, we train the networks on
CIFAR-10 without using data augmentation. The results are
shown in Table 7. The accuracy of the ternary light CNN
and ternary ResNet20 decrease little compared with the fullprecision ones.
AlexNet model is quantized into ternary network on ImageNet by MLQ. We compare the proposed MLQ approach
with TWN and TTQ (shown in Table 8). Both TWN and TTQ add bach
normalization layers by which the baseline of AlexNet can
reach up to 60%. Moreover, the batch normalization layers
also contribute to the convergency of their network during
training. In TTQ, they do not quantize the ﬁrst convolutional layer and the last fully connected layer, that is another
reason of their high performance. Different from them, our
MLQ approach is more robust, we do not change the architecture of the network (without adding batch normalization
layer) and quantize all of layers in ALexNet which can still
achieve comparable results. Another method FGQ conducts ternary quantization without additional training. Our method outperforms FGQ, though we
have more training time cost.
Light CNN ref
Light CNN MLQ
2(ternary)
ResNet20 ref
ResNet20 MLQ
2(ternary)
Table 7: Experiment results of MLQ on CIFAR-10.
Table 8: Experiment results of MLQ on ImageNet.
Compression Ratio and Acceleration
The compression ratio can be easily computed by the bitwidth of the networks. The compression ratio of the 5bit compressed AlexNet is 6×. Besides, the proposed approach can be combined with the pruning strategy to further compress the network. The 5-bit pruned
AlexNet is 53× compressed without accuracy loss. Since
current BLAS libraries on CPU and GPU do not support indirect look-up and relative indexing, accelerators designed
for quantized models can be adopted.
For training time, with one NVIDIA TITAN Xp, the proposed approach takes about 28 hours to accomplish 5-bit
AlexNet quantization on ImageNet.
Conclusion
In this paper, we propose single-level quantization (SLQ)
and multi-level quantization (MLQ) by considering the network quantization from both width and depth level. By
taking the distribution of the parameters into account, the
SLQ obtains accuracy gain in the high-bit quantization of
state-of-the-art networks on two datasets. Besides, the MLQ
achieves impressive results in extremely low-bit quantization (ternary) without changing the architecture of networks.
Acknowledgements
This work is supported in part by NSFC (61425011,
61720106001,
61472234),
(17XD1401900) and Tencent research grant. We would like
to thank Haoyang Yu and Xin Liu from Shenzhen Tencent
Computer System Co.,Ltd. for their valuable discussions
about the paper.