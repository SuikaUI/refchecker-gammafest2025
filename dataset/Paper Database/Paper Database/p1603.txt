Interpretable Machine Learning – A Brief
History, State-of-the-Art and Challenges⋆
Christoph Molnar1[0000−0003−2331−868X], Giuseppe
Casalicchio1[0000−0001−5324−5966], and Bernd Bischl1[0000−0001−6002−6980]
Department of Statistics, LMU Munich
Ludwigstr. 33, 80539 Munich, Germany
 
Abstract. We present a brief history of the ﬁeld of interpretable machine learning (IML), give an overview of state-of-the-art interpretation
methods and discuss challenges. Research in IML has boomed in recent
years. As young as the ﬁeld is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s.
Recently, many new IML methods have been proposed, many of them
model-agnostic, but also interpretation techniques speciﬁc to deep learning and tree-based ensembles. IML methods either directly analyze model
components, study sensitivity to input perturbations, or analyze local or
global surrogate approximations of the ML model. The ﬁeld approaches
a state of readiness and stability, with many methods not only proposed
in research, but also implemented in open-source software. But many
important challenges remain for IML, such as dealing with dependent
features, causal interpretation, and uncertainty estimation, which need
to be resolved for its successful application to scientiﬁc problems. A further challenge is a missing rigorous deﬁnition of interpretability, which is
accepted by the community. To address the challenges and advance the
ﬁeld, we urge to recall our roots of interpretable, data-driven modeling
in statistics and (rule-based) ML, but also to consider other areas such
as sensitivity analysis, causal inference, and the social sciences.
Keywords: Interpretable Machine Learning · Explainable Artiﬁcial Intelligence
Introduction
Interpretability is often a deciding factor when a machine learning (ML) model
is used in a product, a decision process, or in research. Interpretable machine
learning (IML)1 methods can be used to discover knowledge, to debug or justify
⋆This project is funded by the Bavarian State Ministry of Science and the Arts and
coordinated by the Bavarian Research Institute for Digital Transformation (bidt)
and supported by the German Federal Ministry of Education and Research (BMBF)
under Grant No. 01IS18036A. The authors of this work take full responsibilities for
its content.
1 Sometimes the term Explainable AI is used.
 
Molnar et al.
the model and its predictions, and to control and improve the model . In
this paper, we take a look at the historical building blocks of IML and give an
overview of methods to interpret models. We argue that IML has reached a state
of readiness, but some challenges remain.
A Brief History of IML
A lot of IML research happened in the last couple of years. But learning interpretable models from data has a much longer tradition. Linear regression
models were used by Gauss, Legendre, and Quetelet as early as
the beginning of the 19th century and have since then grown into a vast array of
regression analysis tools , for example, generalized additive models 
and elastic net . The philosophy behind these statistical models is usually
to make certain distributional assumptions or to restrict the model complexity
beforehand and thereby imposing intrinsic interpretability of the model.
In ML, a slightly diﬀerent modeling approach is pursued. Instead of restricting the model complexity beforehand, ML algorithms usually follow a non-linear,
non-parametric approach, where model complexity is controlled through one or
more hyperparameters and selected via cross-validation. This ﬂexibility often
results in less interpretable models with good predictive performance. A lot of
ML research began in the second half of the 20th century with research on, for
example, support vector machines in 1974 , early important work on neural
networks in the 1960s , and boosting in 1990 . Rule-based ML, which
covers decision rules and decision trees, has been an active research area since
the middle of the 20th century .
While ML algorithms usually focus on predictive performance, work on interpretability in ML – although underexplored – has existed for many years. The
built-in feature importance measure of random forests was one of the important IML milestones.2 In the 2010s came the deep learning hype, after a deep
neural network won the ImageNet challenge. A few years after that, the IML
ﬁeld really took oﬀ , judging by the frequency of the search terms
”Interpretable Machine Learning” and ”Explainable AI” on Google (Figure 1,
right) and papers published with these terms (Figure 1, left). Since then, many
model-agnostic explanation methods have been introduced, which work for different types of ML models. But also model-speciﬁc explanation methods have
been developed, for example, to interpret deep neural networks or tree ensembles.
Regression analysis and rule-based ML remain important and active research areas to this day and are blending together (e.g., model-based trees , RuleFit
 ). Many extensions of the linear regression model exist and new
extensions are proposed until today . Rule-based ML also remains
an active area of research (for example, ). Both regression models and
2 The random forest paper has been cited over 60,000 times and there are many papers improving the importance measure
( ) which are also cited frequently.
IML - History, Methods, Challenges
Fig. 1. Left: Citation count for research articles with keywords “Interpretable Machine Learning” or “Explainable AI” on Web of Science .
Right: Google search trends for “Interpretable Machine Learning” and “Explainable
AI” .
rule-based ML serve as stand-alone ML algorithms, but also as building blocks
for many IML approaches.
IML has reached a ﬁrst state of readiness. Research-wise, the ﬁeld is maturing in
terms of methods surveys , further consolidation of terms
and knowledge , and work about deﬁning interpretability or
evaluation of IML methods . We have a better understanding of
weaknesses of IML methods in general , but also speciﬁcally for methods
such as permutation feature importance , Shapley values ,
counterfactual explanations , partial dependence plots and saliency
maps . Open source software with implementations of various IML methods
is available, for example, iml and DALEX for R and Alibi and
InterpretML for Python. Regulation such as GDPR and the need for ML
trustability, transparency and fairness have sparked a discussion around further
needs of interpretability . IML has also arrived in industry , there are
startups that focus on ML interpretability and also big tech companies oﬀer
software .
IML Methods
We distinguish IML methods by whether they analyze model components, model
sensitivity3, or surrogate models, illustrated in Figure 4.4
3 Not to be confused with the research ﬁeld of sensitivity analysis, which studies the
uncertainty of outputs in mathematical models and systems. There are methodological overlaps (e.g., Shapley values), but also diﬀerences in methods and how input
data distributions are handled.
4 Some surveys distinguish between ante-hoc (or transparent design, white-box models,
inherently interpretable model) and post-hoc IML method, depending on whether
Molnar et al.
Fig. 2. Some IML approaches work by assigning meaning to individual model components (left), some by analyzing the model predictions for perturbations of the data
(right). The surrogate approach, a mixture of the two other approaches, approximates
the ML model using (perturbed) data and then analyzes the components of the interpretable surrogate model.
Analyzing Components of Interpretable Models
In order to analyze components of a model, it needs to be decomposable into
parts that we can interpret individually. However, it is not necessarily required
that the user understands the model in its entirety (simulatability ). Component analysis is always model-speciﬁc, because it is tied to the structure of
the model.
Inherently interpretable models are models with (learned) structures and
(learned) parameters which can be assigned a certain interpretation. In this context, linear regression models, decision trees and decision rules are considered
to be interpretable . Linear regression models can be interpreted by analyzing components: The model structure, a weighted sum of features, allows to
interpret the weights as the eﬀects that the features have on the prediction.
Decision trees and other rule-based ML models have a learned structure
(e.g.,“IF feature x1 > 0 and feature x2 ∈{A, B}, THEN predict 0.6”). We can
interpret the learned structure to trace how the model makes predictions.
This only works up to a certain point in high-dimensional scenarios. Linear
regression models with hundreds of features and complex interaction terms or
deep decision trees are not that interpretable anymore. Some approaches aim
to reduce the parts to be interpreted. For example, LASSO shrinks the
coeﬃcients in a linear model so that many of them become zero, and pruning
techniques shorten trees.
Analyzing Components of More Complex Models
With a bit more eﬀort, we can also analyze components of more complex blackbox models. 5 For example, the abstract features learned by a deep convolutional
neural network (CNN) can be visualized by ﬁnding or generating images that
interpretability is considered at model design and training or after training, leaving
the (black-box) model unchanged. Another category separates model-agnostic and
model-speciﬁc methods.
5 This blurs the line between an “inherently interpretable” and a “black-box” model.
IML - History, Methods, Challenges
activate a feature map of the CNN . For the random forest, the minimal
depth distribution and the Gini importance analyze the structure of
the trees of the forest and can be used to quantify feature importance. Some
approaches aim to make the parts of a model more interpretable with, for example, a monotonicity constraint or a modiﬁed loss function for disentangling
concepts learned by a convolutional neural network .
If an ML algorithm is well understood and frequently used in a community,
like random forests in ecology research , model component analysis can be
the correct tool, but it has the obvious disadvantage that it is tied to that speciﬁc
model. And it does not combine well with the common model selection approach
in ML, where one usually searches over a large class of diﬀerent ML models via
cross-validation.
Explaining Individual Predictions
Methods that study the sensitivity of an ML model are mostly model-agnostic
and work by manipulating input data and analyzing the respective model predictions. These IML methods often treat the ML model as a closed system that
receives feature values as an input and produces a prediction as output. We
distinguish between local and global explanations.
Local methods explain individual predictions of ML models. Local explanation methods have received much attention and there has been a lot of innovation in the last years. Popular local IML methods are Shapley values and
counterfactual explanations . Counterfactual explanations explain predictions in the form of what-if scenarios, which builds on a rich tradition
in philosophy . According to ﬁndings in the social sciences , counterfactual explanations are “good” explanations because they are contrastive and focus
on a few reasons. A diﬀerent approach originates from collaborative game theory: The Shapley values provide an answer on how to fairly share a payout
among the players of a collaborative game. The collaborative game idea can be
applied to ML where features (i.e., the players) collaborate to make a prediction
(i.e., the payout) .
Some IML methods rely on model-speciﬁc knowledge to analyze how changes
in the input features change the output. Saliency maps, an interpretation method
speciﬁc for CNNs, make use of the network gradients to explain individual classi-
ﬁcations. The explanations are in the form of heatmaps that show how changing
a pixel can change the classiﬁcation. The saliency map methods diﬀer in how
they backpropagate . Additionally, model-agnostic versions
 exist for analyzing image classiﬁers.
Explaining Global Model Behavior
Global model-agnostic explanation methods are used to explain the expected
model behavior, i.e., how the model behaves on average for a given dataset.
A useful distinction of global explanations are feature importance and feature
Molnar et al.
Feature importance ranks features based on how relevant they were for the
prediction. Permutation feature importance is a popular importance measure, originally suggested for random forests . Some importance measures
rely on removing features from the training data and retraining the model .
An alternative are variance-based measures . See for an overview of
importance measures.
The feature eﬀect expresses how a change in a feature changes the predicted
outcome. Popular feature eﬀect plots are partial dependence plots , individual conditional expectation curves , accumulated local eﬀect plots , and
the functional ANOVA . Analyzing inﬂuential data instances, inspired by
statistics, provides a diﬀerent view into the model and describes how inﬂuential
a data point was for a prediction .
Surrogate Models
Surrogate models6 are interpretable models designed to “copy” the behavior of
the ML model. The surrogate approach treats the ML model as a black-box and
only requires the input and output data of the ML model (similar to sensitivity
analysis) to train a surrogate ML model. However, the interpretation is based on
analyzing components of the interpretable surrogate model. Many IML methods
are surrogate model approaches and diﬀer, e.g., in the
targeted ML model, the data sampling strategy, or the interpretable model that
is used. There are also methods for extracting, e.g., decision rules from speciﬁc
models based on their internal components such as neural network weights .
LIME is an example of a local surrogate method that explains individual
predictions by learning an interpretable model with data in proximity to the
data point to be explained. Numerous extensions of LIME exist, which try to ﬁx
issues with the original method, extend it to other tasks and data, or analyze
its properties .
Challenges
This section presents an incomplete overview of challenges for IML, mostly based
Statistical Uncertainty and Inference
Many IML methods such as permutation feature importance or Shapley values
provide explanations without quantifying the uncertainty of the explanation.
The model itself, but also its explanations, are computed from data and hence
are subject to uncertainty. First research is working towards quantifying uncertainty of explanations, for example, for feature importance , layer-wise
relevance propagation , and Shapley values .
6 Surrogate models are related to knowledge distillation and the teacher-student
IML - History, Methods, Challenges
In order to infer meaningful properties of the underlying data generating
process, we have to make structural or distributional assumptions. Whether it
is a classical statistical model, an ML algorithm or an IML procedure, these
assumptions should be clearly stated and we need better diagnostic tools to test
them. If we want to prevent statistical testing problems such as p-hacking to
reappear in IML, we have to become more rigorous in studying and quantifying
the uncertainty of IML methods. For example, most IML methods for feature
importance are not adapted for multiple testing, which is a classic mistake in a
statistical analysis.
Causal Interpretation
Ideally, a model should reﬂect the true causal structure of its underlying phenomena, to enable causal interpretations. Arguably, causal interpretation is usually the goal of modeling if ML is used in science. But most statistical learning
procedures reﬂect mere correlation structures between features and analyze the
surface of the data generation process instead of its true inherent structure.
Such causal structures would also make models more robust against adversarial
attacks , and more useful when used as a basis for decision making. Unfortunately, predictive performance and causality can be conﬂicting goals. For
example, today’s weather directly causes tomorrow’s weather, but we might only
have access to the feature “wet ground”. Using “wet ground” in the prediction
model for “tomorrow’s weather” is useful as it has information about “today’s
weather”, but we are not allowed to interpret it causally, because the confounder
“today’s weather” is missing from the ML model. Further research is needed to
understand when we are allowed to make causal interpretations of an ML model.
First steps have been made for permutation feature importance and Shapley
values .
Feature Dependence
Feature dependence introduces problems with attribution and extrapolation.
Attribution of importance and eﬀects of features becomes diﬃcult when features are, for example, correlated and therefore share information. Correlated
features in random forests are preferred and attributed a higher importance
 . Many sensitivity analysis based methods permute features. When the
permuted feature has some dependence with another feature, this association
is broken and the resulting data points extrapolate to areas outside the distribution. The ML model was never trained on such combinations and will likely
not be confronted with similar data points in an application. Therefore, extrapolation can cause misleading interpretations. There have been attempts to
“ﬁx” permutation-based methods, by using a conditional permutation scheme
that respects the joint distribution of the data . The change from
unconditional to conditional permutation changes the respective interpretation
method , or, in worst case, can break it .
Molnar et al.
Deﬁnition of Interpretability
A lack of deﬁnition for the term ”interpretability” is a common critique of the
ﬁeld . How can we decide if a new method explains ML models better without a satisfying deﬁnition of interpretability? To evaluate the predictive performance of an ML model, we simply compute the prediction error
on test data given the groundtruth label. To evaluate the interpretability of
that same ML model is more diﬃcult. We do not know what the groundtruth
explanation looks like and have no straightforward way to quantify how interpretable a model is or how correct an explanation is. Instead of having
one groundtruth explanation, various quantiﬁable aspects of interpretability are
emerging .
The two main ways of evaluating interpretability are objective evaluations,
which are mathematically quantiﬁable metrics, and human-centered evaluations,
which involve studies with either domain experts or lay persons. Examples of
aspects of interpretability are sparsity, interaction strength, ﬁdelity (how well
an explanation approximates the ML model), sensitivity to perturbations, and
a user’s ability to run a model on a given input (simulatability). The challenge
ahead remains to establish a best practice on how to evaluate interpretation
methods and the explanations they produce. Here, we should also look at the
ﬁeld of human-computer interaction.
More Challenges Ahead
We focused mainly on the methodological, mathematical challenges in a rather
static setting, where a trained ML model and the data are assumed as given
and ﬁxed. But ML models are usually not used in a static and isolated way,
but are embedded in some process or product, and interact with people. A more
dynamic and holistic view of the entire process, from data collection to the ﬁnal
consumption of the explained prediction is needed. This includes thinking how
to explain predictions to individuals with diverse knowledge and backgrounds
and about the need of interpretability on the level of an institution or society in
general. This covers a wide range of ﬁelds, such as human-computer interaction,
psychology and sociology. To solve the challenges ahead, we believe that the ﬁeld
has to reach out horizontally – to other domains – and vertically – drawing from
the rich research in statistics and computer science.