The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Improving GAN with Neighbors Embedding and Gradient Matching
Ngoc-Trung Tran,∗Tuan-Anh Bui,∗Ngai-Man Cheung
ST Electronics - SUTD Cyber Security Laboratory
Singapore University of Technology and Design
We propose two new techniques for training Generative Adversarial Networks (GANs) in the unsupervised setting. Our
objectives are to alleviate mode collapse in GAN and improve the quality of the generated samples. First, we propose
neighbor embedding, a manifold learning-based regularization to explicitly retain local structures of latent samples in
the generated samples. This prevents generator from producing nearly identical data samples from different latent samples, and reduces mode collapse. We propose an inverse t-
SNE regularizer to achieve this. Second, we propose a new
technique, gradient matching, to align the distributions of the
generated samples and the real samples. As it is challenging
to work with high-dimensional sample distributions, we propose to align these distributions through the scalar discriminator scores. We constrain the difference between the discriminator scores of the real samples and generated ones. We
further constrain the difference between the gradients of these
discriminator scores. We derive these constraints from Taylor approximations of the discriminator function. We perform
experiments to demonstrate that our proposed techniques are
computationally simple and easy to be incorporated in existing systems. When Gradient matching and Neighbour embedding are applied together, our GN-GAN achieves outstanding
results on 1D/2D synthetic, CIFAR-10 and STL-10 datasets,
e.g. FID score of 30.80 for the STL-10 dataset. Our code is
available at: 
Introduction
Generative Adversarial Networks (GANs) are popular methods for training
generative models. GAN training is a two-player minimax
game between the discriminator and the generator. While the
discriminator learns to distinguish between the real and generated (fake) samples, the generator creates samples to confuse the discriminator to accept its samples as “real”. This is
an attractive approach. However, stabilizing the training of
GAN is still an on-going important research problem.
Mode collapse is one of the most challenging issues when
training GANs. Many advanced GANs have been proposed
to improve the stability . All rights reserved.
2016; Arjovsky, Chintala, and Bottou 2017; Gulrajani et al.
2017). However, mode collapse is still an issue.
In this work, we propose two techniques to improve GAN
training. First, inspired by t-distribution stochastic neighbors embedding (t-SNE) , which
is a well-known dimensionality reduction method, we propose an inverse t-SNE regularizer to reduce mode collapse. Speciﬁcally, while t-SNE aims to preserve the structure of the high-dimensional data samples in the reduceddimensional manifold of latent samples, we reverse the procedure of t-SNE to explicitly retain local structures of latent samples in the high-dimensional generated samples.
This prevents generator from producing nearly identical data
samples from different latent samples, and reduces mode
collapse. Second, we propose a new objective function for
the generator by aligning the real and generated sample distributions, in order to generate realistic samples. We achieve
the alignment via minimizing the difference between the discriminator scores of the real samples and generated ones. By
using the discriminator and its scores, we can avoid working with high-dimensional data distribution. We further constrain the difference between the gradients of discriminator
scores. We derive these constraints from Taylor approximation of the discriminator function. Our principled approach
is signiﬁcantly different from the standard GAN : our generator does not attempt to directly
fool the discriminator; instead, our generator produces fake
samples that have similar discriminator scores as the real
samples. We found that with this technique the distribution
of the generated samples approximates well that of the real
samples, and the generator can produce more realistic samples.
Related Works
Addressing issues of GANs , including
gradient vanishing and mode collapse, is an important research topic. A popular direction is to focus on improving the discriminator objective. The discriminator can be
formed via the f-divergence , or distance metrics . And the generator is trained by fooling the discriminator via the zero-sum game. Many methods in this direction have to regularize their discriminators; otherwise, they would cause instability issues, as the
discriminator often converges much faster than the generator. Some regularization techniques are weight-clipping , gradient penalty constraints , consensus constraint, , or spectral norm
 . However, over-constraint of the discriminator may cause the cycling issues .
Issues of GAN can also be tackled via the optimizer
regularization: changing optimization process , using two-time scale update rules for better convergence , or averaging network parameters
 .
Regularizing the generator is another direction: i) It can be
achieved by modifying the generator objective function with
feature matching or discriminatorscore distance ii) Or, using
Auto-Encoders (AE) or latent codes to regularize the generator. AAE uses AE to constrain
the generator. The goal is to match the encoded latent distribution to some given prior distribution by the minimax
game. The problem of AAE is that pixel-wise reconstruction with ℓ2-norm would cause the blurry issue. And the
minimax game on the latent samples has the same problems
(e.g., mode collapse) as on the data samples. It is because
AE alone is not powerful enough to overcome these issues.
VAE/GAN combined VAE and GAN
into one single model and used feature-wise distance for
the reconstruction to avoid the blur. The generator is regularized in the VAE model to reduce the mode collapse.
Nevertheless, VAE/GAN has the similar limitation of VAE
 , including re-parameterization
tricks for back-propagation, or, requirement to access to an
exact functional form of prior distribution. ALI and BiGAN jointly train the data/latent samples in GAN framework. This method can learn the AE model implicitly after
training. MDGAN required two discriminators for two separate steps: manifold and diffusion. The
manifold step manages to learn a good AE. The diffusion
step is similar to the original GAN, except that the constructed samples are used as real samples instead. InfoGAN
 learned the disentangled representation
by maximizing the mutual information for inducing latent
codes. MMGAN makes strong assumption that manifolds of real and fake samples are spheres.
First, it aligns real and fake sample statistics by matching
the two manifold spheres (centre and radius), and then it applies correlation matrix to reduce mode collapse. Dist-GAN
 constrains the generator by
the regularized auto-encoder. Furthermore, the authors use
the reconstructed samples to regularize the convergence of
the discriminator.
Auto-encoder can be also used in the discriminator objectives. EBGAN introduces
the energy-based model, in which the discriminator is considered as the energy function minimized via reconstruction
errors. BEGAN extends EBGAN by optimizing Wasserstein distance between
AE loss distributions.
Proposed method
Our proposed system with gradient matching (GM) and
neighbor embedding (NE) constraints, namely GN-GAN,
consists of three main components: the auto-encoder, the
discriminator, and the generator. In our model, we ﬁrst train
the auto-encoder, then the discriminator and ﬁnally the generator as presented in Algorithm 1.
Algorithm 1 Our GN-GAN model
1: Initialize discriminator, encoder and generator D, E, G respectively. Niter is the number of iterations.
x ←Random mini-batch of m data points from dataset.
z ←Random n samples from noise distribution Pz
// Training the auto-encoder using x and z by Eqn. 1
E, G ←min VAE(E, G)
// Training discriminator according to Eqn. 7 on x, z
D ←max VD(D, G)
// Training the generator on x, z according to Eqn. 13.
G ←min VG(D, G)
11: until Niter
12: return D, E, G
Neighbors embedding constraint for Auto-encoder
We use auto-encoder (AE) in our model for two reasons: i)
to prevent the generator from being severely collapsed. ii)
to regularize the generator in producing samples that resemble real ones. However, using AE alone is not adequate to
avoid mode collapse, especially for high-dimensional data.
Therefore, we propose additional regularization as in Eq. 1:
VAE(E, G) = ||x −G(E(x))||2 + λrVR(E, G)
Eq. 1 is the objective of our regularized AE. The ﬁrst term
is reconstruction error in conventional AE. The second term
VR(E, G) is our proposed neighbors embedding constraint,
to be discussed. Here, G is GAN generator (decoder in AE),
E is the encoder and λr is a constant.
Mode collapse is a failure case of GAN when the generator often generates similar samples. The diversity of generated samples is small compared with those of the original
dataset. As discussed in previous work ), with mode collapse, the generator would
map two far-apart latent samples to nearby data points in
the high-dimensional data space with high probability. This
observation motivates our idea to constrain the distances between generated data points in order to alleviate mode collapse. In particular, the data point distances and the corresponding latent sample distances should be consistent.
The motivation of our neighbors-embedding constraint
VR(E, G) is to constrain the relative distance among data
points and their corresponding latent points within the data
and latent manifold respectively (Fig. 1). In our model, we
apply the probabilistic relative distance (PRDist) in t-SNE
Figure 1: Illustration of the neighbor-embedding (NE) constraint. NE regularizes the generator to produce highdimensional data samples such that latent sample distance
and data sample distance are consistent.
 , which takes into account the distributions of latent sample structure and data sample structure. t-SNE has been shown to preserve both the local structure of data space (the relation inside each cluster) and the
global structure (the relation between each pair of clusters).
Notably, our method applies PRDist in the reverse direction of t-SNE for different purpose. While t-SNE aims to
preserve signiﬁcant structures of the high-dimensional data
in the reduced-dimensional samples, in our work, we aim
to preserve the structures in low-dimensional latent samples
in its high-dimensional mappings via the generator. Speciﬁcally, the objective is as shown in Eq. 2:
VR(E, G) =
pi,j log pi,j
The probability distribution of latent structure pi,j is a
joint, symmetric distribution, computed as below:
pi,j = pi|j + pj|i
pi|j and pj|i are the conditional probabilities, whose center
points are zj and zi respectively. Here, i and j are indices of
i-th and j-th samples respectively in a mini-batch of training
data. Accordingly, zi and zj are i-th and j-th latent samples.
n is the number of samples in the mini-batch. The conditional probability pj|i is given by:
(1 + ||zj −zi||2/2σ2
k̸=i(1 + ||zk −zi||2/2σ2z)−1
where σz is the variance of all pairwise distances in a minibatch of latent samples. Similar to t-SNE method, the joint
distribution pi,j is to prevent the problem of outliers in highdimensional space.
Similarly, the probability distribution of data sample
structure qi,j is the joint, symmetric computed from two
conditional probabilities as below:
qi,j = qi|j + qj|i
where qj|i is the conditional probability of pairwise distance
between samples G(zj) and the center point G(zi), computed as follow:
(1 + ||G(zj) −G(zi)||2/2σ2
k̸=i(1 + ||G(zk) −G(zi)||2/2σ2x)−1
σx is the variance of all pairwise distances of data samples
in the mini-batch. The regularization term VR(E, G) is the
dissimilarity between two joint distributions: pi,j and qi,j,
where each distribution represents the neighbor distance distribution. Similar to t-SNE, we set the values of pi,i and qj,j
to zero. The dissimilarity is Kullback-Leibler (KL) divergence as in Eq. 2. {zi} is a merged dataset of encoded and
random latent samples, and {G(zi)} is a merged dataset of
reconstruction and generated samples. Here, the reconstruction samples and their latent samples are considered as the
anchor points of data and latent manifolds respectively to
regularize the generation process.
Discriminator objective
= (1 −α)Ex log D(x) + αVC + Ez log(1 −D(G(z))
Our discriminator objective is shown in Eq. 7. Our model
considers the reconstructed samples as “real” represented by
the term VC = Ex log D(G(E(x)), so that the gradients
from discriminator are not saturated too quickly. This constraint slows down the convergence of discriminator, similar
goal as , and . In our method,
we use a small weight for VC with α = 0.05 for the discriminator objective. We observe that VC is important at the beginning of training. However, towards the end, especially for
complex image datasets, the reconstructed samples may not
be as good as real samples, resulting in low quality of generated images. Here, E is the expectation, λp is a constant,
VP = Ex(||∇ˆxD(ˆx)|| −1)2 and ˆx = µx + (1 −µ)G(z), µ
is a uniform random number µ ∈U . VP enforces sufﬁcient gradients from the discriminator even when approaching convergence. Fig. 2 illustrates gradients at convergence
We also apply hinge loss similar to by
replacing log(D(x)) with min(0, −1 + D(x)). We empirically found that hinge loss could also improve the quality
of generated images in our model. Here, because D(x) ∈
(0, 1), the hinge loss version of Eq. 7 (ignore constants) is
as follows:
D(D, G) = (1 −α)ExD(x) + αVC −EzD(G(z)) −λpVP
Generator objective with gradient matching
In this work, we propose to train the generator via aligning
distributions of generated samples and real samples. However, it is challenging to work with high-dimensional sample
distribution. We propose to overcome this issue in GAN by
using the scalar discriminator scores. In GAN, the discriminator differentiates real and fake samples. Thus, the discriminator score D(x) can be viewed as the probability that sample x drawn from the real data distribution. Although exact
form of D(x) is unknown, but the scores D(x) at some data
points x (from training data) can be computed via the discriminator network. Therefore, we align the distributions by
minimizing the difference between discriminator scores of
real and generated samples. In addition, we constrain the
gradients of these discriminator scores. These constraints
can be derived from Taylor approximation of discriminator
functions as followings.
Assume that the ﬁrst derivative of D exists, and the training set has data samples {x}. For a sample point s, by ﬁrstorder Taylor expansion (TE), we can approximate D(s) with
TE at a data point x:
D(s) = D(x) + ∇xD(x)(s −x) + ϵ(s, x)
Here ϵ(.) is the TE approximation error. Alternatively, we
can approximate D(s) with TE at a generated sample G(z):
D(s) = D(G(z)) + ∇xD(G(z))(s −G(z)) + ϵ(s, G(z))
Our goal is to enforce the distribution of generated sample p(G(z)) to be similar to that of real sample p(x). For a
given s, its discriminator score D(s) can be approximated
by ﬁrst-order TE at x with error ϵ(s, x). Note that, here
we deﬁne ϵ(s, x) to be the approximation error of D(s)
with ﬁrst-order TE at point x. Likewise, ϵ(s, G(z)) is the
approximation error of D(s) with ﬁrst-order TE at point
G(z). If x and G(z) were from the same distribution, then
Exϵ(s, x) ≈Ezϵ(s, G(z)). Therefore, we propose to enforce
Exϵ(s, x) = Ezϵ(s, G(z)) when training the generator. Note
that Ex(D(s)) = Ez(D(s)) = D(s), because D(s) is a constant and is independent of x and z. Therefore, we propose to
enforce Ex(D(s)) −Exϵ(s, x) = Ez(D(s)) −Ezϵ(s, G(z))
in order to align p(G(z)) to real sample distribution p(x).
From Eq. 9, we have:
Ex(D(s)) −Exϵ(s, x)
= Ex(D(x)) + Ex(∇xD(x)(s −x))
= Ex(D(x)) + Ex∇xD(x)s −Ex∇xD(x)x
From Eq. 10, we have:
Ez(D(s)) −Ezϵ(s, G(z))
= Ez(D(G(z))) + Ez∇xD(G(z))s −Ez∇xD(G(z))G(z)
To equate Eqs. 11 and 12, we enforce equality of corresponding terms. This leads to minimization of the following
objective function for the generator:
VG(D, G) = ||ExD(x) −EzD(G(z))||
m||Ex(∇xD(x)) −Ez(∇xD(G(z)))||2
m||Ex(∇xD(x)T x) −Ez(∇xD(G(z))T G(z))||2
Figure 2: We compare our method and Dist-GAN on the 1D synthetic dataset of three
Gaussian modes. Figures are the last frames of the demo
videos (can be found here: 
The blue curve is discriminator scores, the green and orange
modes are the training data the generated data respectively.
Here, we use ℓ1-norm for the ﬁrst term of generator objective, and ℓ2-norm for two last terms. Empirically, we observe that using ℓ2-norm is more stable than using ℓ1-norm.
m = 1.0. In practice, our method is more stable when we implement Ex(∇xD(x)) as Ex||∇xD(x)|| and
Ex(∇xD(x)T x) as Ex||∇xD(x)T x|| in the second and third
term of Eq. 13. Note that this proposed objective can be used
in other GAN models. Note also that a recent work has also used the discriminator score
as constraint. However, our motivation and formulation are
signiﬁcantly different. In the experiment, we show improved
performance compared to .
Experimental Results
Synthetic 1D dataset
For 1D synthetic dataset, we compare our model to Dist-
GAN , a recent state-of-theart GAN. We use the code ( 
for this 1D experiment. Here, we construct the 1D synthetic
data with 3 Gaussian modes (green) as shown in Fig. 2. It is
more challenging than the one-mode demo by Dist-GAN.
We use small networks for both methods. Speciﬁcally, we
create the encoder and generator networks with three fullyconnected layers and the discriminator network with two
fully-connected layers. We use ReLU for hidden layers and
sigmoid for the output layer of the discriminator. The discriminator is smaller than the generator to make the training more challenging. The number of neurons for each hidden layer is 4, the learning rate is 0.001, λp = 0.1 for both
method, λ1
m = 0.1 for our generator objective.
Fig. 2 shows that our model can recover well three modes,
while Dist-GAN cannot (see attached video demos in the
supplementary material). Although both methods have good
gradients of the discriminator scores (decision boundary) for
the middle mode, it’s difﬁcult to recover this mode with
Dist-GAN as gradients computed over generated samples
are not explicitly forced to resemble those of real samples
as in our proposed method. Note that for this 1D experiment
Figure 3: Examples of the number of modes (classes) and
registered points of compared methods.
Table 1: Network structures for 1D synthetic data in our experiments.
Encoder (E)
Generator (G)
Discriminator (D)
and the 2D experiment in the next section, we only evaluate our model with gradient matching (+GM), since we ﬁnd
that our new generator with gradient matching alone is already good enough; neighbors embedding is more useful for
high-dimensional data samples, as will be discussed.
Synthetic 2D dataset
For 2D synthetic data, we follow the experimental setup on
the same 2D synthetic dataset .
The dataset has 25 Gaussian modes in the grid layout (red
points in Fig. 4) that contains 50K training points. We draw
2K generated samples for evaluating the generator. However, the performance reported in is nearly saturated. For example, it can re-cover entirely 25 modes and register more than 90% of the total number of points. It’s hard to see the signiﬁcant improvement of
our method in this case. Therefore, we decrease the number
of hidden layers and their number of neurons for networks
to be more challenging. For a fair comparison, we use equivalent encoder, generator and discriminator networks for all
compared methods.
The detail of network architecture is presented in Table
1. din = 2, dout = 2, dh = 64 are dimensions of input, output and hidden layers respectively. Nh is the number of hidden layers. We use ReLU for hidden layers and
sigmoid for output layers. To have a fair comparison, we
carefully ﬁne-tune other methods to ensure that they can
perform their best on the synthetic data. For evaluation, a
mode is missed if there are less than 20 generated samples
registered in this mode, which is measured by its mean and
variance of 0.01. A method has mode collapse if there are
missing modes. For this experiment, the prior distribution
is the 2D uniform [−1, 1]. We use Adam optimizer with
learning rate lr = 0.001, and the exponent decay rate of
ﬁrst moment β1 = 0.8. The parameters of our model are:
λp = 0.1, λ1
m = 0.1. The learning rate is decayed
every 10K steps with a base of 0.99. This decay rate is to
avoid the learning rate saturating too quickly that is not fair
for slow convergence methods. The mini-batch size is 128.
The training stops after 500 epochs.
In this experiment, we compare our model to several stateof-the-art methods. ALI , VAE-GAN and Dist-GAN
 are recent works using encoder/decoder in their models. WGAN-GP is one of the state-of-the-arts. We also compare to
VAE-based methods: VAE and
β-VAE . The numbers of covered (registered) modes and registered points during training are presented in Fig. 3. The quantitative numbers of last epochs
are in Table 2. In this table, we also report the Total Variation scores to measure the mode balance . The result for each method is the average of
eight runs. Our method outperforms all others on the number of covered modes. Although WGAN-GP and Dist-GAN
are stable with larger networks and this experimental setup
 , they are less stable with our
network architecture, miss many modes and sometimes diverge.VAE based method often address well mode collapse,
but in our experiment setup where the small networks may
affect the reconstruction quality, consequently reduces their
performance. Our method does not suffer serious mode collapse issues for all eight runs. Furthermore, we achieve a
higher number of registered samples than all others. Our
method is also better than the rest with Total Variation (TV).
In addition, we follow to explore the gradient map of the discriminator scores
of compared methods: standard GAN, WGAN-GP, Dist-
GAN and ours as shown in Fig. 4. This map is important
because it shows the potential gradient to pull the generated
samples towards the real samples (red points). The gradient
map of standard GAN is noisy, uncontrolled and vanished
for many regions. The gradient map of WGAN-GP has more
meaningful directions than GAN. Its gradient concentrates
in the centroids (red points) of training data and has gradients around most of the centroids. However, WGAN-GP still
has some modes where gradients are not towards the groundtruth centroids. Both Dist-GAN and our method show better
gradients than WGAN-GP. The gradients of our method are
more informative for the generator to learn when they guide
better directions to all real ground-truths.
CIFAR-10 and STL-10 datasets
For CIFAR-10 and STL-10 datasets, we measure the performance with FID scores . FID can detect intra-class mode dropping, and measure the diversity as
well as the quality of generated samples. We follow the experimental procedure and model architecture in to compare methods. FID is computed from 10K
real samples and 5K generated samples. Our default parameters are used for all experiments λp = 1.0, λr = 1.0, λ1
m = 1.0. Learning rate, β1, β2 for Adam is (lr = 0.0002,
β1 = 0.5, β2 = 0.9). The generator is trained with 350K
updates for logarithm loss version (Eq. 7) and 200K for
“hinge” loss version (Eq. 8) to converge better. The dimen-
Figure 4: Our 2D synthetic data has 25 Gaussian modes (red dots). The black arrows are gradient vectors of the discriminator
computed around the ground-truth modes. Figures from left to right are examples of gradient maps of GAN, WGAN-GP,
Dist-GAN and ours.
Table 2: Results on 2D synthetic data. Columns indicate the number of covered modes, and the number of registered samples
among 2000 generated samples, and two types of Total Variation (TV). We compare our model to state of the art models:
WGAN-GP and Dist-GAN.
#registered modes
#registered points
TV (Differential)
GAN 
14.25 ± 2.49
1013.38 ± 171.73
1.00 ± 0.00
0.90 ± 0.22
ALI 
17.81 ± 1.80
1281.43 ± 117.84
0.99 ± 0.01
0.72 ± 0.19
VAEGAN 
12.75 ± 3.20
1042.38 ± 170.17
1.35 ± 0.70
1.34 ± 0.98
VAE 
13.48 ± 2.31
1265.45 ± 72.47
1.81 ± 0.71
2.16 ± 0.72
β-VAE 
18.00 ± 2.33
1321.17 ± 95.61
1.17 ± 0.24
1.47 ± 0.28
WGAN-GP 
21.71 ± 1.35
1180.25 ± 158.63
0.90 ± 0.07
0.51 ± 0.06
Dist-GAN 
20.71 ± 4.42
1188.62 ± 311.91
0.82 ± 0.19
0.43 ± 0.12
24.39 ± 0.44
1461.83 ± 222.86
0.57 ± 0.17
0.31 ± 0.12
Figure 5: FID scores of our method compared to Dist-GAN.
sion of the prior input is 128. All our experiments are conducted using the unsupervised setting.
In the ﬁrst experiment, we conduct the ablation study with
our new proposed techniques to understand the contribution
of each component into the model. Experiments with standard CNN on the CIFAR-10 dataset. We
use the logarithm version for the discriminator objective (Eq.
7). Our original model is similar to Dist-GAN model, but
we have some modiﬁcations, such as: using lower weights
for the reconstruction constraint as we ﬁnd that it can im-
Table 3: Comparing the FID score to the state of the art
(Smaller is better). Methods with the CNN and ResNet (R)
architectures. FID scores of SN-GAN, Dist-GAN and our
method reported with hinge loss. Results of compared methods are from or gradient matching (+GM),
into our original model, it converges faster and reaches a
better FID score than the original one. Combining two proposed techniques further speeds up the convergence and
reach better FID score than other versions. This experiment
proves that our proposed techniques can improve the diversity of generated samples. Note that in Fig. 5, we compared
Dist-GAN and our model (original) with only discriminator
scores. With GM, our model converges faster and achieves
better FID scores.
Figure 6: Generated samples of our method. Two ﬁrst samples are on CIFAR-10 with CNN and ResNet architectures, and the
last one is on STL-10 with CNN.
We compare our best setting (NE + GM) with a hinge
loss version (Eq. 8) with other methods. Results are shown
in Table 3. The FID score of SN-GAN and Dist-GAN are
also with hinge loss function. We also report our performance with the ResNet (R) architecture 
for CIFAR-10 dataset. For both standard CNN and ResNet
architectures, our model outperforms other state-of-the-art
methods with FID score, especially signiﬁcantly higher on
STL-10 dataset with CNN and on CIFAR-10 dataset with
ResNet. For STL-10 dataset and the ResNet architecture, the
generator is trained with 200K iterations to reduce training
time. Training it longer does not signiﬁcantly improve the
FID score. Fig. 6 are some generated samples of our method
trained on CIFAR-10 and STL-10 datasets.
Our proposed techniques are not only usable in our model,
but can be used for other GAN models. We demonstrate
this by applying them for standard GAN . This experiment is conducted on the CIFAR-10
dataset using the same CNN architecture as . First, we regularize the generator of GAN by our
propose neighbors embedding or gradient matching separately or their combination to replace the original generator
objective of GAN. When applying NE and GM separately,
each of them itself can signiﬁcantly improve FID as shown
in Fig. 6. In addition, from Fig. 7, GM+NE achieves FID
of 26.05 (last iteration), and this is signiﬁcant improvement
compared to GM alone with FID of 31.50 and NE alone
with FID of 38.10. It’s interesting that GM can also reduce
mode collapse, we let the further investigation of it in the
future work. Although both can handle the mode collapse,
NE and GM are very different ideas: NE is a manifold learning based regularization to explicitly prevent mode collapse;
GM aligns distributions of generated samples and real samples. The results (Figs. 5 and 7) show GM+NE leads to better
convergence and FID scores than individual techniques.
To examine the computational time of gradient matching
of our proposed generator objective, we measure its training
time for one mini-batch (size 64) with/without GM (Computer: Intel Xeon Octa-core CPU E5-1260 3.7GHz, 64GB
RAM, GPU Nvidia 1080Ti) with CNN for CIFAR-10. It
takes about 53ms and 43ms to train generator for one minibatch with/without the GM term respectively. For 300K iter-
Figure 7: FID scores of GAN when applying our proposed
techniques for the generator, and its zoomed ﬁgure on the
ations (one mini-batch per iteration), training with GM takes
about one more hour compared to without GM. The difference is not serious. Note that GM includes ℓ1, ℓ2 norms of
the difference of discriminator scores and gradients, which
can be computed easily in Tensorﬂow.
Conclusion
We propose two new techniques to address mode collapse
and improve the diversity of generated samples. First, we
propose an inverse t-SNE regularizer to explicitly retain local structures of latent samples in the generated samples
to reduce mode collapse. Second, we propose a new gradient matching regularization for the generator objective,
which improves convergence and the quality of generated
images. We derived this gradient matching constraint from
Taylor expansion. Extensive experiments demonstrate that
both constraints can improve GAN. The combination of our
proposed techniques leads to state of the art FID scores on
benchmark datasets. Future work applies our model for other
applications, such as: person re-identiﬁcation , anomaly detection .
Acknowledgement
This work was supported by both ST Electronics and the
National Research Foundation(NRF), Prime Minister’s Of-
ﬁce, Singapore under Corporate Laboratory @ University
Scheme (Programme Title: STEE Infosec - SUTD Corporate Laboratory).