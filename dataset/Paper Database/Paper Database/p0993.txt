Heterogeneous Transfer Learning for Image ClassiÔ¨Åcation
Yin Zhu‚Ä†, Yuqiang Chen‚Ä°, Zhongqi Lu‚Ä†,
Sinno Jialin Pan‚àó, Gui-Rong Xue‚Ä°, Yong Yu‚Ä°, and Qiang Yang‚Ä†
‚Ä†Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong
‚Ä°Shanghai Jiao Tong University, Shanghai, China
‚àóInstitute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
‚Ä†{yinz, cs lzxaa, qyang}@cse.ust.hk, ‚Ä°{yuqiangchen, grxue, yyu}@sjtu.edu.cn, ‚àó 
Transfer learning as a new machine learning paradigm has
gained increasing attention lately. In situations where the
training data in a target domain are not sufÔ¨Åcient to learn predictive models effectively, transfer learning leverages auxiliary source data from other related source domains for learning. While most of the existing works in this area only focused on using the source data with the same structure as
the target data, in this paper, we push this boundary further
by proposing a heterogeneous transfer learning framework
for knowledge transfer between text and images. We observe
that for a target-domain classiÔ¨Åcation problem, some annotated images can be found on many social Web sites, which
can serve as a bridge to transfer knowledge from the abundant text documents available over the Web. A key question
is how to effectively transfer the knowledge in the source
data even though the text can be arbitrarily found. Our solution is to enrich the representation of the target images with
semantic concepts extracted from the auxiliary source data
through a novel matrix factorization method. By using the latent semantic features generated by the auxiliary data, we are
able to build a better integrated image classiÔ¨Åer. We empirically demonstrate the effectiveness of our algorithm on the
Caltech-256 image dataset.
Introduction
Image classiÔ¨Åcation has found many applications ranging
from Web search engines to multimedia information delivery. However, it has two major difÔ¨Åculties. First, the labeled
images for training are often in short supply, and labeling
new images incur much human labor. Second, images are
usually ambiguous, e.g. an image can have multiple explanations. How to effectively overcome these difÔ¨Åculties and
build a good classiÔ¨Åer therefore becomes a challenging research problem. While labeled images are expensive, abundant unlabeled text data are easier to obtain. This motivates
us to use the abundantly available text data to help improve
the image classiÔ¨Åcation performance.
In the past, several approaches have been proposed to
solve the lack of label problem in supervised learning, e.g.
semi-supervised learning methods are proposed
to utilize unlabeled data assuming that the labeled and unlabeled data are from the same domain and drawn from
Copyright c‚Éù2011, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.
the same distribution. Recently, transfer learning methods
 are proposed to transfer knowledge from auxiliary data in a different but related domain to
help on target tasks. But a commonality among most transfer learning methods is that data from different domains are
required to be in the same feature space.
In some scenarios, given a target task, one may easily collect a lot of auxiliary data that are represented in a different
feature space. For example, suppose our task is to classify
dolphin pictures (e.g. yes or no). We have only a few labeled images for training. Besides, we can collect a large
amount of text documents from the Web easily. Here, the
target domain is the image domain, where we have a few
labeled data and some unlabeled data, which are both represented by pixels. The auxiliary domain or source domain is
the text domain, where we have large amount of unlabeled
textual documents. Is it possible to use these cheap auxiliary data to help the image classiÔ¨Åcation task? This is an
interesting and difÔ¨Åcult problem, since the relationship between text and images is not given. This can be also referred
to as a Heterogeneous Transfer Learning problem 1. In this paper, we focus on heterogeneous transfer learning for image classiÔ¨Åcation by exploring knowledge
transfer from auxiliary unlabeled images and text data.
In image classiÔ¨Åcation, if the labeled data are extremely
limited, classiÔ¨Åers trained on the original feature representation (e.g. pixels) directly may get very bad performance. One
key issue is to discover a new powerful representation, such
as high level features beyond pixels (e.g. edge, angle), to
boost the performance. In this paper, we are interested in discovering the high-level features for images from both auxiliary image and text data. Although images and text are represented in different feature spaces, they are supposed to share
a latent semantic space, which can be used to represent images when it is well learned. We propose to apply collective
matrix factorization (CMF) techniques on the auxiliary image and text data to discover the semantic space underlying the image and text domains. CMF
techniques assume some correspondences between images
1Heterogeneous transfer learning can be deÔ¨Åned for learning
when auxiliary data have different features or different outputs. In
this paper, we focus on the ‚Äòdifferent features‚Äô version.
Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence
and text data, which may not hold in our problem. To solve
this problem, we make use of the tagged images available
on the social Web, such as tagged images from Flickr, to
construct connections between image and text data. After a
semantic space is learnt by CMF using the auxiliary images
and text documents, a new feature representation called semantic view is created by mapping the target images into
this semantic space.

Figure 1: Source data used for different transfer learning
algorithms. Self-taught learning only uses unlabeled auxiliary images, heterogeneous transfer learning for image clustering uses images and their annotations, while our proposed heterogeneous transfer learning for image classiÔ¨Åcation takes all three information sources as inputs.
Figure 2: Three pictures different in pixel representation, but
have the same the semantic meaning: running.
The main contribution of our work is that we utilize three
kinds of source data, namely auxiliary images, their tags and
auxiliary text documents and build a good latent semantic
space using these data. The structure of the source data is
clearly shown in Figure 1, where self-taught learning can
only utilize the auxiliary images, which have the same feature representation with the source data, a previous heterogeneous transfer learning work for clustering uses annotated images, while our method uses all the
data. The advantage of our method over the two previous
methods is that by leveraging the knowledge in the abundant text documents, we are able to build a better feature
space with semantic meaning.
Motivation and Problem Formulation
Before describing our proposed method in detail, we Ô¨Årst illustrate a motivating example and give a problem statement.
A Motivating Example
Figure 2 shows three pictures with people running. In some
classiÔ¨Åcation tasks, e.g. a working-or-running classiÔ¨Åcation
problem, these three pictures should be classiÔ¨Åed as the
same class. But they are quite dissimilar in the pixel level
representation, thus any classiÔ¨Åer based on pixel level representation would fail the classiÔ¨Åcation task. However, they
look similar in their semantic space. First, tags are found for
an image by comparing it with all tagged auxiliary images,
selecting the top similar images and aggregating their tags.
We can Ô¨Ånd that some of these tags are quite relevant to the
picture, e.g. image (B) has top tags ‚Äúroad‚Äù and ‚Äúcountry‚Äù.
By further exploring more text documents, the similarity between the three images can be found as their tags ‚Äúroad‚Äù,
‚Äútrack‚Äù and ‚Äúgym‚Äù have similar latent meanings in the text.
Table 1: Problem formulation
Learning objective Make predictions on target test images
Target image
Training images: X={xi, yi}n
classiÔ¨Åcation
Testing images: X‚àó={x‚àó
Auxiliary source
Unlabeled annotated images: I={zi, ti}l
Unlabeled text documents: D={di}k
Problem DeÔ¨Ånition
Suppose we are given a few labeled image instances X =
i=1 and testing instances X‚àó= {x‚àó
where xi ‚ààRd is an input vector of image features and
yi is the corresponding label of image i. In this paper, we
use ‚Äúbag-of-words‚Äù to represent image features, whose values are nonnegative. n and m are the
numbers of training and testing instances respectively. In addition, we are also given a set of auxiliary annotated images
I = {zi, ti}l
i=1 and a set of documents D = {di}k
i=1, where
zi ‚ààRd is an image represented by a feature vector as xi,
ti ‚ààRh is its correspondingvector of tags, and h is the number of tags. For example, if an image zi is annotated by tags
Œ± and Œ≤ with Œ±, Œ≤ ‚àà{1, ..., h}, then ti =[0, ..., 1, ..., 1, ...0]
is a vector of dimensionality h with all zeros and 1‚Äôs in the
Œ± and Œ≤ positions. di ‚ààRm is a document represented
by a vector of bag-of-words, and l and k are the numbers
of auxiliary images and documents respectively. Our goal
is to learn an accurate image classiÔ¨Åer f(¬∑) from X, I and
D to make predictions on X‚àó, f(X‚àó). We summarize the
problem deÔ¨Ånition in Table 1. For convenience, we denote
i=1 ‚ààRl√ód and T={ti}l
i=1 ‚ààRl√óh the image features and text tags of the auxiliary images separately. Furthermore, we abuse the notation X, X‚àó, Z, and T to represent the data matrices with instances xi, x‚àó
i , zi and ti being
row vectors in them.
Heterogenous Transfer Learning for Image
ClassiÔ¨Åcation
In this section, we describe the details of our proposed
method. We Ô¨Årst introduce how to build a connection between the auxiliary images and text data. We then show how
to apply the collective matrix factorization method to learn
high-level features behind the connection. Finally, we describe how to construct a new feature presentation for target
images, on which standard classiÔ¨Åers can perform well.
Bridging Images and Text
Given a set of auxiliary images Z ‚ààRl√ód with their corresponding tags T ‚ààRl√óh, and a set of unlabeled documents
D‚ààRk√óm, we now show how to build connections between
them. As illustrated in Figure 1, we can construct a two layer
Bi-partite graph among images, tags and documents. More
speciÔ¨Åcally, the top layer Bi-partite graph is used to represent the relationship between images and tags. Each image
can be annotated by tags, and some images may share one
or multiple tags. If two images are annotated by shared tags,
they tend to be related to each other semantically. Similarly,
if two tags co-occur in annotations of shared images, they
tend to be related to each other. This image-tag Bi-partite
graph has been represented by the tag matrix T. The bottom
layer Bi-partite graph is used to represent the relationship
between tags and documents. If a tag, more precisely, the
text word of the tag, occurs in a document, there is an edge
connecting the tag and the document. We deÔ¨Åne a matrix
F ‚ààRk√óh to represent the document-tag Bi-partite graph,
where Fij=1 if there is an edge between the ith document
and the jth tag, otherwise 0.
Learning Semantic Features for Images
So far, we have built a connection between images and text
through annotating tags. In this section, we try to learn some
semantic features for images by exploiting the relationship
between images and text from the auxiliary sources. Recall
that we have a matrix of images with low-level image features Z and a relational matrix between images and annotations T, we Ô¨Årst deÔ¨Åne a new matrix G = Z‚ä§T ‚ààRd√óh to
denote the correlation between low-level image features and
annotations which can be referred to as high-level concepts.
Note that Gij = 
k zik ¬∑ tkj, where zik ‚â•0 is the value of
the ith visual word in the kth image, and n(i)
the number of images that are annotated by the jth tag and
whose ith visual word is observed at the same time. Gij is
large when n(i)
is large or some of the values of the ith visual word in the images with the jth tag annotation are large.
This implies that if Gij is large then the ith image feature
and the jth tag may have strong correlation.
Motivated by Latent Semantic Analysis (LSA) , in order to extract latent semantic features for each low-level image feature, we can apply matrix
factorization techniques to decompose G into latent factor
matrices as
where U ‚ààRd√óg, V1 ‚ààRh√óg, and g is the number of
latent factors. Then ui can be treated as a latent semantic
representation of the ith image low-level feature, and v1j
can be treated as a latent semantic representation of jth tag.
However, the matrix G may be very sparse, resulting in the
decomposition on G may not be precise.
Recall that we have another relational matrix F‚ààRk√óh
between documents and tags. We can also decompose it as
where W ‚ààRk√óg, V2 ‚ààRh√óg. Then wi can be treated
as a latent semantic representation of document di, and v2j
can be treated as a latent semantic representation of the jth
tag. Since the matrix F is relatively dense compared to G,
the decomposition on F may be more precise. Therefore,
our motivation is to use the results of the decomposition on
F to help the decomposition on G to learn a more precise
U. Note that if we can decompose G and F perfectly, then
we may get V1 = V2 as the tags in the two sides should
have the same latent semantic meanings. Motivated by this
observation, we propose to learn the latent semantic representation U by decomposing G and F jointly with the constraint V1 = V2. This is called collective matrix factorization (CMF), which was Ô¨Årst proposed by Singh and Gordon . It has been shown that when relational matrices
are sparse, decomposing them simultaneously can get better
performance than decomposing them individually.
Hence, our objective can be written as follows,
G ‚àíUV‚ä§
F‚àíWV‚ä§
F + R(U,V,W),
where 0 ‚â§Œª ‚â§1 is a tradeoff parameter to control
the decomposition error between the two matrix factorizations, || ¬∑ ||F denotes the Frobenius norm of matrix, and
R(U, V, W) is the regularization function to control the
complexity of the latent matrices U, V and W. In this paper,
we deÔ¨Åne the regularization function as
R(U, V, W) = Œ≥1 ‚à•U‚à•2
F + Œ≥2 ‚à•V‚à•2
F + Œ≥3 ‚à•W‚à•2
where Œ≥1, Œ≥2 and Œ≥3 are nonnegative parameters to control
the responding regularization terms. In this paper, we set
Œ≥1 =Œ≥2 =Œ≥3 =1.
The optimization problem in (1) is an unconstrained nonconvex optimization problem with three matrix variables U,
V and W, thus only has local optimal solutions. However,
(1) is convex with respect to any one of the three matrices
while Ô¨Åxing the other two. Thus one common technique to
solve this kind of optimization problem is to Ô¨Åx two matrices and optimize the left one iteratively until the results are
convergent. The detailed is shown in Algorithm 1. The empirical study of the convergenceof the algorithm is presented
in the experimental section.
Constructing New Representation
In the previous section, we have described how to learn a
semantic view U for each low-level image feature. In this
section, we show how to map the target images X to the
semantic feature representation for image classiÔ¨Åcation. We
Ô¨Årst transform each target image xi into its semantic space
as xi = xiU. After constructing a new representation for
target images, we can train standard classiÔ¨Åers on {xi, yi}‚Äôs
to make predictions on the testing images X‚àó, on which we
apply the same feature representation construction.
Algorithm 1 Image Semantic View Learning via CMF
Input: A auxiliary image matrix Z with its corresponding
annotation matrix T, a document-tag relational matrix F,
a parameter Œª, and the number of latent factors g.
Output: A new representation U for images Z.
1: Compute G = Z‚ä§T and randomly initialize matrices
U, V and W.
Fix U and V, apply conjugate gradient descent
(CGD) on (1) to update W;
Fix U and W, apply CGD on (1) to update V;
Fix W and V, apply CGD on (1) to update U;
6: until U, V and W are convergent.
Experiments
Dataset and Processing
We use a benchmark dataset, Caltech-256 , as target images. The auxiliary annotated images and
the text documents are crawled from the online photo sharing website Flickr and Google search engine respectively.
Caltech-256 image dataset consists of 256 categories of
images, with each category usually containing hundreds of
images. We randomly select 19 categories from the 256 categories, and build
= 171 pairs of binary classiÔ¨Åcation tasks. The selected 19 categories and the corresponding number of images in each category are summarized as
follows: tennis-racket (298), american-Ô¨Çag (299), schoolbus (361), cake (96), cd (300), chessboard (299), greyhound
(299), fried-egg (300), dog (359), lighthouse (242), llama
(300), minaret (300), motorbike (300), rainbow (300), sheetmusic (300), smokestack (300), starÔ¨Åsh (300), watermelon
(300), zebra (299).
The auxiliary annotated images from Flickr were crawled
during December 2009. We collected 5, 700 images and
64, 021 related tags, among which 2, 795 tags were distinct.
Each of these tags is a single word. These Flickr images are
relevant to the image categories described above. For example, for the image category ‚Äúdog‚Äù, we collect Flickr images
with tags ‚Äúdog‚Äù, ‚Äúgreyhound‚Äù or ‚Äúdoggy‚Äù. In order to obtain
auxiliary text data, we use the Google search engine to crawl
documents from the Web. For each tag, we search the tag
name via Google search engine and get the Ô¨Årst 100 resulting
webpages as the text documents. Each resulting webpage is
treated as an auxiliary document. We collect 279, 500 documents in total. Note that one can also use other data sources,
e.g., articles and images from Wikipedia. In this paper, we
focus on how to use auxiliary data sources to help on target
image classiÔ¨Åcation tasks. We will use other data sources to
test the method in the future.
In our experiments, we use the bag-of-words to represent
images . More speciÔ¨Åcally, for the target and auxiliary images from Flickr, we use SIFT descriptors to identify interesting points. We then use
the K-means clustering algorithm to group all the interesting points into 512 clusters as a codebook. In this way, each
cluster is treated as a feature. For auxiliary documents and
the tags associated to the auxiliary images, we do stemming
on them, and build a tag-document co-occurrence matrix.
Evaluation and Baseline Methods
We use the prediction accuracy on the testing data as the
evaluation criterion, which is deÔ¨Åned as follows,
ACC(f, X‚àó, Y‚àó) =
i ‚ààX‚àóI[f(x‚àó
where f is the trained classiÔ¨Åer, I is an indicator function.
For each binary classiÔ¨Åcation task, there are hundreds of
images. We randomly select 5 of them as training instances
and the rest as testing instances. We repeat this for 30 times
and report the average results. We use linear Support Vector
Machines (SVMs)2 as a base classiÔ¨Åer. In all experiments,
we set the trade off parameter C of linear SVMs to 10.
We compared our proposed method with three different
baselines with different feature presentations for image classiÔ¨Åcation. The three baselines and our proposed method are
summarized as follows,
Orig. This baseline only uses the SIFT image features of
the target images without considering to use any auxiliary
sources to enrich the feature representation.
PCA. In this baseline, we Ô¨Årst apply Principal Component
Analysis (PCA) on the auxiliary images to learn some latent
factors, and use the latent factors as features to represent the
target images for classiÔ¨Åcation. This method is also reported
in , which obtains promising performance
for image classiÔ¨Åcation.
Tag. We implemented the method proposed in as another baseline, which builds a text view for target
images by using some auxiliary annotated images. For each
target image, this method Ô¨Ånds the K most similar images
from the annotated image set and aggregate all the tags associated to these similar images as a text representation. Here,
K is set to 100 in our experiments.
HTLIC. This denotes our proposed method, which uses all
the auxiliary data including annotated images and unlabeled
documents. The parameter setting is discussed in the following section.
For each classiÔ¨Åcation task, PCA, Tag and HTLIC use
the same set of annotated images, that are images relevant to
two categories in the task.
Experimental Results
In the Ô¨Årst experiment, we compare our method with three
baselines on all the classiÔ¨Åcation tasks. Because of the limited space, we are not able to report the results of all 171
tasks. To show results on some representative tasks, we Ô¨Årst
rank all tasks based on the improvement of HTLIC compared to Orig in terms of classiÔ¨Åcation accuracy. We then select 4 tasks with largest improvement and 3 ones with smallest improvement as shown in table 2. Note that the value
of improvement can be negative if HTLIC performs worse
than Orig. The last row in the table shows the average results
2We use LibSVM that is available at 
ntu.edu.tw/Àúcjlin/libsvm/.
Table 2: Comparison results with labeled training instances.
watermelon vs sheet-music
64.66 ¬± 9.99
70.28 ¬± 11.33
78.13 ¬± 14.40
85.29 ¬± 11.94
fried-egg vs american-Ô¨Çag
59.19 ¬± 7.80
60.54 ¬± 9.28
63.70 ¬± 12.54
78.80 ¬± 12.21
fried-egg vs school-bus
65.42 ¬± 10.72
66.73 ¬± 11.01
75.58 ¬± 14.56
83.74 ¬± 11.88
zebra vs motorbikes
69.95 ¬± 11.74
70.55 ¬± 12.37
85.74 ¬± 13.72
86.66 ¬± 12.32
minaret vs lighthouse
53.67 ¬± 7.62
53.61 ¬± 6.18
52.71 ¬± 7.03
53.32 ¬± 6.38
llama vs greyhound
51.48 ¬± 7.11
52.65 ¬± 5.58
50.79 ¬± 5.53
51.94 ¬± 5.40
cd vs cake
62.85 ¬± 10.45
65.20 ¬± 11.87
54.98 ¬± 5.33
57.71 ¬± 8.35
average accuracy
(a) Varying values of Œª.
Number of latent factors
average accuracy
(b) Varying numbers of latent factors.
Percentage of text documents used in learning
average accuracy
(c) Varying size of auxiliary text data.
Percentage of tagged images used in learning
average accuracy
(d) Varying size of annotated image data.
Percentage of non‚àírelevant tagged images used in learning
average accuracy
(e) Varying size of relevant annotated image
Number of iterations
The objective value of Eq. (1)
(f) Varying numbers of iteration.
Figure 3: Experiments on parameter sensitivity.
over all the 171 classiÔ¨Åcation tasks in term of accuracy. In
this experiment, for HTLIC, we set the tradeoff parameter
Œª in (1) to 0.85. As we can see from table 2, our proposed
HTLIC, which only uses semantic features to represent the
target image for classiÔ¨Åcation, outperforms other baselines.
This implies that the semantic features learned by our proposed method is powerful for image classiÔ¨Åcation.
In the second experiment, we study the parameter sensitivity of Œª on the overall performance of HTLIC in image
classiÔ¨Åcation. Figure 3(a) shows the average classiÔ¨Åcation
accuracy of HTLIC over the all 171 image classiÔ¨Åcation
tasks under varying values of Œª. We can Ô¨Ånd that HTLIC
performs best and steadily when Œª falls in the range from 0.8
to 0.95, which implies the jointly decomposition on the auxiliary document-tag matrix can indeed help learning a more
precise latent factor matrix U for low-level image features.
In the third experiment, we study the parameter sensitivity of g, the number of the latent factors in the matrix factorization, on the overall performance of HTLIC in image
classiÔ¨Åcation. Figure 3(b) shows the average classiÔ¨Åcation
accuracy of HTLIC over all image classiÔ¨Åcation tasks under varying numbers of the latent factors g. We can Ô¨Ånd that
HTLIC performs best when g falls in the range [10 30].
We also analyze the impact of quantity of the auxiliary
text data to the overall performance of HTLIC in image
classiÔ¨Åcation. In this experiment, we denote ‚Äústandard‚Äù or
1 for short, to be the whole document set crawled from the
Web, and denote 0.1 to be the 10% documents sampled from
the whole document set. The experimental results are shown
in Figure 3(c). As we can see that when the size of the auxiliary document set increases, the performance of HTLIC increases as well. The reason is that when the number of documents is larger, the document-tag matrix F may be denser,
which makes the decomposed matrix V more precise, resulting in the decomposition on G being more precise.
We also vary the size of the annotated images for each
task. As shown in Figure 3(d), varying auxiliary image size
affect the results for all the methods using auxiliary im-
ages. HTLIC and Tag have a clear curve of improving when
there are more auxiliary images, while PCA improves much
slower. We also did experiments to show how the quality
of annotated images affect the performance of these methods. As shown in Figure 3(e), when the auxiliary images are
gradually substituted by non-relevant images, which are just
random images from Flickr, the result of HTLIC and Tag
have the clear drop, while PCA is quite stable in its performance. Note that our method performs close to PCA when
there is no relevant images at all in the auxiliary image set.
The last experiment is to measure the convergence of
the collective matrix factorization algorithm in Algorithm 1.
Figure 3(f) shows the average objective value of Eq. (1) over
30 random initializations when doing the CMF for task watermelon vs sheet-music. As can be seen in the Ô¨Ågure, after
10 iterations the objective value converges.
Related Work
Transfer learning emphasizes the transferring of knowledge
across different domains or tasks. For example, Wu and Dietteirch investigated methods for improving SVM
classiÔ¨Åers with auxiliary training data. Raina et al. 
proposed a learning strategy known as self-taught learning
which utilizes irrelevant unlabeled data to enhance the classiÔ¨Åcation performance. Pan and Yang surveyed the
Ô¨Åled of transfer learning. Recently, Yang et al. proposed a heterogenous transfer learning algorithm for image
clustering by levering auxiliary annotated images. We also
aim to levering auxiliary annotated images for target image
classiÔ¨Åcation. The difference between our work and theirs is
that other than using the annotated images, we also try to utilize unlabeled text data for further boosting the performance
in image classiÔ¨Åcation. Translated learning 
utilizes the labeled text data to help classify images, while
in our work the auxiliary text data are unlabeled. Our work
also relates to multimedia area, especially works using text
and image together, e.g. leveraging image content for Web
search . We share the same consensus
that Ô¨Ånding the correlation between images and text is critical to the understanding of images. However, our method
is novel in that we use text and images from totally different
sources and also the aim is different. Our work is also related
to works on tagged images, e.g. .
Conclusions
In this paper, we propose the heterogeneous transfer learning
method for image classiÔ¨Åcation. We show that the performance of image classiÔ¨Åcation can be improved by utilizing
textual information. To bridge text documents and images,
we use tagged images and create a semantic view for each
target image by using collective matrix factorization technique, which effectively incorporates information in the auxiliary text into the tagging matrix. The experimental results
also show our method outperforms other baselines when the
labeled data in the target domain are short in supply.
Acknowledgement
We thank the support of Hong Kong RGC/NSFC projects
N HKUST624/09 and 60910123. Gui-Rong Xue thanks the
support by the grants from NSFC project (NO. 60873211),
RGC/NSFCproject (NO. 60910123). Yin thanks Nathan
Liu, Evan Xiang and Fan Zhou for inspiring discussions.