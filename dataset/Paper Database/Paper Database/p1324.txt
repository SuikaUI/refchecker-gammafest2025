HAL Id: hal-01116158
 
Submitted on 16 Sep 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Random forests and kernel methods
Erwan Scornet
To cite this version:
Erwan Scornet. Random forests and kernel methods. IEEE Transactions on Information Theory,
2016. ￿hal-01116158v2￿
Random forests and kernel methods
Erwan Scornet
Sorbonne Universit´es, UPMC Univ Paris 06, F-75005, Paris, France
 
Random forests are ensemble methods which grow trees as base learners
and combine their predictions by averaging. Random forests are known
for their good practical performance, particularly in high-dimensional settings. On the theoretical side, several studies highlight the potentially
fruitful connection between random forests and kernel methods. In this
paper, we work out in full details this connection. In particular, we show
that by slightly modifying their deﬁnition, random forests can be rewritten as kernel methods (called KeRF for Kernel based on Random Forests)
which are more interpretable and easier to analyze. Explicit expressions of
KeRF estimates for some speciﬁc random forest models are given, together
with upper bounds on their rate of consistency. We also show empirically
that KeRF estimates compare favourably to random forest estimates.
Index Terms — Random forests, randomization, consistency, rate of consistency, kernel methods.
2010 Mathematics Subject Classiﬁcation: 62G05, 62G20.
Introduction
Random forests are a class of learning algorithms used to solve pattern recognition problems. As ensemble methods, they grow many trees as base learners and
aggregate them to predict. Growing many diﬀerent trees from a single data set
requires to randomize the tree building process by, for example, sampling the
data set. Thus, there exists a variety of random forests, depending on how trees
are built and how the randomness is introduced in the tree building process.
One of the most popular random forests is that of Breiman which grows
trees based on CART procedure and randomizes both the training set and the splitting directions.
Breiman’s random forests have been under active investigation during
the last decade mainly because of their good practical performance and their
ability to handle high dimensional data sets. Moreover, they are easy to run
since they only depend on few parameters which are easily tunable . They are acknowledged to be state-of-the-art
methods in ﬁelds such as genomics and pattern recognition , just to name a few.
However, even if random forests are known to perform well in many contexts,
little is known about their mathematical properties. Indeed, most authors study
forests whose construction does not depend on the data set. Although, consistency of such simpliﬁed models has been addressed in the literature , these results do not
adapt to Breiman’s forests whose construction strongly depends on the whole
training set. The latest attempts to study the original algorithm are by Mentch
and Hooker and Wager who prove its asymptotic normality or by
Scornet et al. who prove its consistency under appropriate assumptions.
Despite these works, several properties of random forests still remain unexplained. A promising way for understanding their complex mechanisms is to
study the connection between forests and kernel estimates, that is estimates mn
which take the form
i=1 YiKk(Xi, x)
i=1 Kk(Xi, x) ,
where {(Xi, Yi) : 1 ≤i ≤n} is the training set, (Kk)k is a sequence of kernel
functions, and k (k ∈N) is a parameter to be tuned. Unlike the most used
Nadaraya-Watson kernels which satisfy a homogeneous property of the form Kh(Xi, x) = K((x −Xi)/h), kernels Kk are
not necessarily of this form. Therefore, the analysis of kernel estimates deﬁned
by (1) turns out to be more complicated and cannot be based on general results
regarding Nadaraya-Watson kernels.
Breiman was the ﬁrst to notice the link between forest and kernel methods, a link which was later formalized by Geurts et al. . On the practical side, Davies and Ghahramani highlight the fact that a speciﬁc kernel based on random forests can empirically outperform state-of-the-art kernel
methods. Another approach is taken by Lin and Jeon who establish the
connection between random forests and adaptive nearest neighbor, implying
that random forests can be seen as adaptive kernel estimates . The latest study is by Arlot and Genuer who show that
a speciﬁc random forest can be written as a kernel estimate and who exhibit
rates of consistency. However, despite these works, the literature is relatively
sparse regarding the link between forests and kernel methods.
Our objective in the present paper is to prove that a slight modiﬁcation of
random forest procedures have explicit and simple interpretations in terms of
kernel methods. Thus, the resulting kernel based on random forest (called KeRF
in the rest of the paper) estimates are more amenable to mathematical analysis.
They also appear to be empirically as accurate as random forest estimates. To
theoretically support these results, we also make explicit the expression of some
KeRF. We prove upper bounds on their rates of consistency, which compare
favorably to the existing ones.
The paper is organized as follows. Section 2 is devoted to notations and to the
deﬁnition of KeRF estimates. The link between KeRF estimates and random
forest estimates is made explicit in Section 3. In Section 4, two KeRF estimates
are presented and their consistency is proved along with their rate of consistency.
Section 5 contains experiments that highlight the good performance of KeRF
compared to their random forests counterparts. Proofs are postponed to Section
Notations and ﬁrst deﬁnitions
Throughout the paper, we assume to be given a training sample Dn = {(X1, Y1),
. . . , (Xn, Yn)} of d× R-valued independent random variables distributed
as the independent prototype pair (X, Y ), where E[Y 2] < ∞.
predicting the response Y , associated with the random variable X, by estimating
the regression function m(x) = E [Y |X = x]. In this context, we use inﬁnite
random forests (see the deﬁnition below) to build an estimate m∞,n : d →R
of m, based on the data set Dn.
A random forest is a collection of M randomized regression trees . For the j-th
tree in the family, the predicted value at point x is denoted by mn(x, Θj), where
Θ1, . . . , ΘM are independent random variables, distributed as a generic random
variable Θ, independent of the sample Dn. This random variable can be used
to sample the training set or to select the candidate directions or positions for
splitting. The trees are combined to form the ﬁnite forest estimate
mM,n(x, Θ1, . . . , ΘM) = 1
mn(x, Θj).
By the law of large numbers, for all x ∈ d, almost surely, the ﬁnite forest
estimate tends to the inﬁnite forest estimate
m∞,n(x) = EΘ [mn(x, Θ)] ,
where EΘ denotes the expectation with respect to Θ, conditionally on Dn.
As mentioned above, there is a large variety of forests, depending on how trees
are grown and how the random variable Θ inﬂuences the tree construction. For
instance, tree construction can be independent of Dn . On the other
hand, it can depend only on the Xi’s or on the whole training
set . Throughout
the paper, we use three important types of random forests to exemplify our
results: Breiman’s, centred and uniform forests. In Breiman’s original procedure, splits are performed to minimize the variances within the two resulting
cells. The algorithm stops when each cell contains less than a small pre-speciﬁed
number of points .
Centred forests are a simpler procedure which, at each node, uniformly select a
coordinate among {1, . . . , d} and performs splits at the center of the cell along
the pre-chosen coordinate. The algorithm stops when a full binary tree of level
k is built (that is, each cell is cut exactly k times), where k ∈N is a parameter
of the algorithm . Uniform
forests are quite similar to centred forests except that once a split direction is
chosen, the split is drawn uniformly on the side of the cell, along the preselected
coordinate .
Kernel based on random forests (KeRF)
To be more speciﬁc, random forest estimates satisfy, for all x ∈ d,
mM,n(x, Θ1, . . . , ΘM) = 1
Yi1Xi∈An(x,Θj)
where An(x, Θj) is the cell containing x, designed with randomness Θj and data
set Dn, and
Nn(x, Θj) =
1Xi∈An(x,Θj)
is the number of data points falling in An(x, Θj).
Note that, the weights
Wi,j,n(x) of each observation Yi deﬁned by
Wi,j,n(x) = 1Xi∈An(x,Θj)
depend on the number of observations Nn(x, Θj). Thus the contributions of
observations that are in cells with a high density of data points are smaller than
that of observations which belong to less populated cells. This is particularly
true for non adaptive forests (i.e., forests built independently of data) since the
number of observations in each cell cannot be controlled.
Giving important
weights to observations that are in low-density cells can potentially lead to
rough estimates. Indeed, as an extreme example, trees of non adaptive forests
can contain empty cells which leads to a substantial misestimation (since the
prediction in empty cells is set, by default, to zero).
In order to improve the random forest methods and compensate the misestimation induced by random forest weights, a natural idea is to consider KeRF
estimates deﬁned, for all x ∈ d, by
emM,n(x, Θ1, . . . , ΘM) =
j=1 Nn(x, Θj)
Yi1Xi∈An(x,Θj).
Note that ˜mM,n(x, Θ1, . . . , ΘM) is equal to the mean of the Yi’s falling in the
cells containing x in the forest.
Thus, each observation is weighted by the
number of times it appears in the trees of the forests. Consequently, in this
setting, an empty cell does not contribute to the prediction.
The proximity between KeRF estimates emM,n and random forest estimates will
be thoroughly discussed in Section 3. As for now, we focus on (3) and start
by proving that it is indeed a kernel estimate whose expression is given by
Proposition 1.
Proposition 1. Almost surely, for all x ∈ d, we have
emM,n(x, Θ1, . . . , ΘM) =
i=1 YiKM,n(x, Xi)
ℓ=1 KM,n(x, Xℓ) ,
KM,n(x, z) = 1
1z∈An(x,Θj).
We call KM,n the connection function of the M ﬁnite forest.
Proposition 1 states that KeRF estimates have a more interpretable form than
random forest estimates since their kernels are the connection functions of the
forests. This connection function can be seen as a geometrical characteristic
of the cells in the random forest. Indeed, ﬁxing Xi, the quantity KM,n(x, Xi)
is nothing but the empirical probability that Xi and x are connected (i.e. in
the same cell) in the M ﬁnite random forest. Thus, the connection function
is a natural way to build kernel functions from random forests, a fact that
had already been noticed by Breiman . Note that these kernel functions
have the nice property of being positive semi-deﬁnite, as proved by Davies and
Ghahramani .
A natural question is to ask what happens to KeRF estimates when the number
of trees M goes to inﬁnity. To this aim, we deﬁne inﬁnite KeRF estimates em∞,n
by, for all x,
em∞,n(x) = lim
M→∞emM,n(x, Θ1, . . . , ΘM).
In addition, we say that an inﬁnite random forest is discrete (resp. continuous)
if its connection function Kn is piecewise constant (resp.
continuous).
example, Breiman forests and centred forests are discrete but uniform forests
are continuous. Denote by PΘ the probability with respect to Θ, conditionally
on Dn. Proposition 2 extends the results of Proposition 1 to the case of inﬁnite
KeRF estimates.
Proposition 2.
Consider an inﬁnite discrete or continuous forest.
almost surely, for all x, z ∈ d,
M→∞KM,n(x, z) = Kn(x, z),
Kn(x, z) = PΘ [z ∈An(x, Θ)] .
We call Kn the connection function of the inﬁnite random forest. Thus, for all
x ∈ d, one has
em∞,n(x) =
i=1 YiKn(x, Xi)
ℓ=1 Kn(x, Xℓ) .
This lemma shows that inﬁnite KeRF estimates are kernel estimates with kernel
function equal to Kn. Observing that Kn(x, z) is the probability that x and z
are connected in the inﬁnite forest, the function Kn characterizes the shape of
the cells in the inﬁnite random forest.
Now that we know the expression of KeRF estimates, we are ready to study
how close this approximation is to random forest estimates. This link will be
further work out in Section 4 for centred and uniform KeRF and empirically
studied in Section 5.
Relation between KeRF and random forests
In this section, we investigate in which cases KeRF and forest estimates are
close to each other. To achieve this goal, we will need the following assumption.
(H1) Fix x ∈ d, and assume that Y ≥0 a.s.. Then, one of the following
two conditions holds:
(H1.1) There exist sequences (an), (bn) such that, a.s.,
an ≤Nn(x, Θ) ≤bn.
(H1.2) There exist sequences (εn), (an), (bn) such that, a.s.,
• 1 ≤an ≤EΘ [Nn(x, Θ)] ≤bn,
an ≤Nn(x, Θ) ≤bn
(H1) assumes that the number of points in every cell of the forest can be
bounded from above and below. (H1.1) holds for ﬁnite forests for which the
number of points in each cell is controlled almost surely. Typically, (H1.1) is
veriﬁed for adaptive random forests, if the stopping rule is properly chosen. On
the other hand, (H1.2) holds for inﬁnite forests. Note that the ﬁrst condition
EΘ [Nn(x, Θ)] ≥1 in (H1.2) is technical and is true if the level of each tree is
tuned appropriately. Several random forests which satisfy (H1) are discussed
Proposition 3 states that ﬁnite forest estimate mM,n and ﬁnite KeRF estimate
emM,n are close to each other assuming that (H1.1) holds.
Proposition 3.
Assume that (H1.1) is satisﬁed. Thus, almost surely,
mM,n(x, Θ1, . . . , ΘM)
emM,n(x, Θ1, . . . , ΘM) −1
with the convention that 0/0 = 1.
Since KeRF estimates are kernel estimates of the form (1), Proposition 3 stresses
that random forests are close to kernel estimates if the number of points in each
cell is controlled. As highlighted by the following discussion, the assumptions
of Proposition 3 are satisﬁed for some types of random forests.
Centred random forests of level k.
For this model, whenever X is uniformly distributed over d, each cell has a Lebesgue-measure of 2−k. Thus,
ﬁxing x ∈ d, according to the law of the iterated logarithm, for all n large
enough, almost surely,
Nn(x, Θ) −n
√2n log log n
Consequently, (H1.1) is satisﬁed for an = n2−k −√2n log log n/2 and bn =
n2−k + √2n log log n/2. This yields, according to Proposition 3, almost surely,
mM,n(x, Θ1, . . . , ΘM)
emM,n(x, Θ1, . . . , ΘM) −1
√2n log log n
n2−k −√2n log log n/2.
Thus, choosing for example k = (log2 n)/3, centred KeRF estimates are asymptotically equivalent to centred forest estimates as n →∞. The previous inequality can be extended to the case where X has a density f satisfying c ≤f ≤C,
for some constants 0 < c < C < ∞. In that case, almost surely,
mM,n(x, Θ1, . . . , ΘM)
emM,n(x, Θ1, . . . , ΘM) −1
√2n log log n + (C −c)n/2k
nc2−k −√2n log log n/2
However, the right-hand term does not tend to zero as n →∞, meaning that
the uniform assumption on X is crucial to prove the asymptotic equivalence of
mM,n and emM,n in the case of centred forests.
Breiman’s forests.
Each leaf in Breiman’s trees contains a small number of
points (typically between 1 and 5). Thus, if each cell contains exactly one point
(default settings in classiﬁcation problems), (H1.1) holds with an = bn = 1.
Thus, according to Proposition 3, almost surely,
mM,n(x, Θ1, . . . , ΘM) = emM,n(x, Θ1, . . . , ΘM).
More generally, if the number of observations in each cell varies between 1 and
5, one can set an = 1 and bn = 5. Thus, still by Proposition 3, almost surely,
mM,n(x, Θ1, . . . , ΘM)
emM,n(x, Θ1, . . . , ΘM) −1
Median forests of level k.
In this model, each cell of each tree is split at
the empirical median of the observations belonging to the cell. The process is
repeated until every cell is cut exactly k times (where k ∈N is a parameter
chosen by the user). Thus, each cell contains the same number of points ±2
 , and, according to Proposition
3, almost surely,
mM,n(x, Θ1, . . . , ΘM)
emM,n(x, Θ1, . . . , ΘM) −1
Consequently, if the level k of each tree is chosen such that an →∞as n →∞,
median KeRF estimates are equivalent to median forest estimates.
The following lemma extends Proposition 3 to inﬁnite KeRF and forest estimates.
Proposition 4. Assume that (H1.2) is satisﬁed. Thus, almost surely,
|m∞,n(x) −em∞,n(x)| ≤bn −an
em∞,n(x) + nεn
Considering inequalities provided in Proposition 4, we see that inﬁnite KeRF
estimates are close to inﬁnite random forest estimates if the number of observations in each cell is bounded (via an and bn).
It is worth noticing that controlling the number of observations in each cell while
obtaining a simple partition shape is diﬃcult to achieve. On the one hand, if
the tree construction depends on the training set, the algorithm can be stopped
when each leaf contains exactly one point and thus KeRF estimate is equal to
random forest estimate. However, in that case, the probability Kn(x, z) is very
diﬃcult to express since the geometry of each tree partitioning strongly depends
on the training set. On the other hand, if the tree construction is independent
of the training set, the probability Kn(x, z) can be made explicit in some cases,
for example for centred forests (see Section 5). However, the number of points in
each cell is diﬃcult to control (every leaf cannot contain exactly one point with
a non-adaptive cutting strategy) and thus KeRF estimate can be far away from
random forest estimate. Consequently, one cannot deduce an explicit expression
for random forest estimates from the explicit expression of KeRF estimates.
Two particular KeRF estimates
According to Proposition 2, inﬁnite KeRF estimate em∞,n depends only on the
connection function Kn via the following equation
em∞,n(x) =
i=1 YiKn(x, Xi)
ℓ=1 Kn(x, Xℓ) .
To take one step further into the understanding of KeRF, we study in this
section the connection function of two speciﬁc inﬁnite random forests. We focus
on inﬁnite KeRF estimates for two reasons. Firstly, the expressions of inﬁnite
KeRF estimates are more amenable to mathematical analysis since they do not
depend on the particular trees used to build the forest. Secondly, the prediction
accuracy of inﬁnite random forests is known to be better than that of ﬁnite
random forests . Therefore inﬁnite KeRF estimates are
likely to be more accurate than ﬁnite KeRF estimates.
Practically, both inﬁnite KeRF estimates and inﬁnite random forest estimates
can only be approximated by Monte Carlo simulations. Here, we show that
centred KeRF estimates have an explicit expression, that is their connection
function can be made explicit. Thus, inﬁnite centred KeRF estimates and inﬁnite uniform KeRF estimates (up to an approximation detailed below) can be
directly computed using equation (7).
Centred KeRF
As seen above, the construction of centred KeRF of level k
is the same as for centred forests of level k except that predictions are made according to equation (3). Centred random forests are closely related to Breiman’s
forests in a linear regression framework. Indeed, in this context, splits that are
performed at a low level of the trees are roughly located at the middle of each
cell. In that case, Breiman’s forests and centred forests are close to each other,
which justiﬁes the interest for these simpliﬁed models, and thus for centred
In the sequel, the connection function of the centred random forest of level k
is denoted by Kcc
k . This notation is justiﬁed by the fact that the construction
of centred KeRF estimates depends only on the size of the training set through
the choice of k.
Proposition 5. Let k ∈N and consider an inﬁnite centred random forest of
level k. Then, for all x, z ∈ d,
k (x, z) =
k1! . . . kd!
1⌈2kj xj⌉=⌈2kj zj⌉.
Note that ties are broken by imposing that cells are of the form Qd
i=1 Ai where
the Ai are equal to ]ai, bi] or [0, bi], for all 0 < ai < bi ≤1. Figure 1 shows a
graphical representation of the function f deﬁned as
 × 
z = (z1, z2)
Figure 1: Representations of f1, f2 and f5 in 2
Denote by emcc
∞,n the inﬁnite centred KeRF estimate, associated with the connection function Kcc
k , deﬁned as
k (x, Xℓ) .
To pursue the analysis of emcc
∞,n, we will need the following assumption on the
regression model.
(H2) One has
Y = m(X) + ε,
where ε is a centred Gaussian noise, independent of X, with ﬁnite variance
σ2 < ∞. Moreover, X is uniformly distributed on d and m is Lipschitz.
Our theorem states that inﬁnite centred KeRF estimates are consistent whenever
(H2) holds. Moreover, it provides an upper bound on the rate of consistency
of centred KeRF.
Theorem 4.1. Assume that (H2) is satisﬁed. Then, providing k →∞and
n/2k →∞, there exists a constant C1 > 0 such that, for all n > 1, and for all
x ∈ d,
∞,n(x) −m(x)
2 ≤C1n−1/(3+d log 2)(log n)2.
Observe that centred KeRF estimates fail to reach minimax rate of consistency
n−2/(d+2) over the class of Lipschitz functions. A similar upper bound on the
rate of consistency n−3/4d log 2+3 of centred random forests was obtained by Biau
 . It is worth noticing that, for all d ≥9, the upper bound on the rate
of centred KeRF is sharper than that of centred random forests. This theoretical result supports the fact that KeRF procedure has a better performance
compared to centred random forests. This will be supported by simulations in
Section 5 (see Figure 5)
Uniform KeRF
Recall that the inﬁnite uniform KeRF estimates of level k
are the same as inﬁnite uniform forest of level k except that predictions are
computed according to equation (3). Uniform random forests, ﬁrst studied by
Biau et al. , remain under active investigation. They are a nice modelling
of Breiman forests, since with no a priori on the split location, we can consider
that splits are drawn uniformly on the cell edges.
Other related versions of
these forests have been thoroughly investigated by Arlot and Genuer 
who compare the bias of a single tree to that of the whole forest.
As for the connection function of centred random forests, we use the notational
convention Kuf
to denote the connection function of uniform random forests of
Proposition 6. Let k ∈N and consider an inﬁnite uniform random forest of
level k. Then, for all x ∈ d,
k (0, x) =
k1! . . . kd!
with the convention P−1
Proposition 6 gives the explicit expression of Kuf
k (0, x). Figure 2 shows a representation of the functions f1, f2 and f5 deﬁned as
 × 
z = (z1, z2)
where |z −x| = (|z1 −x1|, . . . , |zd −xd|).
Unfortunately, the general expression of the connection function Kuf
k (x, z) is
diﬃcult to obtain. Indeed, for d = 1, cuts are performed along a single axis,
but the probability of connection between two points x and z does not depend
only upon the distance |z −x| but rather on the positions x and z, as stressed
in the following Lemma.
Figure 2: Representations of f1, f2 and f5 in dimension two
Lemma 1. Let x, z ∈ . Then,
1 (x, z) = 1 −|z −x|,
2 (x, z) = 1 −|z −x| + |z −x| log
A natural way to deal with this diﬃculty is to replace the connection function
by the function (x, z) →Kuf
k (0, |z −x|). Indeed, this is a simple manner
to build an invariant-by-translation version of the uniform kernel Kuf
extensive simulations in Section 5 support the fact that estimates of the form
(7) built with these two kernels have similar prediction accuracy. As for inﬁnite centred KeRF estimates, we denote by emuf
∞,n the inﬁnite uniform KeRF
estimates but built with the invariant-by-translation version of Kuf
k , namely
k (0, |Xi −x|)
k (0, |Xℓ−x|)
Our last theorem states the consistency of inﬁnite uniform KeRF estimates along
with an upper bound on their rate of consistency.
Theorem 4.2. Assume that (H2) is satisﬁed. Then, providing k →∞and
n/2k →∞, there exists a constant C1 > 0 such that, for all n > 1 and for all
x ∈ d,
∞,n(x) −m(x)
2 ≤C1n−2/(6+3d log 2)(log n)2.
As for centred KeRF estimates, the rate of consistency does not reach the minimax rate on the class of Lipschitz functions, and is actually worse than that of
centred KeRF estimates, whatever the dimension d is. Besides, centred KeRF
estimates have better performance than uniform KeRF estimates and this will
be highlighted by simulations (Section 5).
Although centred and uniform KeRF estimates are kernel estimates of the form
(1), the usual tools used to prove consistency and to ﬁnd rate of consistency
of kernel methods cannot be applied here . Indeed, the support of z 7→Kcc
k (x, z) and that of z 7→Kuf
k (0, |z −x|)
cannot be contained in a ball centred on x, whose diameter tends to zero (see
Figure 1 and 2). The proof of Theorem 4.1 and 4.2 are then based on the previous work of Greblicki et al. who proved the consistency of kernels with
unbounded support. In particular, we use their bias/variance decomposition of
kernel estimates to exhibit upper bounds on the rate of consistency.
Experiments
Practically speaking, Breiman’s random forests are among the most widely used
forest algorithms. Thus a natural question is to know whether Breiman KeRF
compare favourably to Breiman’s forests. In fact, as seen above, the two algorithms coincide whenever Breiman’s forests are fully grown. But this is not
always the case since by default, each cell of Breiman’s forests contain between
1 and 5 observations.
We start this section by comparing Breiman KeRF and Breiman’s forest estimates for various regression models described below. Some of these models are
toy models (Model 1, 5-8). Model 2 can be found in van der Laan et al.
 and Models 3-4 are presented in Meier et al. . For all regression frameworks, we consider covariates X = (X1, . . . , Xd) that are uniformly
distributed over d. We also let e
Xi = 2(Xi −0.5) for 1 ≤i ≤d.
Model 1: n = 800, d = 50, Y = e
1 + exp(−e
Model 2: n = 600, d = 100, Y = e
6 +N(0, 0.5)
Model 3: n = 600, d = 100, Y = −sin(2 e
X3 −exp(−e
X4)+N(0, 0.5)
Model 4: n = 600, d = 100, Y = e
X2−1)2+sin(2π e
X3)/(2−sin(2π e
X4) + 2 cos(2π e
X4) + 3 sin2(2π e
X4) + 4 cos2(2π e
X4) + N(0, 0.5)
n = 700, d = 20, Y
2) + N(0, 0.5)
Model 6: n = 500, d = 30, Y = P10
k<0 −1N(0,1)>1.25
Model 7: n = 600, d = 300, Y = e
X3 exp(−| e
X8 +N(0, 0.5)
Model 8: n = 500, d = 1000, Y = e
3 −2 exp(−e
All numerical implementations have been performed using the free Python software, available online at For each experiment, the
data set is divided into a training set (80% of the data set) and a test set (the
remaining 20%). Then, the empirical risk (L2 error) is evaluated on the test
To start with, Figure 3 depicts the empirical risk of Breiman’s forests and
Breiman KeRF estimates for two regression models (the conclusions are similar
for the remaining regression models). Default settings were used for Breiman’s
forests (minsamplessplit = 2 , maxfeatures = 0.333) and for Breiman KeRF,
except that we did not bootstrap the data set.
Figure 3 puts in evidence
that Breiman KeRF estimates behave similarly (in terms of empirical risk) to
Breiman forest estimates. It is also interesting to note that bootstrapping the
data set does not change the performance of the two algorithms.
Figure 4 (resp.
Figure 5) shows the risk of uniform (resp.
centred) KeRF
estimates compared to the risk of uniform (resp. centred) forest estimates (only
Figure 3: Empirical risks of Breiman KeRF estimates and Breiman forest estimates.
two models shown). In these two experiments, uniform and centred forests and
their KeRF counterparts have been grown in such a way that each tree is a
complete binary tree of level k = ⌊log2 n⌋. Thus, in that case, each cell contains
on average n/2k ≃1 observation. Once again, the main message of Figure 4 is
that the uniform KeRF accuracy is close to the uniform forest accuracy.
Figure 4: Empirical risks of uniform KeRF and uniform forest.
On the other hand, it turns out that the performance of centred KeRF and
centred forests are not similar (Figure 5). In fact, centred KeRF estimates are
either comparable to centred forest estimates (as, for example, in Model 2), or
have a better accuracy (as, for example, in Model 1). A possible explanation
for this phenomenon is that centred forests are non-adaptive in the sense that
their construction does not depend on the data set.
Therefore, each tree is
likely to contain cells with unbalanced number of data points, which can result
in random forest misestimation. This undesirable eﬀect vanishes using KeRF
methods since they assign the same weights to each observation.
The same series of experiments were conducted, but using bootstrap for computing both KeRF and random forest estimates. The general ﬁnding is that
the results are similar—Figure 6 and 7 depict the accuracy of corresponding
algorithms for a selected choice of regression frameworks.
An important aspect of inﬁnite centred and uniform KeRF is that they can be
explicitly computed (see Proposition 5 and 6). Thus, we have plotted in Figure
Figure 5: Empirical risks of centred KeRF and centred forest.
Figure 6: Empirical risks of uniform KeRF and uniform forest (with bootstrap).
Figure 7: Empirical risks of centred KeRF and centred forests (with bootstrap).
8 the empirical risk of both ﬁnite and inﬁnite centred KeRF estimates for some
examples (for n = 100 and d = 10).
We clearly see in this ﬁgure that the
accuracy of ﬁnite centred KeRF tends to the accuracy of inﬁnite centred KeRF
as M tends to inﬁnity. This corroborates Proposition 2.
The same comments hold for uniform KeRF (see Figure 9). Note however that,
in that case, the proximity between ﬁnite uniform KeRF and inﬁnite uniform
KeRF estimate strengthens the approximation that has been made on inﬁnite
uniform KeRF in Section 4.
The computation time for ﬁnite KeRF estimate is very acceptable for ﬁnite
Figure 8: Risks of ﬁnite and inﬁnite centred KeRF.
Figure 9: Risks of ﬁnite and inﬁnite uniform KeRF.
KeRF and similar to that of random forest (Figure 3-5). However, the story
is diﬀerent for inﬁnite KeRF estimates. In fact, KeRF estimates can only be
evaluated for low dimensional data sets and small sample sizes. To see this, just
note that the explicit formulation of KeRF involves a multinomial distribution
(Proposition 5 and 6). Each evaluation of the multinomial creates computational burden when the dimensions (d and n) of the problems increases. For
example, in Figure 8 and 9, the computation time needed to compute inﬁnite
KeRF estimates ranges between thirty minutes to 3 hours. As a matter of fact,
inﬁnite KeRF methods should be seen as theoretical tools rather than a practical
substitute for random forests.
Proof of Proposition 1. By deﬁnition,
emM,n(x, Θ1, . . . , ΘM) =
i=1 1Xi∈An(x,Θj)
Yi1Xi∈An(x,Θj)
i=1 1Xi∈An(x,Θj)
YiKM,n(x, Xi).
Finally, observe that
1Xi∈An(x,Θj) =
KM,n(x, Xi),
which concludes the proof.
Proof of Proposition 2. We prove the result for d = 2.
The other cases can
be treated similarly.
For the moment, we assume the random forest to be
continuous. Recall that, for all x, z ∈ 2, and for all M ∈N,
KM,n(x, z) = 1
1z∈An(x,Θj).
According to the strong law of large numbers, almost surely, for all x, z ∈
Q2 ∩ 2
M→∞KM,n(x, z) = Kn(x, z).
Set ε > 0 and x, z ∈ 2 where x = (x(1), x(2)) and z = (z(1), z(2)). Assume,
without loss of generality, that x(1) < z(1) and x(2) < z(2). Let
Ax = {u ∈ 2, u(1) ≤x(1) and u(2) ≤x(2)},
and Az = {u ∈ 2, u(1) ≥z(1) and u(2) ≥z(2)}.
Choose x1 ∈Ax ∩Q2 (resp. z2 ∈Az ∩Q2) and take x2 ∈ 2 ∩Q2 (resp.
z1 ∈ 2 ∩Q2) such that x(1)
≤x(1) ≤x(1)
≤x(2) ≤x(2)
≤z(1) ≤z(1)
≤z(2) ≤z(2)
2 , see Figure 10).
Figure 10: Respective positions of x, x1, x2 and z, z1, z2
Observe that, because of the continuity of Kn, one can choose x1, x2 close
enough to x and z2, z1 close enough to z such that
|Kn(x2, x1) −1| ≤ε,
|Kn(z1, z2) −1| ≤ε,
and |Kn(x1, z2) −Kn(x, z)| ≤ε.
Bounding the diﬀerence between KM,n and Kn, we have
|KM,n(x, z) −Kn(x, z)| ≤|KM,n(x, z) −KM,n(x1, z2)|
+ |KM,n(x1, z2) −Kn(x1, z2)|
+ |Kn(x1, z2) −Kn(x, z)| .
To simplify notation, we let x
↔z be the event where x and z are in the same
cell in the tree built with randomness Θj and dataset Dn. We also let x
be the complement event of x
↔z. Accordingly, the ﬁrst term on the right
side in equation (8) is bounded above by
|KM,n(x, z) −KM,n(x1, z2)| ≤1
(given the positions of x, x1, z, z2)
given the respective positions of x, x1, x2 and z, z1, z2. But, since x2, z1, x1, z2 ∈
Q2 ∩ 2, we deduce from inequation (9) that, for all M large enough,
|KM,n(x, z) −KM,n(x1, z2)| ≤1 −Kn(x2, x1) + 1 −Kn(z1, z2) + 2ε.
Combining the last inequality with equation (8), we obtain, for all M large
|KM,n(x, z) −Kn(x, z)| ≤1 −Kn(x2, x1) + 1 −Kn(z1, z2)
+ |KM,n(x1, z2) −Kn(x1, z2)|
+ |Kn(x1, z2) −Kn(x, z)| + 2ε
Consequently, for any continuous random forest, almost surely, for all x, z ∈
M→∞KM,n(x, z) = Kn(x, z).
The proof can be easily adapted to the case of discrete random forests. Thus,
this complete the ﬁrst part of the proof. Next, observe that
i=1 YiKM,n(x, Xi)
j=1 KM,n(x, Xj) =
i=1 YiKn(x, Xi)
j=1 Kn(x, Xj) ,
for all x satisfying Pn
j=1 Kn(x, Xj) ̸= 0. Thus, almost surely for those x,
M→∞emM,n(x) = em∞,n(x).
Now, if there exists any x such that Pn
j=1 Kn(x, Xj) = 0, then x is not connected with any data points in any tree of the forest.
In that case, Pn
KM,n(x, Xj) = 0 and, by convention, em∞,n(x) = emM,n(x) = 0. Finally, formula (10) holds for all x ∈ 2.
Proof of Proposition 3. Fix x ∈ d and assume that, a.s., Y ≥0. By assumption (H1.1), there exist sequences (an), (bn) such that, almost surely,
an ≤Nn(x, Θ) ≤bn.
To simplify notation, we let ¯NM,n(x, Θ) =
j=1 Nn(x, Θj). Thus, almost
|mM,n(x) −emM,n(x)| =
1Xi∈An(x,Θm)
1Xi∈An(x,Θm)
1Xi∈An(x,Θm)
Nn(x, Θm) −1
Proof of Proposition 4. Fix x ∈ d and assume that, almost surely, Y ≥0.
By assumption (H1.2), there exist sequences (an), (bn), (εn) such that, letting
A be the event where
an ≤Nn(x, Θ) ≤bn,
we have, almost surely,
PΘ[A] ≥1 −εn
1 ≤an ≤EΘ [Nn(x, Θ)] ≤bn.
Therefore, a.s.,
|m∞,n(x) −em∞,n(x)|
1Xi∈An(x,Θ)
 1Xi∈An(x,Θ)
EΘ [Nn(x, Θ)]
 1Xi∈An(x,Θ)
EΘ [Nn(x, Θ)]
EΘ [Nn(x, Θ)]
(1A + 1Ac)
em∞,n(x) +
EΘ [Nn(x, Θ)]
em∞,n(x) + n
Consequently, almost surely,
|m∞,n(x) −em∞,n(x)| ≤bn −an
em∞,n(x) + nεn
Proof of Proposition 5. Assume for the moment that d = 1. Take x, z ∈ 
and assume, without loss of generality, that x ≤z. Then the probability that x
and z be in the same cell, after k cuts, is equal to
1⌈2kj xj⌉=⌈2kj zj⌉.
To prove the result in the multivariate case, take x, z ∈ d. Since cuts are
independent, the probability that x and z are in the same cell after k cuts is
given by the following multinomial
k (x, z) =
k1! . . . kd!
1⌈2kj xj⌉=⌈2kj zj⌉.
To prove Theorem 4.1, we need to control the bias of the centred KeRF estimate,
which is done in Theorem 6.1.
Theorem 6.1. Let f be a L-Lipschitz function. Then, for all k,
 d Kcc
k (x, z)f(z)dz1 . . . dzd
 d Kcc
k (x, z)dz1 . . . dzd
Proof of Theorem 6.1. Let x ∈ d and k ∈N. Take f a L-Lipschitz function.
In the rest of the proof, for clarity reasons, we use the notation dz instead of
dz1 . . . dzd. Thus,
 d Kcc
k (x, z)f(z)dz
 d Kcc
k (x, z)dz
 d Kcc
k (x, z)|f(z) −f(x)|dz
 d Kcc
k (x, z)dz
Note that,
 d Kcc
k (x, z)|f(z) −f(x)|dz
 d Kcc
k (x, z)|zℓ−xℓ|dz
k1! . . . kd!
km(xm, zm)dzm
kℓ(xℓ, zℓ)|zℓ−xℓ|dzℓ.
The last integral is upper bounded by
kℓ(xℓ, zℓ)|xℓ−zℓ|dzℓ=
1⌈2kℓxℓ⌉=⌈2kℓzℓ⌉|xℓ−zℓ|dzℓ
1⌈2kℓxℓ⌉=⌈2kℓzℓ⌉dzℓ
kℓ(xℓ, zℓ)dzℓ.
Therefore, combining inequalities (11) and (12), we obtain,
 d Kcc
k (x, z)|f(z) −f(x)|dz
k1! . . . kd!
km(xm, zm)dzm
k1! . . . kd!
since, simple calculations show that, for all xm ∈ and for all km ∈N,
km(xm, zm)dzm =
1⌈2kmxm⌉=⌈2kmzm⌉dzm =
Consequently, we get from inequality (13) that
 d Kcc
k (x, z)|f(z) −f(x)|dz
 d Kcc
k (x, z)dz
k1! . . . kd!
Taking the ﬁrst term of the sum, we obtain
k1! . . . kd!
k1!(k −k1)!
 d Kcc
k (x, z)|f(z) −f(x)|dz
 d Kcc
k (x, z)dz
Proof of Theorem 4.1. Let x ∈ d, ∥m∥∞=
x∈ d |m(x)| and recall that
k (x, Xi) .
Thus, letting
k (x, X)] −E [Y Kcc
k (x, X)] −1
and Mn(x) = E [Y Kcc
k (x, X)] ,
the estimate emcc
∞,n(x) can be rewritten as
∞,n(x) = Mn(x) + An(x)
which leads to
∞,n(x) −m(x) = Mn(x) −m(x) + An(x) −Bn(x)m(x)
According to Theorem 6.1, we have
|Mn(x) −m(x)| =
E [m(X)Kcc
k (x, X)] −m(x)
E [m(X)Kcc
where C1 = Ld. Take α ∈]0, 1/2]. Let Cα(x) be the event on which
|Bn(x)| ≤α
. On the event Cα(x), we have
∞,n(x) −m(x)|2 ≤8|Mn(x) −m(x)|2 + 8|An(x) −Bn(x)m(x)|2
+ 8α2(1 + ∥m∥∞)2.
∞,n(x) −m(x)|21Cα(x)] ≤8C2
+ 8α2(1 + ∥m∥∞)2.
Consequently, to ﬁnd an upper bound on the rate of consistency of emcc
just need to upper bound
∞,n(x) −m(x)|21Ccα(x)
1≤i≤n Yi + m(x)|21Ccα(x)
(since emcc
∞,n is a local averaging estimate)
|2∥m∥∞+ max
1≤i≤n εi|21Ccα(x)
2∥m∥∞+ max
(by Cauchy-Schwarz inequality)
Simple calculations on Gaussian tails show that one can ﬁnd a constant C > 0
such that for all n,
≤C(log n)2.
Thus, there exists C2 such that, for all n > 1,
∞,n(x) −m(x)|21Ccα(x)
≤C2(log n)(P [Cc
α(x)])1/2.
The last probability P [Cc
α(x)] can be upper bounded by using Chebyshev’s
inequality. Indeed, with respect to An(x),
|An(x)| > α
k (x, X)] −E [Y Kcc
k (x, X)])2 E
k (x, X)])2
k (x, X)])2
x,z∈ d Kcc
k (x, z) ≤1)
(according to inequality (14)),
∞+ σ2. Meanwhile with respect to Bn(x), we obtain, still by
Chebyshev’s inequality,
|Bn(x)| > α
x,z∈ d Kcc
k (x, z) ≤1)
Thus, the probability of Cα(x) is given by
 |An(x)| ≥α
 |Bn(x)| ≥α
≥1 −2k(2M 2
Consequently, according to inequality (16), we obtain
∞,n(x) −m(x)|21Ccα(x)
≤C2(log n)
Then using inequality (15),
∞,n(x) −m(x)
∞,n(x) −m(x)|21Cα(x)
∞,n(x) −m(x)|21Ccα(x)
+ 8α2(1 + ∥m∥∞)2 + C2(log n)
Optimizing the right hand side in α, we get
∞,n(x) −m(x)
(log n)22k
for some constant C3 > 0. The last expression is minimized for
where C4 =
. Consequently, there exists a constant
C5 such that, for all n > 1,
∞,n(x) −m(x)
d log 2+3 (log n)2.
Proof of Lemma 1. Let x, z ∈ such that x < z. The ﬁrst statement comes
from the fact that splits are drawn uniformly over . To address the second
one, denote by Z1 (resp. Z2) the position of the ﬁrst (resp. second) split used
to build the cell containing [x, z]. Observe that, given Z1 = z1, Z2 is uniformly
distributed over [z1, 1] (resp. [0, z1]) if z1 ≤x (resp. z1 ≥z). Thus, we have
2 (x, z) =
The ﬁrst term takes the form
= x −(1 −x) log(1 −x).
Similarly, one has
dz1dz2 = (1 −z) log(1 −x),
dz1dz2 = (1 −z) + z log z,
dz1dz2 = −x log z.
Consequently,
2 (x, z) = x −(1 −x) log(1 −x) + (1 −z) log(1 −x)
−x log z + (1 −z) + z log z
= 1 −(z −x) + (z −x) log
Proof of Proposition 6. The result is proved in Technical Proposition 2 in Scornet .
To prove Theorem 4.2, we need to control the bias of uniform KeRF estimates,
which is done in Theorem 6.2.
Theorem 6.2. Let f be a L-Lipschitz function. Then, for all k,
 d Kuf
k (0, |z −x|)f(z)dz1 . . . dzd
 d Kuf
k (0, |z −x|)dz1 . . . dzd
Proof of Theorem 6.2. Let x ∈ d and k ∈N. Let f be a L-Lipschitz function. In the rest of the proof, for clarity reasons, we use the notation dz instead
of dz1 . . . dzd. Thus,
 d Kuf
k (0, |z −x|)f(z)dz
 d Kuf
k (0, |z −x|)dz
 d Kuf
k (0, |z −x|)|f(z) −f(x)|dz
 d Kuf
k (0, |z −x|)dz
Note that,
 d Kuf
k (0, |z −x|)|f(z) −f(x)|dz
 d Kuf
k (0, |z −x|)|zℓ−xℓ|dz
k1! . . . kd!
km(0, |zm −xm|)|zℓ−xℓ|dz
k1! . . . kd!
km(0, |zm −xm|)dzm
kℓ(0, |zℓ−xℓ|)|zℓ−xℓ|dzℓ
k1! . . . kd!
km(0, |zm −xm|)dzm
(according to the second statement of Lemma 2, see below)
k1! . . . kd!
according to the ﬁrst statement of Lemma 2.
Still by Lemma 2 and using
inequality (17), we have,
 d Kuf
k (0, |z −x|)|f(z) −f(x)|dz
 d Kuf
k (0, |z −x|)dz
k1! . . . kd!
Taking the ﬁrst term of the sum, we obtain
k1! . . . kd!
k1!(k −k1)!
 d Kuf
k (0, |z −x|)|f(z) −f(x)|dz
 d Kuf
k (0, |z −x|)dz
Proof of Theorem 4.2. Let x ∈ d, ∥m∥∞=
x∈ d |m(x)| and recall that
k (0, |Xi −x|)
k (0, |Xi −x|)
Thus, letting
k (0, |Xi −x|)
k (0, |X −x|)
k (0, |X −x|)
k (0, |X −x|)
k (0, |Xi −x|)
k (0, |X −x|)
and Mn(x) = E
k (0, |X −x|)
k (0, |X −x|)
the estimate muf
∞,n(x) can be rewritten as
∞,n(x) = Mn(x) + An(x)
which leads to
∞,n(x) −m(x) = Mn(x) −m(x) + An(x) −Bn(x)m(x)
Note that, according to Theorem 6.2, we have
|Mn(x) −m(x)| =
k (0, |X −x|)]
k (0, |X −x|)]
k (0, |X −x|)]
k (0, |X −x|)]
k (0, |X −x|)]
k (0, |X −x|)]
where C1 = L22d+1/3.
Take α ∈]0, 1/2].
Let Cα(x) be the event on which
|An(x)|, |Bn(x)| ≤α
. On the event Cα(x), we have
∞,n(x) −m(x)|2 ≤8|Mn(x) −m(x)|2 + 8|An(x) −Bn(x)m(x)|2
+ 8α2(1 + ∥m∥∞)2.
∞,n(x) −m(x)|21Cα(x)] ≤8C2
+ 8α2(1 + ∥m∥∞)2.
Consequently, to ﬁnd an upper bound on the rate of consistency of muf
just need to upper bound
∞,n(x) −m(x)|21Ccα(x)
1≤i≤n Yi + m(x)|21Ccα(x)
(since emuf
∞,n is a local averaging estimate)
|2∥m∥∞+ max
1≤i≤n εi|21Ccα(x)
2∥m∥∞+ max
(by Cauchy-Schwarz inequality)
Simple calculations on Gaussian tails show that one can ﬁnd a constant C > 0
such that for all n,
≤C(log n)2.
Thus, there exists C2 such that, for all n > 1,
∞,n(x) −m(x)|21Ccα(x)
≤C2(log n)(P [Cc
α(x)])1/2.
The last probability P [Cc
α(x)] can be upper bounded by using Chebyshev’s
inequality. Indeed, with respect to An(x),
|An(x)| > α
k (0, |X −x|)
k (0, |X −x|)]
k (0, |X −x|)]
k (0, |X −x|)]
k (0, |X −x|)])2 E
k (0, |X −x|)2i
k (0, |X −x|)])2
E[m(X)2Kuf
k (0, |X −x|)2]
k (0, |X −x|)2]
which leads to
|An(x)| > α
k (0, |X −x|)]
k (0, |X −x|)])2
x,z∈ d Kuf
k (0, |z −x|) ≤1)
(according to the ﬁrst statement of Lemma 2),
1 = 2d+1(∥m∥2
∞+σ2). Meanwhile with respect to Bn(x), we have, still
by Chebyshev’s inequality,
|Bn(x)| > α
k (0, |Xi −x|)
k (0, |X −x|)]
k (0, |X −x|)]
Thus, the probability of Cα(x) is given by
 |An(x)| ≥α
 |Bn(x)| ≥α
≥1 −2k(M 2
Consequently, according to inequality (19), we obtain
∞,n(x) −m(x)|21Ccα(x)
≤C2(log n)
Then using inequality (18),
∞,n(x) −m(x)
∞,n(x) −m(x)|21Cα(x)
∞,n(x) −m(x)|21Ccα(x)
+ 8α2(1 + ∥m∥∞)2 + C2(log n)
Optimizing the right hand side in α, we get
∞,n(x) −m(x)
(log n)22k
for some constant C3 > 0. The last expression is minimized for
where C4 = −3
Thus, there exists a constant
C5 > 0 such that, for all n > 1,
∞,n(x) −m(x)
2 ≤Cn−2/(6+3d log 2)(log n)2.
Lemma 2. For all k ∈N and x ∈ ,
kl (0, |zl −xl|)dz ≤
kl (0, |zl −xl|)|xl −zl|dzl ≤
kl (0, |zl −xl|)dzl.
Proof of Lemma 2. Let kl ∈N and xl ∈ . We start by proving (i). According to Proposition 6, the connection function of uniform random forests of level
kl takes the form
kl (0, |zl −xl|)dzl =
−log(1−xl)
−2 log(1−xl)
(−2 log xl)i
(−2 log(1 −xl))i
Therefore,
kl (0, |zl −xl|)dzl ≤
kl (0, |zl −xl|)dzl ≥
l + (1 −xl)2 1
Regarding the second statement of Lemma 2, we have
kl (0, |zl −xl|)|xl −zl|dzl
(−log |xl −zl|)j
(−log |xl −zl|)j
(−log |xl −zl|)j
−log(1−xl)
−3 log(xl)/2
−3 log(1−xl)/2
−log(1−xl)
kl (0, |zl −xl|)dzl.
Acknowledgments
We would like to thank Arthur Pajot for his great help in the implementation
of KeRF estimates.