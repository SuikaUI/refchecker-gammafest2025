Greedy Sparsity-Constrained Optimization
Sohail Bahmani
 
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Bhiksha Raj
 
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Petros Boufounos
 
Mitsubishi Electric Research Laboratories
Boston, MA 02139, USA
Sparsity-constrained optimization has wide applicability in machine learning, statistics, and
signal processing problems such as feature selection and Compressive Sensing. A vast body
of work has studied the sparsity-constrained optimization from theoretical, algorithmic,
and application aspects in the context of sparse estimation in linear models where the
ﬁdelity of the estimate is measured by the squared error. In contrast, relatively less eﬀort
has been made in the study of sparsity-constrained optimization in cases where nonlinear
models are involved or the cost function is not quadratic.
In this paper we propose a
greedy algorithm, Gradient Support Pursuit (GraSP), to approximate sparse minima of
cost functions of arbitrary form. Should a cost function have a Stable Restricted Hessian
(SRH) or a Stable Restricted Linearization (SRL), both of which are introduced in this
paper, our algorithm is guaranteed to produce a sparse vector within a bounded distance
from the true sparse optimum. Our approach generalizes known results for quadratic cost
functions that arise in sparse linear regression and Compressive Sensing. We also evaluate
the performance of GraSP through numerical simulations on synthetic data, where the
algorithm is employed for sparse logistic regression with and without ℓ2-regularization.
Sparsity, Optimization, Compressed Sensing, Greedy Algorithm
1. Introduction
The demand for high-dimensional data analysis has grown signiﬁcantly over the past decade
by the emergence of applications such as social networking, bioinformatics, and mathematical ﬁnance. In these applications data samples often have thousands of features using which
an underlying parameter must be inferred or predicted. In many circumstances the number
of collected samples is signiﬁcantly smaller than the dimensionality of the data, rendering
any inference from the data ill-posed. However, it is widely acknowledged that the data sets
that need to be processed usually exhibit signiﬁcant structure, which sparsity models are
often able to capture. This structure can be exploited for robust regression and hypothesis
 
testing, model reduction and variable selection, and more eﬃcient signal acquisition in underdetermined regimes. Estimation of parameters with sparse structure is usually cast as
an optimization problem, formulated according to speciﬁc application requirements. Developing techniques that are robust and computationally tractable to solve these optimization
problems, even only approximately, is therefore critical.
In particular, theoretical and application aspects of sparse estimation in linear models
have been studied extensively in areas such as signal processing, machine learning, and
statistics.
However, sparse estimation in problems where nonlinear models are involved
have received comparatively little attention. Most of the work in this area extend the use
of the ℓ1-norm as a regularizer, eﬀective to induce sparse solutions in linear regression, to
problems with nonlinear models . As a special case, logistic regression with ℓ1 and elastic net
regularization are studied by Bunea . Furthermore, Kakade et al. have studied the accuracy of sparse estimation through ℓ1-regularization for the exponential family
distributions. A more general frame of study is proposed and analyzed by Negahban et al.
 where regularization with “decomposable” norms is considered in M-estimation problems. To provide the accuracy guarantees, these works generalize the Restricted Eigenvalue
condition to ensure that the loss function is strongly convex over a
restriction of its domain. We would like to emphasize that these suﬃcient conditions generally hold with proper constants and with high probability only if one assumes that the
true parameter is bounded. This fact is more apparent in some of the mentioned work , while in some others the
assumption is not explicitly stated. We will elaborate on this matter in §2. Tewari et al.
 also proposed a coordinate-descent type algorithm for minimization of a convex and
smooth objective over the convex signal/parameter models introduced in (Chandrasekaran
et al.). This formulation includes the ℓ1-constrained minimization as a special case, and the
algorithm is shown to converge to the minimum in objective value similar to the standard
results in convex optimization.
Furthermore, Shalev-Shwartz et al. proposed a number of greedy that sparsify
a given estimate at the cost of relatively small increase of the objective function. However, their algorithms are not stand-alone. A generalization of Compressed Sensing is also
proposed in , where the linear measurement operator is replaced by a
nonlinear operator that applies to the sparse signal. Considering the norm of the residual error as the objective, Blumensath shows that if the objective satisﬁes certain
suﬃcient conditions, the sparse signal can be accurately estimated by a generalization of
the Iterative Hard Thresholding algorithm . The formulation of , however, has a limited scope because the metric of error is
deﬁned using a norm. For instance, the formulation does not apply to objectives such as
the logistic loss. More recently, Jalali et al. studied a forward-backward algorithm
using a variant of the suﬃcient conditions introduced in . Similar
to our work, the main result in imposes conditions on the function as
restricted to sparse inputs whose non-zeros are fewer than a multiple of the target sparsity
level. The multiplier used in their results has an objective-dependent value and is never less
than 10. Furthermore, the multiplier is important in their analysis not only for determining the stopping condition of the algorithm, but also in the lower bound assumed for the
minimal magnitude of the non-zero entries. In contrast, the multiplier in our results is ﬁxed
at 4, independent of the objective function itself, and we make no assumptions about the
magnitudes of the non-zero entries.
This paper presents an extended version with improved guarantees of our prior work
in , where we proposed a greedy algorithm, the Gradient Support
Pursuit (GraSP), for sparse estimation problems that arise in applications with general
nonlinear models. We prove the accuracy of GraSP for a class of cost functions that have a
Stable Restricted Hessian (SRH). The SRH, introduced in , characterizes the functions whose restriction to sparse canonical subspaces have well-conditioned
Hessian matrices. Similarly, we analyze the GraSP algorithm for non-smooth functions that
have a Stable Restricted Linearization (SRL), a property introduced in this paper, analogous to SRH. The analysis and the guarantees for smooth and non-smooth cost functions
are similar, except for less stringent conditions derived for smooth cost functions due to
properties of symmetric Hessian matrices. We also prove that the SRH holds for the case
of the ℓ2-penalized logistic loss function.
In the remainder of this paper we use the notation listed in Table 1.
Paper Outline
In §2 we provide a background on sparse parameter estimation which serves as an overview
of prior work. In §3 we state the general formulation of the problem and present our algorithm. Conditions that characterize the cost functions and the main accuracy guarantees
of our algorithm are provided in §3 as well. The guarantees of the algorithm are proved in
Appendices A and B. As an example where our algorithm can be applied, ℓ2-regularized
logistic regression is studied in §4. Some experimental results for logistic regression with
sparsity constraints are presented in §5. Finally, §6 discusses the results and concludes.
2. Background
We ﬁrst brieﬂy review sparse estimation problems studied in the literature.
2.1 Sparse Linear Regression and Compressed Sensing
The special case of sparse estimation in linear models has gained signiﬁcant attention under
the title of Compressed Sensing (CS) . In standard CS problems the aim is
to estimate a sparse vector x⋆from noisy linear measurements y = Ax⋆+ e, where A is
a known n × p measurement matrix with n ≪p and e is the additive measurement noise.
To ﬁnd the sparsest estimate in this underdetermined problem that is consistent with the
measurements y one needs to solve the optimization problem
bx = arg min
s.t. ∥y −Ax∥2 ≤ε,
where ε is a given upper bound for ∥e∥2 . In the absence of noise (i.e.,
when ε = 0), if x⋆is s-sparse (i.e., it has at most s nonzero entries) one merely needs every
Table 1: Notation used in this paper
Description
the set {1, 2, . . . , n} for any n ∈N
calligraphic letters denote sets unless stated otherwise (e.g., N
denotes a normal distribution)
complement of set I
bold face small letters denote column vectors in Rb for some b ∈N
the ℓq-norm of vector v, that is
i=1 |vi|q1/q
, for a real number q ≥1
the “ℓ0-norm” of vector v that merely counts its nonzero entries
depending on the context
1. restriction of vector v to the rows indicated by indices in I, or
2. a vector that equals v except for coordinates in Ic where it is zero
the best r-term approximation of vector v
the support set (i.e., indices of the non-zero entries) of v
bold face capital letters denote matrices in Ra×b for some a, b ∈N
transpose of matrix M
pseudo-inverse of matrix M
restriction of matrix M to the columns enumerated by I
the operator norm of matrix M which is equal to
λmax (MTM)
the identity matrix
restriction of the identity matrix to the columns indicated by I
column vector of all ones
expectation
Hessian of the function f
2s columns of A to be linearly independent to guarantee exact recovery . Unfortunately, the ideal solver (1) is computationally NP-hard in general and one must seek approximate solvers instead.
It is shown in that under certain conditions, minimizing the ℓ1norm as a convex proxy for the ℓ0-norm yields accurate estimates of x⋆. The resulting
approximate solver basically returns the solution to the convex optimization problem
bx = arg min
s.t. ∥y −Ax∥2 ≤ε,
The required conditions for approximate equivalence of (1) and (2), however, generally hold
only if measurements are collected at a higher rate. Ideally, one merely needs n = O (s)
measurements to estimate x⋆, but n = O(s log p/s) measurements are necessary for the
accuracy of (2) to be guaranteed.
The convex program (2) can be solved in polynomial time using interior point methods.
However, these methods do not scale well as the size of the problem grows. Therefore, several
ﬁrst-order convex optimization methods are developed and analyzed as more eﬃcient alternatives and Agarwal et al. ). Another category
of low-complexity algorithms in CS are the non-convex greedy pursuits including Orthogonal Matching Pursuit (OMP) , Compressive
Sampling Matching Pursuit (CoSaMP) , Iterative Hard Thresholding (IHT) , and Subspace Pursuit to name a few. These greedy algorithms implicitly approximate the solution to the
ℓ0-constrained least squares problem
bx = arg min
2 ∥y −Ax∥2
s.t. ∥x∥0 ≤s.
The main theme of these iterative algorithms is to use the residual error from the previous
iteration to successively approximate the position of non-zero entries and estimate their
values. These algorithms have shown to exhibit accuracy guarantees similar to those of
convex optimization methods, though with more stringent requirements.
As mentioned above, to guarantee accuracy of the CS algorithms the measurement matrix should meet certain conditions such as incoherence , Restricted
Isometry Property (RIP) , Nullspace Property ,
etc. Among these conditions RIP is the most commonly used and the best understood
condition.
Matrix A is said to satisfy the RIP of order k—in its symmetric form—with constant
δk, if δk < 1 is the smallest number that
(1 −δk) ∥x∥2
2 ≤(1 + δk) ∥x∥2
holds for all k-sparse vectors x.
Several CS algorithms are shown to produce accurate
solutions provided that the measurement matrix has a suﬃciently small RIP constant of
order ck with c being a small integer. For example, solving (2) is guaranteed to yield an
accurate estimate of s-sparse x⋆if δ2s <
2 −1 . Interested readers can ﬁnd
the best known RIP-based accuracy guarantees for some of the CS algorithms in . Thus, it is desirable to
develop theory and algorithms that apply to a broader class of optimization problems with
sparsity constraints.
The existing studies on this subject are mostly in the context of statistical estimation.
The majority of these studies consider the cost function to be convex everywhere and rely on
the ℓ1-regularization as the means to induce sparsity in the solution. For example, Kakade
et al. have shown that for the exponential family of distributions maximum likelihood
estimation with ℓ1-regularization yields accurate estimates of the underlying sparse parameter. Furthermore, Negahban et al. have developed a unifying framework for analyzing
statistical accuracy of M-estimators regularized by “decomposable” norms in . In particular, in their work ℓ1-regularization is applied to Generalized Linear
Models (GLM) and shown to guarantee a bounded distance
between the estimate and the true statistical parameter. To establish this error bound they
introduced the notion of Restricted Strong Convexity (RSC), which basically requires a lower
bound on the curvature of the cost function around the true parameter in a restricted set
of directions. The achieved error bound in this framework is inversely proportional to this
curvature bound. Furthermore, Agarwal et al. have studied Projected Gradient Descent as a method to solve ℓ1-constrained optimization problems and established accuracy
guarantees using a slightly diﬀerent notion of RSC and Restricted Smoothness (RSM).
Note that the guarantees provided for majority of the ℓ1-regularization algorithms presume that the true parameter is bounded, albeit implicitly. For instance, the error bound
for ℓ1-regularized logistic regression is recognized by Bunea to be dependent on the
true parameter . Moreover, the result proposed by Kakade et al. implicitly requires the true
parameter to have a suﬃciently short length to allow the choice of the desirable regularization coeﬃcient . Negahban et al. also
assume that the true parameter is inside the unit ball to establish the required condition
for their analysis of ℓ1-regularized GLM, although this restriction is not explicitly stated
 . We can better understand why
restricting the length of the true parameter may generally be inevitable by viewing these
estimation problems from the perspective of empirical processes and their convergence. The
empirical processes, including those considered in the studies mentioned above, are generally
good approximations of their corresponding expected process and ). Therefore, if the expected process is not strongly convex over
an unbounded, but perhaps otherwise restricted, set the corresponding empirical process
cannot be strongly convex over the same set. This reasoning applies in many cases including
the studies mentioned above, where it would be impossible to achieve the desired restricted
strong convexity properties—with high probability—if the true parameter is allowed to be
unbounded.
Furthermore, the methods that rely on the ℓ1-norm are known to result in sparse solutions, but, as mentioned in , the sparsity of these solutions is not known
to be optimal in general. One can intuit this fact from deﬁnitions of RSC and RSM. These
two properties bound the curvature of the function from below and above in a restricted
set of directions around the true optimum. For quadratic cost functions, such as squared
error, these curvature bounds are absolute constants. As stated before, for more general
cost functions such as the loss functions in GLMs, however, these constants will depend
on the location of the true optimum. Consequently, depending on the location of the true
optimum these error bounds could be extremely large, albeit ﬁnite. When error bounds
are signiﬁcantly large, the sparsity of the solution obtained by ℓ1-regularization may not be
satisfactory. This motivates investigation of algorithms that do not rely on ℓ1-norm as a
3. Problem Formulation and the GraSP Algorithm
As seen in §2.1, in standard CS the squared error f(x) = 1
2 ∥y −Ax∥2
2 is used to measure
data ﬁdelity. While this is appropriate for a large number of signal acquisition applications,
it is not the right cost in other ﬁelds. Thus, the signiﬁcant advances in CS cannot readily be
applied in these ﬁelds when estimation or prediction of sparse parameters become necessary.
In this paper we focus on a generalization of (3) where a generic cost function replaces the
squared error. Speciﬁcally, for the cost function f : Rp 7→R, it is desirable to approximate
s.t. ∥x∥0 ≤s.
We propose the Gradient Support Pursuit (GraSP) algorithm, which is inspired by and
generalizes the CoSaMP algorithm, to approximate the solution to (4) for a broader class
of cost functions.
Of course, even for a simple quadratic objective, (4) can have combinatorial complexity
and become NP-hard. However, similar to the results of CS, knowing that the cost function
obeys certain properties allows us to obtain accurate estimates through tractable algorithms.
To guarantee that GraSP yields accurate solutions and is a tractable algorithm, we also
require the cost function to have certain properties that will be described in §3.2. These
properties are analogous to and generalize the RIP in the standard CS framework. For
smooth cost functions we introduce the notion of a Stable Restricted Hessian (SRH) and
for non-smooth cost functions we introduce the Stable Restricted Linearization (SRL). Both
of these properties basically bound the Bregman divergence of the cost function restricted
to sparse canonical subspaces. However, the analysis based on the SRH is facilitated by
matrix algebra that results in somewhat less restrictive requirements for the cost function.
3.1 Algorithm Description
GraSP is an iterative algorithm, summarized in Algorithm 1, that maintains and updates
an estimate bx of the sparse optimum at every iteration. The ﬁrst step in each iteration, z =
∇f (bx), evaluates the gradient of the cost function at the current estimate. For nonsmooth
Algorithm 1: The GraSP algorithm
input : f (·) and s
output: ˆx
initialize: bx = 0
compute local gradient: z = ∇f (bx)
identify directions: Z = supp (z2s)
merge supports: T = Z ∪supp (bx)
minimize over support: b = arg min f (x) s.t. x|T c = 0
prune estimate: bx = bs
until halting condition holds
functions, instead of the gradient we use the restricted subgradient z = ∇f (bx) deﬁned in
§3.2. Then 2s coordinates of the vector z that have the largest magnitude are chosen as
the directions in which pursuing the minimization will be most eﬀective. Their indices,
denoted by Z = supp (z2s), are then merged with the support of the current estimate to
obtain T = Z ∪supp (bx). The combined support is a set of at most 3s indices over which the
function f is minimized to produce an intermediate estimate b=arg min f (x) s.t. x|T c =0.
The estimate bx is then updated as the best s-term approximation of the intermediate
estimate b.
The iterations terminate once certain condition, e.g., on the change of the
cost function or the change of the estimated minimum from the previous iteration, holds.
In the special case where the squared error f (x) =
2 ∥y −Ax∥2
2 is the cost function, GraSP reduces to CoSaMP. Speciﬁcally, the gradient step reduces to the proxy step
z = AT (y −Aˆx) and minimization over the restricted support reduces to the constrained
pseudoinverse step b|T = A†
T y, b|T c = 0 in CoSaMP.
Although in this paper we only analyze the standard form of GraSP outlined
in Algorithm 1, other variants of the algorithm can also be studied.
Below we list some of
these variants.
1. Debiasing: In this variant, instead of performing a hard thresholding on the vector
b, the objective is minimized restricted to the support set of bs to obtain the new
bx = arg min
s.t. supp (x) ⊆supp (bs) .
2. Restricted Newton Step: To reduce the computations in each iteration, the minimization that yields b, we can set b|T c = 0 and take a restricted Newton step as
b|T = bx|T −κ
T Hf (bx) PT
−1 bx|T ,
where κ > 0 is a step-size. Of course, here we are assuming that the restricted Hessian,
T Hf (bx) PT , is invertible.
3. Restricted Gradient Descent: The minimization step can be relaxed even further by
applying a restricted gradient descent. In this approach, we again set b|T c = 0 and
b|T = bx|T −κ ∇f (bx)|T .
Since T contains both the support set of bx and the 2s-largest entries of ∇f (bx) , it is
easy to show that each iteration of this alternative method is equivalent to a standard
gradient descent followed by a hard thresholding. In particular, if the squared error
is the cost function as in standard CS, this variant reduces to the IHT algorithm.
3.2 Sparse Reconstruction Conditions
In what follows we characterize the functions for which accuracy of GraSP can be guaranteed. For twice continuously diﬀerentiable functions we rely on Stable Restricted Hessian
(SRH), while for non-smooth cost functions we introduce the Stable Restricted Linearization (SRL). These properties that are analogous to the RIP in the standard CS framework,
basically require that the curvature of the cost function over the sparse subspaces can be
bounded locally from above and below such that the corresponding bounds have the same
order. Below we provide precise deﬁnitions of these two properties.
Deﬁnition 1 (Stable Restricted Hessian). Suppose that f is a twice continuously diﬀerentiable function whose Hessian is denoted by Hf (·). Furthermore, let
Ak (x) = sup
∆THf (x) ∆
|supp (x) ∪supp (∆)| ≤k, ∥∆∥2 = 1
Bk (x) = inf
∆THf (x) ∆
: |supp (x) ∪supp (∆)| ≤k, ∥∆∥2 = 1
for all k-sparse vectors x. Then f is said to have a Stable Restricted Hessian (SRH) with
constant µk, or in short µk-SRH, if 1 ≤Ak(x)
Bk(x) ≤µk.
Remark 1. Since the Hessian of f is symmetric, an equivalent for Deﬁnition 1 is that a twice
continuously diﬀerentiable function f has µk-SRH if the condition number of PKHf (x) PT
is not greater than µk for all k-sparse vectors x and sets K ⊆[p] with |supp (x) ∪K| ≤k.
In the special case when the cost function is the squared error as in (3), we can write
Hf (x) = ATA which is constant. The SRH condition then requires
2 ≤Ak ∥∆∥2
to hold for all k-sparse vectors ∆with Ak/Bk ≤µk. Therefore, in this special case the SRH
condition essentially becomes equivalent to the RIP condition.
Remark 2. Note that the functions that satisfy the SRH are convex over canonical sparse
subspaces, but they are not necessarily convex everywhere. The following two examples
describe some non-convex functions that have SRH.
Example 1. Let f (x) = 1
2xTQx, where Q = 2 × 11T −I. Obviously, we have Hf (x) = Q.
Therefore, (5) and (6) determine the extreme eigenvalues across all of the k × k symmetric
submatrices of Q.
Note that the diagonal entries of Q are all equal to one, while its
oﬀ-diagonal entries are all equal to two.
Therefore, for any 1-sparse signal u we have
uTQu = ∥u∥2
2, meaning that f has µ1-SRH with µ1 = 1. However, for u = [1, −1, 0, . . . , 0]T
we have uTQu < 0, which means that the Hessian of f is not positive semi-deﬁnite (i.e., f
is not convex).
Example 2. Let f (x) = 1
2 + Cx1x2 · · · xk+1 where the dimensionality of x is greater
than k. It is obvious that this function is convex for k-sparse vectors as x1x2 · · · xk+1 = 0
for any k-sparse vector. So we can easily verify that f satisﬁes SRH of order k. However,
for x1 = x2 = · · · = xk+1 = t and xi = 0 for i > k + 1 the restriction of the Hessian of f to
indices in [k + 1] (i.e., PT
[k+1]Hf (x) P[k+1]) is a matrix with diagonal entries all equal to one
and oﬀ-diagonal entries all equal to Ctk−1. Let Q denote this matrix and u be a unit-norm
vector such that ⟨u, 1⟩= 0. Then it is straightforward to verify that uTQu = 1 −Ctk−1,
which can be negative for suﬃciently large values of C and t. Therefore, the Hessian of f
is not positive semi-deﬁnite everywhere, meaning that f is not convex.
To generalize the notion of SRH to the case of nonsmooth functions, ﬁrst we deﬁne the
restricted subgradient of a function.
Deﬁnition 2 (Restricted Subgradient). We say vector ∇f (x) is a restricted subgradient
of f : Rp 7→R at point x if
f (x + ∆) −f (x) ≥⟨∇f (x) , ∆⟩
holds for all k-sparse vectors ∆.
Remark 3. We introduced the notion of restricted subgradient so that the restrictions imposed on f are as minimal as we need. We acknowledge that the existence of restricted
subgradients implies convexity in sparse directions, but it does not imply convexity everywhere.
Remark 4. Obviously, if the function f is convex everywhere, then any subgradient of f
determines a restricted subgradient of f as well. In general one may need to invoke the
axiom of choice to deﬁne the restricted subgradient.
Remark 5. We drop the sparsity level from the notation as it can be understood from the
With a slight abuse of terminology we call
∇f (x) , x′ −x
the restricted Bregman divergence of f : Rp 7→R between points x and x′ where ∇f (·)
gives a restricted subgradient of f (·).
Deﬁnition 3 (Stable Restricted Linearization). Let x be a k-sparse vector in Rp.
function f : Rp 7→R we deﬁne the functions
αk (x) = sup
Bf (x + ∆∥x) | ∆̸= 0 and |supp (x) ∪supp (∆)| ≤k
βk (x) = inf
Bf (x + ∆∥x) | ∆̸= 0 and |supp (x) ∪supp (∆)| ≤k
Then f (·) is said to have a Stable Restricted Linearization with constant µk, or µk-SRL, if
βk(x) ≤µk for all k-sparse vectors x.
Remark 6. The SRH and SRL conditions are similar to various forms of the Restricted
Strong Convexity (RSC) and Restricted Strong Smoothness (RSS) conditions in
the sense that they all bound the curvature of the objective function over a restricted set.
The SRL condition quantiﬁes the curvature in terms of a (restricted) Bregman divergence
similar to RSC and RSS. The quadratic form used in SRH can also be converted to the Bregman divergence form used in RSC and RSS and vice-versa using the mean-value theorem.
However, compared to various forms of RSC and RSS conditions SRH and SRL have some
important distinctions. The main diﬀerence is that the bounds in SRH and SRL conditions
are not global constants; only their ratio is required to be bounded globally. Furthermore,
unlike the SRH and SRL conditions the variants of RSC and RSS, that are used in convex
relaxation methods, are required to hold over a set which is strictly larger than the set of
canonical k-sparse vectors.
There is also a subtle but important diﬀerence regarding the points where the curvature
is evaluated at. Since Negahban et al. analyze a convex program, rather than an
iterative algorithm, they only needed to invoke the RSC and RSS at a neighborhood of the
true parameter. In contrast, the other variants of RSC and RSS ), as well as our SRH and SRL conditions, require the curvature
bounds to hold uniformly over a larger set of points, thereby they are more stringent.
3.3 Main Theorems
Now we can state our main results regarding approximation of
x⋆= arg min f(x) s.t. ∥x∥0 ≤s,
using the GraSP algorithm.
Theorem 1. Suppose that f is a twice continuously diﬀerentiable function that has µ4s-
SRH with µ4s ≤1+
. Furthermore, suppose that for some ϵ > 0 we have ϵ ≤B4s (u) for
all u. Then bx(i), the estimate at the i-th iteration, satisﬁes
2 ≤2−i ∥x⋆∥2 + 6 + 2
∥∇f (x⋆)|I∥2 ,
where I is the position of the 3s largest entries of ∇f (x⋆) in magnitude.
Remark 7. Note that this result indicates that ∇f (x⋆) determines how accurate the estimate can be. In particular, if the sparse minimum x⋆is suﬃciently close to an unconstrained minimum of f then the estimation error ﬂoor is negligible because ∇f (x⋆) has
small magnitude. This result is analogous to accuracy guarantees for estimation from noisy
measurements in CS .
Remark 8. As the derivations required to prove Theorem 1 show, the provided accuracy
guarantee holds for any s-sparse x⋆, even if it does not obey (7). Obviously, for arbitrary
choices of x⋆, ∇f (x⋆)|I may have a large norm that cannot be bounded properly which implies large errors. In statistical estimation problems, often the true parameter that describes
the data is chosen as the target parameter x⋆rather than the minimizer of the average loss
function as in (7). In these problems, the approximation error ∥∇f (x⋆)|I∥2 has statistical
interpretation and can determine the statistical precision of the problem. This property is
easy to verify in linear regression problems. We will also show this for the logistic loss as
an example in §4.
Nonsmooth cost functions should be treated diﬀerently, since we do not have the luxury
of working with Hessian matrices for these type of functions. The following theorem provides
guarantees that are similar to those of Theorem 1 for nonsmooth cost functions that satisfy
the SRL condition.
Theorem 2. Suppose that f is a function that is not necessarily smooth, but it satisﬁes
µ4s-SRL with µ4s ≤3+
. Furthermore, suppose that for β4s (·) in Deﬁnition 3 there exists
some ϵ > 0 such that β4s (x) ≥ϵ holds for all 4s-sparse vectors x. Then bx(i), the estimate
at the i-th iteration, satisﬁes
2 ≤2−i ∥x⋆∥2 + 6 + 2
where I is the position of the 3s largest entries of ∇f (x⋆) in magnitude.
Remark 9. Should the SRH or SRL conditions hold for the objective function, it is straightforward to convert the point accuracy guarantees of Theorems 1 and 2, into accuracy guarantees in terms of the objective value. First we can use SRH or SRL to bound the Bregman
divergence, or its restricted version deﬁned above, for points bx(i) and x⋆. Then we can obtain a bound for the accuracy of the objective value by invoking the results of the theorems.
This indirect approach, however, might not lead to sharp bounds and thus we do not pursue
the detailed analysis in this work.
4. Example: Sparse Minimization of ℓ2-regularized Logistic Regression
One of the models widely used in machine learning and statistics is the logistic model. In
this model the relation between the data, represented by a random vector a ∈Rp, and its
associated label, represented by a random binary variable y ∈{0, 1}, is determined by the
conditional probability
Pr {y | a; x} =
exp (y ⟨a, x⟩)
1 + exp (⟨a, x⟩),
where x denotes a parameter vector. Then, for a set of n independently drawn data samples
{(ai, yi)}n
i=1 the joint likelihood can be written as a function of x. To ﬁnd the maximum
likelihood estimate one should maximize this likelihood function, or equivalently minimize
the negative log-likelihood, the logistic loss,
log (1 + exp (⟨ai, x⟩)) −yi ⟨ai, x⟩.
It is well-known that g (·) is strictly convex for p ≤n provided that the associated design
matrix, A = [a1 a2 . . . an]T, is full-rank. However, in many important applications (e.g.,
feature selection) the problem can be underdetermined (i.e., n < p). In these scenarios the
logistic loss is merely convex and it does not have a unique minimum. Furthermore, it is
possible, especially in underdetermined problems, that the observed data is linearly separable. In that case one can achieve arbitrarily small loss values by tending the parameters
to inﬁnity along certain directions. To compensate for these drawbacks the logistic loss is
usually regularized by some penalty term .
One of the candidates for the penalty function is the (squared) ℓ2-norm of x (i.e., ∥x∥2
Considering a positive penalty coeﬃcient η the regularized loss is
f (x) = g(x) + η
For any convex g (·) this regularized loss is guaranteed to be η-strongly convex, thus it
has a unique minimum. Furthermore, the penalty term implicitly bounds the length of
the minimizer thereby resolving the aforementioned problems. Nevertheless, the ℓ2 penalty
does not promote sparse solutions. Therefore, it is often desirable to impose an explicit
sparsity constraint, in addition to the ℓ2 regularizer.
4.1 Verifying SRH for ℓ2-regularized logistic loss
It is easy to show that the Hessian of the logistic loss at any point x is given by Hg(x) =
4nATΛA, where Λ is an n × n diagonal matrix whose diagonal entries Λii = sech2 1
with sech (·) denoting the hyperbolic secant function. Note that 0 ≼Hg (x) ≼
Therefore, if Hη (x) denotes the Hessian of the ℓ2-regularized logistic loss, we have
2 ≤∆THη (x) ∆≤1
2 + η ∥∆∥2
To verify SRH, the upper and lower bounds achieved at k-sparse vectors ∆are of particular
interest. It only remains to ﬁnd an appropriate upper bound for ∥A∆∥2
2 in terms of ∥∆∥2
To this end we use the following result on Chernoﬀbounds for random matrices due to
Tropp .
Theorem 3 ). Consider a ﬁnite sequence {Mi} of k × k,
independent, random, self-adjoint matrices that satisfy
λmax (Mi) ≤R
almost surely.
Let θmax := λmax (P
i E [Mi]). Then for τ ≥0,
≥(1 + τ) θmax
(τ −(1 + τ) log (1 + τ)
As stated before, in a standard logistic model data samples {ai} are supposed to be
independent instances of a random vector a. In order to apply Theorem 3 we need to make
the following extra assumptions:
Assumption. For every J ⊆[p] with |J | = k,
(i) we have
2 ≤R almost surely, and
(ii) none of the matrices PT
PJ is the zero matrix.
We deﬁne θJ
max := λmax
, where C = E
J ⊆[p] , |J |=kθJ
J ⊆[p] , |J |=kθJ
To simplify the notation henceforth we let h (τ) = (1 + τ) log (1 + τ) −τ.
Corollary 1. With the above assumptions, if n ≥R
 log k + k
 1 + log p
for some τ > 0 and ε ∈(0, 1), then with probability at least 1 −ε the ℓ2-regularized logistic
loss has µk-SRH with µk ≤1 + 1+τ
Proof For any set of k indices J let MJ
i = ai|J ai|T
i PJ . The independence
of the vectors ai implies that the matrix
is a sum of n independent, random, self-adjoint matrices.
Assumption (i) implies that
2 ≤R almost surely. Furthermore, we have
Hence, for any ﬁxed index set J with |J | = k we may apply Theorem 3 for Mi = MJ
θmax = nθJ
max, and τ > 0 to obtain
≥(1 + τ) nθJ
Furthermore, we can write
≥(1 + τ) nθ
≥(1 + τ) nθ
≥(1 + τ) nθJ
Note that Assumption (ii) guarantees that eθ > 0, and thus the above probability bound will
not be vacuous for suﬃciently large n. To ensure a uniform guarantee for all
choices of J we can use the union bound to obtain
log k+k+k log p
k −neθh (τ)
Therefore, for ε ∈(0, 1) and n ≥R
 log k + k
 1 + log p
it follows from
(10) that for any x and any k-sparse ∆,
2 ≤∆THη (x) ∆≤
holds with probability at least 1 −ε. Thus, the ℓ2-regularized logistic loss has an SRH
constant µk ≤1 + 1+τ
4η θ with probability 1 −ε.
Remark 10. One implication of this result is that for a regime in which k and p grow
suﬃciently large while
k remains constant one can achieve small failure rates provided
that n = Ω
. Note that R is deliberately included in the argument of the order
function because in general R depends on k. In other words, the above analysis may require
as the suﬃcient number of observations. This bound is a consequence of
using Theorem 3, but to the best of our knowledge, other results regarding the extreme
eigenvalues of the average of independent random PSD matrices also yield an n of the
same order.
If matrix A has certain additional properties (e.g., independent and sub-
Gaussian entries), however, a better rate of n = Ω
can be achieved without using
the techniques mentioned above.
Remark 11. The analysis provided here is not speciﬁc to the ℓ2-regularized logistic loss and
can be readily extended to any other ℓ2-regularized GLM loss whose log-partition function
has a Lipschitz-continuous derivative.
4.2 Bounding the approximation error
We are going to bound ∥∇f (x⋆)|I∥2 which controls the approximation error in the statement of Theorem 1. In the case of case of ℓ2-regularized logistic loss considered in this
section we have
1 + exp (−⟨ai, x⟩) −yi
1+exp(−⟨ai,x⋆⟩) −yi by vi for i = 1, 2, . . . , n then we can deduce
∥∇f (x⋆)|I∥2 =
vi ai|I + η x⋆|I
I v + η x⋆|I
∥v∥2 + η ∥x⋆|I∥2
i + η ∥x⋆|I∥2 ,
where v = [v1 v2 . . . vn]T. Note that vi’s are n independent copies of the random variable
1+exp(−⟨a,x⋆⟩) −y that is zero-mean and always lie in the interval [−1, 1]. Therefore,
applying the Hoeﬀding’s inequality yields
i ≥(1 + c) σ2
is the variance of v. Furthermore, using the logistic model (8) we can
(y −E [y | a])2 | a
= E [var (y | a)]
1 + exp (⟨a, x⋆⟩) ×
exp (⟨a, x⋆⟩)
1 + exp (⟨a, x⋆⟩)
(because y | a ∼Bernoulli as in (8))
2 + exp (⟨a, x⋆⟩) + exp (−⟨a, x⋆⟩)
(because exp (t) + exp (−t) ≥2).
Therefore, we have 1
4 with high probability. As in the previous subsection one
can also bound
using (11) with k = |I| = 3s. Hence, with
high probability we have
∥∇f (x⋆)|I∥2 ≤1
(1 + τ) θ + η ∥x⋆∥2 .
Interestingly, this analysis can also be extended to the GLMs whose log-partition function
ψ (·) obeys 0 ≤ψ′′ (t) ≤C for all t with C being a positive constant. For these models the
approximation error can be bounded in terms of the variance of vψ = ψ′ (⟨a, x⋆⟩) −y.
5. Experimental Results
Synthetic Data
Algorithms that are used for sparsity-constrained estimation or optimization often induce
sparsity using diﬀerent types of regularizations or constraints. Therefore, the optimized objective function may vary from one algorithm to another, even though all of these algorithms
try to estimate the same sparse parameter and sparsely optimize the same original objective. Because of the discrepancy in the optimized objective functions it is generally diﬃcult
to compare performance of these algorithms. Applying algorithms on real data generally
produces even less reliable results because of the unmanageable or unknown characteristics
of the real data. Hence, we restrict our simulations to application of the logistic model on
synthetically generated data, and evaluate performance of GraSP based on the value of the
original objective and the estimation accuracy for a sparse parameter.
In our simulations the sparse parameter of interest x⋆is a p = 1000 dimensional vector
that has s = 10 nonzero entries drawn independently from the standard Gaussian distribution. An intercept c ∈R is also considered which is drawn independently of the other
parameters according to the standard Gaussian distribution. Each data sample is an independent instance of the random vector a = [a1, a2, . . . , ap]T generated by an autoregressive
process determined by
aj+1 = ρaj +
for all j ∈[p −1]
with a1 ∼N (0, 1), zj ∼N (0, 1), and ρ ∈ being the correlation parameter. The data
model we describe and use above is identical to the experimental model used in , except that we adjusted the coeﬃcients to ensure that E
= 1 for all j ∈[p].
The data labels, y ∈{0, 1} are then drawn randomly according to the Bernoulli distribution
Pr {y = 0 | a} = 1/ (1 + exp (⟨a, x⋆⟩+ c)) .
We compared GraSP to the LASSO algorithm implemented in the GLMnet package , as well as the Orthogonal Matching Pursuit method dubbed
Logit-OMP . To isolate the eﬀect of ℓ2-regularization, both LASSO
and the basic implementation of GraSP did not consider additional ℓ2-regularization terms.
To analyze the eﬀect of an additional ℓ2-regularization we also evaluated the performance
of GraSP with ℓ2-regularized logistic loss, as well as the logistic regression with elastic
net (i.e., mixed ℓ1-ℓ2) penalty also available in the GLMnet package. We conﬁgured the
GLMnet software to produce s-sparse solutions for a fair comparison. For the elastic net
penalty (1 −ω) ∥x∥2
2 /2+ω ∥x∥1 we considered the “mixing parameter” ω to be 0.8. For the
ℓ2-regularized logistic loss we considered η = (1 −ω)
n . For each choice of the number
of measurements n between 50 and 1000 in steps of size 50, and ρ in the set
we generate the data and the associated labels and apply the algorithms.
The average
performance is measured over 200 trials for each pair of (n, ρ).
Fig. 1 compares the average value of the empirical logistic loss achieved by each of
the considered algorithms for a wide range of “sampling ratio” n/p. For GraSP, the curves
labelled by GraSP and GraSP + ℓ2 corresponding to the cases where the algorithm is applied
to unregularized and ℓ2-regularized logistic loss, respectively. Furthermore, the results of
GLMnet for the LASSO and the elastic net regularization are labelled by GLMnet (ℓ1) and
GLMnet (elastic net), respectively. The simulation result of the Logit-OMP algorithm is
also included. To contrast the obtained results we also provided the average of empirical
logistic loss evaluated at the true parameter and one standard deviation above and below
this average on the plots.
Furthermore, we evaluated performance of GraSP with the
debiasing procedure described in §3.1.
As can be seen from the ﬁgure at lower values of the sampling ratio GraSP is not
accurate and does not seem to be converging. This behavior can be explained by the fact
that without regularization at low sampling ratios the training data is linearly separable or
has very few mislabelled samples. In either case, the value of the loss can vary signiﬁcantly
even in small neighborhoods. Therefore, the algorithm can become too sensitive to the
pruning step at the end of each iteration.
At larger sampling ratios, however, the loss
from GraSP begins to decrease rapidly, becoming eﬀectively identical to the loss at the
true parameter for n/p > 0.7. The results show that unlike GraSP, Logit-OMP performs
gracefully at lower sampling ratios. At higher sampling ratios, however, GraSP appears to
yield smaller bias in the loss value. Furthermore, the diﬀerence between the loss obtained
by the LASSO and the loss at the true parameter never drops below a certain threshold,
although the convex method exhibits a more stable behaviour at low sampling ratios.
Interestingly, GraSP becomes more stable at low sampling ratios when the logistic loss is
regularized with the ℓ2-norm. However, this stability comes at the cost of a bias in the loss
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
True Value
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
True Value
(b) ρ = 1/3
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
True Value
(c) ρ = 1/2
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
True Value
Figure 1: Comparison of the average (empirical) logistic loss at solutions obtained via
GraSP, GraSP with ℓ2-penalty, LASSO, the elastic-net regularization, and Logit-OMP. The
results of both GraSP methods with “debiasing” are also included. The average loss at the
true parameter and one standard deviation interval around it are plotted as well.
value at high sampling ratios that is particularly pronounced in Fig. 1d. Nevertheless, for all
of the tested values of ρ, at low sampling ratios GraSP+ℓ2 and at high sampling ratios GraSP
are consistently closer to the true loss value compared to the other methods. Debiasing the
iterates of GraSP also appears to have a stabilizing eﬀect at lower sampling ratios. For
GraSP with ℓ2 regularized cost, the debiasing particularly reduced the undesirable bias at
Fig. 2 illustrates the performance of the same algorithms in terms of the relative error
∥bx −x⋆∥2 / ∥x⋆∥2 where bx denotes the estimate that the algorithms produce.
Not surprisingly, none of the algorithms attain an arbitrarily small relative error. Furthermore,
the parameter ρ does not appear to aﬀect the performance of the algorithms signiﬁcantly.
Without the ℓ2-regularization, at high sampling ratios GraSP provides an estimate that has
a comparable error versus the ℓ1-regularization method. However, for mid to high sampling ratios both GraSP and GLMnet methods are outperformed by Logit-OMP. At low to
mid sampling ratios, GraSP is unstable and does not converge to an estimate close to the
true parameter. Logit-OMP shows similar behavior at lower sampling ratios. Performance
of GraSP changes changes dramatically once we consider the ℓ2-regularization and/or the
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
(b) ρ = 1/3
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
(c) ρ = 1/2
GraSP + ℓ2
GraSP (debias)
GraSP + ℓ2 (debias)
GLMnet (ℓ1)
GLMnet (elastic net)
Figure 2: Comparison of the average relative error (i.e., ∥bx −x⋆∥2 / ∥x⋆∥2) in logarithmic
scale at solutions obtained via GraSP, GraSP with ℓ2-penalty, LASSO, the elastic-net regularization, and Logit-OMP. The results of both GraSP methods with “debiasing” are also
debiasing procedure.
With ℓ2-regularization, GraSP achieves better relative error compared to GLMnet and ordinary GraSP for almost the entire range of tested sampling ratios.
Applying the debiasing procedure has improved the performance of both GraSP methods
except at very low sampling ratios. These variants of GraSP appear to perform better than
Logit-OMP for almost the entire range of n/p.
We also conducted the same simulation on some of the data sets used in NIPS 2003 Workshop on feature extraction , namely the ARCENE and DEXTER data
sets. The logistic loss values at obtained estimates are reported in Tables 2 and 3. For each
data set we applied the sparse logistic regression for a range of sparsity level s. The columns
indicated by “G” correspond to diﬀerent variants of GraSP. Suﬃxes ℓ2 and “d” indicate the
ℓ2-regularization and the debiasing are applied, respectively. The columns indicated by ℓ1
and E-net correspond to the results of the ℓ1-regularization and the elastic-net regulariza-
Table 2: ARCENE
Table 3: DEXTER
tion methods that are performed using the GLMnet package. The last column contains the
result of the Logit-OMP algorithm.
The results for DEXTER data set show that GraSP variants without debiasing and the
convex methods achieve comparable loss values in most cases, whereas the convex methods
show signiﬁcantly better performance on the ARCENE data set. Nevertheless, except for a
few instances where Logit-OMP has the best performance, the smallest loss values in both
data sets are attained by GraSP methods with debiasing step.
6. Discussion and Conclusion
In many applications understanding high dimensional data or systems that involve these
types of data can be reduced to identiﬁcation of a sparse parameter. For example, in gene
selection problems researchers are interested in locating a few genes among thousands of
genes that cause or contribute to a particular disease.
These problems can usually be
cast as sparsity constrained optimizations. In this paper we introduce a greedy algorithm
called the Gradient Support Pursuit(GraSP) as an approximate solver for a wide range of
sparsity-constrained optimization problems.
We provide theoretical convergence guarantees based on the notions of a Stable Restricted Hessian (SRH) for smooth cost functions and a Stable Restricted Linearization
(SRL) for non-smooth cost functions, both of which are introduced in this paper.
algorithm generalizes the well-established sparse recovery algorithm CoSaMP that merely
applies in linear models with squared error loss. The SRH and SRL also generalize the
well-known Restricted Isometry Property for sparse recovery to the case of cost functions
other than the squared error. To provide a concrete example we studied the requirements
of GraSP for ℓ2-regularized logistic loss. Using a similar approach one can verify SRH condition for loss functions that have Lipschitz-continuous gradient that incorporates a broad
family of loss functions.
At medium- and large-scale problems computational cost of the GraSP algorithm is
mostly aﬀected by the inner convex optimization step whose complexity is polynomial in s.
On the other hand, for very large-scale problems, especially with respect to the dimension
of the input, p, the running time of the GraSP algorithm will be dominated by evaluation
of the function and its gradient, whose computational cost grows with p. This problem is
common in algorithms that only have deterministic steps; even ordinary coordinate-descent
methods have this limitation . Similar to improvements gained by using
randomization in coordinate-descent methods , introducing randomization
in the GraSP algorithm could reduce its computational complexity at large-scale problems.
This extension, however, is beyond the scope of this paper and we leave it for future work.