Heat Conduction Process on Community Networks as a Recommendation Model
Yi-Cheng Zhang∗and Marcel Blattner
Physics Department, University of Fribourg, 1700 Fribourg, Switzerland
and Physics Department, Renmin University, Beijing, China
Yi-Kuo Yu†
National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD 20894, USA
 
Using heat conduction mechanism on a social network we develop a systematic method to predict
missing values as recommendations. This method can treat very large matrices that are typical
of internet communities. In particular, with an innovative, exact formulation that accommodates
arbitrary boundary condition, our method is easy to use in real applications. The performance is
assessed by comparing with traditional recommendation methods using real data.
PACS numbers: 44.10.+i, 89.70.+c, 89.20.Hh
With the advent of the internet, there sprout many
web sites that enable large communities to aggregate
and interact. For example livejournal.com allows its 3
million members to share interests and life experiences;
del.icio.us is a social bookmark service for people to share
their ﬁndings on the World Wide Web.
Thousands of
such web sites are built by web entrepreneurs and activists for the public, and their number is growing ever
faster. This brings about massive amount of accessible
information, more than each individual is able or willing
to process. Information search, ﬁltering, and recommendation thus become indispensable in internet era. Ideally
speaking, a good recommendation mechanism should be
able to “guess” what a person may want to select based
on what he or she already selected .
mechanisms are in actual use (like www.amazon.com
proposing its readers with new books), however, jury is
still out as to what is the best model. For a review of
current techniques, see .
Based on the heat conduction (or diﬀusion) process,
we propose a recommendation model capable of handling
individualized boundary conditions (BC). To better explain our model, we ﬁrst illustrate using the friendship
network of N people: each person (member) is a node,
and a pair of nodes is connected by an edge provided
they are mutual friends. The collection of these information forms the symmetric adjacency matrix A: element
Aij = 1, or 0 depending on whether people i and j are
mutual friends (1) or not (0). Although it is possible to
consider asymmetric connection, this generalization will
not be studied here. To recommend friends to any individual member, we ﬁrst set (Dirichlet) BC: to set the
values on the directly connected nodes as 1 and some remote nodes (will be further speciﬁed) as 0. Values on
all other nodes are treated as variables to be determined.
†Corresponding author, email: 
∗email: 
These values can be interpreted as the probabilities that
these nodes might be selected as friends.
We now describe an eﬃcient and eﬀective strategy to
solve the proposed heat conduction problem. From A, we
ﬁrst construct a propagator matrix P = D−1A, where D
is the diagonal degree matrix. Denote H as the temperature vector of N components: the source-components
are high temperature nodes with temperature 1; the sinkcomponents are low temperature nodes with temperature
0. Our task is to ﬁnd, through thermal equilibrium, the
temperatures associated with the remaining nodes that
are neither sinks nor sources. The discrete Laplace operator, analog of −∇2, on this network is L = I −P, where
I is the identity matrix. We only need to solve
where f is the external ﬂux vector. Note that this is the
discrete analog of −κ∇2T (⃗r) = ∇· ⃗J(⃗r) with H(i) plays
the role of κT (⃗r) and f(i) plays the role of ∇· ⃗J(⃗r).
Because Laplace operator conserves total heat and
tend to spread heat from high temperature region to low
temperature region, the only way to maintain the ﬁxed
temperature values at the sources and sinks is to apply
external heat ﬂux (inﬂow at sources and outﬂow at sinks).
For the rest of the nodes, the equilibrium condition demands that no net heat ﬂux should occur. Therefore,
the only allowed nonzero components of f are sourceand sink-components.
The computation of the temperature vector is straightforward. It is convenient to group the source and sink
components together into a block H1, and the rest free
variables another block H2. That is
Likewise, we group the Laplace operator in a similar
fashion and eq. (1) may be expressed as
All we need to solve is the homogeneous equation
L21H1 + L22H2 = 0 ,
without the need to know f. Fixing the values of H1, H2
can be readily found using standard iterative methods .
The above approach, although straightforward, represents a daunting challenge: for each individual, we must
solve the huge matrix problem once – a prohibitively expensive task for a typical internet community having millions of members.
The standard way to get around this dilemma is to
resort to the Green’s function method.
Starting from
eq.(1) we would like to have a Green’s function Ω′ such
that eq.(1) can be inverted:
to get H2 = Ω′
However, Ω′ = L−1 =
(I −P)−1 is divergent: the Laplace operator has a zero
eigenvalue and the inverse L−1 is meaningful only if
(H1, H2)T is in the subspace that is orthogonal to the
eigenvector of zero eigenvalue. A fortunate scenario like
this has occurred in the studies of random resistor networks .
To simultaneously deal with all possible BC, we lose
the freedom to limit the solution to a certain subspace.
Nevertheless, we have a good understanding regarding
this divergence. Basically, the P matrix has an eigenvalue
one with the right eigenvector being a column of 1s
|u0⟩= (1, 1, · · · , 1)T
and with left eigenvector being
d , . . . , dN
where di denotes the degree of node i and d = P
being the sum of degrees. Note that with this notation,
we have ⟨v0|u0⟩= 1.
We may then decompose P into
P = Q + |u0⟩⟨v0|
with Q|u0⟩⟨v0| = |u0⟩⟨v0|Q = 0. Further, the spectral
radius of Q is now guaranteed to be smaller than 1 and
thus (I −Q) is invertible with (I −Q)−1 = P∞
We may then rewrite the eq.(3) as
+ |u0⟩⟨v0|
+ c(H)|u0⟩
where the H-dependent constant may be written as
c(H) = ⟨v0
1|H1⟩+ ⟨v0
2|H2⟩. We need to explain the notation further. Basically |u0
1⟩, represents a column vector
whose components are obtained from the column vector
|u0⟩with component labels corresponding to that of the
sources and the sinks. On the other hand, |u0
2⟩represents a column vector that is the remainder of |u0⟩after
removing the components whose labels correspond to the
sources and sinks. Similarly, we deﬁne ⟨v0
1| to be a row
vector whose components are obtained from the row vector ⟨v0| with component labels corresponding to that of
the sources and the sinks; while ⟨v0
2| represents a row
vector that is the remainder of ⟨v0| after removing the
components whose labels correspond to the sources and
sinks. To simplify the notation, we will represent c(H)
by c without explicitly showing its H dependence.
Note that since Q|u0⟩= 0, upon multiplying Ω≡(I −
Q)−1 to both side of eq.(6) we have
or equivalently
Consequently, we may write H2 in the following form
|H2⟩= c |u0
2⟩+ Ω21Ω−1
11 |H1⟩−c Ω21Ω−1
Using the deﬁnition that c = ⟨v0
2|H2⟩, we obtain
or equivalently
1|H1⟩+ ⟨v0
Substituting this result back to eq. (9), we obtain H2 with
computational complexity solely depending on Ω21Ω−1
Note that we only needs to invert the matrix (I −Q)
once and for all. Upon specifying the boundary nodes,
one needs to reshuﬄe the rows and columns of the matrix
as well as vectors – a relatively eﬃcient operation. This
operation groups the source nodes and sink nodes in one
block to make easy the computation of Ω−1
Let us emphasize that our ﬁnal expression is written in a rather general setting that it can be applied
to cases when P is either row-normalized or columnnormalized.
In the case of column-normalized P, we
will have |u0
col. norm.⟩= (⟨v0
row norm.|)T and ⟨v0
col. norm.| =
row norm.⟩)T . The solution structures (9-10), however,
does not change.
Although an exact Green’s function method with
Dirichlet boundary condition using spectral analysis
(eigenvalues and eigenvectors) has been established by
Chung and Yau , we ﬁnd our method more convenient for computational purpose. With our method, the
Greens function Ωis computed once and can be used for
all diﬀerent BC. This is immensely more eﬃcient than
ﬁnding all the eigenvalues and eigenvectors for every BC
needed for each individual. Furthermore, it would not be
FIG. 1: Comparison between the exact solution (bold line)
eqs.(9-10) and our approximation.
For both cases we plot
the “hottest” nodes. For better visualization we shifted the
proﬁles such that the ﬁrst node coincide in the graph. We
observe a good agreement between the exact solution and the
approximation for M = 10 in our artiﬁcial network.
practical to ﬁnd all the eigenvectors of matrices resulting
from networks of millions of nodes.
To apply our method, one may either choose to fully
invert (I −Q) or take its approximate form.
The direct inversion of (I −Q) may still be computationally
challenging for a matrix of size millions by millions. In
terms of approximations, we ﬁnd the use of (I −Q)−1 ≡
limM→∞Ω(M) particularly useful, with
I + P + · · · + P M −M|u0⟩⟨v0|
This approximation gets better for larger M. This is because the larger M is, the smaller the diﬀerence between
P M and |u0⟩⟨v0|. One may then use Ω21(M)Ω11
in place of Ω21Ω11
−1. The quality of this approximation
may be be veriﬁed comparing the two models: the exact
solution(9-10) versus the approximate one (ie. replacing
−1 by Ω21(M)Ω11
−1(M) in the exact solution).
The convergence of the approximate solution to the exact solution (eqs.(9-10)) was ﬁrst tested on an artiﬁcially
generated random network of 100 nodes. Aside from the
condition that the nodes do not form disjoint clusters,
a pair of nodes has probability p = 0.1 to be connected.
One then randomly selects a sink node and a source node
that are not directly linked. We expect to get very similar shape of the temperature-proﬁle as in the exact case.
This is because for the row-normalized matrix, the |u0⟩
vector being a column vector with 1 in each entry may
induce a small but uniform oﬀset in the approximate solution. In Fig. 1, we plot the “temperature-proﬁle” of
the 15 hottest nodes from the exact solution and the
“temperature-proﬁle” of the same nodes using our approximation solution of various M. A good agreement
between the exact solution and the approximate solution
is reached at about M = 10.
To test the usability of our approach in real world,
we use the movielens database.
MovieLens (movie-
FIG. 2: Prediction performance on movielens database. The
heat conduction model outperforms the mean predictor and
the Pearson correlation based method as well. ξ denotes the
fraction of possible votes in the matrix.
The vertical line,
corresponding approximately to the giant cluster formation
threshold in the movie – movie network, has vote density
ξ ≈2N −1/2M −1/6 , where N is the number of users, M
is the number of movies.
lens.umn.edu; grouplens.org) ratings are recorded on a
ﬁve stars scale and contain additional information, such
as the time at which an evaluation was made. The data
set we downloaded contains N = 6040 users × M = 3952
movies. However, only a fraction ξM = 0.041 of all possible votes were actually expressed. To be able to perform
the calculation in reasonable time, we decide to further
reduce the data size in each dimension by roughly 50%.
To preserve the statistical properties of the original data,
the pruning is done randomly without bias. In particular, we tried to maintain the probability distribution of
the number of votes per users, as well as the sparsity and
the N/M ratio. We want to stress that this is crucial
when testing the performance of predictive algorithms
on real data in an objective way. In fact, many recommender systems can be found in the literature that rely
on dense voting matrices , at least in the traning
data set. Typically, users who have judged too few items
are struck out, as well as items that have received too
few votes. We did not comply to such convention and
made an eﬀort to keep the ﬁltering level as low as possible, although this makes predictions much more diﬃcult.
Once ﬁltered, we cast the data set in a vote matrix V,
with number of users N = 3020 and number of movies
M = 1976. In this reduced vote matrix, the matrix element Vα,i represents the number of stars assigned to
movie j by user α and is set to zero for unexpressed
The total ﬁlling fraction of V is ξM = 0.0468.
The votes in V are then sorted according to their relative
timestamps. The last ntest = 104 expressed votes are collected to form our test set, while the rest of the expressed
votes form our training set. We denote by V(t) the vote
matrix information up to time t. That is, in V(t) all the
unexpressed votes up to time t are set to have zero star.
For the purpose of rating prediction, one will need a
movie – movie network. To accomplish this task, one may
compute the correlation coeﬃcient Cij(t) between movie
i and movie j using the expressed votes up to a certain
time t in the training set. Speciﬁcally, we denote µi(t) ≡
α=1 Vα,i(t) and σ2
α=1[Vα,i(t) −µi(t)]2.
The correlation coeﬃcient reads
α[Vα,i(t) −µi(t)][Vα,j(t) −µj(t)]
σi(t)σj(t)
With a speciﬁed cutoﬀCcut, one obtains an adjacency
matrix A(t), with Aij(t) = θ(Cij(t)−Ccut(t)). The value
of Ccut(t) is set so that the average degree per node k(t)
for the movie – movie network has the same number of
non-zero entries as [V(t)]T [V(t)].
Keeping the test set data ﬁxed, we progressively ﬁll
the vote matrix the training set data over time (using
the relative time stamps), say up to time t.
use A(t) to construct the the propagator D(t) based on
the information accumulated up to t. For each viewer
(user), the BC is simply given by the votes expressed by
the user up to time t. In the event that a user only has
one vote (or none) up to time t, the BC for that user is
given by randomly choosing one (or two) movie(s) and
use the average rating(s) of the movie(s) up to that time
as the boundary values . We then use our algorithm
to make predictions on the entire test set.
This test protocol is intended to reproduce real application tasks, where one aims to predict future votes
–which is, of course, much harder than predicting randomly picked evaluations. It is somewhat less realistic to
ﬁx the test set once and for all, but this has the advantage
to allow for more objective comparisons of the results.
Many diﬀerent accuracy metrics have been proposed to
assess the quality of recommendations (see ref. ), we
choose the Root Square Mean Error:
(β,j)∈test
β,j −Vβ,j)2/ntest,
β,j represents the predicted vote from our algorithm, Vβ,j represents the actual vote (rated by user β
on movie j) in the test set, and the sum runs over all
expressed votes in the test set. In our experiments, the
RSME is calculated, at diﬀerent sparsity values ξ, on a
unique test set.
Fig. 2 summarizes the performance comparison of our
model with the mean predictor (the prediction is simply given by the objects mean value) and the widely
used Pearson correlation based method .
model outperforms both after enough votes (of the order
of N 1/2M 5/6) have been expressed. Since the dimensions
of the vote matrix V is known in a real application, given
the number of expressed votes, it is relatively easy to see
where one stands in terms of information content and
whether our method will perform well using the given
partial information.
In summary, we have devised a recommendation mechanism using analog to heat conduction. The innovation
of our method is its capability to compute the Green’s
function needed just once to accommodate all possible
BC. In terms of generalization, it is apparent that our
method can be applied to network with weighted edges,
with Aij = wij ≥0. Whether such a generalization will
improve the performance will be investigated in a separate publication. Finally, we stress that our study is not
aimed to extract statistical properties out of networks
through constructing model networks mimicking the real
world networks ; nor are we pursuing analysis of
slowly decaying eigenmodes in the absence of boundary condtitions. Instead, our goal is to provide a framework that is capable of providing individualized information extraction from a real world network.
YCZ and MB were partially supported by Swiss National Science Foundation grant 205120-113842. YCZ acknowledges hospitality at Management School, UESTC,
China, where part of the work is done.
The research
of YKY was supported by the Intramural Research Program of the National Library of Medicine at the NIH.
 S. Maslov and Y.C. Zhang, Phys. Rev. Lett. 87, 248701
 M. Blattner, Y.C. Zhang, and S. Maslov, Physica A 373,
753 .
 G. Adomavicius and A. Tuzhilin, IEEE Transactions on
Knowledge and Data Engineering 17, 734 , ISSN
1041-4347.
 W. Press, S. Teukolsky, B. Flannery, and V. Vetterling,
Numerical Recipes in C .
 G. Korniss, M. Hastings, K. Bassler, M. Berryman,
B. Kozma, and D. Abbott, Phys. Lett. A 350, 324 ,
ISSN 1046-8188.
 F. Wu, J. Phys. A 37, 6653 .
 F. Chung and S. Yau, Journal of Combinatorial Theory(A) pp. 141–214 .
 K. Goldberg, T. Roeder, D. Guptra, and C. Perkins, Information Retrieval 4, 133 .
 A. Waern, User Modeling and User-Adapted Interaction
14, 201 .
 Assuming that the vote matrix is ﬁlled randomly, one
can show that the density needs to be ξ ≥N −1/2M −1/6
to have in the movie–movie network a linking probability
p ≥M −1/3, which marks the onset of giant cluster formation. See B. Bollob´as, Random Graphs, chap. 6 .
 This is to avoid the artifact of null information retrieval:
e.g. assume only one boundary node with a speciﬁed temperature, all nodes will reach the same temperature upon
thermal equilibrium.
 J. Herlocker, J. Konstan, L. Terveen, and J. Riedl, ACM
Trans. Inf. Syst. 22, 5 , ISSN 1046-8188.
 P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and
J. Riedl, in Proceedings of ACM 1994 Conference on
Computer Supported Cooperative Work , pp. 175–186.
 J. Herlocker, J. Konstan, and J. Riedl, in Computer Supported Cooperative Work , pp. 241–250.
 M. Newman, SIAM Review 45, 167 .
 J. Park and M. Newman, Phys. Rev. E 70, 066117 .
 K. A. Eriksen, I. Simonsen, S. Maslov, and K. Sneppen,
Phys. Rev. Lett. 90, 148701 .