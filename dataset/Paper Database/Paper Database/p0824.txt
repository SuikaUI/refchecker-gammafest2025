Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2947–2954
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Document-Level Neural Machine Translation
with Hierarchical Attention Networks
Lesly Miculicich† ‡
Dhananjay Ram† ‡
Nikolaos Pappas†
James Henderson†
† Idiap Research Institute, Switzerland
‡ ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Switzerland
{lmiculicich, dram, npappas, jhenderson}@idiap.ch
Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose
a hierarchical attention model to capture the
context in a structured and dynamic manner.
The model is integrated in the original NMT
architecture as another level of abstraction,
conditioning on the NMT model’s own previous hidden states. Experiments show that hierarchical attention signiﬁcantly improves the
BLEU score over a strong NMT baseline with
the state-of-the-art in context-aware methods,
and that both the encoder and decoder beneﬁt
from context in complementary ways.
Introduction
Neural machine translation (NMT) 
trains an encoder-decoder network on sentence
pairs to maximize the likelihood of predicting
a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse
connections between sentences and other valuable
contextual information, this simpliﬁcation potentially degrades the coherence and cohesion of a
translated document . Recent studies
 have demonstrated that adding contextual information to the
NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text . Most of these methods use an additional encoder to extract contextual information from previous source-side sentences.
However, this requires additional parameters and it does not exploit the representations already learned by the
NMT encoder.
More recently, Tu et al. 
have shown that a cache-based memory network
performs better than the above encoder-based
The cache-based memory keeps past
context as a set of words, where each cell corresponds to one unique word keeping the hidden
representations learned by the NMT while translating it. However, in this method, the word representations are stored irrespective of the sentences
where they occur, and those vector representations
are disconnected from the original NMT network.
We propose to use a hierarchical attention network (HAN) to model the
contextual information in a structured manner using word-level and sentence-level abstractions. In
contrast to the hierarchical recurrent neural network (HRNN) used by , here
the attention allows dynamic access to the context
by selectively focusing on different sentences and
words for each predicted word. In addition, we integrate two HANs in the NMT model to account
for target and source context. The HAN encoder
helps in the disambiguation of source-word representations, while the HAN decoder improves the
target-side lexical cohesion and coherence. The
integration is done by (i) re-using the hidden representations from both the encoder and decoder
of previous sentence translations and (ii) providing input to both the encoder and decoder for the
current translation. This integration method enables it to jointly optimize for multiple-sentences.
Furthermore, we extend the original HAN with a
multi-head attention to capture different types of discourse phenomena.
Our main contributions are the following:
(i) We propose a HAN framework for translation
to capture context and inter-sentence connections
in a structured and dynamic manner. (ii) We integrate the HAN in a very competitive NMT ar-
chitecture and show significant improvement over two strong baselines on
multiple data sets. (iii) We perform an ablation
study of the contribution of each HAN conﬁguration, showing that contextual information obtained
from source and target sides are complementary.
The Proposed Approach
The goal of NMT is to maximize the likelihood
of a set of sentences in a target language represented as sequences of words y = (y1, ..., yt)
given a set of input sentences in a source language
x = (x1, ..., xm) as:
log(PΘ(yn|xn))
so, the translation of a document D is made by
translating each of its sentences independently. In
this study, we introduce dependencies on the previous sentences from the source and target sides:
log(PΘ(yn|xn, Dxn, Dyn))
= (xn−k, ..., xn−1) and Dyn
(yn−k, ..., yn−1) denote the previous k sentences
from source and target sides respectively. The contexts Dxn and Dyn are modeled with HANs.
Hierarchical Attention Network
The proposed HAN has two levels of abstraction.
The word-level abstraction summarizes information from each previous sentence j into a vector sj
qw = fw(ht)
sj = MultiHead
where h denotes a hidden state of the NMT network. In particular, ht is the last hidden state of
the word to be encoded, or decoded at time step
i is the last hidden state of the i-th word
of the j-th sentence of the context. The function
fw is a linear transformation to obtain the query
qw. We used the MultiHead attention function proposed by to capture different types of relations among words. It matches the
query against each of the hidden representations
i (used as value and key for the attention).
The sentence-level abstraction summarizes the
contextual information required at time t in dt as:
Figure 1: Integration of HAN during encoding at time
step t, ˜ht is the context-aware hidden state of the word
xt. Similar architecture is used during decoding.
qs = fs(ht)
dt = FFN . Each
layer is followed by a normalization layer .
Context Gating
We use a gate to regulate the
information at sentence-level ht and the contextual
information at document-level dt. The intuition
is that different words require different amount of
context for translation:
λt = σ(Whht + Wddt)
eht = λtht + (1 −λt)dt
where Wh, Wp are parameter matrices, and eht is
the ﬁnal hidden representation for a word xt or yt.
Integrated Model
The context can be used during encoding or decoding a word, and it can be taken from previously
encoded source sentences, previously decoded target sentences, or from previous alignment vectors
 ). The
different conﬁgurations will deﬁne the input query
and values of the attention function. In this work
we experiment with ﬁve of them: one at encoding time, three at decoding time, and one combining both. At encoding time the query is a function of the hidden state hxt of the current word
to be encoded xt, and the values are the encoded
states of previous sentences hj
xi (HAN encoder).
At decoding time, the query is a function of the
hidden state hyt of the current word to be decoded
yt, and the values can be (a) the encoded states
of previous sentences hj
xi (HAN decoder source),
(b) the decoded states of previous sentences hj
(HAN decoder), and (c) the alignment vectors cj
(HAN decoder alignment). Finally, we combine
complementary target-source sides of the context
by joining HAN encoder and HAN decoder. Figure 1 shows the integration of the HAN encoder
with the NMT model; a similar architecture is applied to the decoder. The output ˜ht is used by the
NMT model as replacement of ht during the ﬁnal
classiﬁcation layer.
Experimental Setup
Datasets and Evaluation Metrics
We carry out experiments with Chinese-to-English
(Zh-En) and Spanish-to-English (Es-En) sets on
three different domains: talks, subtitles, and news.
TED Talks is part of the IWSLT 2014 and 2015
 evaluation campaigns1.
We use dev2010 for development; and tst2010-
2012 (Es-En), tst2010-2013 (Zh-En) for testing.
The Zh-En subtitles corpus is a compilation of TV
subtitles designed for research on context . In contrast to the other sets, it has
three references to compare. The Es-En corpus is
a subset of OpenSubtitles2018 2. We randomly select two episodes
for development and testing each. Finally, we use
the Es-En News-Commentaries113 corpus which
has document-level delimitation. We evaluate on
WMT sets : newstest2008 for
development, and newstest2009-2013 for testing.
A similar corpus for Zh-En is too small to be comparable. Table 2 shows the corpus statistics.
For evaluation, we use BLEU score (multi-blue) on tokenized text, and we
measure signiﬁcance with the paired bootstrap resampling method proposed by Koehn ).
Model Conﬁguration and Training
As baselines, we use a NMT transformer, and a
context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by Tu et al.
 , with memory size of 25 words. We used
the OpenNMT implementation
of the transformer network. The conﬁguration is
the same as the model called “base model” in the
1 
2 
3 
original paper . The encoder
and decoder are composed of 6 hidden layers each.
All hidden states have dimension of 512, dropout
of 0.1, and 8 heads for the multi-head attention.
The target and source vocabulary size is 30K. The
optimization and regularization methods were the
same as proposed by Vaswani et al. .
Inspired by Tu et al. we trained the models
in two stages. First we optimize the parameters
for the NMT without the HAN, then we proceed
to optimize the parameters of the whole network.
We use k = 3 previous sentences, which gave the
best performance on the development set.
Experimental Results
Translation Performance
Table 1 shows the BLEU scores for different models. The baseline NMT transformer already has
better performance than previously published results on these datasets, and we replicate previous
previous improvements from the cache method
over the this stronger baseline. All of our proposed
HAN models perform at least as well as the cache
method. The best scores are obtained by the combined encoder and decoder HAN model, which
is signiﬁcantly better than the cache method on
all datasets without compromising training speed
(2.3K vs 2.6K tok/sec). An important portion of
the improvement comes from the HAN encoder,
which can be attributed to the fact that the sourceside always contains correct information, while
the target-side may contain erroneous predictions
at testing time. But combining HAN decoder with
HAN encoder further improves translation performance, showing that they contribute complementary information. The three ways of incorporating
information into the decoder all perform similarly.
Table 3 shows the performance of our best HAN
model with a varying number k of previous sentences in the test-set. We can see that the best performance for TED talks and news is archived with
3, while for subtitles it is similar between 3 and 7.
Accuracy of Pronoun/Noun Translations
We evaluate coreference and anaphora using the
reference-based metric:
accuracy of pronoun
translation , which can be extended for nouns. The list
of evaluated pronouns is predeﬁned in the metric, while the list of nouns was extracted using
NLTK POS tagging . The upper part
NMT transformer
+ cache 
17.32 (+0.45)∗∗∗36.46 (+1.02)∗∗∗28.86 (+0.26)
35.49 (+0.29)
22.36 (+1.00)∗∗∗
+ HAN encoder
17.61 (+0.74)∗∗∗
36.91 (+1.47)∗∗∗
29.35 (+0.75)∗
35.96 (+0.76)∗
22.36 (+1.00)∗∗∗
+ HAN decoder
17.39 (+0.52)∗∗∗37.01 (+1.57)∗∗∗
††† 29.21 (+0.61)∗35.50 (+0.30)
22.62 (+1.26)∗∗∗
+ HAN decoder source
17.56 (+0.69)∗∗∗
36.94 (+1.50)∗∗∗
28.92 (+0.32)
35.71 (+0.51)∗22.68 (+1.32)∗∗∗
+ HAN decoder alignment
17.48 (+0.61)∗∗∗
37.03 (+1.60)∗∗∗
††† 28.87 (+0.27)
35.63 (+0.43)
22.59 (+1.23)∗∗∗
+ HAN encoder + HAN decoder 17.79 (+0.92)∗∗∗
††† 37.24 (+1.80)∗∗∗
††† 29.67 (+1.07)∗∗
36.23 (+1.03)∗∗
†† 22.76 (+1.40)∗∗∗
Table 1: BLEU score for the different conﬁgurations of the HAN model, and two baselines. The highest score
per dataset is marked in bold. ∆denotes the difference in BLEU score with respect of the NMT transformer.
The signiﬁcance values with respect to the NMT and the cache method are denoted by ∗, and † respectively. The
repetitions correspond to the p-values: ∗
† < .05,∗∗
†† < .01,∗∗∗
††† < .001.
Development
Table 2: Dataset statistics in # sentence pairs.
of Table 4 shows the results. For nouns, the joint
HAN achieves the best accuracy with a signiﬁcant
improvement compared to other models, showing
that target and source contextual information are
complementary. Similarity for pronouns, the joint
model has the best result for TED talks and news.
However, HAN encoder alone is better in the case
of subtitles.
Here HAN decoder produces mistakes by repeating past translated personal pronouns. Subtitles is a challenging corpus for personal pronoun disambiguation because it usually
involves dialogue between multiple speakers.
Cohesion and Coherence Evaluation
We use the metric proposed by Wong and Kit
 to evaluate lexical cohesion. It is deﬁned as
the ratio between the number of repeated and lexically similar content words over the total number
of content words in a target document. The lexical similarity is obtained using WordNet. Table 4
(bottom-left) displays the average ratio per tested
document. In some cases, HAN decoder achieves
the best score because it produces a larger quantity of repetitions than other models.
as previously demonstrated in 4.2, repetitions do
not always make the translation better. Although
HAN boosts lexical cohesion, the scores are still
far from the human reference, so there is room for
improvement in this aspect.
For coherence, we use a metric based on Latent
Semantic Analysis (LSA) . LSA
is used to obtain sentence representations, then cosine similarity is calculated from one sentence to
Table 3: Performance for variable context sizes k with
the HAN encoder + HAN decoder.
the next, and the results are averaged to get a document score. We employed the pre-trained LSA
model Wiki-6 from . Table 4 (bottom-right) shows the average coherence
score of documents. The joint HAN model consistently obtains the best coherence score, but close
to other HAN models. Most of the improvement
comes from the HAN decoder.
Qualitative Analysis
Table 5 shows an example where HAN helped
to generate the correct translation. The ﬁrst box
shows the current sentence with the analyzed word
in bold; and the second, the past context at source
and target. For the context visualization we use
the toolkit provided by Pappas and Popescu-Belis
 . Red corresponds to sentences, and blue
to words. The intensity of color is proportional to
the weight. We see that HAN correctly translates
the ambiguous Spanish pronoun “su” into the English “his”. The HAN decoder highlighted a previous mention of “his”, and the HAN encoder highlighted the antecedent “Nathaniel”. This shows
that HAN can capture interpretable inter-sentence
connections. More samples with different attention heads are shown in the Appendix ??.
Related Work
Statistical Machine Translation (SMT) Initial
studies were based on cache memories (Tiede-
4NIST BLEU: NMT transformer 35.99, cache 36.52, and HAN 37.15.
Noun Translation
Pronoun Translation
NMT Transformer
+ HAN encoder
+ HAN decoder
+ HAN encoder + HAN decoder
Lexical cohesion
NMT Transformer
+ HAN encoder
+ HAN decoder
+ HAN enc. + HAN dec.
Human reference
Table 4: Evaluation on discourse phenomena. Noun and pronoun translation: Accuracy with respect to a human
reference. Lexical cohesion: Ratio of repeated and lexically similar words over the number of content words.
Coherence: Average cosine similarity of consecutive sentences (i.e. average of LSA word-vectors)
Currently Translated Sentence
y esto es un escape de su estado atormentado .
and that is an escape from his tormented state .
and this is an escape from its < unk > state .
Cache: and this is an escape from their state .
HAN: and this is an escape from his < unk > state .
Context from Previous Sentences
HAN decoder context with target. Query: his (En)
HAN encoder context with source. Query: su (Es)
Table 5: Example of pronoun disambiguation using
HAN (TED Talks Es-En).
mann, 2010; Gong et al., 2011). However, most
of the work explicitly models discourse phenomena such as lexical cohesion , coherence , and coreference . Hardmeier et al. introduced
the document-level SMT paradigm.
Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks , self-attention
 , and latent variables .
Document-level NMT Tiedemann and Scherrer
 use the concatenation of multiple sentences
as NMT’s input/output, Jean et al. add a
context encoder for the previous source sentence,
Wang et al. includes a HRNN to summarize source-side context, and Tu et al. use
a dynamic cache memory to store representations
of previously translated words.
Recently, Bawden et al. proposed test-sets for evaluating
discourse in NMT, Voita et al. shows that
context-aware NMT improves the of anaphoric
pronouns, and Maruf and Haffari proposed
a document-level NMT using memory-networks.
Conclusion
We proposed a hierarchical multi-head HAN NMT
model5 to capture inter-sentence connections. We
integrated context from source and target sides
by directly connecting representations from previous sentence translations into the current sentence translation. The model signiﬁcantly outperforms two competitive baselines, and the ablation
study shows that target and source context is complementary. It also improves lexical cohesion and
coherence, and the translation of nouns and pronouns.
The qualitative analysis shows that the
model is able to identify important previous sentences and words for the correct prediction. In future work, we plan to explicitly model discourse
connections with the help of annotated data, which
may further improve translation quality.
Acknowledgments
We are grateful for the support of the European
Union under the Horizon 2020 SUMMA project
n. 688139, see www.summa-project.eu.
5Code available at 
Project Towards Document-Level NMT