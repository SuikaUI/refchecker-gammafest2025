“© 2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating
new collective works, for resale or redistribution to servers or lists, or reuse of any
copyrighted component of this work in other works.”
 
Unlabeled Samples Generated by GAN
Improve the Person Re-identiﬁcation Baseline in vitro
Zhedong Zheng
Liang Zheng
Centre for Artiﬁcial Intelligence, University of Technology Sydney
{zdzheng12,liangzheng06,yee.i.yang}@gmail.com
The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set
without collecting extra data. It is challenging in 1) how
to obtain more training data only from the training set and
2) how to use the newly generated data. In this work, the
generative adversarial network (GAN) is used to generate
unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline.
We verify the proposed method on a practical problem:
person re-identiﬁcation (re-ID). This task aims to retrieve a
query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for
sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show
that adding the GAN-generated data effectively improves
the discriminative ability of learned CNN embeddings. On
three large-scale datasets, Market-1501, CUHK03 and
DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46%
improvement in rank-1 precision over the baseline CNN,
respectively. We additionally apply the proposed method
to ﬁne-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at
 
1. Introduction
Unsupervised learning can serve as an important auxiliary task to supervised tasks . In this work,
we propose a semi-supervised pipeline that works on the
original training set without an additional data collection
process. First, the training set is expanded with unlabeled
data using a GAN. Then our model minimizes the sum of
the supervised and the unsupervised losses through a new
∗To whom all correspondence should be addressed.
Figure 1. The pipeline of the proposed method. There are two
components: a generative adversarial model for unsupervised
learning and a convolutional neural network for semi-supervised
“Real Data” represents the labeled data in the given
training set; “Training data” includes both the “Real Data” and
the generated unlabeled data. We aim to learn more discriminative
embeddings with the “Training data”.
regularization method. This method is evaluated with person re-ID, which aims to spot the target person in different
cameras. This has been recently viewed as an image retrieval problem .
This paper addresses three challenges. First, current research in GANs typically considers the quality of the sample generation with and without semi-supervised learning
in vivo . Yet a scientiﬁc problem remains unknown: moving the generated samples out of the
box and using them in currently available learning frameworks. To this end, this work uses unlabeled data produced
by the DCGAN model in conjunction with the labeled
training data. As shown in Fig. 1, our pipeline feeds the
newly generated samples into another learning machine (i.e.
a CNN). Therefore, we use the term “in vitro” to differentiate our method from ; these methods perform
semi-supervised learning in the discriminator of the GANs
(in vivo).
Second, the challenge of performing semi-supervised
learning using labeled and unlabeled data in CNN-based
methods remains. Usually, the unsupervised data is used as
a pre-training step before supervised learning .
Our method uses all the data simultaneously. In , the unlabeled/weak-labeled real data are assigned
labels according to pre-deﬁned training classes, but our
method assumes that the GAN generated data does not belong to any of the existing classes. The proposed LSRO
method neither includes unsupervised pre-training nor label assignments for the known classes. We address semisupervised learning from a new perspective. Since the unlabeled samples do not belong to any of the existing classes,
they are assigned a uniform label distribution over the training classes. The network is trained not to predict a particular
class for the generated data with high conﬁdence.
Third, in person re-ID, data annotation is expensive, because one has to draw a pedestrian bounding box and assign an ID label to it. Recent progress in this ﬁeld can be
attributed to two factors: 1) the availability of large-scale re-
ID datasets and 2) the learned embedding
of pedestrians using a CNN . That being said, the
number of images for each identity is still limited, as shown
in Fig. 2. There are 17.2 images per identities in Market-
1501 , 9.6 images in CUHK03 , and 23.5 images in
DukeMTMC-reID on average. So using additional data
is non-trivial to avoid model overﬁtting. In the literature,
pedestrian images used in training are usually provided by
the training sets, without being expanded. So it is unknown
if a larger training set with unlabeled images would bring
any extra beneﬁt. This observation inspired us to resort to
the GAN samples to enlarge and enrich the training set. It
also motivated us to employ the proposed regularization to
implement a semi-supervised system.
In an attempt to overcome the above-mentioned challenges, this paper 1) adopts GAN in unlabeled data generation, 2) proposes the label smoothing regularization for
outliers (LSRO) for unlabeled data integration, and 3) reports improvements over a CNN baseline on three person
re-ID datasets. In more details, in the ﬁrst step, we train
DCGAN on the original re-ID training set. We generate new pedestrian images by inputting 100-dim random
vectors in which each entry falls within [-1, 1]. Some generated samples are shown in Fig. 3 and Fig. 5. In the
second step, these unlabeled GAN-generated data are fed
into the ResNet model . The LSRO method regularizes the learning process by integrating the unlabeled data
and, thus, reduces the risk of over-ﬁtting. Finally, we evaluate the proposed method on person re-ID and show that the
learned embeddings demonstrate a consistent improvement
over the strong ResNet baseline.
To summarize, our contributions are:
• the introduction of a semi-supervised pipeline that integrates GAN-generated images into the CNN learning
machine in vitro;
• an LSRO method for semi-supervised learning. The
integration of unlabeled data regularizes the CNN
learning process. We show that the LSRO method is
Figure 2. The image distribution per class in the dataset Market-
1501 , CUHK03 and DukeMTMC-reID . We observe
that all these datasets suffer from the limited images per class.
Note that there are only a few classes with more than 20 images.
superior to the two available strategies for dealing with
unlabeled data; and
• a demonstration that the proposed semi-supervised
pipeline has a consistent improvement over the ResNet
baseline on three person re-ID datasets and one ﬁnegrained recognition dataset.
2. Related Work
In this section, we will discuss the relevant works on
GANs, semi-supervised learning and person re-ID.
2.1. Generative Adversarial Networks
The generative adversarial networks (GANs) learn two
sub-networks: a generator and a discriminator. The discriminator reveals whether a sample is generated or real,
while the generator produces samples to cheat the discriminator. The GANs are ﬁrst proposed by Goodfellow et al.
 to generate images and gain insights into neural networks. Then, DCGANs provides some techniques to
improve the stability of training. The discriminator of DC-
GAN can serve as a robust feature extractor. Salimans et al.
 achieve a state-of-art result in semi-supervised classiﬁcation and improves the visual quality of GANs. InfoGAN
 learns interpretable representations by introducing latent
codes. On the other hand, GANs also demonstrate potential in generating images for speciﬁc ﬁelds. Pathak et al.
 propose an encoder-decoder method for image inpainting, where GANs are used as the image generator. Similarly, Yeh et al. improve the inpainting performance
by introducing two loss types. In , 3D object images
are generated by a 3D-GAN. In this work, we do not focus on investigating more sophisticated sample generation
methods. Instead, we use a basic GAN model to generate unlabeled samples from the training data and show that
these samples help improve discriminative learning.
2.2. Semi-supervised Learning
Semi-supervised learning is a sub-class of supervised
learning taking unlabeled data into consideration, especially
Figure 3. Examples of GAN images and real images. (a) The top
two rows show the pedestrian samples generated by DCGAN 
trained on the Market-1501 training set . (b) The bottom row
shows the real samples in training set. Although the generated
images in (a) can be easily recognized as fake images by a human,
they still serve as an effective regularizer in our experiment.
when the volume of annotated data is small. On the one
hand, some research treats unsupervised learning as an auxiliary task to supervised learning. For example, in ,
Hinton et al. learn a stack of unsupervised restricted Boltzmann machines to pre-train the model. Ranzato et al. propose to reconstruct the input at every level of a network
to get a compact representation . In , the auxiliary task of ladder networks is to denoise representations
at every level of the model. On the other hand, several
works assign labels to the unlabeled data. Papandreou et
al. combine strong and weak labels in CNNs using an
expectation-maximization (EM) process for image segmentation. In , Lee assigns a “pseudo label” to the unlabeled
data in the class that has the maximum predicted probability. In , the samples produced by the generator of
the GAN are all taken as one class in the discriminator. Departing from previous semi-supervised works, we adopt a
different regularization approach by assigning a uniform label distribution to the generated samples.
2.3. Person Re-identiﬁcation
Some pioneering works focus on ﬁnding discriminative
handcrafted features . Recent progress in person re-ID mainly consists of advancing CNNs. Yi et al.
 split a pedestrian image into three horizontal parts and
respectively train three part-CNNs to extract features. Similarly, Cheng et al. split the convolutional map into four
parts and fuse the part features with the global feature. In
 , Li et al. add a new layer that multiplies the activation
of two images in different horizontal stripes. They use this
layer to explicitly allow patch matching in the CNN. Later,
Ahmed et al. improve the performance by proposing a
new patch matching layer that compares the activation of
two images in neighboring pixels. In addition, Varior et
al. combine the CNN with some gate functions, aiming to adaptively focus on the salient parts of input image
pairs, this method is limited by computational inefﬁciency
because the input should be image pairs.
A CNN can be very discriminative by itself without
explicit part-matching.
Zheng et al. directly
use a conventional ﬁne-tuning approach (called the IDdiscriminative embedding, or IDE) on the Market-1501
dataset and its performance exceeds many other recent results. Wu et al. combine the CNN embedding
with hand-crafted features. In , Zheng et al. combine
an identiﬁcation model with a veriﬁcation model and improve the ﬁne-tuned CNN performance. In this work, we
adopt the IDE model as a baseline, and show that
the GAN samples and LSRO effectively improve its performance. Recently, Barbosa et al. propose synthesizing human images through a photorealistic body generation
software. These images are used to pre-train an IDE model
before dataset-speciﬁc ﬁne-tuning. Our method is different
from in both data generation and the training strategy.
3. Network Overview
In this section, we describe the pipeline of the proposed
method. As shown in Fig. 1, the real data in the training
set is used to train the GAN model. Then, the real training
data and the newly generated samples are combined into
training input for the CNN. In the following section, we
will illustrate the structure of the two components, i.e., the
GAN and the CNN, in detail. Note that, our system does
not make major changes to the network structures of
the GAN or the CNN with one exception - the number
of neurons in the last fully-connected layer in the CNN
is modiﬁed according to the number of training classes.
3.1. Generative Adversarial Network
Generative adversarial networks have two components:
a generator and a discriminator. For the generator, we follow the settings in . We start with a 100-dim random
vector and enlarge it to 4 × 4 × 16 using a linear function.
To enlarge the tensor, ﬁve deconvolution functions are used
with a kernel size of 5 × 5 and a stride of 2. Every deconvolution is followed by a rectiﬁed linear unit and batch
normalization. Additionally, one optional deconvolutional
layer with a kernel size of 5 × 5 and a stride of 1, and one
tanh function are added to ﬁne-tune the result. A sample
that is 128 × 128 × 3 in size can then be generated.
The input of the discriminator network includes the generated images and the real images in the training set. We use
ﬁve convolutional layers to classify whether the generated
image is fake. Similarly, the size of the convolutional ﬁlters
is 5 × 5 and their stride is 2. We add a fully-connected layer
to perform the binary classiﬁcation (real or fake).
3.2. Convolutional Neural Network
The ResNet-50 model is used in our experiment. We
resize the generated images to 256 × 256 × 3 using bilinear
sampling. The generated images are mixed with the original
training set as the input of the CNN. That is, the labeled and
unlabeled data are simultaneously trained. These training
images are shufﬂed. Following the conventional ﬁne-tuning
strategy , we use a model pre-trained on ImageNet .
We modify the last fully-connected layer to have K neurons to predict the K-classes, where K is the number of the
classes in the original training set (as well as the merged
new training set). Unlike , we do not view the new
samples as an extra class but assign a uniform label distribution over the existing classes. So the last fully-connected
layer remains K-dimensional. The assigned label distribution of the generated images is discussed in the next section.
4. The Proposed Regularization Method
In this section, we ﬁrst revisit the label smoothing regularization (LSR), which is used for fully-supervised learning. We then extend LSR to the scenario of unlabeled learning, yielding the proposed label smoothing regularization
for outliers (LSRO) method.
4.1. Label Smoothing Regularization Revisit
LSR was proposed in the 1980s and recently rediscovered by Szegedy et al. . In a nutshell, LSR assigns small values to the non-ground truth classes instead
of 0. This strategy discourages the network to be tuned towards the ground truth class and thus reduces the chances of
over-ﬁtting. LSR is proposed for use with the cross-entropy
loss .
Formally, let k ∈{1, 2, ..., K} be the pre-deﬁned classes
of the training data, where K is the number of classes. The
cross-entropy loss can be formulated as:
log (p(k))q(k),
where p(k) ∈ is the predicted probability of the input belonging to class k, and can be outputted by CNN. It
is derived from the softmax function which normalizes the
output of the previous fully-connected layer. q(k) is the
ground truth distribution. Let y be the ground truth class
label, q(k) can be deﬁned as:
If we discard the 0 terms in Eq. 1, the cross-entropy loss is
equivalent to only considering the ground truth term in Eq.
l = −log (p(y)).
Figure 4. The label distributions of a real image and a GANgenerated image in our system. We use a classical label distribution (Eq. 2) for the real image (left). For the generated image
(right), we employ the proposed LSRO label distribution (Eq. 6),
e.g. a uniform distribution on every training class because the generated image is assumed to belong to none of the training classes.
We employ a cross-entropy loss that combines the two types of
label distributions as the optimization objective (Eq. 7).
So, minimizing the cross-entropy loss is equivalent to maximizing the predicted probability of the ground-truth class.
In , the label smoothing regularization (LSR) is introduced to take the distribution of the non-ground truth
classes into account. The network is thus encouraged not
to be too conﬁdent towards the ground truth. In , the
label distribution qLSR(k) is written as:
where ε ∈ is a hyperparameter. If ε is zero, Eq. 4
reduces to Eq. 2. If ε is too large, the model may fail to
predict the ground truth label. So in most cases, ε is set to
0.1. Szegedy et al. assume that the non-ground truth classes
take on a uniform label distribution. Considering Eq. 1 and
Eq. 4, the cross-entropy loss evolves to:
lLSR = −(1 −ε) log (p(y)) −ε
log (p(k)).
Compared with Eq. 3, Eq. 5 pays additional attention to the
other classes, rather than only the ground truth class. In this
paper, we do not employ LSR on the IDE baseline because
it yields a slightly lower performance than using Eq. 2 (see
Section 5.3). We re-introduce LSR because it inspires us in
designing the LSRO method.
4.2. Label Smoothing Regularization for Outliers
The label smoothing regularization for outliers (LSRO)
is used to incorporate the unlabeled images in the network.
This extends LSR from the supervised domain to leverage
unsupervised data generated by the GAN.
In LSRO, we propose a virtual label distribution for the
unlabeled images. We set the virtual label distribution to
be uniform over all classes, due to two inspirations. 1) We
assume that the generated samples do not belong to any predeﬁned classes. 2) LSR assumes a uniform distribution over
the all classes to address over-ﬁtting. During testing, we
expect that the maximum class probability of a generated
image will be low, i.e., the network will fail to predict a particular class with high conﬁdence. Formally, for a generated
image, its class label distribution, qLSRO(k), is deﬁned as:
qLSRO(k) = 1
We call Eq. 6 the label smoothing regularization for outliers
The one-hot distribution deﬁned in Eq. 2 will still be
used for the loss computation for the real images in the
training set. Combining Eq. 2, Eq. 6 and Eq. 1, we can
re-write the cross-entropy loss as:
lLSRO = −(1 −Z) log (p(y)) −Z
log (p(k)).
For a real training image, Z = 0. For a generated training
image, Z = 1. So our system actually has two types of
losses, one for real images and one for generated images.
Advantage of LSRO. Using LSRO, we can deal with
more training images (outliers) that are located near the real
training images in the sample space, and introduce more
color, lighting and pose variances to regularize the model.
For instance, if we only have one green-clothed identity in
the training set, the network may be misled into considering
that the color green is a discriminative feature, and this limits the discriminative ability of the model. By adding generated training samples, such as an unlabeled green-clothed
person, the classiﬁer will be penalized if it makes the wrong
prediction towards the labeled green-clothed person. In this
manner, we encourage the network to ﬁnd more underlying
causes and to be less prone to over-ﬁtting. We only use the
GAN trained on the original training set to produce outlier
images. It would be interesting to further evaluate whether
real-world unlabeled images are able to achieve a similar
effect (see Table 4).
Competing methods. We compare LSRO with two alternative methods. Details of both methods are available in
existing literature ; breif descriptions follow.
• All in one. Using , a new class label is created,
i.e., K + 1, and every generated sample is assigned to
this class. CNN training follows in Section 5.2.
• Pseudo label. Using , during network training,
each incoming GAN-image is passed forward through
the current network and is assigned a pseudo label by
taking the maximum value of the probability prediction vector (p(k) in Eq.
This GAN-image can
be thus trained in the network with this pseudo label.
During training, the pseudo label is assigned dynamically, so that the same GAN-image may receive different pseudo labels each time it is fed into the network.
In our experiments, we begin feeding GAN images and
assigning them pseudo labels after 20 epochs. We also
set a global weight to the softmax loss of 0.1 to the
GAN and 1 to the real images.
Our experimental results show that the two methods also
work on the GAN images and that LSRO is superior to “All
in one” and “Pseudo label”. Explanations are provided in
the Section 5.3.
5. Experiment
We mainly evaluate the proposed method using the
Market-1501 dataset, because it is a large scale and has
a ﬁxed training/testing split. We also report results on the
CUHK03 dataset , but due to the computational cost
of 20 training/testing splits, we only use the GAN images
generated from the Market-1501 dataset. In addition,
we evaluate our method on a recently released pedestrian
dataset DukeMTMC-reID and a ﬁne-grained recognition dataset CUB-200-2011 .
5.1. Person Re-id Datasets
Market-1501 is a large-scale person re-ID dataset collected from six cameras. It contains 19,732 images for testing and 12,936 images for training. The images are automatically detected by the deformable part model (DPM) ,
so misalignment is common, and the dataset is close to realistic settings. There are 751 identities in the training set and
750 identities in the testing set. There are 17.2 images per
identity in the training set. We use all the 12,936 detected
images from the training set to train the GAN.
CUHK03 contains 14,097 images of 1,467 identities.
Each identity is captured by two cameras on the CUHK
campus. This dataset contains two image sets. One is annotated by hand-drawn bounding boxes, and the other is produced by the DPM detector . We use the detected set in
this paper. There are 9.6 images per identity in the training
set. We report the averaged result after training/testing 20
times. We use the single shot setting.
DukeMTMC-reID is a subset of the newly-released
multi-target,
multi-camera pedestrian tracking dataset
 . The original dataset contains eight 85-minute highresolution videos from eight different cameras.
Handdrawn pedestrian bounding boxes are available.
work, we use a subset of for image-based re-ID, in the
format of the Market-1501 dataset . We crop pedestrian images from the videos every 120 frames, yielding
36,411 total bounding boxes with IDs annotated by .
The DukeMTMC-reID dataset for re-ID has 1,812 identities from eight cameras. There are 1,404 identities appearing in more than two cameras and 408 identities (distractor
ID) who appear in only one camera. We randomly select
702 IDs as the training set and the remaining 702 IDs as
the testing set. In the testing set, we pick one query image
for each ID in each camera and put the remaining images in
the gallery. As a result, we get 16,522 training images with
702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images. The evaluation protocol is
available on our website . Some example re-ID results
from the DukeMTMC-reID are shown in Fig. 6.
5.2. Implementation Details
CNN re-ID baseline. We adopt the CNN re-ID baseline used in .
Speciﬁcally, the Matconvnet 
package is used. During training, We use the ResNet-50
model and modify the fully-connected layer to have
751, 702 and 1,367 neurons for Market-1501, DukeMTMCreID and CUHK03, respectively. All the images are resized to 256 × 256 before being randomly cropped into
224 × 224 with random horizontal ﬂipping. We insert a
dropout layer before the ﬁnal convolutional layer and set the
dropout rate to 0.5 for CUHK03 and 0.75 for Market-1501
and DukeMTMC-reID, respectively. We use stochastic gradient descent with momentum 0.9. The learning rate of the
convolution layers is set to 0.002 and decay to 0.0002 after
40 epochs and we stop training after the 50th epochs. During testing, we extract the 2,048-dim CNN embedding in
the last convolutional layer for an 224 × 224 input image.
The similarity between two images is calculated by a cosine
distance for ranking.
GAN training and testing. We use Tensorﬂow and
the DCGAN package to train the GAN model using the
provided data in the original training set without preprocessing (e.g., foreground detection). All the images are resized to 128×128 and randomly ﬂipped before training. We
use Adam with the parameters β1 = 0.5, β2 = 0.99.
We stop training after 30 epochs. During GAN testing, we
input a 100-dim random vector in GAN, and the value of
each entry ranges in [-1, 1]. The outputted image is resized
to 256 × 256 and then used in CNN training (with LSRO).
More GAN images are shown in Fig. 5.
5.3. Evaluation
The ResNet baseline. Using the training/testing procedure described in Section 5.2, we report the baseline performance of ResNet in Table 1, Table 5 and Table 3. The rank-
1 accuracy is 73.69%, 71.5% and 60.28% on Market-1501,
CUHK03 and DukeMTMC-reID respectively. Our baseline results are on par with the those reported in .
Note that the baseline alone exceeds many previous works
Figure 5. The newly generated images from a DCGAN model
trained on DukeMTMC-reID and CUB-200-2011.
LSRO, they are added to the training sets of DukeMTMC-reID
and CUB-200-2011 to regularize the CNN model.
 .
The GAN images improve the baseline.
in Table 2, when we add 24, 000 GAN images to the
CNN training, our method signiﬁcantly improves the re-
ID performance on Market-1501. We observe improvement
of +4.37% (from 73.69% to 78.06%) and +4.75% (from
51.48% to 56.23%) in rank-1 accuracy and mAP, respectively. On CUHK03, we observe improvements of +1.6%,
+1.2%, +0.8%, and +1.6% in rank-1, 5, 10 accuracy and
mAP, respectively. The improvement on CUHK03 is relatively small compared to that of Market-1501, because the
DCGAN model is trained on Market-1501 and the generated images share a more similar distribution with Market-
1501 than CUHK03.
We also observe improvements of
+2.46% and +2.14% in rank-1 and mAP, respectively, on
the strong ResNet baseline in the DukeMTMC-reID dataset.
These results indicate that the unlabeled images generated
by the GAN effectively yield improvements over the baseline using the LSRO method.
The impact of using different numbers of GAN images during training.
We evaluate how the number of
GAN images affects the re-ID performance. Since unlabeled data is easy to obtain, we expect the model would
learn more general knowledge as the number of unlabeled
images increases. The results on Market-1501 are shown in
Table 2. We note that the number of real training images in
Market-1501 is 12,936. Two observations are made.
First, the addition of different numbers of GAN images
consistently improves the baseline. Adding approximately
3×GAN images compared to the real training set still has a
+2.38% improvement to rank-1 accuracy.
Second, the peak performance is achieved when 2×GAN
images are added. When too few GAN sample are incorporated into the system, the regularization ability of the LSRO
is inadequate. In contrast, when too many GAN samples
are present, the learning machine tends to converge towards
assigning uniform prediction probabilities to all the training
samples, which is not desirable. Therefore, a trade-off is
recommended to avoid poor regularization and over-ﬁtting
Single Query
Multi. Query
BoW+kissme 
MR CNN 
FisherNet 
S-LSTM 
Gate Reid 
SOMAnet *
Verif.-Identif. *
DeepTransfer *
Basel. *
Basel. + LSRO
Verif-Identif. + LSRO
Table 1. Comparison of the state-of-the-art methods reported on
the Market-1501 dataset. We also provide results of the ﬁne-tuned
ResNet baseline. Rank-1 precision (%) and mAP (%) are listed. *
the respective paper is on ArXiv but not published.
# GAN Img.
All in one
Pseudo label
0 (basel.)
Table 2. Comparison of LSRO, “All in one”, and “Pseudo label”
under different numbers of GAN-generated images on Market-
1501. We show that LSRO is superior to the other two methods
whose best performance is highlighted in blue and red, respectively. Rank-1 accuracy (%) and mAP (%) are shown.
BoW+kissme 
LOMO+XQDA 
Basel. 
Basel. + LSRO
Table 3. Comparison of the baseline on DukeMTMC-reID. Rank-1
accuracy (%) and mAP (%) are shown.
of uniform label distributions.
GAN images vs. real images in training. To further
evaluate the proposed method, we replace the GAN images
with the real images from CUHK03 which are viewed as
unlabeled in training. Since CUHK03 only contains 14,097
images, we randomly select 12,000 for the fair comparison.
Experimental results are shown in Table 4. We compare the results obtained using the 12,000 CUHK03 images
and the 12,000 GAN images. We ﬁnd the real data from
CUHK03 also assists in the regularization and improves the
Unsup. Data
0 (basel.)
CUHK03-Real-12000
Market-1501-GAN-12000
Table 4. We add the 12,000 real pedestrian images in CUHK03 as
outliers to Market-1501. We ﬁnd the model trained on the generated samples slightly out-performs the model trained on CUHK03
real data. Rank-1 accuracy (%) and mAP (%) are shown.
rank-1 rank-5 rank-10 mAP
KISSME 
DeepReID 
BoW+HS 
LOMO+XQDA 
SI-CI 
SOMAnet *
Verif-Identif. *
DeepTransfer *
Basel. *
Basel.+LSRO
Verif-Identif. + LSRO
Table 5. Comparison of the state-of-the-art reports on the
CUHK03 dataset. We list the ﬁne-tuned ResNet baseline as well.
The mAP (%) and rank1 (%) precision are presented. * the respective paper is on ArXiv but not published.
performance. But the model trained with GAN-generated
data is sightly better. In fact, although the images generated
from DCGAN are visually imperfect (see Fig. 3), they still
possess similar regularization ability as the real images.
Comparison with the two competing methods. We
compare the LSRO method with the “All in one” and
“Pseudo label” methods implied in and , respectively. The experimental results on Market-1501 are
summarized in Table 2.
We ﬁrst observe that both strategies yield improvement
over the baseline. The “All in one” method treats all the
unlabeled samples as a new class, which forces the network
to make “careful” predictions for the existing K classes.
The “Pseudo label” method gradually labels the new data,
and thus introduces more variance to the network.
Nevertheless, we ﬁnd that LSRO exceeds both strategies
by approximately +1% ∼+2%. We speculate the reason
is that the “All in one” method makes a coarse label estimation, while the “Pseudo label” originally assumes that all the
unlabeled data belongs to the existing classes which is
not true in person re-ID. While these two methods still use
the one-hot label distribution, the LSRO method makes a
less stronger assumption (label smoothing) towards the labels of the GAN images. These reasons may explain why
LSRO has a superior performance.
Figure 6. Sample retrieval results on DukeMTMC-reID using the
proposed method. The images in the ﬁrst column are the query images. The retrieved images are sorted according to the similarity
scores from left to right. The correct matches are in the blue rectangles, and the false matching images are in the red rectangles.
DukeMTMC-reID is challenging because it contains pedestrians
with occlusions and similar appearance.
Comparison with the state-of-the-art methods. We
compare our method with the state-of-the-art methods on
Market-1501 and CUHK03, listed in Table 1 and Table 5,
respectively. On Market-1501, we achieve rank-1 accuracy = 78.06%, mAP = 56.23% when using the single
query mode, which is the best result compared to the published papers, and the second best among all the available
results including ArXiv papers. On CUHK03, we arrive at
rank-1 accuracy = 73.1%, mAP = 77.4% which is also
very competitive. The previous best result is produced by
combining the identiﬁcation and veriﬁcation losses .
We further investigate whether the LSRO could work on this
model. We ﬁne-tuned the publicly available model in 
with LSRO and achieve state-of-the-art results rank-1 accuracy = 83.97%, mAP = 66.07% on Market-1501. On
CUHK03, we also observe a state-of-the art performance
rank-1 accuracy = 84.6%, mAP = 87.4%. We, therefore,
show that the LSRO method is complementary to previous
methods due to the regularization of the GAN data.
5.4. Fine-grained Recognition
Fine-grained recognition also faces the problem of a lack
of training data and annotations. To further test the effectiveness of our method, we provide results on the CUB-200-
2011 dataset . This dataset contains 200 bird classes
with 29.97 training images per class on average. Bounding
boxes are used in both training and testing. We do not use
part annotations. In our implementation, the ResNet baseline has a recognition accuracy of 82.6%, which is slightly
higher than the 82.3% reported in . This is the baseline
annotation
Zhang et al. 
Zhang et al. 
Liu et al. 
Wang et al. 
Basel. 
Basel.+LSRO
Basel.+LSRO
2×ResNet-50
Table 6. We show the recognition accuracy (%) on CUB-200-2011.
The proposed method has a 0.6% improvement over the competitive baseline. The two-model ensemble shows a competitive result.
we will compare our method with.
Using the same pipeline in Fig. 1, we train DCGAN on
the 5,994 images in the training set, and then we combine
the real images with the generated images (see Fig. 5) to
train the CNN. During testing, we adopt the standard 10crop testing , which uses 256 × 256 images as input
and the averaged prediction as the classiﬁcation result. As
shown in Table 6, the strong baseline outperforms some recent methods, and the proposed method further yields an
improvement of +0.6% (from 82.6% to 83.2%). We also
combine the two models generated by our method with different initializations to form an ensemble. This leads to a
84.4% recognition accuracy. In , Liu et al. report a
85.5% accuracy with a ﬁve-model ensemble using parts and
a global scene. We do not include this result because extra
annotations are used. We focus on the regularization ability
of the GAN, but not on producing a state-of-the-art result.
6. Conclusion
In this paper, we propose an “in vitro” usage of the GANs
for representation learning, i.e., person re-identiﬁcation.
Using a baseline DCGAN model , we show that the imperfect GAN images effectively demonstrate their regularization ability when trained with a ResNet baseline model.
Through the proposed LSRO method, we mix the unlabeled
GAN images with the labeled real training images for simultaneous semi-supervised learning. Albeit simple, we
demonstrate consistent performance improvement over the
re-ID and ﬁne-grained recognition baseline systems, which
sheds light on the practical use of GAN-generated data.
In the future, we will continue to investigate on whether
GAN images of better visual quality yield superior results
when integrated into supervised learning. This paper provides some baseline evaluations using the imperfect GAN
images and the future investigation would be intriguing.
Acknowledgements.
Cooperative
(www.d2dcrc.com.au),
Award and NVIDIA Corporation with the donation of
TITAN X (Pascal) GPU.