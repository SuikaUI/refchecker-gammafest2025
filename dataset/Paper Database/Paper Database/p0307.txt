Cross-Entropy Method
Dirk P. Kroese
School of Mathematics and Physics
The University of Queensland
Brisbane 4072, Australia
 
Abstract: The cross-entropy method is a recent versatile Monte Carlo technique.
This article provides a brief introduction to the cross-entropy method and discusses
how it can be used for rare-event probability estimation and for solving combinatorial,
continuous, constrained and noisy optimization problems.
A comprehensive list of
references on cross-entropy methods and applications is included.
Keywords: cross-entropy, Kullback-Leibler divergence, rare events, importance sampling, stochastic search.
The cross-entropy (CE) method is a recent generic Monte Carlo technique
for solving complicated simulation and optimization problems. The approach
was introduced by R.Y. Rubinstein in , extending his earlier work on
variance minimization methods for rare-event probability estimation .
The CE method can be applied to two types of problem:
1. Estimation: Estimate ℓ= E[H(X)], where X is a random variable or
vector taking values in some set X and H is function on X . An important
special case is the estimation of a probability ℓ= P(S(X) ⩾γ), where S
is another function on X .
2. Optimization: Optimize (that is, maximize or minimize) S(x) over all
x ∈X , where S is some objective function on X . S can be either a
known or a noisy function. In the latter case the objective function needs
to be estimated, e.g., via simulation.
In the estimation setting, the CE method can be viewed as an adaptive
importance sampling procedure that uses the cross-entropy or Kullback-Leibler
divergence as a measure of closeness between two sampling distributions, as
is explained further in Section 1. In the optimization setting, the optimization problem is ﬁrst translated into a rare-event estimation problem, and then
the CE method for estimation is used as an adaptive algorithm to locate the
optimum, as is explained further in Section 2.
An easy tutorial on the CE method is given in . A more comprehensive
treatment can be found in ; see also [46, Chapter 8].
The CE method
homepage can be found at www.cemethod.org .
The CE method has been successfully applied to a diverse range of estimation and optimization problems, including buﬀer allocation , queueing
models of telecommunication systems , optimal control of HIV/AIDS
spread , signal detection , combinatorial auctions , DNA sequence
alignment , scheduling and vehicle routing , neural and reinforcement learning , project management ,
rare-event simulation with light- and heavy-tail distributions ,
clustering analysis . Applications to classical combinatorial optimization problems including the max-cut, traveling salesman, and Hamiltonian cycle
problems are given in . Various CE estimation and noisy optimization problems for reliability systems and network design can be found in
 . Parallel implementations of the CE method are
discussed in , and recent generalizations and advances are explored in
The Cross-Entropy Method for Estimation
Consider the estimation of
ℓ= Ef[H(X)] =
H(x) f(x) dx ,
where H is the sample performance function and f is the probability density of
the random variable (vector) X. (For notational convenience it is assumed that
X is a continuous random variable; if X is a discrete random variable, simply
replace the integral in (1) by a sum.) Let g be another probability density such
that for all x, g(x) = 0 implies that H(x) f(x) = 0. Using the probability
density g, we can represent ℓas
g(x) g(x) dx = Eg
Consequently, if X1, . . . , XN are independent random vectors, each with probability density g, then
H(Xk) f(Xk)
is an unbiased estimator of ℓ. Such an estimator is called an importance sampling estimator. The optimal importance sampling probability density is given
by g∗(x) ∝|H(x)|f(x) (see, e.g., [46, page 132]), which in general is diﬃcult
to obtain. The idea of the CE method is to choose the importance sampling
density g in a speciﬁed class of densities such that the cross-entropy or Kullback-
Leibler divergence between the optimal importance sampling density g∗and g
is minimal. The Kullback-Leibler divergence between two probability densities
g and h is given by
D(g, h) = Eg
g(x) ln g(x)
g(x) ln g(x) dx −
g(x) ln h(x) dx .
In most cases of interest the sample performance function H is non-negative,
and the “nominal” probability density f is parameterized by a ﬁnite-dimensional
vector u; that is, f(x) = f(x; u). It is then customary to choose the importance
sampling probability density g in the same family of probability densities; thus,
g(x) = f(x; v) for some reference parameter v. The CE minimization procedure
then reduces to ﬁnding an optimal reference parameter vector, v∗say, by crossentropy minimization. This v∗turns out to be the solution to the maximization
problem maxv
H(x)f(x; u) ln f(x; v)dx, which in turn can be estimated via
simulation by solving with respect to v, the stochastic counterpart program
H(Xk) f(Xk; u)
f(Xk; w) ln f(Xk; v) ,
where X1, . . . , XN is a random sample from f(·; w), for any reference parameter
w. The maximization (5) can often be solved analytically, in particular when the
class of sampling distributions forms an exponential family; see, for example, [46,
pages 319–320]. Indeed, analytical updating formulas can be found whenever
explicit expressions for the maximal likelihood estimators of the parameters can
be found, cf. [15, page 36].
Often ℓ= P(S(X) ⩾γ), for some performance function S and level γ, in
which case H(x) takes the form of an indicator function: H(x) = I{S(X)⩾γ};
that is, H(x) = 1 if S(x) ⩾γ, and 0 otherwise. A complication in solving
(5) occurs when ℓis a rare-event probability; that is, a very small probability
(say less than 10−5).
Then, for moderate sample size N most or all of the
values H(Xk) in (5) are zero, and the maximization problem becomes useless.
In that case a multi-level CE procedure is used, where a sequence of reference
parameters and levels is constructed with the goal that the ﬁrst converges to
v∗and the second to γ. This leads to the following algorithm; see, e.g., [46,
page 238].
Algorithm 1.1 (CE Algorithm for Rare-Event Estimation)
1. Deﬁne bv0 = u. Let N e = ⌈ϱN⌉. Set t = 1 (iteration counter).
2. Generate a random sample X1, . . . , XN according to the probability density f(·; bvt−1). Calculate the performances S(Xi) for all i, and order them
from smallest to largest, S(1) ⩽. . . ⩽S(N). Let bγt be the sample (1 −ϱ)quantile of performances; that is, bγt = S(N−Ne+1). If bγt > γ, reset bγt to
3. Use the same sample X1, . . . , XN to solve the stochastic program (5),
with w = bvt−1. Denote the solution by bvt.
4. If bγt < γ, set t = t + 1 and reiterate from Step 2; otherwise, proceed with
5. Let T be the ﬁnal iteration counter. Generate a sample X1, . . . , XN1 according to the probability density f(·; bvT ) and estimate ℓvia importance
sampling, as in (3).
Apart from specifying the family of sampling probability densities, the initial
vector bv0, the sample size N and the rarity parameter ϱ (typically between
0.01 and 0.1), the algorithm is completely self-tuning. The sample size N for
determining a good reference parameter can usually be chosen much smaller
than the sample size N1 for the ﬁnal importance sampling estimation, say N =
1000 versus N1 = 100, 000. Under certain technical conditions the deterministic
version of Algorithm 1.1 is guaranteed to terminate (reach level γ) provided that
ϱ is chosen small enough; see Section 3.5 of .
The Cross-Entropy Method for Optimization
Let X be an arbitrary set of states and let S be a real-valued performance
function on X .
Suppose we wish to ﬁnd the maximum of S over X , and
the corresponding state x∗at which this maximum is attained (assuming for
simplicity that there is only one such state). Denote the maximum by γ∗, we
S(x∗) = γ∗= max
x∈X S(x) .
This setting includes many types of optimization problems: discrete (combinatorial), continuous, mixed, and constrained problems. Moreover, if one is
interested in minimizing rather than maximizing S, one can simply maximize
Now associate with the above problem the estimation of the probability
ℓ= P(S(X) ⩾γ), where X has some probability density f(x; u) on X (for
example corresponding to the uniform distribution on X ) and γ is some level.
If γ is chosen close to the unknown γ∗, then ℓis, typically, a rare-event probability, and the CE approach of Section 1 can be used to ﬁnd an importance
sampling distribution close to the theoretically optimal importance sampling
density, which concentrates all its mass on the point x∗. Sampling from such
a distribution thus produces optimal or near-optimal states. A main diﬀerence
with the CE method for rare-event simulation is that in the optimization setting
the ﬁnal level γ = γ∗is not known in advance. The CE method for optimization
produces a sequence of levels {bγt} and reference parameters {bvt} such that the
former tends to the optimal γ∗and the latter to the optimal reference vector
v∗corresponding to the point mass at x∗; see, e.g., [46, page 251].
Algorithm 2.1 (CE Algorithm for Optimization)
1. Choose an initial parameter vector bv0. Let N e = ⌈ϱN⌉. Set t = 1 (level
2. Generate a sample X1, . . . , XN from the probability density f(·; bvt−1).
Calculate the performances S(Xi) for all i, and order them from smallest
to largest, S(1) ⩽. . . ⩽S(N). Let bγt be the sample (1 −ϱ)-quantile of
performances; that is, bγt = S(N−Ne+1).
3. Use the same sample X1, . . . , XN and solve the stochastic program
I{S(Xk)⩾bγt} ln f(Xk; v) .
Denote the solution by bvt.
4. If the stopping criterion is met, stop; otherwise, set t = t + 1, and return
to Step 2.
To run the algorithm, one needs to provide the class of sampling probability
densities, the initial vector bv0, the sample size N, the rarity parameter ϱ, and
the stopping criterion. Any CE algorithm for optimization involves thus the
following two main iterative phases:
1. Generate a random sample of objects in the search space X (trajectories, vectors, etc.) according to a speciﬁed probability distribution.
2. Update the parameters of that distribution, based on the N e best performing samples (the so-called elite samples), using cross-entropy minimization.
Apart from the fact that Step 3 in Algorithm 1.1 is missing in Algorithm 2.1,
another main diﬀerence between the two algorithms is that the likelihood ratio
term f(Xk; u)/f(Xk; bvt−1) in (5) is missing in (7).
Often a smoothed updating rule is used, in which the parameter vector bvt
is taken as
bvt = α evt + (1 −α) bvt−1,
where evt is the solution to (7) and 0 ⩽α ⩽1 is a smoothing parameter. Many
other modiﬁcations can be found in and in the list of references.
When there are two or more optimal solutions the CE algorithm typically “ﬂuctuates” between the solutions before focusing in on one of the solutions. The
eﬀect that smoothing has on convergence is discussed in detail in . In particular, it is shown that with appropriate smoothing the CE method converges
and ﬁnds the optimal solution with probability arbitrarily close to 1. Necessary
conditions and suﬃcient conditions under which the optimal solution is generated eventually with probability 1 are also given. Other convergence results,
including a proof of convergence along the lines of convergence for simulated
annealing can be found in .
Combinatorial Optimization
When the state space X is ﬁnite, the optimization problem (6) is often referred to as a discrete or combinatorial optimization problem. For example,
X could be the space of combinatorial objects such as binary vectors, trees,
paths through graphs, etc. To apply the CE method, one needs to specify ﬁrst
a convenient parameterized random mechanism to generate objects in X . For
example, when X is the set of binary vectors of length n, an easy generation
mechanism is to draw each component independently from a Bernoulli distribution; that is, X = (X1, . . . , Xn) ∼Ber(p), where p = (p1, . . . , pn). Given an
elite sample set E , the updating formula is then [15, page 56]
i = 1, . . . , n .
That is, the updated success probability for the i-th component is mean of the
i-th components of the vectors in the elite set.
A possible stopping rule for combinatorial optimization problems is to stop
when the overall best objective value does not change over a number of iterations.
Alternatively, one could stop when the sampling distribution has
“degenerated” enough. In particular, in the Bernoulli case (9) one could stop
when all {pi} are less than some distance ε away from either 0 or 1.
Continuous Optimization
It is also possible to apply the CE algorithm to continuous optimization problem; in particular, when X = Rn. The sampling distribution on Rn can be
quite arbitrary, and does not need to be related to the function that is being
optimized. However, the generation of a random vector X = (X1, . . . , Xn) ∈Rn
is usually established by drawing the coordinates independently from some 2parameter distribution. In most applications a normal (Gaussian) distribution
is employed for each component. Thus, the sampling distribution for X is characterized by a vector of means µ and a vector of standard deviations σ. At
each iteration of the CE algorithm these parameter vectors are updated simply
as the vectors of sample means and sample standard deviations of the elements
in the elite set; see, for example, . During the course of the algorithm, the
sequence of mean vectors ideally tends to the maximizer x∗, while the vector
of standard deviations tend to the zero vector. In short, one should obtain a
degenerated probability density with all mass concentrated in the vicinity of the
point x∗. A possible stopping criterion is to stop when all standard deviations
are smaller than some ε.
Constrained Optimization
Constrained optimization problems can be put in the framework (6) by taking
X a (non-linear) region deﬁned by some system of inequalities:
i = 1, . . . , L .
To solve the program (6) with constraints (10), two approaches can be adopted.
The ﬁrst approach uses acceptance–rejection: generate a random vector X from,
for example, a multivariate normal distribution with independent components,
and accept or reject it depending on whether the sample falls in X or not. Alternatively, one could sample directly from a truncated distribution (for example,
a truncated normal distribution) or combine such a method with acceptancerejection. Once a ﬁxed number of such vectors has been accepted, the parameters of the normal distribution can be updated in exactly the same way as in the
unconstrained case — simply via the sample mean and standard deviation of
the elite samples. A drawback of this method is that a large number of samples
could be rejected before a feasible sample is found.
The second approach is the penalty approach. Here the idea is to modify
the objective function as follows:
eS(x) = S(x) +
where the {Pi} are penalty functions. Speciﬁcally, the i-th penalty function Pi
(corresponding to the i-th constraint) is deﬁned as
Pi(x) = Hi max(Gi(x), 0)
and Hi > 0 measures the importance (cost) of the i-th penalty.
Thus, by reducing the constrained problem ((6) and (10)) to an unconstrained one ((6) with eS instead of S), one can again apply Algorithm 2.1. Further details on constrained multi-extremal optimization with the CE method
may be found in .
Noisy Optimization
Noisy (or stochastic) optimization problems — in which the objective function
is corrupted with noise — arise in many contexts, for example, in stochastic
scheduling and stochastic shortest/longest path problems, and simulation-based
optimization . The CE method can be easily modiﬁed to deal with noisy optimization problems. Consider the maximization problem (6) and assume that
the performance function is noisy. In particular, suppose that S(x) = EbS(x)
is not available, but that a sample value bS(x) (unbiased estimate of EbS(x)) is
available, for example via simulation. The principal modiﬁcation of the Algorithm 2.1 is to replace S(x) by bS(x). In addition, one may need to increase the
sample size in order to reduce the eﬀect of the noise. Although various applications indicate the usefulness of the CE approach for noisy optimization (see, for
example, , little is still known regarding theoretical convergence
results in the noisy case — Spall [50, Section 2.4] discusses various divergence
results for general types of stochastic methods. A possible stopping criterion
is to stop when the sampling distribution has degenerated enough. Another
possibility is to stop the stochastic process when {bγt} has reached stationarity;
see for example [45, page 207].