Machine Learning, 40, 159–196, 2000
c⃝2000 Kluwer Academic Publishers. Printed in The Netherlands.
MultiBoosting: A Technique for Combining
Boosting and Wagging
GEOFFREY I. WEBB
 
School of Computing and Mathematics, Deakin University, Geelong, Vic, 3217, Australia
Editor: Robert Schapire
MultiBoosting is an extension to the highly successful AdaBoost technique for forming decision
committees. MultiBoosting can be viewed as combining AdaBoost with wagging. It is able to harness both
AdaBoost’s high bias and variance reduction with wagging’s superior variance reduction. Using C4.5 as the base
learning algorithm, MultiBoosting is demonstrated to produce decision committees with lower error than either
AdaBoost or wagging signiﬁcantly more often than the reverse over a large representative cross-section of UCI
data sets. It offers the further advantage over AdaBoost of suiting parallel execution.
boosting, bagging, wagging, aggregation, decision committee, decision tree
Introduction
Decision committee learning has demonstrated spectacular success in reducing classiﬁcation error from learned classiﬁers. These techniques develop a classiﬁer in the form of a
committee of subsidiary classiﬁers. The committee members are applied to a classiﬁcation task and their individual outputs combined to create a single classiﬁcation from the
committee as a whole. This combination of outputs is often performed by majority vote.
Examples of these techniques include classiﬁcation ensembles formed by stochastic search
 , bagging , AdaBoost , Nock and Gascuel’s decision committees, averaged decision trees , and stacked generalization .
Two decision committee learning approaches, AdaBoost and bagging, have received
extensive attention. There is a large body of evidence that AdaBoost, applied to standard
decision tree induction systems, provides a powerful off-the-shelf general-purpose learning
algorithm. Indeed, to date there is good reason to identify AdaBoost with standard decision
tree learners as the off-the-shelf method-of-choice for a learning task where there is no
prior opportunity to evaluate the relative effectiveness of alternative approaches, there is
no a priori knowledge available about the domain, and the primary goal of learning is
to develop a classiﬁer with the lowest possible error suggest that AdaBoost and bagging have quite different
operational proﬁles. In general, it appears that bagging is more consistent, increasing the
error of the base learner less frequently than does AdaBoost. However, AdaBoost appears to
have greater average effect, leading to substantially larger error reductions than bagging on
average. AdaBoost tends to reduce both the bias and variance terms of error while bagging
tends to reduce the variance term only . Another
notable feature of both AdaBoost and bagging is that, on average, error keeps reducing as
committee size is increased, but that the marginal error reduction associated with each additional committee member tends to decrease. Each additional member, on average, has less
impact on a committee’s prediction error than any one of its predecessors . If we assume that both these algorithms are effective through different mechanisms, it
is plausible that a combination of the two will produce even greater effect. It has been noted
that increasing the disagreement in the predictions of committee members without affecting
their individual error rates can increase the accuracy of an ensemble1 . Using multiple approaches to generating committee members should increase diversity in the committee membership which might be expected to increase disagreement
between predictions. If this can be done without greatly increasing the error in the individual
predictions it can be expected to decrease error in the resulting committee’s predictions.
As the earlier members of each type of committee have greatest effect, there may be value
insacriﬁcingthelattermembersofonetypeofcommitteeformemberswiththeeffectiveness
of the initial members of the other type. MultiBoosting is a decision committee technique
that combines AdaBoost with wagging, a variant of bagging that is better suited to the task
than direct bagging. Experimental results suggest that it is in general more effective at error
reduction than either of its constituent methods.
This paper describes AdaBoost, bagging, wagging, and the new MultiBoost algorithm.
The effects of these four algorithms on the error, bias and variance of learned classiﬁers are
explored in a number of experiments. These experiments conﬁrm that AdaBoost reduces
both bias and variance while bagging and wagging have little effect on bias and greater effect
on variance. MultiBoost is shown to achieve most of AdaBoost’s superior bias reduction
coupled with most of bagging’s superior variance reduction. This results, in general, in
lower prediction error than any of AdaBoost, bagging, or wagging when applied to the C4.5
learning algorithm on a wide cross-section of typical machine learning tasks.
AdaBoost and bagging
Before describing the new MultiBoost algorithm, it is desirable to outline its antecedent
algorithms, AdaBoost, bagging, and wagging. All of these algorithms use a base learning
algorithm that forms a single classiﬁer from a training set of examples. This base classiﬁer is
provided with a sequence of training sets that the committee learning algorithm synthesizes
from the original training set. The resulting classiﬁers become the constituent members of
the decision committee.
MULTIBOOSTING
Bagging takes as input a base classiﬁcation learning algorithm L and
training set T , and returns a committee of classiﬁers C∗. T is a vector of n class–description
pairs. Each pair (yi, xi) associates class yi ∈Y with description xi ∈X. L(T ) →(C(X) →
Y) is a function from training sets to classiﬁers, which are in turn functions from descriptions
to classes. A committee C∗of t classiﬁers is formed by applying L to each element of a
vector of derived training sets, T ′
2, . . . , T ′
t . Each resulting classiﬁer Ci is added to C∗.
i is a bootstrap sample from T . For a training set T containing n cases, a bootstrap
sample is formed by uniform probability random selection from T with replacement, n
times. This will create a training set with the same number of cases as the original, but
some cases from the original may be represented more than once while others may not be
represented at all. The expected frequency with which cases from T are represented in a
single bootstrap sample T ′ is described by the discrete Poisson distribution.
To classify a new case with description x, each classiﬁer Ci from C∗is applied to x:
resulting in classiﬁcations C1(x), C2(x), . . . , Ct(x), where each classiﬁer returns a classiﬁcation yi ∈Y. The committee’s classiﬁcation C∗(x) is the class that obtains the most votes
from the committee members when applied to x
C∗(x) = arg max
1(Ci(x) = y)
where 1(·) is the indicator function.
Wagging is variant of bagging, that requires a base learning
algorithm that can utilize training cases with differing weights. Rather than using random
bootstrap samples to form the successive training sets, wagging assigns random weights
to the cases in each training set. Bauer and Kohavi’s original formulation of
wagging used Gaussian noise to vary the instance weights. However, this can lead to some
instance weights being reduced to zero, effectively removing them from the training set.
Instead, following a suggestion from Quinlan the new
technique uses the continuous Poisson distribution2 to assign random instance weights. As
the assignment of instance weights by bagging can be modeled by the discrete Poisson
distribution, use of the continuous Poisson distribution can be viewed as assigning instance
weights using an equivalent distribution to bagging, but over a continuous rather than
discrete space.
Individual random instance weights (approximately) conforming to the continuous Poisson distribution are calculated by the following formula:
Poisson( ) = −log
µRandom(1, . . . , 999)
where Random(min . . . max) returns a random integer value between min and max inclusive. After values are assigned using this random process, the vector of weights is always
standardized to sum to n, for consistency with the implementation of AdaBoost that is used.
AdaBoost, like bagging, forms a committee of classiﬁers by applying a single base
learning algorithm to successive derived training sets formed by sampling from the base
training set. However, AdaBoost differs from bagging by associating a weight with each
example in the training set. When forming the ﬁrst derived training set T1, each weight is
initialized to 1
n . In general, the weight wi, j for the ith item on the jth training set, Tj is a
function of the performance of previous classiﬁers (C1, . . . , C j−1) on the previous derived
training sets (T1, . . . , Tj−1). The AdaBoost algorithm may use this weight during sampling,
forming a bootstrap sample from a distribution over the training data deﬁned by the weights.
Alternatively, all training examples may be used to learn every committee member, with
the weights passed into the base learning algorithm, causing it to pay differential attention
to different objects. Quinlan obtained better results using this reweighting technique
than Freund and Schapire obtained using resampling in another set of experiments,
suggesting that reweighting is more effective than resampling. In consequence, reweighting
has been used in the current research.
A variant of Freund and Schapire’s AdaBoost algorithm is presented in
Table 1. The algorithm presented is Bauer and Kohavi’s variant of the original
AdaBoost.M1. This variant is chosen because—
• It uses a one step weight update process that is less subject to numeric underﬂow than
the original two step process (step 14).
• It prevents numeric underﬂow (step 15).
• Itcontinuesproducingmorecommitteemembersbeyondthepointwhere ϵ ≥0.5(step5).
This measure is claimed to improve predictive accuracy .
The AdaBoost algorithm.
S, a sequence of m examples ⟨(x1, y1), . . . , (xm, ym)⟩with labels yi ∈Y = {1, . . . , k}.
base learning algorithm BaseLearn.
integer T specifying the number of iterations.
S′ = S with all instance weights set to 1.
For t = 1 to T {
Ct = BaseLearn(S′).
x j ∈S′:Ct (x j )̸=y j weight(x j)
[the weighted error on the training set]
if ϵt > 0.5 then
set S′ to a bootstrap sample from S with weight 1 for each instance
goto step 3.
if ϵt = 0 then
set βt to 10−10
set S′ to a bootstrap sample from S with weight 1 for each instance.
otherwise,
For each x j ∈S′,
divide weight(x j) by 2ϵt if Ct(x j) ̸= y j and 2(1 −ϵt) otherwise.
if weight(x j) < 10−8, set weight(x j) to 10−8.
Output the ﬁnal classiﬁer:
C∗(x) = argmax
t:Ct (x)=y
MULTIBOOSTING
Note, however, a minor change. Whereas Bauer & Kohavi halt induction and treat
Ct as having inﬁnite weight if ϵt = 0, the current algorithm sets βt to a very small value
(10−10) in this circumstance and resamples from the training data to restart the boosting
process. This allows the possibility that more than one committee member with zero error will be derived, enabling these to vote against each other and giving other committee
members a casting vote in case of a draw between committee members with zero resubstitution error. This modiﬁcation is made to ensure that in the experiments that are performed,
the new MultiBoost algorithm does not obtain an advantage through always employing its
full quota of constituent classiﬁers while AdaBoost occasionally forgoes some members
due to reaching zero resubstitution error. On most of the data sets used in this research
this modiﬁcation to the algorithm does not affect the predictive accuracy of the resulting
committee. For the few data sets on which this does result in a difference in accuracy, the
change improves performance.
Notealso,whereastheinstanceweightsinAdaBoostarenormallytreatedasadistribution,
summing to 1, for use with the base learning algorithm, C4.5, in this implementation the
weights sum to n.
Bias and variance
A number of recent investigations of decision committees have analyzed error performance
in terms of bias and variance. The decomposition of a learner’s error into bias and variance terms originates from analyses of learning models with numeric outputs . The squared bias is a measure of the contribution to error of the central tendency or most frequent classiﬁcation of the learner when trained on
different training data. The variance is a measure of the contribution to error of deviations
from the central tendency. Bias and variance are evaluated with respect to a distribution of
training sets T , such as a distribution containing all possible training sets of a speciﬁed size
for a speciﬁed domain.
The bias/variance analysis is useful in focusing attention on two signiﬁcant factors that
govern the accuracy of classiﬁers learned by a learning system. If a learning system when
provided different training data develops classiﬁers that differ in their predictions, then the
extent of such variations provides a lower limit on the average error of those classiﬁers
when applied to any subsequent set of test data. If there is only one correct answer and the
answers provided by different classiﬁers differ then not all can be correct!
However, preventing such variance between the classiﬁers will not guarantee the elimination of prediction error. This error is also governed by both the degree to which the correct
answer for an object can differ from that for other objects with identical descriptions (irreducible error) and the accuracy of the learning bias—if the predictions of all classiﬁers are
identical but wrong, then error will also result.
Unfortunately, however, the deﬁnitions of these terms that are appropriate for numeric
regression, where a prediction is not just either right or wrong but there are varying degrees
of error, do not readily translate in an appropriate manner to contexts where the value to
be predicted is categorical. As a result, a number of different formulations of bias and variance have been proposed in the ﬁeld of classiﬁcation learning . Each of these deﬁnitions is able
to offer valuable insight into different aspects of a learner’s performance. Five bias/variance
related metrics are used in this research. Rather than distracting from the central theme of
this paper by presenting the detail of these metrics here, the formal deﬁnitions and details of
their estimation have been placed in the Appendices. Brief informal descriptions, are provided below. Appendix A provides detailed deﬁnitions of these metrics and discusses how
they relate to other bias/variance formulations. Appendix B details how they are estimated
in the current research.
Values for the metrics are estimated by ten runs of three-fold cross-validation. In each
three-fold cross-validation, the data are randomly divided into three sub-sets. Each case
in each subset is classiﬁed once by a classiﬁer learned by the algorithm under evaluation
from the other two subsets. Thus, each case is classiﬁed once for each three-fold crossvalidation, and hence ten times in all, each time by a classiﬁer formed by the learner from a
different random sample of data. Table 2 presents hypothetical classiﬁcations for three cases
across ten such three-fold cross-validation trials by the classiﬁers learned by a hypothetical
learner L. For each case, the resulting estimate for each of the bias and variance measures
is presented. It is assumed that irreducible error is zero in this domain. That is, any two
objects that share a common description also share the same class.
The central tendency is the most frequent classiﬁcation for an object. Error is the proportion of classiﬁcations that are incorrect.
Example of estimation of bias/variance measures for three cases from ten n-fold cross-validation trials.
Correct class
Prediction 1
Prediction 2
Prediction 3
Prediction 4
Prediction 5
Prediction 6
Prediction 7
Prediction 8
Prediction 9
Prediction 10
Central tendency
Contribution of bias to error
Contribution of variance to error
varianceKW x
MULTIBOOSTING
Contribution of bias to error is that portion of the total error across the distribution of test
sets that is due to errors committed by the central tendency of the learning algorithm. This
is the proportion of classiﬁcations that are both incorrect and equal to the central tendency.
Contribution of variance to error is that portion of the total error across the distribution of
test sets that is due to errors that are deviations from the central tendency of the learning
algorithm. This is the proportion of classiﬁcations that are both incorrect and not equal
to the central tendency. These two terms sum to total error. They are used to evaluate the
extent to which variations in the classiﬁers formed from one training set to another affect
the error of the learning algorithm. High contribution of bias to error indicates high error
resulting from the learning bias whereas high contribution of variance to error indicates
high error resulting from the algorithm’s responsiveness to variations between training
sets. These terms equate to Breiman’s bias and variance deﬁnitions except that
irreducible error is aggregated into the two terms. This is desirable in the current context
for two reasons. First, it is difﬁcult to usefully estimate irreducible error in ‘real-world’
learning tasks for which the true underlying class distribution is not known, because there
are usually too few cases at any given point in the instance space to reliably estimate the class
distribution at that point. Second, irreducible error is invariant across learning algorithms
for a single task and hence not a signiﬁcant factor in comparative evaluation.
The bias and variance measures biasKW
x and varianceKW x, developed by Kohavi and
Wolpert are used in this research as representative of deﬁnitions derived from those
for numeric regression. Following Kohavi and Wolpert , irreducible error is aggregated into biasKW
x. Like contributions of bias and variance to error, these two terms sum
Kong and Dietterich’s bias measure, biasKD, is also used. It represents the probability of error for the central classiﬁcation tendency of the learning algorithm, without
consideration of the probability of that bias being established. This can be viewed as a
measure of the quality of the central tendency, without consideration of the frequency or
strength of that central tendency, and is valuable when comparing the learning biases of
different algorithms.
Previous bias/variance analyses of decision committee performance
Breiman shows that bagging can be expected to reduce the variance of a classiﬁer.
This is because bagging can be viewed as a method for developing a classiﬁer that classiﬁes
using an estimate of the central tendency for the learner. While no satisfactory account has
beenofferedofwhysuchareductioninvarianceshouldnotbeaccompaniedbyacorresponding increase in error due to bias, a number of studies using both artiﬁcial and ‘real-world’
data, a number of different base learners, and a variety of different deﬁnitions of bias and
variance have all shown that, on the whole, bagging does tend to decrease variance without
unduly affecting bias .
In contrast (Friedman, Hastie, & Tibshirani, to appear) show that AdaBoost can be viewed
as a form of additive logistic regression. They state that boosting by reweighting “appears to
be a purely ‘bias’ reduction procedure, intended to increase the ﬂexibility of stable (highly
biased) weak learners.”
However, in empirical studies AdaBoost appears to reduce both bias and variance
 . No clear account of why
it should have this effect has yet gained wide acceptance.
Both Breiman and Schapire et al. present results with artiﬁcial data that
suggest that AdaBoost is more effective than bagging at reducing both bias and variance.
However, Bauer and Kohavi cast some doubt upon this result. In their studies with a
cross-section of data sets from the UCI repository , AdaBoost
is on-the-whole more effective at reducing bias than is bagging, but bagging is more effective
than AdaBoost at reducing variance.
Alternative accounts of decision committee performance
A number of alternative explanations have been advanced for the performance of bagging
and AdaBoost. Breiman argues that bagging can transform order correct learners
for an object x (those that have greater probability of forming a classiﬁer that selects
the most probable class for x than any other class) into optimal classiﬁers for that object.
However, he does not present any evidence that the base learners that are normally employed
with bagging will usually be order correct. He also makes the important observation that
instability (responsiveness to changes in the training data) is a prerequisite for bagging to
be effective. A committee of classiﬁers that all agree in all circumstances will give identical
performance to any of its members in isolation. A variance reduction process will have no
effect if there is no variance.
Freund and Schapire present a proof that AdaBoost can rapidly reduce resubstitution error (the error of the learned classiﬁer on the training data). However, reduction
of resubstitution error need not imply reduction of error outside the training data. They
argue that the application of structural risk minimization can produce such a link, but have
shown subsequently that the empirical evidence does not support
this theoretical supposition.
Instead, Schapire et al. argue that AdaBoost is effective because it boosts the
margins of the committee’s weighted classiﬁcations. The margin for a committee voting
to classify an object x is the difference between the sum of weighted votes for the correct
class for x and of the weighted votes for the class that obtains the greatest weighted vote
of the remaining classes. However, Breiman argues that this account is ﬂawed,
demonstrating algorithms that are more effective than AdaBoost at increasing margins but
less effective at reducing error.
Breiman argues that adaptive resampling is the key mechanism by which
AdaBoost reduces error. An adaptive resampling algorithm learns a committee by repeated
sampling from a training set. The probability of a given training case being included in
a given sample is increased if it has been misclassiﬁed by committee members learned
from previous samples. The success of an alternative adaptive resampling algorithm, arcx4, provides some support for this argument. However, Bauer and Kohavi show that
while AdaBoost is equally effective at reducing error using either reweighting or resampling,
arc-x4 is much less effective using reweighting than using resampling. This leaves room for
speculation that adaptive resampling does not account for all of AdaBoost’s performance.
MULTIBOOSTING
It has also been observed that for both bagging and AdaBoost, an increase in committee
size usually leads to a decrease in prediction error, but the relative impact of each successive
addition to a committee is ever diminishing. Most of the effect of each technique is obtained
by the ﬁrst few committee members .
MultiBoosting
The observations that bagging and AdaBoost appear to operate by different mechanisms,
have different effects, and both have greatest effect obtained from the ﬁrst few committee
members, suggest that it might be possible to obtain beneﬁt by combining the two. As the
mechanisms differ, their combination may out-perform either in isolation. Given that
• bagging mainly reduces variance, while AdaBoost reduces both bias and variance; and
• there is evidence that bagging is more effective than AdaBoost at reducing variance
 
their combination may be able to retain AdaBoost’s bias reduction while adding bagging’s
variance reduction to that already obtained by AdaBoost. As most of the effect of each
approach is obtained by the ﬁrst few committee members, it is possible that even quite
small committees formed by a combination of the approaches might obtain most of the
beneﬁt of each approach in isolation.
One method for combining the two might be to simply develop two sub-committees,
one containing members formed by boosting and the other containing members formed
by bagging. However, this raises the non-trivial question of how the votes of the two subcommittees should be combined. AdaBoost weights the votes of its committee members
while bagging does not, making the votes of members of each committee incommensurable.
Rather than pursuing this approach and tackling this question, this research explores bagging
a set of sub-committees each formed by application of AdaBoost. Instead of using bagging
per se, which would reduce the number of training examples available to form each subcommittee, wagging was employed. Maintaining all examples in
the training set was perceived as important, as Quinlan argues that AdaBoost using
reweighting obtains beneﬁt from access to all training examples during induction of every
committee member.
To summarize, MultiBoosting can be considered as wagging (which is in turn a variant of
bagging) committees formed by AdaBoost3. A decision has to be made as to how many subcommittees should be formed for a single run, and the size of those sub-committees. In the
absence of an a-priori reason for selecting any speciﬁc values for these factors, the current
implementation of MultiBoosting, MultiBoost, takes as an argument a single committee
size T , from which it by default sets the number of sub-committees and the size of those
sub-committees to
T . As both these values must be whole numbers, it is necessary round
off the result. The precise method for deriving the values is presented in Table 3. For ease
of implementation, this is achieved by setting a target ﬁnal sub-committee member index,
where each member of the ﬁnal committee is given an index, starting from one. This allows
Determining sub-committee termination indexes.
integer T specifying the number of iterations.
vector of integers Ii specifying the iteration at which each subcommittee i ≥1 should terminate.
For i = 1, . . . , n −1, Ii = ⌈i × T/n⌉.
For i = n, . . . , ∞, Ii = T .
the premature termination of boosting one sub-committee, due to too great or too low error,
to lead to an increase in the size of the next sub-committee. If the last sub-committee is
prematurely terminated, an additional sub-committee is added with a target of completing
the full complement of committee members. Should this additional sub-committee also
fail to reach this target, this process is repeated, adding further sub-committees until the
target total committee size is achieved. The resulting MultiBoost algorithm is presented in
In addition to the bias and variance reduction properties that this algorithm may inherit
from each of its constituent committee learning algorithms, MultiBoost has the potential
computational advantage over AdaBoost that the sub-committees may be learned in parallel,
although this would require a change to the handling of early termination of learning a subcommittee. The AdaBoost process is inherently sequential, minimizing the potential for
parallel computation. However, each classiﬁer learned with wagging is independent of the
rest, allowing parallel computation, a property that MultiBoost inherits at the sub-committee
Evaluation
To evaluate the relative efﬁcacy of AdaBoost, bagging and MultiBoost, they were applied
to a wide cross-section of data sets from the UCI repository .
Many different committee sizes have been studied in previous research. Relatively small
committee sizes are attractive from the perspective of exploring the beneﬁts that can be
obtained with relatively modest increases in computation. Further, as the most beneﬁt is
obtained from the ﬁrst few committee members, subsequent computation, while improving accuracy, provides relatively poor returns for investment of computational resources.
These reasons, and the relative low computational demands involved, support the study of
committees of size ten, and consequently each technique was evaluated with this committee
size. However, it is interesting to compare the performance of MultiBoost, which aggregates
a number of constituent boosted sub-committees, against boosted committees of the size
of those constituents. Also, it is credible that different committee learning techniques may
demonstrate different performance proﬁles at differing committee sizes. For these reasons,
committees of size 100 were also included in the study. A MultiBoost committee of size
100 is composed of ten sub-committees each comprising ten boosted committee members.
MULTIBOOSTING
The MultiBoost algorithm.
MultiBoost
S, a sequence of m labeled examples ⟨(x1, y1), . . . , (xm, ym)⟩with labels yi ∈Y.
base learning algorithm BaseLearn.
integer T specifying the number of iterations.
vector of integers Ii specifying the iteration at which each subcommittee i ≥1 should
terminate.
S′ = S with instance weights assigned to be 1.
set k = 1.
For t = 1 to T {
If Ik = t then
reset S′ to random weights drawn from the continuous Poisson distribution.
standardize S′ to sum to n.
increment k.
Ct = BaseLearn(S′).
x j ∈S′:Ct (x j )̸=y j weight(x j)
[the weighted error on the training set]
if ϵt > 0.5 then
reset S′ to random weights drawn from the continuous Poisson distribution.
standardize S′ to sum to n.
increment k.
go to Step 8.
otherwise if ϵt = 0 then
set βt to 10−10
reset S′ to random weights drawn from the continuous Poisson distribution.
standardize S′ to sum to n.
increment k.
otherwise,
For each x j ∈S′,
divide weight(x j) by 2ϵt if Ct(x j) ̸= y j and 2(1 −ϵt) otherwise.
if weight(x j) < 10−8, set weight(x j) to 10−8.
Output the ﬁnal classiﬁer:
C∗(x) = arg max
t:Ct (x)=y
As MultiBoost uses wagging, rather than bagging, committees formed by wagging using
the continuous Poisson distribution were also included. This is equivalent to MultiBoost
with sub-committees of size 1 and each committee member receiving an equal vote.
The base learning algorithm, C4.5, was also included for comparison.
Thirty-six representative data sets from the UCI repository were used in the experiments.
These data sets were restricted to ‘real world’ data with the exception of the waveform data
that was included due to its use in numerous previous committee learning experiments.
Table 5 presents the number of cases, the number of attributes by which each case is
described, and the number of classes for each data set. All data sets on which the algorithms
have been evaluated are presented herein.
Statistics employed
One motivation for determining comparative performance across a set of different learning
tasks is to assess whether one algorithm demonstrates a general advantage over another. It is
well known that no algorithm can hold a general advantage in terms of off-training-set error
over another across all possible learning tasks . However,
it is credible that ‘real-world’ learning tasks are not uniformly distributed across the set of
all possible tasks, and hence that a general advantage is possible for the set of ‘real-world’
tasks, or at least a speciﬁc sub-set thereof . If we take the
thirty-six data sets used in the experimentation as broadly representative of those in the
UCI repository (as I believe them to be) and hence at the very least as representative of
those commonly used in experimental machine learning research, it should be possible to use
statistical analysis to draw conclusions relating to the expected general relative performance
of the algorithms for at least this restricted class of learning tasks.
This leads, however, to the vexatious issue of what statistical analyses to use. The practice
of using t-tests to compare the performance of two algorithms on sequences of training and
tests sampled from a single data set has come under recent scrutiny . While such a test can provide an accurate evaluation of the probability
of obtaining the observed outcomes by chance, it has limited ability to predict relative
performance even on further training/test set samples from the same domain but from
outside the initial data set, let alone on other domains.
There is a further problem that with large numbers of algorithms and data sets, the
probability of type one error occurring climbs dramatically if individual hypothesis tests
are performed on many algorithm-pair-dataset-tuples.
To avoid these problems I do not perform signiﬁcance tests on individual data sets. Rather,
a single signiﬁcance test is performed for every pair of algorithms that are compared, a sign
test on the win/draw/loss record across all data sets. If the probability of obtaining the
observed outcomes by chance is sufﬁciently low (I use the common 0.05 critical level), I
conclude that the observed performance is indicative of a general underlying advantage to
one algorithm or the other with respect to type of learning task studied in this research.
In addition to this sign test, the following aggregate descriptive statistics are employed.
Mean error.
This is the mean of error across all datasets. This provides a very gross
indicationofrelativeperformance.Itisdebatablewhethererrorratesindifferentdomainsare
commensurable,andhencewhetheraveragingerrorratesacrossdomainsisverymeaningful.
MULTIBOOSTING
Description of data sets.
Balance-scale
Breast cancer Slov.
Breast cancer Wisc.
Credit (Aust.)
Discordant
Echocardiogram
Horse-colic
House-votes-84
Letter-recognition
Lymphography
New-thyroid
Pima diabetes
Primary tumor
Soybean large
Splice junction
Tic-tac-toe
Nonetheless, a low average error rate is indicative of a tendency toward low error rates for
individual domains.
Geometric mean error ratio.
A number of recent papers have used the mean ratio of error
rates across domains as a measure of relative performance . The ratio of error rates on a single domain for algorithms x and y, where E(x) and
E(y) denote the respective mean error of each algorithm, is E(x)/E(y). On the face of it,
this measure allows for the relative difﬁculty of error reduction in different domains. For
example, consider the case where two algorithms x and y achieve error rates of 0.10 and
0.20 for domain A and 0.35 and 0.20 for domain B, respectively. The ratios will be 0.50
and 1.75, giving a mean ratio of 1.125, indicating that, on average, algorithm x has error
12.5% greater than algorithm y. This may seem like a credible summary of their relative
performance. While x out-performed y on the ‘easier’ task, it was out-performed on the
more difﬁcult. However, if we consider the ratio of y to x, we get 0.20/0.10 = 2.0 and
0.20/0.35 = 0.571 giving a mean ratio of 1.286, indicating that, on average algorithm y
has error 28.6% greater than algorithm x! Clearly it is not desirable to conclude that two
algorithms out-perform each other. One solution is to use the geometric mean rather than
the arithmetic mean to aggregate the result over multiple domains. The geometric mean of
a set of values v1, v2, . . . , vn is
i=1 log(vi )
The geometric mean of a set of ratio values a1/b1, a2/b2, . . . , an/bn has the desirable
property that if it is greater than one then the geometric mean of b1/a1, b2/a2, . . . , bn/an
will be less than one, and vice versa. For the example of algorithms x and y on domains A
and B, above, the geometric mean of xi/yi is 0.935 while the geometric mean of yi/xi is
1.069, suggesting that it is x that enjoys the true advantage in terms of error reduction over
the two domains.
However, even this measure should be treated with considerable caution. For example,
increasing average accuracy should be as desirable as reducing average error. We should be
just as interested in algorithms that can, on average, provide relative increases in accuracy, as
we are in algorithms that can, on average, produce relative reductions in error. The accuracy
of an algorithm will equal 1 −error. Hence, to continue the previous example, algorithm x
and y’s accuracy for A is 0.90 and 0.80 and for B is 0.65 and 0.80. The accuracy ratios for
x over y are therefore 1.125 and 0.813 and for y over x, 0.889 and 1.231. The geometric
means of these two ratios are 0.956 and 1.046, respectively, exhibiting the desirable property
that one is greater than 1.0 while the other is below 1.0. However, it is x over y for which
the value is below 1.0. If we take the geometric mean of accuracy/error to indicate a general
tendency to increase or decrease accuracy/error, then we have the undesirable outcome,
when we take this result together with the earlier geometric mean of error ratios, that
algorithm x, in general, reduces both error and accuracy with respect to y. Such an outcome
is possible because error ratio favors an algorithm that performs well when error is low,
whereas accuracy ratio favors an algorithm that performs well when error is high.
MULTIBOOSTING
Despite these problems, the geometric mean of error ratios is provided as a summary
statistic. Like mean error, however, this statistic should be interpreted with caution. It should
be noted that while this statistic cannot be calculated if either algorithm achieves zero error
for any data set, this rarely occurs in machine learning research and did not occur in the
studies reported herein.
Geometric mean bias/variance ratio.
Geometric means of ratios between two algorithms’
bias or variance outcomes are also provided. It should be recalled that these statistics favor
strong performance at low values of the measure being averaged over strong performance
at high values. Like geometric mean error ratios, these statistics should be interpreted with
caution. These statistics cannot be calculated if an algorithm achieves a mean of zero for
the relevant metric on any single data set, but this condition did not arise in the current
Win/Draw/Loss record.
This is the major comparative descriptive statistic employed in
this paper. The three values are, respectively, the number of datasets for which algorithm a1
obtainedbetter,equal,orworseperformanceoutcomesthanalgorithma2 onagivenmeasure.
Sign tests can be applied to these summaries. These tests will indicate the probability of
obtaining the observed record of wins to losses, or more extreme, by chance. If the sign test
result is signiﬁcantly low then it is reasonable to conclude that it is unlikely that the outcome
was obtained by chance and hence that the record of wins to losses represents a systematic
underlying advantage to one of the algorithms with respect to the type of domains on which
they have been tested.
Error rates
Tables 6 and 7 provide comparative summaries of the error of each algorithm across all data
sets. These summary tables have the following format. In the following descriptions, row
indicates the mean error on a data set for the algorithm with which a row is labeled, while col
indicates the mean error for the algorithm with which the column is labeled. The ﬁrst row of
each summary table presents the mean error across all data sets. Rows labeled ˙r present
the geometric mean of the error ratio col/row. Rows labeled s present the win/draw/loss
statistic, where the ﬁrst value is the number of data sets for which col < row, the second
is the number for which col = row and the last is the number for which col > row. Rows
labeled p present the result of a two-tailed sign test on the win-loss record. This is the
probability of obtaining the observed record of wins to losses, or more extreme, if wins
and losses were equi-probable random events. Detailed breakdowns of error by data set are
provided in Appendix C.
At both committee sizes MultiBoost obtains the lowest mean error. At t = 10 bagging
comes next, at t = 100, AdaBoost obtains the second lowest mean error. Wagging has the
highest mean error of the four committee learning techniques at both committee sizes. All
of the comparative metrics rank the algorithms in the same order, with the minor exception,
that the geometric mean of error ratios favors AdaBoost over bagging at t = 10.
Comparison of error for t = 10.
MultiBoost
Mean, all data sets
Comparison of error for t = 100.
MultiBoost
Mean, all data sets
Considering speciﬁcally the relative performance of MultiBoost, it achieves a lower geometric mean ratio of error over that of C4.5 than any other algorithm4. The geometric mean
error ratios of MultiBoost over each other algorithm all favor MultiBoost. In comparison to
every other algorithm, MultiBoost achieves lower error for more data sets than the other algorithm achieves lower error. The frequency of this advantage is not statistically signiﬁcant
at t = 10, but is signiﬁcant against all algorithms at t = 100.
MULTIBOOSTING
MultiBoosting frequently beats each of AdaBoost and bagging. Does it, however, usually
obtain a mid-point between the two, or is it outperforming the better of the two? At t = 10
the error of MultiBoost is lower than the minimum of the error of AdaBoost and bagging
for 11 datasets and higher for 21. At t = 100 MultiBoost obtains lower error for 16 datasets
and higher error for 15 datasets. At both committee sizes MultiBoosting frequently obtains
lower error than either of AdaBoost or bagging.
It is notable that while AdaBoost usually greatly reduces the error of the base learner,
it occasionally increases the error, sometimes substantially. Of the datasets examined, at
t = 100 AdaBoost obtains higher error than C4.5 on seven datasets. Of these seven, Multi-
Boost reduces C4.5’s error on two (balance-scale and echocardiogram), maintains it on one
(iris), increases error but to a lesser degree than AdaBoost on three (adult, breast cancer
Slovenia and horse-colic) and increases the error of AdaBoost on one (lenses). These results
suggest that MultiBoosting moderates this aspect of AdaBoost’s operational proﬁle.
Wagging and bagging restarts in AdaBoost
A variant of Bauer and Kohavi’s variant of the original AdaBoost is used. It differs
from their variant in using wagging to reset weights when committee learning halts due
to too high or too low resubstitution error, rather than bagging. This change was made for
consistency with the MultiBoosting implementation used in the experiments. To evaluate
whether the use of wagging rather than bagging disadvantaged AdaBoost, a variant that
differed only by using bagging in these circumstances was also included in the crossvalidation trials. At t = 10, the two variants obtained different results for only four data
sets. For one of these bagging resulted in lower error, while for three wagging obtained
lower error. The mean error across all domains did not differ when measured to three
decimal places. The geometric mean error ratio of bagged over wagged was 1.002, slightly
favoring the wagged variant. At t = 100 the two variants differed on twelve data sets. For
four data sets, bagging resulted in lower error than wagging and for the remaining eight
wagging obtained the lower error. The mean error rates across all domains were identical
measured to three decimal places and the geometric mean error ratio was 1.000, suggesting
that neither approach enjoyed a measurable general advantage over the other. Substituting
the bagging variant for the wagging variant in the above analyses does not produce any
appreciable difference in the outcomes.
Bias/variance analysis of performance
One of the reasons for developing the MultiBoost approach was the evidence that bagging
was more effective than AdaBoost at reducing variance while the reverse was true with
respect to reducing bias. Given that each algorithm obtained the greatest effect from the
ﬁrst few committee members, combining the two might be expected to achieve most of the
variance reduction of bagging as well as most of the bias reduction of AdaBoost. To gain
insight into the extent to which this analysis can explain MultiBoost’s performance, the
ﬁve measures of these two factors discussed in Section 3 were also evaluated for all of the
Comparison of contribution of bias to error for t = 10.
MultiBoost
Mean, all data sets
Comparison of contribution of bias to error for t = 100.
MultiBoost
Mean, all data sets
experiments. Detailed breakdowns of the mean values for contribution of bias and variance
to error for committee sizes 10 and 100 are presented in Appendix C.
Tables 8 and 9 summarize the relative performance of the algorithms with respect to
contribution of bias to error at committee sizes 10 and 100, respectively. At t = 10, the
algorithms are ordered from lowest to highest on mean value across all data sets, AdaBoost,
MultiBoost, bagging, wagging, then C4.5. This order is repeated at t = 100 except that
wagging and C4.5 exchange places. All other statistics conform to the same ordering,
MULTIBOOSTING
except that wagging loses to C4.5 more often than it wins at t = 10 as well as at t = 100.
Both AdaBoost and MultiBoost achieve statistically signiﬁcant win/draw/loss records over
bagging, wagging, and C4.5. While AdaBoost achieves lower outcomes than MultiBoost
substantially more often than the reverse at t = 10, this advantage is not signiﬁcant at the
0.05 level, and is eroded to a narrow margin at t = 100.
Tables 10 and 11 summarize the relative performance of the algorithms with respect to
contribution of variance to error at committee sizes 10 and 100, respectively. At t = 10, the
Comparison of contribution of variance to error for t = 10.
MultiBoost
Mean, all data sets
Comparison of contribution of variance to error for t = 100.
MultiBoost
Mean, all data sets
order of the algorithms with respect to the mean across all data sets is bagging, wagging,
MultiBoost, AdaBoost, then C4.5. At t = 100, however, the margins have narrowed greatly,
and MultiBoost achieves the greatest reduction by a very narrow margin, the order being
otherwise unchanged.
Bagging outperforms all of the other algorithms on both geometric mean ratio and
win/draw/loss record, although the results on the latter are only signiﬁcant at the 0.05
level with respect to C4.5 and AdaBoost at t = 10. On both metrics, wagging outperforms
C4.5 and AdaBoost at both committee sizes and MultiBoost at t = 10, but is outperformed
on geometric mean error ratio by MultiBoost at t = 100. MultiBoost outperforms AdaBoost
on both measures at both committee sizes, the win/draw/loss record being signiﬁcant at the
0.05 level for t = 100.
It seems that at a committee size of 100, MultiBoost achieves most of AdaBoost’s mean
reduction in error due to bias and actually improves upon wagging’s mean reduction in error
due to variance.
Comparing the outcomes at t = 10 to those at t = 100 a surprising trend emerges. For
all committee algorithms, the average contribution of bias to error increases as the committee
size increases. As all of the committee approaches reduced average contribution of bias to
error at t = 10, the average effect of the additional 90 committee members is to undo some
of the bias reduction effect of the smaller committees. In contrast, all committee algorithms
demonstrate greater average reductions in contribution of variance to error as the committee
size increases. For all algorithms this reduction in variance outweighs the increase in bias,
resulting in an overall reduction in error.
These results are surprising. AdaBoost, for example, appears to perform bias reduction,
and appears to achieve greater beneﬁt the greater the committee size. Why then should larger
committees achieve less bias reduction than smaller committees? A number of possible
explanations present themselves. The ﬁrst is that this unexpected result is simply an artifact
of the use of contribution of bias to error rather than one of the alternative bias deﬁnitions.
However, Kohavi and Wolpert’s bias and variance measures also follow the same
pattern as the contribution of bias and variance to error, as shown by Tables 12 and 13 that
Summary, mean biasKW 2
MultiBoost
Mean, all data sets
Compare C4.5
Mean, all data sets
Compare C4.5
MULTIBOOSTING
Summary mean varianceKWx.
MultiBoost
Mean, all data sets
Compare C4.5
Mean, all data sets
Compare C4.5
Error for AdaBoost at t = 10, 50, 100, and 200.
Mean, all data sets
present the mean values of these terms across all data sets along with the geometric mean
and win/draw/loss records over C4.5 for each of the committee methods.
So it appears that the unexpected behaviour of contribution of bias to error as committee
size increases is not related to the new deﬁnitions of bias and variance employed. Another
possibility is that this is a chance artifact of the two committee sizes. It is, after all, very
dangerous to infer a trend from two points. To test this possibility, further cross-validation
trials were performed for AdaBoost at committee sizes 50 and 200. The same sequence of
cross-validation folds as employed for the previous experiments were used in these subsequent experiments. Tables 14–16 summarize, respectively, the relative error, contribution
of bias to error, and mean biasKW
x of AdaBoost at each of these committee sizes along with
the committee sizes previously examined.
The error shows a steady decrease as the committee size increases, but both measures of
bias show a slight but steady increase (although the geometric mean favors t = 200 over
Contribution of bias to error for AdaBoost at t = 10, 50, 100, and 200.
Mean, all data sets
Mean biasKW2
x for AdaBoost at t = 10, 50, 100, and 200.
Mean, all data sets
both t = 100 and t = 50, and t = 100 over t = 50). While the mean across all data sets
does not increase on either measure from t = 100 to t = 200, the smaller committee size
achieves a lower measure for more data sets than the larger committee size. The results
suggest that at small committee sizes there is a rapid decrease in the contribution of bias
to error as committee size increases (at t = 10 there is a substantial decrease over the
base learner) but at some point this trend reverses. It seems likely that the exact curve will
vary substantially from domain to domain, and there does not appear to be much point in
attempting to map it more precisely.
Another possible explanation is that as the committee size increases the central tendency
changes for the worse. One manner in which to evaluate this explanation is to consider
Kong and Dietterich’s deﬁnition of bias. As explained in Section 3, biasKD can be
considered as a measure of the quality of the central tendency, without consideration of the
MULTIBOOSTING
Summary of biasKD.
MultiBoost
Mean, all data sets
Compare C4.5
Mean, all data sets
Compare C4.5
frequency or strength of that central tendency. A summary of the algorithms’ performance
on this measure is presented in Table 17. As can be seen, the average quality of the central
tendency improves markedly from the base learner to committees of size 10. AdaBoost
demonstrates further improvement as the committee size increases, both in the mean across
all data sets and in the geometric mean over C4.5. This result supports the proposition that
the increase in AdaBoost’s contribution of bias to error as committee size increases is due
to a decrease in variance rather than a deterioration in the quality of the central tendency. It
appears that AdaBoost transforms the bias of the base learner in a manner that has positive
average effect, but that this transformation is achieved less frequently at lower committee
sizes than higher, resulting in greater variance for the smaller committees.
The effect of the increase in committee size on the quality of the central tendency of the
other committee algorithms is not so clear cut, however. For bagging, the mean across all
data sets increases minutely, as does the geometric mean over C4.5, but the win/draw/loss
record against C4.5 improves marginally. Wagging achieves the same mean across all data
sets at both committee sizes, but achieves a slight improvement in the two comparative
statistics against C4.5. MultiBoost experiences a small increase in the mean across all data
sets, a deterioration in the win/draw/loss record against C4.5, but an improvement in the
geometric mean against C4.5.
So, at least for AdaBoost, the effect appears to be real, and not an artifact of the measure
of bias that is employed or of the arbitrary choice of two speciﬁc committee sizes. A
possible explanation of this effect is that the committee induction algorithms perform a
transformation of bias from the base learning algorithm to the committee algorithm, but
that at smaller committee sizes there is considerable ﬂuctuation in the bias that is established.
Thisﬂuctuationmightbebetweenthebasealgorithm’sbiasandthebiasofalargecommittee,
or between different biases that tend to be associated with different committee sizes. As can
be seen from the contribution of bias to error at t = 200, when this new bias is established, on
average, it contributes less to error than the bias of the base algorithm. However, at smaller
committee sizes, different runs establish different biases, leading to greater variance. The
bias is less consistently achieved, and hence contributes less to error.
Comparison of MultiBoost t = 100 against AdaBoost t = 10.
Contrib. bias
varianceKWx
Comparison of MultiBoost t = 100 against AdaBoost t = 10
One of the motivations for studying the two committee sizes 10 and 100, was that this
would allow a comparison of the performance of MultiBoost against that of AdaBoost
with the latter producing committees of the size of the sub-committees employed within
the former. This enables direct comparison of the difference between the elements being
wagged within MultiBoost against the result of that wagging. Table 18 presents a summary of the comparative performance of the two algorithms at these respective committee
sizes. The win/draw/loss records denote MultiBoostt=100’s wins, draws and losses over
AdaBoostt=10. The geometric means relate to the ratios of MultiBoost’s performance over
AdaBoost’s. It is clear that the wagging of sub-committees within MultiBoost is very effective at reducing the error of the sub-committees formed by AdaBoost. However, both
the contribution of bias to error and mean biasKW
x suggest that MultiBoostt=100 slightly
degrades the performance of AdaBoostt=10 in this respect. It should be recalled, though, that
there is reason to believe that the score on these measures is reduced by an increase in variance for AdaBoost at this committee size. Certainly, on biasKD, which measures the error of
the central tendency irrespective of the frequency with which that tendency is established,
MultiBoostt=100 appears to be improving performance over that of AdaBoostt=10 with reasonable consistency (although it is interesting to note that the mean for AdaBoostt=10 is
0.001 less than that for MultiBoostt=100). MultiBoost is remarkably consistent at reducing
the effect of variance, both as measured by the contribution of variance to error and mean
varianceKW x.
On the optimal number of sub-committees
At the outset of this research, in view of the lack of theoretical grounds for favoring the effect
of either AdaBoost or wagging within MultiBoost, it was decided that the sub-committee
size should be selected so as to equally balance the two. It is interesting, however, to consider
other possible trade-offs between the two factors. The more sub-committees there are, the
more wagging is performed. The greater the sub-committee size, the greater the effect of
boosting. The two extreme options were selected, to give some insight into the trade-offs—
two sub-committees each containing 50 members (MB2 × 50) and 50 sub-committees each
containing two members (MB50 × 2). Tables 19–21 present, respectively, comparisons of
the error, contribution of bias to error, and contribution of variance to error of both of these
MultiBoost variants, against each other, the default MultiBoost, wagging, and AdaBoost
with committees of size 100.
MULTIBOOSTING
Summary of error for alternative sub-committee sizes.
MultiBoost
Mean, all data sets
MultiBoost
Summary of contribution of bias to error for alternative sub-committee sizes.
MultiBoost
Mean, all data sets
MultiBoost
MB50 × 2 achieves the lowest mean error across all data sets. However, the win/draw/loss
records show that MB50 × 2 achieves lower error for only two more data sets than MB2 × 50
achieves the lower error. When the geometric mean ratios against each other are compared,
MB2 × 50 comes out ahead. These mixed results suggest that neither approach enjoys a
clear general advantage over the other with respect to error reduction.
Summary of contribution of variance to error for alternative sub-committee sizes.
MultiBoost
Mean, all data sets
MultiBoost
Turning our attention to the contribution of bias and variance to error, there is some evidence that MB2 × 50 improves modestly upon AdaBoost on both these measures (although
the win/draw/loss records are very far from statistical signiﬁcance). MB50 × 2 appears to
enjoy a clear advantage over wagging with respect to both bias and variance reduction.
MB50 × 2 also appears to enjoy a clear advantage over MB2 × 50 with respect to variance.
Further, while the result is not statistically signiﬁcant, there is a suggestion of a small advantage to MB2 × 50 with respect to bias reduction. These results suggest that favoring
AdaBoost is appropriate when the contribution bias to error of the base learner is high, and
favoring bagging is appropriate when variance is the dominant contributor to error.
The application of bagging and wagging in MultiBoost
The original formulation of MultiBoost applied wagging rather than bagging to form the
sub-committees, even though the inspiration for the technique was derived from bagging.
Wagging was employed because AdaBoost appears to derive advantage from being able
to consider all of the training data and wagging enables the data to be perturbed without
completely removing any datum from consideration. However, the investigations reported
above have shown that bagging enjoys a clear advantage over wagging, a result supported
by Bauer and Kohavi . This suggests that MultiBoost might beneﬁt from modiﬁcation
to employ bagging rather than wagging of sub-committees, as was performed in a variant
of MultiBoost described by Zheng and Webb .
To test this hypothesis, MultiBoost was modiﬁed appropriately. Table 22 summarizes the
relative error of the two MultiBoost variants. At t = 10, wagging sub-committees enjoys a
clear advantage on all measures. However, for the larger sub-committees, this advantage is
MULTIBOOSTING
Relative error with wagging and bagging of sub-committees.
Mean, all data sets
Wagging subcmtys
Relative contribution of bias to error with wagging and bagging of sub-committees.
Mean, all data sets
Wagging subcmtys
Relative contribution of variance to error with wagging and bagging of sub-committees.
Mean, all data sets
Wagging subcmtys
eroded: the mean error across all domains for the two variants is identical, but the geometric
mean and win/draw/loss records both still favor wagging, although not signiﬁcantly.
Tables 23 and 24 present the relative performance of the variants with respect to contribution of bias and variance to error. On the former measure, the mean across all data sets
favors bagging, but the geometric mean ratios favor wagging at both committee sizes. Neither enjoys a signiﬁcant win/draw/loss record advantage over the other at either committee
size, suggesting that neither enjoys a general advantage with respect to bias. However, for
variance, wagging enjoys a clear advantage at t = 10, although this appears to be totally
eroded at the larger committee size.
In all, these results suggest that there may be a slight advantage to the use of wagging
within MultiBoost.
MultiBoost combines wagging, which is a variance reduction technique, with AdaBoost,
which performs both bias and variance reduction. This is achieved by wagging a set of
sub-committees of classiﬁers, each sub-committee formed by AdaBoost. When forming
decision committees using C4.5 as the base learning algorithm, MultiBoost is demonstrated
to produce committees with lower error than AdaBoost signiﬁcantly more often than the
reverse across a wide cross-section of UCI data sets. This improved predictive accuracy is
obtained with negligible increase in computational overheads. Indeed, MultiBoost offers a
potentialcomputationaladvantageoverAdaBoostinthatitisamenabletoparallelexecution.
Each sub-committee may be learned independently of the others.
To summarize the experimental results on learning committees of decision trees—
• MultiBoost achieves greater mean error reductions than any of AdaBoost, wagging, or
bagging decision trees at both committee sizes that were investigated. Although the
win/draw/loss records do not reach statistical signiﬁcance for committees of size 10, at
size 100, MultiBoost achieves lower error signiﬁcantly more often than higher error in
comparison to all three of the alternative committee algorithms.
• AdaBoost is shown to, in general, reduce both error due to bias and variance, while
bagging and wagging primarily reduce error due to variance. This result is consistent
with Bauer and Kohavi , Breiman and Schapire et al.’s previous
• Wagging using the continuous Poisson distribution is not as effective at variance reduction
as is bagging. This result is consistent with Bauer and Kohavi’s ﬁndings comparing
wagging using Gaussian noise against bagging.
• Bagging and wagging tend to be more effective at variance reduction than AdaBoost.
This is consistent with Bauer and Kohavi’s results using ‘real-world’ data but
contrary to Breiman and Schapire et al.’s results with artiﬁcial data.
• MultiBoost achieves most of the bias reduction of AdaBoost together with most of the
variance reduction of wagging.
• MultiBoosting using wagging of AdaBoosted sub-committees is more effective than
MultiBoosting using bagging of AdaBoosted sub-committees, at least at small committee
• All committee algorithms have lower mean contributions of bias to error at t = 10 than
at t = 100. At least in the case of AdaBoost, there is evidence that this occurs despite an
improvement in the quality of the central tendency. A possible explanation for this effect
is that at t = 10 variance is increased, decreasing the frequency with which the central
tendency is established and hence decreasing the opportunity for the central tendency to
contribute to error.
MultiBoosthasbeendemonstratedto,ingeneral,achievelowererrorthaneitherAdaBoost
of wagging when applied to a base learning algorithm and learning tasks for which there is
MULTIBOOSTING
sufﬁcient scope for both bias and variance reduction. The beneﬁts of MultiBoosting over
either of its constituents might be expected to be signiﬁcantly lower in an application for
which there is little scope for both bias and variance reduction, such as decision stump
or naive Bayesian learning, where variance is low. For conventional decision tree learning, however, MultiBoosting appears to offer signiﬁcant gains over the standard AdaBoost
algorithm.
Appendix A: Bias and variance measures for classiﬁcation learning
This Appendix provides deﬁnitions of the ﬁve measures of bias and variance used in the
current research, accompanied by a discussion of how these measures relate to previous
bias/variance measures for classiﬁcation learning.
It is important to note that a learner is applied to a ﬁnite vector of objects drawn from
a distribution of training sets. However, its performance is evaluated with respect to test
data drawn from a distribution of objects that may not directly relate to the distribution of
training sets. Following Breiman , the misclassiﬁcation error of L(T ) is deﬁned as
the probability that it will misclassify a (y, x) pair drawn from the test instance distribution
error(L(T )) = PY,X(L(T )(X) ̸= Y)
The intrinsic noise or irreducible prediction error for an instance distribution Y, X is the
lowest expected error that a classiﬁer (learned or otherwise) may obtain. It reﬂects the degree of randomness of the output variable and equals the prediction error of the optimal
Bayes classiﬁer
Y,X (x) = arg max
Note that both the optimal Bayes classiﬁer and the irreducible prediction error can only be
calculated from the test data distribution. Neither can be calculated directly from a training
The central tendency C◦
L,T (x) for learner L over the distribution of training data sets T
is the class with the greatest probability of selection for description x by classiﬁers learned
by L from training sets drawn from T .
L,T (x) = arg max
PT (L(T )(x) = y)
Most bias/variance analyses for classiﬁcation learning decompose classiﬁcation error into
three terms, bias, variance, and irreducible prediction error . While it is useful to take account of irreducible prediction
error for some analyses of classiﬁcation algorithms, for the analysis of classiﬁcation committees, interest centers on the manner in which bias and variance are affected. Further, it
is difﬁcult to estimate irreducible prediction error from the limited samples of data usually
employed in classiﬁcation learning experiments. In view of these two considerations, this
research follows the lead of Kong and Dietterich in decomposing error into only the
bias and variance terms.
Of previous bias/variance deﬁnitions for classiﬁcation learning, the closest to the deﬁnitions for numeric regression are Friedman . However, these deﬁnitions only apply
to the two class case, a limitation that mitigates against their use in the current context.
Also closely capturing the deﬁnitions for numeric regression are Kohavi and Wolpert’s
 deﬁnitions. They deﬁne three terms, biasKW
x, varianceKW x, and σx with relation to
description x.
[PY,X(Y = y | X = x) −PT (L(T )(x) = y)]2
varianceKW x = 1
PT (L(T )(x) = y)2
PY,X(Y = y | X = x)2
These deﬁnitions have the advantage of decomposing error into three terms that sum to error
and measure the strength of the central tendency, the degree of variation between inferred
classiﬁers, and the degree of variation in the output variable (classiﬁcation) for a single input.
However, these measures of bias and variance do not measure the extent to which each of
these underlying quantities contributes to error. For example, consider an input x for a three
class (a, b, and c) classiﬁcation task for which PT (L(T )(x) = a) = 0.6, PT (L(T )(x) =
b) = 0.2, and PT (L(T )(x) = c) = 0.2. Suppose also that PY,X(Y = a | X = x) = 1.0
(there is no intrinsic noise). biasKW
x = 0.12. varianceKW x = 0.28. Now consider the scenario where PT (L(T )(x) = a) = 0.6, PT (L(T )(x) = b) = 0.4, PT (L(T )(x) = c) = 0.0,
and PY,X(Y = a | X = x) = 1.0. This is the same as the previous scenario except that
whereas the errors in the former were equally divided between predictions of classes b and
c, in the new scenario all errors involve predictions of class b. Now, biasKW
x = 0.16 and
varianceKW x = 0.24. Bias has increased. However, the probability of a classiﬁcation deviating from the central tendency has not changed, nor has the strength or nature of the central
tendency. All that has changed is the distribution between other classes of the deviations
from the central tendency, in a manner that does not affect classiﬁcation performance.
In contrast, Kong and Dietterich’s deﬁnition of bias directly measures the error
of the central tendency over the distributions of training sets (T ) and test cases (Y, X).
biasKD = P(Y,X),T (C◦
L,T (X) ̸= Y)
varianceKD = error(L(T )) −biasKD
However, (A.8) does not adequately measure the error due to deviations from the central
tendency. For example, consider a description x for which the central tendency differs from
MULTIBOOSTING
the optimal Bayes classiﬁer. This will occur if the class most commonly selected for x
by the learner is not the most common class for x in the test distribution. In this situation
biasKD > 0.5 and error < biasKD. See, for example, Case 2 of Table 2. As a consequence,
varianceKD will be negative. If the negative values for varianceKD on some descriptions are
perfectly matched by a set of corresponding positive values on other descriptions, the total
varianceKD for a domain may be zero, irrespective of the extent to which the classiﬁers
learned from different training sets differ from one another. To capture the usual intention
for the variance term, it should increase as deviations from the central tendency increase.
Further, while (A.7) measures the error of the central tendency, it can be argued that this
differs from the error due to the central tendency. As a greater proportion of the learned
classiﬁers differ from the central tendency, the contribution of the central tendency to the
total error will decrease, and consequently, it could be argued, a measure of bias should also
decrease. For these reasons, varianceKD does not appear very useful for the comparisons
of alternative learning algorithms to be performed herein, and has not been adopted in this
work. There is value for this research in biasKD, however, which captures the quality of
the central tendency without regard for the consistency with which that central tendency is
established.
Breiman’s formulation provides a simple and intuitive operationalization of
the general bias/variance concept in terms of the contribution of the central tendency and
deviations from the central tendency to error. Breiman ﬁrst decomposes error into irreducible and reducible error. A misclassiﬁcation C(x) will contribute to irreducible error if
C(x) = CBayes
X,Y (x). Any misclassiﬁcation that does not contribute to irreducible error instead
contributes to reducible error. Bias equates to misclassiﬁcations where C(x) contributes to
reducible error and C(x) = C◦
L,T (x). Any reducible error that does not contribute to bias
instead contributes to variance.
biasB = P(Y,X),T
L(T )(X) ̸= Y ∧L(T )(X) ̸= CBayes
∧L(T )(X) = C◦
varianceB = P(Y,X),T
L(T )(X) ̸= Y ∧L(T )(X) ̸= CBayes
∧L(T )(X) ̸= C◦
This decomposition cleanly captures the concept of bias measuring error due to central
tendency and variance measuring error due to deviations from the central tendency. This
differs substantially from Kohavi and Wolpert and Friedman’s deﬁnitions
of variance, which measure the frequency with which predictions of individual learned
classiﬁers differ from the central tendency, irrespective of whether those deviations result
in correct or erroneous classiﬁcations. It is accepted, however, that this latter approach to
variance is more consistent with deﬁnitions of bias and variance for numeric regression.
Nonetheless, there remains the practical obstacle to the application of Breiman’s definitions (other than to artiﬁcial domains for which this factor can be controlled) that its
evaluation requires knowledge, or at least estimation, of the irreducible error. To circumvent this difﬁculty, the current research uses a less complex decomposition that distributes
the irreducible error across the bias and variance terms. This is similar in principle to Bauer
and Kohavi’s practice of aggregating irreducible error with the bias term, but is
more natural in the context of Breiman’s formulation. This results in the following
deﬁnitions.
bias = P(Y,X),T
L(T )(X) ̸= Y ∧L(T )(X) = C◦
variance = P(Y,X),T
L(T )(X) ̸= Y ∧L(T )(X) ̸= C◦
These deﬁnitions have the desirable properties that
• bias is a direct measure of the contribution of the central tendency to total error;
• variance is a direct measure of the contribution to error of deviations from the central
tendency; and
• error = bias + variance.
It does, however, differ markedly from the form of bias/variance analysis used for numeric
regression. In recognition of this shift, these bias and variance terms will be referred to as
contribution of bias to error and contribution of variance to error.
These two terms fail to directly measure the degree of variance irrespective of the target
distribution, which is of interest because it sets a lower limit on the error that may be
achieved by a learner that is independent of the target function. Nor does it capture the
accuracy of the central tendency. As (A.4) and (A.5) provide better measures for the former
purpose and (A.7) provides a better measure for the latter purpose, they will be used in
conjunctionwith(A.11)and(A.12)toprovidea detailedproﬁleofthebias/variancebehavior
of the algorithms. Note that due to the difﬁculty of estimating intrinsic noise, following the
practice of Kohavi and Wolpert , the value reported for biasK W is the sum of (A.6)
and (A.4).
Our primary interest will be in the impact on bias and variance of different committee
learning algorithms. As the irreducible error component of (A.11) and (A.12) will be constant across all learners, its aggregation into the bias and variance terms will not affect our
conclusions. Such aggregation does, however, greatly simplify and increase the accuracy
of the estimation of these terms during experimentation.
Appendix B: A bias/variance estimation method
The deﬁnitions of bias and variance all refer to distributions about which the only knowledge
we have in a standard machine learning experimental situation is a small sample from the
distribution. In consequence, it is necessary to estimate these terms.
Bauer and Kohavi estimate bias and variance using a two stage process. First,
the available data D are randomly divided into a training pool TP1 (2/3) and a test set TS1
(the remaining 1/3). Then ten training sets T 1
1 , . . . , T 10
are formed by random selection,
each containing 1/2 of the objects from TP1. This process is repeated two more times, with
different random TP2, TS2 and TP3, TS3 selections, to produce training sets T 1
2 , . . . , T 10
3 , . . . , T 10
3 . The bias and variance terms are then estimated by evaluation of the predictions
MULTIBOOSTING
i ), . . . , L(T 10
i ) when applied to TSi, for i ∈{1, 2, 3}. Of particular relevance to
estimation of (A.11) and (A.12), the central tendency C◦
L,T (x) for an object x ∈TSi is
estimated by
A similar approach has been employed in the current research. However, instead of the use
of training sets selected from a pool of training objects, which requires the use of relatively
few (in the method described above, only 1/3) of the available objects for each learning
session, three-fold cross-validation has been used. By this technique, the available data D
are divided into three folds, f1, f2, and f3. This process is repeated ten times producing
thirty folds, f 1
1 , . . . , f 10
3 . Each of the folds from a triple is used as a test set
once for a classiﬁer learned from the other two folds in the triple. For convenience we will
use the abbreviations T i
3, and T i
2. Using this approach,
L,T (x) for an object x ∈D is estimated by
The use of cross-validation in this manner has the advantage that every available example
is used the same number of times, both for training and for testing. In contrast Bauer and
Kohavi , by the use of random selection of cases, may use different cases different
numbers of times in a given role. This random process may dramatically effect the results
obtained. Cross validation is performed ten times in order to average out the effect of another
singlerandomeventthatmaysubstantiallyaffecttheresults—thedivisionofcasesintofolds.
The variance between individual cross-validation trials can be very high. Averaging over
a number of cross-validation trials gives a more accurate estimation of the average case
performance of an algorithm for learning from training sets of the speciﬁed size drawn
from the available data.
Once the cross-validation trials have been completed, the relevant measures are estimated
directly from the observed distributions of results.
Appendix C: Detailed results
Tables C.1 to C.3 present the mean error, contribution of bias to error, and contribution of
variance to error for each algorithm on each data set. Tables of values are presented rather
than graphical representations of relative performance as these allow more precise analysis
of comparative performance. Note that due to rounding of values to three decimal places,
in some cases the reported values for bias and variance do not sum precisely to the reported
value for error.
Table C.1.
Mean error for each algorithm.
Balance-scale
Br. cancer Slov.
Br. cancer Wisc.
Credit (Aust.)
Credit (German)
Discordant
Echocardiogram
Horse-colic
House-votes-84
Letter-recognition
Lymphography
New-thyroid
Pima diabetes
Primary tumor
Soybean large
Splice junction
Tic-tac-toe
MULTIBOOSTING
Table C.2.
Mean contribution of bias to error for each algorithm.
Balance-scale
Br. cancer Slov.
Br. cancer Wisc.
Credit (Aust.)
Credit (German)
Discordant
Echocardiogram
Horse-colic
House-votes-84
Letter-recognition
Lymphography
New-thyroid
Pima diabetes
Primary tumor
Soybean large
Splice junction
Tic-tac-toe
Table C.3.
Mean contribution of variance to error for each algorithm.
Balance-scale
Br. cancer Slov.
Br. cancer Wisc.
Credit (Aust.)
Credit (German)
Discordant
Echocardiogram
Horse-colic
House-votes-84
Letter-recognition
Lymphography
New-thyroid
Pima diabetes
Primary tumor
Soybean large
Splice junction
Tic-tac-toe
MULTIBOOSTING
Acknowledgments
I am grateful to Zijian Zheng and Ross Quinlan for fruitful discussions that have helped to
stimulate this research. I am also grateful to John Slaney for suggesting the use of geometric
means for aggregate statistics over ratio values. The Breast Cancer, Lymphography and
Primary Tumor data sets were provided by the Ljubljana Oncology Institute, Slovenia.
Thanks to the UCI Repository’s maintainers and donors, for providing access to the data
sets used herein.
1. Krogh and Vedelsby’s work actually relates to ensembles for numeric regression, but the principle also
applies to committees for classiﬁcation.
2. The continuous Poisson distribution is more commonly known as the exponential distribution.
3. The current implementation differs from strict wagging of boosted sub-committees by using equal instance
weights for the ﬁrst sub-committee. This was done in the belief that boosting with the training set unmodiﬁed
would out-perform boosting from a wagged distribution, and hence it would be beneﬁcial to include this stronger
sub-committee in the ﬁnal committee.
4. As arithmetic mean error ratios are usually reported rather than geometric mean error ratios, it is interesting to
consider whether the use of this non-standard statistic is affecting the results. At t = 10 the arithmetic means
of error ratio for MultiBoost against AdaBoost, bagging, and wagging are 0.982, 0.941, and 0.903. At t = 100
they are 0.990, 0.884, and 0.864, respectively. As can be seen, MultiBoost enjoys an advantage irrespective of
whether arithmetic or geometric means are considered.