Towards Total Recall in Industrial Anomaly Detection
Karsten Roth1,∗, Latha Pemula2, Joaquin Zepeda2, Bernhard Sch¨olkopf2, Thomas Brox2, Peter Gehler2
1University of T¨ubingen
2Amazon AWS
Being able to spot defective parts is a critical component
in large-scale industrial manufacturing. A particular challenge that we address in this work is the cold-start problem:
ﬁt a model using nominal (non-defective) example images
only. While handcrafted solutions per class are possible,
the goal is to build systems that work well simultaneously
on many different tasks automatically. The best peforming approaches combine embeddings from ImageNet models with an outlier detection model. In this paper, we extend
on this line of work and propose PatchCore, which uses
a maximally representative memory bank of nominal patchfeatures. PatchCore offers competitive inference times while
achieving state-of-the-art performance for both detection
and localization. On the challenging, widely used MVTec
AD benchmark PatchCore achieves an image-level anomaly
detection AUROC score of up to 99.6%, more than halving
the error compared to the next best competitor. We further report competitive results on two additional datasets
and also ﬁnd competitive results in the few samples regime.
Code: github.com/amazon-research/patchcore-inspection.
1. Introduction
The ability to detect unusual patterns in images is a feature deeply ingrained in human cognition. Humans can differentiate between expected variance in the data and outliers
after having only seen a small number of normal instances.
In this work we address the computational version of this
problem, cold-start1 anomaly detection for visual inspection of industrial image data. It arises in many industrial
scenarios where it is easy to acquire imagery of normal examples but costly and complicated to specify the expected
defect variations in full. This task is naturally cast as a outof-distribution detection problem where a model needs to
distinguish between samples being drawn from the training
data distribution and those outside its support. Industrial
visual defect classiﬁcation is especially hard, as errors can
∗Work done during a research internship at Amazon AWS.
1Commonly also dubbed one-class classiﬁcation (OCC).
Figure 1. Examples from the MVTec benchmark datasets. Superimposed on the images are the segmentation results from Patch-
Core. The orange boundary denotes anomaly contours of actual
segmentation maps for anomalies such as broken glass, scratches,
burns or structural changes in blue-orange color gradients.
vary from subtle changes such as thin scratches to larger
structural defects like missing components . Some examples from the MVTec AD benchmark along with results
from our proposed method are shown in Figure 1. Existing
work on cold-start, industrial visual anomaly detection relies on learning a model of the nominal distribution via autoencoding methods , GANs , or other
unsupervised adaptation methods . Recently, 
proposed to leverag common deep representations from ImageNet classiﬁcation without adaptation to the target distribution. Despite the missing adaptation, these models offer strong anomaly detection performance and even solid
spatial localization of the defects. The key principle behind these techniques is a feature matching between the test
sample and the nominal samples while exploiting the multiscale nature of deep feature representations. Subtle, ﬁnegrained defect segmentation is covered by high-resolution
features, whereas structural deviations and full image-level
anomaly detection are supposed to be covered by features
at much higher abstraction levels. The inherent downside of
this approach, since it is non-adaptive, is the limited matching conﬁdence at the higher abstraction levels: high-level
abstract features from ImageNet training coincide little with
 
the abstract features required in an industrial environment.
In addition, nominal context usable by these methods at
test time is effectively limited by the small number of extractable high-level feature representations.
In this paper, we present PatchCore as an effective remedy by (1) maximizing nominal information available at
test time, (2) reducing biases towards ImageNet classes
and (3) retaining high inference speeds.
Relying on the
fact that an image can be already classiﬁed as anomalous
as soon as a single patch is anomalous , Patch-
Core achieves this by utilizing locally aggregated, mid-level
features patches.
The usage of mid-level network patch
features allows PatchCore to operate with minimal bias towards ImageNet classes on a high resolution, while a feature aggregation over a local neighbourhood ensures retention of sufﬁcient spatial context. This results in an extensive memory bank allowing PatchCore to optimally leverage available nominal context at test time.
Finally, for
practical applicability, PatchCore additionally introduces
greedy coreset subsampling for nominal feature banks as
a key element to both reduce redundancy in the extracted,
patch-level memory bank as well as signiﬁcantly bringing
down storage memory and inference time, making Patch-
Core very attractive for realistic industrial use cases.
Thorough experiments on the diverse MVTec AD as
well as the specialized Magnetic Tile Defects (MTD) 
industrial anomaly detection benchmarks showcase the
power of PatchCore for industrial anomaly detection.
achieves state-of-the-art image-level detection scores on
MVTec AD and MTD, with nearly perfect scores on MVTec
AD (up to AUROC 99.6%), reducing detection error of
previous methods by more than half, as well as state-ofthe-art industrial anomaly localization performance. Patch-
Core achieves this while retaining fast inference times without requiring training on the dataset at hand. This makes
PatchCore very attractive for practical use in industrial
anomaly detection. In addition, further experiments showcase the high sample efﬁciency of PatchCore, matching existing anomaly detection methods in performance while using only a fraction of the nominal training data.
2. Related Works
Most anomaly detection models rely on the ability to
learn representations inherent to the nominal data.
can be achieved for example through the usage of autoencoding models . To encourage better estimation of the
nominal feature distribution, extensions based on Gaussian
mixture models , generative adversarial training objectives , invariance towards predeﬁned physical augmentations , robustness of hidden features to reintroduction of reconstructions , prototypical memory banks
 , attention-guidance , structural objectives 
or constrained representation spaces have been proposed. Other unsupervised representation learning methods
can similarly be utilised, such as via GANs , learning
to predict predeﬁned geometric transformations or via
normalizing ﬂows . Given respective nominal representations and novel test representations, anomaly detection
can then be a simple matter of reconstruction errors ,
distances to k nearest neighbours or ﬁnetuning of a
one-class classiﬁcation model such as OC-SVMs or
SVDD on top of these features. For the majority
of these approaches, anomaly localization comes naturally
based on pixel-wise reconstruction errors, saliency-based
approaches such as GradCAM or XRAI can be
used for anomaly segmentation as well.
Industrial Anomaly Detection.
While literature on
general anomaly detection through learned nominal representations is vast, industrial image data comes with its own
challenges , for which recent works starting with have
shown state-of-the-art detection performance using models
pretrained on large external natural image datasets such as
ImageNet without any adaptation to the data at hand.
This has given rise to other industrial anomaly detection
methods reliant on better reuse of pretrained features such
as SPADE , which utilizes memory banks comprising
various feature hierarchies for ﬁnegrained, kNN-based 
anomaly segmentation and image-level anomaly detection.
Similarly, recently proposed PaDiM, which utilizes a
locally constrained bag-of-features approach , estimating
patch-level feature distribution moments (mean and covariance) for patch-level Mahalanobis distance measures .
This approach is similar to studied on full images. To
better account for the distribution shift between natural pretraining and industrial image data, subsequent adaptation
can be done, e.g. via student-teacher knowledge distillation such as in or normalizing ﬂows 
trained on top of pretrained network features .
The speciﬁc components used in PatchCore are most
related to SPADE and PaDiM. SPADE makes use of a
memory-bank of nominal features extracted from a pretrained backbone network with separate approaches for
image- and pixel-level anomaly detection. PatchCore similarly uses a memory bank, however with neighbourhoodaware patch-level features critical to achieve higher performance, as more nominal context is retained and a better ﬁtting inductive bias is incorporated. In addition, the memory
bank is coreset-subsampled to ensure low inference cost at
higher performance. Coresets have seen longstanding usage in fundamental kNN and kMeans approaches or
mixture models by ﬁnding subsets that best approximate the structure of some available set and allow for approximate solution ﬁnding with notably reduced cost .
More recently, coreset-based methods have also found their
way into Deep Learning approaches, e.g for network pruning , active learning and increasing effective data
coverage of mini-batches for improved GAN training 
or representation learning . The latter three have found
success utilizing a greedy coreset selection mechanism. As
we aim to approximate memory bank feature space coverage, we similarly adapt a greedy coreset mechanism for
PatchCore .
Finally, our patch-level approach to both
image-level anomaly detection and anomaly segmentation
is related to PaDiM with the goal of encouraging higher
anomaly detection sensitivity. We make use of an efﬁcient
patch-feature memory bank equally accessible to all patches
evaluated at test time, whereas PaDiM limits patch-level
anomaly detection to Mahalanobis distance measures speciﬁc to each patch. In doing so, PatchCore becomes less
reliant on image alignment while also estimating anomalies
using a much larger nominal context. Furthermore, unlike
PaDiM, input images do not require the same shape during
training and testing. Finally, PatchCore makes use of locally aware patch-feature scores to account for local spatial
variance and to reduce bias towards ImageNet classes.
The PatchCore method consists of several parts that we
will describe in sequence: local patch features aggregated
into a memory bank (§3.1), a coreset-reduction method to
increase efﬁciency (§3.2) and ﬁnally the full algorithm that
arrives at detection and localization decisions (§3.3).
3.1. Locally aware patch features
We use XN to denote the set of all nominal images (∀x ∈
XN : yx = 0) available at training time, with yx ∈{0, 1}
denoting if an image x is nominal (0) or anomalous (1).
Accordingly, we deﬁne XT to be the set of samples provided
at test time, with ∀x ∈XT : yx ∈{0, 1}. Following ,
 and , PatchCore uses a network φ pre-trained on
ImageNet. As the features at speciﬁc network hierarchies
plays an important role, we use φi,j = φj(xi) to denote the
features for image xi ∈X (with dataset X) and hierarchylevel j of the pretrained network φ. If not noted otherwise,
in concordance with existing literature, j indexes feature
maps from ResNet-like architectures, such as ResNet-
50 or WideResnet-50 , with j ∈{1, 2, 3, 4} indicating
the ﬁnal output of respective spatial resolution blocks.
One choice for a feature representation would be the last
level in the feature hierarchy of the network. This is done
in or but introduces the following two problems.
Firstly, it loses more localized nominal information .
As the types of anomalies encountered at test time are not
known a priori, this becomes detrimental to the downstream
anomaly detection performance. Secondly, very deep and
abstract features in ImageNet pretrained networks are biased towards the task of natural image classiﬁcation, which
has only little overlap with the cold-start industrial anomaly
detection task and the evaluated data at hand.
We thus propose to use a memory bank M of patch-level
features comprising intermediate or mid-level feature representations to make use of provided training context, avoiding features too generic or too heavily biased towards ImageNet classiﬁcation. In the speciﬁc case of ResNet-like
architectures, this would refer to e.g. j ∈ . To formalize the patch representation we extend the previously introduced notation. Assume the feature map φi,j ∈Rc∗×h∗×w∗
to be a three-dimensional tensor of depth c∗, height h∗and
width w∗. We then use φi,j(h, w) = φj(xi, h, w) ∈Rc∗
to denote the c∗-dimensional feature slice at positions h ∈
{1, . . . , h∗} and w ∈{1, . . . , w∗}. Assuming the receptive
ﬁeld size of each φi,j to be larger than one, this effectively
relates to image-patch feature representations. Ideally, each
patch-representation operates on a large enough receptive
ﬁeld size to account for meaningful anomalous context robust to local spatial variations. While this could be achieved
by strided pooling and going further down the network hierarchy, the thereby created patch-features become more
ImageNet-speciﬁc and thus less relevant for the anomaly
detection task at hand, while training cost increases and effective feature map resolution drops.
This motivates a local neighbourhood aggregation when
composing each patch-level feature representation to increase receptive ﬁeld size and robustness to small spatial deviations without losing spatial resolution or usability of feature maps. For that, we extend above notation for φi,j(h, w)
to account for an uneven patchsizes p (corresponding to the
neighbourhood size considered), incorporating feature vectors from the neighbourhood
= {(a, b)|a ∈[h −⌊p/2⌋, ..., h + ⌊p/2⌋],
b ∈[w −⌊p/2⌋, ..., w + ⌊p/2⌋]},
and locally aware features at position (h, w) as
{φi,j(a, b)|(a, b) ∈N (h,w)
with fagg some aggregation function of feature vectors in
the neighbourhood N (h,w)
. For PatchCore, we use adaptive average pooling. This is similar to local smoothing over
each individual feature map, and results in one single representation at (h, w) of predeﬁned dimensionality d, which
is performed for all pairs (h, w) with h ∈{1, ..., h∗} and
w ∈{1, ..., w∗} and thus retains feature map resolution.
For a feature map tensor φi,j, its locally aware patch-feature
collection Ps,p(φi,j) is
Ps,p(φi,j) = {φi,j(N (h,w)
h, w mod s = 0, h < h∗, w < w∗, h, w ∈N},
with the optional use of a striding parameter s, which we set
to 1 except for ablation experiments done in §4.4.2. Empirically and similar to and , we found aggregation of
Figure 2. Overview of PatchCore. Nominal samples are broken down into a memory bank of neighbourhood-aware patch-level features.
For reduced redundancy and inference time, this memory bank is downsampled via greedy coreset subsampling. At test time, images are
classiﬁed as anomalies if at least one patch is anomalous, and pixel-level anomaly segmentation is generated by scoring each patch-feature.
Figure 3. Comparison: coreset (top) vs. random subsampling
(bottom) (red) for 2D data (iblue) sampled from (a) multimodal
and (b) uniform distributions. Visually, coreset subsampling better approximates the spatial support, random subsampling misses
clusters in the multi-modal case and is less uniform in (b).
multiple feature hierarchies to offer some beneﬁt. However,
to retain the generality of used features as well as the spatial resolution, PatchCore uses only two intermediate feature hierarchies j and j + 1. This is achieved simply by
computing Ps,p(φi,j+1) and aggregating each element with
its corresponding patch feature at the lowest hierarchy level
used (i.e., at the highest resolution), which we achieve by bilinearly rescaling Ps,p(φi,j+1) such that |Ps,p(φi,j+1)| and
|Ps,p(φi,j)| match.
Finally, for all nominal training samples xi ∈XN, the
PatchCore memory bank M is then simply deﬁned as
Ps,p(φj(xi)).
3.2. Coreset-reduced patch-feature memory bank
For increasing sizes of XN, M becomes exceedingly
large and with it both the inference time to evaluate novel
test data and required storage. This issue has already been
noted in SPADE for anomaly segmentation, which
makes use of both low- and high-level feature maps. Due
to computational limitations, SPADE requires a preselection stage of feature maps for pixel-level anomaly detection
based on a weaker image-level anomaly detection mechanism reliant on full-image, deep feature representations,
i.e., global averaging of the last feature map. This results in
low-resolution, ImageNet-biased representations computed
from full images which may negatively impact detection
and localization performance.
These issues can be addressed by making M meaningfully searchable for larger image sizes and counts, allowing for patch-based comparison beneﬁcial to both anomaly
detection and segmentation. This requires that the nominal feature coverage encoded in M is retained. Unfortunately, random subsampling, especially by several magnitudes, will lose signiﬁcant information available in M encoded in the coverage of nominal features (see also experiments done in §4.4.2). In this work we use a coreset subsampling mechanism to reduce M, which we ﬁnd reduces
inference time while retaining performance.
Conceptually, coreset selection aims to ﬁnd a subset S ⊂
A such that problem solutions over A can be most closely
and especially more quickly approximated by those computed over S . Depending on the speciﬁc problem, the
coreset of interest varies. Because PatchCore uses nearest
neighbour computations (next Section), we use a minimax
facility location coreset selection, see e.g., and ,
to ensure approximately similar coverage of the M-coreset
MC in patch-level feature space as compared to the original
memory bank M
C = arg min
n∈MC ∥m −n∥2 .
The exact computation of M∗
C is NP-Hard , we use the
iterative greedy approximation suggested in . To further reduce coreset selection time, we follow , making
use of the Johnson-Lindenstrauss theorem to reduce dimensionalities of elements m ∈M through random linear
projections ψ : Rd →Rd∗with d∗< d. The memory bank
reduction is summarized in Algorithm 1. For notation, we
use PatchCore−n% to denote the percentage n to which the
original memory bank has been subsampled to, e.g., Patch-
Core−1% a 100x times reduction of M. Figure 3 gives a
visual impression of the spatial coverage of greedy coreset
subsampling compared to random selection.
Algorithm 1: PatchCore memory bank.
Input: Pretrained φ, hierarchies j, nominal data
XN, stride s, patchsize p, coreset target l,
random linear projection ψ.
Output: Patch-level Memory bank M.
Algorithm:
for xi ∈XN do
M ←M ∪Ps,p(φj(xi))
/* Apply greedy coreset selection.
for i ∈[0, ..., l −1] do
mi ←arg max
n∈MC ∥ψ(m) −ψ(n)∥2
MC ←MC ∪{mi}
3.3. Anomaly Detection with PatchCore
With the nominal patch-feature memory bank M, we estimate the image-level anomaly score s ∈R for a test image
xtest by the maximum distance score s∗between test patchfeatures in its patch collection P(xtest) = Ps,p(φj(xtest)) to
each respective nearest neighbour m∗in M:
mtest,∗, m∗= arg max
mtest∈P(xtest)
mtest,∗−m∗
To obtain s, we use scaling w on s∗to account for the
behaviour of neighbour patches: If memory bank features
closest to anomaly candidate mtest,∗, m∗, are themselves
far from neighbouring samples and thereby an already rare
nominal occurrence, we increase the anomaly score
exp ∥mtest,∗−m∗∥2
m∈Nb(m∗) exp ∥mtest,∗−m∥2
with Nb(m∗) the b nearest patch-features in M for test
patch-feature m∗. We found this re-weighting to be more
robust than just the maximum patch distance. Given s, segmentations follow directly. The image-level anomaly score
in Eq. 7 (ﬁrst line) requires the computation of the anomaly
score for each patch through the arg max-operation. A segmentation map can be computed in the same step, similar
to , by realigning computed patch anomaly scores based
on their respective spatial location. To match the original
input resolution, (we may want to use intermediate network
features), we upscale the result by bi-linear interpolation.
Additionally, we smoothed the result with a Gaussian of
kernel width σ = 4, but did not optimize this parameter.
4. Experiments
4.1. Experimental Details
Datasets. To study industrial anomaly detection performance, the majority of our experiments are performed on
the MVTec Anomaly Detection benchmark .
MVTec AD contains 15 sub-datasets with a total of 5354
images, 1725 of which are in the test set. Each sub-dataset
is divided into nominal-only training data and test sets containing both nominal and anomalous samples for a speciﬁc product with various defect types as well as respective
anomaly ground truth masks. As in , images are
resized and center cropped to 256 × 256 and 224 × 224, respectively. No data augmentation is applied, as this requires
prior knowledge about class-retaining augmentations.
We also study industrial anomaly detection on more specialized tasks. For that, we leverage the Magnetic Tile Defects (MTD) dataset as used in , which contains
925 defect-free and 392 anomalous magnetic tile images
with varied illumination levels and image sizes. Same as
in , 20% of defect-free images are evaluated against at
test time, with the rest used for cold-start training.
Finally, we also highlight potential applicability of Patch-
Core to non-industrial image data, benchmarking coldstart anomaly localization on Mini Shanghai Tech Campus
(mSTC) as done in e.g. and . mSTC is a subsampled
version of the original STC dataset , only using every
ﬁfth training and test video frame. It contains pedestrian
videos from 12 different scenes. Training videos include
normal pedestrian behaviour while test videos can contain
different behaviours such as ﬁghting or cycling. For comparability of our cold-start experiments, we follow established
mSTC protocols , not making use of any anomaly
supervision and images resized to 256 × 256.
Evaluation Metrics.
Image-level anomaly detection
performance is measured via the area under the receiveroperator curve (AUROC) using produced anomaly scores.
In accordance with prior work we compute on MVTec the
class-average AUROC . To measure segmentation performance, we use both pixel-wise AUROC and the
PRO metric ﬁrst, both following . The PRO score takes
into account the overlap and recovery of connected anomaly
components to better account for varying anomaly sizes in
MVTec AD, see for details.
Table 1. Anomaly Detection Performance (AUROC) on MVTec AD . PaDiM∗denotes a result from with problem-speciﬁc
backbone selection. The total count of misclassiﬁcations was determined as the sum of false-positive and false-negative predictions given a
F1-optimal threshold. We did not have individual anomaly scores for competing methods so could compute this number only for PatchCore.
SPADE 
PatchSVDD 
DifferNet 
PaDiM 
Mah.AD 
PaDiM∗ 
PatchCore−25%
PatchCore−10%
PatchCore−1%
Misclassiﬁcations ↓
Table 2. Anomaly Segmentation Performance (pixelwise AUROC) on MVTec AD .
AESSIM 
γ-VAE + grad. 
CAVGA-Rw 
PatchSVDD 
SPADE 
PaDiM 
PatchCore−25%
PatchCore−10%
PatchCore−1%
Figure 4. Local awareness and network feature depths vs. detection performance. PRO score results in the supplementary.
4.2. Anomaly Detection on MVTec AD
The results for image-level anomaly detection on MVTec
are shown in Table 1. For PatchCore we report on various levels of memory bank subsampling (25%, 10% and
1%). For all cases, PatchCore achieves signiﬁcantly higher
mean image anomaly detection performance with consistently high performance on all sub-datasets (see supplementary B for detailed comparison). Please note, that a reduction from an error of 2.1% (PaDiM) to 0.9% for Patch-
Core−25% means a reduction of the error by 57%. In industrial inspection settings this is a relevant and signiﬁcant
reduction. For MVTec at optimal F1 threshold, there are
only 42 out of 1725 images classiﬁed incorrectly and a third
of all classes are solved perfectly. In the supplementary material B we also show that both with F1-optimal working
point and at full recall, classiﬁcation errors are also lower
when compared to both SPADE and PaDiM. With Patch-
Core, less than 50 images remain misclassiﬁed. In addition,
PatchCore achieves state-of-the-art anomaly segmentation,
both measured by pixelwise AUROC (Table 2, 98.1 versus 97.5 for PaDiM) and PRO metric (Table 3, 93.5 versus
92.1). Sample segmentations in Figure 1 offer qualitative
impressions of the accurate anomaly localization.
In addition, due to the effectiveness of our coreset memory subsampling, we can apply PatchCore−1% on images
of higher resolution (e.g. 280/320 instead of 224) and ensemble systems while retaining inferences times less than
PatchCore−10% on the default resolution. This allows us
to further push image- and pixel-level anomaly detection as
highlighted in Tab. 4 (detailed results in supplementary), in
parts more than halving the error again (e.g. 1% →0.4%
for image-level AUROC).
4.3. Inference Time
The other dimension we are interested in is inference
time. We report results in Table 5 (implementation details in
supp. A) comparing to reimplementations of SPADE 
and PaDiM using WideResNet50 and operations on
GPU where possible.
These inference times include the
forward pass through the backbone. As can be seen, inference time for joint image- and pixel-level anomaly detection of PatchCore−100% (without subsampling) are lower
than SPADE but with higher performance. With coreset subsampling, Patchcore can be made even faster, with
lower inference times than even PaDiM while retaining
state-of-the-art image-level anomaly detection and segmentation performance. Finally, we examine PatchCore−100%
with approximate nearest neighbour search (IVFPQ )
as an orthogonal way of reducing inference time (which
can also be applied to SPADE, however which already performs notably worse than even PatchCore−1%). We ﬁnd a
performance drop, especially for image-level anomaly detection, while inference times are still higher than Patch-
Core−1%. Though even with performance reduction, approximate nearest neighbour search on PatchCore−100%
still outperforms other methods. A combination of coreset
and approximate nearest neighbour would further reduce inference time, allowing scaling to much larger datasets.
4.4. Ablations Study
We report on ablations for the locally aware patchfeatures and the coreset reduction method. Supplementary
experiments show consistency across different backbones
(§C.2), scalability with increased image resolution (§C.3)
and a qualitative analysis of remaining errors (§C.4).
Table 3. Anomaly Detection Performance on MVTec AD as measured in PRO [%] .
AESSIM 
Student 
SPADE 
PaDiM 
PatchCore−25%
PatchCore−10%
PatchCore−1%
PatchCore-1% with higher resolution/larger backbones/ensembles. The coreset subsampling allows for computationally expensive setups while still retaining fast inference.
DenseN-201 & RNext-101 & WRN-101 (2+3), Imagesize 320
WRN-101 (2+3), Imagesize 280
WRN-101 (1+2+3), Imagesize 280
Table 5. Mean inference time per image on MVTec AD. Scores
are (image AUROC, pixel AUROC, PRO metric).
PatchCore−100%
PatchCore−10%
PatchCore−1%
(99.1, 98.0, 93.3)
(99.0, 98.1, 93.5)
(99.0, 98.0, 93.1)
PatchCore−100% + IVFPQ
(98.0, 97.9, 93.0)
(85.3, 96.6, 91.5)
(95.4, 97.3, 91.8)
Figure 5. Performance retention for different subsamplers, results
for PRO score in the supplementary.
Locally aware patch-features and hierarchies
We investigate the importance of locally aware patchfeatures (§3.3) by evaluating changes in anomaly detection
performance over different neighbourhood sizes in Eq. 1.
Results in the top half of Figure 4 show a clear optimum between locality and global context for patch-based anomaly
predictions, thus motivating the neighbourhood size p = 3.
More global context can also be achieved by moving down
the network hierarchy (see e.g. ), however at the
cost of reduced resolution and heavier ImageNet class bias
(§3.1). Indexing the ﬁrst three WideResNet50-blocks with
1 - 3, Fig. 4 (bottom) again highlights an optimum between
highly localized predictions, more global context and ImageNet bias. As can be seen, features from hierarchy level 2
can already achieve state-of-the-art performance, but bene-
ﬁt from additional feature maps taken from subsequent hierarchy levels (2+3, which is chosen as the default setting).
Importance of Coreset subsampling
Figure 5 compares different memory bank M subsampling
methods: Greedy coreset selection, random subsampling
and learning of a set of basis proxies corresponding to the
subsampling target percentage ptarget. For the latter, we sample proxies pi ∈P ⊂Rd with |P| = ptarget ·|M|, which are
then tasked to minimize a basis reconstruction objective
Lrec(mi) =
pj∈P e∥mi−pj∥pk
to ﬁnd N proxies that best describe the memory bank
data M. In Figure 5 we compare the three settings and
ﬁnd that coreset-based subsampling performs better than
the other possible choices.
The performance of no subsampling is comparable to a coreset-reduced memory bank
that is two orders of magnitudes smaller in size. We also
ﬁnd subsampled memory banks to contain much less redundancy. We recorded the percentage of memory bank
samples that are used at test time for non-subsampled and
coreset-subsampled memory banks.
While initially only
less than 30% of memory bank samples are used, coreset
subsampling (to 1%) increases this factor to nearly 95%.
For certain subsampling intervals (between around 50% and
10%), we even ﬁnd joint performance over anomaly detection and localization to partly increase as compared to nonsubsampled PatchCore . Finally, reducing the memory bank
size M by means of increased striding (see Eq. 3) shows
worse performance due to the decrease in resolution context, with stride s = 2 giving an image anomaly detection
AUROC of 97.6%, and stride s = 3 an AUROC of 96.8%.
4.5. Low-shot Anomaly Detection
Having access to limited nominal data is a relevant setting for real-world inspection. Therefore in addition to reporting results on the full MVTec AD, we also study the
performance with fewer training examples. We vary the
amount of training samples from 1 (corresponding to 0.4%
of the total nominal training data) to 50 (21%), and compare
to reimplementations of SPADE and PaDiM using
the same backbone (WideResNet50). Results are summarized in Figure 6, with detailed results available in Supp.
Figure 6. PatchCore shows notably higher sample-efﬁciency than competitors, matching the previous state-of-the-art with a fraction of
nominal training data. Note that PaDiM and SPADE where reimplemented with WideResNet50 for comparability.
Table 6. Anomaly Segmentation on mSTC and anomaly
detection on MTD compared to results reported in .
CAVGA-Ru 
SPADE 
PaDiM 
PatchCore−10
Pixelwise AUROC [%]
GANomaly 
DifferNet 
PatchCore−10
As shown, using only one ﬁfth of nominal training data, PatchCore can still match previous state-of-the-art
performance. In addition, comparing to the 16-shot experiments performed in , we ﬁnd PatchCore to outperform
their approach which adapts a normalizing ﬂows model on
top of already pretrained features. Compared to image-level
memory approaches in , we ﬁnd matching localization
and detection performance with only 5/1 nominal shots.
4.6. Evaluation on other benchmarks
We benchmark PatchCore on two additional anomaly detection performance benchmarks: The ShanghaiTech Campus dataset (STC) and the Magnetic Tile Defects
dataset (MTD) . Evaluation for STC as described in
§4.1 follows , and . We report unsupervised
anomaly localization performance on a subsampled version
of the STC video data (mSTC), with images resized to
256 × 256 . As the detection context is much closer
to natural image data available in ImageNet, we make use
of deeper network feature maps at hierarchy levels 3 and
4, but otherwise do not perform any hyperparameter tuning
for PatchCore. The results in Table 6 (top) show state-ofthe-art anomaly localization performance which suggests
good transferability of PatchCore to such domains.
Finally, we examine MTD, containing magnetic tile defect
images of varying sizes on which spatially rigid approaches
like PaDiM cannot be applied directly. Here, nominal data
already exhibits high variability similar to those encountered in anomalous samples . We follow the protocol
proposed in to measure image-level anomaly detection performance and ﬁnd performance to match (and even
slightly outperform) that of (Table 6, bottom).
5. Conclusion
This paper introduced the PatchCore algorithm for coldstart anomaly detection, in which knowledge of only nominal examples has to be leveraged to detect and segment
anomalous data at test-time. PatchCore strikes a balance
between retaining a maximum amount of nominal context
at test-time through the usage of memory banks comprising
locally aware, nominal patch-level feature representations
extracted from ImageNet pretrained networks, and minimal runtime through coreset subsampling. The result is a
state-of-the-art cold-start image anomaly detection and localization system with low computational cost on industrial
anomaly detection benchmarks. On MVTec, we achieve an
image anomaly detection AUROC over 99% with highest
sample efﬁciency in relevant small training set regimes.
Broader Impact.
As automated industrial anomaly
detection is one of the most successful applications of
Computer Vision, the improvements gained through Patch-
Core can be of notable interest for practitioners in this
As our work focuses speciﬁcally on industrial
anomaly detection, negative societal impact is limited. And
while the fundamental approach can potentially we leveraged for detection systems in more controversial domains,
we don’t believe that our improvements are signiﬁcant
enough to change societal application of such systems.
Limitations. While PatchCore shows high effectiveness
for industrial anomaly detection without the need to specifically adapt to the problem domain at hand, applicability
is generally limited by the transferability of the pretrained
features leveraged. This can be addressed by merging the
effectiveness of PatchCore with adaptation of the utilized
features. We leave this interesting extension to future work.
Acknowledgements
We thank Yasser Jadidi and Alex Smola for setup support of our compute infrastructure. K.R. thanks the International Max Planck Research School for Intelligent Systems
(IMPRS-IS) and the European Laboratory for Learning and
Intelligent Systems (ELLIS) PhD program for support.