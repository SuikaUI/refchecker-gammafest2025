DreamBooth: Fine Tuning Text-to-Image Diffusion Models
for Subject-Driven Generation
Nataniel Ruiz∗,1,2
Yuanzhen Li1
Varun Jampani1
Yael Pritch1
Michael Rubinstein1
Kﬁr Aberman1
1 Google Research
2 Boston University
Figure 1. With just a few images (typically 3-5) of a subject (left), DreamBooth—our AI-powered photo booth—can generate a myriad
of images of the subject in different contexts (right), using the guidance of a text prompt. The results exhibit natural interactions with the
environment, as well as novel articulations and variation in lighting conditions, all while maintaining high ﬁdelity to the key visual features
of the subject.
Large text-to-image models achieved a remarkable leap
in the evolution of AI, enabling high-quality and diverse
synthesis of images from a given text prompt.
these models lack the ability to mimic the appearance of
subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present
a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we ﬁne-tune a pretrained text-to-image model such that
it learns to bind a unique identiﬁer with that speciﬁc subject. Once the subject is embedded in the output domain of
the model, the unique identiﬁer can be used to synthesize
novel photorealistic images of the subject contextualized in
different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-speciﬁc prior
preservation loss, our technique enables synthesizing the
subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks,
including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject’s key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: 
*This research was performed while Nataniel Ruiz was at Google.
1. Introduction
Can you imagine your own dog traveling around the
world, or your favorite bag displayed in the most exclusive
showroom in Paris? What about your parrot being the main
character of an illustrated storybook? Rendering such imaginary scenes is a challenging task that requires synthesizing
instances of speciﬁc subjects (e.g., objects, animals) in new
contexts such that they naturally and seamlessly blend into
the scene.
Recently developed large text-to-image models have
shown unprecedented capabilities, by enabling high-quality
and diverse synthesis of images based on a text prompt written in natural language . One of the main advantages
of such models is the strong semantic prior learned from a
large collection of image-caption pairs. Such a prior learns,
for instance, to bind the word “dog” with various instances
of dogs that can appear in different poses and contexts in
an image. While the synthesis capabilities of these models
are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize
novel renditions of the same subjects in different contexts.
The main reason is that the expressiveness of their output
domain is limited; even the most detailed textual description
of an object may yield instances with different appearances.
 
Furthermore, even models whose text embedding lies in a
shared language-vision space cannot accurately reconstruct the appearance of given subjects but only create variations of the image content (Figure 2).
In this work, we present a new approach for “personalization” of text-to-image diffusion models (adapting them
to user-speciﬁc image generation needs). Our goal is to expand the language-vision dictionary of the model such that
it binds new words with speciﬁc subjects the user wants
to generate. Once the new dictionary is embedded in the
model, it can use these words to synthesize novel photorealistic images of the subject, contextualized in different
scenes, while preserving their key identifying features. The
effect is akin to a “magic photo booth”—once a few images of the subject are taken, the booth generates photos of
the subject in different conditions and scenes, as guided by
simple and intuitive text prompts (Figure 1).
More formally, given a few images of a subject (∼3-
5), our objective is to implant the subject into the output domain of the model such that it can be synthesized
with a unique identiﬁer. To that end, we propose a technique to represent a given subject with rare token identiﬁers
and ﬁne-tune a pre-trained, diffusion-based text-to-image
framework.
We ﬁne-tune the text-to-image model with the input images and text prompts containing a unique identiﬁer followed by the class name of the subject (e.g., “A [V] dog”).
The latter enables the model to use its prior knowledge on
the subject class while the class-speciﬁc instance is bound
with the unique identiﬁer.
In order to prevent language
drift that causes the model to associate the class
name (e.g., “dog”) with the speciﬁc instance, we propose
an autogenous, class-speciﬁc prior preservation loss, which
leverages the semantic prior on the class that is embedded in
the model, and encourages it to generate diverse instances
of the same class as our subject.
We apply our approach to a myriad of text-based image generation applications including recontextualization of
subjects, modiﬁcation of their properties, original art renditions, and more, paving the way to a new stream of previously unassailable tasks. We highlight the contribution
of each component in our method via ablation studies, and
compare with alternative baselines and related work. We
also conduct a user study to evaluate subject and prompt
ﬁdelity in our synthesized images, compared to alternative
approaches.
To the best of our knowledge, ours is the ﬁrst technique
that tackles this new challenging problem of subject-driven
generation, allowing users, from just a few casually captured images of a subject, synthesize novel renditions of the
subject in different contexts while maintaining its distinctive features.
To evaluate this new task, we also construct a new dataset
Figure 2. Subject-driven generation. Given a particular clock
(left), it is hard to generate it while maintaining high ﬁdelity to
its key visual features (second and third columns showing DALL-
E2 image-guided generation and Imagen text-guided
generation; text prompt used for Imagen: “retro style yellow alarm
clock with a white clock face and a yellow number three on the
right part of the clock face in the jungle”). Our approach (right)
can synthesize the clock with high ﬁdelity and in new contexts
(text prompt: “a [V] clock in the jungle”).
that contains various subjects captured in different contexts,
and propose a new evaluation protocol that measures the
subject ﬁdelity and prompt ﬁdelity of the generated results.
We make our dataset and evaluation protocol publicly available on the project webpage.
2. Related work
Image Composition.
Image composition techniques
 aim to clone a given subject into a new background such that the subject melds into the scene.
consider composition in novel poses, one may apply 3D
reconstruction techniques which usually
works on rigid objects and require a larger number of views.
Some drawbacks include scene integration (lighting, shadows, contact) and the inability to generate novel scenes.
In contrast, our approach enable generation of subjects in
novel poses and new contexts.
Text-to-Image Editing and Synthesis. Text-driven image manipulation has recently achieved signiﬁcant progress
using GANs combined with image-text representations such as CLIP , yielding realistic manipulations using text .
These methods
work well on structured scenarios (e.g. human face editing) and can struggle over diverse datasets where subjects are varied. Crowson et al. use VQ-GAN 
and train over more diverse data to alleviate this concern.
Other works exploit the recent diffusion models
 , which achieve state-of-the-art
generation quality over highly diverse datasets, often surpassing GANs . While most works that require only
text are limited to global editing , Bar-Tal et al. 
proposed a text-based localized editing technique without
using masks, showing impressive results. While most of
these editing approaches allow modiﬁcation of global properties or local editing of a given image, none enables generating novel renditions of a given subject in new contexts.
There also exists work on text-to-image synthesis . Recent large
text-to-image models such as Imagen , DALL-E2 ,
Parti , CogView2 and Stable Diffusion demonstrated unprecedented semantic generation. These models
do not provide ﬁne-grained control over a generated image
and use text guidance only. Speciﬁcally, it is challenging or
impossible to preserve the identity of a subject consistently
across synthesized images.
Controllable Generative Models.
There are various
approaches to control generative models, where some of
them might prove to be viable directions for subject-driven
prompt-guided image synthesis.
Liu et al. propose
a diffusion-based technique allowing for image variations
guided by reference image or text. To overcome subject
modiﬁcation, several works assume a user-provided
mask to restrict the modiﬁed area. Inversion 
can be used to preserve a subject while modifying context.
Prompt-to-prompt allows for local and global editing
without an input mask. These methods fall short of identitypreserving novel sample generation of a subject.
In the context of GANs, Pivotal Tuning allows for
real image editing by ﬁnetuning the model with an inverted
latent code anchor, and Nitzan et al. extended this work
to GAN ﬁnetuning on faces to train a personalized prior,
which requires around 100 images and are limited to the
face domain. Casanova et al. propose an instance conditioned GAN that can generate variations of an instance,
although it can struggle with unique subjects and does not
preserve all subject details.
Finally, the concurrent work of Gal et al. proposes
a method to represent visual concepts, like an object or
a style, through new tokens in the embedding space of a
frozen text-to-image model, resulting in small personalized
token embeddings. While this method is limited by the expressiveness of the frozen diffusion model, our ﬁne-tuning
approach enables us to embed the subject within the model’s
output domain, resulting in the generation of novel images
of the subject which preserve its key visual features.
Given only a few (typically 3-5) casually captured images of a speciﬁc subject, without any textual description,
our objective is to generate new images of the subject
with high detail ﬁdelity and with variations guided by text
prompts. Example variations include changing the subject
location, changing subject properties such as color or shape,
modifying the subject’s pose, viewpoint, and other semantic
modiﬁcations. We do not impose any restrictions on input
image capture settings and the subject image can have varying contexts. We next provide some background on textto-image diffusion models (Sec. 3.1), then present our ﬁnetuning technique to bind a unique identiﬁer with a subject
described in a few images (Sec. 3.2), and ﬁnally propose a
class-speciﬁc prior-preservation loss that enables us to overcome language drift in our ﬁne-tuned model (Sec. 3.3).
3.1. Text-to-Image Diffusion Models
Diffusion models are probabilistic generative models
that are trained to learn a data distribution by the gradual
denoising of a variable sampled from a Gaussian distribution. Speciﬁcally, we are interested in a pre-trained text-toimage diffusion model ˆxθ that, given an initial noise map
ϵ ∼N(0, I) and a conditioning vector c = Γ(P) generated
using a text encoder Γ and a text prompt P, generates an
image xgen = ˆxθ(ϵ, c). They are trained using a squared
error loss to denoise a variably-noised image or latent code
zt := αtx + σtϵ as follows:
wt∥ˆxθ(αtx + σtϵ, c) −x∥2
where x is the ground-truth image, c is a conditioning
vector (e.g., obtained from a text prompt), and αt, σt, wt
are terms that control the noise schedule and sample quality, and are functions of the diffusion process time t ∼
U( ). A more detailed description is given in the supplementary material.
3.2. Personalization of Text-to-Image Models
Our ﬁrst task is to implant the subject instance into the
output domain of the model such that we can query the
model for varied novel images of the subject. One natural idea is to ﬁne-tune the model using the few-shot dataset
of the subject.
Careful care had to be taken when ﬁnetuning generative models such as GANs in a few-shot scenario as it can cause overﬁtting and mode-collapse - as
well as not capturing the target distribution sufﬁciently well.
There has been research on techniques to avoid these pitfalls , although, in contrast to our work,
this line of work primarily seeks to generate images that resemble the target distribution but has no requirement of subject preservation. With regards to these pitfalls, we observe
the peculiar ﬁnding that, given a careful ﬁne-tuning setup
Figure 3. Fine-tuning. Given ∼3−5 images of a subject we ﬁnetune a text-to-image diffusion model with the input images paired
with a text prompt containing a unique identiﬁer and the name of
the class the subject belongs to (e.g., “A [V] dog”), in parallel, we
apply a class-speciﬁc prior preservation loss, which leverages the
semantic prior that the model has on the class and encourages it to
generate diverse instances belong to the subject’s class using the
class name in a text prompt (e.g., “A dog”).
using the diffusion loss from Eq 1, large text-to-image diffusion models seem to excel at integrating new information
into their domain without forgetting the prior or overﬁtting
to a small set of training images.
Designing Prompts for Few-Shot Personalization
goal is to “implant” a new (unique identiﬁer, subject) pair
into the diffusion model’s “dictionary” . In order to bypass the overhead of writing detailed image descriptions for
a given image set we opt for a simpler approach and label
all input images of the subject “a [identiﬁer] [class noun]”,
where [identiﬁer] is a unique identiﬁer linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). The class descriptor can
be provided by the user or obtained using a classiﬁer. We
use a class descriptor in the sentence in order to tether the
prior of the class to our unique subject and ﬁnd that using
a wrong class descriptor, or no class descriptor increases
training time and language drift while decreasing performance. In essence, we seek to leverage the model’s prior
of the speciﬁc class and entangle it with the embedding of
our subject’s unique identiﬁer so we can leverage the visual
prior to generate new poses and articulations of the subject
in different contexts.
Rare-token Identiﬁers
We generally ﬁnd existing English words (e.g. “unique”, “special”) suboptimal since the
model has to learn to disentangle them from their original
meaning and to re-entangle them to reference our subject.
This motivates the need for an identiﬁer that has a weak
prior in both the language model and the diffusion model. A
hazardous way of doing this is to select random characters
in the English language and concatenate them to generate a
rare identiﬁer (e.g. “xxy5syt00”). In reality, the tokenizer
might tokenize each letter separately, and the prior for the
diffusion model is strong for these letters. We often ﬁnd that
these tokens incur the similar weaknesses as using common
English words. Our approach is to ﬁnd rare tokens in the
vocabulary, and then invert these tokens into text space, in
order to minimize the probability of the identiﬁer having a
strong prior. We perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identiﬁers f( ˆV),
where f is a tokenizer; a function that maps character sequences to tokens and ˆV is the decoded text stemming from
the tokens f( ˆV). The sequence can be of variable length k,
and ﬁnd that relatively short sequences of k = {1, ..., 3}
work well. Then, by inverting the vocabulary using the detokenizer on f( ˆV) we obtain a sequence of characters that
deﬁne our unique identiﬁer ˆV. For Imagen, we ﬁnd that using uniform random sampling of tokens that correspond to
3 or fewer Unicode characters (without spaces) and using
tokens in the T5-XXL tokenizer range of {5000, ..., 10000}
works well.
3.3. Class-speciﬁc Prior Preservation Loss
In our experience, the best results for maximum subject
ﬁdelity are achieved by ﬁne-tuning all layers of the model.
This includes ﬁne-tuning layers that are conditioned on the
text embeddings, which gives rise to the problem of language drift.
Language drift has been an observed problem in language models , where a model that is
pre-trained on a large text corpus and later ﬁne-tuned for
a speciﬁc task progressively loses syntactic and semantic
knowledge of the language. To the best of our knowledge,
we are the ﬁrst to ﬁnd a similar phenomenon affecting diffusion models, where to model slowly forgets how to generate
subjects of the same class as the target subject.
Another problem is the possibility of reduced output diversity.
Text-to-image diffusion models naturally posses
high amounts of output diversity. When ﬁne-tuning on a
small set of images we would like to be able to generate the
subject in novel viewpoints, poses and articulations. Yet,
there is a risk of reducing the amount of variability in the
output poses and views of the subject (e.g. snapping to the
few-shot views). We observe that this is often the case, especially when the model is trained for too long.
To mitigate the two aforementioned issues, we propose
an autogenous class-speciﬁc prior preservation loss that encourages diversity and counters language drift. In essence,
our method is to supervise the model with its own generated samples, in order for it to retain the prior once the
few-shot ﬁne-tuning begins. This allows it to generate diverse images of the class prior, as well as retain knowledge about the class prior that it can use in conjunction
with knowledge about the subject instance. Speciﬁcally,
we generate data xpr = ˆx(zt1, cpr) by using the ancestral
sampler on the frozen pre-trained diffusion model with random initial noise zt1 ∼N(0, I) and conditioning vector
cpr := Γ(f(”a [class noun]”)). The loss becomes:
Ex,c,ϵ,ϵ′,t[wt∥ˆxθ(αtx + σtϵ, c) −x∥2
λwt′∥ˆxθ(αt′xpr + σt′ϵ′, cpr) −xpr∥2
where the second term is the prior-preservation term that
supervises the model with its own generated images, and λ
controls for the relative weight of this term. Figure 3 illustrates the model ﬁne-tuning with the class-generated samples and prior-preservation loss. Despite being simple, we
ﬁnd this prior-preservation loss is effective in encouraging
output diversity and in overcoming language-drift. We also
ﬁnd that we can train the model for more iterations without risking overﬁtting. We ﬁnd that ∼1000 iterations with
λ = 1 and learning rate 10−5 for Imagen and 5 × 10−6
for Stable Diffusion , and with a subject dataset size
of 3-5 images is enough to achieve good results. During
this process, ∼1000 “a [class noun]” samples are generated - but less can be used. The training process takes about
5 minutes on one TPUv4 for Imagen, and 5 minutes on a
NVIDIA A100 for Stable Diffusion.
4. Experiments
In this section, we show experiments and applications.
Our method enables a large expanse of text-guided semantic
modiﬁcations of our subject instances, including recontextualization, modiﬁcation of subject properties such as material and species, art rendition, and viewpoint modiﬁcation.
Importantly, across all of these modiﬁcations, we are able
to preserve the unique visual features that give the subject its identity and essence. If the task is recontextualization, then the subject features are unmodiﬁed, but appearance (e.g., pose) may change. If the task is a stronger
semantic modiﬁcation, such as crossing between our subject and another species/object, then the key features of the
subject are preserved after modiﬁcation. In this section, we
reference the subject’s unique identiﬁer using [V]. We include speciﬁc Imagen and Stable Diffusion implementation
details in the supp. material.
4.1. Dataset and Evaluation
We collected a dataset of 30 subjects, including
unique objects and pets such as backpacks, stuffed animals,
dogs, cats, sunglasses, cartoons, etc. We separate each subject into two categories: objects and live subjects/pets. 21
of the 30 subjects are objects, and 9 are live subjects/pets.
Figure 4. Comparisons with Textual Inversion Given 4
input images (top row), we compare: DreamBooth Imagen (2nd
row), DreamBooth Stable Diffusion (3rd row), Textual Inversion
(bottom row).
Output images were created with the following
prompts (left to right): “a [V] vase in the snow”, “a [V] vase on
the beach”, “a [V] vase in the jungle”, “a [V] vase with the Eiffel
Tower in the background”. DreamBooth is stronger in both subject
and prompt ﬁdelity.
Real Images
DreamBooth (Imagen)
DreamBooth (Stable Diffusion)
Textual Inversion (Stable Diffusion)
Subject ﬁdelity (DINO, CLIP-I) and prompt ﬁdelity
(CLIP-T, CLIP-T-L) quantitative metric comparison.
Subject Fidelity ↑
Prompt Fidelity ↑
DreamBooth (Stable Diffusion)
Textual Inversion (Stable Diffusion)
Table 2. Subject ﬁdelity and prompt ﬁdelity user preference.
We provide one sample image for each of the subjects in
Images for this dataset were collected by the
authors or sourced from Unsplash . We also collected
25 prompts: 20 recontextualization prompts and 5 property
modiﬁcation prompts for objects; 10 recontextualization, 10
Figure 5. Dataset. Example images for each subject in our proposed dataset.
accessorization, and 5 property modiﬁcation prompts for
live subjects/pets. The full list of prompts can be found in
the supplementary material.
For the evaluation suite we generate four images per subject and per prompt, totaling 3,000 images. This allows us
to robustly measure performances and generalization capabilities of a method. We make our dataset and evaluation
protocol publicly available on the project webpage for future use in evaluating subject-driven generation.
Evaluation Metrics
One important aspect to evaluate is
subject ﬁdelity: the preservation of subject details in generated images. For this, we compute two metrics: CLIP-I and
DINO . CLIP-I is the average pairwise cosine similarity
between CLIP embeddings of generated and real images. Although this metric has been used in other work ,
it is not constructed to distinguish between different subjects that could have highly similar text descriptions (e.g.
two different yellow clocks). Our proposed DINO metric
is the average pairwise cosine similarity between the ViT-
S/16 DINO embeddings of generated and real images. This
is our preferred metric, since, by construction and in contrast to supervised networks, DINO is not trained to ignore
differences between subjects of the same class. Instead, the
self-supervised training objective encourages distinction of
unique features of a subject or image. The second important aspect to evaluate is prompt ﬁdelity, measured as the
average cosine similarity between prompt and image CLIP
embeddings. We denote this as CLIP-T.
Figure 6. Encouraging diversity with prior-preservation loss.
Naive ﬁne-tuning can result in overﬁtting to input image context
and subject appearance (e.g. pose). PPL acts as a regularizer that
alleviates overﬁtting and encourages diversity, allowing for more
pose variability and appearance diversity.
4.2. Comparisons
We compare our results with Textual Inversion, the recent concurrent work of Gal et al. , using the hyperparameters provided in their work. We ﬁnd that this work is
the only comparable work in the literature that is subjectdriven, text-guided and generates novel images. We generate images for DreamBooth using Imagen, DreamBooth
using Stable Diffusion and Textual Inversion using Stable
Diffusion. We compute DINO and CLIP-I subject ﬁdelity
metrics and the CLIP-T prompt ﬁdelity metric. In Table 1
we show sizeable gaps in both subject and prompt ﬁdelity
metrics for DreamBooth over Textual Inversion. We ﬁnd
that DreamBooth (Imagen) achieves higher scores for both
subject and prompt ﬁdelity than DreamBooth (Stable Diffusion), approaching the upper-bound of subject ﬁdelity for
real images. We believe that this is due to the larger expressive power and higher output quality of Imagen.
Further, we compare Textual Inversion (Stable Diffusion) and DreamBooth (Stable Diffusion) by conducting a
user study. For subject ﬁdelity, we asked 72 users to answer questionnaires of 25 comparative questions (3 users
per questionnaire), totaling 1800 answers. Samples are randomly selected from a large pool. Each question shows the
set of real images for a subject, and one generated image of
that subject by each method (with a random prompt). Users
are asked to answer the question: “Which of the two images
best reproduces the identity (e.g. item type and details) of
the reference item?”, and we include a “Cannot Determine
/ Both Equally” option. Similarly for prompt ﬁdelity, we
DreamBooth (Imagen) w/ PPL
DreamBooth (Imagen)
Table 3. Prior preservation loss (PPL) ablation displaying a prior
preservation (PRES) metric, diversity metric (DIV) and subject
and prompt ﬁdelity metrics.
Correct Class
Wrong Class
Table 4. Class name ablation with subject ﬁdelity metrics.
ask “Which of the two images is best described by the reference text?”. We average results using majority voting and
present them in Table 2. We ﬁnd an overwhelming preference for DreamBooth for both subject ﬁdelity and prompt
ﬁdelity. This shines a light on results in Table 1, where
DINO differences of around 0.1 and CLIP-T differences of
0.05 are signiﬁcant in terms of user preference. Finally, we
show qualitative comparisons in Figure 4. We observe that
DreamBooth better preserves subject identity, and is more
faithful to prompts. We show samples of the user study in
the supp. material.
4.3. Ablation Studies
Prior Preservation Loss Ablation
We ﬁne-tune Imagen
on 15 subjects from our dataset, with and without our proposed prior preservation loss (PPL). The prior preservation
loss seeks to combat language drift and preserve the prior.
We compute a prior preservation metric (PRES) by computing the average pairwise DINO embeddings between generated images of random subjects of the prior class and real
images of our speciﬁc subject. The higher this metric, the
more similar random subjects of the class are to our speciﬁc
subject, indicating collapse of the prior. We report results in
Table 3 and observe that PPL substantially counteracts language drift and helps retain the ability to generate diverse
images of the prior class. Additionally, we compute a diversity metric (DIV) using the average LPIPS cosine
similarity between generated images of same subject with
same prompt. We observe that our model trained with PPL
achieves higher diversity (with slightly diminished subject
ﬁdelity), which can also be observed qualitatively in Figure 6, where our model trained with PPL overﬁts less to the
environment of the reference images and can generate the
dog in more diverse poses and articulations.
Class-Prior Ablation
We ﬁnetune Imagen on a subset of
our dataset subjects (5 subjects) with no class noun, a randomly sampled incorrect class noun, and the correct class
noun. With the correct class noun for our subject, we are
able to faithfully ﬁt to the subject, take advantage of the
Figure 7. Recontextualization. We generate images of the subjects in different environments, with high preservation of subject details and
realistic scene-subject interactions. We show the prompts below each image.
class prior, allowing us to generate our subject in various
contexts. When an incorrect class noun (e.g. “can” for a
backpack) is used, we run into contention between our subject and and the class prior - sometimes obtaining cylindrical backpacks, or otherwise misshapen subjects. If we train
with no class noun, the model does not leverage the class
prior, has difﬁculty learning the subject and converging, and
can generate erroneous samples. Subject ﬁdelity results are
shown in Table 4, with substantially higher subject ﬁdelity
for our proposed approach.
4.4. Applications
Recontextualization
We can generate novel images for a
speciﬁc subject in different contexts (Figure 7) with descriptive prompts (“a [V] [class noun] [context description]”).
Importantly, we are able to generate the subject in new
poses and articulations, with previously unseen scene structure and realistic integration of the subject in the scene (e.g.
contact, shadows, reﬂections).
Art Renditions
Given a prompt “a painting of a [V] [class
noun] in the style of [famous painter]” or “a statue of a [V]
[class noun] in the style of [famous sculptor]” we are able
to generate artistic renditions of our subject. Unlike style
transfer, where the source structure is preserved and only
the style is transferred, we are able to generate meaningful, novel variations depending on the artistic style, while
preserving subject identity.
E.g, as shown in Figure 8,
“Michelangelo”, we generated a pose that is novel and not
seen in the input images.
Figure 8. Novel view synthesis, art renditions, and property
modiﬁcations.
We are able to generate novel and meaningful
images while faithfully preserving subject identity and essence.
More applications and examples in the supplementary material.
Novel View Synthesis
We are able to render the subject
under novel viewpoints. In Figure 8, we generate new im-
Failure modes.
Given a rare prompted context the
model might fail at generating the correct environment (a). It is
possible for context and subject appearance to become entangled
(b). Finally, it is possible for the model to overﬁt and generate
images similar to the training set, especially if prompts reﬂect the
original environment of the training set (c).
ages of the input cat (with consistent complex fur patterns)
under new viewpoints. We highlight that the model has not
seen this speciﬁc cat from behind, below, or above - yet it is
able to extrapolate knowledge from the class prior to generate these novel views given only 4 frontal images of the
Property Modiﬁcation
We are able to modify subject
properties. For example, we show crosses between a speciﬁc Chow Chow dog and different animal species in the
bottom row of Figure 8. We prompt the model with sentences of the following structure: “a cross of a [V] dog and
a [target species]”. In particular, we can see in this example that the identity of the dog is well preserved even when
the species changes - the face of the dog has certain unique
features that are well preserved and melded with the target species. Other property modiﬁcations are possible, such
as material modiﬁcation (e.g. “a transparent [V] teapot” in
Figure 7). Some are harder than others and depend on the
prior of the base generation model.
4.5. Limitations
We illustrate some failure models of our method in Figure 9. The ﬁrst is related to not being able to accurately
generate the prompted context. Possible reasons are a weak
prior for these contexts, or difﬁculty in generating both the
subject and speciﬁed concept together due to low probability of co-occurrence in the training set. The second is
context-appearance entanglement, where the appearance of
the subject changes due to the prompted context, exempli-
ﬁed in Figure 9 with color changes of the backpack. Third,
we also observe overﬁtting to the real images that happen
when the prompt is similar to the original setting in which
the subject was seen.
Other limitations are that some subjects are easier to
learn than others (e.g. dogs and cats). Occasionally, with
subjects that are rarer, the model is unable to support as
many subject variations. Finally, there is also variability
in the ﬁdelity of the subject and some generated images
might contain hallucinated subject features, depending on
the strength of the model prior, and the complexity of the
semantic modiﬁcation.
5. Conclusions
We presented an approach for synthesizing novel renditions of a subject using a few images of the subject and the
guidance of a text prompt. Our key idea is to embed a given
subject instance in the output domain of a text-to-image diffusion model by binding the subject to a unique identiﬁer.
Remarkably - this ﬁne-tuning process can work given only
3-5 subject images, making the technique particularly accessible. We demonstrated a variety of applications with
animals and objects in generated photorealistic scenes, in
most cases indistinguishable from real images.
6. Acknowledgement
We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill
Freeman, Dilip Krishnan, Huiwen Chang and Daniel
Cohen-Or for their valuable inputs that helped improve this
work, and to Mohammad Norouzi, Chitwan Saharia and
William Chan for providing us with their support and the
pretrained Imagen models.
Finally, a special thanks to
David Salesin for his feedback, advice and for his support
for the project.