Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 11–28
Florence, Italy, August 1-2, 2019. c⃝2019 Association for Computational Linguistics
Findings of the WMT 2019 Shared Task on Automatic Post-Editing
Rajen Chatterjee(1), Christian Federmann(2) Matteo Negri(3), Marco Turchi(3)
(1) Apple Inc., Cupertino, CA, USA
(2) Microsoft Cloud+AI, Redmond, WA, USA
(3) Fondazione Bruno Kessler, Trento, Italy
We present the results from the 5th round
of the WMT task on MT Automatic Post-
Editing. The task consists in automatically
correcting the output of a “black-box” machine translation system by learning from
human corrections. Keeping the same general evaluation setting of the previous four
rounds, this year we focused on two language pairs (English-German and English-
Russian) and on domain-speciﬁc data (Information Technology). For both the language directions, MT outputs were produced by neural systems unknown to participants. Seven teams participated in the
English-German task, with a total of 18
submitted runs. The evaluation, which was
performed on the same test set used for
the 2018 round, shows further progress in
APE technology: 4 teams achieved better results than last year’s winning system, with improvements up to -0.78 TER
and +1.23 BLEU points over the baseline.
Two teams participated in the English-
Russian task submitting 2 runs each. On
this new language direction, characterized
by a higher quality of the original translations, the task proved to be particularly
challenging.
Indeed, none of the submitted runs improved the very high results of the strong system used to produce
the initial translations (16.16 TER, 76.20
Introduction
MT Automatic Post-Editing (APE) is the task
of automatically correcting errors in a machinetranslated text. As pointed out by , from the application point of view the
task is motivated by its possible uses to:
• Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage;
• Cope with systematic errors of an MT system
whose decoding process is not accessible;
• Provide professional translators with improved MT output quality to reduce (human)
post-editing effort;
• Adapt the output of a general-purpose MT
system to the lexicon/style requested in a speciﬁc application domain.
In its 5th round, the APE shared task organized
within the WMT Conference on Machine Translation kept the same overall evaluation setting of the
previous four rounds. Speciﬁcally, the participating systems had to automatically correct the output
of an unknown “black box” MT system by learning from human revisions of translations produced
by the same system.
This year, the task focused on two language
pairs (English-German and English-Russian) and,
in continuity with the last three rounds, on data
coming from the Information Technology domain.
While in 2018 one of the proposed subtasks was
still focusing on the correction of phrase-based
MT output, this year only neural MT (NMT) output has been considered.
However, this year’s
campaign allows both for a fair assessment of the
progress in APE technology and for tests in more
challenging conditions. On one side, reusing the
same test English-German set used last year, the
evaluation framework allows us for a direct comparison with the last year’s outcomes at least on
one language. On the other side, dealing with a
difﬁcult language like Russian and only with highquality NMT output, also this round presented participants with an increased level of difﬁculty with
respect to the past.
participated
German task, submitting 18 runs in total.
teams participated in the English-Russian task,
submitting 2 runs each. Similar to last year, all
the teams developed their systems based on neural technology, which conﬁrms to be the state-ofthe-art approach to APE. Only in one case, indeed, a participating team achieved its highest results (but with no improvement over the baseline)
with a phrase-based APE system. In most of the
cases, participants experimented with the Transformer architecture , either
directly or by adapting it to the task (see Section 3).
Another common trait of the submitted systems is the reliance on the consolidated
multi-source approach , which is able to exploit
information from both the MT output to be corrected and the corresponding source sentence. The
third aspect common to all submissions is the exploitation of synthetic data, either those provided
together with the task data or
similar, domain-speciﬁc resources created ad-hoc
by participants.
In the English-German task, the evaluation was
performed on the same test set used in 2018,
whose “gold” human post-edits were kept undisclosed to participants for the sake of future comparisons.
Evaluating on the same benchmark
allowed to observe further technology improvements over the past. Last year, the largest gain
over the baseline (16.84 TER, 74.73 BLEU) was
-0.38 TER (16.46) and +0.8 BLEU (75.53). This
year, four teams achieved better results than last
year’s best submission.
The top-ranked system
achieved 16.06 TER (-0.78 with respect to the
baseline) and 75.96 BLEU (+1.23). Most noticeably, the fact that the TER/BLEU differences between the top four primary submissions are not
statistically signiﬁcant indicates that the observed
progress is not isolated.
The newly proposed English-Russian task represents a more challenging evaluation scenario,
mainly due to the higher quality of the NMT output to be corrected. In this case, even the best
submission (16.59 TER, 75.27 TER) was unable
to beat the baseline (16.16 TER, 76.20 BLEU).
These results conﬁrm one of the main ﬁndings of
previous rounds : improving high-quality MT output remains the biggest challenge for APE. This motivates further research on precise and conservative
solutions able to mimic human behaviour by performing only the minimum amount of edit operations needed.
Task description
In continuity with all the previous rounds of the
APE task, participants were provided with training and development data consisting of (source,
target, human post-edit) triplets, and were asked
to return automatic post-edits for a test set of unseen (source, target) pairs.
This year, the evaluation was performed on two
language pairs, English-German and English-
For both the subtasks, data were selected from the Information Technology (IT) domain. As emerged from the previous evaluations,
the selected target domain is speciﬁc and repetitive
enough to allow supervised systems to learn from
the training set useful correction patterns that are
also re-applicable to the test set.
The released training and development sets consist of (source, target, human post-edit) triplets in
• The source (SRC) is a tokenized English sentence;
• The target (TGT) is a tokenized German/Russian translation of the source, which
was produced by a black-box system unknown to participants.
For both the languages, translations were obtained from neural MT systems:1 this implies that their overall quality is generally high, making the task
harder compared to previous rounds, which
1For English-German, the NMT system was trained with
generic and in-domain parallel training data using the attentional encoder-decoder architecture 
implemented in the Nematus toolkit .
We used byte-pair encoding for vocabulary reduction, mini-batches of 100, word embeddings
of 500 dimensions, and gated recurrent unit layers of 1,024
units. Optimization was done using Adam and by re-shufﬂing
the training set at each epoch.
For English-Russian, the
NMT system used was the Microsoft Translator production
system, which was trained with both generic and in-domain
parallel training data.
Number of instances
Development
Additional Resources
English-German
eSCAPE-PBSMT: 7,258,533
eSCAPE-NMT: 7,258,533
Artiﬁcial: 4.5M
English-Russian
eSCAPE-NMT: 7.7 M
Table 1: Data statistics.
focused only or also 
on the correction of the output of phrasebased systems.
• The human post-edit (PE) is a manuallyrevised version of the target, which was produced by professional translators.
Test data consists of (source, target) pairs having similar characteristics of those in the training
set. Human post-edits of the test target instances
are left apart to measure system performance.
For the English-German subtask, the same indomain data2 collected for last year’s round of the
task have been used. The training and development set respectively contain 13,442 and 1,000
triplets, while the test set consists of 1,023 instances.
Participants were also provided with
two additional training resources, which were
widely used in last year’s round.
One (called
“Artiﬁcial” in Table 1) is the corpus of 4.5 million artiﬁcially-generated post-editing triplets described in . The other resource is the English-German
section of the eSCAPE corpus .
It comprises 14.5 million instances, which were
artiﬁcially generated both via phrase-based and
neural translation (7.25 millions each) of the same
source sentences.
For the English-Russian subtask, Microsoft
Ofﬁce localization data have been used.
material, which mainly consists of short segments (menu commands, short messages, etc.), is
shared with the English-Russian Quality Estimation shared task.3 The training and development
set respectively contain 15,089 and 1,000 triplets,
while the test set comprises 1,023 instances. For
this language pair, the eSCAPE corpus has been
extended to provide participants with additional
2Released by the European Project QT21 and the BLEU score
 of the TGT elements (i.e.
the original target translations).
The repetition rate measures the repetitiveness
inside a text by looking at the rate of non-singleton
n-gram types (n=1...4) and combining them using the geometric mean. Larger values indicate a
higher text repetitiveness and, as discussed in , suggest a higher chance of learning from
the training set correction patterns that are applicable also to the test set. In the previous rounds
of the task, we considered the large differences in
repetitiveness across the datasets as a possible explanation for the variable gains over the baseline
obtained by participants. In this perspective, the
low system performance observed in the APE15
task and in the APE17 German-English subtask
was in part ascribed to the low repetition rate in
the data. In contrast, much higher repetition rates
in the data likely contributed to facilitate the problem in the APE16 task and in the APE17 English-
German subtask, in which most of the participants
achieved signiﬁcant gains over the baseline. Although in both the APE18 subtasks the repetition rate values were relatively high, evaluation
results shown that the inﬂuence of data repetitiveness on ﬁnal APE performance is marginal. Indeed, while in the last year’s PBSMT subtask the
improvements over the baseline were impressive . Grey columns refer to data covering different language pairs
and domains with respect to this year’s evaluation round.
6.24 TER, +9.53 BLEU points), in the NMT subtask (whose data were reused this year) the quality
gains were considerably smaller (-0.38 TER and
+0.8 BLEU points). As discussed in Section 4.1,
also this year we observe a similar situation: especially for English-Russian, the high repetition rate
values reported in Table 2, which are the highest
ones across all the APE data released so far, are not
enough to determine quality improvements comparable to previous rounds.
This suggests that,
although it used to play an important role when
dealing with lower quality MT output in the ﬁrst
rounds of the APE task, text repetitiveness has less
inﬂuence on ﬁnal performance compared to other
complexity indicators.
Complexity indicators: MT quality
Indeed, another important aspect that determines
the difﬁculty of APE is the initial quality of the
MT output to be corrected. This can be measured
by computing the TER (↓) and BLEU (↑) scores
(last two rows in Table 2) using the human postedits as reference.
As discussed in , numeric evidence of a higher quality
of the original translations can indicate a smaller
room for improvement for APE systems (having,
at the same time, less to learn during training and
less to correct at test stage).
On one side, indeed, training on good (or near-perfect) automatic
translations can drastically reduce the number of
learned correction patterns.
On the other side,
testing on similarly good translations can drastically reduce the number of corrections required
and the applicability of the learned patterns, thus
making the task more difﬁcult. As observed in
the previous APE evaluation rounds, there is a
noticeable correlation between translation quality
and systems’ performance.
In 2016 and 2017,
on English-German data featuring a similar level
of quality (24.76/24.48 TER, 62.11/62.49 BLEU),
the top systems achieved signiﬁcant improvements
over the baseline .
In 2017, on higher quality German-English data
(15.55 TER, 79.54 BLEU), the observed gains
were much smaller (-0.26 TER, +0.28 BLEU). In
2018, the correction of English-German translations produced by a phrase-based system (24.24
TER, 62.99 BLEU) yielded much larger gains
(up to -6.24 TER and +9.53 BLEU) compared
to the correction of higher-quality neural translations (16.84 TER, 74.73 BLEU), which resulted
in TER/BLEU variations of less than 1.00 point.
As discussed in Section 4, also this year’s results
conﬁrm the strict correlation between the quality
of the initial translations and the actual potential
Complexity indicators: TER
distribution
Further indications about the difﬁculty of the two
subtasks are provided by Figures 1 and 2, which
plot the TER distribution for the items in the two
test sets. As shown in Figure 1, the distribution
for English-German is quite skewed towards low
TER values, with almost 50% of the test test items
having a TER between 0 and 10 that indicates
their very high quality (in other terms, they require few edits). In particular, the proportion of
“perfect” test instances having TER=0 (i.e. items
that should not be modiﬁed by the APE systems)
is quite high (25.2% of the total).5 For these test
5This value is considerably lower than the proportion observed in the challenging APE17 German-English test set
(45.0%) but still a considerably higher value compared to
“easier” test sets released for other rounds of the task.
Figure 1: TER distribution in the English-German
Figure 2: TER distribution in the English-Russian
items, any correction made by the APE systems
will be treated as unnecessary and penalized by
automatic evaluation metrics. This problem calls
for conservative and precise systems able to properly ﬁx errors only in the remaining test items,
leaving the “perfect” ones unmodiﬁed.
Data skewedness is exacerbated in the English-
Russian test set, in which 63.5% of the instances
have a TER between 0 and 10 (in particular, 61.4%
of them are perfect translations). Together with
the high BLEU score, this contributes to make the
English-Russian task considerably more difﬁcult
compared to the English-German one (as well as
compared to most of the APE test sets released so
As discussed in Section 4, also this year’s evaluation results conﬁrm the strict correlation between
the quality of the initial translations and the actual
potential of APE.
Evaluation metrics
System performance was evaluated both by means
of automatic metrics and manually.
metrics were used to compute the distance between automatic and human post-edits of the
machine-translated sentences present in the test
sets. To this aim, TER and BLEU (case-sensitive)
were respectively used as primary and secondary
evaluation metrics. Systems were ranked based on
the average TER calculated on the test set by using
the TERcom6 software: lower average TER scores
correspond to higher ranks.
BLEU was computed using the multi-bleu.perl package7 available
6 
7 
mosesdecoder/blob/master/scripts/
Manual evaluation was conducted via sourcebased direct human assessment 
as implemented by Appraise .
Details are discussed in Section 6.
In continuity with the previous rounds, the ofﬁcial
baseline results were the TER and BLEU scores
calculated by comparing the raw MT output with
the human post-edits.
In practice, the baseline
APE system is a “do-nothing” system that leaves
all the test targets unmodiﬁed. Baseline results,
the same shown in Table 2, are also reported in
Tables 4 and 5 for comparison with participants’
submissions.8
For each submitted run, the statistical significance of performance differences with respect
to the baseline was calculated with the bootstrap
test .
Participants
Seven teams submitted a total of 18 runs for the
English-German subtask.
Two of them participated also in the English-Russian subtask by subgeneric/multi-bleu.perl
8In addition to the do-nothing baseline, in the ﬁrst three
rounds of the task we also compared systems’ performance
with a re-implementation of the phrase-based approach ﬁrstly
proposed by Simard et al. , which represented the common backbone of APE systems before the spread of neural
solutions. As shown in ,
the steady progress of neural APE technology has made the
phrase-based solution not competitive with current methods
reducing the importance of having it as an additional term of
comparison. In 2018, we hence opted for considering only
one baseline.
Participating team
ADAPT Centre & Dublin City University, Ireland 
Fondazione Bruno Kessler, Italy 
Pohang University of Science and Technology, South Korea 
Saarland University, Germany 
Unbabel, Portugal 
USAAR DFKI
Saarland University & German Research Center for Artiﬁcial Intelligence, Germany 
Imperial College London & University of Shefﬁeld, United Kingdom
Table 3: Participants in the WMT19 Automatic Post-Editing task.
mitting 2 runs each. Participants are listed in Table 3, and a short description of their systems is
provided in the following.
ADAPT Centre & Dublin City University.
The ADAPT DCU team participated in both the
subtasks proposed this year.
Their submissions
pursue two main objectives, namely: i) investigating the effect of adding extra information in
the form of preﬁx tokens in a neural APE system; and ii) assessing whether an SMT-based approach can be effective for post-editing NMT output.
The neural APE system exploits a multisource approach based on Marian-NMT.9 Training data were augmented with two types of extra
context tokens that identify partitions of the training set that may be relevant to guide system’s behaviour (i.e. to identify features in the dataset with
a very close relation to the editing patterns the
system is supposed to learn). Such partitions are
based on sentence length and topic information.
Hence, the prepended tokens respectively state the
data partition based on the number of source tokens and the topic induced via LDA clustering
 . The statistical APE models,
which are based on Moses ,
were trained to explore the idea of interleaving
different MT technologies to improve NMT output quality.
All the models are built by taking
advantage of both the released training material
and the provided artiﬁcial data .
Fondazione Bruno Kessler.
Also FBK participated in both the subtasks. Their submissions focus on mitigating the “over-correction” problem
in APE, that is the systems’ tendency to rephrase
and correct MT output that is already acceptable,
thus producing translations that will be penalized
by evaluation against human post-edits. Following
 , the underlying idea is
that over-correction can be prevented by inform-
9 
ing the system about the predicted quality of the
MT output or, in other terms, the expected amount
of corrections needed. The proposed solution is
based on prepending a special token to the source
text and the MT output, so to indicate the required amount of post-editing. Three different tokens are used, namely “no post-edit” (no edits are
required), “light post-edit” (minimal edits are required), and “heavy post-edit” . At test time, tokens are predicted with two approaches. One is
based on a classiﬁer obtained by ﬁne-tuning BERT
 on the in-domain data. The
other approach exploits a retrieval-based method
similar to : given a query
containing the source and the MT output to be
post-edited, it: i) retrieves similar triplets from the
training data, ii) ranks them based on the sentence
level BLEU score between the MT output and the
post-edit, and iii) creates the token based on the
TER computed between the MT output and the
post-edit of the most similar triplet.
The backbone architecture is the multi-source extension of
Transformer described in
 , which is trained both on
the task data and on the available artiﬁcial corpora.
Pohang University of Science and Technology.
POSTECH’s system (English-German subtask) is
a multi-source model that extends the Transformer
implementation of the OpenNMT-py library. It includes: i) a joint encoder that
is able to generate joint representations reﬂecting
the relationship between two input sources (SRC,
TGT) with optional future masking to mimic the
general decoding process of machine translation
systems, and ii) two types of multi-source attention layers in the decoder that computes the atten-
tion between the decoder state and the two outputs
of the encoder. Therefore, four different model
variants were suggested in terms of the existence
of the encoder future mask and the type of the
multi-source attention layer in the decoder. The
eSCAPE corpus was ﬁltered
to contain similar statistics as the ofﬁcial training
dataset. During training, various teacher-forcing
ratios were adjusted to alleviate the exposure bias
problem. After training four variants with various teacher-forcing ratios, the ﬁnal submissions
were obtained from an ensemble of models. These
are: 1) the primary submission that ensembles the
variants with the two best TER scores in each architecture, 2) the contrastive submission that ensembles the variants with the best TER scores in
each architecture, 3) the contrastive submission
that ensembles two variants from each architecture, achieving the best TER and BLEU, respectively.
University.
participation
(English-German subtask) is based on a multisource
Transformer
context-level
machine translation implemented with the Neutron implementation for the Transformer translation
model .
To improve the
robustness of the training, and inspired by , the APE task is jointly trained with
the de-noising encoder task, which adds noises
distribution directly to the post-editing results’
embedding as machine translation outputs and
tries to recover the original post-editing results.
Both Gaussian noise and uniform noise were tried
for the de-noising encoder task.
The synthetic
eSCAPE corpus was also used
for the training.
Contrastive submissions were
generated with the best averaged models of 5
adjacent checkpoints of 2 kinds of noise, and the
primary submission is obtained with the ensemble
of 5 models (4 averaged models + 1 model saved
for every training epoch).
2019), Unbabel’s submission (English-German
subtask) adapts BERT to the
APE task with an encoder-decoder framework.
The system consists in a BERT encoder initialised with the pretrained model’s weights and
a BERT decoder initialised analogously, where
the multi-head context attention is initialised with
the self-attention weights.
Additionally, source
embeddings, target embeddings and projection
layer are shared, as well
as the self-attention weights of the encoder and
The system exploits BERT training
schedule with streams A and B: the encoder
receives as input both the source and the MT
output separated by the special symbol “[SEP]”,
assigning to the ﬁrst “A” segment embeddings and
to the latter “B” segment embeddings. Regarding
the BERT decoder, they use just the post-edit
with “B” segment embeddings. In addition, as the
NMT system has a strong in-domain performance,
a conservativeness factor to avoid over-correction
is explored. Similarly to , a penalty is added during
beam decoding (logits or log probabilities) to
constrain the decoding to be as close as possible
to the input – both the source and the MT output
are considered, which allows to copy from the
source – in order to avoid over edition of the
MT segment.
This penalty is tuned over the
development set. In addition to the shared task
in-domain data, system training exploits a variant
of the eSCAPE corpus built on a closer in-domain
parallel corpus (IT domain) provided by the
Quality Estimation shared task.
Saarland University & German Research Center for Artiﬁcial Intelligence.
USAAR DFKI’s
participation (English-German subtask) is based
on a multi-encoder adaptation of the Transformer
architecture. The system consists in: i) a Transformer encoder block for the source sentence, followed by ii) a Transformer decoder block, but
without masking, for self-attention on the MT segment, which effectively acts as second encoder
combining source and MT output, and iii) feeds
this representation into a ﬁnal decoder block generating the post-edit. The intuition behind the proposed architecture is to generate better representations via both self- and cross- attention and to
further facilitate the learning capacity of the feedforward layer in the decoder block. Also in this
case, model training takes advantage of the eS-
CAPE synthetic data .
University of Shefﬁeld & Imperial College London.
IC USFD’s submission (English-German
subtask) is based on the dual-source Transformer model , which was re-implemented in the
Tensor2Tensor toolkit. The
model was enriched with a copying mechanism
that prevents unnecessary corrections.
In addition to the main training data, the primary submission uses the EN-DE eSCAPE data . The contrastive submission uses data
triplets where source and target are genuine data,
and MT is a modiﬁed target (200K). This modiﬁed
target mimics MT by simulating errors in the task
training data. Sentences where error simulation is
possible are selected from in-domain corpora (eS-
CAPE, as well as the in-domain data released with
the WMT18 Quality Estimation task).
Participants’ results are shown in Tables 4
(English-German) and 5 (English-Russian). The
submitted runs are ranked based on the average
TER (case-sensitive) computed using human postedits of the MT segments as reference, which is the
APE task primary evaluation metric (“TER (pe)”).
The two tables also report the BLEU score computed using human post-edits (“BLEU (pe)” column), which represents our secondary evaluation
metric. These results are discussed in Section 4.1.
Table 4 includes four additional columns, which
show the TER/BLEU scores computed using external references (“TER (ref)” and “BLEU (ref)”)
as well as the multi-reference TER/BLEU scores
computed using human post-edits and external references (“TER (pe+ref)” and “BLEU (pe+ref)”).
In Section 4.2, these ﬁgures are respectively used
to discuss systems’ capability to reﬂect the postediting style of the training data and their tendency
to produce unnecessary corrections of acceptable
MT output. Since external references are available
only for German, this analysis was not feasible for
the English-Russian task.
Automatic metrics computed using
human post-edits
Different from the past, this year the primary
(“TER (pe)”) and secondary evaluation metric
(“BLEU (pe)”) produce slightly different rankings.10
For English-German, system results are
quite close to each other, up to the point that i)
TER differences between the top eight submissions are not statistically signiﬁcant and ii) all the
10The correlation between the ranks obtained by the two
metrics is 0.97 for the English-German subtask and 0.7 for
the English-Russian subtask.
submissions with a TER score equal or lower than
the baseline are concentrated in a performance interval of less than 0.8 TER points and less than 1.2
BLEU points. This compression can contribute
to explain the ranking differences, especially at
higher ranks where discriminating between strong
systems with almost identical performance is particularly difﬁcult. However, for the sake of future analysis or alternative views of this year’s outcomes, it’s worth remarking that the 2nd, 3rd and
5th runs in terms of TER (all by the same team
–POSTECH) respectively represent the top three
submissions in terms of BLEU.
For English-Russian, the distance between the
top and the worst submissions is larger, but also in
this case the BLEU-based ranking is not identical
to the TER-based one. Though with a negligible
margin, the worst run in terms of TER would rank
2nd in terms of BLEU.
English-German subtask.
In order to measure
the progress with respect to last year’s round of the
APE task, for this language pair the evaluation has
been performed with the same data used for the
NMT subtask in 2018. Last year, the majority of
the participants’ scores fell in a range of less than
one TER/BLEU point improvement over the donothing baseline (16.84 TER, 74.73 BLEU), being 16.46 TER (-0.38) and 75.53 BLEU (+0.8)
the scores and the corresponding quality gains
achieved by the top submission. This year, eight
submissions achieved a TER reduction larger than
0.4 points and a BLEU increase of more than 0.9
points. The top submission, in particular, obtained
improvements up to -0.78 TER and +1.23 BLEU
points over the baseline. Although correcting the
output of a neural MT system still proves to be
quite hard, we take the fact that 4 teams achieved
better results than last year’s winning system as an
indicator of technology advancements.
English-Russian subtask.
This subtask proved
to be more challenging compared to the English-
German subtask. Final results are indeed much
worse: none of the four runs submitted by the
two participating teams was able to beat the donothing baseline (16.16 TER, 76.2 BLEU). Even
for the top submission (16.59 TER, 75.27 BLEU),
results’ difference with respect to the baseline is
statistically signiﬁcant. One possible cause of the
higher difﬁculty of the English-Russian subtask is
the fact that dealing with a morphology-rich lan-
UNBABEL Primary
POSTECH Primary
POSTECH Contrastive (var2Ens8)
USAAR DFKI Primary
POSTECH Contrastive (top1Ens4)
UNBABEL Contrastive (2)
UNBABEL Contrastive (1)
FBK Primary
FBK Contrastive
UDS Primary
IC USFD Contrastive
UDS Contrastive (Gaus)
UDS Contrastive (Uni)
IC USFD Primary
ADAPT DCU Contrastive (SMT)
ADAPT DCU Primary
USAAR DFKI Contrastive
ADAPT DCU Contrastive (LEN)
Table 4: Results for the WMT19 APE English-German subtask – average TER (↓), BLEU score (↑). The symbol “*” indicates
results differences between runs that are not statistically signiﬁcant. The symbol “†” indicates a difference from the MT baseline
that is not statistically signiﬁcant.
ADAPT DCU Contrastive
ADAPT DCU Primary
FBK Primary
FBK Contrastive
Table 5: Results for the WMT19 APE English-Russian subtask – average TER (↓), BLEU score (↑).
guage like Russian is problematic not only for MT
but also from the APE standpoint. Under similar
data conditions (the training sets of the two subtasks differ by ∼1,650 instances), the training set
of a morphology-rich language is likely to be more
sparse compared to other languages.
possible explanation lies in the higher quality of
the original translations (our second complexity
indicator discussed in Section 2.1.2), which reduces the room for improvement with APE and,
at the same time, increases the possibility to damage MT output that is already correct. From the
MT quality point of view, according to the baseline results shown in Table 2, the English-Russian
dataset used for this year’s campaign is the second
most difﬁcult benchmark released in ﬁve rounds
of the APE task.
Also the TER distribution of
the test set instances (our third complexity indicator discussed in Section 2.1.3) indicates the higher
difﬁculty of the task, which is characterized by
the highest number of perfect translations across
the ﬁve rounds of the APE shared task (61.4%).
In terms of repetition rate, as observed in Section 2.1.1, English-Russian data considerably differ from those released for the previous rounds of
the task. The much larger values shown in Table
2 are not surprising considering that this material
is drawn from Microsoft Ofﬁce localization data
that mainly consist of short segments (e.g. menu
commands), which are likely produced based on
standardized guidelines. However, also this year
text repetitiveness seems to have a smaller inﬂuence on ﬁnal performance compared to quality of
the initial translations. Besides all these elements,
the higher difﬁculty of the English-Russian subtask is also indirectly suggested by the low number
of participants. Likely, poor results observed on
the development set during system development
(i.e. the difﬁculty to beat the do-nothing baseline)
discouraged other potential participants.
Automatic metrics computed using
external references
By learning from (SRC, TGT, PE) triplets, APE
systems’ goal is to perform a “monolingual translation” from raw MT output into its correct version. In this translation process, the same sentence
can be corrected in many possible ways that make
the space of possible valid outputs potentially very
Ideally, from this space, APE systems
should select solutions that reﬂect as much as possible the post-editing style of the training data (in
real-use settings, this can be the style/lexicon of
speciﬁc users, companies, etc.). However, nothing prevents to end up with outputs that partially
satisfy this constraint. In light of these considerations, TER and BLEU scores computed using
human post-edits as reference represent a reliable
measure of quality but:
1. They provide us with partial information on
how systems’ output reﬂects the post-editing
style of the training data;
2. They are not informative at all about the
amount of valid corrections that are not
present in the human post-edits.
In order to shed light on these aspects, in previous rounds of the task, further analysis was performed by taking advantage of reference translations. In continuity with the past, in Sections
4.2.1 and 4.2.2 we re-propose this analysis for the
English-German subtask, the only one for which
external references are available.
Output style
To gain further insights on point 1.
system’s capability to learn the post-editing style of
the training data), the “TER (ref)” and “BLEU
(ref)” columns in Table 4 show the TER and
BLEU scores computed against independent reference translations. The rational behind their computation is that differences in TER/BLEU(pe) and
TER/BLEU(ref) can be used as indicators of the
“direction” taken by the trained models (i.e. either towards humans’ post-editing style or towards a generic improvement of the MT output).
Since independent references are usually very different from conservative human post-edits of the
same TGT sentences, all the TER/BLEU scores
measured using independent references are expected to be worse.
However, if our hypothesis holds true, visible differences in the baseline
improvements measured with TER/BLEU(pe) and
TER/BLEU(ref) should indicate system’s ability
to model the post-editing style of the training
In particular, larger gains measured with
TER/BLEU(pe) will be associated to this desired
As can be seen in Table 4, systems’ results
on English-German show this tendency.
Looking at the improvements over the baseline, those
measured by computing TER and BLEU scores
against human post-edits (i.e. TER/BLEU(pe)) are
often larger than those computed against independent references (i.e. TER/BLEU(ref)). In terms
of TER, this holds true for most of the submitted
runs, with the best system that shows a difference
of 0.2 TER points in the gains over the baseline
computed with TER(pe) (-0.78) and those computed with TER(ref) (-0.58). On average, for the
runs achieving improvements over the baseline,
the difference in the gains over the baseline computed with TER(pe) and TER(ref) is respectively
-0.41 and -0.08. In terms of BLEU, the differences are more visible. The best system improves
over the baseline by 1.23 points with BLEU(pe)
and 0.75 points with BLEU(ref), while the average difference in the gains over the baseline is 0.8
with BLEU(pe) and 0.2 with BLEU (ref).
larger (0.32/0.6) average improvements over the
baseline observed with TER/BLEU computations
against human post-edits can be explained by systems’ tendency to reﬂect the post-editing style of
the training data.
Over-corrections
To shed light on point 2.
system’s tendency to produce unnecessary corrections of acceptable MT output), the “TER (pe+ref)” and
“BLEU (pe+ref)” columns in Table 4 show the
multi-reference TER and BLEU scores computed against post-edits and independent references.
The rational behind their computation is that differences in TER/BLEU(pe) and
TER/BLEU(pe+ref) can be used to analyze the
quality of the unnecessary corrections performed
by the systems (or, in other words, to study
the impact of systems’ tendency towards “over-
correction”). APE corrections of a given MT output can indeed be of different types, namely: i)
correct edits of a wrong passage, ii) wrong edits of a wrong passage, iii) correct edits of a correct passage and iv) wrong edits of a correct passage.
TER/BLEU scores computed against human post-edits work reasonably well in capturing cases i)-ii) by matching APE systems’ output with human post-edits: for wrong MT output
passages (i.e. those changed by the post-editor),
they inform us about the general quality of automatic corrections (i.e. how close they are to the
post-editor’s actions). Cases iii)-iv), in contrast,
are more problematic since any change performed
by the system to a correct passage (i.e.
that were not changed by the post-editor) will always be penalized by automatic comparisons with
human post-edits.
Although discriminating between the two types of unnecessary corrections is
hard, we hypothesize that a comparison between
TER/BLEU(pe) and TER/BLEU(pe+ref) can be
used as a proxy to quantify those belonging to type
iii). In general, due to the possibility to match
more and longer n-grams in a multi-reference setting, TER/BLEU(pe+ref) scores are expected to be
higher than TER/BLEU(pe) scores. However, if
our hypothesis holds true, visible differences in the
increase observed for the baseline and for the systems should indicate systems’ tendency to produce
acceptable over-corrections (type iii)). In particular, larger gains observed for the APE systems will
be associated to their over-correction tendency towards potentially acceptable edits that should not
be penalized by automatic evaluation metrics.
As expected, Table 4 shows that, on English-
multi-reference
evaluation
references
(TER/BLEU(pe+ref))
compared to single reference evaluation with
post-edits only (TER/BLEU(pe)). The variations
of the do-nothing baseline are -0.57 TER (from
16.84 to 16.27) and 2.1 BLEU (from 74.73 to
76.83) points. In contrast, systems’ scores vary by
-0.46 TER and +2.01 BLEU points on average.
In comparison with the larger variation observed
for the baseline, this indicates that, for most of the
submissions, the multi-reference evaluation does
not indicate a tendency to produce unnecessary
but acceptable corrections.
On a positive note,
while last year this was true for all the systems,
this year four runs perform slightly better than
the baseline in terms of BLEU(pe+ref). Though
minimal, these differences suggest that a certain
amount of corrections made by the top systems
still represent acceptable modiﬁcations of the
original translations.
System/performance analysis
As a complement to global TER/BLEU scores,
also this year we performed a more ﬁne-grained
analysis of the changes made by each system to
the test instances.
Macro indicators: modiﬁed, improved
and deteriorated sentences
Tables 6 and 7 show the number of modiﬁed, improved and deteriorated sentences, respectively for
the English-German and the English-Russian subtasks. It’s worth noting that, as in the previous
rounds and in both the settings, the number of sentences modiﬁed by each system is higher than the
sum of the improved and the deteriorated ones.
This difference is represented by modiﬁed sentences for which the corrections do not yield TER
variations. This grey area, for which quality improvement/degradation can not be automatically
assessed, contributes to motivate the human evaluation discussed in Section 6.
English-German subtask.
As shown in table 6,
the amount of sentences modiﬁed by the participating systems varies considerably. With values
ranging from 4.01% to 39.1%, the average proportion of modiﬁcations (23.53%) is lower compared to last year (32.7%). Considering that about
25.2% (i.e. 257) of the test instances are to be considered as “perfect” (see Figure1), also this year
the reported numbers are, for most of the submissions, far below the target percentage of modiﬁcations (74.8%). Overall, system’s aggressiveness
does not correlate with the ﬁnal ranking: among
both the top ranked systems and those with lower
performance, large differences in the proportion of
modiﬁed sentences can be observed. Indeed, as
expected, what makes the difference is system’s
precision (i.e.
the proportion of improved sentences out of the total amount of modiﬁed test
items). Overall, the average precision is 45.92%,
which represents a signiﬁcant increase from last
year’s value (34.3%). While in 2018 none of the
systems showed a precision higher than 50.0%,
this year seven runs are above this value. As a
Deteriorated
UNBABEL Primary
366 (35.78%)
187 (51.09%)
110 (30.05%)
POSTECH Primary
207 (20.23%)
127 (61.35%)
41 (19.81%)
POSTECH Contrastive (var2Ens8)
210 (20.53%)
125 (59.52%)
45 (21.43%)
USAAR DFKI Primary
301 (29.42%)
157 (52.16%)
83 (27.57%)
POSTECH Contrastive (top1Ens4)
213 (20.82%)
125 (58.69%)
47 (22.07%)
UNBABEL Contrastive (2)
400 (39.1%)
202 (50.50%)
121 (30.25%)
UNBABEL Contrastive (1)
393 (38.42%)
195 (49.62%)
117 (29.77%)
FBK Primary
200 (19.55%)
115 (57.50%)
50 (25.00%)
FBK Contrastive
363 (35.48%)
164 (45.18%)
131 (36.09%)
UDS Primary
96 (9.38%)
42 (43.75%)
36 (37.50%)
IC USFD Contrastive
41 (4.01%)
21 (51.22%)
16 (39.02%)
UDS Contrastive (Gaus)
125 (12.22%)
54 (43.20%)
51 (40.80%)
UDS Contrastive (Uni)
112 (10.95%)
49 (43.75%)
41 (36.61%)
IC USFD Primary
72 (7.04%)
29 (40.28%)
35 (48.61%)
ADAPT DCU Contrastive (SMT)
120 (11.73%)
29 (24.17%)
61 (50.83%)
ADAPT DCU Primary
368 (35.97%)
116 (31.52%)
169 (45.92%)
USAAR DFKI Contrastive
391 (38.22%)
135 (34.53%)
168 (42.97%)
ADAPT DCU Contrastive (LEN)
354 (34.60%)
101 (28.53%)
169 (47.74%)
Table 6: Number of test sentences modiﬁed, improved and deteriorated by each run submitted to the English-German subtask.
Deteriorated
ADAPT DCU Contrastive
92 (8.99%)
17 (18.48%)
49 (53.26%)
ADAPT DCU Primary
245 (23.95%)
57 (23.27%)
130 (53.06%)
FBK Primary
147 (14.37%)
49 (33.33%)
67 (45.58%)
FBK Contrastive
26 (2.54%)
5 (19.23%)
18 (69.23%)
Table 7: Number of test sentences modiﬁed, improved and deteriorated by each run submitted to the English-Russian subtask.
consequence, the percentage of deteriorated sentences out of the total amount of modiﬁed test
items shows a signiﬁcant drop.
On average, a
quality decrease is observed for 35.11% of the test
items, while last year the average was 47.85%.
English-Russian subtask.
As shown in table 7,
also in this subtask the amount of sentences modiﬁed by the submitted systems varies considerably
and does not correlate with systems’ ranking. On
average, the proportion of modiﬁcations is 12.46%
(much less compared to the English-German subtask). With values ranging from 2.54% to 23.95%,
all the four runs are far from the expected value of
38.6% modiﬁcations (recall that 61.4% of the test
items are perfect translations). Systems’ precision
is also lower compared to the English-German
The average proportion of improved sentences is 23.58%, while the deteriorated ones are
on average 55.28%, thus conﬁrming the higher
difﬁculty of the English-Russian evaluation setting.
Overall, the analysis conﬁrms that correcting high-quality translations still remains a hard
task, especially when dealing with higher-quality
English-Russian outputs. On one side, systems’
low precision is an evident limitation. On the other
side, one possible explanation is that the margins
of improvement to the input sentences are reduced
to types of errors (e.g. lexical choice) on which
APE systems are less reliable. The analysis proposed in Section 5.2 aims to explore also this aspect.
Micro indicators: edit operations
In previous rounds of the APE task, the possible differences in the way systems corrected the
test set instances were analyzed by looking at
the distribution of the edit operations done by
each system (insertions, deletions, substitutions
and shifts).
Such distribution was obtained by
computing the TER between the original MT output and the output of each system taken as reference (only for the primary submissions). This
analysis has been performed also this year but it
turned out to be scarcely informative, as shown in
Figure 3: System behaviour (primary submissions) for English-German (a) and English-Russian (b) – TER(MT, APE)
For both the subtasks, the differences in system’s behaviour are indeed barely visible, mainly
due to the fact that, in most of the cases, the
submitted neural APE models implement similar solutions (multi-source, Transformer-based
models trained with the same in-domain and artiﬁcial corpora).
All the submitted runs are
characterized by a large number of substitutions (on average, 53.6% for English-German
and 59.7% for English-Russian), followed by the
deletions (22.6% for English-German and 26.4%
for English-Russian), the insertions (respectively
16.3% and 9.4%) and ﬁnally the shifts (7.4% and
4.5%). These results are in line with previous ﬁndings. Also in 2018, for instance, the high ﬂuency
of neural translations induced the trained models
to perform few reordering operations leaving lexical choice as a main direction of improvement,
as suggested by the larger amount of substitutions
performed by all the systems.
Human evaluation
In order to complement the automatic evaluation
of APE submissions, a manual evaluation of the
primary systems submitted (seven for English-
German, ﬁve for English-Russian) was conducted.
Similarly to the manual evaluation carried out for
last year APE shared task, it was based on the
direct assessment (DA) approach . In this Section, we
present the evaluation procedure as well as the results obtained.
Evaluation procedure
The manual evaluation carried out this year involved 32 native German speakers with full professional proﬁciency in English. All annotators
were paid consultants, sourced by a linguistic service provider company. Each evaluator had experience with the evaluation task through previous
work using the same evaluation platform in order
to be familiar with the user interface and its functionalities. A screenshot of the evaluation interface is presented in Figure 4.
We measure post-editing quality using sourcebased direct assessment (src-DA), as implemented
in Appraise . Scores are collected as x ∈ , focusing on adequacy (and
not ﬂuency, which previous WMT evaluation campaigns have found to be highly correlated with adequacy direct assessment results).
The original DA approach is reference-based and, thus,
needs to be adapted for use in our paraphrase assessment and translation scoring scenarios.
course, this makes translation evaluation more dif-
ﬁcult, as we require bilingual annotators. Src-DA
has previously been used, e.g., in .
Direct assessment initializes mental context for
annotators by asking a priming question. The user
interface shows two sentences:
- the source (src-DA, reference otherwise); and
- the candidate output.
Annotators read the priming question and both
sentences and then assign a score x ∈ to
the candidate shown.
The interpretation of this
score considers the context deﬁned by the priming
question, effectively allowing us to use the same
annotation method to collect assessments wrt. the
different dimensions of quality as deﬁned above.
Our priming questions are shown in Table 8.
Figure 4: Screenshot of the direct assessment user interface.
Priming question used
Post-editing
How accurately does the above candidate text convey the original semantics of the source text?
Slider ranges from Not at all (left) to Perfectly (right).
Table 8: Priming question used for human evaluation of post-editing adequacy.
For adequacy, we ask annotators to assess semantic similarity between source and candidate
text, labeled as “source text” and “candidate translation”, respectively. The annotation interface implements a slider widget to encode perceived similarity as a value x ∈ . Note that the exact value is hidden from the human, and can only
be guessed based on the positioning of the slider.
Candidates are displayed in random order, preventing bias.
evaluation
post-editing
(test.tok.pe)
translation
(test.tok.nmt).
We expect human post-editing to be of higher
quality than output from automatic post-editing
submissions, which in turn should outperform
unedited, neural machine translation output.
Human Evaluation results
English-German subtask.
Score convergence
over time for English-German assessments is presented in Figure 5. This ﬁgure tracks average system adequacy (as measured by Src-DA) over time,
as assessments come in from human annotators.
Note that we use the so-called alternate HIT layout as named in the WMT18 ﬁndings paper, using an 88:12 split between actual assessments and
those reserved for quality control. All annotators
have proven reliable, passing qualiﬁcation tests.
The results of Src-DA for the English-German
subtask are presented in Table 9. Our main ﬁndings are as follows:
• Human post-editing outperforms all auotmatic post-editing systems, the quality difference is signiﬁcant;
• UNBABEL achieves best APE performance;
• USAAR DFKI comes in second;
• POSTECH comes in third;
outperform
unedited NMT output;
• Difference to the remaining APE system is
not statistically signiﬁcant.
Human evaluation does only result in very
coarse result cluster. Thus, in order to order submissions by their respective post-editing quality,
as perceived by human annotators, we also present
win-based results in Table 10.
English-Russian subtask.
For 2019, we did not
run any human evaluation for the English-Russian
subtask, due to lack of participation. Instead, we
focused annotation efforts on English-German.
Figure 5: Score convergence over time for English-German assessments.
Human post-edit
USAAR DFKI
NMT output
DA Human evaluation results for the English-
German subtask in terms of average raw DA (Ave %) and
average standardized scores (Ave z). Dashed lines between
systems indicate clusters according to Wilcoxon signed-rank
test at p-level p ≤0.05.
Conclusion
We presented the results from the ﬁfth shared task
on Automatic Post-Editing.
This year, we proposed two subtasks in which the neural MT output to be corrected was respectively generated by
an English-German system and by an English-
Russian system. Both the subtasks dealt with data
drawn from the information technology domain.
Seven teams participated in the English-German
task, with a total of 18 submitted runs, while
two teams participated in the English-Russian task
submitting two runs each.
Except in one case
Human post-edit
USAAR DFKI
NMT output
Table 10: DA Human evaluation results for the English-
German subtask in terms of average raw DA (Ave %) and
average standardized scores (Ave z). Dashed lines between
systems indicate clusters according to number of wins.
(a contrastive run produced with a phrase-based
system), the submissions are based on neural approaches, which conﬁrm to be the state-of-the-art
in APE. Most of them rely on multi-source models built upon the Transformer and trained by taking advantage of the synthetic corpora released as
additional training material.
For the English-German subtask the evaluation
was carried out on the same test set used last
year, whose human post-edits were not released
for the sake of future comparisons. The results on
these data, indicate further technology improvements with respect to the 2018 round.
shown by: i) the top result (-0.78 TER and +1.23
BLEU points over the baseline), which is significantly better than last year (-0.38 TER and +0.8
BLEU), and ii) the fact that four teams achieved
higher results than last year’s winning system.
The newly proposed English-Russian subtask
proved to be more challenging. None of the submitted runs was able to beat the baseline, whose
high TER (16.16) and BLEU (76.2) indicate a very
high quality of the initial translations. This is also
conﬁrmed by the very skewed TER distribution of
the test set items. With more than 60.0% of the
translations with TER=0 (the highest value across
all the APE datasets released so far), the chance of
damaging a perfect MT output is extremely high.
Despite the high repetition rate of the English-
Russian data (also in this case, the highest across
all datasets), the difﬁculty of handling such a high
level of quality contributes to explain the lower results achieved by the two participating teams.
Overall, also this year the main open problem
remains to mitigate systems’ tendency towards
over-correction.
In light of the steady progress
of NMT technology, handling increasingly better
translations calls for conservative and precise solutions able to avoid the unnecessary modiﬁcation
of correct MT output.
Acknowledgments
We would like to thank Apple and Microsoft for
their support and sponsorship in organizing the
2019 APE shared task. We are also particularly
grateful to the anonymous Appraise annotators
that contributed their human intelligence to this