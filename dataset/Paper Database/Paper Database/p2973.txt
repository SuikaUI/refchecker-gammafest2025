Published as a conference paper at ICLR 2016
MULTI-SCALE
PREDICTION
MEAN SQUARE ERROR
Michael Mathieu1, 2, Camille Couprie2 & Yann LeCun1, 2
1New York University
2Facebook Artiﬁcial Intelligence Research
 , {coupriec,yann}@fb.com
Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and
therefore, to some degree, its content and dynamics. This is why pixel-space
video prediction may be viewed as a promising avenue for unsupervised feature
learning. In addition, while optical ﬂow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still,
many vision applications could beneﬁt from the knowledge of the next frames of
videos, that does not require the complexity of tracking every pixel trajectory. In
this work, we train a convolutional network to generate future frames given an
input sequence. To deal with the inherently blurry predictions obtained from the
standard Mean Squared Error (MSE) loss function, we propose three different and
complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare
our predictions to different published results based on recurrent neural networks
on the UCF101 dataset.
INTRODUCTION
Unsupervised feature learning of video representations is a promising direction of research because
the resources are quasi-unlimited and the progress remaining to achieve in this area are quite important. In this paper, we address the problem of frame prediction. A signiﬁcant difference with the
more classical problem of image reconstruction is that the ability
of a model to predict future frames requires to build accurate, non trivial internal representations,
even in the absence of other constraints (such as sparsity). Therefore, we postulate that the better the
predictions of such system are, the better the feature representation should be. Indeed, the work of
Srivastava et al. demonstrates that learning representations by predicting the next sequence
of image features improves classiﬁcation results on two action recognition datasets. In this work,
however, we focus on predicting directly in pixel space and try to address the inherent problems
related to this approach.
Top performing algorithms for action recognition exploit the temporal information in a supervised
way, such as the 3D convolutional network of Tran et al. , or the spatio-temporal convolutional
model of Simonyan & Zisserman , which can require months of training, and heavily labeled
datasets. This could be reduced using unsupervised learning. The authors in compete with supervised learning performance on ImageNet, by using a siamese architecture
 to mine positive and negative examples from patch triplets of videos in an
unsupervised fashion. Unsupervised learning from video is also exploited in the work of Vondrick
et al. , where a convolutional model is trained to predict sets of future possible actions, or in
 which focuses on learning a feature space equivariant to ego-motion.
Goroshin et al. trained a convolutional network to learn to linearize motion in the code space
and tested it on the NORB dataset. Beside unsupervised learning, a video predictive system may
ﬁnd applications in robotics , video compression and
inpainting to name a few.
 
Published as a conference paper at ICLR 2016
Recently, predicting future video sequences appeared in different settings: Ranzato et al. 
deﬁned a recurrent network architecture inspired from language modeling, predicting the frames in
a discrete space of patch clusters. Srivastava et al. adapted a LSTM model to future frame prediction. Oh et al. deﬁned an action conditional autoencoder model to predict next frames of Atari-like games. In the two works dealing with natural
images, a blur effect is observed in the predictions, due to different causes. In , the transformation back and forth between pixel and clustered spaces involves the averaging
of 64 predictions of overlapping tilings of the image, in order to avoid a blockiness effect in the
result. Short term results from Srivastava et al. are less blurry, however the ℓ2 loss function
inherently produces blurry results. Indeed, using the ℓ2 loss comes from the assumption that the data
is drawn from a Gaussian distribution, and works poorly with multimodal distributions.
In this work, we address the problem of lack of sharpness in the predictions. We assess different loss
functions, show that generative adversarial training 
may be successfully employed for next frame prediction, and ﬁnally introduce a new loss based on
the image gradients, designed to preserve the sharpness of the frames. Combining these two losses
produces the most visually satisfying results.
Our paper is organised as follows: the model section describes the different model architectures:
simple, multi-scale, adversarial, and presents our gradient difference loss function. The experimental section compares the proposed architectures and losses on video sequences from the Sports1m
dataset of Karpathy et al. and UCF101 . We further compare our results
with and . We measure the quality of image generation
by computing similarity and sharpness measures.
Let Y = {Y 1, ..., Y n} be a sequence of frames to predict from input frames X = {X1, ..., Xm} in a
video sequence. Our approach is based on a convolutional network , alternating
convolutions and Rectiﬁed Linear Units (ReLU) .
Figure 1: A basic next frame prediction convnet
feature map
feature map
feature map
feature map
feature map
ReLU conv.
ReLU conv.
ReLU conv.
ReLU conv. Tanh
Such a network G, displayed in Figure 1, can be trained to predict one or several concatenated
frames Y from the concatenated frames X by minimizing a distance, for instance ℓp with p = 1 or
p = 2, between the predicted frame and the true frame:
Lp(X, Y ) = ℓp(G(X), Y ) = ∥G(X) −Y ∥p
However, such a network has at least two major ﬂaws:
1. Convolutions only account for short-range dependencies, limited by the size of their kernels.
However, using pooling would only be part of the solution since the output has to be of the same
resolution as the input. There are a number of ways to avoid the loss of resolution brought about by
pooling/subsampling while preserving long-range dependencies. The simplest and oldest one is to
have no pooling/subsampling but many convolution layers . Another method is to
use connections that “skip” the pooling/unpooling pairs, to preserve the high frequency information
 . Finally, we can combine
multiple scales linearly as in the reconstruction process of a Laplacian pyramid .
This is the approach we use in this paper.
Published as a conference paper at ICLR 2016
2. Using an ℓ2 loss, and to a lesser extent ℓ1, produces blurry predictions, increasingly worse when
predicting further in the future. If the probability distribution for an output pixel has two equally
likely modes v1 and v2, the value vavg = (v1 + v2)/2 minimizes the ℓ2 loss over the data, even if
vavg has very low probability. In the case of an ℓ1 norm, this effect diminishes, but do not disappear,
as the output value would be the median of the set of equally likely values.
MULTI-SCALE NETWORK
We tackle Problem 1 by making the model multi-scale. A multi-scale version of the model is de-
ﬁned as follows: Let s1, . . . , sNscales be the sizes of the inputs of our network. Typically, in our
experiments, we set s1 = 4 × 4, s2 = 8 × 8, s3 = 16 × 16 and s4 = 32 × 32. Let uk be the
upscaling operator toward size sk. Let Xi
k denote the downscaled versions of Xi and Y i of size
sk, and G′
k be a network that learns to predict Yk −uk(Yk−1) from Xk and a coarse guess of Yk.
We recursively deﬁne the network Gk, that makes a prediction ˆYk of size sk, by
ˆYk = Gk(X) = uk( ˆYk−1) + G′
Xk, uk( ˆYk−1)
Therefore, the network makes a series of predictions, starting from the lowest resolution, and uses
the prediction of size sk as a starting point to make the prediction of size sk+1. At the lowest scale
s1, the network takes only X1 as an input. This architecture is illustrated on Figure 2, and the speciﬁc
details are given in Section 3. The set of trainable parameters is denoted WG and the minimization
is performed via Stochastic Gradient Descent (SGD).
Figure 2: Multi-scale architecture
Despite the multi-scale architecture, the search of Y from X without making any assumption on
the space of possible conﬁgurations still leads to blurry predictions, because of Problem 2. In order
to further reduce this effect, the next two sections introduce an adversarial strategy and the image
gradient difference loss.
ADVERSARIAL TRAINING
Generative adversarial networks were introduced by Goodfellow et al. , where images patches
are generated from random noise using two networks trained simultaneously. In that work, the authors propose to use a discriminative network D to estimate the probability that a sample comes
from the dataset instead of being produced by a generative model G. The two models are simultaneously trained so that G learns to generate frames that are hard to classify by D, while D learns to
discriminate the frames generated by G. Ideally, when G is trained, it should not be possible for D
to perform better than chance.
We adapted this approach for the purpose of frame prediction, which constitutes to our knowledge
the ﬁrst application of adversarial training to video prediction. The generative model G is typically
the one described in the previous section. The discriminative model D takes a sequence of frames,
and is trained to predict the probability that the last frames of the sequence are generated by G. Note
only the last frames are either real of generated by G, the rest of the sequence is always from the
dataset. This allows the discriminative model to make use of temporal information, so that G learns
to produce sequences that are temporally coherent with its input. Since G is conditioned on the input
frames X, there is variability in the input of the generator even in the absence of noise, so noise is
Published as a conference paper at ICLR 2016
not a necessity anymore. We trained the network with and without adding noise and did not observe
any difference. The results we present are obtained without random noise.
Our main intuition on why to use an adversarial loss is that it can, theoretically, address the Problem
2 mentioned in Section 2. Imagine a sequence of frames X = (X1, . . . , Xm) for which, in the
dataset, the next frames can either be Y = (Y 1, . . . , Y n) or Y ′ = (Y ′1, . . . , Y ′n), with equal
probability. As explained before, training the network with an ℓ2 loss will result in predicting the
average frames Yavg = (Y + Y ′)/2. However, the sequence (X, Yavg), composed of the frames of
X followed by the frames of Yavg, is not a likely sequence, and D can discriminate them easily. The
only sequences the model D will not be able to classify as fake are (X, Y ) and (X, Y ′).
The discriminative model D is a multi-scale convolutional network with a single scalar output. The
training of the pair (G, D) consists of two alternated steps, described below. For the sake of clarity,
we assume that we use pure SGD (minibatches of size 1), but there is no difﬁculty to generalize the
algorithm to minibatches of size M by summing the losses over the samples.
Training D:
Let (X, Y ) be a sample from the dataset. Note that X (respectively Y ) is a sequence
of m (respectively n) frames. We train D to classify the input (X, Y ) into class 1 and the input
(X, G(X)) into class 0. More precisely, for each scale k, we perform one SGD iteration of Dk
while keeping the weights of G ﬁxed. It is trained with in the target 1 for the datapoint (Xk, Yk),
and the target 0 for (Xk, Gk(Xk)). Therefore, the loss function we use to train D is
adv(X, Y ) =
Lbce(Dk(Xk, Yk), 1) + Lbce(Dk(Xk, Gk(X)), 0)
where Lbce is the binary cross-entropy loss, deﬁned as
Lbce(Y, ˆY ) = −
ˆYi log (Yi) + (1 −ˆYi) log (1 −Yi)
where Yi takes its values in {0, 1} and ˆYi in .
Training G:
Let (X, Y ) be a different data sample. While keeping the weights of D ﬁxed, we
perform one SGD step on G to minimize the adversarial loss:
adv(X, Y )
Lbce(Dk(Xk, Gk(Xk)), 1)
Minimizing this loss means that the generative model G is making the discriminative model D as
“confused” as possible, in the sense that D will not discriminate the prediction correctly. However,
in practice, minimizing this loss alone can lead to instability. G can always generate samples that
“confuse” D, without being close to Y . In turn, D will learn to discriminate these samples, leading
G to generate other “confusing” samples, and so on. To address this problem, we train the generator
with a combined loss composed of the of the adversarial loss and the Lp loss . The generator G
is therefore trained to minimize λadvLG
adv + λℓpLp. There is therefore a tradeoff to adjust, by the
mean of the λadv and λℓp parameters, between sharp predictions due to the adversarial principle,
and similarity with the ground truth brought by the second term. This process is summarized in
Algorithm 1, with minibatches of size M.
IMAGE GRADIENT DIFFERENCE LOSS (GDL)
Another strategy to sharpen the image prediction is to directly penalize the differences of image
gradient predictions in the generative loss function. We deﬁne a new loss function, the Gradient
Difference Loss (GDL), that can be combined with a ℓp and/or adversarial loss function. The GDL
function between the ground truth image Y , and the prediction G(X) = ˆY is given by
Lgdl(X, Y ) = Lgdl( ˆY , Y ) =
|Yi,j −Yi−1,j| −| ˆYi,j −ˆYi−1,j|
|Yi,j−1 −Yi,j| −| ˆYi,j−1 −ˆYi,j|
Published as a conference paper at ICLR 2016
Algorithm 1: Training adversarial networks for next frame generation
Set the learning rates ρD and ρG, and weights λadv, λℓp.
while not converged do
Update the discriminator D:
Get M data samples (X, Y ) = (X(1), Y (1)), . . . , (X(M), Y (M))
WD = WD −ρD
adv(X(i),Y (i))
Update the generator G:
Get M new data samples (X, Y ) = (X(1), Y (1)), . . . , (X(M), Y (M))
WG = WG −ρG
adv(X(i),Y (i))
∂Lℓp(X(i),Y (i))
Table 1: Network architecture (Input: 4 frames – output: 1 frame)
Generative network scales
Number of feature maps
128, 256, 128
128, 256, 128
128, 256, 512, 256, 128
128, 256, 512, 256, 128
Conv. kernel size
3, 3, 3, 3
5, 3, 3, 5
5, 3, 3, 3, 5
7, 5, 5, 5, 5, 7
Adversarial network scales
Number of feature maps
64, 128, 128
128, 256, 256
128, 256, 512, 128
Conv. kernel size (no padding)
7, 7, 5, 5
Fully connected
where α is an integer greater or equal to 1, and |.| denotes the absolute value function. To the best
of our knowledge, the closest related work to this idea is the work of Mahendran & Vedaldi ,
using a total variation regularization to generate images from learned features. Our GDL is fundamentally different: In , the total variation takes only the reconstructed
frame in input, whereas our loss penalises gradient differences between the prediction and the true
output. Second, we chose the simplest possible image gradient by considering the neighbor pixel
intensities differences, rather than adopting a more sophisticated norm on a larger neighborhood, for
the sake of keeping the training time low.
COMBINING LOSSES
In our experiments, we combine the losses previously deﬁned with different weights. The ﬁnal loss
L(X, Y ) = λadvLG
adv(X, Y ) + λℓpLp(X, Y ) + λgdlLgdl(X, Y )
EXPERIMENTS
We now provide a quantitative evaluation of the quality of our video predictions on UCF101 and Sports1m video clips. We train and compare two conﬁgurations: (1) We use 4 input frames to predict one future frame. In order to generate further in
the future, we apply the model recursively by using the newly generated frame as an input. (2)
We use 8 input frames to produce 8 frames simultaneously. This second conﬁguration represents a
signiﬁcantly harder problem and is presented in Appendix.
We use the Sports1m for the training, because most of UCF101 frames only have a very small portion
of the image actually moving, while the rest is just a ﬁxed background. We train our network by
randomly selecting temporal sequences of patches of 32 × 32 pixels after making sure they show
enough movement (quantiﬁed by the ℓ2 difference between the frames). The data patches are ﬁrst
normalized so that their values are comprised between -1 and 1.
Published as a conference paper at ICLR 2016
NETWORK ARCHITECTURE
We present results for several models. Unless otherwise stated, we employed mutliscale architectures. Our baseline models are using ℓ1 and ℓ2 losses. The GDL-ℓ1 (respectively GDL-ℓ2) model is
using a combination of the GDL with α = 1 (respectively α = 2) and p = 1 (respectively p = 2)
loss; the relative weights λgdl and λℓp are both 1. The adversarial (Adv) model uses the adversarial
loss, with p = 2 weighted by λadv = 0.05 and λℓp = 1. Finally, the Adv+GDL model is a combination or the adversarial loss and the GDL, with the same parameters as for Adv with α = 1 and
Generative model training:
The generative model G architecture is presented in Table 1. It
contains padded convolutions interlaced with ReLU non linearities. A Hyperbolic tangent (Tanh) is
added at the end of the model to ensure that the output values are between -1 and 1. The learning rate
ρG starts at 0.04 and is reduced over time to 0.005. The minibatch size is set to 4, or 8 in the case of
the adversarial training, to take advantage of GPU hardware capabilities. We train the network on
small patches, and since it is fully convolutional, we can seamlessly apply it on larger images at test
Adversarial training:
The discriminative model D, also presented in Table 1, uses standard non
padded convolutions followed by fully connected layers and ReLU non linearities. For the largest
scale s4, a 2 × 2 pooling is added after the convolutions.
The network is trained by setting the
learning rate ρD to 0.02.
QUANTITATIVE EVALUATIONS
To evaluate the quality of the image predictions resulting from the different tested systems, we
compute the Peak Signal to Noise Ratio (PSNR) between the true frame Y and the prediction ˆY :
PSNR(Y, ˆY ) = 10 log10
i=0(Yi −ˆYi)2 ,
where max ˆY is the maximum possible value of the image intensities. We also provide the Structural
Similarity Index Measure (SSIM) of Wang et al. . It ranges between -1 and 1, a larger score
meaning a greater similarity between the two images.
To measure the loss of sharpness between the true frame and the prediction, we deﬁne the following
sharpness measure based on the difference of gradients between two images Y and ˆY :
Sharp. diff.(Y, ˆY ) = 10 log10
j |(∇iY + ∇jY ) −(∇i ˆY + ∇j ˆY )|
where ∇iY = |Yi,j −Yi−1,j| and ∇jY = |Yi,j −Yi,j−1|.
Figure 3: Our evaluation of the accuracy of future frames prediction only takes the moving areas of
the images into account. Left: example of our frame predictions in a entire image with ground truth;
Right: images masked with thresholded optical ﬂow.
As for the other measures, a larger score is better. These quantitative measures on 378 test videos
from UCF1011 are given in Table 2. As it is trivial to predict pixel values in static areas, especially on
1We extracted from the test set list video ﬁles every 10 videos, starting at 1, 11, 21 etc.
Published as a conference paper at ICLR 2016
Table 2: Comparison of the accuracy of the predictions on 10% of the UCF101 test images. The
different models have been trained given 4 frames to predict the next one. Similarity and sharpness
measures evaluated only in the areas of movement. Our best model has been ﬁne-tuned on UCF101
after the training on Sports1m.
1st frame prediction scores
2nd frame prediction scores
Similarity
Similarity
single sc. ℓ2
Adv+GDL ﬁne-tuned ∗
Last input
Optical ﬂow
∗models ﬁne-tuned on patches of size 64 × 64.
the UCF101 dataset where most of the images are still, we performed our evaluation in the moving
areas as displayed in Figure 3. To this end, we use the EpicFlow method of Revaud et al. ,
and compute the different quality measures only in the areas where the optical ﬂow is higher than
a ﬁxed threshold 2. Similarity and sharpness measures computed on the whole images are given in
The numbers clearly indicate that all strategies perform better than the ℓ2 predictions in terms of
PSNR, SSIM and sharpness. The multi-scale model brings some improvement, but used with an ℓ2
norm, it does not outperform simple frame copy in the moving areas. The ℓ1 model improves the
results, since it replaces the mean by the median value of individual pixel predictions. The GDL and
adversarial predictions are leading to further gains, and ﬁnally the combination of the multi-scale,
ℓ1 norm, GDL and adversarial training achieves the best PSNR, SSIM and Sharpness difference
It is interesting to note that while we showed that the ℓ2 norm was a poor metric for training predictive models, the PSNR at test time is the worst for models trained optimising the ℓ2 norm, although
the PSNR is based on the ℓ2 metric. We also include the baseline presented in Ranzato et al. 
– courtesy of Piotr Dollar – that extrapolates the pixels of the next frame by propagating the optical
ﬂow from the previous ones.
Figure 4 shows results on test sequences from the Sport1m dataset, as movements are more visible
in this dataset.
COMPARISON TO RANZATO ET AL. 
In this section, we compare our results to . To obtain grayscale images, we
make RGB predictions and extract the Y channel of our Adv+GDL model. Ranzato et al. 
images are generated by averaging 64 results obtained using different tiling to avoid a blockiness
effect, however creating instead a blurriness effect. We compare the PSNR and SSIM values on the
ﬁrst predicted images of Figure 5.
2We use default parameters for the Epic Flow computation, and transformed the .ﬂo ﬁle to png using
the Matlab code If
at least one color channel is lower than 0.2 (image color range between 0 and 1), we replace the corresponding
pixel intensity of the output and ground truth to 0, and compute similarity measures in the resulting masked
Published as a conference paper at ICLR 2016
Figure 4: Results on 3 video clips from Sport1m. Training: 4 inputs, 1 output. Second output
computed recursively.
Input frames
Ground truth
GDL ℓ1 result
Adversarial result
Adversarial+GDL result
Input frames
Ground truth
GDL ℓ1 result
Adversarial result
Adversarial+GDL result
Input frames
Ground truth
GDL ℓ1 result
Adversarial result
Adversarial+GDL result
We note that the results of Ranzato et al. appear slightly lighter than our results because of a normalization that does not take place in the original images, therefore the errors given here are not
reﬂecting the full capacity of their approach. We tried to apply the blind deconvolution method of
Krishnan et al. to improve Ranzato et al. and our different results. As expected, the obtained
sharpness scores are higher, but the image similarity measures are deteriorated because often the
contours of the predictions do not match exactly the targets. More importantly, Ranzato et al. results
appear to be more static in moving areas. Visually, the optical ﬂow result appears similar to the
target, but a closer look at thin details reveals that lines, heads of people are bent or squeezed.
CONCLUSION
We provided a benchmark of several strategies for next frame prediction, by evaluating the quality
of the prediction in terms of Peak Signal to Noise Ratio, Structural Similarity Index Measure and
image sharpness. We display our results on small UCF video clips at 
˜mathieu/iclr2016.html. The presented architectures and losses may be used as building
blocks for more sophisticated prediction models, involving memory and recurrence. Unlike most
optical ﬂow algorithms, the model is fully differentiable, so it can be ﬁne-tuned for another task if
necessary. Future work will deal with the evaluation of the classiﬁcation performances of the learned
Published as a conference paper at ICLR 2016
Figure 5: Comparison of results on the Basketball Dunk and Ice Dancing clips from UCF101 appearing in . We display 2 frame predictions for each method along with 2
zooms of each image. The PSNR and SSIM values are computed in the moving areas of the images
(More than the 2/3 of the pixels in these examples). The values in parenthesis correspond to the
second frame predictions measures.
Prediction using a constant optical ﬂow
PSNR = 25.4 (18.9), SSIM = 0.88 (0.56)
Ranzato et al. result
Adv GDL ℓ1 result
PSNR = 16.3 (15.1), SSIM = 0.70 (0.55)
PSNR = 26.7 (19.0), SSIM = 0.89 (0.59)
Prediction using a constant optical ﬂow
PSNR = 24.7 (20.6), SSIM = 0.84 (0.72)
Ranzato et al. result
Adv GDL ℓ1 result
PSNR = 20.1 (17.8), SSIM = 0.72 (0.65)
PSNR = 24.6 (20.5), SSIM = 0.81 (0.69)
representations in a weakly supervised context, for instance on the UCF101 dataset. Another extension of this work could be the combination of the current system with optical ﬂow predictions.
Alternatively, we could replace optical ﬂow predictions in applications that does not explicitly re-
Published as a conference paper at ICLR 2016
quire optical ﬂow but rather next frame predictions. A simple example is causal (where the next
frame is unknown) segmentation of video streams.
ACKNOWLEDGMENTS
We thank Florent Perronnin for fruitful discussions, and Nitish Srivastava, Marc’Aurelio Ranzato
and Piotr Doll´ar for providing us their results on some video sequences.