HAL Id: hal-03541297
 
Submitted on 24 Jan 2022
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
VICReg: Variance-Invariance-Covariance Regularization
For Self-Supervised Learning
Adrien Bardes, Jean Ponce, Yann Lecun
To cite this version:
Adrien Bardes, Jean Ponce, Yann Lecun. VICReg: Variance-Invariance-Covariance Regularization
For Self-Supervised Learning. ICLR 2022 - International Conference on Learning Representations,
Apr 2022, Online, United States. ￿hal-03541297￿
Published as a conference paper at ICLR 2022
VICREG: VARIANCE-INVARIANCE-COVARIANCE RE-
GULARIZATION FOR SELF-SUPERVISED LEARNING
Adrien Bardes1,2
Jean Ponce2,4
Yann LeCun1,3,4
1Facebook AI Research
2Inria, École normale supérieure, CNRS, PSL Research University
3Courant Institute, New York University
4Center for Data Science, New York University
Recent self-supervised methods for image representation learning maximize the
agreement between embedding vectors produced by encoders fed with different
views of the same image. The main challenge is to prevent a collapse in which
the encoders produce constant or non-informative vectors. We introduce VICReg
(Variance-Invariance-Covariance Regularization), a method that explicitly avoids
the collapse problem with two regularizations terms applied to both embeddings
separately: (1) a term that maintains the variance of each embedding dimension
above a threshold, (2) a term that decorrelates each pair of variables. Unlike
most other approaches to the same problem, VICReg does not require techniques
such as: weight sharing between the branches, batch normalization, feature-wise
normalization, output quantization, stop gradient, memory banks, etc., and achieves
results on par with the state of the art on several downstream tasks. In addition, we
show that our variance regularization term stabilizes the training of other methods
and leads to performance improvements.
INTRODUCTION
Self-supervised representation learning has made signiﬁcant progress over the last years, almost
reaching the performance of supervised baselines on many downstream tasks Bachman et al. ;
Misra & Maaten ; He et al. ; Tian et al. ; Caron et al. ; Grill et al. ;
Chen & He ; Gidaris et al. ; Zbontar et al. . Several recent approaches rely on
a joint embedding architecture in which two networks are trained to produce similar embeddings
for different views of the same image. A popular instance is the Siamese network architecture
Bromley et al. , where the two networks share the same weights. The main challenge with
joint embedding architectures is to prevent a collapse in which the two branches ignore the inputs and
produce identical and constant output vectors. There are two main approaches to preventing collapse:
contrastive methods and information maximization methods. Contrastive Bromley et al. ;
Chopra et al. ; He et al. ; Hjelm et al. ; Chen et al. methods tend to be
costly, require large batch sizes or memory banks, and use a loss that explicitly pushes the embeddings
of dissimilar images away from each other. They often require a mining procedure to search for
offending dissimilar samples from a memory bank He et al. or from the current batch Chen
et al. . Quantization-based approaches Caron et al. force the embeddings of
different samples to belong to different clusters on the unit sphere. Collapse is prevented by ensuring
that the assignment of samples to clusters is as uniform as possible. A similarity term encourages
the cluster assignment score vectors from the two branches to be similar. More recently, a few
methods have appeared that do not rely on contrastive samples or vector quantization, yet produce
high-quality representations, for example BYOL Grill et al. and SimSiam Chen & He .
They exploit several tricks: batch-wise or feature-wise normalization, a "momentum encoder" in
which the parameter vector of one branch is a low-pass-ﬁltered version of the parameter vector of the
other branch Grill et al. ; Richemond et al. , or a stop-gradient operation in one of the
branches Chen & He . The dynamics of learning in these methods, and how they avoid collapse,
is not fully understood, although theoretical and empirical studies point to the crucial importance
of batch-wise or feature-wise normalization Richemond et al. ; Tian et al. . Finally, an
Published as a conference paper at ICLR 2022
: maintain variance
: bring covariance to zero
: minimize distance
: distribution of transformations
: random transformations
: encoders
: expanders
: batch of images
: batches of views
: batches of representations
: batches of embeddings
Figure 1: VICReg: joint embedding architecture with variance, invariance and covariance
regularization. Given a batch of images I, two batches of different views X and X′ are produced
and are then encoded into representations Y and Y ′. The representations are fed to an expander
producing the embeddings Z and Z′. The distance between two embeddings from the same image is
minimized, the variance of each embedding variable over a batch is maintained above a threshold, and
the covariance between pairs of embedding variables over a batch are attracted to zero, decorrelating
the variables from each other. Although the two branches do not require identical architectures nor
share weights, in most of our experiments, they are Siamese with shared weights: the encoders are
ResNet-50 backbones with output dimension 2048. The expanders have 3 fully-connected layers of
size 8192.
alternative class of collapse prevention methods relies on maximizing the information content of
the embedding Zbontar et al. ; Ermolov et al. . These methods prevent informational
collapse by decorrelating every pair of variables of the embedding vectors. This indirectly maximizes
the information content of the embedding vectors. The Barlow Twins method drives the normalized
cross-correlation matrix of the two embeddings towards the identity Zbontar et al. , while the
Whitening-MSE method whitens and spreads out the embedding vectors on the unit sphere Ermolov
et al. .
VICREG: INTUITION
We introduce VICReg (Variance-Invariance-Covariance Regularization), a self-supervised method for
training joint embedding architectures based on the principle of preserving the information content of
the embeddings. The basic idea is to use a loss function with three terms:
• Invariance: the mean square distance between the embedding vectors.
• Variance: a hinge loss to maintain the standard deviation (over a batch) of each variable of
the embedding above a given threshold. This term forces the embedding vectors of samples
within a batch to be different.
• Covariance: a term that attracts the covariances (over a batch) between every pair of
(centered) embedding variables towards zero. This term decorrelates the variables of each
embedding and prevents an informational collapse in which the variables would vary
together or be highly correlated.
Variance and Covariance terms are applied to both branches of the architecture separately, thereby
preserving the information content of each embedding at a certain level and preventing informational
collapse independently for the two branches. The main contribution of this paper is the Variance
preservation term, which explicitly prevents a collapse due to a shrinkage of the embedding vectors
towards zero. The Covariance criterion is borrowed from the Barlow Twins method and prevents
informational collapse due to redundancy between the embedding variables Zbontar et al. .
VICReg is more generally applicable than most of the aforementioned methods because of fewer
constraints on the architecture. In particular, VICReg:
• does not require that the weights of the two branches be shared, not that the architectures be
identical, nor that the inputs be of the same nature;
Published as a conference paper at ICLR 2022
• does not require a memory bank, nor contrastive samples, nor a large batch size;
• does not require batch-wise nor feature-wise normalization; and
• does not require vector quantization nor a predictor module.
Other methods require asymmetric stop gradient operations, as in SimSiam Chen & He ,
weight sharing between the two branches as in classical Siamese nets, or weight sharing through
exponential moving average dampening with stop gradient in one branch, as in BYOL and MoCo He
et al. ; Grill et al. ; Chen et al. , large batches of contrastive samples, as in
SimCLR Chen et al. , or batch-wise and/or feature-wise normalization Caron et al. ;
Grill et al. ; Chen & He ; Zbontar et al. ; Ermolov et al. . One of the
most interesting feature of VICReg is the fact that the two branches are not required to share the
same parameters, architecture, or input modality. This opens the door to the use of non-contrastive
self-supervised joint-embedding for multi-modal signals, such as video and audio. We demonstrate
the effectiveness of the proposed approach by evaluating the representations learned with VICReg on
several downstream image recognition tasks including linear head and semi-supervised evaluation
protocols for image classiﬁcation on ImageNet Deng et al. , and other classiﬁcation, detection, instance segmentation, and retrieval tasks. Furthermore, we show that incorporating variance
preservation into other self-supervised joint-embedding methods yields better training stability and
performance improvement on downstream tasks. More generally, we show that VICReg is an explicit
and effective, yet simple method for preventing collapse in self-supervised joint-embedding learning.
RELATED WORK
Contrastive learning. In contrastive SSL methods applied to joint embedding architectures, the
output embeddings for a sample and its distorted version are brought close to each other, while
other samples and their distortions are pushed away. The method is most often applied to Siamese
architectures in which the two branches have identical architectures and share weights Misra &
Maaten ; He et al. ; Bromley et al. ; Hjelm et al. ; Chen et al. ;
Hadsell et al. ; Ye et al. ; Wu et al. ; van den Oord et al. ; Chen et al.
 . Many authors use the InfoNCE loss van den Oord et al. in which the repulsive force
is larger for contrastive samples that are closer to the reference. While these methods yield good
performance, they require large amounts of contrastive pairs in order to work well. These contrastive
pairs can be sampled from a memory bank as in MoCo He et al. , or given by the current batch
of data as in SimCLR Chen et al. , with a signiﬁcant memory footprint. This downside of
contrastive methods motivates a search for alternatives.
Clustering methods. Instead of viewing each sample as its own class, clustering-based methods
group them into clusters based on some similarity measure Caron et al. ; Bautista et al.
 ; Yang et al. ; Xie et al. ; Huang et al. ; Zhuang et al. ; Caron
et al. ; Asano et al. ; Yan et al. . DeepCluster Caron et al. uses k-means
assignments of representations from previous iterations as pseudo-labels for the new representations,
which requires an expensive clustering phase done asynchronously, and makes the method hard
to scale up. SwAV Caron et al. mitigates this issue by learning the clusters online while
maintaining a balanced partition of the assignments through the Sinkhorn-Knopp transform Cuturi
 . These clustering approaches can be viewed as contrastive learning at the level of clusters
which still requires a lot of negative comparisons to work well.
Distillation methods. Recent proposals such as BYOL, SimSiam, OBoW and variants Grill et al.
 ; Chen & He ; Gidaris et al. ; Richemond et al. ; Gidaris et al. have
shown that collapse can be avoided by using architectural tricks inspired by knowledge distillation
Hinton et al. . These methods train a student network to predict the representations of a
teacher network, for which the weights are a running average of the student network’s weights Grill
et al. , or are shared with the student network, but no gradient is back-propagated through
the teacher Chen & He . These methods are effective, but there is no clear understanding
of why and how they avoid collapse. Alternatively, the images can be represented as bags of word
over a dictionary of visual features, which effectively prevents collapse. In OBoW Gidaris et al.
 and Gidaris et al. the dictionary is obtained by off-line or on-line clustering. By
contrast, our method explicitly prevents collapse in the two branches independently, which removes
Published as a conference paper at ICLR 2022
the requirement for shared weights and identical architecture, opening the door to the application of
joint-embedding SSL to multi-modal signals.
Information maximization methods. A principle to prevent collapse is to maximize the information
content of the embeddings. Two such methods were recently proposed: W-MSE Ermolov et al. 
and Barlow Twins Zbontar et al. . In W-MSE, an extra module transforms the embeddings
into the eigenspace of their covariance matrix (whitening or Karhunen-Loève transform), and forces
the vectors thereby obtained to be uniformly distributed on the unit sphere. In Barlow Twins, a loss
term attempts to make the normalized cross-correlation matrix of the embedding vectors from the
two branches to be close to the identity. Both methods attempt to produce embedding variables that
are decorrelated from each other, thus preventing an informational collapse in which the variables
carry redundant information. Because all variables are normalized over a batch, there is no incentive
for them to shrink nor expand. This seems to sufﬁcient to prevent collapse. Our method borrows the
decorrelation mechanism of Barlow Twins. But it includes an explicit variance-preservation term for
each variable of the two embeddings and thus does not require any normalization.
VICREG: DETAILED DESCRIPTION
VICReg follows recent trends in self-supervised learning Caron et al. ; Grill et al. ;
Chen & He ; Zbontar et al. ; Chen et al. and is based on a joint embedding
architecture. Contrary to many previous approaches, our architecture may be completely symmetric
or completely asymmetric with no shared structure or parameters between the two branches. In
most of our experiments, we use a Siamese net architecture in which the two branches are identical
and share weights. Each branch consists of an encoder fθ that outputs the representations (used for
downstream tasks), followed by an expander hφ that maps the representations into an embedding
space where the loss function will be computed. The role of the expander is twofold: (1) eliminate
the information by which the two representations differ, (2) expand the dimension in a non-linear
fashion so that decorrelating the embedding variables will reduce the dependencies (not just the
correlations) between the variables of the representation vector. The loss function uses a term s that
learns invariance to data transformations and is regularized with a variance term v that prevents norm
collapse and a covariance term c that prevents informational collapse by decorrelating the different
dimensions of the vectors. After pretraining, the expander is discarded and the representations of the
encoder are used for downstream tasks.
Given an image i sampled from a dataset D, two transformations t and t′ are sampled from a
distribution T to produce two different views x = t(i) and x′ = t′(i) of i. These transformations
are random crops of the image, followed by color distortions. The distribution T is described in
Appendix C. The views x and x′ are ﬁrst encoded by fθ into their representations y = fθ(x) and
y′ = fθ(x′), which are then mapped by the expander hφ onto the embeddings z = hφ(y) and
z′ = hφ(y′). The loss is computed at the embedding level on z and z′.
We describe here the variance, invariance and covariance terms that compose our loss function. The
images are processed in batches, and we denote Z = [z1, . . . , zn] and Z′ = [z′
1, . . . , z′
n] the two
batches composed of n vectors of dimension d, of embeddings coming out of the two branches of
the siamese architecture. We denote by zj the vector composed of each value at dimension j in
all vectors in Z. We deﬁne the variance regularization term v as a hinge function on the standard
deviation of the embeddings along the batch dimension:
max(0, γ −S(zj, ϵ)),
where S is the regularized standard deviation deﬁned by:
Var(x) + ϵ,
γ is a constant target value for the standard deviation, ﬁxed to 1 in our experiments, ϵ is a small
scalar preventing numerical instabilities. This criterion encourages the variance inside the current
Published as a conference paper at ICLR 2022
batch to be equal to γ along each dimension, preventing collapse with all the inputs mapped on the
same vector. Using the standard deviation and not directly the variance is crucial. Indeed, if we take
S(x) = Var(x) in the hinge function, the gradient of S with respect to x becomes close to 0 when x
is close to ¯x. In this case, the gradient of v also becomes close to 0 and the embeddings collapse. We
deﬁne the covariance matrix of Z as:
(zi −¯z)(zi −¯z)T ,
Inspired by Barlow Twins Zbontar et al. , we can then deﬁne the covariance regularization
term c as the sum of the squared off-diagonal coefﬁcients of C(Z), with a factor 1/d that scales the
criterion as a function of the dimension:
This term encourages the off-diagonal coefﬁcients of C(Z) to be close to 0, decorrelating the different
dimensions of the embeddings and preventing them from encoding similar information. Decorrelation
at the embedding level ultimately has a decorrelation effect at the representation level, which is a
non trivial phenomenon that we study in Appendix D. We ﬁnally deﬁne the invariance criterion s
between Z and Z′ as the mean-squared euclidean distance between each pair of vectors, without any
normalization:
s(Z, Z′) = 1
The overall loss function is a weighted average of the invariance, variance and covariance terms:
ℓ(Z, Z′) = λs(Z, Z′) + µ[v(Z) + v(Z′)] + ν[c(Z) + c(Z′)],
where λ, µ and ν are hyper-parameters controlling the importance of each term in the loss. In our
experiments, we set ν = 1 and perform a grid search on the values of λ and µ with the base condition
λ = µ > 1. The overall objective function taken on all images over an unlabelled dataset D is given
ℓ(ZI, Z′I),
where ZI and Z′I are the batches of embeddings corresponding to the batch of images I transformed
by t and t′. The objective is minimized for several epochs, over the encoder parameters θ and
expander parameters φ. We illustrate the architecture and loss function of VICReg in Figure 1.
IMPLEMENTATION DETAILS
Implementation details for pretraining with VICReg on the 1000-classes ImagetNet dataset without
labels are as follows. Coefﬁcients λ and µ are 25 and ν is 1 in Eq. (6), and ϵ is 0.0001 in Eq. (1).
We give more details on how we choose the coefﬁcients of the loss function in Appendix D.4. The
encoder network fθ is a standard ResNet-50 backbone He et al. with 2048 output units. The
expander hφ is composed of two fully-connected layers with batch normalization (BN) Ioffe &
Szegedy and ReLU, and a third linear layer. The sizes of all 3 layers were set to 8192. As
with Barlow Twins, performance improves when the size of the expander layers is larger than the
dimension of the representation. The impact of the expander dimension on performance is studied in
Appendix D. The training protocol follows those of BYOL and Barlow Twins: LARS optimizer You
et al. ; Goyal et al. run for 1000 epochs with a weight decay of 10−6 and a learning
rate lr = batch_size/256 × base_lr, where batch_size is set to 2048 by default and base_lr is a
base learning rate set to 0.2. The learning rate follows a cosine decay schedule Loshchilov & Hutter
 , starting from 0 with 10 warmup epochs and with ﬁnal value of 0.002.
In this section, we evaluate the representations obtained after self-supervised pretraining of a ResNet-
50 He et al. backbone with VICReg during 1000 epochs, on the training set of ImageNet,
using the training protocol described in section 4. We also pretrain on pairs of image and text data
and evaluate on retrieval tasks on the MS-COCO dataset.
Published as a conference paper at ICLR 2022
Table 1: Evaluation on ImageNet. Evaluation of the representations obtained with a ResNet-50
backbone pretrained with VICReg on: (1) linear classiﬁcation on top of the frozen representations
from ImageNet; (2) semi-supervised classiﬁcation on top of the ﬁne-tuned representations from
1% and 10% of ImageNet samples. We report Top-1 and Top-5 accuracies (in %). Top-3 best
self-supervised methods are underlined.
Semi-supervised
Supervised
MoCo He et al. 
PIRL Misra & Maaten 
CPC v2 Hénaff et al. 
CMC Tian et al. 
SimCLR Chen et al. 
MoCo v2 Chen et al. 
SimSiam Chen & He 
SwAV Caron et al. 
InfoMin Aug Tian et al. 
OBoW Gidaris et al. 
BYOL Grill et al. 
SwAV (w/ multi-crop) Caron et al. 
Barlow Twins Zbontar et al. 
VICReg (ours)
EVALUATION ON IMAGENET
Following the ImageNet Deng et al. linear evaluation protocol, we train a linear classiﬁer
on top of the frozen representations of the ResNet-50 backbone pretrained with VICReg. We also
evaluate the performance of the backbone when ﬁne-tuned with a linear classiﬁer on a subset of
ImageNet’s training set using 1% or 10% of the labels, using the split of Chen et al. . We
give implementation details about the optimization procedure for these tasks in Appendix C. We
have applied the training procedure described in section 4 with three different random initialization.
The numbers reported in Table 1 for VICReg are the mean scores, and we have observed that the
difference between worse and best run is lower than 0.1% accuracy for linear classiﬁcation, which
shows that VICReg is a very stable algorithm. Lack of time has prevented us from doing the same
for the semi-supervised classiﬁcation experiments, and the experiments of section 5.2 and 6, but
we expect similar conclusion to hold. We compare in Table 1 our results on both tasks against
other methods on the validation set of ImageNet. The performance of VICReg is on par with the
state of the art without using the negative pairs of SimCLR, the clusters of SwAV, the bag-of-words
representations of OBoW, or any asymmetric networks architectural tricks such as the momentum
encoder of BYOL and the stop-gradient operation of SimSiam. The performance is comparable to
that of Barlow Twins, which shows that VICReg’s more explicit way of constraining the variance
and comparing views has the same power than maximizing cross-correlations between pairs of twin
dimensions. The main advantage of VICReg is the modularity of its objective function and the
applicability to multi-modal setups.
TRANSFER TO OTHER DOWNSTREAM TASKS
Following the setup from Misra & Maaten , we train a linear classiﬁer on top of the frozen
representations learnt by our pretrained ResNet-50 backbone on a variety of different datasets: the
Places205 Zhou et al. scene classiﬁcation dataset, the VOC07 Everingham et al. 
multi-label image classiﬁcation dataset and the iNaturalist2018 Horn et al. ﬁne-grained image
classiﬁcation dataset. We then evaluate the quality of the representations by transferring to other
vision tasks including VOC07+12 Everingham et al. object detection using Faster R-CNN Ren
et al. with a R50-C4 backbone, and COCO Lin et al. instance segmentation using
Mask-R-CNN He et al. with a R50-FPN backbone. We report the performance in Table 2,
Published as a conference paper at ICLR 2022
Table 2: Transfer learning on downstream tasks. Evaluation of the representations from a ResNet-
50 backbone pretrained with VICReg on: (1) linear classiﬁcation tasks on top of frozen representations, we report Top-1 accuracy (in %) for Places205 Zhou et al. and iNat18 Horn et al. ,
and mAP for VOC07 Everingham et al. ; (2) object detection with ﬁne-tunning, we report
AP50 for VOC07+12 using Faster R-CNN with C4 backbone Ren et al. ; (3) object detection
and instance segmentation, we report AP for COCO Lin et al. using Mask R-CNN with FPN
backbone He et al. . We use † to denote the experiments run by us. Top-3 best self-supervised
methods are underlined.
Linear Classiﬁcation
Object Detection
Places205 VOC07 iNat18
VOC07+12 COCO det COCO seg
Supervised
MoCo He et al. 
PIRL Misra & Maaten 
SimCLR Chen et al. 
MoCo v2 Chen et al. 
SimSiam Chen & He 
BYOL Grill et al. 
SwAV (m-c) Caron et al. 
OBoW Gidaris et al. 
Barlow Twins Grill et al. 
VICReg (ours)
Table 3: Evaluation on MS-COCO 5K retrieval tasks. Comparison of VICReg with the contrastive
loss of VSE++ Faghri et al. , and with Barlow Twins, pretrain on the training set of MS-COCO.
In all settings, the encoder for text is a word embedding followed by a GRU layer, the encoder for
images is a ResNet-152.
Image-to-text
Text-to-Image
Contrastive (VSE++)
Barlow Twins
VICReg performs on par with most concurrent methods, and better than Barlow Twins, across all
classiﬁcation tasks, but is slightly behind the top-3 on detection tasks.
MULTI-MODAL PRETRAINING ON MS-COCO
One fundamental difference of VICReg compared to Barlow Twins is the way the branches are
regularized. In VICReg, both branches are regularized independently, as the covariance term is
applied on each branch separately, which works better in the scenarios where the branches are
completely different, have different types of architecture and process different types of data. Indeed,
the statistics of the output of the two branches can be very different, and the amount of regularization
required for each may vary a lot. In Barlow Twins, the regularization is applied on the cross-correlation
matrix, which favors the scenarios where the branches produce outputs with similar statistics. We
demonstrate the capabilities of VICReg in a multi-modal experiment where we pretrain on pairs of
images and corresponding captions on the MS-COCO dataset. We regularize each branch with a
different coefﬁcient, which is not possible with Barlow Twins, and we show that VICReg outperforms
Barlow Twins on image and text retrieval downstream tasks. Table 3 reports the performance of
VICReg against the contrastive loss proposed by VSE++ Faghri et al. , and against Barlow
Twins, in the identical setting proposed in Faghri et al. . VICReg outperforms the two by a
signiﬁcant margin.
Published as a conference paper at ICLR 2022
Table 4: Effect of incorporating variance and covariance regularization in different methods.
Top-1 ImageNet accuracy with the linear evaluation protocol after 100 pretraining epochs. For all
methods, pretraining follows the architecture, the optimization and the data augmentation protocol
of the original method using our reimplementation. ME: Momentum Encoder. SG: stop-gradient.
PR: predictor. BN: Batch normalization layers after input and inner linear layers in the expander. No
Reg: No additional regularization. Var Reg: Variance regularization. Var/Cov Reg: Variance and
Covariance regularization. Unmodiﬁed original setups are marked by a †.
Var/Cov Reg
In this section we study how the different components of our method contribute to its performance,
as well as how they interact with components from other self-supervised methods. We also evaluate
different scenarios where the branches have different weights and architecture. All reported results are
obtained on the linear evaluation protocol, using a ResNet-50 backbone if not mentioned otherwise,
and 100 epochs of pretraining, which gives results consistent with those obtained with 1000 epochs
of pretraining. The optimization setting used for each experiment is described in Appendix C.
Asymmetric networks. We study the impact of different components used in asymmetric architectures and the effects of adding variance and covariance regularization, in terms of performance and
training stability. Starting from a simple symmetric architecture with an encoder and an expander
without batch normalization, which correspond to VICReg without batch normalization in the expander, we progressively add batch normalization in the inner layers of the expander, a predictor, a
stop-gradient operation and a momentum encoder. We use the training protocol and architecture of
SimSiam Chen & He when a stop-gradient is used and the training protocol and architecture
of BYOL Grill et al. when a momentum encoder is used. The predictor as used in SimSiam
and BYOL is a learnable module gψ that predicts the embedding of a view given the embedding of
the other view of the same image. If z and z′ are the embeddings of two views of an image, then
p = gψ(z) and p′ = gψ(z′) are the predictions of each view. The invariance loss function of Eq. (5)
is now computed between a batch of embeddings Z = [z1, . . . , zn] and the corresponding batch of
predictions P = [p′
1, . . . , p′
n], then symmetrized:
s(Z, Z′, P, P ′) = 1
where D is a distance function that depends on the method used. BYOL uses the mean square error
between l2-normalized vectors, SimSiam uses the negative cosine similarity loss and VICReg uses
the mean square error without l2-normalization. The variance and covariance terms are regularizing
the output Z and Z′ of the expander, which we empirically found to work better than regularizing
the output of the predictor. We compare different settings in Table 4, based on the default data
augmentation, optimization and architecture settings of the original BYOL, SimSiam and VICReg
methods. In all settings, the absence of BN indicates that BN is also removed in the predictor when
one is used.
We analyse ﬁrst the impact of variance regularization (VR) in the different settings. When using VR,
adding a predictor (PR) to VICReg does not lead to a signiﬁcant change of the performance, which
indicates that PR is redundant with VR. In comparison, without VR, the representations collapse, and
both stop-gradient (SG) and PR are necessary. Batch normalization in the inner layers of the expander
(BN) in VICReg leads to a 1.0% increase in the performance, which is not a big improvement
considering that SG and PR without BN is performing very poorly at 35.1%.
Published as a conference paper at ICLR 2022
Table 5: Impact of sharing weights or not between branches. Top-1 accuracy on linear classiﬁcation with 100 pretraining epochs. The encoder and expander of both branches can share the same
architecture and share their weights (SW), share the same architecture with different weights (DW),
or have different architectures (DA). The encoders can be ResNet-50, ResNet-101 or ViT-S.
DA R50/R101
DA R50/ViT-S
Barlow Twins
Finally, incorporating VR with SG or ME further improves the performance by small margins of
respectively 0.2% and 0.9%, which might be explained by the fact that these architectural tricks that
prevent collapse are not perfectly maintaining the variance of the representations, i.e. very slow
collapse is happening with these methods. We explain this intuition by studying the evolution of the
standard deviation of the representations during pretraining for BYOL and SimSiam in Appendix D.
We then analyse the impact of adding additional covariance regularization (CR) in the different
settings, along with variance regularization. We found that optimization with SG and CR is hard,
even if our analysis of the average correlation coefﬁcient of the representations during pretraining in
Appendix D shows that both fulﬁll the same objective.
The performance of BYOL and SimSiam slightly drops compared to VR only, except when PR is
removed, where SG becomes useless. BN is still useful and improves the performance by 1.3%.
Finally with CR, PR does not harm the performance and even improves it by a very small margin.
VICReg+PR with 1000 epochs of pretraining exactly matches the score of VICReg (73.2% on linear
classiﬁcation).
Weight sharing. Contrary to most self-supervised learning approaches based on Siamese architectures, VICReg has several unique properties: (1) weights do not need to be shared between the
branches, each branch’s weights are updated independently of the other branch’s weights; (2) the
branches are regularized independently, the variance and covariance terms are computed on each
branch individually; (3) no predictor is necessary unlike with methods where one branch predicts
outputs of the other branch. We compare the robustness of VICReg against other methods in different
scenarios where the weights of the branches can be shared (SW), not shared (DW), and where the
encoders can have different architectures (DA). Among other self-supervised methods, SimCLR and
Barlow Twins are the only ones that can handle these scenarios. The asymmetric methods that are
based on a discrepancy between the branches requires either the architecture or the weights to be
shared between the branches. The performance drops by 2.1% with VICReg and 4.5% with Barlow
Twins, between the shared weights scenario (SW) and the different weight scenario (DW). The difference between VICReg and Barlow Twins is also signiﬁcant in scenarios with different architectures,
in particular VICReg performs better than Barlow Twins by 2.8% with ResNet-50/ResNet-101 and
better by 2.3% with ResNet-50/ViT-S Dosovitskiy et al. . This shows that VICReg is more
robust than Barlow Twins in these kind of scenarios. The performance of SimCLR remains stable
across scenarios, but is signiﬁcantly worse than the performance of VICReg. Importantly, the ability of VICReg to function with different parameters, architectures, and input modalities for
the branches widens the applicability to joint-embedding SSL to many applications, including
multi-modal signals.
CONCLUSION
We introduced VICReg, a simple approach to self-supervised learning based on a triple objective:
learning invariance to different views with a invariance term, avoiding collapse of the representations
with a variance preservation term, and maximizing the information content of the representation
with a covariance regularization term. VICReg achieves results on par with the state of the art on
many downstream tasks, but is not subject to the same limitations as most other methods, particularly
because it does not require the embedding branches to be identical or even similar.
Published as a conference paper at ICLR 2022
Acknowledgement. Jean Ponce was supported in part by the French government under management
of Agence Nationale de la Recherche as part of the ”Investissements d’avenir” program, reference
ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton/ENS Chair in Artiﬁcial Intelligence
and the Inria/NYU collaboration. Adrien Bardes was supported in part by a FAIR/Prairie CIFRE
PhD Fellowship. The authors wish to thank Jure Zbontar for the BYOL implementation, Stéphane
Deny for useful comments on the paper, and Li Jing, Yubei Chen, Mikael Henaff, Pascal Vincent
and Geoffrey Zweig for useful discussions. We thank Quentin Duval and the VISSL team for help
obtaining the results of table 2.