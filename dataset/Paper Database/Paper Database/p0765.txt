Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 143–148,
Berlin, Germany, August 7-12, 2016. c⃝2016 Association for Computational Linguistics
Simple PPDB: A Paraphrase Database for Simpliﬁcation
Ellie Pavlick
University of Pennsylvania
 
Chris Callison-Burch
University of Pennsylvania
 
Paraphrase
Database, a subset of of the Paraphrase
Database (PPDB) adapted for the task
of text simpliﬁcation. We train a supervised model to associate simpliﬁcation
scores with each phrase pair, producing
rankings competitive with state-of-theart lexical simpliﬁcation models.
simpliﬁcation
4.5 million paraphrase rules, making it
the largest available resource for lexical
simpliﬁcation.
Motivation
Language is complex, and the process of reading
and understanding language is difﬁcult for many
groups of people. The goal of text simpliﬁcation
is to rewrite text in order to make it easier to understand, for example, by children , language learners , people with disabilities , and even by
machines . Automatic
text simpliﬁcation has the potential to dramatically increase access to information by making written documents available at all
reading levels.
Full text simpliﬁcation involves many steps,
including grammatical restructuring and summarization .
One of the most basic
subtasks is lexical simpliﬁcation – replacing complicated words and phrases
with simpler paraphrases. While there is active research in the area of lexical simpliﬁcation , existing models have been byand-large limited to single words.
Often, howmedical practitioner
legislative texts
hypertension
high blood pressure
very common
signiﬁcant quantity
impact negatively
Table 1: In lexical simpliﬁcation, it is often necessary to replace single words with phrases or phrases with single words.
The above are examples of such lexical simpliﬁcations captured by the Simple PPDB resource.
ever, it is preferable, or even necessary to paraphrase a single complex word with multiple simpler words, or to paraphrase multiple words with a
single word. For example, it is difﬁcult to imagine
a simple, single-word paraphrase of hypertension,
but the three-word phrase high blood pressure is a
very good simpliﬁcation (Table 1). Such phrasal
simpliﬁcations are overlooked by current lexical
simpliﬁcation models, and thus are often unavailable to the end-to-end text simpliﬁcation systems
that require them.
Recent research in data-driven paraphrasing has
produced enormous resources containing millions
of meaning-equivalent phrases . Such resources capture a wide range of
language variation, including the types of lexical
and phrasal simpliﬁcations just described. In this
work, we apply state-of-the-art machine learned
models for lexical simpliﬁcation in order to identify phrase pairs from the Paraphrase Database
(PPDB) applicable to the task of text simpliﬁcation. We introduce Simple PPDB,1 a subset of the
Paraphrase Database containing 4.5 million simplifying paraphrase rules. The large scale of Simple PPDB will support research into increasingly
advanced methods for text simpliﬁcation.
1 
resources/simple-ppdb.tgz
Identifying Simpliﬁcation Rules
Paraphrase Rules
The Paraphrase Database (PPDB) is currently
the largest available collection of paraphrases.
Each paraphrase rule in the database has an
automatically-assigned quality score between 1
and 5 . In this work, we use
the PPDB-TLDR2 dataset, which contains 14 million high-scoring lexical and phrasal paraphrases,
and is intended to give a generally good tradeoff
between precision and recall. To preprocess the
data, we lemmatize all of the phrases, and remove
rules which differ only by morphology, punctuation, or stop words, or which involve phrases
longer than 3 words. The resulting list contains
7.5 million paraphrase rules covering 625K unique
lemmatized words and phrases.
Lexical Simpliﬁcation Model
Our goal is to build a model which can accurately
identify paraphrase rules that both 1) simplify the
input phrase and 2) preserve its meaning. That
is, we want to avoid a model which favors “simple” words (e.g. the, and) even when they capture
none of the meaning of the input phrase. We therefore train our model to make a three-way distinction between rules which simplify the input, rules
which make the input less simple, and rules which
generate bad paraphrases.
We collect our training data in two phases.
First, we sample 1,000 phrases from the vocabulary of the PPDB. We limit ourselves to words
which also appear at least once in the Newsela
corpus for text simplifcation ,
in order to ensure that we focus our model on
the types of words for which the ﬁnal resource is
most likely to be applied. For each of these 1,000
words/phrases, we sample up to 10 candidate paraphrases from PPDB, stratiﬁed evenly across paraphrase quality scores. We ask workers on Amazon Mechanical Turk to rate each of the chosen
paraphrase rules on a scale from 1 to 5 to indicate how well the paraphrase preserves the meaning of the original phrase. We use the same annotation design used in Pavlick et al. . We
have 5 workers judge each pair, omitting workers
who do not provide correct answers on the embedded gold-standard pairs which we draw from
WordNet. For 62% of the paraphrase rules we had
2 
scored, the average human rating falls below 3, indicating that the meaning of the paraphrase differs
substantially from that of the input. We assign all
of these rules to the “bad paraphrase” class.
meaningpreserving paraphrase rules (scored ≥3 in the
above annotation task) and feed them into a
second annotation task, in which we identify
rules that simplify the input.
We use the same
annotation interface as in Pavlick and Nenkova
 , which asks workers to choose which of
the two phrases is simpler, or to indicate that
there is no difference in complexity. We collect 7
judgements per pair and take the majority label,
discarding pairs for which the majority opinion
was that there was no difference. We include each
rule in our training data twice, once as an instance
of a “simplifying” rule, and once in the reverse
direction as an instance of a “complicating” rule.
In the end, our training dataset contains 11,829
pairs, with the majority class being “bad paraphrase” (47%), and the remaining split evenly
between “simplifying” and “complicating” paraphrase rules (26% each).
We use a variety of features that have
been shown in prior work to give good signal
about phrases’ relative complexity.
The features we include are as follows: phrase length
in words and in characters, frequency according
to the Google NGram corpus , number of syllables, the relative frequency
of usage in Simple Wikipedia compared to normal
Wikipedia , character unigrams and bigrams, POS tags, and the averaged Word2Vec word embeddings for the words in
the phrase . For each phrase
pair ⟨e1, e2⟩, for each feature f, we include f(e1),
f(e2) and f(e1)−f(e2).3 We also include the cosine similarity of the averaged word embeddings
and the PPDB paraphrase quality score as features.
We train a multi-class logistic regression
model4 to predict if the application of a paraphrase
rule will result in 1) simpler output, 2) more complex output, or 3) non-sense output.
Performance.
Table 2 shows the performance of
the model on cross-validation, compared to several
baselines. The full model achieves 60% accuracy,
3We do not compute the difference f(e1) −f(e2) for
sparse features, i.e. character ngrams and POS tags.
4 
Simple/Regular Wiki. Ratio
Length in Characters
Google Ngram Frequency
Number of Syllables
Supervised Model, W2V
Supervised Model, Full
Table 2: Accuracy on 10-fold cross-validation, and precision
for identifying simplifying rules. Folds are constructed so
that train and test vocabularies are disjoint.
5 points higher than the strongest baseline, a supervised model which uses only word embeddings
as features.
Simple PPDB
We run the trained model described above over
all 7.5 million paraphrase rules.
From the predictions, we construct Simple PPDB: a list of 4.5
million simplifying paraphrase rules.
Simple PPDB is represented as a triple, consisting of a syntactic category, and input phrase, and
a simpliﬁed output phrase. Each rule is associated
with both a paraphrase quality score from 1 to 5
(taken from PPDB 2.0), and simpliﬁcation con-
ﬁdence score from 0 to 1.0 (our classiﬁer’s con-
ﬁdence in the prediction that the rule belongs to
the “simplifying” class).
Note that ranking via
the conﬁdence scores of a classiﬁcation model has
not, to our knowledge, been explored in previous
work on lexical simpliﬁcation. The remainder of
this paper evaluates the quality of the simpliﬁcation ranking. For an evaluation of the paraphrase
quality ranking, see Pavlick et al. . Table 3
shows examples of some of the top ranked paraphrases according to Simple PPDB’s simpliﬁcation score for several input phrases.
Evaluation
To evaluate Simple PPDB, we apply it in a setting intended to emulate the way it is likely to be
used in practice. We use the Newsela Simpliﬁcation Dataset , a corpus of manually simpliﬁed news articles. This corpus is currently the cleanest available simpliﬁcation dataset
and is likely to be used to train and/or evaluate the
simpliﬁcation systems that we envision beneﬁtting
most from Simple PPDB.
We draw a sample of 100 unique word types
(“targets”) from the corpus for which Simple
PPDB has at least one candidate simpliﬁcation.
For each target, we take Simple PPDB’s full list
of simpliﬁcation rules which are of high quality
according to the PPDB 2.0 paraphrase score5 and
which match the syntactic category of the target.
On average, Simple PPDB proposes 8.8 such candidate simpliﬁcations per target.
Comparison to existing methods.
Our baselines include three existing methods for generating lists of candidates that were proposed in
prior work. The methods we test for generating
lists of candidate paraphrases for a given target
are: the WordNetGenerator, which pulls synonyms from WordNet , the KauchakGenerator,
which generates candidates based on automatic
alignments between Simple Wikipedia and normal
Wikipedia , and the
GlavasGenerator, which generates candidates
from nearby phrases in vector space ).
For each generated list, we follow Horn et al.
 ’s supervised SVM Rank approach to rank
the candidates for simplicity. We reimplement the
main features of their model: namely, word frequencies according to the Google NGrams corpus and the Simple
Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences
 . We omit the language modeling features since our evaluation does
not consider the context in which the substitution
is to be applied.
All of these methods (the three generation methods and the ranker) are implemented as part of the
LEXenstein toolkit .
We use the LEXenstein implementations for the
results reported here, using off-the-shelf conﬁgurations and treating each method as a black box.
We use each of the generate-and-rank
methods to produce a ranked list of simpliﬁcation
candidates for each of the 100 targets drawn from
the Newsela corpus. When a generation method
fails to produce any candidates for a given target, we simply ignore that target for that particular method. This is to avoid giving Simple PPDB
5Heuristically, we deﬁne “high quality” as ≥3.5 for words
and ≥4 for phrases.
employment opportunity
most strongly
opportunity
be removed
business opportunity
be corrected
forget about it
to be resolved
be ignored
Table 3: Examples of top-ranked simpliﬁcations proposed by Simple PPDB for several input words. Often, the best simpliﬁcation for a single word is a multiword phrase, or vice-versa. These many-to-one mappings are overlooked when systems use
only length or frequency as a proxy for simplicity.
an unfair advantage, since, by construction, PPDB
will have full coverage of our list of 100 targets. In
the end, the GlavasGenerator is evaluated over 95,
the WordNetGenerator over 82, and the Kauchak-
Generator over 48. The results in Table 4 do not
change signiﬁcantly if we restrict all systems to
the 48 targets which the KauchakGenerator is capable of handling. Since the GlavasGenerator is
capable of producing an arbitrary number of candidates for each target, we limit the length of each
of its candidate lists to be equal to the number
of candidates produced by Simple PPDB for that
same target.
Human judgments.
For each of the proposed
rules from all four systems, we collect human
judgements on Amazon Mechanical Turk, using
the same annotation interface as before. That is,
we ask 7 workers to view each pair and indicate
which of the two phrases is simpler, or to indicate
that there is no difference. We take the majority label to be the true label for each rule. Workers show
moderate agreement on the 3-way task (κ = 0.4
± 0.03), with 14% of pairs receiving unanimous
agreement and 37% receiving the same label from
6 out of 7 annotators. We note that the κ metric
is likely a lower bound, as it punishes low agreement on pairs for which there is little difference in
complexity, and thus the “correct” answer is not
clear (e.g. for the pair ⟨matter, subject⟩, 3 annotators say that matter is simpler, 2 say that subject is
simpler, and 2 say there is no difference).
Table 4 compares the different methods in terms of how well they rank simplifying
rules above non-simplifying rules. Simple PPDB’s
ranking of the relative simplicity achieves an averaged precision of 0.72 (0.77 P@1), compared
to 0.70 (0.69 P@1) achieved by the Horn et al.
 system– i.e. the KauchakGenerator+SVM
We hypothesize that the performance
difference between these two ranking systems is
Avg. Prec.
Glavas+SVR
Wordnet+SVR
Kauchak+SVR
Simple PPDB
Table 4: Precision of relative simpliﬁcation rankings of three
existing lexical simpliﬁcation methods compared to the Simple PPDB resource in terms of Average Precision and P@1
(both range from 0 to 1 and higher is better). All of the existing methods were evaluated using the implementations as
provided in the LEXenstein toolkit.
likely due to a combination of the additional features applied in Simple PPDB’s model (e.g. word
embeddings) and the difference in training data
 model was trained on 500 words, each
with a ranked list of paraphrases). Table 5 provides examples of the top-ranked simpliﬁcation
candidates proposed by each of the methods described.
enrage, perturb, stun
horrify, dismay, alert, appall, appal
pure, worry
worry, concern, alert
credible, sort, feign, phoney, good naturedness, sincere, sincerely, insincere,
real, actual, unfeigned, literal, echt, true
true, real, actual, honest, sincere
Table 5: Examples of candidate simpliﬁcations proposed by
Simple PPDB and by three other generate-and-rank methods.
Bold words were rated by humans to be simpler than the target word. Note that these candidates are judged on simplicity,
not on their goodness as paraphrases.
In addition, Simple PPDB offers the largest
coverage (Table 6). It has a total vocabulary of
624K unique words and phrases, and provides
the largest number of potential simpliﬁcations for
Glavas+SVR
Kauchak+SVR
Wordnet+SVR
Simple PPDB
Table 6: Overall coverage of three existing lexical simpliﬁcation methods compared to the Simple PPDB resource. Glavas
is marked as ∞since it generates candidates based on nearness in vector space, and in theory could generate as many
words/phrases as are in the vocabulary of the vector space.
each target– for the 100 targets drawn from the
Newsela corpus, PPDB provided an average of 8.8
candidates per target. The next best generator, the
WordNet-based system, produces only 6.7 candidates per target on average, and has a total vocabulary of only 155K words.
Conclusion
We have described Simple PPDB, a subset of the
Paraphrase Database adapted for the task of text
simpliﬁcation.
Simple PPDB is built by applying state-of-the-art machine learned models for
lexical simpliﬁcation to the largest available resource of lexical and phrasal paraphrases, resulting in a web-scale resource capable of supporting
research in data-driven methods for text simpliﬁcation. We have shown that Simple PPDB offers
substantially increased coverage of both words
and multiword phrases, while maintaining high
quality compared to existing methods for lexical
simpliﬁcation. Simple PPDB, along with the human judgements collected as part of its creation, is
freely available with the publication of this paper.6
Acknowledgments
This research was supported by a Facebook Fellowship, and by gifts from the Alfred P. Sloan
Foundation, Google, and Facebook. This material is based in part on research sponsored by the
NSF grant under IIS-1249516 and DARPA under
number FA8750-13-2-0017 (the DEFT program).
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes.
The views and conclusions contained in this publication are those of the authors and should not be
interpreted as representing ofﬁcial policies or endorsements of DARPA and the U.S. Government.
6 
resources/simple-ppdb.tgz
We would especially like to thank Ani Nenkova
for suggesting this line of research and for providing the initial ideas on which this work builds.
We would also like to thank Courtney Napoles and
Wei Xu for valuable discussions, the anonymous
reviewers for thoughtful comments, and the Amazon Mechanical Turk annotators for their contributions.