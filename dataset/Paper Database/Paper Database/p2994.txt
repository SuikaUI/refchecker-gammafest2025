Hierarchical Attention Transfer Network for
Cross-Domain Sentiment Classiﬁcation
Zheng Li, Ying Wei, Yu Zhang, Qiang Yang
Hong Kong University of Science and Technology, Hong Kong
 , , , 
Cross-domain sentiment classiﬁcation aims to leverage useful
information in a source domain to help do sentiment classiﬁcation in a target domain that has no or little supervised information. Existing cross-domain sentiment classiﬁcation methods cannot automatically capture non-pivots, i.e., the domainspeciﬁc sentiment words, and pivots, i.e., the domain-shared
sentiment words, simultaneously. In order to solve this problem, we propose a Hierarchical Attention Transfer Network
(HATN) for cross-domain sentiment classiﬁcation. The proposed HATN provides a hierarchical attention transfer mechanism which can transfer attentions for emotions across domains by automatically capturing pivots and non-pivots. Besides, the hierarchy of the attention mechanism mirrors the
hierarchical structure of documents, which can help locate the
pivots and non-pivots better. The proposed HATN consists of
two hierarchical attention networks, with one named P-net
aiming to ﬁnd the pivots and the other named NP-net aligning the non-pivots by using the pivots as a bridge. Specifically, P-net ﬁrstly conducts individual attention learning to
provide positive and negative pivots for NP-net. Then, Pnet and NP-net conduct joint attention learning such that the
HATN can simultaneously capture pivots and non-pivots and
realize transferring attentions for emotions across domains.
Experiments on the Amazon review dataset demonstrate the
effectiveness of HATN.
Introduction
Users usually express opinions about products or services
on social media or review sites. It is helpful to correctly understand their emotional tendency. Sentiment classiﬁcation,
which aims to automatically determine the overall sentiment
polarity (e.g., positive or negative) of a document, has raised
continuous attentions over the past decades . Supervised learning algorithms that require labeled data have been successfully explored to build sentiment classiﬁers for a speciﬁc domain . However, there
exists insufﬁcient labeled data in a target domain of interest, where labeling data may be time-consuming and expensive. Cross-domain sentiment classiﬁcation, which borrows knowledge from related source domains with abun-
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
dant labeled data to improve the target domain, has become
a promising direction. However, the expression of users’
emotions usually varies across domains. For example, in
the Books domain, words readable and thoughtful are used
to express positive sentiment, whereas insipid and plotless
often indicate negative sentiment. On the other hand, in
the Electronics domain, rubbery and glossy express positive
sentiment, whereas words fuzzy and blurry usually express
negative sentiment. Due to the domain discrepancy, a sentiment classiﬁer trained in a source domain may not work
well when directly applied to a target domain.
To address the problem, researchers have proposed various methods for cross-domain sentiment classiﬁcation.
Blitzer et al. proposed
a Structural Correspondence Learning (SCL) method which
utilizes multiple pivot prediction tasks to infer the correlation between pivots and non-pivots. Pan et al. proposed the Spectral Feature Alignment (SFA) to
ﬁnd an alignment between pivots and non-pivots by using
the cooccurrence between them. However, these methods
need to manually select pivots and they are based on discrete feature representations such as bag-of-words with linear classiﬁers. Recently, deep neural models are explored
to automatically produce superior feature representations
for cross-domain sentiment classiﬁcation. Stacked Denoising Auto-encoders (SDA) have been successfully adopted
to learn hidden representations shared across domains . Yu and
Jiang leveraged two auxiliary tasks
to learn sentence embeddings with a Convolutional Neural
Network (CNN) which works well across domains, while they still rely on manually identifying positive and negative pivots. Ganin et al. proposed the Domain-Adversarial
training of Neural Networks (DANN) which ﬁrst introduces
a domain classiﬁer incapable of discriminating representations from a source or a target domain by reversing the gradient direction of the neural network. In order to improve
the interpretability of deep models, Li et al. 
proposed an Adversarial Memory Network (AMN) to automatically identity the pivots by using the attention mechanism and adversarial training. Nevertheless, AMN only focuses on word-level attention and ignores the hierarchical
structure of documents, which may not accurately capture
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
pivots in long documents. Besides, it cannot automatically
capture and exploit non-pivots, which may result in the degraded performance when source and target domains only
have few overlapping pivot features.
To simultaneously harness the collective power of pivots
and non-pivots and interpret what to transfer, we introduce a
Hierarchical Attention Transfer Network (HATN) for crossdomain sentiment classiﬁcation. The proposed HATN provides a hierarchical attention transfer mechanism, which can
automatically transfer attentions for emotions in both word
and sentence levels across domains to reduce the domain
discrepancy and provide a better interpretability of what to
transfer. Speciﬁcally, our framework consists of two hierarchical attention networks named P-net and NP-net, where
P-net aims to focus on pivots and NP-net is used to identify non-pivots by using the pivots as a bridge. Firstly, P-net
conducts individual attention learning to select positive and
negative pivots for NP-net. Then, P-net and NP-net conduct
joint attention learning such that the HATN can simultaneously identify the pivots and non-pivots and achieve transferring attentions for emotions across domains.
Our contributions are summarized as follows:
• We propose a hierarchical attention transfer mechanism,
which can transfer attentions for emotions across domains
by automatically capturing the pivots and non-pivots simultaneously. Besides, it can tell what to transfer in the
hierarchical attention, which makes the representations
shared by domains more interpretable.
• Empirically the proposed HATN method can signiﬁcantly
outperform the state-of-the-art methods.
Related Work
Domain Adaptation
Domain adaptation tasks such as
cross-domain sentiment classiﬁcation have raised much attention in recent years. One line of work focuses on inducing a low-dimensional feature representation shared across
domains based on the cooccurrence between pivots and nonpivots. Unfortunately, they highly rely on manually selecting
pivots based on term frequencies on both domains , mutual information between
features and labels of a source domain , mutual information between features and domains , and weighted log-likelihood ratio . These pivot selection methods are
very tedious and may not identify the pivots accurately.
Recently, deep learning methods form another line of
work to automatically produce superior feature representations for cross-domain sentiment classiﬁcation. SDA is proposed to learn to discover intermediate representations shared across domains.
In order to improve the speed and scalability of SDA
for high-dimensional data, Chen et al. 
proposed a Marginalized Stacked Denoising Autoencoder
(mSDA). Yu et al. used two auxiliary tasks to help induce sentence embeddings with CNN
across domains. Ganin et al. proposed the DANN, which leverages
the domain adversarial training method to make the neural
network produce representations confusing a domain classi-
ﬁer. In order to improve the interpretability of deep models,
Li et al. incorporated memory networks into
DANN to automatically identify the pivots.
Attention Mechanism in NLP
The attention mechanism has been successfully exploited in various NLP tasks
such as machine translation , question answering , document classiﬁcation and sentiment analysis . The intuition behind the
attention mechanism is that each low-level position contributes a different importance for the high-level representation. Moreover, the hierarchical attention mechanism
has been proved to be better than the word-level attention in various document-based tasks , which mirrors the hierarchical structure of documents in order to extract more powerful features.
Hierarchical Attention Transfer Network
In this section, we introduce the proposed HATN model for
cross-domain sentiment classiﬁcation. We ﬁrst present the
problem deﬁnition and notations, followed by an overview
of the model. Then we detail the model with all components.
Problem Deﬁnition and Notations
We are given two domains Ds and Dt which denote a source
domain and a target domain, respectively. Suppose that we
have a set of labeled data Xl
i=1 and {yi
i=1 as well
as some unlabeled data Xu
s+1 in the source
domain Ds, where Xs = Xl
s. Besides, a set of unlabeled data Xt = {xj
j=1 is available in the target domain
Dt. The goal of cross-domain sentiment classiﬁcation is to
train a robust classiﬁer on labeled samples in the source domain Xl
s and adapt it to predict the sentiment polarity of
unlabeled examples Xt in the target domain, which is also
widely known as unsupervised domain adaptation.
An Overview of the HATN Model
In this section, we present an overview of the proposed
HATN model for cross-domain sentiment classiﬁcation.
The goal of HATN is to transfer attentions for emotions
across domains, i.e, automatically capturing the pivots as
well as non-pivots. Therefore, we design two hierarchical
attention networks with different attentions for the target.
As illustrated in Figure 1, the ﬁrst network is named Pnet, which aims to identify the pivots shared by the source
and target domains such as great in a sample x. The second network is named NP-net which is used to capture the
non-pivots across domains such as readable in a transformed
sample g(x), generated by hiding all the pivots like the positive pivots great in the sample x. We realize the ‘hide’ action
by replacing the pivots with padding words.
In order to demonstrate the effects of each network
clearly, we describe the input, objective and motivation for
the P-net and NP-net, respectively.
 ݔ
 
 
 
 " 
 $" 
%!&
'!(
 
 




Figure 1: The framework of the HATN model.
P-net aims to capture the pivots, which have two attributes:
(1) They are important sentiment words for sentiment classiﬁcation. (2) They are shared by both domains. In order to
achieve this goal, the labeled data Xl
s in the source domain
is fed into the P-net for sentiment classiﬁcation and in the
meanwhile, all the data Xs and Xt in both domains are fed
into the P-net for domain classiﬁcation that aims to predict
the domain label of the samples, i.e, coming from the source
or target domain. Here we perform the domain classiﬁcation
based adversarial training by the Gradient Reversal Layer
(GRL) such that make the domain classiﬁer indiscriminative between the representations from the
source and target domains. In this way, it guarantees representations from the P-net are both domain-shared and useful
for sentiment classiﬁcation, and can identity the pivots with
the attention mechanism.
NP-net aims to capture the non-pivots with two characteristics: (1) They are the useful sentiment words for sentiment
classiﬁcation. (2) They are domain-speciﬁc words. To reach
the goal, the transformed labeled data g(Xl
s) in the source
domain Ds, generated by hiding all the pivots identiﬁed by
the P-net, is fed into the NP-net for sentiment classiﬁcation.
And at the same time, all transformed data g(Xs) and g(Xt)
in both domains Ds and Dt, generated by the same way, is
fed to NP-net for +(positive)/-(negative) pivot predictions.
The two tasks aim to predict whether an original sample x
contains positive or negative pivots based on the transformed
sample g (x). The transformed sample g (x) has two labels,
a label z+ indicating whether x contains at least one positive pivot and a label z−indicating whether x contains at
least one negative pivot. The intuition behind is that positive
non-pivots tend to co-occur with positive pivots and negative non-pivots tend to co-occur with negative pivots. In this
way, the NP-net can discover domain-speciﬁc features with
the pivots as a bridge and capture the non-pivots that are expected to correlate closely to the pivots with the attention
mechanism.
Training Process: Note that the NP-net needs the positive
and negative pivots as a bridge across domains. Different
from traditional methods that need to manually select pivots, the P-net possesses the ability of automatically ﬁnding
the pivots. Therefore, our training process consists of two
• Individual Attention Learning: The P-net is trained for
cross-domain sentiment classiﬁcation. We use the best parameters for P-net with early stopping on the validation
set, and then select a word with the highest word attention
in the sentence with the highest sentence attention in each
positive review of the source domain as a positive pivot.
The negative pivots are obtained in a similar way.
• Joint Attention Learning: The P-net and NP-net are jointly
trained for cross-domain sentiment classiﬁcation. The labeled data Xl
s and its transformed data g(Xl
source domain Ds are simultaneously fed into P-net and
NP-net respectively and their representations are concatenated for sentiment classiﬁcation. Note that, the transformed labeled data g(Xl
s) fed to the NP-net are used for
sentiment classiﬁcation and +(positive)/-(negative) pivot
predictions simultaneously, but all transformed unlabeled
s) and g(Xt) fed to the NP-net can only be used
for the +(positive)/-(negative) pivot predictions.
Hierarchical Content Attention
In the following, we present how to build the document representation progressively from word vectors hierarchically.
Word Attention
The contextual words contribute unequally to the semantic meaning of a sentence, especially
when we focus on a speciﬁc task, e.g., sentiment classiﬁcation. Therefore, we introduce the word-level attention to
weight words of each sentence and output with a weighted
sum of all words’ information.
Assume that a document x is made up of nc sentences
o=1. Given a sentence co ={wor}nw
r=1, we ﬁrst map
each word into its embedding vector as eor =Lwor throughout an embedding matrix L, where eor ∈Rne×1. All contextual word embedding vectors {{eor}nw
o=1 are stacked
to the external memory m ∈Rne×nw×nc, where free memories are padded with zeros vectors. We take each sentence
memory mo ∈Rne×nw and a word-level query vector qw as
the input of the word attention layer. By feeding each word
memory mor into a one-layer neural network, we obtain the
hidden representation of the rth word of the oth sentence as:
hor = tanh (Wwmor+bw) .
The importance weight of this word is therefore measured
as the similarity between hor and qw, which is further normalized through a mask softmax function as:
Mw(o, r) exp
k=1 Mw(o, k) exp (hT
where Mw(o, r) is a word-level mask function in order to
avoid the effect of padding vectors. When the word memory
mor is occupied, Mw(o, r) equals 1 and otherwise 0. The
sentence vector vo
c is ﬁnally output as a weighted sum of all
hidden representations {hor}nw
Note that the word-level query vector qw is expected to be
a high level representation of the query “what is the important word over the sentence for the task”. qw is randomly
initialized and jointly learned during the training process.
Sentence Attention
Similarly, contextual sentences do
not contribute equally to the semantic meaning of a document. In light of this, we introduce the sentence-level attention to weight sentences of each document and output
with a weighted sum of all sentences’ embedding vectors.
We calculate the document vector vd in the same manner
upon the sentence vectors {vo
o=1 we have obtained. We
use a sentence-level query vector qc to acquire the importance weights of sentences. This yields,
ho = tanh (Wcvo
a=1 Mc(a) exp (hTa qc),
where Mc(o) is a sentence-level mask function for avoiding
the effect of padding sentences. When the sentence memory
mo is completely free, Mc(o) equals 0 and otherwise 1. The
sentence-level query vector qc is expected to be a high-level
representation of the query “what is the important sentence
over the document for the task”. qc is randomly initialized
and jointly learned during the training process.
Hierarchical Position Attention
The hierarchical content attention model we have described
above, however, involves no recurrence and no convolution.
To fully take advantage of the order in each sequence, we
add positional encodings to our model. Researchers have
explored various types of positional encodings , including ﬁxed and learned ones, for
different tasks. Different from previous works, we propose
a hierarchical positional encoding scheme consisting of a
word positional encoding pw and a sentence positional encoding pc. Such hierarchical positional encodings stay consistent with the hierarchical content mechanism and consider the order information of both words and sentences.
In terms of the word positional encoding, we update each
piece of memory by adding a vector, i.e., mor = eor +
w, ∀r ∈[1, nw]. As for the sentence positional encoding,
we add a vector to each sentence’s embedding vector, i.e.,
r=1 αorhor + po
c, ∀o ∈[1, nc]. pw and pc shared by
the P-net and NP-net are all randomly initialized and jointly
learned during the training process.
Individual Attention Learning
In this section, we introduce loss functions for training the
P-net and NP-net, respectively.
P-net is used to learn the domain-shared feature
representations that contribute to sentiment classiﬁcation.
Formally, we parameterize the P-net by H(x; θP ) which
maps a sample x to a high-level document representation
vP . The loss used to train the P-net is made up of two parts:
Lp−net = Lsen(H(Xl
s; θP )) + Ldom.
The sentiment loss Lsen(H(Xl
s; θP )) is to minimize the
cross-entropy for the labeled data Xl
s in the source domain:
s; θP ))=−1
yi ln ˆyi+(1−yi) ln (1−ˆyi),
where yi ∈{0, 1} , ˆyi are the groundtruth and sentiment prediction for the ith source labeled sample x, respectively.
The domain adversarial loss Ldom enforces the P-net to
produce such domain-shared representations that the domain
classiﬁer cannot discriminate between domains via the Gradient Reversal Layer (GRL) . Mathematically, we deﬁne the GRL as Qλ(x)=x with a reversal
gradient ∂Qλ(x)
=−λI. As such, the domain classiﬁer can
be denoted as f (Qλ(H (x; θP )); θD) parameterized by θD.
Learning with a GRL is adversarial: on one hand, the reversal gradient enforces f to be maximized w.r.t. θP for all the
data from both domains; on the other hand, θD is optimized
by minimizing the cross-entropy domain classiﬁcation loss:
di ln ˆdi+(1−di) ln
where di ∈{0, 1} , ˆdi are the groundtruth and the domain
prediction for the ith sample.
NP-net is used to discover the domain-speciﬁc
feature representations for both domains and project them
into the domain-shared feature space. We parameterize the
NP-net as H(g(x); θNP ) where θNP maps a transformed
sample g(x) to a high-level document representation vNP .
The loss function to train the NP-net consists of three terms:
Lnp−net = Lsen(H(g(Xl
s); θNP )) + Lpos + Lneg.
The sentiment loss Lsen(H(g(Xl
s); θNP )) is formulated to
minimize the cross-entropy loss for all the transformed labeled data g(Xl
s) in the source domain:
Lsen(H(g(Xl
s); θNP ))=−1
′) ln (1−ˆyi
Table 1: Statistics of the Amazon reviews dataset including
the number of training, testing, and unlabeled reviews for
each domain as well as the portion of negative samples in
the unlabeled data.
Electronics
where yi′ ∈{0, 1} , ˆyi
′ are the groundtruth and prediction for
the ith transformed labeled sample g(x) in the source domain, respectively. Moreover, Lpos and Lneg denote the loss
functions to minimize the cross-entropy of positive and negative pivot predictions, respectively:
+) ln (1−ˆz+
−) ln (1−ˆz−
where zi+ ∈{0, 1} , ˆz+
are the groundtruth and positive
pivot prediction for the ith transformed sample, respectively,
and zi−∈{0, 1} , ˆz−
i are the groundtruth and negative pivot
prediction for the ith transformed sample, respectively.
Joint Attention Learning
Since the representations of P-net and NP-net are complementary, we conduct joint attention learning for them. For
the source labeled data Xl
s and its transformed data g(Xl
the representation H(Xl
s; θP ) produced by the P-net and the
representation H(g(Xl
s); θNP ) produced by the NP-net are
concatenated together for sentiment classiﬁcation. We combine the losses for both the P-net and NP-net together with a
regularizer to constitute the overall objective function:
L = Lsen(H(Xl
s; θP ) ⊕H(g(Xl
s); θNP ))
+ Ldom + Lpos + Lneg + ρLreg,
where ρ is a regularization parameter to balance the regularization term and other terms. The regularization term
Lreg is responsible of avoiding the overﬁtting by placing
the squared ℓ2 regularization on parameters for the sentiment classiﬁer, domain classiﬁer, and +(positive)/-(negative)
pivot predictors. The goal of the joint attention learning is to
minimize L with respect to all the model parameters except
the adversarial training part which will be maximized. All
the parameters are optimized jointly with the standard backpropagation algorithm.
Experiments
In this section, we empirically evaluate the performance of
the proposed HATN model.
Experimental Settings
We conduct the experiments on the Amazon reviews dataset
 , which has been widely
used for cross-domain sentiment classiﬁcation. This dataset
contains reviews from ﬁve products/domains: Books (B),
DVD (D), Electronics (E), Kitchen (K) and Video (V). There
are 6000 labeled reviews for each domain with 3000 positive reviews (higher than 3 stars) and 3000 negative reviews
(lower than 3 stars), as well as 9750 unlabeled reviews for
B, 11843 for D, 17009 for E, 13856 for K and 30180 for V.
Note that unlabeled data, which are imbalanced, consist of
more positive but less negative reviews. Table 1 summarizes
the statistics of the dataset. By following ,
we construct 20 cross-domain sentiment classiﬁcation tasks
like A→B, where A corresponds to the source domain and
B denotes the target domain. For each pair A→B, we randomly choose 2800 positive and 2800 negative reviews from
the source domain A as the training data, the rest from the
source domain A as the validation data, and all labeled reviews (6000) from the target domain B for testing.
Implementation Details
For each transfer pair A→B, we split documents into sentences and tokenize each sentence by NLTK . The memory size nc and nw are set to
20 and 25 respectively. We use the public 300-dimensional
word2vec vectors with the skip-gram model to initialize the embedding matrix L. They are shared
by P-net and NP-net and ﬁne-tuned during the training process. The hidden dimensions of the word attention layer
and sentence attention layer are 300. The weights in networks are randomly initialized from a uniform distribution
U [−0.01, 0.01]. The regularization weight ρ is set to 0.005.
For the pivots learned by P-net, we extract only adjectives,
adverbs, and verbs with a frequency of at least 5 and remove
stop words and negation words.
For training, the model is optimized with the stochastic gradient descent over shufﬂed mini-batches with momentum rate 0.9. Due to different training sizes for different classiﬁers, we use a batch size bs =50 for the sentiment classiﬁer, a batch size bd =100 for the domain classiﬁer with a half coming from the source and target domains, respectively, a batch size bs of source labeled data,
and a batch size bd of unlabeled data from both domains
in turn for the +(positive)/-(negative) pivot predictors. Gradients with the ℓ2 norm larger than 40 are normalized to
be 40. We deﬁne the training progress as p= t
T , where t
and T are current epoch and the maximum one, respectively, and T is set to 100. The learning rate is decayed
as η= max(
(1+10p)0.75 , 0.002) and the adaptation rate is
increased as λ= min(
1+exp(−10p) −1, 0.1) during training.
We perform early stopping on the validation set during the
training process.
Performance Comparison
The baseline methods in the comparison include:
• Source-only: it is a non-adaptive baseline method based
Table 2: Classiﬁcation accuracy on the Amazon reviews dataset
Source-only
on neural networks and uses the most frequent 5000 unigrams and bigrams between domains as features.
• SFA : it is a linear method, which aims to
align non-pivots and pivots by spectral feature alignment.
• DANN : it is based on the adversarial
training. DANN performs domain adaptation on the representation encoded in a 5000-dimension feature vector
of the most frequent unigrams and bigrams between domains.
• DAmSDA : it applies DANN on the
feature representation generated by the mSDA . The new representation is the concatenation of
the output of the 5 layers and the original input. Each example is encoded as a vector of 30000 dimensions.
• CNN-aux : it is based on the CNN
 and makes use of two auxiliary tasks to help
induce sentence embeddings.
• AMN : it learns domain-shared representations based on memory networks and adversarial training.
• P-net: it is the ﬁrst component of the proposed HATN
model without any positional embedding and makes use
of the domain-shared representations.
• NP-net: it is the second component of the proposed
HATN model without any positional embedding and
makes use of the domain-speciﬁc representations.
• HATN & HATNh: they are the proposed models that do
not contain the hierarchical positional encoding and contain the hierarchical positional encoding, respectively.
Table 2 reports the classiﬁcation accuracies of different
methods on the Amazon reviews dataset. We evaluate our
method over 20 transfer pairs, totally 120,000 testing reviews. The proposed HATNh model consistently achieves
the best performance on almost all the tasks. Source-only
performs poorly with 75.92% on average due to no adaptive methods applied. SFA only achieves 78.69% on average due to its poor discrete features and a linear classiﬁer
used. Besides, SFA highly depends on manual pivot selection methods which may not capture pivots accurately. On
the contrary, HATNh can automatically learn to capture pivots using the P-net attentions. Compared to the adversarial
training based approaches, HATNh outperforms DANN by
7.61%, DAmSDA by 4.25% and AMN by 3.82% on average, respectively. Besides, HATNh exceeds CNN-aux which
still needs to manually select positive and negative pivots by
4.63%. Possible reasons are that HATN can automatically
exploit better domain-shared representations with hierarchical attentions and make use of both pivot and non-pivot features which contribute more to the domain-shared representations than those only using the pivot features.
In order to validate the effectiveness of each component,
we compare with variants of the proposed HATNh. First, we
can see that P-net even outperforms the AMN that considers
only word attention by 2.77% on average. which proves that
hierarchical attention is more suitable for learning domainshared representations. To further show the effectiveness of
hierarchical attention, we also compare P-net with its variant that considers only sentence attention. P-net with hierarchical attention surpasses P-net with only sentence attention
(83.66%) by 1.9% on average. Second, it is reasonable that
NP-net only achieves 79.49% on average since the input for
NP-net removes all pivots that contribute more to domainshared features and it is insufﬁcient to do sentiment classiﬁcation. Third, HATN can get 86.20% on average better than
both P-net and NP-net, which proves that the representations
of P-net and NP-net are complementary. Moreover, HATNh
can further improve the performance of HATN by 0.41% on
average, which also validates that P-net and NP-net can be-
Figure 2: Visualization of attention of the HATNh in the B→E task. Label 1 denotes positive sentiment and label 0 denotes
negative sentiment. NP-net assigns zero attention weights to the pivots due to hiding them.
 
  
 
 
 
 
   
  
   
 
 
 
  

 
 
  
 
  
 
 
 
  
Figure 3: Samples of pivots and non-pivots captured by the
HATNh in the B→E task.
have better with hierarchical positional encoding.
Visualization of Attention
In order to validate that our model is able to identity pivots and non-pivots simultaneously with hierarchical attentions, we visualize the word and sentence attention layers of
the P-net and NP-net in Figure 2. Figure 2 shows that P-net
tends to pay higher word attentions to the pivots between
domains, such as positive pivots best, excellent, good and
negative pivots disappointed, poor, annoying. The sentences
that contain these pivots also get higher sentence attentions
in the P-net. Different from P-net, NP-net aims to pay higher
word attentions to the non-pivots in the two domains, such
as source non-pivots readable, insipid in the Books domain
and target non-pivots pixelated, fuzzy, distorted in the Electronics domain. The sentences that contain these non-pivots
also get higher sentence attentions in the NP-net. Overall,
the visualization of attentions illustrates that our model can
achieve transferring attentions between domains. As showed
in Figure 3, we list some examples of pivots and non-pivots
captured based on the attention weights of P-net and NP-net
respectively in the B→E task. These pivots and non-pivots
are crucial for cross-domain sentiment classiﬁcation.
Conclusion
In this paper, we propose the HATN method for crossdomain sentiment classiﬁcation. The proposed HATN can
transfer attentions for emotions in both word and sentence
levels across domains by automatically capturing pivots and
non-pivots, which provides a better interpretability of what
to transfer for emotions. Experiments on the Amazon review
dataset show the effectiveness of HATN. The proposed hierarchical attention transfer mechanism could be adapted to
other domain adaptation tasks such as text classiﬁcation and machine comprehension , which are the focus of our future studies.
Acknowledgement
We thank the support of WeChat-HKUST Joint Lab
on Artiﬁcial Intelligence Technology, National Grant
Fundamental Research (973 Program) of China under
Project 2014CB340304 and Hong Kong CERG projects
(16211214, 16209715 and 16244616), NSFC (61473087
and 61673202), and the Natural Science Foundation of
Jiangsu Province (BK20141340).