Hierarchical Attention Transfer Network for
Cross-Domain Sentiment ClassiÔ¨Åcation
Zheng Li, Ying Wei, Yu Zhang, Qiang Yang
Hong Kong University of Science and Technology, Hong Kong
 , , , 
Cross-domain sentiment classiÔ¨Åcation aims to leverage useful
information in a source domain to help do sentiment classiÔ¨Åcation in a target domain that has no or little supervised information. Existing cross-domain sentiment classiÔ¨Åcation methods cannot automatically capture non-pivots, i.e., the domainspeciÔ¨Åc sentiment words, and pivots, i.e., the domain-shared
sentiment words, simultaneously. In order to solve this problem, we propose a Hierarchical Attention Transfer Network
(HATN) for cross-domain sentiment classiÔ¨Åcation. The proposed HATN provides a hierarchical attention transfer mechanism which can transfer attentions for emotions across domains by automatically capturing pivots and non-pivots. Besides, the hierarchy of the attention mechanism mirrors the
hierarchical structure of documents, which can help locate the
pivots and non-pivots better. The proposed HATN consists of
two hierarchical attention networks, with one named P-net
aiming to Ô¨Ånd the pivots and the other named NP-net aligning the non-pivots by using the pivots as a bridge. Specifically, P-net Ô¨Årstly conducts individual attention learning to
provide positive and negative pivots for NP-net. Then, Pnet and NP-net conduct joint attention learning such that the
HATN can simultaneously capture pivots and non-pivots and
realize transferring attentions for emotions across domains.
Experiments on the Amazon review dataset demonstrate the
effectiveness of HATN.
Introduction
Users usually express opinions about products or services
on social media or review sites. It is helpful to correctly understand their emotional tendency. Sentiment classiÔ¨Åcation,
which aims to automatically determine the overall sentiment
polarity (e.g., positive or negative) of a document, has raised
continuous attentions over the past decades . Supervised learning algorithms that require labeled data have been successfully explored to build sentiment classiÔ¨Åers for a speciÔ¨Åc domain . However, there
exists insufÔ¨Åcient labeled data in a target domain of interest, where labeling data may be time-consuming and expensive. Cross-domain sentiment classiÔ¨Åcation, which borrows knowledge from related source domains with abun-
Copyright c‚Éù2018, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.
dant labeled data to improve the target domain, has become
a promising direction. However, the expression of users‚Äô
emotions usually varies across domains. For example, in
the Books domain, words readable and thoughtful are used
to express positive sentiment, whereas insipid and plotless
often indicate negative sentiment. On the other hand, in
the Electronics domain, rubbery and glossy express positive
sentiment, whereas words fuzzy and blurry usually express
negative sentiment. Due to the domain discrepancy, a sentiment classiÔ¨Åer trained in a source domain may not work
well when directly applied to a target domain.
To address the problem, researchers have proposed various methods for cross-domain sentiment classiÔ¨Åcation.
Blitzer et al. proposed
a Structural Correspondence Learning (SCL) method which
utilizes multiple pivot prediction tasks to infer the correlation between pivots and non-pivots. Pan et al. proposed the Spectral Feature Alignment (SFA) to
Ô¨Ånd an alignment between pivots and non-pivots by using
the cooccurrence between them. However, these methods
need to manually select pivots and they are based on discrete feature representations such as bag-of-words with linear classiÔ¨Åers. Recently, deep neural models are explored
to automatically produce superior feature representations
for cross-domain sentiment classiÔ¨Åcation. Stacked Denoising Auto-encoders (SDA) have been successfully adopted
to learn hidden representations shared across domains . Yu and
Jiang leveraged two auxiliary tasks
to learn sentence embeddings with a Convolutional Neural
Network (CNN) which works well across domains, while they still rely on manually identifying positive and negative pivots. Ganin et al. proposed the Domain-Adversarial
training of Neural Networks (DANN) which Ô¨Årst introduces
a domain classiÔ¨Åer incapable of discriminating representations from a source or a target domain by reversing the gradient direction of the neural network. In order to improve
the interpretability of deep models, Li et al. 
proposed an Adversarial Memory Network (AMN) to automatically identity the pivots by using the attention mechanism and adversarial training. Nevertheless, AMN only focuses on word-level attention and ignores the hierarchical
structure of documents, which may not accurately capture
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
pivots in long documents. Besides, it cannot automatically
capture and exploit non-pivots, which may result in the degraded performance when source and target domains only
have few overlapping pivot features.
To simultaneously harness the collective power of pivots
and non-pivots and interpret what to transfer, we introduce a
Hierarchical Attention Transfer Network (HATN) for crossdomain sentiment classiÔ¨Åcation. The proposed HATN provides a hierarchical attention transfer mechanism, which can
automatically transfer attentions for emotions in both word
and sentence levels across domains to reduce the domain
discrepancy and provide a better interpretability of what to
transfer. SpeciÔ¨Åcally, our framework consists of two hierarchical attention networks named P-net and NP-net, where
P-net aims to focus on pivots and NP-net is used to identify non-pivots by using the pivots as a bridge. Firstly, P-net
conducts individual attention learning to select positive and
negative pivots for NP-net. Then, P-net and NP-net conduct
joint attention learning such that the HATN can simultaneously identify the pivots and non-pivots and achieve transferring attentions for emotions across domains.
Our contributions are summarized as follows:
‚Ä¢ We propose a hierarchical attention transfer mechanism,
which can transfer attentions for emotions across domains
by automatically capturing the pivots and non-pivots simultaneously. Besides, it can tell what to transfer in the
hierarchical attention, which makes the representations
shared by domains more interpretable.
‚Ä¢ Empirically the proposed HATN method can signiÔ¨Åcantly
outperform the state-of-the-art methods.
Related Work
Domain Adaptation
Domain adaptation tasks such as
cross-domain sentiment classiÔ¨Åcation have raised much attention in recent years. One line of work focuses on inducing a low-dimensional feature representation shared across
domains based on the cooccurrence between pivots and nonpivots. Unfortunately, they highly rely on manually selecting
pivots based on term frequencies on both domains , mutual information between
features and labels of a source domain , mutual information between features and domains , and weighted log-likelihood ratio . These pivot selection methods are
very tedious and may not identify the pivots accurately.
Recently, deep learning methods form another line of
work to automatically produce superior feature representations for cross-domain sentiment classiÔ¨Åcation. SDA is proposed to learn to discover intermediate representations shared across domains.
In order to improve the speed and scalability of SDA
for high-dimensional data, Chen et al. 
proposed a Marginalized Stacked Denoising Autoencoder
(mSDA). Yu et al. used two auxiliary tasks to help induce sentence embeddings with CNN
across domains. Ganin et al. proposed the DANN, which leverages
the domain adversarial training method to make the neural
network produce representations confusing a domain classi-
Ô¨Åer. In order to improve the interpretability of deep models,
Li et al. incorporated memory networks into
DANN to automatically identify the pivots.
Attention Mechanism in NLP
The attention mechanism has been successfully exploited in various NLP tasks
such as machine translation , question answering , document classiÔ¨Åcation and sentiment analysis . The intuition behind the
attention mechanism is that each low-level position contributes a different importance for the high-level representation. Moreover, the hierarchical attention mechanism
has been proved to be better than the word-level attention in various document-based tasks , which mirrors the hierarchical structure of documents in order to extract more powerful features.
Hierarchical Attention Transfer Network
In this section, we introduce the proposed HATN model for
cross-domain sentiment classiÔ¨Åcation. We Ô¨Årst present the
problem deÔ¨Ånition and notations, followed by an overview
of the model. Then we detail the model with all components.
Problem DeÔ¨Ånition and Notations
We are given two domains Ds and Dt which denote a source
domain and a target domain, respectively. Suppose that we
have a set of labeled data Xl
i=1 and {yi
i=1 as well
as some unlabeled data Xu
s+1 in the source
domain Ds, where Xs = Xl
s. Besides, a set of unlabeled data Xt = {xj
j=1 is available in the target domain
Dt. The goal of cross-domain sentiment classiÔ¨Åcation is to
train a robust classiÔ¨Åer on labeled samples in the source domain Xl
s and adapt it to predict the sentiment polarity of
unlabeled examples Xt in the target domain, which is also
widely known as unsupervised domain adaptation.
An Overview of the HATN Model
In this section, we present an overview of the proposed
HATN model for cross-domain sentiment classiÔ¨Åcation.
The goal of HATN is to transfer attentions for emotions
across domains, i.e, automatically capturing the pivots as
well as non-pivots. Therefore, we design two hierarchical
attention networks with different attentions for the target.
As illustrated in Figure 1, the Ô¨Årst network is named Pnet, which aims to identify the pivots shared by the source
and target domains such as great in a sample x. The second network is named NP-net which is used to capture the
non-pivots across domains such as readable in a transformed
sample g(x), generated by hiding all the pivots like the positive pivots great in the sample x. We realize the ‚Äòhide‚Äô action
by replacing the pivots with padding words.
In order to demonstrate the effects of each network
clearly, we describe the input, objective and motivation for
the P-net and NP-net, respectively.
 ›î
 
 
 
 " 
 $" 
%!&
'!(
 
 




Figure 1: The framework of the HATN model.
P-net aims to capture the pivots, which have two attributes:
(1) They are important sentiment words for sentiment classiÔ¨Åcation. (2) They are shared by both domains. In order to
achieve this goal, the labeled data Xl
s in the source domain
is fed into the P-net for sentiment classiÔ¨Åcation and in the
meanwhile, all the data Xs and Xt in both domains are fed
into the P-net for domain classiÔ¨Åcation that aims to predict
the domain label of the samples, i.e, coming from the source
or target domain. Here we perform the domain classiÔ¨Åcation
based adversarial training by the Gradient Reversal Layer
(GRL) such that make the domain classiÔ¨Åer indiscriminative between the representations from the
source and target domains. In this way, it guarantees representations from the P-net are both domain-shared and useful
for sentiment classiÔ¨Åcation, and can identity the pivots with
the attention mechanism.
NP-net aims to capture the non-pivots with two characteristics: (1) They are the useful sentiment words for sentiment
classiÔ¨Åcation. (2) They are domain-speciÔ¨Åc words. To reach
the goal, the transformed labeled data g(Xl
s) in the source
domain Ds, generated by hiding all the pivots identiÔ¨Åed by
the P-net, is fed into the NP-net for sentiment classiÔ¨Åcation.
And at the same time, all transformed data g(Xs) and g(Xt)
in both domains Ds and Dt, generated by the same way, is
fed to NP-net for +(positive)/-(negative) pivot predictions.
The two tasks aim to predict whether an original sample x
contains positive or negative pivots based on the transformed
sample g (x). The transformed sample g (x) has two labels,
a label z+ indicating whether x contains at least one positive pivot and a label z‚àíindicating whether x contains at
least one negative pivot. The intuition behind is that positive
non-pivots tend to co-occur with positive pivots and negative non-pivots tend to co-occur with negative pivots. In this
way, the NP-net can discover domain-speciÔ¨Åc features with
the pivots as a bridge and capture the non-pivots that are expected to correlate closely to the pivots with the attention
mechanism.
Training Process: Note that the NP-net needs the positive
and negative pivots as a bridge across domains. Different
from traditional methods that need to manually select pivots, the P-net possesses the ability of automatically Ô¨Ånding
the pivots. Therefore, our training process consists of two
‚Ä¢ Individual Attention Learning: The P-net is trained for
cross-domain sentiment classiÔ¨Åcation. We use the best parameters for P-net with early stopping on the validation
set, and then select a word with the highest word attention
in the sentence with the highest sentence attention in each
positive review of the source domain as a positive pivot.
The negative pivots are obtained in a similar way.
‚Ä¢ Joint Attention Learning: The P-net and NP-net are jointly
trained for cross-domain sentiment classiÔ¨Åcation. The labeled data Xl
s and its transformed data g(Xl
source domain Ds are simultaneously fed into P-net and
NP-net respectively and their representations are concatenated for sentiment classiÔ¨Åcation. Note that, the transformed labeled data g(Xl
s) fed to the NP-net are used for
sentiment classiÔ¨Åcation and +(positive)/-(negative) pivot
predictions simultaneously, but all transformed unlabeled
s) and g(Xt) fed to the NP-net can only be used
for the +(positive)/-(negative) pivot predictions.
Hierarchical Content Attention
In the following, we present how to build the document representation progressively from word vectors hierarchically.
Word Attention
The contextual words contribute unequally to the semantic meaning of a sentence, especially
when we focus on a speciÔ¨Åc task, e.g., sentiment classiÔ¨Åcation. Therefore, we introduce the word-level attention to
weight words of each sentence and output with a weighted
sum of all words‚Äô information.
Assume that a document x is made up of nc sentences
o=1. Given a sentence co ={wor}nw
r=1, we Ô¨Årst map
each word into its embedding vector as eor =Lwor throughout an embedding matrix L, where eor ‚ààRne√ó1. All contextual word embedding vectors {{eor}nw
o=1 are stacked
to the external memory m ‚ààRne√ónw√ónc, where free memories are padded with zeros vectors. We take each sentence
memory mo ‚ààRne√ónw and a word-level query vector qw as
the input of the word attention layer. By feeding each word
memory mor into a one-layer neural network, we obtain the
hidden representation of the rth word of the oth sentence as:
hor = tanh (Wwmor+bw) .
The importance weight of this word is therefore measured
as the similarity between hor and qw, which is further normalized through a mask softmax function as:
Mw(o, r) exp
k=1 Mw(o, k) exp (hT
where Mw(o, r) is a word-level mask function in order to
avoid the effect of padding vectors. When the word memory
mor is occupied, Mw(o, r) equals 1 and otherwise 0. The
sentence vector vo
c is Ô¨Ånally output as a weighted sum of all
hidden representations {hor}nw
Note that the word-level query vector qw is expected to be
a high level representation of the query ‚Äúwhat is the important word over the sentence for the task‚Äù. qw is randomly
initialized and jointly learned during the training process.
Sentence Attention
Similarly, contextual sentences do
not contribute equally to the semantic meaning of a document. In light of this, we introduce the sentence-level attention to weight sentences of each document and output
with a weighted sum of all sentences‚Äô embedding vectors.
We calculate the document vector vd in the same manner
upon the sentence vectors {vo
o=1 we have obtained. We
use a sentence-level query vector qc to acquire the importance weights of sentences. This yields,
ho = tanh (Wcvo
a=1 Mc(a) exp (hTa qc),
where Mc(o) is a sentence-level mask function for avoiding
the effect of padding sentences. When the sentence memory
mo is completely free, Mc(o) equals 0 and otherwise 1. The
sentence-level query vector qc is expected to be a high-level
representation of the query ‚Äúwhat is the important sentence
over the document for the task‚Äù. qc is randomly initialized
and jointly learned during the training process.
Hierarchical Position Attention
The hierarchical content attention model we have described
above, however, involves no recurrence and no convolution.
To fully take advantage of the order in each sequence, we
add positional encodings to our model. Researchers have
explored various types of positional encodings , including Ô¨Åxed and learned ones, for
different tasks. Different from previous works, we propose
a hierarchical positional encoding scheme consisting of a
word positional encoding pw and a sentence positional encoding pc. Such hierarchical positional encodings stay consistent with the hierarchical content mechanism and consider the order information of both words and sentences.
In terms of the word positional encoding, we update each
piece of memory by adding a vector, i.e., mor = eor +
w, ‚àÄr ‚àà[1, nw]. As for the sentence positional encoding,
we add a vector to each sentence‚Äôs embedding vector, i.e.,
r=1 Œ±orhor + po
c, ‚àÄo ‚àà[1, nc]. pw and pc shared by
the P-net and NP-net are all randomly initialized and jointly
learned during the training process.
Individual Attention Learning
In this section, we introduce loss functions for training the
P-net and NP-net, respectively.
P-net is used to learn the domain-shared feature
representations that contribute to sentiment classiÔ¨Åcation.
Formally, we parameterize the P-net by H(x; Œ∏P ) which
maps a sample x to a high-level document representation
vP . The loss used to train the P-net is made up of two parts:
Lp‚àínet = Lsen(H(Xl
s; Œ∏P )) + Ldom.
The sentiment loss Lsen(H(Xl
s; Œ∏P )) is to minimize the
cross-entropy for the labeled data Xl
s in the source domain:
s; Œ∏P ))=‚àí1
yi ln ÀÜyi+(1‚àíyi) ln (1‚àíÀÜyi),
where yi ‚àà{0, 1} , ÀÜyi are the groundtruth and sentiment prediction for the ith source labeled sample x, respectively.
The domain adversarial loss Ldom enforces the P-net to
produce such domain-shared representations that the domain
classiÔ¨Åer cannot discriminate between domains via the Gradient Reversal Layer (GRL) . Mathematically, we deÔ¨Åne the GRL as QŒª(x)=x with a reversal
gradient ‚àÇQŒª(x)
=‚àíŒªI. As such, the domain classiÔ¨Åer can
be denoted as f (QŒª(H (x; Œ∏P )); Œ∏D) parameterized by Œ∏D.
Learning with a GRL is adversarial: on one hand, the reversal gradient enforces f to be maximized w.r.t. Œ∏P for all the
data from both domains; on the other hand, Œ∏D is optimized
by minimizing the cross-entropy domain classiÔ¨Åcation loss:
di ln ÀÜdi+(1‚àídi) ln
where di ‚àà{0, 1} , ÀÜdi are the groundtruth and the domain
prediction for the ith sample.
NP-net is used to discover the domain-speciÔ¨Åc
feature representations for both domains and project them
into the domain-shared feature space. We parameterize the
NP-net as H(g(x); Œ∏NP ) where Œ∏NP maps a transformed
sample g(x) to a high-level document representation vNP .
The loss function to train the NP-net consists of three terms:
Lnp‚àínet = Lsen(H(g(Xl
s); Œ∏NP )) + Lpos + Lneg.
The sentiment loss Lsen(H(g(Xl
s); Œ∏NP )) is formulated to
minimize the cross-entropy loss for all the transformed labeled data g(Xl
s) in the source domain:
Lsen(H(g(Xl
s); Œ∏NP ))=‚àí1
‚Ä≤) ln (1‚àíÀÜyi
Table 1: Statistics of the Amazon reviews dataset including
the number of training, testing, and unlabeled reviews for
each domain as well as the portion of negative samples in
the unlabeled data.
Electronics
where yi‚Ä≤ ‚àà{0, 1} , ÀÜyi
‚Ä≤ are the groundtruth and prediction for
the ith transformed labeled sample g(x) in the source domain, respectively. Moreover, Lpos and Lneg denote the loss
functions to minimize the cross-entropy of positive and negative pivot predictions, respectively:
+) ln (1‚àíÀÜz+
‚àí) ln (1‚àíÀÜz‚àí
where zi+ ‚àà{0, 1} , ÀÜz+
are the groundtruth and positive
pivot prediction for the ith transformed sample, respectively,
and zi‚àí‚àà{0, 1} , ÀÜz‚àí
i are the groundtruth and negative pivot
prediction for the ith transformed sample, respectively.
Joint Attention Learning
Since the representations of P-net and NP-net are complementary, we conduct joint attention learning for them. For
the source labeled data Xl
s and its transformed data g(Xl
the representation H(Xl
s; Œ∏P ) produced by the P-net and the
representation H(g(Xl
s); Œ∏NP ) produced by the NP-net are
concatenated together for sentiment classiÔ¨Åcation. We combine the losses for both the P-net and NP-net together with a
regularizer to constitute the overall objective function:
L = Lsen(H(Xl
s; Œ∏P ) ‚äïH(g(Xl
s); Œ∏NP ))
+ Ldom + Lpos + Lneg + œÅLreg,
where œÅ is a regularization parameter to balance the regularization term and other terms. The regularization term
Lreg is responsible of avoiding the overÔ¨Åtting by placing
the squared ‚Ñì2 regularization on parameters for the sentiment classiÔ¨Åer, domain classiÔ¨Åer, and +(positive)/-(negative)
pivot predictors. The goal of the joint attention learning is to
minimize L with respect to all the model parameters except
the adversarial training part which will be maximized. All
the parameters are optimized jointly with the standard backpropagation algorithm.
Experiments
In this section, we empirically evaluate the performance of
the proposed HATN model.
Experimental Settings
We conduct the experiments on the Amazon reviews dataset
 , which has been widely
used for cross-domain sentiment classiÔ¨Åcation. This dataset
contains reviews from Ô¨Åve products/domains: Books (B),
DVD (D), Electronics (E), Kitchen (K) and Video (V). There
are 6000 labeled reviews for each domain with 3000 positive reviews (higher than 3 stars) and 3000 negative reviews
(lower than 3 stars), as well as 9750 unlabeled reviews for
B, 11843 for D, 17009 for E, 13856 for K and 30180 for V.
Note that unlabeled data, which are imbalanced, consist of
more positive but less negative reviews. Table 1 summarizes
the statistics of the dataset. By following ,
we construct 20 cross-domain sentiment classiÔ¨Åcation tasks
like A‚ÜíB, where A corresponds to the source domain and
B denotes the target domain. For each pair A‚ÜíB, we randomly choose 2800 positive and 2800 negative reviews from
the source domain A as the training data, the rest from the
source domain A as the validation data, and all labeled reviews (6000) from the target domain B for testing.
Implementation Details
For each transfer pair A‚ÜíB, we split documents into sentences and tokenize each sentence by NLTK . The memory size nc and nw are set to
20 and 25 respectively. We use the public 300-dimensional
word2vec vectors with the skip-gram model to initialize the embedding matrix L. They are shared
by P-net and NP-net and Ô¨Åne-tuned during the training process. The hidden dimensions of the word attention layer
and sentence attention layer are 300. The weights in networks are randomly initialized from a uniform distribution
U [‚àí0.01, 0.01]. The regularization weight œÅ is set to 0.005.
For the pivots learned by P-net, we extract only adjectives,
adverbs, and verbs with a frequency of at least 5 and remove
stop words and negation words.
For training, the model is optimized with the stochastic gradient descent over shufÔ¨Çed mini-batches with momentum rate 0.9. Due to different training sizes for different classiÔ¨Åers, we use a batch size bs =50 for the sentiment classiÔ¨Åer, a batch size bd =100 for the domain classiÔ¨Åer with a half coming from the source and target domains, respectively, a batch size bs of source labeled data,
and a batch size bd of unlabeled data from both domains
in turn for the +(positive)/-(negative) pivot predictors. Gradients with the ‚Ñì2 norm larger than 40 are normalized to
be 40. We deÔ¨Åne the training progress as p= t
T , where t
and T are current epoch and the maximum one, respectively, and T is set to 100. The learning rate is decayed
as Œ∑= max(
(1+10p)0.75 , 0.002) and the adaptation rate is
increased as Œª= min(
1+exp(‚àí10p) ‚àí1, 0.1) during training.
We perform early stopping on the validation set during the
training process.
Performance Comparison
The baseline methods in the comparison include:
‚Ä¢ Source-only: it is a non-adaptive baseline method based
Table 2: ClassiÔ¨Åcation accuracy on the Amazon reviews dataset
Source-only
on neural networks and uses the most frequent 5000 unigrams and bigrams between domains as features.
‚Ä¢ SFA : it is a linear method, which aims to
align non-pivots and pivots by spectral feature alignment.
‚Ä¢ DANN : it is based on the adversarial
training. DANN performs domain adaptation on the representation encoded in a 5000-dimension feature vector
of the most frequent unigrams and bigrams between domains.
‚Ä¢ DAmSDA : it applies DANN on the
feature representation generated by the mSDA . The new representation is the concatenation of
the output of the 5 layers and the original input. Each example is encoded as a vector of 30000 dimensions.
‚Ä¢ CNN-aux : it is based on the CNN
 and makes use of two auxiliary tasks to help
induce sentence embeddings.
‚Ä¢ AMN : it learns domain-shared representations based on memory networks and adversarial training.
‚Ä¢ P-net: it is the Ô¨Årst component of the proposed HATN
model without any positional embedding and makes use
of the domain-shared representations.
‚Ä¢ NP-net: it is the second component of the proposed
HATN model without any positional embedding and
makes use of the domain-speciÔ¨Åc representations.
‚Ä¢ HATN & HATNh: they are the proposed models that do
not contain the hierarchical positional encoding and contain the hierarchical positional encoding, respectively.
Table 2 reports the classiÔ¨Åcation accuracies of different
methods on the Amazon reviews dataset. We evaluate our
method over 20 transfer pairs, totally 120,000 testing reviews. The proposed HATNh model consistently achieves
the best performance on almost all the tasks. Source-only
performs poorly with 75.92% on average due to no adaptive methods applied. SFA only achieves 78.69% on average due to its poor discrete features and a linear classiÔ¨Åer
used. Besides, SFA highly depends on manual pivot selection methods which may not capture pivots accurately. On
the contrary, HATNh can automatically learn to capture pivots using the P-net attentions. Compared to the adversarial
training based approaches, HATNh outperforms DANN by
7.61%, DAmSDA by 4.25% and AMN by 3.82% on average, respectively. Besides, HATNh exceeds CNN-aux which
still needs to manually select positive and negative pivots by
4.63%. Possible reasons are that HATN can automatically
exploit better domain-shared representations with hierarchical attentions and make use of both pivot and non-pivot features which contribute more to the domain-shared representations than those only using the pivot features.
In order to validate the effectiveness of each component,
we compare with variants of the proposed HATNh. First, we
can see that P-net even outperforms the AMN that considers
only word attention by 2.77% on average. which proves that
hierarchical attention is more suitable for learning domainshared representations. To further show the effectiveness of
hierarchical attention, we also compare P-net with its variant that considers only sentence attention. P-net with hierarchical attention surpasses P-net with only sentence attention
(83.66%) by 1.9% on average. Second, it is reasonable that
NP-net only achieves 79.49% on average since the input for
NP-net removes all pivots that contribute more to domainshared features and it is insufÔ¨Åcient to do sentiment classiÔ¨Åcation. Third, HATN can get 86.20% on average better than
both P-net and NP-net, which proves that the representations
of P-net and NP-net are complementary. Moreover, HATNh
can further improve the performance of HATN by 0.41% on
average, which also validates that P-net and NP-net can be-
Figure 2: Visualization of attention of the HATNh in the B‚ÜíE task. Label 1 denotes positive sentiment and label 0 denotes
negative sentiment. NP-net assigns zero attention weights to the pivots due to hiding them.
 
  
 
 
 
 
   
  
   
 
 
 
  

 
 
  
 
  
 
 
 
  
Figure 3: Samples of pivots and non-pivots captured by the
HATNh in the B‚ÜíE task.
have better with hierarchical positional encoding.
Visualization of Attention
In order to validate that our model is able to identity pivots and non-pivots simultaneously with hierarchical attentions, we visualize the word and sentence attention layers of
the P-net and NP-net in Figure 2. Figure 2 shows that P-net
tends to pay higher word attentions to the pivots between
domains, such as positive pivots best, excellent, good and
negative pivots disappointed, poor, annoying. The sentences
that contain these pivots also get higher sentence attentions
in the P-net. Different from P-net, NP-net aims to pay higher
word attentions to the non-pivots in the two domains, such
as source non-pivots readable, insipid in the Books domain
and target non-pivots pixelated, fuzzy, distorted in the Electronics domain. The sentences that contain these non-pivots
also get higher sentence attentions in the NP-net. Overall,
the visualization of attentions illustrates that our model can
achieve transferring attentions between domains. As showed
in Figure 3, we list some examples of pivots and non-pivots
captured based on the attention weights of P-net and NP-net
respectively in the B‚ÜíE task. These pivots and non-pivots
are crucial for cross-domain sentiment classiÔ¨Åcation.
Conclusion
In this paper, we propose the HATN method for crossdomain sentiment classiÔ¨Åcation. The proposed HATN can
transfer attentions for emotions in both word and sentence
levels across domains by automatically capturing pivots and
non-pivots, which provides a better interpretability of what
to transfer for emotions. Experiments on the Amazon review
dataset show the effectiveness of HATN. The proposed hierarchical attention transfer mechanism could be adapted to
other domain adaptation tasks such as text classiÔ¨Åcation and machine comprehension , which are the focus of our future studies.
Acknowledgement
We thank the support of WeChat-HKUST Joint Lab
on ArtiÔ¨Åcial Intelligence Technology, National Grant
Fundamental Research (973 Program) of China under
Project 2014CB340304 and Hong Kong CERG projects
(16211214, 16209715 and 16244616), NSFC (61473087
and 61673202), and the Natural Science Foundation of
Jiangsu Province (BK20141340).