Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5938–5951
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Curate and Generate: A Corpus and Method for Joint Control of
Semantics and Style in Neural NLG
Shereen Oraby, Vrindavan Harrison, Abteen Ebrahimi, and Marilyn Walker
Natural Language and Dialog Systems Lab
University of California, Santa Cruz
{soraby,vharriso,aaebrahi,mawalker}@ucsc.edu
Neural natural language generation (NNLG)
from structured meaning representations has
become increasingly popular in recent years.
While we have seen progress with generating
syntactically correct utterances that preserve
semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in
NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style
markup by using data from freely available
and naturally descriptive user reviews, and (2)
systematically exploring how the style markup
enables joint control of semantic and stylistic
aspects of neural model output. We present
YELPNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel
methodology that can be scalably reused to
generate NLG datasets for other domains. The
experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple
style targets without sacriﬁcing semantics.
Introduction
The increasing popularity of personal assistant dialog systems and the success of end-to-end neural models on problems such as machine translation has lead to a surge of interest around data-totext neural natural language generation (NNLG).
State-of-the-art NNLG models commonly use a
sequence-to-sequence framework for end-to-end
neural language generation, taking a meaning representation (MR) as input, and generating a natural language (NL) realization as output . Table 1 shows
some examples of MR to human and system NL realizations from recently popular NNLG datasets.
The real power of NNLG models over traditional statistical generators is their ability to produce natural language output from structured input
in a completely data-driven way, without needing
hand-crafted rules or templates. However, these
models suffer from two critical bottlenecks: (1)
a data bottleneck, i.e. the lack of large parallel
training data of MR to NL, and (2) a control bottleneck, i.e. the inability to systematically control
important aspects of the generated output to allow
for more stylistic variation.
Recent efforts to address the data bottleneck
with large corpora for training neural generators
have relied almost entirely on high-effort, costly
crowdsourcing, asking humans to write references
given an input MR. Table 1 shows two recent efforts: the E2E NLG challenge and the WEBNLG challenge , both with an example of an MR, human reference, and system realization. The largest
dataset, E2E, consists of 50k instances.
datasets, such as the Laptop (13k) and TV (7k)
product review datasets, are similar but smaller
 .
These datasets were created primarily to focus on the task of semantic ﬁdelity, and thus it
is very evident from comparing the human and
system outputs from each system that the model
realizations are less ﬂuent, descriptive, and natural than the human reference. Also, the nature
of the domains (restaurant description, Wikipedia
infoboxes, and technical product reviews) are not
particularly descriptive, exhibiting little variation.
Other work has also focused on the control bottleneck in NNLG, but has zoned in on one particular dimension of style, such as sentiment, length,
1 - E2E 
50k - Crowdsourcing (Domain: Restaurant Description)
MR: name[Blue Spice], eatType[restaurant], food[English],
area[riverside], familyFriendly[yes], near[Rainbow Vegetarian Cafe]
Human: Situated near the Rainbow Vegetarian Cafe in the
riverside area of the city, The Blue Spice restaurant is ideal if
you fancy traditional English food whilst out with the kids.
System: Blue Spice is a family friendly English restaurant in
the riverside area near Rainbow Vegetarian Cafe.
2 - WebNLG 
21k - DBPedia and Crowdsourcing (Domain: Wikipedia)
MR: (Buzz-Aldrin, mission, Apollo-11), (Buzz-Aldrin, birthname, “Edwin Eugene Aldrin Jr.”), (Buzz-Aldrin, awards, 20),
(Apollo-11, operator, NASA)
Human: Buzz Aldrin (born as Edwin Eugene Aldrin Jr) was
a crew member for NASA’s Apollo 11 and had 20 awards.
System: Buzz aldrin, who was born in edwin eugene aldrin
jr., was a crew member of the nasa operated apollo 11. he was
awarded 20 by nasa.
3 - YelpNLG (this work)
300k - Auto. Extraction (Domain: Restaurant Review)
(attr=food,
adj=no-adj,
mention=1),
(attr=food,
val=ﬂour-tortilla,
adj=small,
mention=1),
(attr=food, val=beef, adj=marinated, mention=1), (attr=food,
val=sauce, adj=spicy, mention=1)
+[sentiment=positive, len=long, ﬁrst-person=false, exclamation=false]
Human: The taco was a small ﬂour tortilla topped with
marinated grilled beef, asian slaw and a spicy delicious
System: The taco was a small ﬂour tortilla with marinated
beef and a spicy sauce that was a nice touch.
Table 1: A comparison of popular NNLG datasets.
(1/5 star) I want to curse everyone I know who recommended
this craptacular buffet. [...] It’s absurdly overpriced at more
than $50 a person for dinner. What do you get for that princely
sum? Some cold crab legs (it’s NOT King Crab, either, despite what others are saying) Shrimp cocktail (several of which
weren’t even deveined. GROSS. [...])
(5/5 star) One of my new fave buffets in Vegas! Very cute interior, and lots of yummy foods! [...] The delicious Fresh, delicious king grab legs!! [...]REALLY yummy desserts! [...] All
were grrreat, but that tres leches was ridiculously delicious.
Table 2: Yelp restaurant reviews for the same business.
or formality can this freely available data be used for training NNLG models?, and
(2) is it possible to exploit the variation in the data
to develop models that jointly control multiple interacting aspects of semantics and style?
We address these questions by creating the
YELPNLG corpus, consisting of 300k MR to reference pairs for training NNLGs, collected completely automatically using freely available data
(such as that in Table 2), and off-the-shelf tools.1
Rather than starting with a meaning representation
and collecting human references, we begin with
the references (in the form of review sentences),
and work backwards – systematically constructing
meaning representations for the sentences using
dependency parses and rich sets of lexical, syntactic, and sentiment information, including ontological knowledge from DBPedia. This method
uniquely exploits existing data which is naturally
rich in semantic content, emotion, and varied language. Row 3 of Table 1 shows an example MR
from YELPNLG, consisting of relational tuples of
attributes, values, adjectives, and order information, as well as sentence-level information including sentiment, length, and pronouns.
Once we have created the YELPNLG corpus,
we are in the unique position of being able to
explore, for the ﬁrst time, how varying levels of
supervision in the encoding of content, lexical
choice, and sentiment can be exploited to control
style in NNLG. Our contributions include:
• A new corpus, YELPNLG, larger and more
lexically and stylistically varied than existing
NLG datasets;
• A method for creating corpora such as
YELPNLG, which should be applicable to
other domains;
• Experiments on controlling multiple interacting aspects of style with an NNLG while
maintaining semantic ﬁdelity, and results using a broad range of evaluation methods;
• The ﬁrst experiments, to our knowledge,
showing that an NNLG can be trained to control lexical choice of adjectives.
We leave a detailed review of prior work to Section 5 where we can compare it with our own.
1 
Figure 1: Extracting information from a review sentence parse to create an MR.
Creating the YelpNLG Corpus
We begin with reviews from the Yelp challenge
dataset,2 which is publicly available and includes
structured information for attributes such as location, ambience, and parking availability for over
150k businesses, with around 4 million reviews in
total. We note that this domain and dataset are particularly unique in how naturally descriptive the
language used is, as exempliﬁed in Table 2, especially compared to other datasets previously used
for NLG in domains such as Wikipedia.
For corpus creation, we must ﬁrst sample sentences from reviews in such a way as to allow the
automatic and reliable construction of MRs using
fully automatic tools. To identify restaurant attributes, we use restaurant lexicons from our previous work on template-based NLG . The lexicons include ﬁve attribute types
prevalent in restaurant reviews: restaurant-type,
cuisine, food, service, and staff collected from
Wikipedia and DBpedia, including, for example,
around 4k for foods (e.g. “sushi”), and around 40
for cuisines (e.g. “Italian”). We then expand these
basic lexicons by adding in attributes for ambiance
(e.g. “decoration”) and price (e.g. “cost”) using
vocabulary items from the E2E generation challenge .
To enforce some semantic constraints and “truth
grounding” when selecting sentences without
severely limiting variability, we only select sentences that mention particular food values. A pilot
analysis of random reviews show that some of the
most commonly mentioned foods are meat items,
“meat”, “beef”, “chicken”, “crab”, and
“steak”. Beginning with the original set of over
4 million business reviews, we sentence-tokenize
them and randomly sample a set of 500,000 sentences from restaurant reviews that mention of at
least one of the meat items (spanning around 3k
2 
unique restaurants, 170k users, and 340k reviews).
We ﬁlter to select sentences that are between 4
and 30 words in length: restricting the length increases the likelihood of a successful parse and reduces noise in the process of automatic MR construction. We parse the sentences using Stanford
dependency parser , removing any sentence that is tagged as a fragment.
We show a sample sentence parse in Figure 1.
We identify all nouns and search for them in the
attribute lexicons, constructing (attribute, value)
tuples if a noun is found in a lexicon, including
the full noun compound if applicable, e.g. (food,
chicken-chimichanga) in Figure 1.3 Next, for each
(attribute, value) tuple, we extract all amod, nsubj,
or compound relations between a noun value in
the lexicons and an adjective using the dependency
parse, resulting in (attribute, value, adjective) tuples. We add in “mention order” into the tuple
distinguish values mentioned multiple times in the
same reference.
We also collect sentence-level information to
encode additional style variables. For sentiment,
we tag each sentence with the sentiment inherited
from the “star rating” of the original review it appears in, binned into one of three values for lower
granularity: 1 for low review scores (1-2 stars), 2
for neutral scores (3 star), and 3 for high scores (4-
5 stars).4 To experiment with control of length, we
assign a length bin of short (≤10 words), medium
(10-20 words), and long (≥20 words). We also
include whether the sentence is in ﬁrst person.
For each sentence, we create 4 MR variations.
The simplest variation, BASE, contains only attributes and their values. The +ADJ version adds
adjectives, +SENT adds sentiment, and ﬁnally the
richest MR, +STYLE, adds style information on
3Including noun compounds allows us to identify new values that did not exist in our lexicons, thus automatically expanding them.
4A pilot experiment comparing this method with Stanford
sentiment showed that copying down the
original review ratings gives more reliable sentiment scores.
The chicken chimichanga was tasty but the beef was even better!
(attr=food, val=chicken chimichanga, adj=tasty, mention=1), (attr=food, val=beef, adj=no adj, mention=1)
+[sentiment=positive, len=medium, ﬁrst person=false, exclamation=true]
Food was pretty good ( i had a chicken wrap ) but service was crazy slow.
(attr=food, val=chicken wrap, adj=no adj, mention=1), (attr=service, val=service, adj=slow, mention=1)
+[sentiment=neutral, len=medium, ﬁrst person=true, exclamation=false]
The chicken was a bit bland ; i prefer spicy chicken or well seasoned chicken.
(attr=food, val=chicken, adj=bland, mention=1), (attr=food, val=chicken, adj=spicy, mention=2), (attr=food,
val=chicken, adj=seasoned, mention=3) +[sentiment=neutral, len=medium, ﬁrst person=true, exclamation=false]
The beef and chicken kebabs were succulent and worked well with buttered rice, broiled tomatoes and raw onions.
(attr=food, val=beef chicken kebabs, adj=succulent, mention=1), (attr=food, val=rice, adj=buttered, mention=1),
( attr=food, val=tomatoes, adj=broiled, mention=1), (attr=food, val=onions, adj=raw, mention=1)
+[sentiment=positive, len=long, ﬁrst person=false, exclamation=false]
Table 3: Sample sentences and automatically generated MRs from YELPNLG. Note the stylistic variation that is
marked up in the +STYLE MRs, especially compared to those in other corpora such as E2E or WEBNLG.
mention order, whether the sentence is ﬁrst person, and whether it contains an exclamation. Half
of the sentences are in ﬁrst person and around 10%
contain an exclamation, and both of these can contribute to controllable generation: previous work
has explored the effect of ﬁrst person sentences
on user perceptions of dialog systems , and exclamations may be correlated
with aspects of a hyperbolic style.
Table 3 shows sample sentences for the richest version of the MR (+STYLE) that we create. In Row 1, we see the MR from the example in Figure 1, showing an example of a NN
compound, “chicken chimichanga”, with adjective
“tasty”, and the other food item, “beef”, with no
retrieved adjective. Row 2 shows an example of
a “service” attribute with adjective “slow”, in the
ﬁrst person, and neutral sentiment. Note that in
this example, the method does not retrieve that the
“chicken wrap” is actually described as “good”,
based on the information available in the parse, but
that much of the other information in the sentence
is accurately captured. We expect the language
model to successfully smooth noise in the training data caused by parser or extraction errors.5
Row 3 shows an example of the value “chicken”
mentioned 3 times, each with different adjectives
(“bland”, “spicy”, and “seasoned”). Row 4 shows
an example of 4 foods and very positive sentiment.
Comparison to Previous Datasets
Table 4 compares YELPNLG to previous work
in terms of data size, unique vocab and adjec-
5We note that the Stanford dependency parser has a token-wise labeled attachment score
(LAS) of 90.7, but point out that for our MRs we are primarily
concerned with capturing NN compounds and adjective-noun
relations, which we evaluate in Section 2.2.
tives, entropy,6 average reference length (RefLen),
and examples of stylistic and structural variation
in terms of contrast (markers such as “but” and
“although”), and aggregation (e.g.
“both” and
“also”) , showing how
our dataset is much larger and more varied than
previous work. We note that the Laptop and E2E
datasets (which allow multiple sentences per references) have longer references on average than
YelpNLG (where references are always single sentences and have a maximum of 30 words). We
are interested in experimenting with longer references, possibly with multiple sentences, in future
Figure 2 shows the distribution of MR length,
in terms of the number of attribute-value tuples.
There is naturally a higher density of shorter MRs,
with around 13k instances from the dataset containing around 2.5 attribute-value tuples, but that
the MRs go up to 11 tuples in length.
Train Size
Train Vocab
Train # Adjs
Train Entropy
Train RefLen
% Refs w/ Contrast
% Refs w/ Aggreg.
Table 4: NLG corpus statistics from E2E , LAPTOP , and
YELPNLG (this work).
Quality Evaluation
We examine the quality of the MR extraction with a
qualitative study evaluating YELPNLG MR to NL
6We show the formula for entropy in Sec 4 on evaluation.
Number of Attributes per MR
Number of MRs
Figure 2: MR distribution in YELPNLG train.
pairs on various dimensions. Speciﬁcally, we evaluate content preservation (how much of the MR
content appears in the NL, speciﬁcally, nouns and
their corresponding adjectives from our parses),
ﬂuency (how “natural sounding” the NL is, aiming
for both grammatical errors and general ﬂuency),
and sentiment (what the perceived sentiment of
the NL is). We note that we conduct the same study
over our NNLG test outputs when we generate data
using YELPNLG in Section 4.3.
We randomly sample 200
MRs from the
YELPNLG dataset, along with their corresponding NL references, and ask 5 annotators on Mechanical Turk to rate each output on a 5 point Likert scale (where 1 is low and 5 is high for content and ﬂuency, and where 1 is negative and 5 is
positive for sentiment). For content and ﬂuency,
we compute the average score across all 5 raters
for each item, and average those scores to get a ﬁnal rating for each model, such that higher content
and ﬂuency scores are better. We compute sentiment error by converting the judgments into 3 bins
to match the Yelp review scores (as we did during
MR creation), ﬁnding the average rating for all 5
annotators per item, then computing the difference
between their average score and the true sentiment
rating in the reference text (from the original review), such that lower sentiment error is better.
The average ratings for content and ﬂuency
are high, at 4.63 and 4.44 out of 5, respectively, meaning that there are few mistakes in
marking attribute and value pairs in the NL references, and that the references are also ﬂuent.
This is an important check because correct grammar/spelling/punctuation is not a restriction in
Yelp reviews. For sentiment, the largest error is
0.58 (out of 3), meaning that the perceived sentiment by raters does not diverge greatly, on average, from the Yelp review sentiment assigned in
the MR, and indicates that inheriting sentence sentiment from the review is a reasonable heuristic.
Model Design
In the standard RNN encoder-decoder architecture commonly used for machine translation
 ,
the probability of a target sentence w1:T given a
source sentence x1:S is modeled as p(w1:T |x) =
1 p(wt|w1:t−1, x) .
In our case, the input is not a natural language source sentence as in traditional machine
translation; instead, the input x1:S is a meaning representation,
where each token xn is
itself a tuple of attribute and value features,
(fattr, fval).
input x1:S
as a sequence of attribute-value
pairs from an input MR.
For example, in the
[(attr=food,
val=steak),
(attr=food,
val=chicken)],
x = x1, x2, where x1=(fattr=food,fval=steak),
and x2=(fattr=food,fval=chicken).
The target
sequence is a natural language sentence, which in
this example might be, “The steak was extra juicy
and the chicken was delicious!”
Base encoding. During the encoding phase for
BASE MRs, the model takes as input the MR
as a sequence of attribute-value pairs.
We precompute separate vocabularies for attributes and
MR attributes are represented as vectors
MR values are represented with reduced
dimensional embeddings that get updated during
The attributes and values of the input
MR are concatenated to produce a sequence of
attribute-value pairs that then is encoded using a
multi-layer bidirectional LSTM .
Additional feature encoding.
For the +ADJ,
+SENT, and +STYLE MRs, each MR is a longer
relational tuple, with additional style feature information to encode, such that an input sequence
x1:S = (fattr, fval, f1:N), and where each fn is an
additional feature, such as adjective or mention order. Speciﬁcally in the case of +STYLE MRs, the
additional features may be sentence-level features,
such as sentiment, length, or exclamation.
In this case, we enforce additional constraints
on the models for +ADJ, +SENT, and +STYLE,
changing the conditional probability computation for w1:T given a source sentence x1:S to
p(w1:T |x) = QT
1 p(wt|w1:t−1, x, f), where f is
the set of new feature constraints to the model.
We represent these additional features as a
vector of additional supervision tokens or side
constraints .
construct a vector for each set of features, and
concatenate them to the end of each attributevalue pair, encoding the full sequence as for BASE
Target decoding. At each time step of the decoding phase the decoder computes a new decoder hidden state based on the previously predicted word and an attentionally-weighted average
of the encoder hidden states. The conditional nextword distribution p(wt|w1:t−1, x, f) depends on f,
the stylistic feature constraints added as supervision. This is produced using the decoder hidden
state to compute a distribution over the vocabulary of target side words. The decoder is a unidirectional multi-layer LSTM and attention is calculated as in Luong et al. using the general
method of computing attention scores. We present
model conﬁgurations in Appendix A.
Evaluation
To evaluate whether the models effectively hit semantic and stylistic targets, we randomly split
the YELPNLG corpus into 80% train (∼235k instances), 10% dev and test (∼30k instances each),
and create 4 versions of the corpus: BASE, +ADJ,
+SENT, and +STYLE, each with the same split.7
Table 5 shows examples of output generated by
the models for a given test MR, showing the effects
of training models with increasing information.
Note that we present the longest version of the MR
(that used for the +STYLE model), so the BASE,
+ADJ, and +SENT models use the same MR minus
the additional information. Row 1 shows an example of partially correct sentiment for BASE, and
fully correct sentiment for the rest; +ADJ gets the
adjectives right, +SENT is more descriptive, and
+STYLE hits all targets. Row 2 gives an example
of extra length in +STYLE, “the meat was so ten-
7Since we randomly split the data, we compute the overlap between train and test for each corpus version, noting that
around 14% of test MRs exist in training for the most speciﬁc
+STYLE version (around 4.3k of the 30k), but that less than
0.5% of the 30k full MR-ref pairs from test exist in train.
der and juicy that it melted in your mouth”. Row
3 shows an example of a negative sentiment target,
which is achieved by both the +SENT and +STYLE
models, with interesting descriptions such as “the
breakfast pizza was a joke”, and “the pizza crust
was a little on the bland side”. We show more
+STYLE model outputs in Appendix C.
Automatic Semantic Evaluation
Machine Translation Metrics. We begin with an
automatic evaluation using standard metrics frequently used for machine translation. We use the
script provided by the E2E Generation Challenge8
to compute scores for each of the 4 model test
outputs compared to the original Yelp review sentences in the corresponding test set. Rows 1-4 of
Table 6 summarize the results for BLEU (n-gram
precision), METEOR (n-grams with synonym recall), CIDEr (weighted n-gram cosine similarity),
and NIST (weighted n-gram precision), where
higher numbers indicate better overlap (shown
with the ↑). We note that while these measures
are common for machine translation, they are not
well-suited to this task, since they are based on ngram overlap which is not a constraint within the
model; we include them for comparative purposes.
From the table, we observe that across all metrics, we see a steady increase as more information
is added. Overall, the +STYLE model has the highest scores for all metrics, i.e. +STYLE model outputs are most lexically similar to the references.
Semantic Error Rate. The types of semantic errors the models make are more relevant than how
well they conform to test references. We calculate average Semantic Error Rate (SER), which
is a function of the number of semantic mistakes
the model makes . We ﬁnd counts of two types of common
mistakes: deletions, where the model fails to realize a value from the input MR, and repetitions,
where the model repeats the same value more than
once.9 Thus, we compute SER per MR as SER =
N , where D and R are the number of deletions
and repetitions, and the N is the number of tuples
in the MR, and average across the test outputs.
8 
e2e-metrics
9We note that other types of errors include insertions and
substitutions, but we evaluate these through our human evaluation in Sec 4.3 since our large vocabulary size makes identifying them non-trivial.
(food, porridge, no adj, mention=1), (food, meat, no adj, mention=1), (food, ﬂavor, rich, mention=1),
(food, soup, no adj, mention=1) +[sentiment=positive, len=long, ﬁrst person=false, exclamation=false]
The porridge was good, but the meat lacked ﬂavor and the soup was bland.
The porridge had a lot of meat in it and the ﬂavor of the soup was rich.
The porridge had a lot of meat in it and the ﬂavor of the soup was rich and delicious.
The porridge had a good amount of meat and rich ﬂavor, and the soup was cooked perfectly.
(food, gyro salad, no adj, mention=1), (food, meat, no adj, mention=1)
+[sentiment=positive, len=long, ﬁrst person=true, exclamation=false]
I had the gyro salad and the meat was very good.
I had the gyro salad and the meat was tender and juicy.
I had the gyro salad and the meat was tender.
I had the gyro salad and the meat was so tender and juicy that it melted in your mouth.
(food, eggs, no adj, mention=1), (food, ham steak, small, mention=1), (food, bacon, chewy, mention=1),
(food, breakfast pizza, no adj, mention=1)
+[sentiment=negative, len=long, ﬁrst person=true, exclamation=false]
I had the eggs, ham steak, bacon, and buffalo pizza.
Eggs, ham steak, chewy bacon, and breakfast pizza.
The eggs were over cooked, the ham steak was small, the bacon was chewy, and the breakfast pizza was a joke.
I ordered the eggs benedict and the ham steak was small, the bacon was chewy and the pizza crust was a little
on the bland side.
Table 5: Sample test MR and corresponding outputs for each model. Note that the MR presented is for +STYLE:
the other models all provide less information as described in Section 2.
Table 6: Automatic semantic evaluation (higher is better for all but SER).
Table 6 presents the average SER rates for each
model, where lower rates mean fewer mistakes
(indicated by ↓). It is important to note here that
we compute errors over value and adjective slots
only, since these are the ones that we are able to
identify lexically (we cannot identify whether an
output makes an error on sentiment in this way, so
we measure that with a human evaluation in Section 4.3). This means that the BASE outputs errors are computed over only value slots (since they
don’t contain adjectives), and the rest of the errors
are computed over both value and adjective slots.
Amazingly, overall, Table 6 results show the
SER is extremely low, even while achieving a
large amount of stylistic variation.
Naturally,
BASE, with no access to style information, has
the best (lowest) SER. But we note that there
is not a large increase in SER as more information is added – even for the most difﬁcult setting,
+STYLE, the models make an error on less than
10% of the slots in a given MR, on average.
Automatic Stylistic Evaluation
We compute stylistic metrics to compare the
model outputs, with results shown in Table 7.10
For vocab, we ﬁnd the number of unique words in
all outputs for each model. We ﬁnd the average
sentence length (SentLen) by counting the number of words, and ﬁnd the total number of times
an adjective is used (Row 3) and average number
of adjectives per reference for each model (Row
We compute Shannon text entropy (E) as:
t ), where V is the vocab size in all outputs generated by the model,
f is the frequency of a term (in this case, a trigram), and t counts the number of terms in all outputs. Finally, we count the instances of contrast
(e.g. “but” and “although”), and aggregation (e.g.
“both” and “also”). For all metrics, higher scores
indicate more variability (indicated by ↑).
From the table, we see that overall the vocabulary is large, even when compared to the training data for E2E and Laptop, as shown in Table 4.
First, we see that the simplest, least constrained
BASE model has the largest vocabulary, since it has
the most freedom in terms of word choice, while
the model with the largest amount of supervision,
+STYLE, has the smallest vocab, since we provide it with the most constraints on word choice.
For all other metrics, we see that the +STYLE
10These measures can be compared to Table 4, which includes similar statistics for the YelpNLG training data.
Table 7: Automatic stylistic evaluation metrics (higher
is better). Paired t-test BASE vs. +STYLE all p < 0.05.
model scores highest: these results are especially
interesting when considering that +STYLE has the
smallest vocab; even though word choice is constrained with richer style markup, +STYLE is more
descriptive on average (more adjectives used), and
has the highest entropy (more diverse word collocations). This is also very clear from the signiﬁcantly higher number of contrast and aggregation
operations in the +STYLE outputs.
Language Template Variations. Since our test
set consists of 30k MRs, we are able to broadly
characterize and quantify the kinds of sentence
constructions we get for each set of model outputs.
To make generalized sentence templates,
we delexicalize each reference in the model outputs, i.e. we replace any food item with a token
[FOOD], any service item with [SERVICE], etc.
Then, we ﬁnd the total number of unique templates
each model produces, ﬁnding that each “more informed” model produces more unique templates:
BASE produces 18k, +ADJ produces 22k, +SENT
produces 23k, and +STYLE produces 26k unique
templates. In other words, given the test set of
30k, +STYLE produces a novel templated output
for over 86% of the input MRs.
While it is interesting to note that each
“more informed” model produces more unique
templates, we also want to characterize how
frequently templates are reused. Figure 3 shows
the number of times each model repeats its top
20 most frequently used templates. For example,
the Rank 1 most frequently used template for the
BASE model is “I had the [FOOD] [FOOD].”,
and it is used 550 times (out of the 30k outputs).
For +STYLE, the Rank 1 most frequently used
template is “I had the [FOOD] [FOOD] and it
was delicious.”, and it is only used 130 times. The
number of repetitions decreases as the template
rank moves from 1 to 20, and repetition count is
always signiﬁcantly lower for +STYLE, indicating
more variation. Examples of frequent templates
from the BASE and +STYLE models are are shown
in Appendix B.
Template Rank
Number of Repetitions
Figure 3: Number of output template repetitions for
the 20 most frequent templates (+STYLE has the fewest
repetitions, i.e. it is the most varied).
Achieving Other Style Goals.
The +STYLE
model is the only one with access to ﬁrst-person,
length, and exclamation markup, so we also measure its ability to hit these stylistic goals. The average sentence length for the +STYLE model for
LEN=SHORT is 7.06 words, LEN=MED is 13.08,
and LEN=LONG is 22.74, closely matching the average lengths of the test references in those cases,
6.33, 11.05, and 19.03, respectively.
model correctly hits the target 99% of the time for
ﬁrst person (it is asked to produce this for 15k of
the 30k test instances), and 100% of the time for
exclamation (2k instances require exclamation).
Human Quality Evaluation
We evaluate output quality using human annotators on Mechanical Turk. As in our corpus quality
evaluation from Section 2.2, we randomly sample
200 MRs from the test set, along with the corresponding outputs for each of the 4 models, and
ask 5 annotators to rate each output on a 1-5 Likert scale for content, ﬂuency, and sentiment (1
for very negative, 5 for very positive11). Table 8
shows the average scores by criteria and model.12
For content and ﬂuency, all average ratings are
very high, above 4.3 (out of 5). The differences
between models are small, but it is interesting
11As in Sec 2.2, we scale the sentiment scores into 3 bins
to match our Yelp review sentiment.
12The average correlation between each annotator’s ratings
and the average rating for each item is 0.73.
to note that the BASE and +STYLE models are
almost tied on ﬂuency (although BASE outputs
may appear more ﬂuent due to their comparably
shorter length).
In the case of sentiment error,
the largest error is 0.75 (out of 3), with the smallest sentiment error (0.56) achieved by the +STYLE
model. Examination of the outputs reveals that the
most common sentiment error is producing a neutral sentence when negative sentiment is speciﬁed.
This may be due to the lower frequency of negative sentiment in the corpus as well as noise in
automatic sentiment annotation.
+SENT +STYLE
Sentiment Err
Table 8: Human quality evaluation (higher is better for
content and ﬂuency, lower is better for sentiment error).
Paired t-test for each model vs.+STYLE, * is p < 0.05.
Related Work
Recent efforts on data acquisition for
has relied almost exclusively on crowdsourcing.
Novikova et al. used pictorial representations of restaurant MRs to elicit 50k varied restaurant descriptions through crowdsourcing. Wen et
 also create datasets for the
restaurant (5k), hotel (5k), laptop (13k), and TV
(7k) domains by asking Turkers to write NL realizations for different combinations of input dialog acts in the MR. Work on the WEBNLG challenge has also focused on using existing structured
data, such as DBPedia, as input into an NLG , where matching NL utterances
are also crowdsourced. Other recent work on collecting datasets for dialog modeling also use largescale crowdsourcing .
Here, we completely avoid having to crowdsource any data by working in reverse: we begin
with naturally occurring user reviews, and automatically construct MRs from them. This allows
us to create a novel dataset YELPNLG, the largest
existing NLG dataset, with 300k parallel MR to
sentence pairs with rich information on attribute,
value, description, and mention order, in addition
to a set of sentence-level style information, including sentiment, length, and pronouns.
In terms of control mechanisms, very recent
work in NNLG has begun to explore using an
explicit sentence planning stage and hierarchical
structures . In our own work, we show how we
are able to control various aspects of style with
simple supervision within the input MR, without
requiring a dedicated sentence planner, and in line
with the end-to-end neural generation paradigm.
Previous work has primarily attempted to individually control aspects of content preservation
and style attributes such as formality and verb
tense, sentiment , and personality in different domains such as news and product reviews
 , movie reviews , restaurant descriptions , and customer care dialogs .
To our knowledge,
our work is the very ﬁrst to generate realizations
that both express particular semantics and exhibit
a particular descriptive or lexical style and sentiment. It is also the ﬁrst work to our knowledge that
controls lexical choice in neural generation, a long
standing interest of the NLG community .
Conclusions
This paper presents the YelpNLG corpus, a set of
300,000 parallel sentences and MR pairs generated
by sampling freely available review sentences that
contain attributes of interest, and automatically
constructing MRs for them. The dataset is unique
in its huge range of stylistic variation and language
richness, particularly compared to existing parallel
corpora for NLG. We train different models with
varying levels of information related to attributes,
adjective dependencies, sentiment, and style information, and present a rigorous set of evaluations to
quantify the effect of the style markup on the ability of the models to achieve multiple style goals.
For future work, we plan on exploring other
models for NLG, and on providing models with a
more detailed input representation in order to help
preserve more dependency information, as well
as to encode more information on syntactic structures we want to realize in the output. We are also
interested in including richer, more semanticallygrounded information in our MRs, for example
using Abstract Meaning Representations (AMRs)
 . Finally, we are interested in reproducing our corpus generation method on various
other domains to allow for the creation of numerous useful datasets for the NLG community.