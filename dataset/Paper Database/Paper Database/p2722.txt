Proceedings of the SIGDIAL 2017 Conference, pages 231–240,
Saarbr¨ucken, Germany, 15-17 August 2017. c⃝2017 Association for Computational Linguistics
Neural-based Natural Language Generation in Dialogue
using RNN Encoder-Decoder with Semantic Aggregation
Van-Khanh Tran1,2, Le-Minh Nguyen1,⋆and Satoshi Tojo1
1Japan Advanced Institute of Science and Technology, JAIST
1-1 Asahidai, Nomi, Ishikawa, 923-1292, Japan
{tvkhanh, nguyenml, tojo}@jaist.ac.jp
2University of Information and Communication Technology, ICTU
Thai Nguyen University, Vietnam
 
Natural language generation (NLG) is an
important component in spoken dialogue
This paper presents a model
Encoder-Aggregator-Decoder
which is an extension of an Recurrent
Neural Network based Encoder-Decoder
architecture.
The proposed Semantic
Aggregator consists of two components:
an Aligner and a Reﬁner. The Aligner is
a conventional attention calculated over
the encoded input information,
the Reﬁner is another attention or gating
mechanism stacked over the attentive
Aligner in order to further select and
aggregate the semantic elements.
proposed model can be jointly trained
both text planning and text realization
to produce natural language utterances.
The model was extensively assessed on
four different NLG domains, in which the
results showed that the proposed generator consistently outperforms the previous
methods on all the NLG domains.
Introduction
Natural Language Generation (NLG) plays a critical role in a Spoken Dialogue System (SDS),
and its task is to convert a meaning representation produced by the dialogue manager into natural language sentences. Conventional approaches
to NLG follow a pipeline which typically breaks
down the task into sentence planning and surface
realization. Sentence planning decides the order
and structure of a sentence, followed by a surface realization which converts the sentence structure into ﬁnal utterance. Previous approaches to
NLG still rely on extensive hand-tuning templates
⋆Corresponding author.
and rules that require expert knowledge of linguistic representation. There are some common
and widely used approaches to solve NLG problems, including rule-based , corpus-based n-gram generator , and a trainable generator .
Recurrent Neural Network (RNN)-based approaches have recently shown promising results
in NLG tasks. The RNN-based models have been
used for NLG as a joint training model and an end-to-end training network . A recurring problem in such systems requiring annotated corpora for speciﬁc dialogue acts∗(DAs). More recently, the attentionbased RNN Encoder-Decoder (AREncDec) approaches have been explored to tackle the NLG problems , machine translation .
To ensure that the generated utterance represents the intended meaning of the given DA, the
previous RNN-based models were conditioned on
a 1-hot vector representation of the DA. Wen et al.
 proposed a Long Short-Term Memorybased (HLSTM) model which introduced a heuristic gate to guarantee that the slot-value pairs were
accurately captured during generation.
Subsequently, Wen et al. proposed an LSTMbased generator (SC-LSTM) which jointly learned
the controlling signal and language model. Wen
et al. proposed an AREncDec based generator (ENCDEC) which applied attention mechanism on the slot-value pairs.
∗A combination of an action type and a set of slot-value
pairs). E.g. inform(name=’Piperade’; food=’Basque’).
Table 1: Order issue in natural language generation, in which an incorrect generated sentence has
wrong ordered slots.
Compare(name=Triton 52; ecorating=A+; family=L7; name=Hades 76; ecorating=C; family=L9)
The Triton 52 has an A+ eco rating and is in the L9 product family, the Hades 76 is in the L7 product
family and has a C eco rating.
The Triton 52 is in the L7 product family and has an A+ eco rating, the Hades 76 is in the L9 product
family and has a C eco rating.
Although these RNN-based generators have
worked well, however, they still have some drawbacks, and none of these models signiﬁcantly outperform the others in solving NLG tasks. While
the HLSTM cannot handle cases such as the binary slots (i.e., yes and no) and slots that take
don’t care value in which these slots cannot be directly delexicalized, the SCLSTM model is limited to generalize to the unseen domain, and the
ENCDEC model has difﬁculty to prevent undesirable semantic repetitions during generation.
To address the above issues, we propose a new
architecture, Encoder-Aggregator-Decoder, an extension of the AREncDec model, in which the proposed Aggregator has two main components: (i)
an Aligner which computes the attention over the
input sequence, and (ii) a Reﬁner which are another attention or gating mechanisms to further
select and aggregate the semantic elements. The
proposed model can learn from unaligned data by
jointly training the sentence planning and surface
realization to produce natural language sentences.
We conduct comprehensive experiments on four
NLG domains and ﬁnd that the proposed method
signiﬁcantly outperforms the previous methods regarding BLEU and slot error rate ERR scores . We also
found that our generator can produce high-quality
utterances with correctly ordered than those in the
previous methods (see Table 1). To sum up, we
make two key contributions in this paper:
• We present a semantic component called Aggregator which is easy integrated into existing (attentive) RNN encoder-decoder architecture, resulting in an end-to-end generator that empirically improved performance in
comparison with the previous approaches.
• We present several different choices of attention and gating mechanisms which can be
effectively applied to the proposed semantic
Aggregator.
In Section 2, we review related works. The proposed model is presented in Section 3. Section 4
describes datasets, experimental setups and evaluation metrics. The results and analysis are presented in Section 5. We conclude with a brief of
summary and future work in Section 6.
Related Work
Conventional approaches to NLG traditionally divide the task into a pipeline of sentence planning
and surface realization. The conventional methods
still rely on the handcrafted rule-based generators
or rerankers. Oh and Rudnicky proposed a
class-based n-gram language model (LM) generator which can learn to generate the sentences for
a given dialogue act and then select the best sentences using a rule-based reranker. Ratnaparkhi
 later addressed some of the limitation of
the class-based LMs by proposing a method based
on a syntactic dependency tree. A phrase-based
generator based on factored LMs was introduced
by Mairesse and Young , that can learn from
a semantically aligned corpus.
Recently, RNNs-based approaches have shown
promising results in the NLG domain.
et al. ; Karpathy and Fei-Fei applied RNNs in setting of multi-modal to generate caption for images. Zhang and Lapata 
also proposed a generator using RNNs to create
Chinese poetry. For task-oriented dialogue systems, Wen et al. combined two TNNbased models with a CNN reranker to generate
required utterances. Wen et al. proposed
SC-LSTM generator which proposed an additional
”reading” cell to the traditional LSTM cell to learn
the gating mechanism and language model jointly.
A recurring problem in such systems lacking of
sufﬁcient domain-speciﬁc annotated corpora. Wen
et al. proposed an out-of-domain model
which is trained on counterfeited datasets by using semantic similar slots from the target-domain
dataset instead of the slots belonging to the outof-domain dataset. The empirical results indicated
Lexicalization
dialog act 1-hot
representation
Inform(name=Piperade; food=Basque)
(0, 1, 0, 0, ..., 0, 0, 1, 0, ..., 0, 1, 0, ... )
AGRREGATOR
Basque food
Figure 1: Unfold presentation of the RNN-based
neural language generator.
The encoder part is
subject to various designs, while the decoder is an
RNN network.
that the model can obtain a satisfactory results
with a small amount of in-domain data by ﬁnetuning the target-domain on the out-of-domain
trained model.
attentional
encoderdecoder based models 
have shown improved results in a variety of tasks.
Yang et al. presented a review network in
solving the image captioning task, which produces
a compact thought vector via reviewing all the
input information encoded by the encoder. Mei
et al. proposed attentional RNN encoderdecoder based model by introducing two layers of attention to model content selection and
surface realization.
More close to our work,
Wen et al. proposed an attentive encoderdecoder based generator, which applied the attention mechanism over the slot-value pairs.
model indicated a domain scalability when a very
limited proportion of training data is available.
Recurrent Neural Language Generator
The recurrent language generator proposed in this
paper is based on a neural net language generator
 which consists of three components: an encoder to incorporate the target meaning representation as the model inputs, an aggregator to align and control the encoded information, and a decoder to generate output sentences.
The generator architecture is shown in Figure 1.
While the decoder typically uses an RNN model,
there is a variety of ways to choose the encoder
because it depends on the nature of the meaning
Aggregator
Figure 2: The RNN Encoder-Aggregator-Decoder
for NLG proposed in this paper. The output side
is an RNN network while the input side is a DA
embedding with aggregation mechanism. The Aggregator consists of two parts: an Aligner and a
The lower part Aligner is an attention
over the DA representation calculated by a Bidirectional RNN. Note that the action type embedding a is not included in the attention mechanism
since its task is controlling the style of the sentence. The higher part Reﬁner computes the new
input token xt based on the original input token wt
and the dialogue act attention dt. There are several choices for Reﬁner, i.e., gating mechanism or
attention mechanism.
representation and the interaction between semantic elements. The encoder ﬁrst encodes the input
meaning representation, then the aggregator with a
feature selecting or an attention-based mechanism
is used to aggregate and select the input semantic elements. The input to the RNN decoder at
each time step is a 1-hot encoding of a token† and
the aggregated input vector. The output of RNN
decoder represents the probability distribution of
the next token given the previous token, the dialogue act representation, and the current hidden
At generation time, we can sample from
this conditional distribution to obtain the next token in a generated sentence, and feed it as the next
input to the RNN decoder. This process ﬁnishes
when a stop sign is generated , or some constraint is reached . The network can generate a
sequence of tokens which can be lexicalized‡ to
form the required utterance.
†Input texts are delexicalized in which slot values are replaced by its corresponding slot tokens.
‡The process in which slot token is replaced by its value.
Gated Recurrent Unit
The encoder and decoder of the proposed model
use a Gated Recurrent Unit (GRU) network proposed by Bahdanau et al. , which maps an
input sequence W = [w1, w2, .., wT ] to a sequence
of states H = [h1, h2, .., hT ] as follows:
ri = σ(Wrwwi + Wrhhi−1)
ui = σ(Wuwwi + Wuhhi−1)
˜hi = tanh(Whwwi + ri ⊙Whhhi−1)
hi = ui ⊙hi−1 + (1 −ui) ⊙˜hi
where: ⊙denotes the element-wise multiplication, ri and ui are called the reset and update gates
respectively, and ˜hi is the candidate activation.
The encoder uses a separate parameterization of
the slots and values.
It encodes the source information into a distributed vector representation
zi which is a concatenation of embedding vector
representation of each slot-value pair, and is computed by:
zi = oi ⊕vi
where: oi and vi are the i-th slot and value embedding, respectively. The i index runs over the
given slot-value pairs. In this study, we use a Bidirectional GRU (Bi-GRU) to encode the sequence
of slot-value pairs§ embedding. The Bi-GRU consists of forward and backward GRUs. The forward GRU reads the sequence of slot-value pairs
from left-to-right and calculates the forward hidden states (−→
s1, .., −→
sK). The backward GRU reads
the slot-value pairs from right-to-left, resulting in
a sequence of backward hidden states (←−
s1, .., ←−
We then obtain the sequence of hidden states S =
[s1, s2, .., sK] where si is a sum of the forward hidden state −→
si and the backward one ←−
si as follows:
Aggregator
The Aggregator consists of two components: an
Aligner and a Reﬁner. The Aligner computes the
dialogue act representation while the choices for
Reﬁner can be varied.
§We treat the set of slot-value pairs as a sequence and use
the order speciﬁed by slot’s name (e.g., slot area comes ﬁrst,
price follows area). We have tried treating slot-value pair
sequence as natural order as appear in the DA, which even
yielded worse results.
Firstly, the Aligner calculates dialogue act embedding dt as follows:
where: a is vector embedding of the action type,
⊕is vector concatenation, and αt,i is the weight
of i-th slot-value pair calculated by the attention
mechanism:
j exp(et,j)
et,i = a(si, ht−1)
a(si, ht−1) = v⊤
a tanh(Wasi + Uaht−1)
where: a(., .) is an alignment model,va, Wa, Ua
are the weight matrices to learn.
Secondly, the Reﬁner calculates the new input
xt based on the original input token wt and the
DA representation. There are several choices to
formulate the Reﬁner such as gating mechanism
or attention mechanism. For each input token wt,
the selected mechanism module computes the new
input xt based on the dialog act representation dt
and the input token embedding wt, and is formulated by:
xt = fR(dt, wt)
where: fR is a reﬁnement function, in which each
input token is reﬁned (or ﬁltered) by the dialogue
act attention information before putting into the
RNN decoder. By this way, we can represent the
whole sentence based on this reﬁned input using
RNN model.
Attention Mechanism:
Inspired by work of
Cui et al. , in which an attention-overattention was introduced in solving reading comprehension tasks, we place another attention applied for Reﬁner over the attentive Aligner, resulting in a model Attentional Reﬁner over Attention
• ARoA with Vector (ARoA-V): We use a simple attention where each input token representation is weighted according to dialogue
act attention as follows:
fR(dt, wt) = βt ∗wt
where: Vra is a reﬁnement attention vector
which is used to determine the dialogue act
attention strength, and σ is sigmoid function
to normalize the weight βt between 0 and 1.
• ARoA with Matrix (ARoA-M): ARoA-V uses
only a vector Vra to weight the DA attention.
It may be better to use a matrix to control
the attention information. The Equation 7 is
modiﬁed as follows:
Vra = Wawwt
fR(dt, wt) = βt ∗wt
where: Waw is a reﬁnement attention matrix.
• ARoA with Context (ARoA-C): The attention in ARoA-V and ARoA-M may not capture the relationship between multiple tokens.
In order to add context information into the
attention process, we modify the attention
weights in Equation 8 with additional history
information ht−1:
Vra = Wawwt + Wahht−1
fR(dt, wt, ht−1) = βt ∗wt
where: Waw, Wah are parameters to learn,
Vra is the reﬁnement attention vector same
as above, which contains both DA attention
and context information.
Gating Mechanism:
We use simple elementwise operators (multiplication or addition) to gate
the information between the two vectors dt and wt
as follows:
• Multiplication (GR-MUL): The element-wise
multiplication plays a part in word-level
matching which learns not only the vector similarity, but also preserve information
about the two vectors:
fR(dt, wt) = Wgddt ⊙wt
• Addition (GR-ADD):
fR(dt, wt) = Wgddt + wt
The decoder uses a simple GRU model as described in Section 3.1. In this work, we propose to
apply the DA representation and the reﬁned inputs
deeper into the GRU cell. Firstly, the GRU reset
and update gates can be further inﬂuenced on the
DA attentive information dt. The reset and update
gates are modiﬁed as follows:
rt = σ(Wrxxt + Wrhht−1 + Wrddt)
ut = σ(Wuxxt + Wuhht−1 + Wuddt)
where: Wrd and Wud act like background detectors that learn to control the style of the generating
sentence. By this way, the reset and update gates
learn not only the long-term dependency but also
the attention information from the dialogue act and
the previous hidden state. Secondly, the candidate
activation ˜ht is also modiﬁed to depend on the DA
representation as follows:
˜ht = tanh(Whxxt + rt ⊙Whhht−1
+Whddt) + tanh(Wdcdt)
The hidden state is then computed by:
ht = ut ⊙ht−1 + (1 −ut) ⊙˜ht
Finally, the output distribution is computed by applying a softmax function g, and the distribution is
sampled to obtain the next token,
P(wt+1 | wt, wt−1, ...w0, z) = g(Whoht)
wt+1 ∼P(wt+1 | wt, wt−1, ...w0, z)
The objective function was the negative loglikelihood and simply computed by:
where: yt is the ground truth word distribution,
pt is the predicted word distribution, T is length
of the input sequence. The proposed generators
were trained by treating each sentence as a minibatch with l2 regularization added to the objective function for every 10 training examples. The
pre-trained word vectors 
were used to initialize the model. The generators
were optimized by using stochastic gradient descent and back propagation through time . To prevent over-ﬁtting, we implemented
early stopping using a validation set as suggested
by Mikolov .
The decoding consists of two phases: (i) overgeneration, and (ii) reranking.
In the overgeneration, the generator conditioned on the given
Table 2: Comparison performance on four datasets in terms of the BLEU and the error rate ERR(%)
scores; bold denotes the best and italic shows the second best model. The results were produced by
training each network on 5 random initialization and selected model with the highest validation BLEU
score. ♯denotes the Attention-based Encoder-Decoder model.
Restaurant
Table 3: Comparison performance of variety of the proposed models on four dataset in terms of the
BLEU and the error rate ERR(%) scores; bold denotes the best and italic shows the second best model.
The ﬁrst two models applied gating mechanism to Reﬁner component while the last three models used
attention over attention mechanism. The results were averaged over 5 randomly initialized networks.
Restaurant
DA uses a beam search to generate a set of candidate responses. In the reranking, the cost of the
generator is computed to form the reranking score
R as follows:
t log pt + λERR
where λ is a trade off constant and is set to a
large value in order to severely penalize nonsensical outputs. The slot error rate ERR, which is
the number of slots generated that is either redundant or missing, and is computed by:
ERR = p + q
where: N is the total number of slots in DA, and
p, q is the number of missing and redundant slots,
respectively. Note that the ERR reranking criteria cannot handle arbitrary slot-value pairs such as
binary slots or slots that take the dont care value
because these slots cannot be delexicalized and
Experiments
We conducted an extensive set of experiments to
assess the effectiveness of our model using several
metrics, datasets, and model architectures, in order
to compare to prior methods.
We assessed the proposed models using four different NLG domains: ﬁnding a restaurant, ﬁnding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in
 which contain around 5K utterances and 200 distinct DAs. The Laptop and TV
datasets have been released by Wen et al. .
These datasets contain about 13K distinct DAs in
the Laptop domain and 7K distinct DAs in the
TV. Both Laptop and TV datasets have a much
larger input space but only one training example
for each DA so that the system must learn partial
realization of concepts and be able to recombine
and apply them to unseen DAs. As a result, the
NLG tasks for the Laptop and TV datasets become
much harder.
Experimental Setups
The generators were implemented using the TensorFlow library and trained by
partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1. The hidden layer size was set to be 80 for all cases, and
the generators were trained with a 70% of dropout
rate. We perform 5 runs with different random initialization of the network and the training is terminated by using early stopping as described in Section 3.5. We select a model that yields the highest
BLEU score on the validation set as shown in Table 2. Since the trained models can differ depending on the initialization, we also report the results
which were averaged over 5 randomly initialized
networks. Note that, except the results reported in
Table 2, all the results shown were averaged over
5 randomly initialized networks. The decoder procedure used beam search with a beam width of
10. We set λ to 1000 to severely discourage the
reranker from selecting utterances which contain
either redundant or missing slots. For each DA,
we over-generated 20 candidate utterances and selected the top 5 realizations after reranking. Moreover, in order to better understand the effectiveness
of our proposed methods, we (1) trained the models on the Laptop domain with a varied proportion
of training data, starting from 10% to 100% (Figure 3), and (2) trained general models by merging
all the data from four domains together and tested
them in each individual domain (Figure 4) .
Evaluation Metrics and Baselines
The generator performance was assessed by using two objective evaluation metrics: the BLEU
score and the slot error rate ERR. Both metrics
were computed by adopting code from an open
source benchmark NLG toolkit¶. We compared
our proposed models against three strong baselines from the open source benchmark toolkit.
The results have been recently published as an
NLG benchmarks by the Cambridge Dialogue
Systems Group¶, including HLSTM, SCLSTM,
and ENCDEC models.
Results and Analysis
We conducted extensive experiments on the proposed models with varied setups of Reﬁner and
¶ 
compared against the previous methods. Overall,
the proposed models consistently achieve the better performances regarding both evaluation metrics across all domains.
Table 2 shows a comparison between the
AREncDec based models (the models with ♯symbol) in which the proposed models signiﬁcantly reduce the slot error rate across all datasets by a large
margin about 2% to 4% that are also improved
performances on the BLEU score when comparing the proposed models against the previous approaches. Table 3 further shows the stable strength
of our models since the results’ pattern stays unchanged compared to those in Table 2. The ARoA-
M model shows the best performance over all the
four domains, while it is an interesting observation that the GR-ADD model with simple addition
operator for Reﬁner obtains the second best performance. All these prove the importance of the
proposed component Reﬁner in aggregating and
selecting the attentive information.
Figure 3 illustrates a comparison of four models (ENCDEC, SCLSTM, ARoA-M, and GR-ADD)
which were trained from scratch on the laptop
dataset in a variety of proportion of training data,
from 10% to 100%.
It clearly shows that the
BLEU increases while the slot error rate decreases
as more training data was provided.
presents a comparison performance of general
models as described in Section 4.2. Not surprisingly, the two proposed models still obtain higher
the BLEU score, while the ENCDEC has difﬁculties in reducing the ERR score in all cases.
Both the proposed models show their ability to
generalize in the unseen domains (TV and Laptop datasets) since they consistently outperform
the previous methods no matter how much training data was fed or how training method was
used. These indicate the relevant contribution of
the proposed component Reﬁner to the original
AREncDec architecture, in which the Reﬁner with
gating or attention mechanism can effectively aggregate the information before putting them into
the RNN decoder.
Figure 5 shows a different attention behavior of
the proposed models in a sentence. While all the
three models could attend the slot tokens and their
surrounding words, the ARoA-C model with context shows its ability in attending the consecutive
words. Table 4 shows comparison of responses
generated for some DAs between different models.
Figure 3: Performance comparison of the four models trained on Laptop (unseen) domain.
Figure 4: Performance comparison of the general models on four different domains.
Figure 5: A comparison on attention behavior of three models in a sentence on given DA with sequence
of slots [Name 1, ScreenSizeRange 1, Resolution 1, Name 2, ScreenSizeRange 2, Resolution 2].
The previous approaches (ENCDEC, HLSTM) still
have missing and misplaced information, whereas
the proposed models can generate complete and
correct-order sentences.
Conclusion and Future Work
We present an extension of an Attentional
RNN Encoder-Decoder model named Encoder-
Aggregator-Decoder, in which a Reﬁner component is introduced to select and aggregate the semantic elements produced by the encoder. We also
present several different choices of gating and attention mechanisms which can be effectively applied to the Reﬁner. The extension, which is easily
integrated into an RNN Encoder-Decoder, shows
its ability to reﬁne the inputs and control the ﬂow
information before putting them into the RNN decoder. We evaluated the proposed model on four
domains and compared to the previous generators.
The proposed models empirically show consistent
improvement over the previous methods in both
BLEU and ERR evaluation metrics. In the future,
it would be interesting to further investigate hybrid
models which integrate gating and attention mechanisms in order to leverage the advantages of both
mechanisms.
Acknowledgements
This work was supported by JSPS Kakenhi Grant
Number JP15K16048 and the grant for the research cooperation between JAIST and TIS. The
ﬁrst author would like to thank the Vietnamese
Government Scholarship (911 project).
Table 4: Comparison of top responses generated for some input dialogue acts between different models.
Errors are marked in color (missing, misplaced slot-value pair). † and ♮denotes the baselines and the
proposed models, respectively.
Generated Responses in Laptop domain
compare(name=‘aristaeus 59’; screensizerange=‘large’; resolution=‘1080p’; name=‘charon 61’; screensizerange=‘medium’; resolution=‘720p’)
Compared to aristaeus 59 which is in the large screen size range and has 1080p resolution, charon 61 is in
the medium screen size range and has 720p resolution. Which one do you prefer?
the aristaeus 59 has a large screen , the charon 61 has a medium screen and 1080p resolution [1080p, 720p]
the aristaeus 59 has a large screen size range and has a 1080p resolution and 720p resolution [720p, charon
61, medium]
the aristaeus 59 has a large screen and 1080p resolution , the charon 61 has a medium screen and 720p
resolution
the aristaeus 59 has a large screen size and 1080p resolution , the charon 61 has a medium screen size and
720p resolution
the aristaeus 59 has a large screen size and 1080p resolution , the charon 61 has a medium screen size and
720p resolution .
the aristaeus 59 has a large screen size and 1080p resolution , the charon 61 has a medium screen size , and
has a 720p resolution
the aristaeus 59 has a large screen and 1080p resolution , the charon 61 has a medium screen and 720p
resolution
the aristaeus 59 has a large screen size and 1080p resolution , the charon 61 has a medium screen size range
and 720p resolution