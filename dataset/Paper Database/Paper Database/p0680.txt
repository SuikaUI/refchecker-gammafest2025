IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
Image Segmentation Using Active Contours
Driven by the Bhattacharyya Gradient Flow
Oleg Michailovich, Member, IEEE, Yogesh Rathi, and Allen Tannenbaum, Member, IEEE
Abstract—This paper addresses the problem of image segmentation by means of active contours, whose evolution is driven by the
gradient ﬂow derived from an energy functional that is based on the
Bhattacharyya distance. In particular, given the values of a photometric variable (or of a set thereof), which is to be used for classifying the image pixels, the active contours are designed to converge
to the shape that results in maximal discrepancy between the empirical distributions of the photometric variable inside and outside
of the contours. The above discrepancy is measured by means of the
Bhattacharyya distance that proves to be an extremely useful tool
for solving the problem at hand. The proposed methodology can be
viewed as a generalization of the segmentation methods, in which
active contours maximize the difference between a ﬁnite number
of empirical moments of the “inside” and “outside” distributions.
Furthermore, it is shown that the proposed methodology is very
versatile and ﬂexible in the sense that it allows one to easily accommodate a diversity of the image features based on which the
segmentation should be performed. As an additional contribution,
a method for automatically adjusting the smoothness properties of
the empirical distributions is proposed. Such a procedure is crucial in situations when the number of data samples (supporting a
certain segmentation class) varies considerably in the course of the
evolution of the active contour. In this case, the smoothness properties of the empirical distributions have to be properly adjusted to
avoid either over- or underestimation artifacts. Finally, a number
of relevant segmentation results are demonstrated and some further research directions are discussed.
Index Terms—Active contours, Bhattacharyya distance, image
segmentation, kernel density estimation.
Manuscript received September 20, 2006; revised July 29, 2007. This work
was supported in part by grants from the National Science Foundation, in part
by the Air Force Ofﬁce of Sponsored Research, in part by the Army Research
Ofﬁce, in part by MURI, in part by MRI-HEL, as well as in part by a grant
from the National Institutes of Health (NAC P41 RR-13218) through Brigham
and Women’s Hospital. This work is part of the National Alliance for Medical Image Computing (NAMIC), funded by the National Institutes of Health
through the NIH Roadmap for Medical Research, Grant U54 EB005149. Information on the National Centers for Biomedical Computing can be obtained from
 The associate editor coordinating the
review of this manuscript and approving it for publication was Dr. Mario A. T.
(G. E.) Figueiredo.
O. Michailovich was with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332 USA. He
is currently with the Department of Electrical and Computer Engineering,
University of Waterloo, Waterloo, ON N3L 3G1 Canada (e-mail: ).
Y. Rathi is with the School of Electrical and Computer Engineering, Georgia
Institute of Technology, Atlanta, GA 30332 USA (e-mail: yogesh.rathi@gatech.
A. Tannenbaum is with the School of Electrical and Computer Engineering,
Georgia Institute of Technology, Atlanta, GA 30332 USA and also with the
Department of Electrical and Computer Engineering, The Technion—Israel Institute of Technology, Haifa, Israel (e-mail: ).
Color versions of one or more of the ﬁgures in this paper are available online
at 
Digital Object Identiﬁer 10.1109/TIP.2007.908073
I. INTRODUCTION
HE quintessential goal of image segmentation is to partition the image domain into a number of (mutually exclusive) subdomains over which certain properties of the image appear to be homogeneous. The homogeneity, however, turns out
to be a somewhat vague notion when applied for characterizing
localized regions of natural images, and, therefore, it should be
used with a precaution whenever the problems that involve such
images are dealt with. Thereupon, it is usually more convenient
to look at image segmentation from a more practical perspective
and think of it as a way to decompose an image into a number
of its fragments, each of which can be associated with a distinct
class. The latter is usually distinguished on a semantic basis and
assumed to be either an object or a background. Thus, for example, the object may be associated with a diseased organ in
medical imaging , , an intruder in surveillance video ,
 , a moving part of a machine in robotics , , a maneuvering vehicle in trafﬁc control , , or a target in navigation
and military applications , .
A multitude of diverse image segmentation methods have
been proposed over the last few decades. In spite of this
diversity, however, the majority of these methods seem to
follow a similar algorithmic pattern. The latter involves making
hypotheses regarding the structure and properties of the image
to be segmented, deﬁning a set of the image features based
on which segmentation classes are discriminated, and ﬁnally
applying a decision threshold in either explicit or implicit
manner . Thus, for example, an image feature (or a collection thereof) can be considered as a random variable described
by a set of conditional likelihood functions. Consequently, the
Bayesian decision theory could be used to determine the
Bayesian decision threshold as a minimizer of the posterior
probability of misclassiﬁcation error.
Although the above segmentation approach is by no means
generic for all types of segmentation methods proposed hitherto, it espouses an “ideology” that seems to be common
for many problems of this kind. In particular, independently
of whether the segmentation is based on a local , 
or global , analysis, whether it utilizes deformable
contours , or polygons , , the most successful
segmentation methodology will be the one that minimizes the
probability of misclassiﬁcation error. Hence, this probability
appears to constitute a universal criterion, based on which
image segmentation methods may be designed.
Unfortunately, in many practical settings, little is usually
known about the statistics of segmentation classes, and, as a
result, an explicit deﬁnition of the probability of misclassiﬁcation error is either very complicated or even impossible. In such
1057-7149/$25.00 © 2007 IEEE
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
situations, it is tempting to ﬁnd alternative criteria, which are
simpler to formulate, and based on which one could perform the
segmentation with accuracy comparable to that of the methods
based on explicitly minimizing the error probability.
It is worthwhile noting that, long before the problem of segmenting digital images has become an indispensable part of
modern image processing and computer vision, the quest for
the aforementioned criteria was undertaken in the ﬁelds of communication and radars . Similar to the case of segmentation/classiﬁcation, suitable criteria were sought there to be used
as substitutes for the probability of detection error. In this connection, it was found that some useful choices for such criteria
can be based on the notion of a distance between probability
distributions . The latter, for instance, could be the distributions of observations in a binary hypothesis testing, in which
case it can be shown that the further apart one can make these
distributions, the smaller will be the probability of mistaking
one for the other .
In order to deﬁne a distance between probability distributions, a number of information-theoretic measures can be used
 . Some standard choices (which are also involved in basic
results of information theory) include the Fisher ratio, the Kullback–Leibler (KL) divergence , , and the Bhattacharyya
distance (which is a special case of a more general distance
introduced by Chernoff ).
When comparing the above distances in application to the
problem of signal selection, it was observed in that, in a
number of practically important cases, the Bhattacharyya distance turns out to give better results as compared to the KL
divergence. Furthermore, there is also a “technical” advantage
of using the Bhattacharyya distance for its having a particularly simple analytical form. Speciﬁcally, the Bhattacharyya distance between two probability densities
, is deﬁned as
is the Bhattacharyya
coefﬁcient given by
It is interesting to note that the functions
) belong to the unit sphere of
, and, thus,
can be thought of as a direction cosine between two points on
the sphere. For this reason, the values of
are always conﬁned
within the interval
Lately, the exceptional properties of the Bhattacharyya distance have shown its worth for applications in computer vision.
Thus, for example, in , it was shown that for detection, location, or segmentation algorithms based on the maximum likelihood (ML) or on the minimum-description length principle,
and for realistic cases where the object and background statistical parameters are unknown, the Bhattacharyya distance can
characterize the image difﬁculty for a large family of probability laws. Particularly, it was shown that, for a wide spectrum
of distributions under test, the average number of misclassiﬁed
pixels is a monotonously decreasing, bijective function of the
Bhattacharyya distance between the probability densities of the
object and of the background.
Later on, the Bhattacharyya distance was employed in 
for image segmentation and tracking. In this study, tracked objects were characterized by model distributions over some photometric variable. Consequently, a tracked object was identiﬁed
as the region of image whose interior generates a sample distribution which most closely matches the model distribution. As a
possible measure of the match, the Bhattacharyya coefﬁcient
was used in with apparent success.
This paper takes the idea of a few steps further by extending its applicability to the cases when the model distributions of tracked objects cannot be deﬁned beforehand. In particular, the segmentation method described in this paper identiﬁes
the object as the image region whose interior generates a sample
distribution that maximizes the distance to the sample distribution generated by corresponding exterior. In order to assess the
above distance, we use the Bhattacharyya coefﬁcient
It should be noted that virtually all practically important probability distributions can be uniquely represented by (either ﬁnite
or inﬁnite) sets of their moments. In such cases, the discrepancy between two (or more) distributions could be “translated”
into the discrepancy between their corresponding moments, and
comparing the empirical distributions of a photometric variable
inside and outside of the “object region” could be superseded by
comparing the corresponding empirical moments. Thus, for example, in the seminal work by Chan and Vese , the object of
interest is identiﬁed as the image region, whose interior and exterior differ the most in terms of their corresponding mean intensities. In the case when both the object and the background have
(approximately) identical mean values, second-order moments
can be involved into an analogous procedure . Obviously,
the chain of statistical moments can be extended unlimitedly,
progressively increasing the “discrimination power” of resulting
segmentation algorithms on one hand and complicating their
implementation on the other. Needless to say, working with the
moments also raises the dilemma of determining their number
and orders which would be appropriate for speciﬁc data at hand.
On the other hand, the method proposed in this paper is free of
the above limitations, since it takes into consideration the entire shapes of probability distributions, thereby being capable
of “sensing” the discrepancy between virtually inﬁnite number
of empirical moments.
Apart from providing the explicit formulas necessary for
implementing the proposed segmentation method, the current
study extends its contribution in an additional direction. Since
the proposed method works with empirical distributions (which
are functions of the data samples, whose number varies in
time as the segmentation converges), the issue of estimation
consistency and its preservation during the iterations should
not be overseen. Consequently, in this paper we also introduce
a computationally efﬁcient procedure which allows one to
automatically control and adjust the smoothness properties of
the estimated distributions.
In concluding this introductory section, it should be mentioned that the proposed segmentation method exploits the technique of active contours that has become very popular over the
1Since log is a strictly monotone, increasing function, maximizing the Bhattacharyya distance is equivalent to minimizing the corresponding coefﬁcient.
For this reason, we prefer using the coefﬁcient instead of the distance, as it leads
to simpler analytical expressions.
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
MICHAILOVICH et al.: IMAGE SEGMENTATION USING ACTIVE CONTOURS
last few decades . This methodology is based on the utilization of deformable contours which conform to various object shapes. The contour deformation is typically driven by a
gradient ﬂow stemming from minimization of an energy functional, which can be dependent on either local (e.g., gradient
ﬁeld) or global (e.g., mean intensity) properties of the image.
In the latter case, the resulting active contours are referred to as
region-based, and this is the group of methods to which the approach reported in this paper belongs.
The paper is organized as follows. Section II brieﬂy revises
some fundamental aspects of the level-set framework and
introduces the Bhattacharyya energy functional, as well as the
related gradient ﬂow. The statistical assumptions underpinning
the proposed approach are discussed in Section III. Some
possible choices of the discriminative features are summarized
here, as well. Section IV deals with the problem of automatically controlling the smoothness of empirical distributions and
describes a simple method to perform this control. Section V
discusses a number of possible ways of extending the results
of the preceding sections to multiobject scenarios. Several
key results of our experimental study are demonstrated in
Section VI, while Section VII provides examples of practical
cases in which the proposed methodology can be advantageous
over some existing techniques. Section VIII ﬁnalizes the paper
with a discussion and conclusions.
II. BHATTACHARYYA FLOW
A. Level-Set Representation of Active Contours
In order to facilitate the discussion, we conﬁne the derivations
below to the case of two classes (i.e., when the problem to be
dealt with is that of segmenting an object of interest out of its
background), followed by describing some possible ways to extent the proposed methodology to multiobject scenarios.
In the two-class case, the segmentation problem is reduced to
the problem of partitioning the domain of deﬁnition
) into two mutually exclusive and
complementary subsets
. These subsets can be represented by their respective characteristic functions
which can, in turn, be deﬁned by means of a level-set function
in the following manner. Let
be the Heaviside
function deﬁned in the standard way as
Then, one can deﬁne
Given a level-set function
, its zero level set
is used to implicitly represent a curve—active contour—embedded into
. For the sake of concreteness, we associate the subset
with the support of the object of interest,
is associated with the support of corresponding background. In this case, the objective of active-contour-based image
segmentation is, given an initialization
, to construct a
convergent sequence of level-set functions
) such that the zero level-set of
coincides with the boundary of the object of interest.
The above sequence of level-set functions can be conveniently constructed using the variational framework .
Speciﬁcally, this sequence can be deﬁned by means of a gradient ﬂow that minimizes the value of a properly deﬁned cost
functional . In the case of the present study the latter is
derived in the following way. First, the image to be segmented
is transformed into a vector-valued image of its local
.2 Note that the feature image
ascribes to
every pixel of
-tuple of its associated features, and,
hence, it can be formally represented as a map from
Subsequently, given a level-set function
, the following
two quantities are computed:
are two scalar-valued
functions with either compact or effectively compact supports
(e.g., Gaussian densities). Provided that the kernels
are normalized to have unit integrals with respect to the
feature vector , viz.
given by (3) and (4)
are nothing else but kernel-based estimates of the probability
density functions (pdfs) of the image features observed over the
subdomains
, respectively , .
The core idea of the preset approach is quite intuitive and it is
based on the assumption that, for a properly selected subset of
image features, the “overlap” between the informational contents of the object and of the background has to be minimal.
In other words, if one thinks of the active contour as of a discriminator that separates the image pixels into two subsets, then
the optimal contour should minimize the mutual information between these subsets. It is worthwhile noting that, for the case at
hand, minimizing the mutual information is equivalent to maximizing the KL divergence between the pdfs associated with the
“inside” and “outside” subsets of pixels. For the reasons discussed below, however, instead of the divergence, we propose to
maximize the Bhattacharyya distance between the pdfs. Specifically, the optimal active contour
is deﬁned as
2In the section that follows, we will elaborate on some possible choices of
the feature space. Meanwhile, in order to clarify the meaning standing behind
of this operation, sufﬁce it to note that J could be, for example, the image I(x)
itself or the vector-valued image of its partial derivatives.
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
being given by the (3) and
(4), correspondingly.
B. Gradient Flow
In order to contrive a numerical scheme for minimizing (6),
its ﬁrst variation should be computed ﬁrst. The ﬁrst variation of
(with respect to
) can be easily shown to be given
Differentiating (3) and (4) with respect to
, one obtains
is the delta function, and
are the areas
, respectively.
By substituting (8) and (9) into (7) and combining the corresponding terms, one can arrive at
Assuming the same kernel
is used for computing the
last two terms in (11), i.e.,
, the latter
can be further simpliﬁed to the following form:
Finally, introducing an artiﬁcial time parameter , the gradient
that minimizes (6) is given by
where the subscript
denotes the corresponding partial derivative, and
is deﬁned as given by either (11) or (12).
From the viewpoint of statistical estimation, the cost function (6) can be thought of as accounting for the ﬁdelity of estimation of the optimal level-set function to observed features
. However, this cost function does not take into consideration some plausible properties of the optimal solution, and,
as a result, minimizing (6) alone could be too sensitive to measurement noises and/or errors in the data. In order to alleviate
this sensitivity, one can attempt to ﬁlter out the spectral components of the solution which belong to the noise subspace. For the
case at hand, one can regularize the solution via constraining the
length of the active contour, in which case the optimal level-set
is given by
denotes the operator of gradient,
is the Euclidean
is a regularization constant, which controls the
compromise between ﬁdelity and stability. The gradient ﬂow
associated with minimizing the cost functional in (15) can be
shown to be equal to
is the curvature of the active contour given by
. The form of (15) suggests that
the gradient ﬂow (16) will converge to a (local) solution of minimal curvature, which results in the maximal distance between
the empirical distributions
as measured by
the Bhattacharyya coefﬁcient (1). All the segmentation results
reported in the present study have been obtained via numerically
solving (16) with the value of
set to be equal to 1.
Finally, it should be noted that the gradient ﬂow (16) would
change the level-set function
over a set of zero measure, if
the formal delta function
was exploited in computations.
In order to overcome this “technical” difﬁculty, it is common
to extend the numerical support of the level-set evolution via
replacing the delta function by its smoother version
could be deﬁned as given by, e.g., 
otherwise.
Note that the support of
is ﬁnite and it is controlled though
the user-deﬁned parameter
III. STATICAL FRAMEWORK
A. Assumptions and Limitations
The proposed model for evolving the active contours is based
of the notion of discrepancy between the empirical densities
of the feature vector
associated with the
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
MICHAILOVICH et al.: IMAGE SEGMENTATION USING ACTIVE CONTOURS
object and background classes, respectively. We note that using
pdfs for estimating the optimal shape of the active contour is also
supported by the well-known likelihood principle , which
states that in making either inferences or decisions about a parameter of interest after data is observed, all relevant experimental information is contained in the likelihood function of the
observed data.
However, before we turn to describing some additional
aspects related to solving (15) by means of the gradient ﬂow
(16), the statistical assumptions, based on which the solution
is computed, should be underlined. First of all, it has been
assumed that, for the case of two segmentation classes, the
samples of
can be characterized by the two conditional
, which describe
the distribution of the feature vector
in the object and in the
background class, respectively. Moreover, it has been assumed
that the pixels of the feature image
that pertain to the
object class are independent and identically distributed (i.i.d.)
, whereas the samples of
representing the background are i.i.d. copies of
distributed
according to
An obvious disadvantage of using the above statistical assumptions is the fact that they do not allow one to take into
consideration the dependency structure between the samples of
(or between those of
, for that matter). It goes without
saying that ignoring the interpixel dependency is a simpliﬁcation, which has been mainly used to make the proposed segmentation method feasible from the viewpoint of its practical implementation. Undoubtedly, the precision of the method could be
improved, if the above dependency was accounted for by means
of, e.g., the theory of Markov ﬁelds . However, such a more
rigorous modeling would have necessitated the estimation of the
dependency structure, thereby substantially increasing the computational cost of the overall segmentation process. Hence, in a
certain sense, assuming the interpixel independency “trades” the
model precision for the practicability of the resulting numerical
It should also be noted that the kernel density estimates (3)
and (4) exploit the assumption of ergodicity, which allows one
to estimate the densities based on “spatial” averaging rather than
on ensemble averaging. Assuming the ergodicity could also be
viewed as a limitation, though somewhat less critical one as
compared to the assumption of interpixel independency.
Finally, a few comments should be made regarding the form
of the velocity
as given by (12). One can see that the
latter consists of two terms. The ﬁrst of these terms is independent of the spatial coordinate
, and it results in either increment or decrement of the mean value of the level-set function
by a constant amount depending on the ratio between the
. On the other hand, the second component of
the velocity is coordinate dependent, and it can be viewed as a
smoothed version of the function
deﬁned by (13), whose
form deserves a special attention. Speciﬁcally,
as a difference between the square roots of the likelihood ratios
weighted by corresponding
areas. This fact immediately connects the proposed curve-evolution model to the hypothesis testing by means of likelihood ratio
tests . As a matter of fact, at each iteration, the gradient ﬂow
(16) “performs” a likelihood ratio test so as to alter the contour’s
shape by the forces, which make it include into its interior the
image pixels, which are likely to belong to the object class. The
existence of such an interpretation, as well as its connection to
the classical theory of hypothesis testing, further supports the
reasonability of the proposed variational framework.
B. Examples of Discriminative Features
In this section, a number of possible deﬁnitions of the feature
are discussed. The set of examples given below is by
no means complete, but it rather represents the features most
frequently used in practice.
Formally, the transition from the data image
the vector-valued image
of its features can be
descried by a transformation
applied to
. Hence, the question of selecting a useful set of image
features is essentially equivalent to the question of deﬁning a
proper transformation
. Perhaps, the simplest choice here is
to deﬁne the feature image
to be identical to
corresponds to the case of
being identity and
.3 In this
case, the features used for the classiﬁcation are the gray levels
, and the resulting segmentation procedure is essentially
histogram based .
Although the above choice has proven useful in numerous
practical settings, it is deﬁnitely not the best possible for the
cases when both object and background have similar intensity
patterns. In this case, it seems reasonable to take advantage of a
relative displacement of these patterns with respect to each other
via transforming
to the space of its partial derivatives. This
transformation is performed by setting
, in which case
the feature space becomes two-dimenational, i.e.,
As a next logical step, one can smooth the above gradient
using a set of low-pass ﬁlters with progressively decreasing bandwidths. This construction brings us directly to the
possibility to deﬁne
to be a wavelet transform . Note that,
in this case, each pixel of the resulting
carries information on multiresolution (partial) derivatives of
. It should be
noted that using the wavelet coefﬁcients as discriminative features for image segmentation has long been successfully used in
numerous applications .
The dependency structure between the partial derivatives of
can be captured by the structural tensor deﬁned as given
denoting the corresponding partial derivatives. In this case, the feature
space is 3-D, as for each
is deﬁned to be equal to
that this choice of the feature space should be treated with
precaution, since the latter is no more a linear space. This is
because the structural tensor (18) is positive deﬁnite, and, therefore, the pixels of
deﬁned as above are no more elements
of an Euclidean space, but rather of a nonlinear manifold. Fortunately, the availability of kernel-based methods for estimating
3It should be noted that, even though in the deﬁnitions above the data image
I(x) is scalar-valued, it should not be necessarily so. I(x) may be a color image,
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
probability densities deﬁned over nonlinear manifolds makes it
possible to apply our approach in this situation, as well .
Another interesting choice of
can be followed in the scenarios, in which
appears as an element of a sequence of
tracking images. In this case,
can be deﬁned
to be the vector ﬁeld of local displacements of the gray levels
. Speciﬁcally, let
denote the temporal (partial)
derivative of
. Let also
be a 2-D column vector with
its ﬁrst and second components equal to
, respectively. Then, the gray-level constancy
constraint can be shown to result in the following least
square solution for the local displacement ﬁeld
being the structure tensor given by (18).4 Consequently, in the case when the motion of the tracked object is
independent of that of its background, the feature image can be
deﬁned by setting
. We note that this choice seems
to be reasonable for many tracking scenarios, where the background motion is either negligible or associated with the egomotion of camera, which rarely correlates with the dynamics of
tracked objects. It is also worthwhile noting that motion-based
segmentation represents an independent ﬁeld of research,
which embraces many powerful segmentation techniques. Thus,
the method presented in this study becomes a speciﬁc instance
of this class of approaches, when
is related to motion-based
features of
The local moments of
 , multiresolution versions
thereof , and the local fractal dimension are among
many other image features, which could be used for the segmentation. Note that a combinational use of all the features mentioned above is also possible. Thus, for example, by letting the
feature vector
(whose speciﬁc realization at
is given by
) be composed of the intensity
velocity components (
), one can perform the segmentation based on both gray-scale and motion information, thereby
bridging the gap between the segmentation approaches, which
utilize these features separately. It should be noted that in this
case, it is reasonable to assume that the intensity is independent
of the motion. Consequently, the pdf of
can be factorized as
so as to reduce the computational complexity of segmentation.
IV. UPDATING THE EMPIRICAL DISTRIBUTIONS
A. Kernel-Based Estimation
In the current study, the conditional densities
are estimated using the kernel estimation
method , . In this section, we consider the subject of
properly deﬁning the kernel’s bandwidth, which should be consistent with the data size for the estimates to be reliable .
Before we start, it should be noted that the continuously-deﬁned
form of the kernel estimates in (3) and (4) was used merely for
the convenience of derivation of the gradient ﬂow (16). In what
4Note that, strictly speaking, the tensor T(x) has rank one, and, hence, it is
not invertible. To overcome this technical difﬁculty, it is common to average the
values of the tensor over a small (e.g., 3  3) neighborhood of x.
follows, however, we resort to the discrete form of these estimates, as it is more appropriate to the practical case, where data
images are represented by their samples over
In the discrete setting, the kernel density estimation amounts
to approximating the (unknown) pdf
-dimensional
random vector
independent realizations of . In (20), the
is parameterized by a vector
and has a unit integral, i.e.,
A number of possible deﬁnitions of the kernel
possible , among which the most frequent one is to de-
to be a Gaussian pdf, and this is the choice followed in the current study. Moreover, to facilitate the numerical
implementation of the density estimation, we use a separable
(isotropic) form of the Gaussian pdf, in which case
are the coordinates of , and the standard
deviations
control the extension of
corresponding directions. It should be noted that the separability
of the kernel in (21) does not imply the separability of the estimate
that is now given by
The kernel method of density estimation has proven valuable
in numerous applications. It is well known, however, that effective use of this method requires proper choice of the bandwidth
parameters
. When insufﬁcient smoothing is done (i.e., the
bandwidth parameters are too small), the resulting density estimate is too rough and contains spurious features that are artifacts of the sampling process. On the other hand, when excessive smoothing is done, important features of the underlying
structure are smoothed away. Consequently, to optimize the accuracy of the estimates in (3) and (4), the bandwidth parameters
should be properly deﬁned. This can be done using the
procedure we describe in the next section.
B. Deﬁning the Bandwidth
In order to avoid unnecessary mathematical abstraction, we
conﬁne the discussion below to the case of
. Note that
such a reduction of dimensionality can by no means be considered as a limitation, as the kernel
[as given by (21)] is separable. Consequently, the results derived below can be straightforwardly applied to each 1-D component of
independently.
In the scalar case, the kernel estimate of
observations of
being a scaled version of
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
MICHAILOVICH et al.: IMAGE SEGMENTATION USING ACTIVE CONTOURS
the normal density
. In this case, a standard way to determine the optimal value of the bandwidth parameter
via minimization of the asymptotic mean integrated square
error between the original pdf
and its estimate
optimal value can be shown to be given by 
denotes the
-norm of the second derivative
. Substituting the Gaussian kernel in (23) results in
An exact computation of the above optimal value is obviously
impossible, since it requires knowing the derivative of an unavailable quantity, i.e., of
. In order to overcome this difﬁculty, it is standard to use an approximation of the above derivative instead of its exact value. Constructing approximations of
this kind has long been an intense research topic in statistics,
where all such methodologies proposed so far can now be categorized into the ﬁrst and the second generation methods .
Although, in general, the second generation methods are more
accurate, their implementation often requires iterative solution
of nonlinear equations, which makes these methods difﬁcult to
integrate into the segmentation procedure under consideration.
On the other hand, a considerable gain in computational ef-
ﬁciency (on account of insigniﬁcant loss in accuracy) can be
achieved by exploiting the ﬁrst generation methods. One of the
methods of this kind (which seems to be ﬁrst proposed in )
is based on assuming the unknown
to be Gaussian. In this
case, the resulting optimal bandwidth is given by
is the second moment of
, which can be effortlessly estimated as the sample variance of
. Although
to be Gaussian may be an oversimpliﬁcation, this
assumption has been observed to work quite satisfactory in numerous cases of practical interest. For this reason, in the present
study, we choose to deﬁne the optimal bandwidth according to
In the case of two segmentation classes, the active contour divides the image domain into two subdomains. As a result, the
total number of pixels
can be expressed as the sum of
pixels belonging to the object class and
pixels belonging
to the class of background. Moreover, as the shape of the active
contour constantly changes in the course of its evolution, so do
the numbers
. As a result, the optimal bandwidths
corresponding to the object and its background, respectively, become functions of the iteration time , as they are
are the standard deviations associated with
the conditional densities
respectively. In order to estimate the constant factors
, one can ﬁrst classify the image features using some simple,
“course-level” algorithm, followed by computing the standard
deviations of the classes thus obtained. Alternatively, one can estimate these constants by means of the expectation maximization
algorithm assuming a Gaussian mixture model for the (unconditional) likelihood of
. Yet even simpler method for computing
would be to set them both equal to the standard deviation of the entire (i.e., unclassiﬁed) data set. Though
trivial, the latter approach has been observed to work reliably in
practice, and, for this reason, it was used to derive all the results
reported in the experimental part of this paper.
The variability of the optimal bandwidths in (26) creates the
need for constantly recomputing the kernel density estimates (3)
and (4). It goes without saying that such “re-estimations” are
very undesirable from the practical point of view, as they could
considerably increase the overall computational load. Thus, it
is tempting to ﬁnd a way to update the density estimates in a
computationally efﬁcient manner. A possible solution for this
problem is proposed next.
C. Bandwidth Adjustment via Isotropic Diffusion
To simplify the presentation, let us consider ﬁrst the problem
of estimating the pdf
of a random variable
given a timevarying set of its independent observations
that, at time
consists of
observations,
and, hence, the kernel estimate of
is given by
is the optimal bandwidth corresponding to
is an appropriately scaled Gaussian kernel. Subsequently, suppose that at a latter time
, some of the observations are excluded from
. Let the subset of the indices of these “excluded” observations be denoted by
. Then, the density estimate
can be updated according to
being the size of
It should be noted that computing (28) is “cheap,” as normally
. Denoting the new set of observations by
the updated density estimate (28) can be alternatively represented as given by
The above estimate could be considered as optimal, if the
in (29) was replaced by
the optimal bandwidth corresponding to
. Consequently,
in order to restore the optimality of (29), the kernel
5If the set fz g
had “gained” some observations, the sign in (28) should
have been changed to plus and n(t ) would have been equal to n(t ) + kk,
with  being the indices of the “gained” observations.
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
(29) should be “substituted” by
. In the current paper, we
propose to do this via the process of isotropic diffusion .
Speciﬁcally,
is transformed into
by means of the
following diffusion equation:
denotes the diffusion time (which should not be confused with the “iteration” time ). The closed form solution to
(30) can be readily shown to be given by
denotes, as before, the Gaussian density with its
standard deviation equal to
stands for the operator of
convolution. Moreover, the semigroup property of the diffusion
implies that
. Consequently, by setting
The above expression suggests that the kernel
transformed into
by diffusing the former for the period of
time equal to
. As a result, assuming
that the optimal bandwidth depends on data size according to
(25), the time interval necessary for the above diffusive density
update is given by
Needless to say, the diffusion process can be applied directly
to (29), since the latter is formed as a linear combination of
mutually shifted versions of
. Thus, the entire process of
updating this estimate consists of two steps. First, provided that
the difference
has exceeded a predeﬁned
threshold, the estimate is recomputed according to (28) (or using
an analogous formula for the case when
increases). Subsequently, the resulting density is subjected to the process of diffusion for the time duration of
deﬁned by (34). We note that,
in practice, the diffusion can be implemented either via direct
convolution with
or by means of solving a discrete approximation to (30).
D. Forward versus Backward Diffusion
In the case when the number of data samples decreases, the
update procedure described above leads to the well-posed operation of direct (forward) diffusion. In this case, however, when
this number increases, the time interval in (34) becomes negative, and, as a result, the diffusion needs to be performed in
the backward direction in time. Unfortunately, such an inverse
diffusion is well known to be an ill-conditioned, unstable operation.
The above instability makes it impossible to “run” the inverse diffusion for relatively long time intervals. However, the
fact that it usually takes some time for the inverse diffusion to
Fig. 1. Graph of the function y[n] = n
blow-up can be used to “regularize” the process of density estimation in the following way. When the sample size increases,
one can propagate the diffusion in the backward direction till
the ﬁrst signs of instability start to show up. At this moment,
the diffusion is terminated and the kernel density estimate is
re-estimated by mean of, e.g., the fast Gauss transform . In
practice, however, it was observed that such re-estimations almost never have to be done. This is because, the optimal bandwidth given by (25) is proportional to
, which is a very
slow-varying function of
for relatively large values of the latter
(see Fig. 1). As a result, for
, substantial variations in
the number of data points result in only negligible variations of
corresponding bandwidth. It should be noted, however, that the
situation changes cardinally when, for example, the active contour converges on a small target represented by relatively small
number of image samples. In this case, the variability of
signiﬁcant, and, as a result, the optimal bandwidth is very “sensitive” to variations of
. Thus, for the case of decreasing
updating the smoothness properties of kernel density estimates
becomes crucial. Fortunately, it can be done via the well-posed
and computationally efﬁcient process of forward diffusion as
described in the preceding section.
Finally, we note that, in the current study, the velocity
(16) was computed using the simpliﬁed expression (12), which
utilizes a single kernel function
[as opposed to
in (11)]. This simpliﬁcation, however, is allowed by
the fact that the second component in (12) is always estimated
using a ﬁxed number of data samples, which is deﬁned by the
support of
. In summary, the proposed algorithm can be
summarized in the form of the pseudo-code shown in Table I.
V. MULTIOBJECT CASE
In order to extend the applicability of the proposed method to
the case when images contain more than one object of interest,
its multiclass version should be addressed next. In particular,
in such a scenario, the image domain
is considered to be a
(mutually exclusive) subdomains
of which is associated with a corresponding (conditional) pdf
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
MICHAILOVICH et al.: IMAGE SEGMENTATION USING ACTIVE CONTOURS
PSEUDO-CODE OF THE PROPOSED SEGMENTATION METHOD
. Consequently, the ﬁrst step to be done is to generalize
the deﬁnition of the Bhattacharyya coefﬁcient (1) to the case
densities. Such a natural generalization is known as the
average Bhattacharyya coefﬁcient and it is given by 
. The above coefﬁcient represents a cumulative measure of discrepancy between all the possible pairs
of the densities under consideration. Note that the normalization constant
guarantees that the coefﬁcient (35)
takes its values in the interval
The additivity of the construction in (35) makes it trivial to
derive the corresponding gradient ﬂow. In fact, each additive
term of the cost functional can be differentiated independently
so that the resulting gradient ﬂow is given as a sum of the gradient ﬂows related to the additive components in (35). In order to
complete the multiclass formulation of the Bhattacharyya ﬂow,
however, a few words need to be said regarding the deﬁnition of
the level-set function.
Obviously, in the multiclass case, using only one level-set
function would be insufﬁcient to solve the problem at hand.
Consequently, we follow the multiphase segmentation formulation of that uses
level-set functions
, which are
capable of segmenting the image
into (up to)
The idea standing behind the above approach is as simple as
brilliant. In particular, since one level-set function partitions the
image domain
into two subdomains,
level-set functions can
subdomains, each of which is labelled by the
signs of the level-set functions
in that subdomain.
For example, when
, we obtain four subdomains, viz.
.6 For the reason of space limitation, the
formulas for the multiphase gradient ﬂow corresponding to the
case of (35) are not provided in this paper. However, these formulas can be easily obtained by “plugging” the results of Section II into the “templates” derived in , mutatis mutandis.
6It should be noted that, in the case when 2
> M, any of the unnecessary
2   M phases simply remain idle.
Fig. 2. (Upper row of subplots) Intensity-based segmentation of Zebra. (Lower
row of subplots) Corresponding empirical densities of Zebra and its background.
VI. RESULTS
A. Intensity-Based Segmentation
The experimental study of the present paper consists of three
parts, each of which aims at demonstrating different characteristics of the proposed segmentation method. In the current section,
the test images are segmented based on their intensities alone.
The ﬁrst example here is the image of Zebra, which is considered to be relatively hard to segment due to the multimodality
of the pdf related to the object class. The segmentation results
obtained for this image are shown in Fig. 2, the subplots A1–A4
of which depict the initial, two intermediate and the ﬁnal shape
of the active contour, respectively. The corresponding empirical
densities of the object and background classes are shown in the
lower row of subplots in Fig. 2. We note that the initial position
of the contour was chosen to be such that the number of data
samples in both segmentation classes were equal. One can see
that the algorithm results in a useful segmentation, which well
agrees with the true shape of Zebra. It should be noted, however,
that the ﬁnal segmentation ascribes a portion of the shadow near
Zebra’s hoofs to the object class. It is because that the intensity
levels of the shadow are very close to those of the stripes on
Zebra’s skin. In this case, the intensity information is insufﬁcient to achieve the ideal result.
Our next example is much more challenging than the previous
one. This is a synthetic image generated according to the segmentation mask shown in the leftmost subplot of Fig. 3. This
image is referred below to as the image of Cat, since its object
class resembles the silhouette of a sitting cat. The corresponding
data image is shown in the middle subplot of the same ﬁgure.
To generate this image, its “object” samples were drawn as independent realizations of
obeying the Rayleigh distribution.
At the same time, the samples of the corresponding background
were drawn as independent realization of another random variable
, which was related to
according to
being the mean value of
. It should be noted that the probability densities deﬁned in this way have the same even (central)
moment, while their odd moments are identical up to the sign.
These densities are shown in the rightmost subplot of Fig. 3.
The segmentation results obtained for the Cat image are
demonstrated in Fig. 4. One can see that, even though the cat
is virtually indistinguishable from its background to the eye
of a human observer, the proposed algorithm is capable of
providing a useful segmentation in this case. Note also how
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
Fig. 3. (Right) Segmentation mask of Cat. (Middle) Realization of Cat.
(Left) Original densities of Cat and its background.
Fig. 4. (Upper row of subplots) Intensity-based segmentation of Cat. (Lower
row of subplots) Corresponding empirical densities of Cat and its background.
well the estimated class-conditional densities agree with the
original densities shown in Fig. 3.
B. Segmenting Vector-Valued Features
The object and background classes of the image shown in
Subplot A of Fig. 5 have the same intensity distributions, and,
hence, their discrimination based on gray-level information
alone is by no means possible. It should be noted, however, that
the image pattern is deﬁned to be homogeneous all over the
image domain, except for a round central region (to be understood as the object), which is rotated with respect to its exterior
by 45 . In this case, one can take advantage of the relative
displacement of the object with respect to the background via
deﬁning the feature image
to be the gradient
test image
. The partial derivatives of
in the row and
the column directions are shown in Subplots B and C of Fig. 5,
respectively, while Subplot D of the ﬁgure shows the phase
of the gradient
. It is interesting to observe
that the distributions of the phase values within the object and
background classes appear to be very similar. This fact implies
the impossibility to segment the object of interest based on the
“orientation” information alone. However, using both partial
derivatives as discriminative features makes the segmentation
easily achievable as shown by Subplots E-H of Fig. 5. One
can see that, in this case, the active contour is also capable of
correctly identifying the true object shape.
An additional segmentation result is shown by Subplots A-D
of Fig. 6. In this case, the vector-valued features are composed
of the monochromatic component of the test image of Surfer.
Once again, one can see that the active contour succeeds well in
ﬁnding the correct shape of the object of interest.
Fig. 5. (Subplots A–D, left-to-right) Image of rotated pattern to be segmented,
the row partial derivatives of the image, the column partial derivative of the
image, the phase of the image gradient. (Subplots E–H) Gradient-based segmentation of rotated pattern.
Fig. 6. Color-based segmentation of Surfer.
C. Combined Intensity-Motion Segmentation
The main objective of our last example is to demonstrate the
value of image segmentation based on both “static” and “dynamic” image features. In particular, in this case, the ﬁrst component of the feature image
is set to be equal to the image
, whereas its two last components are set to be
equal to the local displacements (velocities) of the gray levels
due to the motion of both object and camera.
The subplots A1–A4 of Fig. 7 show the result of segmenting
the image of Leopard using the intensity information only.
Obviously, the similarity between the intensity patterns of
Leopard’s fur and of the surrounding terrain, implies a similarity between the intensity distributions within the object
and background classes. Consequently, the intensity-based
segmentation fails to discriminate between the object and
background in a satisfactory manner. In this case, the active
contour converges on the part of the object that differs the most
from the background, while failing to “catch” the whole object.
On the other hand, combined intensity-motion segmentation
provides quite satisfactory results, which are shown in Subplots
B1–B4 of the same ﬁgure. Note that the local velocities (displacements) were computed using two successive images of the
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
MICHAILOVICH et al.: IMAGE SEGMENTATION USING ACTIVE CONTOURS
Fig. 7. (Subplots A1–A4) Intensity-based segmentation of Leopard. (Subplots
B1–B4) Joint intensity-motion-based segmentation of Leopard.
Fig. 8. Local displacement ﬁeld of Leopard.
original sequence according to (19). A subsampled version of
this velocity ﬁeld is depicted in Fig. 8 (where the subsampling
has been done for the sake of clarity of visualization). Therefore, we conclude that motion-based features are quite useful
in dynamic scenarios, in which image intensity cannot provide
sufﬁcient information to perform the image segmentation in a
satisfactory way.
VII. COMPARATIVE STUDY
In the preceding section, the viability of the proposed segmentation method was demonstrated experimentally via its
successful performance on a number of examples of practical interest. Unfortunately, little can be deduced from these
examples as to what advantages are offered by this method
as compared to existing segmentation techniques. In particular, the Bhattacharyya distance is only one instance out of a
number of possible deﬁnitions of distances between probability
densities. Perhaps, the most famous among such distances is
the KL divergence that could have been used instead of the
Bhattacharyya coefﬁcient to derive a level-set evolution .
Hence, it is tempting to identify conditions under which the
KL divergence would result in an inferior segmentation as
compared to the proposed approach.
If the KL divergence had to be used instead of the Bhattacharyya coefﬁcient, then
in (15) would have to be
replaced by
, where the symmetrized KL divergence
is given by
In this case, the corresponding velocity
(which is to be
substituted in the ﬂow (16) can be shown to be equal to
Apart from its being more complicated to compute as compared with the Bhattacharyya velocity (12)–(13), the KL velocity (37)–(38) has an additional drawback that stems from the
properties of the functions involved in its computation. In particular, the logarithm function used in (36)–(38) is known to be
very sensitive to variations of its argument in vicinity of relatively small values of the latter. Moreover, the logarithm is undeﬁned at zero, which makes computing the KL velocity susceptible to numerical errors which should be expected when the
approach zero. The above properties of the
logarithm are obviously disadvantageous, as they make the KL
divergence prone to the errors caused by inaccuracies in estimating the tails of probability densities. On the other hand, the
square root is a well-deﬁned function in vicinity of zero. Moreover, for relatively small values of its argument, the variability
of the square root is considerably smaller than that of the logarithm. Consequently, the Bhattacharyya ﬂow should be much
less susceptible to the inﬂuence of the inaccuracies mentioned
Alternatively to the information-based formulation of ,
the active contours can be propagated using the ML formulation as suggested in . In the notations of this paper, the ML
optimal level-set function is sought as a minimizer of the negative log-likelihood of observed features that is deﬁned as given
denote a priori known probability densities corresponding to the object of interest and its background.
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 11, NOVEMBER 2007
It is interesting to note that, in this case, the velocity
given by the log-likelihood ratio
Thus, for example, in the case when the feature image
formed by image intensities (i.e.,
), and the densities
are Gaussian with their mean values equal
, and their variances equal to
, respectively, the velocity (40) becomes
where the estimates
are supposed to be recomputed at each iteration.
In addition to its being well founded from a theoretical perspective, the ML formulation is also advantageous in providing
a way to predeﬁne an optimal value of the regularization parameter
in (15)–(16). Speciﬁcally, the minimum description length
principle allows one to preset this value to be equal to
as in . Unfortunately, this advantage of the ML formulation seems to be counterbalanced by its dependency on a priori
knowing the class conditional likelihoods
well as by its being sensitive to inaccuracies in deﬁning the
latter. The problematic character of the above properties of the
ML approach is demonstrated through the example that follows.
The leftmost subplot of Fig. 9 shows the original segmentation mask (template) that deﬁnes the object of interest as a
circle over uniform background. The test image shown by the
center subplot of the same ﬁgure was synthesized according
to the above template with both object and background densities deﬁned to be Gaussian pdfs with zero means and variances
equal to 1 and 2, respectively. In parallel, another test image
was synthesized in similar manner except for the fact that 4%
of its (randomly picked) pixels were replaced by independent
realizations of a Gaussian random variable with zero mean and
variance equal to 16. This image is depicted in the rightmost
subplot of Fig. 9. Note that the “substituted” pixels of the latter
image can be thought of as outliers which are expected to introduce errors in estimation/modeling of the class conditional
densities.
The results of intensity-based segmentation of the test images of Fig. 9 are demonstrated in Fig. 10, subplots A1–A3 of
which show the ﬁnal active contours obtained by using the ML
velocity (41), KL velocity (37), and the Bhattacharyya velocity
(12), respectively. In order to compare these results in a quantitative manner, for each method, the probability of misclassi-
Fig. 9. (Left) Original template. (Center) Outliers-free image. (Right) Image
contaminated by outliers.
Fig. 10. (Subplots A1–A3) Segmentation of the outliers-free image of Fig. 9
by the active contours maximizing the log-likelihood, KL divergence, and Bhattacharyya distance, respectively. (Subplots B1–B3) Segmentation of the outliers-contaminated image of Fig. 9 by the active contours maximizing the loglikelihood, KL divergence, and Bhattacharyya distance, respectively.
ﬁcation error was estimated as an expected number of misclassiﬁed pixels normalized to the total number of pixels, with the
expectation being approximated by averaging the results of 200
independent trials. Speciﬁcally, the empirical error probabilities
for the ML, KL, and Bhattacharyya segmentations were found
to be equal to 0.83%, 1.07%, and 0.88%, respectively. One can
see that, in this case, the ML segmentation has the lowest error
due to its using the correct models for the class conditional likelihoods. On the other hand, the KL segmentation appears to be
the worst performer here, because of the numerical errors discussed in the beginning of this section. One can also see that the
error of the proposed method closely approaches that of the ML
segmentation.
As a next stage, the segmentation methods under consideration were applied to the outlier contaminated image shown in
the rightmost subplot of Fig. 9. For this case, typical segmentation results are shown in Subplots B1–B2 of Fig. 10. One can see
that the lack of correspondence (caused by the outliers) between
the assumed and actual class-conditional likelihoods makes the
ML segmentation converge to an incorrect solution (see Subplot
B1). The error probability of this segmentation was found to be
equal to 10.2%. Moreover, the outliers induce relatively small
deviations at the tails of the kernel density estimates. These deviations, in turn, are “translated” into sizable errors in estimation
of the KL divergence (as well as of the related gradient ﬂow) due
to the high sensitivity of the logarithm in vicinity of zero. Consequently, the KL segmentation results in an erroneous solution, as
well (Subplot B2). For this case, the error probability was found
Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on May 21, 2009 at 12:25 from IEEE Xplore. Restrictions apply.
MICHAILOVICH et al.: IMAGE SEGMENTATION USING ACTIVE CONTOURS
to be equal to 9.7%. On the other hand, the Bhattacharyya gradient ﬂow succeeds to converge to a useful solution with 0.91%
error (as shown by Subplot B3), thereby exhibiting remarkable
insensitivity to both model errors and the inﬂuence of outliers.
VIII. DISCUSSION AND CONCLUSIONS
In this paper, a method for image segmentation by active contours has been proposed. As in most of the methods using a
similar classiﬁcation mechanism, the derivation of the proposed
approach is based on a variational analysis, in which the active
contour is driven by the forces stemming from minimization of
a cost functional. In the current work, the latter is formulated
based on the discrimination principle that seems to be intrinsic
in the way the human visual system functions . In particular,
the cost functional is deﬁned as a measure of dissimilarity between the informational contents of segmentation classes, and
as a result, the active contour is forced to converge to the shape
that minimizes the “overlap” between these contents.
In the heart of the proposed segmentation method is the notion of a distance between probability densities. In particular,
the active contours have been evolved to maximize the Bhattacharyya distance between nonparametric (kernel-based) estimates of the probability densities of segmentation classes. In
this case, it would be conceptually analogous, if one tried to
minimize the mutual information shared by the classes via maximizing the KL divergence between the corresponding probability densities. Yet, in the experimental study of Section VII,
it was demonstrated that this alternative criterion may result in
the performance inferior to that of the proposed method due to a
relatively high sensitivity of the KL divergence to inaccuracies
in estimation of the tails of the class conditional densities.
Using the nonparametric estimates of probability densities allows one to apply the proposed segmentation method in the situations when little is known on the distributions of image features within different segmentation classes. However, if the information on distributions was a priori available, the segmentation could have been achieved by means of the ML approach of
 , , which has an advantage of providing a way to prede-
ﬁne an optimal values of the regularization parameter
Unfortunately, the availability of the above information seems
to be rare in practice considering the vast diversity of possible
images and their related features. Moreover, the errors in modeling the class conditional densities are capable of substantially
degrading the performance of the ML segmentation, as it was
demonstrated in Section VII.
As a possible extension, the method proposed in this paper
can be easily modiﬁed to incorporate shape priors as it is suggested, e.g., in . In this case, the principal component analysis (PCA) is used to represent the optimal level-set as an element of a ﬁnite-dimensional vector space spanned by the mean
value and a predeﬁned (relatively small) number of the principal
components which correspond to a given set of training shapes.
An alternative way to impose smoothing constrains on the segmentation can be by using the Sobolev active contours recently
proposed in .
In this paper, kernel density estimation , was
employed to compute the class conditional densities in a
nonparametric manner. In order to increase the computational
efﬁciency of the estimation, the kernel functions were deﬁned
to be isotropic Gaussian densities. It is well known, however,
that better density estimation is possible using anisotropic kernels, as it is shown, e.g., in . Unfortunately, in this case, the
computational load should be expected to grow considerably.
Moreover, using the isotropic kernels is also advantageous, as
it allows further increasing the computational efﬁciency via
updating the kernel bandwidth through the process of isotropic
diffusion, as it is shown in Section IV. This scheme would not
be possible, if anisotropic kernels were used.
It is also interesting to point out the similarity between the
problem of image segmentation by means of active contours and
the problem of blind source separation [as a speciﬁc instance of
independent component analysis (ICA) ]. The latter is a reconstruction problem, in which a number of unknown source
signals have to be recovered from measurements of their algebraic mixtures. This problem has inspired the proposal of
numerous solutions, many of which are based on ﬁnding the
directions—independent components—in the multidimensional
space, along which the mixtures (or, better to say, the projections thereof) are as independent as possible. In this connection,
in order to access the above independency a number of information-based criteria, like the KL divergence, have been intensively used. Returning to the problem of segmentation, one can
think of a data image as a geometric mixture of sources, i.e., of
segmentation classes. In such a case, the active contour acts akin
an independent component when trying to separate the image
into a number of as independent regions as possible. It is quite
interesting to note that the method proposed in the present study
employs the concept and tools, which are very much similar to
those used in ICA.
Finally, we note that the proposed approach constitutes a natural generalization of the segmentation methods, which classify
the image samples based on a ﬁnite number of low-order empirical moments. Moreover, the simplicity of the proposed variational formulation allows one to accommodate and use an arbitrary number of diverse image features. Thus, for example,
in Section VI-C, images are segmented using both “static” and
“dynamic” features. It is also possible to extend the applicability of the method to segmenting the data deﬁned over nonlinear manifolds. This subject well deserves more ample treatment, which deﬁnes one of the directions of our future research.
ACKNOWLEDGMENT
The authors would like to thank all the anonymous reviewers
whose useful comments and suggestions have allowed the authors to substantially improve the quality of the present contribution.