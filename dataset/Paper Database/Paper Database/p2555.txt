Understanding Deep Image Representations by Inverting Them
Aravindh Mahendran
University of Oxford
Andrea Vedaldi
University of Oxford
Image representations, from SIFT and Bag of Visual
Words to Convolutional Neural Networks (CNNs), are a
crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual
information contained in representations by asking the following question: given an encoding of an image, to which
extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert
representations such as HOG and SIFT more accurately
than recent alternatives while being applicable to CNNs
too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the ﬁrst
time. Among our ﬁndings, we show that several layers in
CNNs retain photographically accurate information about
the image, with different degrees of geometric and photometric invariance.
1. Introduction
Most image understanding and computer vision methods
build on image representations such as textons , histogram of oriented gradients (SIFT and HOG ), bag
of visual words , sparse and local coding ,
super vector coding , VLAD , Fisher Vectors ,
and, lately, deep neural networks, particularly of the convolutional variety . However, despite the progress
in the development of visual representations, their design is
still driven empirically and a good understanding of their
properties is lacking. While this is true of shallower handcrafted features, it is even more so for the latest generation
of deep representations, where millions of parameters are
learned from data.
In this paper we conduct a direct analysis of representations by characterising the image information that they retain (Fig. 1). We do so by modeling a representation as a
function Φ(x) of the image x and then computing an approximated inverse φ−1, reconstructing x from the code
Φ(x). A common hypothesis is that representations collapse irrelevant differences in images (e.g. illumination or
Figure 1. What is encoded by a CNN? The ﬁgure shows ﬁve
possible reconstructions of the reference image obtained from the
1,000-dimensional code extracted at the penultimate layer of a reference CNN (before the softmax is applied) trained on the ImageNet data. From the viewpoint of the model, all these images are
practically equivalent. This image is best viewed in color/screen.
viewpoint), so that Φ should not be uniquely invertible.
Hence, we pose this as a reconstruction problem and ﬁnd
a number of possible reconstructions rather than a single
one. By doing so, we obtain insights into the invariances
captured by the representation.
Our contributions are as follows. First, we propose a
general method to invert representations, including SIFT,
HOG, and CNNs (Sect. 2). Crucially, this method uses only
information from the image representation and a generic
natural image prior, starting from random noise as initial
solution, and hence captures only the information contained
in the representation itself. We discuss and evaluate different regularization penalties as natural image priors. Second, we show that, despite its simplicity and generality, this
method recovers signiﬁcantly better reconstructions from
DSIFT and HOG compared to recent alternatives . As
we do so, we emphasise a number of subtle differences between these representations and their effect on invertibility.
Third, we apply the inversion technique to the analysis of
recent deep CNNs, exploring their invariance by sampling
possible approximate reconstructions. We relate this to the
depth of the representation, showing that the CNN gradually
builds an increasing amount of invariance, layer after layer.
Fourth, we study the locality of the information stored in
 
the representations by reconstructing images from selected
groups of neurons, either spatially or by channel.
The rest of the paper is organised as follows. Sect. 2 introduces the inversion method, posing this as a regularised
regression problem and proposing a number of image priors
to aid the reconstruction. Sect. 3 introduces various representations: HOG and DSIFT as examples of shallow representations, and state-of-the-art CNNs as an example of deep
representations. It also shows how HOG and DSIFT can be
implemented as CNNs, simplifying the computation of their
derivatives. Sect. 4 and 5 apply the inversion technique to
the analysis of respectively shallow (HOG and DSIFT) and
deep (CNNs) representations. Finally, Sect. 6 summarises
our ﬁndings.
We use the matconvnet toolbox for implementing
convolutional neural networks.
Related work. There is a signiﬁcant amount of work in understanding representations by means of visualisations. The
works most related to ours are Weinzaepfel et al. and
Vondrick et al. which invert sparse DSIFT and HOG
features respectively. While our goal is similar to theirs,
our method is substantially different from a technical viewpoint, being based on the direct solution of a regularised
regression problem. The beneﬁt is that our technique applies equally to shallow (SIFT, HOG) and deep (CNN) representations.
Compared to existing inversion techniques
for dense shallow representations , it is also shown to
achieve superior results, both quantitatively and qualitatively.
An interesting conclusion of is that, while HOG
and SIFT may not be exactly invertible, they capture a signiﬁcant amount of information about the image. This is in
apparent contradiction with the results of Tatu et al. 
who show that it is possible to make any two images
look nearly identical in SIFT space up to the injection of
adversarial noise. A symmetric effect was demonstrated
for CNNs by Szegedy et al. , where an imperceptible
amount of adversarial noise sufﬁces to change the predicted
class of an image. The apparent inconsistency is easily resolved, however, as the methods of require the injection of high-pass structured noise which is very unlikely
to occur in natural images.
Our work is also related to the DeConvNet method of
Zeiler and Fergus , who backtrack the network computations to identify which image patches are responsible
for certain neural activations. Simonyan et al. , however, demonstrated that DeConvNets can be interpreted as a
sensitivity analysis of the network input/output relation. A
consequence is that DeConvNets do not study the problem
of representation inversion in the sense adopted here, which
has signiﬁcant methodological consequences; for example,
DeConvNets require auxiliary information about the activations in several intermediate layers, while our inversion
uses only the ﬁnal image code. In other words, DeConvNets
look at how certain network outputs are obtained, whereas
we look for what information is preserved by the network
The problem of inverting representations, particularly
CNN-based ones, is related to the problem of inverting
neural networks, which received signiﬁcant attention in the
past. Algorithms similar to the back-propagation technique
developed here were proposed by , along
with alternative optimisation strategies based on sampling.
However, these methods did not use natural image priors as
we do, nor were applied to the current generation of deep
networks. Other works specialised on inverting
networks in the context of dynamical systems and will not
be discussed further here. Others proposed to learn a
second neural network to act as the inverse of the original
one, but this is complicated by the fact that the inverse is
usually not unique. Finally, auto-encoder architectures 
train networks together with their inverses as a form of supervision; here we are interested instead in visualising feedforward and discriminatively-trained CNNs now popular in
computer vision.
2. Inverting representations
This section introduces our method to compute an approximate inverse of an image representation. This is formulated as the problem of ﬁnding an image whose representation best matches the one given . Formally, given
a representation function Φ : RH×W ×C →Rd and a representation Φ0 = Φ(x0) to be inverted, reconstruction ﬁnds
the image x ∈RH×W ×C that minimizes the objective:
x∈RH×W ×C ℓ(Φ(x), Φ0) + λR(x)
where the loss ℓcompares the image representation Φ(x) to
the target one Φ0 and R : RH×W ×C →R is a regulariser
capturing a natural image prior.
Minimising (1) results in an image x∗that “resembles”
x0 from the viewpoint of the representation. While there
may be no unique solution to this problem, sampling the
space of possible reconstructions can be used to characterise the space of images that the representation deems to
be equivalent, revealing its invariances.
We next discusses the choice of loss and regularizer.
Loss function. There are many possible choices of the loss
function ℓ. While we use the Euclidean distance:
ℓ(Φ(x), Φ0) = ∥Φ(x) −Φ0∥2,
it is possible to change the nature of the loss entirely, for example to optimize selected neural responses. The latter was
used in to generate images representative of given
Regularisers.
Discriminatively-trained representations
may discard a signiﬁcant amount of low-level image statis-
tics as these are usually not interesting for high-level tasks.
As this information is nonetheless useful for visualization, it
can be partially recovered by restricting the inversion to the
subset of natural images X ⊂RH×W ×C. However, minimising over X requires addressing the challenge of modeling this set. As a proxy one can incorporate in the reconstruction an appropriate image prior. Here we experiment with two such priors. The ﬁrst one is simply the αnorm Rα(x) = ∥x∥α
α, where x is the vectorised and meansubtracted image. By choosing a relatively large exponent
(α = 6 is used in the experiments) the range of the image
is encouraged to stay within a target interval instead of diverging.
A second richer regulariser is total variation (TV)
RV β(x), encouraging images to consist of piece-wise constant patches. For continuous functions (or distributions)
f : RH×W ⊃Ω→R, the TV norm is given by:
where β = 1. Here images are discrete (x ∈RH×W ) and
the TV norm is replaced by the ﬁnite-difference approximation:
(xi,j+1 −xij)2 + (xi+1,j −xij)2 β
It was observed empirically that the TV regularizer (β = 1)
in the presence of subsampling, also caused by max pooling
in CNNs, leads to “spikes” in the reconstruction. This is a
known problem in TV-based image interpolation (see e.g.
Fig. 3 in ) and is illustrated in Fig. 2.left when inverting
a layer in a CNN. The “spikes” occur at the locations of
the samples because: (1) the TV norm along any path between two samples depends only on the overall amount of
intensity change (not on the sharpness of the changes) and
(2) integrated on the 2D image, it is optimal to concentrate
sharp changes around a boundary with a small perimeter.
Hyper-Laplacian priors with β < 1 are often used as a better
match of the gradient statistics of natural images , but
they only exacerbate this issue. Instead, we trade-off the
sharpness of the image with the removal of such artifacts
by choosing β > 1 which, by penalising large gradients,
distributes changes across regions rather than concentrating
them at a point or curve. We refer to this as the V β regularizer. As seen in Fig. 2 (right), the spikes are removed with
β = 2 but the image is washed out as edges are penalized
more than with β = 1.
When the target of the reconstruction is a colour image,
both regularisers are summed for each colour channel.
Balancing the different terms. Balancing loss and regulariser(s) requires some attention. While an optimal tuning
can be achieved by cross-validation, it is important to start
Figure 2. Left: Spikes in a inverse of norm1 features - detail
shown. Right: Spikes removed by a V β regularizer with β = 2.
from reasonable settings of the parameters. First, the loss is
replaced by the normalized version ∥Φ(x) −Φ0∥2
This ﬁxes its dynamic range, as after normalisation the loss
near the optimum can be expected to be contained in the
[0, 1) interval, touching zero at the optimum. In order to
make the dynamic range of the regulariser(s) comparable
one can aim for a solution x∗which has roughly unitary
Euclidean norm. While representations are largely insensitive to the scaling of the image range, this is not exactly true
for the ﬁrst few layers of CNNs, where biases are tuned to a
“natural” working range. This can be addressed by considering the objective ∥Φ(σx) −Φ0∥2
2 + R(x) where
the scaling σ is the average Euclidean norm of natural images in a training set.
Second, the multiplier λα of the α-norm regularizer
should be selected to encourage the reconstructed image
σx to be contained in a natural range [−B, B] (e.g. in
most CNN implementations B = 128).
If most pixels in σx have a magnitude similar to B, then Rα(x) ≈
HWBα/σα, and λα ≈σα/(HWBα). A similar argument suggests to pick the V β-norm regulariser coefﬁcient
as λV β ≈σβ/(HW(aB)β), where a is a small fraction
(e.g. a = 1%) relating the dynamic range of the image to
that of its gradient.
The ﬁnal form of the objective function is
∥Φ(σx) −Φ0∥2
2 + λαRα(x) + λV βRV β(x) (3)
It is in general non convex because of the nature of Φ. We
next discuss how to optimize it.
2.1. Optimisation
Finding an optimizer of the objective (1) may seem a
hopeless task as most representations Φ involve strong nonlinearities; in particular, deep representations are a chain
of several non-linear layers. Nevertheless, simple gradient
descent (GD) procedures have been shown to be very effective in learning such models from data, which is arguably
an even harder task. Hence, it is not unreasonable to use
GD to solve (1) too. We extend GD to incorporate a few extensions that proved useful in learning deep networks ,
as discussed below.
Momentum. GD is extended to use momentum:
µt+1 ←mµt −ηt∇E(x),
xt+1 ←xt + µt
where E(x) = ℓ(Φ(x), Φ0) + λR(x) is the objective function. The vector µt is a weighed average of the last several
gradients, with decaying factor m = 0.9. Learning proceeds a few hundred iterations with a ﬁxed learning rate ηt
and is reduced tenfold, until convergence.
Computing derivatives. Applying GD requires computing the derivatives of the loss function composed with the
representation Φ(x). While the squared Euclidean loss is
smooth, this is not the case for the representation. A key
feature of CNNs is the ability of computing the derivatives of each computational layer, composing the latter in
an overall derivative of the whole function using backpropagation. Our translation of HOG and DSIFT into CNN
allows us to apply the same technique to these computer
vision representations too.
3. Representations
This section describes the image representations studied in the paper: DSIFT (Dense-SIFT), HOG, and reference deep CNNs. Furthermore, it shows how to implement
DSIFT and HOG in a standard CNN framework in order to
compute their derivatives. Being able to compute derivatives is the only requirement imposed by the algorithm of
Sect. 2.1. Implementing DSIFT and HOG in a standard
CNN framework makes derivative computation convenient.
CNN-A: deep networks.
As a reference deep network
we consider the Caffe-Alex model (CNN-A), which
closely reproduces the network by Krizhevsky et al. .
This and many other similar networks alternate the following computational building blocks: linear convolution,
ReLU gating, spatial max-pooling, and group normalisation. Each such block takes as input a d-dimensional image
and produces as output a k-dimensional one. Blocks can
additionally pad the image (with zeros for the convolutional
blocks and with −∞for max pooling) or subsample the
data. The last several layers are deemed “fully connected”
as the support of the linear ﬁlters coincides with the size of
the image; however, they are equivalent to ﬁltering layers in
all other respects. Table 2 details the structure of CNN-A.
CNN-DSIFT and CNN-HOG. This section shows how
DSIFT and HOG can be implemented as CNNs.
This formalises the relation between CNNs and these standard representations.
It also makes derivative computation for these representations simple; for the inversion algorithm of Sect. 2. The DSIFT and HOG implementations
in the VLFeat library are used as numerical references.
These are equivalent to Lowe’s SIFT and the DPM
V5 HOG .
SIFT and HOG involve: computing and binning image
gradients, pooling binned gradients into cell histograms,
grouping cells into blocks, and normalising the blocks. Denote by g the gradient at a given pixel and consider binning
this into one of K orientations (where K = 8 for SIFT and
K = 18 for HOG). This can be obtained in two steps: directional ﬁltering and gating. The k-th directional ﬁlter is
Gk = u1kGx + u2kGy where
The output of a directional ﬁlter is the projection ⟨g, uk⟩of
the gradient along direction uk. A suitable gating function
implements binning into a histogram element hk. DSIFT
uses bilinear orientation binning, given by
hk = ∥g∥max
2π cos−1 ⟨g, uk⟩
whereas HOG (in the DPM V5 variant) uses hard assignments hk = ∥g∥1 [⟨g, uk⟩> ∥g∥cos π/K]. Filtering is
a standard CNN operation but these binning functions are
not. While their implementation is simple, an interesting
alternative is the approximated bilinear binning:
hk ≈∥g∥max
∝max {0, ⟨g, uk⟩−a∥g∥} ,
a = cos 2π/K.
The norm-dependent offset ∥g∥is still non-standard, but the
ReLU operator is, which shows to which extent approximate binning can be achieved in typical CNNs.
The next step is to pool the binned gradients into cell
histograms using bilinear spatial pooling, followed by extracting blocks of 2 × 2 (HOG) or 4 × 4 (SIFT) cells. Both
such operations can be implemented by banks of linear ﬁlters. Cell blocks are then l2 normalised, which is a special
case of the standard local response normalisation layer. For
HOG, blocks are further decomposed back into cells, which
requires another ﬁlter bank. Finally, the descriptor values
are clamped from above by applying y = min{x, 0.2} to
each component, which can be reduced to a combination of
linear and ReLU layers.
The conclusion is that approximations to DSIFT and
HOG can be implemented with conventional CNN components plus the non-conventional gradient norm offset. However, all the ﬁlters involved are much sparser and simpler
than the generic 3D ﬁlters in learned CNNs. Nonetheless,
in the rest of the paper we will use exact CNN equivalents of
DSIFT and HOG, using modiﬁed or additional CNN components as needed.
1 These CNNs are numerically indis-
1This requires addressing a few more subtleties. In DSIFT gradient
contributions are usually weighted by a Gaussian centered at each descriptor (a 4 × 4 cell block); here we use the VLFeat approximation (fast option) of weighting cells rather than gradients, which can be incorporated in
the block-forming ﬁlters. In UoCTTI HOG, cells contain both oriented and
unoriented gradients (27 components in total) as well as 4 texture components. The latter are ignored for simplicity, while the unoriented gradients
are obtained as average of the oriented ones in the block-forming ﬁlters.
descriptors
Table 1. Average reconstruction error of different representation
inversion methods, applied to HOG and DSIFT. HOGb denotes
HOG with bilinear orientation assignments. The standard deviation shown is the standard deviation of the error and not the standard deviation of the mean error.
Figure 4. Effect of V β regularization. The same inversion algorithm visualized in Fig. 3(d) is used with a smaller (λV β = 0.5),
comparable (λV β = 5.0), and larger (λV β = 50) regularisation
coefﬁcient.
tinguishable from the VLFeat reference implementations,
but, true to their CNN nature, allow computing the feature
derivatives as required by the algorithm of Sect. 2.
Next we apply the algorithm from Sect. 2 on CNN-A,
CNN-DSIFT and CNN-HOG to analyze our method.
4. Experiments with shallow representations
This section evaluates the representation inversion
method of Sect. 2 by applying it to HOG and DSIFT. The
analysis includes both a qualitative (Fig. 3) and quantitative
(Table 1) comparison with existing technique. The quantitative evaluation reports a normalized reconstruction error
∥Φ(x∗) −Φ(xi)∥2/NΦ averaged over 100 images xi from
the ILSVRC 2012 challenge validation data (images 1
to 100). A normalization is essential to place the Euclidean
distance in the context of the volume occupied by the features: if the features are close together, then even an Euclidean distance of 0.1 is very large, but if the features are
spread out, then even an Euclidean distance of 105 may be
very small. We use NΦ to be the average pairwise euclidean
distance between Φ(xi)’s across the 100 test images.
We ﬁx the parameters in equation 3 to λα = 2.16 × 108,
λV β = 5, and β = 2.
The closest alternative to our method is HOGgle, a technique introduced by Vondrick et al. for the visualisation of HOG features.
The HOGgle code is publicly
available from the authors’ website and is used throughout these experiments. Crucially, HOGgle is pre-trained to
invert the UoCTTI implementation of HOG, which is numerically equivalent to CNN-HOG (Sect. 3), allowing for a
direct comparison between algorithms.
Curiously, in UoCTTI HOG the l2 normalisation factor is computed considering only the unoriented gradient components in a block, but applied
to all, which requires modifying the normalization operator. Finally, when
blocks are decomposed back to cells, they are averaged rather than stacked
as in the original Dalal-Triggs HOG, which can be implemented in the
block-decomposition ﬁlters.
Figure 5. Test images for qualitative results.
Compared to our method, HOGgle is fast (2-3s vs 60s
on the same CPU) but not very accurate, as it is apparent
both qualitatively (Fig. 3.c vs d) and quantitatively (66%
vs 28% reconstruction error, see Table. 1). Interestingly,
 propose a direct optimisation method similar to (1),
but show that it does not perform better than HOGgle. This
demonstrates the importance of the choice of regulariser
and the ability of computing the derivative of the representation. The effect of the regularizer λV β is further analysed
in Fig. 4 (and later in Table 3): without this prior information, the reconstructions present a signiﬁcant amount of
discretization artifacts.
In terms of speed, an advantage of optimizing (1) is that
it can be switched to use GPU code immediately given the
underlying CNN framework; doing so results in a ten-fold
speedup. Furthermore the CNN-based implementation of
HOG and DSIFT wastes signiﬁcant resources using generic
ﬁltering code despite the particular nature of the ﬁlters in
these two representations.
Hence we expect that an optimized implementation could be several times faster than
It is also apparent that different representations can be
easier or harder to invert. In particular, modifying HOG
to use bilinear gradient orientation assignments as SIFT
(Sect. 3) signiﬁcantly reduces the reconstruction error (from
28% down to 11%) and improves the reconstruction quality
(Fig. 3.e). More impressive is DSIFT: it is quantitatively
similar to HOG with bilinear orientations, but produces signiﬁcantly more detailed images (Fig. 3.f). Since HOG uses
a ﬁner quantisation of the gradient compared to SIFT but
otherwise the same cell size and sampling, this result can
be imputed to the heavier block-normalisation of HOG that
evidently discards more image information than SIFT.
5. Experiments with deep representations
Figure 8. Effect of V β regularization on CNNs. Inversions of the
last layers of CNN-A for Fig. 5.d with a progressively larger regulariser λV β. This image is best viewed in color/screen.
This section evaluates the inversion method applied to
CNN-A described in Sect. 3.
Compared to CNN-HOG
(c) HOGgle 
(e) HOGb−1
(f) DSIFT−1
Figure 3. Reconstruction quality of different representation inversion methods, applied to HOG and DSIFT. HOGb denotes HOG with
bilinear orientation assignments. This image is best viewed on screen.
conv1 relu1 mpool1 norm1 conv2 relu2 mpool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 mpool5
4096 4096 4096 4096 1000
Table 2. CNN-A structure. The table speciﬁes the structure of CNN-A along with receptive ﬁeld size of each neuron. The ﬁlters in layers
from 16 to 20 operate as “fully connected”: given the standard image input size of 227 × 227 pixels, their support covers the whole image.
Note also that their receptive ﬁeld is larger than 227 pixels, but can be contained in the image domain due to padding.
and CNN-DSIFT, this network is signiﬁcantly larger and
deeper. It seems therefore that the inversion problem should
be considerably harder. Also, CNN-A is not handcrafted but
learned from 1.2M images of the ImageNet ILSVRC 2012
data .
The algorithm of Sect. 2.1 is used to invert the code obtained from each individual CNN layer for 100 ILSVRC
validation images (these were not used to train the CNN-A
model ). Similar to Sect. 4, the normalized inversion error is computed and reported in Table 3. The experiment is
repeated by ﬁxing λα to a ﬁxed value of 2.16×108 and gradually increasing λV β ten-folds, starting from a relatively
small value λ1 = 0.5. The ImageNet ILSVRC mean image is added back to the reconstruction before visualisation
as this is subtracted when training the network. Somewhat
surprisingly, the quantitative results show that CNNs are, in
fact, not much harder to invert than HOG. The error rarely
exceeds 20%, which is comparable to the accuracy of HOG
(Sect. 4). The last layer is in particular easy to invert with
an average error of 8.5%.
We choose the regularizer coefﬁcients for each representation/layer based on a quantitative and qualitative study
of the reconstruction. We pick λ1 = 0.5 for layers 1-6,
λ2 = 5.0 for layers 7-12 and λ3 = 50 for layers 13-20. The
error value corresponding to these parameters is marked in
bold face in table 3. Increasing λV β causes a deterioration
for the ﬁrst layers, but for the latter layers it helps recover a
more visually interpretable reconstruction. Though this parameter can be tuned by cross validation on the normalized
reconstruction error, a selection based on qualitative analysis is preferred because the method should yield images that
are visually meaningful.
Qualitatively, Fig. 6 illustrates the reconstruction for a
test image from each layer of CNN-A. The progression is
remarkable. The ﬁrst few layers are essentially an invertible code of the image. All the convolutional layers maintain a photographically faithful representation of the image,
although with increasing fuzziness. The 4,096-dimensional
fully connected layers are perhaps more interesting, as they
invert back to a composition of parts similar but not identical to the ones found in the original image. Going from
relu7 to fc8 reduces the dimensionality further to just 1,000;
nevertheless some of these visual elements can still be identiﬁed. Similar effects can be observed in the reconstructions
in Fig. 7. This ﬁgure includes also the reconstruction of an
abstract pattern, which is not included in any of the ImageNet classes; still, all CNN codes capture distinctive visual
features of the original pattern, clearly indicating that even
very deep layers capture visual information.
Next, Fig. 7 examines the invariance captured by the
CNN model by considering multiple reconstructions out of
each deep layer. A careful examination of these images re-
Figure 6. CNN reconstruction. Reconstruction of the image of Fig. 5.a from each layer of CNN-A. To generate these results, the regularization coefﬁcient for each layer is chosen to match the highlighted rows in table 3. This ﬁgure is best viewed in color/screen.
conv1 relu1 pool1 norm1 conv2 relu2 pool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 pool5 fc6 relu6 fc7 relu7 fc8
10.0 11.3 21.9 20.3 12.4 12.9 15.5
14.5 16.5 14.9 13.8 12.6 15.6 16.6 12.4 15.8 12.8 10.5 5.3
20.2 22.4 30.3
20.0 17.4 18.2 18.4 14.4 15.1 13.3 14.0 15.4 13.9 15.5 14.2 13.7 15.4 10.8 5.9
40.8 45.2 54.1
39.7 32.8 32.7
25.6 26.9 23.3 23.9 25.7 20.1 19.0 18.6 18.7 17.1 15.5 8.5
Table 3. Inversion error for CNN-A. Average inversion percentage error (normalized) for all the layers of CNN-A and various amounts
of V β regularisation: λ1 = 0.5, λ2 = 10λ1 and λ3 = 100λ1. In bold face are the error values corresponding to the regularizer that works
best both qualitatively and quantitatively. The deviations speciﬁed in this table are the standard deviations of the errors and not the standard
deviations of the mean error value.
Figure 7. CNN invariances. Multiple reconstructions of the images of Fig. 5.c–d from different deep codes obtained from CNN-A. This
ﬁgure is best seen in colour/screen.
veals that the codes capture progressively larger deformations of the object. In the “ﬂamingo” reconstruction, in particular, relu7 and fc8 invert back to multiple copies of the
object/parts at different positions and scales.
Note that all these and the original images are nearly indistinguishable from the viewpoint of the CNN model; it is
therefore interesting to note the lack of detail in the deepest reconstructions, showing that the network captures just a
sketch of the objects, which evidently sufﬁces for classiﬁcation. Considerably lowering the regulariser parameter still
yields very accurate inversions, but this time with barely any
resemblance to a natural image. This conﬁrms that CNNs
have strong non-natural confounders.
We now examine reconstructions obtained from subset
of neural responses in different CNN layers. Fig. 9 explores
the locality of the codes by reconstructing a central 5 × 5
patch of features in each layer. The regulariser encourages
portions of the image that do not contribute to the neural
Figure 9. CNN receptive ﬁeld. Reconstructions of the image of Fig. 5.a from the central 5 × 5 neuron ﬁelds at different depths of CNN-A.
The white box marks the ﬁeld of view of the 5 × 5 neuron ﬁeld. The ﬁeld of view is the entire image for conv5 and relu5.
conv1-grp1
norm1-grp1
norm2-grp1
conv1-grp1
norm1-grp1
norm2-grp1
conv1-grp2
norm1-grp2
norm2-grp2
conv1-grp2
norm1-grp2
norm2-grp2
Figure 10. CNN neural streams. Reconstructions of the images of Fig. 5.c-b from either of the two neural streams of CNN-A. This ﬁgure
is best seen in colour/screen.
responses to be switched off. The locality of the features is
obvious in the ﬁgure; what is less obvious is that the effective receptive ﬁeld of the neurons is in some cases signiﬁcantly smaller than the theoretical one - shown as a white
box in the image.
Finally, Fig. 10 reconstructs images from a subset of feature channels. CNN-A contains in fact two subsets of feature channels which are independent for the ﬁrst several layers (up to norm2) . Reconstructing from each subset
individually, clearly shows that one group is tuned towards
low-frequency colour information whereas the second one
is tuned to towards high-frequency luminance components.
Remarkably, this behaviour emerges naturally in the learned
network without any mechanism directly encouraging this
6. Summary
This paper proposed an optimisation method to invert
shallow and deep representations based on optimizing an
objective function with gradient descent. Compared to alternatives, a key difference is the use of image priors such as
the V β norm that can recover the low-level image statistics
removed by the representation. This tool performs better
Figure 11. Diversity in the CNN model. mpool5 reconstructions
show that the network retains rich information even at such deep
levels. This ﬁgure is best viewed in color/screen (zoom in).
than alternative reconstruction methods for HOG. Applied
to CNNs, the visualisations shed light on the information
represented at each layer. In particular, it is clear that a progressively more invariant and abstract notion of the image
content is formed in the network.
In the future, we shall experiment with more expressive natural image priors and analyze the effect of network
hyper-parameters on the reconstructions. We shall extract
subsets of neurons that encode object parts and try to establish sub-networks that capture different details of the image.