The Annals of Applied Probability
2004, Vol. 14, No. 1, 144–187
© Institute of Mathematical Statistics, 2004
STABILITY AND UNIFORM APPROXIMATION OF
NONLINEAR FILTERS USING THE HILBERT METRIC AND
APPLICATION TO PARTICLE FILTERS1
BY FRANÇOIS LE GLAND AND NADIA OUDJANE
IRISA/INRIA Rennes and EDF R&D Clamart
We study the stability of the optimal ﬁlter w.r.t. its initial condition and
w.r.t. the model for the hidden state and the observations in a general hidden
Markov model, using the Hilbert projective metric. These stability results are
then used to prove, under some mixing assumption, the uniform convergence
to the optimal ﬁlter of several particle ﬁlters, such as the interacting particle
ﬁlter and some other original particle ﬁlters.
1. Introduction.
The stability of the optimal ﬁlter has recently become an
active research area. Ocone and Pardoux proved that the ﬁlter forgets its initial
condition in the Lp sense, without stating any rate of convergence. Recently, a
new approach has been proposed using the Hilbert projective metric. This metric
allows getting rid of the normalization constant in the Bayes formula and reduces
the problem to studying the linear equation satisﬁed by the unnormalized optimal
ﬁlter. Using the Hilbert metric, stability results w.r.t. the initial condition have been
proved by Atar and Zeitouni , and some stability result w.r.t. the model have
been proved by Le Gland and Mevel , for hidden Markov models (HMM)
with ﬁnite state space. The results and methods of have been extended to
HMM with Polish state space by Atar and Zeitouni ; see also . Independently,
Del Moral and Guionnet have adopted in , for the same class of HMM, another
approach based on semigroup techniques and on the Dobrushin ergodic coefﬁcient,
to derive stability results w.r.t. the initial condition, which are used to prove
uniform convergence of the interacting particle system (IPS) approximation to
the optimal predictor. New approaches have been proposed recently, to prove the
stability of the optimal ﬁlter w.r.t. its initial condition, in the case of a noncompact
state space (see, e.g., ).
In this article, we use the approach based on the Hilbert metric to study the
asymptotic behavior of the optimal ﬁlter, and to prove as in the uniform
Received July 2001; revised January 2003.
1Supported in part by CNRS projects Méthodes Particulaires en Filtrage Non-Linéaire (97–
N23/0019, Modélisation et Simulation Numérique programme), Chaînes de Markov Cachées et
Filtrage Particulaire (MathSTIC programme) and Méthodes Particulaires (AS67, DSTIC Action
Spéciﬁque programme).
AMS 2000 subject classiﬁcations. Primary 93E11, 93E15, 62E25; secondary 60B10, 60J27,
62G07, 62G09, 62L10.
Key words and phrases. Hidden Markov model, nonlinear ﬁlter, particle ﬁlter, stability, Hilbert
metric, total variation norm, mixing, regularizing kernel.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
convergence of several particle ﬁlters, such as the interacting particle ﬁlter (IPF)
and some other original particle ﬁlters.
A common assumption to prove stability results (see, e.g., , Theorem 2.4)
is that the Markov transition kernels are mixing, which implies that the hidden
state sequence is ergodic. Our results are obtained under the assumption that the
nonnegative kernels describing the evolution of the unnormalized optimal ﬁlter,
and incorporating simultaneously the Markov transition kernels and the likelihood
functions, are mixing. This is a weaker assumption (see Proposition 3.9), which
allows considering some cases, similar to the case studied in , where the hidden
state sequence is not ergodic; see Example 3.10. This point of view is further
developped by Le Gland and Oudjane and by Oudjane and Rubenthaler .
Our main contribution is to study also the stability of the optimal ﬁlter w.r.t. the
model, when the local error is propagated by mixing kernels and can be estimated
in the Hilbert metric, in the total variation norm, or in a weaker distance suitable
for random probability distributions.
The uniform convergence of the IPS approximation to the optimal predictor is
proved in ( , Theorem 3.1), under the assumption that the likelihood functions
are uniformly bounded away from zero, which is rather strong, and that the
predictor is asymptotically stable. The rate (1/
N )α for some α < 1 is proved
under the stronger assumption that the predictor is exponentially asymptotically
stable, and the rate 1/
N is proved by Del Moral and Miclo ( , page 36) under
an additional assumption which is satisﬁed, for example, if the Markov kernels
are mixing. Our uniform convergence results are obtained under the assumption
that the expected values of the likelihood functions integrated against any possible
predicted probability distribution are bounded away from zero. This assumption is
automatically satisﬁed under our weaker mixing assumption; see Remark 5.7.
Motivated by practical considerations, we introduce a variant of the IPF, where
an adaptive number of particles is used, based on a posteriori estimates. The
resulting sequential particle ﬁlter (SPF) is shown to converge uniformly to the
optimal ﬁlter, independently of any lower bound assumption on the likelihood
functions. The counterpart is that the computational time is random and that the
expected number of particles does depend on the integrated lower bounds of the
likelihood functions. Also motivated by practical considerations, that is, to avoid
the degeneracy of particle weights and the degeneracy of particle locations, which
are two known causes of divergence of particle ﬁlters, we introduce regularized
particle ﬁlters (RPF), which are shown to converge uniformly to the optimal ﬁlter.
The paper is organized as follows: In the next section we deﬁne the framework
of the nonlinear ﬁltering problem and we introduce some notation. In Section 3, we
state some properties of the Hilbert metric, which are used in Section 4 to prove the
stability of the optimal ﬁlter w.r.t. its initial condition and w.r.t. the model. These
stability results are used to prove the uniform convergence of several particle ﬁlters
to the optimal ﬁlter. First, uniform convergence in the weak sense is proved in
F. LE GLAND AND N. OUDJANE
Section 5 for interacting particle ﬁlters, with a rate 1/
N, and sequential particle
ﬁlters with a random number of particles are also considered. Finally, regularized
particle ﬁlters are deﬁned in Section 6, for which uniform convergence in the weak
sense and in the total variation norm are proved.
2. Optimal ﬁlter for general HMM.
We consider the following model, with
a hidden (nonobserved) state sequence {Xn, n ≥0} and an observation sequence
{Yn, n ≥1}, taking values in a complete separable metric space E and in F = Rd,
respectively (in Section 6, it will be assumed that E = Rm).
• The state sequence {Xn, n ≥0} is deﬁned as an inhomogeneous Markov chain,
with transition probability kernel Qn; that is,
P[Xn ∈dx|X0: n−1 = x0: n−1] = P[Xn ∈dx|Xn−1 = xn−1] = Qn(xn−1,dx),
for all n ≥1, and with initial probability distribution µ0. For instance, {Xn,
n ≥0} could be deﬁned by the following equation:
Xn = fn(Xn−1,Wn),
where {Wn, n ≥1} is a sequence of independent random variables, not
necessarily Gaussian, independent of the initial state X0.
• The memoryless channel assumption holds; that is, given the state sequence
{Xn, n ≥0},
the observations {Yn, n ≥1} are independent random variables, and for all
n ≥1, the conditional probability distribution of Yn depends only on Xn.
For instance, the observation sequence {Yn, n ≥1} could be related to the state
sequence {Xn, n ≥0} by
Yn = hn(Xn,Vn)
for all n ≥1, where {Vn, n ≥1} is a sequence of independent random variables,
not necessarily Gaussian, independent of the state sequence {Xn, n ≥0}.
In addition, it is assumed that for all n ≥1, the collection of probability
distributions P[Yn ∈dy|Xn = x] on F , parametrized by x ∈E, is dominated,
P[Yn ∈dy|Xn = x] = gn(x,y)λF
for some nonnegative measure λF
n on F . The corresponding likelihood function
is deﬁned by n(x) = gn(x,Yn), and depends implicitly on the observation Yn.
The following notation and deﬁnitions will be used throughout the paper:
• The set of probability distributions on E and the set of ﬁnite nonnegative
measures on E are denoted by P (E) and M+(E), respectively.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
• The notation ∥· ∥is used for the total variation norm on the set of signed
measures on E and for the supremum norm on the set of bounded measurable
functions deﬁned on E, depending on the context.
• With any nonnegative kernel K deﬁned on E is associated a nonnegative linear
operator denoted by K and deﬁned by
E µ(dx)K(x,dx′)
for any nonnegative measure µ ∈M+(E).
• With any nonnegative measure µ ∈M+(E) is associated the normalized
nonnegative measure (i.e., the probability distribution),
if µ(E) > 0, that is, if µ is nonzero,
otherwise, that is, if µ ≡0,
where ν ∈P (E) is an arbitrary probability distribution.
• With any nonnegative kernel K deﬁned on E is associated the normalized
nonnegative nonlinear operator ¯K, taking values in P (E), and deﬁned for any
nonzero nonnegative measure µ ∈M+(E) by
if (Kµ)(E) > 0, that is, if Kµ is nonzero,
otherwise, that is, if Kµ ≡0,
where ν ∈P (E) is an arbitrary probability distribution. Notice that ¯K(µ) =
¯K( ¯µ) is nonzero by deﬁnition, hence composition of normalized nonnegative
nonlinear operators is well deﬁned.
The problem of nonlinear ﬁltering is to compute at each time n, the conditional
probability distribution µn of the state Xn given the observation sequence Y1: n =
(Y1,...,Yn) up to time n. The transition from µn−1 to µn is described by the
following diagram:
predictionµn|n−1 = Qnµn−1
correctionµn = n · µn|n−1 =
⟨µn|n−1,n⟩,
where “·” denotes the projective product. In general, no explicit expression is
available for the Markov kernel Qn, or it is so complicated that computing integrals
µn|n−1(dx′) = Qnµn−1(dx′) =
E µn−1(dx)Qn(x,dx′)
is practically impossible. Instead, throughout this paper we assume that for any
x ∈E, simulating a r.v. with probability distribution Qn(x,dx′) is easy [this is the
case, e.g., if (1) holds].
F. LE GLAND AND N. OUDJANE
REMARK 2.1.
Notice that the normalizing constant ⟨µn|n−1,n⟩is a.s.
positive; hence the projective product n · µn|n−1 is well deﬁned. Indeed,
P[Yn ∈dy|Y1: n−1] =
E P[Yn ∈dy|Xn = x]P[Xn ∈dx|Y1: n−1]
E gn(x,y)µn|n−1(dx)
n (dy) = ℓn(y)λF
⟨µn|n−1,n⟩=
E gn(x,Yn)µn|n−1(dx) = ℓn(Yn).
P[⟨µn|n−1,n⟩= 0|Y1: n−1] =
F 1{ℓn(y)=0}ℓn(y)λF
n (dy) = 0.
REMARK 2.2.
Notice also that, for any test function ψ deﬁned on F ,
⟨µn|n−1,n⟩
Y1: n−1
Y1: n−1
In particular, if ψ(y) = gn(x,y), then ψ(Yn) = n(x), and
⟨µn|n−1,n⟩
Y1: n−1
F gn(x,y)λF
n (dy) = 1,
for any x ∈E.
For any n ≥1, we introduce the nonnegative kernel
Rn(x,dx′) = Qn(x,dx′)n(x′)
and the associated nonnegative linear operator Rn = nQn on M+(E), deﬁned by
Rnµ(dx′) =
E µ(dx)Qn(x,dx′)n(x′)
for any µ ∈M+(E). Notice that Rn depends on the observation Yn through the
likelihood function n. With this deﬁnition, (Rnµn−1)(E) = ⟨µn|n−1,n⟩is a.s.
positive, and the evolution of the optimal ﬁlter can be written as follows:
µn = n · (Qnµn−1) =
(Rnµn−1)(E) = ¯Rn(µn−1),
and iteration yields
µn = ¯Rn(µn−1) = ¯Rn ◦··· ◦¯Rm(µm−1) = ¯Rn: m(µm−1).
Equation (3) shows clearly that the evolution of the optimal ﬁlter is nonlinear only
because of the normalization term coming from the Bayes rule. In the following
section a projective metric is introduced precisely to get rid of the normalization
and to come down to the analysis of a linear evolution.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
REMARK 2.3.
The model considered here is slightly different from the model
considered in other works (see and references therein), where it is assumed
that an observation Y0 is already available at time 0, and where the object of
study is rather the conditional probability distribution ηn of the state Xn given
the observation sequence Y0: n−1 = (Y0,...,Yn−1) up to time (n −1). With our
notation, the evolution of the optimal predictor in this alternate model can be
written as follows:
ηn+1 = Qn+1(n · ηn),
and iteration yields
ηn+1 = Qn+1 ¯Rn ◦··· ◦¯Rm(m−1 · ηm−1)
= Qn+1 ¯Rn: m(m−1 · ηm−1)
= Qn+1 ˆηn,
with initial condition η0 = µ0.
3. Hilbert metric on the set of ﬁnite nonnegative measures.
In this section
we recall the deﬁnition of the Hilbert metric and its associated contraction
coefﬁcient, the Birkhoff contraction coefﬁcient. We introduce also a mixing
property for nonnegative kernels, and we state some properties relating the Hilbert
metric with other distances on the set of probability distributions, for instance,
the total variation norm, or a weaker distance suitable for random probability
distributions. In the last part of the section, these deﬁnitions and properties are
specialized to the optimal ﬁltering context.
DEFINITION 3.1.
Two nonnegative measures µ,µ′ ∈M+(E) are comparable, if they are both nonzero and if there exist positive constants 0 < a ≤b, such
aµ′(A) ≤µ(A) ≤bµ′(A)
for any Borel subset A ⊂E.
DEFINITION 3.2.
The nonnegative kernel K deﬁned on E is mixing, if there
exist a constant 0 < ε ≤1 and a nonnegative measure λ ∈M+(E), such that
ελ(A) ≤K(x,A) ≤1
for any x ∈E, and any Borel subset A ⊂E.
DEFINITION 3.3.
The Hilbert metric on M+(E) is deﬁned by
h(µ,µ′) :=
log supA: µ′(A)>0 µ(A)/µ′(A)
infA: µ′(A)>0 µ(A)/µ′(A) ,
if µ and µ′ are comparable,
if µ = µ′ ≡0,
otherwise.
F. LE GLAND AND N. OUDJANE
Notice that the two nonnegative measures µ and µ′ are comparable if and only
if µ and µ′ are equivalent, with Radon–Nikodym derivatives dµ
dµ′ and dµ′
dµ bounded
and bounded away from zero, and then the following equality holds:
h(µ,µ′) = log
A: µ′(A)>0
Moreover, h is a projective distance; that is, it is invariant under multiplication by
positive scalars; hence the Hilbert distance between two unnormalized nonnegative
measures is the same as the Hilbert distance between the two corresponding
normalized measures: h(µ,µ′) = h( ¯µ, ¯µ′), for any nonzero µ,µ′ ∈M+(E).
In the nonlinear ﬁltering context, this property will allow us to consider the
linear transformation µ →Rnµ instead of the nonlinear transformation µ →
¯Rn(µ) = Rnµ/(Rnµ)(E). This projective property does not hold for other
distances. Indeed, the following estimates show how the error between two
unnormalized nonnegative measures can be used to bound the error between the
two corresponding normalized measures. If µ = µ′ ≡0, then ¯µ = ¯µ′ = ν, hence
¯µ −¯µ′ ≡0. If both µ and µ′ are nonzero, then
µ(E) −µ′(E)
|⟨¯µ −¯µ′,φ⟩| ≤|⟨µ −µ′,φ⟩|
+ |µ(E) −µ′(E)|
∥¯µ −¯µ′∥≤∥µ −µ′∥
+ |µ(E) −µ′(E)|
Finally, if µ is nonzero and µ′ ≡0, then
|⟨¯µ −¯µ′,φ⟩| ≤|⟨µ,φ⟩|
µ(E) + ∥φ∥
∥¯µ −¯µ′∥≤2,
that is, estimates (6) and (7) still hold [notice that the bounds in estimates
(6) and (7) do not depend on the restarting probability distribution ν].
The following two lemmas give several useful relations between the Hilbert
metric, the total variation norm and a weaker distance suitable for random
probability distributions.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
LEMMA 3.4.
For any µ,µ′ ∈M+(E),
∥¯µ −¯µ′∥≤
log3h(µ,µ′).
If the nonnegative kernel K deﬁned on E is mixing, then for any nonzero µ,µ′ ∈
h(Kµ,Kµ′) ≤1
ε2∥¯µ −¯µ′∥.
If µ = µ′ ≡0, then ¯µ = ¯µ′ = ν hence ∥¯µ −¯µ′∥= 0, while
h(µ,µ′) = 0 by deﬁnition. If µ is nonzero and µ′ ≡0, then h(µ,µ′) = ∞by
deﬁnition. Finally, if both µ and µ′ are nonzero, the proof of the ﬁrst inequality can
be found in . To prove the second inequality, notice ﬁrst that, for any comparable
µ,µ′ ∈M+(E),
h(µ,µ′) = log
A: µ′(A)>0
µ′(A) + log
A: µ′(A)>0
|µ(A) −µ′(A)|
|µ(A) −µ′(A)|
since log(1+x) ≤|x|. In order to apply this bound to h(Kµ,Kµ′) = h(K ¯µ,K ¯µ′),
we notice that Kµ and Kµ′ are comparable for any nonzero µ,µ′ ∈M+(E), since
K is mixing, and we introduce
(A) = (K ¯µ)(A) −(K ¯µ′)(A)
E( ¯µ −¯µ′)(dx)
E( ¯µ −¯µ′)+(dx)
E( ¯µ −¯µ′)−(dx)
(x,A) = K(x,A)
(K ¯µ)(A) ≤1
for any x ∈E and any Borel subset A ⊂E, using the mixing property. By the
Scheffé theorem,
E( ¯µ −¯µ′)+(dx) =
E( ¯µ −¯µ′)−(dx) = 1
2∥¯µ −¯µ′∥,
(A) is positive, then
E( ¯µ −¯µ′)+(dx)
2ε2 ∥¯µ −¯µ′∥,
F. LE GLAND AND N. OUDJANE
and similarly, if
(A) is negative, then
E( ¯µ −¯µ′)−(dx)
2ε2∥¯µ −¯µ′∥.
LEMMA 3.5.
If the nonnegative kernel K deﬁned on E is dominated, that is,
if there exist a constant c > 0 and a nonnegative measure λ ∈M+(E), such that
K(x,A) ≤cλ(A)
for any x ∈E and any Borel subset A ⊂E, then
E∥Kµ −Kµ′∥≤cλ(E)
E|⟨µ −µ′,φ⟩|
for any µ,µ′ ∈M+(E), possibly random.
REMARK 3.6.
If the nonnegative kernel K is mixing, then it is dominated,
with the same nonnegative measure λ ∈M+(E) and with c = 1/ε.
REMARK 3.7.
If in addition the nonnegative kernel K is F -measurable, then
the same estimate holds for conditional expectations w.r.t. F , that is,
E[∥Kµ −Kµ′∥|F ] ≤cλ(E)
E[|⟨µ −µ′,φ⟩||F ].
PROOF OF LEMMA 3.5.
By deﬁnition, if K is dominated, then K(x,·) is
absolutely continuous w.r.t. λ, with Radon–Nikodym derivative k(x,·) bounded
by c, for any x ∈E. Therefore, the total variation norm ∥Kµ −Kµ′∥can be
written as an integral as follows:
∥Kµ −Kµ′∥=
E(µ −µ′)(dx)k(x,x′)
λ(dx′),
hence, taking expectation yields
E∥Kµ −Kµ′∥=
E(µ −µ′)(dx)k(x,x′)
λ(dx′)
E|⟨µ −µ′,φ⟩|
LEMMA 3.8.
The nonnegative kernel K deﬁned on E is a contraction under
the Hilbert metric, and
0<h(µ,µ′)<∞
where the supremum in
H(K) := sup
µ,µ′ h(Kµ,Kµ′),
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
is over nonzero nonnegative measures: τ(K) is called the Birkhoff contraction
coefﬁcient.
The proof can be found in , Chapter XVI, Theorem 3, or in , Theorem 1.
Notice that H(K) < ∞implies τ(K) < 1.
Returning to the ﬁltering problem introduced in Section 2, stability results stated
in the following sections will in general require that for any n ≥1, the nonnegative
kernel Rn is mixing, that is, there exist a constant 0 < εn ≤1 and a nonnegative
measure λn ∈M+(E), such that
εnλn(A) ≤Rn(x,A) ≤1
for any x ∈E and any Borel subset A ⊂E. Notice that in full generality εn and λn
depend on the observation Yn, hence are random variables.
PROPOSITION 3.9.
The nonnegative kernel Rn deﬁned in (2) is a contraction
under the Hilbert metric, with Birkhoff contraction coefﬁcient τn = τ(Rn) ≤1.
(i) If Rn is mixing, with the possibly random constant εn, then
(ii) If the Markov transition kernel Qn is mixing, with the nonrandom
constant εn, then Rn is also mixing, with the same constant εn, without any
condition on the likelihood function n, and
τn ≤τ(Qn) ≤1 −ε2
Throughout the paper, for any integers m ≤n, the contraction coefﬁcient of
the product Rn: m = Rn ···Rm is denoted by τn: m = τ(Rn: m) ≤τn ···τm and by
convention τn: n+1 = τm−1: m = 1.
PROOF OF PROPOSITION 3.9.
It follows immediately from Lemma 3.8 that
Rn is a contraction under the Hilbert metric.
If Rn is mixing, then for any nonzero µ,µ′ ∈M+(E) and any Borel subset
≤εnλn(A) ≤Rnµ(A)
hence Rnµ and Rnµ′ are comparable. Using (5) yields
H(Rn) = sup
µ,µ′ h(Rnµ,Rnµ′) = sup
F. LE GLAND AND N. OUDJANE
where the supremum is taken over nonzero nonnegative measures. Then using
Lemma 3.8 yields
τn = τ(Rn) = tanh
which ends the proof of (i).
If Qn is mixing, then Rn = nQn is also mixing, since
A n(x′)λn(dx′) ≤Rn(x,A) ≤1
A n(x′)λn(dx′)
for any x ∈E and any Borel subset A ⊂E, hence for any nonzero µ,µ′ ∈M+(E),
Rnµ and Rnµ′ are comparable, with Radon–Nikodym derivative
d(Rnµ′)(x′) = d(Qnµ)
d(Qnµ′)(x′)1{n(x′)>0} ≤d(Qnµ)
d(Qnµ′)(x′)
for any x′ ∈E, and similarly with interchanging the role of µ and µ′. Therefore,
H(Rn) ≤sup
= H(Qn) ≤log 1
where the supremum is taken over nonzero nonnegative measures. Then using
Lemma 3.8 again yields τn = τ(Rn) ≤τ(Qn).
The assumption that the nonnegative kernel Rn is mixing is much weaker than
the usual assumption that both the Markov kernel Qn is mixing and the likelihood
function n is bounded away from zero. Indeed, if the Markov kernel Qn is
mixing, then it follows from (ii) that the nonnegative kernel Rn is mixing,
without any assumption on the likelihood function n: in particular, the likelihood
function could take the zero value, or even be compactly supported. This is not
a necessary condition, however, as illustrated by the example below, where the
Markov kernel Qn is not mixing, but the nonnegative kernel Rn is (equivalent, in
a sense to be deﬁned below, to) a mixing kernel.
EXAMPLE 3.10.
Assume that µ0 has compact support C0 ⊂E and that for
any n ≥1, the function n has compact support Cn ⊂E, and the transition
probability kernel Qn is deﬁned by
Qn(x,dx′) = (2π)−m/2 exp
2|x′ −fn(x)|2dx′ = qn(x,x′)λ(dx′),
where the function fn is continuous and where
λ(dx′) = (2π)−m/2 exp
2|x′|2dx′.
Clearly, the Markov kernel Qn is not mixing, but introducing
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
which are both ﬁnite a.s., it holds
 ≤qn(x,x′) ≤exp{
for any x ∈Cn−1 and any x′ ∈Cn. Deﬁne
Rn(x,dx′) = Qn(x,dx′)n(x′),
as usual, and
n(x,dx′) = 1{x∈Cn−1}Rn(x,dx′) + 1{x /∈Cn−1}n(x′)λ(dx′)
1{x∈Cn−1}qn(x,x′) + 1{x /∈Cn−1}
n(x′)λ(dx′).
Notice ﬁrst that the sequence {µn, n ≥0} deﬁned by (3) satisﬁes also
(R•nµn−1)(E).
Moreover, it follows from (12) that
A n(x′),λ(dx′)
n(x,A) ≤exp{
A n(x′)λ(dx′)
for any x ∈E and any Borel subset A ⊂E; that is, the nonnegative kernel R•
is mixing. Therefore, stability and approximation properties of the sequence
{µn, n ≥0} deﬁned by (3), can be obtained directly by studying (13) instead,
which involves mixing kernels.
4. Stability of nonlinear ﬁlters.
In practice one rarely has access to the initial
distribution of the hidden state process; hence it is important to study the stability
of the ﬁlter w.r.t. its initial condition. Moreover, the answer to this question will be
useful in studing the stability of the ﬁlter w.r.t. the model.
Let µn denote the ﬁlter initialized with the correct µ0, and let µ′
n denote the
ﬁlter initialized with a wrong µ′
0; that is, µn = ¯Rn: 1(µ0) and µ′
n = ¯Rn: 1(µ′
are interested in the total variation error at time n induced by the initial error.
THEOREM 4.1.
Without any assumption on the nonnegative kernels, the
following inequality holds:
log3τn: mh(µm−1,µ′
If in addition the nonnegative kernel Rm is mixing, then
log3τn: m+1
F. LE GLAND AND N. OUDJANE
COROLLARY 4.2.
If for any k ≥1, the nonnegative kernel Rk is mixing with
εk ≥ε > 0, then convergence holds uniformly in time, that is,
ε2 log3τ n−m∥µm−1 −µ′
with τ = 1 −ε2
1 + ε2 < 1.
PROOF OF THEOREM 4.1.
Using (8) and the deﬁnition (11) of the Birkhoff
contraction coefﬁcient yields
¯Rn: m(µ) −¯Rn: m(µ′)
log3h(Rn: mµ,Rn: mµ′) ≤
log3τn: mh(µ,µ′),
for any µ,µ′ ∈P (E). If the nonnegative kernel Rm is mixing, then using (9) yields
¯Rn: m(µ) −¯Rn: m(µ′)
log3h(Rn: m+1Rmµ,Rn: m+1Rmµ′)
log3τn: m+1h(Rmµ,Rmµ′)
log3τn: m+1
Taking µ = µm−1 and µ′ = µ′
m−1 completes the proof.
To solve the nonlinear ﬁltering problem, one must have a model to describe
the state/observation system, {Xn, n ≥0}, {Yn, n ≥1}, as presented in Section 2.
The general hidden Markov model is based on the initial condition µ0, on
the transition kernels Qn and on the likelihood functions n, which deﬁne the
evolution operator ¯Rn for the optimal ﬁlter µn. But, as for the initial condition, in
practice one has rarely access to the true model. In particular, the prior information
on the state sequence is in general unknown and the choice of Qn is approximative.
Similarly, the probabilistic relation between the observation and the state is in
general unknown and the choice of n is also approximative. As a result, instead
of using the true model, it is common to work with a wrong model, based on a
wrong transition kernel Q′
n and a wrong likelihood function ′
n, which deﬁne the
evolution operator ¯R′
n for a wrong ﬁlter µ′
Another situation is when the evolution operator ¯Rn is known, but difﬁcult
to compute. For the purpose of practical implementation, one constructs an
approximate ﬁlter µ′
n such that the evolution µ′
n is easy to compute and
close to the true evolution µ′
n−1 →¯Rn(µ′
We are interested in bounding the global error between µ′
n and µn induced
by the local errors committed at each time step. We suppose here that µ0 = µ′
since the problem of a wrong initialization has already been studied above. In
full generality, we assume that {µ′
n, n ≥0} is a random sequence with values
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
in P (E), satisfying the following property: for any n ≥k ≥1 and for any bounded
measurable function F deﬁned on P (E),
k)|Y1: n] = E[F(µ′
k)|Y1: k].
The results stated below are based on the following decomposition of the global
error into a sum of local errors transported by a sequence of normalized evolution
¯Rn: k+1(µ′
k) −¯Rn: k(µ′
¯Rn: k+1(µ′
k) −¯Rn: k+1 ◦¯Rk(µ′
This equation shows the close relation between the stability w.r.t. the initial
condition and the stability w.r.t. the model.
Let us consider ﬁrst the case where we can estimate the local error in the sense
of the Hilbert metric.
ASSUMPTION H (Local error bound in the Hilbert metric).
REMARK 4.3.
If the evolution of the wrong ﬁlter µ′
k is deﬁned by the
nonnegative kernel R′
k(x,dx′) = Q′
k(x,dx′)′
k(x′), and if
Qk(x,dx′) = qk(x,x′)λk(dx′)
k(x,dx′) = q′
k(x,x′)λk(dx′),
then a sufﬁcient condition for Assumption H to hold is that there exist constants
δk ≥0 and ak > 0, such that
ak ≤k(x′)qk(x,x′)
k(x,x′) ≤ak exp(δk)
for all x,x′ ∈E, in which case δH
THEOREM 4.4.
If for any k ≥1, Assumption H holds, then
n −µn∥|Y1: n] ≤
COROLLARY 4.5.
If for any k ≥1, the nonnegative kernel Rk is mixing
with εk ≥ε > 0, and Assumption H holds with δH
k ≤δ, then convergence holds
uniformly in time, that is,
n∥|Y1: n] ≤
F. LE GLAND AND N. OUDJANE
Indeed, (19) follows from
τ n−k = 1 −τ n
1 −τ = 1 + ε2
OF THEOREM 4.4.
Using the decomposition (17), the triangle
inequality and estimate (14), yields
¯Rn: k+1(µ′
k) −¯Rn: k+1 ◦¯Rk(µ′
Taking conditional expectation w.r.t. the observations and using (16), yields (18).
Let us consider next the case where we can estimate the local error in the sense
of the total variation norm
THEOREM 4.6.
If for any k ≥1, the nonnegative kernel Rk is mixing, then
n∥|Y1: n] ≤δTV
COROLLARY 4.7.
If for any k ≥1, the nonnegative kernel Rk is mixing, with
εk ≥ε > 0, and δTV
≤δ, then convergence holds uniformly in time, that is,
n∥|Y1: n] ≤
PROOF OF THEOREM 4.6.
The decomposition (17) is written as
¯Rn: k+1(µ′
k) −¯Rn: k+1 ◦¯Rk(µ′
hence using the triangle inequality and estimate (15) yields
¯Rn: k+1(µ′
k) −¯Rn: k+1 ◦¯Rk(µ′
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
Taking conditional expectation w.r.t. the observations and using (16), yields (20).
Let us consider ﬁnally the case where we can only estimate the local error in the
weak sense
Y1: k
This typically happens if the approximate ﬁlter µ′
k is an empirical probability
distribution associated with
k−1): in this case, bounding the local error
requires using the law of large numbers, which can only provide estimates in the
weak sense. However, if the nonnegative kernel Rk+1 is dominated, then using
Lemma 3.5, the local error transported by Rk+1 can be bounded in total variation
with the same precision δW
k as in the weak sense.
THEOREM 4.8.
If for any k ≥1, the nonnegative kernel Rk is mixing, then
E[|⟨µn −µ′
n,φ⟩||Y1: n]
COROLLARY 4.9.
If for any k ≥1, the nonnegative kernel Rk is mixing with
εk ≥ε > 0, and δW
k ≤δ, then convergence holds uniformly in time, that is,
E[|⟨µn −µ′
n,φ⟩||Y1: n] ≤
PROOF OF THEOREM 4.8.
Using the decomposition (21) and the triangle
inequality yields
¯Rn: k+1(µ′
k) −¯Rn: k+1 ◦¯Rk(µ′
For any 1 ≤k ≤n −2, using estimate (15) yields
¯Rn: k+1(µ′
k) −¯Rn: k+1 ◦¯Rk(µ′
¯Rn: k+2 ◦¯Rk+1(µ′
k) −¯Rn: k+2 ◦¯Rk+1 ◦¯Rk(µ′
log3τn: k+3
k) −¯Rk+1 ◦¯Rk(µ′
F. LE GLAND AND N. OUDJANE
For any 1 ≤k ≤n −1, using estimate (7) yields
k) −¯Rk+1 ◦¯Rk(µ′
≤2∥Rk+1(µ′
and the mixing property yields
k)(E) ≥εk+1λk+1(E).
Taking conditional expectation w.r.t. the observations, using estimate (10) with
K = Rk+1, µ = ¯Rk(µ′
k−1), µ′ = µ′
k and F = Y1: n and using (16), yields
Y1: n
Combining these estimates yields
k) −¯Rk+1 ◦¯Rk(µ′
Finally, taking conditional expectation w.r.t. the observations in (23) yields (22).
5. Uniform convergence of interacting particle ﬁlters.
In this section and
the next we consider again the framework introduced in Section 4, but now
the wrong model is chosen deliberately, such that the wrong ﬁlter can easily
be computed, and remains close to the optimal ﬁlter. More speciﬁcally, we are
interested in particle methods to approximate the optimal ﬁlter numerically and
we provide estimates of the approximation error. The idea common to all particle
ﬁlters is to generate an N-sample (ξ1
n|n−1,...,ξN
n|n−1) of i.i.d. random variables,
called a particle system, with common probability distribution QnµN
n−1, where
n−1 is an approximation of µn−1, and to use the corresponding empirical
probability distribution
as an approximation of µn|n−1 = Qnµn−1. The method is very easy to implement,
even in high-dimensional problems, since it is sufﬁcient in principle to simulate
independent samples of the hidden state sequence. A major and the earliest
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
contribution in this ﬁeld was made by Gordon, Salmond and Smith ,
who proposed to use sampling/importance resampling (SIR) techniques in the
correction step: the positive effect of the resampling step is to automatically
select particles with larger values of the likelihood function, that is, to concentrate
particles in regions of interest of the state space. A very complete account of
the currently available mathematical results can be found in . Theoretical and
practical aspects can be found in .
5.1. Notation and preliminary results.
Throughout the paper, SN(µ) is a
shorthand notation for the empirical probability distribution of an N-sample with
probability distribution µ, that is,
SN(µ) := 1
with (ξ1,...,ξN) i.i.d. ∼µ.
LEMMA 5.1.
For any µ ∈P (E),
E|⟨SN(µ) −µ,φ⟩| ≤
⟨SN(µ) −µ,φ⟩= 1
[φ(ξi) −⟨µ,φ⟩],
E|⟨SN(µ) −µ,φ⟩|2 = 1
⟨µ,φ2⟩−⟨µ,φ⟩2 ≤1
REMARK 5.2.
If in addition φ and µ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (ξ1,...,ξN) are i.i.d. with (conditional) probability
distribution µ, then the same estimate holds for conditional expectation w.r.t. F ,
|⟨SN(µ) −µ,φ⟩|
For any nonnegative and bounded measurable function  deﬁned on E and any
probability distribution µ deﬁned on E, the projective product  · µ is deﬁned by
if ⟨µ,⟩> 0,
otherwise,
F. LE GLAND AND N. OUDJANE
where ν is an arbitrary probability distribution deﬁned on E. If ⟨µ,⟩> 0, it
follows immediately from Lemma 5.1 and using estimate (6), that
E|⟨ · SN(µ) − · µ,φ⟩|
⟨SN(µ) −µ,φ⟩
supx∈E (x)
REMARK 5.3.
If in addition φ,  and µ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (ξ1,...,ξN) are i.i.d. with (conditional) probability
distribution µ, then the same estimate holds for conditional expectation w.r.t. F ,
|⟨ · SN(µ) − · µ,φ⟩|
supx∈E (x)
The following procedure, classical in sequential analysis, can be used alternatively.
LEMMA 5.4.
Let µ ∈P (E), and let  be a nonnegative bounded measurable
function deﬁned on E, such that ⟨µ,⟩> 0. For any δ > 0, deﬁne the stopping
(ξi) ≥sup
with (ξ1,...,ξN,...) i.i.d. ∼µ.
E|⟨ · ST (µ) − · µ,φ⟩| ≤2δ
To obtain an error estimate O(δ), the expected sample size should be O(1/δ2),
δ2 ≤E[T ] ≤ρ
δ2 (1 + δ2),
where ρ = (supx∈E (x))/⟨µ,⟩.
The method proposed here to approximate the posterior probability distribution  · µ is somehow intermediate, between the classical importance
sampling method, which uses a ﬁxed number of random variables, and the acceptance/rejection method, which requires a random number of random variables.
In Lemma 5.4, the number of random variables generated is random as in the acceptance/rejection method, but there is no rejection, since all the random variables
generated are explicitly used in the approximation, as in the importance sampling
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
PROOF OF LEMMA 5.4.
Notice ﬁrst that a.s.
⟨SN(µ),⟩= 1
(ξi) −→⟨µ,⟩> 0,
as N ↑∞, hence the stopping time T is a.s. ﬁnite. By deﬁnition, ⟨ST (µ),⟩> 0,
hence using estimate (6) yields
E|⟨ · ST (µ) − · µ,φ⟩| ≤2
E|⟨ST (µ) −µ,φ⟩|
⟨ST (µ),⟩
and we deﬁne
[(ξi)φ(ξi) −⟨µ,φ⟩]
By deﬁnition of the stopping time T , it holds
δ2 ≤DT = DT −1 + (ξT ) ≤λ
δ2 + λ = λ
δ2 (1 + δ2),
where λ = supx∈E (x), and the Cauchy–Schwarz inequality yields
In addition, for any a > 0,
P(T > N) ≤exp
E exp{−a(x)}µ(dx) < 1,
hence the stopping time T is integrable.
It follows from the Wald identity (see, e.g., , Proposition IV–4–21), that
δ2 ≤E[DT ] = E[T ]⟨µ,⟩≤λ
δ2 (1 + δ2)
T ] = E[T ]
⟨µ,2φ2⟩−⟨µ,φ⟩2
≤E[T ]⟨µ,⟩λ∥φ∥2 ≤λ2
δ2 (1 + δ2)∥φ∥2,
E|⟨ST (µ) −µ,φ⟩|
⟨ST (µ),⟩
δ2 ≤E[T ] ≤ρ
δ2(1 + δ2).
F. LE GLAND AND N. OUDJANE
REMARK 5.5.
If in addition φ,  and µ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (ξ1,...,ξN,...) are i.i.d. with (conditional)
probability distribution µ, then the same estimate holds for conditional expectation
w.r.t. F , that is,
|⟨ · ST (µ) − · µ,φ⟩|
δ2 ≤E[T |F ] ≤ρ
δ2(1 + δ2).
5.2. Interacting particle ﬁlter.
n denote the interacting particle ﬁlter
(IPF) approximation of µn. Initially µN
0 = µ0, and the transition from µN
is described by the following diagram:
prediction
n|n−1 = SN(QnµN
correctionµN
n = n · µN
In practice, the particle approximation
is completely characterized by the particle system (ξ1
n|n−1,...,ξN
n|n−1), and
the transition from (ξ1
n|n−1,...,ξN
n|n−1) to (ξ1
n+1|n,...,ξN
n+1|n) consists of the
following steps:
Step (i) (Correction). If the normalization constant
is positive, then for all i = 1,...,N, compute the weight
otherwise set µN
Step (ii) (Sampled prediction). Independently for all i = 1,...,N, generate a
n+1|n ∼Qn+1µN
n , and set
n+1|n = SN(Qn+1µN
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
The resampling step (ii) can be easily implemented: it requires generating
random variables either according to a weighted discrete probability distribution
or according to the arbitrary restarting probability distribution ν.
Notice that the IPF satisﬁes (16).
REMARK 5.6.
Without the reinitialization procedure, proposed initially by
Del Moral, Jacod and Protter , the normalization constant cn could take zero
value, since the likelihood function n is not necessarily positive, and µN
n|n−1 would not be a well-deﬁned probability distribution. By construction,
the sequential particle ﬁlter deﬁned at the end of this section does not run into this
REMARK 5.7.
If the nonnegative kernel Rn is mixing, then
µ∈P(E)⟨Qnµ,n⟩=
µ∈P(E)(Rnµ)(E) ≥ε2
n(Rnµn−1)(E) = ε2
n⟨µn|n−1,n⟩,
hence a.s.
µ∈P(E)⟨Qnµ,n⟩> 0,
in view of Remark 2.1.
Without loss of generality, it is assumed that the likelihood function is bounded.
ASSUMPTION L.
k(x) < ∞.
If Assumption L holds, and if for any k ≥1, the nonnegative kernel Rk is
mixing, then the following notation is introduced:
supx∈E k(x)
infµ∈P(E)⟨Qkµ,k⟩,
and in view of Remark 5.7, ρk is a.s. ﬁnite.
THEOREM 5.8.
If for any k ≥1, Assumption L holds, and the nonnegative
kernel Rk is mixing, then the IPF estimator satisﬁes
where for any k ≥1,
F. LE GLAND AND N. OUDJANE
The convergence result stated in Theorem 5.8 would still hold with a time
dependent number of particles.
REMARK 5.9.
If the transition kernel Qn+1 is dominated, that is, Qn+1(x,·)
is absolutely continuous w.r.t. λn+1 ∈M+(E), with density qn+1(x,·) bounded
by cn+1 for any x ∈E, then convergence in the weak sense of the particle ﬁlter
can be used to prove convergence in total variation of the particle predictor. Indeed,
using Lemma 3.5 yields
∥µn+1|n −Qn+1µN
 ≤cn+1λn+1(E)
where both µn+1|n and Qn+1µN
n are absolutely continuous w.r.t. λn+1, and
for any x′ ∈E, which can be easily computed.
REMARK 5.10.
In general, it is not realistic to assume that the r.v.s ρk are
uniformly bounded; hence it seems difﬁcult to guarantee that convergence holds
uniformly in time for a given observation sequence. On the other hand, averaging
over observation sequences makes it possible to obtain convergence uniformly in
time, under more realistic assumptions. Indeed, if for any k ≥1, the nonnegative
kernel Rk is mixing with nonrandom εk, and E[ρk] is ﬁnite, then
n ,φ⟩| ≤δn + 2δn−1
where for any k ≥1,
REMARK 5.11.
Notice that, if the nonnegative kernel Rk is mixing, then
supx∈E k(x)
⟨µk|k−1,k⟩≤ρk ≤supx∈E k(x)
k⟨µk|k−1,k⟩,
and it follows from Remark 2.2 that
supx∈E k(x)
⟨µk|k−1,k⟩
Y1: k−1
hence a necessary and sufﬁcient condition for E[ρk] to be ﬁnite is
k (dy) < ∞.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
COROLLARY 5.12.
If for any k ≥1, the nonnegative kernel Rk is mixing with
εk ≥ε > 0 and nonrandom ε, and E[ρk] ≤ρ, then convergence, averaged over
observation sequences, holds uniformly in time, that is,
PROOF OF THEOREM 5.8.
It is sufﬁcient to bound the local error δW
weak sense and to apply Theorem 4.8. Since Rk is mixing, ⟨QkµN
k−1,k⟩> 0,
in view of Remark 5.7. Using estimate (25) with  = k, µ = QkµN
F = σ(Y1: k, µN
k−1) yields
Y1: k, µN
 −k · (QkµN
Y1: k, µN
supx∈E k(x)|
k−1,k⟩∥φ∥≤
REMARK 5.13.
n denote the interacting particle system (IPS) approximation of ηn considered in and in other works by the same authors. Initially
0 = SN(η0), and the transition from ηN
n is described by the following
correction ˆηN
n−1 = n−1 · ηN
prediction
n = SN(Qn ˆηN
Clearly ˆηN
0 = 0 · ηN
0 = 0 · SN(η0), and the transition from ˆηN
n−1 to ˆηN
described by the following diagram:
prediction
n = SN(Qn ˆηN
correction ˆηN
n = n · ηN
which involves exactly the same steps as the transition from µN
n described
above; only the initial conditions ˆηN
0 = 0 · SN(η0) and µN
0 = µ0 are different.
Using the following decomposition of the global error into an initial error and a
sum of local errors transported by a sequence of normalized evolution operators:
¯Rn: k+1(ˆηN
k ) −¯Rn: k(ˆηN
¯Rn: 1(ˆηN
0 ) −¯Rn: 1(ˆη0)
and proceeding as in the proof of Theorem 4.8, yields under the assumptions of
F. LE GLAND AND N. OUDJANE
Theorem 5.8,
|⟨ˆηn −ˆηN
where for any k ≥0,
ρ0 := supx∈E 0(x)
Finally, notice that
n+1 = Qn+1 ˆηN
n −SN(Qn+1 ˆηN
n ) + Qn+1(ˆηn −ˆηN
|⟨ηn+1 −ηN
|⟨ˆηn −ˆηN
This proves the uniform convergence of the IPS approximation to the optimal
predictor, with rate 1/
N, for exactly the model considered in , under our
weaker mixing assumption.
In the proof of Theorem 5.8, if we use ⟨SN(QkµN
k−1),k⟩instead of
k−1,k⟩as the denominator in (27), we see that, for the local error to be
small, the empirical mean of the likelihood function over the predicted particle
system should be large enough. This theoretical argument is also supported by
numerical evidence, in cases where the likelihood function is localized in a small
region of the state space (which typically arises when measurements are accurate).
Indeed, such a region can be so small that it does not contain enough points of
the predicted particle system, which automatically results in a small value of the
predicted empirical mean of the likelihood function. This phenomenon is called
degeneracy of particle weights and is a known cause of divergence of particle
ﬁlters. To solve this degeneracy problem, one idea is to add a regularization step
to the algorithm; the resulting ﬁlters, called regularized particle ﬁlters (RPF), are
studied in the next section. Another idea is to control the predicted empirical mean
k−1),k⟩= 1
by using an adaptive number of particles. To guarantee a local error of order δk,
independently of any lower bound assumption on the likelihood function, we
choose a random number of particles,
k|k−1) ≥sup
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
that will automatically ﬁt the difﬁcult case of localized likelihood functions: the
resulting ﬁlter, called sequential particle ﬁlter (SPF) is studied below.
5.3. Sequential particle ﬁlter.
denote the sequential particle ﬁlter
(SPF) approximation of µn. Initially µN0
= µ0, and the transition from µNn−1
is described by the following diagram:
sequential
prediction
n|n−1 = SNn
correctionµNn
n = n · µNn
In practice, the particle approximation
is completely characterized by the particle system (ξ1
n|n−1,...,ξNn
n|n−1), and
the transition from (ξ1
n|n−1,...,ξNn
n|n−1) to (ξ1
n+1|n,...,ξNn+1
n+1|n) consists of the
following steps:
Step (i) (Correction). For all i = 1,...,Nn, compute the weight
with the normalization constant
n = n · µNn
Step (ii) (Sequential sampled prediction). Independently for all i = 1,...,Nn+1,
generate a r.v. ξi
n+1|n ∼Qn+1µNn
n , where the random number Nn+1 of particles is
deﬁned by the stopping time
Nn+1 = inf
n+1|n) ≥sup
n+1|n = SNn+1
F. LE GLAND AND N. OUDJANE
Exactly as for the IPF, the resampling step (ii) can be easily implemented; it only
requires generating random variables according to a weighted discrete probability
distribution. Notice that a.s.
as N ↑∞, and if the nonnegative kernel Rn is mixing, then ⟨QnµNn−1
n−1 ,n⟩> 0
in view of Remark 5.7, hence the stopping time Nn is a.s. ﬁnite. Moreover, the
normalization constant cn is positive, since
n(x) > 0,
hence n · µNn
n|n−1 is a well-deﬁned probability distribution.
Notice that the SPF satisﬁes (16). The following theorem shows that using a
random number of particles allows controling the local error independently of any
lower bound assumption on the likelihood functions. The counterpart is that the
computational time of the resulting algorithm is random and that the expected
number of particles does depend on the integrated lower bounds of the likelihood
functions.
THEOREM 5.14.
If for any k ≥1, the nonnegative kernel Rk is mixing and the
random number Nk of particles is deﬁned as in (28), then the following inequality
µn −µNn
Y1: n
where for any k ≥1,
≤E[Nk|Y1: k] ≤ρk
COROLLARY 5.15.
If for any k ≥1, the nonnegative kernel Rk is mixing with
εk ≥ε > 0 and the random number Nk of particles is deﬁned as in (28) with δk ≤δ,
then convergence holds uniformly in time, that is,
µn −µNn
Y1: n
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
PROOF OF THEOREM 5.14.
It is sufﬁcient to bound the local error δW
weak sense, and to apply Theorem 4.8. Since Rk is mixing, ⟨QkµNk−1
k−1 ,k⟩> 0
in view of Remark 5.7. Using estimate (26) with  = k, µ = QkµNk−1
F = σ(Y1: k, µNk−1
k−1 ) yields
Y1: k, µNk−1
k · SNk
Y1: k, µNk−1
In this section, we have proved that the IPF and its sequential variant converge
uniformly in time under the mixing assumption. This theoretical argument is also
supported by numerical evidence, for example, in extreme cases where the hidden
state sequence satisﬁes a noise-free state equation. Indeed, because multiple copies
are produced after each resampling step, the diversity of the particle system can
only decrease along the time in such cases, and the particle system ultimately
concentrates on a few points, if not a single point, of the state space. This
phenomenon is called degeneracy of particle locations and is another known
cause of divergence of particle ﬁlters. To solve this degeneracy problem, and
also the problem of degeneracy of particle weights already mentionned, we have
proposed in to add a regularization step in the algorithm, so as to guarantee
the diversity of the particle system along the time: the resulting ﬁlters, called
regularized particle ﬁlters (RPF), are studied in the next section under the same
mixing assumption.
6. Uniform convergence of regularized particle ﬁlters.
The main idea
consists in changing the discrete approximation µN
n for an absolutely continuous
approximation, with the effect that in the resampling step N random variables are
generated according to an absolutely continuous distribution, hence producing a
new particle system with N different particle locations. In doing this, we implicitly
assume that the hidden state sequence takes values in a Euclidean space E = Rm
and that the optimal ﬁlter µn has a smooth density w.r.t. the Lebesgue measure,
which is the case in most applications. From the theoretical point of view, this
additional assumption allows obtaining strong approximations of the optimal ﬁlter,
in total variation or in the Lp sense for any p ≥1. In practice, this provides
approximate ﬁlters which are much more stable along the time than the IPF.
Obtaining an absolutely continuous approximation is achieved by adding a
regularization step in the algorithm, using a kernel method, classical in density
estimation. If the regularization occurs before the correction by the likelihood
function, we obtain the preregularized particle ﬁlter, the numerical analysis of
which was done in , in the general case without the mixing assumption. An
F. LE GLAND AND N. OUDJANE
improved version of the preregularized particle ﬁlter, called the kernel ﬁlter (KF),
is proposed in . If the regularization occurs after the correction by the
likelihood function, we obtain the postregularized particle ﬁlter, which has been
proposed in and and compared with the IPF in some classical tracking
problems, such as bearing only tracking, or range and bearing tracking with a
multiple dynamical model. The local rejection regularized particle ﬁlter (L2RPF),
which generalizes both the KF and the post-RPF, is introduced in , where
further implementation details and applications to tracking problems can be found.
6.1. Notation and preliminary results.
The following notation and deﬁnitions
will be used below. Throughout the end of this paper, E = Rm. For any µ ∈P (E),
E |x|m+1µ(dx)
and if µ is absolutely continuous w.r.t. the Lebesgue measure on E, with density
dx , deﬁne
I (f ) = I (µ) =
E |x|m+1f (x)dx
From the multidimensional Carlson inequality (see , Lemma 7) there exists a
universal constant Am such that, for any absolutely continuous µ ∈P (E),
≤Am(I (µ))m/2.
Hence J(dµ
dx ) is ﬁnite if I (µ) is ﬁnite. Let W 2,1 denote the Sobolev space of
functions deﬁned on E, which together with their derivatives up to order two,
are integrable w.r.t. the Lebesgue measure on E. Let ∥· ∥2,1 and | · |2,1 denote the
corresponding norm and seminorm, that is,
E |Diu(x)|dx
E |Diu(x)|dx,
respectively, where for any multiindex i = (i1,...,im) of order |i| = i1 + ··· + im,
∂i1+···+im
1 ··· ∂xim
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
Let the regularization kernel K be a symmetric probability density on E, such
E K(x)dx = 1,
E xK(x)dx = 0
E |x|2K(x)dx < ∞.
Assume also that the regularization kernel K is square integrable, that is,
and that the symmetric probability density L = K2
β2 satisﬁes
γ := I (L) =
E |x|m+1L(x)dx
For any bandwidth h > 0, deﬁne the rescaled kernel
for any x ∈E.
DEFINITION 6.1.
For any µ ∈M+(E), the nonnegative measure Kh ∗µ is
deﬁned as the measure absolutely continuous w.r.t. the Lebesgue measure, with
E Kh(x −x′)µ(dx′),
where ∗denotes the convolution operator.
Notice that convolution by Kh preserves the total mass, that is, (Kh ∗µ)(E) =
µ(E) for any µ ∈M+(E), hence Kh ∗¯µ is the normalized nonnegative measure
(i.e., probability distribution) associated with the unnormalized nonnegative
measure Kh ∗µ. Moreover, Kh ∗µ approximates µ in the following sense.
LEMMA 6.2.
Let µ ∈M+(E) be absolutely continuous w.r.t. the Lebesgue
measure, with density dµ
dx ∈W 2,1. Then
∥Kh ∗µ −µ∥≤αh2
The proof for the one-dimensional case can be found in , Theorem 7.1,
and for the multidimensional case in , Chapter I, Lemma 4.4, or in ,
Proposition 4.
Given a sample (ξ1,...,ξN) from an unknown probability distribution
µ ∈P (E) with a smooth density, and given a nonnegative function  which can
F. LE GLAND AND N. OUDJANE
be evaluated at any point of E, we are interested in approximating the posterior
probability distribution  · µ by a probability distribution with a smooth density.
By construction, the approximation error can be estimated in a strong sense such
as the total variation. Of course, more sample points are needed to approximate
a whole density than are needed to simply approximate moments, as in the weak
sense approximation. Typically, the sample size depends on the dimension, which
is the usual curse of dimensionality. However, getting an approximation of the
whole density is usually worth the effort, as it allows getting meaningful information, for example, conﬁdence regions.
The classical density estimation theory (see, e.g., for the L2 theory and 
for the L1 theory), has extensively dealt with the problem of estimating µ alone,
which reduces to our problem in the particular case where  is constant. The
solution consists in regularizing the empirical probability distribution associated
with the sample and provides kernel-type estimators Kh ∗SN(µ). Minimization
of the mean errors E∥Kh ∗SN(µ) −µ∥or E∥Kh ∗SN(µ) −µ∥2
2, in the L1 or
L2 sense, over the bandwidth h and the regularization kernel K has also been
studied. Similarly, in our more general setting, we propose two estimators for
 · µ: the preregularized estimator  · (Kh ∗SN(µ)) where regularization occurs
before the correction by , and the postregularized estimator Kh ∗( · SN(µ))
where regularization occurs after the correction by . We immediately see that the
preregularized estimator consists in applying the correction by  to the classical
density estimator Kh ∗SN(µ) and that both estimators reduce to the classical
density estimator when  is constant. Consequently, we will focus below on
estimating the mean error for the postregularized estimator, and results for the
preregularized estimator will follow immediately. In Proposition 6.3, we consider
the mean error between the unnormalized nonnegative measures Kh ∗(SN(µ))
and µ. In the general case, the error between the corresponding probability
distributions Kh ∗( · SN(µ)) and  · µ will then be derived using estimate (7),
but in some particular cases we may derive some sharper bounds. In what follows,
we only state some bounds without trying to optimize over the bandwidth h or the
regularization kernel K.
PROPOSITION 6.3.
Let µ ∈P (E) be absolutely continuous w.r.t. the
Lebesgue measure, with density dµ
dx ∈W 2,1, and let  be a nonnegative bounded
measurable function deﬁned on E, with bounded derivatives up to order two. Then
(x) + αh2
If in addition I (µ) is ﬁnite, then
Nhm (I (µ) + hγ )m/2 sup
(x) + αh2
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
The proof is based on the following decomposition of the error into variation
and bias errors:
 −µ = Kh ∗
 −Kh ∗(µ) + Kh ∗(µ) −µ.
Under the assumptions, the nonnegative measure µ is absolutely continuous
w.r.t. the Lebesgue measure, with density dµ
dx ∈W 2,1, such that
and Lemma 6.2 can be used to bound the bias error. The following lemma is used
to bound the variation error.
LEMMA 6.4.
Let µ ∈P (E), and let  be a nonnegative bounded measurable
function deﬁned on E. Then
 −Kh ∗(µ),φ
If in addition I (µ) is ﬁnite, then
 −Kh ∗(µ)
I (µ) + hγ
Using Lemma 5.1 and the symmetry of the regularization kernel,
 −Kh ∗(µ),φ
SN(µ) −µ,(Kh ∗φ)
∥(Kh ∗φ)∥≤
for any bounded measurable test function φ deﬁned on E, which proves the
estimate in the weak sense.
The proof of the estimate in total variation is classical. By deﬁnition,
 −Kh ∗(µ)
h (x) −fh(x)|dx,
h (x) = d(Kh ∗(SN(µ)))
Kh(x −ξi)(ξi),
fh(x) = d(Kh ∗(µ))
E Kh(x −x′)(x′)µ(dx′),
F. LE GLAND AND N. OUDJANE
and it follows from the proof of Lemma 5.1 that
h (x) −fh(x)| ≤
h(x −x′)2(x′)µ(dx′)
E Lh(x −x′)µ(dx′)
where the symmetric probability density L = K2
β2 satisﬁes K2
hmLh. Therefore
 −Kh ∗(µ)
Using the Minkowski inequality yields
I (Lh ∗µ) =
E |x′ + hu|m+1L(u)µ(dx′)du
E |x′|m+1µ(dx′)
E |u|m+1L(u)du
≤I (µ) + hI (L),
and using estimate (29) yields
≤Am(I (Lh ∗µ))m/2 ≤Am
I (µ) + hγ
REMARK 6.5.
If the probability distribution µ is absolutely continuous w.r.t.
the Lebesgue measure, with probability density f , then the following more precise
bound holds:
= J(Lh ∗f ) ≤J(f ) + J(|Lh ∗f −f |),
where J(|Lh ∗f −f |) goes to zero when h ↓0 (see , Proposition 8).
REMARK 6.6.
If in addition φ,  and µ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (ξ1,...,ξN) are i.i.d. with (conditional) probability
distribution µ, then the same estimates hold for conditional expectation w.r.t. F ,
(x) + αh2
I (µ) + hγ
(x) + αh2
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
6.2. Preregularized particle ﬁlter.
denote the preregularized particle
ﬁlter (pre-RPF) approximation of µn. Initially µN,h
= µ0, and the transition from
n−1 to µN,h
is described by the following diagram:
prediction
n|n−1 = SN(QnµN,h
preregularized
correction
= n · (Kh ∗µN,h
In practice, the particle approximation
is completely characterized by the particle system {ξ1
n|n−1,...,ξN
n|n−1}, and
the transition from {ξ1
n|n−1,...,ξN
n|n−1} to {ξ1
n+1|n,...,ξN
n+1|n} consists of the
following steps.
Step (i) (Preregularized correction). If the normalization constant
(Kh ∗n)(ξi
is positive, then set
n · (Kh ∗µN,h
n(x)Kh(x −ξi
otherwise set µN,h
Step (ii) (Sampled prediction). Independently for all i = 1,...,N, generate
n+1|n ∼Qn+1 µN,h
n+1|n = SN(Qn+1µN,h
The resampling step (ii) requires generating random variables either according
to the arbitrary restarting probability distribution ν, or according to a complex
probability distribution known up to a normalization constant, which can be done
with a rejection algorithm (see ), or with the more efﬁcient local rejection
algorithm (see ), if the kernel K has compact support. In any case, the
implementation is less straightforward than for the IPF.
Notice that the pre-RPF satisﬁes (16).
ASSUMPTION R (Regularity of the Markov kernel).
For any µ ∈P (E), the
probability distribution Qkµ is absolutely continuous w.r.t. the Lebesgue measure,
with density in W 2,1, and
F. LE GLAND AND N. OUDJANE
REMARK 6.7.
Assumption R is equivalent to supposing that the probability
distribution Qk(x,·) is absolutely continuous w.r.t. the Lebesgue measure, with
density qk(x,·) in W 2,1 for any x ∈E, and
|qk(x,·)|2,1 < ∞.
ASSUMPTION M (Existence of moments).
I (Qkµ) < ∞.
REMARK 6.8.
Assumption M is equivalent to supposing that
E |x′|m+1Qk(x,dx′)
Alternatively, if the Markov kernel Qk is mixing, and I (µk|k−1) is ﬁnite, then
I (µk|k−1) < ∞.
THEOREM 6.9.
If for any k ≥1, Assumptions L and R hold, and the
nonnegative kernel Rk is mixing, then the pre-RPF estimator satisﬁes
|⟨µn −µN,h
where for any k ≥1,
If in addition for any k ≥1, Assumption M holds, then
where for any k ≥1,
Nhm (Ik + hγ )m/2 + αh2Dk
The convergence result stated in Theorem 6.9 would still hold with a time
dependent bandwidth, and with a time dependent number of particles.
PROOF OF THEOREM 6.9.
To prove the estimate in the weak sense, it is
sufﬁcient to bound the local error δW
k in the weak sense, and to apply Theorem 4.8.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
Since Rk is mixing, ⟨QkµN,h
k−1,k⟩> 0 in view of Remark 5.7. Using estimate (6)
k−1),φ⟩| =
Kh ∗SN(QkµN,h
 −k · (QkµN,h
≤|⟨Kh ∗SN(QkµN,h
k−1) −QkµN,h
+ |⟨Kh ∗SN(QkµN,h
k−1) −QkµN,h
for any bounded measurable test function φ deﬁned on E. By deﬁnition,
µ∈P(E)⟨Qkµ,k⟩.
Using estimate (30) with  ≡1, µ = QkµN,h
k−1 and F = σ(Y1: k, µN,h
k−1), yields
|⟨Kh ∗SN(QkµN,h
k−1) −QkµN,h
Y1: k, µN,h
To prove the estimate in total variation, it is sufﬁcient to bound the local
in total variation, and to apply Theorem 4.6. Using estimate (7) yields
Kh ∗SN(QkµN,h
 −k · (QkµN,h
≤2 supx∈E k(x)
Kh ∗SN(QkµN,h
k−1) −QkµN,h
Using estimate (31) with  ≡1, µ = QkµN,h
k−1 and F = σ(Y1: k, µN,h
k−1), yields
Kh ∗SN(QkµN,h
k−1) −QkµN,h
Y1: k, µN,h
m/2 + αh2
Nhm (Ik + hγ )m/2 + αh2Dk.
F. LE GLAND AND N. OUDJANE
In the pre-RPF, the correction is applied directly to a regularized probability
distribution, hence each point in the support of the regularized density is updated,
and in principle the degeneracy of particle weights which occurs when the
correction is applied to a discrete probability distribution, as in the IPF, is now
avoided. This intuition is supported by numerical evidence, and by the following
theorem, which shows that it is possible to control the local error, averaged over
observation sequences, independently of any lower bound assumption on the
likelihood functions (notice that the a.s. bounds of Theorem 6.9 still depend on
the integrated lower bounds of the likelihood functions).
THEOREM 6.10.
If for any k ≥1, Assumptions R and M hold, and the
nonnegative kernel Rk is mixing with nonrandom εk, then the pre-RPF estimator
E∥µn −µN,h
where for any k ≥1,
Nhm(Ik + hγ )m/2 + αh2Dk
COROLLARY 6.11.
If for any k ≥1, the nonnegative kernel Rk is mixing with
εk ≥ε > 0 and nonrandom ε, and Assumptions R and M hold with Dk ≤D and
Ik ≤I, then convergence, averaged over observation sequences, holds uniformly
in time, that is,
E∥µn −µN,h
Nhm (I + hγ )m/2 + αh2D
Both the SPF (see Theorem 5.14) and the pre-RPF allow bounding the error
independently of any lower bound assumption on the likelihood functions, and the
computational time of both algorithms is random [recall that a rejection is needed
in the resampling step (ii) of the pre-RPF].
PROOF OF THEOREM 6.10.
It is sufﬁcient to bound the local error E[δTV
total variation, averaged over observation sequences, and to apply Theorem 4.6. If
the nonnegative kernel Rk is mixing, then
k−1,k⟩≥ε2
k⟨µk|k−1,k⟩,
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
in view of Remark 5.7, with nonrandom εk, hence using inequality (7) yields
⟨µk|k−1,k⟩|µk|(dx),
µk = Kh ∗SN(QkµN,h
k−1) −QkµN,h
It follows from Remark 2.2 that
⟨µk|k−1,k⟩
Y1: k−1, µN,h
k|k−1, µN,h
⟨µk|k−1,k⟩
Y1: k−1
Y1: k−1, µN,h
k|k−1, µN,h
Using estimate (31) with  ≡1, µ = Qk µN,h
k−1 and F = σ(µN,h
k−1), yields
E[|µk|(E)|µN,h
Kh ∗SN(QkµN,h
k−1) −QkµN,h
m/2 + αh2
Nhm (Ik + hγ )m/2 + αh2Dk.
REMARK 6.12.
In the same way as for the SPF (see Theorem 5.14), one
could think of using a random number of particles so as to avoid any lower
bound assumption on the likelihood functions. However, for the pre-RPF, it is not
sufﬁcient to evaluate the quantity k(ξi
k|k−1) for each simulated particle, as in (28),
but one has to evaluate the integral
(Kh ∗k)(ξi
E k(x)Kh(x −ξi
instead. This evaluation is in general very costly, which makes the idea of using a
random number of particles for the pre-RPF rather unpractical.
6.3. Post-regularized particle ﬁlter.
denote the post-regularized
particle ﬁlter (post-RPF) approximation of µn. Initially µN,h
= µ0, and the
transition from µN,h
n−1 to µN,h
is described by the following diagram:
prediction
n|n−1 = SN(QnµN,h
postregularized
correction
= Kh ∗(n · µN,h
F. LE GLAND AND N. OUDJANE
In practice, the particle approximation
is completely characterized by the particle system {ξ1
n|n−1,...,ξN
n|n−1}, and
the transition from {ξ1
n|n−1,...,ξN
n|n−1} to {ξ1
n+1|n,...,ξN
n+1|n} consists of the
following steps.
Step (i) (Post-regularized correction). If the normalization constant
is positive, then for all i = 1,...,N, compute the weight
Kh ∗(n · µN,h
otherwise set µN,h
Step (ii) (Sampled prediction). Independently for all i = 1,...,N, generate a
n+1|n ∼Qn+1µN,h
n+1|n = SN(Qn+1µN,h
The resampling step (ii) can be easily implemented: it requires generating
random variables according to a mixture of rescaled kernels, where the mixing
probability distribution is either a weighted discrete probability distribution or the
arbitrary restarting probability distribution ν.
Notice that the post-RPF satisﬁes (16).
ASSUMPTION L′′.
REMARK 6.13.
If k is bounded, with bounded derivatives up to order two,
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
If Assumption L′′ holds, and if for any k ≥1 the nonnegative kernel Rk is
mixing, then the following notation is introduced:
k := supu∈W 2,1(|ku|2,1/∥u∥2,1)
infµ∈P(E)⟨Qkµ,k⟩
and in view of Remark 5.7, ρ′′
k is a.s. ﬁnite.
ASSUMPTION R′′ (Additional regularity of the Markov kernel).
µ ∈P (E), the probability distribution Qkµ is absolutely continuous w.r.t. the
Lebesgue measure, with density in W 2,1, and
REMARK 6.14.
Assumption R′′ is equivalent to supposing that the probability
distribution Qk(x,·) is absolutely continuous w.r.t. the Lebesgue measure, with
density qk(x,·) in W 2,1 for any x ∈E, and
∥qk(x,·)∥2,1 < ∞.
THEOREM 6.15.
If for any k ≥1, Assumptions L′′ and R′′ hold and the
nonnegative kernel Rk is mixing, then the post-RPF estimator satisﬁes
|⟨µn −µN,h
where for any k ≥1,
2ρk + αh2D′′
If in addition for any k ≥1, Assumption M holds, then
where for any k ≥1,
Nhm (Ik + hγ )1/2
ρk + αh2D′′
The proof is similar to the proof of Theorem 6.9 except that estimates
(30) and (31) are used here with  = k. By deﬁnition,
k · SN(QkµN,h
Kh ∗(kSN(Qk µN,h
⟨SN(QkµN,h
SN(QkµN,h
otherwise,
F. LE GLAND AND N. OUDJANE
and since convolution by Kh preserves the total mass,
kSN(QkµN,h
SN(QkµN,h
Finally, recall that the bounds in estimates (6) and (7) do not depend on the
restarting probability distribution.
To prove the estimate in the weak sense, it is sufﬁcient to bound the local
in the weak sense, and to apply Theorem 4.8. Since Rk is mixing,
k−1,k⟩> 0 in view of Remark 5.7. Using estimate (6) yields
k · SN(QkµN,h
 −k(QkµN,h
≤|⟨Kh ∗(kSN(QkµN,h
k−1)) −k(QkµN,h
+ |⟨SN(QkµN,h
k−1) −QkµN,h
for any bounded measurable test function φ deﬁned on E. By deﬁnition,
µ∈P(E)⟨Qkµ,k⟩.
Under Assumption R′′,
Using estimate (30) with  = k, µ = QkµN,h
k−1 and F = σ(Y1: k, µN,h
k−1), yields
kSN(QkµN,h
 −k(QkµN,h
Y1: k, µN,h
Using estimate (24) with φ = k, µ = QkµN,h
k−1 and F = σ(Y1: k, µN,h
k−1), yields
SN(QkµN,h
k−1) −QkµN,h
Y1: k, µN,h
To prove the estimate in total variation, it is sufﬁcient to bound the local
in total variation and to apply Theorem 4.6. Since convolution by Kh
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
preserves the total mass, using estimate (7) yields
k · SN(QkµN,h
 −k · (QkµN,h
≤∥Kh ∗(kSN(QkµN,h
k−1)) −k(QkµN,h
+ |⟨SN(QkµN,h
k−1) −QkµN,h
Using estimate (31) with  = k, µ = QkµN,h
k−1 and F = σ(Y1: k, µN,h
k−1), yields
kSN(QkµN,h
 −k(QkµN,h
Y1: k, µN,h
k(x) + αh2
Nhm (Ik + hγ )m/2 sup
k(x) + αh2D′′
Acknowledgments.
The ﬁrst author gratefully thanks Marion Baudry,
Natacha Caylus and Arnaud Guyader for their careful reading of earlier versions
of this work.