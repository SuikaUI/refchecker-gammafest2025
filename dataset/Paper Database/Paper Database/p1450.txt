The Annals of Applied Probability
2004, Vol. 14, No. 1, 144â€“187
Â© Institute of Mathematical Statistics, 2004
STABILITY AND UNIFORM APPROXIMATION OF
NONLINEAR FILTERS USING THE HILBERT METRIC AND
APPLICATION TO PARTICLE FILTERS1
BY FRANÃ‡OIS LE GLAND AND NADIA OUDJANE
IRISA/INRIA Rennes and EDF R&D Clamart
We study the stability of the optimal ï¬lter w.r.t. its initial condition and
w.r.t. the model for the hidden state and the observations in a general hidden
Markov model, using the Hilbert projective metric. These stability results are
then used to prove, under some mixing assumption, the uniform convergence
to the optimal ï¬lter of several particle ï¬lters, such as the interacting particle
ï¬lter and some other original particle ï¬lters.
1. Introduction.
The stability of the optimal ï¬lter has recently become an
active research area. Ocone and Pardoux proved that the ï¬lter forgets its initial
condition in the Lp sense, without stating any rate of convergence. Recently, a
new approach has been proposed using the Hilbert projective metric. This metric
allows getting rid of the normalization constant in the Bayes formula and reduces
the problem to studying the linear equation satisï¬ed by the unnormalized optimal
ï¬lter. Using the Hilbert metric, stability results w.r.t. the initial condition have been
proved by Atar and Zeitouni , and some stability result w.r.t. the model have
been proved by Le Gland and Mevel , for hidden Markov models (HMM)
with ï¬nite state space. The results and methods of have been extended to
HMM with Polish state space by Atar and Zeitouni ; see also . Independently,
Del Moral and Guionnet have adopted in , for the same class of HMM, another
approach based on semigroup techniques and on the Dobrushin ergodic coefï¬cient,
to derive stability results w.r.t. the initial condition, which are used to prove
uniform convergence of the interacting particle system (IPS) approximation to
the optimal predictor. New approaches have been proposed recently, to prove the
stability of the optimal ï¬lter w.r.t. its initial condition, in the case of a noncompact
state space (see, e.g., ).
In this article, we use the approach based on the Hilbert metric to study the
asymptotic behavior of the optimal ï¬lter, and to prove as in the uniform
Received July 2001; revised January 2003.
1Supported in part by CNRS projects MÃ©thodes Particulaires en Filtrage Non-LinÃ©aire (97â€“
N23/0019, ModÃ©lisation et Simulation NumÃ©rique programme), ChaÃ®nes de Markov CachÃ©es et
Filtrage Particulaire (MathSTIC programme) and MÃ©thodes Particulaires (AS67, DSTIC Action
SpÃ©ciï¬que programme).
AMS 2000 subject classiï¬cations. Primary 93E11, 93E15, 62E25; secondary 60B10, 60J27,
62G07, 62G09, 62L10.
Key words and phrases. Hidden Markov model, nonlinear ï¬lter, particle ï¬lter, stability, Hilbert
metric, total variation norm, mixing, regularizing kernel.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
convergence of several particle ï¬lters, such as the interacting particle ï¬lter (IPF)
and some other original particle ï¬lters.
A common assumption to prove stability results (see, e.g., , Theorem 2.4)
is that the Markov transition kernels are mixing, which implies that the hidden
state sequence is ergodic. Our results are obtained under the assumption that the
nonnegative kernels describing the evolution of the unnormalized optimal ï¬lter,
and incorporating simultaneously the Markov transition kernels and the likelihood
functions, are mixing. This is a weaker assumption (see Proposition 3.9), which
allows considering some cases, similar to the case studied in , where the hidden
state sequence is not ergodic; see Example 3.10. This point of view is further
developped by Le Gland and Oudjane and by Oudjane and Rubenthaler .
Our main contribution is to study also the stability of the optimal ï¬lter w.r.t. the
model, when the local error is propagated by mixing kernels and can be estimated
in the Hilbert metric, in the total variation norm, or in a weaker distance suitable
for random probability distributions.
The uniform convergence of the IPS approximation to the optimal predictor is
proved in ( , Theorem 3.1), under the assumption that the likelihood functions
are uniformly bounded away from zero, which is rather strong, and that the
predictor is asymptotically stable. The rate (1/
N )Î± for some Î± < 1 is proved
under the stronger assumption that the predictor is exponentially asymptotically
stable, and the rate 1/
N is proved by Del Moral and Miclo ( , page 36) under
an additional assumption which is satisï¬ed, for example, if the Markov kernels
are mixing. Our uniform convergence results are obtained under the assumption
that the expected values of the likelihood functions integrated against any possible
predicted probability distribution are bounded away from zero. This assumption is
automatically satisï¬ed under our weaker mixing assumption; see Remark 5.7.
Motivated by practical considerations, we introduce a variant of the IPF, where
an adaptive number of particles is used, based on a posteriori estimates. The
resulting sequential particle ï¬lter (SPF) is shown to converge uniformly to the
optimal ï¬lter, independently of any lower bound assumption on the likelihood
functions. The counterpart is that the computational time is random and that the
expected number of particles does depend on the integrated lower bounds of the
likelihood functions. Also motivated by practical considerations, that is, to avoid
the degeneracy of particle weights and the degeneracy of particle locations, which
are two known causes of divergence of particle ï¬lters, we introduce regularized
particle ï¬lters (RPF), which are shown to converge uniformly to the optimal ï¬lter.
The paper is organized as follows: In the next section we deï¬ne the framework
of the nonlinear ï¬ltering problem and we introduce some notation. In Section 3, we
state some properties of the Hilbert metric, which are used in Section 4 to prove the
stability of the optimal ï¬lter w.r.t. its initial condition and w.r.t. the model. These
stability results are used to prove the uniform convergence of several particle ï¬lters
to the optimal ï¬lter. First, uniform convergence in the weak sense is proved in
F. LE GLAND AND N. OUDJANE
Section 5 for interacting particle ï¬lters, with a rate 1/
N, and sequential particle
ï¬lters with a random number of particles are also considered. Finally, regularized
particle ï¬lters are deï¬ned in Section 6, for which uniform convergence in the weak
sense and in the total variation norm are proved.
2. Optimal ï¬lter for general HMM.
We consider the following model, with
a hidden (nonobserved) state sequence {Xn, n â‰¥0} and an observation sequence
{Yn, n â‰¥1}, taking values in a complete separable metric space E and in F = Rd,
respectively (in Section 6, it will be assumed that E = Rm).
â€¢ The state sequence {Xn, n â‰¥0} is deï¬ned as an inhomogeneous Markov chain,
with transition probability kernel Qn; that is,
P[Xn âˆˆdx|X0: nâˆ’1 = x0: nâˆ’1] = P[Xn âˆˆdx|Xnâˆ’1 = xnâˆ’1] = Qn(xnâˆ’1,dx),
for all n â‰¥1, and with initial probability distribution Âµ0. For instance, {Xn,
n â‰¥0} could be deï¬ned by the following equation:
Xn = fn(Xnâˆ’1,Wn),
where {Wn, n â‰¥1} is a sequence of independent random variables, not
necessarily Gaussian, independent of the initial state X0.
â€¢ The memoryless channel assumption holds; that is, given the state sequence
{Xn, n â‰¥0},
the observations {Yn, n â‰¥1} are independent random variables, and for all
n â‰¥1, the conditional probability distribution of Yn depends only on Xn.
For instance, the observation sequence {Yn, n â‰¥1} could be related to the state
sequence {Xn, n â‰¥0} by
Yn = hn(Xn,Vn)
for all n â‰¥1, where {Vn, n â‰¥1} is a sequence of independent random variables,
not necessarily Gaussian, independent of the state sequence {Xn, n â‰¥0}.
In addition, it is assumed that for all n â‰¥1, the collection of probability
distributions P[Yn âˆˆdy|Xn = x] on F , parametrized by x âˆˆE, is dominated,
P[Yn âˆˆdy|Xn = x] = gn(x,y)Î»F
for some nonnegative measure Î»F
n on F . The corresponding likelihood function
is deï¬ned by n(x) = gn(x,Yn), and depends implicitly on the observation Yn.
The following notation and deï¬nitions will be used throughout the paper:
â€¢ The set of probability distributions on E and the set of ï¬nite nonnegative
measures on E are denoted by P (E) and M+(E), respectively.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
â€¢ The notation âˆ¥Â· âˆ¥is used for the total variation norm on the set of signed
measures on E and for the supremum norm on the set of bounded measurable
functions deï¬ned on E, depending on the context.
â€¢ With any nonnegative kernel K deï¬ned on E is associated a nonnegative linear
operator denoted by K and deï¬ned by
E Âµ(dx)K(x,dxâ€²)
for any nonnegative measure Âµ âˆˆM+(E).
â€¢ With any nonnegative measure Âµ âˆˆM+(E) is associated the normalized
nonnegative measure (i.e., the probability distribution),
if Âµ(E) > 0, that is, if Âµ is nonzero,
otherwise, that is, if Âµ â‰¡0,
where Î½ âˆˆP (E) is an arbitrary probability distribution.
â€¢ With any nonnegative kernel K deï¬ned on E is associated the normalized
nonnegative nonlinear operator Â¯K, taking values in P (E), and deï¬ned for any
nonzero nonnegative measure Âµ âˆˆM+(E) by
if (KÂµ)(E) > 0, that is, if KÂµ is nonzero,
otherwise, that is, if KÂµ â‰¡0,
where Î½ âˆˆP (E) is an arbitrary probability distribution. Notice that Â¯K(Âµ) =
Â¯K( Â¯Âµ) is nonzero by deï¬nition, hence composition of normalized nonnegative
nonlinear operators is well deï¬ned.
The problem of nonlinear ï¬ltering is to compute at each time n, the conditional
probability distribution Âµn of the state Xn given the observation sequence Y1: n =
(Y1,...,Yn) up to time n. The transition from Âµnâˆ’1 to Âµn is described by the
following diagram:
predictionÂµn|nâˆ’1 = QnÂµnâˆ’1
correctionÂµn = n Â· Âµn|nâˆ’1 =
âŸ¨Âµn|nâˆ’1,nâŸ©,
where â€œÂ·â€ denotes the projective product. In general, no explicit expression is
available for the Markov kernel Qn, or it is so complicated that computing integrals
Âµn|nâˆ’1(dxâ€²) = QnÂµnâˆ’1(dxâ€²) =
E Âµnâˆ’1(dx)Qn(x,dxâ€²)
is practically impossible. Instead, throughout this paper we assume that for any
x âˆˆE, simulating a r.v. with probability distribution Qn(x,dxâ€²) is easy [this is the
case, e.g., if (1) holds].
F. LE GLAND AND N. OUDJANE
REMARK 2.1.
Notice that the normalizing constant âŸ¨Âµn|nâˆ’1,nâŸ©is a.s.
positive; hence the projective product n Â· Âµn|nâˆ’1 is well deï¬ned. Indeed,
P[Yn âˆˆdy|Y1: nâˆ’1] =
E P[Yn âˆˆdy|Xn = x]P[Xn âˆˆdx|Y1: nâˆ’1]
E gn(x,y)Âµn|nâˆ’1(dx)
n (dy) = â„“n(y)Î»F
âŸ¨Âµn|nâˆ’1,nâŸ©=
E gn(x,Yn)Âµn|nâˆ’1(dx) = â„“n(Yn).
P[âŸ¨Âµn|nâˆ’1,nâŸ©= 0|Y1: nâˆ’1] =
F 1{â„“n(y)=0}â„“n(y)Î»F
n (dy) = 0.
REMARK 2.2.
Notice also that, for any test function Ïˆ deï¬ned on F ,
âŸ¨Âµn|nâˆ’1,nâŸ©
Y1: nâˆ’1
Y1: nâˆ’1
In particular, if Ïˆ(y) = gn(x,y), then Ïˆ(Yn) = n(x), and
âŸ¨Âµn|nâˆ’1,nâŸ©
Y1: nâˆ’1
F gn(x,y)Î»F
n (dy) = 1,
for any x âˆˆE.
For any n â‰¥1, we introduce the nonnegative kernel
Rn(x,dxâ€²) = Qn(x,dxâ€²)n(xâ€²)
and the associated nonnegative linear operator Rn = nQn on M+(E), deï¬ned by
RnÂµ(dxâ€²) =
E Âµ(dx)Qn(x,dxâ€²)n(xâ€²)
for any Âµ âˆˆM+(E). Notice that Rn depends on the observation Yn through the
likelihood function n. With this deï¬nition, (RnÂµnâˆ’1)(E) = âŸ¨Âµn|nâˆ’1,nâŸ©is a.s.
positive, and the evolution of the optimal ï¬lter can be written as follows:
Âµn = n Â· (QnÂµnâˆ’1) =
(RnÂµnâˆ’1)(E) = Â¯Rn(Âµnâˆ’1),
and iteration yields
Âµn = Â¯Rn(Âµnâˆ’1) = Â¯Rn â—¦Â·Â·Â· â—¦Â¯Rm(Âµmâˆ’1) = Â¯Rn: m(Âµmâˆ’1).
Equation (3) shows clearly that the evolution of the optimal ï¬lter is nonlinear only
because of the normalization term coming from the Bayes rule. In the following
section a projective metric is introduced precisely to get rid of the normalization
and to come down to the analysis of a linear evolution.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
REMARK 2.3.
The model considered here is slightly different from the model
considered in other works (see and references therein), where it is assumed
that an observation Y0 is already available at time 0, and where the object of
study is rather the conditional probability distribution Î·n of the state Xn given
the observation sequence Y0: nâˆ’1 = (Y0,...,Ynâˆ’1) up to time (n âˆ’1). With our
notation, the evolution of the optimal predictor in this alternate model can be
written as follows:
Î·n+1 = Qn+1(n Â· Î·n),
and iteration yields
Î·n+1 = Qn+1 Â¯Rn â—¦Â·Â·Â· â—¦Â¯Rm(mâˆ’1 Â· Î·mâˆ’1)
= Qn+1 Â¯Rn: m(mâˆ’1 Â· Î·mâˆ’1)
= Qn+1 Ë†Î·n,
with initial condition Î·0 = Âµ0.
3. Hilbert metric on the set of ï¬nite nonnegative measures.
In this section
we recall the deï¬nition of the Hilbert metric and its associated contraction
coefï¬cient, the Birkhoff contraction coefï¬cient. We introduce also a mixing
property for nonnegative kernels, and we state some properties relating the Hilbert
metric with other distances on the set of probability distributions, for instance,
the total variation norm, or a weaker distance suitable for random probability
distributions. In the last part of the section, these deï¬nitions and properties are
specialized to the optimal ï¬ltering context.
DEFINITION 3.1.
Two nonnegative measures Âµ,Âµâ€² âˆˆM+(E) are comparable, if they are both nonzero and if there exist positive constants 0 < a â‰¤b, such
aÂµâ€²(A) â‰¤Âµ(A) â‰¤bÂµâ€²(A)
for any Borel subset A âŠ‚E.
DEFINITION 3.2.
The nonnegative kernel K deï¬ned on E is mixing, if there
exist a constant 0 < Îµ â‰¤1 and a nonnegative measure Î» âˆˆM+(E), such that
ÎµÎ»(A) â‰¤K(x,A) â‰¤1
for any x âˆˆE, and any Borel subset A âŠ‚E.
DEFINITION 3.3.
The Hilbert metric on M+(E) is deï¬ned by
h(Âµ,Âµâ€²) :=
log supA: Âµâ€²(A)>0 Âµ(A)/Âµâ€²(A)
infA: Âµâ€²(A)>0 Âµ(A)/Âµâ€²(A) ,
if Âµ and Âµâ€² are comparable,
if Âµ = Âµâ€² â‰¡0,
otherwise.
F. LE GLAND AND N. OUDJANE
Notice that the two nonnegative measures Âµ and Âµâ€² are comparable if and only
if Âµ and Âµâ€² are equivalent, with Radonâ€“Nikodym derivatives dÂµ
dÂµâ€² and dÂµâ€²
dÂµ bounded
and bounded away from zero, and then the following equality holds:
h(Âµ,Âµâ€²) = log
A: Âµâ€²(A)>0
Moreover, h is a projective distance; that is, it is invariant under multiplication by
positive scalars; hence the Hilbert distance between two unnormalized nonnegative
measures is the same as the Hilbert distance between the two corresponding
normalized measures: h(Âµ,Âµâ€²) = h( Â¯Âµ, Â¯Âµâ€²), for any nonzero Âµ,Âµâ€² âˆˆM+(E).
In the nonlinear ï¬ltering context, this property will allow us to consider the
linear transformation Âµ â†’RnÂµ instead of the nonlinear transformation Âµ â†’
Â¯Rn(Âµ) = RnÂµ/(RnÂµ)(E). This projective property does not hold for other
distances. Indeed, the following estimates show how the error between two
unnormalized nonnegative measures can be used to bound the error between the
two corresponding normalized measures. If Âµ = Âµâ€² â‰¡0, then Â¯Âµ = Â¯Âµâ€² = Î½, hence
Â¯Âµ âˆ’Â¯Âµâ€² â‰¡0. If both Âµ and Âµâ€² are nonzero, then
Âµ(E) âˆ’Âµâ€²(E)
|âŸ¨Â¯Âµ âˆ’Â¯Âµâ€²,Ï†âŸ©| â‰¤|âŸ¨Âµ âˆ’Âµâ€²,Ï†âŸ©|
+ |Âµ(E) âˆ’Âµâ€²(E)|
âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥â‰¤âˆ¥Âµ âˆ’Âµâ€²âˆ¥
+ |Âµ(E) âˆ’Âµâ€²(E)|
Finally, if Âµ is nonzero and Âµâ€² â‰¡0, then
|âŸ¨Â¯Âµ âˆ’Â¯Âµâ€²,Ï†âŸ©| â‰¤|âŸ¨Âµ,Ï†âŸ©|
Âµ(E) + âˆ¥Ï†âˆ¥
âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥â‰¤2,
that is, estimates (6) and (7) still hold [notice that the bounds in estimates
(6) and (7) do not depend on the restarting probability distribution Î½].
The following two lemmas give several useful relations between the Hilbert
metric, the total variation norm and a weaker distance suitable for random
probability distributions.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
LEMMA 3.4.
For any Âµ,Âµâ€² âˆˆM+(E),
âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥â‰¤
log3h(Âµ,Âµâ€²).
If the nonnegative kernel K deï¬ned on E is mixing, then for any nonzero Âµ,Âµâ€² âˆˆ
h(KÂµ,KÂµâ€²) â‰¤1
Îµ2âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥.
If Âµ = Âµâ€² â‰¡0, then Â¯Âµ = Â¯Âµâ€² = Î½ hence âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥= 0, while
h(Âµ,Âµâ€²) = 0 by deï¬nition. If Âµ is nonzero and Âµâ€² â‰¡0, then h(Âµ,Âµâ€²) = âˆby
deï¬nition. Finally, if both Âµ and Âµâ€² are nonzero, the proof of the ï¬rst inequality can
be found in . To prove the second inequality, notice ï¬rst that, for any comparable
Âµ,Âµâ€² âˆˆM+(E),
h(Âµ,Âµâ€²) = log
A: Âµâ€²(A)>0
Âµâ€²(A) + log
A: Âµâ€²(A)>0
|Âµ(A) âˆ’Âµâ€²(A)|
|Âµ(A) âˆ’Âµâ€²(A)|
since log(1+x) â‰¤|x|. In order to apply this bound to h(KÂµ,KÂµâ€²) = h(K Â¯Âµ,K Â¯Âµâ€²),
we notice that KÂµ and KÂµâ€² are comparable for any nonzero Âµ,Âµâ€² âˆˆM+(E), since
K is mixing, and we introduce
(A) = (K Â¯Âµ)(A) âˆ’(K Â¯Âµâ€²)(A)
E( Â¯Âµ âˆ’Â¯Âµâ€²)(dx)
E( Â¯Âµ âˆ’Â¯Âµâ€²)+(dx)
E( Â¯Âµ âˆ’Â¯Âµâ€²)âˆ’(dx)
(x,A) = K(x,A)
(K Â¯Âµ)(A) â‰¤1
for any x âˆˆE and any Borel subset A âŠ‚E, using the mixing property. By the
ScheffÃ© theorem,
E( Â¯Âµ âˆ’Â¯Âµâ€²)+(dx) =
E( Â¯Âµ âˆ’Â¯Âµâ€²)âˆ’(dx) = 1
2âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥,
(A) is positive, then
E( Â¯Âµ âˆ’Â¯Âµâ€²)+(dx)
2Îµ2 âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥,
F. LE GLAND AND N. OUDJANE
and similarly, if
(A) is negative, then
E( Â¯Âµ âˆ’Â¯Âµâ€²)âˆ’(dx)
2Îµ2âˆ¥Â¯Âµ âˆ’Â¯Âµâ€²âˆ¥.
LEMMA 3.5.
If the nonnegative kernel K deï¬ned on E is dominated, that is,
if there exist a constant c > 0 and a nonnegative measure Î» âˆˆM+(E), such that
K(x,A) â‰¤cÎ»(A)
for any x âˆˆE and any Borel subset A âŠ‚E, then
Eâˆ¥KÂµ âˆ’KÂµâ€²âˆ¥â‰¤cÎ»(E)
E|âŸ¨Âµ âˆ’Âµâ€²,Ï†âŸ©|
for any Âµ,Âµâ€² âˆˆM+(E), possibly random.
REMARK 3.6.
If the nonnegative kernel K is mixing, then it is dominated,
with the same nonnegative measure Î» âˆˆM+(E) and with c = 1/Îµ.
REMARK 3.7.
If in addition the nonnegative kernel K is F -measurable, then
the same estimate holds for conditional expectations w.r.t. F , that is,
E[âˆ¥KÂµ âˆ’KÂµâ€²âˆ¥|F ] â‰¤cÎ»(E)
E[|âŸ¨Âµ âˆ’Âµâ€²,Ï†âŸ©||F ].
PROOF OF LEMMA 3.5.
By deï¬nition, if K is dominated, then K(x,Â·) is
absolutely continuous w.r.t. Î», with Radonâ€“Nikodym derivative k(x,Â·) bounded
by c, for any x âˆˆE. Therefore, the total variation norm âˆ¥KÂµ âˆ’KÂµâ€²âˆ¥can be
written as an integral as follows:
âˆ¥KÂµ âˆ’KÂµâ€²âˆ¥=
E(Âµ âˆ’Âµâ€²)(dx)k(x,xâ€²)
Î»(dxâ€²),
hence, taking expectation yields
Eâˆ¥KÂµ âˆ’KÂµâ€²âˆ¥=
E(Âµ âˆ’Âµâ€²)(dx)k(x,xâ€²)
Î»(dxâ€²)
E|âŸ¨Âµ âˆ’Âµâ€²,Ï†âŸ©|
LEMMA 3.8.
The nonnegative kernel K deï¬ned on E is a contraction under
the Hilbert metric, and
0<h(Âµ,Âµâ€²)<âˆ
where the supremum in
H(K) := sup
Âµ,Âµâ€² h(KÂµ,KÂµâ€²),
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
is over nonzero nonnegative measures: Ï„(K) is called the Birkhoff contraction
coefï¬cient.
The proof can be found in , Chapter XVI, Theorem 3, or in , Theorem 1.
Notice that H(K) < âˆimplies Ï„(K) < 1.
Returning to the ï¬ltering problem introduced in Section 2, stability results stated
in the following sections will in general require that for any n â‰¥1, the nonnegative
kernel Rn is mixing, that is, there exist a constant 0 < Îµn â‰¤1 and a nonnegative
measure Î»n âˆˆM+(E), such that
ÎµnÎ»n(A) â‰¤Rn(x,A) â‰¤1
for any x âˆˆE and any Borel subset A âŠ‚E. Notice that in full generality Îµn and Î»n
depend on the observation Yn, hence are random variables.
PROPOSITION 3.9.
The nonnegative kernel Rn deï¬ned in (2) is a contraction
under the Hilbert metric, with Birkhoff contraction coefï¬cient Ï„n = Ï„(Rn) â‰¤1.
(i) If Rn is mixing, with the possibly random constant Îµn, then
(ii) If the Markov transition kernel Qn is mixing, with the nonrandom
constant Îµn, then Rn is also mixing, with the same constant Îµn, without any
condition on the likelihood function n, and
Ï„n â‰¤Ï„(Qn) â‰¤1 âˆ’Îµ2
Throughout the paper, for any integers m â‰¤n, the contraction coefï¬cient of
the product Rn: m = Rn Â·Â·Â·Rm is denoted by Ï„n: m = Ï„(Rn: m) â‰¤Ï„n Â·Â·Â·Ï„m and by
convention Ï„n: n+1 = Ï„mâˆ’1: m = 1.
PROOF OF PROPOSITION 3.9.
It follows immediately from Lemma 3.8 that
Rn is a contraction under the Hilbert metric.
If Rn is mixing, then for any nonzero Âµ,Âµâ€² âˆˆM+(E) and any Borel subset
â‰¤ÎµnÎ»n(A) â‰¤RnÂµ(A)
hence RnÂµ and RnÂµâ€² are comparable. Using (5) yields
H(Rn) = sup
Âµ,Âµâ€² h(RnÂµ,RnÂµâ€²) = sup
F. LE GLAND AND N. OUDJANE
where the supremum is taken over nonzero nonnegative measures. Then using
Lemma 3.8 yields
Ï„n = Ï„(Rn) = tanh
which ends the proof of (i).
If Qn is mixing, then Rn = nQn is also mixing, since
A n(xâ€²)Î»n(dxâ€²) â‰¤Rn(x,A) â‰¤1
A n(xâ€²)Î»n(dxâ€²)
for any x âˆˆE and any Borel subset A âŠ‚E, hence for any nonzero Âµ,Âµâ€² âˆˆM+(E),
RnÂµ and RnÂµâ€² are comparable, with Radonâ€“Nikodym derivative
d(RnÂµâ€²)(xâ€²) = d(QnÂµ)
d(QnÂµâ€²)(xâ€²)1{n(xâ€²)>0} â‰¤d(QnÂµ)
d(QnÂµâ€²)(xâ€²)
for any xâ€² âˆˆE, and similarly with interchanging the role of Âµ and Âµâ€². Therefore,
H(Rn) â‰¤sup
= H(Qn) â‰¤log 1
where the supremum is taken over nonzero nonnegative measures. Then using
Lemma 3.8 again yields Ï„n = Ï„(Rn) â‰¤Ï„(Qn).
The assumption that the nonnegative kernel Rn is mixing is much weaker than
the usual assumption that both the Markov kernel Qn is mixing and the likelihood
function n is bounded away from zero. Indeed, if the Markov kernel Qn is
mixing, then it follows from (ii) that the nonnegative kernel Rn is mixing,
without any assumption on the likelihood function n: in particular, the likelihood
function could take the zero value, or even be compactly supported. This is not
a necessary condition, however, as illustrated by the example below, where the
Markov kernel Qn is not mixing, but the nonnegative kernel Rn is (equivalent, in
a sense to be deï¬ned below, to) a mixing kernel.
EXAMPLE 3.10.
Assume that Âµ0 has compact support C0 âŠ‚E and that for
any n â‰¥1, the function n has compact support Cn âŠ‚E, and the transition
probability kernel Qn is deï¬ned by
Qn(x,dxâ€²) = (2Ï€)âˆ’m/2 exp
2|xâ€² âˆ’fn(x)|2dxâ€² = qn(x,xâ€²)Î»(dxâ€²),
where the function fn is continuous and where
Î»(dxâ€²) = (2Ï€)âˆ’m/2 exp
2|xâ€²|2dxâ€².
Clearly, the Markov kernel Qn is not mixing, but introducing
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
which are both ï¬nite a.s., it holds
 â‰¤qn(x,xâ€²) â‰¤exp{
for any x âˆˆCnâˆ’1 and any xâ€² âˆˆCn. Deï¬ne
Rn(x,dxâ€²) = Qn(x,dxâ€²)n(xâ€²),
as usual, and
n(x,dxâ€²) = 1{xâˆˆCnâˆ’1}Rn(x,dxâ€²) + 1{x /âˆˆCnâˆ’1}n(xâ€²)Î»(dxâ€²)
1{xâˆˆCnâˆ’1}qn(x,xâ€²) + 1{x /âˆˆCnâˆ’1}
n(xâ€²)Î»(dxâ€²).
Notice ï¬rst that the sequence {Âµn, n â‰¥0} deï¬ned by (3) satisï¬es also
(Râ€¢nÂµnâˆ’1)(E).
Moreover, it follows from (12) that
A n(xâ€²),Î»(dxâ€²)
n(x,A) â‰¤exp{
A n(xâ€²)Î»(dxâ€²)
for any x âˆˆE and any Borel subset A âŠ‚E; that is, the nonnegative kernel Râ€¢
is mixing. Therefore, stability and approximation properties of the sequence
{Âµn, n â‰¥0} deï¬ned by (3), can be obtained directly by studying (13) instead,
which involves mixing kernels.
4. Stability of nonlinear ï¬lters.
In practice one rarely has access to the initial
distribution of the hidden state process; hence it is important to study the stability
of the ï¬lter w.r.t. its initial condition. Moreover, the answer to this question will be
useful in studing the stability of the ï¬lter w.r.t. the model.
Let Âµn denote the ï¬lter initialized with the correct Âµ0, and let Âµâ€²
n denote the
ï¬lter initialized with a wrong Âµâ€²
0; that is, Âµn = Â¯Rn: 1(Âµ0) and Âµâ€²
n = Â¯Rn: 1(Âµâ€²
are interested in the total variation error at time n induced by the initial error.
THEOREM 4.1.
Without any assumption on the nonnegative kernels, the
following inequality holds:
log3Ï„n: mh(Âµmâˆ’1,Âµâ€²
If in addition the nonnegative kernel Rm is mixing, then
log3Ï„n: m+1
F. LE GLAND AND N. OUDJANE
COROLLARY 4.2.
If for any k â‰¥1, the nonnegative kernel Rk is mixing with
Îµk â‰¥Îµ > 0, then convergence holds uniformly in time, that is,
Îµ2 log3Ï„ nâˆ’mâˆ¥Âµmâˆ’1 âˆ’Âµâ€²
with Ï„ = 1 âˆ’Îµ2
1 + Îµ2 < 1.
PROOF OF THEOREM 4.1.
Using (8) and the deï¬nition (11) of the Birkhoff
contraction coefï¬cient yields
Â¯Rn: m(Âµ) âˆ’Â¯Rn: m(Âµâ€²)
log3h(Rn: mÂµ,Rn: mÂµâ€²) â‰¤
log3Ï„n: mh(Âµ,Âµâ€²),
for any Âµ,Âµâ€² âˆˆP (E). If the nonnegative kernel Rm is mixing, then using (9) yields
Â¯Rn: m(Âµ) âˆ’Â¯Rn: m(Âµâ€²)
log3h(Rn: m+1RmÂµ,Rn: m+1RmÂµâ€²)
log3Ï„n: m+1h(RmÂµ,RmÂµâ€²)
log3Ï„n: m+1
Taking Âµ = Âµmâˆ’1 and Âµâ€² = Âµâ€²
mâˆ’1 completes the proof.
To solve the nonlinear ï¬ltering problem, one must have a model to describe
the state/observation system, {Xn, n â‰¥0}, {Yn, n â‰¥1}, as presented in Section 2.
The general hidden Markov model is based on the initial condition Âµ0, on
the transition kernels Qn and on the likelihood functions n, which deï¬ne the
evolution operator Â¯Rn for the optimal ï¬lter Âµn. But, as for the initial condition, in
practice one has rarely access to the true model. In particular, the prior information
on the state sequence is in general unknown and the choice of Qn is approximative.
Similarly, the probabilistic relation between the observation and the state is in
general unknown and the choice of n is also approximative. As a result, instead
of using the true model, it is common to work with a wrong model, based on a
wrong transition kernel Qâ€²
n and a wrong likelihood function â€²
n, which deï¬ne the
evolution operator Â¯Râ€²
n for a wrong ï¬lter Âµâ€²
Another situation is when the evolution operator Â¯Rn is known, but difï¬cult
to compute. For the purpose of practical implementation, one constructs an
approximate ï¬lter Âµâ€²
n such that the evolution Âµâ€²
n is easy to compute and
close to the true evolution Âµâ€²
nâˆ’1 â†’Â¯Rn(Âµâ€²
We are interested in bounding the global error between Âµâ€²
n and Âµn induced
by the local errors committed at each time step. We suppose here that Âµ0 = Âµâ€²
since the problem of a wrong initialization has already been studied above. In
full generality, we assume that {Âµâ€²
n, n â‰¥0} is a random sequence with values
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
in P (E), satisfying the following property: for any n â‰¥k â‰¥1 and for any bounded
measurable function F deï¬ned on P (E),
k)|Y1: n] = E[F(Âµâ€²
k)|Y1: k].
The results stated below are based on the following decomposition of the global
error into a sum of local errors transported by a sequence of normalized evolution
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k(Âµâ€²
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k+1 â—¦Â¯Rk(Âµâ€²
This equation shows the close relation between the stability w.r.t. the initial
condition and the stability w.r.t. the model.
Let us consider ï¬rst the case where we can estimate the local error in the sense
of the Hilbert metric.
ASSUMPTION H (Local error bound in the Hilbert metric).
REMARK 4.3.
If the evolution of the wrong ï¬lter Âµâ€²
k is deï¬ned by the
nonnegative kernel Râ€²
k(x,dxâ€²) = Qâ€²
k(x,dxâ€²)â€²
k(xâ€²), and if
Qk(x,dxâ€²) = qk(x,xâ€²)Î»k(dxâ€²)
k(x,dxâ€²) = qâ€²
k(x,xâ€²)Î»k(dxâ€²),
then a sufï¬cient condition for Assumption H to hold is that there exist constants
Î´k â‰¥0 and ak > 0, such that
ak â‰¤k(xâ€²)qk(x,xâ€²)
k(x,xâ€²) â‰¤ak exp(Î´k)
for all x,xâ€² âˆˆE, in which case Î´H
THEOREM 4.4.
If for any k â‰¥1, Assumption H holds, then
n âˆ’Âµnâˆ¥|Y1: n] â‰¤
COROLLARY 4.5.
If for any k â‰¥1, the nonnegative kernel Rk is mixing
with Îµk â‰¥Îµ > 0, and Assumption H holds with Î´H
k â‰¤Î´, then convergence holds
uniformly in time, that is,
nâˆ¥|Y1: n] â‰¤
F. LE GLAND AND N. OUDJANE
Indeed, (19) follows from
Ï„ nâˆ’k = 1 âˆ’Ï„ n
1 âˆ’Ï„ = 1 + Îµ2
OF THEOREM 4.4.
Using the decomposition (17), the triangle
inequality and estimate (14), yields
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k+1 â—¦Â¯Rk(Âµâ€²
Taking conditional expectation w.r.t. the observations and using (16), yields (18).
Let us consider next the case where we can estimate the local error in the sense
of the total variation norm
THEOREM 4.6.
If for any k â‰¥1, the nonnegative kernel Rk is mixing, then
nâˆ¥|Y1: n] â‰¤Î´TV
COROLLARY 4.7.
If for any k â‰¥1, the nonnegative kernel Rk is mixing, with
Îµk â‰¥Îµ > 0, and Î´TV
â‰¤Î´, then convergence holds uniformly in time, that is,
nâˆ¥|Y1: n] â‰¤
PROOF OF THEOREM 4.6.
The decomposition (17) is written as
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k+1 â—¦Â¯Rk(Âµâ€²
hence using the triangle inequality and estimate (15) yields
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k+1 â—¦Â¯Rk(Âµâ€²
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
Taking conditional expectation w.r.t. the observations and using (16), yields (20).
Let us consider ï¬nally the case where we can only estimate the local error in the
weak sense
Y1: k
This typically happens if the approximate ï¬lter Âµâ€²
k is an empirical probability
distribution associated with
kâˆ’1): in this case, bounding the local error
requires using the law of large numbers, which can only provide estimates in the
weak sense. However, if the nonnegative kernel Rk+1 is dominated, then using
Lemma 3.5, the local error transported by Rk+1 can be bounded in total variation
with the same precision Î´W
k as in the weak sense.
THEOREM 4.8.
If for any k â‰¥1, the nonnegative kernel Rk is mixing, then
E[|âŸ¨Âµn âˆ’Âµâ€²
n,Ï†âŸ©||Y1: n]
COROLLARY 4.9.
If for any k â‰¥1, the nonnegative kernel Rk is mixing with
Îµk â‰¥Îµ > 0, and Î´W
k â‰¤Î´, then convergence holds uniformly in time, that is,
E[|âŸ¨Âµn âˆ’Âµâ€²
n,Ï†âŸ©||Y1: n] â‰¤
PROOF OF THEOREM 4.8.
Using the decomposition (21) and the triangle
inequality yields
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k+1 â—¦Â¯Rk(Âµâ€²
For any 1 â‰¤k â‰¤n âˆ’2, using estimate (15) yields
Â¯Rn: k+1(Âµâ€²
k) âˆ’Â¯Rn: k+1 â—¦Â¯Rk(Âµâ€²
Â¯Rn: k+2 â—¦Â¯Rk+1(Âµâ€²
k) âˆ’Â¯Rn: k+2 â—¦Â¯Rk+1 â—¦Â¯Rk(Âµâ€²
log3Ï„n: k+3
k) âˆ’Â¯Rk+1 â—¦Â¯Rk(Âµâ€²
F. LE GLAND AND N. OUDJANE
For any 1 â‰¤k â‰¤n âˆ’1, using estimate (7) yields
k) âˆ’Â¯Rk+1 â—¦Â¯Rk(Âµâ€²
â‰¤2âˆ¥Rk+1(Âµâ€²
and the mixing property yields
k)(E) â‰¥Îµk+1Î»k+1(E).
Taking conditional expectation w.r.t. the observations, using estimate (10) with
K = Rk+1, Âµ = Â¯Rk(Âµâ€²
kâˆ’1), Âµâ€² = Âµâ€²
k and F = Y1: n and using (16), yields
Y1: n
Combining these estimates yields
k) âˆ’Â¯Rk+1 â—¦Â¯Rk(Âµâ€²
Finally, taking conditional expectation w.r.t. the observations in (23) yields (22).
5. Uniform convergence of interacting particle ï¬lters.
In this section and
the next we consider again the framework introduced in Section 4, but now
the wrong model is chosen deliberately, such that the wrong ï¬lter can easily
be computed, and remains close to the optimal ï¬lter. More speciï¬cally, we are
interested in particle methods to approximate the optimal ï¬lter numerically and
we provide estimates of the approximation error. The idea common to all particle
ï¬lters is to generate an N-sample (Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1) of i.i.d. random variables,
called a particle system, with common probability distribution QnÂµN
nâˆ’1, where
nâˆ’1 is an approximation of Âµnâˆ’1, and to use the corresponding empirical
probability distribution
as an approximation of Âµn|nâˆ’1 = QnÂµnâˆ’1. The method is very easy to implement,
even in high-dimensional problems, since it is sufï¬cient in principle to simulate
independent samples of the hidden state sequence. A major and the earliest
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
contribution in this ï¬eld was made by Gordon, Salmond and Smith ,
who proposed to use sampling/importance resampling (SIR) techniques in the
correction step: the positive effect of the resampling step is to automatically
select particles with larger values of the likelihood function, that is, to concentrate
particles in regions of interest of the state space. A very complete account of
the currently available mathematical results can be found in . Theoretical and
practical aspects can be found in .
5.1. Notation and preliminary results.
Throughout the paper, SN(Âµ) is a
shorthand notation for the empirical probability distribution of an N-sample with
probability distribution Âµ, that is,
SN(Âµ) := 1
with (Î¾1,...,Î¾N) i.i.d. âˆ¼Âµ.
LEMMA 5.1.
For any Âµ âˆˆP (E),
E|âŸ¨SN(Âµ) âˆ’Âµ,Ï†âŸ©| â‰¤
âŸ¨SN(Âµ) âˆ’Âµ,Ï†âŸ©= 1
[Ï†(Î¾i) âˆ’âŸ¨Âµ,Ï†âŸ©],
E|âŸ¨SN(Âµ) âˆ’Âµ,Ï†âŸ©|2 = 1
âŸ¨Âµ,Ï†2âŸ©âˆ’âŸ¨Âµ,Ï†âŸ©2 â‰¤1
REMARK 5.2.
If in addition Ï† and Âµ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (Î¾1,...,Î¾N) are i.i.d. with (conditional) probability
distribution Âµ, then the same estimate holds for conditional expectation w.r.t. F ,
|âŸ¨SN(Âµ) âˆ’Âµ,Ï†âŸ©|
For any nonnegative and bounded measurable function  deï¬ned on E and any
probability distribution Âµ deï¬ned on E, the projective product  Â· Âµ is deï¬ned by
if âŸ¨Âµ,âŸ©> 0,
otherwise,
F. LE GLAND AND N. OUDJANE
where Î½ is an arbitrary probability distribution deï¬ned on E. If âŸ¨Âµ,âŸ©> 0, it
follows immediately from Lemma 5.1 and using estimate (6), that
E|âŸ¨ Â· SN(Âµ) âˆ’ Â· Âµ,Ï†âŸ©|
âŸ¨SN(Âµ) âˆ’Âµ,Ï†âŸ©
supxâˆˆE (x)
REMARK 5.3.
If in addition Ï†,  and Âµ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (Î¾1,...,Î¾N) are i.i.d. with (conditional) probability
distribution Âµ, then the same estimate holds for conditional expectation w.r.t. F ,
|âŸ¨ Â· SN(Âµ) âˆ’ Â· Âµ,Ï†âŸ©|
supxâˆˆE (x)
The following procedure, classical in sequential analysis, can be used alternatively.
LEMMA 5.4.
Let Âµ âˆˆP (E), and let  be a nonnegative bounded measurable
function deï¬ned on E, such that âŸ¨Âµ,âŸ©> 0. For any Î´ > 0, deï¬ne the stopping
(Î¾i) â‰¥sup
with (Î¾1,...,Î¾N,...) i.i.d. âˆ¼Âµ.
E|âŸ¨ Â· ST (Âµ) âˆ’ Â· Âµ,Ï†âŸ©| â‰¤2Î´
To obtain an error estimate O(Î´), the expected sample size should be O(1/Î´2),
Î´2 â‰¤E[T ] â‰¤Ï
Î´2 (1 + Î´2),
where Ï = (supxâˆˆE (x))/âŸ¨Âµ,âŸ©.
The method proposed here to approximate the posterior probability distribution  Â· Âµ is somehow intermediate, between the classical importance
sampling method, which uses a ï¬xed number of random variables, and the acceptance/rejection method, which requires a random number of random variables.
In Lemma 5.4, the number of random variables generated is random as in the acceptance/rejection method, but there is no rejection, since all the random variables
generated are explicitly used in the approximation, as in the importance sampling
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
PROOF OF LEMMA 5.4.
Notice ï¬rst that a.s.
âŸ¨SN(Âµ),âŸ©= 1
(Î¾i) âˆ’â†’âŸ¨Âµ,âŸ©> 0,
as N â†‘âˆ, hence the stopping time T is a.s. ï¬nite. By deï¬nition, âŸ¨ST (Âµ),âŸ©> 0,
hence using estimate (6) yields
E|âŸ¨ Â· ST (Âµ) âˆ’ Â· Âµ,Ï†âŸ©| â‰¤2
E|âŸ¨ST (Âµ) âˆ’Âµ,Ï†âŸ©|
âŸ¨ST (Âµ),âŸ©
and we deï¬ne
[(Î¾i)Ï†(Î¾i) âˆ’âŸ¨Âµ,Ï†âŸ©]
By deï¬nition of the stopping time T , it holds
Î´2 â‰¤DT = DT âˆ’1 + (Î¾T ) â‰¤Î»
Î´2 + Î» = Î»
Î´2 (1 + Î´2),
where Î» = supxâˆˆE (x), and the Cauchyâ€“Schwarz inequality yields
In addition, for any a > 0,
P(T > N) â‰¤exp
E exp{âˆ’a(x)}Âµ(dx) < 1,
hence the stopping time T is integrable.
It follows from the Wald identity (see, e.g., , Proposition IVâ€“4â€“21), that
Î´2 â‰¤E[DT ] = E[T ]âŸ¨Âµ,âŸ©â‰¤Î»
Î´2 (1 + Î´2)
T ] = E[T ]
âŸ¨Âµ,2Ï†2âŸ©âˆ’âŸ¨Âµ,Ï†âŸ©2
â‰¤E[T ]âŸ¨Âµ,âŸ©Î»âˆ¥Ï†âˆ¥2 â‰¤Î»2
Î´2 (1 + Î´2)âˆ¥Ï†âˆ¥2,
E|âŸ¨ST (Âµ) âˆ’Âµ,Ï†âŸ©|
âŸ¨ST (Âµ),âŸ©
Î´2 â‰¤E[T ] â‰¤Ï
Î´2(1 + Î´2).
F. LE GLAND AND N. OUDJANE
REMARK 5.5.
If in addition Ï†,  and Âµ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (Î¾1,...,Î¾N,...) are i.i.d. with (conditional)
probability distribution Âµ, then the same estimate holds for conditional expectation
w.r.t. F , that is,
|âŸ¨ Â· ST (Âµ) âˆ’ Â· Âµ,Ï†âŸ©|
Î´2 â‰¤E[T |F ] â‰¤Ï
Î´2(1 + Î´2).
5.2. Interacting particle ï¬lter.
n denote the interacting particle ï¬lter
(IPF) approximation of Âµn. Initially ÂµN
0 = Âµ0, and the transition from ÂµN
is described by the following diagram:
prediction
n|nâˆ’1 = SN(QnÂµN
correctionÂµN
n = n Â· ÂµN
In practice, the particle approximation
is completely characterized by the particle system (Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1), and
the transition from (Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1) to (Î¾1
n+1|n,...,Î¾N
n+1|n) consists of the
following steps:
Step (i) (Correction). If the normalization constant
is positive, then for all i = 1,...,N, compute the weight
otherwise set ÂµN
Step (ii) (Sampled prediction). Independently for all i = 1,...,N, generate a
n+1|n âˆ¼Qn+1ÂµN
n , and set
n+1|n = SN(Qn+1ÂµN
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
The resampling step (ii) can be easily implemented: it requires generating
random variables either according to a weighted discrete probability distribution
or according to the arbitrary restarting probability distribution Î½.
Notice that the IPF satisï¬es (16).
REMARK 5.6.
Without the reinitialization procedure, proposed initially by
Del Moral, Jacod and Protter , the normalization constant cn could take zero
value, since the likelihood function n is not necessarily positive, and ÂµN
n|nâˆ’1 would not be a well-deï¬ned probability distribution. By construction,
the sequential particle ï¬lter deï¬ned at the end of this section does not run into this
REMARK 5.7.
If the nonnegative kernel Rn is mixing, then
ÂµâˆˆP(E)âŸ¨QnÂµ,nâŸ©=
ÂµâˆˆP(E)(RnÂµ)(E) â‰¥Îµ2
n(RnÂµnâˆ’1)(E) = Îµ2
nâŸ¨Âµn|nâˆ’1,nâŸ©,
hence a.s.
ÂµâˆˆP(E)âŸ¨QnÂµ,nâŸ©> 0,
in view of Remark 2.1.
Without loss of generality, it is assumed that the likelihood function is bounded.
ASSUMPTION L.
k(x) < âˆ.
If Assumption L holds, and if for any k â‰¥1, the nonnegative kernel Rk is
mixing, then the following notation is introduced:
supxâˆˆE k(x)
infÂµâˆˆP(E)âŸ¨QkÂµ,kâŸ©,
and in view of Remark 5.7, Ïk is a.s. ï¬nite.
THEOREM 5.8.
If for any k â‰¥1, Assumption L holds, and the nonnegative
kernel Rk is mixing, then the IPF estimator satisï¬es
where for any k â‰¥1,
F. LE GLAND AND N. OUDJANE
The convergence result stated in Theorem 5.8 would still hold with a time
dependent number of particles.
REMARK 5.9.
If the transition kernel Qn+1 is dominated, that is, Qn+1(x,Â·)
is absolutely continuous w.r.t. Î»n+1 âˆˆM+(E), with density qn+1(x,Â·) bounded
by cn+1 for any x âˆˆE, then convergence in the weak sense of the particle ï¬lter
can be used to prove convergence in total variation of the particle predictor. Indeed,
using Lemma 3.5 yields
âˆ¥Âµn+1|n âˆ’Qn+1ÂµN
 â‰¤cn+1Î»n+1(E)
where both Âµn+1|n and Qn+1ÂµN
n are absolutely continuous w.r.t. Î»n+1, and
for any xâ€² âˆˆE, which can be easily computed.
REMARK 5.10.
In general, it is not realistic to assume that the r.v.s Ïk are
uniformly bounded; hence it seems difï¬cult to guarantee that convergence holds
uniformly in time for a given observation sequence. On the other hand, averaging
over observation sequences makes it possible to obtain convergence uniformly in
time, under more realistic assumptions. Indeed, if for any k â‰¥1, the nonnegative
kernel Rk is mixing with nonrandom Îµk, and E[Ïk] is ï¬nite, then
n ,Ï†âŸ©| â‰¤Î´n + 2Î´nâˆ’1
where for any k â‰¥1,
REMARK 5.11.
Notice that, if the nonnegative kernel Rk is mixing, then
supxâˆˆE k(x)
âŸ¨Âµk|kâˆ’1,kâŸ©â‰¤Ïk â‰¤supxâˆˆE k(x)
kâŸ¨Âµk|kâˆ’1,kâŸ©,
and it follows from Remark 2.2 that
supxâˆˆE k(x)
âŸ¨Âµk|kâˆ’1,kâŸ©
Y1: kâˆ’1
hence a necessary and sufï¬cient condition for E[Ïk] to be ï¬nite is
k (dy) < âˆ.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
COROLLARY 5.12.
If for any k â‰¥1, the nonnegative kernel Rk is mixing with
Îµk â‰¥Îµ > 0 and nonrandom Îµ, and E[Ïk] â‰¤Ï, then convergence, averaged over
observation sequences, holds uniformly in time, that is,
PROOF OF THEOREM 5.8.
It is sufï¬cient to bound the local error Î´W
weak sense and to apply Theorem 4.8. Since Rk is mixing, âŸ¨QkÂµN
kâˆ’1,kâŸ©> 0,
in view of Remark 5.7. Using estimate (25) with  = k, Âµ = QkÂµN
F = Ïƒ(Y1: k, ÂµN
kâˆ’1) yields
Y1: k, ÂµN
 âˆ’k Â· (QkÂµN
Y1: k, ÂµN
supxâˆˆE k(x)|
kâˆ’1,kâŸ©âˆ¥Ï†âˆ¥â‰¤
REMARK 5.13.
n denote the interacting particle system (IPS) approximation of Î·n considered in and in other works by the same authors. Initially
0 = SN(Î·0), and the transition from Î·N
n is described by the following
correction Ë†Î·N
nâˆ’1 = nâˆ’1 Â· Î·N
prediction
n = SN(Qn Ë†Î·N
Clearly Ë†Î·N
0 = 0 Â· Î·N
0 = 0 Â· SN(Î·0), and the transition from Ë†Î·N
nâˆ’1 to Ë†Î·N
described by the following diagram:
prediction
n = SN(Qn Ë†Î·N
correction Ë†Î·N
n = n Â· Î·N
which involves exactly the same steps as the transition from ÂµN
n described
above; only the initial conditions Ë†Î·N
0 = 0 Â· SN(Î·0) and ÂµN
0 = Âµ0 are different.
Using the following decomposition of the global error into an initial error and a
sum of local errors transported by a sequence of normalized evolution operators:
Â¯Rn: k+1(Ë†Î·N
k ) âˆ’Â¯Rn: k(Ë†Î·N
Â¯Rn: 1(Ë†Î·N
0 ) âˆ’Â¯Rn: 1(Ë†Î·0)
and proceeding as in the proof of Theorem 4.8, yields under the assumptions of
F. LE GLAND AND N. OUDJANE
Theorem 5.8,
|âŸ¨Ë†Î·n âˆ’Ë†Î·N
where for any k â‰¥0,
Ï0 := supxâˆˆE 0(x)
Finally, notice that
n+1 = Qn+1 Ë†Î·N
n âˆ’SN(Qn+1 Ë†Î·N
n ) + Qn+1(Ë†Î·n âˆ’Ë†Î·N
|âŸ¨Î·n+1 âˆ’Î·N
|âŸ¨Ë†Î·n âˆ’Ë†Î·N
This proves the uniform convergence of the IPS approximation to the optimal
predictor, with rate 1/
N, for exactly the model considered in , under our
weaker mixing assumption.
In the proof of Theorem 5.8, if we use âŸ¨SN(QkÂµN
kâˆ’1),kâŸ©instead of
kâˆ’1,kâŸ©as the denominator in (27), we see that, for the local error to be
small, the empirical mean of the likelihood function over the predicted particle
system should be large enough. This theoretical argument is also supported by
numerical evidence, in cases where the likelihood function is localized in a small
region of the state space (which typically arises when measurements are accurate).
Indeed, such a region can be so small that it does not contain enough points of
the predicted particle system, which automatically results in a small value of the
predicted empirical mean of the likelihood function. This phenomenon is called
degeneracy of particle weights and is a known cause of divergence of particle
ï¬lters. To solve this degeneracy problem, one idea is to add a regularization step
to the algorithm; the resulting ï¬lters, called regularized particle ï¬lters (RPF), are
studied in the next section. Another idea is to control the predicted empirical mean
kâˆ’1),kâŸ©= 1
by using an adaptive number of particles. To guarantee a local error of order Î´k,
independently of any lower bound assumption on the likelihood function, we
choose a random number of particles,
k|kâˆ’1) â‰¥sup
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
that will automatically ï¬t the difï¬cult case of localized likelihood functions: the
resulting ï¬lter, called sequential particle ï¬lter (SPF) is studied below.
5.3. Sequential particle ï¬lter.
denote the sequential particle ï¬lter
(SPF) approximation of Âµn. Initially ÂµN0
= Âµ0, and the transition from ÂµNnâˆ’1
is described by the following diagram:
sequential
prediction
n|nâˆ’1 = SNn
correctionÂµNn
n = n Â· ÂµNn
In practice, the particle approximation
is completely characterized by the particle system (Î¾1
n|nâˆ’1,...,Î¾Nn
n|nâˆ’1), and
the transition from (Î¾1
n|nâˆ’1,...,Î¾Nn
n|nâˆ’1) to (Î¾1
n+1|n,...,Î¾Nn+1
n+1|n) consists of the
following steps:
Step (i) (Correction). For all i = 1,...,Nn, compute the weight
with the normalization constant
n = n Â· ÂµNn
Step (ii) (Sequential sampled prediction). Independently for all i = 1,...,Nn+1,
generate a r.v. Î¾i
n+1|n âˆ¼Qn+1ÂµNn
n , where the random number Nn+1 of particles is
deï¬ned by the stopping time
Nn+1 = inf
n+1|n) â‰¥sup
n+1|n = SNn+1
F. LE GLAND AND N. OUDJANE
Exactly as for the IPF, the resampling step (ii) can be easily implemented; it only
requires generating random variables according to a weighted discrete probability
distribution. Notice that a.s.
as N â†‘âˆ, and if the nonnegative kernel Rn is mixing, then âŸ¨QnÂµNnâˆ’1
nâˆ’1 ,nâŸ©> 0
in view of Remark 5.7, hence the stopping time Nn is a.s. ï¬nite. Moreover, the
normalization constant cn is positive, since
n(x) > 0,
hence n Â· ÂµNn
n|nâˆ’1 is a well-deï¬ned probability distribution.
Notice that the SPF satisï¬es (16). The following theorem shows that using a
random number of particles allows controling the local error independently of any
lower bound assumption on the likelihood functions. The counterpart is that the
computational time of the resulting algorithm is random and that the expected
number of particles does depend on the integrated lower bounds of the likelihood
functions.
THEOREM 5.14.
If for any k â‰¥1, the nonnegative kernel Rk is mixing and the
random number Nk of particles is deï¬ned as in (28), then the following inequality
Âµn âˆ’ÂµNn
Y1: n
where for any k â‰¥1,
â‰¤E[Nk|Y1: k] â‰¤Ïk
COROLLARY 5.15.
If for any k â‰¥1, the nonnegative kernel Rk is mixing with
Îµk â‰¥Îµ > 0 and the random number Nk of particles is deï¬ned as in (28) with Î´k â‰¤Î´,
then convergence holds uniformly in time, that is,
Âµn âˆ’ÂµNn
Y1: n
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
PROOF OF THEOREM 5.14.
It is sufï¬cient to bound the local error Î´W
weak sense, and to apply Theorem 4.8. Since Rk is mixing, âŸ¨QkÂµNkâˆ’1
kâˆ’1 ,kâŸ©> 0
in view of Remark 5.7. Using estimate (26) with  = k, Âµ = QkÂµNkâˆ’1
F = Ïƒ(Y1: k, ÂµNkâˆ’1
kâˆ’1 ) yields
Y1: k, ÂµNkâˆ’1
k Â· SNk
Y1: k, ÂµNkâˆ’1
In this section, we have proved that the IPF and its sequential variant converge
uniformly in time under the mixing assumption. This theoretical argument is also
supported by numerical evidence, for example, in extreme cases where the hidden
state sequence satisï¬es a noise-free state equation. Indeed, because multiple copies
are produced after each resampling step, the diversity of the particle system can
only decrease along the time in such cases, and the particle system ultimately
concentrates on a few points, if not a single point, of the state space. This
phenomenon is called degeneracy of particle locations and is another known
cause of divergence of particle ï¬lters. To solve this degeneracy problem, and
also the problem of degeneracy of particle weights already mentionned, we have
proposed in to add a regularization step in the algorithm, so as to guarantee
the diversity of the particle system along the time: the resulting ï¬lters, called
regularized particle ï¬lters (RPF), are studied in the next section under the same
mixing assumption.
6. Uniform convergence of regularized particle ï¬lters.
The main idea
consists in changing the discrete approximation ÂµN
n for an absolutely continuous
approximation, with the effect that in the resampling step N random variables are
generated according to an absolutely continuous distribution, hence producing a
new particle system with N different particle locations. In doing this, we implicitly
assume that the hidden state sequence takes values in a Euclidean space E = Rm
and that the optimal ï¬lter Âµn has a smooth density w.r.t. the Lebesgue measure,
which is the case in most applications. From the theoretical point of view, this
additional assumption allows obtaining strong approximations of the optimal ï¬lter,
in total variation or in the Lp sense for any p â‰¥1. In practice, this provides
approximate ï¬lters which are much more stable along the time than the IPF.
Obtaining an absolutely continuous approximation is achieved by adding a
regularization step in the algorithm, using a kernel method, classical in density
estimation. If the regularization occurs before the correction by the likelihood
function, we obtain the preregularized particle ï¬lter, the numerical analysis of
which was done in , in the general case without the mixing assumption. An
F. LE GLAND AND N. OUDJANE
improved version of the preregularized particle ï¬lter, called the kernel ï¬lter (KF),
is proposed in . If the regularization occurs after the correction by the
likelihood function, we obtain the postregularized particle ï¬lter, which has been
proposed in and and compared with the IPF in some classical tracking
problems, such as bearing only tracking, or range and bearing tracking with a
multiple dynamical model. The local rejection regularized particle ï¬lter (L2RPF),
which generalizes both the KF and the post-RPF, is introduced in , where
further implementation details and applications to tracking problems can be found.
6.1. Notation and preliminary results.
The following notation and deï¬nitions
will be used below. Throughout the end of this paper, E = Rm. For any Âµ âˆˆP (E),
E |x|m+1Âµ(dx)
and if Âµ is absolutely continuous w.r.t. the Lebesgue measure on E, with density
dx , deï¬ne
I (f ) = I (Âµ) =
E |x|m+1f (x)dx
From the multidimensional Carlson inequality (see , Lemma 7) there exists a
universal constant Am such that, for any absolutely continuous Âµ âˆˆP (E),
â‰¤Am(I (Âµ))m/2.
Hence J(dÂµ
dx ) is ï¬nite if I (Âµ) is ï¬nite. Let W 2,1 denote the Sobolev space of
functions deï¬ned on E, which together with their derivatives up to order two,
are integrable w.r.t. the Lebesgue measure on E. Let âˆ¥Â· âˆ¥2,1 and | Â· |2,1 denote the
corresponding norm and seminorm, that is,
E |Diu(x)|dx
E |Diu(x)|dx,
respectively, where for any multiindex i = (i1,...,im) of order |i| = i1 + Â·Â·Â· + im,
âˆ‚i1+Â·Â·Â·+im
1 Â·Â·Â· âˆ‚xim
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
Let the regularization kernel K be a symmetric probability density on E, such
E K(x)dx = 1,
E xK(x)dx = 0
E |x|2K(x)dx < âˆ.
Assume also that the regularization kernel K is square integrable, that is,
and that the symmetric probability density L = K2
Î²2 satisï¬es
Î³ := I (L) =
E |x|m+1L(x)dx
For any bandwidth h > 0, deï¬ne the rescaled kernel
for any x âˆˆE.
DEFINITION 6.1.
For any Âµ âˆˆM+(E), the nonnegative measure Kh âˆ—Âµ is
deï¬ned as the measure absolutely continuous w.r.t. the Lebesgue measure, with
E Kh(x âˆ’xâ€²)Âµ(dxâ€²),
where âˆ—denotes the convolution operator.
Notice that convolution by Kh preserves the total mass, that is, (Kh âˆ—Âµ)(E) =
Âµ(E) for any Âµ âˆˆM+(E), hence Kh âˆ—Â¯Âµ is the normalized nonnegative measure
(i.e., probability distribution) associated with the unnormalized nonnegative
measure Kh âˆ—Âµ. Moreover, Kh âˆ—Âµ approximates Âµ in the following sense.
LEMMA 6.2.
Let Âµ âˆˆM+(E) be absolutely continuous w.r.t. the Lebesgue
measure, with density dÂµ
dx âˆˆW 2,1. Then
âˆ¥Kh âˆ—Âµ âˆ’Âµâˆ¥â‰¤Î±h2
The proof for the one-dimensional case can be found in , Theorem 7.1,
and for the multidimensional case in , Chapter I, Lemma 4.4, or in ,
Proposition 4.
Given a sample (Î¾1,...,Î¾N) from an unknown probability distribution
Âµ âˆˆP (E) with a smooth density, and given a nonnegative function  which can
F. LE GLAND AND N. OUDJANE
be evaluated at any point of E, we are interested in approximating the posterior
probability distribution  Â· Âµ by a probability distribution with a smooth density.
By construction, the approximation error can be estimated in a strong sense such
as the total variation. Of course, more sample points are needed to approximate
a whole density than are needed to simply approximate moments, as in the weak
sense approximation. Typically, the sample size depends on the dimension, which
is the usual curse of dimensionality. However, getting an approximation of the
whole density is usually worth the effort, as it allows getting meaningful information, for example, conï¬dence regions.
The classical density estimation theory (see, e.g., for the L2 theory and 
for the L1 theory), has extensively dealt with the problem of estimating Âµ alone,
which reduces to our problem in the particular case where  is constant. The
solution consists in regularizing the empirical probability distribution associated
with the sample and provides kernel-type estimators Kh âˆ—SN(Âµ). Minimization
of the mean errors Eâˆ¥Kh âˆ—SN(Âµ) âˆ’Âµâˆ¥or Eâˆ¥Kh âˆ—SN(Âµ) âˆ’Âµâˆ¥2
2, in the L1 or
L2 sense, over the bandwidth h and the regularization kernel K has also been
studied. Similarly, in our more general setting, we propose two estimators for
 Â· Âµ: the preregularized estimator  Â· (Kh âˆ—SN(Âµ)) where regularization occurs
before the correction by , and the postregularized estimator Kh âˆ—( Â· SN(Âµ))
where regularization occurs after the correction by . We immediately see that the
preregularized estimator consists in applying the correction by  to the classical
density estimator Kh âˆ—SN(Âµ) and that both estimators reduce to the classical
density estimator when  is constant. Consequently, we will focus below on
estimating the mean error for the postregularized estimator, and results for the
preregularized estimator will follow immediately. In Proposition 6.3, we consider
the mean error between the unnormalized nonnegative measures Kh âˆ—(SN(Âµ))
and Âµ. In the general case, the error between the corresponding probability
distributions Kh âˆ—( Â· SN(Âµ)) and  Â· Âµ will then be derived using estimate (7),
but in some particular cases we may derive some sharper bounds. In what follows,
we only state some bounds without trying to optimize over the bandwidth h or the
regularization kernel K.
PROPOSITION 6.3.
Let Âµ âˆˆP (E) be absolutely continuous w.r.t. the
Lebesgue measure, with density dÂµ
dx âˆˆW 2,1, and let  be a nonnegative bounded
measurable function deï¬ned on E, with bounded derivatives up to order two. Then
(x) + Î±h2
If in addition I (Âµ) is ï¬nite, then
Nhm (I (Âµ) + hÎ³ )m/2 sup
(x) + Î±h2
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
The proof is based on the following decomposition of the error into variation
and bias errors:
 âˆ’Âµ = Kh âˆ—
 âˆ’Kh âˆ—(Âµ) + Kh âˆ—(Âµ) âˆ’Âµ.
Under the assumptions, the nonnegative measure Âµ is absolutely continuous
w.r.t. the Lebesgue measure, with density dÂµ
dx âˆˆW 2,1, such that
and Lemma 6.2 can be used to bound the bias error. The following lemma is used
to bound the variation error.
LEMMA 6.4.
Let Âµ âˆˆP (E), and let  be a nonnegative bounded measurable
function deï¬ned on E. Then
 âˆ’Kh âˆ—(Âµ),Ï†
If in addition I (Âµ) is ï¬nite, then
 âˆ’Kh âˆ—(Âµ)
I (Âµ) + hÎ³
Using Lemma 5.1 and the symmetry of the regularization kernel,
 âˆ’Kh âˆ—(Âµ),Ï†
SN(Âµ) âˆ’Âµ,(Kh âˆ—Ï†)
âˆ¥(Kh âˆ—Ï†)âˆ¥â‰¤
for any bounded measurable test function Ï† deï¬ned on E, which proves the
estimate in the weak sense.
The proof of the estimate in total variation is classical. By deï¬nition,
 âˆ’Kh âˆ—(Âµ)
h (x) âˆ’fh(x)|dx,
h (x) = d(Kh âˆ—(SN(Âµ)))
Kh(x âˆ’Î¾i)(Î¾i),
fh(x) = d(Kh âˆ—(Âµ))
E Kh(x âˆ’xâ€²)(xâ€²)Âµ(dxâ€²),
F. LE GLAND AND N. OUDJANE
and it follows from the proof of Lemma 5.1 that
h (x) âˆ’fh(x)| â‰¤
h(x âˆ’xâ€²)2(xâ€²)Âµ(dxâ€²)
E Lh(x âˆ’xâ€²)Âµ(dxâ€²)
where the symmetric probability density L = K2
Î²2 satisï¬es K2
hmLh. Therefore
 âˆ’Kh âˆ—(Âµ)
Using the Minkowski inequality yields
I (Lh âˆ—Âµ) =
E |xâ€² + hu|m+1L(u)Âµ(dxâ€²)du
E |xâ€²|m+1Âµ(dxâ€²)
E |u|m+1L(u)du
â‰¤I (Âµ) + hI (L),
and using estimate (29) yields
â‰¤Am(I (Lh âˆ—Âµ))m/2 â‰¤Am
I (Âµ) + hÎ³
REMARK 6.5.
If the probability distribution Âµ is absolutely continuous w.r.t.
the Lebesgue measure, with probability density f , then the following more precise
bound holds:
= J(Lh âˆ—f ) â‰¤J(f ) + J(|Lh âˆ—f âˆ’f |),
where J(|Lh âˆ—f âˆ’f |) goes to zero when h â†“0 (see , Proposition 8).
REMARK 6.6.
If in addition Ï†,  and Âµ are F -measurable r.v.s, and if
conditionally w.r.t. F the r.v.s (Î¾1,...,Î¾N) are i.i.d. with (conditional) probability
distribution Âµ, then the same estimates hold for conditional expectation w.r.t. F ,
(x) + Î±h2
I (Âµ) + hÎ³
(x) + Î±h2
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
6.2. Preregularized particle ï¬lter.
denote the preregularized particle
ï¬lter (pre-RPF) approximation of Âµn. Initially ÂµN,h
= Âµ0, and the transition from
nâˆ’1 to ÂµN,h
is described by the following diagram:
prediction
n|nâˆ’1 = SN(QnÂµN,h
preregularized
correction
= n Â· (Kh âˆ—ÂµN,h
In practice, the particle approximation
is completely characterized by the particle system {Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1}, and
the transition from {Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1} to {Î¾1
n+1|n,...,Î¾N
n+1|n} consists of the
following steps.
Step (i) (Preregularized correction). If the normalization constant
(Kh âˆ—n)(Î¾i
is positive, then set
n Â· (Kh âˆ—ÂµN,h
n(x)Kh(x âˆ’Î¾i
otherwise set ÂµN,h
Step (ii) (Sampled prediction). Independently for all i = 1,...,N, generate
n+1|n âˆ¼Qn+1 ÂµN,h
n+1|n = SN(Qn+1ÂµN,h
The resampling step (ii) requires generating random variables either according
to the arbitrary restarting probability distribution Î½, or according to a complex
probability distribution known up to a normalization constant, which can be done
with a rejection algorithm (see ), or with the more efï¬cient local rejection
algorithm (see ), if the kernel K has compact support. In any case, the
implementation is less straightforward than for the IPF.
Notice that the pre-RPF satisï¬es (16).
ASSUMPTION R (Regularity of the Markov kernel).
For any Âµ âˆˆP (E), the
probability distribution QkÂµ is absolutely continuous w.r.t. the Lebesgue measure,
with density in W 2,1, and
F. LE GLAND AND N. OUDJANE
REMARK 6.7.
Assumption R is equivalent to supposing that the probability
distribution Qk(x,Â·) is absolutely continuous w.r.t. the Lebesgue measure, with
density qk(x,Â·) in W 2,1 for any x âˆˆE, and
|qk(x,Â·)|2,1 < âˆ.
ASSUMPTION M (Existence of moments).
I (QkÂµ) < âˆ.
REMARK 6.8.
Assumption M is equivalent to supposing that
E |xâ€²|m+1Qk(x,dxâ€²)
Alternatively, if the Markov kernel Qk is mixing, and I (Âµk|kâˆ’1) is ï¬nite, then
I (Âµk|kâˆ’1) < âˆ.
THEOREM 6.9.
If for any k â‰¥1, Assumptions L and R hold, and the
nonnegative kernel Rk is mixing, then the pre-RPF estimator satisï¬es
|âŸ¨Âµn âˆ’ÂµN,h
where for any k â‰¥1,
If in addition for any k â‰¥1, Assumption M holds, then
where for any k â‰¥1,
Nhm (Ik + hÎ³ )m/2 + Î±h2Dk
The convergence result stated in Theorem 6.9 would still hold with a time
dependent bandwidth, and with a time dependent number of particles.
PROOF OF THEOREM 6.9.
To prove the estimate in the weak sense, it is
sufï¬cient to bound the local error Î´W
k in the weak sense, and to apply Theorem 4.8.
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
Since Rk is mixing, âŸ¨QkÂµN,h
kâˆ’1,kâŸ©> 0 in view of Remark 5.7. Using estimate (6)
kâˆ’1),Ï†âŸ©| =
Kh âˆ—SN(QkÂµN,h
 âˆ’k Â· (QkÂµN,h
â‰¤|âŸ¨Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
+ |âŸ¨Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
for any bounded measurable test function Ï† deï¬ned on E. By deï¬nition,
ÂµâˆˆP(E)âŸ¨QkÂµ,kâŸ©.
Using estimate (30) with  â‰¡1, Âµ = QkÂµN,h
kâˆ’1 and F = Ïƒ(Y1: k, ÂµN,h
kâˆ’1), yields
|âŸ¨Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
Y1: k, ÂµN,h
To prove the estimate in total variation, it is sufï¬cient to bound the local
in total variation, and to apply Theorem 4.6. Using estimate (7) yields
Kh âˆ—SN(QkÂµN,h
 âˆ’k Â· (QkÂµN,h
â‰¤2 supxâˆˆE k(x)
Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
Using estimate (31) with  â‰¡1, Âµ = QkÂµN,h
kâˆ’1 and F = Ïƒ(Y1: k, ÂµN,h
kâˆ’1), yields
Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
Y1: k, ÂµN,h
m/2 + Î±h2
Nhm (Ik + hÎ³ )m/2 + Î±h2Dk.
F. LE GLAND AND N. OUDJANE
In the pre-RPF, the correction is applied directly to a regularized probability
distribution, hence each point in the support of the regularized density is updated,
and in principle the degeneracy of particle weights which occurs when the
correction is applied to a discrete probability distribution, as in the IPF, is now
avoided. This intuition is supported by numerical evidence, and by the following
theorem, which shows that it is possible to control the local error, averaged over
observation sequences, independently of any lower bound assumption on the
likelihood functions (notice that the a.s. bounds of Theorem 6.9 still depend on
the integrated lower bounds of the likelihood functions).
THEOREM 6.10.
If for any k â‰¥1, Assumptions R and M hold, and the
nonnegative kernel Rk is mixing with nonrandom Îµk, then the pre-RPF estimator
Eâˆ¥Âµn âˆ’ÂµN,h
where for any k â‰¥1,
Nhm(Ik + hÎ³ )m/2 + Î±h2Dk
COROLLARY 6.11.
If for any k â‰¥1, the nonnegative kernel Rk is mixing with
Îµk â‰¥Îµ > 0 and nonrandom Îµ, and Assumptions R and M hold with Dk â‰¤D and
Ik â‰¤I, then convergence, averaged over observation sequences, holds uniformly
in time, that is,
Eâˆ¥Âµn âˆ’ÂµN,h
Nhm (I + hÎ³ )m/2 + Î±h2D
Both the SPF (see Theorem 5.14) and the pre-RPF allow bounding the error
independently of any lower bound assumption on the likelihood functions, and the
computational time of both algorithms is random [recall that a rejection is needed
in the resampling step (ii) of the pre-RPF].
PROOF OF THEOREM 6.10.
It is sufï¬cient to bound the local error E[Î´TV
total variation, averaged over observation sequences, and to apply Theorem 4.6. If
the nonnegative kernel Rk is mixing, then
kâˆ’1,kâŸ©â‰¥Îµ2
kâŸ¨Âµk|kâˆ’1,kâŸ©,
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
in view of Remark 5.7, with nonrandom Îµk, hence using inequality (7) yields
âŸ¨Âµk|kâˆ’1,kâŸ©|Âµk|(dx),
Âµk = Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
It follows from Remark 2.2 that
âŸ¨Âµk|kâˆ’1,kâŸ©
Y1: kâˆ’1, ÂµN,h
k|kâˆ’1, ÂµN,h
âŸ¨Âµk|kâˆ’1,kâŸ©
Y1: kâˆ’1
Y1: kâˆ’1, ÂµN,h
k|kâˆ’1, ÂµN,h
Using estimate (31) with  â‰¡1, Âµ = Qk ÂµN,h
kâˆ’1 and F = Ïƒ(ÂµN,h
kâˆ’1), yields
E[|Âµk|(E)|ÂµN,h
Kh âˆ—SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
m/2 + Î±h2
Nhm (Ik + hÎ³ )m/2 + Î±h2Dk.
REMARK 6.12.
In the same way as for the SPF (see Theorem 5.14), one
could think of using a random number of particles so as to avoid any lower
bound assumption on the likelihood functions. However, for the pre-RPF, it is not
sufï¬cient to evaluate the quantity k(Î¾i
k|kâˆ’1) for each simulated particle, as in (28),
but one has to evaluate the integral
(Kh âˆ—k)(Î¾i
E k(x)Kh(x âˆ’Î¾i
instead. This evaluation is in general very costly, which makes the idea of using a
random number of particles for the pre-RPF rather unpractical.
6.3. Post-regularized particle ï¬lter.
denote the post-regularized
particle ï¬lter (post-RPF) approximation of Âµn. Initially ÂµN,h
= Âµ0, and the
transition from ÂµN,h
nâˆ’1 to ÂµN,h
is described by the following diagram:
prediction
n|nâˆ’1 = SN(QnÂµN,h
postregularized
correction
= Kh âˆ—(n Â· ÂµN,h
F. LE GLAND AND N. OUDJANE
In practice, the particle approximation
is completely characterized by the particle system {Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1}, and
the transition from {Î¾1
n|nâˆ’1,...,Î¾N
n|nâˆ’1} to {Î¾1
n+1|n,...,Î¾N
n+1|n} consists of the
following steps.
Step (i) (Post-regularized correction). If the normalization constant
is positive, then for all i = 1,...,N, compute the weight
Kh âˆ—(n Â· ÂµN,h
otherwise set ÂµN,h
Step (ii) (Sampled prediction). Independently for all i = 1,...,N, generate a
n+1|n âˆ¼Qn+1ÂµN,h
n+1|n = SN(Qn+1ÂµN,h
The resampling step (ii) can be easily implemented: it requires generating
random variables according to a mixture of rescaled kernels, where the mixing
probability distribution is either a weighted discrete probability distribution or the
arbitrary restarting probability distribution Î½.
Notice that the post-RPF satisï¬es (16).
ASSUMPTION Lâ€²â€².
REMARK 6.13.
If k is bounded, with bounded derivatives up to order two,
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
If Assumption Lâ€²â€² holds, and if for any k â‰¥1 the nonnegative kernel Rk is
mixing, then the following notation is introduced:
k := supuâˆˆW 2,1(|ku|2,1/âˆ¥uâˆ¥2,1)
infÂµâˆˆP(E)âŸ¨QkÂµ,kâŸ©
and in view of Remark 5.7, Ïâ€²â€²
k is a.s. ï¬nite.
ASSUMPTION Râ€²â€² (Additional regularity of the Markov kernel).
Âµ âˆˆP (E), the probability distribution QkÂµ is absolutely continuous w.r.t. the
Lebesgue measure, with density in W 2,1, and
REMARK 6.14.
Assumption Râ€²â€² is equivalent to supposing that the probability
distribution Qk(x,Â·) is absolutely continuous w.r.t. the Lebesgue measure, with
density qk(x,Â·) in W 2,1 for any x âˆˆE, and
âˆ¥qk(x,Â·)âˆ¥2,1 < âˆ.
THEOREM 6.15.
If for any k â‰¥1, Assumptions Lâ€²â€² and Râ€²â€² hold and the
nonnegative kernel Rk is mixing, then the post-RPF estimator satisï¬es
|âŸ¨Âµn âˆ’ÂµN,h
where for any k â‰¥1,
2Ïk + Î±h2Dâ€²â€²
If in addition for any k â‰¥1, Assumption M holds, then
where for any k â‰¥1,
Nhm (Ik + hÎ³ )1/2
Ïk + Î±h2Dâ€²â€²
The proof is similar to the proof of Theorem 6.9 except that estimates
(30) and (31) are used here with  = k. By deï¬nition,
k Â· SN(QkÂµN,h
Kh âˆ—(kSN(Qk ÂµN,h
âŸ¨SN(QkÂµN,h
SN(QkÂµN,h
otherwise,
F. LE GLAND AND N. OUDJANE
and since convolution by Kh preserves the total mass,
kSN(QkÂµN,h
SN(QkÂµN,h
Finally, recall that the bounds in estimates (6) and (7) do not depend on the
restarting probability distribution.
To prove the estimate in the weak sense, it is sufï¬cient to bound the local
in the weak sense, and to apply Theorem 4.8. Since Rk is mixing,
kâˆ’1,kâŸ©> 0 in view of Remark 5.7. Using estimate (6) yields
k Â· SN(QkÂµN,h
 âˆ’k(QkÂµN,h
â‰¤|âŸ¨Kh âˆ—(kSN(QkÂµN,h
kâˆ’1)) âˆ’k(QkÂµN,h
+ |âŸ¨SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
for any bounded measurable test function Ï† deï¬ned on E. By deï¬nition,
ÂµâˆˆP(E)âŸ¨QkÂµ,kâŸ©.
Under Assumption Râ€²â€²,
Using estimate (30) with  = k, Âµ = QkÂµN,h
kâˆ’1 and F = Ïƒ(Y1: k, ÂµN,h
kâˆ’1), yields
kSN(QkÂµN,h
 âˆ’k(QkÂµN,h
Y1: k, ÂµN,h
Using estimate (24) with Ï† = k, Âµ = QkÂµN,h
kâˆ’1 and F = Ïƒ(Y1: k, ÂµN,h
kâˆ’1), yields
SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
Y1: k, ÂµN,h
To prove the estimate in total variation, it is sufï¬cient to bound the local
in total variation and to apply Theorem 4.6. Since convolution by Kh
STABILITY AND UNIFORM APPROXIMATION OF NONLINEAR FILTERS
preserves the total mass, using estimate (7) yields
k Â· SN(QkÂµN,h
 âˆ’k Â· (QkÂµN,h
â‰¤âˆ¥Kh âˆ—(kSN(QkÂµN,h
kâˆ’1)) âˆ’k(QkÂµN,h
+ |âŸ¨SN(QkÂµN,h
kâˆ’1) âˆ’QkÂµN,h
Using estimate (31) with  = k, Âµ = QkÂµN,h
kâˆ’1 and F = Ïƒ(Y1: k, ÂµN,h
kâˆ’1), yields
kSN(QkÂµN,h
 âˆ’k(QkÂµN,h
Y1: k, ÂµN,h
k(x) + Î±h2
Nhm (Ik + hÎ³ )m/2 sup
k(x) + Î±h2Dâ€²â€²
Acknowledgments.
The ï¬rst author gratefully thanks Marion Baudry,
Natacha Caylus and Arnaud Guyader for their careful reading of earlier versions
of this work.