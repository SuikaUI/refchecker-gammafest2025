Adversarial Attacks on Neural Networks for Graph Data
Daniel Z√ºgner
Amir Akbarnejad
Stephan G√ºnnemann
Technical University of Munich, Germany
{zuegnerd,amir.akbarnejad,guennemann}@in.tum.de
Deep learning models for graphs have achieved strong performance
for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks.
Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily
fooled? In this work, we introduce the first study of adversarial
attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test
time, we tackle the more challenging class of poisoning/causative
attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the
node‚Äôs features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the
perturbations remain unnoticeable by preserving important data
characteristics. To cope with the underlying discrete domain we
propose an efficient algorithm Nettack exploiting incremental
computations. Our experimental study shows that accuracy of node
classification significantly drops even when performing only few
perturbations. Even more, our attacks are transferable: the learned
attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even
when only limited knowledge about the graph is given.
Adversarial machine learning, graph mining, network mining,
graph convolutional networks, semi-supervised learning
INTRODUCTION
Graph data is the core for many high impact applications ranging
from the analysis of social and rating networks (Facebook, Amazon),
over gene interaction networks (BioGRID), to interlinked document
collections (PubMed, Arxiv). One of the most frequently applied
tasks on graph data is node classification: given a single large (attributed) graph and the class labels of a few nodes, the goal is to predict
the labels of the remaining nodes. For example, one might wish to
classify the role of a protein in a biological interaction graph ,
predict the customer type of users in e-commerce networks ,
or assign scientific papers from a citation network into topics .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from .
KDD ‚Äô18, August 19‚Äì23, 2018, London, United Kingdom
¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08...$15.00
 
Train node classification model
Target gets
misclassified
perturbation
target node
attacker node
Nettack-In.
ClassiÔ¨Åcation margin
misclassiÔ¨Åed
Results for attacking Citeseer data
Figure 1: Small perturbations of the graph structure and
node features lead to misclassification of the target.
While many classical approaches have been introduced in the
past to tackle the node classification problem , the last years
have seen a tremendous interest in methods for deep learning on
graphs . Specifically, approaches from the class of graph
convolutional networks have achieved strong performance
in many graph-learning tasks including node classification.
The strength of these methods ‚Äî beyond their non-linear, hierarchical nature ‚Äì relies on their use of the graphs‚Äô relational
information to perform classification: instead of only considering
the instances individually (nodes and their features), the relationships between them are exploited as well (the edges). Put differently:
the instances are not treated independently; we deal with a certain
form of non-i.i.d. data where so-called network effects such as
homophily support the classification.
However, there is one big catch: Many researchers have noticed that deep learning architectures for classical learning tasks
can easily be fooled/attacked . Even only slight, deliberate
perturbations of an instance ‚Äì also known as adversarial perturbations/examples ‚Äì can lead to wrong predictions. Such negative
results significantly hinder the applicability of these models, leading to unintuitive and unreliable results, and they additionally open
the door for attackers that can exploit these vulnerabilities. So far,
however, the question of adversarial perturbations for deep learning
methods on graphs has not been addressed. This is highly critical,
since especially in domains where graph-based learning is used (e.g.
the web) adversaries are common and false data is easy to inject:
spammers add wrong information to social networks; fraudsters
frequently manipulate online reviews and product websites .
In this work, we close this gap and we investigate whether such
manipulations are possible. Can deep learning models for attributed
graphs be easily fooled? How reliable are their results?
The answer to this question is indeed not foreseeable: On one
hand the relational effects might improve robustness since predictions are not based on individual instances only but based on
various instances jointly. On the other hand, the propagation of
information might also lead to cascading effects, where manipulating a single instance affects many others. Indeed, compared to
the existing works on adversarial attacks, our work significantly
differs in various aspects.
 
Opportunities: (1) Since we are operating on an attributed
graph, adversarial perturbations can manifest in two different ways:
by changing the nodes‚Äô features or the graph structure. Manipulating the graph, i.e. the dependency structure between instances, has
not been studied so far, but is a highly likely scenario in real-life.
For example, one might add or remove (fake) friendship relations
to a social network. (2) While existing works were limited to manipulating an instance itself to enforce its wrong prediction1, the
relational effects give us more power: by manipulating one instance,
we might specifically misguide the prediction for another instance.
Again, this scenario is highly realistic. Think about a fraudster
who hijacks some accounts, which he then manipulates to enforce
a wrong prediction for another account he has not under control.
Thus, in graph-based learning scenarios we can distinguish between
(i) nodes which we aim to misclassify, called targets, and (ii) nodes
which we can directly manipulate, called attackers. Figure 1 illustrates the goal of our work and shows the result of our method on
the Citeseer network. Clearly, compared to classical attacks to learning models, graphs enable much richer potential for perturbations.
But likewise, constructing them is far more challenging.
Challenges: (1) Unlike, e.g., images consisting of continuous
features, the graph structure ‚Äì and often also the nodes‚Äô features ‚Äì
is discrete. Therefore, gradient based approaches for finding
perturbations are not suited. How to design efficient algorithms that
are able to find adversarial examples in a discrete domain? (2) Adversarial perturbations are aimed to be unnoticeable (by humans).
For images, one often enforces, e.g., a maximum deviation per pixel
value. How can we capture the notion of ‚Äôunnoticeable changes‚Äô in a
(binary, attributed) graph? (3) Last, node classification is usually
performed in a transductive learning setting. Here, the train and test
data are used jointly to learn a new classification model before the
predictions are performed on the specific test data. This means, that
the predominantly performed evasion attacks ‚Äì where the parameters of the classification model are assumed to be static ‚Äì are not
realistic. The model has to be (re)trained on the manipulated data.
Thus, graph-based learning in a transductive setting is inherently
related to the challenging poisoning/causative attacks .
Given these challenges, we propose a principle for adversarial
perturbations of attributed graphs that aim to fool state-of-the
art deep learning models for graphs. In particular, we focus on
semi-supervised classification models based on graph convolutions
such as GCN and Column Network (CLN) ‚Äì but we will
also showcase our methods‚Äô potential on the unsupervised model
DeepWalk . By default, we assume an attacker with knowledge
about the full data, which can, however, only manipulate parts of
it. This assumption ensures reliable vulnerability analysis in the
worst case. But even when only parts of the data are known, our
attacks are still successful as shown by our experiments. Overall,
our contributions are:
‚Ä¢ Model: We propose a model for adversarial attacks on attributed graphs considering node classification. We introduce new
types of attacks where we explicitly distinguish between the
attacker and the target nodes. Our attacks can manipulate the
1Due to the independence assumption, a misclassification for instance ùëñcan only be
achieved by manipulating instance ùëñitself for the commonly studied evasion (test-time)
attacks. For the less studied poisioning attacks we might have indirect influence.
graph structure and node features while ensuring unnoticeable
changes by preserving important data characteristics (e.g. degree distribution, co-occurence of features).
‚Ä¢ Algorithm: We develop an efficient algorithm Nettack for computing these attacks based on linearization ideas. Our methods
enables incremental computations and exploits the graph‚Äôs sparsity for fast execution.
‚Ä¢ Experiments: We show that our model can dramatically worsen
classification results for the target nodes by only requiring few
changes to the graph. We furthermore show that these results
transfer to other established models, hold for various datasets,
and even work when only parts of the data are observed. Overall,
this highlights the need to handle attacks to graph data.
PRELIMINARIES
We consider the task of (semi-supervised) node classification in
a single large graph having binary node features. Formally, let
ùê∫= (ùê¥,ùëã) be an attributed graph, where ùê¥‚àà{0, 1}ùëÅ√óùëÅis the
adjacency matrix representing the connections and ùëã‚àà{0, 1}ùëÅ√óùê∑
represents the nodes‚Äô features. We denote with ùë•ùë£‚àà{0, 1}ùê∑the
ùê∑-dim. feature vector of node ùë£. W.l.o.g. we assume the node-ids to
be V = {1, . . . , ùëÅ} and the feature-ids to be F = {1, ..., ùê∑}.
Given a subset Vùêø‚äÜV of labeled nodes, with class labels
from C = {1, 2, . . . ,ùëêùêæ}, the goal of node classification is to learn
a function ùëî: V ‚ÜíC which maps each node ùë£‚ààV to one class
in C.2 Since the predictions are done for the given test instances,
which are already known before (and also used during) training,
this corresponds to a typical transductive learning scenario .
In this work, we focus on node classification employing graph
convolution layers. In particular, we will consider the well established work . Here, the hidden layer ùëô+ 1 is defined as
ùêª(ùëô+1) = ùúé
2 ùêª(ùëô)ùëä(ùëô)
where Àúùê¥= ùê¥+ ùêºùëÅis the adjacency matrix of the (undirected) input
graph ùê∫after adding self-loops via the identity matrix ùêºùëÅ. ùëä(ùëô)
is the trainable weight matrix of layer ùëô, Àúùê∑ùëñùëñ= √ç
ùëóÀúùê¥ùëñùëó, and ùúé(¬∑) is
an activation function (usually ReLU). In the first layer we have
ùêª(0) = ùëã, i.e. using the nodes‚Äô features as input. Since the latent
representations ùêªare (recursively) relying on the neighboring ones
(multiplication with Àúùê¥), all instances are coupled together. Following the authors of , we consider GCNs with a single hidden
ùëç= ùëìùúÉ(ùê¥,ùëã) = softmax
where ÀÜùê¥= Àúùê∑‚àí1
2 . The output ùëçùë£ùëêdenotes the probability of
assigning node ùë£to class ùëê. Here, we used ùúÉdo denote the set of
all parameters, i.e. ùúÉ= {ùëä(1),ùëä(2)}. The optimal parameters ùúÉ
are then learned in a semi-supervised fashion by minimizing crossentropy on the output of the labeled samples Vùêø, i.e. minimizing
ùêø(ùúÉ;ùê¥,ùëã) = ‚àí
ùëç= ùëìùúÉ(ùê¥,ùëã)
2Please note the difference to (structured) learning settings where we have multiple
but independent graphs as training input with the goal to perform a prediction for each
graph. In this work, the prediction is done per node (e.g. a person in a social network)
‚Äì and especially we have dependencies between the nodes/data instances via the edges.
where ùëêùë£is the given label of ùë£from the training set. After training,
ùëçdenotes the class probabilities for every instance in the graph.
RELATED WORK
In line with the focus of this work, we briefly describe deep learning
methods for graphs aiming to solve the node classification task.
Deep Learning for Graphs. Mainly two streams of research
can be distinguished: (i) node embeddings ‚Äì that often
operate in an unsupervised setting ‚Äì and (ii) architectures employing layers specifically designed for graphs . In this work,
we focus on the second type of principles and additionally show
that our adversarial attack transfers to node embeddings as well.
Regarding the developed layers, most works seek to adapt conventional CNNs to the graph domain: called graph convolutional layers
or neural message passing . Simply speaking,
they reduce to some form of aggregation over neighbors as seen in
Eq. (2). A more general setting is described in and an overview
of methods given in .
Adversarial Attacks. Attacking machine learning models has
a long history, with seminal works on, e.g., SVMs or logistic regression . In contrast to outliers, e.g. present in attributed graphs
 , adversarial examples are created deliberately to mislead machine learning models and often are designed to be unnoticeable.
Recently, deep neural networks have shown to be highly sensitive
to these small adversarial perturbations to the data . Even
more, the adversarial examples generated for one model are often
also harmful when using another model: known as transferability
 . Many tasks and models have been shown to be sensitive to
adversarial attacks; however, all assume the data instances to be independent. Even , which considers relations between different
tasks for multi-task relationship learning, still deals with the classical scenario of i.i.d. instances within each task. For interrelated
data such as graphs, where the data instances (i.e. nodes) are not
treated independently, no such analysis has been performed yet.
Taxonomies characterizing the attack have been introduced in
 . The two dominant types of attacks are poisoning/causative
attacks which target the training data (specifically, the model‚Äôs training phase is performed after the attack) and evasion/exploratory attacks which target the test data/application phase (here, the learned
model is assumed fixed). Deriving effective poisoning attacks is
usually computationally harder since also the subsequent learning
of the model has to be considered. This categorization is not optimally suited for our setting. In particular, attacks on the test data are
causative as well since the test data is used while training the model
(transductive, semi-supervised learning). Further, even when the
model is fixed (evasion attack), manipulating one instance might
affect all others due to the relational effects imposed by the graph
structure. Our attacks are powerful even in the more challenging
scenario where the model is retrained.
Generating Adversarial Perturbations. While most works
have focused on generating adversarial perturbations for evasion
attacks, poisoning attacks are far less studied since
they require to solve a challenging bi-level optimization problem that considers learning the model. In general, since finding
adversarial perturbations often reduces to some non-convex (bilevel) optimization problem, different approximate principles have
been introduced. Indeed, almost all works exploit the gradient or
other moments of a given differentiable (surrogate) loss function to
guide the search in the neighborhood of legitimate perturbations
 . For discrete data, where gradients are undefined,
such an approach is suboptimal.
Hand in hand with the attacks, the robustification of machine
learning models has been studied ‚Äì known as adversarial machine
learning or robust machine learning. Since this is out of the scope
of the current paper, we do not discuss these approaches here.
Adversarial Attacks when Learning with Graphs. Works on
adversarial attacks for graph learning tasks are almost non-existing.
For graph clustering, the work has measured the changes in the
result when injecting noise to a bi-partite graph that represent DNS
queries. Though, they do not focus on generating attacks in a principled way. Our work considered noise in the graph structure
to improve the robustness when performing spectral clustering.
Similarly, to improve robustness of collective classification via associative Markov networks, the work considers adversarial noise
in the features. They only use label smoothness and assume that
the attacker can manipulate the features of every instance. After
our work was published, introduced the second approach for
adversarial attacks on graphs, where they exploit reinforcement
learning ideas. However, in contrast to our work, they do not consider poisoning attacks nor potential attribute perturbations. They
further restrict to edge deletions for node classification ‚Äì while we
also handle edge insertions. In addition, in our work we even show
that the attacks generated by our strategy successfully transfer to
different models than the one attacked. Overall, no work so far has
considered poisoning/training-time attacks on neural networks for
attributed graphs.
ATTACK MODEL
Given the node classification setting as described in Sec. 2, our goal
is to perform small perturbations on the graph ùê∫(0) = (ùê¥(0),ùëã(0)),
leading to the graph ùê∫‚Ä≤ = (ùê¥‚Ä≤,ùëã‚Ä≤), such that the classification
performance drops. Changes to ùê¥(0), are called structure attacks,
while changes to ùëã(0) are called feature attacks.
Target vs. Attackers. Specifically, our goal is to attack a specific
target node ùë£0 ‚ààV, i.e. we aim to change ùë£0‚Äôs prediction. Due to the
non-i.i.d. nature of the data, ùë£0‚Äôs outcome not only depends on the
node itself, but also on the other nodes in the graph. Thus, we are
not limited to perturbing ùë£0 but we can achieve our aim by changing
other nodes as well. Indeed, this reflects real world scenarios much
better since it is likely that an attacker has access to a few nodes only,
and not to the entire data or ùë£0 itself. Therefore, besides the target
node, we introduce the attacker nodes A ‚äÜV. The perturbations
on ùê∫(0) are constrained to these nodes, i.e. it must hold
ùë¢ùë£‚áíùë¢‚ààA ‚à®ùë£‚ààA
If the target ùë£0 ‚àâA, we call the attack an influencer attack,
since ùë£0 gets not manipulated directly, but only indirectly via some
influencers. If {ùë£0} = A, we call it a direct attack.
To ensure that the attacker can not modify the graph completely,
we further limit the number of allowed changes by a budget Œî:
More advanced ideas will be discussed in Sec. 4.1. For now, we
denote with Pùê∫0
Œî,A the set of all graphs ùê∫‚Ä≤ that fulfill Eq. (4) and (5).
Given this basic set-up, our problem is defined as:
Problem 1. Given a graph ùê∫(0) = (ùê¥(0),ùëã(0)), a target node ùë£0,
and attacker nodes A. Let ùëêùëúùëôùëëdenote the class for ùë£0 based on the
graph ùê∫(0) (predicted or using some ground truth). Determine
(ùê¥‚Ä≤,ùëã‚Ä≤) ‚ààPùê∫0
ùëê‚â†ùëêùëúùëôùëëlnùëç‚àó
subject to ùëç‚àó= ùëìùúÉ‚àó(ùê¥‚Ä≤,ùëã‚Ä≤) with ùúÉ‚àó= arg min
ùêø(ùúÉ;ùê¥‚Ä≤,ùëã‚Ä≤)
That is, we aim to find a perturbed graph ùê∫‚Ä≤ that classifies ùë£0 as
ùëêùëõùëíùë§and has maximal ‚Äôdistance‚Äô (in terms of log-probabilities/logits)
to ùëêùëúùëôùëë. Note that for the perturbated graph ùê∫‚Ä≤, the optimal parameters ùúÉ‚àóare used, matching the transductive learning setting where
the model is learned on the given data. Therefore, we have a bi-level
optimization problem. As a simpler variant, one can also consider
an evasion attack assuming the parameters are static and learned
based on the old graph, ùúÉ‚àó= arg minùúÉùêø(ùúÉ;ùê¥(0),ùëã(0)).
Unnoticeable Perturbations
Typically, in an adversarial attack scenario, the attackers try to
modify the input data such that the changes are unnoticable. Unlike
to image data, where this can easily be verified visually and by using
simple constraints, in the graph setting this is much harder mainly
for two reasons: (i) the graph structure is discrete preventing to use
infinitesimal small changes, and (ii) sufficiently large graphs are
not suitable for visual inspection.
How can we ensure unnoticeable perturbations in our setting? In
particular, we argue that only considering the budget Œî might not be
enough. Especially if a large Œî is required due to complicated data,
we still want realistically looking perturbed graphs ùê∫‚Ä≤. Therefore,
our core idea is to allow only those perturbations that preserve
specific inherent properties of the input graph.
Graph structure preserving perturbations. Undoubtedly, the
most prominent characteristic of the graph structure is its degree
distribution, which often resembles a power-law like shape in real
networks. If two networks show very different degree distributions,
it is easy to tell them apart. Therefore, we aim to only generate perturbations which follow similar power-law behavior as the input.
For this purpose we refer to a statistical two-sample test for
power-law distributions . That is, we estimate whether the two
degree distributions of ùê∫(0) and ùê∫‚Ä≤ stem from the same distribution
or from individual ones, using a likelihood ratio test.
More precisely, the procedure is as follows: We first estimate
the scaling parameter ùõºof the power-law distribution ùëù(ùë•) ‚àùùë•‚àíùõº
referring to the degree distribution of ùê∫(0) (equivalently for ùê∫‚Ä≤).
While there is no exact and closed-form solution to estimate ùõºin
the case of discrete data, derived an approximate expression,
which for our purpose of a graph ùê∫translates to
ùõºùê∫‚âà1 + |Dùê∫| ¬∑
where ùëëùëöùëñùëõdenotes the minimum degree a node needs to have to
be considered in the power-law test and Dùê∫= {ùëëùê∫ùë£| ùë£‚ààV,ùëëùê∫ùë£‚â•
ùëëùëöùëñùëõ} is the multiset containing the list of node degrees, where ùëëùê∫ùë£
is the degree of node ùë£in ùê∫. Using this, we get estimates for the
values ùõºùê∫(0) and ùõºùê∫‚Ä≤. Similarly, we can estimate ùõºùëêùëúùëöùëèusing the
combined samples Dùëêùëúùëöùëè= Dùê∫(0) ‚à™Dùê∫‚Ä≤.
Given the scaling parameter ùõºùë•, the log-likelihood for the samples Dùë•can easily be evaluated as
ùëô(Dùë•) = |Dùë•|¬∑logùõºùë•+|Dùë•|¬∑ùõºùë•¬∑logùëëmin‚àí(ùõºùë•+1)
Using these log-likelihood scores, we set up the significance test,
estimating whether the two samples Dùê∫(0) and Dùê∫‚Ä≤ come from the
same power law distribution (null hypotheses ùêª0) as opposed to
separate ones (ùêª1). That is, we formulate two competing hypotheses
ùëô(ùêª0) = ùëô(Dùëêùëúùëöùëè)
ùëô(ùêª1) = ùëô(Dùê∫(0) ) + ùëô(Dùê∫‚Ä≤)
Following the likelihood ratio test, the final test statistic is
Œõ(ùê∫(0),ùê∫‚Ä≤) = ‚àí2 ¬∑ ùëô(ùêª0) + 2 ¬∑ ùëô(ùêª1).
which for large sample sizes follows a ùúí2 distribution with one
degree of freedom .
A typical ùëù-value for rejecting the null hypothesis ùêª0 (i.e. concluding that both samples come from different distributions) is
0.05, i.e., statistically, in one out of twenty cases we reject the null
hypothesis although it holds (type I error). In our adversarial attack scenario, however, we argue that a human trying to find out
whether the data has been manipulated would be far more conservative and ask the other way: Given that the data was manipulated,
what is the probability of the test falsely not rejecting the null
hypothesis (type II error).
While we cannot compute the type II error in our case easily,
type I and II error probabilities have an inverse relation in general.
Thus, by selecting a very conservative ùëù-value corresponding to a
high type I error, we can reduce the probability of a type II error.
We therefore set the critical ùëù-value to 0.95, i.e. if we were to sample
two degree sequences from the same power law distribution, we
were to reject the null hypothesis in 95% of the times and could
then investigate whether the data has been compromised based on
this initial suspicion. On the other hand, if our modified graph‚Äôs
degree sequence passes this very conservative test, we conclude
that the changes to the degree distribution are unnoticeable.
Using the above ùëù-value in the ùúí2 distribution, we only accept
perturbations ùê∫‚Ä≤ = (ùê¥‚Ä≤,ùëã‚Ä≤) where the degree distribution fulfills
Œõ(ùê∫(0),ùê∫‚Ä≤) < ùúè‚âà0.004
Feature statistics preserving perturbations. While the above
principle could be applied to the nodes‚Äô features as well (e.g. preserving the distribution of feature occurrences), we argue that such
a procedure is too limited. In particular, such a test would not well
reflect the correlation/co-occurence of different features: If two
features have never occured together in ùê∫(0), but they do once in
ùê∫‚Ä≤, the distribution of feature occurences would still be very similar.
Such a change, however, is easily noticable. Think, e.g., about two
words which have never been used together but are suddenly used
in ùê∫‚Ä≤. Thus, we refer to a test based on feature co-occurrence.
Since designing a statistical test based on the co-occurences requires to model the joint distribution over features ‚Äì intractable
for correlated multivariate binary data ‚Äì we refer to a deterministic test. In this regard, setting features to 0 is uncritical since
it does not introduce new co-occurences. The question is: Which
features of a node ùë¢can be set to 1 to be regarded unnoticable?
To answer this question, we consider a probabilistic random
walker on the co-occurence graph ùê∂= (F, ùê∏) of features from ùê∫(0),
i.e. F is the set of features and ùê∏‚äÜF √ó F denotes which features
have occurred together so far. We argue that adding a feature ùëñis
unnoticeable if the probability of reaching it by a random walker
starting at the features originally present for nodeùë¢and performing
one step is significantly large. Formally, let ùëÜùë¢= {ùëó| ùëãùë¢ùëó‚â†0} be
the set of all features originally present for node ùë¢. We consider
addition of feature ùëñ‚àâùëÜùë¢to node ùë¢as unnoticeable if
ùëù(ùëñ| ùëÜùë¢) =
1/ùëëùëó¬∑ ùê∏ùëñùëó> ùúé.
where ùëëùëódenotes the degree in the co-occurrence graph ùê∂. That is,
given that the probabilistic walker has started at any feature ùëó‚ààùëÜùë¢,
after performing one step it would reach the feature ùëñat least with
probability ùúé. In our experiments we simply picked ùúéto be half of
the maximal achievable probably, i.e. ùúé= 0.5 ¬∑
The above principle has two desirable effects: First, features ùëñ
which have co-occurred with many of ùë¢‚Äôs features (i.e. in other
nodes) have a high probability; they are less noticeable when being
added. Second, features ùëñthat only co-occur with features ùëó‚ààùëÜùë¢
that are not specific to the node ùë¢(e.g. features ùëówhich co-occur
with almost every other feature; stopwords) have low probability;
adding ùëñwould be noticeable. Thus, we obtain the desired result.
Using the above test, we only accept perturbations ùê∫‚Ä≤ = (ùê¥‚Ä≤,ùëã‚Ä≤)
where the feature values fulfill
‚àÄùë¢‚ààV : ‚àÄùëñ‚ààF : ùëã‚Ä≤
ùë¢ùëñ= 1 ‚áíùëñ‚ààùëÜùë¢‚à®ùëù(ùëñ|ùëÜùë¢) > ùúé
In summary, to ensure unnoticeable perturbations, we update
our problem definition to:
Problem 2. Same as Problem 1 but replacing Pùê∫0
Œî,A with the more
restricted set ÀÜPùê∫0
Œî,A of graphs that additionally preserve the degree
distribution (Eq. 10) and feature co-occurence (Eq. 12).
GENERATING ADVERSARIAL GRAPHS
Solving Problem 1/2 is highly challenging. While (continuous) bilevel problems for attacks have been addressed in the past by gradient computation based on first-order KKT conditions , such
a solution is not possible in our case due to the data‚Äôs discreteness
and the large number of parameters ùúÉ. Therefore, we propose a
sequential approach, where we first attack a surrogate model, thus,
leading to an attacked graph. This graph is subsequently used to
train the final model. Indeed, this approach can directly be considered as a check for transferability since we do not specifically focus
on the used model but only on a surrogate one.
Surrogate model. To obtain a tractable surrogate model that
still captures the idea of graph convolutions, we perform a linearizion of the model from Eq. 2. That is, we replace the nonlinearity ùúé(.) with a simple linear activation function, leading to:
ùëç‚Ä≤ = softmax
ÀÜùê¥ÀÜùê¥ùëãùëä(1) ùëä(2)
Since ùëä(1) and ùëä(2) are (free) parameters to be learned, they can
be absorbed into a single matrix ùëä‚ààRùê∑√óùêæ.
Since our goal is to maximize the difference in the log-probabilities
of the target ùë£0 (given a certain budget Œî), the instance-dependent
normalization induced by the softmax can be ignored. Thus, the
log-probabilities can simply be reduced to ÀÜùê¥2 ùëãùëä. Accordingly,
given the trained surrogate model on the (uncorrupted) input data
with learned parameters ùëä, we define the surrogate loss
Lùë†(ùê¥,ùëã;ùëä, ùë£0) = max
ùëê‚â†ùëêùëúùëôùëë[ ÀÜùê¥2 ùëãùëä]ùë£0ùëê‚àí[ ÀÜùê¥2 ùëãùëä]ùë£0ùëêùëúùëôùëë
and aim to solve
(ùê¥‚Ä≤,ùëã‚Ä≤) ‚ààÀÜPùê∫0
Lùë†(ùê¥‚Ä≤,ùëã‚Ä≤;ùëä, ùë£0).
While being much simpler, this problem is still intractable to
solve exactly due to the discrete domain and the constraints. Thus,
in the following we introduce a scalable greedy approximation
scheme. For this, we define scoring functions that evaluate the surrogate loss from Eq. (14) obtained after adding/removing a feature
ùëì= (ùë¢,ùëñ) or edge ùëí= (ùë¢, ùë£) to an arbitrary graph ùê∫= (ùê¥,ùëã):
ùë†ùë†ùë°ùëüùë¢ùëêùë°(ùëí;ùê∫, ùë£0) := Lùë†(ùê¥‚Ä≤,ùëã;ùëä, ùë£0)
ùë†ùëìùëíùëéùë°(ùëì;ùê∫, ùë£0) := Lùë†(ùê¥,ùëã‚Ä≤;ùëä, ùë£0)
where ùê¥‚Ä≤ := ùê¥¬± ùëí(i.e. ùëé‚Ä≤ùë¢ùë£= ùëé‚Ä≤ùë£ùë¢= 1 ‚àíùëéùë¢ùë£)3 and ùëã‚Ä≤ := ùëã¬± ùëì(i.e.
ùë¢ùëñ= 1 ‚àíùë•ùë¢ùëñ).
Approximate Solution. Algorithm 1 shows the pseudo-code.
In detail, following a locally optimal strategy, we sequentially ‚Äômanipulate‚Äô the most promising element: either an entry from the adjacency matrix or a feature entry (taking the constraints into account).
That is, given the current state of the graph ùê∫(ùë°), we compute a
candidate set ùê∂ùë†ùë°ùëüùë¢ùëêùë°of allowable elements (ùë¢, ùë£) whose change
from 0 to 1 (or vice versa; hence the ¬± sign in the pseudocode)
does not violate the constraints imposed by ÀÜPùê∫0
Œî,A. Among these elements we pick the one which obtains the highest difference in the
log-probabilites, indicated by the score function ùë†ùë†ùë°ùëüùë¢ùëêùë°(ùëí;ùê∫(ùë°), ùë£0).
Similar, we compute the candidate set ùê∂ùëìùëíùëéùë°and the score function
ùë†ùëìùëíùëéùë°(ùëì;ùê∫(ùë°), ùë£0) for every allowable feature manipulation of feature ùëñand node ùë¢. Whichever change obtains the higher score is
picked and the graph accordingly updated to ùê∫(ùë°+1). This process
is repeated until the budget Œî has been exceeded.
To make Algorithm 1 tractable, two core aspects have to hold:
(i) an efficient computation of the score functions ùë†ùë†ùë°ùëüùë¢ùëêùë°and ùë†ùëìùëíùëéùë°,
and (ii) an efficient check which edges and features are compliant
with our constraints ÀÜPùê∫0
Œî,A, thus, forming the setsùê∂ùë†ùë°ùëüùë¢ùëêùë°andùê∂ùëìùëíùëéùë°.
In the following, we describe these two parts in detail.
Fast computation of score functions
Structural attacks. We start by describing how to compute ùë†ùë†ùë°ùëüùë¢ùëêùë°.
For this, we have to compute the class prediction (in the surrogate
model) of node ùë£0 after adding/removing an edge (ùëö,ùëõ). Since we
are now optimizing w.r.t. ùê¥, the term ùëãùëäin Eq. (14) is a constant
‚Äì we substitute it with ùê∂:= ùëãùëä‚ààRùëÅ√óùêæ. The log-probabilities of
node ùë£0 are then given by ùëîùë£0 = [ ÀÜùê¥2]ùë£0 ¬∑ ùê∂‚ààR1√óùêæwhere [ ÀÜùê¥2]ùë£0
denotes a row vector. Thus, we only have to inspect how this row
vector changes to determine the optimal edge manipulation.
Naively recomputing [ ÀÜùê¥2]ùë£0 for every element from the candidate set, though, is not practicable. An important observation to
3Please note that by modifying a single element ùëí= (ùë¢, ùë£) we always change two
entries, ùëéùë¢ùë£and ùëéùë£ùë¢, of ùê¥since we are operating on an undirected graph.
Algorithm 1: Nettack: Adversarial attacks on graphs
Input: Graph ùê∫(0) ‚Üê(ùê¥(0),ùëã(0)), target node ùë£0,
attacker nodes A, modification budget Œî
Output: Modified Graph ùê∫‚Ä≤ = (ùê¥‚Ä≤,ùëã‚Ä≤)
Train surrogate model on ùê∫(0) to obtain ùëä// Eq. (13);
while |ùê¥(ùë°) ‚àíùê¥(0) | + |ùëã(ùë°) ‚àíùëã(0) | < Œî do
ùê∂ùë†ùë°ùëüùë¢ùëêùë°‚Üêcandidate_edge_perturbations(ùê¥(ùë°), A) ;
ùëí‚àó= (ùë¢‚àó, ùë£‚àó) ‚Üêarg max
ùëí;ùê∫(ùë°), ùë£0
ùê∂ùëìùëíùëéùë°‚Üêcandidate_feature_perturbations(ùëã(ùë°), A) ;
ùëì‚àó= (ùë¢‚àó,ùëñ‚àó) ‚Üêarg max
ùëì;ùê∫(ùë°), ùë£0
if ùë†ùë†ùë°ùëüùë¢ùëêùë°(ùëí‚àó;ùê∫(ùë°), ùë£0) > ùë†ùëìùëíùëéùë°(ùëì‚àó;ùê∫(ùë°), ùë£0) then
ùê∫(ùë°+1) ‚Üêùê∫(ùë°) ¬± ùëí‚àó;
else ùê∫(ùë°+1) ‚Üêùê∫(ùë°) ¬± ùëì‚àó;
return :ùê∫(ùë°)
// Train final graph model on the corrupted graph ùê∫(ùë°);
alleviate this problem is that in the used two-layer GCN the prediction for each node is influenced by its two-hop neighborhood only.
That is, the above row vector is zero for most of the elements. And
even more important, we can derive an incremental update ‚Äì we
don‚Äôt have to recompute the updated [ ÀÜùê¥2]ùë£0 from scratch.
Theorem 5.1. Given an adjacency matrix ùê¥, and its corresponding
matrices Àúùê¥, ÀÜùê¥2, Àúùê∑. Denote with ùê¥‚Ä≤ the adjacency matrix when adding
or removing the element ùëí= (ùëö,ùëõ) from ùê¥. It holds:
[ ÀÜùê¥‚Ä≤2]ùë¢ùë£=
Àúùëëùë¢Àúùëëùë£[ ÀÜùê¥2]ùë¢ùë£‚àíÀúùëéùë¢ùë£
where Àúùëë‚Ä≤, ùëé‚Ä≤, and Àúùëé‚Ä≤, are defined as (using the Iverson bracket I):
ùëò= Àúùëëùëò+ I[ùëò‚àà{ùëö,ùëõ}] ¬∑ (1 ‚àí2 ¬∑ ùëéùëöùëõ)
ùëòùëô= ùëéùëòùëô+ I[{ùëò,ùëô} = {ùëö,ùëõ}] ¬∑ (1 ‚àí2 ¬∑ ùëéùëòùëô)
ùëòùëô= Àúùëéùëòùëô+ I[{ùëò,ùëô} = {ùëö,ùëõ}] ¬∑ (1 ‚àí2 ¬∑ Àúùëéùëòùëô)
Proof. Let ùëÜand ùëÜ‚Ä≤ be defined as ùëÜ= √çùëÅ
. We have [ ÀÜùê¥]ùë¢ùë£=
. If ùë¢‚â†ùë£, then
[ ÀÜùê¥]ùë¢ùëò[ ÀÜùê¥]ùëòùë£=
Having the above equation for ÀÜùê¥‚Ä≤2, we get
 ‚àí[ ÀÜùê¥2]ùë¢ùë£
+ (ùëÜ‚Ä≤ ‚àíùëÜ).
After replacing ùëÜ‚Ä≤ ‚àíùëÜ= ‚àíùëéùë¢ùëöùëéùëöùë£
the above equation, it is straightforward to derive Eq. 17. Deriving
this equation for the case ùë¢= ùë£is similar. Eq. 17 encompasses both
Eq. (17) enables us to update the entries in ÀÜùê¥2 in constant time;
and in a sparse and incremental manner. Remember that all Àúùëéùë¢ùë£,
ùëéùë¢ùë£, and ùëé‚Ä≤ùë¢ùë£are either 1 or 0, and their corresponding matrices are
sparse. Given this highly efficient update of [ ÀÜùê¥2]ùë£0 to [ ÀÜùê¥‚Ä≤2]ùë£0, the
updated log-probabilities and, thus, the final score according to Eq.
(15) can be easily computed.
Feature attacks. The feature attacks are much easier to realize.
Indeed, by fixing the class ùëê‚â†ùëêùëúùëôùëëwith currently largest logprobability score [ ÀÜùê¥2 ùëãùëä]ùë£0ùëê, the problem is linear in ùëãand every
entry of ùëãacts independently. Thus, to find the best node and
feature (ùë¢‚àó,ùëñ‚àó) we only need to compute the gradients
 [ ÀÜùê¥2 ùëãùëä]ùë£0ùëê‚àí[ ÀÜùê¥2 ùëãùëä]ùë£0ùëêùëúùëôùëë
= [ ÀÜùê¥2]ùë£0ùë¢
 [ùëä]ùëñùëê‚àí[ùëä]ùëñùëêùëúùëôùëë
and subsequently pick the one with the highest absolute value
that points into an allowable direction (e.g. if the feature was 0,
the gradient needs to point into the positives). The value of the
score function ùë†ùëìùëíùëéùë°for this best element is then simply obtained
by adding |Œ•ùë¢ùëñ| to the current value of the loss function:
Lùë†(ùê¥,ùëã;ùëä, ùë£0) + |Œ•ùë¢ùëñ| ¬∑ I[(2 ¬∑ ùëãùë¢ùëñ‚àí1) ¬∑ Œ•ùë¢ùëñ< 0]
All this can be done in constant time per feature. The elements
where the gradient points outside the allowable direction should
not be perturbed since they would only hinder the attack ‚Äì thus,
the old score stays unchanged.
Fast computation of candidate sets
Last, we have to make sure that all perturbations are valid according
to the constraints ÀÜPùê∫0
Œî,A. For this, we defined the sets ùê∂ùë†ùë°ùëüùë¢ùëêùë°and
ùê∂ùëìùëíùëéùë°. Clearly, the constraints introduced in Eq. 4 and 5 are easy
to ensure. The budget constraint Œî is fulfilled by the process of
the greedy approach, while the elements which can be perturbed
according to Eq. 4 can be precomputed. Likewise, the node-feature
combinations fulfilling the co-occurence test of Eq. 12 can be precomputed. Thus, the set ùê∂ùëìùëíùëéùë°only needs to be instantiated once.
The significance test for the degree distribution, however, does
not allow such a precomputation since the underlying degree distribution dynamically changes. How can we efficiently check whether
a potential perturbation of the edge (ùëö,ùëõ) still preserves a similar
degree distribution? Indeed, since the individual degrees only interact additively, we can again derive a constant time incremental
update of our test statistic Œõ.
Theorem 5.2. Given graph ùê∫= (ùê¥,ùëã) and the multiset Dùê∫(see
below Eq. 6). Denote with ùëÖùê∫= √ç
ùëëùëñ‚ààDùê∫logùëëùëñthe sum of log degrees.
Let ùëí= (ùëö,ùëõ) be a candidate edge perturbation, and ùëëùëöand ùëëùëõthe
degrees of the nodes in ùê∫. For ùê∫‚Ä≤ = ùê∫¬± ùëíwe have:
ùõºùê∫‚Ä≤ = 1 + ùëõùëíh
ùëÖùê∫‚Ä≤ ‚àíùëõùëílog
 = ùëõùëílogùõºùê∫‚Ä≤ + ùëõùëíùõºùê∫‚Ä≤ logùëëmin ‚àí ùõºùê∫‚Ä≤ + 1 ùëÖùê∫‚Ä≤
Perturbations
Surrogate loss Ls
Nettack-Struct
Nettack-Feat
Perturbations
Surrogate loss Ls
Nettack-In-U
Nettack-In
Figure 2: Average surrogate loss for increasing number of perturbations.
Different variants of our method on the Cora data. Larger is better.
Structure perturbations
No constraint
With constraint
Figure 3: Change in test
statistic Œõ (degree distr.)
Figure 4: Gradient vs.
actual loss
ùë•= 1 ‚àí2 ¬∑ ùëéùëöùëõ
ùëõùëí= |Dùê∫| + (I[ùëëùëö+ 1 ‚àíùëéùëöùëõ= ùëëmin] + I[ùëëùëõ+ 1 ‚àíùëéùëöùëõ= ùëëmin]) ¬∑ùë•
ùëÖùê∫‚Ä≤ = ùëÖùê∫‚àíI[ùëëùëö‚â•ùëëmin] logùëëùëö+ I[ùëëùëö+ ùë•‚â•ùëëmin] log(ùëëùëö+ ùë•)
‚àíI[ùëëùëõ‚â•ùëëmin] logùëëùëõ+ I[ùëëùëõ+ ùë•‚â•ùëëmin] log(ùëëùëõ+ ùë•).
Proof. Firstly, we show that if we incrementally compute ùëõùëí
according to the update equation of Theorem 5.2, ùëõùëíwill be equal
to |Dùê∫‚Ä≤|. The term I[ùëëùëö+ 1 ‚àíùëéùëöùëõ= ùëëùëöùëñùëõ] ¬∑ ùë•will be activated
(i.e. non-zero) only in two cases: 1) ùëéùëöùëõ= 1 (i.e. ùê∫‚Ä≤ = ùê∫‚àíùëí), and
ùëëùëö= ùëëùëöùëñùëõ, then ùë•< 0 and the update equation actually removes
node ùëöfrom Dùê∫. 2) ùëéùëöùëõ= 0 (i.e. ùê∫‚Ä≤ = ùê∫+ ùëí), and ùëëùëö= ùëëùëöùëñùëõ‚àí1,
then ùë•> 0 and the update equation actually adds node ùëöto Dùê∫.
A similar argumentation is applicable for node ùëõ. Accordingly, we
have that ùëõùëí= |Dùê∫‚Ä≤|.
Similarly, one can show the valid incremental update for ùëÖùê∫‚Ä≤
considering that only nodes with degree larger than ùëëùëöùëñùëõare considered and that ùëëùëö+ ùë•is the new degree. Having incremental
updates for ùëõùëíand ùëÖùê∫‚Ä≤, the updates for ùõºùê∫‚Ä≤ and ùëô(Dùê∫‚Ä≤) follow
easily from their definitions.
Given ùê∫(ùë°), we can now incrementally compute ùëô(Dùê∫(ùë°)
= ùê∫(ùë°) ¬± ùëí. Equivalently we get incremental updates for
ùëô(Dùëêùëúùëöùëè) after an edge perturbation. Since all r.h.s. of the equations above can be computed in constant time, also the test statistic
Œõ(ùê∫(0),ùê∫(ùë°)
) can be computed in constant time. Overall, the set
of valid candidate edge perturbations at iteration ùë°is ùê∂ùë†ùë°ùëüùë¢ùëêùë°=
{ùëí=(ùëö,ùëõ) | Œõ(ùê∫(0),ùê∫(ùë°)
) < ùúè‚àß(ùëö‚ààA ‚à®ùëõ‚ààA)}. Since ùëÖùê∫(ùë°)
can be incrementally updated to ùëÖùê∫(ùë°+1) once the best edge perturbation has been performed, the full approach is highly efficient.
Complexity
The candidate set generation (i.e. which edges/features are allowed
to change) and the score functions can be incrementally computed
and exploit the graph‚Äôs sparsity, thus, ensuring scalability. The
runtime complexity of the algorithm can easily be determined as:
O(Œî ¬∑ |A| ¬∑ (ùëÅ¬∑ ùë°‚Ñéùë£0 + ùê∑))
where ùë°‚Ñéùë£0 indicates the size of the two-hop neighborhood of the
node ùë£0 during the run of the algorithm.
In every of the Œî many iterations, each attacker evaluates the potential edge perturbations (ùëÅat most) and feature perturbations (ùê∑
at most). For the former, this requires to update the two-hop neighborhood of the target due to the two convolution layers. Assuming
the graph is sparse, ùë°‚Ñéùë£0 is much smaller than ùëÅ. The feature perturbations are done in constant time per feature. Since all constraints
can be checked in constant time they do not affect the complexity.
EXPERIMENTS
We explore how our attacks affect the surrogate model, and evaluate transferability to other models and for multiple datasets. For
repeatibility, Nettack‚Äôs source code is available on our website:
 
Cora-ML 
CiteSeer 
Pol. Blogs 
Table 1: Dataset statistics. We only consider the largest connected component (LCC).
Setup. We use the well-known Cora-ML and Citeseer networks
as in , and Polblogs . The dataset characteristics are shown in
Table 1. We split the network in labeled (20%) and unlabeled nodes
(80%). We further split the labeled nodes in equal parts training and
validation sets to train our surrogate model. That is, we remove
the labels from the validation set in the training procedure and
use them as the stopping criterion (i.e., stop when validation error
increases). The labels of the unlabeled nodes are never visible to
the surrogate model during training.
We average over five different random initializations/ splits, where
for each we perform the following steps. We first train our surrogate
model on the labeled data and among all nodes from the test set that
have been correctly classified, we select (i) the 10 nodes with highest
margin of classification, i.e. they are clearly correctly classified,
(ii) the 10 nodes with lowest margin (but still correctly classified)
and (iii) 20 more nodes randomly. These will serve as the target
nodes for our attacks. Then, we corrupt the input graph using the
model proposed in this work, called Nettack for direct attacks, and
Nettack-In for influence attacks, respectively (picking 5 random
nodes as attackers from the neighborhood of the target).
Since no other competitors exist, we compare against two baselines:
(i) Fast Gradient Sign Method (FGSM) as a direct attack on
ùë£0 (in our case also making sure that the result is still binary). (ii)
Rnd is an attack in which we modify the structure of the graph.
Given our target node ùë£0, in each step we randomly sample nodes
ùë¢for which ùëêùë£0 ‚â†ùëêùë¢and add the edge ùë¢, ùë£to the graph structure,
assuming unequal class labels are hindering classification.
Class: neural networks
Class: theory
Class: probabilistic models
constrained
unconstrained
constrained
unconstrained
constrained
unconstrained
probabilistic
difference
probability
disjunctive
previously
accomplished
corporation
probabilities
generality
observations
expectation
performing
refinement
represents
estimation
specifications
distributions
functional
independence
difference
observations
acquisition
management
requirement
Table 2: Top-10 feature perturbations per class on Cora
Attacks on the surrogate model
We start by analyzing different variants of our method by inspecting
their influence on the surrogate model. In Fig. 2 (left) we plot the
surrogate loss when performing a specific number of perturbations.
Note that once the surrogate loss is positive, we realized a successful
misclassification. We analyze Nettack, and variants where we
only manipulate features or only the graph structure. As seen,
perturbations in the structure lead to a stronger change in the
surrogate loss compared to feature attacks. Still, combining both
is the most powerful, only requiring around 3 changes to obtain a
misclassification. For comparison we have also added Rnd, which
is clearly not able to achieve good performance.
In Fig. 2 (right) we analyze our method when using a direct vs.
influencer attack. Clearly, direct attacks need fewer perturbations
‚Äì still, influencer attacks are also possible, posing a high risk in
real life scenarios. The figure also shows the result when not using
our constraints as proposed in Section 4.1, indicated by the name
Nettack-U. As seen, even when using our constraints, the attack is
still succesfull. Thus, unnoticable perturbations can be generated.
Perturbations
Runtime (s)
4 influencers
2 influencers
1 influencer
Figure 5: Runtime
It is worth mentioning that
the constraints are indeed necessary. Figure 3 shows the test
statistic Œõ of the resulting graph
with or without our constraints.
As seen the constraint we impose has an effect on our attack;
if not enforced, the power law
distribution of the corrupted graph becomes more and more dissimilar to the original graph‚Äôs. Similarly, Table 2 illustrates the result for
the feature perturbations. For Cora-ML, the features correspond to
the presence of words in the abstracts of papers. For each class (i.e.
set of nodes with same label), we plot the top-10 features that have
been manipulated by the techniques (these account for roughly
50% of all perturbations). Further, we report for each feature its
original occurence within the class. We see that the used features
are indeed different ‚Äì even more, the unconstrained version often
uses words which are ‚Äôunlikely‚Äô for the class (indicated by the small
numbers). Using such words can easily be noticed as manipulations,
e.g. ‚Äôdavid‚Äô in neural networks or ‚Äôchemical‚Äô in probabilistic models.
Our constraint ensures that the changes are more subtle.
Overall, we conclude that attacking the features and structure
simultaneously is very powerful; and the introduced constraints do
not hinder the attack while generating more realistic perturbations.
Direct attacks are clearly easier than influencer attacks.
Lastly, even though not our major focus, we want to analyze
the required runtime of Nettack. In line with the derived complexity, in Fig. 5 we see that our algorithm scales linearly with the
number of perturbations to the graph structure and the number of
influencer nodes considered. Please note that we report runtime
for sequential processing of candidate edges; this can however be
trivially parallelized. Similar results were obtained for the runtime
w.r.t. the graph size, matching the complexity analysis.
Transferability of attacks
After exploring how our attack affects the (fixed) surrogate model,
we will now find out whether our attacks are also successful on
established deep learning models for graphs. For this, we pursue
the approach from before and use a budget of Œî = ùëëùë£0 +2, where ùëëùë£0
is the degree of target node we currently attack. This is motivated
by the observation that high-degree nodes are more difficult to
attack than low-degree ones. In the following we always report the
score ùëã= ùëç‚àóùë£0,ùëêùëúùëôùëë‚àímaxùëê‚â†ùëêùëúùëôùëëùëç‚àóùë£0,ùëêusing the ground truth label
ùëêùëúùëôùëëof the target. We call ùëãthe classification margin. The smaller
ùëã, the better. For values smaller than 0, the targets get misclassified.
Note that this could even happen for the clean graph since the
classification itself might not be perfect.
Evasion vs. Poisoning Attack. In Figure 6a we evaluate Nettack‚Äôs performance for two attack types: evasion attacks, where
the model parameters (here of GCN ) are kept fix based on the
clean graph; and poisoning attacks, where the model is retrained
after the attack (averaged over 10 runs). In the plot, every dot represents one target node. As seen, direct attacks are extremly succesful
‚Äì even for the challening poisoning case almost every target gets
misclassified. We therefore conclude that our surrogate model and
loss are a sufficient approximation of the true loss on the non-linear
model after re-training on the perturbed data. Clearly, influencer
attacks (shown right of the double-line) are harder but they still
work in both cases. Since poisoining attacks are in general harder
and match better the transductive learning scenario, we report in
the following only these results.
Comparison. Figure 6b and 6c show that the corruptions generated by Nettack transfer to different (semi-supervised) graph
convolutional methods: GCN and CLN . Most remarkably,
even the unsupervised model DeepWalk is strongly affected by
our perturbations (Figure 6d). Since DW only handles unattributed
graphs, only structural attacks were performed. Following ,
node classification is performed by training a logistic regression on
Nettack-In.
Nettack-In.
Classifcation margin
(a) Evasion vs. poisoning for GCN
Nettack-In.
(b) Poisoning of GCN
Nettack-In.
(c) Poisoning of Column Network
Nettack-In.
misclassiÔ¨Åed
(d) Poisoning of DeepWalk
Figure 6: Results on Cora data using different attack algorithms. Clean indicates the original data. Lower scores are better.
the learned embeddings. Overall, we see that direct attacks pose
a much harder problem than influencer attacks. In these plots, we
also compare against the two baselines Rnd and FGSM, both operating in the direct attack setting. As shown, Nettack outperforms
both. Again note: All these results are obtained using a challenging
poisoning attack (i.e. retraining of the model).
In Table 3 we summarize the results for different datasets and
classification models. Here, we report the fraction of target nodes
that get correctly classified. Our adversarial perturbations on the surrogate model are transferable to all three models an on all datasets
we evaluated. Not surprisingly, influencer attacks lead to a lower
decrease in performance compared to direct attacks.
We see that FGSM performs worse than Nettack, and we argue
that this comes from the fact that gradient methods are not optimal
for discrete data. Fig. 4 shows why this is the case: we plot the
gradient vs. the actual change in loss when changing elements in ùê¥.
Often the gradients do not approximate the loss well ‚Äì in (b) and
(c) even the signs do not match. One key advantage of Nettack is
that we can precisely and efficiently compute the change in Lùë†.
Last, we also analyzed how the structure of the target, i.e. its
degree, affects the performance.
The table shows results for different degree ranges. As seen, high
degree nodes are slightly harder to attack: they have both, higher
classification accuracy in the clean graph and in the attacked graph.
Limited Knowledge. In the previous experiments, we have assumed full knowledge of the input graph, which is a reasonable
assumption for a worst-case attack. In Fig. 7 we analyze the result
when having limited knowledge: Given a target node ùë£0, we provided our model only subgraphs of increasing size relative to the
size of the Cora graph. We constructed these subgraphs by selecting
nodes with increasing distance from ùë£0, i.e. we first selected 1-hop
neighbors, then 2-hop neighbors and so on, until we have reached
Nettack-In
Table 3: Overview of results. Smaller is better.
Perturbations
Classification margin
Nettack 0.1
Nettack 0.2
Nettack 0.5
Nettack 0.75
(a) Direct attack
Perturbations
Nettack 0.1
Nettack 0.2
Nettack 0.5
Nettack 0.75
(b) Influence attack
Figure 7: Attacks with limited knowledge about the data
the desired graph size. We then perturbed the subgraphs using the
attack strategy proposed in this paper. These perturbations are then
taken over to full graph, where we trained GCN. Note that Nettack
has always only seen the subgraph; and its surrogate model is also
only trained based on it.
Fig. 7 shows the result for a direct attack. As seen, even if only
10% of the graph is observed, we can still significantly attack it.
Clearly, if the attacker knows the full graph, the fewest number
of perturbations is required. For comparison we include the Rnd
attack, also only operating on the subgraphs. In Fig. 7 we see the
influence attack. Here we require more perturbations and 75% of the
graph size for our attack to succeed. Still, this experiment indicates
that full knowledge is not required.
CONCLUSION
We presented the first work on adversarial attacks to (attributed)
graphs, specifically focusing on the task of node classification via
graph convolutional networks. Our attacks target the nodes‚Äô features and the graph structure. Exploiting the relational nature of the
data, we proposed direct and influencer attacks. To ensure unnoticeable changes even in a discrete, relational domain, we proposed to
preserve the graph‚Äôs degree distribution and feature co-occurrences.
Our developed algorithm enables efficient perturbations in a discrete domain. Based on our extensive experiments we can conclude
that even the challenging poisoning attack is successful possible
with our approach. The classification performance is consistently
reduced, even when only partial knowledge of the graph is available or the attack is restricted to a few influencers. Even more, the
attacks generalize to other node classification models.
Studying the robustness of deep learning models for graphs is
an important problem, and this work provides essential insights
for deeper study. As future work we aim to derive extensions of
existing models to become more robust against attacks, and we aim
to study tasks beyond node classification.
ACKNOWLEDGEMENTS
This research was supported by the German Research Foundation,
Emmy Noether grant GU 1409/2-1, and by the Technical University
of Munich - Institute for Advanced Study, funded by the German
Excellence Initiative and the European Union Seventh Framework
Programme under grant agreement no 291763, co-funded by the
European Union.