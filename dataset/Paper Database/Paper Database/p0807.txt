Adversarial Attacks on Neural Networks for Graph Data
Daniel ZÃ¼gner
Amir Akbarnejad
Stephan GÃ¼nnemann
Technical University of Munich, Germany
{zuegnerd,amir.akbarnejad,guennemann}@in.tum.de
Deep learning models for graphs have achieved strong performance
for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks.
Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily
fooled? In this work, we introduce the first study of adversarial
attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test
time, we tackle the more challenging class of poisoning/causative
attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the
nodeâ€™s features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the
perturbations remain unnoticeable by preserving important data
characteristics. To cope with the underlying discrete domain we
propose an efficient algorithm Nettack exploiting incremental
computations. Our experimental study shows that accuracy of node
classification significantly drops even when performing only few
perturbations. Even more, our attacks are transferable: the learned
attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even
when only limited knowledge about the graph is given.
Adversarial machine learning, graph mining, network mining,
graph convolutional networks, semi-supervised learning
INTRODUCTION
Graph data is the core for many high impact applications ranging
from the analysis of social and rating networks (Facebook, Amazon),
over gene interaction networks (BioGRID), to interlinked document
collections (PubMed, Arxiv). One of the most frequently applied
tasks on graph data is node classification: given a single large (attributed) graph and the class labels of a few nodes, the goal is to predict
the labels of the remaining nodes. For example, one might wish to
classify the role of a protein in a biological interaction graph ,
predict the customer type of users in e-commerce networks ,
or assign scientific papers from a citation network into topics .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from .
KDD â€™18, August 19â€“23, 2018, London, United Kingdom
Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08...$15.00
 
Train node classification model
Target gets
misclassified
perturbation
target node
attacker node
Nettack-In.
Classiï¬cation margin
misclassiï¬ed
Results for attacking Citeseer data
Figure 1: Small perturbations of the graph structure and
node features lead to misclassification of the target.
While many classical approaches have been introduced in the
past to tackle the node classification problem , the last years
have seen a tremendous interest in methods for deep learning on
graphs . Specifically, approaches from the class of graph
convolutional networks have achieved strong performance
in many graph-learning tasks including node classification.
The strength of these methods â€” beyond their non-linear, hierarchical nature â€“ relies on their use of the graphsâ€™ relational
information to perform classification: instead of only considering
the instances individually (nodes and their features), the relationships between them are exploited as well (the edges). Put differently:
the instances are not treated independently; we deal with a certain
form of non-i.i.d. data where so-called network effects such as
homophily support the classification.
However, there is one big catch: Many researchers have noticed that deep learning architectures for classical learning tasks
can easily be fooled/attacked . Even only slight, deliberate
perturbations of an instance â€“ also known as adversarial perturbations/examples â€“ can lead to wrong predictions. Such negative
results significantly hinder the applicability of these models, leading to unintuitive and unreliable results, and they additionally open
the door for attackers that can exploit these vulnerabilities. So far,
however, the question of adversarial perturbations for deep learning
methods on graphs has not been addressed. This is highly critical,
since especially in domains where graph-based learning is used (e.g.
the web) adversaries are common and false data is easy to inject:
spammers add wrong information to social networks; fraudsters
frequently manipulate online reviews and product websites .
In this work, we close this gap and we investigate whether such
manipulations are possible. Can deep learning models for attributed
graphs be easily fooled? How reliable are their results?
The answer to this question is indeed not foreseeable: On one
hand the relational effects might improve robustness since predictions are not based on individual instances only but based on
various instances jointly. On the other hand, the propagation of
information might also lead to cascading effects, where manipulating a single instance affects many others. Indeed, compared to
the existing works on adversarial attacks, our work significantly
differs in various aspects.
 
Opportunities: (1) Since we are operating on an attributed
graph, adversarial perturbations can manifest in two different ways:
by changing the nodesâ€™ features or the graph structure. Manipulating the graph, i.e. the dependency structure between instances, has
not been studied so far, but is a highly likely scenario in real-life.
For example, one might add or remove (fake) friendship relations
to a social network. (2) While existing works were limited to manipulating an instance itself to enforce its wrong prediction1, the
relational effects give us more power: by manipulating one instance,
we might specifically misguide the prediction for another instance.
Again, this scenario is highly realistic. Think about a fraudster
who hijacks some accounts, which he then manipulates to enforce
a wrong prediction for another account he has not under control.
Thus, in graph-based learning scenarios we can distinguish between
(i) nodes which we aim to misclassify, called targets, and (ii) nodes
which we can directly manipulate, called attackers. Figure 1 illustrates the goal of our work and shows the result of our method on
the Citeseer network. Clearly, compared to classical attacks to learning models, graphs enable much richer potential for perturbations.
But likewise, constructing them is far more challenging.
Challenges: (1) Unlike, e.g., images consisting of continuous
features, the graph structure â€“ and often also the nodesâ€™ features â€“
is discrete. Therefore, gradient based approaches for finding
perturbations are not suited. How to design efficient algorithms that
are able to find adversarial examples in a discrete domain? (2) Adversarial perturbations are aimed to be unnoticeable (by humans).
For images, one often enforces, e.g., a maximum deviation per pixel
value. How can we capture the notion of â€™unnoticeable changesâ€™ in a
(binary, attributed) graph? (3) Last, node classification is usually
performed in a transductive learning setting. Here, the train and test
data are used jointly to learn a new classification model before the
predictions are performed on the specific test data. This means, that
the predominantly performed evasion attacks â€“ where the parameters of the classification model are assumed to be static â€“ are not
realistic. The model has to be (re)trained on the manipulated data.
Thus, graph-based learning in a transductive setting is inherently
related to the challenging poisoning/causative attacks .
Given these challenges, we propose a principle for adversarial
perturbations of attributed graphs that aim to fool state-of-the
art deep learning models for graphs. In particular, we focus on
semi-supervised classification models based on graph convolutions
such as GCN and Column Network (CLN) â€“ but we will
also showcase our methodsâ€™ potential on the unsupervised model
DeepWalk . By default, we assume an attacker with knowledge
about the full data, which can, however, only manipulate parts of
it. This assumption ensures reliable vulnerability analysis in the
worst case. But even when only parts of the data are known, our
attacks are still successful as shown by our experiments. Overall,
our contributions are:
â€¢ Model: We propose a model for adversarial attacks on attributed graphs considering node classification. We introduce new
types of attacks where we explicitly distinguish between the
attacker and the target nodes. Our attacks can manipulate the
1Due to the independence assumption, a misclassification for instance ğ‘–can only be
achieved by manipulating instance ğ‘–itself for the commonly studied evasion (test-time)
attacks. For the less studied poisioning attacks we might have indirect influence.
graph structure and node features while ensuring unnoticeable
changes by preserving important data characteristics (e.g. degree distribution, co-occurence of features).
â€¢ Algorithm: We develop an efficient algorithm Nettack for computing these attacks based on linearization ideas. Our methods
enables incremental computations and exploits the graphâ€™s sparsity for fast execution.
â€¢ Experiments: We show that our model can dramatically worsen
classification results for the target nodes by only requiring few
changes to the graph. We furthermore show that these results
transfer to other established models, hold for various datasets,
and even work when only parts of the data are observed. Overall,
this highlights the need to handle attacks to graph data.
PRELIMINARIES
We consider the task of (semi-supervised) node classification in
a single large graph having binary node features. Formally, let
ğº= (ğ´,ğ‘‹) be an attributed graph, where ğ´âˆˆ{0, 1}ğ‘Ã—ğ‘is the
adjacency matrix representing the connections and ğ‘‹âˆˆ{0, 1}ğ‘Ã—ğ·
represents the nodesâ€™ features. We denote with ğ‘¥ğ‘£âˆˆ{0, 1}ğ·the
ğ·-dim. feature vector of node ğ‘£. W.l.o.g. we assume the node-ids to
be V = {1, . . . , ğ‘} and the feature-ids to be F = {1, ..., ğ·}.
Given a subset Vğ¿âŠ†V of labeled nodes, with class labels
from C = {1, 2, . . . ,ğ‘ğ¾}, the goal of node classification is to learn
a function ğ‘”: V â†’C which maps each node ğ‘£âˆˆV to one class
in C.2 Since the predictions are done for the given test instances,
which are already known before (and also used during) training,
this corresponds to a typical transductive learning scenario .
In this work, we focus on node classification employing graph
convolution layers. In particular, we will consider the well established work . Here, the hidden layer ğ‘™+ 1 is defined as
ğ»(ğ‘™+1) = ğœ
2 ğ»(ğ‘™)ğ‘Š(ğ‘™)
where Ëœğ´= ğ´+ ğ¼ğ‘is the adjacency matrix of the (undirected) input
graph ğºafter adding self-loops via the identity matrix ğ¼ğ‘. ğ‘Š(ğ‘™)
is the trainable weight matrix of layer ğ‘™, Ëœğ·ğ‘–ğ‘–= Ã
ğ‘—Ëœğ´ğ‘–ğ‘—, and ğœ(Â·) is
an activation function (usually ReLU). In the first layer we have
ğ»(0) = ğ‘‹, i.e. using the nodesâ€™ features as input. Since the latent
representations ğ»are (recursively) relying on the neighboring ones
(multiplication with Ëœğ´), all instances are coupled together. Following the authors of , we consider GCNs with a single hidden
ğ‘= ğ‘“ğœƒ(ğ´,ğ‘‹) = softmax
where Ë†ğ´= Ëœğ·âˆ’1
2 . The output ğ‘ğ‘£ğ‘denotes the probability of
assigning node ğ‘£to class ğ‘. Here, we used ğœƒdo denote the set of
all parameters, i.e. ğœƒ= {ğ‘Š(1),ğ‘Š(2)}. The optimal parameters ğœƒ
are then learned in a semi-supervised fashion by minimizing crossentropy on the output of the labeled samples Vğ¿, i.e. minimizing
ğ¿(ğœƒ;ğ´,ğ‘‹) = âˆ’
ğ‘= ğ‘“ğœƒ(ğ´,ğ‘‹)
2Please note the difference to (structured) learning settings where we have multiple
but independent graphs as training input with the goal to perform a prediction for each
graph. In this work, the prediction is done per node (e.g. a person in a social network)
â€“ and especially we have dependencies between the nodes/data instances via the edges.
where ğ‘ğ‘£is the given label of ğ‘£from the training set. After training,
ğ‘denotes the class probabilities for every instance in the graph.
RELATED WORK
In line with the focus of this work, we briefly describe deep learning
methods for graphs aiming to solve the node classification task.
Deep Learning for Graphs. Mainly two streams of research
can be distinguished: (i) node embeddings â€“ that often
operate in an unsupervised setting â€“ and (ii) architectures employing layers specifically designed for graphs . In this work,
we focus on the second type of principles and additionally show
that our adversarial attack transfers to node embeddings as well.
Regarding the developed layers, most works seek to adapt conventional CNNs to the graph domain: called graph convolutional layers
or neural message passing . Simply speaking,
they reduce to some form of aggregation over neighbors as seen in
Eq. (2). A more general setting is described in and an overview
of methods given in .
Adversarial Attacks. Attacking machine learning models has
a long history, with seminal works on, e.g., SVMs or logistic regression . In contrast to outliers, e.g. present in attributed graphs
 , adversarial examples are created deliberately to mislead machine learning models and often are designed to be unnoticeable.
Recently, deep neural networks have shown to be highly sensitive
to these small adversarial perturbations to the data . Even
more, the adversarial examples generated for one model are often
also harmful when using another model: known as transferability
 . Many tasks and models have been shown to be sensitive to
adversarial attacks; however, all assume the data instances to be independent. Even , which considers relations between different
tasks for multi-task relationship learning, still deals with the classical scenario of i.i.d. instances within each task. For interrelated
data such as graphs, where the data instances (i.e. nodes) are not
treated independently, no such analysis has been performed yet.
Taxonomies characterizing the attack have been introduced in
 . The two dominant types of attacks are poisoning/causative
attacks which target the training data (specifically, the modelâ€™s training phase is performed after the attack) and evasion/exploratory attacks which target the test data/application phase (here, the learned
model is assumed fixed). Deriving effective poisoning attacks is
usually computationally harder since also the subsequent learning
of the model has to be considered. This categorization is not optimally suited for our setting. In particular, attacks on the test data are
causative as well since the test data is used while training the model
(transductive, semi-supervised learning). Further, even when the
model is fixed (evasion attack), manipulating one instance might
affect all others due to the relational effects imposed by the graph
structure. Our attacks are powerful even in the more challenging
scenario where the model is retrained.
Generating Adversarial Perturbations. While most works
have focused on generating adversarial perturbations for evasion
attacks, poisoning attacks are far less studied since
they require to solve a challenging bi-level optimization problem that considers learning the model. In general, since finding
adversarial perturbations often reduces to some non-convex (bilevel) optimization problem, different approximate principles have
been introduced. Indeed, almost all works exploit the gradient or
other moments of a given differentiable (surrogate) loss function to
guide the search in the neighborhood of legitimate perturbations
 . For discrete data, where gradients are undefined,
such an approach is suboptimal.
Hand in hand with the attacks, the robustification of machine
learning models has been studied â€“ known as adversarial machine
learning or robust machine learning. Since this is out of the scope
of the current paper, we do not discuss these approaches here.
Adversarial Attacks when Learning with Graphs. Works on
adversarial attacks for graph learning tasks are almost non-existing.
For graph clustering, the work has measured the changes in the
result when injecting noise to a bi-partite graph that represent DNS
queries. Though, they do not focus on generating attacks in a principled way. Our work considered noise in the graph structure
to improve the robustness when performing spectral clustering.
Similarly, to improve robustness of collective classification via associative Markov networks, the work considers adversarial noise
in the features. They only use label smoothness and assume that
the attacker can manipulate the features of every instance. After
our work was published, introduced the second approach for
adversarial attacks on graphs, where they exploit reinforcement
learning ideas. However, in contrast to our work, they do not consider poisoning attacks nor potential attribute perturbations. They
further restrict to edge deletions for node classification â€“ while we
also handle edge insertions. In addition, in our work we even show
that the attacks generated by our strategy successfully transfer to
different models than the one attacked. Overall, no work so far has
considered poisoning/training-time attacks on neural networks for
attributed graphs.
ATTACK MODEL
Given the node classification setting as described in Sec. 2, our goal
is to perform small perturbations on the graph ğº(0) = (ğ´(0),ğ‘‹(0)),
leading to the graph ğºâ€² = (ğ´â€²,ğ‘‹â€²), such that the classification
performance drops. Changes to ğ´(0), are called structure attacks,
while changes to ğ‘‹(0) are called feature attacks.
Target vs. Attackers. Specifically, our goal is to attack a specific
target node ğ‘£0 âˆˆV, i.e. we aim to change ğ‘£0â€™s prediction. Due to the
non-i.i.d. nature of the data, ğ‘£0â€™s outcome not only depends on the
node itself, but also on the other nodes in the graph. Thus, we are
not limited to perturbing ğ‘£0 but we can achieve our aim by changing
other nodes as well. Indeed, this reflects real world scenarios much
better since it is likely that an attacker has access to a few nodes only,
and not to the entire data or ğ‘£0 itself. Therefore, besides the target
node, we introduce the attacker nodes A âŠ†V. The perturbations
on ğº(0) are constrained to these nodes, i.e. it must hold
ğ‘¢ğ‘£â‡’ğ‘¢âˆˆA âˆ¨ğ‘£âˆˆA
If the target ğ‘£0 âˆ‰A, we call the attack an influencer attack,
since ğ‘£0 gets not manipulated directly, but only indirectly via some
influencers. If {ğ‘£0} = A, we call it a direct attack.
To ensure that the attacker can not modify the graph completely,
we further limit the number of allowed changes by a budget Î”:
More advanced ideas will be discussed in Sec. 4.1. For now, we
denote with Pğº0
Î”,A the set of all graphs ğºâ€² that fulfill Eq. (4) and (5).
Given this basic set-up, our problem is defined as:
Problem 1. Given a graph ğº(0) = (ğ´(0),ğ‘‹(0)), a target node ğ‘£0,
and attacker nodes A. Let ğ‘ğ‘œğ‘™ğ‘‘denote the class for ğ‘£0 based on the
graph ğº(0) (predicted or using some ground truth). Determine
(ğ´â€²,ğ‘‹â€²) âˆˆPğº0
ğ‘â‰ ğ‘ğ‘œğ‘™ğ‘‘lnğ‘âˆ—
subject to ğ‘âˆ—= ğ‘“ğœƒâˆ—(ğ´â€²,ğ‘‹â€²) with ğœƒâˆ—= arg min
ğ¿(ğœƒ;ğ´â€²,ğ‘‹â€²)
That is, we aim to find a perturbed graph ğºâ€² that classifies ğ‘£0 as
ğ‘ğ‘›ğ‘’ğ‘¤and has maximal â€™distanceâ€™ (in terms of log-probabilities/logits)
to ğ‘ğ‘œğ‘™ğ‘‘. Note that for the perturbated graph ğºâ€², the optimal parameters ğœƒâˆ—are used, matching the transductive learning setting where
the model is learned on the given data. Therefore, we have a bi-level
optimization problem. As a simpler variant, one can also consider
an evasion attack assuming the parameters are static and learned
based on the old graph, ğœƒâˆ—= arg minğœƒğ¿(ğœƒ;ğ´(0),ğ‘‹(0)).
Unnoticeable Perturbations
Typically, in an adversarial attack scenario, the attackers try to
modify the input data such that the changes are unnoticable. Unlike
to image data, where this can easily be verified visually and by using
simple constraints, in the graph setting this is much harder mainly
for two reasons: (i) the graph structure is discrete preventing to use
infinitesimal small changes, and (ii) sufficiently large graphs are
not suitable for visual inspection.
How can we ensure unnoticeable perturbations in our setting? In
particular, we argue that only considering the budget Î” might not be
enough. Especially if a large Î” is required due to complicated data,
we still want realistically looking perturbed graphs ğºâ€². Therefore,
our core idea is to allow only those perturbations that preserve
specific inherent properties of the input graph.
Graph structure preserving perturbations. Undoubtedly, the
most prominent characteristic of the graph structure is its degree
distribution, which often resembles a power-law like shape in real
networks. If two networks show very different degree distributions,
it is easy to tell them apart. Therefore, we aim to only generate perturbations which follow similar power-law behavior as the input.
For this purpose we refer to a statistical two-sample test for
power-law distributions . That is, we estimate whether the two
degree distributions of ğº(0) and ğºâ€² stem from the same distribution
or from individual ones, using a likelihood ratio test.
More precisely, the procedure is as follows: We first estimate
the scaling parameter ğ›¼of the power-law distribution ğ‘(ğ‘¥) âˆğ‘¥âˆ’ğ›¼
referring to the degree distribution of ğº(0) (equivalently for ğºâ€²).
While there is no exact and closed-form solution to estimate ğ›¼in
the case of discrete data, derived an approximate expression,
which for our purpose of a graph ğºtranslates to
ğ›¼ğºâ‰ˆ1 + |Dğº| Â·
where ğ‘‘ğ‘šğ‘–ğ‘›denotes the minimum degree a node needs to have to
be considered in the power-law test and Dğº= {ğ‘‘ğºğ‘£| ğ‘£âˆˆV,ğ‘‘ğºğ‘£â‰¥
ğ‘‘ğ‘šğ‘–ğ‘›} is the multiset containing the list of node degrees, where ğ‘‘ğºğ‘£
is the degree of node ğ‘£in ğº. Using this, we get estimates for the
values ğ›¼ğº(0) and ğ›¼ğºâ€². Similarly, we can estimate ğ›¼ğ‘ğ‘œğ‘šğ‘using the
combined samples Dğ‘ğ‘œğ‘šğ‘= Dğº(0) âˆªDğºâ€².
Given the scaling parameter ğ›¼ğ‘¥, the log-likelihood for the samples Dğ‘¥can easily be evaluated as
ğ‘™(Dğ‘¥) = |Dğ‘¥|Â·logğ›¼ğ‘¥+|Dğ‘¥|Â·ğ›¼ğ‘¥Â·logğ‘‘minâˆ’(ğ›¼ğ‘¥+1)
Using these log-likelihood scores, we set up the significance test,
estimating whether the two samples Dğº(0) and Dğºâ€² come from the
same power law distribution (null hypotheses ğ»0) as opposed to
separate ones (ğ»1). That is, we formulate two competing hypotheses
ğ‘™(ğ»0) = ğ‘™(Dğ‘ğ‘œğ‘šğ‘)
ğ‘™(ğ»1) = ğ‘™(Dğº(0) ) + ğ‘™(Dğºâ€²)
Following the likelihood ratio test, the final test statistic is
Î›(ğº(0),ğºâ€²) = âˆ’2 Â· ğ‘™(ğ»0) + 2 Â· ğ‘™(ğ»1).
which for large sample sizes follows a ğœ’2 distribution with one
degree of freedom .
A typical ğ‘-value for rejecting the null hypothesis ğ»0 (i.e. concluding that both samples come from different distributions) is
0.05, i.e., statistically, in one out of twenty cases we reject the null
hypothesis although it holds (type I error). In our adversarial attack scenario, however, we argue that a human trying to find out
whether the data has been manipulated would be far more conservative and ask the other way: Given that the data was manipulated,
what is the probability of the test falsely not rejecting the null
hypothesis (type II error).
While we cannot compute the type II error in our case easily,
type I and II error probabilities have an inverse relation in general.
Thus, by selecting a very conservative ğ‘-value corresponding to a
high type I error, we can reduce the probability of a type II error.
We therefore set the critical ğ‘-value to 0.95, i.e. if we were to sample
two degree sequences from the same power law distribution, we
were to reject the null hypothesis in 95% of the times and could
then investigate whether the data has been compromised based on
this initial suspicion. On the other hand, if our modified graphâ€™s
degree sequence passes this very conservative test, we conclude
that the changes to the degree distribution are unnoticeable.
Using the above ğ‘-value in the ğœ’2 distribution, we only accept
perturbations ğºâ€² = (ğ´â€²,ğ‘‹â€²) where the degree distribution fulfills
Î›(ğº(0),ğºâ€²) < ğœâ‰ˆ0.004
Feature statistics preserving perturbations. While the above
principle could be applied to the nodesâ€™ features as well (e.g. preserving the distribution of feature occurrences), we argue that such
a procedure is too limited. In particular, such a test would not well
reflect the correlation/co-occurence of different features: If two
features have never occured together in ğº(0), but they do once in
ğºâ€², the distribution of feature occurences would still be very similar.
Such a change, however, is easily noticable. Think, e.g., about two
words which have never been used together but are suddenly used
in ğºâ€². Thus, we refer to a test based on feature co-occurrence.
Since designing a statistical test based on the co-occurences requires to model the joint distribution over features â€“ intractable
for correlated multivariate binary data â€“ we refer to a deterministic test. In this regard, setting features to 0 is uncritical since
it does not introduce new co-occurences. The question is: Which
features of a node ğ‘¢can be set to 1 to be regarded unnoticable?
To answer this question, we consider a probabilistic random
walker on the co-occurence graph ğ¶= (F, ğ¸) of features from ğº(0),
i.e. F is the set of features and ğ¸âŠ†F Ã— F denotes which features
have occurred together so far. We argue that adding a feature ğ‘–is
unnoticeable if the probability of reaching it by a random walker
starting at the features originally present for nodeğ‘¢and performing
one step is significantly large. Formally, let ğ‘†ğ‘¢= {ğ‘—| ğ‘‹ğ‘¢ğ‘—â‰ 0} be
the set of all features originally present for node ğ‘¢. We consider
addition of feature ğ‘–âˆ‰ğ‘†ğ‘¢to node ğ‘¢as unnoticeable if
ğ‘(ğ‘–| ğ‘†ğ‘¢) =
1/ğ‘‘ğ‘—Â· ğ¸ğ‘–ğ‘—> ğœ.
where ğ‘‘ğ‘—denotes the degree in the co-occurrence graph ğ¶. That is,
given that the probabilistic walker has started at any feature ğ‘—âˆˆğ‘†ğ‘¢,
after performing one step it would reach the feature ğ‘–at least with
probability ğœ. In our experiments we simply picked ğœto be half of
the maximal achievable probably, i.e. ğœ= 0.5 Â·
The above principle has two desirable effects: First, features ğ‘–
which have co-occurred with many of ğ‘¢â€™s features (i.e. in other
nodes) have a high probability; they are less noticeable when being
added. Second, features ğ‘–that only co-occur with features ğ‘—âˆˆğ‘†ğ‘¢
that are not specific to the node ğ‘¢(e.g. features ğ‘—which co-occur
with almost every other feature; stopwords) have low probability;
adding ğ‘–would be noticeable. Thus, we obtain the desired result.
Using the above test, we only accept perturbations ğºâ€² = (ğ´â€²,ğ‘‹â€²)
where the feature values fulfill
âˆ€ğ‘¢âˆˆV : âˆ€ğ‘–âˆˆF : ğ‘‹â€²
ğ‘¢ğ‘–= 1 â‡’ğ‘–âˆˆğ‘†ğ‘¢âˆ¨ğ‘(ğ‘–|ğ‘†ğ‘¢) > ğœ
In summary, to ensure unnoticeable perturbations, we update
our problem definition to:
Problem 2. Same as Problem 1 but replacing Pğº0
Î”,A with the more
restricted set Ë†Pğº0
Î”,A of graphs that additionally preserve the degree
distribution (Eq. 10) and feature co-occurence (Eq. 12).
GENERATING ADVERSARIAL GRAPHS
Solving Problem 1/2 is highly challenging. While (continuous) bilevel problems for attacks have been addressed in the past by gradient computation based on first-order KKT conditions , such
a solution is not possible in our case due to the dataâ€™s discreteness
and the large number of parameters ğœƒ. Therefore, we propose a
sequential approach, where we first attack a surrogate model, thus,
leading to an attacked graph. This graph is subsequently used to
train the final model. Indeed, this approach can directly be considered as a check for transferability since we do not specifically focus
on the used model but only on a surrogate one.
Surrogate model. To obtain a tractable surrogate model that
still captures the idea of graph convolutions, we perform a linearizion of the model from Eq. 2. That is, we replace the nonlinearity ğœ(.) with a simple linear activation function, leading to:
ğ‘â€² = softmax
Ë†ğ´Ë†ğ´ğ‘‹ğ‘Š(1) ğ‘Š(2)
Since ğ‘Š(1) and ğ‘Š(2) are (free) parameters to be learned, they can
be absorbed into a single matrix ğ‘ŠâˆˆRğ·Ã—ğ¾.
Since our goal is to maximize the difference in the log-probabilities
of the target ğ‘£0 (given a certain budget Î”), the instance-dependent
normalization induced by the softmax can be ignored. Thus, the
log-probabilities can simply be reduced to Ë†ğ´2 ğ‘‹ğ‘Š. Accordingly,
given the trained surrogate model on the (uncorrupted) input data
with learned parameters ğ‘Š, we define the surrogate loss
Lğ‘ (ğ´,ğ‘‹;ğ‘Š, ğ‘£0) = max
ğ‘â‰ ğ‘ğ‘œğ‘™ğ‘‘[ Ë†ğ´2 ğ‘‹ğ‘Š]ğ‘£0ğ‘âˆ’[ Ë†ğ´2 ğ‘‹ğ‘Š]ğ‘£0ğ‘ğ‘œğ‘™ğ‘‘
and aim to solve
(ğ´â€²,ğ‘‹â€²) âˆˆË†Pğº0
Lğ‘ (ğ´â€²,ğ‘‹â€²;ğ‘Š, ğ‘£0).
While being much simpler, this problem is still intractable to
solve exactly due to the discrete domain and the constraints. Thus,
in the following we introduce a scalable greedy approximation
scheme. For this, we define scoring functions that evaluate the surrogate loss from Eq. (14) obtained after adding/removing a feature
ğ‘“= (ğ‘¢,ğ‘–) or edge ğ‘’= (ğ‘¢, ğ‘£) to an arbitrary graph ğº= (ğ´,ğ‘‹):
ğ‘ ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡(ğ‘’;ğº, ğ‘£0) := Lğ‘ (ğ´â€²,ğ‘‹;ğ‘Š, ğ‘£0)
ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡(ğ‘“;ğº, ğ‘£0) := Lğ‘ (ğ´,ğ‘‹â€²;ğ‘Š, ğ‘£0)
where ğ´â€² := ğ´Â± ğ‘’(i.e. ğ‘â€²ğ‘¢ğ‘£= ğ‘â€²ğ‘£ğ‘¢= 1 âˆ’ğ‘ğ‘¢ğ‘£)3 and ğ‘‹â€² := ğ‘‹Â± ğ‘“(i.e.
ğ‘¢ğ‘–= 1 âˆ’ğ‘¥ğ‘¢ğ‘–).
Approximate Solution. Algorithm 1 shows the pseudo-code.
In detail, following a locally optimal strategy, we sequentially â€™manipulateâ€™ the most promising element: either an entry from the adjacency matrix or a feature entry (taking the constraints into account).
That is, given the current state of the graph ğº(ğ‘¡), we compute a
candidate set ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡of allowable elements (ğ‘¢, ğ‘£) whose change
from 0 to 1 (or vice versa; hence the Â± sign in the pseudocode)
does not violate the constraints imposed by Ë†Pğº0
Î”,A. Among these elements we pick the one which obtains the highest difference in the
log-probabilites, indicated by the score function ğ‘ ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡(ğ‘’;ğº(ğ‘¡), ğ‘£0).
Similar, we compute the candidate set ğ¶ğ‘“ğ‘’ğ‘ğ‘¡and the score function
ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡(ğ‘“;ğº(ğ‘¡), ğ‘£0) for every allowable feature manipulation of feature ğ‘–and node ğ‘¢. Whichever change obtains the higher score is
picked and the graph accordingly updated to ğº(ğ‘¡+1). This process
is repeated until the budget Î” has been exceeded.
To make Algorithm 1 tractable, two core aspects have to hold:
(i) an efficient computation of the score functions ğ‘ ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡and ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡,
and (ii) an efficient check which edges and features are compliant
with our constraints Ë†Pğº0
Î”,A, thus, forming the setsğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡andğ¶ğ‘“ğ‘’ğ‘ğ‘¡.
In the following, we describe these two parts in detail.
Fast computation of score functions
Structural attacks. We start by describing how to compute ğ‘ ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡.
For this, we have to compute the class prediction (in the surrogate
model) of node ğ‘£0 after adding/removing an edge (ğ‘š,ğ‘›). Since we
are now optimizing w.r.t. ğ´, the term ğ‘‹ğ‘Šin Eq. (14) is a constant
â€“ we substitute it with ğ¶:= ğ‘‹ğ‘ŠâˆˆRğ‘Ã—ğ¾. The log-probabilities of
node ğ‘£0 are then given by ğ‘”ğ‘£0 = [ Ë†ğ´2]ğ‘£0 Â· ğ¶âˆˆR1Ã—ğ¾where [ Ë†ğ´2]ğ‘£0
denotes a row vector. Thus, we only have to inspect how this row
vector changes to determine the optimal edge manipulation.
Naively recomputing [ Ë†ğ´2]ğ‘£0 for every element from the candidate set, though, is not practicable. An important observation to
3Please note that by modifying a single element ğ‘’= (ğ‘¢, ğ‘£) we always change two
entries, ğ‘ğ‘¢ğ‘£and ğ‘ğ‘£ğ‘¢, of ğ´since we are operating on an undirected graph.
Algorithm 1: Nettack: Adversarial attacks on graphs
Input: Graph ğº(0) â†(ğ´(0),ğ‘‹(0)), target node ğ‘£0,
attacker nodes A, modification budget Î”
Output: Modified Graph ğºâ€² = (ğ´â€²,ğ‘‹â€²)
Train surrogate model on ğº(0) to obtain ğ‘Š// Eq. (13);
while |ğ´(ğ‘¡) âˆ’ğ´(0) | + |ğ‘‹(ğ‘¡) âˆ’ğ‘‹(0) | < Î” do
ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡â†candidate_edge_perturbations(ğ´(ğ‘¡), A) ;
ğ‘’âˆ—= (ğ‘¢âˆ—, ğ‘£âˆ—) â†arg max
ğ‘’;ğº(ğ‘¡), ğ‘£0
ğ¶ğ‘“ğ‘’ğ‘ğ‘¡â†candidate_feature_perturbations(ğ‘‹(ğ‘¡), A) ;
ğ‘“âˆ—= (ğ‘¢âˆ—,ğ‘–âˆ—) â†arg max
ğ‘“;ğº(ğ‘¡), ğ‘£0
if ğ‘ ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡(ğ‘’âˆ—;ğº(ğ‘¡), ğ‘£0) > ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡(ğ‘“âˆ—;ğº(ğ‘¡), ğ‘£0) then
ğº(ğ‘¡+1) â†ğº(ğ‘¡) Â± ğ‘’âˆ—;
else ğº(ğ‘¡+1) â†ğº(ğ‘¡) Â± ğ‘“âˆ—;
return :ğº(ğ‘¡)
// Train final graph model on the corrupted graph ğº(ğ‘¡);
alleviate this problem is that in the used two-layer GCN the prediction for each node is influenced by its two-hop neighborhood only.
That is, the above row vector is zero for most of the elements. And
even more important, we can derive an incremental update â€“ we
donâ€™t have to recompute the updated [ Ë†ğ´2]ğ‘£0 from scratch.
Theorem 5.1. Given an adjacency matrix ğ´, and its corresponding
matrices Ëœğ´, Ë†ğ´2, Ëœğ·. Denote with ğ´â€² the adjacency matrix when adding
or removing the element ğ‘’= (ğ‘š,ğ‘›) from ğ´. It holds:
[ Ë†ğ´â€²2]ğ‘¢ğ‘£=
Ëœğ‘‘ğ‘¢Ëœğ‘‘ğ‘£[ Ë†ğ´2]ğ‘¢ğ‘£âˆ’Ëœğ‘ğ‘¢ğ‘£
where Ëœğ‘‘â€², ğ‘â€², and Ëœğ‘â€², are defined as (using the Iverson bracket I):
ğ‘˜= Ëœğ‘‘ğ‘˜+ I[ğ‘˜âˆˆ{ğ‘š,ğ‘›}] Â· (1 âˆ’2 Â· ğ‘ğ‘šğ‘›)
ğ‘˜ğ‘™= ğ‘ğ‘˜ğ‘™+ I[{ğ‘˜,ğ‘™} = {ğ‘š,ğ‘›}] Â· (1 âˆ’2 Â· ğ‘ğ‘˜ğ‘™)
ğ‘˜ğ‘™= Ëœğ‘ğ‘˜ğ‘™+ I[{ğ‘˜,ğ‘™} = {ğ‘š,ğ‘›}] Â· (1 âˆ’2 Â· Ëœğ‘ğ‘˜ğ‘™)
Proof. Let ğ‘†and ğ‘†â€² be defined as ğ‘†= Ãğ‘
. We have [ Ë†ğ´]ğ‘¢ğ‘£=
. If ğ‘¢â‰ ğ‘£, then
[ Ë†ğ´]ğ‘¢ğ‘˜[ Ë†ğ´]ğ‘˜ğ‘£=
Having the above equation for Ë†ğ´â€²2, we get
 âˆ’[ Ë†ğ´2]ğ‘¢ğ‘£
+ (ğ‘†â€² âˆ’ğ‘†).
After replacing ğ‘†â€² âˆ’ğ‘†= âˆ’ğ‘ğ‘¢ğ‘šğ‘ğ‘šğ‘£
the above equation, it is straightforward to derive Eq. 17. Deriving
this equation for the case ğ‘¢= ğ‘£is similar. Eq. 17 encompasses both
Eq. (17) enables us to update the entries in Ë†ğ´2 in constant time;
and in a sparse and incremental manner. Remember that all Ëœğ‘ğ‘¢ğ‘£,
ğ‘ğ‘¢ğ‘£, and ğ‘â€²ğ‘¢ğ‘£are either 1 or 0, and their corresponding matrices are
sparse. Given this highly efficient update of [ Ë†ğ´2]ğ‘£0 to [ Ë†ğ´â€²2]ğ‘£0, the
updated log-probabilities and, thus, the final score according to Eq.
(15) can be easily computed.
Feature attacks. The feature attacks are much easier to realize.
Indeed, by fixing the class ğ‘â‰ ğ‘ğ‘œğ‘™ğ‘‘with currently largest logprobability score [ Ë†ğ´2 ğ‘‹ğ‘Š]ğ‘£0ğ‘, the problem is linear in ğ‘‹and every
entry of ğ‘‹acts independently. Thus, to find the best node and
feature (ğ‘¢âˆ—,ğ‘–âˆ—) we only need to compute the gradients
 [ Ë†ğ´2 ğ‘‹ğ‘Š]ğ‘£0ğ‘âˆ’[ Ë†ğ´2 ğ‘‹ğ‘Š]ğ‘£0ğ‘ğ‘œğ‘™ğ‘‘
= [ Ë†ğ´2]ğ‘£0ğ‘¢
 [ğ‘Š]ğ‘–ğ‘âˆ’[ğ‘Š]ğ‘–ğ‘ğ‘œğ‘™ğ‘‘
and subsequently pick the one with the highest absolute value
that points into an allowable direction (e.g. if the feature was 0,
the gradient needs to point into the positives). The value of the
score function ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡for this best element is then simply obtained
by adding |Î¥ğ‘¢ğ‘–| to the current value of the loss function:
Lğ‘ (ğ´,ğ‘‹;ğ‘Š, ğ‘£0) + |Î¥ğ‘¢ğ‘–| Â· I[(2 Â· ğ‘‹ğ‘¢ğ‘–âˆ’1) Â· Î¥ğ‘¢ğ‘–< 0]
All this can be done in constant time per feature. The elements
where the gradient points outside the allowable direction should
not be perturbed since they would only hinder the attack â€“ thus,
the old score stays unchanged.
Fast computation of candidate sets
Last, we have to make sure that all perturbations are valid according
to the constraints Ë†Pğº0
Î”,A. For this, we defined the sets ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡and
ğ¶ğ‘“ğ‘’ğ‘ğ‘¡. Clearly, the constraints introduced in Eq. 4 and 5 are easy
to ensure. The budget constraint Î” is fulfilled by the process of
the greedy approach, while the elements which can be perturbed
according to Eq. 4 can be precomputed. Likewise, the node-feature
combinations fulfilling the co-occurence test of Eq. 12 can be precomputed. Thus, the set ğ¶ğ‘“ğ‘’ğ‘ğ‘¡only needs to be instantiated once.
The significance test for the degree distribution, however, does
not allow such a precomputation since the underlying degree distribution dynamically changes. How can we efficiently check whether
a potential perturbation of the edge (ğ‘š,ğ‘›) still preserves a similar
degree distribution? Indeed, since the individual degrees only interact additively, we can again derive a constant time incremental
update of our test statistic Î›.
Theorem 5.2. Given graph ğº= (ğ´,ğ‘‹) and the multiset Dğº(see
below Eq. 6). Denote with ğ‘…ğº= Ã
ğ‘‘ğ‘–âˆˆDğºlogğ‘‘ğ‘–the sum of log degrees.
Let ğ‘’= (ğ‘š,ğ‘›) be a candidate edge perturbation, and ğ‘‘ğ‘šand ğ‘‘ğ‘›the
degrees of the nodes in ğº. For ğºâ€² = ğºÂ± ğ‘’we have:
ğ›¼ğºâ€² = 1 + ğ‘›ğ‘’h
ğ‘…ğºâ€² âˆ’ğ‘›ğ‘’log
 = ğ‘›ğ‘’logğ›¼ğºâ€² + ğ‘›ğ‘’ğ›¼ğºâ€² logğ‘‘min âˆ’ ğ›¼ğºâ€² + 1 ğ‘…ğºâ€²
Perturbations
Surrogate loss Ls
Nettack-Struct
Nettack-Feat
Perturbations
Surrogate loss Ls
Nettack-In-U
Nettack-In
Figure 2: Average surrogate loss for increasing number of perturbations.
Different variants of our method on the Cora data. Larger is better.
Structure perturbations
No constraint
With constraint
Figure 3: Change in test
statistic Î› (degree distr.)
Figure 4: Gradient vs.
actual loss
ğ‘¥= 1 âˆ’2 Â· ğ‘ğ‘šğ‘›
ğ‘›ğ‘’= |Dğº| + (I[ğ‘‘ğ‘š+ 1 âˆ’ğ‘ğ‘šğ‘›= ğ‘‘min] + I[ğ‘‘ğ‘›+ 1 âˆ’ğ‘ğ‘šğ‘›= ğ‘‘min]) Â·ğ‘¥
ğ‘…ğºâ€² = ğ‘…ğºâˆ’I[ğ‘‘ğ‘šâ‰¥ğ‘‘min] logğ‘‘ğ‘š+ I[ğ‘‘ğ‘š+ ğ‘¥â‰¥ğ‘‘min] log(ğ‘‘ğ‘š+ ğ‘¥)
âˆ’I[ğ‘‘ğ‘›â‰¥ğ‘‘min] logğ‘‘ğ‘›+ I[ğ‘‘ğ‘›+ ğ‘¥â‰¥ğ‘‘min] log(ğ‘‘ğ‘›+ ğ‘¥).
Proof. Firstly, we show that if we incrementally compute ğ‘›ğ‘’
according to the update equation of Theorem 5.2, ğ‘›ğ‘’will be equal
to |Dğºâ€²|. The term I[ğ‘‘ğ‘š+ 1 âˆ’ğ‘ğ‘šğ‘›= ğ‘‘ğ‘šğ‘–ğ‘›] Â· ğ‘¥will be activated
(i.e. non-zero) only in two cases: 1) ğ‘ğ‘šğ‘›= 1 (i.e. ğºâ€² = ğºâˆ’ğ‘’), and
ğ‘‘ğ‘š= ğ‘‘ğ‘šğ‘–ğ‘›, then ğ‘¥< 0 and the update equation actually removes
node ğ‘šfrom Dğº. 2) ğ‘ğ‘šğ‘›= 0 (i.e. ğºâ€² = ğº+ ğ‘’), and ğ‘‘ğ‘š= ğ‘‘ğ‘šğ‘–ğ‘›âˆ’1,
then ğ‘¥> 0 and the update equation actually adds node ğ‘što Dğº.
A similar argumentation is applicable for node ğ‘›. Accordingly, we
have that ğ‘›ğ‘’= |Dğºâ€²|.
Similarly, one can show the valid incremental update for ğ‘…ğºâ€²
considering that only nodes with degree larger than ğ‘‘ğ‘šğ‘–ğ‘›are considered and that ğ‘‘ğ‘š+ ğ‘¥is the new degree. Having incremental
updates for ğ‘›ğ‘’and ğ‘…ğºâ€², the updates for ğ›¼ğºâ€² and ğ‘™(Dğºâ€²) follow
easily from their definitions.
Given ğº(ğ‘¡), we can now incrementally compute ğ‘™(Dğº(ğ‘¡)
= ğº(ğ‘¡) Â± ğ‘’. Equivalently we get incremental updates for
ğ‘™(Dğ‘ğ‘œğ‘šğ‘) after an edge perturbation. Since all r.h.s. of the equations above can be computed in constant time, also the test statistic
Î›(ğº(0),ğº(ğ‘¡)
) can be computed in constant time. Overall, the set
of valid candidate edge perturbations at iteration ğ‘¡is ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡=
{ğ‘’=(ğ‘š,ğ‘›) | Î›(ğº(0),ğº(ğ‘¡)
) < ğœâˆ§(ğ‘šâˆˆA âˆ¨ğ‘›âˆˆA)}. Since ğ‘…ğº(ğ‘¡)
can be incrementally updated to ğ‘…ğº(ğ‘¡+1) once the best edge perturbation has been performed, the full approach is highly efficient.
Complexity
The candidate set generation (i.e. which edges/features are allowed
to change) and the score functions can be incrementally computed
and exploit the graphâ€™s sparsity, thus, ensuring scalability. The
runtime complexity of the algorithm can easily be determined as:
O(Î” Â· |A| Â· (ğ‘Â· ğ‘¡â„ğ‘£0 + ğ·))
where ğ‘¡â„ğ‘£0 indicates the size of the two-hop neighborhood of the
node ğ‘£0 during the run of the algorithm.
In every of the Î” many iterations, each attacker evaluates the potential edge perturbations (ğ‘at most) and feature perturbations (ğ·
at most). For the former, this requires to update the two-hop neighborhood of the target due to the two convolution layers. Assuming
the graph is sparse, ğ‘¡â„ğ‘£0 is much smaller than ğ‘. The feature perturbations are done in constant time per feature. Since all constraints
can be checked in constant time they do not affect the complexity.
EXPERIMENTS
We explore how our attacks affect the surrogate model, and evaluate transferability to other models and for multiple datasets. For
repeatibility, Nettackâ€™s source code is available on our website:
 
Cora-ML 
CiteSeer 
Pol. Blogs 
Table 1: Dataset statistics. We only consider the largest connected component (LCC).
Setup. We use the well-known Cora-ML and Citeseer networks
as in , and Polblogs . The dataset characteristics are shown in
Table 1. We split the network in labeled (20%) and unlabeled nodes
(80%). We further split the labeled nodes in equal parts training and
validation sets to train our surrogate model. That is, we remove
the labels from the validation set in the training procedure and
use them as the stopping criterion (i.e., stop when validation error
increases). The labels of the unlabeled nodes are never visible to
the surrogate model during training.
We average over five different random initializations/ splits, where
for each we perform the following steps. We first train our surrogate
model on the labeled data and among all nodes from the test set that
have been correctly classified, we select (i) the 10 nodes with highest
margin of classification, i.e. they are clearly correctly classified,
(ii) the 10 nodes with lowest margin (but still correctly classified)
and (iii) 20 more nodes randomly. These will serve as the target
nodes for our attacks. Then, we corrupt the input graph using the
model proposed in this work, called Nettack for direct attacks, and
Nettack-In for influence attacks, respectively (picking 5 random
nodes as attackers from the neighborhood of the target).
Since no other competitors exist, we compare against two baselines:
(i) Fast Gradient Sign Method (FGSM) as a direct attack on
ğ‘£0 (in our case also making sure that the result is still binary). (ii)
Rnd is an attack in which we modify the structure of the graph.
Given our target node ğ‘£0, in each step we randomly sample nodes
ğ‘¢for which ğ‘ğ‘£0 â‰ ğ‘ğ‘¢and add the edge ğ‘¢, ğ‘£to the graph structure,
assuming unequal class labels are hindering classification.
Class: neural networks
Class: theory
Class: probabilistic models
constrained
unconstrained
constrained
unconstrained
constrained
unconstrained
probabilistic
difference
probability
disjunctive
previously
accomplished
corporation
probabilities
generality
observations
expectation
performing
refinement
represents
estimation
specifications
distributions
functional
independence
difference
observations
acquisition
management
requirement
Table 2: Top-10 feature perturbations per class on Cora
Attacks on the surrogate model
We start by analyzing different variants of our method by inspecting
their influence on the surrogate model. In Fig. 2 (left) we plot the
surrogate loss when performing a specific number of perturbations.
Note that once the surrogate loss is positive, we realized a successful
misclassification. We analyze Nettack, and variants where we
only manipulate features or only the graph structure. As seen,
perturbations in the structure lead to a stronger change in the
surrogate loss compared to feature attacks. Still, combining both
is the most powerful, only requiring around 3 changes to obtain a
misclassification. For comparison we have also added Rnd, which
is clearly not able to achieve good performance.
In Fig. 2 (right) we analyze our method when using a direct vs.
influencer attack. Clearly, direct attacks need fewer perturbations
â€“ still, influencer attacks are also possible, posing a high risk in
real life scenarios. The figure also shows the result when not using
our constraints as proposed in Section 4.1, indicated by the name
Nettack-U. As seen, even when using our constraints, the attack is
still succesfull. Thus, unnoticable perturbations can be generated.
Perturbations
Runtime (s)
4 influencers
2 influencers
1 influencer
Figure 5: Runtime
It is worth mentioning that
the constraints are indeed necessary. Figure 3 shows the test
statistic Î› of the resulting graph
with or without our constraints.
As seen the constraint we impose has an effect on our attack;
if not enforced, the power law
distribution of the corrupted graph becomes more and more dissimilar to the original graphâ€™s. Similarly, Table 2 illustrates the result for
the feature perturbations. For Cora-ML, the features correspond to
the presence of words in the abstracts of papers. For each class (i.e.
set of nodes with same label), we plot the top-10 features that have
been manipulated by the techniques (these account for roughly
50% of all perturbations). Further, we report for each feature its
original occurence within the class. We see that the used features
are indeed different â€“ even more, the unconstrained version often
uses words which are â€™unlikelyâ€™ for the class (indicated by the small
numbers). Using such words can easily be noticed as manipulations,
e.g. â€™davidâ€™ in neural networks or â€™chemicalâ€™ in probabilistic models.
Our constraint ensures that the changes are more subtle.
Overall, we conclude that attacking the features and structure
simultaneously is very powerful; and the introduced constraints do
not hinder the attack while generating more realistic perturbations.
Direct attacks are clearly easier than influencer attacks.
Lastly, even though not our major focus, we want to analyze
the required runtime of Nettack. In line with the derived complexity, in Fig. 5 we see that our algorithm scales linearly with the
number of perturbations to the graph structure and the number of
influencer nodes considered. Please note that we report runtime
for sequential processing of candidate edges; this can however be
trivially parallelized. Similar results were obtained for the runtime
w.r.t. the graph size, matching the complexity analysis.
Transferability of attacks
After exploring how our attack affects the (fixed) surrogate model,
we will now find out whether our attacks are also successful on
established deep learning models for graphs. For this, we pursue
the approach from before and use a budget of Î” = ğ‘‘ğ‘£0 +2, where ğ‘‘ğ‘£0
is the degree of target node we currently attack. This is motivated
by the observation that high-degree nodes are more difficult to
attack than low-degree ones. In the following we always report the
score ğ‘‹= ğ‘âˆ—ğ‘£0,ğ‘ğ‘œğ‘™ğ‘‘âˆ’maxğ‘â‰ ğ‘ğ‘œğ‘™ğ‘‘ğ‘âˆ—ğ‘£0,ğ‘using the ground truth label
ğ‘ğ‘œğ‘™ğ‘‘of the target. We call ğ‘‹the classification margin. The smaller
ğ‘‹, the better. For values smaller than 0, the targets get misclassified.
Note that this could even happen for the clean graph since the
classification itself might not be perfect.
Evasion vs. Poisoning Attack. In Figure 6a we evaluate Nettackâ€™s performance for two attack types: evasion attacks, where
the model parameters (here of GCN ) are kept fix based on the
clean graph; and poisoning attacks, where the model is retrained
after the attack (averaged over 10 runs). In the plot, every dot represents one target node. As seen, direct attacks are extremly succesful
â€“ even for the challening poisoning case almost every target gets
misclassified. We therefore conclude that our surrogate model and
loss are a sufficient approximation of the true loss on the non-linear
model after re-training on the perturbed data. Clearly, influencer
attacks (shown right of the double-line) are harder but they still
work in both cases. Since poisoining attacks are in general harder
and match better the transductive learning scenario, we report in
the following only these results.
Comparison. Figure 6b and 6c show that the corruptions generated by Nettack transfer to different (semi-supervised) graph
convolutional methods: GCN and CLN . Most remarkably,
even the unsupervised model DeepWalk is strongly affected by
our perturbations (Figure 6d). Since DW only handles unattributed
graphs, only structural attacks were performed. Following ,
node classification is performed by training a logistic regression on
Nettack-In.
Nettack-In.
Classifcation margin
(a) Evasion vs. poisoning for GCN
Nettack-In.
(b) Poisoning of GCN
Nettack-In.
(c) Poisoning of Column Network
Nettack-In.
misclassiï¬ed
(d) Poisoning of DeepWalk
Figure 6: Results on Cora data using different attack algorithms. Clean indicates the original data. Lower scores are better.
the learned embeddings. Overall, we see that direct attacks pose
a much harder problem than influencer attacks. In these plots, we
also compare against the two baselines Rnd and FGSM, both operating in the direct attack setting. As shown, Nettack outperforms
both. Again note: All these results are obtained using a challenging
poisoning attack (i.e. retraining of the model).
In Table 3 we summarize the results for different datasets and
classification models. Here, we report the fraction of target nodes
that get correctly classified. Our adversarial perturbations on the surrogate model are transferable to all three models an on all datasets
we evaluated. Not surprisingly, influencer attacks lead to a lower
decrease in performance compared to direct attacks.
We see that FGSM performs worse than Nettack, and we argue
that this comes from the fact that gradient methods are not optimal
for discrete data. Fig. 4 shows why this is the case: we plot the
gradient vs. the actual change in loss when changing elements in ğ´.
Often the gradients do not approximate the loss well â€“ in (b) and
(c) even the signs do not match. One key advantage of Nettack is
that we can precisely and efficiently compute the change in Lğ‘ .
Last, we also analyzed how the structure of the target, i.e. its
degree, affects the performance.
The table shows results for different degree ranges. As seen, high
degree nodes are slightly harder to attack: they have both, higher
classification accuracy in the clean graph and in the attacked graph.
Limited Knowledge. In the previous experiments, we have assumed full knowledge of the input graph, which is a reasonable
assumption for a worst-case attack. In Fig. 7 we analyze the result
when having limited knowledge: Given a target node ğ‘£0, we provided our model only subgraphs of increasing size relative to the
size of the Cora graph. We constructed these subgraphs by selecting
nodes with increasing distance from ğ‘£0, i.e. we first selected 1-hop
neighbors, then 2-hop neighbors and so on, until we have reached
Nettack-In
Table 3: Overview of results. Smaller is better.
Perturbations
Classification margin
Nettack 0.1
Nettack 0.2
Nettack 0.5
Nettack 0.75
(a) Direct attack
Perturbations
Nettack 0.1
Nettack 0.2
Nettack 0.5
Nettack 0.75
(b) Influence attack
Figure 7: Attacks with limited knowledge about the data
the desired graph size. We then perturbed the subgraphs using the
attack strategy proposed in this paper. These perturbations are then
taken over to full graph, where we trained GCN. Note that Nettack
has always only seen the subgraph; and its surrogate model is also
only trained based on it.
Fig. 7 shows the result for a direct attack. As seen, even if only
10% of the graph is observed, we can still significantly attack it.
Clearly, if the attacker knows the full graph, the fewest number
of perturbations is required. For comparison we include the Rnd
attack, also only operating on the subgraphs. In Fig. 7 we see the
influence attack. Here we require more perturbations and 75% of the
graph size for our attack to succeed. Still, this experiment indicates
that full knowledge is not required.
CONCLUSION
We presented the first work on adversarial attacks to (attributed)
graphs, specifically focusing on the task of node classification via
graph convolutional networks. Our attacks target the nodesâ€™ features and the graph structure. Exploiting the relational nature of the
data, we proposed direct and influencer attacks. To ensure unnoticeable changes even in a discrete, relational domain, we proposed to
preserve the graphâ€™s degree distribution and feature co-occurrences.
Our developed algorithm enables efficient perturbations in a discrete domain. Based on our extensive experiments we can conclude
that even the challenging poisoning attack is successful possible
with our approach. The classification performance is consistently
reduced, even when only partial knowledge of the graph is available or the attack is restricted to a few influencers. Even more, the
attacks generalize to other node classification models.
Studying the robustness of deep learning models for graphs is
an important problem, and this work provides essential insights
for deeper study. As future work we aim to derive extensions of
existing models to become more robust against attacks, and we aim
to study tasks beyond node classification.
ACKNOWLEDGEMENTS
This research was supported by the German Research Foundation,
Emmy Noether grant GU 1409/2-1, and by the Technical University
of Munich - Institute for Advanced Study, funded by the German
Excellence Initiative and the European Union Seventh Framework
Programme under grant agreement no 291763, co-funded by the
European Union.