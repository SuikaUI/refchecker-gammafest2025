Automatic Brain Tumor Segmentation using
Cascaded Anisotropic Convolutional Neural
Guotai Wang, Wenqi Li, S´ebastien Ourselin, and Tom Vercauteren
Translational Imaging Group, CMIC, University College London, UK
Wellcome/EPSRC Centre for Interventional and Surgical Sciences, UCL, London, UK
 
Abstract. A cascade of fully convolutional neural networks is proposed
to segment multi-modal Magnetic Resonance (MR) images with brain tumor into background and three hierarchical regions: whole tumor, tumor
core and enhancing tumor core. The cascade is designed to decompose
the multi-class segmentation problem into a sequence of three binary
segmentation problems according to the subregion hierarchy. The whole
tumor is segmented in the ﬁrst step and the bounding box of the result
is used for the tumor core segmentation in the second step. The enhancing tumor core is then segmented based on the bounding box of the
tumor core segmentation result. Our networks consist of multiple layers of anisotropic and dilated convolution ﬁlters, and they are combined
with multi-view fusion to reduce false positives. Residual connections
and multi-scale predictions are employed in these networks to boost the
segmentation performance. Experiments with BraTS 2017 validation set
show that the proposed method achieved average Dice scores of 0.7859,
0.9050, 0.8378 for enhancing tumor core, whole tumor and tumor core,
respectively. The corresponding values for BraTS 2017 testing set were
0.7831, 0.8739, and 0.7748, respectively.
Keywords: Brain tumor, convolutional neural network, segmentation
Introduction
Gliomas are the most common brain tumors that arise from glial cells. They can
be categorized into two basic grades: low-grade gliomas (LGG) that tend to exhibit benign tendencies and indicate a better prognosis for the patient, and highgrade gliomas (HGG) that are malignant and more aggressive. With the development of medical imaging, brain tumors can be imaged by various Magnetic Resonance (MR) sequences, such as T1-weighted, contrast enhanced T1-weighted
(T1c), T2-weighted and Fluid Attenuation Inversion Recovery (FLAIR) images.
Diﬀerent sequences can provide complementary information to analyze diﬀerent
subregions of gliomas. For example, T2 and FLAIR highlight the tumor with
peritumoral edema, designated “whole tumor” as per . T1 and T1c highlight
 
Guotai Wang, Wenqi Li, S´ebastien Ourselin, and Tom Vercauteren
the tumor without peritumoral edema, designated “tumor core” as per . An
enhancing region of the tumor core with hyper-intensity can also be observed in
T1c, designated “enhancing tumor core” as per .
Automatic segmentation of brain tumors and substructures has a potential to
provide accurate and reproducible measurements of the tumors. It has a great
potential for better diagnosis, surgical planning and treatment assessment of
brain tumors . However, this segmentation task is challenging because 1)
the size, shape, and localization of brain tumors have considerable variations
among patients. This limits the usability and usefulness of prior information
about shape and location that are widely used for robust segmentation of many
other anatomical structures ; 2) the boundaries between adjacent structures are often ambiguous due to the smooth intensity gradients, partial volume
eﬀects and bias ﬁeld artifacts.
There have been many studies on automatic brain tumor segmentation over
the past decades . Most current methods use generative or discriminative approaches. Generative approaches explicitly model the probabilistic distributions
of anatomy and appearance of the tumor or healthy tissues . They often
present good generalization to unseen images by incorporating domain-speciﬁc
prior knowledge. However, accurate probabilistic distributions of brain tumors
are hard to model. Discriminative approaches directly learn the relationship between image intensities and tissue classes, and they require a set of annotated
training images for learning. Representative works include classiﬁcation based
on support vector machines and decision trees .
In recent years, discriminative methods based on deep neural networks have
achieved state-of-the-art performance for multi-modal brain tumor segmentation. In , a convolutional neural network (CNN) was proposed to exploit both
local and more global features for robust brain tumor segmentation. However,
their approach works on individual 2D slices without considering 3D contextual information. DeepMedic uses a dual pathway 3D CNN with 11 layers
for brain tumor segmentation. The network processes the input image at multiple scales and the result is post-processed by a fully connected Conditional
Random Field (CRF) to remove false positives. However, DeepMedic works on
local image patches and has a low inference eﬃciency. Recently, several ideas
to improve the segmentation performance of CNNs have been explored in the
literature. 3D U-Net allows end-to-end training and testing for volumetric image segmentation. HighRes3DNet proposes a compact end-to-end 3D CNN
structure that maintains high-resolution multi-scale features with dilated convolution and residual connection . Other works also propose using fully
convolutional networks , incorporating large visual contexts by employing
a mixture of convolution and downsampling operations , and handling
imbalanced training data by designing new loss functions and sampling
strategies .
The contributions of this work are three-fold. First, we propose a cascade
of CNNs to segment brain tumor subregions sequentially. The cascaded CNNs
separate the complex problem of multiple class segmentation into three simpler
Automatic Brain Tumor Segmentation using Cascaded Anisotropic CNNs
Segmentation of
whole tumor
Input multi-modal
Segmentation of
tumor core
Segmentation of
enhancing tumor core
Fig. 1. The proposed triple cascaded framework for brain tumor segmentation. Three
networks are proposed to hierarchically segment whole tumor (WNet), tumor core
(TNet) and enhancing tumor core (ENet) sequentially.
binary segmentation problems, and take advantage of the hierarchical structure
of tumor subregions to reduce false positives. Second, we propose a novel network structure with anisotropic convolution to deal with 3D images as a tradeoﬀamong receptive ﬁeld, model complexity and memory consumption. It uses
dilated convolution, residual connection and multi-scale prediction to improve
segmentation performance. Third, we propose to fuse the output of CNNs in
three orthogonal views for more robust segmentation of brain tumor.
Triple Cascaded Framework
The proposed cascaded framework is shown in Fig. 1. We use three networks to
hierarchically and sequentially segment substructures of brain tumor, and each
of these networks deals with a binary segmentation problem. The ﬁrst network
(WNet) segments the whole tumor from multi-modal 3D volumes of the same
patient. Then a bounding box of the whole tumor is obtained. The cropped region
Guotai Wang, Wenqi Li, S´ebastien Ourselin, and Tom Vercauteren
3x3x1 convolu+on
output channel Co
Residual block with
3x3x1 convolu+on
output channel Cl
2D down-sampling
2D up-sampling
1x1x3 convolu+on
output channel Co
Concatenate
Architecture of WNet and TNet
Architecture of ENet
Fig. 2. Our anisotropic convolutional networks with dilated convolution, residual connection and multi-scale fusion. ENet uses only one downsampling layer considering its
smaller input size.
of the input images based on the bounding box is used as the input of the second
network (TNet) to segment the tumor core. Similarly, image region inside the
bounding box of the tumor core is used as the input of the third network (ENet)
to segment the enhancing tumor core. In the training stage, the bounding boxes
are automatically generated based on the ground truth. In the testing stage, the
bounding boxes are generated based on the binary segmentation results of the
whole tumor and the tumor core, respectively. The segmentation result of WNet
is used as a crisp binary mask for the output of TNet, and the segmentation
result of TNet is used as a crisp binary mask for the output of ENet, which
serves as anatomical constraints for the segmentation.
Anisotropic Convolutional Neural Networks
For 3D neural networks, the balance among receptive ﬁeld, model complexity
and memory consumption should be considered. A small receptive ﬁeld leads the
model to only use local features, and a larger receptive ﬁeld allows the model
to also learn more global features. Many 2D networks use a very large receptive
ﬁeld to capture features from the entire image context, such as FCN and
U-Net . They require a large patch size for training and testing. Using a large
3D receptive ﬁeld also helps to obtain more global features for 3D volumes. However, the resulting large 3D patches for training consume a lot of memory, and
therefore restrict the resolution and number of features in the network, leading
to limited model complexity and low representation ability. As a trade-oﬀ, we
Automatic Brain Tumor Segmentation using Cascaded Anisotropic CNNs
propose anisotropic networks that take a stack of slices as input with a large receptive ﬁeld in 2D and a relatively small receptive ﬁeld in the out-plane direction
that is orthogonal to the 2D slices. The 2D receptive ﬁelds for WNet, TNet and
ENet are 217×217, 217×217, and 113×113, respectively. During training and
testing, the 2D sizes of the inputs are typically smaller than the corresponding
2D receptive ﬁelds. WNet, TNet and ENet have the same out-plane receptive
ﬁeld of 9. The architectures of these proposed networks are shown in Fig. 2.
All of them are fully convolutional and use 10 residual connection blocks with
anisotropic convolution, dilated convolution, and multi-scale prediction.
Anisotropic and Dilated Convolution. To deal with anisotropic receptive
ﬁelds, we decompose a 3D kernel with a size of 3×3×3 into an intra-slice kernel
with a size of 3×3×1 and an inter-slice kernel with a size of 1×1×3. Convolution
layers with either of these kernels have Co output channels and each is followed
by a batch normalization layer and an activation layer, as illustrated by blue
and green blocks in Fig. 2. The activation layers use Parametric Rectiﬁed Linear Units (PReLU) that have been shown better performance than traditional
rectiﬁed units . WNet and TNet use 20 intra-slice convolution layers and
four inter-slice convolution layers with two 2D downsampling layers. ENet uses
the same set of convolution layers as WNet but only one downsampling layer
considering its smaller input size. We only employ up to two layers of downsampling in order to avoid large image resolution reduction and loss of segmentation
details. After the downsampling layers, we use dilated convolution for intra-slice
kernels to enlarge the receptive ﬁeld within a slice. The dilation parameter is set
to 1 to 3 as shown in Fig. 2.
Residual Connection. For eﬀective training of deep CNNs, residual connections were introduced to create identity mapping connections to bypass the
parameterized layers in a network. Our WNet, TNet and ENet have 10 residual
blocks. Each of these blocks contains two intra-slice convolution layers, and the
input of a residual block is directly added to the output, encouraging the block to
learn residual functions with reference to the input. This can make information
propagation smooth and speed the convergence of training .
Multi-scale Prediction. With the kernel sizes used in our networks, shallow
layers learn to represent local and low-level features while deep layers learn to
represent more global and high-level features. To combine features at diﬀerent
scales, we use three 3×3×1 convolution layers at diﬀerent depths of the network
to get multiple intermediate predictions and upsample them to the resolution of
the input. A concatenation of these predictions are fed into an additional 3×3×1
convolution layer to obtain the ﬁnal score map. These layers are illustrated by
red blocks in Fig. 2. The outputs of these layers have Cl channels where Cl is
the number of classes for segmentation in each network. Cl equals to 2 in our
method. A combination of predictions from multiple scales has also been used
in .
Guotai Wang, Wenqi Li, S´ebastien Ourselin, and Tom Vercauteren
Illustration of multi-view fusion at one level of the proposed cascade. Due
to the anisotropic receptive ﬁeld of our networks, we average the softmax outputs in
axial, sagittal and coronal views. The orange boxes show examples of sliding windows
for testing. Multi-view fusion is implemented for WNet, TNet, and ENet, respectively.
Multi-view Fusion. Since the anisotropic convolution has a small receptive
ﬁeld in the out-plane direction, to take advantage of 3D contextual information,
we fuse the segmentation results from three diﬀerent orthogonal views. Each of
WNet, TNet and ENet was trained in axial, sagittal and coronal views respectively. During the testing time, predictions in these three views are fused to get
the ﬁnal segmentation. For the fusion, we average the softmax outputs in these
three views for each level of the cascade of WNet, TNet, and ENet, respectively.
An illustration of multi-view fusion at one level is shown in Fig. 3.
Experiments and Results
Data and Implementation Details. We used the BraTS 20171 
dataset for experiments. The training set contains images from 285 patients (210
HGG and 75 LGG). The BraTS 2017 validation and testing set contain images
from 46 and 146 patients with brain tumors of unknown grade, respectively.
Each patient was scanned with four sequences: T1, T1c, T2 and FLAIR. All the
images were skull-striped and re-sampled to an isotropic 1mm3 resolution, and
the four sequences of the same patient had been co-registered. The ground truth
were obtained by manual segmentation results given by experts. We uploaded
the segmentation results obtained by the experimental algorithms to the BraTS
2017 server, and the server provided quantitative evaluations including Dice score
and Hausdorﬀdistance compared with the ground truth.
Our networks were implemented in Tensorﬂow2 using NiftyNet34 . We
used Adaptive Moment Estimation (Adam) for training, with initial learning
rate 10−3, weight decay 10−7, batch size 5, and maximal iteration 30k. Training
was implemented on an NVIDIA TITAN X GPU. The training patch size was
144×144×19, 96×96×19, and 64×64×19 for WNet, TNet and ENet, respectively.
We set Co to 32 and Cl to 2 for these three types of networks. For pre-processing,
1 
2 
3 
4 
Automatic Brain Tumor Segmentation using Cascaded Anisotropic CNNs
(a) FLAIR image
(b) Ground truth
(c) Without multi-view
(d) With multi-view
Fig. 4. Segmentation result of the brain tumor (HGG) from a training image. Green:
edema; Red: non-enhancing tumor core; Yellow: enhancing tumor core. (c) shows the
result when only networks in axial view are used, with mis-segmentations highlighted
by white arrows. (d) shows the result with multi-view fusion.
the images were normalized by the mean value and standard deviation of the
training images for each sequence. We used the Dice loss function for
training of each network.
Segmentation Results. Fig. 4 and Fig. 5 show examples for HGG and LGG
segmentation from training images, respectively. In both ﬁgures, for simplicity of
visualization, only the FLAIR image is shown. The green, red and yellow colors
show the edema, non-enhancing and enhancing tumor cores, respectively. We
compared the proposed method with its variant that does not use multi-view
fusion. For this variant, we trained and tested the networks only in axial view.
In Fig. 4, segmentation without and with multi-view fusion are presented in
the third and forth columns, respectively. It can be observed that the segmentation without multi-view fusion shown in Fig. 4(c) has some noises for edema
and enhancing tumor core, which is highlighted by white arrows. In contrast,
the segmentation with multi-view fusion shown in Fig. 4(d) is more accurate.
In Fig. 5, the LGG does not contain enhancing regions. The two counterparts
Guotai Wang, Wenqi Li, S´ebastien Ourselin, and Tom Vercauteren
(a) FLAIR image
(b) Ground truth
(c) Without multi-view
(d) With multi-view
Fig. 5. Segmentation result of the brain tumor (LGG) from a training image. Green:
edema; Red: non-enhancing tumor core; Yellow: enhancing tumor core. (c) shows the
result when only networks in axial view are used, with mis-segmentations highlighted
by white arrows. (d) shows the result with multi-view fusion.
achieve similar results for the whole tumor. However, it can be observed that
the segmentation of tumor core is more accurate by using multi-view fusion.
Table 1 presents quantitative evaluations with the BraTS 2017 validation
set. It shows that the proposed method achieves average Dice scores of 0.7859,
0.9050 and 0.8378 for enhancing tumor core, whole tumor and tumor core, respectively. For comparison, the variant without multi-view fusion obtains average
Dice scores of 0.7411, 0.8896 and 0.8255 fore these three regions, respectively.
Table 2 presents quantitative evaluations with the BraTS 2017 testing set.
It shows the mean values, standard deviations, medians, 25 and 75 quantiles of
Dice and Hausdorﬀdistance. Compared with the performance on the validation
set, the performance on the testing set is lower, with average Dice scores of
0.7831, 0.8739, and 0.7748 for enhancing tumor core, whole tumor and tumor
core, respectively. The higher median values show that good segmentation results
Automatic Brain Tumor Segmentation using Cascaded Anisotropic CNNs
Table 1. Mean values of Dice and Hausdorﬀmeasurements of the proposed method on
BraTS 2017 validation set. EN, WT, TC denote enhancing tumor core, whole tumor
and tumor core, respectively.
Hausdorﬀ(mm)
Without multi-view fusion 0.7411 0.8896 0.8255 5.3178 12.4566 9.6616
With multi-view fusion
0.7859 0.9050 0.8378 3.2821 3.8901 6.4790
Table 2. Dice and Hausdorﬀmeasurements of the proposed method on BraTS 2017
testing set. EN, WT, TC denote enhancing tumor core, whole tumor and tumor core,
respectively.
Hausdorﬀ(mm)
0.7831 0.8739 0.7748 15.9003 6.5528 27.0472
Standard deviation 0.2215 0.1319 0.2700 67.8552 10.6915 84.4297
0.8442 0.9162 0.8869 1.7321
25 quantile
0.7287 0.8709 0.7712 1.4142
75 quantile
0.8882 0.9420 0.9342 3.1217
are achieved for most images, and some outliers contributed to the lower average
Discussion and Conclusion
There are several beneﬁts of using a cascaded framework for segmentation of
hierarchical structures . First, compared with using a single network for all
substructures of the brain tumor that requires complex network architectures,
using three binary segmentation networks allows for a simpler network for each
task. Therefore, they are easier to train and can reduce over-ﬁtting. Second, the
cascade helps to reduce false positives since TNet works on the region extracted
by WNet, and ENet works on the region extracted by TNet. Third, the hierarchical pipeline follows anatomical structures of the brain tumor and uses them as
spatial constraints. The binary crisp masks restrict the tumor core to be inside
the whole tumor region and enhancing tumor core to be inside the tumor core
region, respectively. In , the hierarchical structural information was leveraged
to design a Generalised Wasserstein Dice loss function for imbalanced multiclass segmentation. However, that work did not use the hierarchical structural
information as spatial constraints. One drawback of the cascade is that it is not
end-to-end and requires longer time for training and testing compared with its
multi-class counterparts using similar structures. However, we believe this is not
an important issue for automatic brain tumor segmentation. Also, at inference
time, our framework is more computationally eﬃcient than most competitive
approaches including ScaleNet and DeepMedic .
Guotai Wang, Wenqi Li, S´ebastien Ourselin, and Tom Vercauteren
The results show that our proposed method achieved competitive performance for automatic brain tumor segmentation. Fig. 4 and Fig. 5 demonstrate
that the multi-view fusion helps to improve segmentation accuracy. This is
mainly because our networks are designed with anisotropic receptive ﬁelds. The
multi-view fusion is an ensemble of networks in three orthogonal views, which
takes advantage of 3D contextual information to obtain higher accuracy. Considering the diﬀerent imaging resolution in diﬀerent views, it may be more reasonable to use a weighted average of axial, sagittal and coronal views rather than
a simple average of them in the testing stage . Our current results are not
post-processed by CRFs that have been shown eﬀective to get more spatially
regularized segmentation . Therefore, it is of interest to further improve our
segmentation results by using CRFs.
In conclusion, we developed a cascaded system to segment glioma subregions
from multi-modal brain MR images. We convert the multi-class segmentation
problem to three cascaded binary segmentation problems, and use three networks
to segment the whole tumor, tumor core and enhancing tumor core, respectively.
Our networks use an anisotropic structure, which considers the balance among
receptive ﬁeld, model complexity and memory consumption. We also use multiview fusion to reduce noises in the segmentation result. Experimental results
on BraTS 2017 validation set show that the proposed method achieved average
Dice scores of 0.7859, 0.9050, 0.8378 for enhancing tumor core, whole tumor and
tumor core, respectively. The corresponding values for BraTS 2017 testing set
were 0.7831, 0.8739, and 0.7748, respectively.
Acknowledgements. We would like to thank the NiftyNet team. This work
was supported through an Innovative Engineering for Health award by the Wellcome Trust [WT101957], Engineering and Physical Sciences Research Council
(EPSRC) [NS/A000027/1], the National Institute for Health Research University
College London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL
High Impact Initiative), a UCL Overseas Research Scholarship, a UCL Graduate
Research Scholarship, hardware donated by NVIDIA, and the Health Innovation
Challenge Fund [HICF-T4-275, WT 97914], a parallel funding partnership between the Department of Health and Wellcome Trust.