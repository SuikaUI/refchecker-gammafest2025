SmoothGrad: removing noise by adding noise
Daniel Smilkov 1 Nikhil Thorat 1 Been Kim 1 Fernanda Vi´egas 1 Martin Wattenberg 1
Explaining the output of a deep network remains
a challenge. In the case of an image classiﬁer,
one type of explanation is to identify pixels that
strongly inﬂuence the ﬁnal decision.
A starting point for this strategy is the gradient of the
class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that
elaborate on this basic idea. This paper makes
two contributions: it introduces SMOOTHGRAD,
a simple method that can help visually sharpen
gradient-based sensitivity maps, and it discusses
lessons in the visualization of these maps. We
publish the code for our experiments and a website with our results.
1. Introduction
Interpreting complex machine learning models, such as
deep neural networks, remains a challenge. Yet an understanding of how such models function is important both
for building applications and as a problem in its own right.
From health care domains to education , there are many domains where interpretability is important. For example, the pneumonia risk prediction case
study in showed that more interpretable
models can reveal important but surprising patterns in the
data that complex models overlooked. For reviews of interpretable models, see .
One case of interest is image classiﬁcation systems. Finding an “explanation” for a classiﬁcation decision could potentially shed light on the underlying mechanisms of such
systems, as well as helping in enhancing them. For example, the technique of deconvolution helped researchers
identify neurons that failed to learn any meaningful features, knowledge that was used to improve the network, as
in .
1Google Inc..
Correspondence to:
Daniel Smilkov
< >.
Copyright 2017 by the author(s).
A common approach to understanding the decisions of image classiﬁcation systems is to ﬁnd regions of an image
that were particularly inﬂuential to the ﬁnal classiﬁcation.
 . These approaches 
has an additional “de-noising” effect on sensitivity maps.
The two techniques (training with noise, and inferring with
noise) seem to have additive effect; performing them together yields the best results.
This paper compares the SMOOTHGRAD method to several
gradient-based sensitivity map methods and demonstrates
its effects. We provide a conjecture, backed by some empirical evidence, for why the technique works, and why it
might be more reﬂective of how the network is doing classiﬁcation. We also discuss several ways to enhance visualizations of these sensitivity maps. Finally, we also make
the code used to generate all the ﬁgures in this paper available, along with 200+ examples of each compared method
on the web at 
 
Sharper sensitivity maps: removing noise by adding noise
2. Gradients as sensitivity maps
Consider a system that classiﬁes an image into one class
from a set C. Given an input image x, many image classiﬁcation networks 
compute a class activation function Sc for each class c ∈C,
and the ﬁnal classiﬁcation class(x) is determined by which
class has the highest score. That is,
class(x) = argmaxc∈C Sc(x)
A mathematically clean way of locating “important” pixels
in the input image has been proposed by several authors,
e.g., . If the functions Sc are piecewise differentiable, for any image x one can construct a sensitivity map
Mc(x) simply by differentiating Mc with respect to the input, x. In particular, we can deﬁne
Mc(x) = ∂Sc(x)/∂x
Here ∂Sc represents the derivative (i.e. gradient) of Sc. Intuitively speaking, Mc represents how much difference a
tiny change in each pixel of x would make to the classiﬁcation score for class c. As a result, one might hope that the
resulting map Mc would highlight key regions.
In practice, the sensitivity map of a label does seem
to show a correlation with regions where that label is
present .
However, the sensitivity maps based on raw gradients are
typically visually noisy, as shown in Fig. 1. Moreover, as
this image shows, the correlations with regions a human
would pick out as meaningful are rough at best.
Figure 1. A noisy sensitivity map, based on the gradient of the
class score for gazelle for an image classiﬁcation network. Lighter
pixels indicate partial derivatives with higher absolute values. See
Section 3 for details on the visualization.
2.1. Previous work on enhancing sensitivity maps
There are several hypotheses for the apparent noise in raw
gradient visualizations. One possibility, of course, is that
the maps are faithful descriptions of what the network is doing. Perhaps certain pixels scattered, seemingly at random,
across the image are central to how the network is making
a decision. On the other hand, it is also possible that using
the raw gradient as a proxy for feature importance is not
optimal. Seeking better explanations of network decisions,
several prior works have proposed modiﬁcations to the basic technique of gradient sensitivity maps; we summarize a
few key examples here.
One issue with using the gradient as a measure of in-
ﬂuence is that an important feature may “saturate” the
function Sc. In other words, it may have a strong effect
globally, but with a small derivative locally. Several approaches, Layerwise Relevance Propagation , DeepLift , and more recently
Integrated Gradients , attempt to
address this potential problem by estimating the global importance of each pixel, rather than local sensitivity. Maps
created with these techniques are referred to as “saliency”
or “pixel attribution” maps.
Another strategy for enhancing sensitivity maps has been
to change or extend the backpropagation algorithm itself,
with the goal of emphasizing positive contributions to the
ﬁnal outcome. Two examples are the Deconvolution and Guided Backpropagation techniques, which modify the gradients
of ReLU functions by discarding negative values during the
backpropagation calculation. The intention is to perform a
type of “deconvolution” which will more clearly show features that triggered activations of high-level units. Similar
ideas appear in ,
which suggest ways to combine gradients of units at multiple levels.
In what follows, we provide detailed comparisons of
“vanilla” gradient maps with those created by integrated
gradient methods and guided backpropagation.
on terminology:
although the terms “sensitivity map”,
“saliency map”, and “pixel attribution map” have been used
in different contexts, in this paper, we will refer to these
methods collectively as “sensitivity maps.”
2.2. Smoothing noisy gradients
There is a possible explanation for the noise in sensitivity
maps, which to our knowledge has not been directly addressed in the literature: the derivative of the function Sc
may ﬂuctuate sharply at small scales. In other words, the
apparent noise one sees in a sensitivity map may be due
to essentially meaningless local variations in partial derivatives. After all, given typical training techniques there is
no reason to expect derivatives to vary smoothly. Indeed,
the networks in question typically are based on ReLU activation functions, so Sc generally will not even be continu-
Sharper sensitivity maps: removing noise by adding noise
ously differentiable.
Fig. 2 gives example of strongly ﬂuctuating partial derivatives. This ﬁxes a particular image x, and an image pixel
xi, and plots the values of ∂Sc
∂xi (t) as fraction of the maximum entry in the gradient vector, maxi ∂Sc
∂xi (t), for a short
line segment x + tϵ in the space of images parameterized
by t ∈ . We show it as a fraction of the maximum
entry in order to verify that the ﬂuctuations are signiﬁcant.
The length of this segment is small enough that the starting image x and the ﬁnal image x + ϵ looks the same to
a human. Furthermore, each image along the path is correctly classiﬁed by the model. The partial derivatives with
respect to the red, green, and blue components, however,
change signiﬁcantly.
Figure 2. The partial derivative of Sc with respect to the RGB values of a single pixel as a fraction of the maximum entry in the
gradient vector, maxi
∂xi (t), (middle plot) as one slowly moves
away from a baseline image x (left plot) to a ﬁxed location x + ϵ
(right plot). ϵ is one random sample from N(0, 0.012). The ﬁnal image (x + ϵ) is indistinguishable to a human from the origin
Given these rapid ﬂuctuations, the gradient of Sc at any
given point will be less meaningful than a local average
of gradient values. This suggests a new way to create improved sensitivity maps: instead of basing a visualization
directly on the gradient ∂Sc, we could base it on a smoothing of ∂Sc with a Gaussian kernel.
Directly computing such a local average in a highdimensional input space is intractable, but we can compute
a simple stochastic approximation. In particular, we can
take random samples in a neighborhood of an input x, and
average the resulting sensitivity maps. Mathematically, this
means calculating
Mc(x + N(0, σ2))
where n is the number of samples, and N(0, σ2) represents
Gaussian noise with standard deviation σ. We refer to this
method as SMOOTHGRAD throughout the paper.
3. Experiments
To assess the SMOOTHGRAD technique, we performed a
series of experiments using a neural network for image
classiﬁcation .
The results suggest the estimated smoothed gradient, ˆ
leads to visually more coherent sensitivity maps than the
unsmoothed gradient Mc, with the resulting visualizations
aligning better–to the human eye–with meaningful features.
Our experiments were carried out using an Inception v3
model that was trained on the
ILSVRC-2013 dataset and a
convolutional MNIST model based on the TensorFlow tutorial .
3.1. Visualization methods and techniques
Sensitivity maps are typically visualized as heatmaps.
Finding the right mapping from a channel values at a pixel
to a particular color turns out to be surprisingly nuanced,
and can have a large effect on the resulting impression of
the visualization. This section summarizes some visualization techniques and lessons learned in the process of comparing various sensitivity map work. Some of these techniques may be universally useful regardless of the choice
of sensitivity map methods.
Absolute value of gradients
Sensitivity map algorithms often produce signed values.
There is considerable ambiguity in how to convert signed
values to colors. A key choice is whether to represent positive and negative values differently, or to visualize the absolute value only. The utility of taking the absolute values of gradients or not depends on the characteristics of the
dataset of interest. For example, when the object of interest has the same color across the classes ), the
positive gradients indicate positive signal to the class. On
the other hand, for ImageNet dataset , we have found that taking the absolute value of the
gradient produced clearer pictures. One possible explanation for this phenomenon is that the direction is context dependent: many image recognition tasks are invariant under
color and illumination changes. For instance, in classifying
a ball, a dark ball on a bright background would have negative gradient, while white ball on darker background would
have a positive gradient.
Capping outlying values
Another property of the gradient that we observe is the
presence of few pixels that have much higher gradients than
the average. This is not a new discovery — this property
was utilized in generating adversarial examples that are in-
Sharper sensitivity maps: removing noise by adding noise
Figure 3. Effect of noise level (columns) on our method for 5 images of the gazelle class in ImageNet (rows). Each sensitivity map is
obtained by applying Gaussian noise N(0, σ2) to the input pixels for 50 samples, and averaging them. The noise level corresponds to
σ/(xmax −xmin).
distinguishable to humans . These
outlying values have the potential to throw off color scales
completely. Capping those extreme values to a relatively
high value (we ﬁnd 99th percentile to be sufﬁcient) leads
to more visually coherent maps as in . Without this post-processing step, maps may end up
almost entirely black.
Multiplying maps with the input images
Some techniques create a ﬁnal sensitivity map by multiplying gradient-based values and actual pixel values . This multiplication does tend to produce visually simpler and sharper
images, although it can be unclear how much of this can
be attributed to sharpness in the original image itself. For
example, a black/white edge in the input can lead to an
edge-like structure on the ﬁnal visualization even if the underlying sensitivity map has no edges.
However, this may result in undesired side effect. Pixels
with values of 0 will never show up on the sensitivity map.
For example, if we encode black as 0, the image of a classiﬁer that correctly predicts a black ball on a white background will never highlight the black ball in the image.
On the other hand, multiplying gradients with the input images makes sense when we view the importance of the feature as their contribution to the total score, y. For example,
in a linear system y = Wx, it makes sense to consider xiwi
as the contribution of xi to the ﬁnal score y.
For these reasons, we show our results with and without the
image multiplication in Fig. 5.
3.2. Effect of noise level and sample size
SMOOTHGRAD has two hyper-parameters: σ, the noise
level or standard deviation of the Gaussian perturbations,
and n, the number of samples to average over.
Fig. 3 shows the effect of noise level for several example
images from ImageNet . The 2nd
column corresponds to the standard gradient (0% noise),
which we will refer to as the “Vanilla” method throughout
the paper. Since quantitative evaluation of a map remains
an unsolved problem, we again focus on qualitative evaluation. We observe that applying 10%-20% noise (middle
columns) seems to balance the sharpness of sensitivity map
and maintain the structure of the original image.We also
observe that while this range of noise gives generally good
results for Inception, the ideal noise level depends on the
input. See Fig. 10 for a similar experiment on the MNIST
Sample size, n
In Fig. 4 we show the effect of sample size, n. As expected, the estimated gradient becomes smoother as the
Sharper sensitivity maps: removing noise by adding noise
sample size, n, increases. We empirically found a diminishing return — there was little apparent change in the visualizations for n > 50.
3.3. Qualitative comparison to baseline methods
Since there is no ground truth to allow for quantitative evaluation of sensitivity maps, we follow prior work and
focus on two aspects of qualitative evaluation.
First, we inspect visual coherence (e.g., the highlights are
only on the object of interest, not the background). Second,
we test for discriminativity, where in an image with both a
monkey and a spoon, one would expect an explanation for
a monkey classiﬁcation to be concentrated on the monkey
rather than the spoon, and vice versa.
Regarding visual coherence, Fig. 5 shows a side-by-side
comparison between our method and three gradient-based
methods: Integrated Gradients ,
Guided BackProp and vanilla
gradient. Among a random sample of 200 images that we
inspected, we found SMOOTHGRAD to consistently provide more visually coherent maps than Integrated Gradients and vanilla gradient. While Guided BackProp provides the most sharp maps (last three rows of Fig. 5), it
is prone to failure (ﬁrst three rows of Fig. 5), especially
for images with uniform background. On the contrary, our
observation is that SMOOTHGRAD has the highest impact
when the object is surrounded with uniform background
color (ﬁrst three rows of Fig. 5). Exploring this difference
is an interesting area for investigation. It is possible that
the smoothness of the class score function may be related
to spatial statistics of the underlying image; noise may have
a differential effect on the sensitivity to different textures.
Fig. 6 compares the discriminativity of our method to other
methods. Each image has at least two objects of different
classes that the network may recognize. To visually show
discriminativity, we compute the sensitivity maps M1(x)
and M2(x) for both classes, scale both to , and calculate the difference M1(x) −M2(x). We then plot the values on a diverging color map [−1, 0, 1] 7→[blue, gray, red].
For these images, SMOOTHGRAD qualitatively shows better discriminativity over the other methods. It remains an
open question to understand which properties affect the discriminativity of a given method – e.g. understanding why
Guided BackProp seems to show the weakest discriminativity.
3.4. Combining SmoothGrad with other methods
One can think of SMOOTHGRAD as smoothing the vanilla
gradient method using a simple procedure: averaging the
vanilla sensitivity maps of n noisy images. With that in
mind, the same smoothing procedure can be used to augment any gradient-based method. In Fig. 7 we show the
results of applying SMOOTHGRAD in combination with Integrated Gradients and Guided BackProp. We observe that
this augmentation improves the visual coherence of sensitivity maps for both methods.
For further analysis, we point the reader to our web page
at with sensitivity maps of
200+ images and four different methods.
3.5. Adding noise during training
SMOOTHGRAD as discussed so far may be applied to classiﬁcation networks as-is. In situations where there is a premium on legibility, however, it is natural to ask whether
there is a similar way to modify the network weights so
that its sensitivity maps are sharper. One idea that is parallel in some ways to SMOOTHGRAD is the well-known
regularization technique of adding noise to samples during
training . We ﬁnd that the same method also
improves the sharpness of the sensitivity map.
Fig. 8 and Fig. 9 show the effect of adding noise at training
time and/or evaluation time for the MNIST and Inception
model respectively. Interestingly, adding noise at training
time seems to also provide a de-noising effect on the sensitivity map. Lastly, the two techniques (training with noise,
and inferring with noise) seem to have additive effect; performing them together produces the most visually coherent
map of the 4 combinations.
4. Conclusion and future work
The experiments described here suggest that gradientbased sensitivity maps can be sharpened by two forms of
smoothing. First, averaging maps made from many small
perturbations of a given image seems to have a signiﬁcant
smoothing effect. Second, that effect can be enhanced further by training on data that has been perturbed with random noise.
These results suggest several avenues for future research.
First, while we have provided a plausibility argument for
our conjecture that noisy sensitivity maps are due to noisy
gradients, it would be worthwhile to look for further evidence and theoretical arguments that support or disconﬁrm
this hypothesis. It is certainly possible that the sharpening
effect of SMOOTHGRAD has other causes, such as a differential effect of random noise on different textures.
Second, in addition to training with noise, there may be
more direct methods to create systems with smoother class
score functions. For example, one could train with an explicit penalty on the size of partial derivatives. To create
Sharper sensitivity maps: removing noise by adding noise
Figure 4. Effect of sample size on the estimated gradient for inception. 10% noise was applied to each image.
Figure 5. Qualitative evaluation of different methods. First three (last three) rows show examples where applying SMOOTHGRAD had
high (low) impact on the quality of sensitivity map.
Sharper sensitivity maps: removing noise by adding noise
Figure 6. Discriminativity of different methods. For each image, we visualize the difference scale(∂y1/∂x) −scale(∂y2/∂x) where y1
and y2 are the logits for the ﬁrst and the second class (i.e., cat or dog) and scale() normalizes the gradient values to be between .
The values are plotted using a diverging color map [−1, 0, 1] 7→[blue, gray, red]. Each method is represented in columns.
Figure 7. Using SMOOTHGRAD in addition to existing gradient-based methods: Integrated Gradients and Guided BackProp.
Sharper sensitivity maps: removing noise by adding noise
Figure 8. Effect of adding noise during training vs evaluation for
Figure 9. Effect of adding noise during training vs evaluation for
Inception.
more spatial coherent maps, one could add a penalty for
large differences in partial derivatives of the class score
with respect to neighboring pixels. It may also be worth
investigating the geometry of the class score function to
understand why smoothing seems to be more effective on
images with large regions of near-constant pixel values.
A further area for exploration is to ﬁnd better metrics for
comparing sensitivity maps. To measure spatial coherence,
one might use existing databases of image segmentations,
and we are already making progress . Systematic measurements of discriminativity could also be valuable. Finally, a natural question
is whether the de-noising techniques described here generalize to other network architectures and tasks.
Acknowledgements
We thank Chris Olah for generously sharing his code and
helpful discussions, including pointing out the relation to
contractive autoencoders, and Mukund Sundararajan and
Qiqi Yan for useful discussions.