Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1577–1586,
Beijing, China, July 26-31, 2015. c⃝2015 Association for Computational Linguistics
Neural Responding Machine for Short-Text Conversation
Lifeng Shang, Zhengdong Lu, Hang Li
Noah’s Ark Lab
Huawei Technologies Co. Ltd.
Sha Tin, Hong Kong
{shang.lifeng,lu.zhengdong,hangli.hl}@huawei.com
Responding
Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoderdecoder framework: it formalizes the generation of response as a decoding process
based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with
a large amount of one-round conversation
data collected from a microblogging service.
Empirical study shows that NRM
can generate grammatically correct and
content-wise appropriate responses to over
75% of the input text, outperforming stateof-the-arts in the same setting, including
retrieval-based and SMT-based models.
Introduction
Natural language conversation is one of the
most challenging artiﬁcial intelligence problems,
which involves language understanding, reasoning, and the utilization of common sense knowledge. Previous works in this direction mainly focus on either rule-based or learning-based methods .
These types of methods often rely on manual effort
in designing rules or automatic training of model
with a particular learning algorithm and a small
amount of data, which makes it difﬁcult to develop
an extensible open domain conversation system.
Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2,
the amount of conversation data available on the
web has tremendously increased. This makes a
1 
2 
data-driven approach to attack the conversation
problem possible. Instead of multiple rounds of conversation,
the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short
texts, with the former being an input (referred to as
post) from a user and the latter a response given by
the computer. The research on STC may shed light
on understanding the complicated mechanism of
natural language conversation.
Previous methods for STC fall into two categories, 1) the retrieval-based method , and 2) the statistical machine translation
(SMT) based method .
The basic idea of retrievalbased method is to pick a suitable response by
ranking the candidate responses with a linear or
non-linear combination of various matching features (e.g. number of shared words). The main
drawbacks of the retrieval-based method are the
• the responses are pre-existing and hard to customize for the particular text or requirement
from the task, e.g., style or attitude.
• the use of matching features alone is usually not sufﬁcient for distinguishing positive responses from negative ones, even after
time consuming feature engineering. (e.g., a
penalty due to mismatched named entities is
difﬁcult to incorporate into the model)
The SMT-based method, on the other hand, is
generative. Basically it treats the response generation as a translation problem, in which the model is
trained on a parallel corpus of post-response pairs.
Despite its generative nature, the method is intrinsically unsuitable for response generation, because
the responses are not semantically equivalent to
the posts as in translation. Actually one post can
receive responses with completely different content, as manifested through the example in the fol-
lowing ﬁgure:
Having my ﬁsh sandwich right now
For god’s sake, it is 11 in the morning
Enhhhh... sounds yummy
which restaurant exactly?
Empirical studies also showed that SMT-based
methods often yield responses with grammatical
errors and in rigid forms, due to the unnecessary
alignment between the “source” post and the “target” response .
This rigidity is still a serious problem in the recent work
of , despite its use of neural network-based generative model as features in
In this paper, we take a probabilistic model to address the response generation problem, and propose employing a neural encoder-decoder for this
task, named Neural Responding Machine (NRM).
The neural encoder-decoder model, as illustrated
in Figure 1, ﬁrst summarizes the post as a vector
representation, then feeds this representation to a
decoder to generate responses. We further generalize this scheme to allow the post representation
to dynamically change during the generation process, following the idea in 
originally proposed for neural-network-based machine translation with automatic alignment.
Having my fish sandwich right now
For god's sake, it is 11 in the morning
Enhhhh... sounds yummy
which restaurant exactly?
Figure 1: The diagram of encoder-decoder framework for automatic response generation.
NRM essentially estimates the likelihood of a
response given a post. Clearly the estimated probability should be complex enough to represent all
the suitable responses.
Similar framework has
been used for machine translation with a remarkable success . Note that in machine translation, the task is to estimate the probability of a
target language sentence conditioned on the source
language sentence with the same meaning, which
is much easier than the task of STC which we
are considering here. In this paper, we demonstrate that NRM, when equipped with a reasonable
amount of data, can yield a satisfying estimator of
responses (hence response generator) for STC, despite the difﬁculty of the task.
Our main contributions are two-folds: 1) we
propose to use an encoder-decoder-based neural network to generate a response in STC; 2)
we have empirically veriﬁed that the proposed
method, when trained with a reasonable amount of
data, can yield performance better than traditional
retrieval-based and translation-based methods.
In the remainder of this paper, we start with introducing the dataset for STC in Section 2. Then
we elaborate on the model of NRM in Section 3,
followed by the details on training in Section 4.
After that, we report the experimental results in
Section 5. In Section 6 we conclude the paper.
The Dataset for STC
Our models are trained on a corpus of roughly 4.4
million pairs of conversations from Weibo 3.
Conversations on Sina Weibo
Weibo is a popular Twitter-like microblogging service in China, on which a user can post short messages (referred to as post in the reminder of this
paper) visible to the public or a group of users following her/him. Other users make comment on a
published post, which will be referred to as a response. Just like Twitter, Weibo also has the length
limit of 140 Chinese characters on both posts and
responses, making the post-response pair an ideal
surrogate for short-text conversation.
Dataset Description
To construct this million scale dataset, we ﬁrst
crawl hundreds of millions of post-response pairs,
and then clean the raw data in a similar way as
suggested in , including 1) removing trivial responses like “wow”, 2) ﬁltering
out potential advertisements, and 3) removing the
responses after ﬁrst 30 ones for topic consistency.
Table 1 shows some statistics of the dataset used
3 
#responses
#test posts
Labeled Dataset
(retrieval-based)
#responses
#labeled pairs
Fine Tuning
(SMT-based)
#responses
Table 1: Some statistics of the dataset. Labeled
Dataset and Fine Tuning are used by retrievalbased method for learning to rank and SMT-based
method for ﬁne tuning, respectively.
in this work. It can be seen that each post have 20
different responses on average. In addition to the
semantic gap between post and its responses, this
is another key difference to a general parallel data
set used for traditional translation.
Neural Responding Machines for STC
The basic idea of NRM is to build a hidden representation of a post, and then generate the response based on it, as shown in Figure 2.
the particular illustration, the encoder converts
the input sequence x = (x1, · · · , xT ) into a set
of high-dimensional hidden representations h =
(h1, · · · , hT ), which, along with the attention signal at time t (denoted as αt), are fed to the contextgenerator to build the context input to decoder at
time t (denoted as ct). Then ct is linearly transformed by a matrix L (as part of the decoder) into
a stimulus of generating RNN to produce the t-th
word of response (denoted as yt).
In neural translation system, L converts the representation in source language to that of target language. In NRM, L plays a more difﬁcult role: it
needs to transform the representation of post (or
some part of it) to the rich representation of many
plausible responses. It is a bit surprising that this
can be achieved to a reasonable level with a linear
transformation in the “space of representation”, as
validated in Section 5.3, where we show that one
post can actually invoke many different responses
The role of attention signal is to determine
which part of the hidden representation h should
be emphasized during the generation process. It
should be noted that αt could be ﬁxed over time or
Context Generator
Attention Signal
Figure 2: The general framework and dataﬂow of
the encoder-decoder-based NRM.
changes dynamically during the generation of response sequence y. In the dynamic settings, αt
can be function of historically generated subsequence (y1, · · · , yt−1), input sequence x or their
latent representations, more details will be shown
later in Section 3.2.
We use Recurrent Neural Network (RNN) for
both encoder and decoder, for its natural ability
to summarize and generate word sequence of arbitrary lengths .
Figure 3: The graphical model of RNN decoder.
The dashed lines denote the variables related to the
function g(·), and the solid lines denote the variables related to the function f(·).
The Computation in Decoder
Figure 3 gives the graphical model of the decoder, which is essentially a standard RNN language model except conditioned on the context input c. The generation probability of the t-th word
is calculated by
p(yt|yt−1, · · · , y1, x) = g(yt−1, st, ct),
where yt is a one-hot word representation, g(·) is
a softmax activation function, and st is the hidden
state of decoder at time t calculated by
st = f(yt−1, st−1, ct),
and f(·) is a non-linear activation function and
the transformation L is often assigned as parameters of f(·).
Here f(·) can be a logistic
function, the sophisticated long short-term memory (LSTM) unit , or the recently proposed gated recurrent
unit (GRU) .
Compared to “ungated” logistic function, LSTM
and GRU are specially designed for its long term
memory: it can store information over extended
time steps without too much decay. We use GRU
in this work, since it performs comparably to
LSTM on squence modeling , but has less parameters and easier to train.
We adopt the notation of GRU from , the hidden state st at time t is a linear
combination of its previous hidden state st−1 and
a new candidate state ˆst:
st = (1 −zt) ◦st−1 + zt ◦ˆst,
where ◦is point-wise multiplication, zt is the update gate calculated by
zt = σ (Wze(yt−1) + Uzst−1 + Lzct) ,
and ˆst is calculated by
ˆst=tanh (We(yt−1) + U(rt ◦st−1) + Lct) , (2)
where the reset gate rt is calculated by
rt = σ (Wre(yt−1) + Urst−1 + Lrct) .
In Equation (1)-(2), and (3), e(yt−1) is word embedding of the word yt−1, L = {L, Lz, Lr} speciﬁes the transformations to convert a hidden representation from encoder to that of decoder. In
the STC task, L should have the ability to transform one post (or its segments) to multiple different words of appropriate responses.
The Computation in Encoder
We consider three types of encoding schemes,
namely 1) the global scheme, 2) the local scheme,
and the hybrid scheme which combines 1) and 2).
Global Scheme:
Figure 4 shows the graphical
model of the RNN-encoder and related context
generator for a global encoding scheme.
hidden state at time t is calculated by ht
f(xt, ht−1) (i.e. still GRU unit), and with a trivial
context generation operation, we essentially use
the ﬁnal hidden state hT as the global representation of the sentence. The same strategy has been
taken in and for building the intermediate representation
for machine translation. This scheme however has
its drawbacks: a vectorial summarization of the
entire post is often hard to obtain and may lose important details for response generation, especially
when the dimension of the hidden state is not big
enough4. In the reminder of this paper, a NRM
with this global encoding scheme is referred to as
Context Generator
Figure 4: The graphical model of the encoder in
NRM-glo, where the last hidden state is used as
the context vector ct = hT .
 and Graves introduced an attention
mechanism that allows the decoder to dynamically
select and linearly combine different parts of the
input sequence ct = ∑T
j=1 αtjhj, where weighting factors αtj determine which part should be selected to generate the new word yt, which in turn
is a function of hidden states αtj = q(hj, st−1),
as pictorially shown in Figure 5. Basically, the attention mechanism αtj models the alignment between the inputs around position j and the output
at position t, so it can be viewed as a local matching model. This local scheme is devised in for automatic alignment be-
4Sutskever et al. has to use 4, 000 dimension for
satisfying performance on machine translation, while with a smaller dimension perform poorly on translating an entire sentence.
tween the source sentence and the partial target
sentence in machine translation. This scheme enjoys the advantage of adaptively focusing on some
important words of the input text according to the
generated words of response. A NRM with this
local encoding scheme is referred to as NRM-loc.
Attention Signal
Context Generator
Figure 5: The graphical model of the encoder in
NRM-loc, where the weighted sum of hidden sates
is used as the context vector ct = ∑T
j=1 αtjhj.
Extensions: Local and Global Model
In the task of STC, NRM-glo has the summarization of the entire post, while NRM-loc can adaptively select the important words in post for various suitable responses. Since post-response pairs
in STC are not strictly parallel and a word in different context can have different meanings, we conjecture that the global representation in NRM-glo
may provide useful context for extracting the local
context, therefore complementary to the scheme
in NRM-loc. It is therefore a natural extension
to combine the two models by concatenating their
encoded hidden states to form an extended hidden representation for each time stamp, as illustrated in Figure 6. We can see the summarization
T is incorporated into ct and αtj to provide a
global context for local matching. With this hybrid method, we hope both the local and global information can be introduced into the generation of
response. The model with this context generation
mechanism is denoted as NRM-hyb.
It should be noticed that the context generator
in NRM-hyb will evoke different encoding mechanisms in the global encoder and the local encoder,
although they will be combined later in forming
a uniﬁed representation.
More speciﬁcally, the
last hidden state of NRM-glo plays a role different from that of the last state of NRM-loc, since
it has the responsibility to encode the entire input
sentence. This role of NRM-glo, however, tends
to be not adequately emphasized in training the
hybrid encoder when the parameters of the two
encoding RNNs are learned jointly from scratch.
For this we use the following trick: we ﬁrst initialize NRM-hyb with the parameters of NRM-loc
and NRM-glo trained separately, then ﬁne tune the
parameters in encoder along with training the parameters of decoder.
global encoder
local encoder
Attention Signal
Context Generator
Figure 6: The graphical model for the encoder
in NRM-hyb, while context generator function is
j=1 αtj[hl
T ], here [hl
T ] denotes the
concatenation of vectors hl
To learn the parameters of the model, we maximize the likelihood of observing the original response conditioned on the post in the training set.
For a new post, NRMs generate their responses by
using a left-to-right beam search with beam size =
Experiments
We evaluate three different settings of NRM described in Section 3, namely NRM-glo, NRMloc, and NRM-hyb, and compare them to retrievalbased and SMT-based methods.
Implementation Details
We use Stanford Chinese word segmenter 5 to split
the posts and responses into sequences of words.
Although both posts and responses are written in
the same language, the distributions on words for
the two are different: the number of unique words
in post text is 125,237, and that of response text is
679,958. We therefore construct two separate vocabularies for posts and responses by using 40,000
most frequent words on each side, covering 97.8%
5 
usage of words for post and 96.2% for response
respectively. All the remaining words are replaced
by a special token “UNK”. The dimensions of the
hidden states of encoder and decoder are both
1,000. Model parameters are initialized by randomly sampling from a uniform distribution between -0.1 and 0.1. All our models were trained on
a NVIDIA Tesla K40 GPU using stochastic gradient descent (SGD) algorithm with mini-batch.
The training stage of each model took about two
Competitor Models
Retrieval-based:
with retrieval-based models,
for any given post p∗, the response r∗is retrieved
from a big post-response pairs (p, r) repository.
Such models rely on three key components: a big
repository, sets of feature functions Φi(p∗, (p, r)),
and a machine learning model to combine these
In this work, the whole 4.4 million
Weibo pairs are used as the repository, 14 features, ranging from simple cosine similarity to
some deep matching models are
used to determine the suitability of a post to a
given post p∗through the following linear model
score(p∗, (p, r)) =
ωiΦi(p∗, (p, r)).
Following the ranking strategy in ,
we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.4M repository, and manually
label them to obtain labeled 6,017 post-response
We use ranking SVM model for the parameters ωi based on the labeled
dataset. In comparison to NRM, only the top one
response is considered in the evaluation process.
SMT-based:
In SMT-based models, the postresponse pairs are directly used as parallel data
for training a translation model. We use the most
widely used open-source phrase-based translation
model-Moses . Another parallel data consisting of 3000 post-response pairs is
used to tune the system. In ,
the authors used a modiﬁed SMT model to obtain
the “Response” of Twitter “Stimulus”. The main
modiﬁcation is in replacing the standard GIZA++
word alignment model with a
new phrase-pair selection method, in which all the
6we use the default similarity function of Lucene 7
possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields
performance slightly better than default setting8.
Compared to retrieval-based methods, the generated responses by SMT-based methods often have
ﬂuency or even grammatical problems.
work, we choose the Moses with default settings
as our SMT model.
Results and Analysis
Automatic evaluation of response generation is
still an open problem. The widely accepted evaluation methods in translation ) do not apply, since the range
of the suitable responses is so large that it is practically impossible to give reference with adequate
coverage. It is also not reasonable to evaluate with
Perplexity, a generally used measurement in statistical language modeling, because the naturalness
of response and the relatedness to the post can not
be well evaluated. We therefore resort to human
judgement, similar to that taken in but with an important difference.
Evaluation Methods
We adopt human annotation to compare the performance of different models. Five labelers with
at least three-year experience of Sina Weibo are invited to do human evaluation. Responses obtained
from the ﬁve evaluated models are pooled and randomly permuted for each labeler. The labelers are
instructed to imagine that they were the authors
of the original posts and judge whether a response
(generated or retrieved) is appropriate and natural
to a input post. Three levels are assigned to a response with scores from 0 to 2:
• Suitable (+2): the response is evidently an appropriate and natural response to the post;
• Neutral (+1): the response can be a suitable
response in a speciﬁc scenario;
• Unsuitable (0): it is hard or impossible to ﬁnd
a scenario where response is suitable.
To make the annotation task operable, the suitability of generated responses is judged from the
following ﬁve criteria:
(a) Grammar and Fluency: Responses should
be natural language and free of any ﬂuency or
grammatical errors;
8Reported results showed that the new model outperformed the baseline SMT model 57.7% of the time.
ᝅཝ࡟⾷॰䠂㘷ᱥޡѠӰ...འ཮ᕖҼ
There are always 8 players at the Italian restricted area. Unbelievable!
Related Criterion
ᡇᱥᝅཝ࡟䱕Ⲻ⨹䘭θㅿᖻ∊䎑ᔶခȾ
I am a big fan of the Italy team, waiting for the football game to start
Logic Consistency
Unsuitable
Response2 ᝅཝ࡟Ⲻ伕⢟འ㗄઩Ҽ
Italian food is absolutely delicious.
Semantic Relevance
Unsuitable
Unbelievable!
Generality
Response4 ૾૾૾ԃ❬ᱥ0φ0 Ⱦ䘎⋗ⵁࡦ䘑⨹Ⱦ
Ha!Ha!Ha! it is still 0:0, no goal so far.
Scenario Dependence
䘏↙ᱥᝅཝ࡟ᕅ䱨ᆾ䏩⨹Ⱦ
This is exactly the Italian defending style football game.
Figure 7: An example post and its ﬁve candidate responses with human annotation. The content of the
post implies that the football match is already started, while the author of Response1 is still waiting for
the match to start. Response2 talks about the food of Italy. Response3 is a widely used response, but it
is suitable to this post. Response4 states that the current score is still 0:0, it is a suitable response only in
this speciﬁc scenario.
Mean Score
Suitable (+2)
Neutral (+1)
Unsuitable (0)
Rtr.-based
Table 2: The results of evaluated methods. Mean score is the average value of annotated scores over all
annotations. (Rtr.-based means the retrieval-based method)
Rtr.-based
High fever attacks me
HYHU\1HZ<HDU¶V'D\
ཐՇᚥθ໔ᕰރ⯡࣑έ
Rest more to boost
your immune system.
Go to see a doctor
HYHU\1HZ<HDU¶V'D\
Get well soon and stay
+LJKIHYHU«
৾㘷Ҽж኷θ᝕䉘ᡇԢ
I gain one more year.
Grateful to my group, so
Happy birthday! Will
stand by you forever!
Happy birthday! Wish
you always stay young
and pretty!
Getting old now. Time
has no mercy.
I just gain only one
more year.
ਹ㍖ㅢжཟθඐᤷօθ
First day of being a
vegetarian. Hold on,
I come to offer my
support. Keep it up!
Hold on, keep it up.
I have been being a
vegetarian everyday
䘏〃ཟ≊ⵕᓊ䈛ᑜѠᐻ
We should go out with
some cute guys to enjoy
a great outing in such a
nice weather.
Where is it? It is so
beautiful!
䱩ݿ᱄აθᗹ᛻㡈⭻Ⱦ
Such a nice sunny day!
I am in a great mood.
䘏〃ཟ≊ⵕуᱥⴌⲺȾ
It is indeed a very nice
WenShanOHW¶VJRRXW
to get some
inspiration. Ha! Ha!
Figure 8: Some responses generated by different models (originally in Chinese with their literal English
translation), where the words in boldfaces are entity names.
(b) Logic Consistency: Responses should be logically consistent with the test post;
(c) Semantic Relevance: Responses should be
semantically relevant to the test post;
(d) Scenario Dependence:
Responses can depend on a speciﬁc scenario but should not contradict the ﬁrst three criteria;
(e) Generality: Responses can be general but
should not contradict the ﬁrst three criteria;
If any of the ﬁrst three criteria (a), (b), and (c)
is contradicted, the generated response should be
labeled as “Unsuitable”. The responses that are
general or suitable to post in a speciﬁc scenario
should be labeled as “Neutral”. Figure 7 shows
an example of the labeling results of a post and its
responses. The ﬁrst two responses are labeled as
“Unsuitable” because of the logic consistency and
semantic relevance errors. Response4 depends on
the scenario (i.e., the current score is 0:0), and is
therefore annotated as “Neutral”.
(1.463, 1.537) 2.01%
NRM-hyb NRM-glo
(1.434, 1.566) 0.01%
NRM-hyb NRM-loc
(1.465, 1.535) 3.09%
Rtr.-based NRM-glo
(1.512, 1.488) 48.1%
Rtr.-based NRM-loc
(1.533, 1.467) 6.20%
Rtr.-based NRM-hyb (1.552, 1.448) 0.32%
NRM-hyb (1.785, 1.215) 0.00 %
Rtr.-based (1.738, 1.262) 0.00 %
Table 3: p-values and average rankings of Friedman test for pairwise model comparison. (Rtr.based means the retrieval-based method)
Our test set consists of 110 posts that do not appear in the training set, with length between 6 to
22 Chinese words and 12.5 words on average. The
experimental results based on human annotation
are summarized in Table 2, consisting of the ratio of three categories and the agreement among
the ﬁve labelers for each model. The agreement is
evaluated by Fleiss’ kappa , as a statistical measure of inter-rater consistency. Except
the SMT-based model, the value of agreement is
in a range from 0.2 to 0.4 for all the other models, which should be interpreted as “Fair agreement”.
The SMT-based model has a relatively
higher kappa value 0.448, which is larger than 0.4
and considered as “Moderate agreement”, since
the responses generated by the SMT often have the
ﬂuency and grammatical errors, making it easy to
reach an agreement on such unsuitable cases.
From Table 2, we can see the SMT method performs signiﬁcantly worse than the retrieval-based
and NRM models and 74.4% of the generated responses were labeled as unsuitable mainly due to
ﬂuency and relevance errors.
This observation
conﬁrms with our intuition that the STC dataset,
with one post potentially corresponding to many
responses, can not be simply taken as parallel corpus in a SMT model. Surprisingly, more than 60%
of responses generated by all the three NRM are
labeled as “Suitable” or “Neutral”, which means
that most generated responses are ﬂuent and semantically relevant to post. Among all the NRM
• NRM-loc outperforms NRM-glo, suggesting
that a dynamically generated context might
be more effective than a “static” ﬁxed-length
vector for the entire post, which is consistent
with the observation made in for machine translation;
• NRM-hyp outperforms NRM-loc and NRMglo, suggesting that a global representation of
post is complementary to dynamically generated local context.
The retrieval-based model has the similar mean
score as NRM-glo, and its ratio on neutral cases
outperforms all the other methods.
This is because 1) the responses retrieved by retrieval-based
method are actually written by human, so they
do not suffer from grammatical and ﬂuency problems, and 2) the combination of various feature
functions potentially makes sure the picked responses are semantically relevant to test posts.
However the picked responses are not customized
for new test posts, so the ratio of suitable cases is
lower than the three neural generation models.
To test statistical signiﬁcance, we use the
Friedman test , which is a nonparametric test on the differences of several related samples, based on ranking. Table 3 shows
the average rankings over all annotations and the
corresponding p-values for comparisons between
different pairs of methods. The comparison between retrieval-based and NRM-glo is not significant and their difference in ranking is tiny. This
indicates that the retrieval-based method is com-
parable to the NRM-glo method. The NRM-hyb
outperforms all the other methods, and the difference is statistically signiﬁcant (p < 0.05). The
difference between NRM-loc and retrieval-based
method is marginal (p = 0.062). SMT is significantly worse than retrieval-based and NRM-hyb
Case Study
Figure 8 shows some example responses generated by our NRMs (only the one with biggest
likelihood is given) and the comparable retrievalbased model. It is intriguing to notice that three
NRM variants give suitable but quite distinct responses, with different perspectives and choices
of words. This, as we conjecture, is caused by
both the architecture variations among models as
well as the variations from random effects like
the initialization of parameters. Another interesting observation is on the fourth example, where
the retrieval-based method returns a response with
the mismatched entity name “WenShan”, which
is actually a quite common problem for retrievalbased model, where the inconsistency details (e.g.,
dates, named entities), which often render the response unsuitable, cannot be adequately considered in the matching function employed in retrieving the responses.
In contrast, we observe that
NRMs tend to make general response and barely
generate those details.
Post ਹ㍖ㅢжཟθඐᤷօθፊᵫ᣿Ⱦ
First day of being a vegetarian. Hold on, CuiDuoLa
Hold on, Keep up your vegetarian diet.
ᜩ㾷ڛᓭ੍ϋᜩ㾷䮵ስ੍ϋ䛙ቧީ⌞ᡇⲺᗤঐ੝Ⱦ
Would you like to be healthy? Would you like to live
long? If so, follow me on Weibo.
䘏ቧᱥՖ䈪ѣⲺਹ㍖੍ϋ
This is the so-called vegetarian diet ?
Eat up so that you will have enough energy to go for
䓡։ᱥ䶟ળⲺᵢ䫧θу㾷⵶ᙛέ
Health is the essence for winning your future battles.
Figure 9: Multiple responses generated by the
We also use the NRM-hyb as an example to investigate the ability of NRM to generate multiple responses. Figure 9 lists 5 responses to the
same post, which are gotten with beam search with
beam size = 500, among which we keep only the
best one (biggest likelihood) for each ﬁrst word.
It can be seen that the responses are ﬂuent, relevant to the post, and still vastly different from each
other, validating our initial conjecture that NRM,
when fueled with large and rich training corpus,
could work as a generator that can cover a lot of
modes in its density estimation.
It is worth mentioning that automatic evaluation
metrics, such as BLEU as
adopted by machine translation and recently SMTbased responding models , do
not work very well on this task, especially when
the reference responses are few. Our results show
that the average BLEU values are less than 2 for
all models discussed in this paper, including SMTbased ones, on instances with single reference.
Probably more importantly, the ranking given by
the BLEU value diverges greatly from the human
judgment of response quality.
Conclusions and Future Work
In this paper, we explored using encoder-decoderbased neural network system, with coined name
Neural Responding Machine, to generate responses to a post. Empirical studies conﬁrm that
the newly proposed NRMs, especially the hybrid
encoding scheme, can outperform state-of-the-art
retrieval-based and SMT-based methods. Our preliminary study also shows that NRM can generate
multiple responses with great variety to a given
post. In future work, we would consider adding
the intention (or sentiment) of users as an external
signal of decoder to generate responses with speciﬁc goals.
Acknowledgments
The authors would like to thank Tao Cai for technical support. This work is supported in part by
China National 973 project 2014CB340301.