Kernel Learning for Extrinsic Classiﬁcation of Manifold Features
Raviteja Vemulapalli, Jaishanker K. Pillai and Rama Chellappa
Department of Electrical and Computer Engineering
Center for Automation Research, UMIACS, University of Maryland, College Park, MD 20742
In computer vision applications, features often lie on
Riemannian manifolds with known geometry.
learning algorithms such as discriminant analysis, partial
least squares, support vector machines, etc., are not directly
applicable to such features due to the non-Euclidean nature
of the underlying spaces. Hence, classiﬁcation is often performed in an extrinsic manner by mapping the manifolds to
Euclidean spaces using kernels. However, for kernel based
approaches, poor choice of kernel often results in reduced
performance. In this paper, we address the issue of kernelselection for the classiﬁcation of features that lie on Riemannian manifolds using the kernel learning approach. We
propose two criteria for jointly learning the kernel and the
classiﬁer using a single optimization problem. Speciﬁcally,
for the SVM classiﬁer, we formulate the problem of learning a good kernel-classiﬁer combination as a convex optimization problem and solve it efﬁciently following the multiple kernel learning approach. Experimental results on image set-based classiﬁcation and activity recognition clearly
demonstrate the superiority of the proposed approach over
existing methods for classiﬁcation of manifold features.
1. Introduction
Many applications involving images and videos require
classiﬁcation of data that obey speciﬁc constraints. Such
data often lie in non-Euclidean spaces. For instance, popular features and models in computer vision like shapes ,
histograms, covariance features , linear dynamical systems (LDS) , etc., are known to lie on Riemannian manifolds. In such cases, one needs good classiﬁcation techniques that make use of the underlying manifold structure.
For features that lie in Euclidean spaces, classiﬁers based
on discriminative approaches such as linear discriminant
analysis (LDA), partial least squares (PLS) and support vector machines (SVM) have been successfully used in various applications. However, these approaches are not directly applicable to features that lie on Riemannian manifolds. Hence, classiﬁcation is often performed in an extrinsic manner by ﬁrst mapping the manifold to an Euclidean
space, and then learning classiﬁers in the new space. One
such popularly used Euclidean space is the tangent space
at the mean sample . However, tangent spaces preserves only the local structure of the manifold and can often
lead to sub-optimal performance. An alternative approach is
to map the manifold to a reproducing kernel Hilbert space
(RKHS) by using kernels. Though kernel-based
methods have been successfully used in many computer vision applications, poor choice of kernel can often result in
reduced classiﬁcation performance. This is illustrated in ﬁgure 1. This gives rise to an important question: How to ﬁnd
good kernels for Riemannian manifolds ?.
In this paper, we answer this question using the kernel
learning approach , in which appropriate kernels are
learned directly from the data. Since we are interested in
learning good kernels for the purpose of classiﬁcation, we
learn the kernel and the classiﬁer jointly by solving a single optimization problem. To learn a good kernel-classiﬁer
combination for features that lie on Riemannian manifolds,
we propose the following two criteria: (i) Risk functional
associated with the classiﬁer in the mapped space should
be minimized for good classiﬁcation performance, (ii) The
mapping should preserve the underlying manifold structure.
The second criterion acts as a regularizer in learning the
kernel. Our general framework for learning a good kernelclassiﬁer combination can be represented as the following
optimization problem
λ Γs(K) + Γc(W, K),
where Γs(K) and Γc(W, K) are respectively the manifoldstructure and the classiﬁer costs expressed as functions of
the classiﬁer parameters W and the kernel K. Here, λ is the
regularization parameter used to balance the two criteria.
Due to its superior generalization properties, we focus
on using the SVM classiﬁer in this paper. In order to preserve the manifold structure, we constrain the distances in
the mapped space to be close to the manifold distances.
Under this setting, we formulate the problem of learning a
good kernel-classiﬁer combination as a convex optimization
problem. While the resulting formulation is an instance of
semideﬁnite programming (SDP) and can be solved using
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.233
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.233
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.233
Figure 1: Tangent-space mapping or poorly-chosen kernel can often result in a bad classiﬁer (right), whereas the proposed method
(left) learns a mapping that is good for classiﬁcation by using the
classiﬁer cost in the optimization.
standard solvers such as SeDuMi , it is transductive in
nature: both training and test data need to be present while
learning the kernel matrix. Solving SDPs is also computationally expensive for large datasets. To solve both these
issues, we follow the multiple kernel learning (MKL) approach and parametrize the kernel as a linear combination of known base kernels. This formulation results in
a much simpler convex optimization problem, which can be
efﬁciently solved using gradient-based methods.
We performed experiments using two different manifold features: linear subspaces and covariance features, and
three different applications: face recognition using image
sets, object recognition using image sets and human activity recognition. The superior performance of the proposed
approach clearly shows that it can be successfully used in
classiﬁcation applications that use manifold features.
Contributions: 1) We introduce a general framework
for developing extrinsic classiﬁers for features that lie on
Riemannian manifolds using the kernel learning approach.
To the best of our knowledge, the proposed approach is the
ﬁrst one to use kernel learning techniques for classiﬁcation
of features that lie on Riemannian manifolds with known
geometry. 2) We propose to use a geodesic distance-based
regularizer for learning appropriate kernels directly from
the data. 3) Focusing on the SVM classiﬁer, we show that
the problem of learning a good kernel-classiﬁer combination can be formulated as a convex optimization problem.
Organization: We provide a brief review of the existing
literature in section 2 and present the proposed approach in
section 3. Section 4 brieﬂy discusses the Riemannian geometry of two popularly used features, namely linear subspaces and covariance features. We present our experimental results in section 5 and conclude the paper in section 6.
2. Previous Work
Existing classiﬁcation methods for Riemannian manifolds (with known geometry) can be broadly grouped into
three main categories: nearest-neighbor methods, Bayesian
methods, and Euclidean-mapping methods.
Nearest neighbor: The simplest classiﬁer on a manifold is the nearest-neighbor classiﬁer based on some appropriately deﬁned distance or similarity measure. In ,
the trajectories of human joint positions were represented
as subspaces using LDS models, and then classiﬁed using
Martin and Finsler distances. In , LDS models were
used to get subspace representations for shape deformations and the Frobenius distance was used for classiﬁcation. In , image sets were modeled using linear
subspaces and then compared using the largest canonical
correlation in , the direct sum of canonical correlations
in and a weighted sum canonical correlations in .
Bayesian framework: Another possible approach for
classiﬁcation is to use the Bayesian framework by deﬁning
probability density functions (pdfs) on manifolds. In 
parametric pdfs like Gaussian were deﬁned on the tangent
space and then wrapped back on to the manifold to de-
ﬁne intrinsic pdfs for the Grassmann manifold. Alternatively, Parzen-window based non-parametric density estimation was used in for the Stiefel manifold. Both these
approaches along with Bayes classiﬁer were used for human activity recognition and video-based face recognition.
In general, parametric approaches are sensitive to the model
order, whereas the model-free non-parametric approaches
are very sensitive to the choice of window size.
Euclidean mapping: Discriminative approaches like
LDA, PLS, SVM, Boosting, etc., can be extended to manifolds by mapping the manifolds to Euclidean spaces. One
such Euclidean space is the tangent-space. In , a LogitBoost classiﬁer was developed using weak classiﬁers
learned on tangent spaces, and then used for pedestrian detection with covariance features. Tangent spaces only preserves the local structure of the manifold and can often lead
to sub-optimal performance. Alternatively, one can map
manifolds to Euclidean spaces by deﬁning Mercer kernels
on them. In , discriminant analysis was used for image set-based recognition tasks using Grassmann kernels.
In , a kernel deﬁned for the manifold of symmetric positive deﬁnite matrices was used with PLS for image setbased recognition tasks. In , the Binet-Cauchy kernels
deﬁned on non-linear dynamical systems were used for human activity recognition. In general, the success of kernelbased methods is often determined by the choice of kernel.
Hence, in this paper we address the issue of kernel-selection
for the classiﬁcation of manifold features.
The idea of using manifold structure as a regularizer was
previously explored in the context of data manifolds ,
where the given high dimensional data samples were simply assumed to lie on a lower dimensional manifold. Since
the structure of the underlying manifold was unknown, a
graph Laplacian-based empirical estimate of the data distribution was used in . Contrary to this, in this paper, we
are interested in analytical manifolds, like Grassmann manifold and manifold of symmetric positive deﬁnite matrices,
whose underlying geometry is known. Hence, the problem
addressed in this paper is different from the one in .
3. Extrinsic Support Vector Machines
Notations: The standard ℓ2 norm of a vector ⃗w is denoted
by ∥⃗w∥2. We use ⃗1 and ⃗0 to denote the column vectors
of appropriate lengths with all ones and zeros respectively.
We use⃗a ≤⃗b to represent a set of element wise inequalities.
AT denotes the transpose of a matrix A and A o B denotes
the Hadamard product between A and B. K ⪰0 (K ≻0)
means K is symmetric and positive semi-deﬁnite (deﬁnite).
Let M denote the Riemannian manifold on which the
features lie. Let Dtr = {(xi, yi), i = 1, . . . , Ntr} be the
set of training samples where yi ∈{+1, -1}, xi ∈M, and
Dte = {xi, i = Ntr +1, . . . , N} be the set of test samples.
Let Φ be the mapping to be learned from the manifold M
to some inner product space H. Let k(·, ·) be the associated
kernel function, and K be the associated kernel matrix.
Then, Kij = k(xi, xj) = Φ(xi)T Φ(xj), ∀xi, xj ∈M.
Since we are interested in performing classiﬁcation in
the mapped space, we jointly learn the kernel and the classiﬁer using a single optimization problem based on the following two criteria:
(i) Risk minimization: For better classiﬁcation performance, the risk functional associated with the classiﬁer in
the mapped space should be minimized .
(ii) Structure preservation: Since the features lie on a Riemannian manifold with a well deﬁned structure, the mapping should be structure-preserving. This criterion can be
seen as playing the role of a regularizer in kernel-learning.
Combining the above two criteria we formulate the problem of learning a good kernel-classiﬁer combination as
λ Γs(K) + Γc(W, K),
where Γs(K) and Γc(W, K) are the manifold-structure cost
and the classiﬁer cost expressed as functions of classiﬁer
parameters W and kernel matrix K. Here, λ is the regularization parameter used to balance the two criteria. Since the
mapped space is an inner product space, one can use standard machine learning techniques to perform classiﬁcation.
Due to its superior generalization properties, we focus on
the SVM classiﬁer in this paper. However, it is important to
note that the framework introduced here is general and can
be applied to other classiﬁers as well.
SVM classiﬁer in the mapped space: The SVM classiﬁer
in the mapped space is given by f(x) = ⃗w∗⊤Φ(x) + b∗,
where the weight vector ⃗w∗and the bias b∗are given by
⃗w∗, b∗= arg min
subject to yi(⃗w⊤Φ(xi) + b) ≥1 −ηi, ηi ≥0, i =
1, . . . , Ntr. This problem is usually solved in its dual form
⃗y⃗y⊤o Ktr,tr
where Ω = {⃗α ∈RNtr | ⃗0 ≤⃗α ≤C⃗1, ⃗α⊤⃗y = 0}, and
⃗y⊤= [y1, . . . , yNtr].
Preserving the manifold structure: To preserve the manifold structure, we constrain the distances in the mapped
space to be close to the manifold distances. The squared
Euclidean distance between two points xi and xj in the
mapped space can be expressed in terms of kernel values as ∥φ(xi) −φ(xj)∥2
2 = Kii + Kjj −Kij −Kji.
Hence, we wish to minimize N
ij, where ζij =
Kii + Kjj −Kij −Kji −d2
1 ≤i < j ≤N, and dij is
the manifold distance between the points xi and xj. Since
ζij can be positive or negative, we use ζ2
ij in the cost.
Combined formulation: Combining both the classiﬁer and
the structure costs, the joint optimization problem for learning a good kernel-classiﬁer combination is given by
⃗α∈Ω λ∥⃗ζ∥2
⃗y⃗y⊤o Ktr,tr
subject to
Kii + Kjj −Kij −Kji −d2
ij = ζij for 1 ≤i < j ≤N,
where Ω = {⃗α ∈RNtr | ⃗0 ≤⃗α ≤C⃗1, ⃗α⊤⃗y = 0}, ⃗ζ
is the column vector of variables ζij and ⃗y ∈RNtr is the
column vector of class labels. In the above optimization
problem, the centering constraint 
ij Kij = 0 is added
simply to remove the ambiguity associated with the origin
in the mapped space . Note that in (5) we are learning the entire kernel matrix K directly in a non-parametric
fashion, and the classiﬁer term has only Ktr,tr. Therefore,
to ensure meaningful values for Ktr,te and Kte,te, we need
additional constraints between the training and test samples . Hence, we use both the training and test samples
in structure-preserving constraints.
Theorem 1: The optimal K for problem (5) can be found
by solving a semideﬁnite programming problem.
Proof: This can be easily proved by following . Due to
space limitation, we omit the proof here.
SDPs are convex in nature and can be solved using standard solvers such as SeDuMi . Once the kernel matrix
K is obtained, the SVM classiﬁer in the mapped space can
be obtained by solving the SVM dual (4). Note that the
above formulation is transductive in nature: both training
and test data need to be present while learning the kernel
matrix. Also in general, solving SDPs can be computationally expensive for large datasets. Both these issues can be
addressed by using the MKL approach.
3.1. Extrinsic SVM Using MKL Framework
Instead of learning a non-parametric kernel matrix K,
following , we parametrize the kernel as a linear combination of ﬁxed base kernels K1, K2, ...., KM :
m=1 μmKm, where ⃗μ⊤= [μ1, . . . , μm] are positive weights to be learned. Since we use the same linear
model for both training and test data, the weights ⃗μ can be
learned using only the training data, and the kernel values
for test data can be computed using the known base kernels
and the learned weights. Hence, the formulation becomes
inductive. Under the linear combination, the optimization
problem (5) now becomes
⃗α∈Ω λ∥⃗ζ∥2
2⃗α⊤(⃗y⃗y⊤o
subject to
for 1 ≤i < j ≤Ntr and ⃗μ ≥⃗0,
where Ω = {⃗α ∈RNtr | ⃗0 ≤⃗α ≤C⃗1, ⃗α⊤⃗y = 0}. Note
that the centering constraint 
i,j Kij = 0 in (5) is not required for the MKL approach as the origin is automatically
decided based on the base kernels and their weights.
ij denote the squared distance between samples
xi and xj induced by the base kernel Km, i.e., pm
ji . Let J1(⃗μ) and J2(⃗μ) represent the
manifold-structure cost and the classiﬁer cost respectively
in (6). Then,
J2(⃗μ) = max
2⃗α⊤(⃗y⃗y⊤o
Let Φm be the mapping corresponding to the kernel Km and
ℓh(f) be the hinge loss function: ℓh(f) = max(0, 1 −f).
Theorem 2: J2(⃗μ) = J3(⃗μ), where
J3(⃗μ) = min
m Φm(xi) + b
Proof: The proof is based on Lagrangian duality. Please
refer to for details.
Let h(⃗μ) = λJ1(⃗μ) + J3(⃗μ). Using Theorem 2, the
optimization problem (6) can be written as
h(⃗μ) subject to ⃗μ ≥⃗0.
Theorem 3: h(⃗μ) is a differentiable convex function of ⃗μ if
Km ≻0 for m = 1, 2, ...M.
Proof: J1(⃗μ) is a convex quadratic term and hence differentiable with respect to ⃗μ. As shown in , J3(⃗μ) is also
convex and differentiable if all the base kernel matrices Km
are strictly positive deﬁnite. Hence h(⃗μ) is a differentiable
convex function of ⃗μ.
Using Theorem 3, the optimization problem (9) can
be efﬁciently solved using the reduced gradient descent
method or any other standard algorithm used for solving constrained convex optimization problems.
given ⃗μ, J1(⃗μ) can be evaluated directly using (7) and its
gradient can be computed using
Since J3(⃗μ) = J2(⃗μ), it can be computed by solving a standard SVM dual problem with K = M
m=1 μmKm. The
gradient of J3 can be computed using 
where ⃗α∗is the optimal solution for the SVM dual problem
used for computing J3(⃗μ). Once the optimal ⃗μ∗is computed, the classiﬁer in the mapped space can be obtained by
solving the SVM dual (4) with K = M
that Theorem 3 requires the Gram matrices Km to be positive deﬁnite. To enforce this property a small ridge may be
added to their diagonals.
4. Riemannian Manifolds in Computer Vision
In this section we brieﬂy discuss the Riemannian geometry of two popularly used features, namely linear subspaces
and covariance features, and show how these features are
used in various computer vision applications.
4.1. Linear Subspaces - Grassmann Manifold
Grassmann manifold, denoted by Gn,d, is the set of all
d-dimensional linear subspaces of Rn. An element S of
Gn,d can be represented by any n × d orthonormal matrix YS such that span(YS) = S. The geodesic distance between two subspaces S1 and S2 on the Grassmann manifold is given by ∥⃗θ∥2, where ⃗θ = [θ1, . . . , θd] are the
principal angles between S1 and S2. ⃗θ can be computed
using θi = cos−1(αi) ∈[0, π
2 ], where αi are the singular values of Y ⊤
S1YS2. Other popularly used distances for
the Grassmann manifold are the Procrustes metric given by
i=1 sin2(θi/2))1/2, and the Projection metric given by
i=1 sin2θi)1/2. We refer the interested readers to 
for further discussions on the Grassmann manifold.
Grassmann kernels: Grassmann manifold can be mapped
to Euclidean spaces by using Mercer kernels . One popularly used kernel is the Projection kernel given by
KP (Y1, Y2) = ∥Y ⊤
F . The mapping corresponding to
the Projection kernel is given by ΦP (Y ) = Y Y ⊤. Various
kernels can be generated from KP and ΦP using
P (Y1, Y2) = exp
−γ∥ΦP (Y1) −ΦP (Y2)∥2
(Y1, Y2) = (γKP (Y1, Y2))d .
We refer to the family of kernels Krbf
as projection-RBF
kernels and the family of kernels Kpoly
as projectionpolynomial kernels.
4.2. Covariance Features
The d × d symmetric positive deﬁnite (SPD) matrices, i.e., non-singular covariance matrices, can be formulated as a Riemannian manifold , and the resulting afﬁne-invariant geodesic distance (AID) is given by
i=1 ln2λi(C1, C2))1/2 , where λi(C1, C2) are the generalized Eigenvalues of SPD matrices C1 and C2.
Another popularly used distance for SPD matrices is the log-
Euclidean distance (LED) given by ∥log(C1) −log(c2)∥F ,
where log is the ordinary matrix logarithm and ∥• ∥F denotes the matrix Frobenius norm. We refer the interested
readers to for further details.
Kernels for SPD matrices: Similar to the Grassmann manifold, we can deﬁne kernels for the set of SPD matrices.
One such kernel based on the log-Euclidean distance was
derived in : Klog(C1, C2) = trace[log(C1)⊤log(C2)].
The mapping corresponding to Klog is given by Φlog(C) =
log(C). Various kernels can be generated from Klog and
Φlog using
log(C1, C2) = exp
−γ∥Φlog(C1) −Φlog(C2)∥2
log (C1, C2) = (γKlog(C1, C2))d .
We refer to the family of kernels Krbf
log as LED-RBF kernels and the family of kernels Kpoly
as LED-polynomial
4.3. Applications
Recognition using image sets: Given multiple images of
the same face or object, they can be collectively represented using a lower dimensional subspace obtained by applying the principal component analysis (PCA) on the feature vectors representing individual
images. Let S = [s1, s2, . . . , sN] be the mean-subtracted
data matrix of an image set, where si ∈Rn is an ndimensional feature descriptor of i-th image. Let V ΛV T
be the Eigen-decomposition of the data covariance matrix
C = SS⊤/N −1. Then the linear subspace spanned by the
top d Eigenvectors can be used to represent the image set by
a d-dimensional linear subspace. This d-dimensional linear subspace of the original n-dimensional space lies on the
Grassmann manifold. Alternatively, the image set can also
be represented using its natural second-order statistic ,
i.e., the covariance matrix C. Since covariance matrices
are positive semi-deﬁnite in general, a small ridge may be
added to their diagonals to make them positive deﬁnite.
Activity recognition using dynamical models: The autoregressive and moving average (ARMA) model is a dynamical model widely used in computer vision for modeling various kinds of time-series data and has been
successfully used for activity recognition . For an
action video sequence φ, the ARMA model equations are
zφ(t + 1) = A(φ)zφ(t) + vφ(t), vφ(t) ∼N(⃗0, Ξ),
yφ(t) = C(φ)zφ(t) + wφ(t), wφ(t) ∼N(⃗0, Ψ),
where, zφ(t) ∈Rd is the hidden state vector, yφ(t) ∈
Rp is the observed feature vector, A(φ) ∈Rd×d and
C(φ) ∈Rp×d are the transition and measurement matrices. vφ(t) and wφ(t) are the noise components modeled as normal with zero mean and covariances Ξ
Rd×d and Ψ ∈Rp×p respectively.
A closed form solution for parameters (A(φ), C(φ)) of the above model
is available . The expected observation sequence generated by a time-invariant model (A(φ), C(φ)), lies in
the column space of the observability matrix O∞(φ) =
[C(φ), (C(φ)A(φ))⊤, (C(φ)A(φ)2)⊤, . . .] . Following , instead of O∞(φ), we use a ﬁnite length approximation Om(φ) ∈Rmp×d given by O⊤
m(φ) = [C(φ),
(C(φ)A(φ))⊤, . . . , (C(φ)A(φ)m−1)⊤] to represent the action sequence φ.
The column space of Om(φ) is a ddimensional subspace of Rmp and hence is a point on the
Grassmann manifold Gmp,d. The orthonormal basis computed by Gram-Schmidt orthonormalization of Om(φ) can
be used to represent the action sequence φ as a point on the
Grassmann manifold.
5. Experimental Evaluation
In this section, we evaluate the proposed approach using three applications where manifold features are used: (i)
Face recognition using image sets, (ii) Object recognition
using image sets and (iii) Human activity recognition from
videos. We use two different manifold features, namely linear subspaces and covariance features.
5.1. Datasets and Feature Extraction
Face recognition – YouTube Celebrities : This dataset
has 1910 video clips of 47 subjects collected from the
YouTube. Most of them are low resolution and highly compressed videos, making it a challenging dataset for face
recognition. The face region in each image was extracted
using a cascaded face detector, resized into 30 × 30 intensity image, and histogram equalized to eliminate lighting
effects. Each video generated an image set of faces. Figure 2 shows some of the variations in an image set from this
Figure 2: Variations in an image set from YouTube dataset.
Object recognition – ETH80 :
This benchmark
dataset for object recognition task has images of 8 object
categories with each category including 10 different object
instances. Each object instance has 41 images captured under different views, which form an image set. All the images were resized into 20 × 20 intensity images. Figure 3
shows typical variations in an image set from this dataset.
For both of these datasets, we performed experiments
with two different manifold features: covariance matrices
and linear subspaces. As mentioned in section 4.3, to avoid
matrix singularity, we added a small ridge δI to each covariance matrix C, where δ = 10−3 × trace(C) and I is the
identity matrix. For subspace representation, we used 20
dimensional linear subspaces spanned by the top 20 Eigenvectors of C.
Activity recognition – INRIA IXMAS : This dataset
consists of 10 actors performing 11 different actions, each
action executed 3 times at varying rates while freely changing the orientation. We followed the same feature extraction
procedure used in . Speciﬁcally, for each segment of
activity, we built a time series of motion history volumes
using the segmentation results from . Then each action
was modeled as an ARMA process using the 16 × 16 × 16
circular FFT features proposed by . Following ,
the circular FFT features were reduced to 286 dimensions
using PCA before building the dynamical model. The state
space dimension d was chosen to be 5, and the observability
matrix was truncated at m = 5.
5.2. Comparative Methods and Evaluation Settings
We compare our approach with the following methods:
(i) Nearest neighbor baseline (NN): We used three different distances for the Grassmann manifold, namely the
geodesic distance, the Procrustes distance and the Projection metric. We report the best results among the three. For
covariance features, we used two distances, namely the AID
and the LED and report the best results among the two.
(ii) Standard MKL baseline (S-MKL) : In the standard MKL approach, the kernel is learned as a convex combination of ﬁxed base kernels (K = M
m=1 μmKm, ⃗μ ≥
⃗0, ⃗μ⊤⃗1 = 1) , by minimizing the SVM cost (equation (4))
without manifold-based regularization.
iii) Statistical modeling (SM) : This approach uses
parametric (SM-P) and non-parametric (SM-NP) probability density estimation on the manifold followed by Bayes
Figure 3: Variations in an image set from ETH80 dataset
classiﬁcation. For the parametric case, the Gaussian density was used in .
(iv) Grassmann discriminant analysis (GDA) : Performs discriminant analysis followed by NN classiﬁcation
for the Grassmann manifold using the Projection kernel.
(v) PLS with the Projection kernel (Proj+PLS) :
Uses PLS combined with the Projection kernel for the
Grassmann manifold.
(vi) Covariance discriminative learning (CDL) :
Uses discriminant analysis and PLS for covariance features
using a kernel derived from the LED metric. Recently, stateof-the-art results were reported in for image set-based
face and object recognition tasks using this approach.
For the activity recognition experiment using the IN-
RIA IXMAS dataset, we follow the round-robin (leave-oneperson-out) experimental protocol used in . For the
object and face recognition experiments, we follow the settings used in . For the YouTube dataset, for each person,
we use 3 randomly chosen image sets for training and 6 for
testing. For the ETH80 dataset, for each category, we use 5
randomly chosen image sets for training and 5 for testing.
We report the results averaged over 10 random trials. The
recognition rates reported for SM-P and SM-NP methods
are taken from the original paper . For GDA, Proj+PLS
and CDL approaches, we use the recognition rates recently
reported in .
5.3. Base Kernels and Parameters
For both the S-MKL and the proposed approach, we used
several base kernels. For the experiments with linear subspaces, we used multiple projection-RBF and projectionpolynomial kernels deﬁned in (12). For each dataset, the
values for the RBF parameter γ and the polynomial degree d were chosen based on their individual crossvalidation accuracy on the training data.
Speciﬁcally, for the
INRIA IXMAS dataset, we used 6 projection-polynomial
kernels and 13 projection-RBF kernels. For the YouTube
dataset, we used 10 projection-polynomial kernels and 15
projection-RBF kernels. For the ETH80 dataset, we used 10
projection-polynomial kernels and 13 projection-RBF kernels. The values for RBF kernel parameter γ were taken
n2δ, where n is the number of dimensions of ΦP de-
ﬁned in section 4.1, and δ = {−3, −1, . . . , 19, 21} for the
INRIA IXMAS dataset, δ = {−14, −12, . . . , 12, 14} for
the YouTube dataset, δ = {−5, −3, . . . , 17, 19} for the
Figure 4: Normalized kernel weights for the S-MKL(blue) and
the proposed method(red) on the INRIA IXMAS dataset
ETH80 dataset . Polynomial kernels were generated by taking γ = 1
n and varying the degree from 1 to 6 for the INRIA
IXMAS dataset, and from 1 to 10 for the other two datasets.
For the experiments with covariance features, we used
multiple LED-RBF and LED-polynomial kernels deﬁned
in (13), whose parameters were chosen based on their individual crossvalidation performance. Speciﬁcally, for the
YouTube dataset, we used 10 LED-polynomial kernels and
15 LED-RBF kernels. For the ETH80 dataset, we used 10
LED-polynomial kernels and 20 LED-RBF kernels. The
values for the RBF parameter γ were taken as 1
n2δ, where
n is the number of dimensions of Φlog deﬁned in section 4.2, and δ = {−7, −6, . . . , 6, 7} for the YouTube
dataset, δ = {−10, −9, . . . , 8, 9} for the ETH80 dataset.
For both datasets, polynomial kernels were generated by
taking γ = 1
n and varying the degree from 1 to 10.
For both linear subspaces and covariance features,
geodesic distances were used in the distance preserving
constraints. In all the experiments, the parameters for the
S-MKL method (SVM parameter C) and the proposed approach (SVM parameter C and the regularization parameter
λ) were chosen using crossvalidation.
5.4. Results
Table 1 shows the recognition rates for human activity recognition using dynamical models. Tables 2 and 3
show the recognition rates for image set-based object and
face recognition tasks using linear subspaces and covariance features respectively. We can see that the proposed
approach clearly outperforms the nearest neighbor baseline method. On an average, the classiﬁcation accuracy increases by 11.7%. This is expected as the simple nearest
neighbor based classiﬁer may not be powerful enough to
handle the complex visual tasks considered. When compared to the S-MKL approach, the proposed approach performs better in four out of ﬁve experiments, with an average
increase of 4.2% in the classiﬁcation accuracy. This shows
that the proposed manifold-based regularization is indeed
helping in ﬁnding a better kernel for classiﬁcation. On the
INRIA IXMAS dataset, both the S-MKL and the proposed
method gave same recognition rates. Figure 4 shows the
normalized kernel weights for the S-MKL approach (blue)
and the proposed approach (red) on the INRIA IXMAS
dataset. The horizontal axis corresponds to the kernel index
Table 1: Recognition rates for human activity recognition
on the INRIA IXMAS dataset using dynamical models
Table 2: Recognition rates for image set-based face and object recognition tasks using linear subspaces
YouTube 62.8
Table 3: Recognition rates for image set-based face and object recognition tasks using covariance features
YouTube 40.7
varying from 1 to 19 (13 RBF followed by 6 polynomial
kernels). We can see that the kernel weights roughly follow
the same pattern for both the approaches. This explains their
similar performance on the INRIA IXMAS dataset. In the
case of other datasets, the S-MKL approach mostly picked
few base kernels (usually RBF kernels with high γ value or
polynomial kernels of high degree d), whereas the weights
for the proposed approach were distributed over many kernels.
We can also see that the proposed approach clearly performs better than statistical and other kernel-based methods.
The poor performance of SM-P method can be attributed to
the Gaussian assumption. In the case of parametric density estimation, the mismatch between the assumed distribution and the actual underlying distribution often results
in reduced performance. In the case of SM-NP, the poor
performance could be due to the sub-optimal choice of the
kernel width used in . In general, non-parametric density estimation methods are sensitive to the choice of kernel
width and a sub-optimal choice often results in poor performance . The relatively lower performance of the other
kernel-based methods suggests that, it is effective to jointly
learn the kernel and the classiﬁer directly from the data using the proposed framework.
Recently, covariance feature combined with PLS has
been shown to perform better than various other recent methods for image set-based recognition tasks. Our
results show that the classiﬁcation performance can be further improved by combining the covariance feature with the
proposed approach.
6. Conclusion and Future Work
In this paper, we introduced a general framework for
developing extrinsic classiﬁers for features that lie on Riemannian manifolds using the kernel learning approach. We
proposed two criteria for learning a good kernel-classiﬁer
combination for manifold features.
In the case of SVM
classiﬁer, based on the proposed criteria, we showed that
the problem of learning a good kernel-classiﬁer combination can be formulated as a convex optimization problem
and efﬁciently solved following the multiple kernel learning
approach. We performed experiments using two popularly
used manifold features and obtained superior performance
compared to other relevant approaches.
Though we focused on the SVM classiﬁer in this paper,
the proposed approach is general and we plan to extend it
to other classiﬁers in the future. In this paper, the manifold structure has been used as a regularizer using simple
distance-preserving constraints. Another possible direction
of future work is to explore more sophisticated regularizers
that can make use of the underlying manifold structure.
Acknowledgements: This research was supported by a
MURI grant from the US Ofﬁce of Naval Research under
N00014-10-1-0934.