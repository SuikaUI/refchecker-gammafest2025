Temporal Attention augmented Bilinear Network
for Financial Time-Series Data Analysis
Dat Thanh Tran∗, Alexandros Iosiﬁdis‡, Juho Kanniainen† and Moncef Gabbouj∗
∗Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
‡Department of Engineering, Electrical & Computer Engineering, Aarhus University, Aarhus, Denmark
†Laboratory of Industrial and Information Management, Tampere University of Technology, Tampere, Finland
Email:{dat.tranthanh,juho.kanniainen,moncef.gabbouj}@tut.ﬁ, 
Abstract—Financial time-series forecasting has long been a
challenging problem because of the inherently noisy and stochastic nature of the market. In the High-Frequency Trading (HFT),
forecasting for trading purposes is even a more challenging task
since an automated inference system is required to be both
accurate and fast. In this paper, we propose a neural network
layer architecture that incorporates the idea of bilinear projection
as well as an attention mechanism that enables the layer to
detect and focus on crucial temporal information. The resulting
network is highly interpretable, given its ability to highlight the
importance and contribution of each temporal instance, thus
allowing further analysis on the time instances of interest. Our
experiments in a large-scale Limit Order Book (LOB) dataset
show that a two-hidden-layer network utilizing our proposed
layer outperforms by a large margin all existing state-of-the-art
results coming from much deeper architectures while requiring
far fewer computations.
Index Terms—Feed-forward neural network, Bilinear projection, Temporal Attention, Financial data analysis, Time-series
prediction
I. INTRODUCTION
Time-series classiﬁcation and prediction have been extensively studied in different domains. Representative examples
include natural language processing , , , medical data
analysis , , human action/behavior analysis , , ,
meteorology , ﬁnance and econometrics , , 
and generic time-series classiﬁcation , . Due to the
complex dynamics of ﬁnancial markets, the observed data
is highly non-stationary and noisy, representing a limited
perspective of the underlying generating process. This makes
ﬁnancial time-series forecasting one of the most difﬁcult tasks
among time-series predictions . The development of both
software and hardware infrastructure has enabled the extensive
collection of trade data, which is both an opportunity and a
challenge for the traders, especially high-frequency traders.
Apart from long-term investment, HFT is characterized by
high speed and short-term investment horizon. The ability to
efﬁciently process and analyze large chunks of data within
relatively short time is thus critical in HFT.
During the past decades, several mathematical models have
been proposed to extract ﬁnancial features from the noisy,
nonstationary ﬁnancial time-series. Stochastic features and
market indicators , have been widely studied in
quantitative analysis. Notable works include autoregressive
(AR) and moving average (MA) features, which were
later combined as a general framework called autoregressive
moving average (ARMA). Its generalization, also known as
autoregressive integrated moving average (ARIMA) , 
which incorporates the differencing step to eliminate nonstationarity, is among popular methods for time-series analysis
in econometrics. To ensure tractability, these models are often
formulated under many assumptions of the underlying data
distribution, leading to poor generalization to future observations . In recent years, the development of machine
learning techniques, such as support vector regression ,
 and random forest , have been applied to timeseries forecasting problems to alleviate the dependence on
such strong assumptions. As a result, these statistical machines
often outperform the traditional ARIMA model in a variety of
scenarios .
Although the aforementioned machine learning models perform reasonably well, they are not particularly designed to
capture the temporal information within time-series data. A
class of neural network architecture called Recurrent Neural
Networks (RNN) is speciﬁcally designed to extract temporal
information from raw sequential data. Although RNNs were
developed more than two decades ago , they started
to become popular in many different application domains
 , , only recently thanks to the development in
optimization techniques and computation hardware, as well as
the availability of large-scale datasets. Special types of RNNs,
such as Long Short Term Memory (LSTM) and Gated
Recurrent Unit (GRU) networks, which were proposed
to avoid the gradient vanishing problem in very deep RNN,
have become the state-of-the-art in a variety of sequential data
prediction problems , , , . The beauty of deep
neural networks lies in the fact that these architectures allow
end-to-end training, which works directly on the raw data
representations instead of hand-crafted features. As a result,
suitable data-dependent features are automatically extracted,
improving the performance and robustness of the whole system.
While deep neural networks in general and LSTM networks, in particular, are biologically inspired and work well
in practice, the learned structures are generally difﬁcult to
interpret. It has been long known that there exists a visual
attention mechanism in human cortex , , in
which visual stimuli from multiple objects compete for neural
representation. To further imitate the human learning system,
attention mechanisms were developed for several existing
neural network architectures to determine the importance of
different parts of the input during the learning process ,
 , , . This not only improves the performance of
the network being applied to, but also contributes to the
interpretation of the obtained result by focusing at the speciﬁc
parts of the input. For example, in image captioning task ,
by incorporating visual attention into a convolutional LSTM
architecture, the model explicitly exhibits the correspondence
between the generated keywords and the visual subjects.
Similar correspondences between the source phrases and the
translated phrases are highlighted in attention-based neural
machine translation models . While visual and textual
data are easy to interpret and intuitive to humans, generic
time-series data is more difﬁcult to perceive, making the
LSTM models a black box. This hinders the opportunity
for post-training analysis. Few attempts have been made to
employ attention functionality in LSTM in different timeseries forecasting problems such as medial diagnosis 
and weather forecast or ﬁnance . Although adding
attention mechanism into recurrent architectures improves the
performance and comprehensibility, it incurs high computational cost for the whole model. This impedes the practicality
of the model in many ﬁnancial forecasting situations in which
the ability to rapidly train the system and make predictions
with continuously large chunks of incoming data plays an
important role.
Multivariate time-series data is naturally represented as a
second-order tensor. This representation retains the temporal
structure encoded in the data, which is essential for a learning
model to capture temporal interaction and dependency. By applying a vectorization step, traditional vector-based models fail
to capture such temporal cues, leading to inferior performance
compared to tensor-based models that work directly on the
natural representation of the input. Recent progress in mathematical tools and algorithms pertaining to tensor input have
enabled the development of several learning systems based
on multilinear projections. For example, popular discriminant
and regression criteria were extended for tensor inputs, such
as Multilinear Discriminant Analysis (MDA) , Multilinear
Class-speciﬁc Discriminant Analysis (MCSDA) or Tensor
Regression (TR) . Regarding neural network formulations,
attempts have also been made to learn separate projections
for each mode of the input tensors , . Motivation to
replace linear mapping by the multilinear counterpart stems
from the fact that learning separate dependencies between
separate modes of the input alleviates the so-called curse of
dimensionality and greatly reduces the amount of memory
and computation required. While large volume of literature
employing multilinear projections was developed for data
modalities such as image, video, and text, few works have been
dedicated to time-series prediction in general and ﬁnancial
time-series in particular.
In recent work , we have showed that a linear multivariate regression model could outperform other competing
shallow architectures that do not take into account the temporal nature of HFT data. While performing reasonably well
compared to other shallow architectures, the learning model
in has certain short-comings in practice: the analytical
solution is computed based on the entire dataset prohibiting its
application in an online learning scenario; with large amount
of data, this model clearly underﬁts the underlying generating
process with performance inferior to other models based on
deep architectures , . In this work, we propose a
neural network layer which incorporates the idea of bilinear
projection in order to learn two separate dependencies for
the two modes of multivariate time-series data. Moreover, we
augment the proposed layer with an attention mechanism that
enables the layer to focus on important temporal instances of
the input data. By formulation, the layer is differentiable. Thus
it can be trained with any mini-batch gradient descend learning
algorithm.
The contribution of our work is as follows:
• We propose a new type of layer architecture for multivariate time-series data. The proposed layer is designed to
leverage the idea of bilinear projection by incorporating
an attention mechanism in the temporal mode. The formulation of the attention mechanism directly encourages
the competition between neurons representing the same
feature at different time instances. The learned model
utilizing our proposed layer is highly interpretable by
allowing us to look at which speciﬁc time instances the
learning model attends to.
• We show both theoretically and experimentally that the
proposed attention mechanism is highly efﬁcient in terms
of computational complexity, allowing development in
practical ﬁnancial forecasting systems.
• Numerical experiments in a large-scale Limit Order Book
(LOB) dataset that contains more than 4 million limit
orders show that by using a shallow architecture with
only two hidden layers, we are able to outperform by a
large margin existing results of deep networks, such as
CNN and LSTM, leading to new state-of-the-art results.
Furthermore, we show that our proposed attention mechanism can highlight the contribution of different temporal
information, opening up opportunities for further analysis
on the temporal instances of interest.
The rest of the paper is organized as follows: in section
2, we provide an overview of the related works focusing
on time-series prediction problems. Section 3 describes our
proposed layer architecture together with theoretical analysis
on its computational complexity. Section 4 starts by describing
the task of mid-price movement prediction given LOB data before providing details of our experimental procedures, results
and quantitative analysis. Section 5 concludes our work and
discusses possible future extension and analysis.
II. RELATED WORK
Deep neural networks have been shown to be the state-ofthe-art not only in human cognition tasks, such as language
and image understanding but also in the prediction of complex
time-series data. For example, RNN networks based on LSTM
architectures have been used to predict the future rainfall
intensity in different geographic areas , commodity consumption or to recognize patterns in clinical time-series
 . In ﬁnancial data analysis, Deep Belief Networks and Auto-
Encoders were used to derive portfolio trading models ,
 . In addition, Deep Reinforcement Learning methods are
also popular among the class of ﬁnancial asset trading models
 , . Spatial relations between LOB levels was studied
in by a 3-hidden-layer multilayer perceptron (MLP) that
models the joint distribution of bid and ask prices. Due to
the erratic, noisy nature of stock price movement, many deep
neural networks were proposed within a complex forecasting
pipeline. For example, in high-frequency LOB data, the authors proposed to normalize the LOB states by the prior date’s
statistics before feeding them to a CNN or an LSTM
network . A more elaborate pipeline consisting of multiresolution wavelet transform to ﬁlter the noisy input series,
stacked Auto-Encoder to extract high-level representation of
each stock index and an LSTM network to predict future
prices was recently proposed in . Along with popular deep
networks, such as CNN, LSTM being applied to time-series
forecasting problems, a recently proposed Neural Bag of Feature (NBoF) model was also applied to the problem of stock
price movement prediction . The architecture consists of
an NBoF layer which compiles histogram representation of
the input time-series and an MLP that classiﬁes the extracted
histograms. By learning the parameters of the whole network
through back-propagation algorithm, NBoF was shown to
outperform its Bag-of-Feature (BoF) counterpart.
In order to improve both performance and interpretability,
attention mechanism was proposed for the recurrent sequenceto-sequence learning problem (ASeq-RNN) . Given a sequence of multivariate inputs xi ∈RD, 1 ≤i ≤T and an
associated sequence of outputs yj, 1 ≤j ≤T ′, the Seq-RNN
model with attention mechanism learns to generate yj from
xi by using three modules: the encoder, the memory and the
decoder. The encoder maps each input xi to a hidden state he
using the nonlinear transformation he
i = f(xi, he
i−1) coming
from a recurrent unit (LSTM or GRU). From the sequence of
hidden states generated by the encoder he
i, i = 1, . . . , T , the
memory module generates context vectors cj, j = 1, . . . , T ′
for each output value yj, j = 1, . . . , T ′. In a normal recurrent
model, the context vector is simply selected as the last hidden
T while in the attention-based model, the context
vectors provide summaries of the input sequence by linearly
combining the hidden states cj = PT
i through a set
of attention weights αij learned by the following equations:
α tanh(Wαhd
j−1 + Uαhe
k=1 exp(ekj)
The softmax function in Eq. (2) allows the model to produce
the context vectors that focus on some time instances of the
input sequence and hd
j = g(yj−1, hd
j−1, cj), j = 1, . . . , T ′
is the hidden state computed from the recurrent unit in the
decoder module. From the hidden state hd
j which is based
on the previous state hd
j−1, the previous output yj−1 and the
current context cj, the decoder learns to produce the output
yj = Wouthd
Based on the aforementioned attention mechanism, proposed to replace Eq. (1) with a modiﬁed attention calculation
scheme that assumes the existence of pseudo-periods in the
given time-series. Their experiments in energy consumption
and weather forecast showed that the proposed model learned
to attend particular time instances which indicate the pseudoperiods existing in the data. For the future stock price prediction task given current and past prices of multiple stock
indices, the authors in developed a recurrent network with
two-stage attention mechanism which ﬁrst focuses on different
input series then different time instances. We should note that
the formulations above of attention mechanism were proposed
for the recurrent structure.
Our work can be seen as direct extension of in which
the author proposed a regression model based on the bilinear
mapping for the mid-price movement classiﬁcation problem:
f(X) = W1Xw2
∈RD×T is a multivariate time-series containing
T temporal steps. W1
∈R3×D and w2
∈RT ×1 are the
parameters to estimate. By learning two separate mappings that
transform the input LOB states to class-membership vector of
size 3 × 1 corresponding to 3 types of movements in midprice, the regression model in was shown to outperform
other shallow classiﬁers. Other related works that utilize a
bilinear mapping function to construct a neural network layer
include and . While attempted to incorporate the
bilinear mapping into the recurrent structure by processing
a block of temporal instances at each recurrent step, both
 and focus on medium-scale visual-related tasks
such as hand-written digit recognition, image interpolation and
reconstruction.
III. PROPOSED METHOD
A. Bilinear Layer
We start this section by introducing some notations and
deﬁnitions. Throughout the paper, we denote scalar values
by either lower-case or upper-case character (a, b, A, B, . . . ),
vectors by lower-case bold-face characters (x, y, . . . ), matrices by upper-case bold-face characters (X, Y, . . . ). A matrix
X ∈RD×T is a second order tensor which has two modes
with D and T are the dimension of the ﬁrst and second mode
respectively. We denote Xi ∈RD×T , i = 1, . . . , N the set
of N samples, each of which contains a sequence of T past
observations corresponding to its T columns. The time span
of the past values (T ) is termed as history while the time span
in the future value (H) that we would like to predict is known
as prediction horizon. For example, given that the stock prices
are sampled every second and Xi ∈R10×100 contains stock
prices at different LOB levels for the last T = 100 seconds,
the prediction horizon H = 10 corresponds to predicting a
future value, e.g. mid-price, at the next 10 seconds.
Let us denote by X = [x1, . . . , xTl] ∈RD×T the input to
the Bilinear Layer (BL). The layer transforms an input of size
D × T to a matrix of size D′ × T ′ by applying the following
 W1XW2 + B
Fig. 1. Illustration of the proposed Temporal Attention augmented Bilinear
Layer (TABL)
∈RD′×D , W2
∈RT ×T ′, B
∈RD′×T ′ are
the parameters to estimate. φ(·) is an element-wise nonlinear
transformation function, such as ReLU or sigmoid.
One of the obvious advantages of the mapping in Eq. (4) is
that the number of estimated parameters scales linearly with
the dimension of each mode of the input rather than the number of input neurons. For an MLP layer, transforming an input
of size DT to D′T ′ requires the estimation of (DT + 1)D′T ′
parameters (including the bias term), which are much higher
than the number of parameters (DD′+T T ′+D′T ′) estimated
by a bilinear layer.
A more important characteristic of the mapping in Eq. (4),
when it is applied to time-series data, is that the BL models
two dependencies (one for each mode of the input representation), each of which has different semantic meanings. In order
to better understand this, denote each column and row of X
∈RD, t = 1, . . . , T and xrd
∈RT , d = 1, . . . , D,
respectively. Given the input time-series X, the t-th column
represents D different features or aspects of the underlying
process observed at the time instance t, while the d-th row
contains the temporal variations of the d-th feature during the
past T steps. Since
W1xc1, . . . , W1xcT
(xrD)T W2,
Eq. (5) shows that the interaction between different features/aspects at a time instance t = 1, . . . , T is captured by
W1 while in Eq. (6), W2 models the temporal progress of the
d-th feature/aspect. For example, given that X contains stock
prices of D different LOB levels during the history T , the BL
determines how different stock prices interact at a particular
time instance by W1 and how the prices of a particular index
progress over time by W2. It has been shown in that
taking advantage of the spatial structure existing in the LOB
yields better joint distribution of the future best bid and ask
B. Temporal Attention augmented Bilinear Layer
Although the BL learns separate dependencies along each
mode, it is not obvious how a representation at a time instance
interacts with other time instances or which time instances are
important to the prediction at horizon T ′. By incorporating the
position information into the attention calculation scheme, the
authors in showed that the learned model only used a particular time instance in the past sequence to predict the future
value at a given horizon for sequence-to-sequence learning.
In order to learn the importance of each time instance in the
proposed BL, we propose the Temporal Attention augmented
Bilinear Layer (TABL) that maps the input X ∈RD×T to the
output Y ∈RD′×T ′ as follows:
k=1 exp(eik)
˜X = λ( ¯X ⊙A) + (1 −λ) ¯X
  ˜XW2 + B
where αij and eij denote the element at position (i, j) of A
and E, respectively, ⊙denotes the element-wise multiplication
operator and φ(·) is a predeﬁned nonlinear mapping as in Eq.
W1 ∈RD′×D, W ∈RT ×T , W2 ∈RT ×T ′, B ∈RD′×T ′
and λ are the parameters of the proposed TABL. Similar to the
aforementioned BL, TABL models two separate dependencies
through W1 and W2 with the inclusion of the intermediate
attention step learned through W and λ. The forward pass
through TABL consists of 5 steps, which are depicted in Figure
• In Eq. (7), W1 is used to transform the representation of
each time instance xct, t = 1, . . . , T of X (each column)
to a new feature space RD′. This models the dependency
along the ﬁrst mode of X while keeping the temporal
order intact.
• The aim of the second step is to learn how important the
temporal instances are to each other. This is realized by
learning a structured matrix W whose diagonal elements
are ﬁxed to 1/T . Let us denote by ¯xt ∈RD′ and et ∈
RD′ the t-th column of ¯X and E respectively. From Eq.
(8), we could see that et is the weighted combination
of T temporal instances in the feature space RD′, i.e. T
columns of ¯X, with the weight of the t-th time instance
always equal to 1/T since the diagonal elements of W
are ﬁxed to 1/T . Thus, element eij in E encodes the
relative importance of element ¯xij with respect to other
¯xik, k ̸= j.
• By normalizing the importance values in E using the
softmax function in Eq. (9), the proposed layer pushes
many elements to become close to zero, while keeping
the values of few of them positive. This process, produces
the attention mask A.
• The attention mask A obtained from the third step is
used to zero out the effect of unimportant elements in
RD′. Instead of applying a hard attention mechanism,
the learnable scalar λ in Eq. (10) allows the model
to learn a soft attention mechanism. In the early stage
of the learning process, the learned features extracted
from the previous layer can be noisy and might not
be discriminative, thus hard attention might mislead the
model to unimportant information while soft attention
could enable the model to learn discriminative features in
the early stage, i.e. before selecting the most important
ones. Here we should note that λ is constrained to lie in
the range , i.e. 0 ≤λ ≤1
• Similar to BL, the ﬁnal step of the proposed layer
estimates the temporal mapping W2, extracting higherlevel representation after the bias shift and nonlinearity
transformation.
Generally, the introduction of attention mechanism in the
second, third and fourth step of the proposed layer encourages
the competition among neurons representing different temporal
steps of the same feature, i.e. competition between elements
on the same row of
¯X. The competitions are, however,
independent for each feature in RD′, i.e. elements of the same
column of ¯X do not compete to be represented.
The proposed layer architecture is trained jointly with
other layers in the network using the Back-Propagation (BP)
algorithm. During the backward pass of BP, in order to update
the parameters of TABL, the following quantities must be calculated: ∂L/∂W1, ∂L/∂W, ∂L/∂λ, ∂L/∂W2 and ∂L/∂B
with L is the loss function. Derivation of these derivatives is
given in the Appendix A.
C. Complexity Analysis
As mentioned in the previous section, the memory complexity of the BL is O(DD′ + T T ′ + D′T ′). The proposed
TABL requires an additional amount of O(T 2) in memory.
Computation of the BL requires the following steps: matrix
multiplication W1XW2 with the cost of O(D′DT +D′T T ′),
bias shift and nonlinear activation with the cost of O(2D′T ′).
In total, the computational complexity of BL is O(D′DT +
D′T T ′ + 2D′T ′). Since TABL possesses the same computation steps as in BL with additional computation for the
attention step, the total computational complexity of TABL
is O(D′DT + D′T T ′ + 2D′T ′ + D′T 2 + 3D′T ) with the last
two terms contributed from the applying the attention mask
In order to compare our proposed temporal attention mechanism in bilinear structure with the attention mechanism
in a recurrent structure, we estimate the complexity of the
attention-based Seq-RNN (ASeq-RNN) proposed in as a
reference. Let D′ denote the dimension of the hidden units
in the encoder, memory and decoder module. In addition, we
assume that input and output sequence have equal length. The
total memory and computational complexity of ASeq-RNN
are O(3D′D + 11D′2 + 11D′) and O(11T D′2 + 20T D′ +
4T 2D′ +3T D′D +T 2) respectively. Details of the estimation
are given in the Appendix B. While conﬁgurations of the
recurrent and bilinear architecture are not directly comparable,
it is still obvious that ASeq-RNN has much higher memory
and computational complexity as compared to the proposed
TABL. It should be noted that the given complexity of ASeq-
RNN is derived based on GRU, which has lower memory
and computation complexities compared to LSTM. Variants
of ASeq-RNN proposed to time-series data are, however,
based on LSTM units , , making them even more
computationally demanding.
IV. EXPERIMENTS
In this section, we evaluate our proposed architecture on
the mid-price movement prediction problem based on a largescale high-frequency LOB dataset. Before elaborating on experimental setups and numerical results, we start by describing
the dataset and the prediction task.
A. High-Frequency Limit Order Data
In stock markets, traders buy and sell stocks through an
order-driven system that aggregates all out-standing limit
orders in limit order book. A limit order is a type of order to
buy or sell a certain amount of a security at a speciﬁed price or
better. In a limit order, the trader must specify type (buy/sell),
price and the respective volume (number of stock items he/she
wants to trade). Buy and sell limit orders constitute two sides
of the Limit Order Book (LOB), i.e. the bid and ask side. At
time t, the best bid price (p1
b(t)) and best ask price in 5 different horizons (H =
10, 20, 30, 50, 100) corresponding to the future movements in
the next 10, 20, 30, 50, 100 events.
There exist two experimental setups using FI-2010 dataset.
The ﬁrst setting is the standard anchored forward splits provided by the database which we will refer as Setup1. In
Setup1, the dataset is divided into 9 folds based on a day
basis. Speciﬁcally, in the k-th fold, data from the ﬁrst k days
is used as the train set while data in the (k + 1)-th day is used
as a test set with k = 1, . . . , 9. The second setting, referred
as Setup2, comes from recent works , in which deep
network architectures were evaluated. In Setup2, the ﬁrst 7
days are used as the train set while the last 3 days are used as
test set. We provide the evaluation of our proposed architecture
in both settings using the z-score normalized data provided by
the database.
B. Network Architecture
In order to evaluate the bilinear structure in general and
the proposed Temporal Attention augmented Bilinear Layer
(TABL) in particular, we construct three different baseline
network conﬁgurations (A,B,C) with d = {0, 1, 2} hidden
layers that are all Bilinear Layer (BL). Details of the baseline
EXPERIMENT RESULTS IN SETUP1
Accuracy %
Precision %
Prediction Horizon H = 10
Prediction Horizon H = 50
Prediction Horizon H = 100
network conﬁgurations are shown in Figure 2. The input to all
conﬁgurations is a matrix of size 40×10 which contains prices
and volumes of the top 10 orders from bid and ask side (40
values) spanning over a history of 100 events1. Here 120 × 5-
BL denotes the Bilinear Layer with output size 120×5. Based
on the baseline network conﬁgurations, hereby referred as
A(BL), B(BL) and C(BL), we replace the last BL classiﬁcation
layer by the proposed attention layer (TABL) to evaluate the
effectiveness of attention mechanism. The resulting attentionbased conﬁgurations are denoted as A(TABL), B(TABL) and
C(TABL). Although attention mechanism can be placed in any
1Since the feature vector is extracted from a block of 10 events and we
only use the ﬁrst 40 dimensions of the given feature vector, which correspond
to prices and volumes of the last event in the block
EXPERIMENT RESULTS IN SETUP2
Accuracy %
Precision %
Prediction Horizon H = 10
Prediction Horizon H = 20
Prediction Horizon H = 50
layer, we argue that it is more beneﬁcial for the network to
attend to high-level representation, which is similar to visual
attention mechanism that is applied after applying several
convolution layers . In our experiments, we made no
attempt to validate all possible positions to apply attention
mechanism by simply incorporating it into the last layer.
C. Experiment Settings
The following experimental settings were applied to all
network conﬁgurations mentioned in the previous subsection. We have experimented by training the networks with
two types of stochastic optimizers: SGD and Adam
 . For SGD, the Nesterov momentum was set to 0.9
while for Adam, the exponential decay rates of the ﬁrst
and second moment were ﬁxed to 0.9 and 0.999 respectively. The initial learning rate of both optimizers was set to
0.01 and decreased by the following learning rate schedule
SC = {0.01, 0.005, 0.001, 0.0005, 0.0001} when the loss in
the training set stops decreasing. In total, all conﬁgurations
were trained for maximum 200 epochs with the mini-batch
size of 256 samples.
Regarding regularization techniques, we used a combination
of dropout and max-norm , which was shown to improve
generalization capacity of the network. Dropout was applied
to the output of all hidden layers with a ﬁxed percentage
of 0.1. Max-norm regularizer is a type of weight constraint
that enforces an absolute upper bound on the l2 norm of
the incoming weights to a neuron. The maximum norm was
validated from the set {3.0, 5.0, 7.0}. Although weight decay
is a popular regularization technique in deep neural network
training, our exploratory experiments indicated that weight
decay is not a suitable regularization option when training the
bilinear structure.
We followed similar approach proposed in to weight
the contribution of each class in the loss function. Since
the evaluated network structures output the class-membership
probability vector, the weighted entropy loss function was
yi log(˜yi)
where Ni, yi, ˜yi are the number of samples, true probability
and the predicted probability of the i-th class respectively.
c = 1e6 is a constant used to ensure numerical stability by
avoiding the loss values being too small when dividing by Ni.
All evaluated networks were initialized with the random
initialization scheme proposed in except the attention
weight W and λ of the TABL layer. Randomly initializing W
might cause the layer to falsely attend to unimportant input
parts, leading the network to a bad local minima. We thus
initialized λ = 0.5 and all elements in W by a constant equal
to 1/T with T is input dimension of the second mode. By
initializing W with a constant, we ensure that the layer starts
by putting equal focus on all temporal instances.
D. Experiment Results
Following the experimental settings detailed in the previous
subsection, we evaluated the proposed network structures in
both Setup1 and Setup2. Besides the performance of our proposed network structures, we also report here all available experiment results coming from different models including Ridge
Regression (RR), Single-Layer-Feedforward Network (SLFN),
Linear Discriminant Analysis (LDA), Multilinear Discriminant
Analysis (MDA), Multilinear Time-series Regression (MTR),
Weighted Multilinear Time-series Regression (WMTR) ,
Multilinear Class-speciﬁc Discriminant Analysis (MCSDA)
 , Bag-of-Feature (BoF), Neural Bag-of-Feature (N-BoF)
 in Setup1 and Support Vector Machine (SVM), Multilayer
Perceptron (MLP), Convolutional Neural Network (CNN) 
and LSTM in Setup2.
Since the dataset is unbalanced with the majority of samples belonging to the stationary class, we tuned the hyperparameters based on the average F1 score per class, which
is a trade-off between precision and recall, measured on the
training set. With the optimal parameter setting, the average
Average Attention
Movement: decrease
Average Attention
Movement: stationary
Average Attention
Movement: increase
Fig. 3. Average attention of 10 temporal instances during training in 3 types of movement: decrease, stationary, increase. Values taken from conﬁguration
A(TABL) in Setup2, horizon H = 10
performance on the test set over 9 folds is reported in Setup1
while in Setup2, each network conﬁguration is trained 5 times
and the average performance on the test set over 5 runs is
reported. Besides the main performance metric F1, we also
report the corresponding accuracy, average precision per class
and average recall, also known as sensitivity, per class.
Table I and II report the experiment results in Setup1 and
Setup2, respectively. As can be seen in Table I, all the competing models in Setup1 belong to the class of shallow architectures with maximum 2 hidden layers (C(BL), C(TABL) and
N-BoF). It is clear that all of the bilinear structures outperform
other competing models by a large margin for all prediction
horizons with the best performances coming from bilinear networks augmented with attention mechanism. Notably, average
F1 obtained from the 2 hidden-layer conﬁguration with TABL
exceeds the previous best result in Setup1 achieved by WMTR
in by nearly 25%. Although NBoF and C(TABL) are
both neural network-based architectures with 2 hidden layers,
C(TABL) surpasses NBoF by nearly 30% on all horizons.
This is not surprising since a regression model based on
bilinear projection was shown to even outperform NBoF in
 , indicating that by separately learning dependencies in
different modes is crucial in time-series LOB data prediction.
While experiments in Setup1 show that the bilinear structure
in general and the proposed attention mechanism in particular
outperform all of the existing models that exploit shallow
architectures, experiments in Setup2 establish the comparison between conventional deep neural network architectures
and the proposed shallow bilinear architectures. Even with 1
hidden-layer, TABL performs similarly (H = 20) or largely
better than the previous state-of-the-art results obtained from
LSTM network (H = 10, 50). Although being deep with
7 hidden layers, the CNN model is greatly inferior to the
proposed ones. Here we should note that the CNN proposed
in gradually extracts local temporal information by the
convolution layers. On the other hand, the evaluated bilinear
structures fuse global temporal information from the beginning, i.e. the ﬁrst layer. The comparison between CNN model
and bilinear ones might indicate that the global temporal cues
learned in the later stage of CNN (after some convolution
layers) lose the discriminative global information existing in
the raw data.
Comparing BL and TABL, it is clear that adding the
attention mechanism improves the performance of the bilinear networks with only small increase in the number of
parameters. More importantly, the attention mechanism opens
up opportunities for further analyzing the contribution of the
temporal instances being attended to. This can be done by
looking into the attention mask A. During the training process,
each element in A represents the amount of attention the
corresponding element in ¯X receives. In order to observe how
each of the 10 events in the input data contributes to the
AVERAGE COMPUTATION TIME OF STATE-OF-THE-ART MODELS
Forward (ms)
Backward (ms)
Total (ms)
Fig. 4. Corresponding λ during training in A(TABL) in Setup2 and horizon
decision function, we analyze the statistics during the training
process of the conﬁguration A(TABL) in Setup2 with horizon
H = 10. Figure 3 plots the average attention values of each
column of A which correspond to the average attention the
model gives to each temporal instance during the training
process. The three plots correspond to attention patterns in
three types of movement of the mid-price. It is clear that
the given model focuses more on some events such as the
second (t −1), third (t −2) and fourth (t −3) most recent
event in all types of movement. While the attention patterns
are similar for the decrease and increase class, they are both
different when compared to those of the stationary class. This
indicates that when the mid-price is going to move from its
equilibrium, the model can shift its attention to different events
in order to detect the prospective change. Figure 4 shows the
corresponding values of λ from the same model after every
epoch during the training process. As can be seen from Figure
4, λ increases in the ﬁrst few steps before stabilizing close
to 1, which illustrates a soft attention behavior achieved by
λ described in section III-B. The insights into the attention
patterns and the amount of attention received by each event
given by the proposed attention-based layer could facilitate
further quantitative analysis such as casualty or pseudo-period
Table III reports the average computation time of C(BL),
C(TABL), CNN , LSTM measured on the same
machine with CPU core i7-4790 and 32 GB of memory. The
second, third and last column shows the average time (in
millisecond) taken by the forward pass, backward pass and one
training pass of a single sample in the state-of-the-art models.
It is obvious that the proposed attention mechanism only
increases the computational cost by a relatively small margin.
On the contrary, previously proposed deep neural network
architectures require around 3× (CNN) and 10× longer to
train or make inference while having inferior performances
compared to the proposed architecture. This points out that
our proposed architecture excels previous best models not
only in performance but also in efﬁciency and practicality in
applications such as High-Frequency Trading.
V. CONCLUSIONS
In this paper, we proposed a new neural network layer type
for multivariate time-series data analysis. The proposed layer
leverages the idea of bilinear projection and is augmented with
a temporal attention mechanism. We provide theoretical analysis on the complexity of the proposed layer in comparison with
existing attention mechanisms in recurrent structure, indicating
that the proposed layer possesses much lower memory and
computational complexity. Extensive experiments in a largescale Limit Order Book dataset show the effectiveness of
the proposed architecture: with only 2 hidden-layers, we can
surpass existing state-of-the-art models by a large margin. The
proposed temporal attention mechanism not only improves
the performance of the bilinear structure but also enhances
its interpretability. Our quantitative analysis of the attention
patterns during the training process opens up opportunities in
future research of the patterns of interest.
APPENDIX A
TABL DERIVATIVES
In order to calculate the derivatives of TABL, we follow
the notation: given X ∈RI×J and X ∈RM×N, ∂Y/∂X is
a matrix of size IJ × MN with element at (ij, mn) equal to
∂Yij/∂Bmn. Similarly ∂L/∂X ∈R1×MN with L ∈R, X ∈
RM×N. Denote IM ∈RM×M the identity matrix and 1MN ∈
RM×N a matrix with all elements equal to 1. In addition, our
derivation heavily uses the following formulas:
where ⊗denotes the Kronecker product, vec(A) denotes the
vectorization operator that concatenates columns of A into
one vector and diag(x) denotes the diagonal matrix with the
diagonal elements taken from x.
We proceed by calculating the derivate of the left-hand side
with respect to every term on the right-hand side from Eq. (7)
• From Eq. (7)
= ∂(ID′W1X)
∂X = ∂(W1XIT )
• From Eq. (8)
∂¯X = ∂(ID′ ¯XW)
∂W = ∂( ¯XWIT )
• From Eq. (9) ∂A/∂E is calculated by the following
αij(1 −αij)
k̸=j,p exp(eik), ∀p ̸= j
= 0, ∀p ̸= i
• From Eq. (10)
∂A = λ∂( ¯X ⊙A)
∂¯X = ∂([λA + (1 −λ)1D′T ] ⊙¯X)
 vec(λA + (1 −λ)1D′T )
 vec(λA + (1 −λ)1D′T )
∂λ = ( ¯X ⊙A −¯X)
• In Eq. (11), denote ¯Y = ˜XW2 + B, Eq. (11) becomes
Y = φ( ¯Y) and we have:
∂(ID′ ˜XW2)
∂( ˜XW2IT )
∂B = ∂φ( ¯Y)
∂(ID′BIT )
where ∂φ( ¯Y)/∂¯Y is the derivative of the element-wise
activation function, which depends on the form of the
chosen one.
During the backward pass, given ∂L/∂Y and ∂φ( ¯Y)/∂¯Y,
using chain rules and the above results, the derivatives required
in TABL can be calculated as below:
APPENDIX B
COMPLEXITY OF ATTENTION-BASED RNN
The attention-based sequence to sequence learning proposed
in comprises of the following modules:
 Wexi + Ue(re
i−1) + be
i = (1 −ze
α tanh(Wαhd
j−1 + Uαhe
k=1 exp(ekj)
zyj−1 + Ud
j−1 + Czcj + bd
ryj−1 + Ud
j−1 + Crcj + bd
 wdyj−1 + Ud(rd
j−1) + Ccj + bd
j = (1 −zd
where i = 1, . . . , T and j = 1, . . . , T denote the index in
input and output sequence respectively, which we assume having equal length. In order to generate sequence of prediction
rather than probability of a word in a dictionary, we use Eq.
(46) similar to . To simplify the estimation, let the number
of hidden units in the encoder, memory and decoder module
equal to D′, i.e. he
j, vα ∈RD′, and the output yj is a
The encoder module estimates the following parameters:
z ∈RD′×D, Ue, Ue
z ∈RD′×D′, be, be
RD′, which result in O(3D′D + 3D′2 + 3D′) memory and
O(T (3D′D + 3D′2 + 8D′) computation.
The memory module estimates the following parameters:
vα ∈RD′, Wα, Uα ∈RD′×D′, which cost O(2D′2 + D′)
memory and O(2D′2T + 4T 2D′ + T 2) computation.
parameters:
z, Ud, Cz, Cr, C
r, bd, wout
O(6D′2 + 7D′) memory and O(T (12D′ + 6D′2)).
In total, the attention model requires O(3D′D + 11D′2 +
11D′) memory and O(11T D′2+20T D′+4T 2D′+3T D′D+
T 2) computation.