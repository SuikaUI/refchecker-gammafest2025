Department of Statistics Papers
Vision as Bayesian Inference: Analysis by Synthesis?
 
Yuille, Alan L
Kersten, Daniel
Publication Date
eScholarship.org
Powered by the California Digital Library
University of California
Vision as Bayesian Inference: Analysis by Synthesis?
Alan Yuille1 and Daniel Kersten2.
Departments of Statistics UCLA1, Psychology University of Minnesota 2.
emails: , .
We argue that the study of human vision should be aimed at determining how humans perform
natural tasks on natural images. Attempts to understand the phenomenology of vision from artiﬁcial stimuli, though worthwhile as a starting point, risk leading to faulty generalizations about
visual systems. In view of the enormous complexity of natural images, they are similar to trying
to evaluate the performance of a soldier in battle from his ability at playing with a water pistol.
Dealing with this complexity is daunting, but Bayesian inference on structured probability distributions oﬀers the ability to design theories of vision that can deal with the complexity of natural
images and which use analysis by synthesis strategies with intriguing similarities to the brain.
Introduction: Perception as inference
Experimental studies of biological vision systems have typically been performed using artiﬁcial
stimuli and tasks. While much progress has been made, it is questionable how much light these
studies shed on the performance of biological vision systems when faced with the complexity of
natural images and natural tasks such as segmentation and object detection and recognition. For
example, recent studies suggest that the responses of neurons in V1 to natural stimuli cannot
be predicted from their responses to artiﬁcial stimuli .
It is well known to computer vision
researchers that vision algorithms that work on artiﬁcial stimuli almost never generalize to natural
images (e.g. there are many algorithms which solve the correspondence problem and estimate depth
on Julesz random dot stereograms, but give terrible results when applied to natural images).
We argue that the major diﬃculty of vision arises because natural images are both complex and
objectively ambiguous. Similar 3D objects can result in diﬀerent images, and diﬀerent objects can
result in similar images. Moreover, typical images can be highly complex and consist of hundreds
of objects, many of which are overlapping. Therefore theoretical and experimental attempts to
understand biological systems must come to terms with the daunting complexity of images somwhat
analogous to the way that molecular biologists have had to tame the complexity of the genome.
In this article, we argue that the theoretical principles described in this special issue are rich
enough to deal with the complexities and ambiguities of natural images and to perform major
perceptual tasks such as recognition and segmentation. The approach is based on Bayesian inference
using probability distributions deﬁned on structured representations . Vision is treated as
an inverse inference problem, in the spirit of Helmholtz , where the goal is to estimate the
factors that have generated the image. Two major themes follow naturally from this approach.
Firstly, vision as inverse inference presupposes a formalization of how the input is generated,
and which of the causes of that input should be estimated. This probabilistic generative approach
enables us to deﬁne Bayesian Ideal Observers (BIO) based on the principles of Bayesian Decision
Theory and using structured representations and distributions which subsume the standard models
of signal detection theory and give theories of optimal performance against which human
performance can be compared (the article by Griﬃths and Yuille introduces these concepts). Recent
survey papers justify this approach and give an entry point to the growing body of literature on
this approach . An alternative perspective is given by Howe et al who question both
the use of Bayesian Decision Theory and whether the factors being estimated are physical variables
(like reﬂectance, size, i.e. Helmholtz ) or “orderings of visual stimuli” based on past experience
with all such stimuli (as advocated in ).
Secondly, this inverse inference perspective suggests that the inference algorithm should combine
a top-down generative component with bottom-up processing. The generative component allows
the system to internally simulate, or synthesize, from the probability distributions and so is known
as “analysis by synthesis” and relates to the forward and backward projections in the brain 
 (see for how this may relate to neurotransmitters).
But it must be
emphasized that analysis by synthesis is not necessarily required for Bayesian inference (see, for
example, work on the detection of hands ) and so we will give arguments for it in the next section.
The themes in this paper are common to other aspects of cognitive science as described in
this special issue (see introduction by Chater, Tenenbaum and Yuille). The ability to simulate
by generative models also occurs naturally within visuo-motor control where they can be used to
simulate the consequence of actions, cf. . Mental imagery also suggests the ability to
do internal simulation, but over long time scales. At a more theoretical level, statistical inference
on structured representations oﬀers a common mathematical framework for cognitive science which
leads naturally to theoretical models for coupling diﬀerent sensory modalities and for integrating
perception with planning. For example, recent work , has built on theoretical studies , 
to model the coupling of visual cues and haptic cues and shown good ﬁt with experimental data.
The need for Bottom-up and Top-down processing
We now give arguments why visual inference requires bottom-up and top-down components. This
contrasts with standard textbook theories of vision which favour bottom-up processing based on
computing low-level representations such as edge maps (cf. ) or 2-1/2D representations of depth
and shape (cf. ).
First observe that low-level vision is ambiguous. For example, it is extremely diﬃcult to determine whether there is an edge present in a small region of an image . These ﬁndings are
supported by empirical studies of the relative ineﬀectiveness of edge detectors on natural images
 and the limitations of regional cues for segmentation .
By contrast, high-level vision is rarely ambiguous. The patterns of objects, such as faces or other
objects, are complex and rarely occur by chance. Moreover, they are often easy to resolve when
they do. For example, patterns in the bark of a tree may occasionally look like a face, see Figure 5,
but can easily be disambiguated by the alternative explanations for these patterns. This lack of
ambiguity for high-level vision is highlighted by the recent successes of computer vision algorithms
for detecting high-level objects such as faces and text reliably from natural images .
From this perspective, a major problem for vision systems is how to use low-level cues to rapidly
access the correct high-level models so as to quickly resolve the low-level ambiguities. The diﬃculty
is knowing which high-level model(s) to use. Consider the text-book example of the black and white
dalmation dog . Low-level cues for this image contain little evidence to activate a high-level
dog model, and so naive subjects take a long time to detect the dog. But subjects who have seen
the image before, and know that there is a dog in it, can perceive it instantaneously.
This suggest a visual system where low-level cues make bottom-up proposals which are validated
by high-level models. This bottom-up processing should include standard methods such as edge
detection and grouping by regional properties – but it must also include special processes for
detecting important objects such as faces and text , and for rapidly classifying the scene
 . These high-level models access the image, or a ﬁltered version of it, in a top-down process
to ensure consistency of the image interpretation. The early visual areas such as V1/V2 are the
most natural candidates for such an image representation . In certain cases, the bottom-up
cues are suﬃciently unambiguous and so the object, or scene structure, can be detected without
high-level feedback.
Generative Models, Image Parsing and Analysis by Synthesis
We now describe how this visual system can be implemented. We ﬁrst give an introductory example
which illustrates the main points of the approach on simple examples. Then we describe a more
advanced theory that applies to natural images.
Introduction to Generative Models and Analysis by Synthesis
First, we consider models for generating an image. Suppose we start with a simple vocabulary
of shapes and patterns which contains the letters A, B, C, .... We can deﬁne a simple probability
model for generative images built out of this vocabulary by using templates for each letter and
allow the letter to be placed randomly at any position in the image. We can also give an ordering
on the letters so that one letter can completely, or partially, occlude another letter. Sampling from
this distribution will yield images as shown in Figure 1A.
Next, we can make this generative model richer by expanding the vocabulary to include additional objects such as rectangular bars and fragments of letters, Figure 1B. The addition of the
elements allows us to generate more complex images but the richness of the vocabulary makes
some images potentially ambiguous. The same image can be generated in two distinct ways, see
Figure 1B.
This ambiguity in generation leads to potential ambiguities of the inverse process of inference/interpretation. This is resolved by having probabilities on the diﬀerent ways that images can
be generated. The chance alignment of fragments is more likely than a B with an invisible (white)
occluder (see Figure 1B). But a B with a visible (black) occluding bar is more likely than a set of
fragments which are accidentally aligned with the bar.
We now extend the vocabulary of the generative model in several ways, see Figure 1C. Firstly,
we we can allow the letters to have properties such as size, font, and shading pattern. Secondly, we
can put probabilities on the spatial relations between letters so that, for example, they line up to
form words. This extension of the vocabulary leads to further ambiguities.
Vocabulary
Figure 1: Left Panel (A). A simple vocabulary for generating the image. There is little, or no,
ambiguity in interpreting images. At worst, the letter X may be confused with a slanted I partially
occluding a vertical I. Centre Panel (B). A richer vocabulary. A given cause, such as a particular
letter, can be manifest in many diﬀerent images. But there are now multiple ways to generate
identical images, see text.
Right Panel (C). The richer the vocabulary, the greater the image
ambiguity, and the harder it is to interpret the image. This leads to a formidable inference problem.
But now we turn to a diﬀerent issue – how fast can we perform inference to determine the
most likely way the image was generated? Our explanations, so far, have assumed that we can
check all possible ways to generate the image and decide on the most probable. But this becomes
completely impractical when the size of the vocabulary becomes very large. So how can we hope
to solve the inverse inference problem of estimating which conﬁguration of objects is most likely to
have generated the observed image?
We propose an “analysis by synthesis” strategy where low-level cues, combined with spatial
grouping rules (similar to Gestalt laws), make bottom-up proposals which activate hypotheses about
objects and scene structures. These hypotheses are accepted, or rejected, by direct comparison to
the image (or a ﬁltered version of it) in a top-down process. These bottom-up proposals come with
probabilities, which are a measure of their strength. If bottom-up proposals are suﬃciently strong
(i.e. the low-level cues are suﬃciently unambiguous) then they may be accepted without any need
for veriﬁcation at a lower level.
For example, consider an image containing letters, rectangles, and letter fragments, Figure 2.
We can obtain bottom-up cues in several ways. Firstly, we can run an edge detector to discard the
shading information (which is often variable) and then spatially group the edges into segments by
using the principles of continuity, parallelism and colinearity. This will enable us to get cues for the
positions of rectangles and the identity of some letters. These features are used to make bottom-up
proposals regarding possible objects. Remaining ambiguity is eliminated in a synthesis stage that
tests how well the object models explain the image features. Other more sophisticated cues can be
used for the proposals. For example, we can treat text as a type of texture and design detectors
which respond to characteristic patterns of text (e.g. ). We stress that this is an illustration of
this model only, ignoring shading information may be a mistake for other more realistic stimuli.
Natural Image Parsing
The acid test for generative models and analysis by synthesis algorithms is whether they can be
extended to deal with the complexities of natural images. We now describe recent work which
Feature extraction
Synthesis & verification
Figure 2: Analysis by synthesis. A. Low-level processing (left panel) can extract edge features,
such as bars, and use conjunctions of these features to make bottom-up proposals to access the
higher-level models of objects. B. The high-level objects access the image top-down to validate
or reject the bottom-up proposals (right panel). In this example, the low-level cues propose that
the image can be interpreted as an E, an F, or a set of parallel bars. But interpreting it as an F
explains almost all the features in the image and is preferred.
suggests that they can.
This work uses a generative model for images which is similar to a two level probabilistic context free grammar (PCFG) (see article by Griﬃths and Yuille) and hence we refer
to the approach as image parsing.
This model is illustrated in Figure (3) (left panel) where
the root node represents the entire image scene. The ﬁrst level corresponds to the non-terminal
nodes representing parameterized models of objects, such as faces or letters, or of generic regions
such as textures or shaded patterns.
A non-terminal node i has attribute variables (ζi, Li, Θi)
where ζi labels the type of the model (e.g.
face, letter, texture or shading), Θi denotes the
model parameters (e.g. the parameters that determine the shape of the letter), and R(Li) denotes the region in the image that the model generates. These regions are non-overlapping and
the discontinuities across the boundaries are not explicity modeled.
Formally, we can summarize the non-terminal nodes by W = {(ζi, Li, Θi) : i = 1, ..., N} where the number N of nodes
is a random variable.
These non-terminal nodes are obtained by sampling from a distribution
P(W) = p(N) QN
i=1 p(Li)p(ζi|Li)p(Θi|ζi) (this is conceptually similar to applying production rules
to the root node). In turn, the observed intensity values on the image lattice (the terminal nodes
of the graph) are obtained by sampling from generative models p(IR(L)|ζ, L, Θ) for the speciﬁc
regions which depend on their model type and their parameters (similar to applying production
rules to the non-terminal nodes in a PCFG). This includes models for generating the appearance
of faces and letters, see samples in Figure (3) (right panel). Overall, this second level gives a model
p(I|W) = QN
i=1 p(IR(Li)|ζi, Li, Θi) . This combines with the ﬁrst stage to have a full generative
model p(I|W)p(W) for the image.
There are many ways to extend this model by augmenting the number of pattern types, by
including Gestalt laws and other principles of spatial organization , and by having hierarchical
models . In particular, the pattern types can be expanded to include material properties
which are not explicit objects.
The advantages of a generative model for the entire image include the ability to “explain away”.
Submodels corresponding to diﬀerent objects, or processes, compete and cooperate to explain dif-
ferent parts of the image (e.g. the letter B plus bar competes with the interpretation of accidentally
aligned fragments in Figure 1B). A face model might hallucinate a face in the trunk of a tree; but a
tree model can overule this and provide the correct interpretation of the tree trunk, see Figure (5).
In addition, full generative models enforce consistency of the interpretation of the image.
background
Figure 3: A. The image is generated (left panel) by a probabilistic context free grammar shown by
a two layer graph with nodes with properties (ζ, l, θ) corresponding to regions Li in the image. B.
The right panel shows samples from the face model and the letter model – i.e. from p(IR(L)|ζ, L, Θ).
We now switch to the task of performing inference on this generative model to estimate W ∗=
arg maxW P(W|I). This requires a sophisticated inference algorithm that can perform operations
such as creating nodes, deleting nodes, diﬀusing the boundaries, and altering the node attributes.
The strategy used in is to perform analysis by synthesis by a data-driven Markov Chain
Monte Carlo (DDMCMC) algorithm. This algorithm is guaranteed to converge by standard properties of MCMC. Informally, low-level cues are used to make hypotheses about the scene which can
be veriﬁed or rejected by sampling from the models. For example, low-level cues can be
used to hypothesize that there is a face in a region of the image. This hypothesis can be validated
or rejected by sampling from a generative face model. The bottom-up cues propose that there
are faces in the tree bark, but this proposal is rejected by the top-down generative model, see
Figure (5). Inference is performed by applying a set of operators which change the structure of the
parse graph, see Figure (4). These operators are implemented by transition kernels K, see Box 1
for a more technical description of the algorithm. The bottom-up cues are based on discriminative
models which are described in Box 2.
Implications for Cognitive Science
We claim that the above model for image parsing shares key elements with human visual processing.
This claim raises a number of important questions.
background
background
Figure 4: Examples of Markov chain dynamics that change the graph structure or the node attributes of the graph giving rise to diﬀerent ways to parse the image.
Diﬀerent dynamics, for
example creating or deleting nodes, are performed by diﬀerent kernels K.
“Isn’t feedback inconsistent with fast processing in human object recognition?”
We argue that the bottom-up proposals are consistent with fast feedforward processing. If these
proposals are strong, then the high-level percept can occur before top-down validation has begun.
There is evidence that reliable diagnostic information for certain categories is available from very
simple image measurements , and that humans make certain categorical decisions suﬃciently
fast to preclude a veriﬁcation loop (but see and ).
Figure 5: Top left: Input image. Top right: Bottom-up proposals for text and faces are shown
by boxes. A face is “hallucinated” in a tree. Bottom centre: Overall segmentation (bottom left),
Detection of letters and faces. Bottom right: Synthesised image
“Where do the generative models come from?”
Ideally the generative models, the discriminative models, and the stochastic grammar would all
be learnt from natural images. This is not diﬃcult in principle because, as discussed in Griﬃths
and Yuille, learning the model from data is simply another example of statistical inference. The
Helmholtz machine gives an illustration of how a generative model, and an inference algorithm,
can be learnt. This approach, however, has been applied only to simple visual stimuli. Similarly
Friston suggests learning models using the Expectation-Maximization algorithm. Although
this is a useful metaphor, the challenge is to see whether this idea can be translated to algorithms
that can deal with the complexities of natural images.
Learning generative and discriminative models is an extremely diﬃcult problem in practice
due to the large dimensionality of natural images. There has recently, however, been dramatic
progress on the similar, but arguably simpler, problem of learning a stochastic grammar for natural
languages (see article by Chater and Manning). At present, diﬀerent components of the image
parsing model are learnt individually. For example, the discriminative models for text and faces
are trained using labelled examples of “face”, “text”, and “non-face”, “non-text”. Similarly the
generative models for faces and text are learnt from labelled examples of faces and text. This,
however, is far easier than unsupervised learning of the full stochastic grammar.
“Where is attention in the image parsing model?”
The current DDMCMC algorithm contains no explicit attention mechanism. Instead bottom-up
cues act everywhere in parallel to activate the generative models. But there are some mechanisms in
DDMCMC which are similar to attention – for example, proposals which have high conﬁdence are
processed quickly. Moreover, it is possible to add an attentional mechanism whereby processing is
directed to maximize criteria such as the expected gain in information. In other words, uncertainty
in the residual signal helps prioritize where DDMCMC should process the image. This could be part
of the mechanisms for driving extrinsic attention. It is unclear how this relates to standard theories
of attention and the pre/post attentive distinction . But it is also unclear how these types of
theories, developed for artiﬁcial idealized stimuli, relate to the performance of human observers on
realistic natural images. For example, Li et al show that human subjects can pre-attentively
detect complex objects, such as animals, in natural images.
“How does this relate to neural mechanisms?”
There have long been speculations about the relationship between analysis by synthesis and forward
and backward pathways in the cortex (e.g. see ). For a recent review of the ascending and
descending pathways between visual areas, see .
Although it is impractical at present to test the details of analysis by synthesis models there
are some relevent ﬁndings using functional magnetic resonance imaging (fMRI) and high-density
electrical mapping of evoked response potentials (ERP). Earlier fMRI work by a number of groups
has shown that the human lateral occipital complex (LOC) increases activity during the perception
of object completion. Murray et al used fMRI to show that when local visual information is
perceptually organized into whole objects, activity in human primary visual cortex (V1) decreases
over the same approximate period that activity in higher, lateral occipital areas (LOC) increases.
The authors interpret the activity changes in terms of high-level hypotheses that compete to explain
away the incoming retinal data.
This interpretation is also consistent with ﬁndings of another
group, Murray et al , who combined ERP and fMRI. Based on timing measurements, they
concluded that dorsal regions provide input to LOC, where signatures of illusory contour completion
ﬁrst appear, then followed by activity in V1/V2 . Activity in fusiform gyrus has also been associated
with object recognition. Bar et al. used magnetoencephalography (MEG) and fMRI in an object
recognition task to show that low spatial frequency speciﬁc activity appeared in left orbitofrontal
cortex 50 msec earlier than than in fusiform gyrus of temporal cortex. They interpreted their results
as consistent with a fast, spatially coarse analysis that selects probable object interpretations which
are subsequently integrated with bottom-up information.
Conclusion
We have argued that the major goal of vision science should be to determine how biological systems
work under natural conditions and when performing natural tasks. This requires understanding
and modeling the complexity of images. We argue that studies of perceptual abilities on simple
synthetic stimuli can be misleading and unrepresentative of how perceptual systems function under
realistic conditions.
By using recent examples from the computer vision literature, we showed that probability distributions deﬁned on structured representations oﬀer the promise to model natural images. Current
research is extending this model including more sophisticated representations, better bottom-up
cues, and extensions to greater ranges of objects and scenes. One advantage of this type of theory
is that is readily “extensible” in the sense that by designing increasingly sophisticated generative
models of this form we can in principle develop artiﬁcial visual systems of arbitrary eﬀectiveness.
These generative models can be used as Bayesian Ideal Observers.
The inference algorithm for this generative model is intriguing because it naturally follows the
“analysis by synthesis” strategy that may correspond to the forward and backward pathways in
the cortex. This algorithm combines bottom-up and top-down processing by using low-level cues
to activate high-level models which are compared to the image in a top-down process. It is hoped
that as techniques like fMRI, EEG, MEG and multi-unit recording continue to develop it will be
possible to make direct experimental predictions from these models.
Box 1: Data Driven Markov Chain Monte Carlo
The DDMCMC algorithm requires designing transition kernels Ki(W, W ′) for
the graph operations illustrated in Figure (4).
These kernels give a probability to transition from state W to state W ′ and obey the normalization condition
W ′ Ki(W, W ′) = 1, ∀W, i.
They are also designed to obey the detail balance condition P(W|I)Ki(W, W ′) = P(W ′)Ki(W ′, W), which ensures that repeatedly sampling
from these kernels will give samples from the posterior distribution P(W|I) (plus some
technical conditions). The full system combines all these kernels into a single kernel
K(W, W ′) = P
i αiKi(W, W ′). The αi (P
i αi = 1) are probabilities, so at each timestep one kernel (i.e.
type of transition) is selected with probability αi.
The kernels
are designed to be of Metropolis-Hastings form Ki(W, W ′) = qi(W, W ′)ai(W, W ′), where
a transition from W to W ′ is proposed by qi(W, W ′) and accepted, or rejected, by
ai(W, W ′). The proposals qi(W, W ′) are designed to be bottom-up proposals which are
designed using discriminative models Q(W|I) ∝Q
i Q(wi|φi(I)) which give easily computable cues to determine the components wi of the representation W in terms of features
φi(I) computed from the image (see Box 2). The acceptance probabilities ai(W, W ′) are
based on the high-level models, for details see .
Box 2: Generative and Discriminative Models
Originally discriminative methods were deﬁned by decision rules α(I) which can
be described in terms of Bayesian Decision Theory, see box in Griﬃths and Yuille’s article. These decision rules output discrete values (e.g. “face” or “non-face”) and there
was no attempt to model the probability distribution P(I, W). Discriminative methods
of this type include classic techniques like the perceptron and more recent methods such
as AdaBoost and Support Vector Machines . More recently, discriminative methods
have been generalized to include any method that approximates the posterior distribution P(W|I). Intuitively, these methods make decisions but, by including probabilities,
they give a measure of conﬁdence in the decision. This is the sense in which we used
discriminative methods in this article. Discriminative methods can be applied to learn
approximate distributions Q(w1|φ(I)) for components w1 of the full interpretation W,
where φ(I) is a set of features extracted from the image. The key idea, to ensure speed
of discriminative proposals, is that the feature φ(I) can be rapidly extracted and the approximate distribution Q(w1|φ(I)) is rapid to compute. For example, AdaBoost learning
can be used to learn discriminative probabilities for the presence, or absence, of a face
at a speciﬁc scale, orientation, and location in the image.
ACKNOWLEDGMENTS
We would like to acknowledge helpfull feedback from the reviewers and the TICS editors. This
work was supported by ONR N00014-05-1-0124 and NIH RO1 EY015261.