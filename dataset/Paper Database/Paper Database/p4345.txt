Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1535–1546
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1535–1546
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Lexically Constrained Decoding for Sequence Generation Using Grid
Beam Search
Chris Hokamp
ADAPT Centre
Dublin City University
 
ADAPT Centre
Dublin City University
 
We present Grid Beam Search (GBS), an
algorithm which extends beam search to
allow the inclusion of pre-speciﬁed lexical constraints.
The algorithm can be
used with any model that generates a sequence ˆy = {y0 . . . yT }, by maximizing
p(y|x) = Q
p(yt|x; {y0 . . . yt−1}). Lexical constraints take the form of phrases
or words that must be present in the output sequence. This is a very general way
to incorporate additional knowledge into
a model’s output without requiring any
modiﬁcation of the model parameters or
training data. We demonstrate the feasibility and ﬂexibility of Lexically Constrained
Decoding by conducting experiments on
Neural Interactive-Predictive Translation,
as well as Domain Adaptation for Neural
Machine Translation. Experiments show
that GBS can provide large improvements
in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve signiﬁcant gains in performance in domain adaptation scenarios.
Introduction
The output of many natural language processing
models is a sequence of text. Examples include
automatic summarization , machine translation , caption generation , and dialog generation , among others.
In some real-world scenarios, additional information that could inform the search for the optimal output sequence may be available at inference
time. Humans can provide corrections after viewing a system’s initial output, or separate classiﬁcation models may be able to predict parts of the
output with high conﬁdence. When the domain of
the input is known, a domain terminology may be
employed to ensure speciﬁc phrases are present in
a system’s predictions. Our goal in this work is to
ﬁnd a way to force the output of a model to contain
such lexical constraints, while still taking advantage of the distribution learned from training data.
For Machine Translation (MT) usecases in particular, ﬁnal translations are often produced by
combining automatically translated output with
user inputs.
Examples include Post-Editing
(PE) and Interactive-
Predictive MT . These interactive scenarios
can be uniﬁed by considering user inputs to be lexical constraints which guide the search for the optimal output sequence.
In this paper, we formalize the notion of lexical constraints, and propose a decoding algorithm
which allows the speciﬁcation of subsequences
that are required to be present in a model’s output. Individual constraints may be single tokens or
multi-word phrases, and any number of constraints
may be speciﬁed simultaneously.
Although we focus upon interactive applications for MT in our experiments, lexically constrained decoding is relevant to any scenario
where a model is asked to generate a sequence
ˆy = {y0 . . . yT } given both an input x, and a
set {c0...cn}, where each ci is a sub-sequence
{ci0 . . . cij}, that must appear somewhere in ˆy.
This makes our work applicable to a wide range
of text generation scenarios, including image description, dialog generation, abstractive summarization, and question answering.
The rest of this paper is organized as follows:
Section 2 gives the necessary background for our
Figure 1: A visualization of the decoding process for an actual example from our English-German MT experiments. The output
token at each timestep appears at the top of the ﬁgure, with lexical constraints enclosed in boxes. Generation is shown in
blue, Starting new constraints in green, and Continuing constraints in red. The function used to create the hypothesis at each
timestep is written at the bottom. Each box in the grid represents a beam; a colored strip inside a beam represents an individual
hypothesis in the beam’s k-best stack. Hypotheses with circles inside them are closed, all other hypotheses are open. (Best
viewed in colour).
discussion of GBS, Section 3 discusses the lexically constrained decoding algorithm in detail,
Section 4 presents our experiments, and Section 5
gives an overview of closely related work.
Background: Beam Search for
Sequence Generation
Under a model parameterized by θ, let the best
output sequence ˆy given input x be Eq. 1.
ˆy = argmax
where we use {y[T]} to denote the set of all sequences of length T. Because the number of possible sequences for such a model is |v|T , where |v|
is the number of output symbols, the search for ˆy
can be made more tractable by factorizing pθ(y|x)
into Eq. 2:
pθ(yt|x; {y0 . . . yt−1}).
The standard approach is thus to generate the
output sequence from beginning to end, conditioning the output at each timestep upon the input x,
and the already-generated symbols {y0 . . . yi−t}.
However, greedy selection of the most probable
output at each timestep, i.e.:
ˆyt = argmax
p(yi|x; {y0 . . . yt−1}),
risks making locally optimal decisions which are
actually globally sub-optimal. On the other hand,
an exhaustive exploration of the output space
would require scoring |v|T sequences, which is
intractable for most real-world models. Thus, a
search or decoding algorithm is often used as a
compromise between these two extremes. A common solution is to use a heuristic search to attempt to ﬁnd the best output efﬁciently . The key
idea is to discard bad options early, while trying
to avoid discarding candidates that may be locally
risky, but could eventually result in the best overall
Beam search is probably
the most popular search algorithm for decoding sequences. Beam search is simple to implement, and
is ﬂexible in the sense that the semantics of the
Figure 2: Different structures for beam search. Boxes represent beams which hold k-best lists of hypotheses. (A) Chart
Parsing using SCFG rules to cover spans in the input. (B)
Source coverage as used in PB-SMT. (C) Sequence timesteps
(as used in Neural Sequence Models), GBS is an extension of
(C). In (A) and (B), hypotheses are ﬁnished once they reach
the ﬁnal beam. In (C), a hypothesis is only complete if it has
generated an end-of-sequence (EOS) symbol.
graph of beams can be adapted to take advantage
of additional structure that may be available for
speciﬁc tasks. For example, in Phrase-Based Statistical MT (PB-SMT) , beams are
organized by the number of source words that are
covered by the hypotheses in the beam – a hypothesis is “ﬁnished” when it has covered all source
words. In chart-based decoding algorithms such as
CYK, beams are also tied to coverage of the input,
but are organized as cells in a chart, which facilitates search for the optimal latent structure of the
output . Figure 2 visualizes three
common ways to structure search. (A) and (B) depend upon explicit structural information between
the input and output, (C) only assumes that the
output is a sequence where later symbols depend
upon earlier ones. Note also that (C) corresponds
exactly to the bottom rows of Figures 1 and 3.
With the recent success of neural models for
text generation, beam search has become the
de-facto choice for decoding optimal output sequences .
with neural sequence models, we cannot organize
beams by their explicit coverage of the input. A
simpler alternative is to organize beams by output
timesteps from t0 · · · tN, where N is a hyperparameter that can be set heuristically, for example
by multiplying a factor with the length of the input to make an educated guess about the maximum
length of the output . Output sequences are generally considered complete
once a special “end-of-sentence”(EOS) token has
been generated. Beam size in these models is also
typically kept small, and recent work has shown
Visualizing the lexically constrained decoder’s
complete search graph. Each rectangle represents a beam
containing k hypotheses. Dashed (diagonal) edges indicate
starting or continuing constraints. Horizontal edges represent generating from the model’s distribution. The horizontal
axis covers the timesteps in the output sequence, and the vertical axis covers the constraint tokens (one row for each token
in each constraint). Beams on the top level of the grid contain
hypotheses which cover all constraints.
that the performance of some architectures can actually degrade with larger beam size (Tu et al.,
Grid Beam Search
Our goal is to organize decoding in such a way that
we can constrain the search space to outputs which
contain one or more pre-speciﬁed sub-sequences.
We thus wish to use a model’s distribution both to
“place” lexical constraints correctly, and to generate the parts of the output which are not covered
by the constraints.
Algorithm 1 presents the pseudo-code for lexically constrained decoding, see Figures 1 and 3
for visualizations of the search process. Beams
in the grid are indexed by t and c. The t variable tracks the timestep of the search, while the
c variable indicates how many constraint tokens
are covered by the hypotheses in the current beam.
Note that each step of c covers a single constraint
token. In other words, constraints is an array of
sequences, where individual tokens can be indexed
as constraintsij, i.e. tokenj in constrainti. The
numC parameter in Algorithm 1 represents the total number of tokens in all constraints.
The hypotheses in a beam can be separated
into two types (see lines 9-11 and 15-19 of Algorithm 1):
1. open hypotheses can either generate from the
model’s distribution, or start available constraints,
2. closed hypotheses can only generate the next
Algorithm 1 Pseudo-code for Grid Beam Search, note that t and c indices are 0-based
1: procedure CONSTRAINEDSEARCH(model, input, constraints, maxLen, numC, k)
startHyp ⇐model.getStartHyp(input, constraints)
Grid ⇐initGrid(maxLen, numC, k)
▷initialize beams in grid
Grid = startHyp
for t = 1, t++, t < maxLen do
for c = max(0, (numC + t) −maxLen), c++, c ≤min(t, numC) do
n, s, g = ∅
for each hyp ∈Grid[t −1][c] do
if hyp.isOpen() then
g ⇐g S model.generate(hyp, input, constraints)
▷generate new open hyps
if c > 0 then
for each hyp ∈Grid[t −1][c −1] do
if hyp.isOpen() then
n ⇐n S model.start(hyp, input, constraints) ▷start new constrained hyps
s ⇐s S model.continue(hyp, input, constraints)
▷continue unﬁnished
Grid[t][c] = k-argmax
h∈n S s S g
model.score(h)
▷k-best scoring hypotheses stay on the beam
topLevelHyps ⇐Grid[:][numC]
▷get hyps in top-level beams
finishedHyps ⇐hasEOS(topLevelHyps)
▷ﬁnished hyps have generated the EOS token
h∈finishedHyps
model.score(h)
return bestHyp
29: end procedure
token for in a currently unﬁnished constraint.
At each step of the search the beam at
Grid[t][c] is ﬁlled with candidates which may be
created in three ways:
1. the open hypotheses in the beam to the
left (Grid[t −1][c]) may generate continuations from the model’s distribution
pθ(yi|x, {y0 . . . yi−1}),
2. the open hypotheses in the beam to the left
and below (Grid[t−1][c−1]) may start new
constraints,
3. the closed hypotheses in the beam to the left
and below (Grid[t−1][c−1]) may continue
constraints.
Therefore, the model in Algorithm 1 implements an interface with three functions: generate,
start, and continue, which build new hypotheses
in each of the three ways. Note that the scoring
function of the model does not need to be aware of
the existence of constraints, but it may be, for example via a feature which indicates if a hypothesis
is part of a constraint or not.
The beams at the top level of the grid (beams
where c = numConstraints) contain hypotheses which cover all of the constraints. Once a hypothesis on the top level generates the EOS token,
it can be added to the set of ﬁnished hypotheses.
The highest scoring hypothesis in the set of ﬁnished hypotheses is the best sequence which covers all constraints.1
1Our implementation of GBS is available at https:
//github.com/chrishokamp/constrained_
Multi-token Constraints
By distinguishing between open and closed hypotheses, we can allow for arbitrary multi-token
phrases in the search. Thus, the set of constraints
for a particular output may include both individual tokens and phrases.
Each hypothesis maintains a coverage vector to ensure that constraints
cannot be repeated in a search path – hypotheses
which have already covered constrainti can only
generate, or start constraints that have not yet
been covered.
Note also that discontinuous lexical constraints,
such as phrasal verbs in English or German, are
easy to incorporate into GBS, by adding ﬁlters to
the search, which require that one or more conditions must be met before a constraint can be
used. For example, adding the phrasal verb “ask
⟨someone⟩out” as a constraint would mean using
“ask” as constraint0 and “out” as constraint1,
with two ﬁlters: one requiring that constraint1
cannot be used before constraint0, and another
requiring that there must be at least one generated
token between the constraints.
Subword Units
Both the computation of the score for a hypothesis, and the granularity of the tokens (character,
subword, word, etc...) are left to the underlying
model. Because our decoder can handle arbitrary
constraints, there is a risk that constraints will contain tokens that were never observed in the training
data, and thus are unknown by the model. Especially in domain adaptation scenarios, some userspeciﬁed constraints are very likely to contain unseen tokens. Subword representations provide an
elegant way to circumvent this problem, by breaking unknown or rare tokens into character n-grams
which are part of the model’s vocabulary . In the experiments in Section 4, we use this technique to
ensure that no input tokens are unknown, even if
a constraint contains words which never appeared
in the training data.2
Because the number of beams is multiplied by the
number of constraints, the runtime complexity of
a naive implementation of GBS is O(ktc). Standard time-based beam search is O(kt); therefore,
2If a character that was not observed in training data is
observed at prediction time, it will be unknown. However,
we did not observe this in any of our experiments.
some consideration must be given to the efﬁciency
of this algorithm. Note that the beams in each column c of Figure 3 are independent, meaning that
GBS can be parallelized to allow all beams at each
timestep to be ﬁlled simultaneously. Also, we ﬁnd
that the most time is spent computing the states for
the hypothesis candidates, so by keeping the beam
size small, we can make GBS signiﬁcantly faster.
The models used for our experiments are stateof-the-art Neural Machine Translation (NMT) systems using our own implementation of NMT with
attention over the source sequence .
We used Blocks and Fuel to implement our NMT models (van Merrinboer et al.,
To conduct the experiments in the following section, we trained baseline translation
models for English–German (EN-DE), English–
French (EN-FR), and English–Portuguese (EN-
PT). We created a shared subword representation
for each language pair by extracting a vocabulary
of 80000 symbols from the concatenated source
and target data. See the Appendix for more details on our training data and hyperparameter con-
ﬁguration for each language pair. The beamSize
parameter is set to 10 for all experiments.
Because our experiments use NMT models, we
can now be more explicit about the implementations of the generate, start, and continue
functions for this GBS instantiation.
NMT model at timestep t, generate(hypt−1) ﬁrst
computes a vector of output probabilities ot =
softmax(g(yt−1, si, ci))3 using the state information available from hypt−1. and returns the best
k continuations, i.e. Eq. 4:
gt = k-argmax
The start and continue functions simply index
into the softmax output of the model, selecting
speciﬁc tokens instead of doing a k-argmax over
the entire target language vocabulary. For example, to start constraint ci, we ﬁnd the score of token ci0 , i.e. otci0.
Experiments
Pick-Revise for Interactive Post Editing
Pick-Revise is an interaction cycle for MT Post-
Editing proposed by Cheng et al. . Starting
3we use the notation for the g function from Bahdanau
et al. 
Strict Constraints
27.64 (+9.20)
36.66 (+9.01)
43.92 (+7.26)
36.71 (+8.64)
44.84 (+8.13)
45.48 +(0.63)
23.54 (+8.25)
31.14 (+7.60)
35.89 (+4.75)
Relaxed Constraints
26.43 (+7.98)
34.48 (+8.04)
41.82 (+7.34)
33.8 (+5.72)
40.33 (+6.53)
47.0 (+6.67)
23.22 (+7.80)
33.82 (+10.6)
40.75 (+6.93)
Table 1: Results for four simulated editing cycles using WMT test data. EN-DE uses newstest2013, EN-FR uses newstest2014,
and EN-PT uses the Autodesk corpus discussed in Section 4.2. Improvement in BLEU score over the previous cycle is shown
in parentheses. * indicates use of our test corpus created from Autodesk post-editing data.
with the original translation hypothesis, a (simulated) user ﬁrst picks a part of the hypothesis
which is incorrect, and then provides the correct
translation for that portion of the output. The userprovided correction is then used as a constraint for
the next decoding cycle. The Pick-Revise process
can be repeated as many times as necessary, with
a new constraint being added at each cycle.
We modify the experiments of Cheng et al.
 slightly, and assume that the user only provides sequences of up to three words which are
missing from the hypothesis.4 To simulate user
interaction, at each iteration we chose a phrase
of up to three tokens from the reference translation which does not appear in the current MT hypotheses. In the strict setting, the complete phrase
must be missing from the hypothesis. In the relaxed setting, only the ﬁrst word must be missing.
Table 1 shows results for a simulated editing session with four cycles. When a three-token phrase
cannot be found, we backoff to two-token phrases,
then to single tokens as constraints. If a hypothesis already matches the reference, no constraints
are added. By specifying a new constraint of up to
three words at each cycle, an increase of over 20
BLEU points is achieved in all language pairs.
Domain Adaptation via Terminology
The requirement for use of domain-speciﬁc terminologies is common in real-world applications of
MT . Existing approaches incorporate placeholder tokens into NMT systems,
which requires modifying the pre- and post- processing of the data, and training the system with
4NMT models do not use explicit alignment between
source and target, so we cannot use alignment information
to map target phrases to source phrases
data that contains the same placeholders which occur in the test data . The MT
system also loses any possibility to model the tokens in the terminology, since they are represented
by abstract tokens such as “⟨TERM 1⟩”. An attractive alternative is to simply provide term mappings as constraints, allowing any existing system
to adapt to the terminology used in a new test domain.
For the target domain data, we use the Autodesk
Post-Editing corpus , which is a
dataset collected from actual MT post-editing sessions. The corpus is focused upon software localization, a domain which is likely to be very different from the WMT data used to train our general domain models. We divide the corpus into approximately 100,000 training sentences, and 1000
test segments, and automatically generate a terminology by computing the Pointwise Mutual Information (PMI) between
source and target n-grams in the training set. We
extract all n-grams from length 2-5 as terminology
candidates.
pmi(x; y) = log p(x, y)
npmi(x; y) = pmi(x; y)
Equations 5 and 6 show how we compute the
normalized PMI for a terminology candidate pair.
The PMI score is normalized to the range [−1, +1]
by dividing by the entropy h of the joint probability p(x, y). We then ﬁlter the candidates to
only include pairs whose PMI is ≥0.9, and where
both the source and target phrases occur at least
ﬁve times in the corpus.
When source phrases
that match the terminology are observed in the test
data, the corresponding target phrase is added to
the constraints for that segment. Results are shown
in Table 2.
As a sanity check that improvements in BLEU
are not merely due to the presence of the terms
somewhere in the output, i.e. that the placement
of the terms by GBS is reasonable, we also evaluate the results of randomly inserting terms into
the baseline output, and of prepending terms to the
baseline output.
This simple method of domain adaptation leads
to a signiﬁcant improvement in the BLEU score
without any human intervention.
Surprisingly,
even an automatically created terminology combined with GBS yields performance improvements of approximately +2 BLEU points for En-
De and En-Fr, and a gain of almost 14 points
for En-Pt.
The large improvement for En-Pt is
probably due to the training data for this system being very different from the IT domain
(see Appendix). Given the performance improvements from our automatically extracted terminology, manually created domain terminologies with
good coverage of the test domain are likely to lead
to even greater gains. Using a terminology with
GBS is likely to be beneﬁcial in any setting where
the test domain is signiﬁcantly different from the
domain of the model’s original training data.
25.18 (-0.99)
26.44 (+0.26)
27.99 (+1.82)
31.48 (-0.97)
34.51 (+2.05)
35.05 (+2.59)
18.26 (+2.85)
20.43 (+5.02)
29.15 (+13.73)
Table 2: BLEU Results for EN-DE, EN-FR, and EN-PT terminology experiments using the Autodesk Post-Editing Corpus. ”Random’ indicates inserting terminology constraints
at random positions in the baseline translation. ”Beginning”
indicates prepending constraints to baseline translations.
Subjective analysis of decoder output shows that
phrases added as constraints are not only placed
correctly within the output sequence, but also have
global effects upon translation quality. This is a
desirable effect for user interaction, since it implies that users can bootstrap quality by adding the
most critical constraints (i.e. those that are most
essential to the output), ﬁrst. Table 3 shows several
examples from the experiments in Table 1, where
the addition of lexical constraints was able to
guide our NMT systems away from initially quite
low-scoring hypotheses to outputs which perfectly
match the reference translations.
Related Work
Most related work to date has presented modiﬁcations of SMT systems for speciﬁc usecases which
constrain MT output via auxilliary inputs.
largest body of work considers Interactive Machine Translation (IMT): an MT system searches
for the optimal target-language sufﬁx given a complete source sentence and a desired preﬁx for
the target output . IMT can be viewed as subcase of constrained decoding, where there is only
one constraint which is guaranteed to be placed at
the beginning of the output sequence. Wuebker
et al. introduce preﬁx-decoding, which
modiﬁes the SMT beam search to ﬁrst ensure that
the target preﬁx is covered, and only then continues to build hypotheses for the sufﬁx using beams
organized by coverage of the remaining phrases
in the source segment. Wuebker et al. and
Knowles and Koehn also present a simple
modiﬁcation of NMT models for IMT, enabling
models to predict sufﬁxes for user-supplied pre-
Recently, some attention has also been given to
SMT decoding with multiple lexical constraints.
The Pick-Revise (PRIMT) 
framework for Interactive Post Editing introduces
the concept of edit cycles. Translators specify constraints by editing a part of the MT output that is
incorrect, and then asking the system for a new
hypothesis, which must contain the user-provided
correction.
This process is repeated, maintaining constraints from previous iterations and adding
new ones as needed. Importantly, their approach
relies upon the phrase segmentation provided by
the SMT system.
The decoding algorithm can
He was also an anti- smoking activist and took part in several campaigns .
Original Hypothesis
Es war auch ein Anti- Rauch- Aktiv- ist und nahmen an mehreren Kampagnen teil .
Constraints
Ebenso setzte er sich gegen das Rauchen ein und nahm an mehreren Kampagnen teil .
(1) Ebenso setzte er
Constrained Hypothesis
(2) gegen das Rauchen
Ebenso setzte er sich gegen das Rauchen ein und nahm an mehreren Kampagnen teil .
At that point I was no longer afraid of him and I was able to love him .
Original Hypothesis
Je n’avais plus peur de lui et j’`etais capable de l’aimer .
Constraints
L´a je n’ai plus eu peur de lui et j’ai pu l’aimer .
(1) L´a je n’ai
Constrained Hypothesis
(2) j’ai pu
L´a je n’ai plus eu peur de lui et j’ai pu l’aimer .
Mo- dif- y drain- age features by selecting them individually .
Original Hypothesis
- J´a temos as caracter´ısticas de extracc¸˜ao de idade , com eles individualmente .
Constraints
Modi- ﬁque os recursos de drenagem ao selec- ion- ´a-los individualmente .
(1) drenagem ao selec-
Constrained Hypothesis
(2) Modi- ﬁque os
Modi- ﬁque os recursos de drenagem ao selec- ion- ´a-los individualmente .
(3) recursos
Table 3: Manual analysis of examples from lexically constrained decoding experiments. “-” followed by whitespace indicates
the internal segmentation of the translation model (see Section 3.2)
only make use of constraints that match phrase
boundaries, because constraints are implemented
as “rules” enforcing that source phrases must be
translated as the aligned target phrases that have
been selected as constraints. In contrast, our approach decodes at the token level, and is not dependent upon any explicit structure in the underlying model.
Domingo et al. also consider an interactive scenario where users ﬁrst choose portions of
an MT hypothesis to keep, then query for an updated translation which preserves these portions.
The MT system decodes the source phrases which
are not aligned to the user-selected phrases until the source sentence is fully covered. This approach is similar to the system of Cheng et al., and
uses the “XML input” feature in Moses .
Some recent work considers the inclusion of
soft lexical constraints directly into deep models
for dialog generation, and special cases, such as
recipe generation from a list of ingredients . Such constraintaware models are complementary to our work, and
could be used with GBS decoding without any
change to the underlying models.
To the best of our knowledge, ours is the
ﬁrst work which considers general lexically constrained decoding for any model which outputs
sequences, without relying upon alignments between input and output, and without using a search
organized by coverage of the input.
Conclusion
Lexically constrained decoding is a ﬂexible way
to incorporate arbitrary subsequences into the output of any model that generates output sequences
token-by-token. A wide spectrum of popular text
generation models have this characteristic, and
GBS should be straightforward to use with any
model that already uses beam search.
In translation interfaces where translators can
provide corrections to an existing hypothesis,
these user inputs can be used as constraints, generating a new output each time a user ﬁxes an error.
By simulating this scenario, we have shown that
such a workﬂow can provide a large improvement
in translation quality at each iteration.
By using a domain-speciﬁc terminology to generate target-side constraints, we have shown that
a general domain model can be adapted to a new
domain without any retraining. Surprisingly, this
simple method can lead to signiﬁcant performance
gains, even when the terminology is created automatically.
In future work, we hope to evaluate GBS with
models outside of MT, such as automatic summarization, image captioning or dialog generation. We also hope to introduce new constraintaware models, for example via secondary attention
mechanisms over lexical constraints.
Acknowledgments
This project has received funding from Science
Foundation Ireland in the ADAPT Centre for Digital Content Technology (www.adaptcentre.ie) at
Dublin City University funded under the SFI Research Centres Programme (Grant 13/RC/2106)
co-funded under the European Regional Development Fund and the European Union Horizon 2020
research and innovation programme under grant
agreement 645452 (QT21). We thank the anonymous reviewers, as well as Iacer Calixto, Peyman
Passban, and Henry Elder for helpful feedback on
early versions of this work.