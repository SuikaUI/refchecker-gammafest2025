NiftyNet: a deep-learning platform for medical imaging
Eli Gibsona,b,1, Wenqi Lia,1,∗, Carole Sudreb, Lucas Fidona, Dzhoshkun I.
Shakira, Guotai Wanga, Zach Eaton-Rosenb, Robert Grayc, Tom Doela,
Yipeng Hub, Tom Whyntieb, Parashkev Nachevc, Marc Modatb, Dean C.
Barratta,b, S´ebastien Ourselina, M. Jorge Cardosob,2, Tom Vercauterena,2
aWellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS),
University College London, UK
bCentre for Medical Image Computing (CMIC), Departments of Medical Physics &
Biomedical Engineering and Computer Science, University College London, UK
cInstitute of Neurology, University College London, UK & National Hospital for Neurology
and Neurosurgery, London, UK
Background and Objectives Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based
solutions. Established deep-learning platforms are ﬂexible but do not provide
speciﬁc functionality for medical image analysis and adapting them for this domain of application requires substantial implementation eﬀort. Consequently,
there has been substantial duplication of eﬀort and incompatible infrastructure developed across many research groups.
This work presents the opensource NiftyNet platform for deep learning in medical imaging. The ambition
of NiftyNet is to accelerate and simplify the development of these solutions,
and to provide a common mechanism for disseminating research outputs for the
community to use, adapt and build upon.
Methods The NiftyNet infrastructure provides a modular deep-learning pipeline
for a range of medical imaging applications including segmentation, regression,
image generation and representation learning applications. Components of the
NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage
of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such
as TensorBoard visualization of 2D and 3D images and computational graphs
∗Corresponding author
Email: 
Mailing Address:
Wellcome / EPSRC Centre for Interventional and Surgical Sciences
University College London
Gower Street
London, United Kingdom, WC1E 6BT
1Wenqi Li and Eli Gibson contributed equally to this work.
2M. Jorge Cardoso and Tom Vercauteren contributed equally to this work.
 
October 17, 2017
 
by default.
Results We present three illustrative medical image analysis applications
built using NiftyNet infrastructure: (1) segmentation of multiple abdominal
organs from computed tomography; (2) image regression to predict computed
tomography attenuation maps from brain magnetic resonance images; and (3)
generation of simulated ultrasound images for speciﬁed anatomical poses.
Conclusions The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image
generation and representation learning applications, or extend the platform to
new applications.
medical image analysis, deep learning, convolutional neural
network, segmentation, image regression, generative adversarial network
1. Introduction
Computer-aided analysis of medical images plays a critical role at many
stages of the clinical workﬂow from population screening and diagnosis to treatment delivery and monitoring. This role is poised to grow as analysis methods
become more accurate and cost eﬀective. In recent years, a key driver of such
improvements has been the adoption of deep learning and convolutional neural
networks in many medical image analysis and computer-assisted intervention
Deep learning refers to a deeply nested composition of many simple functions
(principally linear combinations such as convolutions, scalar non-linearities and
moment normalizations) parameterized by variables. The particular composition of functions, called the architecture, deﬁnes a parametric function (typically
with millions of parameters) that can be optimized to minimize an objective, or
‘loss’, function, usually using some form of gradient descent.
Although the ﬁrst use of neural networks for medical image analysis dates
back more than twenty years , their usage has increased by orders
of magnitude in the last ﬁve years. Recent reviews have highlighted that deep learning has been applied to a wide
range of medical image analysis tasks (segmentation, classiﬁcation, detection,
registration, image reconstruction, enhancement, etc.) across a wide range of
anatomical sites (brain, heart, lung, abdomen, breast, prostate, musculature,
etc.). Although each of these applications have their own speciﬁcities, there is
substantial overlap in software pipelines implemented by many research groups.
Deep-learning pipelines for medical image analysis comprise many interconnected components. Many of these are common to all deep-learning pipelines:
• separation of data into training, testing and validation sets;
• randomized sampling during training;
• image data loading and sampling;
• data augmentation;
• a network architecture deﬁned as the composition of many simple functions;
• a fast computational framework for optimization and inference;
• metrics for evaluating performance during training and inference.
In medical image analysis, many of these components have domain speciﬁc
idiosyncrasies, detailed in Section 4.
For example, medical images are typically stored in specialized formats that handle large 3D images with anisotropic
voxels and encode additional spatial information and/or patient information,
requiring diﬀerent data loading pipelines. Processing large volumetric images
has high memory requirements and motivates domain-speciﬁc memory-eﬃcient
networks or custom data sampling strategies.
Images are often acquired in
standard anatomical views and can represent physical properties quantitatively,
motivating domain-speciﬁc data augmentation and model priors. Additionally,
the clinical implications of certain errors may warrant custom evaluation metrics. Independent reimplementation of all of this custom infrastructure results
in substantial duplication of eﬀort, poses a barrier to dissemination of research
tools and inhibits fair comparisons between competing methods.
This work presents the open-source NiftyNet3 platform to 1) facilitate ef-
ﬁcient deep learning research in medical image analysis and computer-assisted
intervention; and 2) reduce duplication of eﬀort. The NiftyNet platform comprises an implementation of the common infrastructure and common networks
used in medical imaging, a database of pre-trained networks for speciﬁc applications and tools to facilitate the adaptation of deep learning research to new
clinical applications with a shallow learning curve.
2. Background
The development of common software infrastructure for medical image analysis and computer-assisted intervention has a long history.
Early eﬀorts included the development of medical imaging ﬁle formats , Analyze 7.5 , DICOM MINC , and NIfTI ).
Toolsets to solve common challenges such as registration , ANTs and elastix ),
segmentation ), and biomechanical modeling ) are available for use as part of image analysis pipelines. Pipelines for speciﬁc research applications such as FSL for functional MRI analysis and Freesurfer for structural neuroimaging have reached widespread use. More
general toolkits oﬀering standardized implementations of algorithms ) and application frameworks , MITK and 3D Slicer ) enable others to build their own pipelines. Common software infrastructure has
supported and accelerated medical image analysis and computer-assisted intervention research across hundreds of research groups. However, despite the wide
3Available at 
availability of general purpose deep learning software tools, deep learning technology has limited support in current software infrastructure for medical image
analysis and computer-assisted intervention.
Software infrastructure for general purpose deep learning is a recent development.
Due to the high computational demands of training deep learning
models and the complexity of eﬃciently using modern hardware resources (general purpose graphics processing units and distributed computing, in particular),
numerous deep learning libraries and platforms have been developed and widely
adopted, including cuDNN (Chetlur et al.), TensorFlow ,
Theano , Caﬀe , Torch , CNTK , and MatConvNet , and TensorLayer 
for TensorFlow and Lasagne for Theano. However, by
avoiding assumptions about the application to remain general, the platforms are
unable to provide speciﬁc functionality for medical image analysis and adapting
them for this domain of application requires substantial implementation eﬀort.
Developed concurrently with the NiftyNet platform, the Deep Learning Toolkit4
aims to support fast prototyping and reproducibility by implementing deep
learning methods and modules for medical image analysis. While still in preliminary development, it appears to focus on deep learning building blocks rather
than analysis pipelines.
NifTK 
and Slicer3D plugin) provide infrastructure for distribution of trained deep learning pipelines. Although this
does not address the substantial infrastructure needed for training deep learning pipelines, integration with existing medical image analysis infrastructure
and modular design makes these platforms promising routes for distributing
deep-learning pipelines.
3. Typical deep learning pipeline
Deep learning adopts the typical machine learning pipeline consisting of three
phases: model selection (picking and ﬁtting a model on training data), model
evaluation (measuring the model performance on testing data), and model distribution (sharing the model for use on a wider population).
Within these
4 
Data partitioning
Model selection
Model fitting
Training data
parameters
Model & hyperparameters
Data sampling
Testing data
Validation
initializations
Trained models for
comparison
Evaluation
Validated model
for distribution
Experimental
Validation data
Optimization
Data augmentation
Model inference
Data ﬂow implemented in typical deep learning projects.
Boxes represent the
software infrastructure to be developed and arrows represent the data ﬂow.
simple phases lies substantial complexity, illustrated in Figure 1. The most obvious complexity is in implementing the network being studied. Deep neural
networks generally use simple functions, but compose them in complex hierarchies; researchers must implement the network being tested, as well as previous
networks (often incompletely speciﬁed) for comparison. To train, evaluate and
distribute these networks, however, requires further infrastructure. Data sets
must be correctly partitioned to avoid biassed evaluations, sometimes considering data correlations (e.g. images acquired at the same hospital may be more
similar to each other than to those from other hospitals). The data must be
sampled, loaded and passed to the network, in diﬀerent ways depending on the
phase of the pipeline. Algorithms for tuning hyper-parameters within a family
of models and optimizing model parameters on the training data are needed.
Logging and visualization are needed to debug and dissect models during and
after training. In applications with limited data, data sets must be augmented
by perturbing the training data in realistic ways to prevent over-ﬁtting.
deep learning, it is common practice to adapt previous network architectures,
trained or untrained, in part or in full for similar or diﬀerent tasks; this requires
a community repository (popularly called a model zoo) storing models and parameters in an adaptable format. Much of this infrastructure is recreated by
each researcher or research group undertaking a deep learning project, and much
of it depends on the application domain being addressed.
4. Design considerations for deep learning in medical imaging
Medical image analysis diﬀers from other domains where deep learning is
applied due to characteristics of the data itself, and the applications in which
they are used.
In this section, we present the domain-speciﬁc requirements
driving the design of NiftyNet.
4.1. Data availability
Acquiring, annotating and distributing medical image data sets have higher
costs than in many computer vision tasks. For many medical imaging modalities, generating an image is costly. Annotating images for many applications
requires high levels of expertise from clinicians with limited time. Additionally, due to privacy concerns, sharing data sets between institutions, let alone
internationally, is logistically and legally challenging.
Although recent tools
such as DeepIGeoS for semi-automated annotation and
GIFT-Cloud for data sharing are beginning to reduce these
barriers, typical data sets remain small. Using smaller data sets increases the
importance of data augmentation, regularization, and cross-validation to prevent over-ﬁtting. The additional cost of data set annotation also places a greater
emphasis on semi- and unsupervised learning.
4.2. Data dimensionality and size
Data dimensionality encountered in medical image analysis and computerassisted intervention typically ranges from 2D to 5D. Many medical images,
including MRI, CT, PET and SPECT, capture volumetric images. Longitudinal
imaging (multiple images taken over time) is typical in interventional settings
as well as clinically useful for measuring organ function (e.g. blood ejection
fraction in cardiac imaging) and disease progression (e.g. cortical thinning in
neurodegenerative diseases).
At the same time, capturing high-resolution data in multiple dimensions is
often necessary to detect small but clinically important anatomy and pathology. The combination of these factors results in large data sizes for each sample, which impact computational and memory costs. Deep learning in medical
imaging uses various strategies to account for this challenge. Many networks
are designed to use partial images: 2D slices sampled along one axis from 3D
images , 3D subvolumes , anisotropic convolution Wang et al. , or combinations of subvolumes along multiple
axes .
Other networks use multi-scale representations allowing deeper and wider networks on lower-resolution representations . A third approach uses dense networks
to reuse feature representations multiple times in the network . Smaller batch sizes can reduce the memory cost, but rely on diﬀerent weight normalization functions such as batch renormalization ,
weight normalization or layer normalization .
4.3. Data formatting
Data sets in medical imaging are typically stored in diﬀerent formats than
in many computer vision tasks.
To support the higher-dimensional medical
image data, specialized formats have been adopted (e.g. DICOM, NIfTI, Analyze). These formats frequently also store metadata that is critical to image
interpretation, including spatial information (anatomical orientation and voxel
anisotropy), patient information (demographics and identiﬁers), and acquisition
information (modality types and scanner parameters).
These medical imaging speciﬁc data formats are typically not supported by existing deep learning
frameworks, requiring custom infrastructure for loading images.
4.4. Data properties
The characteristic properties of medical image content pose opportunities
and challenges. Medical images are obtained under controlled conditions, allowing more predictable data distributions.
In many modalities, images are
calibrated such that spatial relationships and image intensities map directly to
physical quantities and are inherently normalized across subjects. For a given
clinical workﬂow, image content is typically consistent, potentially enabling the
characterization of plausible intensity and spatial variation for data augmentation. However, some clinical applications introduce additional challenges. Because small image features can have large clinical importance, and because some
pathology is very rare but life-threatening, medical image analysis must deal
with large class imbalances, motivating special loss functions . Furthermore, diﬀerent types of
error may have very diﬀerent clinical impacts, motivating specialized loss functions and evaluation metrics (e.g.
spatially weighted segmentation metrics).
Applications in computer-assisted intervention where analysis results are used
in real time ; Garcia-Peraza-Herrera et al. )
have additional constraints on analysis latency.
5. NiftyNet: a platform for deep learning in medical imaging
The NiftyNet platform aims to augment the current deep learning infrastructure to address the ideosyncracies of medical imaging described in Section 4, and
lower the barrier to adopting this technology in medical imaging applications.
NiftyNet is built using the TensorFlow library, which provides the tools for
deﬁning computational pipelines and executing them eﬃciently on hardware
resources, but does not provide any speciﬁc functionality for processing medical images, or high-level interfaces for common medical image analysis tasks.
NiftyNet provides a high-level deep learning pipeline with components optimized
for medical imaging applications (data loading, sampling and augmentation,
networks, loss functions, evaluations, and a model zoo) and speciﬁc interfaces
for medical image segmentation, classiﬁcation, regression, image generation and
representation learning applications.
5.1. Design goals
The design of NiftyNet follows several core principles which support a set of
key requirements:
• support a wide variety of application types in medical image analysis and
computer-assisted intervention;
• enable research in one aspect of the deep learning pipeline without the
need for recreating the other parts;
• be simple to use for common use cases, but ﬂexible enough for complex
use cases;
• support built-in TensorFlow features (parallel processing, visualization)
by default;
• support best practices (data augmentation, data set separation) by default;
• support model distribution and adaptation.
5.2. System overview
The NiftyNet platform comprises several modular components. The NiftyNet
ApplicationDriver deﬁnes the common structure across all applications, and is
responsible for instantiating the data analysis pipeline and distributing the computation across the available computational resources. The NiftyNet Application
classes encapsulate standard analysis pipelines for diﬀerent medical image analysis applications, by connecting four components: a Reader to load data from
ﬁles, a Sampler to generate appropriate samples for processing, a Network to
Application
TensorFlow Graph
Image Data Source
ApplicationDriver
Window Sample
Image Windows
Network Outputs
Loss & Optimizer
Network Gradients
TensorFlow Session
Network Output Data
Network Model Data
Modules Initialisation
Training/Inference
CPU/GPU Device
Speciﬁcation
Network Output
Interpretation
Aggregator
Control Flow
Key Functions
TensorFlow APIs
Augmentation
Figure 2: A brief overview of NiftyNet components.
process the inputs, and an output handler (comprising the Loss and Optimizer
during training and an Aggregator during inference and evaluation).
Sampler includes sub-components for data augmentation.
The Network includes sub-components representing individual network blocks or larger conceptual units. These components are brieﬂy depicted in Figure 2 and detailed in
the following sections.
As a concrete illustration, one instantiation of the SegmentationApplication
could use the following modules. During training, it could use a UniformSampler
to generate small image patches and corresponding labels; a vnet Network would
process batches of images to generate segmentations; a Dice LossFunction
would compute the loss used for backpropagation using the Adam Optimizer.
During inference, it could use a GridSampler to generate a set of non-overlapping
patches to cover the image to segment, the same network to generate corresponding segmentations, and a GridSamplesAggregator to aggregate the patches into
a ﬁnal segmentation.
5.3. Component details: ApplicationDriver class
The NiftyNet ApplicationDriver deﬁnes the common structure for all
NiftyNet pipelines. It is responsible for instantiating the data and Application
objects and distributing the workload across and recombining results from the
computational resources (potentially including multiple CPUs and GPUs). It is
also responsible for handling variable initialization, variable saving and restoring, and logging. Implemented as a template design pattern , the ApplicationDriver delegates application-speciﬁc functionality to
separate Application classes.
The ApplicationDriver can be conﬁgured from the command line or programmatically using a human-readable conﬁguration ﬁle. This ﬁle contains the
data set deﬁnitions and all the settings that deviate from the defaults. When
the ApplicationDriver saves its progress, the full conﬁguration (including default parameters) is also saved so that the analysis pipeline can be recreated to
continue training or carry out inference internally or with a distributed model.
5.4. Component details: Application class
Medical image analysis encompasses a wide range of tasks for diﬀerent parts
of the pre-clinical and clinical workﬂow: segmentation, classiﬁcation, detection,
registration, reconstruction, enhancement, model representation and generation. Diﬀerent applications use diﬀerent types of inputs and outputs, diﬀerent
networks, and diﬀerent evaluation metrics; however, there is common structure
and functionality among these applications supported by NiftyNet. NiftyNet
currently supports
• image segmentation,
• image regression,
• image model representation (via auto-encoder applications), and
• image generation (via auto-encoder and generative adversarial networks
and it is designed in a modular way to support the addition of new application
types, by encapsulating typical application workﬂows in Application classes.
The Application class deﬁnes the required data interface for the Network
and Loss, facilitates the instantiation of appropriate Sampler and output handler objects, connects them as needed for the application, and speciﬁes the
training regimen. For example, the SegmentationApplication speciﬁes that
networks accept images (or patches thereof) and generate corresponding labels,
that losses accept generated and reference segmentations and an optional weight
map, and that the optimizer trains all trainable variables in each iteration. In
contrast, the GANApplication speciﬁes that networks accept a noise source,
samples of real data and an optional conditioning image, losses accept logits
denoting if a sample is real or generated, and the optimizer alternates between
training the discriminator sub-network and the generator sub-network.
5.5. Component details: Networks and Layers
The complex composition of simple functions that comprise a deep learning
architecture is simpliﬁed in typical networks by the repeated reuse of conceptual
blocks. In NiftyNet, these conceptual blocks are represented by encapsulated
Layer classes, or inline using TensorFlow’s scoping system. Composite layers,
and even entire networks, can be constructed as simple compositions of NiftyNet
layers and TensorFlow operations. This supports the reuse of existing networks
by clearly demarcating conceptual blocks of code that can be reused and assigning names to corresponding sets of variables that can be reused in other
networks (detailed in Section 5.10). This also enables automatic support for
visualization of the network graph as a hierarchy at diﬀerent levels of detail
using the TensorBoard visualizer as shown in Figure 3. Following the model used in Sonnet , Layer objects deﬁne
a scope upon instantiation, which can be reused repeatedly to allow complex
weight-sharing without breaking encapsulation.
5.6. Component details: data loading
The Reader class is responsible for loading corresponding image ﬁles from
medical ﬁle formats for a speciﬁed data set, and applying image-wide preprocessing. For simple use cases, NiftyNet can automatically identify corresponding
images in a data set by searching a speciﬁed ﬁle path and matching user-speciﬁed
patterns in ﬁle names, but it also allows explicitly tabulated comma-separated
value ﬁles for more complex data set structures (e.g. cross-validation studies).
Input and output of medical ﬁle formats are already supported in multiple existing Python libraries, although each library supports diﬀerent sets of formats. To
facilitate a wide range of formats, NiftyNet uses nibabel as a
core dependency but can fall back on other libraries if they are installed and a ﬁle format is not supported by nibabel.
A pipeline of image-wide preprocessing functions, described in Section 5.8, is
applied to each image before samples are taken.
Figure 3: TensorBoard visualization of a NiftyNet generative adversarial network. Tensor-
Board interactively shows the composition of conceptual blocks (rounded rectangles) and
their interconnections (grey lines) and color-codes similar blocks. Above, the generator and
discriminator blocks and one of the discriminator’s residual blocks are expanded. Font and
block sizes were edited for readability.
5.7. Component details: Samplers and output handlers
To handle the breadth of applications in medical image analysis and computerassisted intervention, NiftyNet provides ﬂexibility in mapping from an input
data set into packets of data to be processed and from the processed data into
useful outputs. The former is encapsulated in Sampler classes, and the latter is
encapsulated in output handlers. Because the sampling and output handling are
tightly coupled and depend on the action being performed (i.e. training, inference or evaluation), the instantiation of matching Sampler objects and output
handlers is delegated to the Application class.
Sampler objects generate a sequence of packets of corresponding data for
processing. Each packet contains all the data for one independent computation
(e.g. one step of gradient descent during training), including images, labels,
classiﬁcations, noise samples or other data needed for processing. During training, samples are taken randomly from the training data, while during inference
and evaluation the samples are taken systematically to process the whole data
set. To feed these samples to TensorFlow, NiftyNet automatically takes advantage of TensorFlow’s data queue support: data can be loaded and sampled
in multiple CPU threads, combined into mini-batches and consumed by one
or more GPUs. NiftyNet includes Sampler classes for sampling image patches
(uniformly or based on speciﬁed criteria), sampling whole images rescaled to
a ﬁxed size and sampling noise; and it supports composing multiple Sampler
objects for more complex inputs.
Output handlers take diﬀerent forms during training and inference. During training, the output handler takes the network output, computes a loss
and the gradient of the loss with respect to the trainable variables, and uses
an inlinecodeOptimizer to iteratively train the model. During inference, the
output handler generates useful outputs by aggregating one or more network
outputs and performing any necessary postprocessing (e.g. resizing the outputs
to the original image size). NiftyNet currently supports Aggregator objects for
combining image patches, resizing images, and computing evaluation metrics.
5.8. Component details: data normalization and augmentation
Data normalization and augmentation are two approaches to compensating for small training data sets in medical image analysis, wherein the training
data set is too sparse to represent the variability in the distribution of images.
Data normalization reduces the variability in the data set by transforming inputs to have speciﬁed invariant properties, such as ﬁxed intensity histograms
or moments (mean and variance). Data augmentation artiﬁcially increases the
variability of the training data set by introducing random perturbations during training, for example applying random spatial transformations or adding
random image noise. In NiftyNet, data augmentation and normalization are
implemented as Layer classes applied in the Sampler, as plausible data transformations will vary between applications. Some of these layers, such as histogram
normalization, are data dependent; these layers compute parameters over the
data set before training begins.
NiftyNet currently supports mean, variance
and histogram intensity data normalization, and ﬂip, rotation and scaling spatial data augmentation.
5.9. Component details: data evaluation
Summarizing and comparing the performance of image analysis pipelines
typically rely on standardized descriptive metrics and error metrics as surrogates for performance.
Because individual metrics are sensitive to diﬀerent
aspects of performance, multiple metrics are reported together. Reference implementations of these metrics reduce the burden of implementation and prevent
implementation inconsistencies. NiftyNet currently supports the calculation of
descriptive and error metrics for segmentation. Descriptive statistics include
spatial metrics (e.g. volume, surface/volume ratio, compactness) and intensity
metrics (e.g. mean, quartiles, skewness of intensity). Error metrics, computed
with respect to a reference segmentation, include overlap metrics (e.g. Dice and
Jaccard scores; voxel-wise sensitivity, speciﬁcity and accuracy), boundary distances (e.g. mean absolute distance and Hausdorﬀdistances) and region-wise
errors (e.g. detection rate; region-wise sensitivity, speciﬁcity and accuracy).
5.10. Component details: model zoo for network reusability
To support the reuse of network architectures and trained models, many deep
learning platforms host a database of existing trained and untrained networks
in a standardized format, called a model zoo. Trained networks can be used
directly (as part of a workﬂow or for performance comparisons), ﬁne-tuned
for diﬀerent data distributions (e.g. a diﬀerent hospital’s images), or used to
initialize networks for other applications (i.e.
transfer learning).
networks or conceptual blocks can be used within new networks.
provides several mechanisms to support the distribution and reuse of networks
and conceptual blocks.
Trained NiftyNet networks can be restored directly using conﬁguration options.
Trained networks developed outside of NiftyNet can be adapted to
NiftyNet by encapsulating the network within a Network class derived from
TrainableLayer. Externally trained weights can be loaded within NiftyNet using a restore initializer, adapted from Sonnet , for
the complete network or individual conceptual blocks. restore initializer
initializes the network weights with those stored in a speciﬁed checkpoint, and
supports variable scope renaming for checkpoints with incompatible scope
names. Smaller conceptual blocks, encapsulated in Layer classes, can be reused
in the same way. Trained networks incorporating previous networks are saved
in a self-contained form to minimize dependencies.
The NiftyNet model zoo contains both untrained networks and vnet for segmentation), as well as trained
networks for some tasks for multiorgan abdominal CT segmentation, wnet for brain tumor
segmentation and simulator gan for generating ultrasound
images). Model zoo entries should follow a standard format comprising:
• Python source code deﬁning any components not included in NiftyNet
(e.g. external Network classes, Loss functions);
• an example conﬁguration ﬁle deﬁning the default settings and the data
• documentation describing the network and assumptions on the input data
(e.g. dimensionality, shape constraints, intensity statistic assumptions).
For trained networks, it should also include:
• a Tensorﬂow checkpoint containing the trained weights;
• documentation describing the data used to train the network and on which
the trained network is expected to perform adequately.
5.11. Platform processes
In addition to the implementation of common functionality, NiftyNet development has adopted good software development processes to support the
ease-of-use, robustness and longevity of the platform as well as the creation of a
vibrant community. The platform supports easy installation via the pip installation tool5 (i.e. pip install niftynet) and provides analysis pipelines that
can be run as part of the command line interface. Examples demonstrating the
platform in multiple use cases are included to reduce the learning curve. The
NiftyNet repository uses continuous integration incorporating system and unit
tests for regression testing. NiftyNet releases will follow the semantic versioning
2.0 standard to ensure clear communication regarding
backwards compatibility.
6. Results: illustrative applications
6.1. Abdominal organ segmentation
Segmentations of anatomy and pathology on medical images can support
image-guided interventional workﬂows by enabling the visualization of hidden
anatomy and pathology during surgical navigation. Here we present an example,
based on a simpliﬁed version of , that illustrates the use of
NiftyNet to train a Dense V-network segmentation network to segment organs
on abdominal CT that are important to pancreatobiliary interventions: the
gastrointestinal tract (esophagus, stomach and duodenum), the pancreas, and
anatomical landmark organs (liver, left kidney, spleen and stomach).
The data used to train the network comprised 90 abdominal CT with manual
segmentations from two publicly available data sets , with additional manual segmentations performed at our centre.
The network was trained and evaluated in a 9-fold cross-validation, using the
network implementation available in NiftyNet. Brieﬂy, the network, available as
dense vnet in NiftyNet, uses a V-shaped structure (with downsampling, upsampling and skip connections) where each downsampling stage is a dense feature
5 
Reference standard
NiftyNet segmentation
Figure 4: Reference standard (left) and NiftyNet (right) multi-organ abdominal CT segmentation for the subject with Dice scores closest to the median. Each segmentation is shown
with a surface rendering view from the posterior direction and with organ labels overlaid on
a transverse CT slice.
stack (i.e. a sequence of convolution blocks where the inputs are concatenated
features from all preceding convolution blocks), upsampling is bilinear upsampling and skip connections are convolutions. The loss is a modiﬁed Dice loss
(with additional hinge losses to mitigate class imbalance) implemented external
to NiftyNet and included via a reference in the conﬁguration ﬁle. The network
Table 1: Median segmentation metrics for 8 organs aggregated over the 9-fold cross-validation.
95th Percentile
Gallbladder
was trained for 3000 iterations on whole images (using the ResizeSampler) with
random aﬃne spatial augmentations.
Segmentation metrics, computed using NiftyNet’s evaluation action, and
aggregated over all folds, are given in Table 1. The segmentation with Dice
scores closest to the median is shown in Figure 4.
6.2. Image regression
Image regression, more speciﬁcally, the ability to predict the content of an
image given a diﬀerent imaging modality of the same object, is of paramount
importance in real-world clinical workﬂows. Image reconstruction and quantitative image analysis algorithms commonly require a minimal set of inputs
that are often not be available for every patient due to the presence of imaging
artefacts, limitations in patient workﬂow (e.g. long acquisition time), image
harmonization, or due to ionising radiation exposure minimization.
An example application of image regression is the process of generating
synthetic CT images from MRI data to enable the attenuation correction of
PET-MRI images . This regression problem has been historically solved with patch-based or multi-atlas propagation methods, a class
of models that are very robust but computationally complex and dependent on
image registration. The same process can now be solved using the deep learning
architectures similar to the ones used in image segmentation.
As a demonstration of this application, a neural network was trained and
evaluated in a 5-fold cross-validation setup using the net regress application
in NiftyNet. Brieﬂy, the network, available as highresnet in NiftyNet, uses a
stack of residual dilated convolutions with increasingly large dilation factors . The root mean square error was used as the loss function and
implemented as part of NiftyNet as rmse. The network was trained for 15000
iterations on patches of size 80×80×80, and using the iSampler for patch selection with random aﬃne spatial augmentations.
Regression metrics, computed using NiftyNet’s ‘evaluation‘ action, and aggregated over all folds, are given in Table 2.
The 25th and 75th percentile
example result with regards to MAE is shown in Figure 5.
6.3. Ultrasound simulation using generative adversarial networks
Generating plausible images with speciﬁed image content can support training for radiological or image-guided interventional tasks.
Conditional GANs
Table 2: The Mean Absolute Error (MAE) and the Mean Error (ME) between the ground
truth and the pseudoCT in Hounsﬁeld units, comparing the NiftyNet method with pCT
 and the UTE-based method of the Siemens Biograph mMR.
Ground-truth CT
Synthetic CT
Figure 5: The input T1 MRI image (left), the ground truth CT (centre) and the NiftyNet
regression output (right).
have shown promise for generating plausible photographic images . Recent work on spatially-conditioned GANs 
suggests that conditional GANs could enable software-based simulation in place
of costly physical ultrasound phantoms used for training. Here we present an example illustrating a pre-trained ultrasound simulation network that was ported
to NiftyNet for inclusion in the NiftyNet model zoo.
The network was originally trained outside of the NiftyNet platform as described in . Brieﬂy, a conditional GAN network was trained to
generate ultrasound images of speciﬁed views of a fetal phantom using 26,000
frames of optically tracked ultrasound. An image can be sampled from the generative model based on a conditioning image (denoting the pixel coordinates in
3D space) and a model parameter (sampled from a 100-D Gaussian distribution).
The network was ported to NiftyNet for inclusion in the model zoo. The
network weights were transferred to the NiftyNet network using NiftyNet’s
restore initializer, adapted from Sonnet , which enables trained variables to be loaded from networks with diﬀerent architectures
or naming schemes.
The network was evaluated multiple times using the linear interpolation
inference in NiftyNet, wherein samples are taken from the generative model
based on one conditioning image and a sequence of model parameters evenly
interpolated between two random samples. Two illustrative results are shown
in Figure 6. The ﬁrst shows the same anatomy, but a smooth transition between diﬀerent levels of ultrasound shadowing artifacts. The second shows a
sharp transition in the interpolation, suggesting the presence of mode collapse,
Figure 6: Interpolated images from the generative model space based on linearly interpolated
model parameters.
The top row shows a smooth variation between diﬀerent amounts of
ultrasound shadow artefacts. The bottom row shows a sharp transition suggesting the presence
of mode collapse in the generative model.
a common issue in GANs .
7. Discussion
7.1. Lessons learned
NiftyNet development was guided by several core principles that impacted
the implementation. Maximizing simplicity for simple use cases motivated many
implementation choices. We envisioned three categories of users: novice users
who are comfortable with running applications, but not with writing new Python
code, intermediate users who are comfortable with writing some code, but not
with modifying the NiftyNet libraries, and advanced users who are comfortable
with modifying the libraries. Support for pip installation simpliﬁes NiftyNet for
novice and intermediate users. In this context, enabling experimental manipulation of individual pipeline components for intermediate users, and downloadable
model zoo entries with modiﬁed components for novice users required a modular
approach with plugin support for externally deﬁned components. Accordingly,
plugins for networks, loss functions and even application logic can be speci-
ﬁed by Python import paths directly in conﬁguration ﬁles without modifying
the NiftyNet library. Intermediate users can customize pipeline components by
writing classes or functions in Python, and can embed them into model zoo
entries for distribution.
Although initially motivated by simplifying variable sharing within networks,
NiftyNet’s named conceptual blocks also simpliﬁed the adaptation of weights
from pre-trained models and the TensorBoard-based hierarchical visualization
of the computation graphs.
The scope of each conceptual blocks maps to a
meaningful subgraph of the computation graph and all associated variables,
meaning that all weights for a conceptual block can be loaded into a new model
with a single scope reference. Furthermore, because these conceptual blocks are
constructed hierarchically through the composition of Layer objects and scopes,
they naturally encode a hierarchical structure for TensorBoard visualization
Supporting machine learning for a wide variety of application types motivated the separation of the ApplicationDriver logic that is common to all
applications from the Application logic that varies between applications. This
facilitated the rapid development of new application types. The early inclusion
of both image segmentation/regression (mapping from images to images) and
image generation (mapping from parameters to images) motivated a ﬂexible
speciﬁcation for the number, type and semantic meaning of inputs and outputs,
encapsulated in the Sampler and Aggregator components.
7.2. Platform availability
The NiftyNet platform is available from The source
code can be accessed from the Git repository6 or installed as a Python library using pip install niftynet. NiftyNet is licensed under an open-source Apache
2.0 license7. The NiftyNet Consortium welcomes contributions to the platform
and seeks inclusion of new community members to the consortium.
7.3. Future direction
The active NiftyNet development roadmap is focused on three key areas:
new application types, a larger model zoo and more advanced experimental design. NiftyNet currently supports image segmentation, regression, generation
and representation learning applications. Future applications under development include image classiﬁcation, registration, and enhancement (e.g. superresolution) as well as pathology detection.
The current NiftyNet model zoo
contains a small number of models as proof of concept; expanding the model
zoo to include state-of-the-art models for common tasks and public challenges
brain tumor segmentation (BRaTS) ); and models trained on large data sets for transfer learning will be critical to accelerating research with NiftyNet. Finally, NiftyNet currently supports
a simpliﬁed machine learning pipeline that trains a single network, but relies on
users for data partitioning and model selection (e.g. hyper-parameter tuning).
Infrastructure to facilitate more complex experiments, such as built-in support
for cross-validation and standardized hyper-parameter tuning will, in the future,
reduce the implementation burden on users.
8. Summary of contributions and conclusions
This work presents the open-source NiftyNet platform for deep learning in
medical imaging. Our modular implementation of the typical medical imaging
machine learning pipeline allows researchers to focus implementation eﬀort on
their speciﬁc innovations, while leveraging the work of others for the remaining pipeline. The NiftyNet platform provides implementations for data loading,
data augmentation, network architectures, loss functions and evaluation metrics
that are tailored for the idiosyncracies of medical image analysis and computerassisted intervention. This infrastructure enables researchers to rapidly develop
6 
7 
deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.
Conﬂict of interest
Acknowledgements
The authors would like to acknowledge all of the contributors to the NiftyNet
platform. This work was supported by the Wellcome/EPSRC [203145Z/16/Z,
WT101957, NS/A000027/1]; Wellcome [106882/Z/15/Z, WT103709]; the Department of Health and Wellcome Trust [HICF-T4-275, WT 97914]; EPSRC
[EP/M020533/1, EP/K503745/1, EP/L016478/1]; the National Institute for
Health Research University College London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL High Impact Initiative); Cancer Research UK
(CRUK) [C28070/A19985]; the Royal Society [RG160569]; a UCL Overseas Research Scholarship, and a UCL Graduate Research Scholarship. The authors
would like to acknowledge that the work presented here made use of Emerald,
a GPU-accelerated High Performance Computer, made available by the Science & Engineering South Consortium operated in partnership with the STFC
Rutherford-Appleton Laboratory; and hardware donated by NVIDIA.