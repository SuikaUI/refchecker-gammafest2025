Boosting Self-Supervised Learning via Knowledge Transfer
Mehdi Noroozi1
Ananth Vinjimoor2
Paolo Favaro1
Hamed Pirsiavash2
University of Bern1
University of Maryland, Baltimore County2
{noroozi,favaro}@inf.unibe.ch
{kan8, }
In self-supervised learning, one trains a model to solve a
so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer
this model to a target domain and task. Currently, the most
effective transfer strategy is ﬁne-tuning, which restricts one
to use the same model or parts thereof for both pretext and
target tasks. In this paper, we present a novel framework
for self-supervised learning that overcomes limitations in
designing and comparing different tasks, models, and data
domains. In particular, our framework decouples the structure of the self-supervised model from the ﬁnal task-speciﬁc
ﬁne-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted
features; 2) show that deeper neural network models can
learn better representations from the same pretext task; 3)
transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves
state-of-the-art performance on the common benchmarks in
PASCAL VOC 2007, ILSVRC12 and Places by a signiﬁcant
margin. Our learned features shrink the mAP gap between
models trained via self-supervised learning and supervised
learning from 5.9% to 2.6% in object detection on PASCAL
1. Introduction
Self-supervised learning (SSL) has gained considerable
popularity since it has been introduced in computer vision
 . Much of the popularity stems from the fact
that SSL methods learn features without using manual annotation by introducing a so-called pretext task. Feature
representations learned through SSL in computer vision are
often transferred to a target data domain and a target task,
such as object classiﬁcation, detection and semantic segmentation in PASCAL VOC. These learned features implicitly deﬁne a metric on the data, i.e., which data samples are
similar and which ones are dissimilar. Thus, the main objective of a pretext task is to learn a metric that makes images
Figure 1: Most current self-supervised learning approaches
use the same architecture both in pre-training and ﬁnetuning. We develop a knowledge transfer method to decouple these two architectures. This allows us to use a deeper
model in pre-training.
of the same object category similar and images of different categories dissimilar. A natural question is then: How
do we design such a task? Some SSL approaches deﬁne
pretext tasks through explicit desirable invariances of the
metric or such that they implicitly require a
good object representation .
Even if we had a clear strategy to relate pretext tasks
to a target task, comparing and understanding which one is
better presents challenges. Most of the recent approaches
transfer their learned features to a common supervised target task. This step, however, is complicated by the need to
use the same model (e.g., AlexNet ) to solve both tasks.
This clearly poses a major limitation on the design choices.
For example, some pretext tasks may exploit several data
domains (e.g., sound, text, videos), or may exploit different datasets sizes and formats, or might require very deep
neural networks to be solved.
There is thus the need to build better representations by
exploring and comparing difﬁcult, but learnable , pre-
 
text tasks and arbitrarily deep architectures. Towards this
goal, one could use methods such as distillation to
transfer the representation in a strong model (the one trained
with the pretext task) to a smaller one (the one employed on
the target task).
In this paper, we propose to transfer knowledge by reducing a learned representation to pseudo-labels on an unlabeled dataset.
First, we compute the features learned
through a pretext task which might use a complex model on
a dataset of unlabeled images. Second, we cluster the features (e.g., using k-means) and use the cluster ID as pseudolabels for unlabeled images. Third, we learn our ﬁnal representation by training a smaller deep network (e.g., AlexNet)
to classify the images based on the pseudo-labels.
By reducing the feature representation of a model to data/pseudolabel pairs, it seems that we are discarding a lot of information. However, we know that given good labels that group
semantically similar images, standard supervised classiﬁcation methods work well. Also, we believe that in a good
representation space, semantically related images should be
close to each other. Hence, pseudo-labels obtained through
a simple clustering algorithm should be a robust estimate of
the learned representation.
Once we have obtained pseudo-labels on some dataset,
we can transfer knowledge by simply training a model to
predict those pseudo-labels. The simplicity and ﬂexibility
of our technique allows us to:
1. Transfer knowledge from any model (different network architecture and training settings) to any other
ﬁnal task model; we can thus capture representations
of complex architectures as well as complicated pretext tasks (see Figure 1).
2. Compare different models (e.g., learned features via
neural network architectures versus handcrafted features), built on different data domains with different
pretext tasks using a common reference model, data,
and task (e.g., AlexNet, PASCAL VOC, and object detection)
Based on this analysis, to show the effectiveness of our algorithm, we design a novel self-supervised task that is more
complicated and uses a deeper model. We start from an
existing pretext task, the jigsaw problem , and make it
more challenging by adding occlusions. We refer to this
task as the Jigsaw++ problem. Furthermore, we boost its
performance by training it on VGG16 and then transferring to AlexNet via our proposed transfer method.
The resulting model achieves state-of-the-art-performance
on several benchmarks shrinking the gap with supervised
learning signiﬁcantly. Particularly, on object detection with
Fast R-CNN on PASCAL VOC 2007, Jigsaw++ achieves
56.5% mAP, while supervised pre-training on ImageNet
achieves 59.1% mAP. Note that the ﬁnal model in both
cases uses the same AlexNet architecture. We believe our
knowledge transfer method can potentially boost the performance of shallow representations with the help of more
complicated pretext tasks and architectures.
2. Prior Work
Self-supervised learning. As mentioned in the introduction, SSL consists of learning features using a pretext task.
Some methods deﬁne as task the reconstruction of data at
the pixel level from partial observations and are thus related
to denoising autoencoders . Notable examples are the
colorization problem , where the task is to reconstruct a color image given its gray scale version. Another
example is image inpainting , where the task is to predict a region of the image given the surrounding. Another
category of methods uses temporal information in videos.
Wang and Gupta learn a similarity metric using the
fact that tracked patches in a video should be semantically
related. Another type of pretext task, is the reconstruction
of more compact signals extracted from the data. For example, Misra et al. and Brattoli et al. train a model
to discover the correct order of video frames. Doersch et
al. train a model that predicts the spatial relation between image patches of the image. Noroozi and Favaro 
propose to solve jigsaw puzzles as a pretext task. Pathak
et al. obtain a supervisory signal by segmenting an
image into foreground and background through the optical
ﬂow between neighboring frames of a video. Then, they
train a model to predict this segmentation from a single image. Other methods also use external signals that may come
freely with visual data. The key idea is to relate images to
information in other data domains like ego-motion 
or sound . Finally, recent work has also exploited
the link between relative transformations of images to de-
ﬁne relative transformations of features.
Currently, the above pretext tasks have been assessed
through transfer learning and benchmarked on common
neural network architectures (e.g., AlexNet) and datasets
(e.g., PASCAL). However, so far it is unclear how to design
or how to further improve the performance of SSL methods.
One natural direction is the combination of multiple tasks
 . However, this strategy does not seem to scale well
as it becomes quickly demanding in terms of computational
resources. Moreover, a possible impediment to progress is
the requirement of using the same model both for training
on the pretext task and to transfer to another task/domain.
In fact, as we show in our experiments, one can learn better representations from challenging tasks by using deep
models, than by using shallower ones. Through our knowledge transfer method we map the representation in the deep
model to a reference model (e.g., AlexNet) and show that
it is better than the representation learned directly with the
reference model. This allows to improve SSL methods by
exploring: 1) designs of architectures that may be more suitable for learning a speciﬁc pretext task; 2) data formats and
types different from the target domain; 3) more challenging pretext tasks. To illustrate these advantages, we make
the method of Noroozi and Favaro more challenging
by incorporating occlusions in the tiles.
Knowledge distillation. Since we transfer knowledge from
a model trained on a pretext task to a target model, our work
is related to model distillation. perform knowledge
distillation by training a target model that mimics the output
probability distribution of the source model. extend
that method to regressing neuron activations, an approach
that is more suitable to our case. Our approach is fundamentally different. We are only interested in preserving the
essential metric of the learned representation (the cluster associations), rather than regressing the exact activations. A
clustering technique used in Dosovitskiy et al. is very
related to our method. They also use clustering to reduce the
classiﬁcation ambiguity in their task, when too many surrogate classes are used. However, they only train and re-train
the same network and do not exploit it for knowledge transfer. Other related work uses clustering in the feature space
of a supervised task to extract labels from unlabeled data for
novel categories or building hash functions . Neither of these works uses clustering to transfer knowledge
from a deep network to a shallow one. Our HOG experiment is related to which shows the initial layers of VGG
perform similarly to hand-crafted SIFT features.
3. Transferring Knowledge
The common practice in SSL is to learn a rich representation by training a model on a SSL pretext task with a
large scale unlabeled dataset and then ﬁne-tune it for a ﬁnal supervised task (e.g., PASCAL object detection) by using a limited amount of labeled training data. This framework has an inherent limitation: The ﬁnal task model and
the SSL model must use the same architecture. This limitation becomes more important as the community moves
on to more sophisticated SSL pretext tasks with larger scale
datasets that need more complicated deeper model architectures. Since we do not want to change the architecture of
the ﬁnal supervised task, we need to develop a novel way
of transferring the learned knowledge from the SSL task to
the ﬁnal supervised model. Moreover, when trained on the
pretext task, the model learns some extra knowledge that
should not be transferred to the ﬁnal task. For instance, in
standard ﬁne-tuning, we usually copy the weights only up
to some intermediate convolutional layers and ignore the ﬁnal layers since they are very speciﬁc to the pretext task and
are not useful for general visual recognition.
In this section, we propose an algorithm to transfer the
part of knowledge learned in SSL that is useful for visual
pre-trained
(no labels)
clustering
cluster centers
pre-trained
(no labels)
clustering
pseudo-labels
assignment
pseudo-labels
classiﬁcation
dataset (no labels)
(from scratch)
Figure 2: Knowledge transfer pipeline. We break down
the four steps of our proposed method for knowledge transfer: (a) an arbitrary model is pre-trained on an SSL pretext
task; (b) the features extracted from this model are clustered
and cluster centers are extracted; (c) pseudo-labels are de-
ﬁned for each image in the dataset by ﬁnding the closest
cluster center; (d) training of the target model on the classi-
ﬁcation of the pseudo-labels.
recognition to the target task. Our idea is based on the intuition that in the space of a good visual representation, semantically similar data points should be close to each other.
The common practice to evaluate this is to search for nearest neighbors and make sure that all retrieved results are semantically related to the query image. This means a simple
clustering algorithm based on the Euclidean distance should
group semantically similar images in the same cluster. Our
idea is to perform this clustering in the feature space and to
obtain the cluster assignments of each image in the dataset
as pseudo-labels. We then train a classiﬁer network with the
target task architecture on the pseudo-labels to learn a novel
representation. We illustrate our pipeline in Figure 2 and
describe it here below.
(a) Self-Supervised Learning Pre-Training. Suppose that
we are given a pretext task, a model and a dataset. Our ﬁrst
step in SSL is to train our model on the pretext task with the
given dataset (see Figure 2 (a)). Typically, the models of
choice are convolutional neural networks, and one considers as feature the output of some intermediate layer (shown
as a grey rectangle in Figure 2 (a)).
(b) Clustering. Our next step is to compute feature vectors for all the unlabeled images in our dataset. Then, we
use the k-means algorithm with the Euclidean distance to
cluster the features (see Figure 2 (b)). Ideally, when performing this clustering on ImageNet images, we want the
cluster centers to be aligned with object categories. In the
experiments, we typically use 2,000 clusters.
(c) Extracting Pseudo-Labels. The cluster centers computed in the previous section can be considered as virtual
categories. Indeed, we can assign feature vectors to the
closest cluster center to determine a pseudo-label associated to the chosen cluster. This operation is illustrated in
Figure 2 (c). Notice that the dataset used in this operation
might be different from that used in the clustering step or in
the SSL pre-training.
(d) Cluster Classiﬁcation. Finally, we train a simple classiﬁer using the architecture of the target task so that, given
an input image (from the dataset used to extract the pseudolabels), predicts the corresponding pseudo-label (see Figure 2 (d)). This classiﬁer learns a new representation in
the target architecture that maps images that were originally
close to each other in the pre-trained feature space to close
4. The Jigsaw++ Pretext Task
Recent work has shown that deeper architectures can help in SSL with PASCAL recognition tasks (e.g.,
ResNet). However, those methods use the same deep architecture for both SSL and ﬁne-tuning. Hence, they are
not comparable with previous methods that use a simpler
AlexNet architecture in ﬁne-tuning. We are interested in
knowing how far one can improve the SSL pre-training of
AlexNet for PASCAL tasks. Since in our framework the
SSL task is not restricted to use the same architecture as in
the ﬁnal supervised task, we can increase the difﬁculty of
the SSL task along with the capacity of the architecture and
still use AlexNet at the ﬁne-tuning stage.
Towards this goal, we build on the method of Okanohara
et al. to learn representations in the text domain. They
Figure 3: The Jigsaw++ task. (a) the main image. (b) a
random image. (c) a puzzle from the original formulation
of , where all tiles come from the same image. (d) a
puzzle in the Jigsaw++ task, where at most 2 tiles can come
from a random image.
replace a word at random in a sentence and train a model
to distinguish the original sentence from the corrupt one.
We combine this idea with the jigsaw task by replacing
tiles in the image puzzle with a random tile from other images. We call this the Jigsaw++ task. The original pretext
task is to ﬁnd a reordering of tiles from a 3 × 3 grid of
a square region cropped from an image. In Jigsaw++, we
replace a random number of tiles in the grid (up to 2) with
(occluding) tiles from another random image (see Figure 3).
The number of occluding tiles (0, 1 or 2 in our experiments)
as well as their location are randomly selected. The occluding tiles make the task remarkably more complex. First,
the model needs to detect the occluding tiles and second,
it needs to solve the jigsaw problem by using only the remaining patches. To make sure we are not adding ambiguities to the task, we remove similar permutations so that
the minimum Hamming distance between any two permutations is at least 3. In this way, there is a unique solution to
the jigsaw task for any number of occlusions in our training
setting. Our ﬁnal training permutation set includes 701 permutations, in which the average and minimum Hamming
distance is .86 and 3 respectively. In addition to applying
the mean and std normalization independently at each image tile, we train the network 70% of the time on gray scale
images. In this way, we prevent the network from using
low level statistics to detect occlusions and solve the jig-
Table 1: Impact of the number of cluster centers
mAP on voc-classiﬁcation
Table 2: Impact of the data domain used in clustering and
pseudo-labels: We perform experiments where the training of clustering and extracting pseudo-labels (inference of
clustering) are done on different datasets. We see just a little reduction in VOC2007 classiﬁcation which means the
clustering is not relying on the ImageNet bias.
clustering on
pseudo-labels on
mAP on voc-classiﬁcation
saw task. We train the Jigsaw++ task on both VGG16 and
AlexNet architectures. By having a larger capacity with
VGG16, the network is better equipped to handle the increased complexity of the Jigsaw++ task and is capable of
extracting better representations from the data. Following
our pipeline in Figure 2, we train our models with this new
SSL task, transfer the knowledge by: 1) clustering the features, 2) assigning pseudo-labels, and 3) training AlexNet to
classify the pseudo-labels. We execute the whole pipeline
by training VGG16 and AlexNet on the Jigsaw++ task. Our
experiments show that when we train VGG16 with the Jigsaw++ task, there is a signiﬁcantly better performance in
ﬁne-tuning. This conﬁrms that training on a deeper network
leads to a better representation and corresponding pseudolabels.
5. Experiments
We extensively evaluate the knowledge transfer method
and the Jigsaw++ task on several transfer learning benchmarks including: ﬁne-tuning on PASCAL VOC, nonlinear classiﬁcation on ImageNet, and linear classiﬁcation
on Places and ImageNet. We also perform ablation studies to show the effect of the number of clusters and the
datasets used for clustering and pseudo-label assignment
in the transferred knowledge. Our experiments show that
pre-training on the Jigsaw++ task yields features that outperform current self-supervised learning methods. In all
the evaluations, the weights of the convolutional layers of
the target AlexNet model are copied from the corresponding layers of the AlexNet model trained on the pretext task,
and the fully connected layers are randomly initialized. We
evaluate our knowledge transfer method in several cases
while the cluster classiﬁcation network is always AlexNet:
1) Jigsaw++ trained with VGG16:
This procedure
achieves the best performance on all the benchmarks.
2) Jigsaw++ trained with AlexNet: Our experiments show
that the performance does not drop in this case. This implies
that a pretext task can be reduced to a classiﬁcation task, if
it has learned a proper similarity metric.
3) The method of Doersch et al. trained on AlexNet:
Due to the difﬁculty of their task, they use batch normalization during training. This makes the ﬁne-tuning challenging, because of the difference in the settings of the target framework. To address this issue, Kr¨ahenb¨uhl et al. 
rescale the weights and thus boost the performance in ﬁnetuning. Our experiments show that our pipeline is doing
signiﬁcantly better than in this case.
4) HOG: We cluster the HOG features to obtain the pseudolabels. Surprisingly, HOG pseudo-labels yield a high performance in ﬁne-tuning on classiﬁcation in Pascal VOC.
5) Pseudo-labels of a random network: Zhang et al. 
showed that AlexNet can be trained to predict random labels.
We perform an experiment in the same spirit and
obtain random pseudo-labels by clustering the features of
a randomly initialized network. To make the cluster classiﬁcation network converge in this case, we decreased the
initial learning rate to 0.001. By using this settings, the network is able to predict random pseudo-labels on the training set with a high accuracy. However, its prediction accuracy on the validation set is close to random chance as expected. We found that the classiﬁer network trained on random pseudo-labels yields the same performance on transfer
learning on PASCAL VOC as random initialization.
6) Knowledge distillation: The standard knowledge distillation is not directly applicable in many SSL tasks
where the loss function does not involve any probability distribution.
Similar to , we train a student network to regress conv4-5 layers of the teacher network (both
AlexNet). The student gets 58.5% on VOC classiﬁcation
while the teacher gets 69.8%.
We compare our method
to that minimizes the original loss function as well
in distillation. We use the Jigsaw++ network trained on
VGG as the teacher network and minimize eq. (5) of 
on AlexNet (the student), where L has been replaced by the
Jigsaw++ loss. As it is shown in Table 3, this method does
not boost sensibly the performance on Pascal-VOC dataset.
Implementation Details. We extract the features for all
the methods from conv4 layer and max-pool them to
5 × 5 × 384 in the case of AlexNet and 4 × 4 × 512 in
the case of VGG16. We implement the standard k-means
algorithm on a GPU with k = 2K. It takes around 4 hours
to cluster the 1.3M images of ImageNet on a single Titan
X. We use the standard AlexNet and ImageNet classiﬁcation settings to train the pseudo-label classiﬁer network.
5.1. Ablation Studies
All the ablation studies are carried out with AlexNet
pre-trained on the Jigsaw++ task with ImageNet as dataset
(trainset without the labels).
The pseudo-labels are also
Table 3: PASCAL VOC ﬁne-tuning. Evaluation of SSL
methods on classiﬁcation, detection and semantic segmentation. All rows use the AlexNet architecture at ﬁne-tuning.
“CC+” stands for “cluster classiﬁcation”, our knowledge
transfer method. “vgg-” means the VGG16 architecture is
used before the “CC+” step. In “CC+vgg-Jigsaw++”, we
train Jigsaw++ using VGG16, cluster, and train AlexNet
to predict the clusters.
We also transfer using in
“ +vgg-Jigsaw++”. In “CC+HOG”, we cluster bag of
words of HOG and predict them with AlexNet; this is not
fully unsupervised since HOG is hand-crafted.
Surprisingly, it outperforms many SSL algorithms on classiﬁcation.
Supervised 
CC+HOG 
ego-motion 
ContextEncoder 
Video 
Colorization 
Split-Brain 
Context 
Context ∗
Counting 
WatchingObjectsMove 
Jigsaw 
CC+Context-ColorDrop 
CC+Context-ColorProjection 
CC+Jigsaw++
 +vgg-Jigsaw++
CC+vgg-Context 
CC+vgg-Jigsaw++
assigned to ImageNet data unless speciﬁed otherwise. The
knowledge transfer is then completed by training AlexNet
on the pseudo-labels. Finally, this model is ﬁne-tuned on
PASCAL VOC 2007 for object classiﬁcation.
What is the impact of the number of clusters?
k-means clustering algorithm needs the user to choose
the number of clusters. In principle, too few clusters will
not lead to discriminative features and too many clusters
will not generalize. Thus, we explore different choices to
measure the sensitivity of our knowledge transfer algorithm. Since each cluster corresponds to a pseudo-label,
we can loosely say that the number of clusters determines
the number of object categories that the ﬁnal network
will be able to discern. Therefore, one might wonder if a
network trained with very few pseudo-labels develops a
worse learning than a network with a very large number of
pseudo-labels. This analysis is analogous to work done on
the ImageNet labels . Indeed, as shown in Table 1, we
ﬁnd that the network is not too sensitive to the number of
Table 4: ImageNet classiﬁcation with a linear classiﬁer.
We use the publicly available code and conﬁguration of
 . Every column shows the top-1 accuracy of AlexNet
on the classiﬁcation task. The learned weights from conv1
up to the displayed layer are frozen. The features of each
layer are spatially resized until there are fewer than 9K dimensions left. A fully connected layer followed by softmax
is trained on a 1000-way object classiﬁcation task.
Supervised 
CC+HOG 
Context 
ContextEncoder 
Colorization 
Split-Brain 
Counting 
CC+Jigsaw++
CC+vgg-Jigsaw++
clusters. Perhaps, one aspect to further investigate is that,
as the number of clusters increases, the number of data
samples per cluster decreases. This decrease might cause
the network to overﬁt and thus reduce its performance
despite its ﬁner categorization capabilities.
What is the impact of the cluster data domain?
knowledge transfer method is quite ﬂexible. It allows us to
pre-train on a dataset, cluster on another, and then deﬁne
pseudo-labels on a third one.
In this study, we explore
some of these options to illustrate the different biases of
the datasets.
The results are shown in Table 2.
these experiments, we pre-train AlexNet on the Jigsaw++
task with ImageNet. Then, we decouple the training and
inference of the clustering algorithm. For instance, in the
right column of Table 2, we learn cluster centers on conv4
features of Jigsaw++ extracted on Places and then run the
assignment of clustering on ImageNet to get pseudo-labels
to be used in the training of the ﬁnal AlexNet.
only a small reduction in performance, which implies that
our clustering method is not relying on particular biases
inherent in the ImageNet dataset.
5.2. Transfer Learning Evaluation
We evaluate the features learned with different SSL
methods on PASCAL VOC for object classiﬁcation, detection, and semantic segmentation. Also, we apply our
knowledge transfer method to some of these SSL methods
under relevant settings. In particular, we apply our knowledge transfer to: Context , Context-ColorDropping ,
Context-ColorProjection , Jigsaw++, and HOG .
Table 5: Places classiﬁcation with a linear classiﬁer. We
use the same setting as in Table 4 except that to evaluate
generalization across datasets, the model is pre-trained on
ImageNet (with no labels) and then tested with frozen layers
on Places (with labels).
Places labels 
ImageNet labels 
CC+HOG 
Context 
Jigsaw 
Context encoder 
Sound 
Colorization 
Split-Brain 
Counting 
CC+Jigsaw++
CC+vgg-Jigsaw++
Fine-Tuning on PASCAL VOC. In this set of experiments,
we use ﬁne-tuning on PASCAL VOC as a common benchmark for all the SSL methods. The comparisons are based
on object classiﬁcation and detection on VOC2007 using
the framework of and Fast-RCNN respectively.
We also report semantic segmentation result on VOC2012
dataset using the framework of . We found that in most
recent SSL papers, the settings of the detection task used
for the evaluation of SSL methods are not the same as the
ones used for supervised learning. More speciﬁcally, most
SSL methods are using multi-scale ﬁne-tuning for 150K iterations, with the basic learning rate of 0.001 and dividing the learning rate by 10 every 50K iterations. Moreover,
some of the methods have been evaluated using the multiscale test. We found that ﬁne-tuning supervised weights
with these settings achieves 59.1% and 59.9% with multi
scale and single scale test respectively. We believe that it
would be useful to use these as the baseline. We follow the
same settings in all of our evaluations and report the results
for both cases. We have locked the ﬁrst layer in all cases
including supervised weights as it is the default settings of
Fast-RCNN. In Table 3, we use “CC+” when our knowledge transfer method is used and “vgg-” when VGG16 is
used in pre-training. All methods in Table 3 use AlexNet for
cluster prediction and ﬁne-tuning. In the case of HOG features we only apply the cluster classiﬁcation on pseudolabels obtained from HOG on ImageNet. Surprisingly, these
handcrafted features yield a very high performance in all
three tasks. Our knowledge transfer method does not have a
signiﬁcant impact on the performance when the source and
destination architectures are the same. However, when pretraining on VGG16, there is a signiﬁcant boost of 2.6% in
Table 6: ImageNet classiﬁcation with a nonlinear classiﬁer as in . Every column shows top-1 accuracy of
AlexNet on the classiﬁcation task.
The learned weights
from conv1 up to the displayed layer are frozen. The rest
of the network is randomly initialized and retrained. Notice
that the reported results of are based on the original paper. All evaluations are done with 10 croppings per image.
Supervised 
Video 
Counting 
Context 
Jigsaw 
CC+-vgg-Context
CC+Jigsaw++
CC+vgg-Jigsaw++
classiﬁcation, 1.6% in detection, and 2.6% in semantic segmentation. These results show state-of-the-art performance
on all tasks. More importantly, the gap between SSL methods and supervised learning methods is further shrinking by
a signiﬁcant margin. We believe that our method allows to
use larger scale datasets and deeper models in pre-training,
while still using AlexNet in ﬁne-tuning.
Linear Classiﬁcation.
We also evaluate the SSL methods by using a linear classiﬁer on the features extracted
from AlexNet at different convolutional layers . We
apply this on both ImageNet and Places and
evaluate the classiﬁcation performance on the respective
We illustrate the performance on ImageNet in
Table 4 and on Places in Table 5.
As can be observed,
the performance of Jigsaw++ is comparable to prior stateof-the-art methods. Surprisingly, our knowledge transfer
method seems to be beneﬁcial to the transferred model
CC+Jigsaw++.
Consistently with other experiments, we
also observe that pre-training with VGG16 in CC+vgg-
Jigsaw++ gives a further substantial boost (an average of
almost 2% improvement). We also notice that HOG features do not demonstrate a performance in line with the performance observed on PASCAL VOC. A similar scenario
is observed on the Places dataset. Notice that the performance obtained with CC+vgg-Jigsaw++ is quite close to
the performance achieved with supervised pre-training on
ImageNet labels.
Nonlinear Classiﬁcation.
We freeze several layers initialized with evaluating weights and retrain the remaining
layers from scratch. For completeness, we also evaluate
SSL features as done in , by freezing a few initial layers and training the remaining layers from random initialization.
In comparison to the previous experiments, the
main difference is that here we use a nonlinear classiﬁer
that consists of the ﬁnal layers of AlexNet. This experi-
Figure 4: conv1 ﬁlters of the cluster classiﬁcation network using the AlexNet architecture trained with different pseudolabels obtained from: (a) a randomly initialized AlexNet network, (b) CC+HOG, (c) Doersch et al. trained with Color-
Dropping, (d) Doersch et al. trained with ColorProjection, (e) CC+Jigsaw++ task trained on AlexNet, (f) the CC+vgg-
Jigsaw++ task trained on VGG16.
Figure 5: Some cluster samples used to train CC+vgg-
Jigsaw++. Each row shows the 11 closest images to their
corresponding cluster center.
ments is another way to illustrate the alignment between
the pseudo-labels obtained from cluster classiﬁcation and
the ground truth labels. We show the comparisons in Table 6. The performance obtained by most SSL methods
seems to conﬁrm the pattern observed in the previous experiments. However, the difference between the previous
state-of-the-art and CC+vgg-Jigsaw++ is quite remarkable.
Also, the boost in performance of other prior work such as
 through pre-training with VGG16 is up to 9% at conv5.
5.3. Visualizations
We show some ﬁlters of the cluster classiﬁer network
in Figure 4. We can see the impact of the pre-trained network on the cluster classiﬁer. Interestingly, there is no color
in the ﬁlters of the “color dropping” method of after
knowledge transfer. This is consistent with the fact that this
method does not see any color image in the pre-training
stage. We also show some sample clusters used in training CC+vgg-Jigsaw++ in Figure 5. Each row corresponds
to images closest to the center of a single cluster. Ideally,
for high-quality representations, we expect images from the
same category on each row.
6. Conclusions
Self-supervised learning is an attractive research area in
computer vision since unlabeled data is abundantly available and supervised learning has serious issues with scaling to large datasets. Most recent SSL algorithms are restricted to using the same network architecture in both the
pre-training task and the ﬁnal ﬁne-tuning task. This limits our ability in using large scale datasets due to limited
capacity of the ﬁnal task model. We have relaxed this constraint by decoupling the pre-training model and the ﬁnal
task model by developing a simple but efﬁcient knowledge
transfer method based on clustering the learned features.
Moreover, to truly show the beneﬁt, we increase the complexity of a known SSL algorithm, the jigsaw task, and use
a VGG network to solve it. We show that after applying our
ideas to transfer the knowledge back to AlexNet, it outperforms all state-of-the-art SSL models with a good margin
shrinking the gap between supervised and SSL models from
%5.9 to %2.6 on PASCAL VOC 2007 object detection task.
Acknowledgements.
PF has been supported by the
Swiss National Science Foundation (SNSF) grant number
200021 169622. HP has been supported by GE Research
and Verisk Analytics.