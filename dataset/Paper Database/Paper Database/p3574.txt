Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241–2252
Copenhagen, Denmark, September 7–11, 2017. c⃝2017 Association for Computational Linguistics
Why We Need New Evaluation Metrics for NLG
Jekaterina Novikova, Ondˇrej Duˇsek, Amanda Cercas Curry and Verena Rieser
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
j.novikova, o.dusek, ac293, 
The majority of NLG evaluation relies
on automatic metrics, such as BLEU.
this paper, we motivate the need for
novel, system- and data-independent automatic evaluation methods:
We investigate a wide range of metrics, including state-of-the-art word-based and novel
grammar-based ones, and demonstrate that
they only weakly reﬂect human judgements of system outputs as generated by
data-driven, end-to-end NLG. We also
show that metric performance is data- and
system-speciﬁc. Nevertheless, our results
also suggest that automatic metrics perform reliably at system-level and can support system development by ﬁnding cases
where a system performs poorly.
Introduction
Automatic evaluation measures, such as BLEU , are used with increasing frequency to evaluate Natural Language Generation
(NLG) systems:
Up to 60% of NLG research
published between 2012–2015 relies on automatic
metrics .
Automatic evaluation is popular because it is cheaper
and faster to run than human evaluation, and it is
needed for automatic benchmarking and tuning of
algorithms. The use of such metrics is, however,
only sensible if they are known to be sufﬁciently
correlated with human preferences. This is rarely
the case, as shown by various studies in NLG
 , as well as in related ﬁelds, such
as dialogue systems , machine
translation (MT) , and
image captioning . This paper follows on from the
above previous work and presents another evaluation study into automatic metrics with the aim
to ﬁrmly establish the need for new metrics. We
consider this paper to be the most complete study
to date, across metrics, systems, datasets and domains, focusing on recent advances in data-driven
NLG. In contrast to previous work, we are the ﬁrst
• Target end-to-end data-driven NLG, where we
compare 3 different approaches.
In contrast to
NLG methods evaluated in previous work, our systems can produce ungrammatical output by (a)
generating word-by-word, and (b) learning from
noisy data.
• Compare a large number of 21 automated metrics, including novel grammar-based ones.
• Report results on two different domains and
three different datasets, which allows us to draw
more general conclusions.
• Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at
the sentence-level.
• Make all associated code and data publicly available, including detailed analysis results.1
End-to-End NLG Systems
In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence
planning and surface realisation from non-aligned
data . These approaches do not require
costly semantic alignment between Meaning Representations (MR) and human references (also referred to as “ground truth” or “targets”), but are
1Available for download at: 
jeknov/EMNLP_17_submission
Table 1: Number of NLG system outputs from different datasets and systems used in this study.
based on parallel datasets, which can be collected
in sufﬁcient quality and quantity using effective
crowdsourcing techniques, e.g. , and as such, enable rapid development of
NLG components in new domains. In particular,
we compare the performance of the following systems:
• RNNLG:2 The system by Wen et al. uses
a Long Short-term Memory (LSTM) network to
jointly address sentence planning and surface realisation. It augments each LSTM cell with a gate
that conditions it on the input MR, which allows it
to keep track of MR contents generated so far.
The system by Duˇsek and Jurˇc´ıˇcek
 learns to incrementally generate deepsyntax dependency trees of candidate sentence
plans (i.e. which MR elements to mention and the
overall sentence structure). Surface realisation is
performed using a separate, domain-independent
rule-based module.
• LOLS:4 The system by Lampouras and Vlachos
 learns sentence planning and surface realisation using Locally Optimal Learning to Search
(LOLS), an imitation learning framework which
learns using BLEU and ROUGE as non-decomposable
loss functions.
We consider the following crowdsourced datasets,
which target utterance generation for spoken dialogue systems. Table 1 shows the number of system outputs for each dataset. Each data instance
consists of one MR and one or more natural language references as produced by humans, such
as the following example, taken from the BAGEL
2 
3 
4 
5Note that we use lexicalised versions of SFHOTEL and
SFREST and a partially lexicalised version of BAGEL, where
proper names and place names are replaced by placeholders
(“X”), in correspondence with the outputs generated by the
inform(name=X,
pricerange=moderate,
type=restaurant)
Reference: “X is a moderately priced restaurant in X.”
• SFHOTEL & SFREST provide information about hotels and restaurants in
San Francisco. There are 8 system dialogue act
types, such as inform, conﬁrm, goodbye etc. Each
domain contains 12 attributes, where some are
common to both domains, such as name, type,
pricerange, address, area, etc., and the others are
domain-speciﬁc, e.g. food and kids-allowed for
restaurants; hasinternet and dogs-allowed for hotels. For each domain, around 5K human references were collected with 2.3K unique human utterances for SFHOTEL and 1.6K for SFREST. The
number of unique system outputs produced is
1181 for SFREST and 875 for SFHOTEL.
• BAGEL provides information about restaurants in Cambridge. The dataset
contains 202 aligned pairs of MRs and 2 corresponding references each. The domain is a subset
of SFREST, including only the inform act and 8 attributes.
Word-based Metrics (WBMs)
NLG evaluation has borrowed a number of automatic metrics from related ﬁelds, such as MT,
summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to
this group as word-based metrics. In general, the
higher these scores are, the better or more similar to the human references the output is.6 The
following order reﬂects the degree these metrics
move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words.
• Word-overlap Metrics (WOMs): We consider
frequently used metrics, including TER , BLEU , ROUGE
 , NIST , LEPOR , CIDEr , and
METEOR .
• Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han
et al. .
This measure is based on distributional similarity and Latent Semantic Analysis
systems, as provided by the system authors.
6Except for TER whose scale is reversed.
(LSA) and is further complemented with semantic
relations extracted from WordNet.
Grammar-based metrics (GBMs)
Grammar-based measures have been explored in
related ﬁelds, such as MT or grammatical error correction , and, in contrast to WBMs, do not rely
on ground-truth references. To our knowledge, we
are the ﬁrst to consider GBMs for sentence-level
NLG evaluation. We focus on two important properties of texts here – readability and grammaticality:
• Readability quantiﬁes the difﬁculty with which
a reader understands a text, as used for e.g. evaluating summarisation or text
simpliﬁcation . We
measure readability by the Flesch Reading Ease
score (RE) , which calculates a ratio between the number of characters per sentence,
the number of words per sentence, and the number of syllables per word. Higher RE score indicates a less complex utterance that is easier to read
and understand.
We also consider related measures, such as characters per utterance (len) and
per word (cpw), words per sentence (wps), syllables per sentence (sps) and per word (spw), as
well as polysyllabic words per utterance (pol) and
per word (ppw). The higher these scores, the more
complex the utterance.
• Grammaticality: In contrast to previous NLG
methods, our corpus-based end-to-end systems
can produce ungrammatical output by (a) generating word-by-word, and (b) learning from noisy
As a ﬁrst approximation of grammaticality, we measure the number of misspellings (msp)
and the parsing score as returned by the Stanford
parser (prs). The lower the msp, the more grammatically correct an utterance is.
The Stanford
parser score is not designed to measure grammaticality, however, it will generally prefer a grammatical parse to a non-grammatical one.7 Thus,
lower parser scores indicate less grammaticallycorrect utterances. In future work, we aim to use
speciﬁcally designed grammar-scoring functions,
e.g. , once they become publicly available.
7 
parser-faq.shtml
Human Data Collection
To collect human rankings, we presented the MR
together with 2 utterances generated by different systems side-by-side to crowdworkers, which
were asked to score each utterance on a 6-point
Likert scale for:
• Informativeness: Does the utterance provide all
the useful information from the meaning representation?
• Naturalness: Could the utterance have been
produced by a native speaker?
• Quality: How do you judge the overall quality
of the utterance in terms of its grammatical correctness and ﬂuency?
Each system output (see Table 1) was scored by
3 different crowdworkers. To reduce participants’
bias, the order of appearance of utterances produced by each system was randomised and crowdworkers were restricted to evaluate a maximum of
20 utterances. The crowdworkers were selected
from English-speaking countries only, based on
their IP addresses, and asked to conﬁrm that English was their native language.
To assess the reliability of ratings, we calculated
the intra-class correlation coefﬁcient (ICC), which
measures inter-observer reliability on ordinal data
for more than two raters .
The overall ICC across all three datasets is 0.45
(p < 0.001), which corresponds to a moderate
agreement. In general, we ﬁnd consistent differences in inter-annotator agreement per system and
dataset, with lower agreements for LOLS than for
RNNLG and TGEN. Agreement is highest for the
SFHOTEL dataset, followed by SFREST and BAGEL
(details provided in supplementary material).
System Evaluation
Table 2 summarises the individual systems’ overall corpus-level performance in terms of automatic
and human scores (details are provided in the supplementary material).
All WOMs produce similar results, with SIM
showing different results for the restaurant domain
(BAGEL and SFREST). Most GBMs show the same
trend (with different levels of statistical signiﬁcance), but RE is showing inverse results. System
performance is dataset-speciﬁc: For WBMs, the
LOLS system consistently produces better results
on BAGEL compared to TGEN, while for SFREST
and SFHOTEL, LOLS is outperformed by RNNLG in
More overlap
More overlap*
More overlap*
More similar
More similar*
More similar
Better grammar(*)
Better grammar(*)
Better grammar
More complex*
More complex*
More complex*
4.77(Sd=1.09)
4.91(Sd=1.23)
5.47*(Sd=0.81)
5.27(Sd=1.02)
5.29*(Sd=0.94)
5.16(Sd=1.07)
4.76(Sd=1.26)
4.67(Sd=1.25)
4.99*(Sd=1.13)
4.62(Sd=1.28)
4.86 (Sd=1.13)
4.74(Sd=1.23)
4.77(Sd=1.19)
4.54(Sd=1.28)
4.54 (Sd=1.18)
4.53(Sd=1.26)
4.51 (Sd=1.14)
4.58(Sd=1.33)
Table 2: System performance per dataset (summarised over metrics), where “*” denotes p < 0.05 for all
the metrics and “(*)” shows signiﬁcance on p < 0.05 level for the majority of the metrics.
terms of WBMs. We observe that human informativeness ratings follow the same pattern as WBMs,
while the average similarity score (SIM) seems to
be related to human quality ratings.
Looking at GBMs, we observe that they seem
to be related to naturalness and quality ratings.
Less complex utterances, as measured by readability (RE) and word length (cpw), have higher
naturalness ratings. More complex utterances, as
measured in terms of their length (len), number
of words (wps), syllables (sps, spw) and polysyllables (pol, ppw), have lower quality evaluation.
Utterances measured as more grammatical are on
average evaluated higher in terms of naturalness.
These initial results suggest a relation between
automatic metrics and human ratings at system
level. However, average scores can be misleading, as they do not identify worst-case scenarios.
This leads us to inspect the correlation of human
and automatic metrics for each MR-system output
pair at utterance level.
Relation of Human and Automatic
Human Correlation Analysis
We calculate the correlation between automatic
metrics and human ratings using the Spearman
coefﬁcient (ρ).
We split the data per dataset
and system in order to make valid pairwise comparisons.
To handle outliers within human ratings, we use the median score of the three human
raters.8 Following Kilickaya et al. , we use
the Williams’ test to determine
signiﬁcant differences between correlations. Table 3 summarises the utterance-level correlation
8As an alternative to using the median human judgment
for each item, a more effective way to use all the human
judgments could be to use Hovy et al. ’s MACE tool
for inferring the reliability of judges.
results between automatic metrics and human ratings, listing the best (i.e. highest absolute ρ) results for each type of metric (details provided in
supplementary material). Our results suggest that:
• In sum, no metric produces an even moderate
correlation with human ratings, independently of
dataset, system, or aspect of human rating. This
contrasts with our initially promising results on the
system level (see Section 6) and will be further discussed in Section 8. Note that similar inconsistencies between document- and sentence-level evaluation results are observed in MT (Specia et al.,
• Similar to our results in Section 6, we ﬁnd that
WBMs show better correlations to human ratings
of informativeness (which reﬂects content selection), whereas GBMs show better correlations to
quality and naturalness.
• Human ratings for informativeness, naturalness
and quality are highly correlated with each other,
with the highest correlation between the latter two
(ρ = 0.81) reﬂecting that they both target surface
realisation.
• All WBMs produce similar results (see Figure 1
and 2): They are strongly correlated with each
other, and most of them produce correlations with
human ratings which are not signiﬁcantly different
from each other. GBMs, on the other hand, show
greater diversity.
• Correlation results are system- and datasetspeciﬁc (details provided in supplementary material). We observe the highest correlation for TGEN
on BAGEL (Figures 1 and 2) and LOLS on SFREST,
whereas RNNLG often shows low correlation between metrics and human ratings.
This lets us
conclude that WBMs and GBMs are sensitive to
different systems and datasets.
• The highest positive correlation is observed between the number of words (wps) and informative-
0.30* (BLEU-1)
0.20* (ROUGE)
0.09 (BLEU-1)
0.14* (LEPOR)
0.13* (SIM)
0.28* (LEPOR)
-0.19* (TER)
-0.19* (TER)
0.10* (METEOR)
-0.20* (TER)
0.17* (ROUGE)
0.19* (METEOR)
-0.16* (TER)
0.16* (METEOR)
0.10* (METEOR)
-0.12* (TER)
0.09* (METEOR)
0.18* (LEPOR)
0.33* (wps)
0.16* (ppw)
-0.09 (ppw)
0.13* (cpw)
0.11* (len)
0.21* (len)
-0.25* (len)
-0.28* (wps)
-0.17* (len)
-0.18* (sps)
-0.19* (wps)
-0.21* (sps)
-0.19* (cpw)
0.31* (prs)
-0.16* (ppw)
-0.17* (spw)
0.11* (prs)
-0.16* (sps)
Table 3: Highest absolute Spearman correlation between metrics and human ratings, with “*” denoting
p < 0.05 (metric with the highest absolute value of ρ given in brackets).
Figure 1: Spearman correlation results for TGEN on BAGEL. Bordered area shows correlations between
human ratings and automatic metrics, the rest shows correlations among the metrics. Blue colour of
circles indicates positive correlation, while red indicates negative correlation. The size of circles denotes
the correlation strength.
Williams test results:
X represents
a non-signiﬁcant difference between correlations
(p < 0.05; top: WBMs, bottom: GBMs).
ness for the TGEN system on BAGEL (ρ = 0.33,
p < 0.01, see Figure 1). However, the wps metric (amongst most others) is not robust across systems and datasets: Its correlation on other datasets
is very weak, (ρ ≤.18) and its correlation with informativeness ratings of LOLS outputs is insigniﬁcant.
• As a sanity check, we also measure a random
score [0.0, 1.0] which proves to have a close-tozero correlation with human ratings (highest ρ =
Accuracy of Relative Rankings
We now evaluate a more coarse measure, namely
the metrics’ ability to predict relative human ratings. That is, we compute the score of each metric
for two system output sentences corresponding to
the same MR. The prediction of a metric is correct if it orders the sentences in the same way as
median human ratings (note that ties are allowed).
Following previous work , we mainly concentrate
on WBMs. Results summarised in Table 4 show
that most metrics’ performance is not signiﬁcantly
different from that of a random score (Wilcoxon
signed rank test). While the random score ﬂuctuates between 25.4–44.5% prediction accuracy,
the metrics achieve an accuracy of between 30.6–
49.8%. Again, the performance of the metrics is
dataset-speciﬁc: Metrics perform best on BAGEL
data; for SFHOTEL, metrics show mixed performance while for SFREST, metrics perform worst.
naturalness
TER, BLEU1-4,
ROUGE, NIST,
LEPOR, CIDEr,
METEOR, SIM
TER, BLEU1-4,
ROUGE, NIST,
LEPOR, CIDEr,
METEOR, SIM
TER, BLEU1-4,
ROUGE, NIST,
LEPOR, CIDEr,
METEOR, SIM
TER, BLEU1-4,
ROUGE, LEPOR,
CIDEr, METEOR,
TER, BLEU1-4,
ROUGE, NIST,
LEPOR, CIDEr,
Table 4: Metrics predicting relative human rating
with signiﬁcantly higher accuracy than a random
Discussion: Our data differs from the one used
in previous work , which uses explicit relative rankings (“Which output do you prefer?”), whereas we
compare two Likert-scale ratings.
As such, we
have 3 possible outcomes (allowing ties).
way, we can account for equally valid system
outputs, which is one of the main drawbacks of
forced-choice approaches .
Our results are akin to previous
work: Kilickaya et al. report results between 60-74% accuracy for binary classiﬁcation
on machine-machine data, which is comparable to
our results for 3-way classiﬁcation.
Still, we observe a mismatch between the ordinal human ratings and the continuous metrics.
For example, humans might rate system A and
system B both as a 6, whereas BLEU, for example, might assign 0.98 and 1.0 respectively, meaning that BLEU will declare system B as the winner.
In order to account for this mismatch, we
quantise our metric data to the same scale as the
median scores from our human ratings.9 Applied
to SFREST, where we previously got our worst re-
9Note that this mismatch can also be accounted for by
continuous rating scales, as suggested by Belz and Kow
sults, we can see an improvement for predicting
informativeness, where all WBMs now perform
signiﬁcantly better than the random baseline ,
where the task is simpliﬁed to distinguishing correct from incorrect output.
Error Analysis
In this section, we attempt to uncover why automatic metrics perform so poorly.
We ﬁrst explore the hypothesis that metrics are
good in distinguishing extreme cases, i.e. system
outputs which are rated as clearly good or bad by
the human judges, but do not perform well for utterances rated in the middle of the Likert scale, as
suggested by Kilickaya et al. . We ‘bin’ our
data into three groups: bad, which comprises low
ratings (≤2); good, comprising high ratings (≥5);
and ﬁnally a group comprising average ratings.
We ﬁnd that utterances with low human ratings
of informativeness and naturalness correlate signiﬁcantly better (p < 0.05) with automatic metrics
than those with average and good human ratings.
For example, as shown in Figure 3, the correlation
between WBMs and human ratings for utterances
with low informativeness scores ranges between
0.3 ≤ρ ≤0.5 (moderate correlation), while the
highest correlation for utterances of average and
high informativeness barely reaches ρ ≤0.2 (very
weak correlation). The same pattern can be observed for correlations with quality and naturalness ratings.
This discrepancy in correlation results between
low and other user ratings, together with the
fact that the majority of system outputs are rated
“good” for informativeness (79%), naturalness
(64%) and quality (58%), whereas low ratings do
not exceed 7% in total, could explain why the
overall correlations are low (Section 7) despite the
observed trends in relationship between average
system-level performance scores (Section 6). It
also explains why the RNNLG system, which contains very few instances of low user ratings, shows
poor correlation between human ratings and automatic metrics.
system output
human reference
inform(name = the donatello,
hasinternet = yes)
well there is a hotel with internet access called the donatello
the donatello has internet
inform nomatch(area =
embarcadero, kidsallowed=
yes, pricerange= expensive)
i but i but i but i but i but i
but i but i but i but i
unfortunately i could not ﬁnd
any expensive restaurants in
embarcadero that allow kids.
inform(name = X, area =
riverside, eattype =
restaurant, food = fastfood,
pricerange = cheap)
x is a restaurant on the
riverside called located at
the riverside and at is
x is a cheap fastfood restaurant located near the riverside
inform nomatch(kidsallowed
= yes, food = moroccan)
i am sorry, i did not ﬁnd any
restaurants that allows kids
and serve moroccan.
sorry, there are no restaurants allowing kids and serving moroccan food
Table 5: Example pairs of MRs and system outputs from our data, contrasting the average of wordoverlap metrics (normalised in the 1-6 range) and semantic similarity (SIM) with human ratings (median
of all measures).
Figure 3: Correlation between automatic metrics
(WBMs) and human ratings for utterances of bad
informativeness (top), and average and good informativeness (bottom).
Impact of Target Data
Characteristics of Data: In Section 7.1, we observed that datasets have a signiﬁcant impact on
how well automatic metrics reﬂect human ratings.
A closer inspection shows that BAGEL data differs
signiﬁcantly from SFREST and SFHOTEL, both in
terms of grammatical and MR properties. BAGEL
has signiﬁcantly shorter references both in terms
of number of characters and words compared to
the other two datasets. Although being shorter, the
words in BAGEL references are signiﬁcantly more
often polysyllabic. Furthermore, BAGEL only consists of utterances generated from inform MRs,
while SFREST and SFHOTEL also have less complex
MR types, such as conﬁrm, goodbye, etc. Utterances produced from inform MRs are signiﬁcantly
longer and have a signiﬁcantly higher correlation
with human ratings of informativeness and naturalness than non-inform utterance types. In other
words, BAGEL is the most complex dataset to generate from. Even though it is more complex, metrics perform most reliably on BAGEL here (note that
the correlation is still only weak). One possible
explanation is that BAGEL only contains two human
references per MR, whereas SFHOTEL and SFREST
both contain 5.35 references per MR on average.
Having more references means that WBMs naturally will return higher scores (‘anything goes’).
This problem could possibly be solved by weighting multiple references according to their quality,
as suggested by , or following
a reference-less approach .
Quality of Data: Our corpora contain crowdsourced human references that have grammatical
errors, e.g. “Fifth Floor does not allow childs”
(SFREST reference). Corpus-based methods may
pick up these errors, and word-based metrics will
rate these system utterances as correct, whereas
we can expect human judges to be sensitive to
ungrammatical utterances.
Note that the parsing score (while being a crude approximation of
grammaticality) achieves one of our highest correlation results against human ratings, with |ρ| =
.31. Grammatical errors raise questions about the
quality of the training data, especially when being crowdsourced. For example, Belz and Reiter
 ﬁnd that human experts assign low rankings to their original corpus text. Again, weighting
 or reference-less approaches
 might remedy this issue.
Example-based Analysis
As shown in previous sections, word-based metrics moderately agree with humans on bad quality
output, but cannot distinguish output of good or
medium quality. Table 5 provides examples from
Dimension of human ratings
Sentence Planning
Surface Realisation
this paper
weak positive (ρ = 0.33, WPS)
weak negative (ρ = 0. −31, parser)
NLG, restaurant/hotel search
 
strong positive (Pearson’s r = 0.96, NIST)
NLG, weather forecast
 
weak positive (ρ = 0.47, LSA)
negative (ρ = −0.56, NIST)
paraphrasing of news
 
weak positive (ρ = 0.35, BLEU-4)
dialogue/Twitter pairs
 
positive (ρ = 0.53, METEOR)
image caption
 
positive (ρ = 0.64, SPICE)
image caption
 
negative (ρ = −0.64, ROUGE)
NLG, German news texts
 
weak positive (ρ = 0.43, TER)
positive (ρ = 0.62, BLEU-4)
NLG, news texts
Table 6: Best correlation results achieved by our and previous work. Dimensions targeted towards Sentence Planning include ‘accuracy’, ‘adequacy’, ‘correctness’, ‘informativeness’. Dimensions for Surface
Realisation include ‘clarity’, ‘ﬂuency’, ‘naturalness’.
our three systems.10
Again, we observe different behaviour between WOMs and SIM scores. In
Example 1, LOLS generates a grammatically correct English sentence, which represents the meaning of the MR well, and, as a result, this utterance received high human ratings (median = 6) for
informativeness, naturalness and quality. However, WOMs rate this utterance low, i.e. scores of
BLEU1-4, NIST, LEPOR, CIDEr, ROUGE and METEOR normalised into the 1-6 range all stay below 1.5. This
is because the system-generated utterance has low
overlap with the human/corpus references. Note
that the SIM score is high (5), as it ignores human
references and computes distributional semantic
similarity between the MR and the system output.
Examples 2 and 3 show outputs which receive low
scores from both automatic metrics and humans.
WOMs score these system outputs low due to little or no overlap with human references, whereas
humans are sensitive to ungrammatical output and
missing information (the former is partially captured by GBMs). Examples 2 and 3 also illustrate inconsistencies in human ratings since system output 2 is clearly worse than output 3 and
both are rated by human with a median score of 1.
Example 4 shows an output of the RNNLG system
which is semantically very similar to the reference
(SIM=4) and rated high by humans, but WOMs fail
to capture this similarity. GBMs show more accurate results for this utterance, with mean of readability scores 4 and parsing score 3.5.
Related Work
Table 6 summarises results published by previous
studies in related ﬁelds which investigate the relation between human scores and automatic met-
10Please note that WBMs tend to match against the reference that is closest to the generated output. Therefore, we
only include the closest match in Table 5 for simplicity.
These studies mainly considered WBMs,
while we are the ﬁrst study to consider GBMs.
Some studies ask users to provide separate ratings
for surface realisation (e.g. asking about ‘clarity’
or ‘ﬂuency’), whereas other studies focus only on
sentence planning (e.g. ‘accuracy’, ‘adequacy’, or
‘correctness’). In general, correlations reported by
previous work range from weak to strong. The results conﬁrm that metrics can be reliable indicators at system-level , while
they perform less reliably at sentence-level . Also, the results show that the metrics capture realization better than sentence planning. There is a general trend showing that bestperforming metrics tend to be the more complex
ones, combining word-overlap, semantic similarity and term frequency weighting. Note, however,
that the majority of previous works do not report
whether any of the metric correlations are signiﬁcantly different from each other.
Conclusions
This paper shows that state-of-the-art automatic
evaluation metrics for NLG systems do not suf-
ﬁciently reﬂect human ratings, which stresses the
need for human evaluations. This result is opposed
to the current trend of relying on automatic evaluation identiﬁed in .
A detailed error analysis suggests that automatic metrics are particularly weak in distinguishing outputs of medium and good quality, which
can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance
is data- and system-speciﬁc.
Nevertheless, our results also suggest that automatic metrics can be useful for error analysis by
helping to ﬁnd cases where the system is performing poorly. In addition, we ﬁnd reliable results on
system-level, which suggests that metrics can be
useful for system development.
Future Directions
Word-based metrics make two strong assumptions: They treat human-generated references as
a gold standard, which is correct and complete.
We argue that these assumptions are invalid for
corpus-based NLG, especially when using crowdsourced datasets. Grammar-based metrics, on the
other hand, do not rely on human-generated references and are not inﬂuenced by their quality.
However, these metrics can be easily manipulated
with grammatically correct and easily readable
output that is unrelated to the input.
experimented with combining WBMs and GBMs
using ensemble-based learning. However, while
our model achieved high correlation with humans
within a single domain, its cross-domain performance is insufﬁcient.
Our paper clearly demonstrates the need for
more advanced metrics, as used in related ﬁelds,
including: assessing output quality within the dialogue context, e.g. ;
extrinsic evaluation metrics, such as NLG’s contribution to task success, e.g. ; building
discriminative models, e.g. , ; or
reference-less quality prediction as used in MT,
e.g. . We see our paper as
a ﬁrst step towards reference-less evaluation for
NLG by introducing grammar-based metrics. In
current work , we investigate a
reference-less quality estimation approach based
on recurrent neural networks, which predicts a
quality score for a NLG system output by comparing it to the source meaning representation only.
Finally, note that the datasets considered in this
study are fairly small (between 404 and 2.3k human references per domain). To remedy this, systems train on de-lexicalised versions , which bears the danger of ungrammatical
lexicalisation and a possible overlap between testing and training set . There are ongoing efforts to release larger and more diverse data sets,
e.g. .
Acknowledgements
This research received funding from the EPSRC
projects DILiGENt (EP/M005429/1) and MaDrIgAL (EP/N017536/1). The Titan Xp used for this
research was donated by the NVIDIA Corporation.