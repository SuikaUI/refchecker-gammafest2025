Multilingual Denoising Pre-training for Neural Machine Translation
Yinhan Liu*, Jiatao Gu*, Naman Goyal*, Xian Li, Sergey Edunov
Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer
Facebook AI Research
{yinhanliu,jgu,naman,xianl,edunov
ghazvini,mikelewis,lsz} @fb.com
This paper demonstrates that multilingual
denoising pre-training produces signiﬁcant
performance gains across a wide variety of
machine translation (MT) tasks. We present
mBART – a sequence-to-sequence denoising auto-encoder pre-trained on large-scale
monolingual corpora in many languages using the BART objective .
mBART is the ﬁrst method for pre-training
a complete sequence-to-sequence model by
denoising full texts in multiple languages,
while previous approaches have focused
only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete
model allows it to be directly ﬁne tuned
for supervised (both sentence-level and
document-level) and unsupervised machine
translation, with no task-speciﬁc modiﬁcations. We demonstrate that adding mBART
initialization produces performance gains in
all but the highest-resource settings, including up to 12 BLEU points for low resource
MT and over 5 BLEU points for many
document-level and unsupervised models.
We also show it also enables new types of
transfer to language pairs with no bi-text or
that were not in the pre-training corpus, and
present extensive analysis of which factors
contribute the most to effective pre-training.
Introduction
Despite its wide adoption for other NLP tasks , selfsupervised pretraining is not yet common practice in machine translation (MT). Existing MT
approaches only pre-train parts of the model, including the encoder 
and the decoder , or use pretraining objectives that only reconstruct parts of
text , or only focus on English
* Equal contribution.
corpora . In
this paper, we show that signiﬁcant performance
gains are possible by pre-training a complete autoregressive model with an objective that noises
and reconstructs full texts across many languages.
In this work, we present mBART – a multilingual sequence-to-sequence (Seq2Seq) denoising
auto-encoder. mBART is trained by applying the
BART to large-scale monolingual corpora across many languages. The input
texts are noised by masking phrases and permuting sentences, and a single Transformer model is learned to recover the texts.
Different from other pre-training approaches for
MT , mBART pre-trains a complete autoregressive Seq2Seq model. mBART is trained once for
all languages, providing a set of parameters that
can be ﬁne-tuned for any of the language pairs in
both supervised and unsupervised settings, without any task-speciﬁc or language-speciﬁc modiﬁcations or initialization schemes.
Extensive experiments demonstrate that this
simple approach works remarkably well. We ﬁrst
focus on existing MT benchmarks. For supervised
sentence-level MT, mBART initialization leads to
signiﬁcant gains (up to 12 BLEU points) across
low/medium-resource pairs (<10M bi-text pairs),
without sacriﬁcing performance in high-resource
settings. These results further improve with backtranslation (BT), setting a new state-of-the-art on
WMT16 English-Romanian and the FloRes test
sets. For document-level MT, our document-level
pre-training improves results by up to 5.5. For
the unsupervised case, we see consistent gains
and produce the ﬁrst non-degenerate results for
less related language pairs (e.g., 9.5 BLEU gain
on Nepali-English). Previous pre-training schemes
have only considered subsets of these tasks, but we
compare performance where possible and demonstrate that mBART consistently performs the best.
 
We also show that mBART enables new types
of transfer across language pairs. For example,
ﬁne-tuning on bi-text in one language pair (e.g.,
Korean-English) creates a model that can translate from all other languages in the monolingual
pre-training set (e.g., Italian-English), with no further training. We also show that languages not
in pre-training corpora can beneﬁt from mBART,
strongly suggesting that the initialization is at least
partially language universal. Finally, we present a
detailed analysis of which factors contribute the
most to effective pre-training, including the number of languages and their overall similarity.
Multilingual Denoising Pre-training
We use a large-scale common crawl (CC) corpus
(§2.1) to pre-train BART models (§2.2). Our experiments in the later sections involve ﬁnetuning a
range of models pre-trained on different subsets of
the CC languages §2.3).
Data: CC25 corpus
We pre-train on a subset of 25 languages – CC25 – extracted from the Common
Crawl (CC) 1. CC25 includes languages from different
families and with varied amounts of text (Table 1).
Following Lample and Conneau , we rebalanced the corpus by up/down-sampling text
from each language i with a ratio λi:
where pi is the percentage of each language in CC-
25. We use the smoothing parameter α = 0.7.
Pre-processing
We tokenize with a sentencepiece model 
learned on the full CC data that includes 250, 000
subword tokens. While not all of these languages
are used for pre-training, this tokenization supports ﬁne-tuning on additional languages. We do
not apply additional preprocessing, such as truecasing or normalizing punctuation/characters.
Model: mBART
Our models follow the BART 
sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages.
1 
Vietnamese
Chinese (Sim)
Lithuanian
Table 1: Languages and Statistics of the CC25 Corpus. A list of 25 languages ranked with monolingual
corpus size. Throughout this paper, we replace the language names with their ISO codes for simplicity. (*)
Chinese and Japanese corpus are not segmented, so the
tokens counts here are sentences counts
Architecture
We use a standard sequence-tosequence Transformer architecture , with 12 layers of encoder and 12 layers
of decoder with model dimension of 1024 on 16
heads (∼680M parameters). We include an additional layer-normalization layer on top of both the
encoder and decoder, which we found stabilized
training at FP16 precision.
Our training data covers K languages:
D = {D1, ..., DK} where each Di is a collection
of monolingual documents in language i. We (1)
assume access to a noising function g, deﬁned below, that corrupts text, and (2) train the model to
predict the original text X given g(X). More formally, we aim to maximize Lθ:
log P(X|g(X); θ) ,
where X is an instance in language i and the distribution P is deﬁned by the Seq2Seq model.
Where did __ from ? </s> Who __ I __ </s> <En>
<En> Who am I ? </s> Where did I come from ? </s>
Who am I ? </s> Where did I come from ? </s> <En>
Who am I ? </s> <En>
Transformer Encoder
Transformer Decoder
ᐺ΅抑Ҙ </s> <Ja>
<Ja> ᐺ΅抑Ҙ </s>
Transformer Encoder
Transformer Decoder
BBก෭̶ </s> ͳ΢BBV!<Ja>
<Ja> ͳ΢ͮΙ͘ ̵V!΀͵ก෭̶ </s>
ͳ΢ͮΙ͘ ̵V!΀͵ก෭̶ </s> <Ja>
Transformer Encoder
Transformer Decoder
Multilingual Denoising Pre-Training (mBART)
Fine-tuning on Machine Translation
ͳ΢ͮΙ͘ ̵V!΀͵ก෭̶ </s> <Ja>
Transformer Encoder
Transformer Decoder
:HOOWKHQV! See you tomorrow .</s> <En>
<En> :HOOWKHQV! See you tomorrow .</s>
Figure 1: Framework for our Multilingual Denoising Pre-training (left) and ﬁne-tuning on downstream MT tasks
(right), where we use (1) sentence permutation (2) word-span masking as the injected noise. A special language id
token is added at both the encoder and decoder. One multilingual pre-trained model is used for all tasks.
Noise function
Following Lewis et al. ,
we use two types of noise in g. We ﬁrst remove
spans of text and replace them with a mask token. We mask 35% of the words in each instance
by random sampling a span length according to a
Poisson distribution (λ = 3.5). We also permute
the order of sentences within each instance. The
decoder input is the original text with one position offset. A language id symbol <LID> is used
as the initial token to predict the sentence. It is also
possible to use other noise types, such as those in
Lample et al. , but we leave the exploration
of the optimal noising strategy to future work.
Instance format
For each instance of a batch,
we sample a language id symbol <LID>, and
we pack as many consecutive sentences as possible sampled from the corresponding corpus of
<LID>, until either it hits the document boundary
or reaches the 512 max token length. Sentences
in the instance are separated by the end of sentence (</S>) token. Then, we append the selected
<LID> token to represent the end of this instance.
Pre-training at “multi-sentence” level enables us to
work on both sentence and document translation.
Optimization
Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs
(32GB) for 500K steps. The total batch size
is around 128K tokens per GPU, matching
BART conﬁguration. We use
the Adam optimizer (ϵ = 1e−6, β2 = 0.98) and
linear learning rate decay scheduling. The total
training time was approximately 2.5 weeks. We
started the training with dropout 0.1 and reduced it
to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq .
Pre-trained Models
To better measure the effects of different levels
of multilinguality during pre-training, we built a
range of models as follows:
• mBART25 We pre-train a model on all 25 languages, using the setting described in §2.2.
• mBART06 To explore the effect of pre-training
on related languages, we pretrain a model on a
subset of six European languages: Ro, It, Cs, Fr,
Es and En. For a fair comparison, we use ∼1/4
of the mBART25 batch size, which allows our
model to have the same number of updates per
language during pre-training.
• mBART02 We pre-train bilingual models, using English and one other language for four
language pairs: En-De, En-Ro, En-It. We use a
batch size of ∼1/12 of that in the mBART25.
• BART-En/Ro To help establish baseline performance levels, we also train monolingual
BART models on the same En and Ro corpus
• Random As additional baselines, we will also
include a comparison with a model randomly
initialized without pre-training for each translation task. Since the sizes of different downstream datasets vary, we always grid-search the
hyper-parameters (architecture, dropout, etc.) to
ﬁnd the best non-pretrained conﬁguration.
All models use the same vocabulary (§2.1). Not
all tokens will frequently occur in all pre-training
corpora, but later experiments show that this large
vocabulary can improve generalization in multilingual settings even for unseen languages.
Data Source
Data Source
Data Source
Table 2: Low/Medium Resource Machine Translation Pre-training consistently improves over a randomly initialized baseline, with particularly large gains on low resource language pairs (e.g. Vi-En).
Table 3: High Resource Machine Translation where
all the datasets are from their latest WMT competitions.
We only evaluate our models on En-X translation.
Sentence-level Machine Translation
This section shows that mBART pre-training provides consistent performance gains in low to
medium resource sentence-level MT settings, including bi-text only and with back translation, and
outperforms other existing pre-training schemes
(§3.2). We also present a detailed analysis to understand better which factors contribute the most
to these gains (§3.3), and show that pre-training
can even improve performance for languages not
present in the pre-training data at all (§3.4).
Experimental Settings
We gather 24 pairs of publicly available parallel corpora that cover all the languages
in CC25 (Table 1). Most pairs are from previous
WMT (Gu, Kk, Tr, Ro, Et, Lt, Fi, Lv, Cs, Es,
Zh, De, Ru, Fr ↔En) and IWSLT (Vi, Ja, Ko,
Nl, Ar, It ↔En) competitions. We also use FLo-
Res pairs , En-Hi from IITB ,
and En-My from WAT19 .
We divide the datasets into three categories – low
resource (<1M sentence pairs), medium resource
(>1M and <10M), and high resource (>10M).
Fine-tuning & Decoding
We ﬁne-tune our multilingual pre-trained models on a single pair of bitext data, feeding the source language into the encoder and decoding the target language. As shown
in Figure 1, we load the pre-trained weights and
train the MT model on bi-texts with teacher forcing. For all directions, we train with 0.3 dropout,
0.2 label smoothing, 2500 warm-up steps, 3e−5
maximum learning rate. We use a maximum of
40K training updates for all low and medium resource pairs and 100K for high resource pairs. The
ﬁnal models are selected based on validation likelihood. For decoding, we use beam-search with
beam size 5 for all directions. The ﬁnal results
are reported in BLEU with
language-speciﬁc settings, see appendix A.
Main Results
As shown in Table 2, initializing with the pretrained mBART25 weights shows gains on all the
low and medium resource pairs when compared
with randomly initialized baselines. We observe
gains of 12+ BLEU on low resource pairs such as
En-Vi, En-Tr, and noisily aligned pairs like En-Hi.
Fine-tuning fails in extremely low-resource setting
such as En-Gu, which only have roughly 10k ex-
+BT iterations
Finetuning BLEU
+BT iterations
+BT iterations
+BT iterations
Figure 2: Pre-training + Back Translation on FLoRes with two iterations of BT.
Pre-training
Fine-tuning
XLM 
MASS 
BART 
XLM-R 
Table 4: Comparison with Other Pre-training Approaches on WMT16 Ro-En.
amples for tuning. In these settings, unsupervised
translation is more appropriate, see §5.2.
For high resource cases (Table 3), we do not
observe consistent gains, and pre-training slightly
hurts performance when >25M parallel sentence
are available. When a signiﬁcant amount of bi-text
data is given, we suspect that supervised training
washes out the pre-trained weights completely.
+ Back Translation
Back-translation is a standard approach to augment bi-text with target side monolingual data. We
combine our pre-training with BT and test it on
low resource language pairs – En-Si and En-Ne –
using the FLoRes dataset .
For a fair comparison, we use the same monolingual data as to generate BT data. Figure 2 shows that initializing the
model with our mBART25 pre-trained parameters
improves BLEU scores at each iteration of back
translation, resulting in new state-of-the-art results
in all four translation directions.
v.s. Other Pre-training Approaches
compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only
pair with established results. Our mBART model
outperforms all the other pre-trained models, both
with and without BT augmentation. We also show
comparisons with the conventional BART model
trained on the same En and Ro data only. Both
have improvements over baselines, while worse
than mBART results, indicating pre-training in a
multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a
new state-of-the-art for Ro-En translation.
We also present additional analysis, to better quantify when our pre-training helps.
How many languages should you pre-train on?
We investigate when it is helpful for pre-training
to include languages other than the targeted language pair that will be used during ﬁne tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the
target language monolingual data is limited (e.g.
En-My, the size of My is around 0.5% of En).
In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages
slightly hurts the ﬁnal results (<1 BLEU). In these
cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to
mBART02 on Ro-En suggests that pre-training
with similar languages is particularly helpful.
How many pre-training steps are needed?
plot Ro-En BLEU score v.s. Pre-training steps in
Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same ﬁne-tuning process described in §3.1. Without any pre-training,
our model overﬁts and performs much worse than
the baseline. However, after just 25K steps (5% of
training), both models outperform the best baseline. The models keep improving by over 3 BLEU
for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently
Table 5: Pretraining Languages on En-X translation.
The size refers to the size of monolingual data for X.
The size of En is shown as reference. All the pretrained
models were controlled to see the same number of English instances during training.
Training Cost
Random 
5 + 300 + 350
300∼3000 + 40
Table 6: Comparison with Back-Translation on My-En
translation using same mono-lingual data. We also estimate the computational costs for both pre-training and
back-translation based on Nvidia V100 GPUs.
slightly worse than mBART02.
How does the size of bitexts inference the gain
from pre-training?
Tables 2 and 3 show that
pre-training consistently improves for low and
medium resource language pairs. To verify this
trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take
the full En-De corpus (28M pairs) and randomly
sample 10K, 50K, 100K, 500K, 1M, 5M, 10M
datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve
over 20 BLEU with only 10K training examples,
while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the
gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result con-
ﬁrms our observation in §3.2 that our pre-training
does not help translation in high-resource pairs.
Is pre-training complementary to BT?
Figure 2 presents that our pre-trained models can
be combined with iterative back-translation (BT)
on additional data, however, it is still not a fair
comparison. Table 6 shows the results when using
pretraining steps (K)
Finetuning BLEU
Figure 3: Fine-tuning curves for Ro-En along with
Pre-training steps. Both mBART25 and mBART02
outperform the best baseline system after 25K steps.
Bi-text Size (# of sentence pairs)
Finetuning BLEU
Figure 4: Fine-tuning curves for En-De along with
size of bitext. The x-axis is on a log scale.
same monolingual data where we use 79M En and
29M My sentences following Chen et al. .
With the same amount of monolingual corpus,
mBART pre-training achieves the same performance on En→My as BT, while still 3 BLEU
worse on My→En. We suspect BT beneﬁts from
bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further
gains even with same monolingual data. Besides,
we also provide estimated training costs where BT
has a longer pipeline involving training a baseline
system (5h), translating monolingual data (300h)
and formal training (350h). Instead, most of training costs of mBART lies in the pre-training part
and can be easily adjusted to be more efﬁcient.
Generalization to Languages NOT in
Pre-training
In this section, we show that mBART can improve performance even with ﬁne tuning for languages that did not appear in the pre-training corpora, suggesting that the pre-training has language
universal aspects, especially within the parameters
learned at the Transformer layers.
Monolingual
34.6 (-8.7)
29.3 (-5.5)
27.5 (-10.1)
16.9 (-4.7)
21.3 (-6.4)
20.9 (-5.2)
41.4 (-2.9)
34.5 (-0.3)
34.9 (-2.7)
21.2 (-0.4)
26.1 (-1.6)
25.4 (-0.7)
En Ro Cs It Fr Es
43.1 (-0.2)
34.6 (-0.2)
37.3 (-0.3)
21.1 (-0.5)
26.4 (-1.3)
25.3 (-0.8)
Table 7: Generalization to Unseen Languages Language transfer results, ﬁne-tuning on language-pairs without
pre-training on them. mBART25 uses all languages during pre-training, while other settings contain at least one
unseen language pair. For each model, we also show the gap to mBART25 results.
Experimental Settings
We analyze the results
of three pairs: Nl-En, Ar-En and De-Nl using the
pre-trained mBART25, mBART06 and mBART02
(EnRo) models. During pre-training, mBART06
and EnRo Bilingual do not contain Arabic (Ar),
German (De) or Dutch (Nl) data, but all languages
are in mBART25. Both De and Nl are European
languages and are related to En, Ro and other the
languages in mBART06 pre-training data.
mBART25 uses all languages during
pre-training, but other settings contain at least one
unseen language. We ﬁnd large gains from pretraining on English-Romanian, even when translating a distantly related unseen language (Arabic)
and two unseen languages (German and Dutch).
The best results are achieved when pre-training includes both test languages, however pre-training
on other languages is surprisingly competitive.
Unseen Vocabularies
Arabic is distantly related
to the languages in mBART02 and mBART06, and
its use of a disjoint character set means that it word
embeddings will be largely untrained. However,
we obtain similar improvements on Ar-En pairs to
those on Nl-En. This result suggests that the pretrained Transformer layers learn universal properties of language that generalize well even with
minimal lexical overlap.
Unseen Source or Target Languages
shows different performance when the unseen languages are on the source side, target side, or both
sides. If both sides are unseen, the performance
(in terms of difference from mBART25) is worse
than where at least one language is seen during pre-training. Furthermore, although the En-X
pairs perform similarly, mBART06 outperforms
mBART02 by a margin on X-En pairs. Fine-tuning
unseen languages on source side is more difﬁcult,
deserving more extensive future study.
WMT19 En-De
TED15 Zh-En
Table 8: Statistics for the Document-level Corpus of
WMT19 En-De and TED15 Zh-En. # of instances is
the # of training examples in document model.
Document-level Machine Translation
We evaluate mBART on document-level machine
translation tasks, where the goal is to translate segments of text that contain more than one sentence
(up to an entire document). During pre-training,
we use document fragments of up to 512 tokens,
allowing the models to learn dependencies between sentences. We show that this pre-training
signiﬁcantly improves document-level translation.
Experimental Settings
We evaluate performance on two common document-level MT datasets: WMT19 En-De
and TED15 Zh-En (statistics in Table 8). For En-
De, we use the document data from WMT19 to
train our model, without any additional sentencelevel data; Zh-En dataset is from the IWSLT 2014
and 2015 evaluation campaigns . Following Miculicich et al. ,
we use 2010-2013 TED as the test set.
Pre-processing
We use the same pre-processing
as that in pre-training. For each block, sentences
are separated by end of sentence symbols (</S>)
and the entire instance is ended with the speciﬁc
language id (<LID>). The numbers of segmented
instances are also shown in Table 8 where on average, every document is split into 2-4 instances.
Fine-tuning & Decoding
We use the same ﬁnetuning scheme as for sentence-level translation
(§3.1), without using any task-speciﬁc techniques
developed by previous work (Miculicich et al.,
(a) Sentence- and Document-level BLEU scores on En-De
(b) Document-level BLEU scores on Zh-En
HAN 
Table 9: Document-Level Machine Translation on En-De and Zh-En. (×) The randomly initialized Doc-MT
model cannot produce translations aligned to the original sentences, so only document evaluation is possible.
2018; Li et al., 2019), such as constrained contexts or restricted attention. For decoding, we simply pack the source sentences into blocks, and
translate each instance block autoregressively. The
model does not know how many sentences to generate in advance and decoding stops when <LID>
is predicted. We use beam size 5 by default.
Baselines & Evaluation
We train 4 models: a
document-level (Doc-) MT model (§4.1) and a
corresponded sentence-level (Sent-) MT model
(§3.1) as the baseline, both with and without pretraining. We use mBART25 as the common pretrained model for En-De and Zh-En. For En-De,
even though our mBART25 Doc-MT model decodes multiple sentences together, the translated
sentences can be aligned to the source sentences,
which allows us to evaluate BLEU scores both on
sentence-level (s-BLEU) and document-level (d-
BLEU) 2. For Zh-En, however, we cannot produce
the same number of translated sentences as the reference due to alignment errors in the test data. We
only provide the d-BLEU scores on this direction.
We also compare our models with Hierarchical Attention Networks on Zh-En, which is the state-of-the-art nonpretraining approach for document-level translation for this pair. They combine two layers of attention – ﬁrst within and then across sentences.
Main Results
We show the main results for both En-De and Zh-
En are presented in Table 9.
Random v.s. Pre-trained
The MT models initialized with pre-trained weights outperform randomly initialized models by large margins, for
both sentence-level and document-level training.
Our mBART25 models (both Sent-MT and Doc-
MT) also outperform HAN 3, despite the fact that they are not customized for document-level MT in any way.
Sent-MT v.s. Doc-MT
For cases (En-De, En-
Zh), the mBART25 Doc-MT models outperform
themselves ﬁne-tuned at sentence-level by a margin, which is completely opposite for models without pre-training. For both datasets, randomly initialized Doc-MT fail to work, resulting in much
worse results than the sentence-level models. Such
large performance gaps indicate that pre-training
is critical for document level performance. It is in
general difﬁcult to collect high quality documentlevel data in large quantities, suggesting that pretraining may be a strong strategy for future work.
We also include a sampled example in appendix B.
Unsupervised Machine Translation
In addition to supervised machine translation, we
also evaluate our model on tasks where no bi-text
is available for the target language pair. We deﬁne
three types of unsupervised translation:
1. No bi-text of any kind is given. A common solution is to learn from back-translation (BT)
 . We
show that mBART provides a simple and effective initialize scheme for these methods.
2. No bi-text for the target pair is available, but
the target languages both appear in bi-text corpora for other language pairs. Previous work
has shown that zero-shot transfer is possible via
massively multi-lingual MT
 or distillation through
pivoting . We limit our focus to building MT models for single language
pairs, and leave multi-lingual pre-training for
multi-lingual MT to future work.
3. No bi-text for the target pair is available, but
there is bi-text for translating from some other
3d-BLEU is recomputed from the provided system output.
Generated En Text
Monolingual Ne Text
Generated Ne Text
Monolingual En Text
Parallel Hi Text
Parallel En Text
(no train)
Generated En Text
Figure 5: Illustrated frameworks for unsupervised machine translation via (a) back-translation (b) language transfer
where Ne-En is used as an example. For both cases, we initialize from multilingual pre-training (e.g. mBART25).
language into the target language. This is a new
evaluation regime, where we will show that
mBART supports effective transfer, even if the
source language has no bi-text of any form.
In this section, we demonstrate the effectiveness
of multilingual pre-training in unsupervised machine translation via (1) back-translation ( §5.1)
and (3) language transfer (§5.2). An illustration of
both approaches are presented in Figure 5.
Unsupervised Machine Translation via
Back-Translation
We evaluate our pre-trained models on
both similar (En-De, En-Ro) and dissimilar pairs
(En-Ne, En-Si), which are determined by measuring the subword units that are shared between the
source and target languages. We use the same test
sets as the supervised benchmarks §3.1, and directly use the pre-training data (CC25) for backtranslation to avoid introducing new information.
Following the same procedure described in Lample et al. ; Lample and
Conneau , we ﬁrst initialize the translation model with the pre-trained weights, and then
learn to predict the monolingual sentences conditioned on source sentences generated by on-the-
ﬂy back-translation (BT). Lample and Conneau
 only pre-train an encoder, so perform additional de-noising training to learn a seq2seq model
– a step which is unnecessary for mBART’s pretrained seq2seq model. However, we do constrain
mBART to only generating tokens in target language 4 for the ﬁrst 1000 steps of on-the-ﬂy BT, to
avoid it simply copying the source text.
Table 10 shows the unsupervised translation results compared with non-pretrained mod-
4We mask out the output probability of predicting tokens
which appear less than 1% in the target monolingual corpus.
els, as well as models with existing pre-training
methods. Our models achieve large gains over
non-pretrained models for all directions, and outperform XLM signiﬁcantly for dissimilar pairs
(En-Ne, En-Si) where the existing approaches
completely fail. For similar pairs, our model also
performs well against XLM and MASS, with the
best numbers for En-X pairs.
Unsupervised Machine Translation via
Language Transfer
The second case of unsupervised machine translation assumes the target language appears in a bitext corpus with some other source language.
We only consider X→En translation,
and choose the bitexts of 12 language pairs from
§3.1, covering Indic languages (Ne, Hi, Si, Gu),
European languages (Ro, It, Cs, Nl), East Asian
languages (Zh, Ja, Ko) and Arabic languages (Ar).
As illustrated in Figure 5 (b), we take
the pre-trained mBART25 model and ﬁnetune on
each language pair, and then directly apply them
to the rest of pairs, as seen in Table 11. We also
present the direct ﬁne-tuning performance (§3) on
the diagonal, for reference. We can always obtain reasonable transferring scores at all pairs over
different ﬁne-tuned models except from Gu-En
where the supervised model completely fails (0.3
BLEU). In some cases, we can achieve similar
(Cs-En) or even much better (Ne-En, Gu-En) results compared to the supervised results.
As a comparison, we also apply the same procedure on randomly initialized models without pretraining, which always ends up with ≈0 BLEU.
This indicates that multilingual pre-training is
essential and produces universal representations
across languages, so that once the model learns
to translate one language to En, it learns to trans-
Similar Pairs
Dissimilar Pairs
XLM 
MASS 
Table 10: Unsupervised MT via Back-Translation. En-De, En-Ro are initialized by mBART02, while En-Ne,
En-Si are initialized by mBART25. Our models are trained on monolingual data used in pre-training.
Fine-tuning Languages
Testing Languages
Table 11: Unsupervised MT via Language Transfer on X-En translations. The model ﬁne-tuned on one language
pair is directly tested on another. We use gray color to show the direct ﬁne-tuning results, and lightgray color to
show language transfer within similar language groups. We bold the highest transferring score for each pair.
Table 12: Back-Translation v.s. Language Transfer
for Unsupervised MT. We present the best transferring scores together with the pairs transferred from.
late all languages with similar representations. We
also present three examples of language transferring between Zh, Ja and Ko in appendix B.
When is language transfer useful?
also shows mixed results at each pair. First, for
most pairs, language transfer works better when
ﬁne-tuning is also conducted in the same language
family, especially between Indic languages (Hi,
Ne, Gu). However, signiﬁcant vocabulary sharing
is not required for effective transfer. For instance,
Zh-En and It-En achieve the best transfer learning
results on Ko-En and Ar-En, respectively. However, the vocabulary overlapping (even character
overlapping) between Zh and Ko, It and Ar is low.
w/ Back-Translation
We also present the comparison on 4 pairs of unsupervised MT with backtranslation (BT) v.s. language transfer in Table 12.
The results are also mixed. If there exists high
quality (similar languages) bi-text data, or translating between dissimilar pairs, language transfer
is able to beat the conventional methods with BT.
Furthermore, we also show promising results for
combining these two techniques. In such cases, we
start from the best transferred model and apply (iterative) BT on the same monolingual corpus used
in pre-training. Table 12 presents the results with 1
iteration of BT. For all pairs, we see improvements
by combining both techniques.
Related Work
Pre-training for Text Generation
inherits from the recent success brought by selfsupervised pre-training for NLP applications , especially for text generation tasks where different self-supervised objectives are designed for
training big neural models on enormous unlabeled
text corpora The pre-trained models are usually
used as the initialization for ﬁne-tuning variant
downstream tasks such as controllable language
modeling , machine
translation , summarization and dialogue generation . In contrast to most prior work, we
focus on a deep exploration of applying denoising
pre-training for various translation applications.
Multilinguality in NLP tasks
This work is also
related to the continual trend of multilingual language learning, including aligning multilingual
word embeddings into universal
space, and learning cross-lingual models to exploit shared representations across languages.
For machine translation, the most relevant ﬁeld
is multilingual translation where the ultimate goal is to
jointly train one translation model that translates
multiple language directions at the same time, and
shares representations to improve the translation
performance on low-resource languages . In this paper, we mainly focus on multilingualism in the pre-training stage and ﬁne-tune the
learned model in the standard bi-lingual scenario.
Compared to multilingual translation, we do not
require parallel data across multiple languages but
the targeted direction, which potentially improves
the scalability to low-resource languages and speciﬁc domains. Moreover, multilingual pre-training
is unlikely to suffer the interference problems between dissimilar languages, which is typical for
regular multilingual translation models.
Document Translation
As one of the key applications, this work also links to previous efforts for
incorporating document-level contexts into neural machine translation . Li et al.
 is the most relevant work which also utilized pre-trained encoder (BERT) for handling
longer context. However, none of these works had
shown positive results on pure Seq2Seq models
at document-level, which involved task-speciﬁc
techniques, and usually only worked on sentencelevel translation with a constrained range of context. To the extent of our knowledge, our multilingual pre-trained model is the ﬁrst-of-its-kind
work that shows improved results on documentlevel translation with standard Seq2Seq learning.
Unsupervised Translation
This work also summarizes the previous efforts of learning to translate
between languages without a direct parallel corpus, and re-deﬁnes them as unsupervised machine
translation with three categories where in this
work, we only focus on applications to the ﬁrst and
the third kinds (§5). When no parallel corpus of
any kind is available, Artetxe et al. ; Lample
et al. proposed to jointly learn denoising auto-encoder and back-translation from both
directions, which, however, required good initialization and only worked well on similar language
pairs; Wu et al. replaced back-translation
with retrieved similar sentences from target monolingual data; Wu et al. solves the problem
by mining sentences from Wikipedia and use them
as weakly supervised translation pairs. Similar to
Lample and Conneau ; Song et al. ,
we follow the ﬁrst approach and treat our pretrained model as the initialization step. Besides,
we investigate unsupervised translation using language transfer, which is similar to Pourdamghani
et al. where the authors generate translationese of the source language and train a system on high-resource languages to correct these
intermediate utterances. It is also closely related
to Conneau et al. ; Artetxe et al. for
cross-lingual representation learning.
Conclusion
We demonstrate that multilingual de-noising pretraining is able to signiﬁcantly improve both supervised and unsupervised machine translation at
both the sentence level and document level. We
analyze when and how pre-training is most effective and can be combined with other approaches
such as back-translation. Our results also show the
transfer learning ability of the learned representations from multilingual pre-training.
In future work, we will scale-up the current pretraining to more languages, e.g., an mBART100
model. The size of our model makes it expensive
to deploy in production – future work will explore
pre-training more efﬁcient models.
Acknowledgements
We thank Marc’Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun
Shen for details about FloRes and WAT datasets.
We also thank our colleagues at FAIR and FAIAR
for valuable feedback.