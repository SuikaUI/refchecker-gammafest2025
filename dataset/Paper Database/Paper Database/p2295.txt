Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1268–1283,
November 16–20, 2020. c⃝2020 Association for Computational Linguistics
SSMBA: Self-Supervised Manifold Based Data Augmentation for
Improving Out-of-Domain Robustness
University of Toronto
Vector Institute
Kyunghyun Cho
New York University
Marzyeh Ghassemi
University of Toronto
Vector Institute
Models that perform well on a training domain often fail to generalize to out-of-domain
(OOD) examples. Data augmentation is a common method used to prevent overﬁtting and improve OOD generalization. However, in natural language, it is difﬁcult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training
examples by using a pair of corruption and reconstruction functions to move randomly on
a data manifold.
We investigate the use of
SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct
corrupted text with masked language models. In experiments on robustness benchmarks
across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both
in-domain and OOD data, achieving gains
of 0.8% accuracy on OOD Amazon reviews,
1.8% accuracy on OOD MNLI, and 1.4 BLEU
on in-domain IWSLT14 German-English. 1
Introduction
Training distributions often do not cover all of the
test distributions we would like a supervised classiﬁer or model to perform well on. Often, this
is caused by biased dataset collection or test distribution drift over time
 . Therefore, a key
challenge in training machine learning models in
these settings is ensuring they are robust to unseen
examples. Since it is impossible to generalize to
the entire distribution, methods often focus on the
adjacent goal of out-of-domain robustness.
Data augmentation is a common technique used
to improve out-of-domain (OOD) robustness by
synthetically generating new training examples
 
nng555/ssmba
Figure 1: SSMBA moves along the data manifold M
by using a corruption function to perturb an example x
off the data manifold, then using a reconstruction function to project it back on.
 , often by perturbing existing examples in the input space . If data concentrates on a low-dimensional
manifold , then these synthetic examples should lie in a manifold neighborhood of the original examples . Training models to be robust to such local perturbations has been shown to be effective in
improving performance and generalization in semisupervised and self-supervised settings . When the underlying data manifold exhibits
easy-to-characterize properties, as in natural images, simple transformations such as translation
and rotation can quickly generate local training
examples. However, in domains such as natural
language, it is much more difﬁcult to ﬁnd a set of
invariances that preserves meaning or semantics.
In this paper we propose Self-Supervised
Manifold Based Data Augmentation (SSMBA): a
data augmentation method for generating synthetic
examples in domains where the data manifold is
difﬁcult to heuristically characterize. Motivated by
the use of denoising auto-encoders as generative
models , we use a corruption
function to stochastically perturb examples off the
data manifold, then use a reconstruction function to
project them back on (Figure 1). This ensures new
examples lie within the manifold neighborhood of
the original example. SSMBA is applicable to any
supervised task, requires no task-speciﬁc knowledge, and does not rely on class- or dataset-speciﬁc
ﬁne-tuning.
We investigate the use of SSMBA in the natural
language domain on 3 diverse tasks spanning both
classiﬁcation and sequence modelling: sentiment
analysis, natural language inference, and machine
translation. In experiments across 9 datasets and 4
model types, we show SSMBA consistently outperforms baseline models and other data augmentation
methods on both in-domain and OOD data.
Background and Related Work
Data Augmentation in NLP
The problem of domain adaptation and OOD robustness is well established in NLP .
Existing work on improving generalization has focused on data augmentation, where synthetically
generated training examples are used to augment an
existing dataset. It is hypothesized that these examples induce robustness to local perturbations, which
has been shown to be effective in semi-supervised
and self-supervised settings .
Existing task-speciﬁc methods and word-level methods are based on
human-designed heuristics. Back-translation from
or through another language has been applied in
the context of machine translation , question answering , and
consistency training . More recent work has used word embeddings and LSTM language models to perform word replacement. Other
methods focus on ﬁne-tuning contextual language
models or large generative models to generate synthetic examples.
VRM and the Manifold Assumption
Vicinal Risk Minimization (VRM) formalizes data augmentation as enlarging
the training set support by drawing samples from
a vicinity of existing training examples. Typically
the vicinity of a training example is deﬁned using
dataset-dependent heuristics. For example, in com-
Figure 2: To sample from an MLM DAE, we apply the
MLM corruption q to the original sentence then reconstruct the corrupted sentence using our DAE r.
puter vision, examples are generated using scale
augmentation ,
color augmentation , and
translation and rotation .
The manifold assumption states that high dimensional data concentrates around a low-dimensional
manifold . This assumption
allows us to deﬁne the vicinity of a training example as its manifold neighborhood, the portion of
the neighborhood that lies on the data manifold.
Recent methods have used the manifold assumption to improve robustness by moving examples
towards a decision boundary ,
generating adversarial examples , interpolating between
pairs of examples , or ﬁnding
afﬁne transforms .
Sampling from Denoising Autoencoders
A denoising autoencoder (DAE) is an autoencoder trained to reconstruct a clean input x from
a stochastically corrupted one x′ ∼q(x′|x) by
learning a conditional distribution Pθ(x|x′) . We can sample from a DAE
by successively corrupting and reconstructing an
input using the following pseudo-Gibbs Markov
t ∼q(x′|xt−1), xt ∼Pθ(x|x′
the number of training examples increases, the
asymptotic distribution πn(x) of the generated samples approximate the true data-generating distribution P(x) . This corruptionreconstruction process allows for sampling directly
along the manifold that P(x) concentrates on.
Masked Language Models
Recent advances in unsupervised representation
learning for natural language have relied on pretraining models on a masked language modeling
(MLM) objective . In the MLM objective, a percentage of the
input tokens are randomly corrupted and the model
is asked to reconstruct the original token given its
Figure 3: SSMBA generates synthetic examples by corrupting then reconstructing the original training inputs.
To form the augmented dataset, corresponding outputs
are preserved from the original data or generated from
a supervised model f trained on the original data.
left and right context in the corrupted sentence. We
use MLMs as DAEs to sample
from the underlying natural language distribution
by corrupting and reconstructing inputs (Figure 2).
SSMBA: Self-Supervised Manifold
Based Augmentation
Algorithm 1 SSMBA
1: Require: perturbation function q
reconstruction function r
2: Input: Dataset D = {(x1, y1) . . . (xn, yn)}
number of augmented examples m
3: function SSMBA(D, m)
train a model f on D
for (xi, yi) ∈D do
for j ∈1 . . . m do
sample perturbed x′
ij ∼q(x′|xi)
sample reconstructed ˆxij ∼r(ˆx|x′
generate ˆyij ←f(ˆxij) or preserve
the original yi
let Daug = {(ˆxij, ˆyij)}i=1...n,j=1...m
augment D′ ←D ∪Daug
15: end function
We now describe Self-Supervised Manifold Based
Data Augmentation.
Let our original dataset
D consist of pairs of input and output vectors
D = {(x1, y1) . . . (xn, yn)}. We assume the input points concentrate around an underlying lower
dimensional data manifold M. Let q be a corruption function from which we can draw a sample
x′ ∼q(x′|x) such that x′ no longer lies on M. Let
r be a reconstruction function from which we can
draw a sample ˆx ∼r(ˆx|x′) such that ˆx lies on M.
To generate an augmented dataset, we take
each pair (xi, yi) ∈D and sample a perturbed
i ∼q(x′|xi). We then sample a reconstructed
ˆxij ∼r(ˆx|x′
i). A corresponding vector ˆyij can
be generated by preserving yi, or, since examples
in the manifold neighborhood may cross decision
boundaries on more sensitive tasks, by using a
teacher model trained on the original data. This
operation can be repeated to generate multiple augmented examples for each input example. These
new examples form a dataset that we can augment
the original training set with. We can then train an
augmented model on the new augmented dataset.
In this paper we investigate SSMBA’s use on natural language tasks, using the MLM training corruption function as our corruption function q and
a pre-trained BERT model as our reconstruction
model r. Different from other data augmentation
methods, SSMBA does not rely on task-speciﬁc
knowledge, requires no dataset-speciﬁc ﬁne-tuning,
and is applicable to any supervised natural language task. SSMBA requires only a pair of functions q and r used to generate data.
To empirically evaluate our proposed algorithm,
we select 9 datasets – 4 sentiment analysis datasets,
2 natural language inference (NLI) datasets, and
3 machine translation (MT) datasets. Table 1 and
Appendix A provide dataset summary statistics. All
datasets either contain metadata that can be used to
split the samples into separate domains or similar
datasets that are treated as separate domains.
Sentiment Analysis
The Amazon Review Dataset 
contains product reviews from Amazon. Following Hendrycks et al. 2020, we form two datasets:
AR-Full contains reviews from the 10 largest categories, and AR-Clothing contains reviews in the
clothing category separated into subcategories by
metadata. Since the reviews in AR-Clothing come
from the same top-level category, the amount of
domain shift is much less than that of AR-Full.
Models predict a review’s 1 to 5 star rating.
SST2 contains movie review excerpts. Following Hendrycks et al. 2020 we
pair this dataset with the IMDb dataset , which contains full length movie reviews.
We call this pair the Movies dataset. Models predict a movie review’s binary sentiment.
The Yelp Review Dataset contains restaurant
reviews with associated business metadata which
we preprocess following Hendrycks et al. 2020.
Models predict a review’s 1 to 5 star rating.
Natural Language Inference
MNLI is a corpus of NLI
data from 10 distinct genres of written and spoken
English. We train on the 5 genres with training data
and test on all 10 genres. Since the dataset does
not include labeled test data, we use the validation
set as our test set and sample 2000 examples from
each training set for validation.
ANLI is a corpus of NLI data
designed adversarially by humans such that stateof-the-art models fail to classify examples correctly.
The dataset consists of three different levels of dif-
ﬁculty which we treat as separate textual domains.
Machine Translation
Following M¨uller et al. 2019, we consider two
translation directions, German→English (de→en)
and German→Romansh (de→rm). Romansh is a
low-resource language with an estimated 40,000
native speakers where OOD robustness is of practical relevance .
In the de→en direction, we use IWSLT14
de→en as a widely-used
benchmark to test in-domain performance. We
also use the OPUS dataset to
test OOD generalization. We train on highly speciﬁc in-domain data (medical texts) and disparate
out-of-domain data (Koran text, Ubuntu localization ﬁles, movie subtitles, and legal text). Since
domains share very little similarities in language,
generalization to out-of-domain text is extremely
difﬁcult. In the de→rm direction, we use a training set consisting of the Allegra corpus and Swiss press releases. We
use blog posts from Convivenza as a test domain.
Experimental Setup
Model Types
For sentiment analysis tasks, we investigate LSTMs
 and convolutional neural networks (CNNs). For NLI tasks, we
investigate ﬁne-tuned RoBERTaBASE models , which are pretrained bidirectional
transformers . On both tasks,
representations from the encoder are fed into an
feed-forward neural network for classiﬁcation. For
MT tasks, we train transformers . For all models, word embeddings are initialized randomly and trained end-to-end with the
model. We do not initialize with pre-trained word
embeddings to maintain consistency across all models and tasks. Model hyperparameters are tuned
to maximize performance on in-domain validation
data. Training details and hyperparameters for all
models are provided in Appendix C.
SSMBA Settings
For all experiments we use the MLM corruption
function as our corruption function q. We tune tune
the total percentage of tokens corrupted, leaving
the percentages of speciﬁc corruption operations
(80% masked, 10% random, 10% unmasked) the
same. For sentiment analysis and NLI experiments
we use a pre-trained RoBERTaBASE model as our
reconstruction function r, and for translation experiments we use a pre-trained German BERT model
 . For each input example, we
generate 5 augmented examples using unrestricted
sampling. For translation experiments, target side
translations are generated with beam search with
width 5. SSMBA hyperparameters, including augmented example labelling method and corruption
percentage, are chosen based on in-domain validation performance. Hyperparameters for each
dataset are provided in Appendix D.
On sentiment analysis and NLI tasks, we compare
against 3 data augmentation methods. Easy Data
Augmentation (EDA) is a
heuristic method that randomly replaces synonyms
and inserts, swaps, and deletes words. Conditional
Bert Contextual Augmentation (CBERT) ﬁnetunes a class-conditional BERT model
and uses it to generate sentences in a process similar to our own. Unsupervised Data Augmentation
(UDA) translates data to and from
a pivot language to generate paraphrases. We adapt
UDA for supervised classiﬁcation tasks by training
directly on the backtranslated data.
On translation tasks, we compare only against
methods which do not require additional target side
monolingual data. Word dropout randomly chooses words in the source sentence to set to zero embeddings. Reward Augmented Maximum Likelihood (RAML) samples noisy target sentences based
on an exponential of their Hamming distance from
the original sentence.
SwitchOut applies a noise function similar to RAML to
both the source and target side. We use publicly
available implementations for all methods.
Evaluation Method
We train LSTM and CNN models with 10 random
seeds, RoBERTa models with 5 random seeds, and
transformer models with 3 random seeds. Models
are trained separately on each domain then evaluated on all domains, and performance is averaged
across seeds and test domains. We report the average in-domain (ID) and OOD performance across
all train domains. On sentiment analysis and NLI
tasks we report accuracy, and on translation we
report uncased tokenized BLEU for IWSLT and cased, detokenized BLEU
with SacreBLEU2 for all others. Statistical testing details are in Appendix E.
Sentiment Analysis
Table 2 present results on sentiment analysis.
Across all datasets, models trained with SSMBA
2Signature: BLEU+c.mixed+#1+s.exp+tok.13a+v.1.4.3
outperform baseline models and all other data augmentation methods on OOD data. On ID data,
SSMBA outperforms baseline models and other
data augmentation methods on all datasets for CNN
models, and 3/4 datasets for RNN models. On average, SSMBA improves OOD performance by 1.1%
for RNN models and 0.7% for CNN models, and ID
performance by 0.8% for RNN models and 0.4%
for CNN model. Other methods achieve much
smaller OOD generalization gains and perform
worse than baseline models on multiple datasets.
On the AR-Full dataset, RNNs trained with
SSMBA demonstrate improvements in OOD accuracy of 1.1% over baseline models. On the AR-
Clothing dataset, which exhibits less domain shift
than AR-Full, RNNs trained with SSMBA exhibit
slightly lower OOD improvement. CNN models exhibit about the same boost in OOD accuracy across
both Amazon review datasets.
On the Movies dataset where we observe a large
difference in average sentence length between the
two domains, SSMBA still manages to present considerable gains in OOD performance. Although
RNNs trained with SSMBA fail to improve ID performance, their OOD performance in this setting
still beats other data augmentation methods.
On the Yelp dataset, we observe large performance gains on both ID and OOD data for RNN
models. The improvements on CNN models are
more modest, but notably our method is the only
one that improves OOD generalization.
Natural Language Inference
Table 3 presents results on NLI tasks. Models
trained with SSMBA outperform or match baseline models and data augmentation methods on
both ID and OOD data. Even with a more difﬁcult task and stronger baseline model, SSMBA still
confers large accuracy gains. On MNLI, SSMBA
improves OOD accuracy by 1.8%, while the best
performing baseline achieves only 0.3% improvement. Our method also improves ID accuracy by
1.4%. All other baseline methods hurt both ID and
OOD accuracy, or confer negligible improvements.
On the intentionally difﬁcult ANLI, SSMBA
maintains baseline OOD accuracy while conferring a large 6% improvement on ID data. Other
augmentation methods improve ID accuracy by a
much smaller margin while degrading OOD accuracy. Surprisingly, pseudo-labelling augmented
examples in the R2 and R3 domains produced the
AR-Clothing
Augmentation
67.41∗† 70.19
68.60∗† 89.61
62.83∗† 70.96
64.81∗† 72.11
Table 2: Average in-domain (ID) and out-of-domain (OOD) accuracy (%) for models trained on sentiment analysis datasets. Average performance across datasets is weighted by number of domains contained in each dataset.
Accuracies marked with a ∗and † are statistically signiﬁcantly higher than unaugmented models and the next best
model respectively, both with p < 0.01.
Augmentation
82.44∗† 48.46∗† 43.80
Table 3: Average in-domain and out-of-domain accuracy (%) for RoBERTa models trained on NLI tasks.
Accuracies marked with a ∗and † are statistically signiﬁcantly higher than unaugmented models and the
next best model respectively, both with p < 0.01.
ConvS2S 
Transformer 
DynamicConv 
Transformer (ours)
+ Word Dropout
+ SwitchOut
Table 4: Results on IWSLT de→en for models trained
with different data augmentation methods.
marked with a ∗and † are statistically signiﬁcantly
higher than baseline transformers and the next best
model, both with p < 0.01.
best results, even when the labelling model had
poor in-domain performance.
Machine Translation
Table 4 presents results on IWSLT14 de→en. We
compare our results with convolutional models
Augmentation
Word Dropout
Table 5: Average in-domain and out-of-domain BLEU
for models trained on OPUS (de→en) and de→rm data.
Scores marked with a ∗and † are statistically signiﬁcantly higher than baseline transformers and the next
best model, both with p < 0.01.
 and strong baseline transformer and dynamic convolution models . SSMBA improves BLEU by almost 1.5
points, outperforming all other baseline and comparison models. Compared to SSMBA, other augmentation methods offer much smaller improvements or even degrade performance.
Table 5 presents results on OPUS and de→rm.
On OPUS, where the training domain contains
highly specialized language and differs signiﬁcantly both from other domains and the learned
MLM manifold, SSMBA offers a small boost in
OOD BLEU but degrades ID performance. All
other augmentation methods degrade both ID and
OOD performance. On de→rm, SSMBA improves
OOD BLEU by a large margin of 2.4 points, and
ID BLEU by 0.4 points. Other augmentation methods offer much smaller OOD improvements while
degrading ID performance.
Training Set Size
OOD Accuracy (%)
No Augmentation
Figure 4: OOD accuracy of models trained on successively subsampled datasets. The full training set contains 25k examples. Error bars show standard deviation
in OOD accuracy across models.
Analysis and Discussion
In this section, we analyze the factors that inﬂuence
SSMBA’s performance. Due to its relatively small
size (25k sentences), number of OOD domains (3),
and amount of domain shift, we focus our analysis on the Baby domain within the AR-Clothing
dataset. Ablations are performed on a single domain rather than all domains, so error bars correspond to variance in models trained with different
seeds and results are not comparable with those in
Table 2. Unless otherwise stated, we train CNN
models and augment with SSMBA, corrupting 45%
of tokens, performing unrestricted sampling when
reconstructing, and using self-supervised soft labelling, generating 5 synthetic examples for each
training example.
Training Set Size
We ﬁrst investigate how the size of the initial
dataset affects SSMBA’s effectiveness. Since a
smaller dataset covers less of the training distribution, we might expect the data generated by
SSMBA to explore less of the data manifold and
reduce its effectiveness. We subsample 25% of the
original dataset to form a new training set, then
repeat this process successively to form exponentially smaller and smaller datasets. The smallest
dataset contains only 24 examples. For each dataset
fraction, we train 10 models and average performance, tuning a set of SSMBA hyperparameters on
the same ID validation data. Figure 4 shows that
SSMBA offers OOD performance gains across almost all dataset sizes, even in low resource settings
with less than 100 training examples.
OOD Accuracy Boost (%)
Table 6: Boost in OOD accuracy (%) of models trained
with SSMBA augmented data generated with different
reconstruction functions.
Corruption Percentage
Boost in OOD Accuracy (%)
Boost in OOD accuracy (%) of models
trained with SSMBA augmentation applied with different percentages of corrupted tokens.
Reconstruction Model Capacity
Since SSMBA relies on a reconstruction function
that approximates the underlying data manifold,
we might expect a larger and more expressive
model to generate higher quality examples. We
investigate three models of varying size: Distil-
RoBERTa with 82M parameters, RoBERTaBASE with 125M parameters, and
RoBERTaLARGE with 355M parameters. For each
reconstruction model, we generate a set of 10 augmented datasets and train a set of 10 models on
each augmented dataset. We average performance
across models and datasests. Table 6 shows that
SSMBA displays robustness to the choice of reconstruction model, with all models conferring similar
improvements to OOD accuracy. Using the smaller
DistilRoBERTa model only degrades performance
by a small margin.
Corruption Amount
How sensitive is SSMBA to the particular amount
of corruption applied? Empirically, tasks that were
more sensitive to input noise, like sentiment analysis, required less corruption than those that were
more robust, like NLI. To analyze the effect of tuning the corruption amount, we generate 10 sets of
augmented data with varying percentages of corruption, then train 10 models on each dataset, averaging performance across all 100 models. Figure 5
shows that for corruption percentages below 50%,
unrestricted
Boost in OOD Accuracy (%)
Boost in OOD accuracy (%) of models
trained with SSMBA augmentation using different sampling methods. Error bars show standard deviation in
OOD accuracy across models.
our algorithm is relatively robust to the speciﬁc
amount of corruption applied. OOD performance
peaks at 45% corruption, decreasing thereafter as
corruption increases. Very large amounts of corruption tend to degrade performance, although surprisingly all augmented models still outperform
unaugmented models, even when 95% of tokens
are corrupted. In experiments on the more input
sensitive NLI task, large amounts of noise degraded
performance below baselines.
Sample Generation Methods
Next we investigate methods for generating the
reconstructed examples ˆx ∼r(ˆx|x′). Top-k sampling draws samples from the MLM distribution
on the top-k most probable tokens, leading to augmented data that explores higher probability regions of the manifold. We investigate top1, top5,
top10, top20, and top50 sampling. Unrestricted
sampling draws samples from the full probability distribution of tokens. This method explores a
larger area of the underlying data distribution but
can often lead to augmented data in low probability
For each sample generation method, we generate
5 sets of augmented data and train 10 models on
each dataset. OOD accuracy is averaged across
all models for a given sampling method. Figure 6
shows that unrestricted sampling provides the greatest increase in OOD accuracy, with top-k sampling
methods all performing similarly. This suggests
that SSMBA works best when it is able to explore
the manifold without any restrictions.
# Augmented Sentences
OOD Accuracy (%)
Figure 7: OOD accuracy (%) of models trained with
different amounts of SSMBA augmentation. 0 augmentation corresponds to a baseline model. Error bars show
standard deviation in OOD accuracy across models.
Amount of Augmentation
How does OOD accuracy change as we generate
more sentences and explore more of the manifold
neighborhood? To investigate we select various
augmentation amounts and generate 5 datasets for
each amount, training 10 models on each dataset
and averaging OOD accuracy across all 50 models.
Figure 7 shows that increasing the amount of augmentation increases the amount by which SSMBA
improves OOD accuracy, as well as decreasing the
variance in the OOD accuracy of trained models.
Label Generation
We investigate 3 methods to generate a label ˆyij
for a synthetic example ˆxij. Label preservation
preserves the original label yi. Since the manifold
neighborhood of an example may cross a decision
boundary, we also investigate using a supervised
model f trained on the original set of unaugmented
data for hard labelling of a one-hot class label ˆyij
and soft labelling of a class distribution ˆyij.
We train a CNN model to varying levels of convergence and validation accuracy, then label a set of
5 augmented datasets with each labelling method.
When training with soft labels, we optimize the
KL-divergence between the output distribution and
soft label distribution. For each dataset we train 10
models and average performance across all models
and datasets. Results are shown in Figure 8.
Unsurprisingly, soft and hard labelling with a
low accuracy model degrades performance. As
our supervision classiﬁer improves, so does the
performance of models trained with soft and hard
labelled data. Once we pass a certain accuracy
threshold, models trained with soft labels begin
Labelling Model ID Validation Accuracy
Boost in OOD Accuracy (%)
Preserve Label
Soft Label
Hard Label
Boost in OOD accuracy (%) of models
trained with augmented data labelled with different supervision models and label generation methods.
outperforming all other models. This threshold
varies depending on the difﬁculty of the dataset and
task. In ANLI experiments, labelling augmented
examples even with a poor performing model still
improved downstream accuracy.
Conclusion
In this paper, we introduce SSMBA, a method
for generating synthetic data in settings where
the underlying data manifold is difﬁcult to characterize. In contrast to other data augmentation
methods, SSMBA is applicable to any supervised
task, requires no task-speciﬁc knowledge, and
does not rely on dataset-speciﬁc ﬁne-tuning. We
demonstrate SSMBA’s effectiveness on three NLP
tasks spanning classiﬁcation and sequence modeling: sentiment analysis, natural language inference, and machine translation. We achieve gains
of 0.8% accuracy on OOD Amazon reviews, 1.8%
accuracy on OOD MNLI, and 1.4 BLEU on indomain IWSLT14 de→en. Our analysis shows that
SSMBA is robust to the initial dataset size, reconstruction model choice, and corruption amount, offering OOD robustness improvements in most settings. Future work will explore applying SSMBA
to the target side manifold in structured prediction
tasks, as well as other natural language tasks and
settings where data augmentation is difﬁcult.
Acknowledgements
Resources used in preparing this research were
provided, in part, by the Province of Ontario,
the Government of Canada through CIFAR, and
companies sponsoring the Vector Institute www.
vectorinstitute.ai/#partners. This work was
partly supported by Samsung Advanced Institute of
Technology (Next Generation Deep Learning: from
pattern recognition to AI) and Samsung Research
(Improving Deep Learning using Latent Structure).
We thank Julian McAuley, Vishaal Prasad, Taylor
Killian, Victoria Cheng, and Aparna Balagopalan
for helpful comments and discussion.