ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning
Framework for Natural Language Generation
Dongling Xiao∗, Han Zhang∗, Yukun Li , Yu Sun , Hao Tian ,
Hua Wu and Haifeng Wang
Baidu Inc., China
{xiaodongling, zhanghan17, liyukun01, sunyu02, tianhao, wu hua, wanghaifeng}@baidu.com
Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-ﬂow sequence
to sequence pre-training and ﬁne-tuning framework
named ERNIE-GEN, which bridges the discrepancy between training and inference with an inﬁlling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a
span-by-span generation ﬂow that trains the model
to predict semantically-complete spans consecutively rather than predicting word by word. Unlike
existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves
state-of-the-art results with a much smaller amount
of pre-training data and parameters on a range
of language generation tasks, including abstractive
summarization (Gigaword and CNN/DailyMail),
question generation (SQuAD), dialogue response
generation (Persona-Chat) and generative question
answering (CoQA). The source codes and pretrained models have been released at 
com/PaddlePaddle/ERNIE.
Introduction
Pre-trained on large-scale unlabeled text corpora and ﬁnetuned on downstream tasks, self-supervised representation
models such as GPT [Radford et al., 2018], BERT [Devlin
et al., 2019] and XLNet [Yang et al., 2019b] have achieved
remarkable improvements in natural language understanding
(NLU). Different from encoder-only pre-training like BERT
or decoder-only pre-training like GPT, natural language generation (NLG) relies on the sequence to sequence generation framework (seq2seq) which consists of a bidirectional
encoder and a unidirectional decoder. Current pre-training
works in NLG such as MASS [Song et al., 2019] and UNILM
∗indicates equal contribution.
Span-by-span flow
p ~sentence length
(c) Data strategy of UniLM
(a) Typical generation mechanism
(b) Infilling generation mechanism
Word-by-word flow
(d) Data strategy of MASS
(e) Data strategy of ERNIE-GEN
target fragments
50% of Source
Figure 1: Schematic of two generation mechanisms (left) and data
strategies for pre-training (right). Blocks in green, orange and blue
denote source texts, target texts and artiﬁcial symbols.
[Dong et al., 2019] mainly focus on jointly pre-training encoder and decoder on different self-supervised tasks. However, these works pay little attention to the exposure bias issue
[Ranzato et al., 2016], a major drawback of teacher-forcing
training. This issue is due to the fact that groundtruth words
are used during training, while generated words, whether
predicted correctly or not, are used for inference where
mistakes tend to accumulate.
To alleviate this issue, we
present ERNIE-GEN, an enhanced multi-ﬂow seq2seq training framework characterized by a carefully-designed Multi-
Flow Attention architecture based on Transformer [Vaswani
et al., 2017], as illustrated in Figure 2. ERNIE-GEN incorporates a novel inﬁlling generation mechanism and a noiseaware generation method into pre-training and ﬁne-tuning,
which is proved to be effective through experiments in §4.3.
• Inﬁlling generation.
Instead of using last groundtruth
word in training or last generated word in inference, we
adopt an inserted artiﬁcial symbol [ATTN] along with its
position to gather history contextual representations at each
step in both training and inference, which diverts model’s
attention away from last word and coerces it into focusing
on all former representations, thus alleviating negative in-
ﬂuence of previous mistakes to subsequent generation, as
shown in Figure 1(b).
• Noise-Aware generation. We corrupt the input target se-
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)
Multi-Flow
Span-by-span
Generation Flow
Word-by-word
Generation Flow
Generation
Multi-Granularity
Target Fragments
Text Corpora
Supervised
Training Data
Pre-trained
Pre-training
Fine-tuning
Language Model
Figure 2: Overview of ERNIE-GEN framework. S, T and Y donate
source, target, and generated texts, T ′ is the noised version of T .
quence by randomly replacing words to arbitrary words in
the vocabulary. This setup, despite its simplicity, proves to
be an effective way to make the model be aware of mistakes
in training, so that the model is able to detect mistakes and
ignore them during inference.
Moreover, in light of the fact that entities, phrases and sentences in human writing are organized in a coherent manner,
we incorporate a span-by-span generation task into ERNIE-
GEN as a new generation ﬂow to train the model to predict
semantically-complete spans consecutively rather than predicting word by word as traditional models do. This task is
implemented through the inﬁlling generation mechanism in
parallel with an inﬁlling-based word-by-word generation ﬂow
to facilitate convergence in training, as shown in Figure 1b.
In addition, as shown in Figure 1(c-d), recent pre-training
works for NLG like UNILM and MASS only sample a single
continuous segment as target sequence. However, this sampling method compromises the correlation between encoder
and decoder when it comes to pre-training of long texts (typically 512 words), given that adjacent segments are often relevant semantically. ERNIE-GEN adopts a multi-granularity
target fragments sampling strategy to force decoder to rely
more on the encoder representations other than the previous
generated words, thus enhancing the correlation between encoder and decoder, as shown in Figure 1e.
Empirically, ERNIE-GEN is particularly effective and
achieves state-of-the-art results on a range of NLG tasks
including abstractive summarization (Gigaword and CN-
N/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA), utilizing a much smaller amount of pretraining data and parameters.
Related Work
Pre-training for NLP Tasks.
Recently, pre-training methods have achieved state-of-the-art results in multiple NLU
tasks. ELMo [Peters et al., 2018] pre-trains two unidirectional language models (LMs) with forward and backward
direction respectively to feature downstream tasks. GPT utilizes an adjusted Transformer [Vaswani et al., 2017] to learn
a forward LM and then ﬁne-tunes the forward LM on supervised datasets. BERT proposes a masked language modeling
(MLM) task to learn deep bidirectional representations. Nevertheless, above methods are usually implemented by just one
encoder or decoder, which is less effective in encoder-decoder
based generation tasks, thus several works have preliminarily explored the pre-training towards NLG by incorporating
BERT’s MLM into the seq2seq framework and shown excellent performance on a range of generation tasks. MASS
masks a consecutive fragment (50%) of the input sentence
with [MASK] symbols to predict.
UNILM masks several
words in the input sequence which is a pair of segments for
encoder and decoder, and then predicts the masked words in
accordance with BERT’s MLM.
Exposure Bias Issue.
NLG tasks suffer from the exposure
bias which is caused by teacher-forcing training. To address
such issue, RNN-based variational autoencoders (VAEs) are
leveraged in [Yang et al., 2019a; Wang et al., 2019], whereas
it requires inference for both posterior and prior distribution.
Reinforcement learning is also adopted to text generation against exposure bias issue [Ranzato et al., 2016;
Wang et al., 2018], which is, however, inefﬁcient during
training because of the word-by-word sampling procedure.
These methods are inefﬁcient and less practical for pretraining that relies on large-scale unlabeled text corpora.
Span-level Pre-training.
[Sun et al., 2019; Sun et al.,
2020; Joshi et al., 2019] verify that predicting spans reaches
substantially better performance on NLU tasks. Meanwhile,
inspired by characteristics of human expression, we hope the
model have the foresight to generate a semantically-complete
span at each step rather than a word. Consequently, a spanby-span generating task is proposed to make the model capable of generating texts more human-like.
Proposed Framework
Built on inﬁlling generation mechanism, ERNIE-GEN adopts
a Multi-Flow Attention architecture to train the model on
word-by-word and span-by-span generation tasks in parallel. In this section, we describe ERNIE-GEN according to
the training process shown in Figure 2.
Multi-Granularity Target Fragments
Given an input sequence S = {s1, ..., sn}, we ﬁrst sample a length distribution Di from a distribution set D =
{D1, ..., D|D|} with probability pi for target fragments, and
then select fragments according to Di in S iteratively until the
fragment budget has been spent (e.g. 25% of S). We denote
j as the j-th fragment which is sampled in length distribution Di. Sampled fragments are then removed from S and
stitched together to form target sequence T = [T1, ..., Tk] =
1, ..., Si
k]. We denote S′ as the left input sequence after
removing sampled fragments.
ERNIE-GEN performs pretraining by predicting the fragmented target sequence T and
minimizing the negative log likelihood:
L(θ; S, Di) = −logP(T |S′, Di; θ)
P(Tj|T<j, S′, Di; θ).
where the target sequence T is sorted by the positions of sampled fragments. For each fragment T = {t1, ..., t|T |} in T ,
we have P(T) = Q|T |
j=1 P(tj|t<j).
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)
[A][A][A] [A]
Word-by-word
Contextual
Contextual
Span-by-span
Multi-Flow Attention
words in decoder side
can see themself
cannot see words
(b) Word-by-word generation flow
(c) Span-by-span generation flow
t1 t2 t3 t4
[A][A][A] [A]
t1 t2 t3 t4
Input Tokens
Output Tokens
Positional
Multi-Head
Multi-Head
cannot see the word
with the same position
in the same span
Contextual flow
Contextual flow: [ , ]
Figure 3: Illustration of the Multi-Flow Attention module. (a):Overview of multi-ﬂow attention. The encoder and the decoder share the parameters of multi-layer Transformer. (b):Word-by-word generation ﬂow with history contextual representations from Contextual Flow. (c):Spanby-span generation ﬂow with shared Contextual Flow. (d):The attention mask matrixes of word-by-word generation ﬂow (MW ), contextual
ﬂow (MC) and span-by-span generation ﬂow (MS). The i-th generated token yi is calculated by argmax(softmax(Fc(a(L−1)
Following preliminary trials, we set a hyperparameter γ =
0.25, which denotes the ratio of length of all fragments to that
of the input sequence S. Besides, we introduce two uniform
distributions D = {U(1, 4), U(4, 32)} with probability of
0.4 and 0.6 respectively to sample fragments, which aims to
learn representations from different perspectives. On the one
hand, short fragments beneﬁt learning of semantic relation
between words; on the other hand, longer fragments help to
memorize sentence-level expressions.
Noise-Aware Generation
To train a generation model which can detect the false prediction and mitigate its impact on subsequent generation, we
corrupt the groundtruth sequence T with a procedure where
words are being replaced randomly, and the corrupted T is
represented as T ′. There are two hyperparameters, ρp and
ρf, denoting the noising rate in pre-training and ﬁne-tuning
respectively.
Architecture: Multi-Flow Attention
Formally, given a source sequence S = {s1, ..., sn}, a noised
target sequence T ′ = {t1, ..., tm}, we denote the inference of
seq2seq network based on shared Transformer as follows:
←MH-Attn(Q = s(l)
i , KV = S(l)).
←MH-Attn(Q = t(l)
S(l), t(l)
where Q, K, V denote the query, key, and value in Multi-
Head attention [Vaswani et al., 2017]. s(l)
the i-th vector representations of the l-th layer of Multi-Head
Attention for the encoder and the decoder respectively, [·] denotes the concatenation operation. In this work, we call the
above procedure the Contextual Flow.
Word-by-word Generation Flow.
Based on inﬁlling generation mechanism, this ﬂow utilizes an inserted [ATTN]
symbol to gather history representations word by word (see
Figure 1b). To facilitate this process, we place all inserted
[ATTN] together to construct an artiﬁcial symbol sequence
AW ={[ATTN]1, ...,[ATTN]m} which has the same length
as T ′, as shown in Figure 3b. To be speciﬁc, the word-byword generation ﬂow is updated as follow:
←MH-Attn(Q = a(l)
S(l), t(l)
where a(l)
indicates the i-th vector representation of the l-th
layer for the artiﬁcial symbol sequence AW .
Span-by-span Generation Flow.
Different from word-byword generation ﬂow, span-by-span ﬂow uses [ATTN] symbols to predict spans consecutively, as shown in Figure 3c.
Formally, given a list of span boundaries B = {b1, ..., b|B|},
we conduct the span-by-span generation ﬂow as:
←MH-Attn(Q = a(l)
S(l), t(l)
where j ∈[bi, bi+1), and a(l)
denotes the (j −bi)-th vector representation of the i-th span. Essentially, the model
is trained to predict a whole span {tbi, ..., tbi+1−1} with the
same history context [S, t<bi]. Instead of randomly sampling
spans, we prefer sampling spans with semantical information
and knowledge. Speciﬁcally, we consider the following two
steps to sample spans consecutively in T ′:
• Firstly, we implement a T-test to compute t-statistic scores
of all bigrams and trigrams, which is based on an initial
hypothesis H0: a random span of n arbitrary words w =
{w1, ..., wn} with probability p′(w) = Qn
i=1 p(wi) cannot
be a statistical n-gram. The t-statistic score is calculated by
(p(w)−p′(w))
, where p(w) = Count(w)
and σ2= p(w)(1 −
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)
p(w)), indicating the statistic probability and the standard
deviation of w respectively, N denotes the total number of
n-grams appearing in the training data. According to the
t-statistic scores, we select the top 200,000 bigrams, top
50,000 trigrams and all unigrams to construct a speciﬁc
vocabulary of spans, which is represented as Vspan.
• Secondly, we search the trigram, bigram and unigram in
order, starting with current word until a span (n-gram, n ≤
3) is retrieved in Vspan.
Multi-Flow Attention.
To integrate the word-by-word generation ﬂow and span-by-span generation ﬂow, we apply them
in parallel with a shared contextual ﬂow by leveraging the
multi-ﬂow attention architecture, as Figure 3a describes. The
multi-ﬂow attention is computed as:
X(l+1) ←MH-Attn(Q=X(l), KV =X(l), MC)
←MH-Attn(Q=A(l)
X(l), A(l)
←MH-Attn(Q=A(l)
X(l), A(l)
where X denotes the concatenation of S and T ′, X(l) is
the vector sequence of the l-th layer for the contextual ﬂow.
S are vector sequences of the l-th layer for the wordby-word and span-by-span generation ﬂow respectively. As
shown in Figure 3d, attention mask matrix M determines
whether query and key can attend to each other by modifying
the attention weight W=softmax( QKT
√dk + M) [Vaswani et
al., 2017] . Speciﬁcally, M is assigned as:
can be attended
−∞, prevent from attending
While training, we add the loss of the word-by-word and
span-by-span generation ﬂow with an coefﬁcient λ:
L(T ) = λLW ord(T ) + (1 −λ)LSpan(T )
= −λlogP(T |A(L−1)
) −(1 −λ)logP(T |A(L−1)
where T indicates the unnoised target sequence, and L(·) denotes the cross entropy loss function. In detail, we set λ = 0.5
and λ = 1.0 respectively in pre-training and ﬁne-tuning.
Inference: Inﬁlling Decoding
During inference, the target sequence T is unknown, we insert symbol [ATTN] step by step to gather the representation
of history context instead of preparing an artiﬁcial symbol
sequence A in advance. Meanwhile, for the purpose of efﬁciency, we need to drop the inserted [ATTN] after inference
at each step, as detailed in Figure 4.
Experiments
In this section, we compare our ERNIE-GEN with previous
works and conduct several ablation experiments to assess the
performance of proposed methods in §3.
Pre-training and Implementation
Analogous to BERT and UNILM, ERNIE-GEN is trained
on English Wikipedia1 and BookCorpus, totaling 16GB.
1English Wikipedia version: enwiki-20181101.
Figure 4: Schematic of inﬁlling decoding: the particular procedures
in inﬁlling decoding including dropping and inserting (left) and the
attention mask matrixes at each step (right).
The input sequence is lowercased and truncated to a maximum length of 512.
We train a base model ERNIE-
GENBASE (L=12, H=768, A=12, Total Parameters=110M)2
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively. Speciﬁcally,
Adam optimizer with β1 = 0.9, β2 = 0.999, ϵ = 10−9 is
employed. The peak learning rate is 5e-5 with warmup over
the ﬁrst 4,000 steps and linear decay scheduling. The noising
rate ρp for pre-training is 0.05. Batches are organized by limiting the maximum number of tokens to 196,608. Pre-training
experiments are carried out on PaddlePaddle platforms3 and
Nvidia Tesla V100 GPU. By virtue of ﬂoat16 mixed precision training, it takes almost 4 days for 400,000 steps to train
ERNIE-GENBASE while almost 7 days for 450,000 steps to
train ERNIE-GENLARGE.
Fine-tuning on Downstream Tasks
Abstractive Summarization
aims at generating ﬂuent and
concise summaries without being constrained to extracting
sub-sequences from the input articles. We execute experiments on Gigaword dataset [Rush et al., 2015] and CNN/DailyMail dataset [Hermann et al., 2015]. Gigaword dataset
contains 3.8M articles extracted from the Gigaword corpus,
while CNN/DailyMail dataset consists of 93k articles and
220k articles from the CNN and Daily Mail respectively.
RG-1 / RG-2 / RG-L
* 10k training samples : Gigaword 10k
MASS [Song et al., 2019]
25.03 / 9.48 / 23.48
UNILMLARGE [Dong et al., 2019]
32.96 / 14.68 / 30.56
ERNIE-GENBASE
33.75 / 15.23 / 31.35
ERNIE-GENLARGE
35.05 / 16.10 / 32.50
* Fully 3.8M training samples
MASS [Song et al., 2019]
37.66 / 18.53 / 34.89
BERTSHARE [Rothe et al., 2019]
38.13 / 19.81 / 35.62
UNILMLARGE [Dong et al., 2019]
38.45 / 19.45 / 35.75
PEGASUS(C4) [Zhang et al., 2019]
38.75 / 19.96 / 36.14
PEGASUS(HugeNews) [Zhang et al., 2019]
39.12 / 19.86 / 36.24
ERNIE-GENBASE
38.83 / 20.04 / 36.20
ERNIE-GENLARGE
39.25 / 20.25 / 36.53
Table 2: Comparison on Gigaword dataset with state-of-the-art results. Models in the upper block use 10k sample for ﬁne-tuning. We
also report the size of pre-training data and parameters utilized for
each listed model (columns 2-3). RG is short for ROUGE.
2We donate the number of layers as L, the hidden size as H and
the number of self-attention heads as A.
3 
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)
Learning Rate
Noising Rate ρf
Dropout Rate
Evaluation Metric
BLEU-4, METEOR (MTR), ROUGE-L (RG-L)
CNN/DailyMail
ROUGE-F1 scores:
ROUGE-1 (RG-1), ROUGE-2 (RG-2), ROUGE-L (RG-L)
Persona-Chat
BLEU-1, BLEU-2, Distinct-1, Distinct-2
Generative CoQA
Table 1: Hyperparamters of ﬁne-tuning for ERNIE-GENBASE and ERNIE-GENLARGE.
RG-1 / RG-2 / RG-L
BERTSHARE [Rothe et al., 2019]
39.25 / 18.09 / 36.45
BERTSUMABS [Liu and Lapata, 2019]
41.72 / 19.39 / 38.76
MASS [Song et al., 2019]
42.12 / 19.50 / 39.01
UNILMLARGE [Dong et al., 2019]
43.33 / 20.21 / 40.51
T5LARGE [Raffel et al., 2019]
42.50 / 20.68 / 39.75
T5XLARGE [Raffel et al., 2019]
43.52 / 21.55 / 40.69
BARTLARGE [Lewis et al., 2019]
44.16 / 21.28 / 40.90
PEGASUS(C4) [Zhang et al., 2019]
43.90 / 21.20 / 40.76
PEGASUS(HugeNews) [Zhang et al., 2019]
44.17 / 21.47 / 41.11
ERNIE-GENBASE
42.30 / 19.92 / 39.68
ERNIE-GENLARGE
44.02 / 21.17 / 41.26
Table 3: Evaluation results on CNN/DailyMail. C4 and HugeNews
are two massive datasets of 750G and 3.8T respectively.
The results on Gigaword task with two scales (10k and
3.8M) are presented in Table 2, and the ﬁne-tuning settings
are shown in Table 1. On the low-resource task (Gigaword
10k), ERNIE-GENLARGE yields a gain of +1.94 ROUGE-
L compared with UNILMLARGE. On the full training set,
ERNIE-GENLARGE creates the state-of-the-art results, outperforming various previous methods. Speciﬁcally, ERNIE-
GENBASE outperforms PEGASUS (568M and 750G) by using only 110M parameters and 16G training data.
Table 3 shows the performance on CNN/DailyMail. With a
similar amount of pre-training data and parameters, ERNIE-
GENBASE outperforms MASS by +0.67 ROUGE-L scores.
Fairly compared with UNILMLARGE, ERNIE-GENLARGE
obtains substantial gain of +0.73 ROUGE-L scores. Meanwhile, in spite of small pre-training data and parameters, our
large model also achieves state-of-the-art result on ROUGE-L
and comparable performance on ROUGE-1/2.
Question Generation
is to generate a question according
to a given input passage and a corresponding answer. We
evaluate on the SQuAD 1.1 dataset [Rajpurkar et al., 2016]
for question generation task (called SQuAD QG). Following UNILM, we redistribute the original dataset into a new
METEOR ROUGE-L
SemQG [Zhang and Bansal, 2019]
UNILMLARGE [Dong et al., 2019]
ERNIE-GENBASE (beam size = 1)
ERNIE-GENLARGE (beam size = 1)
ERNIE-GENLARGE (beam size = 5)
* Reversed test ↔dev split
MP-GSN [Zhao et al., 2018]
SemQG [Zhang and Bansal, 2019]
UNILMLARGE [Dong et al., 2019]
ERNIE-GENBASE (beam size = 1)
ERNIE-GENLARGE (beam size = 1)
ERNIE-GENLARGE (beam size = 5)
Table 4: Question generation results on SQuAD. Models in the upper block and the lower block use different test ↔dev split method.
Distinct-1/2
LIC [Bao et al., 2020]
40.5 / 32.0
0.019 / 0.113
PLATOw/o latent [Bao et al., 2020]
45.8 / 35.7
0.012 / 0.064
PLATO [Bao et al., 2020]
40.6 / 31.5
0.021 / 0.121
ERNIE-GENLARGE
46.8 / 36.4
0.023 / 0.168
Table 5: Comparison with state-of-the-art results on Persona-Chat.
training set and testing set with the original development set
unchanged. We also conduct experiment with the reversed
dev↔test split as [Zhao et al., 2018] indicates. In Table 4,
we present the results of ERNIE-GEN and several previous
works. Again, ERNIE-GEN outperforms UNILMLARGE and
achieves a new state-of-the-art result on question generation
by giving +1.82 BLEU-4 scores.
Generative Question Answering / Dialogue Response
multi-turn conversations are challenging because of complex
background knowledge and diverse utterances. We conduct
an experiment on Persona-Chat dataset [Zhang et al., 2018]
to generate responses according to given multi-turn conversations and persona proﬁle. Table 5 shows that ERNIE-GEN
outperforms current task-speciﬁc pre-training model on dialogue generation. Beside, we also execute an experiment on
CoQA dataset [Reddy et al., 2019] to generate free-form answers for input questions and conversations. As shown in
Table 6, our generative question answering model works considerably better than early works by +2.0 F1-scores.
Seq2Seq [Reddy et al., 2019]
PGNet [Reddy et al., 2019]
UNILMLARGE [Dong et al., 2019]
ERNIE-GENLARGE
Table 6: Generative question answering results on the development
set of CoQA.
Ablation Studies
To better understand the importance of each proposed generation methods, we conduct experiments concerning the following two aspects:
• The robustness of inﬁlling generation mechanism and
noise-aware generation method against the exposure bias.
• The effectiveness of span-by-span generation task and the
complete ERNIE-GEN model.
In Table 8, we compare two ERNIE-GENBASE variants
that are pre-trained with typical generation mechanism and
inﬁlling generation mechanism and that generate word by
Row 1-3 shows that without noising groundtruth
texts, inﬁlling generation outperforms typical generation
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)
Figure 5: Results of ablation studies. (a):Comparisons between typical generation and inﬁlling generation on Gigaword 10k and SQuAD QG
with different ﬁne-tuning noising rate ρf. (b):Noising Analysis, average attention weights of source words, unnoised target words and noised
target words for diverse ﬁne-tuning noising rate ρf. (c):Ablation study on Gigaword 10k, the x-axis shows ﬁne-tuning epochs.
# Fine-tuning method
1 Noising ﬁne-tuning: Fine-tuning with noise-aware generation
2 Masking ﬁne-tuning: Only updating the gradients of masked words
Gigaword 10k
CNN/DailyMail 10k
Gigaword 10k
CNN/DailyMail 10k
RG-1 / RG-2 / RG-L
RG-1 / RG-2 / RG-L
Bleu-4 / MTR / RG-L RG-1 / RG-2 / RG-L
RG-1 / RG-2 / RG-L
Bleu-4 / MTR / RG-L
1 ERNIE-GEN
33.75 / 15.23 / 31.35
39.92 / 17.46 / 37.40
23.52 / 25.61 / 51.45
33.30 / 15.04 / 31.22
39.54 / 17.61 / 37.00
22.99 / 25.14 / 51.31
- noise-aware
33.57 / 15.15 / 31.28
39.78 / 17.63 / 37.23
23.40 / 25.50 / 51.36
33.01 / 14.94 / 31.00
39.53 / 17.61 / 36.97
23.09 / 25.15 / 51.41
- span-by-span
33.43 / 15.04 / 31.14
39.75 / 17.62 / 37.21
23.37 / 25.56 / 51.32
32.97 / 14.92 / 30.94
39.54 / 17.57 / 36.95
23.10 / 25.14 / 51.42
33.23 / 14.77 / 31.00
39.71 / 17.55 / 37.18
23.34 / 25.54 / 51.30
32.57 / 14.68 / 30.60
39.49 / 17.66 / 36.96
22.89 / 25.08 / 51.28
Table 7: Ablation study for ERNIE-GENBASE and its variants. Particularly, We set ρp = 0.05 in pre-training (row 1), while removing the
span-by-span generation task (row 3), we set ρp =0.2 because the pre-training becomes easier.
# Task (Metrics)
Typical generation
Inﬁlling generation
Fine-tuning without noise-aware generation
1 Gigaword 10k (RG-1 / RG-2 / RG-L) 32.98 / 14.67 / 30.51 32.93 /14.46 / 30.53
2 CNN/DM 10k (RG-1 / RG-2 / RG-L) 39.25 / 16.70 / 36.65 39.56 / 16.93 / 36.94
3 SQuAD QG (Bleu-4 / MTR / RG-L)
21.95 / 24.53 / 50.34 22.13 / 24.66 / 50.51
Fine-tuning with noise-aware generation
4 Gigaword 10k (RG-1 / RG-2 / RG-L) 32.99 / 14.83 / 30.84 33.23 / 14.77 / 31.00
5 CNN/DM 10k (RG-1 / RG-2 / RG-L) 39.34 / 17.30 / 36.75 39.71 / 17.55 / 37.18
6 SQuAD QG (Bleu-4 / MTR / RG-L)
23.23 / 25.47 / 51.25 23.34 / 25.54 / 51.30
Table 8: Results of models pre-trained with typical generation and
inﬁlling generation. Tasks in the upper block are ﬁne-tuned without
noising, while the others are ﬁne-tuned with noise-aware generation.
across tasks.
Furthermore, both variants achieve remarkable improvements by ﬁne-tuning with noise-aware generation method (row 4-6). Speciﬁcally, Figure 5a shows the results with diverse choices of noising rate ρf on two tasks,
indicating that appropriate noising substantially beneﬁts the
training and alleviates the training-inference discrepancy. To
further analyze the excellence of inﬁlling generation mechanism with noising, we compute the average attention weights
of source tokens, unnoised target tokens and noised target tokens in the last self-attention layer respectively on 1,000 samples. Average attention weights with diverse noising rate ρf
are shown in Figure 5b, which tells us that the model pays
more attention on the decoder side to ﬁgure out noised points
and assign them less attention weights as the noising rate ρf
increased in ﬁne-tuning. Thereby, the model is able to detect
and ignore the false predictions properly to alleviate accumulating mistakes while inference.
In column 1 of Table 7, we compare four base size variants
on three tasks. We see that noise-aware generation method
and span-by-span generation task (rows 2-3 of Table 7) play
an important role in ERNIE-GEN pre-training and significantly outperform the baseline model which is only pretrained with word-by-word inﬁlling generation ﬂow (row 4
of Table 7). After integrating noise-aware generation method
and span-by-span generation task, ERNIE-GEN boosts the
performance across all three tasks, as shown in row 1 of Table
7. In addition, UNILM is ﬁne-tuned by masking words in the
encoder and decoder to predict, which is also a case of noising for generation. To verify the idea that ﬁne-tuning with
masking language modeling like UNILM is inefﬁcient due to
the coupling of masking (noising) and predicting that only
the masked (noised) position will be learned, we also list the
ﬁne-tuning results obtained by predicting masked words with
masking probability of 0.7, as shown in column 2 of Table 7.
We observe that our noise-aware generation method signiﬁcantly outperforms the mask language modeling in seq2seq
ﬁne-tuning by predicting all words in the decoder side.
Conclusions
We present an enhanced multi-ﬂow seq2seq pre-training and
ﬁne-tuning framework named ERNIE-GEN for language generation, which incorporates an inﬁlling generation mechanism and a noise-aware generation method to alleviate the exposure bias. Besides, ERNIE-GEN integrates a new span-byspan generation task to train the model to generate texts like
human writing, which further improves the performance on
downstream tasks. Through extensive experiments, ERNIE-
GEN achieves state-of-the-art results on a range of NLG tasks.
Future work includes incorporating reinforcement learning
into pre-training for exposure bias and applying ERNIE-GEN
to more NLG tasks such as machine translation.
Acknowledgments
This work was supported by the National Key Research and
Development Project of China (No. 2018AAA0101900).
Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)