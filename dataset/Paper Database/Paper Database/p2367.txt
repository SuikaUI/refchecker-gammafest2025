Image-based Recommendations on Styles and Substitutes
Julian McAuley∗1, Christopher Targett†2, Qinfeng (‘Javen’) Shi‡2, and Anton van den Hengel§2,3
1Department of Computer Science, UC San Diego
2School of Computer Science, University of Adelaide
3Australian Centre for Robot Vision
June 17, 2015
Humans inevitably develop a sense of the relationships between
objects, some of which are based on their appearance. Some
pairs of objects might be seen as being alternatives to each other
(such as two pairs of jeans), while others may be seen as being
complementary (such as a pair of jeans and a matching shirt).
This information guides many of the choices that people make,
from buying clothes to their interactions with each other. We
seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not
based on ﬁne-grained modeling of user annotations but rather
on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem
deﬁned on graphs of related images, and provide a large-scale
dataset for the training and evaluation of the same. The system
we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a
host of other applications.
Introduction
We are interested here in uncovering relationships between the
appearances of pairs of objects, and particularly in modeling
the human notion of which objects complement each other and
which might be seen as acceptable alternatives. We thus seek to
model what is a fundamentally human notion of the visual relationship between a pair of objects, rather than merely modeling
the visual similarity between them. There has been some interest of late in modeling the visual style of places , and
objects . We, in contrast, are not seeking to model the individual appearances of objects, but rather how the appearance
of one object might inﬂuence the desirable visual attributes of
There are a range of situations in which the appearance of
an object might have an impact on the desired appearance of
another. Questions such as ‘Which frame goes with this pic-
∗ 
† 
‡ 
§ 
Figure 1: A query image and a matching accessory, pants, and
ture’, ‘Where is the lid to this’, and ‘Which shirt matches these
shoes’ (see Figure 1) inherently involve a calculation of more
than just visual similarity, but rather a model of the higher-level
relationships between objects. The primary commercial application for such technology is in recommending items to a user
based on other items they have already showed interest in. Such
systems are of considerable economic value, and are typically
built by analysing meta-data, reviews, and previous purchasing patterns. By introducing into these systems the ability to
examine the appearance of the objects in question we aim to
overcome some of their limitations, including the ‘cold start’
problem .
The problem we pose inherently requires modeling human
visual preferences. In most cases there is no intrinsic connection between a pair of objects, only a human notion that they
are more suited to each other than are other potential partners.
The most common approach to modeling such human notions
exploits a set of hand-labeled images created for the task. The
labeling effort required means that most such datasets are typically relatively small, although there are a few notable exceptions. A small dataset means that complex procedures are required to extract as much information as possible without over-
ﬁtting (see for example). It also means that the results are unlikely to be transferable to related problems. Creating a labeled dataset is particularly onerous when modeling
pairwise distances because the number of annotations required
scales with the square of the number of elements.
We propose here instead that one might operate over a much
larger dataset, even if it is only tangentially related to the ultimate goal. Thus, rather than devising a process (or budget) for
manually annotating images, we instead seek a freely available
source of a large amount of data which may be more loosely
related to the information we seek. Large-scale databases have
been collected from the web (without other annotation) pre-
 
viously . What distinguishes the approach we propose
here, however, is the fact that it succeeds despite the indirectness of the connection between the dataset and the quantity we
hope to model.
A visual dataset of styles and substitutes
We have developed a dataset suitable for the purposes described
above based on the Amazon web store. The dataset contains
over 180 million relationships between a pool of almost 6 million objects. These relationships are a result of visiting Amazon
and recording the product recommendations that it provides
given our (apparent) interest in the subject of a particular web
page. The statistics of the dataset are shown in Table 1. An image and a category label are available for each object, as is the
set of users who reviewed it. We have made this dataset available for academic use, along with all code used in this paper
to ensure that our results are reproducible and extensible.1 We
label this the Styles and Substitutes dataset.
The recorded relationships describe two speciﬁc notions of
‘compatibility’ that are of interest, namely those of substitute
and complement goods. Substitute goods are those that can be
interchanged (such as one pair of pants for another), while complements are those that might be purchased together (such as a
pair of pants and a matching shirt) . Speciﬁcally, there are
4 categories of relationship represented in the dataset: 1) ‘users
who viewed X also viewed Y’ (65M edges); 2) ‘users who
viewed X eventually bought Y’ (7.3M edges); 3) ‘users who
bought X also bought Y’ (104M edges); and 4) ‘users bought
X and Y simultaneously’ (3.4M edges). Critically, categories
1 and 2 indicate (up to some noise) that two products may be
substitutable, while 3 and 4 indicate that two products may be
complementary. According to Amazon’s own tech report 
the above relationships are collected simply by ranking products according to the cosine similarity of the sets of users who
purchased/viewed them.
Note that the dataset does not document users’ preferences
for pairs of images, but rather Amazon’s estimate of the set of
relationships between pairs objects. The human notion of the
visual compatibility of these images is only one factor amongst
many which give rise to these estimated relationships, and it
is not a factor used by Amazon in creating them. We thus do
not wish to summarize the Amazon data, but rather to use what
it tells us about the images of related products to develop a
sense of which objects a human might feel are visually compatible. This is signiﬁcant because many of the relationships
between objects present in the data are not based on their appearance. People co-purchase hammers and nails due to their
functions, for example, not their appearances. Our hope is that
the non-visual decision factors will appear as uniformly distributed noise to a method which considers only appearance,
and that the visual decision factors might reinforce each other
to overcome the effect of this noise.
1 
Related work
The closest systems to what we propose above are contentbased recommender systems which attempt to model each
user’s preference toward particular types of goods. This is typically achieved by analyzing metadata from the user’s previous activities. This is as compared to collaborative recommendation approaches which match the user to proﬁles generated
based on the purchases/behavior of other users (see for
surveys). Combinations of the two have been shown
to help address the sparsity of the review data available, and
the cold-start problem (where new products don’t have reviews
and are thus invisible to the recommender system) .
The approach we propose here could also help address these
There are a range of services such as Jinni2 which promise
content-based recommendations for TV shows and similar media, but the features they expoit are based on reviews and metadata (such as cast, director etc.), and their ontology is handcrafted. The Netﬂix prize was a well publicized competition
to build a better personalized video recommender system, but
there again no actual image analysis is taking place . Hu et
al. describe a system for identifying a user’s style, and then
making clothing recommendations, but this is achieved through
analysis of ‘likes’ rather than visual features.
Content-based image retrieval gives rise to the problem of
bridging the ‘semantic-gap’ , which requires returning results which have similar semantic content to a search image,
even when the pixels bear no relationship to each other.
thus bears some similarity to the visual recommendation problem, as both require modeling a human preference which is not
satisﬁed by mere visual similarity. There are a variety of approaches to this problem, many of which seek a set of results
which are visually similar to the query and then separately ﬁnd
images depicting objects of the same class as those in the query
image; see , for example. Within the Information Retrieval community there has been considerable interest
of late in incorporating user data into image retrieval systems
 , for example through browsing and click-through behavior , or by making use of social tags . Also worth
mentioning with respect to image retrieval is , which also
considered using images crawled from Amazon, albeit for a
different task (similar-image search) than the one considered
There have been a variety of approaches to modeling human
notions of similarity between different types of images ,
forms of music , or even tweets , amongst other data
types. Beyond measuring similarity, there has also been work
on measuring more general notions of compatibility. Murillo
et al. , for instance, analyze photos of groups of people
collected from social media to identify which groups might be
more likely to socialize with each other, thus implying a distance measure between images. This is achieved by estimating
which of a manually-speciﬁed set of ‘urban tribes’ each group
belongs to, possibly because only 340 images were available.
Yamaguchi et al. capture a notion of visual style when
2 
25,875,237
51,276,522
Cell Phones & Accessories
Clothing, Shoes & Jewelry
25,361,968
16,508,162
Digital Music
Electronics
11,355,142
Grocery & Gourmet Food
Home & Kitchen
Movies & TV
Musical Instruments
Ofﬁce Products
Toys & Games
13,921,925
20,980,320
143,663,229
180,827,502
Table 1: The types of objects from a few categories in our dataset and the number of relationships between them.
parsing clothing, but do so by retrieving visually similar items
from a database.
This idea was extended by Kiapour et
al. to identify discriminating characteristics between different styles (hipster vs. goth for example). Di et al. also
identify aspects of style using a bag-of-words approach and
manual annotations.
A few other works that consider visual features speciﬁcally
for the task of clothing recommendation include . In
 and the authors build methods to parse complete out-
ﬁts from single images, in by building a carefully labeled
dataset of street images annotated by ‘fashionistas’, and in 
by building algorithms to automatically detect and segment
items from clothing images. In the authors propose an approach to learn relationships between clothing items and events
(e.g. birthday parties, funerals) in order to recommend eventappropriate items.
Although related to our approach, these
methods are designed for the speciﬁc task of clothing recommendation, requiring hand-crafted methods and carefully annotated data; in contrast our goal is to build a general-purpose
method to understand relationships between objects from large
volumes of unlabeled data. Although our setting is perhaps
most natural for categories like clothing images, we obtain surprisingly accurate performance when predicting relationships
in a variety of categories, from recommending outﬁts to predicting which books will be co-purchased based on their cover
In summary, our approach is distinct from the above in that
we aim to generalize the idea of a visual distance measure beyond measuring only similarity. Doing so demands a very large
amount of training data, and our reluctance for manual annotation necessitates a more opportunistic data collection strategy.
The scale of the data, and the fact that we don’t have control
over its acquisition, demands a suitably scalable and robust
modeling approach. The novelty in what we propose is thus
in the quantity we choose to model, the data we gather to do so,
and the method for extracting one from the other.
explanation
feature vector calculated from object image i
feature dimension (i.e., xi ∈RF )
a relationship between objects i and j
the set of relationships between all objects
parameterized distance between xi and xj
F × F Mahalanobis transform matrix
an F × K matrix, such that YYT = M
diagonal user-personalization matrix for user u
shifted sigmoid function with parameter c
R plus a random sample of non-relationships
training, validation, and test subsets of R∗
K-dimension embedding of xi into ‘style-space’
Table 2: Notation.
A visual and relational recommender system
We label the process we develop for exploiting this data a visual and relational recommender system as we aim to model
human visual preferences, and the system might be used to recommend one object on the basis of a user’s apparent interest
in another. The system shares these characteristics with more
common forms of recommender system, but does so on the basis of the appearance of the object, rather than metadata, reviews, or similar.
Our notation is deﬁned in Table 2.
We seek a method for representing the preferences of users
for the visual appearance of one object given that of another. A
number of suitable models might be devised for this purpose,
but very few of them will scale to the volume of data available.
For every object in the dataset we calculate an F-dimensional feature vector x ∈RF using a convolutional neural network as described in Section 2.3. The dataset contains a set R
of relationships where rij ∈R relates objects i and j. Each re-
σc(−d(i, j))
Figure 2: Shifted (and inverted) sigmoid with parameter c = 2.
lationship is of one of the four classes listed above. Our goal is
to learn a parameterized distance transform d(xi,xj) such that
feature vectors {xi,xj} for objects that are related (rij ∈R)
are assigned a lower distance than those that are not (rij /∈R).
Speciﬁcally, we seek d(·,·) such that P(rij ∈R) grows monotonically with −d(xi,xj).
Distances and probabilities: We use a shifted sigmoid function to relate distance to probability thus
P(rij ∈R) = σc(−d(xi,xj)) =
1 + ed(xi,xj)−c .
This is depicted in Figure 2. This decision allows us to cast
the problem as logistic regression, which we do for reasons
of scalability. Intuitively, if two items i and j have distance
d(xi,xj) = c, then they have probability 0.5 of being related;
the probability increases above 0.5 for d(xi,xj) < c, and decreases as d(xi,xj) > c. Note that we do not specify c in advance, but rather c is chosen to maximize prediction accuracy.
We now describe a set of potential distance functions.
Weighted nearest neighbor: Given that different feature dimensions are likely to be more important to different relationships, the simplest method we consider is to learn which feature
dimensions are relevant for a particular relationship. We thus
ﬁt a distance function of the form
dw(xi,xj) = ∥w ◦(xi −xj)∥2
where ◦is the Hadamard product.
Mahalanobis transform: (eq. 2) is limited to modeling the
visual similarity between objects, albeit with varying emphasis
per feature dimension. It is not expressive enough to model
subtler notions, such as which pairs of pants and shoes belong
to the same ‘style’, despite having different appearances. For
this we need to learn how different feature dimensions relate
to each other, i.e., how the features of a pair of pants might be
transformed to help identify a compatible pair of shoes.
To identify such a transformation, we relate image features via a Mahalanobis distance, which essentially generalizes (eq. 2) so that weights are deﬁned at the level of pairs of
features. Speciﬁcally we ﬁt
dM(xi,xj) = (xi −xj)M(xi −xj)T .
A full rank p.s.d. matrix M has too many parameters to ﬁt
tractably given the size of the dataset.
For example, using
features with dimension F = 212, learning a transform as in
(eq. 3) requires us to ﬁt approximately 8 million parameters;
not only would this be prone to overﬁtting, it is simply not practical for existing solvers.
To address these issues, and given the fact that M parameterises a Mahanalobis distance, we approximate M such that
M ≃YYT where Y is a matrix of dimension F × K. We
therefore deﬁne
dY(xi,xj) = (xi −xj)YYT (xi −xj)T
= ∥(xi −xj)Y∥2
Note that all distances (as well as their derivatives) can be computed in O(FK), which is signiﬁcant for the scalability of the
method. Similar ideas appear in , which also consider
the problem of metric learning via low-rank embeddings, albeit using a different objective than the one we consider here.
Style space
In addition to being computationally useful, the low-rank transform in (eq. 4) has a convenient interpretation. Speciﬁcally, if
we consider the K-dimensional vector si = xiY, then (eq. 4)
can be rewritten as
dY(xi,xj) = ∥si −sj∥2
In other words, (eq. 4) yields a low-dimensional embedding
of the features xi and xj. We refer to this low-dimensional
representation as the product’s embedding into ‘style-space’,
in the hope that we might identify Y such that related objects
fall close to each other despite being visually dissimilar. The
notion of ‘style’ is learned automatically by training the model
on pairs of objects which Amazon considers to be related.
Personalizing styles to individual users
So far we have developed a model to learn a global notion of
which products go together, by learning a notion of ‘style’ such
that related products should have similar styles. As an addition
to this model we can personalize this notion by learning for
each individual user which dimensions of style they consider
to be important.
To do so, we shall learn personalized distance functions
dY,u(xi, xj) that measure the distance between the items i and
j according to the user u. We choose the distance function
dY,u(xi, xj) = (xi −xj)YD(u)YT (xi −xj)T
where D(u) is a K ×K diagonal (positive semideﬁnite) matrix.
In this way the entry D(u)
kk indicates the extent to which the user
u ‘cares about’ the kth style dimension.
In practice we ﬁt a U × K matrix X such that D(u)
Much like the simpliﬁcation in (eq. 5), the distance
dY,u(xi, xj) can be conveniently written as
dY,u(xi, xj) = ∥(si −sj) ◦Xu∥2
In other words, Xu is a personalized weighting of the projected
style-space dimensions.
The construction in (eq. 6 and 7) only makes sense if there
are users associated with each edge in our dataset, which is not
true of the four graph types we have presented so far. Thus
to study the issue of user personalization we make use of our
rating and review data (see Table 1). From this we sample a
dataset of triples (i,j,u) of products i and j that were both purchased by user u (i.e., u reviewed them both). We describe this
further when we outline our experimental protocol in Section
Features are calculated from the original images using the Caffe
deep learning framework . In particular, we used a Caffe
reference model3 with 5 convolutional layers followed by 3
fully-connected layers, which has been pre-trained on 1.2 million ImageNet (ILSVRC2010) images. We use the output of
FC7, the second fully-connected layer, which results in a feature vector of length F = 4096.
Since we have deﬁned a probability associated with the presence (or absence) of each relationship, we can proceed by maximizing the likelihood of an observed relationship set R. In order to do so we randomly select a negative set Q = {rij|rij /∈
R} such that |Q| = |R| and optimize the log likelihood
l(Y,c|R, Q) =
log(σc(−dY(xi,xj)))+
log(1 −σc(−dY(xi,xj))).
Learning then proceeds by optimizing l(Y,c|R, Q) over both
Y and c which we achieve by gradient ascent. We use (hybrid)
L-BFGS, a quasi-Newton method for non-linear optimization
of problems with many variables . Likelihood (eq. 8) and
derivative computations can be na¨ıvely parallelized over all
pairs rij ∈R ∪Q. Training on our largest dataset (Amazon
books) with a rank K = 100 transform required around one
day on a 12 core machine.
Experiments
We compare our model against the following baselines:
We compare against Weighted Nearest Neighbor (WNN)
classiﬁcation, as is described in Section 1.3. We also compare
against a method we label Category Tree (CT); CT is based
on using Amazon’s detailed category tree directly (which we
have collected for Clothing data, and use for later experiments),
which allows us to assess how effective an image-based classi-
ﬁcation approach could be, if it were perfect. We then compute
a matrix of coocurrences between categories from the training
data, and label two products (a,b) as ‘related’ if the category
of b belongs to one of the top 50% of most commonly linked
categories for products of category a.4 Nearest neighbor results
3bvlc reference caffenet from caffe.berkeleyvision.org
4We experimented with several variations on this theme, and this approach
yielded the best performance.
(calculated by optimizing a threshold on the ℓ2 distance using
the training data) were not signiﬁcantly better than random, and
have been suppressed for brevity.
Comparison against non-visual baselines As a non-visual
comparison, we trained topic models on the reviews of each
product (i.e., each document di is the set of reviews of the product i) and ﬁt weighted nearest neighbor classiﬁers of the form
dw(θi, θj) = ∥w ◦(θi −θj)∥2
where θi and θj are topic vectors derived from the reviews of
the products i and j. In other words, we simply adapted our
WNN baseline to make use of topic vectors rather than image
features.5 We used a 100-dimensional topic model trained using Vowpal Wabbit .
However, this baseline proved not to be competitive against
the alternatives described above (e.g. only 60% accuracy on our
largest dataset, ‘Books’). One explanation may simply be that
is is difﬁcult to effectively train topic models at the 1M+ document scale; another explanation is simply that the vast majority
of products have few reviews. Not surprisingly, the number of
reviews per product follows a power-law, e.g. for Men’s Clothing:
number of reviews
Men’s clothing
This issue is in fact exacerbated in our setting, as to predict a
relationship between products we require both to have reliable
feature representations, which will be true only if both products
have several reviews.
Although we believe that predicting such relationships using
text is a promising direction of future research (and one we are
exploring), we simply wish to highlight the fact that there appears to be no ‘silver bullet’ to predict such relationships using
text, primarily due to the ‘cold start’ issue that arises due to
the long tail of obscure products with little text associated with
them. Indeed, this is a strong argument in favor of building
predictors based on visual features, since images are available
even for brand new products which are yet to receive even a
single review.
Experimental protocol
We split the dataset into its top-level categories (Books,
Movies, Music, etc.) and further split the Clothing category
into second-level categories (Men’s, Women’s, Boys, Girls,
etc.). We focus on results from a few representative subcategories. Complete code for all experiments and all baselines is
available online.6
5We tried the same approach at the word (rather than the topic) level, though
this led to slightly worse results.
6 
K = 10, no personalization
K = 10, personalized
K = 10, no personalization
K = 10, personalized
Table 3: Performance of our model at predicting copurchases
with a user personalization term (eqs. 6 and 7).
For each category, we consider the subset of relationships
from R that connect products within that category. After generating random samples of non-relationships, we separate R
and Q into training, validation, and test sets (80/10/10%, up to
a maximum of two million training relationships). Although
we do not ﬁt hyperparameters (and therefore do not make use
of the validation set), we maintain this split in case it proves
useful to those wishing to benchmark their algorithms on this
data. While we did experiment with simple ℓ2 regularizers, we
found ourselves blessed with a sufﬁcient overabundance of data
that overﬁtting never presented an issue (i.e., the validation error was rarely signiﬁcantly higher than the training error).
To be completely clear, our protocol consists of the following:
1. Each category and graph type forms a single experiment
(e.g. predict ‘bought together’ relationships for Women’s
clothing).
2. Our goal is to distinguish relationships from non-relationships (i.e., link prediction). Relationships are identiﬁed
when our predictor (eq. 1) outputs P(rij ∈R) > 0.5.
3. We consider all positive relationships and a random sample of non-relationships (i.e., ‘distractors’) of equal size.
Thus the performance of a random classiﬁer is 50% for all
experiments.
4. All results are reported on the test set.
Results on a selection of top-level categories are shown in
Table 4, with further results for clothing data shown in Table
5. Recall when interpreting these results that the learned model
has reference to the object images only. It is thus estimating the
existence of a speciﬁed form of relationship purely on the basis
of appearance.
In every case the proposed method outperforms both the
category-based method and weighted nearest neighbor, and the
increase from K = 10 to K = 100 uniformly improves performance. Interestingly, the performance on compliments vs.
substitutes is approximately the same. The extent to which the
K = 100 results improve upon the WNN results may be seen as
an indication of the degree to which visual similarity between
images fails to capture a more complex human visual notion
Figure 3: Examples of closely-clustered items in style space
(Men’s and Women’s clothing ‘also viewed’ data).
of which objects might be seen as being substitutes or compliments for each other. This distinction is smallest for ‘Books’
and greatest for ‘Clothing Shoes and Jewelery’ as might be expected.
We have no ground truth relating the true human visual preference for pairs of objects, of course, and thus evaluate above
against our dataset. This has the disadvantage that the dataset
contains all of the Amazon recommendations, rather than just
those based on decisions made by humans on the basis of object appearance. This means that in addition to documenting
the performance of the proposed method, the results may also
be taken to indicate the extent to which visual factors impact
upon the decisions of Amazon customers.
The comparison
across categories is particularly interesting. It is to be expected
that appearance would be a signiﬁcant factor in Clothing decisions, but it was not expected that they would be a factor in the
Figure 4: A selection of widely separated members of a single K-means cluster, demonstrating an apparent stylistic coherence.
Figure 5: Examples of K-means clusters in style space (Books
‘also viewed’ and ‘also bought’ data). Although ‘styles’ for
categories like books are not so readily interpretable as they
are for clothes, visual features are nevertheless able to uncover
meaningful distinctions between different product categories,
e.g. the ﬁrst four rows above above appear to be children’s
books, self-help books, romance novels, and graphic novels.
purchase of Books. One possible interpretation of this effect
might be that customers have preferences for particular genres
of books and that individual genres have characteristic styles of
Personalized recommendations
Finally we evaluate the ability of our model to personalize copurchasing recommendations to individual users, that is we examine the effect of the user personalization term in (eqs. 6
and 7). Here we do not use the graphs from Tables 4 and 5,
since those are ‘population level’ graphs which are not annotated in terms of the individual users who co-purchased and cobrowsed each pair of products. Instead for this task we build a
dataset of co-purchases from products that users have reviewed.
That is, we build a dataset of tuples of the form (i,j,u) for
pairs of products i and j that were purchased by user u. We
train on users with at least 20 purchases, and randomly sample 50 co-purchases and 50 non-co-purchases from each user
in order to build a balanced dataset. Results are shown in Table 3; here we see that the addition of a user personalization
term yields a small but signiﬁcant improvement when predicting co-purchases (similar results on other categories withheld
for brevity).
Figure 6: Navigating to distant products: each column shows a
low-cost path between two objects such that adjacent products
in the path are visually consistent, even when the end points are
Figure 7: A 2-dimensional embedding of a small sample of
Boys clothing images (‘also viewed’ data).
Visualizing Style Space
Recall that each image is projected into ‘style-space’ by the
transformation si = xiY, and note that the fact that it is based
on pairwise distances alone means that the embedding is invariant under isomorphism. That is, applying rotations, translations, or reﬂections to si and sj will preserve their distance
in (eq. 5). In light of these factors we perform k-means clustering on the K dimensional embedded coordinates of the data in
order to visualize the effect of the embedding.
Figure 3 shows images whose projections are close to the
centers of a set of selected representative clusters for Men’s and
Women’s clothing (using a model trained on the ‘also viewed’
graph with K = 100). Naturally items cluster around colors
and shapes (e.g. shoes, t-shirts, tank tops, watches, jewelery),
but more subtle characterizations exist as well. For instance,
leather boots are separated from ugg (that is sheep skin) boots,
despite the fact that the visual differences are subtle. This is
presumably because these items are preferred by different sets
of Amazon users. Watches cluster into different color proﬁles,
face shapes, and digital versus analogue. Other clusters cross
multiple categories, for instance we ﬁnd clusters of highlycolorful items, items containing love hearts, and items containing animals. Figure 4 shows a set of images which project to
locations that span a cluster.
Although performance is admittedly not outstanding for a
Figure 8: Outﬁts generated by our algorithm (Women’s outﬁts
at left; Men’s outﬁts at right). The ﬁrst column shows a ‘query’
item that is randomly selected from the product catalogue. The
right three columns match the query item with a top, pants,
shoes, and an accessory, (minus whichever category contains
the query item).
category such as books, it is somewhat surprising that an accuracy of even 70% can be achieved when predicting book
co-purchases.
Figure 5 visualizes a few examples of stylespace clusters derived from Books data. Here it seems that
there is at least some meaningful information in the cover of a
book to predict which products might be purchased together—
children’s books, self-help books, romance novels, and comics
(for example) all seem to have characteristic visual features
which are identiﬁed by our model.
In Figure 6 we show how our model can be used to navigate
between related items—here we randomly select two items that
are unlikely to be co-browsed, and ﬁnd a low cost path between
them as measured by our learned distance measure. Subjectively, the model identiﬁes visually smooth transitions between
the source and the target items.
Figure 7 provides a visualization of the embedding of Boys
clothing achieved by setting K = 2 (on co-browsing data).
Sporting shoes drift smoothly toward slippers and sandals, and
underwear drifts gradually toward shirts and coats.
Generating Recommendations
We here demonstrate that the proposed model can be used to
generate recommendations that might be useful to a user of a
web store. Given a query item (e.g. a product a user is currently
browsing, or has just purchased), our goal is to recommend a
selection of other items that might complement it. For example,
if a user is browsing pants, we might want to recommend a
shirt, shoes, or accessories that belong to the same style.
Here, Amazon’s rich and detailed category hierarchy can
help us. For categories such as women’s or men’s clothing,
we might deﬁne an ‘outﬁt’ as a combination of pants, a top,
shoes, and an accessory (we do this for the sake of demonstration, though far more complex combinations are possible—our
category tree for clothing alone has hundreds of nodes). Then,
given a query item our goal is simply to select items from each
of these categories that are most likely to be connected based
on their visual style.
Speciﬁcally, given a query item xq, for each category C (represented as a set of item indices), we generate recommendations according to
PY(rqj ∈R),
i.e., the minimum distance according to our measure (eq. 4)
amongst objects belonging to the desired category. Examples
of such recommendations are shown in Figures 1 and 8, with
randomly chosen queries from women’s and men’s clothing.
Generally speaking the model produces apparently reasonable
recommendations, with clothes in each category usually being
of a consistent style.
Outﬁts in The Wild
An alternate application of the model is to make assessments
about outﬁts (or otherwise combinations of items) that we observe ‘in the wild’. That is, to the extent that the tastes and
preferences of Amazon customers reﬂect the zeitgeist of society at large, this can be seen as a measurement of whether a
candidate outﬁt is well coordinated visually.
To assess this possibility, we have built two small datasets
of real outﬁts, one consisting of twenty-ﬁve outﬁts worn by
the hosts of Top Gear (Jeremy Clarkson, Richard Hammond,
and James May), and another consisting of seventeen ‘before’
and ‘after’ pairs of outﬁts from participants on the television
show What Not to Wear (US seasons 9 and 10). For each out-
ﬁt, we cropped each clothing item from the image, and then
used Google’s reverse image search to identify images of similar items (examples are shown in Figure 9).
Next we rank outﬁts according to the average log-likelihood
of their pairs of components being related using a model trained
on Men’s/Women’s co-purchases (we take the average so that
there is no bias toward outﬁts with more or fewer components).
All outﬁts have at least two items.7 Figure 9 shows the most
and least coordinated outﬁts on Top Gear; here we ﬁnd considerable separation between the level of coordination for each
presenter; Richard Hammond is typically the least coordinated,
James May the most, while Jeremy Clarkson wears a combination of highly coordinated and highly uncoordinated outﬁts.
A slightly more quantitative evaluation comes from the television show What Not to Wear: here participants receive an
‘outﬁt makeover’, hopefully meaning that their made-over out-
ﬁt is more coordinated than the original. Examples of participants before and after their makeover, along with the change
in log likelihood are shown in Figure 10. Indeed we ﬁnd that
made-over outﬁts have a higher log likelihood in 12 of the 17
cases we observed (p ≃7%; log-likelihoods are normalized to
7Our measure of coordination is thus undeﬁned for a subject wearing only
a single item, though in general such an outﬁt would be a poor fashion choice
in the opinion of the authors.
Figure 9: Least (top) and most (bottom) coordinated outﬁts from our Top Gear dataset. Richard Hammond’s outﬁts typically
have low coordination, James May’s have high coordination, and Jeremy Clarkson straddles both ends of the coordination
spectrum. Pairwise distances are normalized by the number of components in the outﬁt so that there is no bias towards outﬁts
with fewer/more components.
correct any potential bias due to the number of components in
the outﬁt). This is an important result, as it provides external
(albeit small) validation of the learned model which is independent of our dataset.
Conclusion
We have shown that it is possible to model the human notion
of what is visually related by investigation of a suitably large
dataset, even where that information is somewhat tangentially
contained therein. We have also demonstrated that the proposed
method is capable of modeling a variety of visual relationships
beyond simple visual similarity. Perhaps what distinguishes
our method most is thus its ability to model what makes items
complementary. To our knowledge this is the ﬁrst attempt to
model human preference for the appearance of one object given
that of another in terms of more than just the visual similarity
between the two. It is almost certainly the ﬁrst time that it has
been attempted directly and at this scale.
We also proposed visual and relational recommender systems as a potential problem of interest to the information retrieval community, and provided a large dataset for their training and evaluation. In the process we managed to ﬁgure out
what not to wear, how to judge a book by its cover, and to show
that James May is more fashionable than Richard Hammond.
Acknowledgements. This research was supported by the Data 2 Decisions Cooperative Research Centre, and the Australian Research
Council Discovery Projects funding scheme DP140102270.