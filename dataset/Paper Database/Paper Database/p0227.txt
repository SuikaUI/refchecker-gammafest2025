Published as a conference paper at ICLR 2019
OPTIMAL COMPLETION DISTILLATION
FOR SEQUENCE LEARNING
Sara Sabour, William Chan, Mohammad Norouzi
{sasabour, williamchan, mnorouzi}@google.com
Google Brain
We present Optimal Completion Distillation (OCD), a training procedure for
optimizing sequence to sequence models based on edit distance. OCD is efﬁcient,
has no hyper-parameters of its own, and does not require pretraining or joint
optimization with conditional log-likelihood. Given a partial sequence generated
by the model, we ﬁrst identify the set of optimal sufﬁxes that minimize the total
edit distance, using an efﬁcient dynamic programming algorithm. Then, for each
position of the generated sequence, we deﬁne a target distribution that puts an equal
probability on the ﬁrst token of each optimal sufﬁx. OCD achieves the state-of-theart performance on end-to-end speech recognition, on both Wall Street Journal and
Librispeech datasets, achieving 9.3% and 4.5% word error rates, respectively.
INTRODUCTION
Recent advances in natural language processing and speech recognition hinge on the development
of expressive neural network architectures for sequence to sequence (seq2seq) learning . Such encoder-decoder architectures are adopted in both machine
translation and speech recognition
systems achieving impressive performance above traditional multi-stage pipelines . Improving
the building blocks of seq2seq models can fundamentally advance machine translation and speech
recognition, and positively impact other domains such as image captioning , parsing
 , summarization , and program synthesis .
To improve the key components of seq2seq models, one can either design better architectures, or
develop better learning algorithms. Recent architectures using convolution 
and self attention have proved to be useful, especially to facilitate efﬁcient
training. On the other hand, despite many attempts to mitigate the limitations of Maximum Likelihood
Estimation (MLE) , MLE is still considered the dominant approach for training seq2seq
models. Current alternative approaches require pre-training or joint optimization with conditional
log-likelihood. They are difﬁcult to implement and require careful tuning of new hyper-parameters
(e.g. mixing ratios). In addition, alternative approaches typically do not offer a substantial performance
improvement over a well tuned MLE baseline, especially when label smoothing and scheduled sampling are used.
In this paper, we borrow ideas from search-based structured prediction and policy distillation and develop an efﬁcient algorithm for optimizing
seq2seq models based on edit distance1. Our key observation is that given an arbitrary preﬁx (e.g. a
partial sequence generated by sampling from the model), we can exactly and efﬁciently identify all of
the sufﬁxes that result in a minimum total edit distance (v.s. the ground truth target). Our training
procedure, called Optimal Completion Distillation (OCD), is summarized as follows:
1. We always train on preﬁxes generated by sampling from the model that is being optimized.
2. For each generated preﬁx, we identify all of the optimal sufﬁxes that result in a minimum total
edit distance v.s. the ground truth target using an efﬁcient dynamic programming algorithm.
3. We teach the model to optimally extend each generated preﬁx by maximizing the average log
probability of the ﬁrst token of each optimal sufﬁx identiﬁed in step 2.
1Edit distance between two sequences u and v is the minimum number of insertion, deletion, and substitution
edits required to convert u to v and vice versa.
 
Published as a conference paper at ICLR 2019
The proposed OCD algorithm is efﬁcient, straightforward to implement, and has no tunable hyperparameters of its own. Our key contributions include:
• We propose OCD, a stand-alone algorithm for optimizing seq2seq models based on edit distance. OCD is scalable to real-world datasets with long sequences and large vocabularies, and
consistently outperforms Maximum Likelihood Estimation (MLE) by a large margin.
• Given a target sequence of length m and a generated sequence of length n, we present an O(nm)
algorithm that identiﬁes all of the optimal extensions for each preﬁx of the generated sequence.
• We demonstrate the effectiveness of OCD on end-to-end speech recognition using attentionbased seq2seq models. On the Wall Street Journal dataset, OCD achieves a Character Error
Rate (CER) of 3.1% and a Word Error Rate (WER) of 9.3% without language model rescoring,
outperforming all prior work (Table 4). On Librispeech, OCD achieves state-of-the-art WER of
4.5% on “test-clean” and 13.3% on “test-other” sets (Table 5).
BACKGROUND: SEQUENCE LEARNING WITH MLE
Given a dataset of input output pairs D ≡{(x, y∗)i}N
i=1, we are interested in learning a mapping
x →y from an input x to a target output sequence y∗∈Y. Let Y denote the set of all sequences of
tokens from a ﬁnite vocabulary V with variable but ﬁnite lengths. Often learning a mapping x →y
is formulated as optimizing the parameters of a conditional distribution pθ(y | x). Then, the ﬁnal
sequence prediction under the probabilistic model pθ is performed by exact or approximate inference
(e.g. via beam search) as:
ˆy ≈argmax y∈Y pθ(y | x) .
Similar to the use of log loss for supervised classiﬁcation, the standard approach to optimize the
parameters θ of the conditional probabilistic model entails maximizing a conditional log-likelihood
objective, OMLE(θ) = E(x,y∗)∼pD log pθ(y∗| x). This approach to learning the parameters is called
Maximum Likelihood Estimation (MLE) and is commonly used in sequence to sequence learning.
Sutskever et al. propose the use of recurrent neural networks (RNNs) for autoregressive
seq2seq modeling to tractably optimize OMLE(θ). An autoregressive model estimates the conditional
probability of the target sequence given the source one token at a time, often from left-to-right. A
special end-of-sequence token is appended at the end of all of target sequences to handle variable
length. The conditional probability of y∗given x is decomposed via the chain rule as,
pθ(y∗| x) ≡
t=1 pθ,t(y∗
1, . . . , y∗
t−1) denotes a preﬁx of the sequence y∗. To estimate the probability of
a token a given a preﬁx y∗
<t and an input x, denoted pθ,t(a | y∗
<t, x), different architectures have
been proposed. Some papers ) have investigated the use of LSTM and GRU cells, while others proposed new architecturs based on soft attention , convolution , and
self-attention . Nonetheless, all of these techniques rely on MLE for learning,
OMLE(θ) = E(x,y∗)∼pD
t=1 log pθ,t(y∗
where pD denotes the empirical data distribution, uniform across the dataset D. We present a new
objective function for optimizing autoregressive seq2seq models applicable to any neural architecture.
LIMITATIONS OF MLE FOR AUTOREGRESSIVE MODELS
In order to maximize the conditional log-likelihood (3) of an autoregressive seq2seq model (2), one
provides the model with a preﬁx of t −1 tokens from the ground truth target sequence, denoted y∗
and maximizes the log-probability of y∗
t as the next token. This resembles a teacher walking a student
through a sequence of perfect decisions, where the student learns as a passive observer. However,
during inference one uses beam search (1), wherein the student needs to generate each token ˆyt by
conditioning on its own previous outputs, i.e. ˆy<t instead of y∗
<t. This creates a discrepancy between
training and test known as exposure bias . Appendix B expands this further.
Concretely, we highlight two limitations with the use of MLE for autoregressive seq2seq modeling:
Published as a conference paper at ICLR 2019
1. There is a mismatch between the preﬁxes seen by the model during training and inference.
When the distribution of ˆy<t is different from the distribution of y∗
<t, then the student will
ﬁnd themselves in a novel situation that they have not been trained for. This can result in poor
generalization, especially when the training set is small or the model size is large.
2. There is a mismatch between the training loss and the task evaluation metric. During training,
one optimizes the log-probability of the ground truth output sequence, which is often different
from the task evaluation metric (e.g. edit distance for speech recognition).
There has been a recent surge of interest in understanding and mitigating the limitations of MLE for
autoregressive seq2seq modeling. In Section 4 we discuss prior work in detail after presenting our
approach below.
OPTIMAL COMPLETION DISTILLATION
To alleviate the mismatch between inference and training, we never train on ground truth target
sequences. Instead, we always train on sequences generated by sampling from the current model
that is being optimized. Let ey denote a sequence generated by sampling from the current model,
and y∗denote the ground truth target. Applying MLE to autoregressive models casts the problem of
sequence learning as optimizing a mapping (x, y∗
t from ground truth preﬁxes to correct next
tokens. By contrast, the key question that arises when training on model samples is the choice of
targets for learning a similar mapping (x, ey<t) →?? from generated preﬁxes to next tokens. Instead
of using a set of pre-speciﬁed targets, OCD solves a preﬁx-speciﬁc problem to ﬁnd optimal extensions
that lead to the best completions according to the task evaluation metric. Then, OCD encourages the
model to extend each preﬁx with the set of optimal choices for the next token.
Our notion of optimal completion depends on the task evaluation metric denoted R(·, ·), which
measures the similarity between two complete sequences, e.g. the ground truth target v.s. a generated
sequence. Edit distance is a common task metric. Our goal in sequence learning is to train a
model, which achieves high scores of R(y∗, ey). Drawing connection with the goal of reinforcement
learning , let us recall the notion of optimal Q-values. Optimal Q-values
for a state-action pair (s, a), denoted Q∗(s, a), represent the maximum future reward that an agent
can accumulate after taking an action a at a state s by following with optimal subsequent actions.
Similarly, we deﬁne Q-values for a preﬁx ey<t and the extending token a, as the maximum score
attainable by concatenating [ey<t, a] with an optimal sufﬁx y to create a full sequence [ey<t, a, y].
Q∗(ey<t, a) = max
y∈Y R(y∗, [ey<t, a, y]) .
Then, the optimal extension for a preﬁx ey<t can be deﬁned as tokens that attain the maximal Q-values,
i.e. argmaxaQ∗(ey<t, a). This formulation allows for a preﬁx ey<t to be sampled on-policy from the
model pθ, or drawn off-policy in any way. Table 1 includes an example ground truth target from the
Wall Street Journal dataset and the corresponding generated sample from a model. We illustrate that
for some preﬁxes there exist more than a single optimal extension leading to the same edit distance.
Given Q-values for our preﬁx-token pairs, we use an exponential transform followed by normalization
to convert Q-values to a soft optimal policy over the next token extension,
π∗(a | ey<t) =
exp(Q∗(ey<t, a)/τ)
a′ exp (Q∗(ey<t, a′)/τ) ,
where τ ≥0 is a temperature parameter. Note the similarity of τ and the label smoothing parameter
helpful within MLE. In our experiments, we used the limit of τ →0 resulting in hard targets and no
hyper-parameter tuning.
Given a training example (x, y∗), we ﬁrst draw a full sequence ey ∼pθ(· | x) i.i.d. from the current
model, and then minimize a per-step KL divergence between the optimal policy and the model
distribution over the next token extension at each time step t. The OCD objective is expressed as,
E(x,y∗)∼pD Eey∼pθ(·|x)
t=1 KL (π∗(· | ey<t) ∥pθ,t(· | ey<t, x)) .
For every preﬁx ey<t, we compute the optimal Q-values and use (5) to construct the optimal policy
distribution π∗. Then, we distill the knowledge of the optimal policy for each preﬁx ey<t into the
parametric model using a KL loss. For the important class of sequence learning problems where edit
Published as a conference paper at ICLR 2019
Target sequence y∗
a s _ h e _ t a l k s _ h i s _ w i f e
Generated sequence ey
a s _ e e _ t a l k s _ w h o s e _ w i f e
Optimal extensions for
a s _ h e _ t a l k s _ h i i s _ _ w i f e
edit distance (OCD
Table 1: A sample sequence y∗from the Wall Street Journal dataset, where the model’s prediction ey
is not perfect. The optimal next characters for each preﬁx of ey based on edit distance are shown in
blue. For example, for the preﬁx “as_e” there are 3 optimal next characters of “e”, “h”, and “_”. All
of these 3 characters when combined with proper sufﬁxes will result in a total edit distance of 1.
distance is the evaluation metric, we develop a dynamic programming algorithm to calculate optimal
Q-values exactly and efﬁciently for all preﬁxes of a sequence ey, discussed below.
OPTIMAL Q-VALUES FOR EDIT DISTANCE
We propose a dynamic programming algorithm to calculate optimal Q-values exactly and efﬁciently
for the reward metric of negative edit distance, i.e. R(y∗, ey) = −Dedit(y∗, ey). Given two sequences
y∗and ey, we compute the Q-values for every preﬁx ey<t and any extending token a ∈V with an
asymptotic complexity of O(|y∗|.|ey| + |V|.|ey|). Assuming that |y∗| ≈|ey| ≤|V|, our algorithm does
not increase the time complexity over MLE, since computing the cross-entropy losses in MLE also
requires a complexity of O(|y∗|.|V|). When this assumption does not hold, e.g. genetic applications,
OCD is less efﬁcient than MLE. However, in practice, the wall clock time is dominated by the forward
and backward passes of a neural networks, and the OCD cost is often negligible. We discuss the
efﬁciency of OCD further in Appendix A.
Recall the Levenshtein algorithm for calculating the minimum number of edits
(insertion, deletion and substitution) required to convert sequences ey and y∗to each other based on,
Dedit(ey<−1, :) = ∞
Dedit(:, y∗
Dedit(ey<0, y∗
Dedit(ey<i, y∗
Dedit(ey<i−1, y∗
Dedit(ey<i, y∗
Dedit(ey<i−1, y∗
<j−1) + 1[˜yi ̸= y∗
Table 2 shows an example edit distance table for sequences “Satrapy” and “Sunday”. Our goal is to
identify the set of all optimal sufﬁxes y ∈Y that result in a full sequences [ey<i, y] with a minimum
edit distance v.s. y∗.
Lemma 1. The edit distance resulting from any potential sufﬁx y ∈Y is lower bounded by mi,
∀y ∈Y, Dedit([ey<i, y], y∗) ≥
0≤j≤|y∗| Dedit(ey<i, y∗
<j) = mi .
Proof. Let’s consider the path P that traces Dedit([ey<i, y], y∗) back to Dedit(ey<0, y∗
<0) connecting
each cell to an adjacent parent cell, which provides the minimum value among the three options
in (7). Such a path for tracing edit distance between “Satrapy” and “Sunday” is shown in Table 2.
Table 2: Each row corresponds to a preﬁx of “SATRAPY” and shows edit distances with all preﬁxes
of “SUNDAY”. We also show OCD targets (optimal extensions) for each preﬁx, and minimum value
along each row, denoted mi (see (8)). We highlight the trace path for Dedit(“Satrapy”, “Sunday”).
Edit Distance Table
OCD Targets
U, N, D, A
Published as a conference paper at ICLR 2019
Suppose the path P crosses row i at a cell (i, k). Since the operations in (7) are non-decreasing, the
edit distance along the path cannot decrease, so Dedit([ey<i, y], y∗) ≥Dedit(ey<i, y∗
Then, consider any k such that Dedit(ey<i, y∗
<k) = mi. Let y∗
k, . . . , y∗
|y∗|) denote a sufﬁx of
y∗. We conclude that Dedit([ey<i, y∗
≥k], y∗) = mi, because on the one hand there is a particular edit
path that results in mi edits, and on the other hand mi is a lower bound according to Lemma 1. Hence
any such y∗
≥k is an optimal sufﬁx for ey<i. Further, it is straightforward to prove by contradiction that
the set of optimal sufﬁxes is limited to sufﬁxes y∗
≥k corresponding to Dedit(ey<i, y∗
Since the set of optimal completions for ey<i is limited to y∗
≥k, the only extensions that can lead to
maximum reward are the starting token of such sufﬁxes (y∗
k). Since Dedit(ey<i, y∗
<k) = mi as well,
we can identify the optimal extensions by calculating the edit distances between all preﬁxes of ey
and all preﬁxes of y∗which can be efﬁciently calculated by dynamic programming in O(|ey|.|y∗|).
For a preﬁx ey<i after we calculate the minimum edit distance mi among all preﬁxes of y∗, we set
the Q∗(ey<i, y∗
k) = −mi for all k where y∗
<k has edit distance equal to mi. We set the Q∗for any
other token to −mi −1. We provide the details of our modiﬁed Levenshtein algorithm to efﬁciently
compute the Q∗(ey<i, a) for all i and a in Appendix A.
RELATED WORK
Our work builds upon Learning to Search and Imitation Learning
techniques , where a student policy is
optimized to imitate an expert teacher. DAgger in particular is closely related,
where a dataset of trajectories from an expert teacher is aggregated with samples from past student
models, and a policy is optimized to mimic a given expert policy π∗at various states. Similarly,
OCD aims to mimic an optimal policy π∗at all preﬁxes, but in OCD, the behavior policy is directly
obtained from an online student. Further, the oracle policy is not provided during training, and we
obtain the optimal policy by ﬁnding optimal Q-values. AggreVaTeD assumes
access to an unbiased estimate of Q-values and relies on variance reduction techniques and conjugate
gradients to address a policy optimization problem. OCD calculates exact Q-values and uses regular
SGD for optimization. Importantly, our roll-in preﬁxes are drawn only from the student model, and
we do not require mixing in ground truth (a.k.a. expert) samples. Cheng and Boots showed
that mixing in ground truth samples is an essential regularizer for value aggregation convergence in
imitation learning.
Our work is closely related to Policy Distillation , where a Deep Q-Network
(DQN) agent that is previously optimized is used as the expert teacher. Then,
action sequences are sampled from the teacher and the learned Q-value estimates are distilled into a smaller student network using a KL loss. OCD adopts a similar loss function,
but rather than estimating Q-values using bootstrapping, we estimate exact Q-values using dynamic
programming. Moreover, we draw samples from the student rather than the teacher.
Similar to OCD, the learning to search (L2S) techniques such as LOLS and
Goodman et al. also attempt to estimate the Q-values for each state-action pair. Such techniques
examine multiple roll-outs of a generated preﬁx and aggregate the return values. SeaRNN approximates the cost-to-go for each token by computing the task loss for as many
roll-outs as the vocabulary size at each time step with a per step complexity of O(V T). It is often
difﬁcult to scale approaches based on multiple roll-outs to real world datasets, where either the
sequences are long or the vocabulary is large. OCD exploits the special structure in edit distance
and ﬁnd exact Q-values efﬁciently in O(V + T) per step. Unlike L2S and SeaRNN, which require
ground truth preﬁxes to stabilize training, we solely train on model samples.
Approaches based on Reinforcement Learning (RL) have also been applied to sequence prediction
problems, including REINFORCE , Actor-Critic and
Self-critical Sequence Training . These methods sample sequences from the
model’s distribution and backpropagate a sequence-level task objective (e.g. edit distance). Beam
Search Optimization and Edit-based Minimum Bayes Risk (EMBR) is similar, but the sampling procedure is replaced with beam search. These
training methods suffer from high variances and credit assignment problems. By contrast, OCD takes
advantage of the decomposition of the sequence-level objective into token level optimal completion
targets. This reduces the variance of the gradient and stabilizes the model. Crucially, unlike most
Published as a conference paper at ICLR 2019
Figure 1: Fraction of OCD training preﬁx tokens
on WSJ which does not match ground truth.
Figure 2: WSJ validation Character Error Rate
(CER) per training CER for MLE and OCD.
RL-based approaches, we neither need MLE pretraining or joint optimization with log-likelihood.
Bahdanau et al. also noticed some of the nice structure of edit distance, but they optimize the
model by regressing its outputs to edit distance values leading to suboptimal performance. Rather,
we ﬁrst construct the optimal policy and then use knowledge distillation for training. Independently,
Karita et al. also decomposed edit distance into the contribution of individual tokens and used
this decomposition within the EMBR framework. That said, Karita et al. do not theoretically
justify this particular choice of decomposition and report high variance in their gradient estimates.
Reward Augmented Maximum Likelihood (RAML) and its variants are also similiar to RL-based approaches. Instead
of sampling from the model’s distribution, RAML samples sequences from the true exponentiated
reward distribution. However, sampling from the true distribution is often difﬁcult and intractable.
RAML suffers from the same problems as RL-based methods in credit assignment. SPG changes the policy gradient formulation to sample from a reward shaped model
distribution. Therefore, its samples are closer than RAML to the model’s samples. In order to
facilitate sampling from their proposed distribution SPG provides a heuristic to decompose ROUGE
score. Although SPG has a lower variance due to their biased samples, it suffers from the same
problems as RAML and RL-based methods in credit assignment.
Generally, OCD excels at training from scratch, which makes it an ideal substitution for MLE. Hence,
OCD is orthogonal to methods which require MLE pretraining or joint optimization.
EXPERIMENTS
We conduct our experiments on speech recogntion on the Wall Street Journal (WSJ) and Librispeech benchmarks. We only compare end-to-end
speech recognition approaches that do not incorporate language model rescoring. On both WSJ
and Librispeech, our proposed OCD (Optimal Completion Distillation) algorithm signiﬁcantly
outperforms our own strong baselines including MLE (Maximum Likelihood Estimation with label
smoothing) and SS (scheduled sampling with a well-tuned schedule). Moreover, OCD signiﬁcantly
outperforms all prior work, achieving a new state-of-the-art on two competitive benchmarks.
WALL STREET JOURNAL
The WSJ dataset is readings of three separate years of the Wall Street Journal. We use the standard
conﬁguration of si284 for training, dev93 for validation and report both test Character Error Rate
(CER) and Word Error Rate (WER) on eval92. We tokenize the dataset to English characters and
punctuation. Our model is an attention-based seq2seq network with a deep convolutional frontend
as used in Zhang et al. . During inference, we use beam search with a beam size of 16 for
all of our models. We describe the architecture and hyperparameter details in Appendix C. We ﬁrst
analyze some key characteristics of the OCD model separately, and then compare our results with
other baselines and state-of-the-art methods.
Training preﬁxes and generalization. We emphasize that during training, the generated preﬁxes
sampled from the model do not match the ground truth sequence, even at the end of training. We
Published as a conference paper at ICLR 2019
Table 3: WSJ Character Error Rate (CER) and Word Error Rate (WER) of different baselines.
Schedule Sampling optimizes for Hamming distance and mixes samples from the model and ground
truth with a probability schedule (start-of-training →end-of-training). OCD always samples from
the model and optimizes for all characters which minimize Edit distance. Optimal Completion Target
optimizes for one character which minimizes edit distance and another criteria (shortest or same
Training Strategy
Schedule Sampling (1.0 →1.0)
Schedule Sampling (0.0 →1.0)
Schedule Sampling (0.0 →0.55)
Optimal Completion Target (Shortest)
Optimal Completion Target (Same #Words)
Optimal Completion Distillation
Table 4: Character Error Rate (CER) and Word Error Rate (WER) results on the end-to-end speech
recognition WSJ task. We report results of our Optimal Completion Distillation (OCD) model, and
well-tuned implementations of maximum likelihood estimation (MLE) and Scheduled Sampling (SS).
Prior Work
CTC 
CTC + REINFORCE 
Gram-CTC 
seq2seq 
seq2seq + TLE 
seq2seq + LS 
seq2seq + CNN 
seq2seq + LSD 
seq2seq + CTC 
seq2seq + TwinNet 
seq2seq + MLE + REINFORCE 
Our Implementation
seq2seq + MLE
seq2seq + SS
seq2seq + OCD
deﬁne OCD preﬁx mismatch as the fraction of OCD training tokens that do not match corresponding
ground truth training tokens at each position. Assuming that the generated preﬁx sequence is perfectly
matched with the ground truth sequence, then the OCD targets would simply be the following tokens
of the ground truth sequence. Hence, OCD becomes equivalent to MLE. Figure 1 shows that OCD
preﬁxes mismatch is more than 25% for the most of the training. This suggests that OCD and MLE
are training on very different input preﬁx trajectories. Further, Figure 2 depicts validation CER as a
function of training CER for different model checkpoints during training, where we use beam search
on both training and validation sets to obtain CER values. Even at the same training CER, we observe
better validation error for OCD, which suggests that OCD improves generalization of MLE, possibly
because OCD alleviates the mismatch between training and inference.
Impact of edit distance. We further investigate the role of the optimizer by experimenting with
different losses. Table 3 compares the test CER and WER of the schedule sampling with a ﬁxed
probability schedule of (1.0 →1.0) and OCD model. Both of the models are trained only on
sampled trajectories. The main difference is their optimizers, where the SS(1.0 →1.0) model is
optimizing the log likelihood of ground truth (a.k.a. Hamming distance). The signiﬁcant drop in CER
of SS(1.0 →1.0) emphasizes the necessity of pretraining or joint training with MLE for models
such as SS. OCD is trained from random initialization and does not require MLE pretraining, nor
Published as a conference paper at ICLR 2019
Figure 3: Librispeech training and validation WER per training epoch for OCD and MLE.
does it require joint optimization with MLE. We also emphasize that unlike SS, we do not need to
tune an exploration schedule, OCD preﬁxes are simply always sampled from the model from the start
of training. We note that even ﬁne tuning a pre-trained SS model which achieves 3.6% CER with
100% sampling increases the CER to 3.8%. This emphasizes the importance of making the loss a
function of the model input preﬁxes, as opposed to the ground truth preﬁxes. Appendix D covers
another aspect of optimizing Edit distance rather than Hamming distance.
Target distribution. Another baseline which is closer to MLE framework is selecting only one
correct target. Table 3 compares OCD with several Optimal Completion Target (OCT) models. In
OCT, we optimize the log-likelihood of one target, which at each step we pick dynamically based on
the minimum edit distance completion similar to OCD. We experiment with several different strategies
when there is more than one character that can lead to minimum CER. In the OCT (Shortest), we
select the token that would minimize the CER and the ﬁnal length of the sequence. In the OCT (Same
#Words), we select the token that in addition to minimum CER, would lead to the closest number of
words to the target sequence. We show that OCD achieves signiﬁcantly better CER and WER over
the other optimization strategies compared in Table 3. This highlights the importance of optimizing
for the entire set of optimal completion targets, as opposed to a single target.
State-of-the-art. Our model trained with OCD optimizes for CER; we achieve 3.1% CER and 9.3%
WER, substantially outperforming our baseline by 14% relatively on CER and 12% relatively on
WER. In terms of CER, our work substantially outperforms prior work as compared in Table 4, with
the closest being Tjandra et al. trained with policy gradients on CER. In terms of WER, our
work is also outperforming Chan et al. , which uses subword units while our model emits
characters.
LIBRISPEECH
For the Librispeech dataset, we train on the full training set (960h audio data) and validate our results
on the dev-other set. We report the results both on the “clean” and “other” test set. We use Byte Pair
Encoding (BPE) for the output token segmentation. BPE token set is an open
Table 5: Character Error Rate (CER) and Word Error Rate (WER) on LibriSpeech test sets.
test-clean
test-other
Prior Work
Wav2letter 
Gated ConvNet 
Cold Fusion 
Invariant Representation Learning 
Pretraining+seq2seq+CTC 
Our Implementation
seq2seq + MLE
seq2seq + OCD
Published as a conference paper at ICLR 2019
vocabulary set since it includes the characters as well as common words and n-grams. We use 10k
BPE tokens and report both CER and WER as the evaluation metric. We describe the architecture
and hyperparameter details in Appendix C.
Fig. 3 shows the validation and training WER curves for MLE and OCD. OCD starts outperforming
MLE on training decodings after training for 13 epochs and on validation decodings after 9 epochs.
Our MLE baseline achieves 5.7% WER, while OCD achieves 4.5% WER on test-clean (21% improvement) and improves the state-of-the-art results over Zeyer et al. . test-other is the more
challenging test split ranked by the WER of a model trained on WSJ mainly
because readers accents deviate more from US-English accents. On test-other our MLE baseline
achieves 15.4%, while our OCD model achieves 13.3% WER, outperforming the 15.4% WER of
Zeyer et al. . Table 5 compares our results with other recent works and the MLE baseline on
Librispeech.
CONCLUSION
This paper presents Optimal Completion Distillation (OCD), a training procedure for optimizing
autoregressive sequence models base on edit distance. OCD is applicable to on-policy or off-policy
trajectories, and in this paper, we demonstrate its effectiveness on samples drawn from the model in
an online fashion. Given any preﬁx, OCD creates an optimal extension policy by computing the exact
optimal Q-values via dynamic programming. The optimal extension policy is distilled by minimizing
a KL divergence between the optimal policy and the model. OCD does not require MLE initialization
or joint optimization with conditional log-likelihood. OCD achieves 3.1% CER and 9.3% WER on
the competitive WSJ speech recognition task, and 4.5% WER on Librispeech without any language
model. OCD outperforms all published work on end-to-end speech recognition, including our own
well-tuned MLE and scheduled sampling baselines without introducing new hyper-parameters.
ACKNOWLEDGEMENTS
We thank Geoffrey Hinton for his valuable feedback and reviews. We thank Samy Bengio, Navdeep
Jaitly, and Jamie Kiros for their help in reviewing the manuscript as well. We thank Zhifeng Chen
and Yonghui Wu for their generous technical help.