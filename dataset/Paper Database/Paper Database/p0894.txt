Unreliable
School of Computer
Los Alamos
University
Los Alamos,
Pittsburgh,
chal @lanLgov
avrim @theory.cs.cmu
We introduce
a new model
for learning
with membership
near the boundary of a target
may receive
responses.
In partial
compensation,
we assume the distribution
of examples
probability
mass on the boundary
The motivation
this model is that the reason for the
(or “don’t
is that these
are extremely
rare in practice.
does not matter how the learner classifies them.
We present
We show how to learn the intersection
two halfspaces
when membership
queries near the
may be answered incorrectly.
Our algorithm
is an extension
of an algorithm
of Baum 
intersections
homogeneous
halfspaces
the PAC-with-membership-queries
We also describe
algorithms
for learning
several subclasses of monotone
DNF formulas.
Introduction
In most of the theoretical
in concept
it is assumed that there is a well-defined
separating positive from
In practice,
classification
For example,
a membership
query algorithm
for learning
to recognize
the number 3 from
pixel images.
strategy would involve
taking a 3 and
a non-3 (maybe a picture of a 2) and asking for classifications
“Supported
in pam by NSF National
Investigator
CCR-9357793
and a Sloan Foundation
Research Fellowship.
t Supported
in part by NSF Grant
CCR-9 110108 and NSF National Young Investigator grant CCR-9357707 with matching funds
provided by Xerox Corporation.
t Supported by NSF Grant CCR-93-1 0888.
Permission to make digitol/hard
copies of all or pmt of this material without fee is granted
provided that the copies
are not made or distributed
for profit
or commercial
advantage,
copyright/server
of the publication
and its dote appear,
and notice
that copyright
is by permission
of the Association
for Computing
Machinery,
otherwise,
to republish,to
post on servers
redistribute
permission
Santa Cruz,
0-89723-5/95/0007.
Sally A. Goldmant
K. Slonim$
Dept. of Computer
Washington
University
545 Technology
St. Louis,
Cambridge,
 
 .
of examples
them until
two nearby examples with
classification
are found.
this type of approachl,
as noticed
by Lang and Baum
that questions of this sort that are near the concept boundary
may result in unreliable
and a 3 tends to produce
that (A) looks a bit like
both and (B) we don’t
care about
don’t expect to see one in practice.
generally,
one unrealistic
PAC-withmembership-query
is that it relies much more heavily
on its assumptions
than the passive PAC model.
for instance, the above situation
of learning
images of 3’s in
which the learner is using some simple hypothesis
class (say
a simple neural network).
For a passive algorithm,
want the data seen to be consistent
some hypothesis
the class (or nearly so). For a membership
query algorithm,
however, one needs the stronger condition
that the target concept can actually
be represented
in such a simple
difference
is that typical
images of 3’s maybe
from images of other characters that many simple
consistent
hypotheses
if one were to probe
of the “3”
find it has a
complicated
that even depends on which
In this paper we propose
membership
that addresses the above
The basic idea of our model
is that queries near the boundary of a target class may receive
care” responses,
in partial
compensation,
the distribution
of examples has zero probability
mass on the
(The motivation
is that the oracle
care” answers because these examples do
not actually
appear in the world
and thus it does not matter
how the learner classifies them.)
We do not require
the oracle
to answer incorrectly
care” in the boundary
since that would just make the learning
that of learning a different
(perhaps ternary) target concept in the standard
for instance,
then simply
search between
the boundary
and non-boundary
the purpose of the model.
One way of viewing
is a bit more general)
is that the
true target concept is in fact some horribly
complicated
1Particularly when a human “expert”
serves as the membership
query oracle.
but differs
in a boundary
region that has zero probability
The contributions
of this work are: (1) the introduction
model of learning
with unreliable
queries, (2) an efficient
that PAC-leams
the intersection
of two halfspaces with membership
queries when the boundary
are noisy, and (3) efficient
algorithms
to exactly
learn (with
membership
subclasses of monotone
when there are one-sided
false positive
errors in the
queries for a small boundary
Definitions
We assume the reader
is familiru
approximately
and Angluin’s
model of learning
with membership
and equivalence
We use PAC-memb
to refer to the variation
the PAC model
the learner can make membership
we say that a concept
class is exactly
if it is learnable
with membership
and equivalence
Given a concept f over an instance space that has a distance
metric, we say that the distance to the boundary
of an example
t is the distance to the nearest example
y such that ~(x)
continuous input spaces we use the infimum
to y’s such that
In the boolean
domain we use the Hamming
distance as our metric.
example is at distance 2 from the boundary
if it is possible to
flip two bit positions
and change its classification.
the boz.mda~
r to be the set of examples
whose distance
to the boundary
is at most r.
We define the
r to be the set of all
x in the boundary
such that ~(x)
We now define the unreliable
query (UBQ)
This model
is the same as the standard PAC-memb
except for the following
difference:
there is a value r (the
to an example
in a boundary
r may receive
an incorrect
response, and (B) the example
distribution
D has zero probability
measure in that boundary
In the incomplete
query (IBQ)
the learner
never receives an
to a query,
but in the boundary
the answer
We also consider
a one-sided
false-positive-only
learner may receive
false positive
answers to any queries in
the negative
and the distribution
to have zero probability
on the negative
we extend these definitions
to the exact learning model by requiring
that counterexamples
to equivalence
queries not be chosen from the boundary
Related Work
There has been much theoretical
on PAC or mistakebound learning
in cases where the training
examples maybe
mislabeled
13] and additional
The p-concepts
model of Kearns and Schapire
 falls
category and is related to our work since their model
a “graded”
between the positive
and negative
portions of the instance space.
There have also been a number
of results
on learning
generated noisy responses to membership
Sakakibara 
considered
the case where each membership
query is incorrectly
answered with a fixed probability,
increase reliability
the same membership query a sufficient
number of times and taking
a majority
vote. A more realistic
model is that ofpersistent
membership
query noise in which
repeated queries to the same example
receive the same answer as in the first call.
and Schapire
 gave a positive
result for learning
classes of read-once
under this noise model.
work uses membership
queries to simulate
a particular
distribution.
Frazier and Pitt showed that CLASSIC
are learnable in this noise model, using the fact that many distinct membership
queries can be fomulated
that redundantly
give the same information.
and Slonim
 introduced
of incomplete
membership
queries, in which a membership
query on a given
instance may persistently
generate a “don’t
The “don’t
instances are chosen uniformly
from the entire domain
and may account for up to a constant
of the instances.
Additional
results in this
were obtained
by Goldman
and Mathias
model allows
for a lame number
instances.
but positive
results in ~his model
are typically
dent on the precisely
nature of the noise.
introduced
the limited
membership
guery model.
In this model,
an adversary
may arbitrarily
select some number /of
examples on which it refuses to answer
membership
queries (or answers “don’t
but the number of queries the learner asks maybe
polynomial
in L Sloan
and Turan presented algorithms
in this-model
for learning
class of monotone
membership
queries alone and the class of monotone
DNF formulas
membership
and equivalence
introduced
model of malicious
membership
the adversary may respond with
instead of “don’t
paper proved
that the class
of monotone
is learnable
in this model.
 has shown that read-once
with malicious
membership
The main difference
in motivation
those above is that instead of supposing
that there is a clear
the positive
and negative
some noise included,
we are attempting
very different
the classification
of examples in the boundary
is just not well
(for example,
is more difficult
than those above in that the membership
query errors
or omissions
are chosen by an adversary
algorithms
must run in time that is polynomial
in the usual
parameters regardless of the number of queries that might receive incorrect
answers (unlike
 ). For example,
case of a 1-term monotone
the boundary
1, there may be exponentially
(in n) instances in the boundary
let X4X7X9 be the
target term.
Then all positive
instances,
and all negative
instances with exactly
one of {X4, X7, 29} turned off, are in the
1.) On the other hand, to partially
compensate
for this difficulty,
we restrict
membership
errors or omissions
to the boundary
and we require
that counterexamples
to equivalence
queries be chosen from
outside the boundary
In other related
Frazier, Goldman,
and Pitt 
introduced
a learning
there is incomplete
information
the target function
due to an ill-defined
the omissions
may be adversarially
placed, all examples
(indicating
classification)
must be consistent
the concept
the classification
of any instance
not be possible
to determine
of the concept
class and the
and negative
instances.
They require
the learner to
{O, 1,?} that, with
probability,
classifies
most randomly
instances,
One of the key differences
their model
and ours is that they allow
time polynomial
in the complexity
of that ternary
thus if the “?”
has a complicated shape, then their learner
is allowed
a correspondingly
Once again,
a goal of our work
is to produce
algorithms
is polynomial
in the usual
parameters
of the number
of queries
that might
In the model
of Frazier,
the time complexity
depends heavily
on the placement
In their paper they give positive
results for
the classes of monotone
and d-dimensional
Negatively,
they show that learning
the conjunctions
clauses in their
is as hard as learning
They also give a general technique
for converting
a standard
PAC (or PAC-memb)
for any concept class closed
under union
and intersection
to an algorithm
that learns in
their model.
an Intersection
of l%vo Halfspaces
We now describe
one of our main positive
an algorithm
for learning
an intersection
halfspaces
for any boundary
radius r (see Figure
is an extension
of an algorithm
 for
the simpler
class of intersections
of two homogeneous
halfspaces
in the standard PAC-with-queries
The idea of Baum’s
is to reduce the problem
an intersection
of two homogeneous
halfspaces
the problem
of learning
of halfspaces,
PAC algorithm
a hypothesis that is the threshold
of a degree-2 polynomial.)
idea of the reduction
is to notice
that negative
the quadrant
the positive
quadrant—the
troublesome examples
the data set from being consistent
of halfspaces—are
those examples
2A halfspace
is homogeneous
if its bounding
hyperplane
the origin.
such that –~
is positive.
His algorithm
is as follows:
Draw a sufficient]
y large set S of examples.3
all of the negative
examples 2 E S which have the
that a membership
“positive”.
Then find a linear function
P such that
for all the marked
(negative)
0 for all the positives.
the XOR-of-halfspaces
find a hypothesis
that correctly
classifies
~ O}. The final hypothesis
0 then predict
else predict
Baum’s algorithm
seems appropriate
for our model because it
does not explicitly
try to query examples
near the boundary.
it is “almost
that if a negative
distance at least r from
the boundary,
then the example
has distance
at least r
the boundary
fails only on the negative examples
in the “A-shaped”
shown in Figure
Our algorithm
for learning
an intersection
of (not necessarily
homogeneous)
halfspaces
in the UBQ
is a small extension of Baum’s
algorithm,
the analysis
bit more care. In our algorithm,
instead of reflecting
the origin,
we reflect
a positive
a potential
that some “good”
example for reflection
must exist.
(The algorithm
tries all of
Specifically,
our algorithm
is the following:
Draw a sufficiently
large set S of examples,
For each positive
E S do the following.
For each negative
the example
2ZP0, – ~n,~,
the response to that query is “positive”,
then mark & ~g.
Now, attempt to find a linear
P such that
for all the marked
(negative)
z Ofor all the positives.
If no such function exists, then repeat this step using
a different
FPOSE S (we prove
this step must succeed for some positive
(assume we have found a legal linear function
be the set of
S such that
and use the XOR-of-halfspaces
learning algorithm
to find a hypothesis
that correctly
classifies the examples
in S~. The final hypothesis
0 then predict
else predict
any radius
r of the boundary
succeeds in the UBQ model.
of correctness,
out the simplifying
observation
is invariant
translation.
If we add some vector
Z to each Z c
VC-dimension
of the hypothesis
class is O(n2)
so a corresponding
of examples
are needed.
1: An intersection
of 2 halfspaces.
is shaded.
that its apex is curved,
complicates
proof somewhat.
hyperplane.
Forclarity,
ZPO. istheonly
examp~es liewithin
thedark-shaded
that theintersection
ofthisregion
thenon-boundmy
separable from thepositive
Thehyperplane
isthelinear
that lemma.
results inadding
of the form 2~POS—~~~gas
Inparticuku-,
this means that if we can prove that our
succeeds when the hyperplanes
are homogeneous,
then this implies
that our algorithm
also succeeds in the general (non-homogeneous)
Therefore,
in our proof
for simplicity
that the hyperplanes
are, in fact,
homogeneous.
We now fix some notation.
Let r be the radius of the boundary
is not used by the algorithm).
target concept is defined by two unit vectors @land 17z,and the
~1 . Z ~ O and ~2 . F > O}. We
define the “opposite
: ~1 . E <0
Z < O}. We say a point
(or example)
is “non-boundary”
is not within
the boundary
The negative
non-boundary
is the set of negative points not in the boundary
O)andd(F,POS)>
that if either ~1 . ii
or 172. Z <
in N.EG.b,
these are not necesswy
conditions
1). In fact, let us define
C1. i’<-ror@z.
so iVEG~aT
To get necessary conditions
in the region
notice that if
Oandt7EN13G.b
then it must be the case that (2 + r~l ). z7z<0
(otherwise
would be in the positive
Similarly,
r~2) . ~1 <0.
{Z:(~l.2<0
and#z2<-r~~.
(~2-Z<Oand~l
We begin by showing
that the negative
in the opposite quadrant
do in fact get marked
by our algorithm.
2 For any non-boundary
example gPo. and
any negative
e,wzmple t?n~~ in the opposite
– i?ne~ is a non-boundary
Since ;fi.g
lies in the opposite
@Z. (22,.,
~Z .25,08>
to be shown
is that there
a positive
2P0. such that the set of negative
reflection
is linearly
the positives.
In particular,
we show the positive
Z c S that minimizes
. E + r) will
FPO, be that example
and a 1 = ~1 . ;PO, and a2 =
ZZ. FPo., we show that a legal separator is the linear equation
3 Let ZPO, be the example
i? E S minimizfig
r) and let al = ~1 . ;PO$ andaz
= P2 . XPOS.
Then the linear function
is at least 2for each positive
5 and at most 2for each
example 5 marked when using 5PO~for
reelection.
we consider
the positive
be some positive
a and /3 so that
@(al + r) and (E’ ~2
= /3(ar +r).
definition
of ZPo~ we have a,B >
1, and by definition
both a and ,B are at least O. These
inequalities
that a +/3
the negative
The set of examples
the property
that 2ZPO$– Z might
be classified
by a membership
query is pictured
in Figure 2. This
set is contained
Z~2a1+randg72
the possible
(1) (cases 1 and 2 below
handle the possibility
Case 1. Suppose76
n {Z : ~1 . ~ < –r},
Case 2. Suppose;
: ~2 . S < –r}.
Case 3. Suppose Z E {2
: ji’l .2<0
+ “(-~~~~+1]
<1at most 2 since az > r.
Case 4. Suppose i? E {2:
Same reasoning
as Case 3.
of Theorem
1: Follows
from Lemmas
Subclasses
of Monotone
In this section we describe algorithms
to learn two subclasses
of monotone
in the UBQ
onesided false-positive
error, for small
values of the boundary
radius r. (Learnability
in the one-sided
UB Q model
learnability
in the lBQ model
each “don’t
care” response as positive.)
Specifically,
we give an algorithm
to learn the class of “readonce monotone
DNF formulas
in which each term has size at
least 4“ in the UBQ model with boundary
radius r =
this is clearly
a highly-restrictive
class, it is as hard to learn
as general DNF formulas
in the passive PAC model.
does demonstrate
that unreliable
queries provide
some power
over the passive model
in a boolean
We also give
an algorithm
to properly
a subclass of constant-term
DNF formulas
for r any constant.
the class of k-term
DNF formulas
is learnable
in the passive model, membership
queries are required
for proper learnability.
the n boolean
variables,
zn ) denote an example.
As is commonly
view the sample space, {O, 1}n, as a lattice
with top element
and bottom
The elements
are partially
ordered by ~, where v ~ w if and only if each bit in v is less
than or equal to the corresponding
bit in w. The descendants
(respectively,
ancestors)
of a vector v are all vectors w in the
sample space such that w < v (respectively,
term, by moving
down in the lattice (i.e. changing
a 1 to O), the term
be “turned
Thus every
term can be uniquely
represented by the minimum
vector in the ordering
< for which
it is true.
v) be the set of examples obtained by flipping
i zeros to ones in vector v. The parents
of v are the elements
of A( 1, v),
and the grandparents
of v are the elements
v) are the set of examples obtained
by flipping
i ones to zeros in v, and for
of examples
we let II(i,
We define the
of v as all elements in D( 1, v), and the siblings
are all elements in D(l,
of examples
as terms and vice-versa,
associating
a monotone
term the minimal
that satisfies it.
For example
v let term(v)
denote the most
specific monotone
term that v satisfies.
Thus, we say an example is a sibling
of a term, meaning
that it is a sibling
associated
an example
z we define
to be the set of variables
set to 1 by z. We also treat
a term t asthe set of variables
it contains.
the high-level
that is used to
obtain both of our results.
our definitions
in Section 2, we say an example
x is in the boundary
region of term
= O, but there exists an d
> x such that t(d)
and dist(z,
< r. Our hypothesis
h contains candidates for
terms of the target function
~, and possibly
some additional
terms used to ensure that provided
counterexamples
in the boundary
of any known
We begin with
h = $, and perform
a loop in which
we make an equivalence
query with our current hypothesis,
and then perform
a collection of membership
queries to update our hypothesis
of the counterexample
a successful
equivalence query is made.
We maintain
the invariant
that after i
counterexamples
have been received,
ht contains
terms tl,,,.,
tiof the target concept
(and possibly
other terms that may or may not be in the target concept).
We define the set of bounda~
[ v is in the
or boundary
of some term in h rl j}.
We now describe how a counterexample
x is processed so that
we can maintain
this invariant.
it receives a negative
counterexample,
our algorithm
terms that classify
x as positive.
no term from ~ (or
even terms in the boundwy
of ~) will
be removed.
a positive
counterexample
we first run the procedure
Boundaiy(x),
an example
v @ B such that
is positive
could be a false positive).
We then run the following
to “reduce”
v so it is
a new target term. To ensure that we do not rediscover
term, the procedure
Move-Further
that is not in B.
is our procedure,
guaranteed to add a new term from
v be the example
Exit-Boundary.
(So v is positive
or in the boundary
region of a new term
Now, so long as v has some child to which a membership
“positive”,
v by that child
repeat. (Note that since v @B and the target formula
it follows
that no child
of v could be in B.)
We now call a procedure
Move-Further(v)
one of the two cases will
Case 1: Move-Further(v)
returns an example
has strictly
is positive,
In this case we return to step 2
using v’ as the current example.
Move-Further(v)
reports failure.
Here we are
guaranteed
that v is “near”
a new term ti+lof ~.
we have that
lt~+~ – vars(v)l
That is, thenumberof
irrelevant
variables in vars(v)
is at most the number
of relevant
ti+lmissing from vars(v),
which in turn is at most
In this case we call the procedure
Candidates(v)
that returns a polynomial-sized
T of terms with
the guarantee
then add all terms in T to h.
At this point
our algorithm
is ready to make its next equivalence query.
4 Given that Exit-Boundary,
Move-Further,
Generate-Candidates
satisfy the stated conditions
in polynomial
timeforsubclass
Cof monotone
DNFformulas,
the above procedure
learns C in the false-positive-only
model (or the IBQ model}
in polynomial
there are no counterexamples
are done. The number of positive
counterexamples
is at most
the number
in the target
Exit-Boundary
is completed
all examples
that the “reduce”
process recurses on have the property
are positive examples
false positive)
not in the positive
of any term of h (1 ~.
the correctness of Move-Further
and Generate-Candidates,
are guaranteed
that a new term is added to h after at most
n calls to Move-Further,
Furthermore,
each negative
counterexample
removes at least one “extra”
term placed in h by
Generate-Candidates
and we are guaranteed
that there are
at most a polynomial
of such terms.
Thus, there are
only a polynomial
of negative
counterexamples,
our algorithm
runs in polynomial
time as long as all of the
procedures
A Subclass
of Read-Once
to complete
the generic
above to obtain
an algorithm
that learns the class C of “readonce monotone
DNF formulas
in which each term has size at
least 4“ in the UBQ
by describing
Study-Example,
used in the algorithm.
This routine
takes an example returned
by Move-Further
is guaranteed
to be “near to” some
term of the target) and produces a more usefu~ approximation
to that term.
The desired
of the routine
is specified
in Property
be a function
in C, and let v be an example
such that there exists a term tit
such that v is
equal to, a sibling
or a child-of
t,+ 1. Then Study-
Example produces an approximation
along with
one of these two guarantees:
is equal to t,+l
(so it is a superset
i?i+l is equal to t,+l
or a child
(so it is a subset
The Study-Example
asks a membership
be the set of siblings
the membership
oracle replied
Then Study-Example
outputs based on the following
If P = @,let fi+l
and report “subset”.
Otherwise.
let u be the term containing
exactly the variables in UPepvars(p).
Iterrn(v)l
1, let ~,+1 =
u and report
“superset”.
> Itemn(v)l
+ 1), if some variable
is “responsible
at least two of
the variables
in u – vars(v)
in the sense that two
in u — vars(v)
are set to 1 in examples
setting yi to O, then let fj+ I = u – {Vi}
and report
there are several
such variables
just pick one.)
Else let fit
I = u and report “superset”.
separate this case from case (a) just for convenience
in the proof.)
5 The routine
Study-Example
as described
Note that no siblings
of v are in the boundary
of any of the other
in the target
because a sibling
of v may have at most two variables
1 that are not in ti+~,and every other term must have at Ieast
four variables
not in t~+I
(since they all have size at least 4
and the target function
is read-once).
Thus, we may analyze
the routine
as if ti+1were the only term in the target function.
We can see that Study-Example
behaves correctly
(1) by noting
that if v is positive
but none of v’s siblings
(or false positive),
then term(v)
is either ti+l
The correctness
of case (2a) is similarly
to see because if v is a child
of tit1 then u equals ti+land
u is a parent of t$+l.Notice
that if v is a child
ti+ 1 then either case (1) or (2a) occurs.
The reasoning
for case (2b) is as follows.
If v = t then it is
On the other hand, if v is a sibling
only variable
y, in v that can possibly
be “responsible
more than one other variable
in u – vars(v)
is the variable
not in term t~+I.
fact, if we are not in cases (1) or (2aj
and v is a sibling
of ti+l,then case (2b) must occur because
be responsible
for the variable
(several variables may be responsible
for this one) as well as
any other variables added to u. Thus, case (2c) holds because
it can only be reached if v = t.
We now prove
subsection.
6 The class of read-once
DNF formulas where each term in the target formula
has at least four
is exactly learnable
in the false-positive-only
model (or the IBQ model] for
polynomial
queries and time.
4 and 5 we know
the invariant
that after i positive
counterexamples
have been received,
h contains
i distinct
terms of the target function
and a collection
of approximations
;l, ;2, . . . . ~i each corresponding
to different
ti,t2,...,tioff,
and labeled
as “subset”
or “superset”
appropriately.
We also maintain
the invariant
that all children
of terms in h (1
f are in h. This invariant
is initially
by Generate-
Candidates
a new term is ~laced in
a term of f
or any of its children’are
cannot be removed
by any negative
counterexample
of a term in f
is in the boundary
any positive
counterexample
is not in 1?. So Exit-
Boundary(z)
returns z.
We now describe the procedure
Move-Further(v).
the input v has the property
that it is not in B and MQ(v)=l.
. . . . i) labeled
as “subset”,
in vars(fj
) to O in v. (This new example
is still in the positive
or boundary
of a new term
is read-once.
is monotone
is not in B.)
We fix these variables
at O for the
procedure.
Let V be the set of variables
set to O by u and not fixed
to O in Step 1. For each variable
V, consider
example v’ obtained
from v by flipping
to Oall variables
in the terms & that contain
and then flipping
1. Let P be the set of all such examples
membership
query reports “positive”.
If there is an example
in P that has fewer
v, then return this example.
If not, then query all children
and grandchildren
in P and if one of them has fewer
than v and is reported
as positive,
then return
Move-Further
the invariant
that v @ B and v is in
the positive
or boundary
region of a new term of f.
already argued that this holds after Step 1. Thus, at this point,
there exists some term ti+1c f,distinct
from t1,....t,,such
that v sets to O at most one variable
The reason is
that since w @B, it immediately
that v is not
in the boundary
of any of the terms tl,
. . . . t~.
We now argue that each example in P has at most one variable
term tj for
obtained by flipping
to 1 some variable appearing in, say, term
tj (~ < i) then either (A) fj
is a “subset”
of tjand therefore
this is the only variable
that vars(v’
) has in common
(since all others in tj have been fixed to O) or else (B) i$j is a
“superset”
case this variable
is also in fj
so to obtain
v’ we flipped
all the rest of the variables
O. Thus no example
in P is in B.
So if an example is returned in step [3a) or (3b) then it has the
desired property.
We now argue that if step (3c) reports failure
then w satisfies
– t~+l I <
– vars(v)l
We have already argued that v is the the positive
or boundary
region of a new target term, t;+ 1,and thus at most one relevant
ti+ 1 is missing
Furthermore,
the variables
in ti+ 1 then all
irrelevant
variables would be removed by the standard reduce
procedure.
If vars(v)
is missing
one relevant
from ti+lthen when ye is added in step (2), the membership
query would
be positive
and thus this example
is added to V.
Now if there were two or more variables in vars(v)
not in t,+1then the example
two of those variables
were set to O would
be returned
step (3a) or (3b).
Thus if we reach step (3c) the required
must hold.
The procedure
Generate-Candidates(v)
first calls Study-
Example(v).
Let tbe the term returned.
If Study-Example
reports “subset”
then place t and its children
Otherwise, if Study-Example
reports “superset”
then place t and
its parents into T.
From Lemma
5 it follows
that ti+lc T.
return T U D(
1)-Separable
We now show that a subclass of monotone
DNF formulas are properly
in the false-positive-only
for any constant
We say that two
terms tiand tj
are l-separable
if there are 1 variables
that are not in ti,and there are ~ variables
in t,that are not
,4 monotone
DNFformula
f is !-separable
if all pairs
(ti, tj)of terms of ~ are l-separable.
7 The class of (r + 1)-separable
k-term monotone
DNF formulas
is exactly learnable
in the false-positive-only
UBQ model (or the IBQ model) using polynomial
queries and
r and k constant).
Furthermore,
equivalence
queries made by our algorithm
1)-separable
DNFformulas.
We first prove this result
under the assumption
Generate-Candidate
not only finds a set of candidates
some new term of the target formula,
but has the
to “guess”
one is right.
a subset of the terms of the target.
that our algorithm
can be modified
to remove this assumption.
Exit-Boundary(v)
a membership
query on examples not in B obtained
by setting
up to (k –
1)?’ variables
in vars(v)
to O. It returns the first such example
the membership
Exit-Boundary
is correct.
8 The procedure
Exit-Boundary
successfully
returns an eazzmple v @ B for
which MQ(v)
We must show that given a positive
counterexample
to hypothesis
h, Exit-Boundary(v)
returns an example v that
is not in B and for which
Since v was a positive
counterexample,
non-boundary
of some term tn e~ in f
Suppose it is also in the boundary
of some terms in h.
Consider one such term tkfiown.
Since f is (r + 1)-separable,
if we set to O the variables
in vars(v)
E t~nOWn —tnewthen
v wdl no longer be m the boundary
of tknowm.However,
still know that all variables
in tneW are in vars(v)
do not change any variables
in tn,w. (In fact,
if all r +
in tknown— tneware 1 in v, then it suffices to pick
any r of them to set to O, since we know
that v is already
the boundary
of tknown.)We can repeat this for the
at most (k – 1) other terms in h. Thus after setting
r(k – 1) variables in var(v)
to Owe obtain an example that is
not in B and is in the truly positive
region of t~~~.
Since this
is one of the examples
by Exit-Boundary
that at least one membership
course, it could be that another membership
query responds
in this case we are still
guaranteed
v returned
is not in B (since we do not query those
in B), and MQ(v)
is positive.
The procedure
Move-Further
works as follows.
For each z
such that r + 1 < i s
rk, itperforms
a membership
v’ in D(i,
v)) and returns
v’ if MQ(v’)
If no such examples
after all values of i have been
tried, then it returns “failure.”
We now argue that when Move-Further(u)
the following
two properties
v sets to O at most
r variables
of the target formula
– vars(v)]
The number of variables
not in t;~lthat are one in v is
-,at most the number
of variables
that are zero in
– vars(v)l).
be in the positive
or boundary
of some new term
adversary can reply
“positive”
only on an example
to O at most r variables
a term in f,
the first property
We now prove that the second property
t ~+1be any term of ~ for which v has 1 ~ r variables
Suppose that the second property
Thus there are at least
not in t,+l
that are one in v, Since the target is
I)-separable
is not in B.
Since tt+l
has at most r variables
set to O in v, at least one example
v) has all variables
in t~+ j set to 1, When adding
r 1‘s, we have at worst just
set to 1 r variables
in each of
Thus there exist at most rk variables
when set to O takes us to an example
Since we try
all i such that r +
1 ~ i ~ kr, we know that we query some
in the positive
of t~+ 1 that is not in B (since we
must query ti+ 1 or one of its ancestors).
But this contradicts
the fact that v was returned.
Thus the second property
note that only
a polynomial
of examples
r and k are constant).
Thus this procedure
runs in polynomial
Generate-Candidates(v)
non-deterministically
guesses which
one is in ~.
It follows
the correctness
of Generate-Candidates
placed in T,
the need to non-deterministically
select the right
T we just try all guesses halting
when failure
because a negative
counterexample
is received
1)st positive
counterexample
is received.
and k are constant,
only a polynomial
of runs occur
and thus the overall
complexity
polynomial.
of Theorem
7 can be extended
the following
9 The class of 2-ternt
DNF formulas
is exactly
by the class of 2-term
in polynomial
time in the false-positive-only
model (or the IBQ model)
with a boundary
region of radius
r (jior constant
Let f = t]+ t2.If tland tz are (r +
l)separable then the result immediately
Thus, without
loss of generality,
assume that t2has all variables
except at most r of them, as well as any number of additional
variables.
If tl is placed in
h first then no counterexample
created by t2since it is entirely
the boundary
region oft 1.If t2 is placed in h first, then we receive a positive counterexample
1 (unless it is contained
case we are done).
This counterexample
is processed to add t1to h.
Concluding
We have introduced
two related models of learning
with noise
near the boundary
of the target concept,
and we have presented positive
in these models
continuous
and discrete domains.
there is much more work to
The algorithms
here learn fairly
We do not yet know
results to learn general monotone
DNF formulas
or the intersection of more than two halfspaces.
One eventual goal might
be a general
describing
ways to transform
classes of
or exact-learning
algorithms
to work in the IBQ
or UBQ model.
Acknowledgements
author thanks Lenny
Pitt for valuable
discussions
on this material.