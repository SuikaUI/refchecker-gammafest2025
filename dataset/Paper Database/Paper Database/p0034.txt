Multiscale Vision Transformers
Haoqi Fan *, 1
Bo Xiong *, 1
Karttikeya Mangalam *, 1, 2
Yanghao Li *, 1
Zhicheng Yan 1
Jitendra Malik 1, 2
Christoph Feichtenhofer *, 1
1Facebook AI Research
2UC Berkeley
We present Multiscale Vision Transformers (MViT) for
video and image recognition, by connecting the seminal idea
of multiscale feature hierarchies with transformer models.
Multiscale Transformers have several channel-resolution
scale stages. Starting from the input resolution and a small
channel dimension, the stages hierarchically expand the
channel capacity while reducing the spatial resolution. This
creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple
low-level visual information, and deeper layers at spatially
coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the
dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and
are 5-10× more costly in computation and parameters. We
further remove the temporal dimension and apply our model
for image classiﬁcation where it outperforms prior work
on vision transformers.
Code is available at: https:
//github.com/facebookresearch/SlowFast.
1. Introduction
We begin with the intellectual history of neural network
models for computer vision. Based on their studies of cat
and monkey visual cortex, Hubel and Wiesel developed
a hierarchical model of the visual pathway with neurons
in lower areas such as V1 responding to features such as
oriented edges and bars, and in higher areas to more speciﬁc stimuli. Fukushima proposed the Neocognitron , a
neural network architecture for pattern recognition explicitly motivated by Hubel and Wiesel’s hierarchy. His model
had alternating layers of simple cells and complex cells, thus
incorporating downsampling, and shift invariance, thus incorporating convolutional structure. LeCun et al. took the
additional step of using backpropagation to train the weights
of this network. But already the main aspects of hierarchy of
visual processing had been established: (i) Reduction in spatial resolution as one goes up the processing hierarchy and
(ii) Increase in the number of different “channels”, with each
*Equal technical contribution.
Figure 1. Multiscale Vision Transformers learn a hierarchy from
dense (in space) and simple (in channels) to coarse and complex
features. Several resolution-channel scale stages progressively
increase the channel capacity of the intermediate latent sequence
while reducing its length and thereby spatial resolution.
channel corresponding to ever more specialized features.
In a parallel development, the computer vision community developed multiscale processing, sometimes called
“pyramid” strategies, with Rosenfeld and Thurston , Burt
and Adelson , Koenderink , among the key papers.
There were two motivations (i) To decrease the computing requirements by working at lower resolutions and (ii) A better
sense of “context” at the lower resolutions, which could then
guide the processing at higher resolutions (this is a precursor
to the beneﬁt of “depth” in today’s neural networks.)
The Transformer architecture allows learning arbitrary functions deﬁned over sets and has been scalably
successful in sequence tasks such as language comprehension and machine translation . Fundamentally, a
transformer uses blocks with two basic operations. First,
is an attention operation for modeling inter-element relations. Second, is a multi-layer perceptron (MLP), which
models relations within an element. Intertwining these operations with normalization and residual connections 
allows transformers to generalize to a wide variety of tasks.
Recently, transformers have been applied to key computer vision tasks such as image classiﬁcation. In the spirit
of architectural universalism, vision transformers 
approach performance of convolutional models across a variety of data and compute regimes. By only having a ﬁrst
layer that ‘patchiﬁes’ the input in spirit of a 2D convolution, followed by a stack of transformer blocks, the vision
transformer aims to showcase the power of the transformer
architecture using little inductive bias.
 
In this paper, our intention is to connect the seminal idea
of multiscale feature hierarchies with the transformer model.
We posit that the fundamental vision principle of resolution
and channel scaling, can be beneﬁcial for transformer models
across a variety of visual recognition tasks.
We present Multiscale Vision Transformers (MViT), a
transformer architecture for modeling visual data such as images and videos. Consider an input image as shown in Fig. 1.
Unlike conventional transformers, which maintain a constant
channel capacity and resolution throughout the network,
Multiscale Transformers have several channel-resolution
‘scale’ stages. Starting from the image resolution and a small
channel dimension, the stages hierarchically expand the
channel capacity while reducing the spatial resolution. This
creates a multiscale pyramid of feature activations inside the
transformer network, effectively connecting the principles
of transformers with multi scale feature hierarchies.
Our conceptual idea provides an effective design advantage for vision transformer models. The early layers of our
architecture can operate at high spatial resolution to model
simple low-level visual information, due to the lightweight
channel capacity. In turn, the deeper layers can effectively
focus on spatially coarse but complex high-level features
to model visual semantics. The fundamental advantage of
our multiscale transformer arises from the extremely dense
nature of visual signals, a phenomenon that is even more
pronounced for space-time visual signals captured in video.
A noteworthy beneﬁt of our design is the presence of
strong implicit temporal bias in video multiscale models. We
show that vision transformer models trained on natural
video suffer no performance decay when tested on videos
with shufﬂed frames. This indicates that these models are not
effectively using the temporal information and instead rely
heavily on appearance. In contrast, when testing our MViT
models on shufﬂed frames, we observe signiﬁcant accuracy
decay, indicating strong use of temporal information.
Our focus in this paper is video recognition, and we design and evaluate MViT for video tasks (Kinetics ,
Charades , SSv2 and AVA ). MViT provides
a signiﬁcant performance gain over concurrent video transformers , without any external pre-training data.
In Fig. A.4 we show the computation/accuracy trade-off
for video-level inference, when varying the number of temporal clips used in MViT. The vertical axis shows accuracy
on Kinetics-400 and the horizontal axis the overall inference cost in FLOPs for different models, MViT and concurrent ViT video variants: VTN , TimeSformer ,
ViViT . To achieve similar accuracy level as MViT, these
models require signiﬁcant more computation and parameters
(e.g. ViViT-L has 6.8× higher FLOPs and 8.5× more parameters at equal accuracy, more analysis in §A.1) and need
large-scale external pre-training on ImageNet-21K (which
contains around 60× more labels than Kinetics-400).
at 1/5 FLOPs
at 1/3 Params
without ImageNet
MViT-B 16x4
MViT-B 32x2
 ViViT-L ImageNet-21K
 TimeSformer ImageNet-21K
 VTN ImageNet-1K / 21K
Inference cost per video in TFLOPs (# of multiply-adds x 1012)
Kinetics top-1 val accuracy (%)
Figure 2. Accuracy/complexity trade-off on Kinetics-400 for
varying # of inference clips per video shown in MViT curves.
Concurrent vision-transformer based methods require over
5× more computation and large-scale external pre-training on
ImageNet-21K (IN-21K), to achieve equivalent MViT accuracy.
We further apply our architecture to an image classiﬁcation task on ImageNet , by simply removing the temporal dimension of the video model found with ablation
experiments on Kinetics, and show signiﬁcant gains over
single-scale vision transformers for image recognition.
2. Related Work
Convolutional networks (ConvNets). Incorporating downsampling, shift invariance, and shared weights, ConvNets
are de-facto standard backbones for computer vision tasks
for image and
video .
Self-attention in ConvNets.
Self-attention mechanisms
has been used for image understanding , unsupervised object recognition as well as vision and
language . Hybrids of self-attention operations and
convolutional networks have also been applied to image
understanding and video recognition .
Vision Transformers. Much of current enthusiasm in application of Transformers to vision tasks commences
with the Vision Transformer (ViT) and Detection Transformer . We build directly upon with a staged model
allowing channel expansion and resolution downsampling.
DeiT proposes a data efﬁcient approach to training ViT.
Our training recipe builds on, and we compare our image
classiﬁcation models to, DeiT under identical settings.
An emerging thread of work aims at applying transformers to vision tasks such as object detection , semantic
segmentation , 3D reconstruction , pose estimation , generative modeling , image retrieval ,
medical image segmentation , point clouds ,
video instance segmentation , object re-identiﬁcation
 , video retrieval , video dialogue , video object
detection and multi-modal tasks .
A separate line of works attempts at modeling visual data
with learnt discretized token sequences .
Efﬁcient Transformers. Recent works reduce the quadratic attention complexity to make
transformers more efﬁcient for natural language processing
applications, which is complementary to our approach.
Three concurrent works propose a ViT-based architecture
for video . However, these methods rely on pretraining on vast amount of external data such as ImageNet-
21K , and thus use the vanilla ViT with minimal
adaptations. In contrast, our MViT introduces multiscale
feature hierarchies for transformers, allowing effective modeling of dense visual input without large-scale external data.
3. Multiscale Vision Transformer (MViT)
Our generic Multiscale Transformer architecture builds
on the core concept of stages. Each stage consists of multiple
transformer blocks with speciﬁc space-time resolution and
channel dimension. The main idea of Multiscale Transformers is to progressively expand the channel capacity, while
pooling the resolution from input to output of the network.
3.1. Multi Head Pooling Attention
We ﬁrst describe Multi Head Pooling Attention (MHPA),
a self attention operator that enables ﬂexible resolution modeling in a transformer block allowing Multiscale Transformers to operate at progressively changing spatiotemporal resolution. In contrast to original Multi Head Attention (MHA)
operators , where the channel dimension and the spatiotemporal resolution remains ﬁxed, MHPA pools the sequence
of latent tensors to reduce the sequence length (resolution)
of the attended input. Fig. 3 shows the concept.
Concretely, consider a D dimensional input tensor X
of sequence length L, X ∈RL×D. Following MHA ,
MHPA projects the input X into intermediate query tensor
ˆQ ∈RL×D, key tensor ˆK ∈RL×D and value tensor ˆV ∈
RL×D with linear operations
/ with weights WQ, WK, WV of dimensions D × D. The
obtained intermediate tensors are then pooled in sequence
length, with a pooling operator P as described below.
Pooling Operator. Before attending the input, the intermediate tensors ˆQ, ˆK, ˆV are pooled with the pooling operator
P(·; Θ) which is the cornerstone of our MHPA and, by extension, of our Multiscale Transformer architecture.
The operator P(·; Θ) performs a pooling kernel computation on the input tensor along each of the dimensions.
Unpacking Θ as Θ := (k, s, p), the operator employs a
pooling kernel k of dimensions kT × kH × kW , a stride s
of corresponding dimensions sT × sH × sW and a padding
p of corresponding dimensions pT × pH × pW to reduce an
MatMul & Scale
Add & Norm
Figure 3. Pooling Attention is a ﬂexible attention mechanism that
(i) allows obtaining the reduced space-time resolution ( ˆT ˆH ˆ
the input (THW) by pooling the query, Q = P( ˆQ; ΘQ), and/or
(ii) computes attention on a reduced length ( ˜T ˜H ˜
W) by pooling the
key, K = P( ˆK; ΘK), and value, V = P( ˆV ; ΘV ), sequences.
input tensor of dimensions L = T × H × W to ˜L given by,
L + 2p −k
with the equation applying coordinate-wise. The pooled
tensor is ﬂattened again yielding the output of P(Y ; Θ) ∈
R˜L×D with reduced sequence length, ˜L = ˜T × ˜H × ˜W.
By default we use overlapping kernels k with shapepreserving padding p in our pooling attention operators, so
that ˜L , the sequence length of the output tensor P(Y ; Θ),
experiences an overall reduction by a factor of sT sHsW .
Pooling Attention. The pooling operator P(·; Θ) is applied
to all the intermediate tensors ˆQ, ˆK and ˆV independently
with chosen pooling kernels k, stride s and padding p. Denoting θ yielding the pre-attention vectors Q = P( ˆQ; ΘQ),
K = P( ˆK; ΘK) and V = P( ˆV ; ΘV ) with reduced sequence lengths. Attention is now computed on these shortened vectors, with the operation,
Attention(Q, K, V ) = Softmax(QKT /
Naturally, the operation induces the constraints sK ≡sV
on the pooling operators. In summary, pooling attention is
computed as,
PA(·) = Softmax(P(Q; ΘQ)P(K; ΘK)T /
d)P(V ; ΘV ),
d is normalizing the inner product matrix row-wise.
The output of the Pooling attention operation thus has its
sequence length reduced by a stride factor of sQ
W following the shortening of the query vector Q in P(·).
output sizes
data layer
stride τ×1×1
1×16×16, D
stride 1×16×16
Table 1. Vision Transformers (ViT) base model starts from a
data layer that samples visual input with rate τ×1×1 to T×H×W
resolution, where T is the number of frames H height and W width.
The ﬁrst layer, patch1 projects patches (of shape 1×16×16) to form
a sequence, processed by a stack of N transformer blocks (stage2)
at uniform channel dimension (D) and resolution (T× H
Multiple heads. As in the computation can be parallelized by considering h heads where each head is performing the pooling attention on a non overlapping subset of D/h
channels of the D dimensional input tensor X.
Computational Analysis. Since attention computation
scales quadratically w.r.t. the sequence length, pooling the
key, query and value tensors has dramatic beneﬁts on the
fundamental compute and memory requirements of the Multiscale Transformer model. Denoting the sequence length
reduction factors by fQ, fK and fV we have,
W , ∀j ∈{Q, K, V }.
Considering the input tensor to P(; Θ) to have dimensions
D × T × H × W, the run-time complexity of MHPA is
O(THWD/h(D + THW/fQfK)) per head and the memory complexity is O(THWh(D/h + THW/fQfK)).
This trade-off between the number of channels D and
sequence length term THW/fQfK informs our design
choices about architectural parameters such as number of
heads and width of layers. We refer the reader to the supplement for a detailed analysis and discussions on the timememory complexity trade-off.
3.2. Multiscale Transformer Networks
Building upon Multi Head Pooling Attention (Sec. 3.1),
we describe the Multiscale Transformer model for visual
representation learning using exclusively MHPA and MLP
layers. First, we present a brief review of the Vision Transformer Model that informs our design.
Preliminaries: Vision Transformer (ViT). The Vision
Transformer (ViT) architecture starts by dicing the input
video of resolution T×H×W, where T is the number of
frames H the height and W the width, into non-overlapping
patches of size 1×16×16 each, followed by point-wise application of linear layer on the ﬂattened image patches to to
project them into the latent dimension, D, of the transformer.
This is equivalent to a convolution with equal kernel size
and stride of 1×16×16 and is shown as patch1 stage in the
model deﬁnition in Table 1.
Next, a positional embedding E ∈RL×D is added to
each element of the projected sequence of length L with
output sizes
data layer
stride τ×1×1
cT ×cH×cW , D
stride sT ×4×4
Table 2. Multiscale Vision Transformers (MViT) base model.
Layer cube1, projects dense space-time cubes (of shape ct×cy×cw)
to D channels to reduce spatio-temporal resolution to
The subsequent stages progressively down-sample this resolution
(at beginning of a stage) with MHPA while simultaneously increasing the channel dimension, in MLP layers, (at the end of a stage).
Each stage consists of N∗transformer blocks, denoted in [brackets].
dimension D to encode the positional information and break
permutation invariance. A learnable class embedding is
appended to the projected image patches.
The resulting sequence of length of L + 1 is then processed sequentially by a stack of N transformer blocks, each
one performing attention (MHA ), multi-layer perceptron (MLP) and layer normalization (LN ) operations.
Considering X to be the input of the block, the output of a
single transformer block, Block(X) is computed by
X1 = MHA(LN(X)) + X
Block(X) = MLP(LN(X1)) + X1.
The resulting sequence after N consecutive blocks is layernormalized and the class embedding is extracted and passed
through a linear layer to predict the desired output (e.g. class).
By default, the hidden dimension of the MLP is 4D. We
refer the reader to for details.
In context of the present paper, it is noteworthy that ViT
maintains a constant channel capacity and spatial resolution
throughout all the blocks (see Table 1).
Multiscale Vision Transformers (MViT). Our key concept is to progressively grow the channel resolution (i.e. dimension), while simultaneously reducing the spatiotemporal
resolution (i.e. sequence length) throughout the network. By
design, our MViT architecture has ﬁne spacetime (and coarse
channel) resolution in early layers that is up-/downsampled
to a coarse spacetime (and ﬁne channel) resolution in late
layers. MViT is shown in Table 2.
Scale stages. A scale stage is deﬁned as a set of N transformer blocks that operate on the same scale with identical resolution across channels and space-time dimensions
D×T×H×W. At the input (cube1 in Table 2), we project
the patches (or cubes if they have a temporal extent) to a
smaller channel dimension (e.g. 8× smaller than a typical
ViT model), but long sequence (e.g. 4×4 = 16× denser than
a typical ViT model; cf. Table 1).
output sizes
stride 8×1×1
1×16×16, 768
768×8×14×14
stride 1×16×16
768×8×14×14
(a) ViT-B with 179.6G FLOPs, 87.2M param,
16.8G memory, and 68.5% top-1 accuracy.
output sizes
stride 4×1×1
16×224×224
96×8×56×56
stride 2×4×4
96×8×56×56
192×8×28×28
384×8×14×14
(b) MViT-B with 70.5G FLOPs, 36.5M param,
6.8G memory, and 77.2% top-1 accuracy.
output sizes
stride 4×1×1
16×224×224
3×8×8, 128
128×8×28×28
stride 2×8×8
128×8×28×28
256×8×14×14
(c) MViT-S with 32.9G FLOPs, 26.1M param,
4.3G memory, and 74.3% top-1 accuracy.
Table 3. Comparing ViT-B to two instantiations of MViT with varying complexity, MViT-S in (c) and MViT-B in (b). MViT-S operates at
a lower spatial resolution and lacks a ﬁrst high-resolution stage. The top-1 accuracy corresponds to 5-Center view testing on K400. FLOPs
correspond to a single inference clip, and memory is for a training batch of 4 clips. See Table 2 for the general MViT-B structure.
At a stage transition (e.g. scale1 to scale2 to in Table 2),
the channel dimension of the processed sequence is upsampled while the length of the sequence is down-sampled.
This effectively reduces the spatio-temporal resolution of the
underlying visual data while allowing the network to assimilate the processed information in more complex features.
Channel expansion. When transitioning from one stage
to the next, we expand the channel dimension by increasing the output of the ﬁnal MLP layer in the previous stage
by a factor that is relative to the resolution change introduced at the stage. Concretely, if we down-sample the
space-time resolution by 4×, we increase the channel dimension by 2×. For example, scale3 to scale4 changes resolution from 2D× T
8 to 4D× T
16 in Table 2.
This roughly preserves the computational complexity across
stages, and is similar to ConvNet design principles .
Query pooling. The pooling attention operation affords
ﬂexibility not only in the length of key and value vectors
but also in the length of the query, and thereby output, sequence. Pooling the query vector P(Q; k; p; s) with a kernel
W ) leads to sequence reduction by a factor of
W . Since, our intention is to decrease resolution
at the beginning of a stage and then preserve this resolution
throughout the stage, only the ﬁrst pooling attention operator
of each stage operates at non-degenerate query stride sQ > 1,
with all other operators constrained to sQ ≡(1, 1, 1).
Key-Value pooling. Unlike Query pooling, changing the sequence length of key K and value V tensors, does not change
the output sequence length and, hence, the space-time resolution. However, they play a key role in overall computational
requirements of the pooling attention operator.
We decouple the usage of K, V and Q pooling, with
Q pooling being used in the ﬁrst layer of each stage and
K, V pooling being employed in all other layers. Since the
sequence length of key and value tensors need to be identical
to allow attention weight calculation, the pooling stride used
on K and value V tensors needs to be identical. In our
default setting, we constrain all pooling parameters (k; p; s)
to be identical i.e. ΘK ≡ΘV within a stage, but vary s
adaptively w.r.t. to the scale across stages.
Skip connections. Since the channel dimension and sequence length change inside a residual block, we pool the
skip connection to adapt to the dimension mismatch between
its two ends. MHPA handles this mismatch by adding the
query pooling operator P(·; ΘQ) to the residual path. As
shown in Fig. 3, instead of directly adding the input X of
MHPA to the output, we add the pooled input X to the
output, thereby matching the resolution to attended query Q.
For handling the channel dimension mismatch between
stage changes, we employ an extra linear layer that operates
on the layer-normalized output of our MHPA operation. Note
that this differs from the other (resolution-preserving) skipconnections that operate on the un-normalized signal.
3.3. Network instantiation details
Table 3 shows concrete instantiations of the base models for Vision Transformers and our Multiscale Vision
Transformers. ViT-Base (Table 3b) initially projects
the input to patches of shape 1×16×16 with dimension
D = 768, followed by stacking N = 12 transformer
blocks. With an 8×224×224 input the resolution is ﬁxed to
768×8×14×14 throughout all layers. The sequence length
(spacetime resolution + class token) is 8·14·14+1 = 1569.
Our MViT-Base (Table 3b) is comprised of 4 scale stages,
each having several transformer blocks of consistent channel
dimension. MViT-B initially projects the input to a channel
dimension of D = 96 with overlapping space-time cubes of
shape 3×7×7. The resulting sequence of length 8∗56∗56+
1 = 25089 is reduced by a factor of 4 for each additional
stage, to a ﬁnal sequence length of 8 ∗7 ∗7 + 1 = 393 at
scale4. In tandem, the channel dimension is up-sampled by
a factor of 2 at each stage, increasing to 768 at scale4. Note
that all pooling operations, and hence the resolution downsampling, is performed only on the data sequence without
involving the processed class token embedding.
We set the number of MHPA heads to h = 1 in the scale1
stage and increase the number of heads with the channel
dimension (channels per-head D/h remain consistent at 96).
At each stage transition, the previous stage output MLP
dimension is increased by 2× and MHPA pools on Q tensors
with sQ = (1, 2, 2) at the input of the next stage.
top-1 top-5 FLOPs×views Param
Two-Stream I3D 
ip-CSN-152 
SlowFast 8×8 +NL 
SlowFast 16×8 +NL 
X3D-M 
X3D-XL 
ViT-B-VTN 
ImageNet-1K
ViT-B-VTN 
ImageNet-21K 78.6
ViT-B-TimeSformer 
ImageNet-21K 80.7
ViT-L-ViViT 
ImageNet-21K 81.3
ViT-B (our baseline)
ImageNet-21K 79.3
ViT-B (our baseline)
MViT-B, 16×4
MViT-B, 32×3
MViT-B, 64×3
Table 4. Comparison with previous work on Kinetics-400. We
report the inference cost with a single “view" (temporal clip with
spatial crop) × the number of views (FLOPs×viewspace×viewtime).
Magnitudes are Giga (109) for FLOPs and Mega (106) for Param.
Accuracy of models trained with external data is de-emphasized.
We employ K, V pooling in all MHPA blocks, with
ΘK ≡ΘV and sQ = (1, 8, 8) in scale1 and adaptively
decay this stride w.r.t. to the scale across stages such that the
K, V tensors have consistent scale across all blocks.
4. Experiments: Video Recognition
Datasets. We use Kinetics-400 (K400) (∼240k training videos in 400 classes) and Kinetics-600 . We further assess transfer learning performance for on Something-
Something-v2 , Charades , and AVA .
We report top-1 and top-5 classiﬁcation accuracy (%) on
the validation set, computational cost (in FLOPs) of a single,
spatially center-cropped clip and the number of clips used.
Training. By default, all models are trained from random
initialization (“from scratch”) on Kinetics, without using
ImageNet or other pre-training. Our training recipe and
augmentations follow . For Kinetics, we train for 200
epochs with 2 repeated augmentation repetitions.
We report ViT baselines that are ﬁne-tuned from ImageNet, using a 30-epoch version of the training recipe in .
For the temporal domain, we sample a clip from the fulllength video, and the input to the network are T frames with
a temporal stride of τ; denoted as T × τ .
Inference. We apply two testing strategies following : (i) Temporally, uniformly samples K clips (e.g. K=5)
from a video, scales the shorter spatial side to 256 pixels and
takes a 224×224 center crop, and (ii), the same as (i) temporally, but take 3 crops of 224×224 to cover the longer spatial
axis. We average the scores for all individual predictions.
All implementation speciﬁcs are in §D.
4.1. Main Results
Kinetics-400. Table 4 compares to prior work. From topto-bottom, it has four sections and we discuss them in turn.
pretrain top-1 top-5 GFLOPs×views Param
SlowFast 16×8 +NL 
ViT-B-TimeSformer 
IN-21K 82.4
ViT-L-ViViT 
IN-21K 83.0
MViT-B, 16×4
MViT-B, 32×3
MViT-B-24, 32×3
Table 5. Comparison with previous work on Kinetics-600.
The ﬁrst Table 4 section shows prior art using ConvNets.
The second section shows concurrent work using Vision
Transformers for video classiﬁcation . Both approaches rely on ImageNet pre-trained base models. ViT-B-
VTN achieves 75.6% top-1 accuracy, which is boosted
by 3% to 78.6% by merely changing the pre-training from
ImageNet-1K to ImageNet-21K. ViT-B-TimeSformer 
shows another 2.1% gain on top of VTN, at higher cost of
7140G FLOPs and 121.4M parameters. ViViT improves
accuracy further with an even larger ViT-L model.
The third section in Table 4 shows our ViT baselines. We
ﬁrst list our ViT-B, also pre-trained on the ImageNet-21K,
which achieves 79.3%, thereby being 1.4% lower than ViT-B-
TimeSformer, but is with 4.4× fewer FLOPs and 1.4× fewer
parameters. This result shows that simply ﬁne-tuning an
off-the-shelf ViT-B model from ImageNet-21K provides
a strong baseline on Kinetics. However, training this model
from-scratch with the same ﬁne-tuning recipe will result
in 34.3%. Using our “training-from-scratch” recipe will
produce 68.5% for this ViT-B model, using the same 1×5,
spatial × temporal, views for video-level inference.
The ﬁnal section of Table 4 lists our MViT results. All our
models are trained-from-scratch using this recipe, without
any external pre-training. Our small model, MViT-S produces 76.0% while being relatively lightweight with 26.1M
param and 32.9×5=164.5G FLOPs, outperforming ViT-B
by +7.5% at 5.5× less compute in identical train/val setting.
Our base model, MViT-B provides 78.4%, a +9.9% accuracy boost over ViT-B under identical settings, while having
2.6×/2.4×fewer FLOPs/parameters. When changing the
frame sampling from 16×4 to 32×3 performance increases
to 80.2%. Finally, we take this model and ﬁne-tune it for 5
epochs with longer 64 frame input, after interpolating the
temporal positional embedding, to reach 81.2% top-1 using
3 spatial and 3 temporal views for inference (it is sufﬁcient
test with fewer temporal views if a clip has more frames).
Further quantitative and qualitative results are in §A.
Kinetics-600 is a larger version of Kinetics. Results
are in Table 5. We train MViT from-scratch, without any
pre-training. MViT-B, 16×4 achieves 82.1% top-1 accuracy. We further train a deeper 24-layer model with longer
sampling, MViT-B-24, 32×3, to investigate model scale on
this larger dataset. MViT achieves state-of-the-art of 83.4%
with 5-clip center crop testing while having 56.0× fewer
FLOPs and 8.4× fewer parameters than ViT-L-ViViT 
which relies on large-scale ImageNet-21K pre-training.
top-1 top-5 FLOPs×views Param
TSM-RGB 
IN-1K+K400 63.3
MSNet 
ViT-B-TimeSformer 
ViT-B (our baseline)
SlowFast R50, 8×8 
SlowFast R101, 8×8 
MViT-B, 16×4
MViT-B, 32×3
MViT-B, 64×3
MViT-B, 16×4
MViT-B, 32×3
MViT-B-24, 32×3
Table 6. Comparison with previous work on SSv2.
Something-Something-v2 (SSv2) is a dataset with
videos containing object interactions, which is known as
a ‘temporal modeling‘ task. Table 6 compares our method
with the state-of-the-art. We ﬁrst report a simple ViT-B
(our baseline) that uses ImageNet-21K pre-training. Our
MViT-B with 16 frames has 64.7% top-1 accuracy, which is
better than the SlowFast R101 which shares the same
setting (K400 pre-training and 3×1 view testing). With more
input frames, our MViT-B achieves 67.7% and the deeper
MViT-B-24 achieves 68.7% using our K600 pre-trained
model of above. In general, Table 6 veriﬁes the capability of
temporal modeling for MViT.
FLOPs×views Param
Nonlocal 
IN-1K+K400
STRG +NL 
Timeception 
LFB +NL 
SlowFast 50, 8×8 
SlowFast 101+NL, 16×8 
X3D-XL 
MViT-B, 16×4
MViT-B, 32×3
MViT-B, 64×3
SlowFast R101+NL, 16×8 
X3D-XL 
MViT-B, 16×4
MViT-B, 32×3
MViT-B-24, 32×3
Table 7. Comparison with previous work on Charades.
Charades is a dataset with longer range activities. We
validate our model in Table 7. With similar FLOPs and
parameters, our MViT-B 16×4 achieves better results (+2.0
mAP) than SlowFast R50 . As shown in the Table, the
performance of MViT-B is further improved by increasing
the number of input frames and MViT-B layers and using
K600 pre-trained models.
AVA is a dataset with for spatiotemporal-localization
of human actions. We validate our MViT on this detection
task. Details about the detection architecture of MViT can
be found in §D.2. Table 8 shows the results of our MViT
models compared with SlowFast and X3D . We
observe that MViT-B can be competitive to SlowFast and
X3D using the same pre-training and testing strategy.
pretrain val mAP FLOPs Param
SlowFast, 4×16, R50 
SlowFast, 8×8, R101 
MViT-B, 16×4
MViT-B, 32×3
MViT-B, 64×3
SlowFast, 8×8 R101+NL 
SlowFast, 16×8 R101+NL 
X3D-XL 
MViT-B, 16×4
MViT-B, 32×3
MViT-B-24, 32×3
Table 8. Comparison with previvous work on AVA v2.2. All
methods use single center crop inference following .
4.2. Ablations on Kinetics
We carry out ablations on Kinetics-400 (K400) using 5clip center 224×224 crop testing. We show top-1 accuracy
(Acc), as well as computational complexity measured in
GFLOPs for a single clip input of spatial size 2242. Inference computational cost is proportional as a ﬁxed number
of 5 clips is used (to roughly cover the inferred videos with
T×τ=16×4 sampling.) We also report Parameters in M(106)
and training GPU memory in G(109) for a batch size of 4. By
default all MViT ablations are with MViT-B, T×τ=16×4
and max-pooling in MHSA.
70.1 (−7.1)
68.4 (−0.1)
Table 9. Shufﬂing frames in inference. MViT-B severely drops
(−7.1%) for shufﬂed temporal input, but ViT-B models appear to
ignore temporal information as accuracy remains similar (−0.1%).
Frame shufﬂing. Table 9 shows results for randomly shuf-
ﬂing the input frames in time during testing. All models are
trained without any shufﬂing and have temporal embeddings.
We notice that our MViT-B architecture suffers a signiﬁcant
accuracy drop of -7.1% (77.2 →70.1) for shufﬂing inference frames. By contrast, ViT-B is surprisingly robust for
shufﬂing the temporal order of the input.
This indicates that a naïve application of ViT to video
does not model temporal information, and the temporal positional embedding in ViT-B seems to be fully ignored. We
also veriﬁed this with the 79.3% ImageNet-21K pre-trained
ViT-B of Table 4, which has the same accuracy of 79.3% for
shufﬂing test frames, suggesting that it implicitly performs
bag-of-frames video classiﬁcation in Kinetics.
[N1, N2] FLOPs (G)
2-scale ViT-B, Q pool
111.1 (−68.5) 9.8 (−7.0) 71.0 (+1.5)
ViT-B, K, V pool
148.4 (−31.2) 8.9 (−7.9) 69.1 (+0.6)
Table 10. Query (scale stage) and Key-Value pooling on ViT-
B. Introducing a single extra resolution stage into ViT-B boosts
accuracy by +1.5%. Pooling K, V provides +0.6% accuracy. Both
techniques allow dramatic FLOPs/memory savings.
Two scales in ViT. We provide a simple experiment that
ablates the effectiveness of scale-stage design on ViT-B. For
this we add a single scale stage to the ViT-B model. To
isolate the effect of having different scales in ViT, we do
not alter the channel dimensionality for this experiment. We
do so by performing Q-Pooling with sQ ≡(1, 2, 2) after 6
Transformer blocks (cf. Table 3). Table 10 shows the results.
Adding a single scale stage to the ViT-B baseline boosts accuracy by +1.5% while deceasing FLOPs and memory cost by
38% and 41%. Pooling Key-Value tensors reduces compute
and memory cost while slightly increasing accuracy.
positional embedding
space-only
joint space-time
separate in space & time
Table 11. Effect of separate space-time positional embedding.
Backbone: MViT-B, 16×4. FLOPs are 70.5G for all variants.
Separate space & time embeddings in MViT. In Table 11,
we ablate using (i) none, (ii) space-only, (iii) joint space-time,
and (iv) a separate space and time (our default), positional
embeddings. We observe that no embedding (i) decays accuracy by -0.9% over using just a spatial one (ii) which is
roughly equivalent to a joint spatiotemporal one (iii). Our
separate space-time embedding (iv) is best, and also has
2.1M fewer parameters than a joint spacetime embedding.
Table 12. Input sampling: We vary sampling rate T × τ, the size
c=cT ×cH×cW and stride of s=sT ×sH×sW the cube1 layer that
projects space-time cubes. Cubes with temporal extent cT > 1 are
beneﬁcial. Our default setting is underlined.
Input Sampling Rate. Table 12 shows results for different
cubiﬁcation kernel size c and sampling stride s (cf. Table 2).
We observe that sampling patches, cT = 1, performs worse
than sampling cubes with cT > 1. Further, sampling twice
as many frames, T= 16, with twice the cube stride, sT = 2,
keeps the cost constant but boosts performance by +1.3%
(75.9% →77.2%). Also, sampling overlapping input cubes
s < c allows better information ﬂow and beneﬁts performance. While cT > 1 helps, very large temporal kernel size
(cT = 7) doesn’t futher improve performance.
Stage distribution. The ablation in Table 13 shows the results for distributing the number of transformer blocks in
each individual scale stage. The overall number of transformer blocks, N=16 is consistent. We observe that having
more blocks in early stages increases memory and having
[N2, N3, N4, N5]
 
 
 
 
 
 
Table 13. Scale blocks: We ablate the stage conﬁguration as the
number of blocks N in stages of MViT-B (i.e. where to pool Q).
The overall number of transformer blocks is constant with N=16.
more blocks later stages the parameters of the architecture.
Shifting the majority of blocks to the scale4 stage (Variant
V5 and V6 in Table 13) achieves the best trade-off.
adaptive FLOPs
Table 14. Key-Value pooling: Vary stride s = sT × sH × sW , for
pooling K and V . “adaptive” reduces stride w.r.t. stage resolution.
Key-Value pooling. The ablation in Table 14 analyzes the
pooling stride s = sT × sH × sW , for pooling K and V
tensors. Here, we compare an “adaptive” pooling that uses a
stride w.r.t. stage resolution, and keeps the K, V resolution
ﬁxed across all stages, against a non-adaptive version that
uses the same stride at every block. First, we compare the
baseline which uses no K, V pooling with non-adaptive
pooling with a ﬁxed stride of 2×4×4 across all stages: this
drops accuracy from 77.6% to 74.8 (and reduces FLOPs
and memory by over 50%). Using an adaptive stride that is
1×8×8 in the scale1 stage, 1×4×4 in scale2, and 1×2×2 in
scale3 gives the best accuracy of 77.2% while still preserving
most of the efﬁciency gains in FLOPs and memory.
pooling func
Table 15. Pooling function: Varying the kernel k as a function of
stride s. Functions are average or max pooling and conv which is a
learnable, channel-wise convolution.
Pooling function. The ablation in Table 15 looks at the
kernel size k w.r.t. the stride s, and the pooling function
(max/average/conv). First, we see that having equivalent
kernel and stride k=s provides 76.1%, increasing the kernel
size to k=2s+1 decays to 75.5%, but using a kernel k=s+1
gives a clear beneﬁt of 77.2%. This indicates that overlapping pooling is effective, but a too large overlap (2s+1) hurts.
Second, we investigate average instead of max-pooling and
observe that accuracy decays by from 77.2% to 75.4%.
Third, we use conv-pooling by a learnable, channelwise
convolution followed by LN. This variant has +1.2% over
max pooling and is used for all experiments in §4.1 and §5.
FLOPs×views
X3D-M 
SlowFast R50 
SlowFast R101 
ViT-B 
MViT-S, max-pool
MViT-B, max-pool
MViT-S, conv-pool
MViT-B, conv-pool
Table 16. Speed-Accuracy tradeoff on Kinetics-400.
throughput is measured in clips/s. MViT is fast and accurate.
Speed-Accuracy tradeoff. In Table 16, we analyze the
speed/accuracy trade-off of our MViT models, along with
their counterparts vision transformer (ViT ) and ConvNets (SlowFast 8×8 R50, SlowFast 8×8 R101 , &
X3D-L ). We measure training throughput as the number of video clips per second on a single M40 GPU.
We observe that both MViT-S and MViT-B models are
not only signiﬁcantly more accurate but also much faster
than both the ViT-B baseline and convolutional models. Concretely, MViT-S has 3.4× higher throughput speed (clips/s),
is +5.8% more accurate (Acc), and has 3.3× fewer parameters (Param) than ViT-B. Using a conv instead of maxpooling in MHSA, we observe a training speed reduction of
∼20% for convolution and additional parameter updates.
5. Experiments: Image Recognition
We apply our video models on static image recognition by
using them with single frame, T = 1, on ImageNet-1K .
Training. Our recipe is identical to DeiT and summarized in the supplementary material. Training is for 300
epochs and results improve for training longer .
5.1. Main Results
For this experiment, we take our models which were designed by ablation studies for video classiﬁcation on Kinetics
and simply remove the temporal dimension. Then we train
and validate them (“from scratch”) on ImageNet.
Table 17 shows the comparison with previous work. From
top to bottom, the table contains RegNet and Efﬁcient-
Net as ConvNet examples, and DeiT , with DeiT-B
being identical to ViT-B but trained with the improved
recipe in . Therefore, this is the vision transformer counterpart we are interested in comparing to.
The bottom section in Table 17 shows results for our
Multiscale Vision Transformer (MViT) models.
Acc FLOPs (G) Param (M)
RegNetZ-4GF 
RegNetZ-16GF 
EfﬁcientNet-B7 
DeiT-S 
DeiT-B 
DeiT-B ↑3842 
MViT-B-16, max-pool
MViT-B-24, max-pool
MViT-B-24-wide-3202, max-pool 84.3
MViT-B-24-wide-3202
Table 17. Comparison to prior work on ImageNet.
and EfﬁcientNet are ConvNet examples that use different training
recipes. DeiT/MViT are ViT-based and use identical recipes .
We show models of different depth, MViT-B-Depth, (16,
24, and 32), where MViT-B-16 is our base model and the
deeper variants are simply created by repeating the number of blocks N∗in each scale stage (cf. Table 3b). “wide”
denotes a larger channel dimension of D = 112. All our
models are trained using the identical recipe as DeiT .
We make the following observations:
(i) Our lightweight MViT-B-16 achieves 82.5% top-1
accuracy, with only 7.8 GFLOPs, which outperforms the
DeiT-B counterpart by +0.7% with lower computation cost
(2.3×fewer FLOPs and Parameters). If we use conv instead
of max-pooling, this number is increased by +0.5% to 83.0%.
(ii) Our deeper model MViT-B-24, provides a gain of
+0.6% accuracy at slight increase in computation.
(iii) A larger model, MViT-B-24-wide with input resolution 3202 reaches 84.3%, corresponding to a +1.2% gain, at
1.7×fewer FLOPs, over DeiT-B↑3842. Using convolutional,
instead of max-pooling elevates this to 84.8%.
These results suggest that Multiscale Vision Transformers
have an architectural advantage over Vision Transformers.
6. Conclusion
We have presented Multiscale Vision Transformers that
aim to connect the fundamental concept of multiscale feature
hierarchies with the transformer model. MViT hierarchically
expands the feature complexity while reducing visual resolution. In empirical evaluation, MViT shows a fundamental
advantage over single-scale vision transformers for video
and image recognition. We hope that our approach will foster
further research in visual recognition.
In this appendix, §A contains further ablations for Kinetics (§A.1) & ImageNet (§A.2), §C contains an analysis on
computational complexity of MHPA, and §B qualitative observations in MViT and ViT models. §D contains additional
implementation details for: Kinetics (§D.1), AVA (§D.2),
Charades (§D.3), SSv2 (§D.4), and ImageNet (§D.5).
SlowFast 16x8, R101+NL
SlowFast 8x8, R101+NL
MViT-B 16x4
MViT-B 32x2
ViViT-L ImageNet-21K
TimeSformer ImageNet-21K
VTN ImageNet-1K / 21K
Inference cost per video in TFLOPs (# of multiply-adds x 1012)
Kinetics top-1 val accuracy (%)
Inference cost per video in FLOPs (# of multiply-adds), log-scale
Kinetics top-1 val accuracy (%)
SlowFast 16x8, R101+NL
SlowFast 8x8, R101+NL
MViT-B 16x4
MViT-B 32x2
ViViT-L ImageNet-21K
TimeSformer ImageNet-21K
VTN ImageNet-1K / 21K
Figure A.4. Accuracy/complexity trade-off on K400-val for varying # of inference clips per video. The top-1 accuracy (vertical axis) is
obtained by K-Center clip testing where the number of temporal clips K ∈{1, 3, 5, 7, 10} is shown in each curve. The horizontal axis
measures the full inference cost per video. The left-sided plots show a linear and the right plots a logarithmic (log) scale.
A. Additional Results
A.1. Ablations: Kinetics Action Classiﬁcation
Inference cost. In the spirit of we aim to provide further ablations for the effect of using fewer testing clips for
efﬁcient video-level inference. In Fig. A.4 we analyze the
trade-off for the full inference of a video, when varying the
number of temporal clips used. The vertical axis shows
the top-1 accuracy on K400-val and the horizontal axis the
overall inference cost in FLOPs for different model families: MViT, X3D , SlowFast , and concurrent ViT
models, VTN ViT-B-TimeSformer ViT-L-ViViT ,
pre-trained on ImageNet-21K.
We ﬁrst compare MViT with concurrent Transformerbased methods in the left plot in Fig. A.4. All these methods,
VTN , TimeSformer and ViViT , pre-train on
ImageNet-21K and use the ViT model with modiﬁcations on top of it. The inference FLOPs of these methods
are around 5-10×higher than MViT models with equivalent
performance; for example, ViT-L-ViViT uses 4 clips of
1446G FLOPs (i.e. 5.78 TFLOPs) each to produce 80.3%
accuracy while MViT-B, 32×3 uses 5 clips of 170G FLOPs
(i.e. 0.85 TFLOPs) to produce 80.2% accuracy. Therefore,
MViT-L can provide similar accuracy at 6.8× lower FLOPs
(and 8.5× lower parameters), than concurrent ViViT-L .
More importantly, the MViT result is achieved without external data. All concurrent Transformer based works 
require the huge scale ImageNet-21K to be competitive, and
the performance degrades signiﬁcantly (-3% accuracy, see
IN-1K in Fig. A.4 for VTN ). These works further report
failure of training without ImageNet initialization.
The plot in Fig. A.4 right shows this same plot with a
logarithmic scale applied to the FLOPs axis. Using this scaling it is clearer to observe that smaller models convolutional
models (X3D-S and X3D-M) can still provide more efﬁcient
inference in terms of multiply-add operations and MViT-B
compute/accuracy trade-off is similar to X3D-XL.
Ablations on skip-connections. Recall that, at each scalestage transition in MViT, we expand the channel dimension
by increasing the output dimension of the previous stages’
MLP layer; therefore, it is not possible to directly apply
the original skip-connection design , because the input
channel dimension (Din) differs from the output channel
dimension (Dout). We ablate three strategies for this:
(a) First normalize the input with layer normalization
and then expand its channel dimension to match the output
dimension with a linear layer (Fig. A.5a); this is our default.
(b) Directly expand the channel dimension of the input
by using a linear layer to match the dimension (Fig. A.5b).
(c) No skip-connection for stage-transitions (Fig. A.5c).
Linear (Din, Dout)
THW × Dout
normalized
skipconnection
THW × Dout
Linear (Din, Dout)
(b) unnormalized skipconnection
THW × Dout
skipconnection
Figure A.5. Skip-connections at stage-transitions. Three skipconnection variants for expanding channel dimensions: (a) ﬁrst
normalize the input with layer normalization (Norm) and then
expand its channel dimension; (b) directly expand the channel
dimension of the input; (c) no skip-connection at stage-transitions.
normalized skip-connection
unnormalized skip-connection
no skip-connection
Table A.1. Skip-connections at stage-transitions on K400. We
use our base model, MViT-B 16×4.
Normalizing the skipconnection at channel expansion is essential for good performance.
Table A.1 shows the Kinetics-400 ablations for all 3 variants. Our default of using a normalized skip-connection (a)
obtains the best results with 77.2% top-1 accuracy, while
using an un-normalized skip-connection after channel expansion (b) decays signiﬁcantly to 74.6% and using no skipconnection for all stage-transitions (c) has a similar result.
We hypothesize that for expanding the channel dimension,
normalizing the signal is essential to foster optimization, and
use this design as our default in all other experiments.
SlowFast R50, 8×8
SlowFast R50, 8×8
SlowFast R101, 8×8
SlowFast R101, 8×8
Table A.2. SlowFast models with MViT recipe on Kinetics-400.
The default recipe is using the recipe from the original paper. Accuracy is evaluated on 10×3 views.
SlowFast with MViT recipe. To investigate if our training
recipe can beneﬁt ConvNet models, we apply the same augmentations and training recipe as for MViT to SlowFast in
Table A.2. The results suggest that SlowFast models do not
beneﬁt from the MViT recipe directly and more studies are
required to understand the effect of applying our trainingfrom-scratch recipe to ConvNets, as it seems higher capacity
ConvNets (R101) perform worse when using our recipe.
A.2. Ablations: ImageNet Image Classiﬁcation
We carry out ablations on ImageNet with the MViT-B-16
model with 16 layers, and show top-1 accuracy (Acc) as well
as computational complexity measured in GFLOPs (ﬂoatingpoint operations). We also report Parameters in M(106) and
training GPU memory in G(109) for a batch size of 512.
Table A.3. ImageNet: Key-Value pooling: We vary stride sH ×
sW , for pooling K and V . We use “adaptive” pooling that reduces
stride w.r.t. stage resolution.
Key-Value pooling for image classiﬁcation. The ablation
in Table A.3 analyzes the pooling stride s = sH × sW , for
pooling K and V tensors. Here, we use our default ‘adaptive’
pooling that uses a stride w.r.t. stage resolution, and keeps
the K, V resolution ﬁxed across all stages.
First, we compare the baseline which uses pooling with
a ﬁxed stride of 4×4 with a model has a stride of 8×8: this
drops accuracy from 82.5% to 81.6%, and reduces FLOPs
and memory by 0.6G and 2.9G.
Second, we reduce the stride to 2×2, which increases
FLOPs and memory signiﬁcantly but performs 0.7% worse
than our default stride of 4×4.
Third, we remove the K, V pooling completely which
increases FLOPs by 33% and memory consumption by 45%,
while providing lower accuracy than our default.
Overall, the results show that our K, V pooling is an
effective technique to increase accuracy and decrease cost
(FLOPs/memory) for image classiﬁcation.
B. Qualitative Experiments: Kinetics
In Figure A.6, we plot the mean attention distance for all
heads across all the layers of our Multiscale Transformer
model and its Vision Transformer counterpart, at initialization with random weights, and at convergence after training.
Each head represents a point in the plots (ViT-B has more
heads). Both the models use the exact same weight initialization scheme and the difference in the attention signature
stems purely from the multiscale skeleton in MViT. We observe that the dynamic range of attention distance is about
4× larger in the MViT model than ViT at initialization itself (A.6a vs. A.6b). This signals the strong inductive bias
stemming from the multiscale design of MViT. Also note
that while at initialization, every layer in ViT has roughly the
same mean attention distance, the MViT layers have strikingly different mean attention signatures indicating distinct
predilections towards global and local features.
The bottom row of Fig. A.6 shows the same plot for a
converged Vision Transformer (A.6c) and Multiscale Vision
Transformer (A.6d) model.
We notice very different trends between the two models
after training. While the ViT model (A.6c) has a consistent
increase in attention distance across layers, the MViT model
(A.6d) is not monotonic at all. Further, the intra-head variation in the ViT model decreases as the depth saturates, while,
for MViT, different heads are still focusing on different features even in the higher layers. This suggests that some of
the capacity in the ViT model might indeed be wasted with
redundant computation while the lean MViT heads are more
judiciously utilizing their compute. Noticeable is further a
larger delta (between initialization in Fig. A.6a and convergence in A.6c) in the overall attention distance signature in
the ViT model, compared to MViT’s location distribution.
(a) ViT-B at initialization
(b) MViT-B at initialization
(c) ViT-B at convergence
(d) MViT-B at convergence
Figure A.6. Mean attention distance across layers at initialization/convergence for Vision Transformer (a)/(c) & Multiscale Vision
Transformers (b)/(d). Each point shows the normalized average attention distance (weighted by the attention scores, with 1.0 being maximum
possible distance) for each head in a layer. MViT attends close and distant features throughout the network hierarchy.
C. Computational Analysis
Since attention is quadratic in compute and memory complexity, pooling the key, query and value vectors have direct beneﬁts on the fundamental compute and memory requirements of the pooling operator and by extension, on
the complete Multiscale Transformer model. Consider an
input tensor of dimensions T × H × W and corresponding
sequence length L = T · H · W. Further, assume the key,
query and value strides to be sK, sQ and sV . As described
in Sec. 3.1 in main paper, each of the vectors would experience a sptio-temporal resolution downsampling by a factor
of their corresponding strides. Equivalently, the sequence
length of query, key and value vectors would be reduced by
a factor of f Q, f K and f V respectively, where,
W , ∀j ∈{Q, K, V }.
Computational complexity. Using these shorter sequences
yields a corresponding reduction in space and runtime complexities for the pooling attention operator. Considering
key, query and value vectors to have sequence lengths L/fk,
L/fq and L/fv after pooling, the overall runtime complexity of computing the key, query and value embeddings is
O(THWD2/h) per head, where h is the number of heads
in MHPA. Further, the runtime complexity for calculating the
full attention matrix and the weighed sum of value vectors
with reduced sequence lengths is O(T 2H2W 2D/fqfhh)
per head. Computational complexity for pooling is
T(P(·; Θ)) = O
THW · D · kT kW kH
which is negligible compared to the quadratic complexity
of the attention computation and hence can be ignored in
asymptotic notation. Thus, the ﬁnal runtime complexity of
MHPA is O(THWD(D + THW/fqfk)).
Memory complexity. The space complexity for storing
the sequence itself and other tensors of similar sizes is
O(THWD). Complexity for storing the full attention matrix is O(T 2H2W 2h/fqfk). Thus the total space complexity of MHPA is O(THWh(D/h + THW/fqfk)).
Design choice. Note the trade-off between the number of
channels D and the sequence length term THW/fqfk in
both space and runtime complexity. This tradeoff in multi
head pooling attention informs two critical design choices
of Multiscale Transformer architecture.
First, as the effective spatio-temporal resolution decreases
with layers because of diminishing THW/fqfk, the channel
capacity is increased to keep the computational time spent
(FLOPs) roughly the same for each stage.
Second, for a ﬁxed channel dimension, D, higher number
of heads h cause a prohibitively larger memory requirement
because of the (D + h ∗THW/fqfk) term. Hence, Multiscale Transformer starts with a small number of heads which
is increased as the resolution factor THW/fqfk decreases,
to hold the effect of (D +h∗THW/fqfk) roughly constant.
D. Additional Implementation Details
We implement our model with PySlowFast . Code
and models are available at: 
facebookresearch/SlowFast.
D.1. Details: Kinetics Action Classiﬁcation
Architecture details. As in original ViT , we use residual connections and Layer Normalization (LN) in
the pre-normalization conﬁguration that applies LN at the
beginning of the residual function, and our MLPs consist of
two linear layers with GELU activation , where the ﬁrst
layer expands the dimension from D to 4D, and the second
restores the input dimension D, except at the end of a scalestage, where we increase this channel dimensions to match
the input of the next scale-stage. At such stage-transitions,
our skip connections receive an extra linear layer that takes
as input the layer-normalized signal which is also fed into
the MLP. In case of Q-pooling at scale-stage transitions, we
correspondingly pool the skip-connection signal.
Optimization details. We use the truncated normal distribution initialization in and adopt synchronized
AdamW training on 128 GPUs following the recipe
in . For Kinetics, we train for 200 epochs with 2
repeated augmentation repetitions. The mini-batch size
is 4 clips per GPU (so the overall batchsize is 512).
We adopt a half-period cosine schedule of learning
rate decaying: the learning rate at the n-th iteration is η ·
nmax π) + 1], where nmax is the maximum training
iterations and the base learning rate η is set as 1.6 · 10−3.
We linearly scale the base learning rate w.r.t. the overall
batch-size, η = 1.6·10−3 batchsize
, and use a linear warm-up
strategy in the ﬁrst 30 epochs . The cosine schedule is
completed when reaching a ﬁnal learning rate of 1.6 · 10−5.
We extract the class token after the last stage and use it as
the input to the ﬁnal linear layer to predict the output classes.
For Kinetics-600 all hyper-parameters are identical to K400.
Regularization details. We use weight decay of 5·10-2,
a dropout of 0.5 before the ﬁnal classiﬁer, labelsmoothing of 0.1 and use stochastic depth (i.e.
drop-connect) with rate 0.2.
Our data augmentation is performed on input clips by
applying the same transformation across all frames. To each
clip, we apply a random horizontal ﬂip, Mixup with
α = 0.8 to half of the clips in a batch and CutMix to
the other half, Random Erasing with probability 0.25,
and Rand Augment with probability of 0.5 for 4 layers
of maximum magnitude 7.
For the temporal domain, we randomly sample a clip
from the full-length video, and the input to the network are
T frames with a temporal stride of τ; denoted as T × τ .
For the spatial domain, we use Inception-style cropping
that randomly resizes the input area between a [min, max],
scale of [0.08, 1.00], and jitters aspect ratio between 3/4 to
4/3, before taking an H × W = 224×224 crop.
Fine-tuning from ImageNet. To ﬁne-tune our ViT-B baseline, we extend it to take a video clip of T = 8 frames
as input and initialize the model weights from the ViT-B
model pre-trained on ImageNet-21K dataset. The positional embedding is duplicated for each frame. We ﬁne-tune
the model for 30 epochs with SGD using the recipe in .
The mini-batch size is 2 clips per GPU and a half-period
cosine learning rate decay is used. We linearly scale the base
learning rate w.r.t. the overall batch-size, η = 10−3 batchsize
Weight decay is set to 10−4.
D.2. Details: AVA Action Detection
Dataset. The AVA dataset has bounding box annotations for spatiotemporal localization of (possibly multiple)
human actions. It has 211k training and 57k validation video
segments. We follow the standard protocol reporting mean
Average Precision (mAP) on 60 classes on AVA v2.2.
Detection architecture. We follow the detection architecture in to allow direct comparison of MViT against
SlowFast networks as a backbone.
First, we reinterpret our transformer spacetime cube outputs from MViT as a spatial-temporal feature map by concatenating them according to the corresponding temporal
and spatial location.
Second, we employ a the detector similar to Faster R-
CNN with minimal modiﬁcations adapted for video.
Region-of-interest (RoI) features are extracted at the
generated feature map from MViT by extending a 2D proposal at a frame into a 3D RoI by replicating it along the
temporal axis, similar as done in previous work ,
followed by application of frame-wise RoIAlign and
temporal global average pooling. The RoI features are then
max-pooled and fed to a per-class, sigmoid classiﬁer for
prediction.
Training. We initialize the network weights from the Kinetics models and adopt synchronized SGD training on 64
GPUs. We use 8 clips per GPU as the mini-batch size and a
half-period cosine schedule of learning rate decaying. The
base learning rate is set as 0.6. We train for 30 epochs
with linear warm-up for the ﬁrst 5 epochs and use a
weight decay of 10−8 and stochastic depth with rate
0.4. Ground-truth boxes, and proposals overlapping with
ground-truth boxes by IoU > 0.9, are used as the samples
for training. The region proposals are identical to the ones
used in .
Inference. We perform inference on a single clip with
T frames sampled with stride τ centered at the frame that is
to be evaluated.
D.3. Details: Charades Action Classiﬁcation
Dataset. Charades has ∼9.8k training videos and 1.8k
validation videos in 157 classes in a multi-label classiﬁcation
setting of longer activities spanning ∼30 seconds on average.
Performance is measured in mean Average Precision (mAP).
Training. We ﬁne-tune our MViT models from the Kinetics
models. A per-class sigmoid output is used to account for
the multi-class nature. We train with SGD on 32 GPUs for
200 epochs using 8 clips per GPU. The base learning rate
is set as 0.6 with half-period cosine decay. We use weight
decay of 10-7 and stochastic depth with rate 0.45. We
perform the same data augmentation schemes as for Kinetics
in §D.1, except of using Mixup.
Inference. To infer the actions over a single video, we
spatio-temporally max-pool prediction scores from multiple
clips in testing .
D.4. Details: Something-Something V2 (SSv2)
Dataset. The Something-Something V2 dataset contains 169k training, and 25k validation videos. The videos
show human-object interactions to be classiﬁed into 174
classes. We report accuracy on the validation set.
Training. We ﬁne-tune the pre-trained Kinetics models. We
train for 100 epochs using 64 GPUs with 8 clips per GPU
and a base learning rate of 0.02 with half-period cosine
decay . Weight decay is set to 10−4 and stochastic
depth rate is 0.4. Our training augmentation is the same
as in §D.1, but as SSv2 requires distinguishing between
directions, we disable random ﬂipping in training. We use
segment-based input frame sampling that splits each
video into segments, and from each of them, we sample one
frame to form a clip.
Inference. We take single clip with 3 spatial crops to form
predictions over a single video in testing.
D.5. Details: ImageNet
Datasets. For image classiﬁcation experiments, we perform our experiments on ImageNet-1K dataset that
has ∼1.28M images in 1000 classes. We train models on the
train set and report top-1 and top-5 classiﬁcation accuracy
(%) on the val set. Inference cost (in FLOPs) is measured
from a single center-crop with resolution of 2242 if the input
resolution was not speciﬁcally mentioned.
Training. We use the training recipe of DeiT and summarize it here for completeness. We train for 100 epochs
with 3 repeated augmentation repetitions (overall computation equals 300 epochs), using a batch size of 4096 in
64 GPUs. We use truncated normal distribution initialization and adopt synchronized AdamW optimization
with a base learning rate of 0.0005 per 512 batch-size that
is warmed up and decayed as half-period cosine, as in .
We use a weight decay of 0.05, label-smoothing of
0.1. Stochastic depth (i.e. drop-connect) is also used
with rate 0.1 for model with depth of 16 (MViT-B-16), and
rate 0.3 for deeper models (MViT-B-24). Mixup with
α = 0.8 to half of the clips in a batch and CutMix to
the other half, Random Erasing with probability 0.25,
and Rand Augment with maximum magnitude 9 and
probability of 0.5 for 4 layers (for max-pooling) or 6 layers
(for conv-pooling).
Acknowledgements
We are grateful for discussions with Chao-Yuan Wu,
Ross Girshick, and Kaiming He.