Image Augmentations for GAN Training
Zhengli Zhao∗
 
Zizhao Zhang, Ting Chen
Google Research
Sameer Singh
Google Research
Data augmentations have been widely studied to improve the accuracy and robustness of classiﬁers. However, the potential of image augmentation in improving
GAN models for image synthesis has not been thoroughly investigated in previous
studies. In this work, we systematically study the effectiveness of various existing
augmentation techniques for GAN training in a variety of settings. We provide
insights and guidelines on how to augment images for both vanilla GANs and
GANs with regularizations, improving the ﬁdelity of the generated images substantially. Surprisingly, we ﬁnd that vanilla GANs attain generation quality on par with
recent state-of-the-art results if we use augmentations on both real and generated
images. When this GAN training is combined with other augmentation-based
regularization techniques, such as contrastive loss and consistency regularization,
the augmentations further improve the quality of generated images. We provide
new state-of-the-art results for conditional generation on CIFAR-10 with both
consistency loss and contrastive loss as additional regularizations.
Introduction
Data Augmentation has played an important role in deep representation learning. It increases the
amount of training data in a way that is natural/useful for the domain, and thus reduces over-ﬁtting
when training deep neural networks with millions of parameters. In the image domain, a variety
of augmentation techniques have been proposed to improve the performance of different visual
recognition tasks such as image classiﬁcation , object detection , and semantic
segmentation . The augmentation strategies also range from the basic operations like random
crop and horizontal ﬂip to more sophisticated handcrafted operations , or even the
strategies directly learned by the neural network . However, previous studies have not provided
a systematic study of the impact of the data augmentation strategies for deep generative models,
especially for image generation using Generative Adversarial Networks (GANs) , making it
unclear how to select the augmentation techniques, which images to apply them to, how to incorporate
them in the loss, and therefore, how useful they actually are.
Compared with visual recognition tasks, making the right choices for the augmentation strategies for
image generation is substantially more challenging. Since most of the GAN models only augment real
images as they are fed into the discriminator, the discriminator mistakenly learns that the augmented
images are part of the image distribution. The generator thus learns to produce images with undesired
augmentation artifacts, such as cutout regions and jittered color if advanced image augmentation
operations are used . Therefore, the state-of-the-art GAN models 
prefer to use random crop and ﬂip as the only augmentation strategies. In unsupervised and selfsupervised learning communities, image augmentation becomes a critical component of consistency
regularization . Recently, Zhang et al. studied the effect of several augmentation
strategies when applying consistency regularization in GANs, where they enforced the discriminator
∗Work done as an intern on Google Brain team.
 
 
Original Image
Translation
TranslationX
TranslationY
Brightness
InstanceNoise
CutOut 
CutMix 
Figure 1: Different augmentation techniques applied to the original image.
outputs to be unchanged when applying several perturbations to the real images. Zhao et al. 
have further improved the generation quality by adding augmentations on both the generated samples
and real images. However, it remains unclear about the best strategy to use augmented data in
GANs: Which image augmentation operation is more effective in GANs? Is it necessary to add
augmentations in generated images as in Zhao et al. ? Should we always couple augmentation
with consistency loss like by Zhang et al. ? Can we apply augmentations together with other loss
constraints besides consistency?
In this paper, we comprehensively evaluate a broad set of common image transformations as augmentations in GANs. We ﬁrst apply them in the conventional way—only to the real images fed
into the discriminator. We vary the strength for each augmentation and compare the generated
samples in FID to demonstrate the efﬁcacy and robustness for each augmentation. We then
evaluation the quality of generation when we add each augmentation to both real images and samples
generated during GAN training. Through extensive experiments, we conclude that only augmenting
real images is ineffective for GAN training, whereas augmenting both real and generated images
consistently improve GAN generation performance signiﬁcantly. We further improve the results by
adding consistency regularization on top of augmentation strategies and demonstrate such
regularization is necessary to achieve superior results. Finally, we apply consistency loss together
with contrastive loss, and show that combining regularization constraints with the best augmentation
strategy achieves the new state-of-the-art results.
In summary, our contributions are as follows:
• We conduct extensive experiments to assess the efﬁcacy and robustness for different augmentations in GANs to guide researchers and practitioners for future exploration.
• We provide a thorough empirical analysis to demonstrate augmentations should be added to
both real and fake images, with the help of which we improve the FID of vanilla BigGAN to
11.03, outperforming BigGAN with consistency regularization in Zhang et al. .
• We demonstrate that adding regularization on top of augmentation furthers boost the quality.
Consistency loss compares favorably against contrastive loss as the regularization approach.
• We achieve new state-of-the-art for image generation by applying contrastive loss and
consistency loss on top of the best augmentation we ﬁnd. We improve the state-of-the-art
FID of conditional image generation for CIFAR-10 from 9.21 to 8.30.
Augmentations and Experiment Settings
We ﬁrst introduce the image augmentation techniques we study in this paper, and then elaborate on
the datasets, GAN architectures, hyperparameters, and evaluation metric used in the experiments.
Image Augmentations. Our goal is to investigate how each image operation performs in the GAN
setting. Therefore, instead of chaining augmentations , we have selected 10 basic image
augmentation operations and 3 advanced image augmentation techniques as the candidates T , which
are illustrated in Figure 1. The original image I0 of size (H, W) is normalized with the pixel range
in . For each augmentation t ∼T , the strength λaug is chosen uniformly in the space ranging
from the weakest to the strongest one. We note that t(I0) is the augmented image and we detail each
augmentation in Section B in the appendix.
Data. We validate all the augmentation strategies on the CIFAR-10 dataset , which consists of
60K of 32x32 images in 10 classes. The size of this dataset is suitable for a large scale study in
GANs . Following previous work, we use 50K images for training and 10K for evaluation.
Evaluation metric. We adopt Fréchet Inception Distance (FID) as the metric for quantitative
evaluation. We admit that better (i.e., lower) FID does not always imply better image quality, but
FID is proved to be more consistent with human evaluation and widely used for GAN evaluation.
Following Kurach et al. , we carry out experiments with different random seeds, and aggregate
all runs and report FID of the top 15% trained models. FID is calculated on the test dataset with 10K
generated samples and 10K test images.
GAN architectures and training hyperparameters. The search space for GANs is prohibitively
large. As our main purpose is to evaluate different augmentation strategies, we select two commonly
used settings and GANs architectures for evaluation, namely SNDCGAN for unconditional
image generation and BigGAN for conditional image generation. As in previous work , we
train SNDCGAN with batch size 64 and the total training step is 200k. For conditional BigGAN, we
set batch size as 256 and train for 100k steps. We choose hinge loss for all the experiments.
More details of hyperparameter settings can be found in appendix.
We ﬁrst study augmentations on vanilla SNDCGAN and BigGAN without additional regularizations
in Section 3, then move onto these GANs with additional regularizations that utilize augmentations,
namely consistency regularization (detailed in Section 4) and contrastive loss (detailed in Section 5).
Effect of Image Augmentations for Vanilla GAN
In this section, we ﬁrst study the effect of image augmentations when used conventionally—only augmenting real images. Then we propose and study a novel way where both real and generated images
are augmented before fed into the discriminator, which substantially improves GANs’ performance.
Augmenting Only Real Images Does Not Help with GAN Training
Figure 2: FID comparisons of SNDCGAN trained on augmented real images only. It shows only
augmenting real images is not helpful with vanilla GAN training, which is consistent with the result
in Section 4.1 of Zhang et al. . Corresponding plots of BigGAN results are in the appendix.
Figure 3: FID comparisons of SNDCGAN on CIFAR-10. The red dashed horizontal line shows the
baseline FID=24.73 of SNDCGAN trained without data augmentation. ‘vanilla_rf’ (Section 3.2)
represents training vanilla SNDCGAN and augmenting both real images as well as generated fake
images concurrently before fed into the discriminator. And ‘bcr’ (Section 4) corresponds to training
SNDCGAN with Balanced Consistency Regularization on augmented real and fake images. This
ﬁgure can be utilized as general guidelines for training GAN with augmentations. The main implications are: (1) Simply augmenting real and fake images can make the vanilla GAN’s performance
on par with recent proposed CR-GAN . (2) With the help of BCR on augmented real and fake
images, the generation ﬁdelity can be improved by even larger margins. (3) Spatial augmentations
outperform visual augmentations. (4) Augmentations that result in images out of the natural data
manifold, e.g. InstanceNoise, cannot help with improving GAN performance.
We ﬁrst compare the effect of image augmentations when only applied to the real images, the de-facto
way of image augmentations in GANs . Figure 2 illustrates the FID of the generated images
with different strengths of each augmentation. We ﬁnd augmenting only real images in GANs worsens
the FID regardless of the augmentation strengths or strategies. For example, the baseline SNDCGAN
trained without any image augmentation achieves 24.73 in FID , while translation, even with its
smallest strength, gets 31.03. Moreover, FID increases monotonically as we increase the strength of
the augmentation. This conclusion is surprising given the wide adoption of this conventional image
augmentations in GANs. We note that the discriminator is likely to view the augmented data as part
of the data distribution in such case. As shown by Figures 7 to 10 in the appendix, the generated
images are prone to contain augmentation artifacts. Since FID is calculating the feature distance
between generated samples and unaugmented real images, we believe the augmented artifacts in the
synthesized samples are the underlying reason for the inferior FID.
Augmenting Both Real and Fake Images Improves GANs Consistently
Based on the above observation, it is natural to wonder whether augmenting generated images in
the same way before feeding them into the discriminator can alleviate the problem. In this way,
augmentation artifacts cannot be used to distinguish real and fake images by the discriminator.
To evaluate the augmentation of synthetic images, we train SNDCGAN and BigGAN by augmenting
both real images as well as generated images concurrently before feeding them into the discriminator
during training. Different from augmenting real images, we keep the gradients for augmented
generated images to train the generator. The discriminator is now trained to differentiate between the
augmented real image t(Ireal) and the augmented fake image t(G(z)). We present the generation FID
of SNDCGAN and BigGAN in Figures 3 and 5 (denoted as ‘vanilla_rf’), where the horizontal lines
show the baseline FIDs without any augmentations. As illustrated by Figure 3, this new augmentation
strategy considerably improves the FID for different augmentations with varying strengths. By
comparing the results in Figure 3 and Figure 2, we conclude that augmenting both real and fake
images can substantially improves the generation performance of GAN. Moreover, for SNDCGAN,
Figure 4: Distances between real and fake distributions with different augmentations. Note that we
report FID(Ireal, Ifake) normally throughout the paper. While here to show the changes of real and
fake image distributions with augmentations, we also calculate FID(t(Ireal), t(Ifake)) and present its
fraction over normal FID as y-axis. The Frechet Inception Distance between real and fake images
gets smaller with augmentations, while stronger augmentations result in more distribution overlaps.
we ﬁnd the best FID 18.94 achieved by translation of strength 0.1 is comparable to the FID 18.72
reported in Zhang et al. with consistency regularization only on augmented real images. This
observation holds for BigGAN as well, where we get FID 11.03 and the FID of CRGAN is 11.48.
These results suggest that image augmentations for both real and fake images considerably improve
the training of vanilla GANs, which has not been studied by previous work, to our best knowledge.
We compare the effectiveness of augmentation operations in Figures 3 and 5. The operations in the
top row such as translation, zoomin, and zoomout, are much more effective than the operations in the
bottom rows, such as brightness, colorness, and mixup. We conclude that augmentations that result in
spatial changes improve the GAN performance more than those that induce mostly visual changes.
Augmentations Increase the Support Overlap between Real and Fake Distributions
In this section, we investigate the reasons why augmenting both real and fake images improves GAN
performance considerably. Roughly, GANs’ objective corresponds to making the generated image
distribution close to real image distribution. However, as mentioned by previous work , the
difﬁculty of training GANs stems from these two being concentrated distributions whose support
do not overlap: the real image distribution is often assumed to concentrate on or around a lowdimensional manifold, and similarly, generated image distribution is degenerate by construction.
Therefore, Sønderby et al. propose to add instance noise (i.e., Gaussian Noise) as augmentation
for both real images and fakes image to increase the overlap of support between these two distributions.
We argue that other semantic-preserving image augmentations have a similar effect to increase the
overlap, and are much more effective for image generation.
In Figure 4, we show that augmentations t ∼T can lower FID between augmented t(Ireal) and
t(Ifake), which indicates that the support of image distribution and the support of model distribution
have more overlaps with augmentations. However, not all augmentations or strengths can improve
the quality of generated images, which suggests naively pulling distribution together may not always
improve the generation quality. We hypothesize certain types of augmentations and augmentations of
high strengths can result in images that are far away from the natural image distribution; we leave the
theoretical justiﬁcation for future work.
Effect of Image Augmentations for Consistency Regularized GANs
We now turn to more advanced regularized GANs that built on their usage of augmentations. Consistency Regularized GAN (CR-GAN) has demonstrated that consistency regularization can
signiﬁcantly improve GAN training stability and generation performance. Zhao et al. improves
this method by introducing Balanced Consistency Regularization (BCR), which applying BCR to
both real and fake images. Both methods requires images to be augmented for processing, and we
brieﬂy summarize BCR-GAN with Algorithm 1 in the appendix.
Figure 5: FID mean and std of BigGAN on CIFAR-10. The blue dashed horizontal line shows the
baseline FID=14.73 of BigGAN trained without augmentation. ‘vanilla_rf’ (Section 3.2) represents
training vanilla BigGAN with both real and fake images augmented. ‘bcr’ (Section 4) corresponds
to training BigGAN with BCR on augmented real and fake images. This ﬁgure can be utilized as
general guidelines for training GAN with augmentations, sharing similar implications as in Figure 3.
However, neither of the works studies the impact and importance of individual augmentation and only
very basic geometric transformations are used as augmentation. We believe an in-depth analysis of
augmentation techniques can strengthen the down-stream applications of consistency regularization
in GANs. Here we mainly focus on analyzing the efﬁcacy of different augmentations on BCR-GAN.
We set the BCR strength λBCR = 10 in Algorithm 1 according to the best practice. We present
the generation FID of SNDCGAN and BigGAN with BCR on augmented real and fake images in
Figures 3 and 5 (denoted as ‘bcr’), where the horizontal lines show the baseline FIDs without any
augmentation. Experimental results suggest that consistency regularization on augmentations for
real and fake images can further boost the generation performance.
More importantly, we can also signiﬁcantly outperform the state of the art by carefully selecting the
augmentation type and strength. For SNDCGAN, the best FID 14.72 is with zoomout of strength
0.4, while the corresponding FID reported in Zhao et al. is 15.87 where basic translation of
4 pixels and ﬂipping are applied. The best BigGAN FID 8.65 is with translation of strength 0.4,
outperforming the corresponding FID 9.21 reported in Zhao et al. .
Similarly as in Section 3.2, augmentation techniques can be roughly categorized into two groups, in
the descending order of effectiveness: spatial transforms, zoomout, zoomin, translation, translationx,
translationy, cutout, cutmix; and visual transforms, brightness, redness, greenness, blueness, mixup.
Spatial transforms, which retain the major content while introducing spatial variances, can substantially improve GAN performance together with BCR. On the other hand, instance noise , which
may be able to help stabilize GAN training, cannot improve generation performance.
Effect of Images Augmentations for GANs with Contrastive Loss
Image augmentation is also an essential component of contrastive learning, which has recently led
to substantially improved performance on self-supervised learning . Given the success of
contrastive loss for representation learning and the success of consistency regularization in GANs, it
naturally raises the question of whether adding such a regularization term helps in training GANs?
In this section, we ﬁrst demonstrate how we apply contrastive loss (CntrLoss) to regularizing
GAN training. Then we analyze on how the performance of Cntr-GAN is affected by different
augmentations, including variations of an augmentation set in existing work .
Contrastive Loss for GAN Training
The contrastive loss was originally introduced by Hadsell
et al. in such a way that corresponding positive pairs are pulled together while negative pairs
are pushed apart. Here we propose Cntr-GAN, where contrastive loss is applied to regularizing the
discriminator on two random augmented copies of both real and fake images. CntrLoss encourages
the discriminator to push different image representations apart, while drawing augmentations of the
same image closer. Due to space limit, we detail the CntrLoss in Appendix D and illustrate how our
Cntr-GAN is trained with augmenting both real and fake images (Algorithm 2) in the appendix.
For augmentation techniques, we adopt and sample the augmentation as described in Chen et al. ,
referring it as simclr. Details of simclr augmentation can be found in the appendix (Section B). Due
to the preference of large batch size for CntrLoss, we mainly experiment on BigGAN which has
higher model capacity. As shown in Table 1, Cntr-GAN outperforms baseline BigGAN without any
augmentation, but is inferior to BCR-GAN.
Table 1: BigGAN and regularizations.
Regularization
InceptionScore
Since both BCR and CntrLoss utilize augmentations
but are complementary in how they draw positive image
pairs closer and push negative pairs apart, we further experiment on regularizing BigGAN with both CntrLoss
and BCR. We are able to achieve new state-of-the-art
FID = 8.30 with λCntr = 0.1, λBCR = 5. Table 1
compares the performance of vanilla BigGAN against
BigGAN with different regularizations on augmentations, and Figure 12 in the appendix shows how the
strengths affect the results. While BCR enforces the
consistency loss directly on the discriminator logits, with Cntr together, it further helps to learn better
representations which can be reﬂected in generation performance eventually.
Cntr-GAN Beneﬁts From Stronger Augmentations
In Table 1, we adopt default augmenations
in the literature for BCR and CntrLoss . Now we further study which image transform used by
simclr affects Cntr-GAN the most, and also the effectiveness of the other augmentations we consider
in this paper. We conducted extensive experiment on Cntr-GAN with different augmentations and
report the most representative ones in Figure 6.
Overall, we ﬁnd Cntr-GAN prefers stronger augmentation transforms compared to BCR-GAN.
Spatial augmentations still work better than visual augmentations, which is consistent with our
observation that changing the color jittering strength of simclr cannot help improve performance. In
Figure 6, we present the results of changing the cropping/resizing strength in ‘simclr’, along with the
other representative augmentation methods that are helpful to Cntr-GAN. For most augmentations,
CntrGAN reaches the best performance with higher augmentation strength around 0.5. For CntrGAN,
we achieve the best FID of 11.87 applying adjusted simclr augmentations with the cropping/resizing
strength of 0.3.
Discussion
Here we provide additional analysis and discussion for several different aspects. Due to space limit,
we summarize our ﬁndings below and include visualization of the results in the appendix.
Figure 6: BigGAN regularized by CntrLoss with different image augmentations. The blue dashed
horizontal line shows the baseline FID=14.73 of BigGAN trained without augmentation. Here we
adjust the strength of cropping-resizing in the default simclr. Cntr-GAN consistently outperforms
vanilla GAN with preferance on spatial augmentations.
Artifacts. Zhao et al. show that imbalanced (only applied to real images) augmentations and
regularizations can result in corresponding generation artifacts for GAN models. Therefore, we
present qualitative images sampled randomly for different augmentations and settings of GAN training
in the appendix (Section E). For vanilla GAN, augmenting both real and fake images can reduce
generation artifacts substantially than only augmenting real images. With additional contrastive loss
and consistency regularization, the generation quality can be improved further.
Annealing Augmentation Strength. We have extensively experimented with ﬁrst setting λaug, which
constrains the augmentation strength, then sampling augmentations randomly. But how would GANs’
performance change if we anneal λaug during training? Our experiments show that annealing the
strength of augmentations during training would reduce the effect of the augmentation, without
changing the relative efﬁcacy of different augmentations. Augmentations that improve GAN training
would alleviate their improvements with annealing; and vice versa.
Composition of Transforms. Besides a single augmentation transform, the composition of multiple
transforms are also used . Though the dimension of random composition of transforms is
out of this paper’s scope, we experiment with applying both translation and brightness, as spatial
and visual transforms respectively, to BCR-GAN training. Preliminary results show that this chained
augmentation can achieve the best FID=8.42, while with the single augmentation translation the best
FID achieved is 8.58, which suggests this combination is dominant by the more effective translation.
We leave it to future work to search for the best strategy of augmentation composition automatically.
Related Work
Data augmentation has shown to be critical to improve the robustness and generalization of deep
learning models, and thus it is becoming an essential component of visual recognition systems
 . More recently, it also becomes one of the most impetus on semi-supervised
learning and unsupervised learning . The augmentation operations also evolve from
the basic random cropping and image mirroring to more complicated strategies including geometric
distortions (e.g., changes in scale, translation and rotation), color jittering (e.g, perturbations in
brightness, contrast and saturation) and combination of multiple image statistics .
Nevertheless, these augmentations are still mainly studied in image classiﬁcation tasks. As for image
augmentations in GANs , the progress is very limited: from DCGAN to BigGAN and
StyleGAN2 , the mainstream work is only using random cropping and horizontal ﬂipping as
the exclusive augmentation strategy. It remains unclear to the research community whether other
augmentations can improve quality of generated samples. Recently, Zhang et al. stabilized
GAN training by mixing both the input and the label for real samples and generated ones. Sønderby
et al. added Gaussian noise to the input images and annealed its strength linearly during the
training to achieve better convergence of GAN models. Arjovsky and Bottou derived the same
idea independently from a theoretical perspective. They have shown adding Gaussian noise to both
real and fake images can alleviate training instability when the support of data distribution and model
distribution do not overlap. Salimans et al. further extended the idea by adding Gaussian noise to
the output of each layer of the discriminator. Jahanian et al. found data augmentation improves
steerability of GAN models, but they failed to generate realistic samples on CIFAR-10 when jointly
optimizing the model and linear walk parameters. Besides simply adding augmentation to the data,
some recent work further added the regularization on top of augmentations to improve the
model performance. For example, Self-Supervised GANs make the discriminator to predict
the angle of rotated images and CRGAN enforce consistency for different image perturbations.
Conclusion
In this work, we have conducted a thorough analysis on the performance of different augmentations for
improving generation quality of GANs. We have empirically shown adding the augmentation to both
real images and generated samples is critical for producing realistic samples. Moreover, we observe
that applying consistency regularization onto augmentations can further boost the performance and
it is superior to applying contrastive loss. Finally, we achieve state-of-the-art image generation
performance by combining constrastive loss and consistency loss. We hope our ﬁndings can lay a
solid foundation and help ease the research in applying augmentations to wider applications of GANs.
Acknowledgments
The authors would like to thank Marvin Ritter, Xiaohua Zhai, Tomer Kaftan, Jiri Simsa, Yanhua Sun,
and Ruoxin Sang for support on questions of codebases; as well as Abhishek Kumar, Honglak Lee,
and Pouya Pezeshkpour for helpful discussions.