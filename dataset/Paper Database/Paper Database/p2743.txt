Edinburgh Research Explorer
Augmenting Image Classifiers Using Data Augmentation
Generative Adversarial Networks
Citation for published version:
Antoniou, A, Storkey, A & Edwards, H 2018, Augmenting Image Classifiers Using Data Augmentation
Generative Adversarial Networks. in Artificial Neural Networks and Machine Learning – ICANN 2018.
Lecture Notes in Computer Science, vol. 11141, Rhodes, Greece, pp. 594-603, 27th International
Conference on Artificial Neural Networks , Rhodes, Greece, 4/10/18. 
01424-7_58
Digital Object Identifier (DOI):
10.1007/978-3-030-01424-7_58
Link to publication record in Edinburgh Research Explorer
Document Version:
Peer reviewed version
 
Artificial Neural Networks and Machine Learning – ICANN 2018
General rights
Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)
and / or other copyright owners and it is a condition of accessing these publications that users recognise and
abide by the legal requirements associated with these rights.
Take down policy
The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer
content complies with UK legislation. If you believe that the public display of this file breaches copyright please
contact providing details, and we will remove access to the work immediately and
investigate your claim.
Download date: 26. Mar. 2025
Augmenting Image Classiﬁers using Data
Augmentation Generative Adversarial Networks
Antreas Antoniou1, Amos Storkey1, and Harrison Edwards1,2
1 University of Edinburgh, Edinburgh, UK
{a.antoniou,a.storkey,h.l.edwards}@sms.ed.ac.uk
 
 
Abstract. Eﬀective training of neural networks requires much data. In
the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more eﬀectively, but standard data augmentation produces
only limited plausible alternative data. Given the potential to generate
a much broader set of augmentations, we design and train a generative
model to do data augmentation. The model, based on image conditional
Generative Adversarial Networks, uses data from a source domain and
learns to take a data item and augment it by generating other withinclass data items. As this generative process does not depend on the
classes themselves, it can be applied to novel unseen classes. We demonstrate that a Data Augmentation Generative Adversarial Network (DA-
GAN) augments classiﬁers well on Omniglot, EMNIST and VGG-Face.
Introduction
Over the last decade Deep Neural Networks have enabled unprecedented performance on a number of tasks. They have been demonstrated in many domains 
including image classiﬁcation , machine translation , natural language processing , speech recognition , and synthesis , learning
from human play and reinforcement learning among others.
In all cases, very large datasets have been utilized, or in the case of reinforcement learning, extensive play. In many realistic settings we need to achieve goals
with limited datasets; in those cases deep neural networks seem to fall short,
overﬁtting on the training set and producing poor generalisation on the test set.
Techniques have been developed over the years to help combat overﬁtting
such as L1/L2 reqularization , dropout , batch normalization , batch
renormalisation or layer normalization . However in low data regimes,
even these techniques fall short, since the the ﬂexibility of the network is so
high. These methods are not able to capitalise on known input invariances that
might form good prior knowledge for informing the parameter learning.
It is also possible to generate more data from existing data by applying various transformations to the original dataset. These transformations include
Augmenting Image Classiﬁers using DAGAN
random translations, rotations and ﬂips as well as addition of Gaussian noise.
Such methods capitalize on transformations that we know should not aﬀect the
class. This technique seems to be vital, not only for the low-data cases but for
any size of dataset, in fact even models trained on some of the largest datasets
such as Imagenet can beneﬁt from this practice.
Typical data augmentation techniques use a limited set of known invariances
that are easy to invoke. Here, we recognize that we can learn a model of a
much larger invariance space through training a form of conditional generative
adversarial network (GAN) in some source domain. This can then be applied
in the low-data domain of interest, the target domain. We show that such a
Data Augmentation Generative Adversarial Network (DAGAN) enables eﬀective
neural network training even in low-data target domains. As the DAGAN does
not depend on the classes themselves it captures the cross-class transformations,
moving data-points to other points of equivalent class.
In this paper we train a DAGAN and then evaluate its performance on lowdata tasks using standard stochastic gradient descent neural network training.
We use 3 datasets, the Omniglot dataset, the EMNIST dataset and the more
complex VGG-Face dataset. The DAGAN trained on Omniglot was used for
augmenting both the Omniglot and EMNIST classiﬁers to demonstrate beneﬁt
even when transferring between substantially diﬀerent domains. The VGG-Face
dataset provides a considerably more challenging test for the DAGAN. VGG-
Face was used to evaluate whether the DAGAN training scheme could work on
human faces, which are notoriously hard to model using a generator. Furthermore
the usefulness of the generated faces was measured when used as augmentation
data in the classiﬁcation training.
Background
Transfer Learning and Dataset Shift: The term dataset shift generalises
the concept of covariate shift to multiple cases of changes between
domains. For data augmentation, we may learn a generative distribution that
maintains class consistency on one set of classes and apply that consistency transformation to new unseen classes, on the understanding the the transformations
that maintain consistency generalise across classes.
Generative Adversarial Networks: GANs , and speciﬁcally Deep
Convolutional GANs (DCGAN) use the ability to discriminate between
true and generated examples as a learning objective for generative models. GAN
approaches can learn complex joint densities. Recent improvements in the optimization process have reduced some of the failure modes of the GAN
learning process as well as produced objectives that correlate well with sample
quality . Furthermore image conditional GANs have been used to achieve
image to image translation , as well as augment datasets . However the work relating to the enhancement of datasets only uses the GAN to
either ﬁne tune simulated data or generate data by attempting to reconstruct
Augmenting Image Classiﬁers using DAGAN
existing data points. Whereas our model is explicitly trained to produce data
augmentations as a manifold of samples around real data samples.
As demonstrated in , the Wasserstein formulation for training GANs has
shown superior sample diversity and quality in multiple instances. Additionally
the Wasserstein GANs (WGAN) with Gradient Penalty (GP) have the additional beneﬁt of being trainable using advanced architectures such as ResNets
 . This is especially important since most GAN formulations can only be
successfully trained using very speciﬁc and often less expressive model architectures. Furthermore WGAN with GP discriminator losses have been empirically
observed to correlate with sample quality. Taking into consideration available
state of the art methods including standard GAN, LS-GAN, WGAN with clipping and WGAN with Spectral normalization, we focus on the use WGAN with
GP training in this paper due to its versatility in terms of architectures and its
superior qualitative performance. Our own experiments with other approaches
conﬁrm the stated beneﬁts; we found WGAN with GP to produce the most stable models with the best sample quality both qualitatively and quantitatively.
Data Augmentation: Data augmentation similar to is routinely used
in classiﬁcation problems. Often it is non-trivial to encode known invariances
in a model. It can be easier to encode those invariances in the data instead
by generating additional data items through transformations from existing data
items. For example the labels of handwritten characters should be invariant to
small shifts in location, small rotations or shears, changes in intensity, changes
in stroke thickness, changes in size etc. Almost all cases of data augmentation
are from a priori known invariance. Various attempts at augmenting features
instead of data are investigated in . Moreover, the eﬀectiveness of data
augmentation has also been shown in other domains except images. Two such
domains is sound and text . There has been little previous work that
attempts to learn data augmentation strategies. One paper that is worthy of
note is the work of , where the authors learn augmentation strategies on a
class by class basis. Additional papers that attempt to learn models for data
augmentation include . These approaches do not transfer to the setting
where completely new classes are considered.
If we know that a class label should be invariant to a particular transformation
then we can apply that transformation to generate additional data. If we do
not know what transformations might be valid, but we have other data from related problems, we can attempt to learn valid transformations from those related
problems that we can apply to our setting. This is an example of meta-learning;
we learn on other problems how to improve learning for our target problem.
Model Overview
Consider a collection of datasets [(xc
i|i = 1, 2, . . . N c)|c ∈C], with each dataset
labelled by c, the class, taken from the set of classes C, and with each element
Augmenting Image Classiﬁers using DAGAN
in a dataset c indexed by i and denoted by xc
i ∈X, the space of inputs.
In this paper X will be a space of input images.
The goal is to learn a mapping between a conditional sample xc
i of a certain
class c to other samples xc
j from that same class, using training data [(xc
1, 2, . . . N c)|c ∈C]. To do so we learn a diﬀerentiable function G which we
call a generator. Given some random standard Gaussian vector z, we require a
mapping G : (xc
i, z) such that, ∀j, xc
j has high probability under the density of
z mapped through G. Since G is diﬀerentiable, z maps out a whole manifold in
X space associated with input xc
i in a class consistent way. Yet G does not have
access to the class c itself, thus enabling the DAGAN to generalize to unseen
classes. We parameterize our generator function ˜x = G(xc
i, z) as a neural network
and we train it as a GAN using the WGAN with GP formulation. Training a
GAN also requires a discriminator network, denoted as D, to be trained along
with the generator network. The discriminator network attempts to discriminate
between real and fake samples whilst the generator attempts to minimize the
discriminator’s performance in guessing real from fake.
Model Objective Deﬁnition
We modify the WGAN with GP formulation to account for the fact that we
are using an image-conditional GAN with a discriminator that takes as input
2 images, instead of 1. Figure 1 shows the high level overview of our training
setup. Our generator and discriminator objectives can be expressed as:
(||∇ˆxD(xc
i, ˆx)||2 −1)
where x represents real samples, xc
j represent two separate instances of
samples from class c, ˜x represents generated samples from the generator G. ˆx
is, as deﬁned in , randomly sampled points on linear interpolations between
the samples of the real distribution Pr and generated distribution Pg. The only
diﬀerence from the original WGAN with GP formulation is the use of 2 entries
in the discriminator arguments, one for the conditional sample xc
i and one for
the target sample xc
j (for real case) or ˜x (for fake case).
Architectures
We chose to use a state of the art Densenet discriminator and, for the generator, a powerful combination of two standard networks, UNet and ResNet,
which we henceforth call a UResNet. The code for this paper is available3, and
that provides the full implementation of the networks. However we describe the
implementational details here.
3 
Augmenting Image Classiﬁers using DAGAN
Proj. Noise pi
Low-D. repr. ri
True Image xc
Discriminator
Gen Image x̃
True Image xc
Fig. 1. DAGAN Architecture. Left: the generator network is composed of an encoder
taking an input image and projecting it to a lower dimensional manifold. A random
vector (z) is transformed and concatenated with the bottleneck vector; these are both
passed to the decoder network which generates a within-class image. Right: the adversarial discriminator network is trained to discriminate between the samples from the
real distribution (two real images from the same class) and the fake distribution (a real
sample and a generated sample). Adversarial training enables the network to generate
within-class images that look diﬀerent enough to be considered a diﬀerent sample.
The UResNet generator has a total of 8 blocks, each block having 4 convolutional layers (with leaky rectiﬁed linear (ReLU) activations and batch renormalisation (batchrenorm) ) followed by one downscaling or upscaling layer.
Downscaling layers (in blocks 1-4) were convolutions with stride 2 followed by
leaky ReLU, batch normalisation and dropout. Upscaling layers were implemented by employing a nearest neighbour upscale, followed by a convolution,
leaky ReLU, batch renormalisation and dropout. For Omniglot and EMNIST
experiments, all layers had 64 ﬁlters. For the VGG-Face experiments the ﬁrst 2
blocks of the encoder and the last 2 blocks of the decoder had 64 ﬁlters and the
last 2 blocks of the encoder and the ﬁrst 2 blocks of the decoder 128 ﬁlters.
In addition each block of the UResNet generator had skip connections. As
with a standard ResNet, we used either a summation skip connection between
layers with equivalent spacial dimensions or a strided 1x1 convolution for between
layers with diﬀerent spacial dimensions, thus bypassing the between block nonlinearity to help gradient ﬂow. Finally skip connections were introduced between
equivalent sized ﬁlters at each end of the network (as with UNet).
We used a DenseNet discriminator, using layer normalization instead
of batch normalization; the latter would break the assumptions of the WGAN
objective function (as mentioned in [ chapter 4). The DenseNet was composed
of 4 Dense Blocks and 4 Transition Layers, as deﬁned in . We used a growth
rate of k = 64 and each Dense Block had 5 convolutional layers. We removed
Augmenting Image Classiﬁers using DAGAN
the 1x1 convolutions usually before the 3x3 convolutions as we observed this
improved sample quality. For the discriminator we used dropout at the last
convolutional layer of each Dense Block; this too improved sample quality.
For each classiﬁcation experiment we used a DenseNet classiﬁer composed of
4 Dense Blocks and 4 Transition Layers with a k = 64, each Dense Block had
3 convolutional layers within it. The classiﬁers were a total of 17 layers (i.e. 16
layers and 1 softmax layer). Furthermore we applied a dropout of 0.5 on the last
convolutional layer in each Dense Block.
Datasets and Experiments
We tested the DAGAN augmentation on 3 datasets: Omniglot, EMNIST, and
VGG-Face. All datasets were split randomly into source domain sets, validation
domain sets and test domain sets.
For classiﬁer networks, data for each character (handwritten or person) was
further split into 2 test cases (for all datasets), 3 validation cases and a varying
number of training cases depending on the experiment. Classiﬁer training was
done on the training cases for all examples in all domains; hyperparameter choice
used validation cases. Test performance was reported only on the test cases for
the target domain set. Case splits were randomized across each test run.
The Omniglot data was split into source domain and target domain
similarly to the split in . The class ids were sorted in an increasing manner.
The ﬁrst 1200 were used as a source domain set, 1201-1412 as a validation domain
set and 1412-1623 as a target domain test set.
The EMNIST data was split into a source domain that included classes 0-34
(after random shuﬄing of the classes), the validation domain set included classes
35-42 and the test domain set included classes 42-47. Since the EMNIST dataset
has thousands of samples per class we chose only a subset of 100 for each class,
so that we could make our task a low-data one.
In the VGG-Face dataset case, we randomly chose 100 samples from each
class that had 100 or more, uncorrupted images, resulting in 2396 of the full 2622
classes available in the dataset. After shuﬄing, we split the resulting dataset into
a source domain that included the ﬁrst 1802 classes. The test domain set included
classes 1803-2300 and the validation domain set included classes 2300-2396.
Training of DAGAN in source domain
A DAGAN was trained on Source Omniglot domains using a variety of architectures: standard VGG, U-Net, and ResNet inspired architectures. Increasingly
powerful networks proved better generators, with the UResNet described in Section 3.3 generator being our model of choice. Examples of generated data are
given in Figure 2. We trained each DAGAN for 200K iterations, using a learning
rate of 0.0001, and an Adam optimizer with Adam parameters of β1 = 0 and
β2 = 0.9. We used a pretrained DenseNet classiﬁer to quantify the performance
of the generated data in terms of how well they classify in real classes. We chose
the model that had the best validation accuracy performance on this classiﬁer.
Augmenting Image Classiﬁers using DAGAN
Fig. 2. An Interpolated spherical subspace of the GAN generation space on Omniglot and VGG-Face respectively. The only real image (xc
i) in each ﬁgure is the one in
the top-left corner, the rest are generated to augment that example using a DAGAN.
Classiﬁers
The primary question of this paper is how well the DAGAN can augment vanilla
classiﬁers trained on each target domain. A DenseNet classiﬁer (as described in
Section 3.3) was trained ﬁrst on just real data (with standard data augmentation) with 5 to 100 examples per class (depending on dataset). In the second
case, the classiﬁer was was also trained on DAGAN generated data. The real or
fake label was also passed to the network, via adding 1 ﬁlter before each convolution of either zeros (fake) or ones (real) to enable the network to learn how best
to emphasise true over generated data. This last step proved crucial to maximizing the potential of the DAGAN augmentations. In each training cycle, varying
numbers of augmented samples were provided for each real example (ranging
from 1-10). The best hyperparameters were selected via performance on the validation domain. The classiﬁer was trained with standard augmentation: random
Gaussian noise was added to images (with 50% probability), random shifts along
x and y axis (with 50% probability), and random 90 degree rotations (all with
equal probability of being chosen). Classiﬁers were trained for 200 epochs, with
a learning rate of 0.001, and an Adam optimizer with β1 = 0.9 and β2 = 0.99.
The results on the held out test cases from the target domain is given in Table 1.
In every case the augmentation improves the classiﬁcation.
Conclusions
Data augmentation is a widely applicable approach to improving performance
in low-data settings. The DAGAN is a ﬂexible model to automatically learn
to augment data. We demonstrate that a DAGAN can improve performance
of classiﬁers even after standard data-augmentation. Furthermore, it is worth
Augmenting Image Classiﬁers using DAGAN
Samples Per Class Augment with DAGAN Omniglot EMNIST VGG-Face
Table 1. Classiﬁcation Results: All results are averages over 5 independent runs. The
DAGAN augmentation improves the classiﬁer performance in all cases. Test accuracy
is the result on the test cases in the test domain. Here for the purposes of compactness
we omit the number of generated samples per real sample hyperparameter since that
would produce more than 100 rows of data. We should note however that the optimal
number of generated samples per real image was found to be 3.
noting that a DAGAN can be easily combined with other model types, including
few shot learning models. Further work is needed to evaluate the usefulness in
the few shot learning. However the ﬂexibility of the DAGAN makes it a powerful
means of enhancing models working with a small amount of data.
Acknowledgements This work was supported in by the EPSRC Centre for
Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council and the University of Edinburgh as well as by
the European Unions Horizon 2020 research and innovation programme under
grant agreement No 732204 (Bonseyes) and by the Swiss State Secretariat for
Education Research and Innovation (SERI) under contract number 16.0159. The
opinions expressed and arguments employed herein do not necessarily reﬂect the
oﬃcial views of these funding bodies.