Communicated by David Haussler
A Practical Bayesian Framework for Backpropagation
David J. C. MacKay’
Computation and Neural Systems, California lnstitute of Technology 139-74,
Pasadena, C A 91125 USA
A quantitative and practical Bayesian framework is described for learn-
ing of mappings in feedforward networks. The framework makes
possible (1) objective comparisons between solutions using alternative
network architectures, (2) objective stopping rules for network prun-
ing or growing procedures, (3) objective choice of magnitude and type
of weight decay terms or additive regularizers (for penalizing large
weights, etc.), (4) a measure of the effective number of well-determined
parameters in a model, (5) quantified estimates of the error bars on net-
work parameters and on network output, and (6) objective comparisons
with alternative learning and interpolation models such as splines and
radial basis functions. The Bayesian “evidence” automatically embod-
ies ”Occam’s razor,’’ penalizing overflexible and overcomplex models.
The Bayesian approach helps detect poor underlying assumptions in
learning models. For learning models well matched to a problem, a
good correlation between generalization ability and the Bayesian evi-
dence is obtained.
This paper makes use of the Bayesian framework for regularization and
model comparison described in the companion paper “Bayesian Inter-
polation” . This framework is due to Gull and Skilling
 .
1 The Gaps in Backprop
There are many knobs on the black box of “backprop” [learning by back-
propagation of errors (Rumelhart et al. 198611. Generally these knobs are
set by rules of thumb, trial and error, and the use of reserved test data
to assess generalization ability (or more sophisticated cross-validation).
The knobs fall into two classes: (1) parameters that change the effective
learning model, for example, number of hidden units, and weight decay
‘Present address: Darwin College, Cambridge CB3 9EU, U.K.
Neural Computation 4,448-472 @ 1992 Massachusetts Institute of Technology
Bayesian Framework for Backpropagation Networks
terms; and (2) parameters concerned with function optimization tech-
nique, for example, "momentum" terms. This paper is concerned with
making objective the choice of the parameters in the first class, and with
ranking alternative solutions to a learning problem in a way that makes
full use of all the available data. Bayesian techniques will be described
that are both theoretically well-founded and practically implementable.
Let us review the basic framework for learning in networks, then
discuss the points at which objective techniques are needed. The training
set for the mapping to be learned is a set of input-target pairs D =
{xm, t"}, where rn is a label running over the pairs. A neural network
architecture A is invented, consisting of a specification of the number of
layers, the number of units in each layer, the type of activation function
performed by each unit, and the available connections between the units.
If a set of values w is assigned to the connections in the network, the
network defines a mapping y(x; w, A) from the input activities x to the
output activities y.' The distance of this mapping to the training set is
measured by some error function; for example, the error for the entire
data set is commonly taken to be
ED(D 1 W, d) = C 2 [Y(X"; W, d) - t"]'
The task of "learning" is to find a set of connections w that gives a
mapping that fits the training set well, that is, has small error ED; it is
also hoped that the learned connections will "generalize" well to new
examples. Plain backpropagation learns by performing gradient descent
on ED in w-space. Modifications include the addition of a "momentum"
term, and the inclusion of noise in the descent process. More efficient
optimization techniques may also be used, such as conjugate gradients
or variable metric methods. This paper will not discuss computational
modifications concerned only with speeding the optimization. It will
address, however, those modifications to the plain backprop algorithm
that implicitly or explicitly modify the objective function, with decay
terms or regularizers.
It is moderately common for extra regularizing terms Ew(w) to be
added to ED; for example, terms that penalize large weights may be
introduced, in the hope of achieving a smoother or simpler mapping
 . Some of the "hints" in Abu-Mostafa also
fall into the category of additive weight-dependent energies. A sample
weight energy term is
'The framework developed in this paper will apply not only to networks composed
of "neurons," but to any regression model for which we can compute the derivatives
of the outputs with respect to the parameters, *(x;w,d)/dw.
David J. C. MacKay
The weight energy may be implicit, for example, “weight decay” (sub-
traction of a multiple of w in the weight change rule) corresponds to
the energy in equation 1.2. Gradient-based optimization is then used to
minimize the combined function:
M = @E~(w I d) + PED(D I W, d)
where a and P are “black box” parameters.
The constant cy. should not be confused with the ”momentum” pa-
rameter sometimes introduced into backprop; in the present context a
is a decay rate or regularizing constant. Also note that a should not be
viewed as causing ”forgetting”; ED is defined as the error on the entire
data set, so gradient descent on M treats all data points equally irrespec-
tive of the order in which they were acquired.
1.1 What Is Lacking. The above procedures include a host of free
parameters such as the choice of neural network architecture, and of the
regularizing constant a. There are not yet established ways of objectively
setting these parameters, though there are many rules of thumb .
One popular way of comparing networks trained with different pa-
rameter values is to assess their performance by measuring the error on
an unseen test set or by similar cross-validation techniques. The data are
divided into two sets, a training set that is used to optimize the param-
eters w of the network, and a test set that is used to optimize control
parameters such as a and the architecture A. However, the utility of
these techniques in determining values for the parameters a and /3 or
for comparing alternative network solutions, etc., is limited because a
large test set may be needed to reduce the signal-to-noise ratio in the test
error, and cross-validation is computationally demanding. Furthermore,
if there are several parameters such as a and p, it is out of the question
to optimize such parameters by repeating the learning with all possible
values of these parameters and using a test set. Such parameters must
be optimized on line.
It is, therefore, interesting to study objective criteria for setting free
parameters and comparing alternative solutions, that depend only on
the data set used for the training. Such criteria will prove especially
important in applications where the total amount of data is limited, so
that one does not want to sacrifice good data for use as a test set. Rather,
we wish to find a way to use all our data in the process of optimizing the
parameters w and in the process of optimizing control parameters such
as a and A.
This paper will describe practical Bayesian methods for filling the
following holes in the neural network framework just described:
1. Objective criteria for comparing alternative neural network solutions, in
particular with different architectures A. Given a single architecture
Bayesian Framework for Backpropagation Networks
A, there may be more than one minimum of the objective function
M. If there is a large disparity in M between the minima then it is
plausible to choose the solution with smallest M. But where the dif-
ference is not so great it is desirable to be able to assign an objective
preference to the alternatives. It is also desirable to be able to assign
preferences to neural network solutions using different numbers of
hidden units, and different activation functions. Here there is an
"Occam's razor" problem: the more free parameters a model has,
the smaller the data error ED it can achieve. So we cannot simply
choose the architecture with smallest data error. That would lead
us to an overcomplex network that generalizes poorly. The use of
weight decay does not fully alleviate this problem; networks with
too many hidden units still generalize worse, even if weight decay
is used (see Section 4).
2. Objective criteria for setting the decay rate a. As in the choice of A
above, there is an "Occam's razor" problem: a small value of a
in equation 1.3 allows the weights to become large and overfit the
noise in the data. This leads to a small value of the data error ED
(and a small value of M), so we cannot base our choice of a only on
ED or M. The Bayesian solution presented here can be implemented
on-line, that is, it is not necessary to do multiple learning runs with
different values of a in order to find the best.
3. Objective choice of regularizing function Ew.
4. Objective criteria for choosing between a neural network solution and a
solution using a different learning or interpolation model, for example,
splines or radial basis functions.
1.2 The Probability Connection. Tishby et al. introduced a
probabilistic view of learning that is an important step toward solving the
problems listed above. The idea is to force a probabilistic interpretation
onto the neural network technique so as to be able to make objective
statements. This interpretation does not involve the addition of any new
arbitrary functions or parameters, but it involves assigning a meaning to
the functions and parameters that are already used.
My work is based on the same probabilistic framework, and extends
it using concepts and techniques adapted from Gull and Skilling's Bayesian image reconstruction methods. This paper also adopts
a shift in emphasis from Tishby et al.'s paper. Their work concentrated
on predicting the average generalization ability of one network trained
on a task drawn from a known prior ensemble of tasks. This is called
forward probability. In this paper the emphasis will be on quantifying the
relative plausibilities of many alternative solutions to an interpolation or
classification task; that task is defined by a single data set produced by
the real world, and we do not know the prior ensemble from which the
David J. C. MacKay
task comes. This is called inverse probability. This paper avoids using
the language of statistical physics, in order to maintain wider readability,
and to avoid concepts that would sound strange in that language; for
example, ”the probability distribution of the temperature” is unfamiliar
in physics, but “the probability distribution of the noise variance’’ is its
innocent counterpart in literal terms.
Let us now review the probabilistic interpretation of network learning.
0 Likelihood. A network with specified architecture A and connections
w is viewed as making predictions about the target outputs as a
function of input x in accordance with the probability distribution:
where Z,(P)
= Jdt exp(-PE). E is the error for a single datum,
and P is a measure of the presumed noise included in t. If E is the
quadratic error function then this corresponds to the assumption
that t includes additive gaussian noise with variance u‘, = l/P.
0 Prior. A prior probability is assigned to alternative network con-
nection strengths w, written in the form:
where ZW = Jdkw exp(-aEw). Here a is a measure of the char-
acteristic expected connection magnitude. If Ew is quadratic as
specified in equation 1.2 then weights are expected to come from
a gaussian with zero mean and variance o& = l/a. Alternative
“regularizers” R (each using a different energy function Ew) im-
plicitly correspond to alternative hypotheses about the statistics of
the environment.
0 The posterior probability of the network connections w is then
where Z M ( ~ ,
P) = Jdkw exp(-aEw-PED). Notice that the exponent
in this expression is the same as (minus) the objective function M
defined in equation 1.3.
So under this framework, minimization of M = aEw + BED is identi-
cal to finding the (locally) most probable parameters WMP; minimization
of ED alone is identical to finding the maximum likelihood parameters
WML. Thus an interpretation has been given to backpropagation’s en-
ergy functions ED and Ew, and to the parameters a and P. It should be
Bayesian Framework for Backpropagation Networks
emphasized that "the probability of the connections w" is a measure of
plausibility that the model's parameters should have a specified value w;
this has nothing to do with the probability that a particular algorithm
might converge to w.
This framework offers some partial enhancements for backprop meth-
ods: The work of Levin et al. makes it possible to predict the aver-
age generalization ability of neural networks trained on one of a defined
class of problems. However, it is not clear whether this will lead to a
practical technique for choosing between alternative network architec-
tures for real data sets.
Le Cun ef al. have demonstrated how to estimate the "saliency"
of a weight, which is the change in M when the weight is deleted. They
have used this measure successfully to simplify large neural networks.
However no stopping rule for weight deletion was offered other than
measuring performance on a test set.
Also Denker and Le Cun demonstrated how the Hessian of M
can be used to assign error bars to the parameters of a network and to
its outputs. However, these error bars can be quantified only once ,6 is
quantified, and how to do this without prior knowledge or extra data
has not been demonstrated. In fact p can be estimated from the training
data alone.
2 Review of Bayesian Regularization and Model Comparison ___
In the companion paper it was demonstrated how the
control parameters a and /3 are assigned by Bayes, and how alternative
interpolation models can be compared. It was noted there that it is not
satisfactory to optimize a and p by finding the joint maximum likelihood
value of w, a, p; the likelihood has a skew peak whose maximum is not
located at the most probable values of the control parameters. MacKay
 also reviewed how the Bayesian choice of a and ,B is neatly ex-
pressed in terms of a measure of the number of well-determined parame-
ters in a model, y. However that paper assumed that M(w) has only one
significant minimum that was well approximated as quadratic. [All the
interpolation models discussed in MacKay can be interpreted as
two-layer networks with a fixed nonlinear first layer and adaptive linear
second layer.] In this section I briefly review the Bayesian framework,
retaining that assumption. The following section will then discuss how
the framework can be modified to handle neural networks, where the
landscape of M(w) is certainly not quadratic.
2.1 Determination of a and P. By Bayes' rule, the posterior proba-
bility for these parameters is
David J. C. MacKay
Now if we assign a uniform prior to (a,,@, the quantity of interest for
assigning preferences to (a,P) is the first term on the right-hand side,
the evidence for a, p, which can be written as2
where ZM and ZW were defined earlier and ZD = JdNDe-BEo,
Let us use the simple quadratic energy functions defined in equations
1.1 and 1.2. This makes the analysis easier, but more complex cases can
still in principle be handled by the same approach. Let the number of
degrees of freedom in the data set, that is, the number of output units
times the number of data pairs, be N, and let the number of free parame-
ters, that is, the dimension of w, be k. Then we can immediately evaluate
the gaussian integrals ZD and ZW: ZD = ( 2 ~ / @ ) ~ / ~ ,
and ZW = (27~/a)~/~.
NOW we want to find Z M ( ~ ,
P) = Jdkw exp[-M(w, a, P)]. Supposing for
now that M has a single minimum as a function of w, at WMP, and as-
suming we can locally approximate M as quadratic there, the integral ZM
is approximated by
ZM N e-M(WMF’) (2a)k/2det-’/2A
where A = VVM is the Hessian of M evaluated at WMP.
The maximum of P( D I a, p, A, R) has the following useful properties:
where y is the effective number of parameters determined by the data,
where A, are the eigenvalues of the quadratic form @ED in the natural
basis of Ew.
2.2 Comparison of Different Models. To rank alternative architec-
tures and penalty functions Ew in the light of the data, we simply evaluate
the evidence, P(D 1 d,R),
which appeared as the normalizing constant
in equation 2.1. Integrating the evidence for (a, P), we have:
P(D I A, R) = 1 P(D I a, P, A, RIP(&, P) da dP
The evidence is the Bayesian’s transportable quantity for comparing mod-
els in the light of the data.
2The same notation, and the same abuses thereof, will be used as in MacKay .
Bayesian Framework for Backpmpagation Networks
3 Adapting the Framework
For neural networks, M(w) is not quadratic. Indeed it is well known that
M typically has many local minima. And if the network has a symmetry
under permutation of its parameters, then we know that M(w) must
share that symmetry, so that every single minimum belongs to a family
of symmetric minima of M. For example, if there are H hidden units in
a single layer then each nondegenerate minimum is in a family of size
g = H! 2ff. Now it may be the case that the significant minima of M
are locally quadratic, so we might be able to evaluate ZM by evaluating
equation 2.3 at each significant minimum and adding up the ZMS; but the
number of those minima is unknown, and this approach to evaluating
ZM would seem dubious.
Luckily, however, we do not actually want to evaluate ZM. We would
need to evaluate ZM in order to assign a posterior probability over al
for an entire model, and to evaluate the evidence for alternative entire
models. This is not quite what we wish to do: when we use a neural
network to perform a mapping, we typically implement only one neural
network at a time, and this network will have its parameters set to a
particular solution of the learning problem. Therefore, the alternatives we
wish to rank are the different solutions of the learning problem, that is,
the different minima of M. We would want the evidence as a function
of the number of hidden units only if we were somehow able to simul-
taneously implement the entire posterior ensemble of networks for one
number of hidden units. Similarly, we do not want the posterior over a, /3
for the entire posterior ensemble; rather, it is reasonable to allow each
solution (each minimum of M) to choose its own optimal value for these
parameters. The same method of chopping up a complex model space
is used in the unsupervised classification system, Autoclass .
Having adopted this slight shift in objective, it turns out that to set
a and P and to compare alternative solutions to a learning problem, the
integral we now need to evaluate is a local version of ZM. Assume that
the posterior probability consists of well-separated islands in parameter
space each centered on a minimum of M. We wish to evaluate how
much posterior probability mass is in each of these islands. Consider a
minimum located at w*, and define a solution S,.
as the ensemble of
networks in the neighborhood of w*, and all symmetric permutations of
that ensemble. Let us evaluate the posterior probability for alternative
soIutions Sw*, and the parameters a and P:
where g is the permutation factor, and
David J. C. MacKay
where the integral is performed only over the neighborhood of the min-
imum at w*.
I will refer to the quantity g[ZE(w*, a, @)/&(a)ZD(@)] as
the evidence for a, P, S,.
. The parameters cr and P will be chosen to max-
imize this evidence. Then the quantity we want to evaluate to compare
alternative solutions is the evidence3 for Sw.,
This paper uses the gaussian approximation for Zt:
where A = VVM is the Hessian of M evaluated at w*. For general a
and P this approximation is probably unacceptable; however, we need
it only to be accurate for the small range of a and @ close to their most
probable value. The regime in which this approximation will definitely
break down is when the number of constraints, N, is small relative to the
number of free parameters, k. For large N/k the central limit theorem
encourages us to use the gaussian approximation . It is a
matter for further research to establish how large N/k must be for this
approximation to be reliable.
What obstacles remain to prevent us from evaluating the local Zt?
We need to evaluate or approximate the inverse Hessian of M, and we
need to evaluate or approximate its determinant and/or trace and Le Cun et aZ. have already dis-
cussed how to approximate the Hessian of ED for the purpose of evaluat-
ing weight saliency and for assigning error bars to weights and network
outputs. The Hessian can be evaluated in the same way that backpropa-
gation evaluates VED . Alternatively A can
be evaluated by numerical methods, for example second differences. A
third option: if variable metric methods are used to minimize M instead
of gradient descent, then the inverse Hessian is automatically generated
during the search for the minimum. It is important, for the success of
this Bayesian method, that the off-diagonal terms of the Hessian should
be evaluated. Denker et aZ.'s method can do this without any additional
complexity. The diagonal approximation is no good because of the strong
posterior correlations in the parameters.
3Bayesian model comparison is performed by evaluating and comparing the evi-
dence for alternative models. Gull and Skilling defined the evidence for a model 3-1
to be P(D I 1-I). The existence of multiple minima in neural network parameter space
complicates model comparison. The quantity in equation 3.2 is not P(D I S,.
(it includes the prior for S,.
but I have called it the evidence because it is the
quantity we should evaluate to compare alternative solutions with each other and with
other models.
Bayesian Framework for Backpropagation Networks
4 Demonstration
This demonstration examines the evidence for various neural net solu-
tions to a small interpolation problem, the mapping for a two joint robot
(el, 0,) + (ya,yb) = rl cos 6J1 + r2 cos(O1 + 02), r1 sin B1 + r2 sin(& + 0,)
For the training set I used rl = 2.0 and r;! = 1.3, random samples from a
restricted range of (el, 0,) were made, and gaussian noise of magnitude
0.05 was added to the outputs. The neural nets used had one hidden
layer of sigmoid units and linear output units. During optimization,
the regularizer (equation 1.2) was used initially, and an alternative reg-
ularizer was introduced later; ,O was fixed to its true value (to enable
demonstration of the properties of the quantity y), and a was allowed to
adapt to its locally most probable value.
Figure 1 illustrates the performance of a typical neural network trained
in this way. Each output is accompanied by error bars evaluated using
Denker et aZ.'s method, including of-diagonal Hessian terns. If ,O had not
been known in advance, it could have been inferred from the data using
equation 2.5. For the solution displayed, the model's estimate of ,O in
fact differed negligibly from the true value, so the displayed error bars
are the same as if ,O had been inferred from the data.
Figure 2 shows the data misfit versus the nwber of hidden units.
Notice that, as expected, the data error tends to decrease monotonically
with increasing number of parameters. Figure 3 shows the error of these
same solutions on an unseen test set, which does not show the same
trend as the data error. The data misfit cannot serve as a criterion for
choosing between solutions.
Figure 4 shows the evidence for about 100 different solutions using
different numbers of hidden units. Notice how the evidence maximum
has the characteristic shape of an "Occam hill" -
steep on the side with
too few parameters, and shallow on the side with too many parame-
ters. The quadratic approximations break down when the number of
parameters becomes too big compared with the number of data points.
Figure 5 introduces the quantity y, discussed in MacKay , the
number of well-measured parameters. In cases where the evaluation of
the evidence proves difficult, it may be that y will serve as a useful tool.
For example, sampling theory predicts that the addition of redundant
parameters to a model should reduce x& by one unit per well-measured
parameter; a stopping criterion could detect the point at which, as pa-
rameters are deleted, x; started to increase faster than with gradient l
with decreasing y (Figure 6).* This use of y requires prior knowledge of
the noise level P; that is why ,O was fixed to its known value for these
demonstrations.
4This suggestion is closely related to Moody's "generalized prediction error,"
GPE = (& + 2 7 ) / N .
David J. C. MacKay
Figure 1: Typical neural network output (inset - training set). This is the out-
put space (y,,yb) of the network. The target outputs are displayed as small
x's, and the output of the network with 1u error bars is shown as a a dot sur-
rounded by an ellipse. The network was trained on samples in two regions in
the lower and upper half planes (inset). The outputs illustrated here are for
inputs extending a short distance outside the training regions, and bridging
the gap between them. Notice that the error bars get much larger around the
perimeter. They also increase slightly in the gap between the training regions.
These pleasing properties would not have been obtained had the diagonal Hes-
sian approximation of Denker and Le Cun been used. The above solution
was created by a three layer network with 19 hidden units.
Now the question is how good a predictor of network quality the
evidence is. The fact that the evidence has a maximum at a reasonable
number of hidden units is promising. A comparison with Figure 3 shows
that the performance of the solutions on an unseen test set has similar
overall structure to the evidence. However, Figure 7 shows the evidence
against the performance on a test set, and it can be seen that a significant
number of solutions with poor evidence actually perform well on the test
set. Something is wrong! It is time for a discussion of the relationship
between the evidence and generalization ability. We will return later to
the failure in Figure 7 and see that it is rectified by the development of
new, more probable regularizers.
Bayesian Framework for Backpropagation Networks
Number of hidden U n i t s
Figure 2: Data error versus number of hidden units. Each point represents
one converged neural network, trained on a 200 i/o pair training set. Each
neural net was initialized with different random weights and with a different
initial value of u& = 1/a. The two point-styles correspond to small and large
initial values for UW. The error is shown in dimensionless x2 units such that
the expectation of error relative to the truth is 400 f
20. The solid line is 400 - k,
where k is the number of free parameters.
4.1 Relation to "Generalization Error". What is the relationship be-
tween the evidence and the generalization error (or its close relative,
cross-validation)? A correlation between the two is certainly expected.
But the evidence is not necessarily a good predictor of generalization er-
ror . First, as illustrated in Figure 8, the
error on a test set is a noisy quantity, and many data have to be devoted
to the test set to get an acceptable signal-to-noise ratio. Furthermore,
imagine that two models have generated solutions to an interpolation
problem, and that their two most probable interpolants are completely
identical. In this case, the generalization error for the two solutions must
be the same, but the evidence will not in general be the same: typically,
the model that was a priori more complex will suffer a larger Occam
factor and will have smaller evidence. Also, the evidence is a measure of
plausibility of the whole ensemble of networks about the optimum, not
David J. C. MacKay
Number of hidden u n i t s
Figure 3: Test error versus number of hidden units. The training set and test
set both had 200 data points. The test error for solutions found using the first
regularizer is shown in dimensionless x2 units such that the expectation of error
relative to the truth is 400 f
just the optimal network. Thus, there is more to the evidence than there
is to the generalization error.
4.2 What If the Bayesian Method Fails? I do not want to dismiss
the utility of the generalization error: it can be important for detecting
failures of the model being used. For example, if we obtain a poor
correlation between the evidence and the generalization error, such that
Bayes fails to assign a strong preference to solutions that actually perform
well on test data, then we are able to detect and attempt to correct such
A failure indicates one of two things, and in either case we are able to
learn and improve: either numerical inaccuracies in the evaluation of the
probabilities caused the failure, or else the alternative models that were
offered to Bayes were a poor selection, ill-matched to the real world
(for example, using inappropriate regularizers). When such a failure
is detected, it prompts us to examine our models and try to discover
the implicit assumptions in the model that the data did not agree with;
Bayesian Framework for Backpropagation Networks
"Small-start" 0
"Large-start"
Figure 4: Log evidence for solutions using the first regularizer. For each solu-
tion, the evidence was evaluated. Notice that an evidence maximum is achieved
by neural network solutions using 10, 11, and 12 hidden units. For more than
-19 hidden units, the quadratic approximations used to evaluate the evidence
are believed to break down. The number of data points N is 400 (i.e., 200 i/o
pairs); cf. number of parameters in a net with 20 hidden units = 102.
alternative models can be tried until one is found that makes the data
more probable.
We have just met exactly such a failure. Let us now establish what
assumption in our model caused this failure and learn from it. Note that
this mechanism for human learning is not available to those who just use
the test error as their performance criterion. Going by the test error alone,
there would have been no indication that there was a serious mismatch
between the model and the data.
4.3 Back to the Demonstration: Comparing Different Regularizers.
The demonstrations thus far used the regularizer of equation 1.2. This is
equivalent to a prior that expects all the weights to have the same charac-
teristic size. This is actually an inconsistent prior: the input and output
variables and hidden unit activities could all be arbitrarily rescaled; if the
same mapping is to be performed (a simple consistency requirement),
such transformations of the variables would imply independent rescaling
David J. C. MacKay
total numher of parameters, k
Figure 5: The number of well-determined parameters. This figure displays
a function of k, for the same network solutions as iR Figure 4.
of the weights to the hidden layer and to the output layer. Thus the scales
of the two layers of weights are unrelated, and it is inconsistent to force
the characteristic decay rates of these different classes of weights to be
the same. This inconsistency is the major cause of the failure illustrated
in Figure 7. All the networks deviating substantially from the desired trend have
weights to the output layer far larger than the weights to the input layer; this
poor match to the model implicit in the regularizer causes the evidence
for those solutions to be small.
This failure enables us to progress with insight to new regularizers.
The alternative that I now present is a prior that is not inconsistent in
the way explained above, so there are theoretical reasons to expect it to
be "better." However, we will allow the data to choose, by evaluating
the evidence for solutions using the new prior; we will find that the new
prior is indeed more probable.
The second prior has three independent regularizing constants, corre-
sponding to the characteristic magnitudes of the weights in three differ-
ent classes c, namely hidden unit weights, hidden unit biases, and output
weights and biases (see Fig. 9). The term aEw is replaced by CcaCE&,
where Eh = CiEc
4 / 2 . Nowlan has used a similar prior modeling
Bayesian Framework for Backpropagation Networks
Small-start 0
Large-start
Figure 6 Data misfit versus y. This figure shows & against y, and a line of
gradient -1. Toward the right, the data's misfit & is reduced by 1 for every
well-measured parameter. When the model has too few parameters, however
(toward the left), the misfit gets worse at a greater rate.
weights as coming from a gaussian mixture, and used Bayesian reestima-
tion techniques to update the mixture parameters; he found such a model
was good at discovering elegant solutions to problems with translation
invariances.
Using the second prior, each regularizing constant is independently
adapted to its most probable value by evaluating the number of well-
measured parameters T~ associated with each regularizing function, and
finding the optimum where 2a,Eb = -ye. The increased complexity of this
prior model is penalized by an Occam factor for each new parameter ac
 . Let me preempt questions along the lines of "why
didn't you use four weight classes, or nonzero means?" - any other
way of assigning weight decays is just another model, and you can try
as many as you like; by evaluating the evidence you can then find out
what preference the data have for the alternative decay schemes.
New solutions have been found using this second prior, and the ev-
idence evaluated. The evidence for these new solutions with the new
prior is shown in Figure 10. Notice that the evidence has increased com-
David J. C. MacKay
8- sma 11 - s t a r t "
"1.arge-st.art"
Test error
Figure 7: Log evidence versus test error for the first regularizer. The desired
correlation between the evidence and the test error has negative slope. A signif-
icant number of points on the lower left violate this desired trend, so we have a
failure of Bayesian prediction. The points that violate the trend are networks in
which there is a significant difference in typical weight magnitude between the
two layers. They are all networks whose learning was initialized with a large
value of UW. The first regularizer is ill-matched to such networks, and the low
evidence is a reflection of this poor prior hypothesis.
pared to the evidence for the first prior. For some solutions the new prior
is more probable by a factor of lo3'.
Now the crunch: does this more probable model make good predic-
tions? The evidence for the second prior is shown against the test error in
Figure 11. The correlation between the two is greatly improved. Notice
furthermore that not only is the second prior more probable, the best test
error achieved by solutions found using the second prior is slightly better
than any achieved using the first prior, and the number of good solutions
has increased substantially. Thus the Bayesian evidence is a good predic-
tor of generalization ability, and the Bayesian choice of regularizers has
enabled the best solutions to be found.
Bayesian Framework for Backpropagation Networks
Test error 1
Figure 8: Comparison of two test errors. This figure illustrates how noisy a
performance measure the test error is. Each point compares the error of a
trained network on two different test sets. Both test sets consist of 200 data
points from the same distribution as the training set.
5 Discussion
The Bayesian method that has been presented is well-founded theoret-
ically, and it works practically, though it remains to be seen how this
approach will scale to larger problems. For a particular data set, the
evaluation of the evidence has led us objectively from an inconsistent reg-
ularizer to a more probable one. The evidence is maximized for a sensible
number of hidden units, showing that Occam’s razor has been success-
fully embodied with no ad hoc terms. Furthermore the solutions with
greatest evidence perform better on a test set than any other solutions
found. I believe there is currently no other technique that could reliably
find and identify better solutions using only the training set. Essential
to this success was the simultaneous Bayesian optimization of the three
regularizing constants (decay terms) a,. Optimization of these param-
eters by any orthodox search technique such as cross-validation would
be laborious; if there were many more than three regularizing constants,
David J. C. MacKay
Figure 9: The three classes of weights under the second prior. (1) Hidden unit
weights. (2) Hidden unit biases. (3) Output unit weights and biases. The
weights in one class c share the same decay constant ac.
as could easily be the case in larger problems, it is hard to imagine any
such search being po~sible.~
This brings up the question of how these Bayesian calculations scale
with problem size. In terms of the number of parameters k, calculation of
the determinant and inverse of the Hessian scales as k3. Note that this is
a computation that needs to be carried out only a small number of times
compared with the immense number of derivative calculations involved
in a typical learning session. However, for large problems it may be too
demanding to evaluate the determinant of the Hessian. If this is the case,
numerical methods are available to approximate the determinant or trace
of a matrix in k2 time .
5.1 Application to Classification Problems. This paper has thus far
discussed the evaluation of the evidence for backprop networks trained
on interpolation problems. Neural networks can also be trained to per-
5Radford Neal (personal communication) has pointed out that it is possible to eval-
uate the gradient of a validation error with respect to parameters such as {ac}, using
aEv,1/8ac = aEval/aW~~.aWM~/aac.
The first quantity could be evaluated by backprop,
and the second term could be found within the quadratic approximation which gives
a w M p / a a , = A-’IcwMp, where I, is the identity matrix for the weights regularized
and zero elsewhere. Alternatively, Radford Neal has suggested that the gradi-
ents OEv,I /Oa, could be more efficiently calculated using “recurrent backpropagation”
 , viewing w as the vector of activities of a recurrent network, and WMP as
the fixed point whose error E,I we wish to minimize.
Bayesian Framework for Backpropagation Networks
" small-st art '' 0
"large-start" +
"derived" 0
"symmetries-detected"
Number of hidden units
Figure 10: Log evidence versus number of hidden units for the second prior.
The different point styles correspond to networks with learning initialized with
small and large values of UW; networks previously trained using the first reg-
ularizer and subsequently trained on the second regularizer; and networks in
which a weight symmetry was detected (in such cases the evidence evaluation
is possibly less reliable).
form classification tasks. A future publication will demon-
strate that the Bayesian framework for model comparison can be applied
to these problems too.
5.2 Relation to V-C Dimension. Some papers advocate the use of
V-C dimension as a criterion for penalizing over-
complex models . V-C di-
mension is most often applied to classification problems; the evidence,
on the other hand, can be evaluated equally easily for interpolation and
classification problems. V-C dimension is a worst case measure, so it
yields different results from Bayesian analysis . For
example, V-C dimension is indifferent to the use of regularizers like equa-
tion 1.2, and to the value of a, because the use of such regularizers does
not rule out absolutely any particular network parameters. Thus V-C
dimension assigns the same complexity to a model whether or not it is
David J. C. MacKay
"small-start'' 0
"large-start" +
"derived" 0
"syrwetries-detected" x
Test error
Figure 11: Log evidence for the second prior versus test error. The correlation
between the evidence and the test error for the second prior is very good. Note
that the largest value of evidence has increased relative to Figure 7, and the
smallest test error has also decreased.
regularized.6 So it cannot be used to set regularizing constants a or to
compare alternative regularizers. In contrast, the preceding demonstra-
tions show that careful objective choice of regularizer and a is essential
for the best solutions to be obtained.
Worst case analysis has a complementary role alongside Bayesian
methods. Neither can substitute for the other.
5.3 Future Tasks. Further work is needed to formalize the relation-
ship of this framework to the pragmatic model comparison technique
of cross-validation. Moody's work on "generalized prediction er-
ror" (GPE) is an interesting contribution in this direction. His sampling
theory approach predicts that the generalization error, in x2 units, will
be (& + 2y)lN. However, I have evaluated the GPE for the interpo-
lation models in this paper's demonstration, and found the correlation
6However, E. Levin (personal communication) has mentioned that a measure of
"effective V-C dimension" of a regularized model is being developed. In some cases
this measure is identical to 7, equation 2.6.
Bayesian Framework for Backpropagation Networks
between GPE and the actual test error was poor. More work is needed
to understand this.
The gaussian approximation used to evaluate the evidence breaks
down when the number of data points is small compared to the number
of parameters. For the model problems I have studied so far, the gaussian
approximation seemed to break down significantly for N / k < 3 f
a matter for further research to characterize this failure and investigate
techniques for improving the evaluation of the integral Zb, for example
the use of random walks on M in the neighborhood of a solution.
It is expected that evaluation of the evidence should provide an objec-
tive rule for deciding whether a network pruning or growing procedure
should be stopped, but a careful study of this idea has yet to be per-
It will be interesting to see the results of evaluating the evidence for
networks applied to larger real-world problems.
6 Appendix: Numerical Methods
6.1 Quick and Dirty Version. The three numerical tasks are auto-
matic optimization of N, and p, calculation of error bars, and evaluation
of the evidence. I will describe a cheap approximation for solving the
first of these tasks without evaluating the Hessian. If we neglect the
distinction between well-determined and poorly determined parameters,
we obtain the following update rules for CI and P:
O, := kC/2EE,
p := N/2ED
If you want an easy-to-program taste of what a Bayesian framework can
offer, try using this procedure to update your decay terms.
6.2 Hessian Evaluation. The Hessian of M, A, is needed to evaluate
y (which relates to TraceA-I), to evaluate the evidence (which relates to
det A), and to assign error bars to network outputs (using A-’1.
I used two methods for evaluating A: (1) an approximate analytic
method and (2) second differences. The approximate analytic method
was, following Denker et al., to use backprop to obtain the second deriva-
tives, neglecting terms in f”, where f is the activation function of a neu-
ron. The Hessian is built up as a sum of outer products of gradient
where gy = dy;(xm)/dw. Unlike Denker et al., I did not ignore the off-
diagonal terms; the diagonal approximation is not good enough! For
David J. C. MacKay
the evaluation of y the two methods gave similar results, and either ap-
proach seemed satisfactory. However, for the evaluation of the evidence,
the approximate analytic method failed to give satisfactory results. The
”Occam factors” are very weak, scaling only as log N, and the above ap-
proximation apparently introduces systematic errors greater than these.
The reason that the evidence evaluation is more sensitive to errors than
the y evaluation is because y is related to the sum of eigenvalues, whereas
the evidence is related to the product; errors in small eigenvalues jeop-
ardize the product more than the sum. I expect an exact analytic evalua-
tion of the second derivatives would resolve this. To save
programming effort I instead used second differences, which is compu-
tationally more demanding (- kN backprops) than the analytic approach
( w N backprops). There were still problems with errors in small eigenval-
ues, but it was possible to correct these errors, by detecting eigenvalues
that were smaller than theoretically permitted.
6.3 Demonstrations. The demonstrations were performed as follows:
Initial weight configuration: random weights drawn from a gaussian
with crw = 0.3. Optimization algorithm for M(w): variable metric meth-
ods, using code from Press et al. , used several times in sequence
with values of the fractional tolerance decreasing from
to lo-’. Ev-
ery other loop, the regularizing constants ac were allowed to adapt in
accordance with the reestimation formula:
aC := yC/2E&
6.4 Precaution. When evaluating the evidence, care must be taken to
verify that the permutation term g is appropriately set. It may be the
case (probably mainly in toy problems) that the regularizer makes two
or more hidden units in a network adopt identical connection values;
alternatively, some hidden units might switch off, with all weights set
to zero; in these cases the permutation term should be smaller. Also
in these cases, it is likely that the quadratic approximation will perform
badly (quartic rather than quadratic minima are likely), so it is preferable
to automate the deletion of such redundant units.
Acknowledgments
I thank Mike Lewicki, Nick Weir, and Haim Sompolinsky for helpful
conversations, and Andreas Herz for comments on the manuscript. This
work was supported by a Caltech Fellowship and a Studentship from
Bayesian Framework for Backpropagation Networks