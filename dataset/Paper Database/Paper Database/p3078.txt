Dynamic Deep Neural Networks: Optimizing
Accuracy-Efﬁciency Trade-Offs by Selective Execution
Lanlan Liu, Jia Deng
University of Michigan, Ann Arbor
2260 Hayward Street
Ann Arbor, Michigan, 48109
We introduce Dynamic Deep Neural Networks (D2NN), a
new type of feed-forward deep neural network that allows
selective execution. Given an input, only a subset of D2NN
neurons are executed, and the particular subset is determined
by the D2NN itself. By pruning unnecessary computation depending on input, D2NNs provide a way to improve computational efﬁciency. To achieve dynamic selective execution, a D2NN augments a feed-forward deep neural network
(directed acyclic graph of differentiable modules) with controller modules. Each controller module is a sub-network
whose output is a decision that controls whether other modules can execute. A D2NN is trained end to end. Both regular and controller modules in a D2NN are learnable and are
jointly trained to optimize both accuracy and efﬁciency. Such
training is achieved by integrating backpropagation with reinforcement learning. With extensive experiments of various
D2NN architectures on image classiﬁcation tasks, we demonstrate that D2NNs are general and ﬂexible, and can effectively
optimize accuracy-efﬁciency trade-offs.
Introduction
This paper introduces Dynamic Deep Neural Networks
(D2NN), a new type of feed-forward deep neural network
(DNN) that allows selective execution. That is, given an input, only a subset of neurons are executed, and the particular
subset is determined by the network itself based on the particular input. In other words, the amount of computation and
computation sequence are dynamic based on input. This is
different from standard feed-forward networks that always
execute the same computation sequence regardless of input.
A D2NN is a feed-forward deep neural network (directed
acyclic graph of differentiable modules) augmented with
one or more control modules. A control module is a subnetwork whose output is a decision that controls whether
other modules can execute. Fig. 1 (left) illustrates a simple
D2NN with one control module (Q) and two regular modules (N1, N2), where the controller Q outputs a binary decision on whether module N2 executes. For certain inputs, the
controller may decide that N2 is unnecessary and instead execute a dummy node D to save on computation. As an example application, this D2NN can be used for binary classiﬁca-
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
tion of images, where some images can be rapidly classiﬁed
as negative after only a small amount of computation.
D2NNs are motivated by the need for computational ef-
ﬁciency, in particular, by the need to deploy deep networks
on mobile devices and data centers. Mobile devices are constrained by energy and power, limiting the amount of computation that can be executed. Data centers need energy ef-
ﬁciency to scale to higher throughput and to save operating
cost. D2NNs provide a way to improve computational efﬁciency by selective execution, pruning unnecessary computation depending on input. D2NNs also make it possible to
use a bigger network under a computation budget by executing only a subset of the neurons each time.
A D2NN is trained end to end. That is, regular modules
and control modules are jointly trained to optimize both accuracy and efﬁciency. We achieve such training by integrating backpropagation with reinforcement learning, necessitated by the non-differentiability of control modules.
Compared to prior work that optimizes computational ef-
ﬁciency in computer vision and machine learning, our work
is distinctive in four aspects: (1) the decisions on selective
execution are part of the network inference and are learned
end to end together with the rest of the network, as opposed to hand-designed or separately learned ; (2)
D2NNs allow more ﬂexible network architectures and execution sequences including parallel paths, as opposed to architectures with less variance ; (3) our D2NNs directly optimize arbitrary efﬁciency metric that is deﬁned by the user, while previous work has no such ﬂexibility because they improve ef-
ﬁciency indirectly through sparsity constraints . (4) our method optimizes metrics such as the F-score
that does not decompose over individual examples. This is
an issue not addressed in prior work. We will elaborate on
these differences in the Related Work section of this paper.
We perform extensive experiments to validate our D2NNs
algorithms. We evaluate various D2NN architectures on several tasks. They demonstrate that D2NNs are general, ﬂexible, and can effectively improve computational efﬁciency.
Our main contribution is the D2NN framework that allows
a user to augment a static feed-forward network with control
modules to achieve dynamic selective execution. We show
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
Figure 1: Two D2NN examples. Input and output nodes are drawn as circles with the output nodes shaded. Function nodes
are drawn as rectangles (regular nodes) or diamonds (control nodes). Dummy nodes are shaded. Data edges are drawn as solid
arrows and control edges as dashed arrows. A data edge with a user deﬁned default value is decorated with a circle.
that D2NNs allow a wide variety of topologies while sharing a uniﬁed training algorithm. To our knowledge, D2NN is
the ﬁrst single framework that can support various qualitatively different efﬁcient network designs, including cascade
designs and coarse-to-ﬁne designs. Our D2NN framework
thus provides a new tool for designing and training computationally efﬁcient neural network models.
Related work
Input-dependent execution has been widely used in computer vision, from cascaded detectors to hierarchical classiﬁcation . The key difference of our work from prior
work is that we jointly learn both visual features and control decisions end to end, whereas prior work either handdesigns features and control decisions (e.g. thresholding), or
learns them separately.
In the context of deep networks, two lines of prior work
have attempted to improve computational efﬁciency. One
line of work tries to eliminate redundancy in data or computation in a way that is input-independent. The methods include pruning networks , approximating layers with
simpler functions ,
and using number representations of limited precision . The other line of work exploits the fact that not all inputs require the same amount
of computation, and explores input-dependent execution of
DNNs. Our work belongs to the second line, and we will
contrast our work mainly with them. In fact, our inputdependent D2NN can be combined with input-independent
methods to achieve even better efﬁciency.
Among methods leveraging input-dependent execution,
some use pre-deﬁned execution-control policies. For example, cascade methods rely on manually-selected thresholds to control execution; Dynamic Capacity Network 
designs a way to directly calculate a saliency map for execution control. Our D2NNs, instead, are fully learn-able; the
execution-control policies of D2NNs do not require manual
design and are learned together with the rest of the network.
Our work is closely related to conditional computation methods
 , which activate part
of a network depending on input. They learn policies to
encourage sparse neural activations or
sparse expert networks . Our work differs from these methods in several ways. First, our control policies are learned to directly optimize arbitrary userdeﬁned global performance metrics, whereas conditional
computation methods have only learned policies that encourage sparsity. In addition, D2NNs allow more ﬂexible
control topologies. For example, in , a
neuron (or block of neurons) is the unit controllee of their
control policies; in , an expert is the
unit controllee. Compared to their ﬁxed types of controllees,
our control modules can be added in any point of the network and control arbitrary subnetworks. Also, various policy parametrization can be used in the same D2NN framework. We show a variety of parameterizations (as different
controller networks) in our D2NN examples, whereas previous conditional computation works have used some ﬁxed
format: For example, control policies are parametrized as the
sigmoid or softmax of an afﬁne transformation of neurons or
inputs .
Our work is also related to attention models . Note that attention models can be categorized as hard attention versus soft . Hard attention models only process the salient parts
and discard others (e.g. processing only a subset of image
subwindows); in contrast, soft attention models process all
parts but up-weight the salient parts. Thus only hard attention models perform input-dependent execution as D2NNs
do. However, hard attention models differ from D2NNs because hard attention models have typically involved only
one attention module whereas D2NNs can have multiple attention (controller) modules — conventional hard attention
models are “single-threaded” whereas D2NN can be “multithreaded”. In addition, prior work in hard attention models
have not directly optimized for accuracy-efﬁciency tradeoffs. It is also worth noting that many mixture-of-experts
methods also involve soft attention by soft gating experts: they process all experts but
only up-weight useful experts, thus saving no computation.
D2NNs also bear some similarity to Deep Sequential Neural Networks (DSNN) in terms
of input-dependent execution. However, it is important to
note that although DSNNs’ structures can in principle be
used to optimize accuracy-efﬁciency trade-offs, DSNNs are
not for the task of improving efﬁciency and have no learning
method proposed to optimize efﬁciency. And the method to
effectively optimize for efﬁciency-accuracy trade-off is non-
trivial as is shown in the following sections. Also, DSNNs
are single-threaded: it always activates exactly one path in
the computation graph, whereas for D2NNs it is possible to
have multiple paths or even the entire graph activated.
Deﬁnition and Semantics of D2NNs
Here we precisely deﬁne a D2NN and describe its semantics,
i.e. how a D2NN performs inference.
D2NN deﬁnition
A D2NN is deﬁned as directed acyclic
graph (DAG) without duplicated edges. Each node can be
one of the three types: input nodes, output nodes, and function nodes. An input or output node represents an input or
output of the network (e.g. a vector). A function node represents a (differentiable) function that maps a vector to another
vector. Each edge can be one of the two types: data edges
and control edges. A data edge represents a vector sent from
one node to another, the same as in a conventional DNN. A
control edge represents a control signal, a scalar, sent from
one node to another. A data edge can optionally have a userdeﬁned “default value”, representing the output that will still
be sent even if the function node does not execute.
For simplicity, we have a few restrictions on valid D2NNs:
(1) the outgoing edges from a node are either all data edges
or all control edges (i.e. cannot be a mix of data edges and
control edges); (2) if a node has an incoming control edge,
it cannot have an outgoing control edge. Note that these two
simplicity constraints do not in any way restrict the expressiveness of a D2NN. For example, to achieve the effect of a
node with a mix of outgoing data edges and control edges,
we can just feed its data output to a new node with outgoing
control edges and let the new node be identity function.
We call a function node a control node if its outgoing
edges are control edges. We call a function node a regular
node if its outgoing edges are data edges. Note that it is possible for a function node to take no data input and output
a constant value. We call such nodes “dummy” nodes. We
will see that the “default values” and “dummy” nodes can
signiﬁcantly extend the ﬂexibility of D2NNs. Hereafter we
may also call function nodes “subnetwork”, or “modules”
and will use these terms interchangeably. Fig. 1 illustrates
simple D2NNs with all kinds of nodes and edges.
D2NN Semantics
Given a D2NN, we perform inference
by traversing the graph starting from the input nodes. Because a D2NN is a DAG, we can execute each node in a
topological order (the parents of a node are ordered before it;
we take both data edges and control edges in consideration),
same as conventional DNNs except that the control nodes
can cause the computation of some nodes to be skipped.
After we execute a control node, it outputs a set of control scores, one for each of its outgoing control edges. The
control edge with the highest score is “activated”, meaning
that the node being controlled is allowed to execute. The rest
of the control edges are not activated, and their controllees
are not allowed to execute. For example, in Fig 1 (right), the
node Q controls N2 and N3. Either N2 or N3 will execute
depending on which has the higher control score.
Although the main idea of the inference (skipping nodes)
seems simple, due to D2NNs’ ﬂexibility, the inference topology can be far more complicated. For example, in the case
of a node with multiple incoming control edges (i.e. controlled by multiple controllers), it should execute if any of
the control edges are activated. Also, when the execution of
a node is skipped, its output will be either the default value or
null. If the output is the default value, subsequent execution
will continue as usual. If the output is null, any downstream
nodes that depend on this output will in turn skip execution
and have a null output unless a default value has been set.
This “null” effect will propagate to the rest of the graph.
Fig. 1 (right) shows a slightly more complicated example
with default values: if N2 skips execution and outputs null,
so will N4 and N6. But N8 will execute regardless because
its input data edge has a default value. In our Experiments
Section, we will demonstrate more sophisticated D2NNs.
We can summarize the semantics of D2NNs as follows:
a D2NN executes the same way as a conventional DNN except that there are control edges that can cause some nodes
to be skipped. A control edge is active if and only if it has
the highest score among all outgoing control edges from a
node. A node is skipped if it has incoming control edges
and none of them is active, or if one of its inputs is null. If
a node is skipped, its output will be either null or a userdeﬁned default value. A null will cause downstream nodes
to be skipped whereas a default value will not.
A D2NN can also be thought of as a program with conditional statements. Each data edge is equivalent to a variable
that is initialized to either a default value or null. Executing
a function node is equivalent to executing a command assigning the output of the function to the variable. A control
edge is equivalent to a boolean variable initialized to False.
A control node is equivalent to a “switch-case” statement
that computes a score for each of the boolean variables and
sets the one with the largest score to True. Checking the conditions to determine whether to execute a function is equivalent to enclosing the function with an “if-then” statement.
A conventional DNN is a program with only function calls
and variable assignment without any conditional statements,
whereas a D2NN introduces conditional statements with the
conditions themselves generated by learnable functions.
D2NN Learning
Due to the control nodes, a D2NN cannot be trained the
same way as a conventional DNN. The output of the network cannot be expressed as a differentiable function of all
trainable parameters, especially those in the control nodes.
As a result, backpropagation cannot be directly applied.
The main difﬁculty lies in the control nodes, whose outputs are discretized into control decisions. This is similar to
the situation with hard attention models , which use reinforcement
learning. Here we adopt the same general strategy.
Learning a Single Control Node
For simplicity of exposition we start with a special case where there is only
one control node. We further assume that all parameters except those of this control node have been learned and ﬁxed.
That is, the goal is to learn the parameters of the control
node to maximize a user-deﬁned reward, which in our case
is a combination of accuracy and efﬁciency. This results in
a classical reinforcement learning setting: learning a control policy to take actions so as to maximize reward. We
base our learning method on Q-learning . We let each outgoing control edge represent an action, and let the control node approximate the
action-value (Q) function, which is the expected return of an
action given the current state (the input to the control node).
It is worth noting that unlike many prior works that use
deep reinforcement learning, a D2NN is not recurrent. For
each input to the network (e.g. an image), each control node
only executes once. And the decisions of a control node
completely depend on the current input. As a result, an action taken on one input has no effect on another input. That
is, our reinforcement learning task consists of only one time
step. Our one time-step reinforcement learning task can also
be seen as a contextual bandit problem, where the context
vector is the input to the control module, and the arms are
the possible action outputs of the module. The one time-step
setting simpliﬁes our Q-learning objective to that of the following regression task:
L = (Q(s, a) −r)2,
where r is a user-deﬁned reward, a is an action, s is the input to control node, and Q is computed by the control node.
As we can see, training a control node here is the same as
training a network to predict the reward for each action under an L2 loss. We use mini-batch gradient descent; for each
training example in a mini-batch, we pick the action with the
largest Q, execute the rest of the network, observe a reward,
and perform backpropagation using the L2 loss in Eqn. 1.
During training we also perform ϵ-greedy exploration —
instead of always choosing the action with the best Q value,
we choose a random action with probability ϵ. The hyperparameter ϵ is initialized to 1 and decreases over time. The
reward r is user deﬁned. Since our goal is to optimize the
trade-off between accuracy and efﬁciency, in our experiments we deﬁne the reward as a combination of an accuracy
metric A (for example, F-score) and an efﬁciency metric E
(for example, the inverse of the number of multiplications),
that is, λA + (1 −λ)E where λ balances the trade-off.
Mini-Bags for Set-Based Metrics
Our training algorithm
so far has deﬁned the state as a single training example, i.e.,
the control node takes actions and observe rewards on each
training example independent of others. This setup, however, introduces a difﬁculty for optimizing for accuracy metrics that cannot be decomposed over individual examples.
Consider precision in the context of binary classiﬁcation.
Given predictions on a set of examples and the ground truth,
precision is deﬁned as the proportion of true positives among
the predicted positives. Although precision can be deﬁned
on a single example, precision on a set of examples does
not generally equal the average of the precisions of individual examples. In other words, precision as a metric does
not decompose over individual examples and can only be
computed using a set of examples jointly. This is different
from decomposable metrics such as error rate, which can
be computed as the average of the error rates of individual
examples. If we use precision as our accuracy metric, it is
not clear how to deﬁne a reward independently for each example such that maximizing this reward independently for
each example would optimize the overall precision. In general, for many metrics, including precision and F-score, we
cannot compute them on individual examples and average
the results. Instead, we must compute them using a set of examples as a whole. We call such metrics “set-based metrics”.
Our learning setup so far is ill-equipped for such metrics because a reward is deﬁned on each example independently.
To address this issue we generalize the deﬁnition of a state
from a single input to a set of inputs. We deﬁne such a set
of inputs as a mini-bag. With a mini-bag of images, any setbased metric can be computed and can be used to directly
deﬁne a reward. Note that a mini-bag is different from a
mini-batch which is commonly used for batch updates in
gradient decent methods. Actually in our training, we calculate gradients using a mini-batch of mini-bags. Now, an
action on a mini-bag s = (s1, . . . , sm) is now a joint action
a = (a1, . . . , am) consisting of individual actions ai on example si. Let Q(s, a) be the joint action-value function on
the mini-bag s and the joint action a. We constrain the parametric form of Q to decompose over individual examples:
Q(si, ai),
where Q(si, ai) is a score given by the control node when
choosing the action ai for example si. We then deﬁne our
new learning objective on a mini-bag of size m as
L = (r −Q(s, a))2 = (r −
Q(si, ai))2,
where r is the reward observed by choosing the joint action
a on mini-bag s. That is, the control node predicts an actionvalue for each example such that their sum approximates the
reward deﬁned on the whole mini-bag.
It is worth noting that the decomposition of Q into sums
(Eqn. 2) enjoys a nice property: the best joint action a∗under
the joint action-value Q(s, a) is simply the concatenation of
the best actions for individual examples because maximizing
a∗= arg maxa(Q(s, a)) = arg maxa(m
i=1 Q(si, ai)) is
equivalent to maximizing the individual summands: a∗
arg maxai Q(si, ai), i = 1, 2...m. That is, during test time
we still perform inference on each example independently.
Another implication of the mini-bag formulation is:
∂xi = 2(r −m
j=1 Q(sj, aj)) ∂Q(si,ai)
, where xi is the
output of any internal neuron for example i in the mini-bag.
This shows that there is no change to the implementation of
backpropagation except that we scale the gradient using the
difference between the mini-bag Q-value Q and reward r.
Joint Training of All Nodes
We have described how to
train a single control node. We now describe how to extend
this strategy to all nodes including additional control nodes
as well as regular nodes. If a D2NN has multiple control
nodes, we simply train them together. For each mini-bag,
we perform backpropagation for multiple losses together.
   
   
   
    
)*+,$
Figure 2: The accuracy-cost or fscore-cost curves of various D2NN architectures, as well as conventional DNN baselines
consisting of only regular nodes.
Figure 3: Four different D2NN architectures.
Speciﬁcally, we perform inference using the current parameters, observe a reward for the whole network, and then use
the same reward (which is a result of the actions of all control nodes) to backpropagate for each control node.
For regular nodes, we can place losses on them the same
as on conventional DNNs. And we perform backpropagation on these losses together with the control nodes. The
implementation of backpropagation is the same as conventional DNNs except that each training example have a different network topology (execution sequence). And if a node is
skipped for a particular training example, then the node does
not have a gradient from the example.
It is worth noting that our D2NN framework allows arbitrary losses to be used for regular nodes. For example, for
classiﬁcation we can use the cross-entropy loss on a regular node. One important detail is that the losses on regular
nodes need to be properly weighted against the losses on the
control nodes; otherwise the regular losses may dominate,
rendering the control nodes ineffective. One way to eliminate this issue is to use Q-learning losses on regular nodes
as well, i.e. treating the outputs of a regular node as actionvalues. For example, instead of using the cross-entropy loss
on the classiﬁcation scores, we treat the classiﬁcation scores
as action-values—an estimated reward of each classiﬁcation
decision. This way Q-learning is applied to all nodes in a
uniﬁed way and no additional hyperparameters are needed
to balance different kinds of losses. In our experiments unless otherwise noted we adopt this uniﬁed approach.
Experiments
We here demonstrate four D2NN structures motivated by
different demands of efﬁcient network design to show its
ﬂexibility and effectiveness, and compare D2NNs’ ability to
optimize efﬁciency-accuracy trade-offs with prior work.
We implement the D2NN framework in Torch. Torch provides functions to specify the subnetwork architecture inside
a function node. Our framework handles the high-level communication and loss propagation.
High-Low Capacity D2NN
Our ﬁrst experiment is with
a simple D2NN architecture that we call “high-low capacity D2NN”. It is motivated by that we can save computation
by choosing a low-capacity subnetwork for easy examples.
It consists of a single control nodes (Q) and three regular
nodes (N1-N3) as in Fig. 3a). The control node Q chooses
between a high-capacity N2 and a low-capacity N3; the N3
has fewer neurons and uses less computation. The control
node itself has orders of magnitude fewer computation than
regular nodes (this is true for all D2NNs demonstrated).
We test this hypothesis using a binary classiﬁcation task in
which the network classiﬁes an input image as face or nonface. We use the Labeled Faces in the Wild dataset. Speciﬁcally, we use the
13k ground truth face crops (112×112 pixels) as positive examples and randomly sampled 130k background crops (with
an intersection over union less than 0.3) as negative examples. We hold out 11k images for validation and 22k for testing. We refer to this dataset as LFW-B and use it as a testbed
to validate the effectiveness of our new D2NN framework.
To evaluate performace we measure accuracy using the
F1 score, a better metric than percentage of correct predictions for an unbalanced dataset. We measure computational cost using the number of multiplications following
prior work and
for reproductivity. Speciﬁcally, we use the number of mul-
tiplications (control nodes included), normalized by a conventional DNN consisting of N1 and N2, that is, the highcapacity execution path. Note that our D2NNs also allow to
use other efﬁciency measurement such as run-time, latency.
During training we deﬁne the Q-learning reward as a linear combination of accuracy A and efﬁciency E (negative
cost): r = λA + (1 −λ)E where λ ∈ . We train instances of high-low capacity D2NNs using different λ’s. As
λ increases, the learned D2NN trades off efﬁciency for accuracy. Fig. 2a) plots the accuracy-cost curve on the test set; it
also plots the accuracy and efﬁciency achieved by a conventional DNN with only the high capacity path N1+N2 (High
NN) and a conventional DNN with only the low capacity
path N1+N3 (Low NN). As we can see, the D2NN achieves
a trade-off curve close to the upperbound: there are points
on the curve that are as fast as the low-capacity node and
as accurate as the high-capacity node. Fig. 4(left) plots the
distribution of examples going through different execution
paths. It shows that as λ increases, accuracy becomes more
important and more examples go through the high-capacity
node. These results suggest that our learning algorithm is
effective for networks with a single control node.
With inference efﬁciency improved, we also observe that
for training, a D2NN typically takes 2-4 times more iterations to converge than a DNN, depending on particular
model capacities, conﬁgurations and trade-offs.
Cascade D2NN
We next experiment with a more sophisticated design that we call a “cascade D2NN” (Fig. 3b). It is
inspired by the standard cascade design commonly used in
computer vision. The intuition is that many negative examples may be rejected early using simple features. The cascade D2NN consists of seven regular nodes (N1-N7) and
three control nodes (Q1-Q3). N1-N7 form 4 cascade stages
(i.e. 4 conventional DNNs, from small to large) of the cascade: N1+N2, N3+N4, N5+N6, N7. Each control node decides whether to execute the next cascade stage or not.
We evaluate the network on the same LFW-B face classiﬁcation task using the same evaluation protocol as in the highlow capacity D2NN. Fig. 2b) plots the accuracy-cost tradeoff curve for the D2NN. Also included are the accuracycost curve (“static NNs”) achieved by the four conventional
DNNs as baselines, each trained with a cross-entropy loss.
We can see that the cascade D2NN can achieve a close to
optimal trade-off, reducing computation signiﬁcantly with
negligible loss of accuracy. In addition, we can see that our
D2NN curve outperforms the trade-off curve achieved by
varying the design and capacity of static conventional networks. This result demonstrates that our algorithm is successful for jointly training multiple control nodes.
For a cascade, wall time of inference is often an important
consideration. Thus we also measure the inference wall time
(excluding data loading with 5 runs) in this Cascade D2NN.
We ﬁnd that a 82% wall-time cost corresponds to a 53%
number-of-multiplication cost; and a 95% corresponds to a
70%. Deﬁning reward directly using wall time can further
reduce the gap.
Chain D2NN
Our third design is a “Chain D2NN”
(Fig. 3c). The network is shaped as a chain, where each link
consists of a control node selecting between two (or more)
regular nodes. In other words, we perform a sequence of
vector-to-vector transforms; for each transform we choose
between several subnetworks. One scenario that we can use
this D2NN is that the conﬁguration of a conventional DNN
(e.g. number of layers, ﬁlter sizes) cannot be fully decided.
Also, it can simulate shortcuts between any two layers by using an identity function as one of the transforms. This chain
D2NN is qualitatively different from other D2NNs with a
tree-shaped data graph because it allows two divergent data
paths to merge again. That is, the number of possible execution paths can be exponential to the number of nodes.
In Fig. 3c), the ﬁrst link is that Q1 chooses between a
low-capacity N2 and a high-capacity N3. If one of them is
chosen, the other will output a default value zero. The node
N4 adds the outputs of N2 and N3 together. Fig. 2c) plots the
accuracy-cost curve on the LFW-B task. The two baselines
are: a conventional DNN with the lowest capacity path (N1-
N2-N5-N8-N10), and a conventional DNN with the highest
capacity path (N1-N3-N6-N9-N10). The cost is measured as
the number of multiplications, normalized by the cost of the
high-capacity baseline. Fig. 2c) shows that the chain D2NN
achieves a trade-off curve close to optimal and can speed
up computation signiﬁcantly with little accuracy loss. This
shows that our learning algorithm is effective for a D2NN
whose data graph is a general DAG instead of a tree.
Hierarchical D2NN
In this experiment we design a D2NN
for hierarchical multiclass classiﬁcation. The idea is to ﬁrst
classify images to coarse categories and then to ﬁne categories. This idea has been explored by numerous prior
works , but here we show that the same idea can
be implemented via a D2NN trained end to end.
We use ILSVRC-10, a subset of the ILSVRC-65 . In ILSVRC-10, 10 classes are organized into a 3layer hierarchy: 2 superclasses, 5 coarse classes and 10 leaf
classes. Each class has 500 training images, 50 validation
images, and 150 test images. As in Fig. 3d), the hierarchy in
this D2NN mirrors the semantic hierarchy in ILSVRC-10.
An image ﬁrst goes through the root N1. Then Q1 decides
whether to descend the left branch (N2 and its children), and
Q2 decides whether to descend the right branch (N3 and its
children). The leaf nodes N4-N8 are each responsible for
classifying two ﬁne-grained leaf classes. It is important to
note that an input image can go down parallel paths in the
hierarchy, e.g. descending both the left branch and the right
branch, because Q1 and Q2 make separate decisions. This
“multi-threading” allows the network to avoid committing
to a single path prematurely if an input image is ambiguous.
Fig. 2d) plots the accuracy-cost curve of our hierarchical
D2NN. The accuracy is measured as the proportion of correctly classiﬁed test examples. The cost is measured as the
number of multiplications, normalized by the cost of a conventional DNN consisting only of the regular nodes (denoted
as NN in the ﬁgure). We can see that the hierarchical D2NN
can match the accuracy of the full network with about half of
the computational cost. Fig. 4(right) plots for the hierarchical D2NN the distribution of examples going through exe-
Figure 4: Distribution of examples going through different execution paths. Skipped nodes are in grey. The hyperparameter λ
controls the trade-off between accuracy and efﬁciency. A bigger λ values accuracy more. Left: for the high-low capacity D2NN.
Right: for the hierarchical D2NN. The X-axis is the number of nodes activated.
Figure 5: Examples with different paths in a high-low D2NN (left) and a hierarchical D2NN (right).

Figure 6: Accuracy-cost curve for a chain D2NN on the CM-
NIST task compared to DCN .
cution sequences with different numbers of nodes activated.
Due to the parallelism of D2NN, there can be many different
execution sequences. We also see that as λ increases, accuracy is given more weight and more nodes are activated.
Comparison with Dynamic Capacity Networks
experiment we empirically compare our approach to closely
related prior work. Here we compare D2NNs with Dynamic
Capacity Networks (DCN) , for
which efﬁcency measurement is the absolute number of multiplications. Given an image, a DCN applies an additional
high capacity subnetwork to a set of image patches, selected
using a hand-designed saliency based policy. The idea is that
more intensive processing is only necessary for certain image regions. To compare, we evaluate with the same multiclass classiﬁcation task on the Cluttered MNIST , which consists of MNIST digits randomly placed
on a background cluttered with fragments of other digits.
We train a chain D2NN of length 4 , which implements the
same idea of choosing a high-capacity alternative subnetwork for certain inputs. Fig. 6 plots the accuracy-cost curve
of our D2NN as well as the accuracy-cost point achieved by
the DCN in —an accuracy of 0.9861
and and a cost of 2.77 × 107. The closest point on our curve
is an slightly lower accuracy of 0.9698 but slightly better
efﬁciency (a cost of 2.66 × 107). Note that although our accuracy of 0.9698 is lower, it compares favorably to those of
other state-of-the-art methods such as DRAW : 0.9664 and RAM : 0.9189.
Visualization of Examples in Different Paths
(left), we show face examples in the high-low D2NN for
λ=0.4. Examples in low-capacity path are generally easier (e.g. more frontal) than examples in high-capacity path.
In Fig. 5 (right), we show car examples in the hierarchical
D2NN with 1) a single path executed and 2) the full graph
executed (for λ=1). They match our intuition that examples
with a single path executed should be easier (e.g. less occlusion) to classify than examples with the full graph executed.
CIFAR-10 Results
We train a Cascade D2NN on CIFAR-
10 where the corresponded DNN baseline is the ResNet-110.
We see a 16% improvement of efﬁciency with a 1% loss on
accuracy, and a 42% improvement of efﬁciency with a 4%
loss on accuracy. The D2NN’s ability to improve efﬁciency
relies on the assumption that not all inputs require the same
amount of computation. In CIFAR-10, all images are low
resolution (32 × 32), and it is likely that few images are signiﬁcantly easier to classify than others. As a result, the efﬁciency improvement is modest compared to other datasets.
Conclusions
We have introduced Dynamic Deep Neural Networks
(D2NN), a new type of feed-forward deep neural networks
that allow selective execution. Extensive experiments have
demonstrated that D2NNs are ﬂexible and effective for optimizing accuracy-efﬁciency trade-offs.
Acknowledgments
This work is partially supported by the National Science
Foundation under Grant No. 1539011 and gifts from Intel.