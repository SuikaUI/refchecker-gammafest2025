Multiview Deep Learning For Land-Use
Classiﬁcation
F. P. S. Luus, B. P. Salmon, F. van den Bergh, and B. T. J. Maharaj
Abstract—A multiscale input strategy for multiview deep
learning is proposed for supervised multispectral land-use classiﬁcation and it is validated on a well-known dataset. The hypothesis
that simultaneous multiscale views can improve compositionbased inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end
learning system learns a hierarchical feature representation with
the aid of convolutional layers to shift the burden of feature
determination from hand-engineering to a deep convolutional
neural network. This allows the classiﬁer to obtain problemspeciﬁc features that are optimal for minimizing the multinomial
logistic regression objective, as opposed to user-deﬁned features
which trades optimality for generality. A heuristic approach to
the optimization of the deep convolutional neural network hyperparameters is used, based on empirical performance evidence.
It is shown that a single deep convolutional neural network
can be trained simultaneously with multiscale views to improve
prediction accuracy over multiple single-scale views. Competitive
performance is achieved for the UC Merced dataset where
the 93.48% accuracy of multiview deep learning outperforms
the 85.37% accuracy of SIFT-based methods and the 90.26%
accuracy of unsupervised feature learning.
Index Terms—Neural network applications, neural network
architecture, feature extraction, urban areas, remote sensing.
I. INTRODUCTION
EATURE design has been a mainstay in classiﬁer applications and much effort has been invested in handengineering speciﬁc features that are suitable only for select use-cases. The advent of GPU-accelerated computational
resources made feasible the implementation of multilayer
convolutional neural network (CNN) approaches for classiﬁcation. Deep learning discovers optimal features for the given
problem in order to minimize the log loss cost function during
classiﬁcation. It is important to investigate the performance
beneﬁts of using the optimal problem-speciﬁc features learned
by deep learning instead of using user-deﬁned features that
trades problem-speciﬁc optimality for general applicability.
The features discovered by deep learning are optimal in the
sense that they minimize the multinomial logistic regression
objective, and improved accuracy is expected compared to the
use of more general user-deﬁned features like SIFT and Gabor
features. The objective of this research was to design a deep
convolutional neural network (DCNN) for the UC Merced
F. P. S. Luus, and B. T. J. Maharaj are with the Department of Electrical,
Electronic and Computer Engineering, University of Pretoria, Pretoria 0002,
South Africa (e-mail: ).
B. P. Salmon is with the School of Engineering and ICT, University of
Tasmania, Hobart, Tas 7001, Australia.
F. van den Bergh is with the Remote Sensing Research Unit, Meraka
Institute, Council for Scientic and Industrial Research, Pretoria 0001, South
land-use dataset , a dataset compiled in 2010 and used
as a benchmark in several land-use classiﬁcation studies –
 . The challenge is to optimize classiﬁcation accuracy by
ﬁnding a proper selection of DCNN hyper-parameters, which
are deﬁned as the DCNN settings, such as the architecture
design, convolutional ﬁlter bank speciﬁcations, pooling layer
speciﬁcations, and learning rate and momentum values, excluding the learned neuron weights and biases.
While the hyper-parameter selection and the reduction of
overﬁtting through data augmentation do have a signiﬁcant
impact on deep learning performance, an additional strategy
is needed to achieve competitive performance. This requires
moving beyond simple label-preserving transformations such
as mirroring and rotation to augment the input dataset, while
still adhering to the guiding principle of minimum intervention
so that the majority of the feature learning burden can be
delegated to the deep learning solution.
The approach contributed in this letter is a generalization
of the multiview strategy used by Krizhevsky et al. to
admit multiple view scales used to extract partial input sample
patches. Classes with size-varying objects, such as storage
tanks, can then potentially be recognized more accurately if
consensus of multiscale views is used, a hypothesis tested in
this research.
Deep learning has been used previously in remote sensing
for hierarchically extracting deep features with deep belief
networks or stacked auto-encoders in combination with
principal component analysis (PCA) and logistic regression
for hyperspectral data classiﬁcation . A hybrid DCNN was
presented by Chen et al. for improved vehicle detection
in satellite images where variable-scale features are extracted
through the use of multiple blocks of variable receptive ﬁeld
sizes or max-pool ﬁeld sizes. Remote sensing image fusion
with deep neural networks (DNN) has been done by Huang
et al. using stacked modiﬁed sparse denoising autoencoders for pretraining the hidden layers of the DNN to
avoid the “diffusion of gradients” caused by random neuron
initialization.
An overview of the UC Merced dataset is given in Section
II, and the methodology and design approach is discussed
in Section III. The benchmark setup and description of the
empirical investigation of different deep learning architectures
is then given in Section IV. The results are also presented in
Section IV where class confusion, convergence, visualization
of the inner workings of the network, and comparison to the
results of other published methods are addressed before a
conclusion is reached in Section V.
II. DATASET
The UC Merced land-use dataset is investigated, which
is a set of aerial ortho-imagery with a 0.3048 m pixel resolution extracted from United States Geological Survey (USGS)
national maps. The UC Merced dataset has been used as
a benchmark for land-use classiﬁer evaluation in numerous
publications – .
The dataset consists of 21 land-use classes containing a
variety of spatial patterns, some with texture and/or color
homogeneity and others with heterogeneous presentation. The
dataset was compiled from a manual selection of 100 images
per class, each RGB image being approximately 256×256
pixels. The 21 land-use types include agricultural, airplane,
baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection, medium
density residential, mobile home park, overpass, parking lot,
river, runway, sparse residential, storage tanks, and tennis court
III. METHODOLOGY
In this section an overview of the training of a DCNN
is ﬁrst given, followed by a description of the important
processing layers of a DCNN. The speciﬁc DCNN architecture
instantiation developed for the UC Merced dataset is then
deﬁned and then methods of reducing training overﬁtting are
given. A multiscale multiview input strategy is then described
that utilizes the deﬁned DCNN.
A. Deep Learning
Deep learning is characterized as an end-to-end learning
system typically consisting of more than 5 processing layers,
which is usually supervised and produces a discriminative
classiﬁcation for a given input. The burden of feature determination is shifted to a DCNN, which learns the optimal
features for the given problem in order to minimize a loss
cost function. The features are learned in a hierarchical manner
where higher-level features are learned in deeper convolutional
layers as combinations of lower-level features determined in
shallow layers.
An improved accuracy is expected by directly learning the
features that minimize the multiclass log loss cost function
j=1 yi,j log(pi,j) for a given dataset with
N samples, compared to using predetermined features. The
natural logarithm of the probability pi,j of sample i belonging
to class j is counted by setting yi,j = 1 only if i belongs to
class j. Stochastic gradient descent can be used since the loss
function is a sum of differentiable functions, and Nesterov’s
accelerated gradient in particular has been shown to be effective despite the use of noisy gradient estimates . The
update increment vt+1 and the updated network parameters
wt+1 are calculated as follows, with momentum µ and learning
vt+1 = µ · vt −ǫ ·
wi+1 = wi + vt+1
The loss gradient estimate ∂L
∂w is determined for the average
loss over a smaller batch Bi of input samples for the DCNN
parameters equal to wi + µ · vt.
B. Architecture deﬁnitions
1) Convolutional layers: A CNN consists of convolutional
layers, each followed by optional sub-sampling and regularization layers, and ending in fully-connected 1D hidden layers.
A convolutional layer receives a 3D input and creates a 3D
output that measures the ﬁlter responses at each input location,
calculated as the sum of the element-wise incidence product
between the ﬁlter and image window. This convolutional
response encodes the input in terms of learned templates to
systematically reduce input dimensionality as a part of feature
determination.
2) Activation functions: Each ﬁlter response becomes the
input to a nonlinear activation function, which should be nonsaturating in order to accelerate learning. Rectiﬁed linear units
(ReLU) (f(x) = max(0, x)) are used in lieu of saturating nonlinearities after every convolutional and fully-connected layer,
except for the ﬁnal dense layer which uses softmax activation
(f(xj) = exj/ P
k exk) to maximize the multinomial logistic
regression objective. Network implementation is simpliﬁed
with the use of ReLU, as this activation function does not
require input normalization to avoid saturation, although local
normalization can promote improved generalization .
3) Sub-sampling layers: Sub-sampling layers normally proceed convolutional layers to further reduce feature dimensionality, but also to achieve translation invariance in the case
of max-pool sub-sampling layers . E.g. a 2×2 max-pool
layer divides the convolutional layer output into a set of
non-overlapping 2 × 2 cells and only records the maximum
activated ﬁlter response in each cell, thereby halving the
input dimensions and producing features that are increasingly
invariant to image object translations.
C. Architecture instantiation
The DCNN design given in this subsection was heuristically
selected based on experimental investigation that adhered to
the objective of layer dimension reduction, since it develops a strong hierarchical feature representation. The DCNN
designed for the UC Merced dataset accepts a 96 × 96 × 3
input, which can be converted from an RGB to HSV (Hue-
Saturation-Value) color model. The HSV color model can
more directly concentrate chromaticity to single ﬁlter layers,
which can potentially simplify features and allow for the
reduction of network complexity.
The input is converted to 45×45×64 neurons with the ﬁrst
convolutional layer using 64 ﬁlters of 7×7×3 operating at a
stride of (2, 2), before being sub-sampled with a 2×2 maxpool layer to obtain a 23×23×64 output with 10% dropout.
The second convolutional layer uses 192 ﬁlters of 3×3×64 to
produce a 21×21×192 output, which is sub-sampled with a
2×2 max-pool to give a 11×11×192 output with 20% dropout
as shown in Figure 1. A third convolutional layer with 192
ﬁlters of 3×3×192 produces a 9×9×192 output followed by
a 2×2 max-pool layer which outputs 5×5×192 neurons with
Dense 256x21
Dense 256x256
+ Dropout 50%
Dense 896x256
+ Dropout 40%
Max-pool 2x2
Convolutional
+ Dropout 30%
Max-pool 2x2
Convolutional
+ Dropout 20%
Max-pool 2x2
Convolutional
+ Dropout 10%
Max-pool 2x2
Convolutional
(2,2) stride
Figure 1. CNN architecture with 4 convolutional layers accepting 96×96×3
inputs and resolving to a 21-class softmax output layer.
30% dropout. The ﬁnal convolutional layer has 224 ﬁlters of
2×2×192 and gives a 4×4×224 output, which is max-pooled
with 2×2 cells to render a 2×2×224 output with 40% dropout.
A fully-connected dense layer with 256 hidden units are used
with ReLU activation and 50% dropout follows, after which
another dense layer with 256 hidden units are used before
resolving to 21 units in a softmax output layer.
All neuron biases are set to 0 and network weights are
initialized randomly according to normalized initialization
given by Glorot et al. ,
where nj and nj+1 are the number of neurons in layers j and
j+1, respectively. The ﬁnal DCNN weight and bias parameters
are based on the epoch registering the minimum value for the
log loss cost function on the training data.
D. Reducing overﬁtting
1) Dropout: Convolutional and fully-connected layers can
be interconnected so that hidden neuron outputs are deactivated with probability p during training, with the remainder
of the outputs multiplied by
1−p. This strategy reduces the
co-adaptation of neurons, since dropout forces neurons to
provide more useful and robust contributions in combination
with arbitrary active neuron combinations . The set of
dropped neurons changes randomly at every epoch, which
changes the architecture and reduces overﬁtting at the cost
of approximately
1−p times the convergence period compared
to training without dropout.
2) Data augmentation: The original input dataset can be
expanded with label-preserving transformations such as horizontal and vertical ﬂips and rotation. This presents the network
with an enlarged set of inputs which may contain examples
present in the test dataset but not in the original training
dataset, thus improving classiﬁcation accuracy. During training
all views are ﬂipped horizontally or vertically with probability
of 0.5, but for testing the model averaging only considers the
untransformed views. The classiﬁer is trained with transformed
views so that any untransformed view can be recognized
during testing.
E. Multiview deep learning
Another form of data augmentation involves the use of
multiple partial views of a given input sample to train with,
(0:127 , 0:127)
Lower right:
(32:127 , 0:95)
Upper right:
(32:127 , 32:127)
Lower left:
(0:95 , 0:95)
Upper left:
(0:95 , 32:127)
Scale: 256→128
scaled image
input image
(64:159 ,64:159)
Lower right:
(96:191 , 0:95)
Upper right:
(96:191 , 96:191)
Lower left:
(0:95 , 0:95)
Upper left:
( 0:95 , 96:191)
Scale: 256→192
scaled image
input image
Partial view selection speciﬁcations for composing a multiview
input dataset consisting of 10 × 96 × 96 × 3 inputs per sample.
and classifying test samples with the mean softmax output
averaged over a predetermined set of classiﬁed patches or
views, i.e. model averaging . Some classes are distinguished
by the presence of certain objects, such as airplanes and
storage tanks, which only occupy a portion of a given sample.
If these objects vary in size across different samples then
multiscale views can potentially produce stronger activations
with higher probability than single-scale views.
The main contribution proposed is that a single DCNN
can be trained with multiscale views to obtain improved
classiﬁcation accuracy compared to using multiple views at
one particular scale only. The UC Merced dataset samples are
downsampled from 256×256 to 96×96 based on empirical
evaluation of the optimal input size, and 10 multiscale views
are extracted as follows. The ﬁrst 4 augmenting views are
acquired at the image corners at 75% input coverage, while
the ﬁfth view has 100% coverage. Views 6 to 10 are obtained at
the corners and center at 50% input coverage, and all extracted
views are scaled to the input size of 96 × 96 as shown in
IV. RESULTS AND DISCUSSION
A. Experimental setup
The standard benchmark conditions for the UC Merced
dataset ﬁrst stipulated in is followed to measure classi-
ﬁcation accuracy. Five-fold stratiﬁed cross-validation is used
for all experiments, where four folds are used for training and
model selection, and the remaining unseen fold is classiﬁed
to measure accuracy. Initial empirical evaluation indicated that
the hyper-parameters that most inﬂuence accuracy include the
input size, ﬁrst convolutional ﬁlter size and ﬁlter amount, and
the network learning rate. Hyper-parameter range selections
are based around values that resulted in high classiﬁcation
accuracy during an initial evaluation. Various architecture
instantiations are evaluated empirically to optimizing the
aforementioned hyper-parameters.
FIVE-FOLD CROSS-VALIDATION ACCURACY FOR VARIOUS DCNN
ARCHITECTURES. ALL INSTANTIATIONS USE NESTEROV’S ACCELERATED
GRADIENT (LINEAR MOMENTUM µ = 0.9 →0.999, LINEAR LEARNING
RATE DECREASE TO 0.0001, BATCH SIZES |Bi| = 128)
Architecture
Input size
Filter 1 size
Learning rate
0.005 0.005
Max epochs
Multiscale
HSV: Acc. (µ
90.53 91.18
±1.74 ±2.88 ±1.87 ±1.62 ±1.25 ±1.98
RGB: Acc. (µ
91.10 92.76 93.48
±3.77 ±0.80 ±1.46 ±0.82
Test − multiview
Averaged ﬁve-fold cross-validation accuracy graphs for multiview
architecture #5 (see Table I).
B. Architecture selection
Several architectures have been evaluated to obtain the best
performing DCNN for the UC Merced dataset, and the results
are shown in Table I. The important design choices include
the reduction in learning rate, using model averaging with an
increasing number of multiple views, and ﬁnding the optimal
input size of 96×96.
The single-scale multiview input of Krizhevsky et al. 
has been implemented in arch. #3, but its 91.1% accuracy is
outperformed by the 92.75% of multiscale input (arch. #4).
Using the ﬁrst ﬁve views (arch. #4 in Table I) speciﬁed in
Figure 2 improved test accuracy from 87.14% to 92.76%, but
using model averaging with all ten views (arch. #5 in Table I)
resulted in an accuracy of 93.48% for RGB inputs.
The DCNN training convergence rate is illustrated for architecture #5 (see Table I) in Figure 3, comparing the progression
of training and testing accuracies in terms of training epochs.
The single-view test accuracy is also shown, which performs
poorer than with multiview model averaging.
Figure 4 displays a visualization of the trained singleview DCNN architecture #2 (see Table I), showing the ﬁrst
convolutional ﬁlters and the convolutional responses for a
selection of UC Merced classes. The ﬁrst max-pool and
dropout outputs are also shown to illustrate their functions
of sub-sampling and omission noise. The second, third, and
fourth convolutional ﬁlter banks are too large to display in the
letter and are not included. The convolutional ﬁlters are the
core features that are learnt by the DCNN and it is seen that
the network reduces convolutional response dimensions to a
ﬁnal single-dimensional response appropriate for the use of
softmax activation.
UC MERCED ACCURACY COMPARISON.
Accuracy (%)
SPCK++ 
Saliency-UFL 
82.72±1.18
Bag-of-SIFT 
85.37±1.56
Single-view deep learning
88.00±2.88
SAL-LDA 
Pyramid of spatial relatons 
90.26±1.51
Multiview deep learning
93.48±0.82
C. Accuracy comparison
A ﬁve-fold stratiﬁed cross-validation comparison of all the
important methods employed in the literature for the UC
Merced dataset is shown in Table II. The highest accuracies for
the UC Merced dataset have been achieved with unsupervised
feature learning (UFL) and the vector of locally aggregated
tensors (VLAT) method , which is an extension of visual
dictionary approaches like bag-of-words. Single-view DCNN
is outperformed by these methods, but the 90.26% accuracy
of UFL can be improved upon with a multiview DCNN which
achieves 93.48%.
A confusion analysis was also performed for DCNN architecture #2 (see Table I) and the most notable class confusion
was between medium density residential and dense residential,
as well as between buildings and storage tanks. The classes
with the least accurate predictions are storage tanks, buildings,
medium density residential, and tennis court classes.
The classes that beneﬁted most from multiscaling the ﬁveview input were the sparse residential, runway, dense residential, storage tanks, freeway, river and overpass classes, while
the agricultural class performed the worst. This gives evidence
for the hypothesis that object-based classes can beneﬁt from
multiscale views if the objects tend to vary in size, such as in
the storage tanks and sparse residential classes.
D. Implementation details
For the ten-view DCNN instantiation #5 (Table I) a running
time of 36.6 seconds per epoch was attained on an Amazon
Elastic Compute Cloud g2.2xlarge instance with a GRID K520
GPU possessing 1536 CUDA cores and 4 GB video memory
of which 1 GB was used. A Python implementation was
used based on Theano and Lasagne , which provides a
GPU-accelerated computational differentiation platform which
automatically computes gradients for complex systems.
V. CONCLUSION
An end-to-end learning system with hierarchical feature
representation was designed in this letter for complex landuse classiﬁcation of high-resolution multispectral aerial imagery. DCNN architectures were optimized in terms of crossvalidation accuracy on the UC Merced land-use dataset, and
it was shown that multiscale views can be used to train a
single network and increase classiﬁcation accuracy compared
(a) Filters - Convolution 1: Trained 7 × 7 × 3 convolutional ﬁlters (64 ﬁlters)
(b) Convolution 1: 45 × 45 × 64 output from 96 × 96 × 3 input convoluted with 7 × 7 × 3 ﬁlters and (2, 2) stride
Agricultural:
Buildings:
residential:
residential:
Storage tanks:
(c) Max-pool 1: 23 × 23 × 64 output from 45 × 45 × 64 input max-pooled with (2, 2). Outputs shown for dense residential and storage tanks.
(d) Dropout 1: 23 × 23 × 64 output from 45 × 45 × 64 max-pooled input with 10% dropout. Outputs shown for dense residential and storage tanks.
(e) Convolution 2: 21 × 21 × 192 output from 23 × 23 × 64 input convoluted with 3 × 3 × 64 ﬁlters. Outputs for dense residential and storage tanks.
(f) Convolution 3: 9 × 9 × 192 output from 11 × 11 × 192 input convoluted with 3 × 3 × 192 ﬁlters. Dense residential (above) and storage tanks (below).
(g) Convolution 4: 4 × 4 × 224 output from 5 × 5 × 192 input convoluted with 2 × 2 × 192 ﬁlters. Dense residential (above) and storage tanks (below).
Filters and CNN layer outputs for single-view architecture #2 (see Table I) and inputs from a selection of classes. Output visuals are mapped to
full channel range and combined in some cases to occupy all RGB channels.
to using single-view samples. Competitive performance was
shown where multiview DCNN outperformed both SIFT-based
methods and unsupervised feature learning. Future research
may investigate the performance beneﬁts of a combination of
DCNN cascaded with secondary neural networks and the use
of only one view scale per network.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their astute observations and keen advice. This work was
supported by the National Research Foundation (NRF) of
South Africa. Opinions expressed and conclusions arrived at,
are those of the authors and are not necessarily to be attributed
to the NRF.