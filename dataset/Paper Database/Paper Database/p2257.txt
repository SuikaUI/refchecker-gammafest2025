26 March 2025
Alma Mater Studiorum Università di Bologna
Archivio istituzionale della ricerca
LISANTI, G., MASI, I., BAGDANOV, A.D., DEL BIMBO, A. . Person Re-identification by Iterative Reweighted Sparse Ranking. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 37,
1629-1642 [10.1109/TPAMI.2014.2369055].
Published Version:
Person Re-identification by Iterative Re-weighted Sparse Ranking
Published:
DOI: 
Terms of use:
(Article begins on next page)
Some rights reserved. The terms and conditions for the reuse of this version of the manuscript are
specified in the publishing policy. For all terms of use and more information see the publisher's website.
Availability:
This version is available at: since: 2019-01-11
This is the final peer-reviewed author’s accepted manuscript (postprint) of the following publication:
This item was downloaded from IRIS Università di Bologna ( 
When citing, please refer to the published version.
This item was downloaded from IRIS Università di Bologna ( 
When citing, please refer to the published version.
This is the final peer-reviewed accepted manuscript of:
G. Lisanti, I. Masi, A. D. Bagdanov and A. D. Bimbo, "Person Re-Identification by
Iterative Re-Weighted Sparse Ranking," in IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 37, no. 8, pp. 1629-1642, 1 Aug. 2015.
 
Rights / License:
The terms and conditions for the reuse of this version of the manuscript are specified in the
publishing policy. For all terms of use and more information see the publisher's website.
Person Re-identiﬁcation by
Iterative Re-weighted Sparse Ranking
Giuseppe Lisanti, Iacopo Masi, Andrew D. Bagdanov, and Alberto Del Bimbo,
Abstract—In this paper we introduce a method for person re-identiﬁcation based on discriminative, sparse basis expansions of targets
in terms of a labeled gallery of known individuals. We propose an iterative extension to sparse discriminative classiﬁers capable of
ranking many candidate targets. The approach makes use of soft- and hard- re-weighting to redistribute energy among the most
relevant contributing elements and to ensure that the best candidates are ranked at each iteration. Our approach also leverages
a novel visual descriptor which we show to be discriminative while remaining robust to pose and illumination variations. An extensive
comparative evaluation is given demonstrating that our approach achieves state-of-the-art performance on single- and multi-shot person
re-identiﬁcation scenarios on the VIPeR, i-LIDS, ETHZ, and CAVIAR4REID datasets. The combination of our descriptor and iterative
sparse basis expansion improves state-of-the-art rank-1 performance by 6 percentage points on VIPeR and by 20 on CAVIAR4REID
compared to other methods with a single gallery image per person. With multiple gallery and probe images per person our approach
improves by 17 percentage points the state-of-the-art on i-LIDS and by 72 on CAVIAR4REID at rank-1. The approach is also quite
efﬁcient, capable of single-shot person re-identiﬁcation over galleries containing hundreds of individuals at about 30 re-identiﬁcations
per second.
Index Terms—person re-identiﬁcation, video surveillance, sparse methods.
INTRODUCTION
ERSON re-identiﬁcation is the task of recognizing
a person, captured by one or more cameras, over
a range of candidate targets represented as a gallery
of already-labeled subjects. This gallery may contain
imagery of known subjects from one or more sensors,
and there may be no guarantee that the unknown person
observed has already been imaged from the same point
of view or in the same conditions. In fact, some of the
main complications in person re-identiﬁcation are due
to the fact that the same person is usually acquired
at different times, by different disjoint cameras, and
this can result in large variations in target appearance
because of different illumination conditions, different
poses or partial occlusions.
Person re-identiﬁcation is a critical component of modern surveillance systems as it is a way of maintaining
identity information about targets in multiple views over
potentially long periods of time. This matching across
cameras is traditionally cast as a retrieval problem: given
one or more images of an unknown target, the reidentiﬁcation task is to rank all individuals in a gallery of
known target images in terms of similarity to the person
to be recognized.
Much of the research on person re-identiﬁcation has
concentrated on human appearance modeling – .
A number of descriptors of image content have been
proposed to discriminate identities while compensating
Integration and Communication Center (MICC), Universit`a di Firenze,
Barcelona. Email: {giuseppe.lisanti, iacopo.masi, alberto.delbimbo}@uniﬁ.it;
 
for appearance variability due to changes in pose, illumination and camera viewpoint. Re-identiﬁcation has
also been cast as a learning problem in which either
metrics or discriminative models are learned. Metric
learning approaches – require labeled training
data and most of them also require new training data
when camera settings change. Discriminative models, on
the other hand, can suffer from lack of training data
in small gallery image sets and are often unsuitable
for ordering large numbers of candidates due to their
inability to reliably rank all but a few of the best ones.
The literature on person re-identiﬁcation focuses on
several different modalities or scenarios that are recognized as de facto standards for performance evaluation.
These modalities are characterized in terms of how many
images of each individual are known a priori to be in
the gallery and probe sets, and according to whether or
not it is known that multiple images in the probe set
correspond to a single target. The three most common
are: the single-versus-single (SvsS) modality where there
is a single exemplar for each person in the gallery and
at least one exemplar for each person in the probe set
(multiple exemplars of the same identity are considered
independently); the multi-versus-single (MvsS) modality
in which there is one group of multiple exemplars for
each person in the gallery and a single exemplar of
each person in the probe set; and the multi-versus-multi
(MvsM) modality in which there is a group of multiple
exemplars for each person in the gallery and group of
multiple images of each person in the probe set.
In this article we propose a robust and efﬁcient approach to person re-identiﬁcation that is applicable to
all modalities considered in the literature. Our technique
builds on sparse basis expansions that have been demonstrated to be a powerful tool for face recognition . The
use of sparse basis expansions for recognition problems
is based on the observation that, even if data is high
dimensional, samples from the same class tend to lie on
the same low-dimensional subspace of the original feature space. If a good basis can be found, regularization
can then be used to enforce sparsity and leverage this
subspace structure to discriminate new test examples.
Since faces under changing illumination lie near a linear
subspace of the original feature space , sparse linear
reconstructions are able to explain both noise (through
linear reconstruction) and identity (by sparseness) in
the model. However, the approach of does not
directly generalize to the person re-identiﬁcation problem because, on the one hand, re-identiﬁcation imagery
does not have the same beneﬁt of controlled imaging
conditions, and on the other hand ℓ1-regularized basis
expansions, by their very nature, can only support ranking of a limited number of individuals. Our intuition,
nevertheless, is that this type of sparse, discriminative
approach can be also applied to re-identiﬁcation problems after addressing these issues.
In more detail, variations in pose, changing target
appearance due to articulated motion, and illumination
changes make it unlikely that linear reconstructions of
unknown targets both explain noise in the model and
discriminate identities well. Instead of operating directly
on images, as in , we use a feature representation
for target appearance that approximate the desired invariants so that the sparse linear reconstruction model
does not have to explain both noise and target identity.
We thus propose a novel descriptor of person appearance, demonstrate its robustness to pose and illumination variations, and show that its use in our sparse
discriminative framework yields state-of-the-art results.
Our descriptor has the additional advantage of requiring
no foreground/background segmentation or body part
localization.
At the same time, person re-identiﬁcation is an application where recall is often important. In fact, since
whole-body appearance is a less persistent biometric
than faces, in many re-identiﬁcation scenarios recall is
important in order to maximize the likelihood of ﬁnding
the correct identity in the ﬁrst ten or even twenty results.
Sparse reconstructions, however, by their very nature
can provide inadequate support for ranking more than
a few gallery individuals. We address this problem by
analyzing the reconstruction error and partially ranking
the gallery in terms of similarity to the query probe.
We then re-weight this initial solution in order to mute
the response of vectors contributing little to the initial
expansion. Through the use of this novel, iterative reweighting algorithm, we can then proceed to rank the
remaining gallery individuals through analysis of reweighted sparse basis expansions.
reidentiﬁcation literature. Our approach to describing the
visual appearance of persons is given in Sec. 3, and in
Sec. 4 we show how to perform re-identiﬁcation with
sparse basis expansions. In Sec. 5 we give an extensive
comparative evaluation of our technique with respect
to the state-of-the-art on four publicly available datasets
and give a detailed analysis of each component of our
approach. Finally, in Sec. 6 we draw some conclusions
and discuss new directions for research.
RELATED WORK
Many recent works have addressed the problem of
person re-identiﬁcation. Most focus primarily on either
new descriptors for person appearance, or on learning
techniques for person re-identiﬁcation.
Descriptors for person re-identiﬁcation: Much research
on person re-identiﬁcation has addressed the deﬁnition
of discriminative features for person appearance. The
ﬁrst work that considered the problem of appearance
models for person recognition, reacquisition and tracking was that of Gray et al. . The authors argue that,
until then, these problems had been evaluated independently and that there is a need for metrics that apply
to complete systems , . They proposed a standard
protocol to compare results using the Cumulative Match
Curve (CMC) and introduced the VIPeR dataset for reidentiﬁcation. The ﬁrst work based on these guidelines
was in which the authors propose an algorithm that
learns a domain-speciﬁc similarity function using an
ensemble of local features and AdaBoost. Features are
raw color channels in many color spaces and texture
information captured by Schmid and Gabor ﬁlters.
Descriptors of visual appearance for person recognition can be highly susceptible to background clutter,
and many approaches to person re-identiﬁcation use
background modeling – or part-based person appearance models , to separate foreground from
background signals. In the authors use a sophisticated
appearance model, the Symmetry-Driven Accumulation
of Local Features (SDALF) descriptor that models human
body parts by estimating the axis of symmetry of a
person and obtaining the head, torso, and legs positions.
Each part is then represented by weighted HSV color
histograms, maximally stable color region descriptors,
and recurrent highly-structured patches. This work also
applies a strong, generative background prior that enhances the discriminative power of the descriptor by
segmenting the person from the background . In 
and a multi-shot appearance model similar to 
is proposed in order to condense a set of frames of
the same individual into a highly informative signature,
which they call the Histogram Plus Epitome (HPE).
In the authors employ an estimate of body pose
to guide feature extraction. They extend the Pictorial
Structure (PS) model with their Custom Pictorial
Structure (CPS), which is a two-step iterative process
that alternates between estimating pose and updating
the appearance model.
Another state-of-the-art approach with performance
similar to is proposed in . The authors use an
appearance model that, in contrast with and ,
does not rely on body parts. The approach is based
on a descriptor called the Mean Riemannian Covariance Grid (MRCG), which is an extension of Spatial
Covariance Regions (SCR) , that is the covariance
of a vector of eleven cues derived from equalized RGB
colors. The MRCG descriptor is computed as a mean
of gallery examples and is only applicable to multi-shot
re-identiﬁcation modalities. The person re-identiﬁcation
problem was extended to groups in . The authors
show that groups represent a contextual cue that can be
exploited to improve person re-identiﬁcation.
Re-identiﬁcation problems are often characterized by
poor and variable image quality on which it can be hard
to ﬁt background or part-based models without relying
on scene-speciﬁc information. Our approach, on the
other hand, is able to exploit multiple gallery examples
and does not require sophisticated background or body
part modeling.
Learning-based re-identiﬁcation: Among the methods
that interpret re-identiﬁcation as a learning problem, the
authors of propose a discriminative model created
using Partial Least Squares (PLS) which weights features
according to their discriminative power for each different gallery instance. In , a metric learning framework
is used to obtain a robust Mahalanobis metric for Large
Margin Nearest Neighbor classiﬁcation with Rejection
(LMNN-R). The approach in is a supervised technique that uses pairs of similar and dissimilar images
and a relaxed RankSVM algorithm to rank probe images. Another metric learning approach is that of 
which learns a Mahalanobis distance from equivalence
constraints derived from target labels.
The Probabilistic Distance Comparison (PRDC) approach introduces a novel comparison model which
aims to maximize the probability of a pair of correctly
matched images having a smaller distance than that of
an incorrectly matched pair. The same authors in 
model person re-identiﬁcation as a transfer ranking
problem where the goal is to transfer similarity observations from a small gallery to a larger, unlabeled probe set.
A set-based discriminative ranking approach was also
recently proposed which alternates between optimizing
a set-to-set geometric distance and a feature space projection, resulting in a discriminative set-distance-based
model . Camera transfer approaches have also been
proposed that use images of the same person captured
from different cameras to learn metrics , . In 
the authors apply learning in a covariance metric space
using an entropy-driven criterion to select the most descriptive features for a speciﬁc class of objects. Recently
saliency has been considered when matching people
across views and a novel method eSDC has been
proposed in order to learn saliency parts of a human in
a unsupervised fashion.
Learning-based approaches have recently reported
higher re-identiﬁcation accuracy with respect to the
state-of-the-art. However, re-identiﬁcation problems are
often also characterized by a lack of reliably labeled data.
The need to label image data for each scenario, camera
conﬁguration and parameter settings is a disadvantage
of metric learning approaches. Our approach outperforms the state-of-the-art at rank-1 in most modalities
without learning metrics or ﬁtting discriminative models
to gallery image sets.
WEIGHTED HISTOGRAMS OF OVERLAPPING
STRIPES (WHOS)
We have designed a discriminative and efﬁcient descriptor of person appearance for re-identiﬁcation based on
coarse, striped pooling of local features. It exploits a
simple yet effective center support kernel to approximately segment foreground from background. The entire
descriptor construction process is illustrated in Fig. 1.
Given an input image of a target, it is scaled to
a canonical size W × H (64 × 128 pixels in all our
experiments) and a spatial pyramid is built by dividing
the person image into overlapping horizontal stripes of
16 pixels in height.
From each stripe we extract Hue-Saturation (HS) and
RGB histograms. Each pixel’s contribution to its corresponding histogram bin is weighted using Epanechnikov
kernel centered on the image:
W )2 + ( y
where W and H are the image width and height, respectively, and are the only parameters of the Epanechnikov
kernel. To the HS and RGB histograms we concatenate
a Histogram of Oriented Gradient (HOG) descriptor
computed on a grid over the image as described in .
The HS histograms contain 8 × 8 bins, while RGB
is quantized to 4 × 4 × 4 bins. Both the HS and RGB
histograms are computed for the 15 levels of the pyramid
(8 stripes for the ﬁrst level plus 7 for the second level of
overlapping stripes). The result is a total of 1, 920 color
histogram bins. The HOG is extracted from a sub-image
obtained by removing 8 pixels from top, bottom, left
and right of the original in order to remove background
details. Each block of the HOG consists of a grid of 2×2
cells of 8×8 pixels. For each cell we compute the gradient
histogram over only 4 angular bins (to capture vertical,
horizontal and diagonal patterns) for each HOG block.
Given the canonical image size used in our experiments,
the dimension of the HOG component is 1, 040 bins, and
the ﬁnal descriptor dimensionality is thus 2, 960. As the
ﬁnal stage of descriptor computation, we take the square
root of all descriptor bins.
Histogram of Oriented
Epanechnikov Mask
Horizontal Stripes
First Level
Horizontal Stripes
Second Level
(Overlapped)
HS + RGB Histogram
of the first level
HS + RGB Histogram
on the second level
HOG Cell Block
sub-division
Fig. 1: Our feature descriptor. (a) An Epanechnikov kernel
weights the contribution of each pixel to HS and RGB histograms computed on overlapping stripes (b) and (c). Overlapping HOG descriptors are concatenated with these (d).
The motivations for a composite descriptor such as
ours are many:
• The striped pooling model grants a degree of pose
invariance in the representation. Horizontal stripes
capture information about vertical color distribution in the image, while overlapping stripes maintain color correlation information between adjacent
stripes in the ﬁnal descriptor.
• Color information is captured by HS and RGB histograms, and local texture by the HOG component.
The use of HS histograms renders a portion of the
descriptor invariant to illumination variations, while
the RGB histograms capture more discriminative
color information, especially for dark and greyish
colors. We equalize RGB color channels before extracting histograms.
• The Epanechnikov kernel approximately segments
the foreground by diminishing the inﬂuence of
background information near the image boundary.
This avoids learning a background model for each
scenario, gaining in simplicity and efﬁciency compared to techniques that use complex background
or part-based models.
• Taking the square root of descriptor bins is a wellknown technique in image classiﬁcation that
helps to reduce the “burstiness” of features by discounting the effect of small changes in bins that
already have signiﬁcant weight. In preliminary experiments we found this to improve robustness of
Euclidean distances between descriptors.
An extensive evaluation of the performance of our descriptor conﬁrming these motivations is given in Sec. 5.2.
Though it requires no complex segmentation or ﬁtting of
body part models, our descriptor in combination with
our sparse framework performs comparably to or better
than the state-of-the-art.
EXPANSIONS
IDENTIFICATION
In this section, we ﬁrst describe basis expansions for
classiﬁcation and show how this basic approach does not
generalize in a straightforward way to problems like reidentiﬁcation due to its inability to rank all but a few
conﬁdently classiﬁed individuals. Hence, we introduce
an iterative algorithm for ranking with sparse basis expansions that addresses these shortcomings and permits
its effective application to re-identiﬁcation.
Sparse basis expansion
The main idea behind the use of basis expansions for
building discriminative classiﬁers is that, given sufﬁcient
samples ti,1, . . . , ti,ni from some class i, a test sample y
of the same class should approximately lie in the linear
span of the training samples:
αi,1ti,1 + αi,2ti,2 + . . . + αi,ni, ti,ni
for some optimal choice of scalar coefﬁcients of reconstruction αi,j, for j = 1, . . . , ni. We use Ti to represent the matrix of basis vectors for class i, and αi =
[αi,1, . . . , αi,ni]T to represent the vector of reconstruction
coefﬁcients for the same class.
The general, multi-class basis expansion for C classes
then becomes:
[T1 T2 · · · TC] [α1 α2 · · · αC]T
The basis T can be highly overcomplete, but if y is an
instance of a person we desire that the energy in the
basis expansion be concentrated in the relatively few
coefﬁcients from the gallery examples corresponding to
the identity of y. We can impose this sparsity constraint
on the solution by formulating it as an ℓ1-regularized
least squares problem:
bα = arg min
α ∥y −Tα∥2
2 + λ∥α∥1,
where λ controls the tradeoff between minimization
of the ℓ2 reconstruction error and the ℓ1 norm of the
coefﬁcients used to reconstruct y. This formulation is
known as Lasso Regression in the statistics literature and
there exist very efﬁcient algorithms for solving it .
Regularized basis expansions of this type are generally
referred to as sparse because the ℓ1 regularization term,
depending on the sparseness factor λ, tends to cause the
coefﬁcients of reconstruction to collapse to zero except
for a few important basis vectors. The form of Eq. (6)
is particularly convenient because it represents a whole
class of solutions to the approximate reconstruction
problem of Eq. (5). When λ = 0, Eq. (6) results in a
standard least squares solution. For λ > 0, we obtain
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Coefficients
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Coefficients
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Coefficients
Fig. 2: Basis expansion for MvsS re-identiﬁcation on ETHZ1.
Top: (left) probe sample, (right) the ﬁrst 15 samples in the
gallery, two instances for each subject (N = 2). Bottom: reconstruction coefﬁcients for least squares (λ = 0), sparse (λ = 0.2)
and nearest neighbour (λ = 0.6). Each color represents a single
subject which has two instances.
solutions of increasing sparseness with increasing λ.
Eventually, as λ →∞, only a single non-zero coefﬁcient
will be admitted in the solution of Eq. (5). We refer to
this last solution, with λ →∞, as the nearest neighbor
solution since only the ℓ2-closest training sample to y
will have a corresponding non-zero coefﬁcient in bα.
In Fig. 2 we illustrate these three types of solutions
for an MvsS re-identiﬁcation problem. The top row of
Fig. 2 illustrates the probe and gallery images for a reidentiﬁcation query. The plot in the second row shows
the coefﬁcients of a least squares solution (λ = 0),
followed by a sparse solution (λ = 0.2), and ﬁnally the
nearest neighbor solution (λ = 0.6 for this example).
We can derive a decision rule for classiﬁcation by
analyzing the reconstruction error for solutions to Eq. (5)
restricted to basis vectors corresponding to individual
gallery subjects. The normalized reconstruction error
corresponding to the i-th subject is:
ei = ∥y −Ti bα|i∥2
, for i ∈{1, . . . , C}.
where bα|i represents the sparse solution of Eq. (6) restricted to the coefﬁcients corresponding to gallery examples of class i. That is, bα|i is equal to bα at coefﬁcients
corresponding to gallery examples from individual i and
zero elsewhere.
Our decision rule is:
class(y) = arg min
This decision rule based on sparse discriminative basis
expansion performs very well for classiﬁcation problems . However, as mentioned in the introduction,
recall can be critical for re-identiﬁcation and it is important to be able to rank gallery individuals. We can extend
the decision rule of Eq. (8) in a straightforward manner
to rank candidate individuals using their corresponding
residual ei.
In Fig. 3(a) we show two views of an MvsS reidentiﬁcation problem in terms of normalized reconstruction error ei with respect to the probe y. In the
middle are illustrated the coefﬁcients bα of a probe
reconstruction in terms of a multi-shot gallery. Below
are illustrated the normalized reconstruction errors ei
corresponding to each gallery individual. Each error on
the bottom thus corresponds to two coefﬁcients in the
middle, since the gallery is multi-shot with N = 2. The
fundamental problem with using discriminative sparse
basis expansions derived from solutions to problems like
Eq. (6) is that, for many reasonable values of λ, we are
deliberately forcing the majority of coefﬁcients to zero,
which limits the number of ranks the basis expansion
can support. In Fig. 3(a) we see that after the ﬁrst
few individuals (ranks), the coefﬁcient energy collapses
and we have no more information upon which to base
ranking decisions. The result is that beyond this point
we cannot rank the remaining gallery individuals.
A more subtle problem is that in many cases we
may be basing ranking decisions on inadequate evidence
from the basis expansion. After the ﬁrst eight individuals
in Fig. 3(a), even before collapsing to zero, there is very
little coefﬁcient energy upon which to base individual
ranking decisions. In the next section we introduce an
iterative sparse basis expansion technique that addresses
these problems of lack of sufﬁcient ranking support in
sparse reconstructions.
Iterative sparse re-weighting
In this section we develop an iterative technique to address the problems with applying sparse discriminative
classiﬁers to ranking. We arrive in the process at an algorithm that is able to robustly perform re-identiﬁcation
up to all ranks. Our approach is an iterative extension
of the weighting described in which we use to ﬁrst
re-weight basis vectors in the sparse solutions of Eq. (6)
and arrive at a more robust solution that does not rely on
basis vectors contributing little to the reconstruction. A
similar weighting approach is then used to proceed with
ranking after damping the inﬂuence of basis vectors that
have already contributed to ranking.
Soft-weighting for ranking robustness: The ﬁrst reﬁnement step we perform is a sort of soft-weighting that is
used to remove those coefﬁcients that weakly contribute
to the reconstruction of the given test sample. Assume
we have computed sparse reconstruction coefﬁcients bα
for a given instance of a re-identiﬁcation problem. At
each iteration we deﬁne for each element in the basis
a weight that is inversely proportional to its coefﬁcient
magnitude in the initial reconstruction:
|bαi,j| + ε
for i ∈{1 . . . C} and j ∈{1 . . . ni},
where ε is chosen to be slightly smaller than the minimum nonzero coefﬁcient of bα to avoid division by zero
and to not inﬂuence the solution with respect to the other
1 2 3 4 5 6 7 8 9 1011 1213 14 1516 1718 19 2021 2223 24 2526 2728 29 30
No evidence at all,
ranking is by chance
Little evidence not
good for ranking
Normalized Error
Little evidence not
good for ranking
No evidence at all,
ranking is by chance
Reconstruction Coefficients
Reconstruction Error
(a) Ranking with limited information
Soft weighting
Hard weighting
Coefficients
(b) Soft- and hard-weighting effects
CMC − iLIDS SvsS
Rank Score
Recognition Rate
Wright’s approach (68.6)
OUR without soft−weighting (85.5)
OUR with soft− and hard−weighting (89.0)
(c) Soft-weighting results
Fig. 3: (a) Ranking with limited information from a single basis expansion (MvsS, N=2). Ranking decisions must be made on the
basis of little information (low coefﬁcient energy) or no information (zero coefﬁcient energy). In the middle are reconstruction
coefﬁcients, at the bottom the corresponding normalized reconstruction errors for each multi-shot probe. (b) Effects of softand hard-weighting. Top: reconstruction coefﬁcients from the ﬁrst solution bαs at iteration s; Middle: reﬁned reconstruction
coefﬁcients after soft-weighting bα′s; and Bottom: coefﬁcients bαs+1 at iteration s + 1 after hard-weighting. (c) The effects of
soft-weighting on performance on the i-LIDS dataset.
coefﬁcients. We then solve a weighted Lasso problem by
weighting the regularization magnitudes using the wi,j
deﬁned above:
bα′ = arg min
α ∥y −Tα∥2
∥diag(wi)αi∥1,
where wi = [wi,1, wi,2, . . . , wi,ni] are the weights from
Eq. (9) corresponding to the basis vectors for individual
i and diag(wi) denotes the diagonal matrix with vector
wi on its diagonal. Just like the unweighted counterpart
in Eq. (6), this convex problem can also be efﬁciently
solved as a linear program. The weights wi,j are free
parameters in the convex relaxation, whose values can
be used to penalize or favor speciﬁc basis vectors in the
regularized expansion. The new solution bα′ is the reﬁned
solution that is used to rank individuals with respect to
Fig. 3(b) graphically illustrates this soft-weighting procedure. The initial solution contains a few dominant
coefﬁcients that contribute most to the reconstruction of
the probe y. It also contains a number of basis vectors
that contribute little to the overall reconstruction, as
indicated by very small coefﬁcients in the initial solution
shown in the top plot of Fig. 3(b). The reﬁned solution
using the weights from Eq. (9) is shown in the middle
plot of Fig. 3(b). Note that this reﬁnement eliminates
small coefﬁcients from the solution and redistributes
their energy among more relevant basis vectors. It is not
equivalent to a simple thresholding of coefﬁcients.
Hard re-weighting for improved recall: A more serious
problem related to using a sparse discriminative classi-
ﬁer for re-identiﬁcation is the lack of sufﬁcient non-zero
support in sparse solutions. It is often the case that only
a few gallery individuals can be ranked by analyzing the
initial, reﬁned sparse solution. To address this problem
a set of hard weights are maintained that are used to
exclude those elements that have already contributed to
ranking an individual against the probe y:
i,j are the coefﬁcients from the soft-weighted
solution bα′. The hard weights vector wh is used in
the next step of an iterative process that repeats the
soft-weighting and ranking procedure. In the bottom
plot of Fig. 3(b) we show the solution to the weighted
Lasso problem using these weights. The difference in the
distribution of coefﬁcients between the hard-weighted
solution and the original solution in the top plot of
Fig. 3(b) is quite noticeable.
In Fig. 3(c) we give a comparison of our approach,
including soft- and hard-weighting to rank the entire
gallery, with our approach without soft-weighting and
the technique of . On the iLIDS dataset, the basic
sparse reconstruction approach of can rank, on
average, only about 12 gallery persons per probe with
a recognition rate of 70%. At rank 20, our approach
without soft-weighting reaches a recognition rate of about
73%, while our ﬁnal approach with soft-weighting gains
another 5% in accuracy and reaches 78% recall in just
the ﬁrst twenty ranks. Note also how the gain due to
soft-weighting improves for higher ranks and how softweighting provides a slightly different recall at ﬁrst rank.
This is because soft-weighting effectively defers the decision to rank an individual with low coefﬁcient energy
to future iterations. Without soft-weighting, persons can
be ranked on the basis of very little evidence.
As we will see in more detail in the experimental
results, the problem of lack of ranking support does
not only have an impact on very low or very high
ranks. Moreover, though low ranks are clearly the most
important, there will always be applications where recall
is also crucial.
Algorithm 1: R(T, Y, λ) : Iterative Sparse Ranking
Input: T = [T1 T2 · · · TC], the gallery templates;
Y = {y1, . . . , ym}, the probe templates; and
λ, the regularization factor.
Output: R, the ranked list of gallery individuals.
1 Initialize hard weights: wh
2 Initialize iteration count: s ←1.
3 Initialize list of gallery individuals: R ←∅.
4 while |R| < C do
Hard-weighting:
bα ←arg min
α,y∈Y ∥y −Tα∥2
2 + λ∥diag(wh)α∥1
Soft-weighting:
|bαi,j|+ε for i ∈{1 . . . C} and j ∈{1 . . . ni}
bα′ ←arg min
α,y∈Y ∥y −Tα∥2
2 + λ∥diag(w)α∥1
if person i has not yet been ranked then
R ←R ∪{(i, s, ei)}
(ei from Eq. 7)
17 R ordered by (i, s, e) ≤(i′, s′, e′) ⇔s < s′ ∨(s =
s′ ∧e ≤e′)
Iterative sparse person re-identiﬁcation
Putting it all together, our approach to person reidentiﬁcation up to arbitrary ranks is an iterative process
of both soft- and hard-weighting of ℓ1 regularized probes
reconstructions using gallery examples. The steps are as
1) Reconstruct probe(s) using Eq. (10) with hard
weights deﬁned as in Eq. (11) to eliminate already
ranked persons (if any).
2) Use soft-weighting from Eq. (9) in a weighted reconstruction using Eq. (10) to eliminate coefﬁcients
contributing little to the reconstruction of the probe
and distribute their energy among more relevant
basis vectors.
3) Rank gallery individuals who have non-zero coefﬁcient energy (i.e. those individuals who have
normalized reconstruction error ei < 1, where ei
is deﬁned as in Eq. (7)).
4) Update hard weights as in Eq. (11) to eliminate
from subsequent iterations those basis vectors contributing to ranking in the current round.
5) Repeat until all gallery individuals are ranked.
Algorithm 1 formalizes each of these steps and how
they ﬁt together to rank all individuals in the gallery.
Note that gallery individuals are not ranked by normalized reconstruction error alone. The normalized reconstruction error is used for ranking within an iteration,
but those individuals ranked in an iteration will always
be ranked higher than those in subsequent ones. Algorithm 1 is designed to work with single- and multi-shot
gallery and probe sets. For this reason, the optimizations
in lines 5 and 7 differ slightly from Eq. (10) in that
they simultaneously optimize over coefﬁcients α and all
probes y ∈Y . In this way, Algorithm 1 can be used for
each of the re-identiﬁcation modalities considered:
re-identiﬁcation For SvsS re-identiﬁcation
problems, the ranked list of triples returned by
Algorithm 1 uniquely ranks each gallery individual.
As soon as a triple (i, s, e) is added to the ranked
list R, hard-weighting of the single template for
individual i in the gallery prevents it from further
consideration. The ordered list R returned by Algorithm 1 thus represents the ordering of the gallery
with respect to the probe y.
• MvsS re-identiﬁcation For multi-shot galleries like
MvsS in which more than one example of each
person is present, hard-weighting does not necessarily eliminate a ranked individual from consideration in subsequent iterations. Due to the sparseness
constraint, not all examples corresponding to one
individual are necessarily used in the regularized
basis expansion. Since hard-weighting by Eq. (11)
only eliminates those basis vectors already used for
ranking, it is possible that some templates corresponding to already ranked individuals remain. The
guard in line 9 of Algorithm 1, however, guarantees
that a person only occurs once in the ordered list
R and thus the rank of each gallery individual is
unambiguously deﬁned in the output.
• MvsM re-identiﬁcation When both the gallery and
the probe sets are multi-shot the minimization over
both α and all probe templates y ∈Y in lines 5 and 7
of Algorithm 1 uniquely deﬁnes the reconstruction
error ei in terms of the minimum error over all
probe templates. Thus, re-identiﬁcation with multishot probes is similar to running multiple MvsS reidentiﬁcations (one for each probe image of each
person) and using the minimum error to represent
the overall reconstruction error for the multi-shot
EXPERIMENTAL RESULTS
In this section we report on an extensive set of experiments performed to compare our method with the
state-of-the-art and to evaluate in detail each component of our approach. All experiments were conducted on standard, publicly available datasets (ETHZ,
VIPeR, i-LIDS and CAVIAR4REID), and we compare our
results with the following state-of-the-art approaches:
SDALF , HPE , AHPE , SCR , ELF ,
CPS , MRCG , ContextB , PRDC , PRSVM
 , SBDR , EIML , COSMATI , RPLM 
and eSDC . Note that not all techniques report results
on all four datasets or on all three modalities (SvsS,
SvsS MvsM MvsM
SvsS MvsM MvsM
SvsS MvsM MvsM
EIML* 
RPLM* 
eSDC* 
TABLE 1: Performance at rank-1 with respect to the stateof-the-art on ETHZ. Techniques indicated by “*” set aside a
portion of the data for metric learning. Recognition rates in %.
MvsS and MvsM). For example, with the exception of
COSMATI, metric learning approaches report results
only for SvsS scenarios. To provide the most comprehensive comparison possible, we test our method on
all modalities and include all reported results from the
above methods, when available.
The principal metric used for evaluating person reidentiﬁcation is the Cumulative Match Characteristic
(CMC) curve which summarizes overall performance
by reporting recall over a range of cutoff points. A
CMC curve represents the expectation of ﬁnding the
correct match in the top r matches, where r is the rank
considered in the ﬁnal ranking result. Unless otherwise
noted, all results were computed by averaging over 50
random, independent splits of dataset into gallery and
probe sets, except for VIPeR where we use the ten splits
from . We also report, and compare with the state-ofthe-art when available, the normalized Area Under the
Curve (nAUC). The nAUC is calculated as the total area
under a CMC divided by 100 × N, where N is the total
number of gallery individuals. It gives an overall score
of how well a method performs over all ranks. For many
applications, the most important cutoff rank is one. We
thus also report a comparison of our rank-1 performance
with respect to the state-of-the-art on all datasets.
We refer to our approach as Iterative Sparse Ranking
(ISR) in all tables and ﬁgures that follow. Unless otherwise qualiﬁed, we use our full descriptor as described in
Sec. 3. In our experiments reported in Sect. 5.3 we found
λ = 0.2 to be a good trade-off between recognition rate
and number of iterations. Accordingly, we ﬁxed λ = 0.2
for all experiments reported here. In practice, the optimal
λ will be descriptor- and dataset-dependent and could
be cross-validated given a labeled validation set.
Comparison with the state-of-the-art
In this section we compare ISR with the state-of-theart on the ETHZ, VIPeR, i-LIDS, and CAVIAR4REID
Performance on the ETH Zurich datasets:
Zurich dataset consists of three sequences used for tracking, from which Schwartz and Davis extracted a set
of samples of each person in the videos. We performed
CAVIAR4REID
SvsS MvsM MvsM
ContextB 
PRSVM* 
PRDC* 
SBDR* 
EIML* 
COSMATI* 
eSCD (ocsvm)* 
RPLM* 
TABLE 2: Performance at rank-1 with respect to the stateof-the-art on VIPeR, iLIDS and CAVIAR4REID. Techniques
indicated with a “*” set aside a portion of the dataset for
learning. Recognition rates in %.
SvsS and MvsM experiments, varying the number of
elements in both the probe and gallery.
In table 1 we report rank-1 results for each sequence
of the ETHZ dataset for the SvsS and MvsM (N
{5, 10}) modalities. ISR outperforms current methods for
MvsM, and performs comparably to others for SvsS.
More extensive results and CMC curves comparing ISR
with the state-of-the-art on ETHZ can be found in the
supplementary material accompanying this article.
Performance on the VIPeR dataset: The VIPeR dataset
consists of 632 people imaged by two non-overlapping
cameras. Image pairs exhibit viewpoint changes of up to
180 degrees and illumination changes that result in large
intra-class variations. The dataset has only two samples
of each person (one from each view), and thus can only
be used for SvsS re-identiﬁcation.
On VIPeR we use the publicly available splits into
gallery and probe sets provided by the authors of
SDALF . Table 2 compares the rank-1 performance of
ISR and the state-of-the-art on VIPeR. From this table
we see that ISR improves by about 6 percentage points
on the state-of-the-art performance on VIPeR, except for
learning-based methods like RPLM and eSDC ,
which perform similarly to us at rank-1.
Fig. 4(a) gives the CMC curves up to rank 50 comparing ISR with the state-of-the-art. We outperform all stateof-the-art techniques not based on metric learning up to
all but the highest ranks. After about rank-5, techniques
that learn on a part of the data like EIML , RPLM ,
and eSDC begin to outperform us. Note that such
techniques are not strictly comparable with ours since
they set aside a portion (up to half) of the dataset on
which to learn metrics. The gallery and probe sets are
drawn from the remaining data and thus the standard
splits cannot be used.
Performance on the i-LIDS dataset: The i-LIDS dataset
contains images from multiple camera views in a busy
airport arrival hall. As shown in Fig. 4(b), ISR outper-
Rank Score
Recognition Rate
CMC − VIPeR SvsS
CPS (93.60)
eSDC (ocsvm)
ISR (94.10)
CMC − iLIDS SvsS
Recognition Rate
Context−based
CPS (87.77)
ISR (89.14)
Rank Score
CMC − iLIDS MvsS
Recognition Rate
Rank Score
CMC − iLIDS MvsM
Recognition Rate
Rank Score
SDALF N=3 (91.55)
SDALF N=2 (91.12)
CPS N=3 (93.52)
COSMATI N=2
ISR N=2 (95.5)
ISR N=3 (95.8)
Fig. 4: Comparative performance evaluation on VIPeR and i-LIDS. (a) SvsS on VIPeR. (b) SvsS on i-LIDS. (c) MvsS on i-LIDS
(N ∈{2, 3}). (d) MvsM on i-LIDS (N ∈{2, 3}). Dashed curves distinguish techniques that set aside a portion of the dataset for
learning. In the legends we report the normalized area under the CMC curve (nAUC), when available.
CMC − CAVIAR4REID SvsS
Recognition Rate
Rank Score
SDALF (68.65)
CPS (72.36)
ISR (79.33)
CMC − CAVIAR4REID MvsM
Recognition Rate
Rank Score
SDALF N=3 (73.81)
SDALF N=5 (76.24)
CPS N=5 (82.99)
CPS N=3 (79.93)
ISR N=3 (97.3)
ISR N=5 (99.5)
Fig. 5: Performance on CAVIAR4REID with respect to the stateof-the-art. (a) SvsS. (b) MvsM for N ∈{3, 5}. In the legends
we report the nAUC, when available.
forms the state-of-the-art at low ranks. After about rank-
4, however, techniques based on metric learning begin
to outperform us. Note that, due to having to use a
portion of available data for learning, the SBDR and
PRSVM methods only consider, respectively, 80 and
108 out of the 119 people in the dataset.
Table 2 summarizes the rank-1 performance of ISR
and the state-of-the-art for SvsS and MvsM on i-LIDS.
From this table we see that we slightly outperform other
approaches on SvsS, while we signiﬁcantly outperform
competing methods by about 17 percentage points for
For MvsS and MvsM, where we are able to exploit
multiple images of each gallery individual, our improvement over the state-of-the-art is dramatic. As seen in
Fig. 4(c) for MvsS (N = 2) we exceed the state-of-theart at rank-1 by nearly 19 percentage points. We similarly
improve for MvsS (N = 3) where we outperform SDALF
by nearly 11 percentage points at rank-1. For MvsM
we report results for N
∈{2, 3} in Fig. 4(d) along
with results of other methods tested on this dataset. We
outperform the state-of-the-art at all ranks for the MvsM
Performance on the CAVIAR4REID dataset:
CAVIAR4REID dataset contains 72 unique individuals
captured in a shopping center scenario. This dataset
was designed to maximize variability with respect to
resolution changes, illumination conditions, occlusions,
and pose changes.
We compare the rank-1 recognition rate of ISR and
the state-of-the-art on CAVIAR4REID in Table 2. We
signiﬁcantly outperform competing methods at rank-1
in all modalities on this dataset. For MvsM we improve
on the state-of-the-art by nearly 62 percentage points for
MvsM (N = 3) and by 72.5 for MvsM (N = 5).
In Fig. 5(a) we report the CMC curves for ISR and
the state-of-the-art for SvsS on CAVIAR4REID. Our approach outperforms current methods up to about rank-
20. The improvement over the state-of-the-art at ﬁrst
rank is particularly noticeable: there is a difference of
20.5 percentage points at rank-1 between our performance and competing methods. In the legend we also
report the nAUC for each method, which gives an idea
of the trend of the curve across all ranks. Fig. 5(b) gives
CMC curves for MvsM on CAVIAR4REID for N ∈{3, 5}.
In the MvsM modality, as for i-LIDS, we signiﬁcantly
outperform the state-of-the-art at all ranks.
Evaluation of our person descriptor
In this section, we ﬁrst give a comparison between our
descriptor detailed in Sec. 3 and some of the most used
descriptors from the re-identiﬁcation literature. We also
quantify the contribution of our background separation
and pooling models with respect to other models used in
the literature. Then we analyze the contribution of each
component of our descriptor to the overall performance
of the method. Finally we provide an extensive evaluation of the sensitivity of ISR to illumination variations,
viewpoint changes and misalignments of the person
Comparison of person descriptors:
We compare our
descriptor with the SDALF, PS and PRDC descriptors.
Direct, side-by-side comparison is difﬁcult because for
CMC − iLIDS N=1
Rank Score
Recognition Rate
ISR (Full descriptor) MvsM N=2 (95.3)
ISR (HS component) MvsM N=2 (91)
ISR (PS−HSV component) MvsM N=2 (87.7)
ISR (SDALF−HSV component) MvsM N=2 (88.6)
ISR (PRDC descriptor) MvsM N=2 (86.2)
ISR (Full descriptor) SvsS (89.1)
ISR (HS component) SvsS (81)
ISR (PS−HSV component) SvsS (79.5)
ISR (SDALF−HSV component) SvsS (81.1)
ISR (PRDC descriptor) SvsS (77.9)
CMC − VIPeR − BG Model
Rank Score
Recognition Rate
ISR (Full descriptor) (94.10)
ISR (Gaussian) [σx = 16, σy = 32] (93.3)
ISR (Gaussian) [σx = 32, σy = 64] (92.42)
ISR (Gaussian) [σx = 32, σy = 64] (91.9)
ISR (STEL with Gaussian weighting) (93.9)
ISR (STEL without Gaussian weighting) (84.3)
CMC − VIPeR − Pooling Model
Rank Score
Recognition Rate
ISR (15 overlapping stripes) (94.1)
ISR (8 non−overlapping stripes) (93.4)
ISR (SDALF + Gaussian weighting) (91.1)
ISR (PS) (91.2)
CMC − VIPeR
Rank Score
Recognition Rate
ISR (HS+RGB+HOG+Kernel+Sqrt) (94.10)
ISR (HS+RGB+HOG+Kernel) (90.73)
ISR (HS+RGB+HOG) (87.74)
ISR (HS+RGB) (84.18)
ISR (HOG) (65.28)
ISR (RGB) (71.39)
ISR (HS) (80.40)
Fig. 6: Comparison with state-of-the-art descriptors and analysis of the contribution of each descriptor component. (a) Stateof-the-art descriptors in our ISR framework on i-LIDS. Solid lines represent MvsM (N=2) and dashed lines represent SvsS. (b)
ISR with different background models on VIPeR. (c) ISR with different pooling models on VIPeR. (d) The contribution of each
descriptor component on the VIPeR dataset.
some methods there is no publicly available code. Also,
SDALF and PS use HSV histograms with an MSCR
descriptor of variable-length, and is thus not suitable for
reconstruction by basis expansion. In order to embed
these descriptors into our framework, and considering
that MSCR contributes little to re-identiﬁcation with
respect to the HSV histogram , we used only the
HSV component computed on the symmetry parts for
SDALF, and the HSV component computed on the person parts for PS. For fairness of comparison, we report
performance of our full descriptor and the HS histogram
component only. In Fig. 6(a) we report performance on
i-LIDS for both SvsS and MvsM (N = 2), averaged
over ten trials. Our descriptor outperforms the others,
although it uses neither part-based modeling nor datadriven foreground segmentation.
Comparison of background separation models:
compare the contribution of the Epanechnikov kernel
with respect to a Gaussian center-support background
model with varying diagonal covariance and the STEL
component analysis background model, with and without Gaussian weighting . The performance ﬁgures
were obtained with our ISR re-identiﬁcation approach
and the full descriptor with the corresponding background model.
In Fig. 6(b) we report performance on VIPeR, averaged
over ten random trials. We see that the Epanechnikov
kernel consistently outperforms Gaussian weighting,
likely due to the difﬁculty of tuning σx and σy to balance
the ﬂatness (to include the subject) and peakedness
(to exclude background). The STEL component analysis
model performs similarly at high ranks. However, STEL
relies on a previously learned person model and requires
inference at re-identiﬁcation time to segment the person
from the background.
Comparison of pooling models: We compare our overlapping striped pooling model with a non-overlapping
version of the same striped model, the symmetry-driven
SDALF model , and the part-based Pictorial Structures
Illumination variation
Viewpoint change (degrees)
Fig. 7: Sensitivity to (a) illumination and (b) viewpoint changes.
model . In all cases, we pool the local HS and RGB
histograms for each region and then concatenate the
HOG of the full image. We report performance on VIPeR
averaged over ten trials in Fig. 6(c). We see that our
striped pooling strategy, together with the Epanechnikov
kernel as background model, outperforms the more complex part-based approaches.
Contribution of each descriptor component: In Fig. 6(d)
we show the contribution of each component of our
descriptor to overall performance: the HS histogram,
the RGB histogram, the HOG, the Epanechnikov kernel and the application of square root to each bin of
the descriptor. The experiments were performed on the
VIPeR dataset, and results were averaged over ten trials.
The plots show that the addition of each component
improves performance.
Sensitivity to illumination changes:
We performed a
series of experiments on VIPeR to quantify the sensitivity
of our descriptor to illumination changes. For each pair
of images in the VIPeR dataset we estimate the difference
in illumination by applying a Gaussian smoothing kernel
to the value channel in the HSV color space for each
image, weighting the ﬁltered intensity images with the
Epanechnikov kernel, and then computing the difference
ETHZ SvsS Rank−1 recognition rate in function of misalignment error
% of misalignment error
Rank−1 Recognition Rate
ISR (Full descriptor)
ISR (HS+RGB)
ISR with PS (Full descriptor)
ISR with PS (HS+RGB)
Fig. 8: Rank-1 accuracy of ISR with our descriptor and PS over
a range of misalignment error. Images are random samples of
misaligned imagery.
in average intensity between the two images. The full set
of intensity variations between corresponding images in
the dataset was quantized in 16 bins. We then performed
leave-one-out cross validation to estimate sensitivity to
illumination changes: each image was used as a probe
and compared against all other images. Results are
shown in Fig. 7(a). The rank at which the correct gallery
image was returned is recorded as a function of illumination change. All possible ranks were also quantized into
16 bins and represented with different colors from dark
blue (bin 1) to red (bin 16). Looking at the ﬁrst bin of
illumination variations, where all the image pairs have
almost equal illumination, we see that the correct image
always appears in the ﬁrst ranking bin. On average, reidentiﬁcation in the ﬁrst ranking bin is unaffected by
changes in illumination in about 40% of cases.
Sensitivity to viewpoint changes:
We also performed
experiments to evaluate the sensitivity to viewpoint
changes, exploiting the fact that VIPeR contains ground
truth viewpoint annotations for all subjects (it is in
fact the only publicly available dataset with such annotations). Four different changes of viewpoints were
considered: 45, 90, 135, and 180 degrees. Also in this
case, the experiments were performed using leave-oneout cross validation. All possible ranks were quantized
into 16 bins and represented with different colors. From
Fig. 7(b), we see that the our full descriptor is robust
to viewpoint variations and that re-identiﬁcation in the
ﬁrst ranking bin is approximately pose invariant in 45%
of the cases.
Sensitivity to misaligned detection windows: Finally,
we analyzed the sensitivity of the full descriptor to conditions where the detection window is not well-centered
on the person. To this end we used the ETHZ datasets
because they also provide the original video frames from
which each person image was extracted. This allowed
us to artiﬁcially generate detection misalignments by
shifting the location of the person bounding box in the
original frame. Fig. 8 shows the SvsS recognition rate at
rank-1 for different degrees of misalignment applied to
the probe images. Results were averaged over ten trials,
over all person images in all three ETHZ datasets. In the
same ﬁgure we also report the performance of ISR with
the PS pooling model in the presence of misalignment.
We see that the ISR method with our descriptor (blue
curves) outperforms ISR with PS pooling (red curves)
for misalignments up to 17%. For higher misalignments
of the detection window, the adaptive PS pooling model
performs better and the additional complexity of ﬁtting
body-part models may be warranted. Note that the HOG
component (solid lines) does not negatively affect the
overall ISR performance at any degree of misalignment.
In contrast, the HOG component appears to improve
performance of ISR with PS pooling at small misalignments and affect it negatively at higher ones. From the
sample thumbnails in Fig. 8, it is anyway clear that
for misalignments beyond 30% the person image has
insufﬁcient visual content for accurate re-identiﬁcation.
Evaluation of Iterative Sparse Reconstruction
In this section we investigate how ℓ1-regularized sparse
basis expansion aids in re-identiﬁcation in comparison
to nearest-neighbor, least-squares, and nearest-subspace
classiﬁcation. We also demonstrate how iteration contributes to improve recall, and thus to high recognition
rates, across all ranks.
Contribution of sparse reconstruction: In Fig. 9(a) we
show the average rank-1 re-identiﬁcation accuracy for
a range of solutions to the regularized least squares
problem of Eq. (6) over 10 trials on the VIPeR and i-LIDS
datasets. Shown are the least squares (λ = 0) solution,
sparse solutions for a range of λ > 0, and the nearest
neighbor solution when λ is sufﬁciently high to constrain
the solution to a single non-zero coefﬁcient. The sparse
approach, for appropriate λ, outperforms the nearest
neighbor and least squares solutions on both datasets.
In Fig. 9(b) we give a comparison of ISR with
Nearest Subspace Classiﬁer (NSC) for MvsM on
CAVIAR4REID. We chose NSC as a baseline since it is
representative of linear, non-sparse approaches to recognition. A drawback of NSC is that it cannot effectively
learn a subspace if the number of instances per person
is low, while ISR is robust with as few as two or three
gallery examples per person. This is especially evident
in Fig. 9(b) for N = 3 gallery images per person. We
used the CAVIAR4REID dataset for these experiments
because of the need for more than four images per
person to learn subspaces.
Contribution of iteration to recall:
Iteration of our
sparse ranking algorithm is effective not only for high
ranks, but at middle and low ranks as well. Fig. 10(a)
shows the average cutoff ranks for each iteration on the
i-LIDS dataset and recognition rate (recall) across ranks.
Least Squares - Sparse - Nearest Neighbor
Lambda (sparseness)
Rank-1 Recognition Rate
Sparse−based Classifier (iLIDS SvsS)
Nearest Neighbour (iLIDS SvsS)
Least Squares (iLIDS SvsS)
Nearest Neighbour (VIPeR SvsS)
Least Squares (VIPeR SvsS)
Sparse−based Classifier (VIPeR SvsS)
Rank Score
Recognition Rate
CMC − CAVIAR4REID MvsM
NSC N=3, k=2
NSC N=5, k=4
Fig. 9: (a) Rank-1 accuracy on VIPeR and i-LIDS for SvsS.
Accuracy is plotted for varying sparseness (λ), including least
squares (λ = 0) and the nearest neighbor (λ ≈0.6) solutions.
(b) Comparison of ISR with the Nearest Subspace Classiﬁer
on CAVIAR4REID. In the legend we report the number of
instances per person (N) and number of learned subspaces (k).
10 20 30 40 50 60 70 80 90 100 110 120
Average Recall with Varying Iteration
Recognition Rate
Rank Score
ISR (Full Iteration)
3rd Iteration
2nd Iteration
1st Iteration
Rank Score
Ranking Expectation Rate
RER and Recognition by Rank
1st Iteration
2nd Iteration
3rd Iteration
4th Iteration
5th Iteration
6th Iteration
7th Iteration
8th Iteration
Full Iteration
Recognition Rate
Fig. 10: Iterative ranking and its effect on recall. (a) Average
recognition rates with cutoffs for each iteration. (b) RER and
recognition rate as function of rank.
These experiments were performed for MvsS (N = 2)
and averaged over 50 random splits. We see that with
a single iteration we can rank seven gallery persons, on
average, and achieve an average recognition rate of less
than 70%. After this point, the red curve (corresponding
to the ﬁrst iteration) levels off since no more gallery persons can be ranked, on average. Note how the ﬁrst three
iterations yield a steep increase in average recognition
rate at low and middle ranks, leveling off on average
at rank 14 and 24, respectively. The remaining iterations
contribute more slowly to recall at higher ranks.
Fig. 10(b) shines more light on the contribution of each
iteration to recall at all ranks. This plot makes use of a
metric we introduce to quantify the expected number of
ranks Algorithm 1 returns in each iteration, on average.
The Ranking Expectation Rate RER(r, s) is the expectation
of ranking at least r elements in s iterations or less. Using
the notation of Algorithm 1 we deﬁne the RER for a
single-shot probe set Y = {y1, . . . , ym} as:
RER(r, s) = 1
: |Rs(T, {y}, λ)| ≥r
where Rs(T, {y}, λ) denotes the restriction of the ranking of Algorithm 1 to iteration s or before:
Rs(T, {y}, λ) = {(i′, s′, e′) ∈R(T, {y}, λ) : s′ ≤s} . (13)
Fig. 10(b) illustrates the RER as a function of rank for
varying numbers of iterations in Algorithm 1. Superimposed on this plot is also the corresponding recognition
rate at each cutoff rank. Note that iteration contributes
to increased RER, and consequently recognition rate, not
only at high ranks but at all ranks. For many probes only
a few gallery persons are ranked by a single iteration,
and thus iteration contributes signiﬁcantly to increasing
recall also at low ranks. We plot recall until rank 73 in
this plot, at which point it saturates after eight iterations.
Discussion
In this section we summarize our contribution terms of
performance with respect to the state-of-the-art and in
terms of computational efﬁciency.
General performance considerations:
The trend that
emerges from the experimental evaluation is that, with
the exception of some learning-based approaches discussed below, ISR exceeds the state-of-the-art at rank-
1. This can be seen in the SvsS modality on all datasets,
but the increase in performance is dramatic on MvsS and
MvsM on i-LIDS, ETHZ and CAVIAR4REID. On these
multi-shot datasets ISR exceeds the state-of-the-art at all
Ranking based on sparse, ℓ1-regularized basis expansions allows ISR to exploit multiple aspects of person
appearance in the multi-image galleries of the MvsS and
MvsM modalities. This is noticeable from the trend of
the curve on the i-LIDS dataset in Fig. 4(d) where we
quickly reach a 90% recognition rate at around rank-15
and on CAVIAR4REID in Fig. 5(b) where we reach 100%
accuracy around rank-20. In a simple experiment on i-
LIDS (MvsM with N = 3) to explore the effect of number
of gallery exemplars per person, we found that reducing
each gallery set to two exemplars had almost no effect
on recognition rate at all ranks, while reducing to only
one noticeably reduced performance. With one exemplar
per gallery person (and a multi-shot probe) performance
was still better than for SvsS, but the trend seems to be
that reducing the number of exemplars in the gallery
converges towards SvsS performance.
Comparison with metric learning: Some metric learning approaches outperform the ISR method at higher
ranks for SvsS on VIPeR and i-LIDS. By setting aside a
portion of the labeled data they are able to learn a metric
that better captures the intrinsic properties of the scene,
of the cameras used, and of the camera positioning and
imaging conditions. This increase in performance at high
rank comes at a cost. On VIPeR, for example, as much
as half of all available labeled data is used for metric
learning and this limits the availability of data for actual
testing. Not only does this render experimental results
Timings in function of iterations
Iterations
(a) Time as a function of iterations.
Timings in function of nAUC
(b) Time as a function of desired nAUC.
Timings in function of the gallery size
Gallery Size
(c) Time as a function of gallery size.
Fig. 11: Time required for re-identiﬁcation. (a) Time as a function of the number of iterations of sparse re-weighted ranking on
the i-LIDS dataset. (b) Time for a desired nAUC on the i-LIDS dataset. (c) Time as a function of the total number of gallery
images on the ETHZ1 dataset for the MvsS N = 10 modality. Timings are averaged over all probes in ﬁfty random splits.
not strictly comparable, it is also a severe limitation in
real application scenarios where no labeled data may be
available a priori. An important advantage of ISR with
respect to learning-based ones is that learned distance
metrics cannot be easily updated when camera settings
or positions change, while we can easily integrate new
instances per person and discard old gallery images.
Computational efﬁciency:
Our approach is implemented in MATLAB using the optimized SPAMS library
for sparse modeling . All tests were performed on
an Intel (8-core) with 12 GB RAM.1
Descriptor extraction in MATLAB requires about 0.016s
per person image and is included in all timing numbers
reported here.
In Fig. 11 we report three views of the computational
requirements of ISR. In Fig. 11(a) we vary the number
of iterations of sparse re-weighted ranking we perform
in order to quantify how computational requirements
change with increasing iterations (and increasing accuracy). Fig. 11(b), on the other hand, quantiﬁes the
relationship between the time required for performing
a single re-identiﬁcation and the area under the curve.
From these curves we see that, if we are interested only
in ﬁrst rank, we can perform re-identiﬁcation of a single
probe in about 0.036s. In real application scenarios ISR
can thus perform rank-1 SvsS person re-identiﬁcation at
about 30 re-identiﬁcations per second.
If we are interested in higher ranks, for example in an
interactive application in which a human operator will
sift through re-identiﬁcation results, ISR might require
more than one iteration. From Fig. 11(b) we see that after
7 iterations we arrive at a nAUC of about 88%, requiring
0.08s to compute this result (which works out to about 12
re-identiﬁcations per second). In the MvsM modality ISR
requires about 0.14s (7 re-identiﬁcations per second), but
yields a nAUC of more than 94%. These ﬁrst two tests
were carried out on the i-LIDS dataset.
In Fig. 11(c) we show how ISR scales as a function of
the gallery size. The time for a single re-identiﬁcation
increases approximately linearly when increasing the
 
source-code/re-id/
number of images in the basis up to 600 images; then
the trend becomes superlinear from 600 to 900. It is interesting that this non-linearity is more pronounced with
increasing number of iterations. This test was carried out
on the ETHZ1 dataset which contains the most images,
and all measurements obtained by averaging over 50
random splits of gallery/probe image sets.
CONCLUSIONS
In this paper we described an approach to person reidentiﬁcation that is based on sparse, ℓ1-regularized
basis expansions of probes in terms of a set of gallery
examples used as basis vectors. We showed how to
extend, through iteration and re-weighting, the concept
of a Sparse Discriminative Classiﬁer to problems requiring ranked output. Our algorithm is efﬁcient and
obtains state-of-the-art performance on both multi- and
single-shot person re-identiﬁcation modalities. Our results demonstrate how sparse reconstruction generally
leads to higher performance at ﬁrst rank, while also
yielding higher nAUC using the proposed iterative ranking. It is feature agnostic and it can be applied to any
feature that is encoded as a ﬁxed-length vector. ISR is
also competitive with respect to metric learning-based
methods which set aside data for training.
Our approach makes use of a simple, yet discriminative descriptor of person appearance. It requires no
foreground/background separation or body part segmentation. It is simple and extremely efﬁcient to calculate, and the performance of our approach demonstrates
that simple descriptors can be successfully applied in reidentiﬁcation scenarios.
Iterative sparse ranking is a general approach and
can be applied to retrieval problems beyond person reidentiﬁcation. Our use of ℓ1-regularized basis expansions
for ranking shares some similarities with iterative algorithms such as LARS used to solve weighted Lasso
problems like Eq. (10). An interesting line of research
would investigate the possibility of directly incorporating soft- and hard-weighting of coefﬁcients into a single
regularization path capable of robustly ranking many
candidates in a single iterative pass over basis vectors.
ACKNOWLEDGMENTS
This work was partially supported by Thales Italia. G.
Lisanti acknowledges the support of the AQUIS-CH
Fellowship ,
and A. D. Bagdanov the support of a Ramon y Cajal
Fellowship .