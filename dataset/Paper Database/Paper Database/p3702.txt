Accurate Intelligible Models with Pairwise Interactions
Dept. of Computer Science
Cornell University
 
Rich Caruana
Microsoft Research
Microsoft Corporation
 
Johannes Gehrke
Dept. of Computer Science
Cornell University
 
Giles Hooker
Dept. of Statistical Science
Cornell University
 
Standard generalized additive models (GAMs) usually model
the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs
can be interpreted by users, their accuracy is signiﬁcantly
less than more complex models that permit interactions.
In this paper, we suggest adding selected terms of interacting pairs of features to standard GAMs. The resulting
models, which we call GA2M-models, for Generalized Additive Models plus Interactions, consist of univariate terms and
a small number of pairwise interaction terms. Since these
models only include one- and two-dimensional components,
the components of GA2M-models can be visualized and interpreted by users. To explore the huge (quadratic) number
of pairs of features, we develop a novel, computationally ef-
ﬁcient method called FAST for ranking all possible pairs of
features as candidates for inclusion into the model.
In a large-scale empirical study, we show the eﬀectiveness
of FAST in ranking candidate pairs of features. In addition,
we show the surprising result that GA2M-models have almost the same performance as the best full-complexity models on a number of real datasets. Thus this paper postulates
that for many problems, GA2M-models can yield models
that are both intelligible and accurate.
Categories and Subject Descriptors
I.2.6 [Computing Methodologies]: Learning—Induction
classiﬁcation, regression, interaction detection
INTRODUCTION
Many machine learning techniques such as boosted or
bagged trees, SVMs with RBF kernels, or deep neural nets
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
KDD’13, August 11–14, 2013, Chicago, Illinois, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2174-7/13/08 ...$15.00.
are powerful classiﬁcation and regression models for highdimensional prediction problems.
However, due to their
complexity, the resulting models are hard to interpret for
the user. But in many applications, intelligibility is as important as accuracy , and thus building models that users
can understand is a crucial requirement.
Generalized additive models (GAMs) are the gold standard for intelligibility when only univariate terms are considered . Standard GAMs have the form
where g is the link function. Standard GAMs are easy to
interpret since users can visualize the relationship between
the univariate terms of the GAM and the dependent variable through a plot fi(xi) vs. xi. However there is unfortunately a signiﬁcant gap between the performance of the
best standard GAMs and full complexity models
particular, Equation 1 does not model any interactions between features, and it is this limitation that lies at the core
of the lack of accuracy of standard GAMs as compared to
full complexity models.
Example 1. Consider the function F(x) = log(x2
F has a pairwise interaction (x2, x3), but no interactions between (x1, x2) or (x1, x3), since log(x2
2 log(x1) + log(x3), which is additive.
Our ﬁrst contribution in this paper is to build models
that are more powerful than GAMs, but are still intelligible.
We observe that two-dimensional interactions can still be
rendered as heatmaps of fij(xi, xj) on the two-dimensional
xi, xj-plane, and thus a model that includes only one- and
two-dimensional components is still intelligible. Therefore
in this paper, we propose building models of the form
fij(xi, xj);
we call the resulting model class Generalized Additive Models
plus Interactions, or short GA2Ms.
The main challenge in building GA2Ms is the large number of pairs of features to consider. We thus only want to
include “true” interactions that pass some statistical test.
To this end, we focus on problems with up to thousands of
features since for truly high dimensional problems (e.g., millions of features), it is almost intractable to test all possible
pairwise interactions (e.g., trillions of feature pairs).
Existing approaches for detecting statistical interactions
can be divided into two classes. One class of methods directly models and compares the interaction eﬀects and additive eﬀects . One drawback of these methods is that spurious interactions may be reported over lowdensity regions . The second class of methods measures
the performance drop in the model if certain interaction is
not included; they compare the performance between restricted and unrestricted models, where restricted models
are not allowed to model an interaction in question .
Although this class of methods does not suﬀer from the
problem of low-density regions, they are computationally
extremely expensive even for pairwise interaction detection.
Our second contribution in this paper is to scale the construction of GA2Ms by proposing a novel, extremely eﬃcient
method called FAST to measure and rank the strength of the
interaction of all pairs of variables. Our experiments show
that FAST can eﬃciently rank all pairwise interactions close
to a ground truth ranking.
Our third contribution is an extensive empirical evaluation of GA2M-models. Surprisingly, on many of the datasets
included in our study, the performance of GA2M-models is
close and sometimes better than the performance of fullcomplexity models. These results indicate that GA2M-models
not only make a signiﬁcant step in improving accuracy over
standard GAMs, but in some cases they actually come all the
way to the performance of full-complexity models. The performance may be due to the diﬃculty of estimating intrinsically high dimensional functions from limited data, suggesting that the bias associated with the GA2M structure is
outweighed by a drop in variance. We also demonstrate that
the resulting models are intelligible through a case study.
In this paper we make the following contributions:
• We introduce the model class GA2M.
• We introduce our new method FAST for eﬃcient interaction detection. (Section 4)
• We show through an extensive experimental evaluation that (1) GA2Ms have accuracy comparable to fullcomplexity models; (2) FAST accurately ranks interactions as compared to a gold standard; and (3) FAST
is computationally eﬃcient. (Section 5)
We start with a problem deﬁnition and a survey of related
work in Sections 2 and 3.
PROBLEM DEFINITION
Let D = {(xi, yi)}N
denote a dataset of size N, where
xi = (xi1, ..., xin) is a feature vector with n features and yi
is the response. Let x = (x1, ..., xn) denote the variables or
features in the dataset. For u ⊆{1, ..., n}, we denote by xu
the subset of variables whose indices are in u. Similarly x−u
will indicate the variables with indices not in u. To simplify
notation, we denote U1 = {{i}|1 ≤i ≤n}, U2 = {{i, j}|1 ≤
i < j ≤n}, and U = U1 ∪U2, i.e., U contains all indices for
all features and pairs of features.
For any u ∈U, let Hu denote the Hilbert space of Lebesgue
measurable functions fu(xu), such that E[fu] = 0 and E[f 2
< ∞, equipped with the inner product ⟨fu, f ′
u⟩= E[fuf ′
Let H1 = P
u∈U1 Hu denote the Hilbert space of functions that have additive form F(x) = P
u∈U1 fu(xu) on
univariate compnents; we call those components shape functions . Similarly let H = P
u∈U Hu denote the Hilbert
space of functions of x = (x1, ..., xn) that have additive form
u∈U fu(xu) on both one- and two-dimensional
shape functions.
Models described by sums of low-order
components are called generalized additive models (GAMs),
and in the remainder of the paper, we use GAMs to denote
models that only consist of univariate terms.
We want to ﬁnd the best model F ∈H that minimizes
the following objective function:
F ∈H E[L(y, F(x))],
where L(·, ·) is a non-negative convex loss function. When L
is the squared loss, our problem becomes a regression problem, and if L is logistic loss function, we are dealing with a
classiﬁcation problem.
EXISTING APPROACHES
Fitting Generalized Additive Models
Terms in GAMs can be represented by a variety of functions, including splines , regression trees, or tree ensembles . There are two popular methods of ﬁtting GAMs:
Backﬁtting and gradient boosting . When the shape
function is spline, ﬁtting GAMs reduces to ﬁtting generalized linear models with diﬀerent bases, which can be solved
by least squares or iteratively reweighted least squares .
Spline-based methods become ineﬃcient when modeling
higher order interactions because the number of parameters to estimate grows exponentially; tree-based methods
are more suitable in this case.
Standard additive modeling only involves modeling individual features (also called
feature shaping).
Previous research showed that gradient
boosting with ensembles of shallow regression trees is the
most accurate method among a number of alternatives .
Interaction Detection
In this section, we brieﬂy review existing approaches to
interaction detection.
ANOVA. An additive model is ﬁt with all pairwise interaction terms and the signiﬁcance of interaction terms is
measured through an analysis of variance (ANOVA) test .
The corresponding p-value for each pair can then be computed; however, this requires the computation of the full
model, which is prohibitively expensive.
Partial Dependence Function. Friedman and Popescu
proposed the following statistic to measure the strength of
pairwise interactions,
k=1[ ˆFij(xki, xkj) −ˆFi(xki) −ˆFj(xkj))]2
ij(xki, xkj)
where ˆFu(xu) = Ex−u[F(xu, x−u)] is the partial dependence
function (PDF) and F is a complex multi-dimensional
function learned on the dataset. Computing ˆFu(xu) on the
whole dataset is expensive, thus one often speciﬁes a subset
of size m on which to compute ˆFu(xu). The complexity is
then O(m2). However, since partial dependence functions
are computed based on uniform sampling, they may detect
spurious interactions over low-density regions .
GUIDE. GUIDE tests pairwise interactions based on the
χ2 test . An additive model F is ﬁt in H1 and residuals
are obtained.
To detect interactions for (xi, xj), GUIDE
divides the (xi, xj)-space into four quadrants by splitting the
range of each variable into two halves at the sample median.
Then GUIDE constructs a 2×4 contingency table using the
residual signs as rows and the quadrants as columns. The
cell values in the table are the number of “+”s and “-”s in
each quadrant. These counts permit the computation of a
p-value to measure the interaction strength of a pair. While
this might be more robust to outliers, in practice it is less
powerful than the method we propose.
Grove. Sorokina et al. proposed a grove-based method to
detect statistical interactions . To measure the strength
of a pair (xi, xj), they build both the restricted model Rij(x)
and unrestricted model F(x), where Rij(x) is prevented
from modeling an interaction (xi, xj):
Rij(x) =f\i(x1, ..., xi−1, xi+1, ..., xn)
+ f\j(x1, ..., xj−1, xj+1, ..., xn).
To correctly estimate interaction strength, such method requires a model to be highly predictive when certain interaction is not allowed to appear, and therefore many learning
algorithms are not applicable (e.g., bagged decision trees).
To this end, they choose to use Additive Groves .
They measure the performance as standardized root mean
squared error (RMSE) and quantify the interaction strength
Iij by the diﬀerence between Rij(x) and F(x),
stRMSE(F(x)) = RMSE(F(x))
StD(F ∗(x))
Iij = stRMSE(Rij(x)) −stRMSE(F(x))
where Std(F ∗(x)) is calculated as standard deviation of the
response values in the training set. The ranking of all pairs
can be generated based on the strength Iij.
To handle correlations among features, they use a variant of backward elimination to do feature selection.
Although Grove is accurate in practice, building restricted
and unrestricted models are computationally expensive and
therefore this method is almost infeasible for large high dimensional datasets.
OUR APPROACH
For simplicity and without loss of generality, we focus
in this exposition on regression problems. Since there are
O(n2) pairwise interactions, it is very hard to detect pairwise interactions when n is large. Therefore we propose a
framework using greedy forward stagewise selection strategy
to build the most accurate model in H.
Algorithm 1 summarizes our approach called GA2M. We
maintain two sets S and Z, where S contains the selected
pairs so far and Z is the set of the remaining pairs (Line 1-
2). We start with the best additive model F so far in Hilbert
space H1 +P
u∈S Hu (Line 4) and detect interactions on the
residual R (Line 5). Then for each pair in Z, we build an
interaction model on the residual R (Line 6-7). We select
the best interaction pair and include it in S (Line 9-10). We
then repeat this process until there is no gain in accuracy.
Note that Algorithm 1 will ﬁnd an overcomplete set S by
the greedy nature of the forward selection strategy. When
features are correlated, it is also possible that the algorithm
includes false pairs. For example, consider the function in
Example 1. If x1 is highly correlated with x3, then (x1, x2)
may look like an interaction pair, and it may be included in
S before we select (x2, x3). But since we will reﬁt the model
every time we include a new pair, it is expected that F will
Algorithm 1 GA2M Framework
3: while not converge do
F ←arg minF ∈H1+P
2E[(y −F(x))2]
R ←y −F(x)
for all u ∈Z do
Fu ←E[R|xu]
u∗←arg minu∈Z 1
2E[(R −Fu(xu))2]
S ←S ∪{u∗}
Z ←Z −{u∗}
Illustration for searching cuts on input
space of xi and xj. On the left we show a heat map
on the target for diﬀerent values of xi and xj. ci and
cj are cuts for xi and xj, respectively. On the right
we show an extremely simple predictor of modeling
pairwise interaction.
perfectly model (x2, x3) and therefore (x1, x2) will become
a less important term in F.
For large high-dimensional datasets, however, Algorithm
1 is very expensive for two reasons. First, ﬁtting interaction
models for O(n2) pairs in Z can be very expensive if the
model is non-trivial. Second, every time we add a pair, we
need to reﬁt the whole model, which is also very expensive
for large datasets. As we will see in Section 4.1 and Section 4.2, we will relax some of the constraints in Algorithm 1
to achieve better scalability while still staying accurate.
Fast Interaction Detection
Consider the conceptual additive model in Equation 2,
given a pair of variables (xi, xj) we wish to measure how
much beneﬁt we can get if we model fij(xi, xj) instead of
fi(xi) + fj(xj). Since we start with shaping individual features and always detect interactions on the residual, fi(xi)+
fj(xj) are presumably modeled and therefore we only need
to look at the residual sum of squares (RSS) for the interaction model fij. The intuition is that when (xi, xj) is a
strong interaction, modeling fij can signiﬁcantly reduce the
RSS. However, we do not wish to fully build fij since this
is a very expensive operation; instead we are looking for a
cheap substitute.
Our idea is to build an extremely simple model for fij
using cuts on the input space of xi and xj, as illustrated
in Figure 1. The simplest model we can build is to place
one cut on each variable, i.e., we place one ci and one cut
a = pre-computed
Figure 2: Illustration for computing sum of targets
for each quadrant. Given that the value of red quadrant is known, we can easily recover values in other
quadrant using marginal cumulative histograms.
cj on xi and xj, respectively.
Those cuts are parallel to
The interaction predictor Tij is constructed by
taking the mean of all points in each quadrant. We search
for all possible (ci, cj) and pick the best Tij with the lowest
RSS, which is assigned as weight for (xi, xj) to measure the
strength of interaction.
Constructing Predictors
Na¨ıve implementation of FAST is straightforward, but
careless implementation has very high complexity since we
need to repeatedly build a lot of Tij for diﬀerent cuts. The
key insight for faster version of FAST is that we do not
need to scan through the dataset each time to compute Tij
and compute its RSS.
We show that by using very simple bookkeeping data structures, we can greatly reduce the
complexity.
Let dom(xi) = {v1
i , ..., vdi
i } be a sorted set of possible
values for variable xi, where di = |dom(xi)|. Deﬁne Ht
as the sum of targets when xi = v, and deﬁne Hw
the sum of weights (or counts) when xi = v. Intuitively,
these are the standard histograms when constructing regression trees.
Similarly, we deﬁne CHt
i (v) and CHw
as the cumulative histogram for sum of targets and sum
of weights, respectively, i.e., CHt
Accordingly, deﬁne CHt
i (u) = CHt
i (v) and deﬁne CHw
i (u) = CHw
i (v). Furthermore, deﬁne
ij(u, v) and Hw
ij(u, v) as the sum of targets and the sum
of weights, respectively, when (xi, xj) = (u, v).
Consider again the input space for (xi, xj), we need a
quick way to compute the sum of targets and sum of weights
for each quadrant. Figure 2 shows an example for computing
sum of targets on each quadrant. Given the above notations,
we already know the marginal cumulative histograms for xi
and xj, but unfortunately using these marginal values only
can not recover values on four quadrants. Thus, we have to
compute value for one quadrant.
We show that it is very easy and eﬃcient to compute all
possible values for the red quadrant given any cuts (ci, cj)
using dynamic programming. Once that quadrant is known,
we can easily recover values in other quadrant using marginal
cumulative histograms. We store those values into lookup
tables. Let Lt(ci, cj) = [a, b, c, d] be the lookup table for sum
Algorithm 2 ConstructLookupTable
2: for q = 1 to dj do
sum ←sum + Ht
a [q] ←sum
j ) ←ComputeV alues(CHt
j, a [q])
6: for p = 2 to di do
for q = 1 to dj do
sum ←sum + Ht
a[p][q] ←sum + a[p −1][q]
j ) ←ComputeV alues(CHt
j, a[p][q])
of targets on cuts (ci, cj), and denote Lw(ci, cj) = [a, b, c, d]
as the lookup table for sum of weights on cuts (ci, cj).
Algorithm 2 describes how to compute the lookup table
We focus on computing quadrant a and other quadrants can be easily computed, which is handled by subroutine ComputeV alues. Given Ht
ij, we ﬁrst compute as for
the ﬁrst row of Lt (Line 3-5). Let a[p][q] denote the value
for cuts (p, q). Note a[p][q] = a[p −1][q] + P
Thus we can eﬃciently compute the rest of the lookup table
row by row (Line 6-11).
Once we have Lt and Lw, given any cuts (ci, cj), we can
easily construct Tij. For example, we can set the leftmost
leaf value in Tij as Lt(ci, cj).a/Lw(ci, cj).a. It is easy to see
that with those bookkeeping data structures, we can reduce
the complexity of building predictors to O(1).
Calculating RSS
In this section, we show that calculating RSS for Tij
can be very eﬃcient. Consider the deﬁnition of RSS. Let
Tij.r denote the prediction value on region r, where r ∈
{a, b, c, d}.
(yk −Tij(xk))2
Tij.rLt.r +
(Tij.r)2Lw.r
In practical implementation, we only need to care about
r(Tij.r)2Lw.r−2 P
r Tij.rLt.r since we are only interested
in relative ordering of RSS, and it is easy to see the complexity of computing RSS for Tij is O(1).
Complexity Analysis
For each pair (xi, xj), computing the histograms and cumulative histograms needs to scan through the data and
therefore its complexity is O(N). Constructing the lookup
tables takes O(didj + N) time. Thus, the time complexity
of FAST is O(didj + N) for one pair (xi, xj). Besides, Since
we need to store di-by-dj matrices for each pair, the space
complexity is O(didj).
For continuous features, didj can be quite large. However,
we can discretize the features into b equi-frequency bins.
Such feature discretizing usually does not hurt the performance of regression tree . As we will see in Section 5,
FAST is not sensitive to a wide range of bs. Therefore, the
complexity can be reduced to O(b2 + N) per pair when we
discretize features into b bins. For small bs (b ≤256), we
can quickly process each pair.
Two-stage Construction
With FAST, we can quickly rank of all pairs in Z, the remaining pair set, and add the best interaction to the model.
However, reﬁtting the whole model after each pair is added
can be very expensive for large high-dimensional datasets.
Therefore, we propose a two-stage construction approach.
1. In Stage 1, build the best additive model F in H1 using
only one-dimensional components.
2. In Stage 2, ﬁx the one-dimensional functions, and build
models for pairwise interactions on residuals.
Implementation Details
To scale up to large datasets and many features, we discretize the features into 256 equi-frequency bins for continuous features.1
We ﬁnd such feature discretization rarely
hurts the performance but substantially reduces the running time and memory footprint since we can use one byte
to store a feature value. Besides, discretizing the features removes the sorting requirement for continuous features when
searching for the best cuts in the space.
Previous research showed that feature shaping using gradient boosting with shallow regression tree ensembles
can achieve the best accuracy . We follow similar approach (i.e., gradient boosting with shallow tree-like ensembles) in this work. However, a regression tree is not the ideal
learning method for each component for two reasons. First,
while regression trees are good as a generic shape functions
for any xu, shaping a single feature is equivalent to cutting
on a line, but line cutting can be made more eﬃcient than
regression tree. Second, using regression tree to shape pairwise functions can be problematic. Recall that in Stage 1,
we obtain the best additive model after gradient boosting
converges. This means adding more cuts to any one feature
does not reduce the error, and equivalently, any cut on a single feature is random. Therefore, when we begin to shape
pairwise interactions, the root test in a regression tree that
is constructed greedily top-down is random.
Similar to , to eﬀectively shape pairwise interactions,
we build shallow tree-like models on the residuals as illustrated in Figure 3.
We enumerate all possible cuts ci on
xi. Given this cut, we greedily search the best cut c1
region above ci and similarly greedily search the best cut c2
in the region below ci. Note we can reuse the lookup table
Lt and Lw we developed for FAST for fast search of those
three cuts. Figure 3 shows an example of computing the leaf
values given ci, c1
j. Similarly, we can quickly compute
the RSS given any combination of 3 cuts once the leaf values
are available, just as we did in Section 4.1.4, and therefore
it is very fast to search for the best combination of cuts in
this space. Similarly, we search for the best combination of
3 cuts with 1 cut on xj and 2 cuts on xi and pick the better
model with lower RSS. It is easy to see the complexity is
O(N + b2), where b is the number of bins for each feature
and b = 256 in our case.
1Note that this is not the number of bins used in FAST,
the interaction detection process. Here we use 256 bins for
feature/pair shaping.
a = Lt(ci, c1
j).a/Lw(ci, c1
b = Lt(ci, c1
j).b/Lw(ci, c1
c = Lt(ci, c2
j).c/Lw(ci, c2
d = Lt(ci, c2
j).d/Lw(ci, c2
Figure 3: Illustration for computing shape function
for pairwise interaction.
Attributes
CalHousing
Table 1: Datasets.
Further Relaxation
For large datasets, even reﬁtting the model on selected
pairs can be very expensive. Therefore, we propose to use
the ranking of FAST right after Stage 1, to select the top-K
pairs to S, and ﬁt a model using the pairs in S on the residual
R, where K is chosen according to computing power.
Diagnostics
Models that combine both accuracy and intelligibility are
important. Usually S will still be an overcomplete set. For
intelligibility, once we have learned the best model in H,
we would like to rank all terms (one- and two-dimensional
components) so that we can focus on the most important
features, or pairwise interactions.
Therefore, we need to
assign weights for each term. We use
E[f 2u], the standard
deviation of fu (since E[fu] = 0), as the weight for term
u. Note this is a natural generalization of the weights in
the linear models; this is easy to see since fi(xi) = wixi,
i ] is equivalent to |wi| if features are normalized so
EXPERIMENTS
In this section we report experimental results on both synthetic and real datasets.
The results in Section 5.1 show
GA2M learns models that are nearly as accurate as fullcomplexity random forest models while using terms that depend only on single features and pairwise interactions and
thus are intelligible. The results in Section 5.2 demonstrate
that FAST ﬁnds the most important interactions of O(n2)
feature pairs to include in the model. Section 5.3 compares
the computational cost of FAST and GA2M to competing
methods. Section 5.4 brieﬂy discusses several important de-
CalHousing
Linear Regression
30.41±0.24
21.62±0.38
11.37±0.38
11.61±0.43
GA2M Order
10.81±0.29
10.59±0.35
Random Forests
11.38±1.03
RMSE for regression datasets.
Each cell contains the mean RMSE ± one standard deviation.
Average normalized score is shown in the last column, calculated as relative improvement over GAM.
Logistic Regression
15.78±3.28
17.11±0.08
27.54±0.27
30.02±0.37
14.85±0.28
17.84±0.20
28.83±0.24
28.82±0.25
28.74±0.37
GA2M Order
28.76±0.34
13.88±0.32
28.20±0.18
Random Forests
12.45±0.64
28.48±0.40
Table 3: Error rate for classiﬁcation datasets. Each cell contains the error rate ± one standard deviation.
Average normalized score is shown in the last column, calculated as relative improvement over GAM.
sign choices made for FAST and GA2M. Finally, Section 5.5
concludes with a case study.
Model Accuracy on Real Datasets
We run experiments on ten real datasets to show the accuracy that GA2M can achieve with models that depend only
on 1-d features and pairwise feature interactions.
Table 1 summarizes the 10 datasets. Five are regression
problems: “Delta” is the task of controlling the ailerons of
an F16 aircraft . “CompAct” is from the Delve repository
and describes the state of multiuser computers . “Pole”
describes a telecommunication problem . “CalHousing”
describes how housing prices depend on census variables .
“MSLR10k” is a learning-to-rank dataset but we treat relevance as regression targets . The other ﬁve datasets are binary classiﬁcation problems: The “Spambase”, “Magic” and
“Letter” datasets are from the UCI repository . “Gisette”
is from the NIPS feature selection challenge . “Physics” is
from the KDD Cup 2004 .
The features in all datasets are discretized into 256 equifrequency bins.
For each model we include at most 1000
feature pairs; we include all feature pairs in the six problems
with least dimension, and the top 1000 feature pairs found
by FAST on the “Pole”, “MSLR10k”, “Spambase”, “Gisette”,
and “Physics” datasets. Although it is possible that higher
accuracy might be obtained by including more or fewer feature pairs, search for the optimal number of pairs is expensive and GA2M is reasonably robust to excess feature pairs.
However, it is too expensive to include all feature pairs on
problems with many features. We use 8 bins for FAST in
all experiments.
We compare GA2M to linear/logistic regression, feature
shaping (GAMs) without interactions, and full-complexity
random forests.
For regression problems we report root
mean squared error (RMSE) and for classiﬁcation problems
we report 0/1 loss. To compare results across diﬀerent datasets,
we normalize results by the error of GAMs on each dataset.
For all experiments, we train on 80% of the data and hold
aside 20% of the data as test sets.
In addition to FAST, we also consider three baseline methods on ﬁve high dimensional datasets, i.e., GA2M Rand,
GA2M Coef and GA2M Order. GA2M Rand means we add
same number of random pairs to GAM. GA2M Order and
GA2M Coef use the weights of 1-d features in GAM to propose pairs; GA2M Order generates pairs by the order of 1-d
features and GA2M Coef generates pairs by the product of
weights of 1-d features.
The regression and classiﬁcation results are presented in
Table 2 and Table 3. As expected, the improvement over
linear models from shaping individual features (GAMs) is
substantial: on average feature shaping reduces RMSE 34%
on the regression problems, and reduces 0/1 loss 44% on
the classiﬁcation problems. What is surprising, however, is
that by adding shaped pairwise interactions to the models,
GA2M FAST substantially closes the accuracy gap between
unintelligible full-complexity models such as random forests
and GAMs. On some datasets, GA2M FAST even outperforms the best random forest model. Also, none of the baseline methods perform comparably GA2M FAST.
Detecting Feature Interactions with FAST
In this section we evaluate how accurately FAST detects
feature interactions on synthetic problems.
Sensitivity to the Number of Bins
To evaluate sensitivity of FAST we use the synthetic function generator in to generate random functions. Because
these are synthetic function, we know the ground truth interacting pairs and use average precision (area under the
precision-recall curve evaluated at true points) as the eval-
Average Precision
Number of Bins
(a) 10 features.
Average Precision
Number of Bins
(b) 100 features.
Figure 4: Sensitivity of FAST to the number of bins.
Average Precision
Figure 5: Precision/Cost on synthetic function.
uation metric.
We vary b = 2, 4, ..., 256 and the dataset
size N = 102, 103, ..., 106.
For each ﬁxed N, we generate
datasets with n features and k higher order interactions xu,
where |u| = ⌊1.5 + r⌋and r is drawn from an exponential
distribution with mean λ = 1.
We experiment with two
cases: 10 features with 25 higher order interactions and 100
features with 1000 higher order interactions.
Figure 4 shows the mean average precision and variance
for 100 trials at each setting.
As expected, average precision increases as dataset size increases, and decreases as
the number of features increases from 10 (left graph) to 100
(right graph). When there are only 10 features and as many
as 106 samples, FAST ranks all true interactions above all
non-interacting pairs (average precision = 1) in most cases,
but as the sample size decreases or the problem diﬃculty
increases average precision drops below 1. In the graph on
the right with 100 features there are 4950 feature pairs, and
FAST needs large sample sizes (106 or greater) to achieve average precision above 0.7, and as expected performs poorly
when there are fewer samples than pairs of features.
On these test problems the optimal number of bins appears to be about b = 8, with average precision falling
slightly for number of bins larger and smaller than 8. This is
a classic bias-variance tradeoﬀ: smaller b reduces the chances
of overﬁtting but at the risk of failing to model some kinds
of interactions, while large b allows more complex interactions to be modeled but at the risk of allowing some false
interactions to be confused with weak true interactions.
The previous section showed that FAST accurately detects feature interactions when the number of samples is
much larger than the number of feature pairs, but that accuracy drops as the number of feature pairs grows comparable to and then larger than the number of samples. In
this section we compare the accuracy of FAST to the interaction detection methods discussed in Section 3.2. For
ANOVA, we use R package mgcv to compute p-values under a Wald test . For PDF, we use RuleFit package and
we choose m = 100, 200, 400, 800, where m is the sample size
that trades oﬀeﬃciency and accuracy . Grove is available
in TreeExtra package .
Here we conduct experiments on synthetic data generated
by the following function .
F(x) = πx1x2√
2x3 −sin−1(x4) + log(x3 + x5)−
Variables x4, x5, x8, x10 are uniformly distributed in [0.6, 1]
and the other variables are uniformly distributed in .
We generate 10, 000 points for these experiments. Figure 5(a)
shows the average precision of the methods. On this problem, the Grove and ANOVA methods are accurate and rank
all 11 true pairs in the top of the list. FAST is almost as
good and correctly ranks the top ten pairs. The other methods are signiﬁcantly less accurate than Grove, ANOVA, and
To understand why FAST does not pick up the 11th pair,
we plot heat maps of the residuals of selected pairs in Figure 6. (x1, x2) and (x2, x7) are two of the correctly ranked
true pairs, (x1, x7) is a false pair ranked below the true pairs
FAST detects correctly but above the true pair it misses, and
(x8, x10) is the true pair FAST misses and ranks below this
false pair. The heat maps show strong interactions are easy
to distinguish, but some false interactions such as (x1, x7)
can have signal as strong as that of weak true interactions
such as (x8, x10).
In fact, Sorokina et al.
found that x8
is a weak feature, and do not consider pairs that use x8 as
interactions on 5, 000 samples , so we are near the threshold of detectability of (x8, x10) going from 5, 000 to 10, 000
Feature Correlation and Spurious Pairs
If features are correlated, spurious interactions may be
detected because it is diﬃcult to tell the diﬀerence between
a true interaction between x1 and x2 and the spurious interaction between x1 and x3 when x3 is strongly correlated
with x2; any interaction detection method such as FAST
that examines pairs in isolation will have this problem. With
GA2M, however, it is ﬁne to include some false positive pairs
because GA2M is able to post-ﬁlter false positive pairs by
looking at the term weights of shaped interactions in the
ﬁnal model.
To demonstrate this, we use the synthetic function in
Equation 10, but make x6 correlated to x1. We generate
2 datasets, one with ρ(x1, x6) = 0.5 and the other with
ρ(x1, x6) = 0.95, where ρ is the correlation coeﬃcient. We
run FAST on residuals after feature shaping. We give the
top 20 pairs found by FAST to GA2M, which then uses gradient boosting to shape those pairwise interactions. Figure 7
illustrates how the weights of selected pairwise interactions
evolve after each step of gradient boosting. Although the
pair (x2, x6) can be incorrectly introduced by FAST because
of the high correlation between x1 and x6, the weight on this
false pair decreases quickly as boosting proceeds, indicating
that this pair is spurious. This not only allows the model
trained on the pairs to remain accurate in the face of spurious pairs, but also reduces the weight (and ranking) given
to this shaped term so that intelligibility is not be hurt by
the spurious term.
"hm/0.1.txt" u 1:2:3
"hm/1.6.txt" u 1:2:3
"hm/0.6.txt" u 1:2:3
"hm/7.9.txt" u 1:2:3
Figure 6: True/Spurious heat maps.
Features are
discretized into 32 bins for visualization.
(a) ρ(x1, x6) = 0.5
(b) ρ(x1, x6) = 0.95
Figure 7: Weights for pairwise interaction terms in
the model.
Scalability
Figure 5(b) illustrates the running time of diﬀerent methods on 10, 000 samples from Equation 10. Model building
time is included.
FAST takes about 10 seconds to rank
all possible pairs while the two other accurate methods,
ANOVA and Grove, are 3-4 orders of magnitude slower.
Grove, which is probably the most accurate interaction detection method currently available, takes almost a week to
run once on this data. This shows the advantage of FAST;
it is very fast with high accuracy. On this problem FAST
takes less than 1 second to rank all pairs and the majority
of time is devoted to building the additive model.
Figure 8 shows the running time of FAST per pair on real
It is clear that on real datasets, FAST is both
accurate and eﬃcient.
Design Choices
An alternate to interaction detection that we considered
was to build ensembles of trees on residuals after shaping the
individual features and then look at tree statistics to ﬁnd
combinations of features that co-occur in paths more often
than their independent rate warrants. By using 1-step lookahead at the root we also hoped to partially mitigate the
myopia of greedy feature installation to make interactions
more likely to be detected.
Unfortunately, features with
high “co-occurence counts” did not correlate well with true
interactions on synthetic test problems, and the best treebased methods we could devise did not detect interactions
as well as FAST, and were considerably more expensive.
Size of dataset
Time (s) per pair
CalHousing
Figure 8: Computational cost on real datasets.
Case Study: Learning to Rank
Learning-to-rank is an important research topic in the
data mining, machine learning and information retrieval communities. In this section, we train intelligible models with
shaped one-dimensional features and pairwise interactions
on the “MSLR10k” dataset. A complete description of features can be found in .
We show the top 10 most important individual features and their shape functions in ﬁrst
two rows of Figure 9. The number above each plot is the
weight for the corresponding term in the model. Interestingly, we found BM25 , usually considered as a powerful
feature for ranking, ranked 70th (BM25 url) in the list after shaping. Other features such as IDF (inverse document
frequency) enjoy much higher weight in the learned model.
The last two rows of Figure 9 show the 10 most important
pairwise interactions and their term strengths. Each of them
shows a clear interaction that could not be modeled by additive terms. The non-linear shaping of the individual features
in the top plots and the pairwise interactions in the bottom
plots are intelligible to experts and feature engineers, but
would be well hidden in full-complexity models.
CONCLUSIONS
We present a framework called GA2M for building intelligible models with pairwise interactions. Adding pairwise
interactions to traditional GAMs retains intelligibility, while
substantially increasing model accuracy. To scale up pairwise interaction detection, we propose a novel method called
FAST that eﬃciently measures the strength of all potential
pairwise interactions.
Acknowledgements. We thank the anonymous reviewers for their valuable comments, and we thank Nick Craswell
of Microsoft Bing for insightful discussions. This research
has been supported by the NSF under Grants IIS-0911036
and IIS-1012593. Any opinions, ﬁndings and conclusions or
recommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of the NSF.