Balanced Distribution Adaptation for Transfer Learning
Jindong Wang∗†‡, Yiqiang Chen∗†‡, Shuji Hao§, Wenjie Feng∗‡††, Zhiqi Shen∥
∗Beijing Key Laboratory of Mobile Computing and Pervasive Device
††CAS Key Laboratory of Network Data Science & Technology
†Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
‡University of Chinese Academy of Sciences,§Institute of High Performance Computing, A*STAR
∥School of Computer Science and Engineering, Nanyang Technological University, Singapore
Email:{wangjindong,yqchen}@ict.ac.cn, , , 
Abstract—Transfer learning has achieved promising results
by leveraging knowledge from the source domain to annotate
the target domain which has few or none labels. Existing
methods often seek to minimize the distribution divergence
between domains, such as the marginal distribution, the conditional distribution or both. However, these two distances are
often treated equally in existing algorithms, which will result
in poor performance in real applications. Moreover, existing
methods usually assume that the dataset is balanced, which also
limits their performances on imbalanced tasks that are quite
common in real problems. To tackle the distribution adaptation
problem, in this paper, we propose a novel transfer learning
approach, named as Balanced Distribution Adaptation (BDA),
which can adaptively leverage the importance of the marginal
and conditional distribution discrepancies, and several existing
methods can be treated as special cases of BDA. Based on
BDA, we also propose a novel Weighted Balanced Distribution
Adaptation (W-BDA) algorithm to tackle the class imbalance
issue in transfer learning. W-BDA not only considers the
distribution adaptation between domains but also adaptively
changes the weight of each class. To evaluate the proposed
methods, we conduct extensive experiments on several transfer
learning tasks, which demonstrate the effectiveness of our
proposed algorithms over several state-of-the-art methods.
Keywords-Transfer learning, domain adaptation, distribution
adaptation, class imbalance
I. INTRODUCTION
Preparing labeled data is crucial for training machine
learning algorithms. However, it is often expensive and timeconsuming to obtain sufﬁcient labeled data in real applications. In this case, transfer learning has been a promising
approach by transferring knowledge from a labeled source
domain to the target domain. Transfer learning often assumes
the training and testing data are from similar but different
distributions . For instance, the images of an object
taken in different angles, backgrounds and illuminations
could lead to different marginal or conditional distributions.
By observing this, existing transfer learning methods are
mainly focusing on distribution adaptation to minimize the
distribution divergence between domains , , .
Most of the existing distribution adaptation methods adapt
either marginal distribution , conditional distribution 
or both , . It is shown in that adapting both could
achieve better performance. The work of , , also
proposed several approaches to adapt the joint distribution.
However, those two distributions are often treated equally
in existing methods, while the importance of each other is
not leveraged. When the datasets are much more dissimilar,
it means the marginal distributions are more dominant;
when the datasets are similar, it means the conditional
distributions needs more attention. Hence, it will deteriorate
the performance of algorithms by only adding them together
with equal weight. Therefore, how to adaptively leverage the
importance of each distribution is a critical problem.
Moreover, class imbalance often exists in many transfer
learning scenarios. When the class proportion of domains
is highly imbalanced, it needs to be considered carefully
for distribution adaptation. Existing methods , often
ignore this issue by treating the classes as balanced across
domains, or they only handle the bias on one domain ,
and this may hinder the effectiveness of transfer learning.
Therefore, how to handle the class imbalance situation in
transfer learning is another important challenge.
In this paper, we propose two novel methods to tackle the
above two issues. For distribution adaptation, we propose
Balanced Distribution Adaptation (BDA). BDA can not
only adapt both the marginal and conditional distributions
between domains, but also leverage the importance of those
two distributions, thus it can be effectively adjusted to
speciﬁc transfer learning tasks. Several existing methods
can be regarded as special cases of BDA. Based on BDA,
we also propose a novel Weighted Balanced Distribution
Adaptation (W-BDA) algorithm to tackle the class imbalance issue in transfer learning. The proposed W-BDA can
adaptively change the weight of each class when performing
distribution adaptation. To evaluate BDA and W-BDA, we
conduct extensive experiments on ﬁve image datasets.
To sum up, our contributions are mainly three-fold:
1) We propose a novel transfer learning method, which
is named as BDA to balance the marginal and conditional
distribution adaptation. BDA can adaptively adjust the importance of those two distances and can achieve a better
performance. Several transfer learning algorithms can be
regarded as special cases of BDA.
2) We also propose another novel method W-BDA by
extending BDA to handle the class imbalance problem which
is common in transfer learning. The proposed W-BDA not
only considers the distribution adaptation of domains but
also adaptively changes the weight of each class, thus it can
handle the class imbalance problem for transfer learning.
3) We conduct extensive experiments on ﬁve image
datasets to evaluate the BDA and W-BDA methods, indicating their superiority against other state-of-the-art methods.
II. RELATED WORK
Transfer learning has been widely applied to activity
recognition , incremental learning , and online learning , . Our proposed BDA and W-BDA are mainly
related to the feature-based transfer learning methods. Thus,
in this section, we present a detailed discussion on this
category, speciﬁcally on two aspects.
Joint distribution adaptation. proposed to jointly
select feature and preserve structural properties. Long et
al. proposed joint distribution adaptation method (JDA)
to match both marginal and conditional distribution between domains. Others extended JDA by adding structural
consistency , domain invariant clustering , and target
selection . Those methods tend to ignore the importance between two distinct distributions by just adding
them together. However, when there is a large discrepancy
between both distributions, those methods cannot evaluate
the importance of each distribution, and may not generalize
well in most cases. Our work is capable of investigating
the importance of each distribution. Thus it can be more
generalized to transfer learning scenarios with complex data
distributions.
Class imbalance problem. Previous sample re-weighting
methods only learned weights of speciﬁc samples,
but ignore the class weights balance for different classes.
 developed a Closest Common Space Learning (CCSL)
method to adapt the cross-domain weights. CCSL is an
instance selection method, while ours is a feature based
approach. Multiset feature learning was proposed in to
learn discriminant features. proposed weighted maximum
mean discrepancy to construct a source reference collection
on the target domain but it only adapted the prior of source
domain, while our method could adapt the priors from both
source and target domains. tackled the imbalance issue
when target domain has some labels, while in our method,
target domain has no labels. adjusted the weights of
different samples according to their predictions, while our
work focuses on adjusting the weight of each class.
III. BALANCED DISTRIBUTION ADAPTATION
This section elaborates our proposed algorithms. First,
we introduce the problem deﬁnition. Then, we present the
Balanced Distribution Adaptation (BDA) approach. Finally,
the Weighted BDA (W-BDA) method is introduced.
A. Problem Deﬁnition
Given a labeled source domain {xsi, ysi}n
i=1, an unlabeled target domain {xtj}m
j=1, and assume feature space
Xs = Xt, label space Ys = Yt but marginal distributions
Ps(xs) ̸= Pt(xt) with conditional distributions Ps(ys|xs) ̸=
Ps(yt|xt). Transfer learning aims to learn the labels yt of
Dt using the source domain Ds.
Balanced distribution adaptation solves the transfer learning problem by adaptively minimizing the marginal and
conditional distribution discrepancy between domains, and
handle the class imbalance problem, i.e. to minimize the
discrepancies between: 1) P(xs) and P(xt), 2) P(ys|xs)
and P(yt|xt).
B. Balanced Distribution Adaptation
Transfer learning methods often seek to adapt both the
marginal and conditional distributions between domains ,
 . Speciﬁcally, this refers to minimizing the distance
D(Ds, Dt) ≈D(P(xs), P(xt))
+ D(P(ys|xs), P(yt|xt))
However, simply matching both distributions is not
equally important, and that implicit assumption does not
hold. In this section, we propose Balanced Distribution
Adaptation (BDA) to adaptively adjust the importance of
both the marginal and conditional distributions based on
each speciﬁc tasks. Concretely speaking, BDA exploits a
balance factor µ to leverage the different importance of
distributions:
D(Ds, Dt) ≈(1 −µ)D(P(xs), P(xt))
+ µD(P(ys|xs), P(yt|xt))
where µ ∈ . When µ →0, it means the datasets
are more dissimilar, so the marginal distribution is more
dominant; when µ →1, it reveals the datasets are similar,
so the conditional distribution is more important to adapt.
Therefore, the balance factor µ can adaptively leverage the
importance of each distribution and lead to good results.
It is worth noting that, since the target domain Dt has
no labels, it is not feasible to evaluate the conditional
distribution P(yt|xt). Instead, we use the class conditional
distribution P(xt|yt) to approximate P(yt|xt). Because
P(xt|yt) and P(yt|xt) can be quite involved according to
the sufﬁcient statistics when sample sizes are large . In
order to compute P(xt|yt), we apply prediction on Dt using
some base classiﬁer trained on Ds to get the soft labels for
Dt. The soft labels may be less reliable, so we iteratively
reﬁne the them.
In order to compute the marginal and conditional distribution divergences in Eq. (2), we adopt maximum mean
discrepancy (MMD) to empirically estimate both distribution discrepancies. As a nonparametric measurement,
MMD has been widely applied to many existing transfer
learning approaches , . Formally speaking, Eq. (2) can
be represented as
D(Ds, Dt) ≈(1 −µ)
where H denotes the reproducing kernel Hilbert space
(RKHS), c ∈{1, 2, · · · , C} is the distinct class label, n, m
denote the number of samples in the source / target domain,
denote the samples belonging to class c
in source and target domain, respectively. nc = |D(c)
t |, denoting the number of samples belonging to D(c)
t , respectively. The ﬁrst term denotes the marginal
distribution distance between domains, while the second
term is the conditional distribution distance.
By further taking advantage of matrix tricks and regularization, Eq. (2) can be formalized as:
(1 −µ)M0 + µ
s.t. A⊤XHX⊤A = I,
There are two terms in Eq. (4): the adaptation of marginal
and conditional distribution with balance factor (term 1),
and the regularization term (term 2). λ is the regularization
parameter with ∥·∥2
F the Frobenius norm. Two constraints
are involved in Eq. (4): the ﬁrst constraint ensures that
the transformed data (A⊤X) should preserve the inner
properties of the original data. The second constraint denotes
the range of the balance factor µ.
More speciﬁcally, in Eq. (4), X denotes the input data
matrix composed of xs and xt, A denotes the transformation
matrix, I ∈R(n+m)×(n+m) is the identity matrix, and H
is the centering matrix i.e. H = I −(1/n)1. Similar as
in work , M0 and Mc are MMD matrices and can be
constructed in the following ways:
xi, xj ∈Ds
xi, xj ∈Dt
xi, xj ∈D(c)
xi, xj ∈D(c)
s , xj ∈D(c)
t , xj ∈D(c)
Learning algorithm: Denote Φ = (φ1, φ2, · · · , φd) as
Lagrange multipliers, then Lagrange function for Eq. (4) is
(1 −µ)M0 + µ
 (I −A⊤XHX⊤A)Φ
Set derivative ∂L/∂A = 0, the optimization can be derived
as a generalized eigendecomposition problem
(1 −µ)M0 + µ
A = XHX⊤AΦ
Finally, the optimal transformation matrix A can be obtained
by solving Eq. (8) and ﬁnding its d smallest eigenvectors.
Estimation of µ: Note that µ is technically not a free
parameter like λ and it has to be estimated according to data
distributions. However, there is no effective solution for its
estimation. For now, we evaluate the performance of µ by
searching its values in experiments. For real application, we
recommend getting the optimal µ through cross-validation.
C. Weighted Balanced Distribution Adaptation
BDA is able to adaptively leverage the importance of
marginal and conditional distributions between domains.
BDA indicates when the marginal distributions are relatively close, the performance of transfer learning is highly
dependent on the conditional distribution distance. When
computing the conditional distributions, BDA utilizes class
conditional distributions instead, i.e. P(x|y) is used to
approximate P(y|x). This implicitly assumes that the probability of this class in each domain is similar, which is usually
not the case in real world. In this section, we propose a more
robust approximation of the conditional distribution for class
imbalance problem:
∥P(ys|xs) −P(yt|xt)∥2
P(xs)P(xs|ys) −P(yt)
P(xt)P(xt|yt)
= ∥αsP(xs|ys) −αtP(xt|yt)∥2
Technically, we approximate αs and αt by the class
prior of both domains. To this end, weighted balanced
distribution adaptation (W-BDA) is proposed to balance the
class proportion of each domain. Evaluating the conditional
distribution divergence in Eq. (9) requires to estimate the
marginal distributions P(xs) and P(xt). However, it is nontrivial. Since BDA is fully capable of adapting P(xs) and
P(xt), we do not estimate them in this step and assume they
are unchanged. Then, we construct a weight matrix Wc for
each class:
xi, xj ∈D(c)
xi, xj ∈D(c)
s , xj ∈D(c)
t , xj ∈D(c)
denote the class prior on class
c in the source and target domain, respectively.
Embedding Eq. (10) into BDA, we get the trace optimization problem of W-BDA:
(1 −µ)M0 + µ
s.t. A⊤XHX⊤A = I,
Remark: Eq. (5) of BDA and Eq. (10) of W-BDA are
much similar in spirit. Their differences are: 1) Eq. (5) of
BDA only considers the number of samples in each class,
while Eq. (10) also considers the class prior. 2) Eq. (10)
provides more accurate approximation to the conditional
distributions than Eq. (5) when handling the class imbalance.
Kernelization: When applied to nonlinear problems, we
can use a kernel map ψ: x 7→ψ(x), and a kernel matrix
K = ψ(X)⊤ψ(X). The kernel matrix K ∈R(n+m)×(n+m)
can be constructed using linear or RBF kernel.
In summary, Algorithm 1 presents the detail of BDA and
W-BDA methods.
Algorithm 1 BDA: Balanced Distribution Adaptation
Source and target feature matrix Xs and Xt,
source label vector ys, #dimension d, balance factor µ,
regularization parameter λ
Transformation matrix A and classiﬁer f
1: Train a base classiﬁer on Xs and apply prediction on
Xt to get its soft labels ˆyt. Construct X = [Xs, Xt],
initialize M0 and Mc by Eq. (5) and (6) (or Wc using
Eq. (10) for W-BDA)
Solve the eigendecomposition problem in Eq. (8) (or
Eq. (11) for W-BDA) and use d smallest eigenvectors
to build A
Train a classiﬁer f on {A⊤Xs, ys}
Update the soft labels of Dt: ˆyt = f(A⊤Xt)
Update matrix Mc using Eq. (6) (or update Wc using
Eq. (10) for W-BDA)
7: until Convergence
8: return Classiﬁer f
IV. EXPERIMENTS
In this section, we evaluate the performance of the proposed methods through extensive experiments.
A. Datasets
We adopt ﬁve widely-used datasets: USPS + MNIST,
COIL20 and Ofﬁce + Caltech. Table I shows the details
of the datasets. USPS (U) and MNIST (M) are standard
digit recognition datasets containing handwritten digits from
0-9. USPS consists of 7,291 training images and 2,007
test images. MNIST contains 60,000 training images and
10,000 test images. COIL20 (CO) includes 1,440 images
belonging to 20 objects. Ofﬁce is composed of three realworld object domains: Amazon, Webcam and DSLR. It
has 4,652 images with 31 object categories. Caltech-256
(C) contains 30,607 images and 256 categories. Detailed
descriptions about those datasets can be found in . For all
the datasets, we follow to construct 16 different tasks.
INTRODUCTION OF THE FIVE DIGIT/OBJECT DATASETS.
B. Comparison Methods
We choose six state-of-the-art comparison methods:
• 1 Nearest Neighbor classiﬁer (1NN)
• Principal Component Analysis (PCA) + 1NN
• Geodesic Flow Kernel (GFK) + 1NN
• Transfer Component Analysis (TCA) + 1NN
• Joint Distribution Adaptation (JDA) + 1NN
• Transfer Subspace Learning (TSL) + 1NN
Among those methods, 1NN and PCA are traditional
learning methods, while GFK, TCA, JDA, and TSL are stateof-the-art transfer learning approaches.
C. Implementation Details
PCA, TCA, JDA, TSL, and BDA are acting as dimensionality reduction process, then 1NN is applied. For GFK,
1NN is applied after we get the geodesic ﬂow kernel. For
BDA and W-BDA, µ is searched in {0, 0.1, · · · , 0.9, 1.0}.
Since BDA can achieve a stable performance under a wide
range of parameter values, for the comparison study, we set
d = 100; λ = 0.1 for MNIST + USPS / Ofﬁce + Caltech
datasets and λ = 0.01 for COIL20 dataset. For the kernelbased methods, we use linear kernel. The iteration number
for JDA and TCA is set to be T = 10. The codes of BDA
and W-BDA are available online1. Classiﬁcation accuracy
on target domain is adopted as the evaluation metric which
is widely used in literatures , .
1Code available at 
ACCURACY (%) OF BDA AND OTHER METHODS ON 16 TASKS.
# Iterations
MMD distance
(a) MMD distance
# Iterations
Accuracy (%)
(b) Accuracy
MMD distance and classiﬁcation accuracy comparison of TCA,
JDA and BDA on U →M. It can be noted that BDA achieves better
accuracy with relatively small MMD distance.
D. Performance Evaluation of BDA
1) Classiﬁcation accuracy: We test the performance of
BDA and the other comparison methods on 16 cross-domain
learning tasks. The results are shown in Table II, based on
which, we can draw the following observations.
First, BDA outperforms most of the existing methods (15
out of 16 tasks). Speciﬁcally, the average classiﬁcation
accuracy of BDA is 55.56%, which shows an average
improvement of 2.38% compared to the best comparison
method JDA. JDA is only capable of adapting the marginal
and conditional distribution with the equal weight (µ = 0.5).
Thus JDA can be considered as a special case of BDA.
However, BDA can dramatically improve the accuracy by
adjusting the balance parameter µ to adapt various scenarios.
Second, TCA is also a special case of BDA (µ = 0)
since it only adapts the marginal distribution. Therefore, the
performance of TCA is worse than JDA and BDA.
Third, TSL only adapts the marginal distributions which
highly relies on the distribution density. The performance
of GFK is better on object recognition tasks. The reason is
that GFK learns a global geodesic ﬂow kernel on the lowdimension representation, which may be enough to transit
smoothly for the object datasets. But as for the digit tasks,
it may not enough to construct smooth transit when the
marginal distribution distance is large.
Last, all transfer learning methods perform better than
traditional learning approaches due to the large distribution
Accuracy(%)
Classiﬁcation accuracy w.r.t. µ on different tasks. Dashed lines
are the best comparison methods.
gap between different domains. This indicates the effectiveness of transfer learning methods, among which BDA could
achieve the best performance.
2) Effectiveness of distribution adaptation: We further
verify the effectiveness of BDA by comparing its distribution
adaptation with other two distribution adaptation methods:
TCA and JDA. Speciﬁcally, we investigate their performance
with MMD distances calculated using Eq. (4).
Fig. 1(a) and Fig. 1(b) show the MMD distance and
accuracy of TCA, JDA, and BDA with increasing iteration,
respectively. Based on the results, we can observe: a) MMD
distances of all methods can be reduced. This indicates the
effectiveness of TCA, JDA and BDA; b) MMD distance of
TCA is not reduced largely as it only adapts the marginal
distribution distance and requires no iteration; c) MMD
distance of JDA is obviously larger than BDA, since BDA
could balance the importance of marginal and conditional
distribution via µ; d) BDA achieves the best performance.
E. Effectiveness of Balance Factor
In this section, we evaluate the effectiveness of the balance
factor µ. We run BDA with µ ∈{0, 0.1, · · · , 1.0} on some
tasks and compare the performances with the best baseline
method. Fig. 2 shows the results. It is obvious that the
optimal µ varies on different tasks, indicating the importance to balance the marginal and conditional distributions
between domains. In comparison, the best baseline JDA
(dash lines) is only the special case of BDA (µ = 0.5),
which means to treat those distributions equally. However,
this assumption does not hold. In task A →W with optimal
µ = 0.8, it means the marginal distributions are almost the
same so the performance of transfer learning mostly depends
on conditional distributions. In task M →U with optimal
µ = 0.1, it means the marginal distributions contribute most
to the discrepancy, so µ is relatively small. In other 13 tasks,
the observations are similar. It indicates in cross-domain
learning problems, µ is extremely important to balance both
the marginal and conditional distributions. Therefore, BDA
is more capable of achieving good performance.
To be noticed, there may be more than one optimal µ for
some tasks (A →D), and the tendency of µ is not always
stable (CO2 →CO1). The problems behind those facts still
need to be addressed in future research.
F. Effectiveness of Weighted BDA
We extensively verify the effectiveness of proposed W-
BDA. We choose some tasks with highly imbalanced class
distributions and compare the performance of W-BDA with
BDA and JDA. TABLE. III demonstrates the classiﬁcation
accuracy of 6 tasks. Note that for comparison, classes on
tasks 1 ∼4 are rather imbalanced, while classes are rather
balanced on tasks 5 ∼6.
ACCURACY OF JDA, BDA AND W-BDA ON SOME TASKS.
From the results, we can observe: 1) JDA achieves the
worst results since it does not consider the gap between
marginal and conditional distributions. BDA is able to
handle the distribution discrepancy and outperforms JDA in
all situations. 2) For the ﬁrst four tasks which are under
imbalanced class distributions, W-BDA could improve the
performance by adaptively weighting each class. In the other
two tasks where class distributions are rather balanced, W-
BDA still achieves comparable results. On the other 10 tasks,
the results follow the same tendency. To sum up, the results
demonstrate that in transfer learning, W-BDA remains an
effective method to balance the different class distribution
between domains.
In addition, BDA and W-BDA have other two parameters:
feature dimension d and regularization parameter λ. Their
sensitivity evaluations are omitted due to page limit. In our
actual experiments, BDA and W-BDA are relatively robust
to those two parameters.
V. CONCLUSION
Balancing the probability distributions and class distributions between domains are both two important problems
in transfer learning. In this paper, we propose Balanced
Distribution Adaptation (BDA) to adaptively weight the
importance of both marginal and conditional distribution
adaptations. Thus, it could signiﬁcantly improve the transfer
learning performance. Moreover, we consider handling the
class imbalance problem for transfer learning by proposing
Weighted BDA (W-BDA). Extensive experiments on ﬁve
image datasets demonstrate the superiority of our methods
over several state-of-the-art methods. In the future, we will
continue the exploration in these two aspects: by developing
more strategies to leverage the distributions and handle the
class imbalance problem.
ACKNOWLEDGMENT
This work is supported in part by National Key R
(No.2016YFB1001200),
(No.61572471),
Technology
Commission
(No.Z161100000216140
Z171100000117013).