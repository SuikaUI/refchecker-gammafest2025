Supersizing Self-supervision: Learning to Grasp
from 50K Tries and 700 Robot Hours
Lerrel Pinto and Abhinav Gupta
The Robotics Institute, Carnegie Mellon University
(lerrelp, abhinavg)@cs.cmu.edu
Abstract— Current
learning-based
approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology:
(a) since each object can be grasped in multiple ways, manually
labeling grasp locations is not a trivial task; (b) human labeling
is biased by semantics. While there have been attempts to train
robots using trial-and-error experiments, the amount of data
used in such experiments remains substantially low and hence
makes the learner prone to over-ﬁtting. In this paper, we take
the leap of increasing the available training data to 40 times
more than prior work, leading to a dataset size of 50K data
points collected over 700 hours of robot grasping attempts. This
allows us to train a Convolutional Neural Network (CNN) for
the task of predicting grasp locations without severe overﬁtting.
In our formulation, we recast the regression problem to an 18way binary classiﬁcation over image patches. We also present
a multi-stage learning approach where a CNN trained in one
stage is used to collect hard negatives in subsequent stages.
Our experiments clearly show the beneﬁt of using large-scale
datasets (and multi-stage training) for the task of grasping.
We also compare to several baselines and show state-of-the-art
performance on generalization to unseen objects for grasping.
I. INTRODUCTION
Consider the object shown in Fig. 1(a). How do we predict
grasp locations for this object? One approach is to ﬁt 3D
models to these objects, or to use a 3D depth sensor, and
perform analytical 3D reasoning to predict the grasp locations – . However, such an approach has two drawbacks:
(a) ﬁtting 3D models is an extremely difﬁcult problem by
itself; but more importantly, (b) a geometry based-approach
ignores the densities and mass distribution of the object
which may be vital in predicting the grasp locations. Therefore, a more practical approach is to use visual recognition
to predict grasp locations and conﬁgurations, since it does
not require explicit modelling of objects. For example, one
can create a grasp location training dataset for hundreds
and thousands of objects and use standard machine learning
algorithms such as CNNs , or autoencoders to
predict grasp locations in the test data. However, creating
a grasp dataset using human labeling can itself be quite
challenging for two reasons. First, most objects can be
grasped in multiple ways which makes exhaustive labeling
impossible (and hence negative data is hard to get; see
Fig. 1(b)). Second, human notions of grasping are biased by
semantics. For example, humans tend to label handles as the
grasp location for objects like cups even though they might
be graspable from several other locations and conﬁgurations.
Hence, a randomly sampled patch cannot be assumed to be
We present an approach to train robot grasping using 50K trial
and error grasps. Some of the sample objects and our setup are shown in
(a). Note that each object in the dataset can be grasped in multiple ways (b)
and therefore exhaustive human labeling of this task is extremely difﬁcult.
a negative data point, even if it was not marked as a positive
grasp location by a human. Due to these challenges, even
the biggest vision-based grasping dataset has about only
1K images of objects in isolation (only one object visible
without any clutter).
In this paper, we break the mold of using manually labeled
grasp datasets for training grasp models. We believe such an
approach is not scalable. Instead, inspired by reinforcement
learning (and human experiential learning), we present a selfsupervising algorithm that learns to predict grasp locations
via trial and error. But how much training data do we need
to train high capacity models such as Convolutional Neural
Networks (CNNs) to predict meaningful grasp locations
for new unseen objects? Recent approaches have tried to use
 
reinforcement learning with a few hundred datapoints and
learn a CNN with hundreds of thousand parameters . We
believe that such an approach, where the training data is
substantially fewer than the number of model parameters,
is bound to overﬁt and would fail to generalize to new
unseen objects. Therefore, what we need is a way to collect
hundreds and thousands of data points (possibly by having
a robot interact with objects 24/7) to learn a meaningful
representation for this task. But is it really possible to scale
trial and errors experiments to learn visual representations
for the task of grasp prediction?
Given the success of high-capacity learning algorithms
such as CNNs, we believe it is time to develop large-scale
robot datasets for foundational tasks such as grasping. Therefore, we present a large-scale experimental study that not
only substantially increases the amount of data for learning
to grasp, but provides complete labeling in terms of whether
an object can be grasped at a particular location and angle.
This dataset, collected with robot executed interactions, will
be released for research use to the community. We use this
dataset to ﬁne-tune an AlexNet CNN model pre-trained
on ImageNet, with 18M new parameters to learn in the fully
connected layers, for the task of prediction of grasp location.
Instead of using regression loss, we formulate the problem
of grasping as an 18-way binary classiﬁcation over 18 angle
bins. Inspired by the reinforcement learning paradigm ,
 , we also present a staged-curriculum based learning
algorithm where we learn how to grasp, and use the most
recently learned model to collect more data.
The contributions of the paper are three-fold: (a) we
introduce one of the largest robot datasets for the task of
grasping. Our dataset has more than 50K datapoints and has
been collected using 700 hours of trial and error experiments
using the Baxter robot. (b) We present a novel formulation of
CNN for the task of grasping. We predict grasping locations
by sampling image patches and predicting the grasping angle.
Note that since an object may be graspable at multiple
angles, we model the output layer as an 18-way binary
classiﬁer. (c) We present a multi-stage learning approach to
collect hard-negatives and learn a better grasping model. Our
experiments clearly indicate that a larger amount of data is
helpful in learning a better grasping model. We also show the
importance of multi-stage learning using ablation studies and
compare our approach to several baselines. Real robot testing
is performed to validate our method and show generalization
to grasping unseen objects.
II. RELATED WORK
Object manipulation is one of the oldest problems in the
ﬁeld of robotics. A comprehensive literature review of this
area can be found in , . Early attempts in the ﬁeld
focused on using analytical methods and 3D reasoning for
predicting grasp locations and conﬁgurations – . These
approaches assumed the availability of complete knowledge
of the objects to be grasped, such as the complete 3D model
of the given object, along with the object’s surface friction
properties and mass distribution. However, perception and
inference of 3D models and other attributes such as friction/mass from RGB or RGBD cameras is an extremely
difﬁcult problem. To solve these problems people have
constructed grasp databases , . Grasps are sampled
and ranked based on similarities to grasp instances in a preexisting database. These methods however do not generalize
well to objects outside the database.
Other approaches to predict grasping includes using simulators such as Graspit! , . In these approaches,
one samples grasp candidates and ranks them based on
an analytical formulation. However questions often arise
as to how well a simulated environment mirrors the real
world. , , offer reasons as to why a simulated
environment and an analytic metric would not parallel the
real world which is highly unstructured.
Recently, there has been more focus on using visual
learning to predict grasp locations directly from RGB or
RGB-D images , . For example, uses vision
based features (edge and texture ﬁlter responses) and learns a
logistic regressor over synthetic data. On the other hand, ,
 use human annotated grasp data to train grasp synthesis
models over RGB-D data. However, as discussed above,
large-scale collection of training data for the task of grasp
prediction is not trivial and has several issues. Therefore,
none of the above approaches are scalable to use big data.
Another common way to collect data for robotic tasks is
using the robot’s own trial and error experiences – .
However, even recent approaches such as , only use
a few hundred trial and error runs to train high capacity deep
networks. We believe this causes the network to overﬁt and
often no results are shown on generalizability to new unseen
objects. Other approaches in this domain such as use
reinforcement learning to learn grasp attributes over depth
images of a cluttered scene. However the grasp attributes are
based on supervoxel segmentation and facet detection. This
creates a prior on grasp synthesis and may not be desirable
for complex objects.
Deep neural networks have seen immense success in image
classiﬁcation and object detection . Deep networks
have also been exploited in robotics systems for grasp regression or learning policy for variety of tasks . Furthermore DAgger shows a simple and practical method of
sampling the interesting regions of a state space by dataset
aggregation. In this paper, we propose an approach to scale
up the learning from few hundred examples to thousands of
examples. We present an end-to-end self-supervising staged
curriculum learning system that uses thousands of trial-error
runs to learn deep networks. The learned deep network is
then used to collect greater amounts of positive and hard
negative (model thinks as graspable but in general are not)
data which helps the network to learn faster.
III. APPROACH
We ﬁrst explain our robotic grasping system and how we
use it to collect more than 50K data points. Given these
training data points, we train a CNN-based classiﬁer which
given an input image patch predicts the grasp likelihood for
Overview of how random grasp actions are sampled and executed.
different grasp directions. Finally, we explain our stagedcurriculum learning framework which helps our system to
ﬁnd hard negatives: data points on which the model performs poorly and hence causes high loss with greater back
propagation signal.
Robot Grasping System: Our experiments are carried out
on a Baxter robot from Rethink Robotics and we use ROS
 as our development system. For gripping we use the
stock two ﬁngered parallel gripper with a maximum width
(open state) of 75mm and a minimum width (close state) of
A Kinect V2 is attached to the head of the robot that
provides 1920×1280 resolution image of the workspace(dull
white colored table-top). Furthermore, a 1280 × 720 resolution camera is attached onto each of Baxter’s end effector
which provides rich images of the objects Baxter interacts
with. For the purposes of trajectory planning a stock Expansive Space Tree (EST) planner is used. It should be
noted that we use both the robot arms to collect the data
more quickly.
During experiments, human involvement is limited to
switching on the robot and placing the objects on the table
in an arbitrary manner. Apart from initialization, we have
no human involvement in the process of data collection.
Also, in order to gather data as close to real world test
conditions, we perform trial and error grasping experiments
in cluttered environment. Grasped objects, on being dropped,
at times bounce/roll off the robot workspace, however using
cluttered environments also ensures that the robot always
has an object to grasp. This experimental setup negates the
need for constant human supervision. The Baxter robot is
also robust against break down, with experiments running
for 8-10 hours a day.
Gripper Conﬁguration Space and Parametrization: In
this paper, we focus on the planar grasps only. A planar
grasp is one where the grasp conﬁguration is along and perpendicular to the workspace. Hence the grasp conﬁguration
lies in 3 dimensions, (x, y): position of grasp point on the
surface of table and θ: angle of grasp.
A. Trial and Error Experiments
The data collection methodology is succinctly described
in Fig. 2. The workspace is ﬁrst setup with multiple objects
of varying difﬁculty of graspability placed haphazardly on a
table with a dull white background. Multiple random trials
are then executed in succession.
(a) We use 1.5 times the gripper size image patch to predict
the grasp-ability of a location and the angle at which it can be grasped.
Visualization for showing the grasp location and the angle of gripper for
grasping is derived from . (b) At test time we sample patches at different
positions and choose the top graspable location and corresponding gripper
A single instance of a random trial goes as follows:
Region of Interest Sampling: An image of the table,
queried from the head-mounted Kinect, is passed through
an off-the-shelf Mixture of Gaussians (MOG) background
subtraction algorithm that identiﬁes regions of interest in the
image. This is done solely to reduce the number of random
trials in empty spaces without objects in the vicinity. A
random region in this image is then selected to be the region
of interest for the speciﬁc trial instance.
Grasp Conﬁguration Sampling: Given a speciﬁc region
of interest, the robot arm moves to 25cm above the object.
Now a random point is uniformly sampled from the space in
the region of interest. This will be the robot’s grasp point.
To complete the grasp conﬁguration, an angle is now chosen
randomly in range(0, π) since the two ﬁngered gripper is
symmetric.
Grasp Execution and Annotation: Now given the grasp
conﬁguration, the robot arm executes a pick grasp on the
object. The object is then raised by 20cm and annotated as a
success or a failure depending on the gripper’s force sensor
Images from all the cameras, robot arm trajectories and
gripping history are recorded to disk during the execution of
these random trials.
B. Problem Formulation
The grasp synthesis problem is formulated as ﬁnding a
successful grasp conﬁguration (xS, yS, θS) given an image
of an object I. A grasp on the object can be visualised using
the rectangle representation in Fig. 3. In this paper, we use
CNNs to predict grasp locations and angle. We now explain
the input and output to the CNN.
Sample patches used for training the Convolutional Neural Network.
Input: The input to our CNN is an image patch extracted
around the grasp point. For our experiments, we use patches
1.5 times as large as the projection of gripper ﬁngertips
on the image, to include context as well. The patch size
used in experiments is 380x380. This patch is resized to
227x227 which is the input image size of the ImageNettrained AlexNet .
Output: One can train the grasping problem as a regression
problem: that is, given an input image predict (x, y, θ).
However, this formulation is problematic since: (a) there
are multiple grasp locations for each object; (b) CNNs are
signiﬁcantly better at classiﬁcation than the regressing to a
structured output space. Another possibility is to formulate
this as a two-step classiﬁcation: that is, ﬁrst learn a binary
classiﬁer model that classiﬁes the patch as graspable or
not and then selects the grasp angle for positive patches.
However graspability of an image patch is a function of the
angle of the gripper, and therefore an image patch can be
labeled as both graspable and non-graspable.
Instead, in our case, given an image patch we estimate
an 18-dimensional likelihood vector where each dimension
represents the likelihood of whether the center of the patch
is graspable at 0◦, 10◦, ... 170◦. Therefore, our problem can
be thought of an 18-way binary classiﬁcation problem.
Testing: Given an image patch, our CNN outputs whether
an object is graspable at the center of the patch for the 18
grasping angles. At test time on the robot, given an image,
we sample grasp locations and extract patches which is fed
into the CNN. For each patch, the output is 18 values which
depict the graspability scores for each of the 18 angles. We
select the maximum score across all angles and all patches,
and execute grasp at the corresponding grasp location and
C. Training Approach
preparation: Given a trial experiment datapoint
(xi, yi, θi), we sample 380x380 patch with (xi, yi) being the
center. To increase the amount of data seen by the network,
we use rotation transformations: rotate the dataset patches
by θrand and label the corresponding grasp orientation as
{θi + θrand}. Some of these patches can be seen in Fig. 4
Network Design: Our CNN, seen in Fig. 5, is a standard
network architecture: our ﬁrst ﬁve convolutional layers are
taken from the AlexNet , pretrained on ImageNet.
We also use two fully connected layers with 4096 and 1024
neurons respectively. The two fully connected layers, fc6 and
fc7 are trained with gaussian initialisation.
Loss Function: The loss of the network is formalized as
follows. Given a batch size B, with a patch instance Pi, let
the label corresponding to angle θi be deﬁned by li ∈{0, 1}
and the forward pass binary activations Aji (vector of length
2) on the angle bin j
we deﬁne our batch loss LB as:
δ(j, θi)· softmax(Aji, li)
where, δ(j, θi) = 1 when θi corresponds to jth bin.
Note that the last layer of the network involves 18 binary
layers instead of one multiclass layer to predict the ﬁnal
graspability scores. Therefore, for a single patch, only the
loss corresponding to the trial angle bin is backpropagated.
D. Staged Learning
Given the network trained on the random trial experience
dataset, the robot now uses this model as a prior on grasping.
Image patch
AlexNet Pretrained Parameters
Learnt Parameters
Our CNN architecture is similar to AlexNet . We initialize our convolutional layers from ImageNet-trained Alexnet.
Highly ranked patches from learnt algorithm (a) focus more on the
objects in comparison to random patches (b).
At this stage of data collection, we use both previously
seen objects and novel objects. This ensures that in the next
iteration, the robot corrects for incorrect grasp modalities
while reinforcing the correct ones. Fig. 6 shows how top
ranked patches from a learned model focus more on important regions of the image compared to random patches.
Using novel objects further enriches the model and avoids
over-ﬁtting.
Note that for every trial of object grasp at this stage,
800 patches are randomly sampled and evaluated by the
deep network learnt in the previous iteration. This produces
a 800 × 18 grasp-ability prior matrix where entry (i, j)
corresponds to the network activation on the jth angle bin for
the ith patch. Grasp execution is now decided by importance
sampling over the grasp-ability prior matrix.
Inspired by data aggregation techniques , during training of iteration k, the dataset Dk is given by {Dk} =
{Dk−1, Γdk}, where dk is the data collected using the model
from iteration k−1. Note that D0 is the random grasp dataset
and iteration 0 is simply trained on D0. The importance
factor Γ is kept at 3 as a design choice.
The deep network to be used for the kth stage is trained by
ﬁnetuning the previously trained network with dataset Dk.
Learning rate for iteration 0 is chosen as 0.01 and trained
over 20 epochs. The remaining iterations are trained with a
learning rate of 0.001 over 5 epochs.
IV. RESULTS
A. Training dataset
The training dataset is collected over 150 objects with
varying graspability. A subset of these objects can be seen
in Fig. 7. At the time of data collection, we use a cluttered
Random Grasp Sampling Scenario: Our data is collected in clutter
rather than objects in isolation. This allows us to generalize and tackle tasks
like clutter removal.
table rather than objects in isolation. Through our large
data collection and learning approach, we collect 50K grasp
experience interactions. A brief summary of the data statistics
can be found in Table I.
GRASP DATASET STATISTICS
Data Collection
Grasp Rate
Random Trials
Multi-Staged
B. Testing and evaluation setting
For comparisons with baselines and to understand the
relative importance of the various components in our learning
method, we report results on a held out test set with objects
not seen in the training (Fig. 9). Grasps in the test set are
collected via 3K physical robot interactions on 15 novel and
diverse test objects in multiple poses. Note that this test set
is balanced by random sampling from the collected robot
interactions. The accuracy measure used to evaluate is binary
classiﬁcation i.e. given a patch and executed grasp angle in
the test set, to predict whether the object was grasped or not.
Evaluation by this method preserves two important aspects
for grasping: (a) It ensures that the test data is exactly the
same for comparison which isn’t possible with real robot
experiments. (b) The data is from a real robot which means
methods that work well on this test set should work well on
the real robot. Our deep learning based approach followed
by multi-stage reinforcement yields an accuracy of 79.5%
on this test set. A summary of the baselines can be seen in
Table. II.
We ﬁnally demonstrate evaluation in the real robot settings
for grasping objects in isolation and show results on clearing
a clutter of objects.
C. Comparison with heuristic baselines
A strong baseline is the ”common-sense” heuristic which
is discussed in . The heuristic, modiﬁed for the RGB
image input task, encodes obvious grasping rules:
1) Grasp about the center of the patch. This rule is
implicit in our formulation of patch based grasping.
2) Grasp about the smallest object width. This is implemented via object segmentation followed by eigenvector analysis. Heuristic’s optimal grasp is chosen along
the direction of the smallest eigenvalue. If the test set
executed successful grasp is within an error threshold
of the heuristic grasp, the prediction is a success. This
leads to an accuracy of 53.4%
3) Do not grasp too thin objects, since the gripper doesn’t
close completely. If the largest eigenvalue is smaller
than the mapping of the gripper’s minimum width in
image space, the heuristic predicts no viable grasps;
i.e no object is large enough to be grasped. This leads
to an accuracy of 59.9%
By iterating over all possible parameters (error thresholds
and eigenvalue limits) in the above heuristic over the test
set, the maximal accuracy obtained was 62.11% which is
signiﬁcantly lower than our method’s accuracy. The low
accuracy is understandable since the heuristic doesn’t work
well for objects in clutter.
D. Comparison with learning based baselines
We now compare with a couple of learning based algorithms. We use HoG features in both the following
baselines since it preserves rotational variance which is
important to grasping:
1) k Nearest Neighbours (kNN): For every element in the
test set, kNN based classiﬁcation is performed over
elements in the train set that belong to the same angle
class. Maximal accuracy over varying k (optimistic
kNN) is 69.4%.
2) Linear SVM: 18 binary SVMs are learnt for each of the
18 angle bins. After choosing regularisation parameters
via validation, the maximal accuracy obtained is 73.3%
Novel objects
Seen objects
Fig. 8. Comparison of the performance of our learner over different training
set sizes. Clear improvements in accuracy can be seen in both seen and
unseen objects with increasing amounts of data.
E. Ablative analysis
Effects of data: It is seen in Fig. 8 that adding more data
deﬁnitely helps in increasing accuracy. This increase is more
prominent till about 20K data points after which the increase
Effects of pretraining: An important question is how much
boost does using pretrained network give. Our experiments
suggest that this boost is signiﬁcant: from accuracy of
64.6% on scratch network to 76.9% on pretrained networks.
This means that visual features learnt from task of image
classiﬁcation aides the task of grasping objects.
Effects of multi-staged learning: After one stage of reinforcement, testing accuracy increases from 76.9% to 79.3%.
This shows the effect of hard negatives in training where
just 2K grasps improve more than from 20K random grasps.
However this improvement in accuracy saturates to 79.5%
after 3 stages.
Effects of data aggregation: We notice that without aggregating data, and training the grasp model only with data
from the current stage, accuracy falls from 76.9% to 72.3%.
F. Robot testing results
Testing is performed over novel objects never seen by the
robot before as well as some objects previously seen by the
robot. Some of the novel objects can be seen in Fig. 9.
Re-ranking Grasps: One of the issues with Baxter is the
precision of the arm. Therefore, to account for the imprecision, we sample the top 10 grasps and re-rank them based on
neighborhood analysis: given an instance (P i
topK) of a
top patch, we further sample 10 patches in the neighbourhood
topK. The average of the best angle scores for the
neighbourhood patches is assigned as the new patch score
topK for the grasp conﬁguration deﬁned by (P i
The grasp conﬁguration associated with the largest Ri
then executed. This step ensures that even if the execution of
the grasp is off by a few millimeters, it should be successful.
Grasp Results: We test the learnt grasp model both on novel
objects and training objects under different pose conditions.
A subset of the objects grasped along with failures in
grasping can be seen in Fig. 10. Note that some of the
grasp such as the red gun in the second row are reasonable
but still not successful due to the gripper size not being
compatible with the width of the object. Other times even
COMPARING OUR METHOD WITH BASELINES
Learning based
eigenvalue
Eigenvalue
Optimistic
param. select
Deep Net + Multi-stage
Robot Testing Tasks: At test time we use both novel objects and
training objects with different conditions. Clutter Removal is performed to
show robustness of the grasping model
though the grasp is “successful”, the object falls out due to
slipping (green toy-gun in the third row). Finally, sometimes
the impreciseness of Baxter also causes some failures in
precision grasps. Overall, of the 150 tries, Baxter grasps and
raises novel objects to a height of 20 cm at a success rate of
66%. The grasping success rate for previously seen objects
but in different conditions is 73%.
Clutter Removal: Since our data collection involves objects
in clutter, we show that our model works not only on the
objects in isolation but also on the challenging task of clutter
removal . We attempted 5 tries at removing a clutter of
10 objects drawn from a mix of novel and previously seen
objects. On an average, Baxter is successfully able to clear
the clutter in 26 interactions.
V. CONCLUSION
We have presented a framework to self-supervise robot
grasping task and shown that large-scale trial-error experiments are now possible. Unlike traditional grasping
datasets/experiments which use a few hundred examples
for training, we increase the training data 40x and collect
50K tries over 700 robot hours. Because of the scale of
data collection, we show how we can train a high-capacity
convolutional network for this task. Even though we initialize
using an Imagenet pre-trained network, our CNN has 18M
new parameters to be trained. We compare our learnt grasp
network to baselines and perform ablative studies for a
deeper understanding on grasping. We ﬁnally show our
network has good generalization performance with the grasp
rate for novel objects being 66%. While this is just a small
step in bringing big data to the ﬁeld of robotics, we hope
this will inspire the creation of several other public datasets
for robot interactions.
ACKNOWLEDGMENT
This work was supported by ONR MURI N000141010934
and NSF IIS-1320083.