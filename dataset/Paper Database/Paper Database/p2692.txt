Actions and Attributes from Wholes and Parts
Georgia Gkioxari
UC Berkeley
 
Ross Girshick
Microsoft Research
 
Jitendra Malik
UC Berkeley
 
We investigate the importance of parts for the tasks of
action and attribute classiÔ¨Åcation. We develop a part-based
approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part
detectors are a deep version of poselets and capture parts
of the human body under a distinct set of poses. For the
tasks of action and attribute classiÔ¨Åcation, we train holistic
convolutional neural networks and show that adding parts
leads to top-performing results for both tasks. In addition,
we demonstrate the effectiveness of our approach when we
replace an oracle person detector, as is the default in the
current evaluation protocol for both tasks, with a state-ofthe-art person detection system.
1. Introduction
For the tasks of human attribute and action classiÔ¨Åcation,
it is difÔ¨Åcult to infer from the recent literature if part-based
modeling is essential or, to the contrary, obsolete. Consider
action classiÔ¨Åcation. Here, the method from Oquab et al.
 uses a holistic CNN classiÔ¨Åer that outperforms partbased approaches . Turning to attribute classiÔ¨Åcation, Zhang et al.‚Äôs CNN-based PANDA system shows
that parts bring dramatic improvements over a holistic CNN
model. How should we interpret these results? We aim to
bring clarity by presenting a single approach for both tasks
that shows consistent results.
We develop a part-based system, leveraging convolutional network features, and apply it to attribute and action classiÔ¨Åcation. For both tasks, we Ô¨Ånd that a properly
trained holistic model matches current approaches, while
parts contribute further. Using deep CNNs we establish new
top-performing results on the standard PASCAL human attribute and action classiÔ¨Åcation benchmarks.
Figure 1 gives an outline of our approach. We compute
CNN features on a set of bounding boxes associated with
the instance to classify. One of these bounding boxes corresponds to the whole instance and is either provided by an
oracle or comes from a person detector. The other bounding
)LQH*UDLQHG
&ODVVLILFDWLRQ
5LGLQJ+RUVH
:HDUV76KLUW
*LYHQDQLQVWDQFH
K\SRWKHVLVZHGHWHFWSDUWV
7KHLQVWDQFHDQGLWVSDUWVDUH
IHGLQWRRXUFODVVLILFDWLRQHQJLQH
Figure 1: Schematic overview of our overall approach. (a) Given
an R-CNN person detection (red box), we detect parts using a
novel, deep version of poselets (Section 3).
(b) The detected
whole-person and part bouding boxes are input into a Ô¨Åne-grained
classiÔ¨Åcation engine to produce predictions for actions and attributes (Section 4).
boxes (three in our implementation) come from poselet-like
part detectors.
Our part detectors are a novel ‚Äúdeep‚Äù version of poselets. We deÔ¨Åne three human body parts (head, torso, and
legs) and cluster the keypoints of each part into several distinct poselets. Traditional poselets would then operate as sliding-window detectors on top of low-level gradient
orientation features, such as HOG . Instead, we train a
sliding-window detector for each poselet on top of a deep
feature pyramid, using the implementation of . Unlike
HOG-based poselets, our parts are capable of Ô¨Åring on difÔ¨Åcult to detect structures, such as sitting versus standing legs.
Also, unlike recent deep parts based on bottom-up regions
 , our sliding-window parts can span useful, but inhomogeneous regions, that are unlikely to group together through
a bottom-up process (e.g., bare arms and a t-shrit).
Another important aspect of our approach is task-speciÔ¨Åc
CNN Ô¨Åne-tuning. We show that a Ô¨Åne-tuned holistic model
(i.e., no parts) is capable of matching the attribute classiÔ¨Åcation performance of the part-based PANDA system .
Then, when we add parts our system outperforms PANDA.
This result indicates that PANDA‚Äôs dramatic improvement
from parts comes primarily from the weak holistic classi-
Ô¨Åer baseline used in their work, rather than from the parts
themselves. While we also observe an improvement from
adding parts, our marginal gain over the holistic model is
 
smaller, and the gain becomes even smaller as our network
becomes deeper. This observation suggests a possible trend:
as more powerful convolutional network architectures are
engineered, the marginal gain from explicit parts may vanish.
As a Ô¨Ånal contribution, we show that our system can operate ‚Äúwithout training wheels.‚Äù In the standard evaluation
protocol for benchmarking attributes and actions , an
oracle provides a perfect bounding box for each test instance. While this was a reasonable ‚Äúcheat‚Äù a couple of
years ago, it is worth revisiting. Due to recent substantial
advances in detection performance, we believe it is time
to drop the oracle bounding box at test time. We show,
for the Ô¨Årst time, experiments doing just this; we replace
ground-truth bounding boxes with person detections from a
state-of-the-art R-CNN person detector . Doing so only
results in a modest drop in performance compared to the
traditional oracle setting.
2. Related Work
Low-level image features.
Part-based approaches using
low-level features have been successful for a variety of computer vision tasks. DPMs capture different aspects of an
object using mixture components and deforable parts, leading to good performance on object detection and attribute
classiÔ¨Åcation . Similarly, poselets 
are an ensemble of models which capture parts of an object under different viewpoints and have been used for object detection, action and attribute classiÔ¨Åcation and pose
estimation. Pictorial structures and its variants 
explicitly model parts of objects and their geometric relationship in order to accurately predict their location.
Convolutional network features.
Turning away from
hand-designed feature representations, convolutional networks (CNNs) have shown remarkable results on computer
vision tasks, such as digit recognition and more recently image classiÔ¨Åcation . Girshick et al. 
show that a holistic CNN-based approach performs significantly better than previous methods on object detection.
They classify region proposals using a CNN Ô¨Åne-tuned on
object boxes. Even though their design has no explicit part
or component structure, it is able to detect objects under a
wide variety of appearance and occlusion patterns.
Hybrid feature approaches. Even more recently, a number
of methods incorporate HOG-based parts into deep models, showing signiÔ¨Åcant improvements. Zhang et al. 
use HOG-poselet activations and train CNNs, one for each
poselet type, for the task of attribute classiÔ¨Åcation. They
show a large improvement on the task compared to HOGbased approaches. However, their approach includes a number of suboptimal choices. They use pretrained HOG poselets to detect parts and they train a ‚Äúshallow‚Äù CNN (by
today‚Äôs standards) from scratch using a relatively small
dataset of 25k images. We train poselet-like part detectors
on a much richer feature representation than HOG, derived
from the pool5 layer of . Indeed, show an impressive jump in object detection performance using pool5
instead of HOG. In addition, the task-speciÔ¨Åc CNN that we
use for action or attribute classiÔ¨Åcation shares the architecture of and is initialized by pre-training on the large
ImageNet-1k dataset prior to task-speciÔ¨Åc Ô¨Åne-tuning.
In the same vein, Branson et al. tackle the problem
of bird species categorization by Ô¨Årst detecting bird parts
with a HOG-DPM and then extracting CNN features from
the aligned parts. They experimentally show the superiority of CNN-based features to hand-crafted representations.
However, they work from relatively weak HOG-DPM part
detections, using CNNs solely for classiÔ¨Åcation purposes.
Switching to the person category, HOG-DPM does not generate accurate part/keypoint predictions as shown by ,
and thus cannot be regarded as a source for well aligned
body parts.
Deep parts.
Zhang et al. introduce part-based R-
CNNs for the task of bird species classiÔ¨Åcation. They discover parts of birds from region proposals and combine
them for classiÔ¨Åcation. They gain from using parts and also
from Ô¨Åne-tuning a CNN for the task starting from ImageNet
weights. However, region proposals are not guaranteed to
produce parts. Most techniques, such as , are designed
to generate candidate regions that contain whole objects
based on bottom-up cues. While this approach works for
birds, it may fail in general as parts can be deÔ¨Åned arbitrarily in an object and need not be of distinct color and
texture with regard to the rest of the object. Our slidingwindow parts provide a more general solution. Indeed, we
Ô¨Ånd that the recall of selective search regions for our parts
is 15.6% lower than our sliding-window parts across parts
at 50% intersection-over-union.
Tompson et al. and Chen and Yuille train keypoint speciÔ¨Åc part detectors, in a CNN framework, for human body pose estimation and show signiÔ¨Åcant improvement compared to . Their models assume that all parts
are visible or self-occluded, which is reasonable for the
datasets they show results on. The data for our task contain
signiÔ¨Åcantly more clutter, truncation, and occlusion and so
our system is designed to handle missing parts.
Bourdev et al. introduce a form of deep poselets by
training a network with a cross entropy loss. Their system
uses a hybrid approach which Ô¨Årst uses HOG poselets to
bootstrap the collection of training data. They substitute
deep poselets in the poselet detection pipeline to create person hypothesis. Their network is smaller than 
and they train it from scratch without hard negative mining. They show a marginal improvement over R-CNN for
person detection, after feeding their hypothesis through R-
CNN for rescoring and bounding box regression. Our parts
look very much like poselets, since they capture parts of a
pose. However, we cluster the space of poses instead of
relying on random selection and train our models using a
state-of-the-art network with hard negative mining.
3. Deep part detectors
Figure 2 schematically outlines the design of our deep
part detectors, which can be viewed as a multi-scale fully
convolutional network. The Ô¨Årst stage produces a feature
pyramid by convolving the levels of the gaussian pyramid
of the input image with a 5-layer CNN, similar to Girshick
et al. for training DeepPyramid DPMs. The second
stage outputs a pyramid of part scores by convolving the
feature pyramid with the part models.
3.1. Feature pyramid
Feature pyramids allow for object and part detections
at multiple scales while the corresponding models are designed at a single scale. This is one of the oldest ‚Äútricks‚Äù
in computer vision and has been implemented by slidingwindow object detection approaches throughout the years
 .
Given an input image, the construction of the feature
pyramid starts by creating the gaussian pyramid for the image for a Ô¨Åxed number of scales and subsequently extracting features from each scale. For feature extraction, we use
a CNN and more precisely, we use a variant of the singlescale network proposed by Krizhevsky et al. . More details can be found in . Their software is publicly available, we draw on their implementation.
3.2. Part models
We design models to capture parts of the human body
under a particular viewpoint and pose. Ideally, part models
should be (a) pose-sensitive, i.e. produce strong activations
on examples of similar pose and viewpoint, (b) inclusive,
i.e. cover all the examples in the training set, and (c) discriminative, i.e. score higher on the object than on the background. To achieve all the above properties, we build part
models by clustering the keypoint conÔ¨Ågurations of all the
examples in the training set and train linear SVMs on pool5
features with hard negative mining.
Designing parts
We model the human body with three high-level parts: the
head, the torso and the legs. Even though the pose of the
parts is tied with the global pose of the person, each one has
it own degrees of freedom. In addition, there is a large, yet
not inÔ¨Ånite due to the kinematic constraints of the human
body, number of possible part combinations that cover the
space of possible human poses.
We design parts deÔ¨Åned by the three body areas, head
(H), torso (T) and legs (L).
Assume t ‚àà{H, T, L}
the set of 2D keypoints of the i-th training example corresponding to part t. The keypoints correspond
to predeÔ¨Åned landmarks of the human body. SpeciÔ¨Åcally,
KH = {Eyes, Nose, Shoulders}, KT = {Shoulders, Hips}
and for KL = {Hips, Knees, Ankles}.
For each t, we cluster the set of K(i)
t , i = 1, ..., N, where
N is the size of the training set. The output is a set of clusters Ct = {cj}Pt
j=1, where Pt is the number of clusters for
t, and correspond to distinct part conÔ¨Ågurations
Ct = cluster
We use a greedy clustering algorithm, similar to .
Examples are processed in a random order. An example is
added to an existing cluster if its distance to the center is less
than œµ, otherwise it starts a new cluster. The distance of two
examples is deÔ¨Åned as the euclidean distance of their normalized keypoint distributions. For each cluster c ‚ààCt, we
collect the M closest cluster members to its center. Those
form the set of positive examples that represent the cluster.
From now on, we describe a part by its body part type t and
its cluster index j, with cj ‚ààCt while St,j represents the
set of positive examples for part (t, j).
Figure 3 (left) shows examples of clusters as produced
by our clustering algorithm with œµ = 1 and M = 100. We
show 4 examples for each cluster example. We use the PAS-
CAL VOC 2012 train set, along with keypoint annotations
as provided by , to design and train the part detectors. In
total we obtain 30 parts, 13 for head, 11 for torso and 6 for
Learning part models
For each part (t, j), we deÔ¨Åne the part model to be the vector of weights wt,j which when convolved with a feature
pyramid gives stronger activations near the ground-truth location and scale (right most part of Figure 2).
One could view the whole pipeline shown in Figure 2 as
a fully convolutional model and thus one could train it endto-end, optimizing the weights of the CNN for the pool5
feature extraction and the weights of the part models jointly.
We choose to simplify the problem by decoupling it. We use
the publicly available ImageNet weights of the CNN 
to extract pool5 feature pyramids. Subsequently, we train
linear SVMs for the part models. For each part (t, j) we
train a linear SVM with positives from St,j to obtain model
weights wt,j ‚ààR8√ó8√ó256. We use hard negative mining
from images of no people to train the model.
Figure 3 (right) shows the top few detections of a subset
of parts on PASCAL VOC val 2009 set. Each row shows
activations of a different part, which is displayed at the left
part of the same row.
)RUHDFKOHYHOO
DQGSDUWPRGHOL
LPDJHS\UDPLG
SRROS\UDPLG
SDUWPRGHOVFRUH
&RORULPDJH
7UXQFDWHG6XSHU9LVLRQ
3RROIHDWXUHS\UDPLG
3DUWPRGHOV
3DUWPRGHOVFRUH
Figure 2: Schematic overview of our part detectors. (a) A gaussian pyramid is build from an input image. (b) Each level of the pyramid
is fed into a truncated SuperVision CNN. (c) The output is a pyramid of pool5 feature maps. (d) Each level of the feature pyramid is
convolved with the part models. (e) The output is a pyramid of part model scores
Figure 3: Examples of clusters for the three body areas, head,
torso and legs (left) and their top few detections on PASCAL VOC
val 2009 (right). The Ô¨Årst two rows correspond to cluster examples
for head, the following two for torso and the last two for legs.
Evalutation of part models.
We quantify the performance of our part detectors by computing the average precision (AP) - similar to object detection PASCAL VOC - on
val 2009. For every image, we detect part activations at all
scales and locations which we non-maximum suppress with
a threshold of 0.3 across all parts of the same type. Since
there are available keypoint annotations on the val set, we
are able to construct ground-truth part boxes. A detection
AP (%) œÉ = 0.2 œÉ = 0.3 œÉ = 0.4 œÉ = 0.5
Table 1: AP for each part type on PASCAL VOC val 2009. We
evaluate the part activations and measure AP for different thresholds of intersection-over-union.
is marked as positive if the intersection-over-union with a
ground-truth part box is more than œÉ. In PASCAL VOC,
œÉ is set to 0.5. However, this threshold is rather strict for
small objects, such as our parts. We report AP for various
values of œÉ for a fair assessment of the quality of our parts.
Table 1 shows the results.
Mapping parts to instances.
Since our part models operate independently, we need to group part activations and
link them to an instance in question. Given a candidate region box in an image I, for each part t we keep the highest
scoring part within box
j‚àó= argmax
(x,y)‚ààbox wt,j ‚àóF(x,y)(I),
where F(x,y)(I) is the point in feature pyramid for I corresponding to the image coordinates (x, y). This results in
three parts being associated with each box, as shown in Figure 1. A part is considered absent if the score of the part
activation is below a threshold, here the threshold is set to
In the case when an oracle gives ground-truth bounding
boxes at test time, one can reÔ¨Åne the search of parts even
further. If box is the oracle box in question, we retrieve the
k nearest neighbor instances i = {i1, ..., ik}from the training set based on the L2-norm of their pool5 feature maps
F(¬∑), i.e.
F (box)T F (boxij )
||F (box)||¬∑||F (boxij )||. If Kij are the keypoints for
the nearest examples, we consider the average keypoint locations Kbox =
j=1 Kij to be an estimate of the keypoints for the test instance box. Based on Kbox we can
reduce the regions of interest for each part within box by
only searching for them in the corresponding estimates of
the body parts.
4. Part-based ClassiÔ¨Åcation
In this section we investigate the role of parts for Ô¨Ånegrained classiÔ¨Åcation tasks. We focus on the tasks of action classiÔ¨Åcation (e.g. running, reading, etc.) and attribute
classiÔ¨Åcation (e.g. male, wears hat, etc.). Figure 4 schematically outlines our approach at test time. We start with the
part activations mapped to an instance and forward propagate the corresponding part and instance boxes through a
CNN. The output is a fc7 feature vector for each part as well
as the whole instance. We concatenate the feature vectors
and classify the example with a linear SVM, which predicts
the conÔ¨Ådence for each class (action or attribute).
4.1. System variations
For each task, we consider four variants of our approach
in order to understand which design factors are important.
This approach is our baseline and does not use
part detectors. Instead, each instance is classiÔ¨Åed according
to the fc7 feature vector computed from the instance bounding box. The CNN used for this system is Ô¨Åne-tuned from
an ImageNet initialization, as in , on jittered instance
bounding boxes.
Instance Ô¨Åne-tuning.
This method uses our part detectors. Each instance is classiÔ¨Åed based on concatenated fc7
feature vectors from the instance and all three parts. The
CNN used for this system is Ô¨Åne-tuned on instances, just
as in the ‚Äúno parts‚Äù system. We note that because some
instances are occluded, and due to jittering, training samples may resemble parts, though typically only the head and
torso (since occlusion tends to happen from the torso down).
Joint Ô¨Åne-tuning.
This method also uses our part detectors and concatenated fc7 feature vectors. However, unlike
the previous two methods we Ô¨Åne-tune the CNN jointly using instance and part boxes from each training sample. During Ô¨Åne-tuning the network can be seen as a four-stream
CNN, with one stream for each bounding box. Importantly,
we tie weights between the streams so that the number of
CNN parameters is the same in all system variants. This
design explicitly forces the CNN to see each part box during Ô¨Åne-tuning.
3-way split.
To test the importance of our part detectors, we employ a baseline that vertically splits the instance
bounding box into three (top, middle, and bottom) in order
to simulate crude part detectors. This variation uses a CNN
Ô¨Åne-tuned on instances.
4.2. Action ClassiÔ¨Åcation
We focus on the problem of action classiÔ¨Åcation as de-
Ô¨Åned by the PASCAL VOC action challenge. The task involves predicting actions from a set of predeÔ¨Åned action categories.
Learning details.
We train all networks with backpropagation using Caffe , starting from the ImageNet
weights, similar to the Ô¨Åne-tuning procedure introduced in
 . A small learning rate of 10‚àí5 and a dropout ratio of
50% were used. During training, and at test time, if a part
is absent from an instance then we use a box Ô¨Ålled with the
ImageNet mean image values (i.e., all zeros after mean subtraction). Subsequently, we train linear SVMs, one for each
action, on the concatenated fc7 feature vectors.
In order to make the most of the context in the
image, we rescore our predictions by using the output of
R-CNN for the 20 PASCAL VOC object categories
and the presence of other people performing actions. We
train a linear SVM on the action score of the test instance,
the maximum scores of other instances (if any) and the object scores, to obtain a Ô¨Ånal prediction. Context rescoring is
used for all system variations on the test set.
Table 2 shows the result of our approach on the
PASCAL VOC 2012 test set. These results are in the standard setting, where an oracle gives ground-truth person
bounds at test time. We conduct experiments using two different network architectures: a 8-layer CNN as deÔ¨Åned in
 , and a 16-layer as deÔ¨Åned in . Ours (no parts) is
the baseline approach, with no parts. Ours is our full approach when we include the parts. For the 8-layer network,
we use the CNN trained on instances, while for the 16-layer
network we use the CNN trained jointly on instances and
their parts based on results on the val set (Table 3). For
our Ô¨Ånal system, we also present results when we add features extracted from the whole image, using a 16-layer network trained on ImageNet-1k (Ours (w/ image features)).
We show results as reported by action poselets , a partbased approach, using action speciÔ¨Åc poselets with HOG
features, Oquab et al. , Hoai and Simonyan and
Zisserman , three CNN-based approaches on the task.
The best performing method by uses a 16- and 19-layer
network. Their 16-layer network is equivalent to Ours (no
parts) with 16 layers, thus the additional boost in perfor-
&RORULPDJH
UHJLRQVRISDUWV
)FIHDWXUHV
/LQHDU690
FODVVLILHU
&ODVVSUHGLFWLRQV
Figure 4: Schematic overview of our approach for Ô¨Åne grained classiÔ¨Åcation using parts. (a) We consider regions of part activations. (b)
Each part is forward propagated through a CNN. (c) The output is the fc7 feature vector for each input. (d) The features are concatenated
and fed into linear SVM classiÔ¨Åers. (e) The classiÔ¨Åers produce scores for each class.
mance comes from the 19-layer network. This is not surprising, since deeper networks perform better, as is also evident from our experiments. From the comparison with the
baseline, we conclude that parts improve the performance.
For the 8-layer CNN, parts contribute 3% of mAP, with the
biggest improvement coming from Phoning, Reading and
Taking Photo. For the 16-layer CNN, the improvement from
parts is smaller, 1.7 % of mAP, and the actions beneÔ¨Åted the
most are Reading, Taking Photo and Using Computer. The
image features capture cues from the scene and give an additional boost to our Ô¨Ånal performance.
Table 3 shows results on the PASCAL VOC action val set
for a variety of different implementations of our approach.
Ours (no parts) is the baseline approach, with no parts,
while Ours (3-way split) uses as parts the three horizontal
splits comprising the instance box. Ours (joint Ô¨Åne-tuning)
shows the results when using a CNN Ô¨Åne-tuned jointly on
instances and parts, while Ours (instance Ô¨Åne-tuning) shows
our approach when using a CNN Ô¨Åne-tuned on instances
only. We note that all variations that use parts signiÔ¨Åcantly
outperform the no-parts system.
We also show results of our best system when groundtruth information is not available at test time Ours (R-CNN
bbox). In place of oracle boxes we use R-CNN detections
for person. For evaluation purposes, we associate a R-CNN
detection to a ground-truth instance as following: we pick
the highest scoring detection for person that overlaps more
than 0.5 with the ground truth. Another option would be
to deÔ¨Åne object categories as ‚Äúperson+action‚Äù and then just
follow the standard detection AP protocol. However, this is
not possible because not all people are marked in the dataset
(this is true for the attribute dataset as well). We report numbers on the val action dataset. We observe a drop in performance, as expected due to the imperfect person detector, but
our method still works reasonably well under those circumstances. Figure 5 shows the top few predictions on the test
set. Each block corresponds to a different action.
4.3. Attribute ClassiÔ¨Åcation
We focus on the problem of attribute classiÔ¨Åcation, as de-
Ô¨Åned by . There are 9 different categories of attributes,
such as Is Male, Has Long Hair, and the task involves predicting attributes, given the location of the people. Our approach is shown in Figure 4. We use the Berkeley Attributes
of People Dataset as proposed by .
Learning details.
Similar to the task of action classiÔ¨Åcation, we separately learn the parameters of the CNN and the
linear SVM. Again, we Ô¨Åne-tune a CNN for the task in question with the difference that the softmax layer is replaced by
a cross entropy layer (sum of logistic regressions).
Table 4 shows AP on the test set. We show results of our approach with and without parts, as well as results as reported by Zhang et al. , the state-of-the-art on
the task, on the same test set. With an 8-layer network,
parts improve the performance of all categories, indicating their impact on attribute classiÔ¨Åcation. Also, a network
jointly Ô¨Åne-tuned on instances and parts seems to work signiÔ¨Åcantly better than a CNN trained solely on the instance
boxes. In the case of a 16-layer network, joint Ô¨Åne-tuning
and instance Ô¨Åne-tuning seem to work equally well. The
gain in performance from adding parts is less signiÔ¨Åcant in
this case. This might be because of the already high performance achieved by the holistic network. Interestingly, our
8-layer holistic approach matches the current state-of-theart on this task, PANDA showcasing the importance of
deeper networks and good initialization.
Table 4 also shows the effectiveness of our best model,
namely the jointly Ô¨Åne-tuned 16-layer CNN, when we use
CNN layers Jumping Phoning Playing Instrument Reading Riding Bike Riding Horse Running Taking Photo Using Computer Walking mAP
Action Poselets 
Oquab et al. 
Simonyan & Zisserman 
Ours (no parts)
Ours (no parts)
Ours (w/ image features)
Table 2: AP on the PASCAL VOC 2012 Actions test set. The Ô¨Årst three rows show results of two other methods. Action Poselets is a
part based approach using HOG features, while Oquab et al. , Hoai and Simonyan & Zisserman are CNN based approaches.
Ours (no parts) is the baseline approach of our method, when only the ground truth box is considered, while Ours is the full approach,
including parts. All approaches use ground truth boxes at test time.
CNN layers Jumping Phoning Playing Instrument Reading Riding Bike Riding Horse Running Taking Photo Using Computer Walking mAP
Ours (no parts)
Ours (3-way split)
Ours (instance Ô¨Åne-tuning)
Ours (joint Ô¨Åne-tuning)
Ours (no parts)
Ours (instance Ô¨Åne-tuning)
Ours (joint Ô¨Åne-tuning)
Ours (R-CNN bbox)
Ours (R-CNN bbox)
Table 3: AP on the PASCAL VOC 2012 Actions val set of our approach. Ours (no parts) is our approach without parts. Ours (3-way
split) is our approach when parts are deÔ¨Åned as the three horizontal splits comprising an instance box. Ours (joint Ô¨Åne-tuning) uses a CNN
Ô¨Åne-tuned jointly on the instances and the parts, while Ours (instance Ô¨Åne-tuning) uses a single CNN Ô¨Åne-tuned just on the instance box.
All the above variations of our approach use ground truth information at test time as the object bound. Ours (R-CNN bbox) uses R-CNN
detections for person.
CNN layers Is Male Has Long Hair Has Glasses Has Hat Has T-Shirt Has Long Sleeves Has Shorts Has Jeans Has Long Pants mAP
PANDA 
Ours (no parts)
Ours (3-way split)
Ours (instance Ô¨Åne-tuning)
Ours (joint Ô¨Åne-tuning)
Ours (no parts)
Ours (instance Ô¨Åne-tuning)
Ours (joint Ô¨Åne-tuning)
Ours (R-CNN bbox)
Ours (R-CNN bbox)
Table 4: AP on the test set of the Berkeley Attributes of People Dataset. All approaches on the top use ground truth boxes for evaluation.
Ours (no parts) is the baseline approach with no parts. Ours (3-way split) is a variant of our approach, where parts are deÔ¨Åned as the
three horizontal splits comprising an instance box. Ours (instance Ô¨Åne-tuning) uses a CNN Ô¨Åne-tuned on instance boxes, while Ours (joint
Ô¨Åne-tuning) uses a CNN Ô¨Åne-tuned jointly on instances and parts. We also show the effectiveness of our approach Ours (R-CNN bbox),
when no ground truth boxes are given at test time.
R-CNN detections instead of ground truth boxes on the
Berkeley Attributes of People test set. Figure 7 shows the
top few predictions on the test set. Each block corresponds
to a different attribute. Figure 6 shows top errors for two of
our lowest performing attribute classes.
Figure 6: Top errors of classiÔ¨Åcation for two of the attribute categories, Has Glasses (top) and Has T-Shirt (bottom).
Figure 5: Top action predictions on the test set. Different blocks correspond to different actions.
Figure 7: Top attribute predictions on the test set. Each block corresponds to a different attribute