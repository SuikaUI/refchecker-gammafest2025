Optimal Bayesian Design Applied
to Logistic Regression Experiments
Kathryn Chaloner and Kinley Larntz
School of Statistics
University of Minnesota
Technical Report No. 483
December 1986
UNIVERSITY OF MINNESOTA
SCHOOL OF STATISTICS
b';;;, i.rr
Optimal Bayesian Design Applied
to Logistic Regression Experiments
Kathryn Chaloner and Kinley Larntz
School of Statistics
University of Minnesota
Technical Report No. 483
December 1986
A traditional way to design a binary response experiment is to design the
experiment to be most efficient for a best guess of the parameter values.
design which is optimal for a best guess however may not be efficient for
parameter values close to that best guess.
We propose designs which formally
account for the prior uncertainty in the parameter values.
A design for a
situation where the best guess has substantial uncertainty attatched to it is
very different from a design for a situation where approximate values of the
parameters are known.
We derive a general theory for concave design criteria
for non-linear models and then apply the theory to logistic regression.
numerical algorithms are described, an algorithm using the Nelder and Mead
version of the simplex algorithm, which requires that the number of design
points be specified, and an algorithm like the Wynn-Federov algorithm for linear
design. The Wynn-Federov algorithm is much the slower of the two. Designs found
by the Nelder-Mead algorithm are examined for a range of prior distributions and
a range of criteria.
The theoretical results are used to verify that the
designs are indeed optimal~ A general finding is that as the uncertainty in the
prior distribution increases so does the number of support points in the optimal
KEY WORDS: Algorithm; A-optimality; Binary data; D-optimality; Equivalence
theorem; •Nelder-Mead algorithm; Optimization; Simplex method.
Kathryn Chaloner is Assistant Professor, Department of Applied Statistics,
University of Minnesota, St Paul MN 55108.
Kinley Larntz is Director and
Professor, Program in Statistics, Washington State University, Pullman WA
99164-6212.
The research of Kathryn Chaloner was supported by a single quarter
leave award from the University of Minnesota.
1. Introduction
Designing an experiment for a binary response is important in the biological
sciences for bioassay experiments and in the engineering sciences for
reliability testing. Ye will focus specifically on a logistic regression
experiment.
A logistic regression experiment is just one example of a non-linear design
problem. The non-linear problem is intrinsically much harder than the linear
problem as the efficiency of a design depends on the unknown parameters. A
common approach is to design an experiment to be efficient for a best guess of
the parameter values. This approach leads to what are termed "locally optimal"
designs, the term being introduced by Chernoff(l953). A natural generalization
of this approach is to use a prior distribution on the unknown parameters rather
than a single guess and it is this approach we use here and justify using
Bayesian arguments.
Non-Bayesian design for logistic regression is discussed at length in
Finney and in Abdelbasit and Plackett . Various Bayesian approaches
are given in Tsutakawa , Zacks , Freeman and Owen .
There is a large literature on sequential design for binary response models, see
for example Yu , but we will not consider the sequential approach in this
paper. In many experimental situations sequential design is impractical or too
expensive, ).
In this paper we first present a unifying theory of optimal Bayesian design
for non-linear problems. This theory parallels the theory for linear problems as
presented in Silvey(l980) and Fedorov . The theory is seen to be valid,
under reg~larity conditions, for general concave optimality criteria. The
general theory developed in section 2 discusses an equivalence theorem of
Yhittle(l973) and shows how the theorem applies to Bayesian design problems.
The theorem also provides a method of verifying whether or not a particular
design is optimal, or close to optimal.
In section 3 we discuss the logistic
regression model and derive expressions for the directional derivatives needed
to use the theorem. Readers interested in the specific results on how to design
a logistic regression experiment may wish to omit these sections and go straight
to section 4 where numerical algorithms are discussed.
Two concave criteria will be used. The first criterion corresponds to
maximizing the average over the prior distribution of the log of the determinant
of the information matrix. This maximizes, approximately, the expected increase
in Shannon information provided by the experiment as suggested by Lindley(l956)
and given a decision theoretic basis by Bernardo(l979). It was used by
Ll:iuter for mixtures of linear models. This criterion is equivalent to
D-optimality in linear design.
The second criterion corresponds to minimizing
the approximate expected posterior variance of the quantities of interest. This
criterion requires the experimenter to specify what is to be estimated or what
is to be predicted and the relative importance of these predictions. A weighted
trace of the information matrix is then averaged over the prior distribution
and minimized. In linear problems this criterion is called A-optimality.
In section 4 we discuss two algorithm developed for finding these designs. The
first algorithm is an implementation. of the version of the simplex algorithm
developed by Nelder and Mead(l965). This algorithm finds the best design for a
fixed number of design points.
The second algorithm is suggested by the theory
of section 2 and is an algorithm similar to that of Wynn and
Fedorov . The•first algorithm is much faster than the second and probably
the most useful in practice. The second algorithm is useful for verifying that a
design obtained from the first algorithm is indeed optimal, or almost so, and
that the number of design points is sufficient.
In section 5 an experiment to investigate these designs for a range of prior
distributions is described and the results discussed. The most noticeable result
is that as the uncertainty in the prior distribution increases so does the
number of support points required. In section 6 the designs from section 5 are
compared and contrasted to the corresponding locally optimal designs. The
locally optimal designs can be very inefficient in terms of the Bayesian
criteria. In addition the locally D-optimal design takes observations on only
two design points and does not allow for any test of the adequacy of the model.
We also examine whether designs that maximize the average log determinant of the
information matrix are generally efficient in terms of minimizing average
variances. In section 7 we examine a few designs in more detail.
Tsutakawa and Abdelbasit and Plackett restrict designs to
take observations at equally spaced design points with equal numbers of
observations at each point. No such restrictions are made here.
The theory of optimal Bayesian design
2.1 The criteria
To introduce the notation, suppose that an experiment is to be designed by
choosing n values of the design variable x from an experimental region X. At
each of then values of xi an independent observation yi will be available. Let
the data be denoted y
(y1 , ... ,yn) and the unknown parameters
(0 1 , ... ,Ok).
The contribution to the likelihood for a single observation
y. made at xis p(y. 10,x).
For most models using exact posterior distributions is intractable and
asymptotic arguments must be used. Under easily satisfied assumptions the
posterior distribution of O is approximately a multivariate normal distribution
with mean the maximum likelihood estimate, 0, and variance covariance matrix the
inverse of the observed Fisher information matrix 1(0). Recall that this is the
matrix of second derivatives of the log likelihood function evaluated at the
maximum likelihood estimate of 0. The estimated expected Fisher information
matrix could also be used but is generally less accurate.
These approximations
are given, for example, in Berger(l985) page 224.
By expanding the definition of a design to include any probability measure ry
on X we define the normalized matrix I(O,~) by
The posterior distribution of O using a design measure~ is therefore
approximately multivariate normal Nk(O,(nl(O,~))
Preposterior expected losses can therefore be approximated using the prior
distribution of O for the predictive distribution of 0.
The first criterion we
consider is to choose the design measure~ maximizing ~1(~) where
For a design measure~ for which I(O,~) is singular for O values of
non-zero prior probability we define ~1(~) to be-~.
The second criterion is ~2(~). which requires that the parameters to be
estimated are specified and possibly weighted. The criterion is
where B(O) is a symmetric k by k matrix. If linear combinations of the 0. are of
interest then B(O) does not depend on O and is a matrix of known weights. If
non-linear combinations of the Oi's are of interest then B(O) has entries which
are functions of 0.
We now define H to be the set of all probability measures on X and view the
design problem as finding a measure in H that maximizes;(~). We will only
consider concave functions; on Hand if we assume His compact there will be an
optimal design measure in H. Both ; 1 and ; 2 are concave on H for logistic
regression experiments and many other design problems.
2.2 General equivalence theorem
In many linear design problems the information matrix does not depend on 0
and there is a compact set of possible information matrices to optimize over.
The set is a finite dimensional subset of Euclidean space. The general
equivalence theorems of optimal design are usually stated in terms of this set
and directional derivatives defined on this set. This is the approach in
Silvey(l980). For the non-linear problem we need to optimize over the set of
measures H directly and define directional derivatives on H. Whittle(l973) uses
this approach and it is this version of the general equivalence theorem we need
to invoke.
For two measures ~land ~2 in H the derivative at ~l in the direction of ry 2
is denoted by F(~1 .~2) and is defined, when the limit exists, by
F(~1 .~2) - lim E (~((1-E)~l + E~2) - ~(~1)) .
The extreme points of Hare the measures which put point mass at a single point
We denote such a measure ~x· If F(~1 .~2) is linear in ~2 then~ is said
to be differentiable and
We further define d(~,x) to represent F(~.~x).
Theorem 1 of Whittle is usually used in the context of linear design
but we use it here for non-linear design. This theorem forms the basis for a
numerical algorithm used later and is reproduced below in our notation.
Theorem )
a) If~ is concave, then a ~-optimal design ~O can be equivalently characterized
by any of the three conditions
~o maximized ~(~)
~o minimizes sup d(~,x)
sup d(~0 ,x) - o.
b) The point <~o.~o> is a saddle point of Fin that
c) If~ is differentiable, then the support of ~O is contained in the set of x
for which d(~0 ,x)-O almost everywhere in ~O measure.
The proof is given by Whittle and requires only that His compact and that the
derivatives are continuous in x.
The algorithms and results of linear optimal design theory therefore apply.
The only results from linear theory not available are those which require the
existence of a finite dimensional set of information matrices. We cannot
therefore guarantee that there is an an optimal design on at most k(k+l)/2
support points as we can in linear problems. Furthermore we cannot even
guarantee that there is an optimal design on a finite number of support points.
2.3 The criteria ~land ~2
The two criteria ~land ~2 are now examined and their derivatives derived.
If log det 1(8,~) and -tr B(8) 1(8,~)
are both concave functions over H almost
everywhere in 8, for the prior measure on 8, then ~land ~2 are concave
functions on H.
This lemma can be proved easily by using the concavity for each 8 and then
taking expectation over 8. As expectation is a linear operator the concavity is
preserved.
For many non-linear design problems, including logistic regression,
we have concavity for each 8.
For measures~ such that 1(8,~) is non-singular almost everywhere in the prior
measure on 8 we have
F(~1.~2) - E8tr
The proof of this lemma is straightforward, paralleling results for linear
theory (see, for example, Silyey(l980) pages 21 and 48). As expectation is a
linear operator the expressions are the expectations of those if 8 were known.
We will specifically exclude from consideration situations where the optimal
design ~O is singular for 9 values of positive prior measure. This condition is
not particularly restrictive for f 1-optimality which implicitly assumes that all
the parameters are of interest. This condition .does however exclude some cases
of f 2-optimality where there are nuisance parameters but it does not exclude
logistic regression experiments where there are only two parameters.
We now proceed to use these general results for designing logistic regression
experiments.
3. The logistic regression design problem
The logistic regression model is such that, for an observation at a value x
of the explanatory variable, a Bernoulli response yi is observed. There are two
parameters 9
(µ,p) and the probability of success is
p(x,9) - 1/(1 + exp(-P(x-µ))
The model is a generalized linear model with the logit transformation of the
expected response being linear in x. The parameterµ is the value of x at which
the probability of success is .5 and pis the slope in the logit scale. The
parametersµ and P can be thought of as representing the location and scale of
an underlying tolerance distribution.
We will assume that Xis closed and bounded and therefore H will be compact.
We also suppose that the support of the prior measure is bounded. This
requirement insures non-singularity of 1(9,~) for all measures~ except for
those corresponding to point mass at a particular x. These conditions are
sufficient for the results of Section 2 to be applied to logistic regression.
For a design measure, ~. on X putting ni weight at xi' i-1, .. k, Ini-1, define
wi - p(xi,9)(1-p(xi,9))
t - l niwi
Note that wi, t, x ands all depend on 9, but this dependence has been dropped
to simplify the notation.
The information matrix can be written as
1(9,~) - ('it
-.BtCx - JJ>
-.Bt<x - JJ>
s + t<x - µ) 2
The inverse of the information matrix is therefore
11: + ex -
,8(x - µ)/s
,8(x - µ)/s)
The ~1-criterion and several versions of ~2-optimality will now be examined
and expressions will be found for the criteria and their derivatives.
3.1 21-optimality
For ~1-optimality, maximizing the average log determinant, the expressions
simplify to give
and if we define
w(x,9) - p(x,9)(1 - p(x,9))
For any design~. we see that if lxl ➔~then w(x,9) ➔ 0 at an exponential rate,
and therefore d(~,x) ➔ -2. In Section 2 it was shown that at points of support
of the optimal design this derivative must be exactly zero. For sufficiently
wide X, the design points of the optimal design will therefore not be at the
boundary of X. Rather if ~O is the optimal design the support points will be at
the roots of the function d(~O,x).
3.2 ~2-optimality in general
The weighted trace criterion of ~2-optimality is perhaps more appealing than
~1-optimality as it corresponds to the familiar squared error loss function. It
requires that the experimenter carefully specify exactly what is to be estimated
or predicted.
The specification of what is to be estimated provides the matrix B(9). If,
for example, the only parameter of interest isµ, then B(9) can be written as
cc with c-(1,O) . If bothµ and pare of equal interest then B(9) is the
identity matrix.
It is often of interest, especially in bioassay experiments, to estimate the
value of x at which the probability of success, p(x,8), is a particular value.
Suppose that we wish to estimate x0 where logit(p(x0 ,8))
which is a non-linear function of 8. Standard asymptotic arguments give
B(8) - c(8)c(8)T where c(8) is a vector of derivatives of the non-linear
function. Specifically, c(8)
(l,-7~ ). If several percentile response
points are of interest then a distribution could be put on 7 and B(8) averaged
over this distribution. This is a standard way of using the weighted trace
criterion in a Bayesian framework (see, for example, Chaloner(l984), for this
approach in linear models).
We implement the design algorithm for three choices of B(8).
choice is whereµ is the only parameter of interest. The second is where the
only parameter of interest is the value x such that p(x,8) -
.95. The third is
when we are interested in estimating the value of x at which logit(p(x,8)) -
and 7 has a uniform distribution over [-1,1]. This range on 7 gives
probabilities between .23 and .87. These three choices are by no means
exhaustive of the many which might occur in real experimental situations but
they represent a range of criteria which might be appropriate in particular
experiments.
These three examples of ~2-optimality will be examined and optimal designs
found for a range of prior distributions. The criterion functions and
derivatives are now derived for these examples of ~2-optimality. Recall that the
derivative involves the weighted trace of the matrix 1(8,~)
1(8,~x) 1(8,~)
Expressions for the entries of this matrix are given in the Appendix.
3.3 ~2-optimality for the estimation of u
When interest is in the estimation ofµ alone then the (1,1) entry of B(O) is
1 and all other entries are zero. For any design '7, with more than one support
E6{w(x,6)(,8st)
[t(x - x)(x - µ) + s) } + 4,2 (11)
Estimatingµ alone is of interest in some biological applications, the parameter
µ is often referred to as the LDSO as it is the 50th percentile of the
underlying logistic tolerance distribution.
3.4 ~2-optimality for any or several percentile response points
Following the earlier discussion in 3.2, to estimateµ+ 7,8-l we have
E6{w(x,6)(,8 st) [t(x - x)(,B(x - µ) - 7) + .8 s] }
Substituting 7 - 0 gives the expressions in 3.3 for the special case of
estimatingµ alone. Substituting 7-2.944 gives the situation where we want to
estimate the value of x at which p(x,9)-.95. This value of xis referred to as
the I.095.
If a distribution is put over 7 to represent interest in several percentile
response points then define
The expectation is over the distribution on 7. An example where 7 has a uniform
distribution on [-1,1) will later be used.
In this case E(7)-0 and E(72)- 1/3
and the criterion and derivative become
d(~.x) - E9(w(x,9)(P st)
[p (t(x-x)(x-µ) + s)
+ t (x-x) 3
A uniform (-1,1) distribution on 7 represents interest in calibrating the
central part of the logistic response curve.
4. Two algorithms for finding optimal designs
Ye have used two basic algorithms to find optimal designs. The most
effective seems to be to use the Nelder and Mead(l965) version of the simplex
algorithm for optimizing a function of several variables. This method is a
numerical optimization method which does not require derivatives. This method
works well, especially if the number of dimensions is less than about 10. It
does require that the number of design points is specified.
The algorithm also
requires that there are no constraints on the variables and so the variables
must be transformed to satisfy this.
By evaluating the derivative d(~,x) over
possible values of x it is possible to verify whether or not a design obtained
by this Nelder-Mead algorithm is indeed optimal, or very close to optimal and
whether or not a sufficient number of design points have been used.
The second algorithm parallels the Wynn-Fedorov algorithm developed by
Wynn and Fedorov . A sequence of measures (~i} is generated by
(l - ai) ~i + ai ~x(i+l)
where x(i+l) is the value of x maximizing d(~i,x). Therefore at each stage of
the algorithm first x(i+l) is chosen and then ai is chosen. This algorithm is
straightforward to program.
The choice of the region Xis not very important for either algorithm if the
region is large enough to give optimal design points in the interior not on the
boundary. If a design~ is optimal for a particular X and the derivative d(~.x)
is negative outside of X the value of¢ cannot be improved upon by expanding the
The implementation of these two algorithms will now be described. Both
implementations used the same numerical integration routine. The numerical
integration routine was the routine adapt from the core math library from the
National Bureau of Standards (cmlib), which was edited to use double precision
arithmetic. All calculations were performed on a Vax 11/750.
4.2 The Nelder-Mead algorithm
The fortran subroutine of O'Neil1 was used. The algorithm is for
unconstrained minimization and the problem must therefore be transformed.
Suppose that we wish to find the optimal design on k points. Let the design
points be xi constrained to lie in X-[a,b] and the design weights be ni which
are constrained to sum to 1. The transformed problem is an unconstrained problem
in 2k-1 variables denoted by z1 , ....... ,z2k-l" Define
and d - w/(a+b+2e) .
Then the transformations used were:
zi - tan (d(xi - c)}
i - 1,2 ... ,k
zk+i - log (ni/°k}
i - 1,2 ... ,k-1
After this transformation using the algorithm is straightforward.
The output of the algorithm can be examined and the values possibly rounded.
For example if the optimal design is in fact on less thank points then one or
both of two things may happen. The algorithm may produce two or more x.'s very
close together or some of the optimal ni's may be extremely small.
The starting designs used in the implementation were usually uniform on k
equally spaced points.
4.3 The Wynn-Fedorov algorithm
This algorithm was implemented for some examples but the results of using it
are not reported here as the Nelder-Mead algorithm was so much more efficient.
We describe the algorithm however as it is an effective, if slow, method of
finding a design. It is extremely easy to program.
This algorithm was implemented by dividing X into a number of points and at
each stage of the algorithm evaluating the derivative at each point in order to
choose x(i+l). There are several options for the choice of step lengths, Qi.
Wynn's approach is to use 1/i and Fedorov's approach is to choose the Q. which
maximizes the increase in~ (for D-optimality a closed form expression for this
Q. can be found). Fedorov's algorithm gives therefore a monotonically increasing
criterion function whereas Wynn's step lengths do not. A combination of these
two was actually implemented by starting with an Qi of 1/i and if this choice of
Qi gave an increase in the criterion then it was used, otherwise Qi was
decreased by one half until it did give an increase (up to a maximum of 20
decreases). In this way we obtain a monotone sequence. The algorithm was stopped
when the maximum derivative was less than .001. Setting the convergence
criterion at a smaller value of the maximum derivative was impractical as the
algorithm could be very slow and take many hours of CPU time, especially for
prior distributions which represent considerable uncertainty.
This implementation could be improved upon by using a more efficient way of
choosing x(i+l) and perhaps a different way of choosing Qi.
The Nelder-Mead
algorithm was much faster for the examples we examined.
4.4 Symmetry
Both of these algorithms were modified to be much more efficient for
situations where the optimal design measure is symmetric. Symmetry occurs for
example when the prior distribution forµ is independent of that for p and the
prior distribution forµ is symmetric around some value, µ0 say, at the center
of the interval X. In addition the criterion must be invariant to which of the
Bernoulli responses is labelled "success" and which "failure"; the ~l criterion
is always so.
Specifically, for any design~ let~* be the design which is the reflection
of~ around µ0 and suppose that~ is such that~(~)-~(~*). Then by concavity the
symmetric design (~(x)+~*(x))/2 is at least as good as~- Therefore for every
asymmetric design~ there is a symmetric design which is at least as good.
The symmetric version of the algorithms are implemented by setting µ0 to zero
and X-[-a,a] . The design points and the design weights are both symmetric
around zero.
The symmetric version of the Nelder-Mead algorithm is a optimization O'rer
only k-1 variables. If k is odd, one design point is fixed at zero and th3 other
design points are at x and -x for (k-1)/2 values of x. The design weight at xis
identical to that at -x. If k is even there are k/2 values of x and -x and k/2
weights subject to one constraint. The reduction of dimensions by a facto~ of 2
is very important, especially when the optimal design is on about 10 poin~s.
In the Yynn-Fedorov algorithm the number of derivatives that need to be
evaluated at each.stage is reduced by one half. In this formulation the set of
measures His taken to be the set of symmetric measures an X and the extreme
points~ taken to be measures with mass .5 at x and .5 at -x.
As with the non-symmetric versions of the algorithm judicious rounding can
simplify the designs obtained from implementing these algorithms.
The symmetric
versions are clearly much faster than the asymmetric versions when they are
appropriate.
5.A numerical investigation
A numerical investigation using the 'Nelder-Mead algorithm will now be
described. Designs were found for a range of prior distributions and a range of
criteria. It was assumed thatµ and p were independent in the prior distr~bution
and each had a uniform distribution on an interval. Three different intervals
were chosen for each parameter and all nine combinations were used to>generate
nine different prior distributions in a 3x3 factorial structure. The intervals
forµ were all centered at zero and were [-.1,.1], [-.3,.3] and [-1.0,1.0]. The
intervals for p were [6.9,7.1], [6.0,8.0] and [4.0,10.0] and so they were all
centered at 7. These intervals were motivated by the example in Brown(l966) and
represent a range of prior distributions from quite precise to quite vague. Even
the vague prior distribution, however, recognizes that the sign of Pis known
and a range is known forµ. This knowledge would be reasonable to assume in many
real experimental situations.
For each of the nine prior distributions optimal designs were found, using
the Nelder-Mead algorithm, for each value of k between 2 and 11, (recall that k
is the number of support points).
Four criteria were used, ~1-optimality and
three versions of ~2-optimality which were: first estimating the LOSO (1-0),
second estimating LD95 (1-2.944) and third estimating the values of x where the
logit of the probability of response is 1 and 1 has a uniform distribution over
[-1,1]. For ~1-optimality and for the ~2-optimality criteria other than
estimating LD95 the optimal design is symmetric and therefore for these three
criteria the symmetric version of the algorithm was used.
The design space X was chosen to be wide enough so that the optimal design
points did not occur on the boundary.
The starting designs were uniformly
spaced design points with uniform weights.
The results of this experiment are summarized in the next few sections.
5.1 i 1-optimality
Table 1 highlights some of the results for maximizing the average log
determinant. The table gives the maximum value of the determinant after rounding
to 4 decimal places on a maximum of 11 support points. The table also gives the
minimum number of support points which gave this value after rounding.
Examination of Table 1 reveals that as our uncertainty increases so does the
minimum number of support points required to attain the optimal value. Forµ
uniform on [-.1,.1) optimal designs for all three prior distributions on pare
on only k-2 points. Forµ uniform on [-.3,.3) the optimal design is on k-3
points and forµ uniform on [-1.0,1.0] more than 7 points are needed.
designs appear to be much more sensitive to the prior distribution onµ than on
These designs are displayed graphically in figure 1 as probability mass
functions on [-1,1).
5.2 ~2-optimality for the estimation of u
Table 2 gives the corresponding results for minimizing the expected variance
ofµ. Again, _as for ~1-optimality, the more uncertainty there is in the prior
distribution the more support points are needed. Figure 2 is a graphical display
of the designs and we can clearly see the number of support points increasing as
the uncertainty increases.
5.3 ~2-optimality for the estimation of LD95
In engineering problems it may well be an extreme percentile, such as the
point where the probability of response is .95, that is of interest. We
therefore examine the ~2-optimal designs for this problem. Unlike the two
previous examples the optimal design is not necessarily symmetric about the
prior mean forµ.
Table 3 is a table derived using the same method as for tables 1 and 2. These
designs are illustrated in figure 3. In figure 3 the designs are illustrated as
measures on the interval [-1.2,1.2] but are otherwise on the same scale as the
other figures.
Note that the central plot in the figure corresponding toµ
uniform on [-.3,.3] and p uniform on , is supposed to be a 4 point design
but is graphically indistinguishable from a 3 point design. Although the design
found has mass .55 at .426 and mass .20 at .425, we have drawn the design with
mass .75 at .4265.
We examine this design again in section 7 and we will see
that a plot of the derivative function indicates that the optimal design should
probably have 3 support points.
The designs in table 3 are interesting. They are clearly not symmetric and
can be quite different depending on the prior distribution.
We noted that the
criterion function to be optimized in this case does not appear to be as well
behaved as for the other criteria studied and could well be causing irregular
behavior in the optimization algorithm.
5.4 ~2-optimality for the average percentile response point
As described earlier we used a criterion designed to minimize the average
variance of µ+~p-l where~ has a uniform distribution over [-1,1].
That is we
are interested in calibrating the central part of the response curve. The
results are summarized in table 4 and figure 4, using the same method of
rounding to 4 decimal places. The designs are symmetric around zero and all are
within the interval [-1,1].
Again we see that as prior uncertainty increases
so does the number of support points.
5.5 The locally optimal design
In these examples the best guess for (µ,p) would be (0,7). The locally ~1optimal design is to put weight .5 at each of +.2205 and -.2205. These are the
points at which the probability p(x,8), for these values of the parameters 8,
are .176 and .824.
5.6 General conclusions
In general as the uncertainty in the prior distribution increases so does the
number of support points. This result is in accord with the results of
Tsutakawa and Abdelbasit and Plackett(l983) who restricted their
designs to be equally spaced and have equal weights. The designs we found make
no such restriction and indeed the designs, in general, are neither equally
spaced nor have equal weight on all design points.
A feature common to all of the four design criteria was that as the
uncertainty in the prior distribution increased so did the length of time the
Nelder-Mead algorithm took to converge.
In Section 7 designs for two prior
distributions will be examined in more detail and we will see that as the
uncertainty in the prior distribution increases the derivative function d(~ 1 x)
of a design close to optimal can be very flat compared to the derivative
function for a very informative prior distribution.
This is consistent with the
criterion value being fairly constant in the region close to the optimal value.
6. Is the ~1-optimal design really "all purpose"?
A claim often made for D-optimality is that it is an all purpose criterion
and using this criterion to design experiments should lead to generally
i~formative experiments.
A natural question to ask is whether this is, in fact,
Table 5, therefore, gives the efficiency of the locally D-optimal design,
as described in section 5.5, in terms of the three ~2-optimality criteria and
Table 6 the efficiencies of the ~1-optimal designs, described in section 5.1.
The entries are the value of the ~2-criterion divided by the value at the
optimal designs as given in Tables 2, 3 and 4.
Under all three criteria the locally optimal design performs abismally when
the prior distribution onµ is uniform on [-1,l].
The ~1-optimal design,
alternatively, takes account of the uncertainty in the parameters.
~1-optimal design is, however, inefficient for estimating the LD95.
In summary, for these prior distributions, the ~1-optimality criterion does
seem to be reasonably efficient in terms of the two symmetric ~2-optimality
criteria. If the purpose of the experiment is clearly specified, however, in
terms of ~2-optimality then ~2-optimality should be used.
7. Two examples
In order to further understand these designs we take two particular prior
distributions and study the optimal designs in detail. Ye take the prior
distribution where pis between 6 and 8 and two choices of the prior
distribution onµ, namelyµ is between -.3 and .3 and between -1 and 1.
two prior distributions represent a moderate amount of prior information and
quite vague prior information. These two examp~es illustrate the difference
caused by changing the prior distribution onµ.
First consider the prior distribution representing a moderate amount of
information,µ uniform on (-.3,.3] and, independently, p uniform on (6,8]. That
the ~1-optimal design in table 7 is indeed optimal can be verified by plotting
the derivative function, d(~,x). The plot is given in figure 5.1 and it is clear
that the function is negative almost everywhere and has exactly three roots at
the design points. Plots are also given of the corresponding derivatives for the
criteria for the other three optimal designs in 5.2, 5.3
and 5.4. Again, the
derivative functions are not exactly everywhere negative, but almost so. For
~1-optimality and for estimating the LD95 the roots are clearly seen on the plot
and correspond almost exactly to the support points. Recall that the LD95
optimal design was on four points with two very close together. The derivative
plot indicates that only 3 points should be required. The other two criteria,
~2-optimality for the LOSO and for the average of percentile response points
give very similar derivative functions. The functions do have maxima at,
approximately, the design points but they are hard to discern from the plot as
the function is so flat.
In contrast, now consider the prior distribution, with vaguer prior
information, whereµ is uniform on [-1,1] and pis again uniform on . The
designs found in section 5 for the four different criteria are given in table 8.
Figure 6 gives the corresponding plot of the derivative function d(~,x), again
for values of x between -1.2 and 1.2. In comparison to figure 5 we see that the
derivative is much flatter in all four cases. The flatness of the function makes
it difficult to see the roots.
As the function is so flat changing the design a
little can change the roots considerably, but change the criterion value very
little. Nevertheless, the Nelder-Mead algorithm has clearly succeeded in finding
a design which is close to optimal under each of the four criteria.
8.Conclusion
The Nelder-Mead algorithm is efficient in finding good design which are close
to optimal using these Bayesian criteria.
The results of section 2 are useful
in understanding the problem and in verifying that the designs obtained from the
Nelder-Mead algorithm are indeed close to optimal and a sufficient number of
support points are being used.
The prior distributions used in this paper, uniform and independent prior
distributions forµ and p, clearly do not always apply. The methodology we have
presented, however, applies to any situation where bounds are available on the
parameter space. We believe that we have demonstrated that this methodology is
preferable to designing for a single best guess of the parameter values. We also
believe that this methodology provides a framework for the careful examination
of any experimental procedure in which the data analysis will include fitting a
logistic regression model.
These design are easy to derive. Even if an experimenter can not use one of
these design exactly they provide a useful benchmark with which to calibrate
other designs. They promise to be quite useful as the number of support points
is generally greater than the number of parameters and therefore the designs
enable the appropriateness of the model to be examined after the experiment is
performed. This methodology is clearly effective for the important problem of
experimental d~sign for logistic regression and may well be useful in other nonlinear design problems.
Denote the matrix A by
A - I(O,~)-l I(O,~) I(O,~)-l
then the derivative for the ~2-criterion is
The entries of the matrix A can be written as the following:
A11 - w(x,O)(pst)
[t(x - x)(x - µ) + s]
A12 - w(x,O)(pts)
(x - x)[t(x - x)(x - µ) + s]
A22 - w(x,O)(x - x) s
These expressions simplify the algebra required to find the derivatives for
different choices of B(O).