ETH Library
One-Shot Video Object
Segmentation
Conference Paper
Author(s):
Caelles, Sergi; Maninis, Kevis-Kokitsi; Pont-Tuset, Jordi; Leal-Taixé, Laura; Cremers, Daniel; Van Gool, Luc
Publication date:
Permanent link:
 
Rights / license:
In Copyright - Non-Commercial Use Permitted
Originally published in:
 
This page was generated automatically upon download from the ETH Zurich Research Collection.
For more information, please consult the Terms of use.
One-Shot Video Object Segmentation
S. Caelles1,*
K.-K. Maninis1,∗
J. Pont-Tuset1
L. Leal-Taix´e2
D. Cremers2
L. Van Gool1
1ETH Z¨urich
2TU M¨unchen
Figure 1. Example result of our technique: The segmentation of the ﬁrst frame (red) is used to learn the model of the speciﬁc object to
track, which is segmented in the rest of the frames independently (green). One every 20 frames shown of 90 in total.
This paper tackles the task of semi-supervised video object segmentation, i.e., the separation of an object from the
background in a video, given the mask of the ﬁrst frame.
We present One-Shot Video Object Segmentation (OSVOS),
based on a fully-convolutional neural network architecture
that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and ﬁnally to learning the appearance of a single annotated object of the test sequence (hence one-shot).
Although all frames are processed independently, the results are temporally coherent and stable. We perform experiments on two annotated video segmentation databases,
which show that OSVOS is fast and improves the state of the
art by a signiﬁcant margin (79.8% vs 68.0%).
1. Introduction
From Pre-Trained Networks...
Convolutional Neural Networks (CNNs) are revolutionizing many ﬁelds of computer vision. For instance, they
have dramatically boosted the performance for problems
like image classiﬁcation and object detection . Image segmentation has also been taken
over by CNNs recently , with deep architectures pre-trained on the weakly related task of image classi-
ﬁcation on ImageNet . One of the major downsides of
deep network approaches is their hunger for training data.
Yet, with various pre-trained network architectures one may
ask how much training data do we really need for the speciﬁc problem at hand? This paper investigates segmenting
an object along an entire video, when we only have one single labeled training example, e.g. the ﬁrst frame.
*First two authors contributed equally
...to One-Shot Video Object Segmentation
This paper presents One-Shot Video Object Segmentation (OSVOS), a CNN architecture to tackle the problem
of semi-supervised video object segmentation, that is, the
classiﬁcation of all pixels of a video sequence into background and foreground, given the manual annotation of one
(or more) of its frames. Figure 1 shows an example result
of OSVOS, where the input is the segmentation of the ﬁrst
frame (in red), and the output is the mask of the object in
the 90 frames of the sequence (in green).
The ﬁrst contribution of the paper is to adapt the CNN to
a particular object instance given a single annotated image
(hence one-shot). To do so, we adapt a CNN pre-trained on
image recognition to video object segmentation. This
is achieved by training it on a set of videos with manually
segmented objects. Finally, it is ﬁne-tuned at test time on a
speciﬁc object that is manually segmented in a single frame.
Figure 2 shows the overview of the method. Our proposal
tallies with the observation that leveraging these different
levels of information to perform object segmentation would
stand to reason: from generic semantic information of a
large amount of categories, passing through the knowledge
of the usual shapes of objects, down to the speciﬁc properties of a particular object we are interested in segmenting.
The second contribution of this paper is that OSVOS processes each frame of a video independently, obtaining temporal consistency as a by-product rather than as the result of
an explicitly imposed, expensive constraint. In other words,
we cast video object segmentation as a per-frame segmentation problem given the model of the object from one (or
various) manually-segmented frames. This stands in contrast to the dominant approach where temporal consistency
plays the central role, assuming that objects do not change
too much between one frame and the next.
Such methods adapt their single-frame models smoothly throughout
 
Results on frame N
of test sequence
Base Network
Pre-trained on ImageNet
Parent Network
Trained on DAVIS training set
Test Network
Fine-tuned on frame 1 of test sequence
Figure 2. Overview of OSVOS: (1) We start with a pre-trained base CNN for image labeling on ImageNet; its results in terms of segmentation, although conform with some image features, are not useful. (2) We then train a parent network on the training set of DAVIS; the
segmentation results improve but are not focused on an speciﬁc object yet. (3) By ﬁne-tuning on a segmentation example for the speciﬁc
target object in a single frame, the network rapidly focuses on that target.
the video, looking for targets whose shape and appearance
vary gradually in consecutive frames, but fail when those
constraints do not apply, unable to recover from relatively
common situations such as occlusions and abrupt motion.
In this context, motion estimation has emerged as a
key ingredient for state-of-the-art video segmentation algorithms . Exploiting it is not a trivial task however, as one e.g. has to compute temporal matches in the
form of optical ﬂow or dense trajectories , which can be
an even harder problem.
We argue that temporal consistency was needed in the
past, as one had to overcome major drawbacks of the then
inaccurate shape or appearance models. On the other hand,
in this paper deep learning will be shown to provide a sufﬁciently accurate model of the target object to produce temporally stable results even when processing each frame independently. This has some natural advantages: OSVOS
is able to segment objects through occlusions, it is not limited to certain ranges of motion, it does not need to process
frames sequentially, and errors are not temporally propagated. In practice, this allows OSVOS to handle e.g. interlaced videos of surveillance scenarios, where cameras can
go blind for a while before coming back on again.
Our third contribution is that OSVOS can work at various points of the trade-off between speed and accuracy.
In this sense, it can be adapted in two ways. First, given
one annotated frame, the user can choose the level of ﬁnetuning of OSVOS, giving him/her the freedom between a
faster method or more accurate results. Experimentally, we
show that OSVOS can run at 181 ms per frame and 71.5%
accuracy, and up to 79.7% when processing each frame in
7.83 s. Second, the user can annotate more frames, those
on which the current segmentation is less satisfying, upon
which OSVOS will reﬁne the result. We show in the experiments that the results indeed improve gradually with more
supervision, reaching an outstanding level of 84.6% with
two annotated frames per sequence, and 86.9% with four,
from 79.8% from one annotation.
Technically, we adopt the architecture of Fully Convolutional Networks (FCN) , suitable for dense
predictions. FCNs have recently become popular due to
their performance both in terms of accuracy and computational efﬁciency . Arguably, the Achilles’ heel
of FCNs when it comes to segmentation is the coarse scale
of the deeper layers, which leads to inaccurately localized
predictions.
To overcome this, a large variety of works
from different ﬁelds use skip connections of larger feature
maps , or learnable ﬁlters to improve upscaling . To the best of our knowledge, this work is the
ﬁrst to use FCNs for the task of video segmentation.
We perform experiments on two video object segmentation datasets (DAVIS and Youtube-Objects )
and show that OSVOS signiﬁcantly improves the state of
the art 79.8% vs 68.0%. Our technique is able to process a
frame of DAVIS (480×854 pixels) in 102 ms. By increasing
the level of supervision, OSVOS can further improve its results to 86.9% with just four annotated frames per sequence,
thus providing a vastly accelerated rotoscoping tool.
All resources of this paper, including training and testing code, pre-computed results, and pre-trained models
are publicly available at www.vision.ee.ethz.ch/
˜cvlsegmentation/osvos/.
2. Related Work
Video Object Segmentation and Tracking:
Most of the
current literature on semi-supervised video object segmentation enforces temporal consistency in video sequences to
propagate the initial mask into the following frames. First of
all, in order to reduce the computational complexity some
works make use of superpixels , patches ,
or even object proposals . M¨arki et al. cast the
problem into a bilateral space in order to solve it more ef-
ﬁciently. After that, an optimization using one of the previous aggregations of pixels is usually performed; which
can consider the full video sequence , a subset of
frames , or only the results in frame n to obtain the mask
in n + 1 . As part of their pipeline, some of the
methods include the computation of optical ﬂow ,
which considerably reduces speed. Concurrent works have
also used deep learning to address Video Object Segmentation. MaskTrack learns to reﬁne the detected masks
frame by frame, by using the detections of the previous
frame, along with Optical Flow and post-processing with
CRFs. In , the authors combine training of a CNN with
ideas of bilateral ﬁltering. Different from those approaches,
OSVOS is a simpler pipeline which segments each frame
independently, and produces more accurate results, while
also being signiﬁcantly faster.
In the case of visual tracking (bounding boxes instead
of segmentation) Nam and Han use a CNN to learn a
representation of the object to be tracked, but only to look
for the most similar window in frame n+1 given the object
in frame n. In contrast, our CNN learns a single model from
frame 1 and segments the rest of the frames from this model.
FCNs for Segmentation:
Segmentation research has
closely followed the innovative ideas of CNNs in the last
few years.
The advances observed in image recognition have been beneﬁcial to segmentation in
many forms (semantic , instance- level ,
biomedical , generic , etc.). Many of the current
best performing methods have in common a deep architecture, usually pre-trained on ImageNet, trainable end-to-end.
The idea of dense predictions with CNNs was pioneered
by and formulated by in the form of Fully Convolutional Networks (FCNs) for semantic segmentation. The
authors noticed that by changing the last fully connected
layers to 1 × 1 convolutions it is possible to train on images of arbitrary size, by predicting correspondingly-sized
outputs. Their approach boosts efﬁciency over patch-based
approaches where one needs to perform redundant computations in overlapping patches. More importantly, by removing the parameter-intensive fully connected layers, the
number of trainable parameters drops signiﬁcantly, facilitating training with relatively few labeled data.
In most CNN architectures , activations of
the intermediate layers gradually decrease in size, because
of spatial pooling operations or convolutions with a stride.
Making dense predictions from downsampled activations
results in coarsely localized outputs . Deconvolutional
layers that learn how to upsample are used in .
In , activations from shallow layers are gradually injected into the prediction to favor localization. However,
these architectures come with many more trainable parameters and their use is limited to cases with sufﬁcient data.
Following the ideas of FCNs, Xie and Tu separately
supervised the intermediate layers of a deep network for
contour detection. The duality between multiscale contours
and hierarchical segmentation was further studied by
Maninis et al. by bringing CNNs to the ﬁeld of generic
image segmentation. In this work we explore how to train
an FCN for accurately localized dense prediction based on
very limited annotation: a single segmented frame.
3. One-Shot Deep Learning
Let us assume that one would like to segment an object in
a video, for which the only available piece of information is
its foreground/background segmentation in one frame. Intuitively, one could analyze the entity, create a model, and
search for it in the rest of the frames. For humans, this very
limited amount of information is more than enough, and
changes in appearance, shape, occlusions, etc. do not pose
a signiﬁcant challenge, because we leverage strong priors:
ﬁrst “It is an object,” and then “It is this particular object.”
Our method is inspired by this gradual reﬁnement.
We train a Fully Convolutional Neural Network (FCN)
for the binary classiﬁcation task of separating the foreground object from the background. We use two successive
training steps: First we train on a large variety of objects,
ofﬂine, to construct a model that is able to discriminate the
general notion of a foreground object, i.e., “It is an object.”
Then, at test time, we ﬁne-tune the network for a small number of iterations on the particular instance that we aim to
segment, i.e., “It is this particular object.” The overview of
our method is illustrated in Figure 2.
3.1. End-to-end trainable foreground FCN
Ideally, we would like our CNN architecture to satisfy
the following criteria:
1. Accurately localized segmentation output, as discussed in Section 2.
2. Relatively small number of parameters to train from a
limited amount of annotation data.
3. Relatively fast testing times.
We draw inspiration from the CNN architecture of ,
originally used for biomedical image segmentation. It is
based on the VGG network, modiﬁed for accurately
localized dense prediction (Point 1). The fully-connected
layers needed for classiﬁcation are removed (Point 2), and
efﬁcient image-to-image inference is performed (Point 3).
The VGG architecture consists of groups of convolutional
plus Rectiﬁed Linear Units (ReLU) layers grouped into 5
stages. Between the stages, pooling operations downscale
the feature maps as we go deeper into the network. We connect convolutional layers to form separate skip paths from
the last layer of each stage (before pooling). Upscaling operations take place wherever necessary, and feature maps
from the separate paths are concatenated to construct a volume with information from different levels of detail. We
linearly fuse the feature maps to a single output which has
the same dimensions as the image, and we assign a loss
function to it. The proposed architecture is shown in Figure 4 (1), foreground branch.
The pixel-wise cross-entropy loss for binary classiﬁcation (we keep the notation of Xie and Tu ) is in this case
deﬁned as:
yjlogP (yj=1|X;W)+(1−yj)log (1−P (yj=1|X;W))
logP (yj=1|X;W) −
logP (yj=0|X; W)
where W are the standard trainable parameters of a CNN,
X is the input image, yj ∈0, 1, j = 1, .., |X| is the pixelwise binary label of X, and Y+ and Y−are the positive
and negative labeled pixels. P(·) is obtained by applying a
sigmoid to the activation of the ﬁnal layer.
In order to handle the imbalance between the two binary
classes, Xie and Tu proposed a modiﬁed version of the
cost function, originally used for contour detection (we drop
W for the sake of readability):
logP (yj=1|X) −(1−β)
logP (yj=0|X)
where β = |Y−|/|Y |. Equation 1 allows training for imbalanced binary tasks .
3.2. Training details
Ofﬂine training:
The base CNN of our architecture 
is pre-trained on ImageNet for image labeling, which has
proven to be a very good initialization to other tasks . Without further training, the network
is not capable of performing segmentation, as illustrated in
Figure 2 (1). We refer to this network as the “base network.”
We therefore further train the network on the binary
masks of the training set of DAVIS, to learn a generic notion of how to segment objects from their background, their
usual shapes, etc.
We use Stochastic Gradient Descent
(SGD) with momentum 0.9 for 50000 iterations. We augment the data by mirroring and zooming in. The learning
rate is set to 10−8, and is gradually decreased. After ofﬂine
training, the network learns to segment foreground objects
from the background, as illustrated in Figure 2 (2). We refer
to this network as the “parent network.”
Online training/testing:
With the parent network available, we can proceed to our main task (“test network” in
Figure 2 (3)): Segmenting a particular entity in a video,
given the image and the segmentation of the ﬁrst frame. We
proceed by further training (ﬁne-tuning) the parent network
for the particular image/ground-truth pair, and then testing
on the entire sequence, using the new weights. The timing
of our method is therefore affected by two times: the ﬁnetuning time (once per annotated mask) and the segmentation of all frames (once per frame). In the former we have a
Figure 3. Qualitative evolution of the ﬁne tuning: Results at 10
seconds and 1 minute per sequence.
trade-off between quality and time: the more iterations we
allow the technique to learn, the better results but the longer
the user will have to wait for results. The latter does not depend on the training time: OSVOS is able to segment each
480p frame (480 × 854) in 102 ms.
Regarding the ﬁne-tuning time, we present two different modes: One can either need to ﬁne-tune online, by segmenting a frame and waiting for the results in the entire
sequence, or ofﬂine, having access to the object to segment
beforehand. Especially in the former mode, there is the need
to control the amount of time dedicated to training: the more
time allocated for ﬁne-tuning, the more the user waits and
the better the results are. In order to explore this trade-off,
in our experiments we train for a period between 10 seconds
and 10 minutes per sequence. Figure 3 shows a qualitative
example of the evolution of the results’ quality depending
on the time allowed for ﬁne-tuning.
In the experiments section, Figure 8 quantiﬁes this evolution. Ablation analysis shows that both ofﬂine and online
training are crucial for good performance: If we perform
our online training directly from the ImageNet model, the
performance drops signiﬁcantly. Only dropping the online
training for a speciﬁc object also yields a signiﬁcantly worse
performance, as already transpired from Figure 2 (2).
3.3. Contour snapping
In the ﬁeld of image classiﬁcation , where
our base network was designed and trained, spatial invariance is a design choice: no matter where an object appears
in the image, the classiﬁcation result should be the same.
This is in contrast to the accurate localization of the object contours that we expect in (video) object segmentation.
Despite the use of skip connections to minimize the loss of spatial accuracy, we observe that OSVOS’s
segmentations have some room for improvement in terms
of contour localization. We propose two different strategies
to improve the results in this regard.
First, we propose the use of the Fast Bilateral Solver
(FBS) to snap the background prediction to the image edges.
It performs a Gaussian smoothing in the
ﬁve-dimensional color-location space, which results in a
smoothing of the input signal (foreground segmentation)
that preserves the edges of the image. It is useful in practice
because it is fast (≈60 ms per frame), and it is differentiable
so it can be included in an end-to-end trainable deep learn-
Boundary Snapping
Snap the foreground mask to accurate contours
Foreground Branch
Speciﬁc object - Less accurate contours
Contour Branch
Accurate contours - Generic objects
Figure 4. Two-stream FCN architecture: The main foreground
branch (1) is complemented by a contour branch (2) which improves the localization of the boundaries (3).
ing architecture. The drawback of this approach, though, is
that it preserves naive image gradients, i.e. pixels with high
Euclidean differences in the color channels.
To overcome this limitation, our second approach snaps
the results to learned contours instead of simple image gradients. To this end, we propose a complementary CNN in a
second branch, that is trained to detect object contours. The
proposed architecture is presented in Figure 4: (1) shows
the main foreground branch, where the foreground pixels
are estimated; (2) shows the contour branch, which detects
all contours in the scene (not only those of the foreground
object). This allows us to train ofﬂine, without the need
to ﬁne-tune on a speciﬁc example online. We used the exact same architecture in the two branches, but training for
different losses. We noticed that jointly training a network
with shared layers for both tasks rather degrades the obtained results thus we kept the computations for the two objectives uncorrelated. This allows us to train the contour
branch only ofﬂine and thus it does not affect the online
timing. Since there is need for high recall in the contours,
we train on the PASCAL-Context database, which provides contour annotations for the full scene of an image.
Finally, in the boundary snapping step (Figure 4 (3), we
compute superpixels that align to the computed contours (2)
by means of an Ultrametric Contour Map (UCM) ,
which we threshold at a low value. We then take a foreground mask (1) and we select superpixels via majority voting (those that overlap with the foreground mask over 50%)
to form the ﬁnal foreground segmentation.
In this second case, we trade accuracy for speed, since
the snapping process takes longer (400 ms instead of 60 ms
per frame), but we achieve more accurate results. Both re-
ﬁnement processes result in a further boost in performance,
and are fully modular, meaning that depending on the requirements one can choose not to use them, sacriﬁcing accuracy for execution time, since both modules come with a
small, yet avoidable computational overhead.
4. Experimental Validation
Databases, state-of-the-art, and measures:
part of our experiments is done on the recently-released
DAVIS database , which consists of 50 full-HD video
sequences with all of their frames segmented with pixellevel accuracy. We use three measures: region similarity in
terms of intersection over union (J ), contour accuracy (F),
and temporal instability of the masks (T ). All evaluation
results are computed on the validation set of DAVIS.
We compare to a large set of state-of-the-art methods, including two very recent semi-supervised techniques,
OFL , BVS , as well as the methods originally
compared on the DAVIS benchmark: FCP , JMP ,
HVS , SEA , and TSP . We also add the unsupervised techniques: FST , SAL , KEY , MSG ,
TRC , CVOS , and NLC . We add two informative bounds: the quality that an oracle would reach
by selecting the best segmented object proposal out of two
state-of-the-art techniques (COB and MCG ), and
by selecting the best superpixels from COB (COB|SP).
For completeness, we also experiment on Youtubeobjects , manually segmented by Jain and Grauman . We compare to OFL , BVS , LTV ,
HBT , AFS , SCF , and JFS and take the
pre-computed evaluation results from previous work.
Ablation Study on DAVIS:
To analyze and quantify the
importance and need of each of the proposed blocks of
our algorithm, Table 1 shows the evaluation of OSVOS
compared to ablated versions without each of its building
blocks. Each column shows: the original method without
boundary snapping (-BS), without pre-training the parent
network on DAVIS (-PN), or without performing the oneshot learning on the speciﬁc sequence (-OS). In smaller and
italic font we show the loss (in blue) or gain (in red) on each
metric with respect to our ﬁnal approach.
We can see that both the pre-training of the parent network and the one-shot learning play an important role (we
lose 15.2 and 27.3 points in J without them, respectively).
Removing both, i.e., using the Imagenet raw CNN, the results in terms of segmentation (J =17.6%) are completely
random. The boundary snapping adds 2.4 points of im-
Recall O ↑
Recall O ↑
Table 1. Ablation study on DAVIS: Comparison of OSVOS
against downgraded versions without some of its components.
Semi-Supervised
Unsupervised
Recall O ↑
Recall O ↑
Table 2. DAVIS Validation: OSVOS versus the state of the art, and practical bounds.
Figure 5. Error analysis of our method: Errors divided into False
Positives (FP-Close and FP-Far) and False Negatives (FN). Values
are total error pixels relative to the error in the -BS case.
provement, and is faster than conventional methods such as
adding a CRF on top of the segmentation .
Figure 5 further analyzes the type of errors that OSVOS
produces (with and without boundary snapping), by dividing them into False Positives (FP) and False Negatives (FN).
FP are further divided into close and far, setting the division
at 20 pixels from the object. We can observe that the majority of the errors come from false negatives. Boundary
snapping mainly reduces the false positives, both the ones
close to the boundaries (more accurate contours) and the
spurious detections far from the object, because they do not
align with the trained generic contours.
Comparison to the State of the Art on DAVIS:
compares OSVOS to the rest of the state of the art. In terms
of region similarity J , OSVOS is 11.8 points above the second best technique and 19.8 above the third best. In terms of
contour accuracy F, OSVOS is 17.2 and 21.8 points above
them. Our results are better than those obtained by an oracle selecting the best object proposal from the state-of-theart object proposals COB. Even if the oracle would select
the best set of superpixels to form each mask (COB|SP),
OSVOS would be only 6.7 points below.
Table 3 shows an evaluation with respect to different attributes annotated in the DAVIS dataset, by comparing the
performance of the methods on the sequences with a given
attribute (challenge) versus the performance on those without it. OSVOS has the best performance on all attributes,
and it has a signiﬁcant resilience to these challenges (smallest decrease of performance when the attribute is present numbers in italics).
Figure 6 shows the results per sequence compared to the
Attribute-based performance: Quality of the techniques on sequences with a certain attribute and the gain with
respect to this quality in the sequences without it (in italics and
smaller font). See DAVIS for the meaning of the acronyms.
state of the art. OSVOS has the best performance in the majority of sequences and is very close to the best in the rest.
The results are especially impressive in sequences such as
Drift-Chicane or Bmx-Trees, where the majority of techniques fail. Figure 7 shows the qualitative results on these
two sequences. In the ﬁrst row, the problem is especially
challenging because of the smoke and the small initial size
of the car. In the second row, OSVOS’ worse sequence,
despite vastly outperforming the rest of techniques. In this
case, OSVOS loses track of the biker when he is occluded,
but recovers when he is visible again. The rest of techniques
lose the object because of the heavy occlusions.
Number of training images (parent network):
To evaluate how much annotated data are needed to retrain a parent network, Table 4 shows the performance of OSVOS (-
BS) when using a subset of the DAVIS train set. We randomly selected a ﬁxed percentage of the annotated frames
in each video. We conclude that by using only ~200 anno-
Training data
Quality (J )
Table 4. Amount of training data: Region similarity (J ) as a
function of the number of training images. Full DAVIS is 2079.
tated frames, we are able to reach almost the same performance than when using the full DAVIS train split, thus not
requiring full video annotations for the training procedure.
The computational efﬁciency of video object
segmentation is crucial for the algorithms to be usable in
OSVOS can adapt to different timing requirements, providing progressively better results the more time
Car-Roundabout
Car-Shadow
Drift-Chicane
Motocross-Jump
Horsejump-High
Scooter-Black
Breakdance
Drift-Straight
Dance-Twirl
Paragliding-Launch
Figure 6. DAVIS Validation: Per-sequence results of region similarity (J ).
Drift-Chicane
Figure 7. Qualitative results: First row, an especially difﬁcult sequence which OSVOS segments well. Second row, OSVOS’ worst result.
we can afford, by letting the ﬁne-tuning algorithm at test
time do more or fewer iterations. To show this behavior,
Figure 8 shows the quality of the result with respect to the
time it takes to process each 480p frame. As introduced before, OSVOS’ time can be divided into the ﬁne-tuning time
plus the time to process each frame independently.
ﬁrst mode we evaluate is -OS-BS ( ), in which we do not
ﬁne-tune to the particular sequence, and thus use the parent
network directly. In this case, the quality is not very good
(although comparable to some previous techniques), but we
only need to do a forward pass of the CNN for each frame.
Time per frame (s)
Region similarity (J )
Figure 8. Quality versus timing: Region similarity with respect
to the processing time per frame.
To take into account the ﬁne-tuning time, we can consider two scenarios.
First, in Ours (
) or -BS (
we average the ﬁne-tuning time (done once per sequence)
over the length of that sequence. This way, the curves show
the gain in quality with respect to the ﬁne-tuning time, plus
the forward pass on each frame. Using the same notation
than in the ablation study, the two different curves refer to
whether we do not perform boundary snapping (-BS) or we
snap to the learned contours (Ours). The better results come
at the price of adding the snapping cost so depending on the
needed speed, one of the two can be chosen.
Since OSVOS processes frames independently, one
could also perform the ﬁne-tuning ofﬂine, by training on a
picture of the object to be segmented beforehand (e.g. take
a picture of a racing horse before the race). In this scenario,
OSVOS can process each frame by one forward pass of the
CNN (Ours Pre , -BS Pre ), and so be considerably fast.
Compared to other techniques, OSVOS is signiﬁcantly
faster and/or more accurate at all regimes, from fast modes:
74.7 versus 60.0 of BVS ( ) at 400 ms, and 79.8 versus 68.0
of OFL ( ) at lower speeds.
Reﬁnement of results:
Another advantage of our technique is that we can naturally incorporate more supervision
in the form of more annotated frames. In a production environment, for instance, one needs a certain quality below
which results are not usable. In this scenario, OSVOS can
provide the results with one annotated frame, then the operator can decide whether the quality is good enough, and if
not, segment another frame. OSVOS can then incorporate
that knowledge into further ﬁne-tuning the result.
To model this scenario, we take the results with N manual annotations, select the frame in which OSVOS performs worse, similarly to what an operator would do, i.e.
select a frame where the result is not satisfactory; and add
the ground-truth annotation into the ﬁne-tuning. Table 5
shows the evolution of the quality when more annotations
are added (0 means we test the parent network directly,
i.e. zero-shot). We can see that the quality signiﬁcantly increases from one to two annotations and saturates at around
Annotations
Quality (J )
Table 5. Progressive reﬁnement: Quality achieved with respect
to the number of annotated frames OSVOS trains from.
(a) Annotated frame 0
(c) Annotated frame 88
(e) Annotated frame 46
(b) Result frame 35
(d) Result frame 35
(f) Result frame 35
Figure 9. Qualitative incremental results: The segmentation on
frame 35 improves after frames 0, 88, and 46 are annotated.
ﬁve. As a measure of the upper bound of OSVOS, we ﬁnetuned on all annotated frames and tested on the same ones
(last column), which indeed shows us that ﬁve annotated
frames almost get the most out of this architecture.
Figure 9 shows a qualitative example of this process,
where the user annotates frame 0, where only one camel
is visible (a). In frame 35, OSVOS also segments the second camel that appears (b), which has almost the exact same
appearance. This can be solved (f) by annotating two more
frames, 88 (c) and 46 (e), which allows OSVOS to learn
the difference between these two extremely similar objects,
even without taking temporal consistency into account.
Evaluation as a tracker:
Video object segmentation
could also be evaluated as a Visual Object Tracking
(VOT) algorithm, by computing the bounding box
around each of the segmentations. We compare to the winner of the VOT Challenge 2015 : MDNET . Since
we cannot compare in the original dataset of the VOT Challenge (the ground-truth objects are not segmented so we
cannot ﬁne-tune on it), we run MDNET on DAVIS. Table 6 shows the percentage of bounding boxes coming from
each technique that have an intersection over union with the
ground-truth bounding box above different thresholds. The
higher the threshold, the more alignment with the ground
truth is required. We can see that OSVOS has signiﬁcant
better results as tracker than MDNET at all regimes, with
more margin at higher thresholds.
Results on Youtube-Objects:
For completeness, we also
do experiments on Youtube-objects , where we take
the pre-computed evaluation from other papers. Table 7
shows that we perform slightly better than the state of the art
OFL, which is signiﬁcantly slower, and despite the fact that
the sequences in this database have signiﬁcant less occlu-
MDNET 
Table 6. Evaluation as a tracker: Percentage of bounding boxes
that match with the ground truth at different levels of overlap.
Table 7. Youtube-Objects evaluation: Per-category mean intersection over union (J ).
sions and motion than in DAVIS, which favors techniques
that enforce temporal consistency.
5. Conclusions
Deep learning approaches often require a huge amount
of training data in order to solve a speciﬁc problem such
as segmenting an object in a video. Quite in contrast, human observers can solve similar challenges with only a single training example. In this paper, we demonstrate that
one can reproduce this capacity of one-shot learning in a
machine: Based on a network architecture pre-trained on
generic datasets, we propose One-Shot Video Object Segmentation (OSVOS) as a method which ﬁne-tunes it on
merely one training sample and subsequently outperforms
the state-of-the-art on DAVIS by 11.8 points. Interestingly,
our approach does not require explicit modeling of temporal consistency using optical ﬂow algorithms or temporal
smoothing and thus does not suffer from error propagation
over time (drift). Instead, OSVOS processes each frame of
the video independently and gives rise to highly accurate
and temporally consistent segmentations. All resources of
this paper can be found at www.vision.ee.ethz.ch/
˜cvlsegmentation/osvos/
Acknowledgements:
Research funded by the EU Framework Programme for Research and Innovation Horizon
2020 (Grant No. 645331, EurEyeCase), the Swiss Commission for Technology and Innovation (CTI, Grant No.
19015.1 PFES-ES, NeGeVA), and the ERC Consolidator
Grant “3D Reloaded”. The authors gratefully acknowledge
support by armasuisse and thank NVidia Corporation for
donating the GPUs used in this project.