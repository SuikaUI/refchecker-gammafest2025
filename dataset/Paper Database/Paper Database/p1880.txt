Deep Compositional Captioning:
Describing Novel Object Categories without Paired Training Data
Lisa Anne Hendricks1
Subhashini Venugopalan3
Marcus Rohrbach1,2
Raymond Mooney3
Kate Saenko4
Trevor Darrell1,2
1 UC Berkeley, 2 ICSI, Berkeley, 3 UT Austin, 4 UMass Lowell
While recent deep neural network models have achieved
promising results on the image captioning task, they rely
largely on the availability of corpora with paired image and sentence captions to describe objects in context.
In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions
of novel objects which are not present in paired imagesentence datasets. Our method achieves this by leveraging
large object recognition datasets and external text corpora
and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora,
despite the fact that they are pre-trained with large object
recognition datasets, namely ImageNet. In contrast, our
model can compose sentences that describe novel objects
and their interactions with other objects. We demonstrate
our model’s ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show
qualitative results on ImageNet images of objects for which
no paired image-sentence data exist. Further, we extend
our approach to generate descriptions of objects in video
clips. Our results show that DCC has distinct advantages
over existing image and video captioning approaches for
generating descriptions of new objects in context.
1. Introduction
In the past year, several deep recurrent neural network
models have demonstrated promising results on the task of
generating descriptions for images and videos . Large corpora of paired images and descriptions,
such as MSCOCO and Flickr30k have been an
important factor contributing to the success of these methods. However, these datasets describe a relatively small variety of objects in comparison to the number of labeled objects in object recognition datasets, such as ImageNet .
Consequently, though modern object recognition systems
Compositional
Paired Image-
Sentence Data
A bus driving
Unpaired Text Data
A otter that is sitting
in the water.
A dog sitting on a
boat in the water.
Unpaired Image Data
Yummy pizza
sitting on the
Otters live in a
variety of
environments.
Pepperoni is a
popular pizza
Figure 1: Existing deep caption methods are unable to generate
sentences about objects unseen in caption corpora (like otter). In
contrast, our model effectively incorporates information from independent image datasets and text corpora to compose descriptions about novel objects without any paired image-sentence data.
have the capacity to recognize thousands of object classes,
existing state-of-the-art caption models lack the ability to
form compositional structures which integrate new objects
with known concepts without explicit examples of imagesentence pairs.
To address this limitation, we propose
the Deep Compositional Captioner (DCC) which can combine visual groundings of lexical units to generate descriptions about objects which are not present in caption corpora (paired image-sentence data), but are present in object
recognition datasets (unpaired image data) and text corpora
(unpaired text data).
DCC builds on recent deep captioning models which combine convolutional and recurrent
networks for visual description. However unlike previous
models which can only describe objects that are present in
paired image-sentence data, DCC is compositional in the
sense that it can seamlessly construct sentences about new
objects by combining them with already seen linguistic expressions in paired training data. To illustrate, consider the
image of the otter in Figure 1. To describe the image ac-
 
curately, any captioning model needs to identify the constituent visual elements such as “otter”, “water” and “sitting” and combine them to generate a coherent sentence.
While previous deep caption models learn to combine visual elements into a cohesive description exclusively from
image and caption pairs, DCC can compose a caption to
describe a new visual element such as the “otter” by understanding that “otters” are similar to “animals” and can
thus be composed in the same way with other lexical expressions.
To effectively describe new objects, our model incorporates two key design elements. First, DCC consists of a
separate lexical classiﬁer and language model, which can
each be trained independently on unpaired image data and
unpaired text data. Additionally, the lexical classiﬁer and
language model can be combined into a deep caption model
which is trained jointly on paired image-sentence data. Second, and crucial for generating compositional captions, is
the multimodal layer where knowledge from known objects
in paired image-sentence datasets can be transferred to new
objects only seen in unpaired datasets. In this work, we
leverage external text corpora to relate novel objects to concepts seen in paired data and propose two mechanisms to
transfer knowledge from known objects to novel objects.
We demonstrate the ability of DCC to generate captions
about new objects by empirically studying results on a training split of the MSCOCO dataset which excludes certain
objects. Qualitatively, we show that our model can be used
to describe a variety of objects in the Imagenet 7k dataset
which are not present in caption datasets. Furthermore, we
demonstrate that the efﬁcacy of DCC is not limited to images, but can also be used to describe new objects in videos
by presenting results on a collection of Youtube video clips.
2. Related Work
Deep Captioning.
In the last year, a variety of models
 have achieved promising results on
the image captioning task. Some follow a CNN-
RNN framework: ﬁrst high-level features are extracted from
a CNN trained on the image classiﬁcation task, and then
a recurrent model learns to predict subsequent words of a
caption conditioned on image features and previously predicted words. Others adopt a multimodal framework in which recurrent language features and image features are embedded in a multimodal space.
The multimodal embedding is then used to predict the caption word
by word. Retrieval methods based on comparing the knearest neighbors of training and test images in a deep image feature space, have also achieved competitive results on
the captioning task. However, retrieval methods are limited
to words and descriptions which appear in a training set of
paired image-sentence data. As opposed to using high level
image features extracted from a CNN, another approach
 is to train classiﬁers on visual concepts such as objects, attributes and scenes. A language model, such as an
LSTM or maximum entropy model , then generates
a visual description conditioned on the presence of classi-
ﬁed visual elements. Our model most closely resembles the
framework suggested in which uses a multimodal space
to combine features from image and language, however our
approach modiﬁes this framework considerably to describe
concepts that are never seen in paired image-sentence data.
Zero-Shot Learning. Zero-shot learning has received substantial attention in computer vision 
since it becomes difﬁcult to obtain sufﬁcient labeled images as the number of object categories grows. In particular, our method draws on previous zero-shot learning work
that mines object relationships from external text data . uses text corpora to determine how objects are
related to each other, then classiﬁes unknown objects based
on their relationship to known objects. In , images
are mapped to semantic word vectors corresponding to their
classes, and the resulting image embeddings are used to detect and distinguish between unseen and seen classes. We
also exploit transfer learning via an intermediate-level semantic word vector representation, however, the above approaches focus speciﬁcally on assigning a category label,
while our method generates full sentence descriptions. In
 , zero-shot object detectors are learned by transferring
information about how network weights trained on the classiﬁcation task differ from weights trained on the detection
task. We explore a similar transfer method to transfer information from weights which are trained on image-sentence
data to weights which are only trained on text data.
Describing New Objects in Context. Many early caption
models rely on ﬁrst discerning visual elements from an image, such as subjects, objects, scenes, and
actions, then ﬁlling in a sentence template to create a coherent visual description. These models are capable of describing objects without being provided with paired imagesentence examples containing the objects, but are restricted
to generating descriptions using a ﬁxed, predetermined template. More recently, explore describing new objects
with a deep caption model with only a few paired imagesentence examples during training. However, do not
consider how to describe objects when no paired imagesentence data is available. Our model provides a mechanism to include information from existing vision datasets as
well as unpaired text data, whereas relies on additional
image-sentence annotations to describe novel concepts.
3. Deep Compositional Captioner
DCC composes novel sentences about objects unseen in
paired image-sentence data. Although it is common to pretrain deep caption models on unpaired image data, unlike
existing models, we are able to describe objects present
in unpaired image data but not present in paired imagesentence data. Additionally, to enhance the language structure, we train our model on independent text corpora. Further, we explore methods to transfer knowledge between semantically related words to compose descriptions of new
objects. Our method consists of three stages: 1) training
a deep lexical classiﬁer and deep language model with unpaired data, then, 2) combining the lexical classiﬁer and language model into a caption model which is trained on paired
image-sentence data, and, ﬁnally, 3) transferring knowledge
from words which appear in paired image-sentence data to
words which do not appear in paired image-sentence data.
3.1. Deep Lexical Classiﬁer
The lexical classiﬁer (Fig 2, left) is a CNN which maps
images to semantic concepts.
In order to train the lexical classiﬁer, we ﬁrst mine concepts which are common in
paired image-text data by extracting the part-of-speech of
each word and then select the most common adjectives,
verbs, and nouns. We do not reﬁne the mined concepts,
which means some of the concepts, such as “use”, are not
strictly visual. In addition to concepts common in paired
image-sentence data, the classiﬁer is also trained on objects
that we wish to describe outside of the caption datasets.
The lexical classiﬁer is trained by ﬁne-tuning a CNN
which is pre-trained on the training split of the ILSVRC-
2012 dataset. When describing images, multiple visual
concepts from the image inﬂuence the description. For example, the sentence “An alpaca stands in the green grass.”
includes the visual concepts “alpaca”, “stands”, “green”,
and “grass”. In order to apply multiple labels to each image, we use a sigmoid cross-entropy loss. We denote the
image feature output by the lexical classiﬁer as fI, where
each index of fI corresponds to the probability that a particular concept is present in the image. Our idea of learning visual classiﬁers from text descriptions for captioning is
similar to who learn classiﬁers for objects, verbs, and
locations and who learn visual concepts using multiple
instance learning.
3.2. Language Model
The language model (Fig 2, right) learns sentence structure using only unpaired text data and includes an embedding layer which maps a one-hot-vector word representation
to a lower dimensional space, an LSTM , and a word
prediction layer. The language model is trained to predict a
word given previous words in a sentence. At each time step,
the previous word is input into the embedding layer. The
embedded word is input into an LSTM, which learns the recurrent structure inherent in language. The embedded word
and LSTM output are concatenated to form the language
features, fL. fL is input to an inner product layer which
outputs the next word in a generated sequence. At training
Figure 2: DCC consists of a lexical classiﬁer, which maps pixels to
semantic concepts and is trained only on unpaired image data, and
a language model, which learns the structure of natural language
and is trained on unpaired text data. The multimodal unit of DCC
integrates the lexical classiﬁer and language model and is trained
on paired image-sentence data.
time, the ground truth word is always used as an input to the
language model, but at test time we input the previous word
predicted by our model. We also ﬁnd that results improve
by enforcing a constraint that the model cannot predict the
same word twice in a row. We explore a variety of sources
for unpaired text corpora as described in Section 4.1.
3.3. Caption Model
The caption model integrates the lexical classiﬁer and
the language model to learn a joint model for image description. As shown in Fig 2 (center) the multimodal unit
in the caption model combines the image features, fI and
the language features, fL. The multimodal unit we use is an
afﬁne transformation of the image and language features:
pw = softmax(fIWI + fLWL + b)
where WI, WL, and b are learned weight matrices and pw
is a probability distribution over the predicted word.
Intuitively, the weights in WI learn to predict a set of
words which are likely to occur in a caption given the visual elements discerned by the lexical classiﬁer. In contrast,
WL learns the sequential structure of natural language by
learning to predict the next word in a sequence given the
previous words. By summing fIWI and fLWL, the multimodal unit combines the visual information learned by the
lexical classiﬁer with the knowledge of language structure
learned by the language model to form a coherent description of an image.
Both the language model and caption model are trained
to predict a sequence of words, whereas the lexical classiﬁer
is trained to predict a ﬁxed set of candidate visual elements
for a given image. Consequently, the weights WL, which
map language features to a predicted word are learned when
training the language model, but the weights WI are not.
Weights in WL are pretrained using unpaired text data before ﬁne-tuning with paired image-sentence data, WI are
trained purely with image-sentence data. Though we use a
linear multimodal unit, our results are comparable to results
achieved by other methods which include a nonlinear layer
for word prediction. For example, on the MSCOCO validation set achieves a METEOR score of 23.7, and DCC
achieves a METEOR of 23.2.
The caption model is designed to enable easy transfer
of learned weights from words which appear in the paired
image-sentence data to words which do not appear in the
image-sentence data. First, by using a lexical classiﬁer to
extract image features, image features have explicit semantic meaning. Consequently, it is trivial to expand the image feature to include new objects and to adjust weights in
the multimodal unit which correspond to speciﬁc objects.
Second, by learning language features using unpaired text
data, we ensure that the model learns a good embedding for
words which are not present in paired image-sentence data.
Finally, by using a single-layer, linear multimodal unit, the
dependence between image and language features and predicted words is straightforward to understand and easy to
exploit for semantic transfer.
3.4. Transferring Information Between Objects
Direct Transfer.
The ﬁrst method we explore to transfer
weights between objects directly transfers learned weights
in WI, WL and b from words that appear in the paired
image-sentence dataset to words which do not appear in a
paired image-sentence dataset (Fig 3). Intuitively, the direct transfer model requires that a new word is described in
the same way that semantically similar words are described.
To illustrate, consider the new word “alpaca” which is semantically close to the known word “sheep”. Let va and
vs indicate the index of the words alpaca and sheep in the
vocabulary. Given image and language features, fI and fL
respectively, the probability of predicting the word “sheep”
is proportional to:
fIWI[:, vs] + fLWL[:, vs] + b[vs]
In order to construct sentences with “alpaca” in the same
way sentences are constructed with the word “sheep”, we
ﬁrst directly transfer the weights WI[:, vs], WL[:, vs], and
b[vs] (indicated in red in Fig 3) to WI[:, va], WL[:, va], and
b[va] (indicated in green in Fig 3). Additionally, we expect
the prediction of the word “sheep” to be highly dependent
on the likelihood that a “sheep” is present in the image. In
other words, we expect WI[:, cs] to strongly weight the output of the lexical classiﬁer which corresponds to the word
“sheep”. However, WI[:, ca] should strongly weight the lex-
Figure 3: Method for transferring knowledge from words trained
with paired image-sentence data to words trained without imagesentence data. See Section 3.4 for details.
ical classiﬁer which corresponds to the word “alpaca”. To
enforce this, we set WI[ra, ca] = WI[rs, cs] where ra and
rs indicate the index in the image features which correspond
to the alpaca and sheep classiﬁers respectively. Finally, we
do not expect the output of the word “alpaca” to depend on
the presence of a sheep in the image and vice versa. Consequently, we set WI[rs, ca] = WI[ra, cs] = 0.
Delta Transfer.
Instead of directly transferring weights,
we can also transfer how weights change when trained on
paired image-text data.
Again, consider transferring the
word “sheep” to the word “alpaca”. We determine ∆L for a
given word as:
∆L = WL−caption[:, vs] −WL−language[:, vs]
where WL−caption are weights learned when training with
both images and sentences and WL−language are weights
learned when training only with language. The weights for
the new word “alpaca” are updated as:
WL−caption[:, va] = WL−language[:, va] + ∆L
Delta transfer may be advantageous because, unlike direct
transfer, it does not overwrite pretrained weights in WL during transfer. When performing delta transfer for WL, we
still use direct transfer for weights in WI.
Determining Concept Similarity.
Determining which
words in the paired image-sentence data are semantically
similar to words out of the paired image-sentence data is
key for transfer.
We determine semantic similarity with
the word2vec CBOW model which we trained on the
British National Corpus (BNC), UkWaC, and Wikipedia,
and estimate word similarity using cosine distance. Additionally, we restrict words that are transferred to new words
to be in the lexical layer.
4. Experimental Framework
4.1. Datasets
Image Description.
To empirically evaluate our method
we create a subset of the MSCOCO training set (denoted as the held-out MSCOCO training set) which excludes all image-sentence pairs which describe at least one
of eight MSCOCO objects. To ensure that excluded objects
are at least similar to some included ones, we cluster the 80
objects annotated in the MSCOCO segmentation challenge
using the vectors from the word2vec embedding described
in Section 3.4 and exclude one object from each cluster. The
following words are chosen: “bottle”, “bus”, “couch”, “microwave”, “pizza”, “racket”, “suitcase”, and “zebra”. We
randomly select 50% of the MSCOCO validation set for validation, and set aside the other 50% for testing. We use the
validation set to determine all model hyperparameters, and
present all results on the test set. We label the visual concepts in each image based on the ﬁve ground truth caption
annotations provided in the MSCOCO dataset. If any of the
ground truth captions mention an object, the corresponding
image is considered a positive example for that object.
In addition to empirically evaluating our model, we also
qualitatively examine the performance of DCC at a large
scale by describing objects outside of the paired imagesentence corpora. Speciﬁcally, we select 642 objects from
the full ImageNet object recognition dataset which do
not occur in MSCOCO and are also present in the WebCorpus text dataset (see section 4.3) vocabulary. We do no manual concept pruning; consequently some selected concepts
refer to a broad variety of objects (e.g., the class “fauna”
contains all animals) and other classes only contain a small
number of images (e.g., there are three “discus” images).
We use 75% of images from each class to train the lexical
classiﬁer, and evaluate on the rest. We stress that we do not
have any descriptions for these categories.
Video Description. For empirical evaluation on video description, we use a collection of Youtube clips from the Microsoft Video description (MSVD) corpus , which contains 1,970 short annotated clips. Our basic experimental
setting follows previous video description works .
However, we hold out paired video-sentence data for some
objects during training. Because there is signiﬁcant variation in the number of video clips containing each object
in the MSVD dataset, we hold out objects in the MSVD
dataset which appear in ﬁve or fewer training videos and
at least one test video and also appear in the ILSVRC2015
video object detection challenge set.1 Our MSVD held-out
set excludes paired video-sentence training data which include “zebra”, “hamster”, “broccoli”, and “turtle”.
We also qualitatively evaluate our method on the
ILSVRC object detection challenge videos (initial release)
1 
which consists of 1,952 video snippets of the 30 objects
from the ILSVRC2015 object detection in video. Objects
which we describe in the detection challenge videos include
“whale”, “fox”, “hamster”, “lion”, “zebra”, and “turtle”.
4.2. Training the Lexical Classiﬁer
Image description. We consider both MSCOCO and ImageNet as sources of labeled image data to train the lexical classiﬁer. For all objects in paired image-sentence data,
we use COCO images which are labeled with 471 visual
concepts to train the lexical classiﬁer. For the eight objects which do not appear in the paired image-sentence data,
we explore training the lexical classiﬁer using MSCOCO
images (in-domain) and ImageNet images (out-of-domain).
For qualitative experiments on ImageNet objects, we use
Imagenet images to train the lexical classiﬁer on new visual
concepts. The lexical classiﬁer is trained by ﬁne-tuning a
deep convolutional model (VGG-16 layer ) trained on
the ILSVRC-2012 object recognition training subset
of the ImageNet dataset.
Video Description.
Unlike images, videos consist of a
sequence of frames which need to be mapped to a set of semantic concepts by the lexical classiﬁer. To build a lexical
classiﬁer for videos, we mean-pool fc7 features across all
frames in a video clip before classiﬁcation. We use both
MSVD and ImageNet videos to train the lexical classiﬁer.
We use the VGG-16 layer model to extract fc7 layer features from video frames.
4.3. Training the Language Model
Image Description.
We consider three different sources
for unpaired text data to train the language model: (1)
MSCOCO consists of all captions from the MSCOCO train
set (2) Text from Image Description Corpora (Caption-
Txt) consists of text data from other paired image and video
description datasets: Flickr1M , Flickr30k , Pascal-
1k and ImageCLEF-2012 and sentence descriptions of Youtube clips from the MSVD training corpus.
This corpus does not include sentences from MSCOCO.
(3) External text (WebCorpus) consists of 60 million sentences from the British National Corpus (BNC), UkWaC,
and Wikipedia.
Video Description.
We consider two sources of text to
train the video description language model.
The ﬁrst is
the WebCorpus text described above.
We also consider
a slight variant on the CaptionTxt described above which
includes descriptions from MSCOCO, Flickr-30k ,
Pascal-1k and the MSVD sentence descriptions.
4.4. Training the Caption Model
After training the lexical classiﬁer and language model,
the weights in the multimodal layer of the caption model
are trained with paired image-sentence data. For the direct
No Transfer
Table 1: We compare DCC before transfer (No Transfer) to DCC
with delta transfer (∆T) and DCC with direct transfer (DT). We
also compare to another competitive caption generation model
(LRCN). We measure our models ability to insert new words into
a generated sentence with the F1-score. We also report Bleu-1 and
METEOR, which indicates overall sentence quality. DCC successfully incorporates new words and improves sentence quality.
(Values in %)
transfer method, we simply train the weights in the multimodal unit (WI and WL) while freezing all other weights.
For the delta transfer method, if weights in WL, which are
pretrained when training the language model, diverge too
much from their original values, transfer does not work
well. Consequently, we ﬁrst hold weights in WL constant,
training only WI, before jointly training WL and WI. The
caption model is trained the same way for both image and
video description.
4.5. Metrics
To evaluate our transfer methods, we must choose a metric that indicates whether or not a generated sentence includes a new object.
Common caption metrics such as
BLEU and METEOR measure overall sentence
meaning and ﬂuency. However, for many objects, it is possible to achieve good BLEU and METEOR scores without mentioning the new object (e.g., consider sentences describing the boy playing tennis in Figure 4). To deﬁnitively
report our model’s ability to integrate new vocabulary, we
also report the F1-score. The F1-score considers “false positives” (when a word appears in a sentence it should not appear in), “false negatives” (when a word does not appear in
a sentence it should appear in), and “true positives” (when
a word appears in a sentence it should appear in). We consider generated sentences “positive” if they contain at least
one mention of a held out word and ground truth sentences
“positive” if a word is mentioned in any ground truth annotation that describes an image.
We train our models using Caffe . 2
5. Results
5.1. Image Description
As shown in Figure 4, DCC is capable of integrating new
vocabulary into image descriptions in a cohesive manner.
Direct Transfer Versus Delta Transfer.
Table 1 compares the average F1-score across the eight held-out training classes for the delta transfer and direct transfer meth-
2Code can be found at 
˜lisa_anne/dcc_project_page.html.
Pair Supervision: A pizza with a lot of toppings on it.
No Transfer: A plate of food with a glass of wine.
DCC (in): A pizza sitting on a wooden table with a glass of wine behind it.
DCC (out): A pizza sitting on top of a wooden table.
Pair Supervision: A dog laying on a couch with a large brown dog.
No Transfer: A dog laying on a bed with a large brown dog.
DCC (in): A dog laying on a couch with a large window in the background.
DCC (out): A dog laying on a couch in a room.
Pair Supervision: A white microwave oven sitting on top of a counter.
No Transfer: A white and black cat is sitting on a toilet.
DCC (in): A white microwave sitting on a brick wall.
DCC (out): A white microwave sitting next to a white oven.
Pair Supervision: A boy is holding a tennis racket on a court.
No Transfer: A boy is playing tennis on a court.
DCC (in): A boy is playing with a racket on a court.
DCC (out): A young boy is playing racket on a racket.
Pair Supervision: A car with a suitcase on the seat in the back seat of a car.
No Transfer: A car with a bag of bananas in the back.
DCC (in): A car with a suitcase and a plastic suitcase behind it.
DCC (out): A car with a suitcase inside of it ' s back.
Pair Supervision: A zebra is grazing in a grassy area.
No Transfer: Two giraffes are eating grass in the field.
DCC (in): Two zebra grazing in a green grass field.
DCC (out): Two zebra standing in a field with grass in the background.
Pair Supervision: A group of three different colored vases with different designs.
No Transfer: A table with many different types of wine.
DCC (in): A table with many bottle of bottle of bottle.
DCC (out): A counter with a lot of bottle and bottle of bottle.
Pair Supervision: A bus is driving down the street in front of a bus stop.
No Transfer: A green and white street sign on a city street.
DCC (in): A green and white bus parked on the side of the street.
DCC (out): A green and white bus driving down the street.
Figure 4: Image Description: Comparison of captions generated by model without transfer, DCC with in-domain training
(MSCOCO), with out-of-domain training (ImageNet and WebCorpus), and a model trained with paired image-sentence supervision
for all MSCOCO objects.
DCC is capable of integrating new
words and generates sentences similar to those generated when
paired image-sentences for all objects are present during training.
CaptionTxt
Table 3: Image Description: We compare the effect of pre-training
the lexical classiﬁer and language model with different unpaired
image and text data sets. As expected, we see the best result when
using in domain MSCOCO data to train the lexical classiﬁer and
language model, though training with out of domain corpora is
comparable. (Values in %)
ods. We additionally train LRCN ( ) 3 on our MSCOCO
held-out dataset and note that our model without transfer
yields comparable results to LRCN, and performs considerably better on all metrics after transfer. As shown by the
F1-scores reported in Table 1, both the delta transfer and direct transfer methods are capable of integrating new words
into their vocabulary. We also report the BLEU-1 score,
which measures the overlap between generated words and
3For fair comparison, we train LRCN on VGG, ﬁne-tune through the
entire network, and do not use beam search.
Pair Supervision
Table 2: Image Description: Comparison of F1 scores for direct transfer DCC model (DT) and a model trained with image-sentence
training examples for all objects. (Values in %)
words in reference sentences. By measuring the METEOR
score, we ensure that our model maintains sentence ﬂuency
when inserting new objects. DCC consistently increases
METEOR scores indicating that overall sentence quality
improves with DCC. The direct transfer method improves
F1-scores, BLEU, and METEOR scores by a larger amount
than the delta transfer method and is thus used for the remainder of our experiments.
Importantly, BLEU and METEOR scores do not decrease for objects which are present in the held-out training data set. When trained with all image-sentence training
examples, our model achieves an average BLEU-1 of 69.36
and METEOR of 23.98 on held-out classes.
To illustrate which words our model works best on, we
report the F1-score for individual objects in Table 2. We
compare to a model which is trained with image-sentence
pairs for the eight held-out objects. For all objects, DCC is
able to compose sentences which include the object.
Analysis of Transfer Words.
In general,
determining word similarity with a word2vec embedding
works well.
Words such as “zebra”/“giraffe” and “microwave”/“refrigerator” are close in embedding space and
are also used in similar ways in natural language, suggesting they will work well for transfer. Some transfer pairs
(“racket”/“tennis” and “bus”/“stop”) are used together frequently but play different structural roles in sentences. Consequently, the word “racket” is frequently used like the word
“tennis” leading to grammatical errors. However, similar
errors do not occur when transferring “stop” to “bus”.
Pre-Training with Out-of-Domain Data.
In the above
experiments the lexical classiﬁer and language model are
pre-trained using MSCOCO images and text. In a real world
scenario, it is unlikely that available unpaired image and
text data will be from the same domain as paired imagesentence data. However, it is essential that the model learns
good image and language features. Naturally, if the lexical
classiﬁer is unable to recognize certain objects, DCC will
not be able to describe the objects. Perhaps more subtly, if
the language model is not trained with unpaired text which
includes an object, it will not learn a proper embedding for
the new word and will not produce cohesive descriptions
about new objects.
Table 3 demonstrates the impact of using outside image
and text corpora to train the lexical classiﬁer and language
model. Our model performs best when provided with indomain image and text for all training stages, but perfor-
Dress → Tutu, Dress → Chiffon
No transfer: A woman in a dress shirt is holding a tennis racket.
DCC: A woman in a chiffon tutu.
Plane → Spaceship
No transfer: A blue and white airplane is flying in the air.
DCC: A spaceship is flying through the air.
Tree → Baobab
No transfer: A large giraffe standing in a tree.
DCC: A large baobab in a field with trees in the background.
Kite → Trapeze
No transfer: A woman is holding a skateboard in the air.
DCC: A woman is holding a trapeze in the air.
Bird → Toad
No transfer: A green and white bird sitting on top of a green field.
DCC: A toad is sitting on the ground.
Giraffe → Impala
No transfer: A close up of a bird on a field.
DCC: A impala is standing in the grass.
Cake → Scone
No transfer: A close up of a pizza on a plate.
DCC: A close up of a scone on a plate.
Bird → Otter
No transfer: A couple of birds standing on top of a lush green field.
DCC: A otter standing on top of a lush green field.
Figure 5: Image Description: DCC is able to describe Imagenet
objects (bolded) which are not mentioned in any of the paired
image-sentence data, and therefore cannot be described by existing deep caption models. X →Y indicates that the known word X
is transferred to the new word Y.
mance is comparable when using ImageNet images to train
the lexical classiﬁer and CaptionTxt or WebCorpus text data
to train the language model.
5.2. Describing ImageNet Objects
We qualitatively assess our model by describing a variety of ImageNet objects which are not included in the
MSCOCO data set (Fig 5). DCC accurately describes 335
new words including entry-level words like “toad” as well
as ﬁne-grained categories like “baobab”. Though most Imagenet words we transfer are nouns, we are able to successfully transfer some adjectives such as “chiffon”. DCC
achieves more than simple noun replacement. For example,
the sentence “A large giraffe standing in a tree” changes
signiﬁcantly to “A large baobab in a ﬁeld with trees in the
background” after transfer. Importantly, our model is able
to compose sentences by placing objects in the correct context. For example, comparing Fig 5 (top) to the image in
Error: New object (lifejacket) not mentioned
DCC: A group of people sitting on a bench.
Error: Grammatically Incorrect
DCC: A woman is playing gymnastics on a gymnastics court.
Error: Object Hallucination
DCC: A woman in a snorkel is holding a surfboard.
Error: Irrelevant description
DCC: A dog is sitting on a white bench.
Figure 6: Image Description: We highlight four common error
types generated by the DCC. See Section 5.2 for details.
Fig 1, the object “otter” is correctly described as either “sitting in the water” or “standing on top of a lush green ﬁeld”
depending on visual context.
Figure 6 examines a few common error types:
New Object Not Mentioned. (Figure 6, top) For some images, DCC produces relevant sentences, but fails to mention
the new object.
Grammatically Incorrect.
(Figure 6, second row) Some
sentences incorporate new words, but are grammatically
incorrect. For example, though DCC describes sentences
with the word “gymnastics”, the resulting sentences are frequently grammatically incorrect (e.g., “A woman playing
gymnastics on a gymnastics court”). This is likely because
the word “tennis” is transferred to “gymnastics”. Though
both of these words are sports, one does not “play” gymnastics and gymnastics is not performed on a “court.”
Object Hallucination.
(Figure 6, third row) DCC frequently hallucinates objects which commonly occur in a
speciﬁc visual context. For example, in a beach image, the
model commonly includes the word “surfboard”.
Irrelevant Description. (Figure 6, bottom) Some captions
do not mention any salient objects correctly. Such errors
can be caused by poor image recognition or because the
language model is unable to construct a reasonable sentence
from constituent visual elements.
More examples are in our supplemental material.
5.3. Video description
We believe DCC can be especially beneﬁcial in domains,
such as video description, where the amount of paired training data is small. Table 6 presents empirical results of direct transfer DCC on videos in the MSVD corpus (Section 4.1). We report the average F1 score on all held-out
classes, and METEOR scores on the complete test dataset.
As seen by the F1 score, transferring weights allows us to
describe new objects in video. Additionally, the METEOR
score improves with transfer demonstrating that DCC im-
Model (Video)
Baseline (No Transfer)
+ ILSVRC Videos (No Transfer)
+ ILSVRC Videos + DT
Table 4: Video Description: METEOR scores across the test
dataset and average F1 scores for the four held-out categories (All
values in %) using direct transfer (DT). The DCC models were
trained on videos with 4 objects removed and the language model
was trained on in-domain sentences.
A hamster is eating food in a bowl.
A turtles are running.
A zebra is eating some grass.
Figure 7: Video Description: Captions generated by DCC on
videos of novel objects unseen in paired training data.
proves overall sentence quality. Similar to the trend seen
for image captioning, training on in-domain text corpora
achieves slightly better performance than training on external text. When adding ImageNet videos, both F1 and
METEOR increase suggesting that including outside image
data is beneﬁcial. Including ImageNet videos to learn better
lexical classiﬁers especially improves the F1 score, which
increases from 6.0 to 22.2. Figure 7 presents qualitative results of our best model on snippets with the held out objects
in MSVD corpus and the ILSVRC validation set.
6. Conclusion
We present the Deep Compositional Captioner (DCC)
which can be used to describe new objects which are not
present in current caption copora.
Our quantitative and
qualitative results demonstrate our model’s ability to integrate new vocabulary into generated image and video descriptions by effectively using existing vision datasets and
unpaired text data.
By integrating data from disparate
sources and transferring knowledge between semantically
related concepts, DCC improves upon current deep caption
models by providing rich descriptions which are not limited
by the availability of paired image-sentence corpora.
Acknowledgements
Lisa Anne Hendricks is supported by the NDSEG Fellowship.
Marcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD). Trevor Darrell was supported in part by DARPA; AFRL; DoD MURI award
N000141110688; NSF awards IIS-1212798, IIS-1427425,
and IIS-1536003, and the Berkeley Vision and Learning
Center. Raymond Mooney and Kate Saenko were supported
in part by DARPA under AFRL grant FA8750-13-2-0026
and a Google Grant. Mooney was also supported by ONR
ATL Grant N00014-11-1-010.
SUPPLEMENTAL MATERIAL
We present further empirical and qualitative results for
both image and video description. For the image description task, we explore averaging weight vectors before transfer, illustrate errors made by the model when no unpaired
text data is used during training and provide descriptions
generated by DCC for a large variety of novel object categories in ImageNet. For the video description task, we
include results when training the language model with an
external text corpora and more DCC descriptions of novel
objects in video.
A. Image Description
A.1. Transferring from Multiple Words
In the main paper, we transfer the most similar word in
the paired image-sentence data to new objects in the multimodal unit. However, we could also average weights across
multiple similar words before transfer. For direct transfer,
averaging weights before transfer hurts performance substantially. In contrast, averaging weights before delta transfer does not signiﬁcantly impact results (Table 5).
A.2. Transfer with No Language model
As mentioned in the paper, if unpaired text data is not
used to train the language model, descriptions are poor because the caption model never learns good language features for new object categories. This is illustrated in Figure 8. Though a model which is trained on paired imagesentence data and unpaired image data can insert new words
into a sentence, the generated sentences are not cohesive
because the underlying language model has never seen the
new object categories. For example, without training on
unpaired text data, the model produces repetitive sentences
like “A zebra with several zebra and zebra of zebra.” or
ungrammatical phrases like “a pizza bowl of food”.
A.3. Qualitative Analysis of ImageNet Descriptions
Comparison to sentences generated with no transfer
Figure 9 compares generated sentences for new object categories before and after transfer and also indicates which
word in the paired image-sentence data is transferred to
each new word.
DCC does not simply substitute words
Table 5: Image Description: Comparison of delta transfer method
when averaging N closest weight vectors before transfer. Averaging weight vectors before transfer has little impact on performance.
seen in the image-sentence data with new object categories.
Rather, subsequent words in the sentence are impacted by
the use of a new vocabulary word. Consider the image of the
candelabra in the top row of Figure 9. Without transfer, the
model describes “A table with a vase of ﬂowers in it.”. However, after transfer, the model describes “A candelabra is
sitting on a table in a room.” Though the word vase is transferred to the word candelabra, candelabra is described differently. This is possibly because the language model learns
to describe the words “vase” and “candelabra” in slightly
different ways. For example, “ﬂowers” are not frequently
in candelabras. Furthermore, sentences generated before
transfer do not necessarily include the known word which
is transferred to the new category. Consider the image of
the centrifuge in the second row of Figure 9. Though the
word “refrigerator” is transferred to the word “centrifuge”,
the sentence generated before transfer does not include the
word “refrigerator”. However, the word “centrifuge” is accurately described after transfer.
Successful Descriptions
We highlight sentences generated by DCC in Figure 10, 11, and 12. By placing different
images of the same object side-by-side, we can compare
how the same object is described in different contexts. For
example, in Figure 10, a gecko is described as “A person
holding a gecko in their hand” and “A gecko is standing on
the branch of a tree” demonstrating that DCC is able to describe a single object in a variety of ways to reﬂect different
visual contexts.
We highlight common errors generated by DCC.
Figure 13 shows examples in which the description does not
mention the new object category, but is still highly relevant.
Sometimes, DCC produces accurate descriptions without
mentioning the new object category. Other times, DCC correctly describes other elements of the scene, but misclassiﬁes the new object category. For example, in Figure 13
right, the model classiﬁes the “alpaca” as a “sheep”.
Figure 14 provides examples of descriptions which do
include a new object category, but describe it in the incorrect
DCC commonly hallucinates objects which are
likely given the object context. For example, when describing an amphitheater, DCC produces the sentence “A group
of people standing around a amphitheater” even though no
Figure 8: Image Description: Example captions generated by
DCC direct transfer with and without pretraining the language
model with unpaired text data. Without pretraining on text data,
generated sentence ﬂuency is poor. For example, the model will
repeat the word “zebra” or insert a ungrammatical phrase like
“pizza bowl of food”.
people are present in the image. Such errors can be caused
because the language model learns that people and amphitheaters occur together or because the lexical classiﬁer
mistakenly classiﬁes “people” in the image. When describing new types of buildings (e.g. chapel, fortress, or watchtower), DCC will frequently include the word “clock” in
the description. This is likely because MSCOCO vocabulary words like “tower” and “building” which are transferred to new words like “chapel” are frequently pictured
with clocks. Sometimes the model includes objects in the
description which are not contextually likely. For example, when describing a little girl in a tutu, the model also
describes an umbrella. This is likely because the visual features for “umbrella” overpower the language model. Describing objects out of context is common for images which
include a single object and a monochromatic background,
as is commonly seen in ImageNet images. Because only
one object strongly activates the lexical layer, the model will
frequently hallucinate objects which are not present in the
image (e.g., “A man is sitting on a bench with a chainsaw”
when there is no man or bench).
Figure 15 provides examples of grammatical errors. As
mentioned in the main paper, grammatical errors arise when
a poor word is chosen for transfer. Examples of poor grammatical sentences when the word “dog” is transferred to
“foxhunting” and the word “frisbee” is transferred to “trampoline” are shown in Figure 15. Another common grammatical mistake is for the model to list two objects in a row such
as a “vole bear” or for the object to repeat a phrase such
as “a unicycle on a unicycle”. One possible explanation
for such errors is that the pretrained language model does
not learn good language features for these words. Consequently, after a new word is generated, the model generates
a poor subsequent word.
Finally, Figure 16 shows images with irrelevant captions.
Model (Video, WebCorpus LM)
Baseline (No Transfer)
+ ILSVRC Videos (No Transfer)
+ ILSVRC Videos + DT
Table 6: Video Description: METEOR scores across the test
dataset and average F1 scores for the four held-out categories (All
values in %). The DCC models were trained on videos with 4 objects removed and the language model were trained on WebCorpus
sentences.
B. Video Description
B.1. Empirical Results
As was presented for image description in the main paper, we also explore the effects of training the language
model used for video description with out-of-domain unpaired text data in Table 6. Comparing to Table 4 in the
main paper, METEOR drops when training the language
model with out-of-domain unpaired text (29.1 to 28.8 when
including ImageNet videos during training and using transfer). The F1-score also drops when trained with ImageNet
videos, but without training on ImageNet videos the F1score actually increases from 6.0 to 13.3.
B.2. Qualitative Results
We present qualitative results on ImageNet videos. The
lexical classiﬁer is trained with ImageNet and MSVD
videos and the language model is trained with in-domain
text data. In addition to the objects held-out in the main
paper (“zebra”, “hamster”, “broccoli”, and “turtle”) we
also describe videos which include the objects “fox” and
“whale”. The caption model is never provided any paired
video-sentence data which include these objects during
training. In Figure 17, we show the top ﬁve captions predicted by DCC for videos which include “whale”, “fox”,
and “hamster”. For “whale” and “hamster”, DCC predicts
the correct object in the most probable caption. However,
for “fox” the model predicts that “dog” is more probable
than “fox”, though “fox” is predicted in the second most
probably caption.
Figure 18 compares descriptions generated by the model
without transfer and after transfer. The model is correctly
able to identify and generate sentences to describe “hamster”, “lion”, “turtle”, “whale”, and “zebra” after transfer.
In comparison to the sentences produced by the model before transfer, the sentences generated after transfer describe
the object and the context more accurately e.g. in the video
containing a “whale”, the sentence before transfer says “A
woman is riding a jet ski” whereas after transfer DCC says
“A whale is swimming” which appropriately describes the
object (whale) and the context (swimming) correctly.
Figure 17 also includes an example where the model has
Figure 9: Image Description: Comparison of descriptions generated by DCC for a variety of ImageNet objects with and without transfer.
X →Y indicates that the known word X is transferred to the new word Y. DCC does not simply substitute new words in place of words
present in image-text data. Further, the sentences generated by a model without transfer do not need to contain the word X for the sentences
generated by a model after transfer to include the word Y.
difﬁculty choosing the correct object. The ﬁve highest probability results each include a different animal in the description which indicates that the model is unsure which object
is present.
A person is holding a gecko
in their hand.
A gecko is standing on a
branch of a tree.
A coyote is standing in
the middle of a field.
A coyote is standing in
A warship is sitting on the
A large group of people
standing around a warship.
A large blimp in a blue sky.
A large white blimp on a
A large verandah is in the
middle of a house.
A verandah is sitting on the
side of a window.
A finch standing on a small
A finch standing on a tree
A large crocodile in a small
body of water.
A man standing on a beach
with a large crocodile.
A hyena is standing in the
A hyena is standing in the
Figure 10: Image Description: Example DCC descriptions for eight different objects. DCC is able to describe objects in different contexts.
A display of food in a
delicatessen on display.
Delicatessen
A woman in a delicatessen
on a counter. top.
A black and white
skunk is eating grass.
A black and white skunk is
laying on a white surface.
A couple of lychee are
sitting on a table.
A bunch of lychee are in a
Hollandaise
A plate of food with a fork
and a hollandaise.
A plate with a sandwich and
a hollandaise.
A yellow and black toucan
is sitting on a branch.
A close up of a toucan on a
green field.
A woman in a sari is sitting
A woman is standing in a
blue sari.
Footbridge
A man is standing on a
footbridge.
A large body of water with
a footbridge.
A green rhubarb with a
green plant on it.
A red and white rhubarb is
sitting on a table.
Figure 11: Image Description: Example DCC descriptions for eight different objects. DCC is able to describe objects in different contexts.
Figure 12: Image Description: Example DCC descriptions for eight different objects. DCC is able to describe objects in different contexts.
A close up of a bowl of
food on a table.
A dog laying on a grass
covered in a field.
A large crowd of people
standing around a large room.
A sheep standing in a
field with a fence.
Discotheque
Figure 13: Image Description: Example descriptions generated by DCC which are relevant, but do not describe a new object category.
Sometimes new objects are misclassiﬁed (e.g., “sheep” instead of “alpaca”) but still describe the correct context. Other times, the new
object category is not needed to accurately describe the image.
Figure 14: Image Description: Example descriptions generated by DCC in which a new object is described, but the context is described
incorrectly. This is especially common in images, like the image of the chainsaw on the far right, which only include a single object on a
monochromatic background.
A group of people standing
around a foxhunting on a field.
Foxhunting
A young boy is playing
a game of trampoline.
A vole bear is sitting in a field.
A woman is riding a
unicycle on a unicycle.
Trampoline
Figure 15: Image Description: Example descriptions generated by DCC with poor grammar. Poor grammar can be caused by a poor
transfer word (the transfer words for “foxhunting” and “trampoline” are “dog” and “frisbee” respectively) or because the language model
learns poor language features for the new object category.
Figure 16: Image Description: Though most descriptions are relevant, some descriptions are incorrect.
A dog is playing with a dog.
A fox is playing.
A fox is eating.
A cat is playing.
A dog is playing.
A whale is swimming.
A whale is swimming in the water.
A man is playing a guitar.
A turtle is swimming.
A whale is swimming in water.
A hamster is eating.
A hamster is eating sunflower seeds.
A hamster is eating something.
A hamster is drinking water.
A hamster is drinking water from a cup.
A dog is playing with a dog.
A antelope is eating grass.
A fox is eating.
A lion is eating.
A dog is running.
Figure 17: Video Description: Five most likely captions generated by DCC on novel objects unseen in paired training data. Captions are
sorted by likelihood with the top caption corresponding to the most likely caption.
No Transfer: A woman is riding a jet ski.
DCC: A whale is swimming.
No Transfer: A woman is riding a horse.
DCC: A lion is riding.
No Transfer: A horse is riding on a horse.
DCC: A zebra is walking around in the wild.
No Transfer: A man is playing guitar.
DCC: A turtle is playing.
No Transfer: A cat is playing.
DCC: A hamster is eating.
No Transfer: A man is playing a guitar.
DCC: A whale is swimming.
Figure 18: Video Description: Comparison of descriptions generated by DCC for some of the objects in the ImageNet video dataset with
and without transfer. Note that our model does not simply substitute added words in the place of words present in paired image-sentence