DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
Context-Dependent Pre-trained Deep Neural
Networks for Large Vocabulary Speech Recognition
George E. Dahl, Student Member, IEEE, Dong Yu, Senior Member, IEEE, Li Deng, Fellow, IEEE,
and Alex Acero, Fellow, IEEE
Abstract—We propose a novel context-dependent (CD) model
for large vocabulary speech recognition (LVSR) that leverages
recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden
Markov model (DNN-HMM) hybrid architecture that trains the
DNN to produce a distribution over senones (tied triphone states)
as its output. The deep belief network pre-training algorithm
is a robust and often helpful way to initialize deep neural
networks generatively that can aid in optimization and reduce
generalization error. We illustrate the key components of our
model, describe the procedure for applying CD-DNN-HMMs to
LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset
demonstrate that CD-DNN-HMMs can signiﬁcantly outperform
the conventional context-dependent Gaussian mixture model
(GMM)-HMMs, with an absolute sentence accuracy improvement
of 5.8% and 9.2% (or relative error reduction of 16.0% and
23.2%) over the CD-GMM-HMMs trained using the minimum
phone error rate (MPE) and maximum likelihood (ML) criteria,
respectively.
Terms—Speech
recognition,
context-dependent phone, LVSR, DNN-HMM, ANN-HMM
I. INTRODUCTION
VEN after decades of research and many successfully
deployed commercial products, the performance of automatic speech recognition (ASR) systems in real usage scenarios lags behind human level performance (e.g., , ).
There have been some notable recent advances in discriminative training (see an overview in ; e.g., maximum mutual
information (MMI) estimation , minimum classiﬁcation
error (MCE) training , , and minimum phone error
(MPE) training , ), in large-margin techniques (such
as large margin estimation , , large margin hidden
Markov model (HMM) , large-margin MCE – ,
and boosted MMI ), as well as in novel acoustic models
(such as conditional random ﬁelds (CRFs) – , hidden
Copyright (c) 2010 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to .
Manuscript received September 5, 2010.
This manuscript greatly extends the work presented at ICASSP 2011 .
G. E. Dahl is afﬁliated with the University of Toronto. He contributed
to this work while working as an intern at Microsoft Research (email:
 ).
D. Yu is with the Speech Research Group, Microsoft Research, One
Microsoft Way, Redmond, WA, 98034 USA (corresponding author, phone:
+1-425-707-9282, fax: +1-425-936-7329, e-mail: ).
L. Deng is with the Speech Research Group, Microsoft Research, One
Microsoft Way, Redmond, WA, 98034 USA (email: ).
A. Acero is with the Speech Research Group, Microsoft Research, One
Microsoft Way, Redmond, WA, 98034 USA (email: ).
CRFs , , and segmental CRFs ). Despite these
advances, the elusive goal of human level accuracy in realworld conditions requires continued, vibrant research.
Recently, a major advance has been made in training densely
connected, directed belief nets with many hidden layers.
The resulting deep belief nets learn a hierarchy of nonlinear
feature detectors that can capture complex statistical patterns
in data. The deep belief net training algorithm suggested in
 ﬁrst initializes the weights of each layer individually in
a purely unsupervised1 way and then ﬁne-tunes the entire
network using labeled data. This semi-supervised approach
using deep models has proved effective in a number of
applications, including coding and classiﬁcation for speech,
audio, text, and image data ( – ). These advances
triggered interest in developing acoustic models based on pretrained neural networks and other deep learning techniques
for ASR. For example, context-independent pre-trained, deep
neural network HMM hybrid architectures have recently been
proposed for phone recognition – and have achieved
very competitive performance. Using pre-training to initialize
the weights of a deep neural network has two main potential
beneﬁts that have been discussed in the literature. In ,
evidence was presented that is consistent with viewing pretraining as a peculiar sort of data-dependent regularizer whose
effect on generalization error does not diminish with more
data, even when the dataset is so vast that training cases are
never repeated. The regularization effect from using information in the distribution of inputs can allow highly expressive
models to be trained on comparably small quantities of labeled
data. Additionally, , , and others have also reported
experimental evidence consistent with pre-training aiding the
subsequent optimization, typically performed by stochastic
gradient descent. Thus, pre-trained neural networks often also
achieve lower training error than neural networks that are
not pre-trained (although this effect can often be confounded
by the use of early stopping). These effects are especially
pronounced in deep autoencoders.
Deep belief network pre-training was the ﬁrst pre-training
method to be widely studied, although many other techniques
now exist in the literature (e.g. ). After showed that
deep auto-encoders could be trained effectively using deep
belief net pre-training, there was a resurgence of interest in
using deeper neural networks for applications. Although less
pathological deep architectures than deep autoencoders can in
1In the context of ASR, we use the term “unsupervised” to mean acoustic
data with no transcriptions of any kind.
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
some cases be trained without pre-training, for many problems
and model architectures, researchers have reported pre-training
to be helpful (even in some cases for large single hidden
layer neural networks trained on massive datasets, as in ).
We view the various unsupervised pre-training techniques as
convenient and robust ways to help train neural networks with
many hidden layers that are generally helpful, rarely hurtful,
and sometimes essential.
In this paper, we propose a novel acoustic model, a hybrid between a pre-trained, deep neural network (DNN) and
a context-dependent (CD) hidden Markov model. The pretraining algorithm we use is the deep belief network (DBN)
pre-training algorithm of , but we will denote our model
with the abbreviation DNN-HMM to help distinguish it from
a dynamic Bayes net (which we will not abreviate in this
article) and to make it clear that we abandon the deep belief
network once pre-training is complete and only retain and
continue training the recognition weights. CD-DNN-HMMs
combine the representational power of deep neural networks
and the sequential modeling ability of context-dependent hidden Markov models (HMMs). In this paper, we illustrate the
key ingredients of the model, describe the procedure to learn
the CD-DNN-HMMs’ parameters, analyze how various important design choices affect the recognition performance, and
demonstrate that CD-DNN-HMMs can signiﬁcantly outperform strong discriminatively-trained context-dependent Gaussian mixture model hidden Markov model (CD-GMM-HMM)
baselines on the challenging business search dataset of ,
collected under actual usage conditions. To our best knowledge, this is the ﬁrst time DNN-HMMs, which are formerly
only used for phone recognition, are successfully applied to
large vocabulary speech recognition (LVSR) problems.
A. Previous work using neural network acoustic models
The combination of artiﬁcial neural networks (ANNs) and
HMMs as an alternative paradigm for ASR started between
the end of 1980s and the beginning of the 1990s. A variety
of different architectures and training algorithms have been
proposed in the literature (see the comprehensive survey in
 ). Among these techniques, the ones most relevant to this
work are those that use the ANNs to estimate the HMM stateposterior probabilities – , which have been referred to
as ANN-HMM hybrid models in the literature. In these ANN-
HMM hybrid architectures, each output unit of the ANN is
trained to estimate the posterior probability of a continuous
density HMMs’ state given the acoustic observations. ANN-
HMM hybrid models were seen as a promising technique for
LVSR in the mid-1990s. In addition to their inherently discriminative nature, ANN-HMMs have two additional advantages:
the training can be performed using the embedded Viterbi
algorithm and the decoding is generally quite efﬁcient.
Most early work (e.g., ) on the hybrid approach
used context-independent phone states as labels for ANN
training and considered small vocabulary tasks. ANN-HMMs
were later extended to model context-dependent phones and
were applied to mid-vocabulary and some large vocabulary
ASR tasks (e.g. in , which also employed recurrent neural
architectures). However, in earlier work on context dependent
ANN-HMM hybrid architectures , the posterior probability
of the context-dependent phone was modeled as either
p(si, cj|xt) = p(si|xt)p(ci|sj, xt)
p(si, cj|xt) = p(ci|xt)p(si|cj, xt),
where xt is the acoustic observation at time t, cj is one of
the clustered context classes C = {c1, · · · , cJ}, si is either a
context-independent phone or a state in a context-independent
phone. ANNs were used to estimate p(si|xt) and p(ci|sj, xt)
(alternatively p(ci|xt) and p(si|cj, xt)). Note that although
these types of context-dependent ANN-HMMs outperformed
GMM-HMMs for some tasks, the improvements were small.
These earlier hybrid attempts had some important limitations. For example, using only backpropagation to train
the ANN makes it challenging (although not impossible) to
exploit more than two hidden layers well and the contextdependent model described above does not take advantage
of the numerous effective techniques developed for GMM-
HMMs. Around 1999, the desire to use HMM advances from
the speech research community directly without developing
replacement techniques and tools contributed to a shift from
using neural nets to predict phonetic states to using neural
nets to augment features for later use in a conventional GMM-
HMM recognizer (e.g., ). In this work, however, we do not
take that approach, but instead we try to improve the earlier
hybrid approaches by replacing more traditional neural nets
with deeper, pre-trained neural nets and by using the senones
 (tied triphone states) of a GMM-HMM tri-phone model
as the output units of the neural network, in line with stateof-the-art HMM systems.
Although this work uses the hybrid approach, as alluded to
above, much recent work using neural networks in acoustic
modeling uses the so-called TANDEM approach, ﬁrst proposed in . The TANDEM approach augments the input to
a GMM-HMM system with features derived from the suitably
transformed output of one or more neural networks, typically
trained to produce distributions over monophone targets. In
a similar vein, uses features derived from an earlier
“bottle-neck” hidden layer instead of using the neural network
outputs directly. Many recent papers (e.g. – ) train
neural networks on LVSR datasets (often in excess of 1000
hours of data) and use variants of these approaches, either
augmenting the input to a GMM-HMM system with features
based on the neural network outputs or some earlier hidden
layer. Although a neural network nominally containing three
hidden layers (the largest number of layers investigated in
 ) might be used to create bottle-neck features, if the feature
layer is the middle hidden layer then the resulting features are
only produced by an encoder with a single hidden layer.
Neural networks for producing bottle-neck features are very
similar architecturally to autoencoders since both typically
have a small code layer. Deeper neural networks, especially
deeper autoencoders, are known to be difﬁcult to train with
backpropagation alone. For example, reports in one
experiment that they are unable to get results nearly so good
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
as those possible with deep belief network pre-training when
training a deep (the encoder and decoder in their architecture
both had three hidden layers) autoencoder with a nonlinear
conjugate gradient algorithm. Both and investigate
why training deep feed-forward neural networks can often
be easier with some form of pre-training or a sophisticated
optimizer of the sort used in .
Since the time of the early hybrid architectures, the vector
processing capabilities of modern GPUs and the advent of
more effective training algorithms for deep neural nets have
made much more powerful architectures feasible. Much previous hybrid ANN-HMM work focused on context-independent
or rudimentary context-dependent phone models and small
to mid-vocabulary tasks (with notable exceptions such as
 ), possibly masking some of the potential advantages of
the ANN-HMM hybrid approach. Additionally, GMM-HMM
training is much easier to parallelize in a computer cluster
setting, which historically gave such systems a signiﬁcant
advantage in scalability. Also, since speaker and environment
adaptation is generally easier for GMM-HMM systems, the
GMM-HMM approach has been the dominant one in the past
two decades for speech recognition. That being said, if we
consider the wider use of neural networks in acoustic modeling
beyond the hybrid approach, neural network feature extraction
is an important component of many state-of-the-art acoustic
B. Introduction to the DNN-HMM approach
The primary contributions of this work are the development of a context-dependent, pre-trained, deep neural network
HMM hybrid acoustic model (CD-DNN-HMM); a description
of our recipe for applying this sort of model to LVSR problems; and an analysis of our results which show substantial
improvements in recognition accuracy for a difﬁcult LVSR
task over discriminatively-trained pure CD-GMM-HMM systems. Our work differs from earlier context-dependent ANN-
HMMs in two key respects. First, we used deeper,
more expressive neural network architectures and thus employed the unsupervised DBN pre-training algorithm to make
sure training would be effective. Second, we used posterior
probabilities of senones (tied triphone HMM states) as
the output of the neural network, instead of the combination of
context-independent phone and context class used previously
in hybrid architectures. This second difference also distinguishes our work from earlier uses of DNN-HMM hybrids for
phone recognition – , . Note that , which also
appears in this issue, is the context-independent version of our
approach and builds the foundation for our work. The work in
this paper focuses on context-dependent DNN-HMMs using
posterior probabilities of senones as network outputs and can
be successfully applied to large vocabulary tasks. Training the
neural network to predict a distribution over senones causes
more bits of information to be present in the neural network
training labels. It also incorporates context-dependence into
the neural network outputs (which, since we are not using a
Tandem approach, lets us use a decoder based on triphone
HMMs), and it may have additional beneﬁts. Our evaluation
was done on LVSR instead of phoneme recognition tasks as
was the case in – , . It represents the ﬁrst large
vocabulary application of a pre-trained, deep neural network
approach. Our results show that our CD-DNN-HMM system provides dramatic improvements over a discriminatively
trained CD-GMM-HMM baseline.
The remainder of this paper is organized as follows. In
section II we brieﬂy introduce restricted Boltzmann machines
(RBMs) and deep belief nets, and outline the general pretraining strategy we use. In section III, we describe the
basic ideas, the key properties, and the training and decoding
strategies of our CD-DNN-HMMs. In section IV we analyze
experimental results on a 65K+ vocabulary business search
dataset collected from the Bing mobile voice search application (formerly known as Live Search for mobile , )
under real usage scenarios. Section V offers conclusions and
directions for future work.
II. DEEP BELIEF NETWORKS
Deep belief networks (DBNs) are probabilistic generative
models with multiple layers of stochastic hidden units above
a single bottom layer of observed variables that represent a
data vector. DBNs have undirected connections between the
top two layers and directed connections to all other layers from
the layer above. There is an efﬁcient unsupervised algorithm,
ﬁrst described in , for learning the connection weights in a
DBN that is equivalent to training each adjacent pair of layers
as an restricted Boltzmann machine (RBM). There is also a
fast, approximate, bottom-up inference algorithm to infer the
states of all hidden units conditioned on a data vector. After the
unsupervised, or pre-training phase, Hinton et al. used the
up-down algorithm to optimize all of the DBN weights jointly.
During this ﬁne-tuning phase, a supervised objective function
could also be optimized.
In this work, we use the DBN weights resulting from the
unsupervised pre-training algorithm to initialize the weights of
a deep, but otherwise standard, feed-forward neural network
and then simply use the backpropagation algorithm to
ﬁne-tune the network weights with respect to a supervised
criterion. Pre-training followed by stochastic gradient descent
is our method of choice for training deep neural networks
because it often outperforms random initialization for the
deeper architectures we are interested in training and provides
results very robust to the initial random seed. The generative
model learned during pre-training helps prevent overﬁtting,
even when using models with very high capacity and can aid
in the subsequent optimization of the recognition weights.
Although empirical results ultimately are the best reason for
the use of a technique, our motivation for even trying to ﬁnd
and apply deeper models that might be capable of learning
rich, distributed representations of their input is also based on
formal and informal arguments by other researchers in the
machine learning community. As argued in and ,
insufﬁciently deep architectures can require an exponential
blow-up in the number of computational elements needed to
represent certain functions satisfactorily. Thus one primary
motivation for using deeper models such as neural networks
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
with many layers is that they have the potential to be much
more representationally efﬁcient for some problems than shallower models like GMMs. Furthermore, GMMs as used in
speech recognition typically have a large number of Gaussians
with independently parameterized means which may result in
those Gaussians being highly localized and thus may result in
such models only performing local generalization. In effect,
such a GMM would partition the input space into regions each
modeled by a single Gaussian. proved that constant leaf
decision trees require a number of training cases exponential in
their input dimensionality to learn certain rapidly varying functions. also makes more general and less formal arguments
that models that create a single hard or soft partitioning of the
input space and use separately parameterized simple models
for each region are doomed to have similar generalization
issues when trained on rapidly varying functions. In a related
vein, also proves an analogous “curse of rapidly-varying
functions” for a large class of local kernel machines that
include both supervised learning algorithms (e.g., SVMs with
Gaussian kernels) and many semi-supervised algorithms and
unsupervised manifold learning algorithms. It is our fear that
functions important for solving difﬁcult perceptual tasks in
domains such as computer vision and computer audition will
have a componential structure that makes them vary rapidly
even though there is perhaps only a comparatively small number of factors that cause these variations. Although it remains
to be seen to what extent these arguments about architectural
depth and local generalization apply to speech recognition,
one of our hopes in this work is to demonstrate that replacing
GMMs with deeper models can reduce recognition error in
a difﬁcult LVSR task, even if we are unable to show that
our proposed system performs well because of some sort of
avoidance of the potential issues we discuss above.
A. Restricted Boltzmann Machines
Restricted Boltzmann Machines (RBMs) are a type of
undirected graphical model constructed from a layer of binary
stochastic hidden units and a layer of stochastic visible units
that, for the purposes of this work, will either be Bernoulli
or Gaussian distributed conditional on the hidden units. The
visible and hidden units form a bipartite graph with no visiblevisible or hidden-hidden connections. For concreteness, we
will assume the visible units are binary for the moment (we
always assume binary hidden units in this work) and describe
how we deal with real-valued speech data at the end of this
section. An RBM assigns an energy to every conﬁguration of
visible and hidden state vectors, denoted v and h respectively,
according to:
E(v, h) = −bTv −cTh −vTWh,
where W is the matrix of visible/hidden connection weights,
b is a visible unit bias, and c is a hidden unit bias. The
probability of any particular setting of the visible and hidden
units is given in terms of the energy of that conﬁguration by:
P(v, h) = e−E(v,h)
where the normalization factor Z = P
v,h e−E(v,h) is known
as the partition function.
The lack of direct connections within each layer enables us
to derive simple exact expressions for P(v|h) and P(h|v),
since the visible units are conditionally independent given the
hidden unit states and vice versa. We perform this derivation
for P(h|v) below. We will refer to the term in (3) dependent
on hi as γi(v, hi) = −(ci + vTW∗,i)hi, with W∗,i denoting
the ith column of W. Starting with the deﬁnition of P(h|v),
we obtain (see for another version of this derivation along
with other useful ones):
˜h e−E(v,˜h)
ebTv+cTh+vTWh
˜h ebTv+cT˜h+vTW˜h
˜h ecT˜h+vTW˜h
i ecihi+vTW∗,ihi
˜h1 · · · P
i eci˜hi+vTW∗,i˜hi
i e−γi(v,hi)
˜h1 · · · P
i e−γi(v,˜hi)
i e−γi(v,hi)
˜hi e−γi(v,˜hi)
e−γi(v,hi)
˜hi e−γi(v,˜hi)
Since the hi ∈{0, 1}, the sum in the denominator of equation
(5) has only two terms and thus
P(hi = 1|v) =
e−γi(v,1) + e−γi(v,0)
= σ(ci + vTW∗,i),
P(h = 1|v) = σ(c + vTW),
where σ denotes the (elementwise) logistic sigmoid, σ(x) =
(1 + e−x)−1. For the binary visible unit case to which we
restrict ourselves to at the moment, a completely symmetric
derivation lets us obtain
P(v = 1|h) = σ(b + hTWT).
The form of (7) is what allows us to use the weights of
an RBM to initialize a feed-forward neural network with
sigmoidal hidden units because we can equate the inference
for RBM hidden units with forward propagation in a neural
Before writing an expression for the log probability assigned
by an RBM to some visible vector v, it is convenient to deﬁne
a quantity known as the free energy:
F(v) = −log
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
Using F(v), we can write the per-training-case log likelihood
ℓ(θ) = −F(v) −log
with θ denoting the model parameters.
To train an RBM, we perform stochastic gradient descent
on the negative log likelihood. In the experiments in this work,
we use the following expression for the t + 1st weight update
for some typical model parameter wij:
∆wij(t + 1) = m∆wij(t) −α ∂ℓ
where α is the learning rate/step size and m is the “momentum” factor used to smooth out the weight updates. Unlike in
a GMM, in an RBM the gradient of the log likelihood of the
data is not feasible to compute exactly. The general form of
the derivative of the log likelihood of the data is:
∂θ ⟩data −⟨∂E
In particular, for the visible-hidden weight updates we have:
= ⟨vihj⟩data −⟨vihj⟩model
The ﬁrst expectation, ⟨vihj⟩data, is the frequency with which
the visible unit vi and the hidden unit hj are on together
in the training set and ⟨vihj⟩model is that same expectation
under the distribution deﬁned by the model. Unfortunately,
the term ⟨.⟩model takes exponential time to compute exactly,
so we are forced to use an approximation. Since RBMs are
in the intersection between Boltzmann machines and product
of experts models, they can be trained using contrastive divergence as described in . The one-step contrastive divergence
approximation for the gradient w.r.t. the visible-hidden weights
≈⟨vihj⟩data −⟨vihj⟩1
where ⟨.⟩1 denotes the expectation over one-step reconstructions. In other words, an expectation computed with samples
generated by running the Gibbs sampler (deﬁned using equations (7) and (8)) initialized at the data for one full step.
Similar update rules for the other model parameters are easy
to derive by simply replacing
∂wij = vihj in equation (11)
with the appropriate partial derivative of the energy function
(or by creating a hidden unit and a visible unit both with the
constant activation of one to derive the updates for the biases).
Although RBMs with the energy function of equation (3)
are suitable for binary data, in speech recognition the acoustic
input is typically represented with real-valued feature vectors. The Gaussian-Bernoulli restricted Boltzmann machine
(GRBM) only requires a slight modiﬁcation of equation (3)
(see for a generalization of RBMs to any distribution in
the exponential family). The GRBM energy function we use
in this work is given by:
E(v, h) = 1
2(v −b)T(v −b) −cTh −vTWh,
Note that equation 12 implicitly assumes that the visible units
have a diagonal covariance Gaussian noise model with a
variance of 1 on each dimension. In the GRBM case, equation
(7) does not change, but equation (8) becomes:
P(v|h) = N(v; b + hTWT, I),
where I is the appropriate identity matrix. However, when
actually training a GRBM and creating a reconstruction, we
never actually sample from the distribution above; we simply
set the visible units to be equal to their means. The only
difference between our training procedure for GRBMs using
the energy function in equation 12 and binary RBMs using the
energy function in equation 3 is how the reconstructions are
generated, all positive and negative statistics used for gradients
are the same.
B. Deep Belief Network Pre-training
Now that we have described using contrastive divergence
to train an RBM and the two types of RBMs we use in this
work, we will discuss how to perform deep belief network pretraining. Once we have trained an RBM on data, we can use
the RBM to re-represent our data. For each data vector, v, we
use equation (7) to compute a vector of hidden unit activation
probabilities h. We use these hidden activation probabilities as
training data for a new RBM. Thus each set of RBM weights
can be used to extract features from the output of the previous
layer. Once we stop training RBMs, we have the initial values
for all the weights of the hidden layers of a neural net with
a number of hidden layers equal to the number of RBMs
we trained. With pre-training complete, we add a randomly
initialized softmax output layer and use backpropagation to
ﬁne-tune all the weights in the network discriminatively. Since
only the supervised ﬁne-tuning phase requires labeled data,
we can potentially leverage a large quantity of unlabeled
data during pre-training, although this capability is not yet
important for our LVSR experiments due to the abundance
of weakly supervised data.
III. CD-DNN-HMM
Hidden Markov models (HMMs) have been the dominant
technique for LVSR for at least two decades. An HMM is
a generative model in which the observable acoustic features
are assumed to be generated from a hidden Markov process
that transitions between states S = {s1, · · · , sK}. The key
parameters in the HMM are the initial state probability distribution π = {p(q0 = si)}, where qt is the state at time t;
the transition probabilities aij = p(qt = sj|qt−1 = si); and a
model to estimate the observation probabilities p(xt|si).
In conventional HMMs used for ASR, the observation probabilities are modeled using GMMs. These GMM-HMMs are
typically trained to maximize the likelihood of generating the
observed features. Recently, discriminative training strategies
such as MMI , MCE , , MPE , , and large-margin
techniques – have been proposed. The potential of
these discriminative techniques, however, is restricted by the
limitations of the GMM emission distribution model. The
recently proposed CRF – and HCRF , models
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
use log-linear models to replace GMM-HMMs. These models
typically use manually designed features and have been shown
to be equivalent to the GMM-HMM in their modeling
ability if only the ﬁrst and second order statistics are used as
the features.
A. Architecture of CD-DNN-HMMs
Figure 1 illustrates the architecture of our proposed CD-
DNN-HMMs. The foundation of the hybrid approach is the
use of a forced alignment to obtain a frame level labeling for
training the ANN. The key difference between the CD-DNN-
HMM architecture and earlier ANN-HMM hybrid architectures (and context-independent DNN-HMMs) is that we model
senones as the DNN output units directly. The idea of using
senones as the modeling unit has been proposed in where
the posterior probabilities of senones were estimated using
deep-structured conditional random ﬁelds (CRFs) and only one
audio frame was used as the input of the posterior probability
estimator. This change offers two primary advantages. First,
we can implement a CD-DNN-HMM system with only minimal modiﬁcations to an existing CD-GMM-HMM system, as
we will show in section III-B. Second, any improvements in
modeling units that are incorporated into the CD-GMM-HMM
baseline system, such as cross-word triphone models, will be
accessible to the DNN through the use of the shared training
If DNNs can be trained to better predict senones, then CD-
DNN-HMMs can achieve better recognition accuracy than
tri-phone GMM-HMMs. More precisely, in our CD-DNN-
HMMs, the decoded word sequence ˆw is determined as
ˆw = argmax
p(w|x) = argmax
p(x|w)p(w)/p(x)
where p(w) is the language model (LM) probability, and
p(x, q|w)p(q|w)
∼= maxπ(q0)
is the acoustic model (AM) probability. Note that the observation probability is:
p(xt|qt) = p(qt|xt)p(xt)/p(qt),
where p(qt|xt) is the state (senone) posterior probability
estimated from the DNN, p(qt) is the prior probability of each
state (senone) estimated from the training set, and p(xt) is
independent of the word sequence and thus can be ignored.
Although dividing by the prior probability p(qt) (called scaled
likelihood estimation by , , ) may not give improved recognition accuracy under some conditions, we have
found it to be very important in alleviating the label bias
problem, especially when the training utterances contain long
silence segments.
Diagram of our hybrid architecture employing a deep neural network.
The HMM models the sequential property of the speech signal, and the DNN
models the scaled observation likelihood of all the senones (tied tri-phone
states). The same DNN is replicated over different points in time.
B. Training Procedure of CD-DNN-HMMs
CD-DNN-HMMs can be trained using the embedded Viterbi
algorithm. The main steps involved are summarized in Algorithm 1, which takes advantage of the triphone tying structures
and the HMMs of the CD-GMM-HMM system. Note that
the logical triphone HMMs that are effectively equivalent
are clustered and represented by a physical triphone (i.e.,
several logical triphones are mapped to the same physical
triphone). Each physical triphone has several (typically 3)
states which are tied and represented by senones. Each senone
is given a senoneid as the label to ﬁne-tune the DNN. The
state2id mapping maps each physical triphone state to the
corresponding senoneid.
To support the training and decoding of CD-DNN-HMMs,
we needed to develop a series of tools, the most important
of which were: 1) the tool to convert the CD-GMM-HMMs
to CD-DNN-HMMs, 2) the tool to do forced alignment using
CD-DNN-HMMs, and 3) the CD-DNN-HMM decoder. We
have found that it is relatively easy to develop these tools by
modifying the corresponding HTK tools if the format of the
CD-DNN-HMM model ﬁles is wisely speciﬁed.
In our speciﬁc implementation, each senone in the CD-
DNN-HMM is identiﬁed as a (pseudo) single Gaussian whose
dimension equals the total number of senones. The variance
(precision) of the Gaussian is irrelevant, so it can be set to
any positive value (e.g., always set to 1). The value of the ﬁrst
dimension of each senone’s mean is set to the corresponding
senoneid determined in Step 2 in Algorithm 1. The values of
other dimensions are not important and can be set to any value
such as 0. Using this trick, evaluating each senone is equivalent
to a table lookup of the features (log-likelihood) produced by
the DNN with the index indicated by the senoneid.
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
Algorithm 1 Main Steps to Train CD-DNN-HMMs
1: Train a best tied-state CD-GMM-HMM system where
state tying is determined based on the data-driven decision
tree. Denote the CD-GMM-HMM gmm-hmm.
2: Parse gmm-hmm and give each senone name an ordered
senoneid starting from 0. The senoneid will be served
as the training label for DNN ﬁne-tuning.
3: Parse gmm-hmm and generate a mapping from each
physical tri-phone state (e.g., b-ah+t.s2) to the corresponding senoneid. Denote this mapping state2id.
4: Convert gmm-hmm to the corresponding CD-DNN-
HMM dnn-hmm1 by borrowing the tri-phone and senone
structure as well as the transition probabilities from
5: Pre-train each layer in the DNN bottom-up layer by layer
and call the result ptdnn.
6: Use gmm-hmm to generate a state-level alignment on the
training set. Denote the alignment align-raw.
7: Convert align-raw to align where each physical triphone state is converted to senoneid.
8: Use the senoneid associated with each frame in align
to ﬁne-tune the DBN using back-propagation or other
approaches, starting from ptdnn. Denote the DBN dnn.
9: Estimate the prior probability p(si) = n(si)/n, where
n(si) is the number of frames associated with senone si
in align and n is the total number of frames.
10: Re-estimate the transition probabilities using dnn and
dnn-hmm1 to maximize the likelihood of observing the
features. Denote the new CD-DNN-HMM dnn-hmm2.
11: Exit if no recognition accuracy improvement is observed in the development set; Otherwise use dnn and
dnn-hmm2 to generate a new state-level alignment
align-raw on the training set and go to Step 7.
IV. EXPERIMENTAL RESULTS
To evaluate the proposed CD-DNN-HMMs and to understand the effect of different decisions made at each step
of CD-DNN-HMM training, we have conducted a series of
experiments on a business search dataset collected from the
Bing mobile voice search application (formerly known as Live
Search for mobile ) – a real-world large-vocabulary
spontaneous speech recognition task. In this section, we report
our experimental setup and results, demonstrate the efﬁcacy of
the proposed approach, and analyze the training and decoding
A. Dataset Description
The Bing mobile voice search application allows users to do
US-wide business and web search from their mobile phones
via voice. The business search dataset used in our experiments
was collected under real usage scenarios in 2008, at which
time the application was restricted to do location and business
lookup. All audio ﬁles collected were sampled at 8 kHz and
encoded with the GSM codec. Some examples of typical
queries in the dataset are “Mc-Donalds,” “Denny’s restaurant,”
and “oak ridge church.” This is a challenging task since the
INFORMATION ON THE BUSINESS SEARCH DATASET
Number of Utterances
Training Set
Development Set
dataset contains all kinds of variations: noise, music, sidespeech, accents, sloppy pronunciation, hesitation, repetition,
interruption, and different audio channels.
The dataset was split into a training set, a development set,
and a test set. To simulate the real data collection and training
procedure, and to avoid having overlap between training,
development, and test sets, the dataset was split based on
the time stamp of the queries. All queries in the training set
were collected before those in the development set, which
were in turn collected before those in the test set. For the
sake of easy comparisons, we have used the public lexicon
from Carnegie Mellon University. The normalized nationwide
language model (LM) used in the evaluation contains 65K
word unigrams, 3.2 million word bi-grams, and 1.5 million
word tri-grams, and was trained using the data feed and
collected query logs; the perplexity is 117.
Table I summarizes the number of utterances and total
duration of audio ﬁles (in hours) in the training, development,
and test sets. All 24 hours of training data included in the
training set are manually transcribed. We used 24 hours of
training data in this study since it lets us run more experiments
(training our CD-DNN-HMM systems is time consuming
compared to training CD-GMM-HMMs).
Performance on this task was evaluated using sentence
accuracy (SA) instead of word accuracy for a variety of
reasons. In order to compare our results with , we would
need to compute sentence accuracy anyway. The average
sentence length is 2.1 tokens, so sentences are typically quite
short. Also, the users care most about whether they can ﬁnd
the business or location they seek in the fewest attempts. They
typically will repeat what they have said if one of the words is
mis-recognized. Additionally, there is signiﬁcant inconsistency
in spelling that makes using sentence accuracy more convenient. For example, “Mc-Donalds” sometime is spelled as
“McDonalds,” “Walmart” sometimes is spelled as “Wal-mart”,
and “7-eleven” sometimes is spelled as “7 eleven” or “seveneleven”. For these reasons, when calculating sentence accuracy
we concatenate all the words in the utterance and remove
hyphens and apostrophes before comparing the recognition
outputs with the references so that we can remove some of
the effects caused by the LM and poor text normalization and
focus on the AM. The sentence out-of-vocabulary rate (OOV)
using the 65K vocabulary LM is 6% on both the development
and test sets. In other words, the best possible SA we can
achieve is 94% using this setup.
B. CD-GMM-HMM Baseline Systems
To compare our proposed CD-DNN-HMM model with
standard discriminatively trained, GMM-based systems, we
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
THE CD-GMM-HMM BASELINE RESULTS
Dev Accuracy
Test Accuracy
have trained clustered cross-word triphone GMM-HMMs with
maximum likelihood (ML), maximum mutual information
(MMI), and minimum phone error (MPE) criteria. The 39-dim
features used in the experiments include the 13-dim static Melfrequency cepstral coefﬁcient (MFCC) (with C0 replaced with
energy) and its ﬁrst and second derivatives. The features were
pre-processed with the cepstral mean normalization (CMN)
algorithm.
We optimized the baseline systems by tuning the tying structures, number of senones, and Gaussian splitting strategies on
the development set. The performance of the best CD-GMM-
HMM conﬁguration is summarized in table II. All systems
reported in II have 53K logical and 2K physical tri-phones with
761 shared states (senones), each of which is a GMM with
24 mixture components. Note that our ML baseline of 60.4%
trained using 24 hours of data is only 2.5% worse than the
62.9% obtained in , even though the latter used 130 hours
of manually transcribed data and about 2000 hours of userclick conﬁrmed data (90% accuracy). This small difference
in accuracy indicates that the baseline we compare with in
this paper is not weak. Since we did not personally obtain the
result from , there may be other differences between our
setup and the one used in in addition to the larger training
The discriminative training of the CD-GMM-HMM was
carried out using the HTK.2 The lattices were generated using
HDecode3 and, when generating the lattices, the weak word
unigram LM estimated from the training transcription was
used. As shown in table II, the MPE-trained CD-GMM-HMM
outperformed both the ML- and MMI-trained CD-GMM-
HMM with a sentence accuracy of 65.5% and 63.8% on the
development and test sets respectively.
C. CD-DNN-HMM Results and Analysis
Many decisions need to be made when training CD-DNN-
HMMs. In this sub-section, we will examine how these choices
affect recognition accuracy. In particular, we will empirically
compare the performance difference between using a monophone alignment and a tri-phone alignment, using monophone
state labels and tri-phone senone labels, using 1.5K and 2K
hidden units in each layer, using an ANN-HMM and a DNN-
HMM, and tuning and not tuning the transition probabilities.
For all experiments reported below, we have used 11 frames
2The lattice probability scale factor LATPROBSCALE was set to 1/LMW
parameters
ISMOOTHTAU,
ISMOOTHTAUT, and ISMOOTHTAUW were set to 100, 10, and 10 respectively
for the MMI training, and 50, 10, and 10 respectively for the MPE training.
3We used HDecode.exe with command line parameters “-t 250.0 -v 200.0
-u 5000 -n 32 -s 15.0 -p 0.0” for the denominator and “-t 1500.0 -n 64 -s
15.0 -p 0.0” for the numerator.
PERFORMANCE OF SINGLE HIDDEN LAYER MODELS USING MONOPHONE
AND TRIPHONE HMM ALIGNMENT LABELS
# Hidden Units
Dev Accuracy
Monophone State
Monophone State
COMPARISON OF CONTEXT-INDEPENDENT MONOPHONE STATE LABELS
AND CONTEXT-DEPENDENT TRIPHONE SENONE LABELS
Monophone States
Triphone Senones
Monophone States
Triphone Senones
(5-1-5) of MFCCs as the input features of the DNNs, following
 and . During pre-training we used a learning rate of
0.004 for all layers. For ﬁne-tuning, we used a learning rate
of 0.08 for the ﬁrst 6 epochs and a learning rate of 0.002 for
the last 6 epochs. In all our experiments, we averaged updates
over minibatchs of 256 training cases before applying them.
To all weight updates, we added a “momentum” term of 0.9
times the previous update (see equation 9). We selected the
values of these hyperparameters by hand, based on preliminary
single hidden layer experiments so it may be possible to obtain
even better performance with the deeper models using a more
exhaustive hyperparameter search.
Our ﬁrst experiment used an alignment generated from a
monophone GMM-HMM and used the monophone states as
the DNN training labels. Such a setup only achieved 55.5%
sentence accuracy on the development set if a single 1.5K
hidden layer is used, as shown in table III. Switching to
an alignment generated from an ML-trained triphone GMM-
HMM, but still using monophone states as labels for the DNN,
increased accuracy to 59.1%.
The performance can be further improved to 59.3% if we
use 2K instead of 1.5K hidden units, as shown in table IV.
However, an even larger performance improvement occurred
when we used triphone senones as the DNN training labels,
which yields 68.1% sentence accuracy on the development set,
even with only one hidden layer. Note that this accuracy is
already 2.6% higher than the 65.5% achieved using the MPEtrained CD-GMM-HMMs. The accuracy increased to 69.6%
when three hidden layers were used. Table IV shows that
models trained using senone labels perform much better than
those trained using monophone state labels when either one
or three hidden layers were used. Using senone labels has
been the single largest source of improvement of all the design
decisions we analyzed.
An obvious question to ask is whether the pre-training step
in the DNN is truly necessary or helpful. To answer this
question, we compared CD-DNN-HMMs with and without
pre-training in table V. As expected, if only one hidden
layer was used, systems with and without pre-training have
comparable performance. However, when two hidden layers
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
CONTEXT-DEPENDENT MODELS WITH AND WITHOUT PRE-TRAINING
without pre-training
without pre-training
with pre-training
with pre-training
were used, the accuracy of 69.6% obtained with pre-training
applied noticeably surpassed the accuracy of 68.2% obtained
without pre-training on the development set. The pre-trained
two layer model had a frame-level misclassiﬁcation rate of
31.13%, whereas the un-pre-trained two layer model had a
frame-level misclassiﬁcation rate of 32.83%. The cross entropy
loss per case of the two hidden layer models was 1.73 and 1.18
bits, respectively. Our general anecdotal experience (built in
part from other speech datasets) has been that pre-training on
acoustic data never hurts the frame-level error of models we try
and can be especially helpful when using very large models.
Even the largest models we use in this work are comparable
in size to ones used on TIMIT by , even though we use a
much larger dataset here. We hope to use much larger models
still in the future and make better use of the regularization
effect of generative pre-training. That being said, the pretraining phase seems to give a clear improvement in the two
hidden layer experiment we describe in table V.
Figure 2 demonstrates how the sentence accuracy improves
as more layers are added in the CD-DNN-HMM. When three
hidden layers were used, the accuracy increased to 69.6%.
The accuracy further improved to 70.2% with four hidden
layers and 70.3% with ﬁve hidden layers. Overall, using the
ﬁve hidden-layer models provides us with a 2.2% accuracy
improvement over the single hidden-layer system when the
same alignment is used. Although it is possible that using
even more than ﬁve hidden layers would continue to improve
the accuracy, we expect any such gains to be modest at best,
so we restricted ourselves to at most ﬁve hidden layers in the
rest of this work.
In order to demonstrate the efﬁciency of parameterization
enjoyed by deeper neural networks, we have also trained a
single hidden layer neural network with 16K hidden units, a
number chosen to guarantee that the weights required a little
more space to store than the weights for our 5 hidden layer
models. We were able to obtain an accuracy of 68.6% on the
development set, which is slightly more than the 2K hidden
unit single layer result of 68.1% in ﬁgure 2, but well below
even the two layer result of 69.5% (let alone the ﬁve layer
result of 70.3%).
Table VI shows our results after the main steps of Algorithm
1. All systems in table VI use a DNN with ﬁve hidden layers of
2K units each and senone labels. As we have shown in table
III, using a better alignment to generate training labels for
the DNN can improve the accuracy. This observation is also
conﬁrmed in table VI. Using alignments generated with MPEtrained CD-GMM-HMMs, we can obtain 70.7% and 68.8%
accuracies on the development and test sets, respectively.
The relationship between the recognition accuracy and the number of
layers. Context-dependent models with 2K hidden units per layer were used
to obtain the results.
EFFECTS OF ALIGNMENT AND TRANSITION PROBABILITY TUNING ON
BEST DNN ARCHITECTURE
Tune Trans.
from CD-GMM-HMM ML
from CD-GMM-HMM MPE
from CD-GMM-HMM MPE
from CD-DNN-HMM
from CD-DNN-HMM
These results are 0.4% higher than those we achieved using
the ML CD-GMM-HMM alignments.
Table VI also demonstrates that tuning the transition probabilities in the CD-DNN-HMMs also seems to help slightly.
Tuning the transition probabilities comes with another beneﬁt.
When we use transition probabilities directly borrowed from
the CD-GMM-HMMs, the best decoding performance usually
was obtained when the AM weight was set to 2. However,
after tuning the transition probabilities, we no longer need to
tune the AM weights.
Once we have trained our best CD-DNN-HMM using a CD-
GMM-HMM alignment, we can use the CD-DNN-HMM to
generate an even better alignment. Table VI shows that the
accuracies on the development and test sets can be increased
to 71.7% and 69.6%, respectively, from 71.0% and 69.0%,
which were obtained using dnn-hmm1. Tuning the transition
probabilities again only marginally improves the performance.
Overall, our proposed CD-DNN-HMMs obtained 69.6% accuracy on the test set, which is 5.8% (or 9.2%) higher than those
obtained using the MPE (or ML)-trained CD-GMM-HMMs.
This improvement translates to a 16.0% (or 23.2%) relative
error rate reduction over the MPE (or ML)-trained CD-GMM-
HMMs and is statistically signiﬁcant at signiﬁcant level of 1%
according to McNemar’s test.
D. Training and Decoding Time
We have just shown that CD-DNN-HMMs substantially outperform CD-GMM-HMMs in terms of recognition accuracy
on our task. A natural question to ask is whether the gain
was obtained at a signiﬁcantly higher computational cost for
training and decoding.
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
SUMMARY OF TRAINING TIME USING 24 HOURS OF TRAINING DATA
AND 2K HIDDEN UNITS PER LAYER
# of Layers
Time Per Epoch
# of Epochs
Table VII summarizes the DNN training time using 24
hours of training data, 2K hidden units, and 11 frames of
MFCCs as input features. The time recorded in the table is
based on a trainer written in Python. The training was carried
out on a Dell Precision T3500 workstation, which is a quad
core computer with a CPU clock speed of 2.66GHz, 8MB
of L3 CPU cache, and 12GB of 1066MHz DDR3 SDRAM.
The training also used an NVIDIA Tesla C1060 general
purpose graphical processing unit (GPGPU), which contains
4GB of GDDR3 RAM and 240 processing cores. We used the
CUDAMat library to perform matrix operations on the
GPU from our Python code.
From table VII we can observe that to train a ﬁve-layer CD-
DNN-HMM, pre-training takes about 0.2×50+0.5×20+0.6×
20 + 0.7 × 20 + 0.8 × 20 = 62 hours. Fine-tuning takes about
1.4 × 12 = 16.8 hours. To achieve the best result reported
in this paper, we have to run two passes of ﬁne-tuning, one
with the MPE CD-GMM-HMM alignment, and one with the
CD-DNN-HMM alignment. The total ﬁne-tuning time is thus
16.8 × 2 = 33.6 hours. To train the system, we also need to
spend time to normalize the MFCC features to allow each to
have zero-mean and unit-variance, and to generate alignments.
However, these tasks can be easily parallelized and the time
spent on them is very small compared to the DNN training
time. The total time spent to train the system from scratch is
about four days. We have observed that using a GPU speeds
up training by about a factor of 30 faster than just using the
CPU in our setup. Without using a GPU, it would take about
three months to train the best system.
The bottleneck in the training process is the mini-batch
stochastic gradient descend (SGD) algorithm used to train
the DNNs. SGD is inherently sequential and is difﬁcult to
parallelize across machines. So far SGD with a GPU is the
best training strategy for CD-DNN-HMMs since the GPU at
least can exploit the parallelism in the layered DNN structure.
When more training data is available, the time spent on
each epoch increases. However, fewer epochs will be needed
when more training data is available. We speculate that using
a strategy similar to our current one described in this paper,
it should be possible to train an effective CD-DNN-HMM
system that exploits 2000 hours of training data in about 50
days (using a single GPU).
While training is considerably more expensive than for
CD-GMM-HMM systems, decoding is still very efﬁcient.
Table VIII summarizes the decoding time on our four and
TABLE VIII
SUMMARY OF DECODING TIME
Processing
Search Time
ﬁve-layer 2K hidden unit CD-DNN-HMM systems with and
without using GPUs. Note that in our implementation, the
search is always done using CPUs. It takes only 0.58 and
0.67 times real time to decode with four and ﬁve-layer CD-
DNN-HMMs, respectively, without using GPUs. Using a GPU
reduces decoding time to 0.17 times real time, at which point
DNN computations no longer dominate. For reference, our
baseline CD-GMM-HMM system decodes in 0.54 times real
V. CONCLUSION AND FUTURE WORK
We have described a context-dependent DNN-HMM model
for LVSR that achieves substantially better results than strong,
discriminatively trained CD-GMM-HMM baselines on a challenging business search dataset. Although our experiments
show that CD-DNN-HMMs provide dramatic improvements
in recognition accuracy, training CD-DNN-HMMs is quite
expensive compared to training CD-GMM-HMMs (although
on a similar scale as other neural-network-based acoustic
models and certainly feasible for large datasets, if one can
afford weeks of training time). This is primarily because the
CD-DNN-HMM training algorithms we have discussed are not
easy to parallelize across computers and need to be carried out
on a single GPU machine. That being said, decoding in CD-
DNN-HMMs is very efﬁcient so test time is not an issue in
real-world applications.
We believe our work on CD-DNN-HMMs is only the ﬁrst
step towards a more powerful acoustic model for LVSR;
many issues remain to be resolved. Here are a few we
view as particularly important. First, although CD-DNN-HMM
training is asymptotically quite scalable, in practice it is quite
challenging to train CD-DNN-HMMs on tens of thousands of
hours of data. To achieve this level of practical scalability,
we must parallelize training not just at the matrix arithmetic
level. Finding new ways to parallelize training may require a
better theoretical understanding of deep learning. Second, we
must ﬁnd highly effective speaker and environment adaptation
algorithms for DNN-HMMs, ideally ones that are completely
unsupervised and integrated with the pre-training phase. Inspiration for such algorithms may come from the ANN-HMM
literature (e.g. , ) or the many successful adaptation
techniques developed in the past decades for GMM-HMMs
(e.g., MLLR , MAP , joint compensation of distortions
 , variable parameter HMMs ). Third, the training in
this study used the embedded Viterbi algorithm, which is not
optimal. We believe additional improvement may be achieved
by optimizing an objective function based on the full sequence,
as we have already demonstrated on the TIMIT dataset with
DRAFT ACCEPTED BY IEEE TRANS. ON AUDIO, SPEECH, AND LANGUAGE PROCESSING
some success . In addition, we view the treatment of the
time dimension of speech by DNN-HMM and GMM-HMMs
alike as a very crude way of dealing with the intricate temporal
properties of speech. The weaknesses in how HMMs deal with
the temporal dimension of speech inputs have been analyzed in
detail in – . There is a vast space to explore in the deep
learning framework using the insights gained from temporalcentric generative modeling research in neural networks and in
speech (e.g., , , , ). Finally, although Gaussian
RBMs can learn an initial distributed representation of their
input, they still produce a diagonal covariance Gaussian for
the conditional distribution over the input space given the
latent state (as diagonal covariance GMMs also do). A more
powerful ﬁrst layer model, namely the mean-covariance restricted Boltzmann machine signiﬁcantly enhanced the
performance of context-independent DNN-HMMs for phone
recognition in . We therefore view applying similar models
to LVSR as an enticing area of future work.
ACKNOWLEDGMENT
The authors would like to thank Dr. Patrick Nguyen at Microsoft Research for preparing the dataset, providing the MLtrained CD-GMM-HMM baseline, and engaging in valuable
discussions. Thanks also go to Dr. Chaojun Liu at Microsoft
Corporation speech product team for his assistance in getting
the discriminatively-trained CD-GMM-HMM baselines, Dr.
Jasha Droppo at Microsoft Research for his parallel computing
support, Prof. Geoff Hinton at University of Toronto for advice
and encouragement of this work, and Prof. Nelson Morgan
at University of California-Berkeley for discussions on prior
work on ANN-HMM approaches.