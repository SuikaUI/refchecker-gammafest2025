IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Hierarchical Image Saliency Detection
on Extended CSSD
Jianping Shi, Student Member, IEEE Qiong Yan, Member, IEEE Li Xu, Member, IEEE
Jiaya Jia, Senior Member, IEEE
Abstract—Complex structures commonly exist in natural images. When an image contains small-scale high-contrast patterns
either in the background or foreground, saliency detection could be adversely affected, resulting erroneous and non-uniform
saliency assignment. The issue forms a fundamental challenge for prior methods. We tackle it from a scale point of view and
propose a multi-layer approach to analyze saliency cues. Different from varying patch sizes or downsizing images, we measure
region-based scales. The ﬁnal saliency values are inferred optimally combining all the saliency cues in different scales using
hierarchical inference. Through our inference model, single-scale information is selected to obtain a saliency map. Our method
improves detection quality on many images that cannot be handled well traditionally. We also construct an extended Complex
Scene Saliency Dataset (ECSSD) to include complex but general natural images.
Index Terms—saliency detection, region scale
INTRODUCTION
Regarding saliency, each existing method mainly focuses on one of the following tasks – i.e., eye-
ﬁxation prediction, image-based salient object detection and objectness estimation. Among them, imagebased salient object detection , , , , 
is an important stream, which can beneﬁt several
applications including detection , classiﬁcation
 , retrieval , and object co-segmentation ,
for optimizing and saving computation. The goal is
to detect and segment out important regions from
natural images.
By deﬁning pixel/region uniqueness in either local or
global context, existing image salient object detection
methods can be classiﬁed to two categories. Local
methods , rely on pixel/region difference in
the vicinity, while global methods , , rely
mainly on color uniqueness statistically.
Albeit many methods were proposed, a few common
issues still endure. They are related to complexity
of patterns in natural images. A few examples are
shown in Fig. 1. For the ﬁrst two examples, the boards
containing characters are salient foreground objects.
But the results in (b), produced by a previous local
method, only highlight a few edges that scatter in the
image. The global method results in (c) also cannot
clearly distinguish among regions. Similar challenge
arises when the background is with complex patterns,
as shown in the last example of Fig. 1. The yellow
ﬂowers lying on grass stand out by previous methods.
• Jianping Shi and Jiaya Jia are with the Department of Computer Science
and Engineering, The Chinese University of Hong Kong. The contact
is in 
But they are actually part of the background when
viewing the picture as a whole.
These examples are not special, and exhibit one common problem – that is, when objects contain salient
small-scale patterns, saliency could generally be misled
by their complexity. Given texture existing in many
natural images, this problem cannot be escaped. It
easily turns extracting salient objects to ﬁnding cluttered fragments of local details, complicating detection and making results not usable in, for example,
object recognition , where connected regions with
reasonable sizes are favored.
Aiming to solve this notorious and universal problem, we propose a hierarchical framework, to analyze
saliency cues from multiple levels of structure, and
then integrate them for the ﬁnal saliency map through
hierarchical inference. Our framework ﬁnds foundation from studies in psychology , , which
show the selection process in human attention system
operates from more than one levels, and the interaction between levels is more complex than a feedforward scheme. Our multi-level analysis helps deal
with salient small-scale structures. The hierarchical inference plays an important role in fusing information
to get accurate saliency maps.
Our contributions in this paper also include 1) a new
measure of region scales, which is compatible with
human perception on object scales, and 2) extension
of Complex Scene Saliency Dataset (CSSD), which
contains 1000 challenging natural images for saliency
detection. Our method yields improvement over others on the new extended CSSD dataset as well as other
benchmark datasets.
This manuscript extends the conference version 
with the following major differences. First, we provide
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(b) AC 
(c) RC 
(e) Ground truth
Fig. 1. Saliency detection with structure confusion. Small-scale strong details easily inﬂuence the process and
cause erroneous results.
more analysis on region scale computation and region
merge. Second, we build a new hierarchical inference
model with a local consistency scheme, which leads
to more natural saliency results compared to previous
tree-structured model. Further, we build an extended
Complex Scene Saliency Dataset (ECSSD) with more
challenging natural images.
The rest of the paper is organized as follows. Section
2 reviews literature in saliency detection. In Section
3, we introduce our hierarchical solutions for saliency
detection. We conduct experiments in Section 4 and
conclude this paper in Section 5.
RELATED WORK
Saliency analysis generally follows eye ﬁxation location and object-based attention formation . Eye
ﬁxation location methods physically obtain human
attention shift continuously with eye tracking, while
object-based approaches aim to ﬁnd salient objects
from the input. The salient object detection is further
extended to “objectness estimation” in object recognition. Both of them are important and beneﬁt different
applications in high-level scene analysis. Extensive
review was provided in , . Below we discuss
a few. Note that in this paper we only address imagebased salient object detection problem.
Eye Fixation Prediction and Objectness
Eye ﬁxation methods compute a saliency map to match eye
movement. The early method used an image
pyramid to calculate pixel contrast based on color
and orientation features. Ma and Zhang directly
computed center-surrounding color difference in a
ﬁxed neighborhood for each pixel. Harel et al. 
proposed a method to non-linearly combine local
uniqueness maps from different feature channels to
concentrate conspicuity. Judd et al. combined
high level human detector and center priors into eye
ﬁxation prediction. Borji and Itti considered local
and global image patch rarities in two color space,
and fuse information.
Objectness is anther direction on saliency detection. It
is to ﬁnd potential objects based on the low level
clues of the input image independent of their classes.
In particular, it measures whether a particular bounding box represents an object. Endres and Hoiem 
proposed an object proposal method based on segmentation. Alexe et al.
 integrated saliency clues
for object prediction. Cheng et al.
 developed a
gradient feature for objectness estimation.
Salient Object Detection
Salient object detection,
different from above problems, segments salient objects out. Local methods extract saliency features regarding a neighboring region. In , three patchbased features are learned and connected via conditional random ﬁeld. Achanta et al. deﬁned local
pixel saliency using local luminance and color. This
method needs to choose an appropriate surrounding
patch size. Besides, high-contrast edges are not necessarily in the foreground as illustrated in Fig. 1.
Global methods mostly consider color statistics. Zhai
and Shah introduced image histograms to calculate color saliency. To deal with RGB color, Achanta
et al. provided an approximate by subtracting the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(a) Input image
(b) Final saliency map
(c) Image layers
(d) Cue maps
(e) Inference
An overview of our hierarchical framework. We extract three image layers from the input, and then
compute saliency cues from each of these layers. They are ﬁnally fed into a hierarchical inference model to get
the ﬁnal results.
average color from the low-pass ﬁltered input. Cheng
et al. extended the histogram to 3D color space.
These methods ﬁnd pixels/regions with colors much
different from the dominant one, but do not consider
spatial locations. To compensate the lost spatial information, Perazzi et al. measured the variance
of spatial distribution for each color. Global methods
have their difﬁculty in distinguishing among similar
colors in both foreground and background. Recent
methods exploit background smoothness , .
Note smooth structure assumption could be invalid
for many natural images, as explained in Section 1.
High-level priors were also used based on common knowledge and experience. Face detector was
adopted in , . The concept of center bias – that
is, image center is more likely to contain salient objects
than other regions – was employed in , , .
In , it is assumed that warm colors are more attractive to human. Learning techniques are popular in
several recent methods , , . Unique features
or patterns are learned from a large set of labeled
images or a single image in an unsupervised manner.
 links the eye ﬁxation prediction and
salient object detection via segmentation candidates.
Prior work does not consider the situation that locally smooth regions could be inside a salient object
and globally salient color, contrarily, could be from
the background. These difﬁculties boil down to the
same type of problems and indicate that saliency is
ambiguous in one single scale. As image structures
exhibit different characteristics when varying resolutions, they should be treated differently to embody
diversity. Our hierarchical framework is a uniﬁed one
to address these issues.
HIERARCHICAL FRAMEWORK
Our method starts from layer extraction, by which
we extract images of different scales from the input.
Then we compute saliency cues for each layer, which
are then used to infer the ﬁnal saliency conﬁdence
in a local consistent hierarchical inference model. The
framework is illustrated in Fig. 2.
Image Layer Extraction
Image layers, as shown in Fig. 2(c), are coarse representation of the input with different degrees of details,
balancing between expression capability and structure
complexity. The layer number is ﬁxed to 3 in our
experiments. In the bottom level, ﬁnest details such
as ﬂower are retained, while in the top level largescale structures are produced.
Layer Generation
To produce the three layers, we ﬁrst generate an
initial over-segmentation as illustrated in Fig. 3(b) by
the watershed-like method . For each segmented
region, we compute a scale value, where the process
is elaborated on in the next subsection. They enable
us to apply an iterative process to merge neighboring
segments. Speciﬁcally, we sort all regions in the initial
map according to their scales in an ascending order. If
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(b) Over-segmentation
(c) Layer L1
(d) Layer L2
(e) Layer L3
Fig. 3. Region-merge results under different scales.
Our region scale is deﬁned as the largest
square that a region can contain. In this illustration, the
scales of regions a and b are less than 5, and that of c
is larger than 5.
a region scale is below a selected threshold, we merge
it to its nearest region, in terms of average CIELUV
color distance, and update its scale. We also update
the color of the region as their average color. After
all regions are processed, we take the resulting region
map as the bottom layer, denoted as L1. The superscript here indexes the ﬁrst layer among the three
ones we operate. In what follows without further
explanation, the super-script is the layer index.
The middle and top layers L2 and L3 are generated
similarly from L1 and L2 with larger scale thresholds.
In our experiment, we set thresholds for the three
layers as {5, 17, 33} for typical 400×300 images. Three
layers are shown in Fig. 3(c)-(e). More details on scale
computation and region merge are described in the
following subsections. Note a region in the middle or
top layer embraces corresponding ones in the lower
levels. We use the relationship for saliency inference
described in Section 3.3.
Region Scale Deﬁnition
In methods of , and many others, the region
size is measured by the number of pixels. Our research and extensive experiments suggest this measure could be wildly inappropriate for processing and
understanding general natural images. In fact, a large
pixel number does not necessarily correspond to a
large-scale region in human perception.
An example is shown in Fig. 4. Long curved region
a contains many pixels. But it is not regarded as
a large region in human perception due to its high
inhomogeneity. Region b could look bigger although
its pixel number is not larger. With this fact, we deﬁne
a new encompassment scale measure based on shape
Efﬁcient computation of scale transform. (a)
Initial region map. (b) Map labels and the box ﬁlter. (c)
Filtered region map. As shown in (c), all colors in R1
are updated compared to the input, indicating a scale
smaller than 3.
uniformities and use it to obtain region sizes in the
merging process.
Region R encompassing region R′ means
there exists at least one location to put R′ completely inside
R, denoted as R′ ⊆R.
With this relation, we deﬁne the scale of region R as
scale(R) = arg max
t {Rt×t|Rt×t ⊆R},
where Rt×t is a t×t square region. In Fig. 4, the scales
of regions a and b are smaller than 5 while the scale
of c is above it.
Efﬁcient Algorithm to Compute Region Scale
To determine the scale for a region, naive computation
following the deﬁnition in Eq. (1) needs exhaustive
search and comparison, which could be costly. In fact,
in the merging process in a level, we only need to
know whether the scale of a region is below the given
threshold t or not. We resort to a fast method by
spatial convolution.
Given a map M with each pixel labeled by its region
index in the region list R, we apply a box ﬁlter kt of
size t × t, which produces a blurred map kt ◦M (◦
denotes 2D convolution).
With computation of absolute difference Dt = |M −
kt ◦M|, we screen out regions in R with their scales
smaller than t. The scale for a region Ri is smaller
than t if and only if
y {Dt(y)|y ∈Ri}
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Algorithm 1 Scale Estimation
1: input: Region list R, scale threshold t
2: Create a map M with each pixel labeled by its
region index in R;
3: Create a box ﬁlter kt of size t × t;
4: Dt ←|M −kt ◦M|;
6: for each region Ri in R do
x ←miny{Dt(y)|y ∈Ri};
If x > 0 then Rt ←Rt
9: end for
10: output: Region list Rt
where y indexes pixels. It is based on the observation
that if all the label values for region Ri in M are
altered after the convolution, Ri cannot encompass
kt. Thus, the scale of the region is smaller than t.
We present the scale estimation process in Algorithm
1. After obtaining regions whose scales are smaller
than t, we merge each of them to its closest neighboring region in CIELUV color space. The merging
process is shown in Algorithm 2.
Single-Layer Saliency Cues
For each layer we extract, saliency cues are applied
to ﬁnd important regions from the perspectives of
color, position and size. We present two cues that are
particularly useful.
Local contrast
Image regions contrasting their surroundings are general eye-catching . We deﬁne the local contrast
saliency cue for Ri in an image with a total of n
regions as a weighed sum of color difference from
other regions:
w(Rj)φ(i, j)||ci −cj||2,
where ci and cj are colors of regions Ri and Rj
respectively. w(Rj) counts the number of pixels in
Rj. Regions with more pixels contribute higher localcontrast weights than those containing only a few
pixels. φ(i, j) is set to exp{−D(Ri, Rj)/σ2} controlling
the spatial distance inﬂuence between two regions
i and j, where D(Ri, Rj) is a square of Euclidean
distances between region centers of Ri and Rj. With
the φ(i, j) term, close regions have larger impact than
distant ones. Hence, Eq. (3) measures color contrast
mainly to surroundings. Parameter σ2 controls how
large the neighborhood is. It is set to the product of
(0.2)2 and the particular scale threshold for the current
layer. In the top layer, σ2 is large, making all regions
be compared in a near-global manner.
Algorithm 2 Region Merge
1: input: Region list R, scale threshold t
Get region list Rt by Algorithm 1;
for each region Ri in Rt do
Find the neighboring region Rj ∈R with
the minimum Euclidian distance to Ri in
CIELUV color space;
Merge Ri to Rj;
Set the color of Rj to the average of Ri and
9: until Rt = ∅
10: output: Region list R
Location heuristic
Human attention favors central regions . So pixels
close to the image center could be good candidates,
which have been exploited in , . Our location
heuristic is written as
exp{−λ∥xi −xc∥2},
where {x0, x1 · · · } is the set of pixel coordinates in
region Ri, and xc is the coordinate of the image center.
Hi makes regions close to image center have large
weights. λ is a parameter used when Hi is combined
with Ci, expressed as
¯si = Ci · Hi.
Since the local contrast and location cues have been
normalized to range [0, 1), their importance is balanced by λ, set to 9 in general. After computing ¯si for
all layers, we obtain initial saliency maps separately,
as demonstrated in Fig. 6(b)-(d).
In what follows, we describe how Fig. 6(e) is obtained
from the three single-layer saliency maps through our
local consistent hierarchical inference. This strategy is
updated from the one presented in our conference version paper in both construction and optimization.
It leads to improved performance.
Local Consistent Hierarchical Inference
Cue maps reveal saliency in different scales and could
be quite different. At the bottom level, small regions
are produced while top layers contain large-scale
structures. Due to possible diversity, none of the single
layer information is guaranteed to be perfect. It is also
hard to determine which layer is the best by heuristics.
Multi-layer fusion by naively averaging all maps
is not a good choice, considering possibly complex
background and/or foreground. Note in our region
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(b) Cue map in L1
(c) Cue map in L2
(d) Cue map in L3
(e) Final saliency
Fig. 6. Saliency cue maps in three layers and our ﬁnal saliency map.
merging steps, a segment is guaranteed to be encompassed by the corresponding ones in upper levels.
This makes a hierarchy of regions in different layers
naturally form. An example is shown in Fig. 2(e). In
the graph, the nodes in three layers correspond to
regions from the three image layers. The connection
between them in neighboring layers is due to the
“belonging” relationship. For instance, the blue node
j corresponds to the blue region in (d). It contains two
segments in the lower level and thus introduces two
children nodes, marked red and green respectively.
Without considering the connections between nodes
in the same layer, the graph actually can be seen as a
tree structure after adding a virtual node representing
the entire image. The structure inspires a hierarchical
inference model to take into account the inﬂuence
of regions from neighboring layers, so that largescale structures in upper layers can guide saliency
assignment in lower layers.
In addition, if an object is narrow and small, pixels
could be mistakenly merged to background regions,
such as the ﬁrst example shown in Fig. 7. In such
cases, considering only the inﬂuence of corresponding
regions in neighboring layers is insufﬁcient. In our
inference model, we count in a local consistency term
between adjacent regions. Accordingly, in the graph
shown in Fig. 2(e), connection between nodes in the
same layer is built. We describe the process below.
For a node corresponding to region i in layer Lk, we
deﬁne a saliency variable sk
i . Set S contains all of
them. We minimize the following energy function for
the hierarchical inference
The energy consists of three parts. Data term ED(sk
is to gather separate saliency conﬁdence, and hence is
deﬁned, for every node, as
i ) = βk||sk
where βk controls the layer conﬁdence and ¯sk
initial saliency value calculated in Eq. (5). The data
term follows a common deﬁnition.
The hierarchy term EH(sk
), building cross-layer
linkages, enforces consistency between corresponding
regions in different layers. This term is important in
our inference model. It not only connects multi-layer
information, but also enables reliable combination of
saliency results among different scales. In detail, if
i and Rk+1
are corresponding in two layers, we
must have Rk
based on our encompassment
deﬁnition and the segment generation procedure. EH
is deﬁned on them as
) = λk||sk
where λk controls the strength of consistency between
layers. The hierarchical term makes saliency assignment for corresponding regions in different levels
similar, beneﬁcial to effectively correcting single-layer
saliency errors.
The last term is a local consistency term, which
enforces intra-layer smoothness. It is used to make
saliency assignment smooth between adjacent similar
regions. Notation A(Rk
i ) in Eq. (6) represents a set
containing all adjacent regions of Rk
i in layer Lk. If
i ), the consistency penalty between regions
j is expressed as
j ) = γkwk
where γk determines the strength of consistency for
each layer. wk
i,j is the inﬂuence between adjacent
regions Rk
j . It should be large when the two
regions are similar in color and structure. We deﬁne
it as regional similarity in the CIELUV color space:
j are mean colors of respective regions
and σc is a normalization parameter. The intra-layer
consistency brings local regions into consideration.
Thus the inference is robust to hierarchical errors.
Our energy function including these three terms considers multi-layer saliency cues, making ﬁnal results
have less errors occurred in each single scale.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(b) Layer L1
(c) Layer L2
(d) Layer L3
(e) Without Local Term
(f) With Local Term
Fig. 7. Comparison of inference models with and without the local consistency term. Enforcing local connection
makes the ﬁnal saliency map in (f) less affected by similar color in the background.
Optimization
Our objective function in Eq. (6) forms a simple hierarchical graph model. Since it contains loops inside each
layer, we adopt common loopy belief propagation 
for optimization. It starts from an initial set of belief
propagation messages, and then iterates through each
node by applying message passing until convergence.
The propagation scheme can be summarized as follows. Let mτ
i→j be the message passed from region Ri
to an adjacent region Rj at the τ-th iteration (we omit
layer indicator for simplicity here). At each iteration,
if Ri and Rj are in the same layer, the message is
i→j(sj) = min
ED(si) + EC(si, sj) +
where set N{Ri} contains connected region nodes of
Ri, including inter- and intra-layer ones. If Ri and Rj
are regions in different layers, the passed message is
i→j(sj) = min
ED(si) + EH(si, sj) +
After message passing converges at T -th iteration, the
optimal value of each saliency variable can be computed via minimizing its belief function, expressed as
(sj)∗= arg min
Finally, we collect the saliency variables in layer L1
to compute the ﬁnal saliency map. An example is
shown in Fig. 6(e). Although saliency assignment in
the original resolution is erroneous, our ﬁnal saliency
map correctly labels the woman and horse as salient.
The background containing small-scale structures is
with low and smooth saliency values, as expected
from our model construction.
More Discussions
Relation to the Inference Model in 
inference model proposed in our conference version
Example images from Extended Complex
Scene Saliency Dataset (ECSSD). The images in the
ﬁrst row contain complex structures either in the salient
foreground or the non-salient background. The second
row shows the corresponding objects marked by human.
paper is a simpliﬁed version of Eq. (6), where γks
are set to 0 and linkage between adjacent regions in
the same layer does not exist. It is written as
forming a tree structure instead. This scheme enables
simpler and more efﬁcient optimization where belief
propagation is still applicable. In this case, message
passing is expressed in a single form by Eq. (12). It
makes exact inference with global optimum achievable within two passes . This type of inference is
actually equivalent to applying a weighted average
to all single-layer saliency cue maps, with optimally
determined weight for each region.
The tree-structured inference model is capable to
ﬁnd commonly salient regions. However, without
intra-layer propagation, narrow objects could be mismerged to background, as exempliﬁed in Fig. 7. In
the two examples, foreground and background pixels
are mistakenly joined due to object similarity. Our
new model counts in image layer information through
the local consistency term and reduces such errors, as
shown in (f). In Section 4, we show more quantitative
and qualitative evaluation results.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(a)Input &GT
(b) HS 
Fig. 9. An example that our new CHS model does not
outperform previous HS model.
Our new model, adding a local consistency term
based on regional color similarity, aims to correct
saliency score due to mistaken merge. When the
foreground and background have almost the same
color, the new term may degrade the performance,
as shown in Fig. 9. Under this extreme situation, our
previous tree-structured inference model HS in is
more suitable.
Relation to other Hierarchical Models
Similar ideas
of combining information in a hierarchical structure
were employed in this community. Wu et al.
proposed a tree-structured hierarchal model for shortand long-range motion perception. Ladicky et al. 
developed an associative hierarchical random ﬁelds
for semantic segmentation. In saliency related applications, Sun et al. modeled eye movement for visual
object attention via a hierarchical understanding of
scene/object. The saliency output varies with gaze
motion. A stream of bottom-up saliency models ,
 , explore information in different layers. These
methods exploit independent information in different
layers while our model connects saliency maps via
a graphical model to produce ﬁnal saliency scores.
The interaction between layers enhances optimality of
the system. Besides, our methods generally produce
clearer boundaries than previous local-contrast approaches, as we do not employ downsampled images
in different layers.
EXPERIMENTS
Our current un-optimized implementation takes on
average 2.02s to process one image with resolution
400 × 300 in the benchmark data on a PC equipped
with a 3.40GHz CPU and 8GB memory. The most time
consuming part, taking 86% of the total time, is the
local consistent hierarchical inference. Our implementation of this step is based on the Matlab package 
for loopy belief propagation. Acceleration could be
achieved by more efﬁcient implementation. In our
experiments, βi is ﬁxed as {0.5, 4, 2} for i = {1, 2, 3},
and λ1 = λ2 = 4.
In what follows, we ﬁrst introduce our Extended
Complex Scene Saliency Dataset (ECSSD) for saliency
evaluation. Then we show both qualitative and quantitative experiment results of our method on this new
and other benchmark datasets.
Extended Complex Scene Saliency Dataset
Although images from MSRA-1000 have a large
variety in their content, background structures are
primarily simple and smooth. To represent the situations that natural images generally fall into, we extend
our Complex Scene Saliency Dataset (CSSD) in 
to a larger dataset (ECSSD) with 1000 images, which
includes many semantically meaningful but structurally complex images for evaluation. The images are
acquired from the internet and 5 helpers were asked
to produce the ground truth masks individually.
We asked the helpers to label multiple salient objects
if they think these objects exist. We evaluate the intersubject label consistency by F-measure using four
labels for ground-truth and the rest one for testing
following the protocol of . To build the groundtruth mask, we average the binary mask of the four
labelers and set the threshold to 0.5. The averaged
F-measure among the 5 cases in all ECSSD images
is 0.9549, which indicates the inter-subject label is
quite consistent although our dataset contains complex scenes. The ﬁnal results are selected by majority
vote, i.e., averaging 5 candidate masks and setting the
threshold to 0.5. All images and ground truth masks
in the dataset are publicly available.
Dataset Properties
Images in our dataset fall into
various categories. The examples shown in Fig. 8
include images containing natural objects like vegetables, ﬂowers, mammals, insects, and human. There
are also images of man-made objects, such as cups,
vehicles, and clocks. For each example, we has its
corresponding salient object mask created by human.
Backgrounds of many of these examples are not uniform but contain small-scale structures or are composed of several parts. Some of the salient objects
marked by human also do not have a sharply clear
boundary or obvious difference with the background.
Natural intensity change due to illumination also
exists. The fourth image in the upper row of Fig. 8
is a typical example because the background contains
many ﬂowers diversiﬁed in color and edge distributions; the foreground butterﬂy itself has high-contrast
patterns. Considering only local center-surround contrast could regard all these high-contrast pixels as
salient. Results by several recent methods are shown
in Section 4.2.
In our image dataset, it is also noteworthy that multiple objects possibly exist in one image, while part of or
all of them are regarded as salient decided by human.
In the fourth example in third row of Fig. 8, several
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
0.05 0.15 0.25
0.35 0.45 0.55 0.65 0.75
Foreground/Background Distribution Difference
MSRA_L_channel
ECSSD_L_channel
MSRA_A_channel
ECSSD_A_channel
MSRA_B_channel
ECSSD_B_channel
Difference in
Difference in
Difference in Channel
Difference in
Difference in
Difference in Channel
(a) MSRA-1000 example
(b) ECSSD example
(c) Dataset statistics comparison
Number of images
Foreground/Background Difference
Fig. 10. Dataset complexity comparison. (a) and (b) are from MSRA-1000 and ECSSD respectively. The latter
is visually complex and also has a small foreground/background difference. Figure (c) shows the histogram of
foreground/background difference on two datasets, evaluated on Ł, a,
channels separately. It manifests that our
dataset has more similar foreground/background pairs, thus becomes more difﬁcult for saliency detection.
(c) FT 
(d) CA 
(e) HC 
(f) RC 
(g) RCC 
(h) HS 
Fig. 11. Visual Comparisons on ECSSD.
balls with different colors are put together. Because
the central ball has a smiling face, it is naturally more
attractive. This is a challenging example for saliency
detection.
In addition, this ECSSD dataset contains transparent objects with their color affected by background
patterns, causing large ambiguity in detection. These
salient objects nevertheless are easy to be determined
by human. We hope, by including these difﬁcult images, new deﬁnitions and solutions can be brought
into the community in future for more successful and
powerful saliency detection.
Complexity Evaluation
We quantitatively evaluate
the complexity of our dataset via the difference of
foreground/background distribution in CIELab color
space. Given the ground truth mask, we separate each
image into foreground and background pixels. Then
Chi-square distance is computed on the distributions
of these two sets considering the Ł, a and ¯channels.
Large difference values mean foreground and background can be easily separable, while a small difference increases the difﬁculty to distinguish foreground
from background.
Two image examples are shown in Fig. 10(a) and (b)
with their respective foreground/background distribution difference values. In Fig. 10(c), we plot the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
histogram of the difference for all images included
in MSRA-1000 and ECSSD respectively. It manifests
that our dataset has many more images with low foreground/background difference compared to those in
MSRA-1000. Put differently, our new dataset contains
more complex images for saliency detection.
Evaluation on ECSSD
We evaluate our method on the ECSSD dataset and
compare our results with those from several prior
methods, including local schemes – IT , GB ,
AC – and global schemes – LC , FT , CA
 , HC , RC , RCC , LR , SR . The
abbreviations are the same as those in , except for
LR, which represents the low rank method of . For
IT, GB, AC, FT, CA, HC, RC, RCC, LR and SR, we run
authors’ codes. For LC, we use the implementation
provided in . We denote the tree-structured based
method we proposed in as HS, and our new
method as CHS.
The visual comparison is given in Fig. 11. Our
methods can handle complex foreground and background with different details, giving accurate and
uniform saliency assignment. Compared with the treestructured algorithm , our new local consistent
hierarchical inference produces less turbulent saliency
values among similar adjacent regions. More importantly, it is able to correct some foreground pixels
that are mistakenly merged to the background. More
results will be available on our project website.
In quantitative evaluation, we plot the precision-recall
curves Fig. 12(a). Our experiment follows the setting
in , , where saliency maps are binarized at each
possible threshold within range . Our method
achieves the top precision in almost the entire recall
range . It is because combining saliency information from three scales makes the saliency estimation
be considered both locally and globally. Only sufﬁciently salient objects through all scales are detected in
this case. The non-salient background is then with low
scores generally. Besides, adding the local consistency
term improves performance by preserving consistent
saliency between adjacent regions.
In many applications, high precision and high recall
are required. We thus estimate the F-Measure as
Fβ = (1 + β2) · precision · recall
β2 · precision + recall
Thresholding is applied and β2 is set to 0.3 as suggested in . The F-measure is plotted in Fig. 12(b).
Our methods have high F-scores compared to others
in most range, indicating less sensitivity to picking a
threshold in both versions.
We have further investigated the performance of
Mean absolute error (MAE) following Perazzi et al.
 . MAE is a better representation for segmenting
salient objects. Table 1 demonstrates that our HS
and CHS outperform most existing methods by a
large margin. Note that the result of HS is slightly
better than CHS under this measure, since in our
ECSSD dataset, there are many data with insigniﬁcant
foreground/background difference. Recent work of
RCC performs similarly as ours. For relatively
small thresholds, it enforces a boundary prior to
produce clean background in the saliency map. The
SaliencyCut step iteratively updates the saliency map
to produce nearly binary maps. On complex background without conﬁdent saliency map initialization,
the SaliencyCut step would remove the less conﬁdent
region, such as the last two examples in Fig. 11.
MSRA-1000 , and 5000 Dataset
We also test our method on the saliency datasets
MSRA-1000 and MSRA-5000 , where
MSRA-1000 is a subset of MSRA-5000, containing
1000 natural images. We show comparisons with the
following ones, including local methods – IT , MZ
 , GB , AC , and global methods – LC , FT
 , CA HC , RC , RCC , SF , SR .
For IT, GB, CA, RCC and SR, we run authors’ codes.
For AC, FT, HC, LC, MZ, RC and SF, we directly use
author-provided saliency results. We omit result of
HS here since the performance in this two datasets is
rather close. Images of MSRA-1000 and MSRA-5000
are relatively more uniform; hence beneﬁt of the local
consistency term is not obvious.
Visual comparison is shown in Fig. 13. Follow previous settings, we also quantitatively compare our
method with several others with their saliency maps
available. The precision-recall curves for the MSRA-
1000 and 5000 datasets are plotted in Fig. 14(a) and (c).
The F-measure curves for the two datasets are plotted
in Fig. 14(b) and (d). The MAE measures are listed
in Tables 1. On these simpler datasets, all methods,
including ours, perform much better. However, the
advantage of our method is still clear.
Pascal-S Dataset 
We also compared our CHS and HS on Pascal-
S dataset with SF , HC , IT , FT ,
RCC , and GBVS-CMPC . Pascal-S dataset is
newly proposed for benchmarking complexity images
in saliency detection. Both Pascal-S and our ECSSD
datasets contain complex images for saliency evaluation. The images in Pascal-S usually involve several
different objects, such as a person in a bedroom
with decorations and furniture. In our dataset, many
similar foreground and background color/structure
distributions make the images difﬁcult for saliency
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Precision-Recall
Fig. 12. Quantitative comparison on ECSSD.
Quantitative comparison for MAE on ECSSD, MSRA-1000, and MSRA-5000 datasets.
(c) MZ 
(d) LC 
(e) GB 
(f) RC 
(g) SF 
(h) RCC 
Fig. 13. Visual comparison on MSRA-1000 .
detection. Most of our data contain the only salient
object as ground truth without ambiguity (see examples in Fig. 11).
The precision-recall comparison is shown in Fig. 15.
The success of GBVS-CMPC is due to the fact that
the CPMC segmentation algorithm it used has already
produced decent object-level proposals. Salient object
detection is achieved by assigning a uniform score
map to those conﬁdent objects. GBVS-CMPC shows
that the eye-ﬁxation together with an excellent segmentation approach is a promising direction.
Comparison with Single-Layer
Our hierarchical framework utilizes information from
multiple image layers, gaining special beneﬁt. Singlelayer saliency computation does not work similarly
well. To validate it, we take ¯si in Eq. (5) in different
layers as well as the average of them as the saliency
values. We evaluate how they work respectively when
applied to our ECSSD image data.
We compare all single layer results, averaged result,
result by tree-structured inference in and result
by our local consistent hierarchical inference, denoted
as Layer1, Layer2, Layer3, Average, HS, and CHS
respectively. For each of them, we take the threshold
that maximizes F-measure, and plot the correspond-
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Precision-Recall
(a) PR curve on MSRA-1000 dataset
(b) F-Measure on MSRA-1000 dataset
Precision-Recall
(c) PR curve on MSRA-5000 dataset
(d) F-Measure on MSRA-5000 dataset
Fig. 14. Precision-recall / F-measure curve on MSRA-1000 and MSRA-5000 datasets.
GBVS_CMPC 
Precision-Recall
Fig. 15. Precision-recall curve on Pascal-S dataset.
ing precision, recall, and F-measure in Table 2.
Results from all single layers are close. But the performance decreases. The reason is that, as more smallscale structures are removed, the extracted image
layers are prone to segmentation errors especially
for the structurally complex images in our dataset.
On the other hand, large-scale image layers bene-
ﬁt large-scale result representation. Compared with
naive averaging of all layers, our inference algorithm
optimally aggregates conﬁdent saliency values from
these layers, surely yielding better performance. By
enforcing smoothness locally in our new inference
model, CHS also produces better results compared to
the simpler HS implementation.
Performance of Single-layer vs. multi-layer. For each
method, we take the threshold that corresponds to the
highest F-measure, and list the precision, recall and
F-measure together.
Traditional
Performance of traditional scale measure vs. our scale
measure for F-measure under 7 different scales.
Region Scale and Layer Number Evaluation
To evaluate the effectiveness of our new scale measure
presented in Section 3.1, we compare our results with
those produced using the traditional scale measure,
i.e., number of pixels in the region. We replace the
scale measure by counting pixel number and set scale
thresholds {5, 9, 13, 17, 21, 25, 29, 33} for our measure
and squares of these values for the traditional one.
For images containing text- or curve-like high contrast
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(a) Our measure
(b) Traditional measure
Fig. 16. Scale measure comparison illustration.
The maximun F-measure for different layer number.
regions that should not be classiﬁed as salient alone,
our method performs much better. The resulting Fmeasure scores for the representative images from
ECSSD are listed in Table 3, indicating that our new
region scale measure is effective. Fig. 16 shows an
image result comparison. Uniformity-enforced scale
measure proﬁts general saliency detection and can remove errors caused by detecting many narrow regions
in the ﬁne level.
We also evaluate how the number of layers affects
our result. We experiment using different number
of layers and adjust the single layer scale for the
best performance on ECSSD. F-measure is reported in
Table 4. The three-layer case produces the best result.
Two layers cannot take a comparable advantage of
scale variation. With more than three layers, more
errors could be introduced in large scales. Also the
computational time increases accordingly.
CONCLUDING AND FUTURE WORK
We have tackled a fundamental problem that smallscale structures would adversely affect salient detection. This problem is ubiquitous in natural images due
to common texture. In order to obtain a uniformly
high-response saliency map, we propose a hierarchical
framework that infers importance values from three
image layers in different scales. Our proposed method
achieves high performance and broadens the feasibility to apply saliency detection to more applications
handling different natural images. The future work
includes incorporating object segmentation clues, and
applying the hierarchical insight to salient eye-ﬁxation
and objectness methods.
ACKNOWLEDGEMENTS
We thank Xin Tao and Qi Zhang for their help to
build the extended Complex Scene Saliency Dataset.
The work described in this paper was supported
by a grant from the Research Grants Council of the
Hong Kong Special Administrative Region (Project
No. 413110).