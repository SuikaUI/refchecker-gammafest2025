Deep Factors for Forecasting
Yuyang Wang 1 Alex Smola 1 Danielle C. Maddix 1 Jan Gasthaus 1 Dean Foster 1 Tim Januschowski 1
Producing probabilistic forecasts for large collections of similar and/or dependent time series is
a practically relevant and challenging task. Classical time series models fail to capture complex
patterns in the data, and multivariate techniques
struggle to scale to large problem sizes. Their
reliance on strong structural assumptions makes
them data-efﬁcient, and allows them to provide
uncertainty estimates. The converse is true for
models based on deep neural networks, which can
learn complex patterns and dependencies given
enough data. In this paper, we propose a hybrid
model that incorporates the beneﬁts of both approaches. Our new method is data-driven and
scalable via a latent, global, deep component. It
also handles uncertainty through a local classical
model. We provide both theoretical and empirical evidence for the soundness of our approach
through a necessary and sufﬁcient decomposition
of exchangeable time series into a global and a local part. Our experiments demonstrate the advantages of our model both in term of data efﬁciency,
accuracy and computational complexity.
1. Introduction
Time series forecasting is a key ingredient in the automation
and optimization of business processes. In retail, decisions
about which products to stock, when to (re)order them, and
where to store them depend on forecasts of future demand in
different regions; in (cloud) computing, the estimated future
usage of services and infrastructure components guides capacity planning; regional forecasts of energy consumption
are used to plan and optimize the generation of power; and
workforce scheduling in warehouses and factories depends
on forecasts of the future workload.
The prevalent forecasting methods in statistics and econo-
1Amazon Research.
Correspondence to:
Yuyang Wang
< >.
Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).
metrics have been developed in the context of forecasting
individual or small groups of time series. The core of
these methods is formed by comparatively simple (often
linear) models, which require manual feature engineering
and model design by domain experts to achieve good performance . Recently, there has been a paradigm
shift from these model-based methods to fully-automated
data-driven approaches. This shift can be attributed to the
availability of large and diverse time series datasets in a
wide variety of ﬁelds, e.g. energy consumption of households, server load in a data center, online user behavior, and
demand for all products that a large retailer offers. These
large datasets make it possible and necessary to learn models from data without signiﬁcant manual work (B¨ose et al.,
A collection of time series can exhibit various dependency
relationships between the individual time series that can be
leveraged in forecasting. These include: (1) local co-variate
relationships (e.g. the price and demand for a product, which
tend to be (negatively) correlated), (2) indirect relationships
through shared latent causes (e.g. demand for multiple products increasing because an advertising campaign is driving
trafﬁc to the site), (3) subtle dependencies through smoothness, temporal dynamics, and noise characteristics of time
series that are measurements of similar underlying phenomena (e.g. product sales time series tend to be similar to each
other, but different from energy consumption time series).
The data in practical forecasting problems typically has all
of these forms of dependencies. Making use of this data
from related time series allows more complex and potentially more accurate models to be ﬁtted without overﬁtting.
Classical time series models have been extended to address
the above dependencies of types (1) and (2) by allowing
exogenous variables (e.g. the ARIMAX model and control
inputs in linear dynamical systems), and employing multivariate time series models that impose a certain covariance
structure (dynamic factor models), respectively. Neural
network-based models have been recently shown to excel in
extracting complex patterns from large datasets of related
time series by exploiting similar smoothness and temporal
dynamics, and common responses to exogenous input, i.e.
dependencies of type (1) and (3) .
These models struggle in producing calibrated uncertainty
 
Deep Factors for Forecasting
estimates. They can also be sample-inefﬁcient, and cannot
handle type (2) dependencies. See 
for a recent survey on traditional and modern methods for
forecasting.
The two main challenges that arise in the fully-automated
data-driven approaches are: how to build statistical models
that are able to borrow statistical strength and effectively
learn to forecast from large and diverse data sources exhibiting all forms of dependencies, and how to combine the
data efﬁciency and uncertainty characterization of classical
time series models with the expressive power of deep neural
networks. In this paper, we propose a family of models
that efﬁciently (in terms of sample complexity) and effectively (in terms of computational complexity) addresses
these aforementioned challenges.
1.1. Background
Classical time series models, such as general State-Space
Models (SSMs), including ARIMA and exponential smoothing, excel at modeling the complex dynamics of individual time series of sufﬁciently long history. For Gaussian
State-Space Models, these methods are computationally
efﬁcient, e.g. via a Kalman ﬁlter, and provide uncertainty estimates. Uncertainty estimates are critical for optimal downstream decision making. Gaussian Processes (GPs) are another family
of the models that have been applied to time series forecasting .
These methods are local, that is, they learn one model per
time series. As a consequence, they cannot effectively extract information across multiple time series. Finally, these
classical methods struggle with cold-start problems, where
more time series are added or removed over time.
Mixed effect models consist of two kinds
of effects: ﬁxed (global) effects that describe the whole
population, and random (local) effects that capture the idiosyncratic of individuals or subgroups. A similar mixed
approach is used in Hierarchical Bayesian methods, which combine global and local models to
jointly model a population of related statistical problems.
In , other combined
local and global models are detailed.
Dynamic factor models (DFMs) have been studied in econometrics for decades to model the co-evolvement of multiple
time series . DFMs can be thought as an
extension of principal component analysis in the temporal
setting. All the time series are assumed to be driven by a
small number of dynamic (latent) factors. Similar to other
models in classical statistics, theories and techniques are developed with assuming that the data is normally distributed
and stationary. Desired theoretical properties are often lost
when generalizing to other likelihoods. Closely related are
the matrix factorization (MF) techniques and tensor factorization , which have been applied to the
time series matrix with temporal regularization to ensure
the regularity of the latent time series. These methods are
not probabilistic in nature, and cannot provide uncertainty
estimation for non-Gaussian observations.
1.2. Main Contributions
In this paper, we propose a novel global-local method, Deep
Factor Models with Random Effects. It is based on a global
DNN backbone and local probabilistic graphical models
for computational efﬁciency. The global-local structure extracts complex non-linear patterns globally while capturing
individual random effects for each time series locally.
The main idea of our approach is to represent each time
series, or its latent function, as a combination of a global
time series and a corresponding local model. The global
part is given by a linear combination of a set of deep dynamic factors, where the loading is temporally determined
by attentions. The local model is stochastic. Typical local
choices include white noise processes, linear dynamical systems (LDS) or Gaussian processes (GPs). The stochastic
local component allows for the uncertainty to propagate
forward in time. Our contributions are as follows: i) Provide
a unique characterization of exchangeable time series (Section 2); ii) Propose a novel global-local framework for time
series forecasting, based on i), that systematically marries
deep neural networks and probabilistic models (Section 3);
iii) Develop an efﬁcient and scalable inference algorithm
for non-Gaussian likelihoods that is generally applicable
to any normally distributed probabilistic models, such as
SSMs and GPs (Section 3). As a byproduct, we obtain new
approximate inference methods for SSMs/GPs with non-
Gaussian likelihoods; iv) Show how state-of-the-art time
series forecasting methods can be subsumed in the proposed
framework (Section 4); v) Demonstrate the accuracy and
data efﬁciency of our approach through scientiﬁc experiments (Section 5).
2. Exchangeable Series
In this section, we formulate a general model for exchangeable time series. A distribution over objects is exchangeable,
if the probability of the objects is invariant under any permutation. Exchangeable time series are a common occurrence.
For instance, user purchase behavior is exchangeable, since
there is no speciﬁc reason to assign a particular coordinate
to a particular user. Other practical examples include sales
statistics over similar products, prices of securities on the
stock market and the use of electricity.
Deep Factors for Forecasting
2.1. Characterization
Let zi ∈ZT , where zi denotes the ith exchangeable time
series, Z denotes the domain of observations and T ∈N
denotes the length of the time series.1 We denote individual
observations at some time t as zi,t. We assume that we
observe zi,t at discrete time steps to have a proper time
series rather than a marked point process.
Theorem 1. Let p be a distribution over exchangeable time
series zi over Z with length T, where 1 ≤i ≤N. Then p
admits the form
p(gt|g1:t−1)
p(zi,t|zi,1:t−1, gt)dg.
In other words, p(z) decomposes into a global time series g
and N local times series zi, which are conditionally independent given the latent series g.
Proof. It follows from de Finetti’s theorem that
p(zi|g)dg.
Since zi are time series, we can decompose p(zi|g) in the
causal direction using the chain rule as
p(zi,t|zi,1:t−1, g).
Substituting this into the de Finetti factorization in Eqn. (1)
p(zi,t|zi,1:t−1, g)dg.
Lastly, we can decompose g, such that gt contains a sufﬁcient statistic of g with respect to z·,t. This holds trivially
by setting gt = g, but defeats the purpose of the subsequent
models. Using the chain rule on p(g) and substituting the
result in proves the claim.
Theorem 2. For tree-wise exchangeable time series, that is
time series that can be grouped hierarchically into exchangeable sets, there exists a corresponding set of hierarchical
latent variable models.
The proof is analogous to that of Theorem 1, and follows
from a hierarchical extension of de Finetti’s theorem . This decomposition is useful when
dealing with product hierarchies. For instance, the sales
events within the category of iPhone charger cables and the
category of electronics cables may be exchangeable.
1Without loss of generality and to avoid notational clutter, we
omit the extension to time series beginning at different points of
time. Our approach is general and covers these cases as well.
2.2. Practical Considerations
We now review some common design decisions used in modeling time series. The ﬁrst is to replace the decomposition
t=1 p(zi,t|zi,1:t−1) by a tractable, approximate statistic ht
of the past, such that p(zi,t|zi,1:t−1) ≈p(zi,t|hi,t). Here,
ht typically assumes the form of a latent variable model
via p(hi,t|hi,t−1, zi,t−1). Popular choices for real-valued
random variables are SSMs and GPs.
The second is to assume that the global variable gt is drawn
from some p(gt|gt−1). The inference in this model is costly,
since it requires constant interaction, via Sequential Monte
Carlo, variational inference or a similar procedure between
local and global variables at prediction time. One way to
reduce these expensive calculations is to incorporate past
local observations z·,t−1 explicitly. While this somewhat
negates the simplicity of Theorem 1, it yields signiﬁcantly
higher accuracy for a limited computational budget, gt ∼
p(gt|gt−1, z·,t−1).
Lastly, the time series often comes with observed covariates,
such as a user’s location or a detailed description of an item
being sold. We add these covariates xi,t to the time series
signal to obtain the following model:
p(gt|gt−1, x·,t, z·,t−1) ×
p(hi,t|gt, hi,t−1, zi,t−1, xi,t) ×
p(zi,t|gt, hi,t, zi,t−1, xi,t)
Even though this model is rarely used in its full generality,
Eqn. (2) is relevant because it is by some measure the
most general model to consider, based on the de Finetti
factorization in Theorem 1.
2.3. Special Cases
The global-local structure has been used previously in a
number of special contexts . For instance,
in Temporal LDA we assume that we
have a common ﬁxed Dirichlet-Multinomial distribution
capturing the distribution of tokens per topic, and a timevariant set of latent random variables capturing the changes
in user preferences. This is a special case of the above
model, where the global time series does not depend on
time, but is stationary instead.
A more closely related case is the Neural Survival Recommender model of . This models the
temporal dynamics of return times of a user to an app via
survival analysis. In particular, it uses a LSTM for the global
dynamics and LSTMs for the local survival probabilities. In
Deep Factors for Forecasting
this form, it falls into the category of models described by
Theorem 1. Unlike the models we propose in this paper, it
does not capture local uncertainty accurately. It also primarily deals with point processes rather than proper time series,
and the inference algorithm differs quite signiﬁcantly.
3. Deep Factor Models with Random Effects
Motivated by the characterization of exchangeable time
series, in this section, we propose a general framework for
global-local forecasting models, called Deep Factor Models
with Random Effects, that follows the structure given by
the decomposition in Theorem 1. We describe the family of
the methods, show three concrete instantiations (DF-RNN,
DF-LDS, and DF-GP), and derive the general inference and
learning algorithm. Further models that can be obtained
within the same framework, and additional details about the
design choices, are described in Appendix A.
We are given a set of N time series, with the ith time series
consisting of tuples (xi,t, zi,t) ∈Rd × R, t = 1, · · · , T,
where xi,t are the input co-variates, and zi,t is the corresponding observation at time t. Given a forecast horizon
τ ∈N+, our goal is to calculate the joint predictive distribution of future observations,
p({zi,T +1:T +τ}N
i=1|{xi,1:T +τ, zi,1:T }N
i.e. the joint distribution over future observations given all
co-variates (including future ones) and past observations.
3.1. Generative Model
Our key modeling assumption is that each time series
zi,t, t = 1, 2, . . . is governed by a ﬁxed global (non-random)
and a random component, whose prior is speciﬁed by a generative model Ri. In particular, we assume the following
generative process:
global factors :
gk(·) = RNNk(·),
k = 1, · · · , K,
ﬁxed effect :
wi,k · gk(·),
random effect :
ri(·) ∼Ri,
i = 1, · · · , N,
latent function :
ui(·) = fi(·) + ri(·),
emission :
zi,t ∼p(zi,t|ui(xi,t)),
The observation model p can be any parametric distribution,
such as Gaussian, Poisson or Negative Binomial. All the
functions gk(·), ri(·), ui(·) take features xi,t as input, and
we deﬁne ui,t := ui(xi,t), the embedding wi := [wi,k]k.
3.1.1. GLOBAL EFFECTS (COMMON PATTERNS)
The global effects are given by linear combinations of K
latent global deep factors modeled by RNNs. These deep
Figure 1. Plate graph of the proposed Deep Factor Model with Random Effects. The diamond nodes represent deterministic states.
factors can be thought of as dynamic principal components
or eigen time series that drive the underlying dynamics of all
the time series. As mentioned in Section 2.2, we restrict the
global effects to be deterministic to avoid costly inference
at the global level that depends on all time series.
The novel formulation of the ﬁxed effects from the RNN in
Eqn. (3) has advantages in comparison to a standard RNN
forecaster. Figure 2 compares the generalization errors and
average running times of using Eqn. (3) with the L2 loss
and a standard RNN forecaster with the same L2 loss and
a comparable number of parameters on a real-word dataset
electricity . Our
ﬁxed effect formulation shows signiﬁcant data and computational efﬁciency improvement. The proposed model has less
variance in comparison to the standard structure. Detailed
empirical explorations of the proposed structures can be
found in Section 5.1.
Training Set Size (data points per time series)
Mean Absolute Percentage Error (test set)
Average running time (seconds)
RNNForecaster
DeepFactor
Figure 2. Generalization Error (solid line), Mean Absolute Percentage Error (MAPE) on the test set and running time in seconds
(dashed line) vs. the size of the training set, in terms of data points
per time series. The experiments are repeated over 10 runs.
Deep Factors for Forecasting
DESCRIPTION
LIKELIHOOD (GAUSSIAN CASE)
Zero-mean Gaussian noise
process given by RNN
ri,t ∼N(0, σ2
t N(zi,t −fi,t|0, σ2
State-space models
ri,t ∼LDSi,t (cf. Eqn. (6))
p(zi) given by Kalman Filter
Zero-mean Gaussian Process
ri,t ∼GPi,t (cf. Eqn. (7))
p(zi) = N(zi −fi|0, Ki + σ2
Table 1. Summary table of Deep Factor Models with Random Effects. The likelihood column is under the assumption of Gaussian noise.
3.1.2. RANDOM EFFECTS (LOCAL FLUCTUATIONS)
The random effects ri(·) in Eqn. (4) can be chosen to be any
classical probabilistic time series model Ri. To efﬁciently
compute their marginal likelihood p(zi|Ri), ri,t should be
chosen to satisfy the normal distributed observation assumption. Table 1 summarizes the three models we consider for
the local random effects models Ri.
The ﬁrst local model, DF-RNN, is deﬁned as ri,t
i,t), where σi,t is given by a noise RNN that takes
input feature xi,t. The noise process becomes correlated
with the covariance function implicitly deﬁned by the RNN,
resulting in a simple deep generative model.
The second local model, DF-LDS, is a part of a special
and robust family of SSMs, Innovation State-Space Models
(ISSMs) . This
gives the following generative model:
hi,t = Fi,thi,t−1 + qi,tϵi,t,
ϵi,t ∼N(0, 1),
The latent state hi,t contains information about the level,
trend, and seasonality patterns. It evolves by a deterministic transition matrix Fi,t and a random innovation qi,t.
The structure of the transition matrix Fi,t and innovation
strength qi,t determines which kinds of time series patterns
the latent state hi,t encodes (cf. Appendix A.2 for the concrete choice of ISSM).
The third local model, DF-GP, is deﬁned as the Gaussian
ri,t ∼GP(0, Ki(·, ·)),
where Ki(·, ·) denotes the kernel function. In this model,
with each time series has its own set of GP hyperparameters
to be learned.
3.2. Inference and Learning
Given a set of N time series, our goal is to jointly estimate
Θ, the parameters in the global RNNs, the embeddings and
the hyper-parameters in the local models. We use maximum
likelihood estimation, where Θ = argmax P
i log p(zi|Θ).
Computing the marginal likelihood may require doing inference over the latent variables. The general learning algorithm is summarized in Algorithm 1.
Algorithm 1 Training Procedure for Deep Factor Models
with Random Effects.
1: for each time series {(xi, zi)} do
Sample the estimated latent representation from the
variational encoder eui ∼qφ(·|zi) for non-Gaussian
likelihood, otherwise eui := zi.
With the current estimate of the model parameters Θ,
compute the ﬁxed effect fi,t = PK
k=1 wi,k ·gk(xi,t),
and corresponding ISSM parameters for DF-LDS or
the kernel matrix Ki for DF-GP.
Calculate the marginal likelihood p(zi) as in Table 1
or its variational lower bound as in Eqn. (10).
5: end for
6: Accumulate the loss in the current mini-batch, and perform stochastic gradient descent.
3.2.1. GAUSSIAN LIKELIHOOD
When the observation model p(·|ui,t) is Gaussian, zi,t ∼
N(ui,t, σ2
i,t), the marginal likelihood can be easily computed for all three models. Evaluating the marginal likelihood for DF-RNN is straightforward (see Table 1).
For DF-LDS and DF-GP, the Gaussian noise can be absorbed into the local model, yielding zi,t = ui,t = fi,t +
ri,t, where ri,t, instead of coming from the noiseless LDS
and GP, is generated by the noisy versions. More precisely,
for DF-LDS, ri,t = a⊤
i,thi,t +νi,t and νi,t ∼N(0, σ2
the marginal likelihood is obtained with a Kalman ﬁlter. In
DF-GP, it amounts to adding σ2
i,t · δ(·, ·) to Eqn (7), where
δ(·, ·) is the Dirac delta function. The marginal likelihood
becomes the standard GP marginal likelihood, which is the
multivariate normal with mean fi := [fi(xi,t)]t and covariance matrix Ki + σ2
i I, where Ki := [K(xi,t, xi,t)]t and I
is the identity matrix of suitable size.
3.2.2. NON-GAUSSIAN LIKELIHOOD
When the likelihood is not Gaussian, the exact marginal
likelihood is intractable. We use variational inference, and
optimize a variational lower bound of the marginal likelihood log p(z):
p(z, u, h) ⩾IEqφ(u,h) log
p(z, u, h)
Deep Factors for Forecasting
where u is the latent function values, and h is the latent
states in the local probabilistic models 2. Optimizing this
stochastic variational lower bound for the joint model over
all time series is computationally expensive.
We propose a new method that leverages the structure of
the local probabilistic model to enable fast inference at
the per-time series level.
This enables parallelism and
efﬁcient inference that scales up to a large collection
(millions) of time series. Motivated by , we choose the following structural approximation
qφ(h, u|z) := qφ(u|z)p(h|u), where the second term
matches the exact conditional posterior with the random
effect probabilistic model R. With this form, given u, the
inference becomes the canonical inference problem with
R, from Section 3.2.1. The ﬁrst term qφ(u|z) is given
by another neural network parameterized by φ, called a
recognition network in the variational Auto-Encoder (VAE)
framework .
After massaging the equations, the stochastic variational
lower bound in Eqn. (8) becomes
+ log p(u)
L (log p(z|euj) + log p(euj) −log qφ(euj|z)) ,
with euj ∼qφ(u) for j = 1, · · · , L sampled from the recognition network. The ﬁrst and third terms in Eqn. (10) are
straightforward to compute. For the second term, we drop
the sample index j to obtain the marginal likelihood log p(eu)
under the normally distributed random effect models. This
term is computed in the same manner as in Section 3.2.1,
with zi substituted by eu. When the likelihood is Gaussian,
the latent function values u are equal to z, and we arrive at
log p(z) from Eqn. (9).
4. Related Work and Discussions
Effectively combining probabilistic graphical models and
deep learning approaches has been an active research area.
Several approaches have been proposed for marrying RNN
with SSMs through either one or both of the following:
(i) extending the Gaussian emission to complex likelihood
models; (ii) making the transition equation non-linear via a
multi-layer perceptron (MLP) or interlacing SSM with transition matrices temporally speciﬁed by RNNs. Deep Markov
Models (DMMs), proposed by ,
keep the Gaussian transition dynamics with mean and covariance matrix parameterized by MLPs. Stochastic RNNs
(SRNNs) explicitly incorporate the
deterministic dynamics from RNNs that do not depend on
latent variables by interlacing them with a SSM. Chung et al.
 ﬁrst proposed Variational RNNs (VRNNs), which is
2For cleaner notation, we omit the time series index i
another way to make the transition equation non-linear, by
cutting ties between the latent states and associating them
with deterministic states. This makes the state transition
non-linearly determined by the RNN. VRNNs are also used
in Latent LSTM Allocation (LLA) and
State-Space LSTM (SSL) . These models require expensive inference at the global level through
a recognition network, with is in stark contrast with Deep
Factors, where the structural assumption of the variational
approximation decomposes the inference problem to local
probabilistic inference that is easily parallelizable and global
standard RNN learning (cf. Section 3.2).
In Fraccaro et al. and the recent Deep State Models , the linear Gaussian transition
structure is kept intact, so that the highly efﬁcient Kalman
ﬁlter/smoother is readily applicable. Our model differs from
the former in that we avoid sampling the latent states in
the ELBO, and eliminate the variance coming from the
Monte-Carlo estimate of the second integral in Eqn. (10).
Deep State is designed for a Gaussian likelihood with time
varying SSM parameters per time series. In contrast, with
time invariant local SSMs and ﬂexible global effects, our
model DF-LDS offers a parsimonious representation that
can handle non-Gaussian observations.
Deep Gaussian Processes have
attracted growing interests in recent years. Inspired by GP-
LVM structure , Deep GPs stacks GPs on
top of latent variables, resulting in more expressive mappings. Our framework provides an alternative approach to
utilize GPs more efﬁciently.
Due to its ﬂexibility of interpolating between purely local and purely global models, there are a variety of common methods that can be subsumed in our proposed model
framework. Deep Factor with one factor and no random
effects, accompanied with autoregressive inputs, reduces
to DeepAR . One difference in our
formulation is that the scale of each time series is automatically estimated rather than pre-speciﬁed as it is in DeepAR.
Changing the emission probability to Gaussian Mixtures
results in AR-MDN . Sequenceto-Sequence models for forecasting are
another family of models that are a part of our framework.
These methods make predictions discriminatively rather
than generatively. By dropping the random effects, using
GPs as the prior and removing the restriction of RNNs on
the global factors, we recover the semi-parametric latent factor model . By dropping the global effects,
we arrive at the standard SSM or GP. Our newly developed
general variational inference algorithm applies to both of
these methods and other normally distributed local stochastic models (cf. subsubsection 3.2.2). While we have built
upon existing works, to the best of our knowledge, Deep
Deep Factors for Forecasting
Factors provide the ﬁrst model framework that incorporate
SSMs and GPs with DNNs in a systematic manner.
5. Experiments
We conduct experiments with synthetic and real-world data
to provide evidence for the practical effectiveness of our approach. We use a p3.8xlarge SageMaker instance in all our
experiments. Our algorithms are implemented in MXNet
Gluon , and make extensive use of its
Linear Algebra library . Further experiments with GPs as the local model are
detailed in .
5.1. Model Understanding and Exploration
The ﬁrst set of experiments compares our proposed structure
in Eqn. (3) with no probabilistic component to the standard
RNN structure on the electricity dataset. For each
time series i, we have its embedding wi ∈IR10, and two
time features xt ∈IR2, day of the week and hour of the day.
Given an RNN Cell (LSTM, GRU, etc.), the RNN Forecaster
predicts the time series values ˆzi,t = RNNt(xi,t), where
xi,t = [wi; xt]. Deep Factor generates the point forecast
as ˆzi,t = w⊤
i RNNt(xt). The RNN cell for RNN Forecaster
has an output dimension of 1 while that of DeepFactor is of
dimension 10. The resulting number of parameters of both
structures are roughly the same.
Figure 2 demonstrates that the proposed structure signiﬁcantly improves the data efﬁciency while having less variance. The runtime of Deep Factor scales better than that of
the standard RNN structure. By using the concatenation of
[wi; xt], the standard RNN structure operates on the outer
product space of xt and wi (of dimension 12 × T), while
Deep Factor computes the intrinsic structure (of dimension
12 and T separately).
Next, we focus on the DF-LDS model, and investigate (i)
the effect of the recognition network for non-Gaussian observations with the purely local part (fi,t = 0) (variational
LDS cf. Appendix A.2), and (ii) the recovery of the global
factors in the presence of Gaussian and non-Gaussian noise.
We generate data according to the following model, which
is adapted from Example 24.3 in . The twodimensional latent vector ht is rotated at each iteration, and
then projected to produce a scalar observation,
ht+1 = Aht + ϵh,
where ϵh ∼N(0, α2I2), ut+1 = eT
1 ht+1 + ϵv, ϵv ∼
N(0, σ2), I2 is the 2 × 2 identity matrix, and e1
R2 is the standard basis vector.
The true observations are generated by a Poisson distribution, zt
Poisson[λ(ut)], where λ(ut) = log[1 + exp(ut)]. This
could be used to model seasonal products, where most of
the sales happen when an item is in season, e.g. snowboards
normally sell shortly before or during winters, and less
so afterwards. Figure 3 shows the reconstructed intensity
function λ(ut), as well as corresponding forecasts for each
choice of recognition network. Visual inspections reveal
that RNNs are superior over MLPs as recognition networks.
This is expected because the time series are sequential. We
also test the ability of our algorithm to recover the underlying global factors. Our experiments show that even with the
Poisson noise model, we are able to identify the true latent
factors in the sense of distances of the subspaces spanned
by the global factors (cf. Appendix B.1).
5.2. Empirical Studies
In this subsection, we test how our model performs
on several real-world and publicly available datasets:
electricity (E) and traffic (T) from the UCI data
set ,
nyc taxi (N) and uber
(U) (cf. Appendix B.2).
In the experiments, we choose the DF-RNN (DF) model
with a Gaussian likelihood. To assess the performance of
our algorithm, we compare with DeepAR (DA), a state-ofart RNN-based probabilistic forecasting algorithm on the
publicly available AWS SageMaker , MQ-RNN (MR), a sequence model that generates
conditional predictive quantiles , and
Prophet (P), a Bayesian structural time series model . The Deep Factor model has 10 global
factors with a LSTM cell of 1-layer and 50 hidden units.
The noise LSTM has 1-layer and 5 hidden units. For a fair
comparison with DeepAR, we use a comparable number
of model parameters, that is, an embedding size of 10 with
1-layer and 50 hidden LSTM units. The student-t likelihood
in DeepAR is chosen for its robust performance. The same
model structure is chosen for MQ-RNN, and the decoder
MLP has a single hidden layer of 20 units. We use the adam
optimization method with the default parameters in Gluon
to train the DF-RNN and MQ-RNN. We use the default
training parameters for DeepAR.
We use the quantile loss to evaluate the probabilistic forecasts. For a given quantile ρ ∈(0, 1), a target value zt and
ρ-quantile prediction bzt(ρ), the ρ-quantile loss is deﬁned as
QLρ[zt, bzt(ρ)] = 2
ρ(zt −bzt(ρ))Izt−bzt(ρ)>0
+ (1 −ρ)(bzt(ρ) −zt)Izt−bzt(ρ)⩽0
normalized
i,t QLρ[zi,t, bzi,t(ρ)]/ P
i,t |zi,t|,
quantile losses for a given span across all time series. We
include results for ρ = 0.5, 0.9, which we abbreviate as
the P50QL (mean absolute percentage error (MAPE)) and
P90QL, respectively.
Deep Factors for Forecasting
observed time series
true latent itensity
observed time series
forecast (MLP rec) with P10 and P90
observed time series
forecast (Bi-LSTM rec) with P10 and P90
Figure 3. DeepFactor (DF-LDS) with no global effects (Variational LDS). Left: reconstructed Intensity (ut) with different recognition
networks. Center and Right: predictive distributions with MLP (center) and Bidirectional LSTM (right).
0.216±0.054
0.204±0.042
0.101± 0.006
0.182 ±0.077
0.088±0.008
0.049±0.004
0.272±0.078
0.185±0.042
0.112±0.012
0.100±0.013
0.083±0.008
0.059 ±0.013
0.468±0.043
0.418±0.031
0.212±0.044
0.175±0.005
0.267±0.038
0.104±0.028
0.390±0.042
0.358±0.029
0.239±0.037
0.167± 0.005
0.248±0.088
0.139±0.035
0.337±0.026
0.383±0.040
0.190±0.048
0.184±0.051
0.226±0.017
0.127±0.026
0.296 ±0.021
0.331±0.011
0.225±0.050
0.149±0.011
0.154±0.020
0.159±0.059
0.417±0.011
0.730±0.031
0.344±0.033
0.296±0.017
0.577±0.059
0.190±0.013
0.353±0.009
0.879±0.156
0.425±0.063
0.238±0.009
0.489±0.069
0.238±0.026
Table 2. Results for the short-term (72-hour) and near-term (24-hour) forecast scenarios with one week of training data.
electricity
DeepFactor
Figure 4. P50QL (MAPE) results for the short-term (72-hour) forecast in Table 2. Purple denotes the proposed method.
For all the datasets, we limit the training length to only one
week of time series (168 observations per time series). This
represents a relevant scenario that occurs frequently in demand forecasting, where products often have only limited
historical sales data. We average the Deep Factor, MQ-
RNN and DeepAR results over ten trials. We use one trial
for Prophet, since classical methods are typically less variable than neural-network based models. Figure 4 illustrates
the performances of the different algorithms in terms of the
MAPE (P50 quantile loss) for the 72 hour forecast horizon.
Table 2 shows the full results, and that our model outperforms the others in terms of accuracy and variability in most
of the cases. For DeepAR, using SageMaker’s HPO, our preliminary results (cf. Appendix B.2) show that with a larger
model, it achieves a performance that is on-par with our
method, which has much less parameters. In addition, the
sequence-to-sequence structure of DeepAR and MQ-RNN
limits their ability to react ﬂexibly to changing forecasting
scenarios, e.g. during on-demand forecasting, or interactive
scenarios. For a forecasting scenario with a longer prediction horizon than during training horizon, DeepAR needs
to be retrained to reﬂect the changes in the decoder length.
Similarly, they cannot generate forecasts that are longer than
the length of the training time series, for example, the case
in Figure 2. Our method has no difﬁculty performing this
task and has greater data efﬁciency.
6. Conclusion
We propose a novel global-local framework for forecasting
a collection of related time series, accompanied with a result that uniquely characterizes exchangeable time series.
Our main contribution is a general, powerful and practically
relevant modeling framework that scales, and obtains stateof-the-art performance. Future work includes comparing
variational dropout or Deep Ensemble of non-probabilistic
DNN models (e.g., RNNForecaster (cf. 5.1)) for uncertainty.
Deep Factors for Forecasting