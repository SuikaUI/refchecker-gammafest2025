On Multi-View Active Learning and the Combination with
Semi-Supervised Learning
 
Zhi-Hua Zhou
 
National Key Laboratory for Novel Software Technology, Nanjing University, China
Multi-view learning has become a hot topic
during the past few years.
In this paper,
we ﬁrst characterize the sample complexity
of multi-view active learning. Under the αexpansion assumption, we get an exponential improvement in the sample complexity
from usual eO( 1
ϵ ) to eO(log 1
ϵ ), requiring neither strong assumption on data distribution
such as the data is distributed uniformly over
the unit sphere in Rd nor strong assumption
on hypothesis class such as linear separators
through the origin. We also give an upper
bound of the error rate when the α-expansion
assumption does not hold. Then, we analyze
the combination of multi-view active learning and semi-supervised learning and get a
further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which veriﬁes that
the combination of multi-view active learning
and semi-supervised learning is eﬃcient.
1. Introduction
Learning from labeled data is well-established in machine learning, but labeling the training data is time
consuming, sometimes may be very expensive since
it may need human eﬀorts. In many machine learning applications, unlabeled data can often be obtained
abundantly and cheaply, so there has recently been
substantive interest in using large amount of unlabeled
data together with labeled data to achieve better learning performance.
There are two popular paradigms for using unlabeled data to complement labeled data. One is semi-
Appearing in Proceedings of the 25 th International Conference on Machine Learning, Helsinki, Finland, 2008. Copyright 2008 by the author(s)/owner(s).
supervised learning.
Some approaches use a generative model for the classiﬁer and employ EM to model
the label estimation or parameter estimation process
 ; some approaches use the unlabeled data
to regularize the learning process in various ways, e.g.,
deﬁning a graph on the data set and then enforcing
the label smoothness over the graph as a regularization
term ; some approaches use the multi-view setting to
train learners and then let the learners to label unlabeled examples . The multi-view setting
is ﬁrst formalized by Blum and Mitchell , where
there are several disjoint subsets of features (each subset is called as a view), each of which is suﬃcient for
learning the target concept.
For example, the web
page classiﬁcation task has two views, i.e., the text appearing on the page itself and the anchor text attached
to hyper-links pointing to this page ; the speech recognition task also has two views,
i.e., sound and lip motion .
Another important paradigm for using unlabeled data
to complement labeled data, which is the focus of this
paper, is active learning . In active learning, the learners actively ask the
user to label the most informative examples and hope
to learn a good classiﬁer with as few labeled examples
as possible.
There have been many theoretical analyses on the sample complexity of single-view active learning. For some
simple learning tasks the sample complexity of active
learning can be O(log 1
ϵ ) which is exponentially improved in contrast to O( 1
ϵ ) of passive learning taking
into account the desired accuracy bound ϵ. Unfortunately, such an exponential improvement is not always
achievable in active learning. Dasgupta illustrated that if the hypothesis class H is linear separators in R2 and if the data distribution is some density
On Multi-View Active Learning and the Combination with Semi-Supervised Learning
supported on the perimeter of the unit circle, there are
some target hypotheses in H for which Ω( 1
ϵ ) labels are
needed to ﬁnd a classiﬁer with error rate less than ϵ, no
matter what active learning approach is used. Under
the strong assumptions that the hypothesis class is linear separators through the origin, that the data is distributed uniformly over the unit sphere in Rd, and that
the learning task is a realizable case (i.e., there exists a
hypothesis perfectly separating the data), the sample
complexity of active learning is eO(d log 1
ϵ ) taking into
account the desired accuracy bound ϵ 1. For some known data
distribution D and speciﬁc hypothesis class, Dasgupta
 gave the coarse sample complexity bounds for
realizable active learning. The study of sample complexity of active learning for realizable case without
strong assumptions on the data distribution and the
hypothesis class remains an open problem.
All the above results were obtained under the singleview setting. The ﬁrst algorithm for active learning
in multi-view setting is co-testing .
It focuses on the set of contention points (i.e., unlabeled examples on which different views predict diﬀerent labels) and asks the user
to label some of them. This is somewhat related to
Query-by-Committee since cotesting also uses more than one learners to identify
the most informative unlabeled examples to query, but
the typical Query-by-Committee works under a singleview setting while co-testing exploits the multi-views
explicitly. It was reported that co-testing outperforms
existing active learners on a variety of real-world domains such as wrapper induction, Web page classiﬁcation, advertisement removal and discourse tree parsing.
To the best of our knowledge, however, there
is no theoretical result on the sample complexity of
multi-view active learning.
In this paper, we ﬁrst theoretically analyze the sample
complexity of multi-view active learning under the αexpansion assumption which is ﬁrst mentioned by Balcan et al. and prove that the sample complexity of multi-view active learning can be exponentially
improved to eO(log 1
ϵ ). A clear advantage is that we
do not use strong assumptions which were employed
in most previous studies, such as the hypothesis class
is linear separators through the origin and the data
is distributed uniformly over the unit sphere in Rd.
In case the α-expansion assumption does not hold, we
give an upper bound of the error rate. Second, we analyze the combination of multi-view active learning and
1The eO notation is used to hide factors log log( 1
ϵ ), log(d)
and log( 1
semi-supervised learning and get an further improvement in the sample complexity. Finally, we study the
empirical behavior of the two paradigms, which veriﬁes that the combination of multi-view active learning and semi-supervised learning is more eﬃcient than
pure multi-view active learning.
The rest of this paper is organized as follows. After introducing some preliminaries in Section 2, we analyze
the sample complexity of multi-view active learning in
Section 3.
Then we analyze the sample complexity
of the combination of multi-view active learning and
semi-supervised learning in Section 4 and study the
empirical behavior in Section 5. Finally we conclude
the paper in Section 6.
2. Preliminaries
In the multi-view setting, an example x is described
with several diﬀerent disjoint sets of features. Without loss of generality, in this paper we only consider
the two-view setting for the sake of simplicity. Suppose
that the example space X = X1 ×X2 is with some unknown distribution D, X1 and X2 are the two views,
and Y = {−1, 1} is the label space. Let c = (c1, c2)
be the underlying target concept, where c1 and c2 are
the underlying target concepts in the two views, respectively. Suppose that the example space is consistent, that is, there is no such example x = (x1, x2)
that c1(x1) ̸= c2(x2) in X.
Let H1 and H2 be the
hypothesis class in each view, respectively.
hj ∈Hj and x = (x1, x2) we say xj ∈hj if and only if
hj(xj) = cj(xj) (j = 1, 2). In this way any hypothesis
in Hj can be thought of as a subset of Xj.
In each round of iterative multi-view active learning,
the learners ask the user to label some unlabeled examples and add them into the labeled training data.
These newly labeled examples provide more information about the data distribution.
In this paper, we
consider the co-testing-like Paradigm 1 described in
Table 1. In Paradigm 1, the learners ask the user to
label some contention points to reﬁne the classiﬁers. If
the conﬁdent set of each view is expanding by considering the other view together, Paradigm 1 may succeed.
Intuitively, we can use the α-expansion assumption to
analyze the process.
Suppose S1 ⊆X1 and S2 ⊆X2 denote the examples
that are correctly classiﬁed in each view, respectively.
Let Pr(S1 ∧S2) denote the probability mass on examples that are correctly classiﬁed in both views, while
Pr(S1 ⊕S2) denotes the probability mass on examples
that are correctly classiﬁed only in one view (i.e., examples disagreed by the two classiﬁers). Now we give
On Multi-View Active Learning and the Combination with Semi-Supervised Learning
Unlabeled data set U = {x1, x2, · · · , }, where each example xt is given as a pair (xt
Ask the user to label m0 unlabeled examples drawn randomly from D to compose the labeled data set L
Iterate i = 0, 1, · · · , s
Train two classiﬁers hi
2 consistent with L in each view, respectively;
2 to the unlabeled data set U and ﬁnd out the contention points set Qi;
Ask the user to label mi+1 unlabeled examples drawn randomly from Qi, then add them into L and
delete them from U.
hfinal = combine(hs
Table 1. Paradigm 1: Multi-view active learning
our deﬁnition on α-expansion.
Deﬁnition 1 D is α-expansion if for any S1 ⊆X1,
S2 ⊆X2, we have
Pr(S1 ⊕S2) ≥α min[Pr(S1 ∧S2), Pr(S1 ∧S2)].
We say that D is α-expanding with respect to hypothesis class H1×H2 if the above holds for all S1 ∈H1∩X1,
S2 ∈H2∩X2 (here we denote by Hj∩Xj the set {h∩Xj
: h ∈Hj} for j = 1, 2).
Note that Deﬁnition 1 on α-expansion is almost the
same as that in Balcan et al. . To guarantee
the success of iterative co-training, they made several
assumptions such as that the learning algorithm used
in each view is conﬁdent about being positive and is
able to learn from positive examples only, and that
the distribution D+ over positive examples is expanding. There are many concept classes, however, are not
learnable from positive examples only. Apparently, all
problems which satisfy the deﬁnition of Balcan et al.
 also satisfy our deﬁnition.
We will make use of the following lemma when deriving
our sample complexity bound (Anthony & Bartlett,
Lemma 1 Let H be a set of functions from X to
{−1, 1} with ﬁnite VC-dimension V
be an arbitrary, but ﬁxed probability distribution over
X × {−1, 1}. For any ϵ, δ > 0, if we draw a sample
from P of size N(ϵ, δ) = 1
ϵ (4V log( 1
ϵ ) + 2 log( 2
δ )), then
with probability 1 −δ, all hypotheses with error ≥ϵ
are inconsistent with the data.
3. Sample Complexity of Multi-View
Active Learning
There are many strategies to combine the classiﬁers
in Paradigm 1, for example, weighted voting, majority
voting or winner-take-all . In this
paper, we use the following simple combination scheme
for binary classiﬁcation:
1(x1) = hi
random guess
1(x1) ̸= hi
Assuming that the data distribution D is α-expanding
with respect to hypothesis class H1 × H2, we will analyze how many labels the user should label to achieve
classiﬁers with error rate no larger than ϵ. We consider
the iterative process and let Si
1 ⊆X1 and Si
2 corresponds to the classiﬁers hi
2 ∈H2 in the i-th round, respectively. The initial m0 unlabeled examples are randomly picked from
D and labeled by the user according to the target concept c. Suppose m0 is suﬃcient for learning two classiﬁers h0
2 whose error rates are at most 1/4
(i.e., Pr(S0
1) ≥1 −1/4 and Pr(S0
2) ≥1 −1/4), and
thus Pr(S0
2) ≥1/2. The α-expansion condition
2) ≥αPr(S0
In each round of Paradigm 1, the learners ask the user
to label some unlabeled examples according to the target concept c and add them into the labeled data set.
Then the two classiﬁers are reﬁned. Some example x
in X might be predicted with diﬀerent labels between
the i-th and (i + 1)-th round. Intuitively, in order to
get the classiﬁers improved in Paradigm 1, the reduced
size of conﬁdent set should be no more than the size of
contention set. Moreover, considering that there is no
noise in the labeled data since all the labels are given
by the user according to the target concept, and that
the amount of labeled training examples are monotonically increasing, the asymptotic performance of PAC
learners increase, we can assume that
2) ≤αPr(Si
2) (j ∈{1, 2}) (2)
On Multi-View Active Learning and the Combination with Semi-Supervised Learning
Intuitively, by multiplying the denominator at the
right-hand to the left-hand (16 is used for a faster
convergence; it can be 2 for an easier understanding),
Eq. 2 implies that the total reduced size of conﬁdent
sets on both views after using the newly labeled contention points is no more than the size of contention
set. Apparently, all problems that satisfy the assumption of Balcan et al. also satisfy Eq. 2. Now we
give our main theorem.
Theorem 1 For data distribution D α-expanding with
respect to hypothesis class H1 × H2, let ϵ and δ denote
the ﬁnal desired accuracy and conﬁdence parameters.
If s = ⌈log α
C ⌉and mi = 16
α (4V log( 16
α )+2 log( 8(s+1)
(i = 0, 1, · · · , s), Paradigm 1 will generate a classiﬁer
with error rate no more than ϵ with probability 1 −δ.
Here, V = max[V C(H1), V C(H2)] where V C(H) denotes the VC-dimension of the hypothesis class H and
constant C = α/4+1/α
In Paradigm 1, we use Eq. 1 to combine
the two classiﬁers, thus the error rate of the combined
classiﬁer hi
2) + Pr(Si
α (4V log( 16
α ) + 2 log( 8(s+1)
Lemma 1 we have Pr(S0
16 and Pr(S0
with probability 1 −
4(s+1). Generally, we have that
an arbitrary Si
j (j = 1, 2) being consistent with the
examples in L has an error rate at most α
16 with probability 1 −
4(s+1). So we have Pr(Si
with probability 1−
2(s+1). Without loss of generality,
consider 0 < α ≤1 and therefore 1 −α
2. Thus the
α-expansion condition suggests
2) ≥αPr(Si
For i ≥1, the learners ask the user to label mi
unlabeled examples drawn randomly from Si−1
according to the target concept c and obtain
two new classiﬁers Si
Similarly, if mi =
α (4V log( 16
α )+2 log( 8(s+1)
)), using Lemma 1 we have
16 (j ∈{1, 2})
with probability 1 −
4(s+1). So we get that
with probability 1−
2(s+1). Considering Eq. 2 we have
) ≤αPr(Si−1
2) = Pr(Si
) + Pr(Si−1
From Eq. 3 we can get that
) ≤Pr(Si−1
Thus, considering
) = Pr(Si−1
) + Pr(Si−1
) + Pr(Si−1
) + Pr(Si−1
) + Pr(Si−1
) + Pr(Si−1
Now we get
(α/4 + 1/α
1 + 1/α )sPr(S0
8 (α/4 + 1/α
1 + 1/α )s .
So when s = ⌈log α
C ⌉where C is a constant and
< 1, we have Pr(Ss
words, we get a classiﬁer hs
com whose error rate is no
more than ϵ with probability 1 −δ.
From Theorem 1 we know that we only need to label
i=0 mi = O(log 1
ϵ log(log 1
ϵ )) examples to get a classiﬁer with error rate no more than ϵ with probability
On Multi-View Active Learning and the Combination with Semi-Supervised Learning
1−δ. Thus, we achieve an exponential improvement in
sample complexity from eO( 1
ϵ ) to eO(log 1
ϵ ) as in Dasgupta et al. and Balcan et al. . Note
that we have not assumed a speciﬁc data distribution
and a speciﬁc hypothesis class which were assumed in
the studies of Dasgupta et al. and Balcan et al.
 . From the proof of Theorem 1 we can also know
that the proportion α
16 in Eq. 2 can be relaxed to close
2 . Such relaxation will not aﬀect the exponential
improvement, but will reduce the convergence speed.
Further, considering that not every data distribution
D is α-expanding with respect to hypothesis class
H1 × H2, we will give a coarse upper bound of the
generalization error for Paradigm 1 for cases when the
α-expansion assumption does not hold.
2) = αiPr(Si
2) (i = 0, 1, · · ·). If the
α-expansion assumption does not hold in Paradigm 1,
for any ϵ > 0 and any integer N > 0, the size of the set
{αi: i > N ∧αi < ϵ} is inﬁnite. We set a parameter
ϵc > 0 as the stop condition. When Pr(Si
less than ϵc, we terminate the iteration in Paradigm 1.
Now we make the deﬁnition on expanded region with
respect to ϵc.
Deﬁnition 2 Let γϵc denote the expanded region with
respect to ϵc in Paradigm 1,
γϵc = Pr(S0
where i = min{i : Pr(Si
2) < ϵc ∧i ≥1}.
After i rounds the region in which both classiﬁers
wrongly predict becomes smaller and smaller, from
2) to Pr(Si
2). This expanded region can
be thought of as an approximation of Σi
Theorem 2 When the α-expansion assumption does
not hold, set ϵc > 0 to terminate Paradigm 1. The
error rate of hi
com can be smaller than h0
com for γϵc +
Considering
2) and Pr(Si
2) < ϵc, we
com −errorhi
Theorem 2 implies that Paradigm 1 could not boost
the performance to arbitrarily high and gives a coarse
upper bound of the error rate, when the α-expansion
assumption does not hold.
The improvement depends on the expanded region γ and the disagreement between the initial two classiﬁers.
The larger
the expanded region γ, the better the improvement of
Paradigm 1. Theorem 2 can also be applied to oneshot co-training .
4. Sample Complexity of Combination
of Multi-View Active Learning and
Semi-Supervised Learning
We can try to reduce the sample complexity further
by combining multi-view active learning with semisupervised learning.
Previously this has been tried
in some applications and led to good results , yet to the best of our knowledge, there
is no theoretical analysis which supports such argument. For computational simplicity, we consider the
following case in this section. Suppose that the hypothesis class Hj is the subset of mappings from Xj
to [−1, 1] and y = sign(c(x)), c = (c1, c2) is the underlying target concept, where c1 and c2 is the underlying
target concept in each view, respectively. Let d(f, g)
denote the probability that the two classiﬁers f ∈Hj
and g ∈Hj predict diﬀerent labels on an example xj
drawn randomly from Xj, then
d(f, g) = Prxj∈Xj
Suppose that for any f, g ∈Hj, there exists some
constant L1 > 0 to hold that |f(xj) −g(xj)| ≤
L1 · d(f, g) · ∥xj∥2, where ∥xj∥2 denotes the 2-norm
of xj. Without loss of generality, suppose that there
exists some constant L2 > 0 to hold that ∥xj∥2 ≤L2
for xj ∈Xj (j = 1, 2). Now we have the following theorem for Paradigm 2 which combines multi-view active
learning with semi-supervised learning.
Theorem 3 For data distribution D α-expanding with
respect to hypothesis class H1 × H2, let ϵ and δ denote
the ﬁnal desired accuracy and conﬁdence parameters.
If s = ⌈log α
C ⌉, m0 = 1
L(4V log( 1
L)+2 log( 8(s+1)
α (4V log( 16
α ) + 2 log( 8(s+1)
)) (i = 1, 2, · · · ,),
Paradigm 2 will generate a classiﬁer with error rate
no more than ϵ with probability 1 −δ.
Here, V = max[V C(H1), V C(H2)] where V C(H) denotes the VC-dimension of the hypothesis class H, constant C = α/4+1/α
and constant L = min[ α
In Paradigm 2, we also use Eq. 1 to combine the two classiﬁers. With m0 =
L(4V log( 1
2 log( 8(s+1)
)) where constant L = min[ α
16L1L2 ], using Lemma 1 we have Pr(S0
L and Pr(S0
with probability 1−
4(s+1). Generally, we have that an
On Multi-View Active Learning and the Combination with Semi-Supervised Learning
Unlabeled data set U = {x1, x2, · · · , }, where each example xt is given as a pair (xt
Threshold thr
Ask the user to label m0 unlabeled examples drawn randomly from D to compose the labeled data set L
Iterate i = 0, 1, · · · , s
Set counter ni+1
to 0. If D is expanding, set counter ni+1
to +∞; Otherwise, set counter ni+1
Train two classiﬁers hi
2 consistent with L in each view, respectively;
2 to the unlabeled data set U and ﬁnd out the contention points set Qi;
for k = 1, · · · , mi+1
Draw an example xk = (xk
2) randomly from Qi;
1)| > thr then yk = sign(hi
else if |hi
2)| > thr then yk = sign(hi
else ask the user to label xk and ni+1
Add (xk, yk) into L and delete it from U and Qi.
for w = 1, 2, · · ·
≥mi+1 −ni+1
Draw an example xw = (xw
2 ) randomly from U −Qi;
1 )| > thr then yw = sign(hi
else if |hi
2 )| > thr then yw = sign(hi
else ask the user to label xw and ni+1
Add (xw, yw) into L and delete it from U.
hfinal = combine(hs
Table 2. Paradigm 2: Combination of multi-view active learning and semi-supervised learning
arbitrary Si
j (j = 1, 2) being consistent with the examples in L has an error rate at most 1
L with probability
4(s+1). So, for any example x = (x1, x2),
j(xj) −cj(xj)| ≤L1 · L2 · d(hi
We can set the threshold thr in Paradigm 2 to
j and cj make the same prediction on
When s = ⌈log α
C ⌉, from the proof of Theorem
1 we have Pr(Ss
2) ≤ϵ. Thus we get a classiﬁer
com whose error rate is no more than ϵ with probability 1 −δ using Paradigm 2.
The sample complexity of Paradigm 2 is m0+Ps
which is much smaller than that of Paradigm 1. From
Theorem 3 we know that the sample complexity can
be further reduced by combining multi-view active
learning with semi-supervised learning, however, it
needs a stronger assumption on the hypothesis class
If this assumption holds, in contrast to
Paradigm 1, when α-expansion does not hold, we can
i=1(mi −ni
1) more examples on which both
classiﬁers have small margin, which can help to reduce
the size of the region S1 ∧S2.
5. Empirical Study
In this section we empirically study the performance of
the Paradigms 1 and 2 on a real-world data set, i.e., the
course data set . This data set
has two views (pages view and links view) and contains
1,051 examples each corresponds to a web page, and
the task is to predict whether an unseen web page is
a course page or not. There are 230 positive examples
(roughly 22%). We randomly use 25% data as the test
set and use the remaining 75% data as the unlabeled
set U in Tables 1 and 2. Then, we randomly draw 10
positive and 30 negative examples from U to generate
the initial m0 labeled examples.
In practice, the thr in Paradigm 2 can be determined
by cross validation on labeled examples. Here in our
experiments, for the ease of comparison, we do not set
thr and instead, we ﬁx the number of examples to be
queried in both Paradigms. Thus, we can study their
performance under the same number of queries.
detail, in the i-th round, Paradigm 1 picks out two
contention points randomly to query; while Paradigm
On Multi-View Active Learning and the Combination with Semi-Supervised Learning
number of queried labels
error rate
Paradigm 1
Paradigm 2
Random Sampling
Figure 1. Comparison of the performances
2 picks out the example with the smallest absolute sum
of the two classiﬁers’ outputs from Qi and U −Qi respectively to query, and picks out the example with
the largest absolute sum of the two classiﬁers’ outputs from Qi and U −Qi respectively to label as
1(x1) + hi
. That is, the two examples to
be queried in Paradigm 2 are arg minx∈Qi
¯¯) and arg minx∈U−Qi
1(x1) + hi
¯¯), while
the two examples Paradigm 2 labels for itself by
semi-supervised learning are arg maxx∈Qi
¯¯) and arg maxx∈U−Qi
1(x1) + hi
use Random Sampling as the baseline and implement
the classiﬁers with SMO in WEKA . The experiments are repeated for 20 runs and
Figure 1 plots the average error rates of the three
methods against the number of examples that have
been queried.
It can be found from Figure 1 that with the same
number of queried examples, although there are some
ﬂuctuation, the performance of Paradigm 1 is generally better than that of Random Sampling, while the
performance of Paradigm 2 is better than that of the
In particular, the advantage of Paradigm 2
becomes more prominent as the number of queries
increases.
This is not diﬃcult to understand since
with more labeled data the learners become stronger
and thus the labels obtained from the semi-supervised
learning process become more helpful.
Overall, the empirical study veriﬁes that comparing
with pure active learning, the combination of multiview active learning and semi-supervised learning can
reduce the sample complexity.
6. Conclusion
In this paper, we ﬁrst characterize the sample complexity of multi-view active learning and get an exponential
improvement in the sample complexity from eO( 1
ϵ ). The α-expansion assumption we employed
is weaker than assumptions taken by previous theoretical studies on active learning, such as that the data is
distributed uniformly over the unit sphere in Rd and
that the hypothesis class is linear separators through
the origin. We also give an upper bound of the error rate for cases where the α-expansion assumption
does not hold. Then, we analyze the combination of
multi-view active learning with semi-supervised learning and get that such a combination can reduce the
sample complexity further, which is veriﬁed by an empirical study.
This provides an explanation to that
why the method described in can
lead to good results.
Our work is the ﬁrst theoretical analysis on the sample complexity of realizable multi-view active learning.
Recently, non-realizable active learning, where there
does not exist a hypothesis perfectly separating the
data, starts to attract attention . Extending
our work to non-realizable multi-view active learning
is a future work.
Acknowledgments
This research was supported by the National Science
Foundation of China (60635030, 60721002), the Foundation for the Author of National Excellent Doctoral
Dissertation of China (200343) and the National High
Technology Research and Development Program of
China (2007AA01Z169).