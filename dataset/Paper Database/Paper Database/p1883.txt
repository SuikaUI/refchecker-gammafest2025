MONOGRAPHS ON
STATISTICS A N D APPLIED P R O B A B I L I T Y
General Editors
D. R. Cox and I), V. Hinkley
Probability, Staristics and Tinte
M . S. Bartlett
The Statistical Airnlysis gf Spatial Patterri
M. S. Bartlett
S/ochastic Popltlntion Modeis in Ecology and Epickmioloy~
S. Bgrtlett
Risk Theory
R. E, Beard, T. PentlkBInen and E. Pesonen
Residcals and Irflue~tcc~
in Regressiotr
R. D. Cook and S, Weisberg
Point Procexses
D. R. Cox and V. lsham
Analysis of Binary Data
Tltr Starisiical Atza~sis of Series of Et-etrts
D, R. Cox and P. A. W. Lewis
D. R. Cox and W. L. Smith
Srochasric Ahunduttcc Models
The Analysis of Cottringenc~~
B. S. Everitt
Finile Mi,~ture Distributiorrs
B. S. Everitt and D. J. Hand
Population Genetics
W . J. Ewens
MONOGRAPHS ON
STATISTICS AND APPLIED PROBABILITY
General Editors
D. R. Cox and D. V. Hinkley
Probability. Statistics and Tinv
M . S. Bartlett
The Statistical Atlolysis of Spalial Pattern
M . S. Bartlett
S/oclrastic Poplilalion Models it1 Ecology and Epidenlioloyy
M . S . Bartlett
Risk Theory
R. E. Beard, T. Pentikainen and E. Pesonen
Resi(!tlals mu1 Ittflumcr in Regression
R. D. Cook and S. Weisberg
Point Processes
D. R. Cox and V. lsham
Analysis of Binnry Data
The Statislical Atlalj?ris of Series of Erents
D. R. Cox and P. A. W. Lewis
D. R. Cox and W. L. Smith
Stochastic Abundat~ce Models
The Analysis of C'ontingmcy Tables
B. S. Everitt
Finite Mi.rture Distributions
B. S. Everitt and D. J. Hand
Population Genetics
W. J . Ewens
C'lrr.aificrrtiotr
A. D. Gordon
Morrre C ~ r l o
J . M. Hammersley and D. C. Handscomb
I(ienlificufiorr of' Oulliers
D. M. Hawkins
I)i.~trihiitiorr~/'ree
Statistical Metho(ls
J . S . Maritz
Alrtlrii~mimte Atlalysis it1 Beharioriral Research
A. E. Maxwell
Some Basic Tlreory for Statistical It$eretrce
E. J . G . Pitman
Statistical Inference
S. D. Silvey
Models in Reyressiotz and Related Topics
Seqrrentinl Methods in Statistics
G . B. Wetherill
(Full details concerning this series are available from the Publishers)
Residuals and Influence
in Regression
R. Dennis Cook and Sanford Weisberg
Scltool oj' Staristics
University of Minr~esota
CHAPMAN A N D HALL
First puhli.r/led 1982 hy
Clt 
Bibliography: p.
Includes index.
1. Regression analysis. I. Weisberg.
Sanford. 1947-
. 11. Title. 111. Series.
QA278.2.C665
ISBN 0-412-24280-X
British Library Cataloguing in Publication Data
Cook, R. Dennis
Residuals and influence in regression.-(Monographs
on statistics and applied probability)
1. Regression analysis
11. Weisberg, Sanford
111. Series
1 Introduction
1.1 Cloud seeding
1.2 The basic paradigm
2 Diagnostic methods using residuals
2.1 The ordinary residuals
The /rat titatris
The role qf' V in data analyses
2. I .3 Use q/' t/re ordinary residuals
I//rrstratiott of' hills
2.2 Other types of residuals
Studett~ized residrrtrls
ltiternal Studenti:atiotl
E.utertia1 Studetttization
Mean sltiji orrtlier model
E.uanlple 2.2.1
Adaptire score data no. I
E.uatirple 2.2.2
Cloud seeding 110. 2
Accuracy oJ't11e Botderroni bo~ittdjor ttte orttlier
E.uantple 2.2.3
Adaptive score data no. 2
Multiple cases
Cotnputations
E.ratnp1e 2.2.4
Adaptice score data no. 3
Other outlier titoriels
2.2.3 Predicted residuals
Uncorrelated residlrals
BL U S residuals
Recursive residuals
2.3 Plotting methods
Standard residual plots
Example 2.3. I
Cloud seeding no. 3
E.~antple 2.3.2
Old Faitlful Geyser
Example 2.3.3
Residuals in place
2.3.2 Added variable plots
Non-null behavior
Conlputat ions
E.ratnple 2.3.4
Jer Jiyhters no. I
Partial residual plots
Esantple 2.3.5
Jef fiyhrers no. 2
2.3.4 Probability plots
E.ratrtpIe 2.3.6
Cloud seeditty no. 4
2.4 Transformations
Fanlilies of fratlsjorr~ratiot~s
Selectb~g a trat~s/'orrtlatiott
Itr~.ariattce
Nor~rrality
Choice c?/'trtodel i ~ t t c i scalirlg the predictors
It~f'erettce
E.uan~ple 2.4.1
Tree data 110. I
Diagnostic ntetttods
A tkinson 's method
Andrew' nterhod
Application to the power family
Tukey's test
E.rample 2.4.2
Tree data no. 2
E.~antple 2.4.3
Jet fiyhters no. 3
Trarrsfor~r~ing
the esplana~ory ~lariahles
Esatrtple 2.4.4
Tree data no. 3
Esatrtple 2.4.5
Cloud see(fittg no. 5
Residual analysis in two-way tables
Nottaciditirity
E.\-antple 2.5.1
A y ricultural e-rperintenl
3 Assessment of influence
3.1 Motivation
3.2 The influence curve
E.uatrtple 3.2.1
The sanlple auerage
3.3 The influence curve in the linear model
3.4 Sample versions of the influence curve
E~rtpiricnl ittfl~iet~i-e
Sotrtple it~fltrettce curres
Esntilple 3.4.1
seeding tlo. 6
E.uittr~ple 3.4.2
Partial Ffesrs
3.5 Applications of the sample influence curve
Estert~al scalit~g
Euatrtple 3.5.1
C'omhinafion.s qf r f , vii
Esanlple 3.5.2
Alrernatice full rank choices for M, c
Lower dintensior~al trortrts
E.ratiiple 3.5.3
Cloud seeding 110. 7
Predictions
bttertrul sc(t1itrg
Ordering usiny a t~ltrltivtlriate o~irlirr statistic'
Jackknife metl~oti
Esantple 3.5.4
C'louti seedirly no. 8
Graphical aids
Esanlple 3.5.5
Clolid seeding no. Y
Esarrlple 3.5.6
Jet fiy11ter.s no. 4
Multiple cases
Other norrns
Generalized potential
E.uati~ple 3.6.1
Alrernariile tileasures of'porenritrl
E.vat~ple 3.6.2 Adaptive score data no. 4
Conlpurittg Dl
E.raritple 3.6.3
Cli~lid seeding no. 10
Esantple 3.6.4
Drill duto
4 Alternative approaches to influence
Volume of confidence ellipsoids
4.2 The Andrews and Pregibon diagnostic
4.3 Predictive influence
Kullback- Leihler divergences (tnd prer1icvil.e
rletlsities wirli u2 knoirtl
Predictit,e influence junctiotts, a2 kno~rn
Esantple 4.3.1
Predictive itlp~ience \c.lretl
q = I, tit = 1, a2 kt~o\vtt
Predictire blpuence jirnctions, u2 ~trllitro~~~n
An a1terttcrtit.e to clroosing X, = X
4.4 A comparison of influence measures
Esattlple 4.4.1
Cloltd seeding no. I I
Calibration
5 Assessment of influence in other problems
5.1 A general definition of residuals
E.uantple 5.1 .I
Leuketrtiu dura no. I
5.2 A general approach to influence
E.rantple 5.2.1
Leukertlict data no. 2
Nonlinear least squares
Exan~ple 5.3.1
Dimcan's data
Logistic regression and generalized linear models
E.uurrtple 5.4.1
I,e~tkt,ntitr clrrtn no. 3
Esnttlple 5.4.2
Lc~ukc~rrriil ti(rta no. 4
Robust regression
E.uar)tple 5.5.1
Cloltd seeding no. I2
E.uanlple 5.5.2
Clolid seeding no. 13
Other problems
5.6.1 Correlotiotl coelJicitv~r
5.6.2 Discrir~li~lut~t
atlm():sis
5.6.3 Lir~ear reyressiot~ with ir~cor~rplete
Weighted least squares
A.2 Updating formulae
Residual correlations
Bibliography
Author and Subject indexes
Residuals are used in many procedures designed to detect various types
of disagreement between data and an assumed model. Many of the
common methods of residual analysis are founded on work in the early
1960s by F. Anscombe, J. W. Tukey, G. E. P. Box. D. R. Cox,
C. Daniel and K. S. Srikantan. The methodology grew steadily
through the early 1970s and by 1975 residual analysis was widely
regarded as an integral part of any regression problem, and many
methods using residuals had been incorporated into generally dis-
tributed computer packages. An implicit presumption at that time
seems to be that most deficiencies are correctable through proper
choice of scales, weights, model and method of fitting, and that
residual analysis was used only to produce stronger, compelling
conclusions. During the late 1970s interest in residual analysis was
renewed by the development and rapid acceptance of methods for
assessing the influence of individual observations. These developments
allow a more complete understanding of an analysis, and have
stimulated an awareness that some deficiencies may not be removable
and thus inherent weaknesses in conclusions may necessarily remain.
In the first part of this monograph, we present a detailed account of
the residual based methods that we have found to be most useful, and
brief summaries of other selected methods. Where possible, we present
a unified treatment to allow standard options to be viewed in a larger
context. Our emphasis is on graphical methods rather than on formal
testing. In the remainder, we give a comprehensive account of a variety
of methods for the study of influence.
In writing this book, we have assumed that the reader is familiar
with, or concurrently studying, linear models and regression methods
at the level of Seber , or, with some supplementation. Draper
and Smith or Weisberg . An early version of this
monograph was used as the basis of a course in Winter 1981 at the
University of Minnesota, and many of the comments of the particip-
ants have resulted in substantial improvements. Norton Holschuh
read the final version and corrected many errors that might otherwise
remain. Typing and other organizational matters were ably handled by
Carol Lieb and Linda D. Anderson-Courtney. Almost all of the figures
in this work were computer drawn at the University of Minnesota.
St. Puul, Mit~nesofa
Janitar)- 1982
R. Dennis Cook
Sanford Weisberg
C H A P T E R 1
Introduction
'Complicated phenomena. in which several causes concurring, opposing. or
quite independent of each other. operate at once. so as to produce a compound
effect, may be simplified by subducting the effect of all the known causes. as
well as thenature ofthecase permits,either by deductive reasoning or by appeal
to experience, and thus leaving. as it were, a residttal pltettomenon to be
explained. It is by this process, in fact, that science, in its present advanced state.
is chiefly promoted.'
J O H N F. W. H E R S C H E L (1830). A Prelitnitrary Discntrrse
or1 the Study of Nurural Philosoph~
The collection of statistical methods that has come to be associated
with the term 'regression' is certainly valued and widely used. And yet,
an annoying and often sizeable gap remains between the necessarily
idealized theoretical basis for these methods and their routine appli-
cation in practice. It is well known, for example, that inferences based
on ordinary least squares regression can be strongly influenced by only
a few cases in thedata, and the fitted model may reflect unusual features
of those cases rather than the overall elations ship between the
variables. Here, case refers to a particular observation on the response
variable in combination with the associated values for the explanatory
variables.
There appear to be two major ways in which the gap between theory
and practice is being narrowed. One is by the continued development of
robust or resistant methods of estimation and testing that require
progressively fewer untenable assumptions. Robust regression
methods, for example, are a step ahead of least squares regression in
this regard. The other line of inquiry is through the development of
diagnostic tools that identify aspects of a problem that do not conform
to the hypothesized modeling process. For example, the scatterplot of
residuals versus fitted values that accompanies a linear least square fit is
a standard tool used to diagnose nonconstant variance, curvature, and
outliers. Diagnostic tools such as this plot have two important uses.
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
First, they may result in the recognition of important phenomena that
might otherwise have gone unnoticed. Outlier detection is an example
of this, where an outlying case may indicate conditions under which a
process works differently, possibly worse or better. It can happen that
studies of the outlying cases have greater scientific importance than the
study of the bulk of the data. Second, the diagnostic methods can be
used to suggest appropriate remedial action to the analysis of the
These lines of development, robust methods and diagnostics, are not
mutually exclusive. When robust regression is viewed as iteratively re-
weighted least squares, for example, the weights associated with the
individual cases may be useful indicators of outliers .
While it seems true that these approaches are in some ways competitive,
one is not likely to replace the other in the foreseeable future. As long as
least squares methods are in widespread use, the need for correspond-
ing diagnostics will exist. Indeed, the use of robust methods does not
abrogate the usefulness of diagnostics in general, although it may
render certain of them unnecessary.
This book is about diagnostics. The major emphasis is on diagnostic
tools for data analyses based on linear models in combination with
least squares methods of estimation. This material is given in Chapters
2 4 . In Chapter 5 we discuss corresponding tools for other selected
In the remainder of this chapter we introduce a data set that will be
used for illustration throughout the rest of this book and suggest a
basic paradigm for regression analysis. While many other data sets will
be introduced in later chapters, a complete and detailed discussion of
each of these is not possible. We hope that the following discussion can
serve as a model for a useful, but perhaps not universally applicable,
perspective on the use of diagnostics in data analyses.
1.1 Cloud seeding
Judging the success of cloud seeding experiments intended to increase
rainfall is an important statistical problem . Results
from past experiments are mixed. It is generally recognized that,
depending on various contributing environmental factors, seeding will
produce an increase or decrease in rainfall, or have no effect. Moreover,
the critical factors controlling the response are, for the most part,
unknown. This fundamental treatment-unit nonadditivity makes judg-
ments about the effects of seeding difficult.
INTRODUCTION
In 1975 the Florida Area Cumulus Experiment (FACE) was
conducted to determine the merits of using silver iodide to increase
rainfall and to isolate some of the factors contributing to the treatment-
unit nonadditivity .
The target consisted of an area of about 3 0 0 square miles to the north
and east of Coral Gables, Florida. In this experiment, 24 days in the
summer of 1975 were judged suitable for seeding based on a daily
suitability criterion of S - Ne 2.1.5, where S (seedability) is the
predicted difference between the maximum height of a cloud if seeded
and the same cloud if not seeded, and Ne is a factor which increases
with conditions leading to naturally rainy days. Generally, suitable
days are those on which the seedability is large, and the natural
rainfall early in the day is small. On each suitable day, the decision to
seed was based on unrestricted randomization; as it happened, 12 days
were seeded and 12 were unseeded.
The response variable Y is the amount of rain (in cubic meters x lo7)
that fell in the target area for a 6 hour period on each suitable day. To
provide for the possibilities of reducing the variability and discovering
some factors that may be contributing to the nonadditivity,
the following explanatory variables were recorded on each suitable
Echo coverage (C) = per cent cloud cover in the experimental
area, measured using radar in Coral
Gables, Florida,
Prewetness (P) = total rainfall in the target area 1 hour before
seeding (in cubic meters x lo7),
Echo motion (E) = a classification indicating a moving radar
echo (1) or a stationary radar echo (2),
Action ( A ) = a classification indicating seeding (1) or
no segding (0).
The data as presented by Woodley et al. are reproduced in,
Table 1.1.1.
In addition to selecting days based on suitability (S- Ne), the
investigators attempted to use only days with C < 13 %. A disturbed
day was defined as C > 13 %. From 'Table 1.1.1, the first two
experimental days are disturbed with the second day being highly
disturbed (C = 37.9 %).
As a first step in the analysis of the re!;ults of this experiment, we
suppose that there exists a vector-valued fiunction G such that the true
or 'best' relationship between the response and the explanatory
RESIDUALS A N D INFLUENCE IN REGRESSION
Table 1.1.1 Cloud seeding data. Source: Woodley et a/. 
variables is of the form
Y = G(A, C, El P, S - Ne; B; 8)
where Y is the 24-vector of responses, B is the vector of unknown
parameters whose dimension p' depends on G, 8 is a 24-vector of
unobservable random errors, and the remaining arguments indicate
that G may depend on the values of the explanatory variables A, C, E, P
and S - Ne. For further progress the form of G must be specified. Since
theoretical considerations that might suggest a form are lacking, we
proceed by imposing tentative assumptionsthat seem reasonable and
are not contradicted by available information.
Initially, we suppose that G is of the form
INTRODUCTION
where X is an 24 x p' full rank matrix whose columns correspond to
explanatory variables,including but not limited to those given in (1.1.1).
The choice of this form is based on convenience and the general notion
that linear models with additive errors often serve as reasonable local
approximations to more complex models; we have no firm information
to support or deny this supposition.
We next choose the complete set of explanatory variables (that is. the
columns of X). First, since regression through the origin does not seem
sensible here, we include a constant column of ones. Second, to allow
for the possibility of nonadditivity, we include all cross-product terms
between action A and the other explanatory variables listed in (1.1.1).
Finally, we include the number of days 7' after the first day of the
experiment as an explanatory variable. This variable,
which is also listed in Table 1.1.1, is potentially relevant because there
may have been a trend in natural rainfall or modification in the
experimental technique.
With the five explanatory variables given in (1.1.1), X now contains
p' = 11 columns. In general, we set p' = p + 1 if X contains a column of
ones and set p' = p otherwise, so p is always the number of explanatory
variables excluding the constant.
In scalar form, the model may be written as
Now that the form of the model has been specified the goals of our
analysis can be made more specific. The main goal is to describe the
difference A Y between the rainfall for seeded and unseeded days,
Thus, the additive effect and the four possible interaction terms are of
primary interest. The prediction of rainfall by itself is of secondary
Inferences concerning /I will be conditional on X and our analysis
will be based, at least initially, on least squares methods since these are
by far the most convenient and straightforward. For this to be sensible,
however, a number of additional assumptions are needed: for each
i = l , 2 ,..., 24,
RESIDUALS A N D INFLUENCE IN REGRESSION
(1) Eei = 0 (appropriate model);
( 2 ) var (ei) = a 2 (constant variance);
(3) cov (ei, E
= 0, i # j (uncorrelated errors);
(4) any measurement errors in the elements of X are small relative to a;
(5) the errors ci are (approximately) normally distributed.
If all of the structure that we have imposed so far is appropriate then
the usual normal theory inferences based on the fitted model given in
Table 1.1.2 will be accurate. But much of this structure lacks substant-
ive support and if we are to have faith in the conclusions we must be
convinced that our assumptions are not seriously violated and that
reasonable alternative structures will not produce severely different
conclusions. Answers to the following questions will surely help:
(1) Case 2 is considered to be a disturbed day and thus the process
under study may differ under the conditions of case 2. Is case 2
outside the local region of applicability of the assumed model?
More generally, are there outliers in the data?
(2) Is there evidence to suggest that the variances are not constant or
that the distribution of the errors deviates from normality in
important ways?
(3) Is there evidence to suggest that the form of the model (EY = X j?)
is not appropriate?
Table 1.1.2 Fitted model, cloud seeding data
A x ( S - N e )
INTRODUCTION
In 1975 the Florida Area Cumulus Experiment (FACE) was
conducted to determine the merits of using silver iodide to increase
rainfall and to isolate some of the factors contributing to the treatment-
unit nonadditivity .
The target consisted of an area of about 3 0 0 square miles to the north
and east of Coral Gables, Florida. In this experiment, 24 days in the
summer of 1975 were judged suitable for seeding based on a daily
suitability criterion of S - Ne 2.1.5, where S (seedability) is the
predicted difference between the maximum height of a cloud if seeded
and the same cloud if not seeded, and Ne is a factor which increases
with conditions leading to naturally rainy days. Generally, suitable
days are those on which the seedability is large, and the natural
rainfall early in the day is small. On each suitable day, the decision to
seed was based on unrestricted randomization; as it happened, 12 days
were seeded and 12 were unseeded.
The response variable Y is the amount of rain (in cubic meters x lo7)
that fell in the target area for a 6 hour period on each suitable day. To
provide for the possibilities of reducing the variability and discovering
some factors that may be contributing to the nonadditivity,
the following explanatory variables were recorded on each suitable
Echo coverage (C) = per cent cloud cover in the experimental
area, measured using radar in Coral
Gables, Florida,
Prewetness (P) = total rainfall in the target area 1 hour before
seeding (in cubic meters x lo7),
Echo motion (E) = a classification indicating a moving radar
echo (1) or a stationary radar echo (2),
Action ( A ) = a classification indicating seeding (1) or
no segding (0).
The data as presented by Woodley et al. are reproduced in,
Table 1.1.1.
In addition to selecting days based on suitability (S- Ne), the
investigators attempted to use only days with C < 13 %. A disturbed
day was defined as C > 13 %. From 'Table 1.1.1, the first two
experimental days are disturbed with the second day being highly
disturbed (C = 37.9 %).
As a first step in the analysis of the results of this experiment, we
suppose that there exists a vector-valued fiunction G such that the true
or 'best' relationship between the response and the explanatory
RESIDUALS A N D lNFLUENCE IN REGRESSION
in the left-hand square. More frequently, the method of fitting is as
much determined by available software as by assumptions. In the vast
majority of regression analyses, least squares is used to carry out fitting.
Whatever method of fitting is used, the right-hand square corresponds
to the fitted model, including estimates, tests, and so forth. The
treatment of the cloud seeding data in Section 1.1 is essentially an
estimation step.
The bottom arrow in Fig. 1.2.1 is labeled criticism. It is meant to
describe the act of critical assessment of the assumptions and the
assumed model, given the fit in the right square and the actual data
values. Criticism of a model may lead to modification of assumptions
and thus further iteration through the system. The questions at the end
of Section 1.1 may help guide this process.
Most of the work on model building, both for the statistician
developing methods and for the scientist applying them, has con-
centrated on the upper estimation path. In precomputer days, the
reason for this was clearly evident: fitting was laborious and time
consuming. One of the earliest books on regression by Ezekiel rarely strays far from the computational problems of regression,
and barely ventures beyond models with two predictors. Even
more recent books 
still discuss time-consuming methods of inverting matrices via
calculator. Since the fitting of models was inherently so difficult, it is not
unreasonable that methods of criticism would be slow to develop and
rarely used.
The availability of computers and the appearance of Draper and
Smith changed this trend. The problems of the estimation
step, at least via least squares, are now easily and quickly solved, and the
analyst can consider inherently more complicated problems of criti-
cism. Most of the methods for criticism (diagnostics) require compu-
tation of statistics that have values for each case in a data set, such as
residuals and related statistics. As a class we call these case statistics,
and call an analysis using these statistics case analysis.
For the unwary, there is an inherent danger that is caused by the
recent explosion of available methods for criticism. If every recom-
mended diagnostic is calculated for a single problem the resulting
'hodgepodge' of numbers and graphs may be more of a hindrance than
a help and will undoubtedly take much time to comprehend. Life is
short and we cannot spend an entire career on the analysis of a single set
of data. The cautious analyst will select a few diagnostics for
INTRODUCTION
application in every problem and will make aqadditional parsimonious
selection from the remaining diagnostics that correspond to the most
probable or important potential failings in the problem at hand.
It is always possible, of course, that this procedure will overlook
some problems that otherwise could be detected and that the urge to
always apply 'just one more' diagnostic will be overwhelming. The
truth is: If everything that can go wrong does go wrong, the situation is
surely hopeless.
Diagnostic methods using residuals
'Most of the phenomena which nature presents are very complicated; and
when the effects of all known causes are estimated with exactness, and
subducted, the residual facts are constantly appearing in the form of
phenomena altogether new, and leading to the most important conclusions.'
The residuals carry important information concerning the appropriate-
ness of assumptions. Analyses may include informal graphics to display
general features of the residuals as well as formal tests to detect specific
departures from underlying assumptions. Such formal and informal
procedures are complementary, and both have a place in residual
Most residual based tests for specific alternatives for the errors are
sensitive to several alternatives. These tests should be treated skepti-
cally, or perhaps avoided entirely, until other alternatives that may
account for an observed characteristic of the residuals have been
eliminated. For example, outliers will affect all formal procedures that
use residuals. Outlier detection procedures should usually be consid-
ered before any formal testing is done. On the other hand, informal
graphical procedures can give a general impression of the acceptability
of assumptions, even in the presence of outliers.
Anscombe demonstrates that the whole of the
data may contain relevant information about the errors beyond that
available from the residuals alone. However, in the absence of specific
alternative models or special design considerations, the residuals, or
transformations thereof, provide the most useful single construction.
2.1 The ordinary residuals
The usual model for linear regression is summarized by
D I A G N O S T I C M E T H O D S U S I N G R E S I D U A L S
where X is an n x p' full rank matrix of known constants, Y is an
,I-vector of observable responses, /? is a p'-vector of unknown
parameters, and E is an n-vector of unobservable errors with the
indicated distributional properties. To assess the appropriateness of
this model for a given problem, it is necessary to determine if the
assumptions about the errors are reasonable. Since the errors c are
unobservable, this must be done indirectly using residuals.
For linear least squares, the vector of ordinary residuals e is given by
where V = (oil) = X(XTX)-'XT and %' = (ji)
is the vector of fitted
values. The relationship between e and E is found by substituting
X/?+E for Y,
e = (I - V)(X/?+&)
= (I - V)E
or, in scalar form, for i = 1 , 2 . . . . , n,
This identity demonstrates clearly that the relationship between e and E
depends only on V. If the vijs are sufficiently small, e will serve as a
reasonable substitute for E, otherwise the usefulness of e may be
limited. For a sound understanding of the relationship between eand E ,
and most diagnostics in general, an understanding of the behavior of V
is important.
THE H A T MATRIX
The matrix V is symmetric (VT = V) and idempotent ( V Z = V), and it
is the linear transformation that orthogonally projects any n-vector
onto the space spanned by the columns of X. John W. Tukey has
dubbed V the'hat'matrix since it maps Y into %',%'
= VY . Since V is idempotent and symmetric it follows that
trace(V) = rank(V) = p',
v;j = rlii
and that V is invariant under nonsingular linear reparameterizations.
This latter property implies that, aside from computational concerns,
RESIDUALS A N D INFLUENCE IN REGRESSION
collinearity between the columns of X is irrelevant to an understanding
of how V behaves.
The projection onto the column space of X can be divided into the
sum of two or more projections as follows: Partition X = (X,, X,),
where X1 is r i x q rank q, and let U = X,(X:X,)-'Xi
projection matrix for the column space of XI. Next, let XT be the
component of X, orthogonal to XI,
XT = (I - U)X,. Then,
is the operator which projects onto the subspace of the column space of
X orthogonal to the column space of X,, and
This representation shows that the diagonal elements vii are non-
decreasing in the number of explanatory variables p'. It can also be
shown that, for fixed p', the vii are nonincreasing in,n.
Let X, = 1, an n-vector of ones. Then from (2.1.6) it follows
immediately that
V = llT/n+ 5(P
where % is the ti x p matrix of centered explanatory variables and x: is
the i-th row of 5. For simple regression, yi = /I, + /Ilxi +ei, vii = l/n
+ (si - X ) ~ / X ( S ~
For p > 1, contours of constant vii in
p-dimensional space are ellipsoids, centered at the vector of sample
The magnitudes of the diagonal elements of V play an important role
in case analysis. From (2.1.8), oii 2 lln, i = 1,2, . . . , n, provided the
model contains a constant. Upper bounds for vii depend on c, the
number of times that the i-th row of X, x:,
is replicated. If xj = xi, then
lTij = tlii and, using the symmetry and idempotency of V,
which implies that vii I l/c. Thus,
D I A G N O S T I C METHODS USING RESIDUALS
For models without a constant, the lower bound in (2.1.9) must be
replaced by zero. The value of vii can attain its absolute maximum of 1
only if xi occurs only once, and only if vij = 0, j # i. In this situation
Ei = yi and the i-th case will be fitted exactly. In effect, a single
parameter is then devoted to a single case. This situation is pathological
and will rarely occur in practice except when a variable is added to
model an outlier as in Section 2.2.2. It can, however. occur with some
frequency in multiple case generalizations.
The magnitude of oii depends on the relationship between xi and the
remaining rows of %. Characteristics of xi which cause uii to be
relatively large or small can be seen as follows : Assuming that the intercept is included in the model, let p, 2 11,
2 . . . 2 ppdenote the eigenvalues of ST
and let p,, . . . . pp denote
the correspondingeigenvectors. Then, by the spectral decomposition of
the corrected cross product matrix,
Further, letting Oli denote the angle between p, and xi we obtain
Thus, vii is large if: (1) xT xi is large, that is, xi is well removed from the
bulk of the cases; and (2) xi is substantially in a direction of an
eigenvector corresponding to a small eigenvalue of 2P S.
On the other
hand, if x f x i is small, vii will be small regardless of its direction.
The elements of Vareconveniently computed from any orthonormal
basis for the column space of X, such as that obtained from the singular
value decomposition of X, or the first p' columns of the matrix Q
from the QR decomposition . If
q: and qJ are the i-th and j-th rows of the first p' columns of Q, then
V . . = qTq..
Alternatively, the Choleski factor R (where R is upper triangular and
RTR = XTX)can beused tocompute the vij without invertinga matrix,
' I = xT(XTX)-'xj
= xT(RTR)-'xj
RESIDUALS A N D INFLUENCE IN REGRESSION
where ai = R - T ~ i
is a pf-vector. Now ai can be computed without
inversion by the method of back substitution since R is upper
triangular .
THE ROLE OF V I N DATA ANALYSES
The distribution of e, the vector of ordinary residuals, follows
immediately from (2.1.3): If E - N(0, rr21) then e follows a singular
normal distribution with E(e) = 0 and Var(e) = u2(1 - V), and the
variation in e is controlled by V.
The discussion of the previous section shows that cases remote in the
factor space will have relatively large values of vii. Since var(ji) = viia2
and var(ei) = (1 -uii)a2, fitted values at remote points will have
relatively large variances and the corresponding residuals will have
relatively small variances. Because of the analogy between var(ji) and
the variance of the sample average based on a simple random sample
(a2/n), Huber calls l/vii the effective number of cases
determining ji. Indeed, we have seen that when vii = I, ji = yi.
Many authors have hinted that the vii may play an important role in
understanding an analysis based on (2.1.1). Behnken and Draper 
study the pattern of variation in the vii and note that wide variation
reflects nonhomogeneous spacing of the rows of X. Huber and
Davies and Hutton point out that if max(vii) is not considerably
smaller than 1, it is probable that an outlier will go undetected when the
residuals are examined. The average of the vii is p'/n and thus max(vii)
2 pl/n. Accordingly, it may be difficult to identify outlying cases unless
11 is considerably larger than p'. Box and Draper suggest that for
a designed experiment to be insensitive to outliers, the vii should be
small, and approximately equal.
The importance of the vii is not limited to least squares analyses.
Huber cautions that robust regression may not be effective, or
work at all, if max(vii) is close to 1. Huber's rationale is that it will be
difficult for outliers to be identified and thus downweighted in robust
regression if max(cii) is large.
The max(rii) is also important in determining the asymptotic
character of least squares estimates: Let z denote a p'-vector with finite
elements. Then a necessary and sufficient condition for all least squares
estimates of the form zT$ to be asymptotically normal is max(vii) + Oas
11 --+ .I..
 . If max (qi) is not small a normal approximation
of the distribution of zT$ may be suspect, at least for some z .
DIAGNOSTIC METHODS USING RESIDUALS
Hoaglin and Welsch suggest a direct use of the I.,, as a
diagnostic to identify 'high-leverage points'. The motivation behind
this suggestion is based on the representation
The fitted value ji will be dominated by tiii y, if aii is large relative to
the remaining terms. They interpret vii as the amount of leverage or
influence exerted on ji by y i . It is clear, however, that for any L!,, > 0, ji
will be dominated by viiy, if yi is sufficiently different from the other
elements of Y (that is, an outlier).
When the fitted model is incorrect, the distribution of the unobservable
errors E and hence of the residual e will change. The goal in the study of
the residuals is to infer any incorrect assumptions concerning e from an
examination of e. Unfortunately, the correspondence between e and e is
less than perfect. In some problems, model failures will not be usefully
transmitted to e. In others, observed symptoms may be attributable to
more than one incorrect assumption.
Consider as an alternative to (2.1.1) the model
where the n-vector B = (b,) represents the bias in fitting (2.1.1) to a
particular set of n cases. Often, the bias may be viewed, at least
approximately, as B = Zq5, where 4 is an unobservable parameter
vector. The columns of Z may represent important variables not
included in X, or nonlinear transformations of the columns of X,
perhaps polynomials or cross products. If (2.1.1) is fitted but (2.1.12) is
the correct model, then
E(ei) = (1 - vii)bi-
Bias would be diagnosed by a systematic feature in a plot of residuals
against a column of Z, if Z were known However, the use of residuals
for cases with large vii in this or other diagnostic procedures is likely to
be limited, unless the bias at that case is extreme, since both terms on
the right of (2.1.13) approach zero as vii -t 1. If vii is small, ei may
behave more like an average of the elements of B than like b,. Most
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
procedures that use e to detect model bias will tend to emphasize the fit
of a model in the neighborhood of i (vii small) while ignoring the
relatively remote points (uii large).
E X A M P L E 2.1.1.
I L L U S T R A T I O N OF BIAS. Suppose that Xis given
by the first two columns of Table 2.1.1 and Z is given by the third
column. If the correct model is Y = X p i - Zdf 8 , but the fitted model
is Y = X ~ + E ,
the uii, hi, and E(ei) are as given in the next three
columns of the table. Even in this small example, the differences
between hi and E(ei) are clear. Cases wiih small vii will have hi
accurately reflected (on the average) by the ei, but cases with large aii do
not share this property. If the bias in the model was largest at extreme
cases (cases with larger values of vii), we would not expect the residuals
to diagnose this problem accurately.
Table 2.1.1
Dart1 stlt illlrsfra~irry hias \c~llrr~
sortle vii arc large
Now suppose that (2.1.1) is correct except that Var (e) = o Z W - ' , for
some unknown positive definite symmetric matrix W. If (2.1.1) is fitted
assuming that Var ( E ) = 0'1,
then E(e) = 0, but Var (e) = 02
( I - 1' I W - ' ( I - V). Depending on W - ', the actual variances of the
residuals may be quite different from a 2 (1 - vii), and from the variances
for the residuals that would be obtained if the correct weighted least
squares model were used (see Appendix A.l). For example, suppose
so only case I has variance potentially different from a'.
p f j = I.:,/[( 1 - rii) ( 1 - qj)].
the squared correlation between the i-th
LIIAGNOSTIC METf-1OI)S U S I N G RESIL>UALS
and j-th residuals (Appendix A.3). Then an easy calculation shows that
The effect of n~, # 1 depends on the values of \v,, t : , , . and p:,. If p:, is
small, var ( e , ) will be the only term seriously affected by the noncons-
tant variance. However, ifp:, is large, then thechange in var ( e j ) will be
comparable to the change in var (el ). If b r , is large. so c:, is less variable
than the other errors, the true variances of the residuals will be smaller
than their nominal values (in addition. the residual mean square will
underestimate 02). If W , is small, then all the variances can become
large. Analogous results for general W are more complicated, but it is
clear that the residuals need not reflect nonconstant variances in the E, if
some of the pizj are large.
When both bias and nonconstant variance are present, the residuals
will have both nonzero means and variances other than those given by
the usual formulae. However, examination of the residuals will not
generally allow the analyst to distinguish between these two problems,
since both can lead to the same symptoms in the residuals.
Other types of residuals
For use in diagnostic procedures, several transformations of the
ordinary residuals have been suggested to overcome partially some
of their shortcomings. We first consider in Section 2.2.1 the
Studentization of residuals to obtain a set of residuals that have null
distiibutions that are independent of the scale parameters. These
residuals are shown to be closely related to a mean shift model for
outliers (Section 2.2.2) and to the residuals obtained when each case in
turn is left out of the data (Section 2.2.3). Alternatively. the residuals
can be transformed to have a selected covariance structure: The usual
suggestion is to obtain a vector of length n -p' of residuals with
uncorrelated elements. The methodology and usefulness of these
residuals is briefly outlined in Section 2.2.4.
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
STUDENTIZED R E S I D U A L S
The ordinary residuals have a distribution that is scale dependent since
the variance of each ei is a function of both u2 and vii. For many
diagnostic procedures, it is useful to define a Studentized version of the
residuals that does not depend on either of these quantities. Following
Margolin , we use the term Studentization to describe thedivision
of a scale dependent statistic, say U , by a scale estimate 7'so that the
resulting ratio S = U / T has a distribution that is free of the nuisance
scale parameters. David makes a further distinction between
itlternul Studentization, in which U and Tare generally derived from the
same data and are dependent, and exterrtal Studentization, where U
and Tare independent.
lnterrlal Studentizntion
In least squares regression, the internally Studentized residuals are
defined by
where 6' = I; e ; / ( t ~
- p i ) is the residual mean square. We reserve the
term Studentized residual to refer to (2.2.1). This transformation of
residuals has been studied by Srikantan , Anscornbe and Tukey
 , Ellenberg , Beckman and Trussell , Prescott
 , and many others. Many of these studies were motivated by a
concern about outliers.
Ellenberg provides the joint distribution of a subset of
Studentized residuals, assuming that (2.1.1) holds
and E - N (0, d l ) . The derivation of the joint distribution uses some
interesting properties of the residuals and proceeds as follows. Suppose
that an m-vector I = ( i l , i2, . . . , i,)*
indexes the m Studentized
residuals of interest, and define RI and el to be m-vectors whose j-th
elements are riJ and e,), respectively. Also, define VI to be the m x m
minor of V given by the intersection of the rows and columns indexed
by I. The rank of V, is no greater than p', and its eigenvalues are
bounded between 0 and 1 . The tn x m matrix I - Vl is positive definite
whenever the maximum eigenvalue of V, is less than 1.
The random vector el follows a N (0, a2(1 - V,)) distribution. If we
can find a quadratic form in e that is independent of el, then the joint
distribution of e, and that quadratic form can be easily written. The
DIAGNOSTIC METHODS USING RESIDUALS
joint distribution of R, is then found by a change of variables and
integration.
Provided that the inverse exists, the required quadratic form is given
s:,, = eTe - e:(I - V,)- ' e l
where S:,, - aZX2
(n - p' - ni) and St,, and el are independent. These
facts can be proved using Appendix A.2 to show that S;! is the residual
sum of squares for (2.1.1) with the cases indexed by I removed from the
The joint density of e, and S& is then
where v = (n - p' - m)/2. Next, let D = diag ( 1 - vi,
i l , . . . , 1 -
and make the transformations
R, = 6 - ' ~ - 1 / 2 ~ ,
6 = [(S~)+e~(I-V,)-1el)/(n-p')]1~2
Computing the Jacobian. substituting (2.2.4) into (2.2.3), and integrat-
ing over 6 will give the density of Rl. If C, = D-'I2 ( I - V,)D- ' I 2 , the
correlation matrix of the residulas indexed by I, then the density of R, is
over the region rTC; ' r I
n - p', and zero elsewhere. Form (2.2.5) can
be recognized as an inverted Student function. Contours of constant
density of (2.2.5) are ellipsoids of the form rTC; ' r = c. For the special
case of m = 1, (2.2.5) reduces to
r ( v + + )
f ( r ) = ~ ( v ) ~ ( + ) ( n - p l ) l " n - p
; Irl I ( n - p')'12 (2.2.6)
Hence, r'/(n - p') follows a Beta distribution, with parameters 112 and
(n -pl- 1)/2, and it follows that E(ri) = 0, var (ri) = 1 and, from
(2.2.5), cov (ri, r j ) = - v i j / [ ( l - vil)(l - v j j ) ] ' I 2 , i # j.
RESIDUALS A N D INFLUENCE IN REGRESSION
The Studentized residuals are used as replacements for the ordinary
residuals in graphical procedures, such as the plot against fitted values
 . They are
also basic building blocks for most of the case statistics to be discussed
in this and later chapters.
External Studentization
For externally Studentized residuals, an estimator of 02 that is
independent of ei is required. Under normality of the errors,
Equation (2.2.2) provides such an estimate. Defining 6;) to be the
residual mean square computed without the i-th case, it follows from
Equation (2.2.2) that
(n - p')G2 - ez/(l - uii)
n - p i - 1
Under normality, 6;, and ei are independent, and the externally
Studentized residuals are defined by
(1 - uii)ll2
The distribution of ti is Student's t with n - p' - 1 degrees of freedom.
The relationship between ti and ri is found by substituting (2.2.8) into
ti=ri( n-p , -ri '.)
which shows that t: is a monotonic transformation of rf.
MEAN S H I F T OUTLIER MODEL
Suppose that the i-th case is suspected as being an outlier. A useful
framework used to study outliers is the mean shift outlier modei,
Y = X/?+di$+&
E ( E ) = 0,
Var (E) = 021
where d, is an ,I-vector with i-th element equal to one, and all other
elements equal to zero. Nonzero values of 4 imply the i-th case is an
D I A G N O S T I C M E T H O D S U S I N G RESIDUALS
Under this model an outlier may occur in y,, xi, or both. Suppose, for
example, that yi is not an outlier while the i-th row of X is in error by an
unknown amount 6,; that is, observed (xi) = xi - 6,. Then,
which is in the form of (2.2.1 1) with 4 = 6f /?.
It is instructive to rewrite (2.2.11) by making the added variable
orthogonal to the columns of X (as described near (2.1.5)),
where /?* is not the same as /? in (2.2.11), but 4 is the same in
both formulations. Because of the orthogonality, (2.2.13) can be fitted
in two steps. First, fit the usual regression of Y on X, ignoring the
additional variable. Next, estimate $J
from the regression of the
residuals e = (I - V)Y computed in the first step on the added
variable (I - V)di
$ = 1 - V) (I - V
( I - ( I - V )
The sum of squares for regression on X is YTVY, while the additional
sum of squares for regression on (I - V)di is J2(d:(1 - V)'di)
= eT/(1 -vii). Hence, the residual sum squares for (2.2.13) is
YT(I - V)Y - eZl(1 - uii). Assuming normality, the t-statistic for a
test of d = 0 is
which follows a t(n - p l - 1) distribution under the null hypothesis.
However, comparison of (2.2.15) with (2.2.7) and (2.2.9) shows that this
test statistic for the shift model is identical to the externally Studentized
Under the mean shift outlier model, the nonnull distribution of
when 4 # 0 is noncentral F with noncentrality parameter
42(1 - vii)/a2. Since the noncentrality parameter is relatively small for
vi, near 1, finding outliers at remote points will be more difficult than
finding outliers at cases with vii small. Yet it is precisely the former cases
where interest in outliers is greatest. Also, since oii is increasing in p'.
outliers become more difficult to detect as the model isenlarged.
RESIDUALS AND INFLUENCE IN REGRESSION
When the candidate case for an outlier is unknown, the test is usually
based on the maximum of the t; over all i. A multiple testing procedure,
such as one based on the first Bonferroni inequality ,
must be used to find significance levels. A nominal level a, two-tailed
test for a single outlier will reject if maxi Iti 1 > t(a/n; n - p' - 1).
Cook and Weisberg suggest the alternative rule max, I ti I = I ti I
> t(viia/pl; n - p' - 1). This rule maintains the overall significance level
but provides an increase in power at cases with large vii. Special tables
for the outlier test are provided by Lund , Bailey and
Weisberg . Moses provides useful charts. Tietjen, Moore
and Beckman give critical values for simple linear regression.
EXAMPLE 2.2.1.
ADAPTIVE SCORE DATA NO. 1. The simple re-
gression data shown in Table 2.2.1 are from a study carried out at the
University of Calfornia at Los Angeles on cyanotic heart disease in
children. Here, x is the age of a child in months at first word and y is the
Gesell adaptive score for each of n = 21 children. The data are given by
Mickey, Dunn and Clark and have since been analyzed
extensively in the statistical literature.
Table 2.2.1 Gesell adaptive score (y) and age atfirst word
(x), in months, for 21 children. Source: Mickey et al. 
The lower triangular part of the symmetric matrix V is given in
Table 2.2.2. Since even for simple regression V is n x n, it is rarely
computed in full, but we present it here for completeness. Examination
of this matrix indicates that most of the vii are small (19 of the 21 are in
Table 2.2.2
Projection matrix, V, for adaptive score data
4 0.04 -0.00
0.02 0.05 0.07
0.03 0.05 0.06 0.06
0.06 0.05 0.03 0.04
9 0.04 -0.01
0.07 0.04 0.02 0.03
0.02 0.05 0.07 0.06
11 0.04-0.02
0.08 0.04 0.01 0.03
12 0.04 -0.00
0.07 0.04 0.02 0.03
0.07 0.05 0.03 0.04
0.06 0.05 0.03 0.04
0.06 0.05 0.03 0.04
0.07 0.05 0.03 0.04
0.06 0.05 0.04 0.04
18 0.06 W - 0 . 0 5 -0.07
0.06 0.17 0.13 -0.03 -0.09
0.17 -0.11 -0.07 -0.05 -0.03 -0.03 -0.05 -0.00 @
0.04 0.05 0.06 0.06
0.06 0.05 0.03 0.04
0.05 -0.03
0.07 0.05 0.03 0.04
0.06 -0.05
0.04 0.06 0.06
RESIDUALS A N D I N F L U E N C E IN REGRESSION
the range 0.05-0.09), the only exceptions being
= 0.15 and v,,, ,,
= 0.65. Since Xuii = 2, the diagonal for case 18 is relatively large. (The
role of case 18 in this data set will be discussed at some length in
succeeding sections.) Also, the oij, i # j,are generally small and positive,
the exceptions again being associated with cases 2 and 18.
Next consider the linear regression model, y, = Po + P,xi + ci for the
data in Table 2.2.1. A scatter plot of the data is given in Fig. 2.2.1; the
numbers on the graph give the case number of the closest points. From
this graph, a straight line model appears plausible although cases 19,18,
and possibly 2 appear to dominate our perception of this plot. If the
points for these three cases were removed, the perceived linearity would
be less pronounced. Cases 18 and 2 fall near the perceived (and the
fitted) regression line, while case 19 is quite distant.
Figure 2.2.1
Scatter plot of the adaptive score data
Figure 2.2.2, an index plot (plot against case number) of the 4,
the comments of the last paragraph, with the residual for case 19 clearly
larger than the others. Figure 2.2.3 provides a plot of ri versus );i, a
DIAGNOSTIC METHODS USING RESIDUALS
Case number
Figure 2.2.2 Index plot of residuals, adaptive scores data
standard plot used to find various problems that might be a function of
the fitted values. Cases 18 and 19 stand apart in this data since o,,, ,,
= 0.65 and rI9 = 2.823. The statistic t , , = 3.607 computed from ri can
be used to test case 19 as an outlier; the Bonferroni upper bound for the
p-value for this test is 0.0425.
The importance of case 19 in fitting the model can best be judged by
deleting it and refitting the line, as summarized in Table 2.2.3. Deletion
of the case has little effect on the estimated slope and intercept but it
does clearly reduce the estimated variance. The role or influence of this
case, as contrasted with cases 2 and 18, will be pursued in Chapter 3 . 0
EXAMPLE 2.2.2,
CLOUD SEEDING NO. 2. AS pointed out in
Chapter 1, case 2 is an extremely disturbed day, and we may have prior
interest in testing case 2 as an outlier. Because of the prior interest, the
outlier statistic for case 2, t, = 1.60, can be compared to t ( n -p' - 1)
to obtain significance levels. However, since 0,. , = 0.9766, the power of
the test for this case is relatively small, and we cannot expect to detect
anything but extreme deviations from model (1.1.3). O
RESIIIIIALS AN11 I N F L U E N C E IN R E G R E S S I O N
Fitted values
Figurc 2.2.3 ri versus fitted values, adaptive score data
Table 2.2.3
Regressiotl sunlrtlaries wit11 U I I ~
without case 19, adaptive score
Case 19 deleted
df = 19; 6' = 121.50; R2 = 0.41
df = 18; 6' = 74.45; R' = 0.57
Accuracy of the Bonferro~zi bound for rlle outlier test
Under the outlier test that uses the rejection rule max ( 1 ; ) > F (crln; 1, n
I), the first Bonferroni upper bound for the true p-value is p-
value I n Pr ( F > t i ) where F follows an F(1, n -pl- 1) distribution
and t i is the observed value of max (t?). Cook and Prescott 
provide a relatively simple method for assessing the accuracy of this
DIAGNOSTIC METHODS USING RESIDUALS
upper bound. The advantage of this method is that numerical inte-
gration is not required.
Let pij denote the correlation between ei and ej (i # j).
let r,,, denote the observed value of the Studentized residual cor-
responding to max ( t f ) and define
dk) = {(i,j)Ii <J, r i < hn-P') (1 & P~,)}
u - /I+ - b- I p-value I u
a = n P r [ ~ > t ; ]
B" = x Pr[F > ri(n-p'-l)/()(n-pf)(l+p,,)-r;)]
/3- = x Pr [F > r;(n - p' - l)/()(n
(1 -pi,) - r;)]
It follows immediately from (2.2.17) that the upper bound is exact when
c( +) and c( -) are empty, or equivalently if
1 + max Ipij( < 2ri/(n - p')
This is equivalent to the sufficient conditions given by Prescott ,
Stefansky , and Srikantan . Note also that since
ri/(n -pf) < 1, the upper bound can never be exact if pi, = + 1 for
some i # j.
Calculation of the lower bound in (2.2.17) requires knowledge of the
pijs. In many designed experiments, these will have a simple structure
so that the lower bound can be calculated without difficulty. For
example, in a two-way table with one observation per cell there are only
three distinct residual correlations. Residual correlations for selected
models of 2k designs are given by Cook and Prescott . In other
cases, the lower bound may be approximated further by replacing pij in
/3+ and p- by max,,+,(pij) and min,(-,(pij), respectively. Our
experience suggests that this will often be adequate.
E X A M P L E 2.2.3.
A D A P T I V E SCORE DATA NO. 2. We have seen
previously that the upper bound on the p-value for the outlier test for
case 19 is 0.0425. While refining this value may be unnecessary from a
hypothesis testing point of view, it may be desirable to judge the
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
accuracy of the upper bound when p-values are used to assess the
weight of evidence against the null hypothesis. This can be done using
the lower bound (2.2.17).
Direct application of this bound would require evaluation of about
420 probability statements. While it would be straightforward to wrhe
a code to perform the required calculations, it will usually be sufficient
to employ the further approximation, so that the number ofprobability
statements that must be evaluated is reduced. A small number of
evaluations can be handled easily on many hand-held calculators.
Inspection of the residual correlations in Table 2.2.4 shows that all
correlations are in the interval [-0.556,0.202].
A first lower bound on
r - p+ - /I-
can be obtained by replacing each pij in the expressions for
by the lower and upper bounds, respectively. However, this
results in negative values for the lower bound on u - fit - fl- at
rlg = 2.923, so a closer approximation is required.
A second inspection of the residual correlations reveals that one pair
has a correlation of -0.556, two other pairs have correlations of
- 0.300, and of the remaining pairs 17 correlations lie in the interval
[0.002,0.202] and 190 lie in [- 0.221, - 0.0161. A second lower bound
on LY - /I+
- /3- can be obtained by using the four values { - 0.556,
-0.300, -0.016, 0.202) in combination with their respective frequen-
cies 11. 2. 190, 17) to evaluate /It and the four values {-0.556,
-0.300, -0.221,0.002) in combination with the same respective
frequencies to evaluate p-. This procedure, which requires the
evaluation of only eight probability statements, produces
< 0.0016. In short, the true p-value corresponding to rlg = 2.823 is
between 0.0409 and 0.0425.
As further illustration, the lower bounds obtained by using this
procedure for x = 0.01, 0.05, and 0.1 are 0.00997, 0.0476, and 0.086,
respectively. Clearly, this procedure produces useful bounds in each
Mltlriple cclses
As before. let I be an m-vector of case subscripts and let el, V, be as
defined previously. Multiple outlying cases can be modeled under a
shift model by
a.herc I) is 11 x 111 with k-th column di, and + is an m-vector of unknown
parameters. The normal theory statistic r:for testing 4 = 0 is obtained
Table 2.2.4 Residual correlations for the adaorive score data
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
in analogy to the development leading to (2.2.15) ,
(eT(1- V,)- 'el) (11 - p' - m)
((n - p')62 - eT(1- vl)- ' el)(m)
The null distribution of this statistic under normality is F(m, n -
p' - m). Critical values for the multiple case outlier test can be based on
the Bonferroni inequality, but these critical values are likely to be very
conservative.
The multiple case analogue of the internally Studentized residual,
since Var (el) = aZ(I - VI), is
The relationship between t:and r:is given by
Computations. Computing r: can be simplified by the use of
appropriate matrix factorization. Gentleman , for example, has
used a Choleski factorization of I - VI: There is an m x m upper
triangular matrix R such that RT R = I - V, . Given this factorization, the Studentized
residual can be computed in two steps. First, solve for a in the
triangular system RTa = el. Then, compute r:= aTa/;,. This method
has the advantage that if I* is the subset consisting of the first m* < m
cases included in I, then r:. = aT, a,/;,
where a, is the first m* elements
Alternatively, let Vl = r A r T be the spectral decomposition of VI,
with the columns of r (eigenvectors) denoted by y,, y,, . . . , y, and
the diagonals of A denoted by A, I . . . I
Following Cook and
Weisberg ,
provided that
< 1. If 2, = 1, deletion of the cases in I results in a
rank deficit model and a test to see if I is an outlying set is not possible
using the mean shift outlier model.
Finding the set I of m cases most likely to be an outlying set requires
D I A G N O S T I C M E T H O D S U S I N G R E S I D U A L S
finding I to maximize r t over all
possible subsets of size 111. Even
for modest n, if m is bigger than 2 or 3, this can be very expensive. This
problem can be approached in at least two ways. First, some lineac
models have a special structure for V (and thus also for the Vl) and this
structure can be exploited. Gentleman has used this idea to
obtain an algorithm for outliers in an r x c table with one observation
per cell. She finds, for example, if t ? ~ = 2. V1 can only be one of three
possible 2 x 2 matrices, while for nr = 5, V, will be one of 354 possible
5 x 5 matrices. The factorization of VI or I - V1 need only be computed
once, and then r:can be calculated for all I with a common value for VI.
Alternatively, sequential outlier detection methods can be used. These
methods have the disadvantage of failing to account for the signs of the
residuals and their relative position in the observation space; the nl
cases with the largest residuals need not be the best candidates for an nl-
case outlier. Furthermore, residuals of opposite sign or on opposite
sides of the observation space can mask each other so none appear as
outliers if considered one at a time. Sequential outlier methods have a
long history, dating at least to Pearson and Sekar ; see also
Grubbs and Dixon . Mickey et al. provide a
modification of the sequential methods based on fitting models using
stepwise regression methods that add dummy variables to delete
outliers. The Furnival and Wilson algorithm can be used to
perform the same function.
E X A M P L E 2.2.4.
A D A P T I V E SCORE DATA NO. 3 The eight pairs
of cases with largest r:or t : are listed in Table 2.2.5. All these pairs
Table 2.2.5 Eight largest rtfor the 11duprit.e
score data
R E S I D U A L S A N D INFLUENCE IN REGRESSION
include case 19; in fact, the subsets with the 20 largest rfall include case
19. However, since the critical value for the m = 2 outlier test at level
0.05 based on the Bonferroni bound is 14.18, none of the pairs would be
declared as an outlying pair by this conservative test. Clearly little is
gained here by considering cases in pairs. Case 19 is found to be an
outlier by itself, and we should not be surprised to find that it remains
an outlier when considered with other cases. In problems where the
cases have a natural relationship, perhaps in space or in time, pairs of
cases that include individual outliers may well be of interest. This is not
so in this example.
Orller otrtlier models
The mean shift outlier model is not the only plausible model for
outliers. As might be expected, alternative formulations can lead to
different procedures. For example, Cook, Holschuh, and Weisberg
 consider a variance-shift model in which the homoscedastic
model (2.1.1) holds for all but one unknown case with variance
wa2, \rl > 1. Assuming normality and maximum likelihood estimation
for (w, B; u2), the case selected as the most likely outlier need not be the
case with largest ei or r,, and thus at least the maximum likelihood
procedure based on this model is not equivalent to the mean-shift
model. However, if the case with the largest Studentized residual ri also
has the largest ordinary residual e,, then that case will be identified as
the most likely outlier under both the mean and variance shift models.
Another outlier model assumes that data is sampled from a mixture
q(x1 = ?l; (x) + (1 - nu2 (x), with mixing parameter 7c. This formu-
lation can include both location and scale shift models by appropriate
choice of J; and f2. Aitkin and Wilson consider maximum
likelihood estimation of n and the parameters off, and f, assuming
that the densities are normal. Marks and Rao present a similar
example with n assumed known. Since for this problem the likelihood
function is often multimodal, the solution obtained, necessarily by an
iterative method, will be sensitive to choice of starting values. Such
mixture models have also been considered in a Bayesian framework
 .
All of these methods differ from the outlier procedure based on the
maximum Studentized residual in the philosophy of handling outliers
since they are designed to accommodate outliers in the process of
making inferences about the other parameters. Our approach is to
identify outliers for further study. The action to be taken as a result of
DIAGNOSTIC METHODS USING RESIDUALS
finding an outlier, such as case deletion or downweighting, will depend
on the context of the problem at hand. This approach is more
consistent with the overall goal of identifying interesting cases.
The outlier problem has recently received more detailed treatment by
Barnett and Lewis and Hawkins .
The ordinary and Studentized residuals are based on a fit to all the data.
In contrast, the i-th predicted residual e(i, is based on a fit to the data
with the i-th case excluded. Let Bdenote the least squares estimate of /I
based on the full data and let
be the corresponding estimate with the
i-th case excluded. Then, the i-th predicted residual is
e ( , ) = y i - x T ~ and Allen use
PRESS = Ce;, (the predicted residual sum of squares) as a criterion for
model selection, better models corresponding to relatively small values
of PRESS. Much the same motivation, except from a Bayesian-
predictivist point of view, is provided by Lee and Geisser .
Stone and Mosteller and Tukey discuss the related ideas
of cross validation in which the data are split into two or more subsets,
and parameters estimated on one subset are used to obtain fitted values
for the other subsets to validate the model. A limit of this process, which
gives rise to e(i,, is obtained by dividing the data into n subsets, each
consisting of a single case.
A relationship between e,i, and ei is easily obtained using the
formulae in Appendix A.2,
which is identical to the estimate of $(2.2.14) under the mean shift
model. Deleting case i and predicting at xi is therefore equivalent to
adding a dummy variable di to the model and estimating a coefficient.
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
Moreover, the i-th predicted residual divided by the least squares
estimate of the standard error of prediction based on the reduced data
is equal to the i-th externally Studentized residual,
where X,i, is obtained from X by deleting the i-th row xT.
It is clear that the e,,, are normally distributed (if the ti are normally
distributed] with mean zero and variance cr2/(l - vii), and have the same
correlation structure as the ei. Use of the p~dicted residuals in place of
the ordinary residuals in case analysis will tend to emphasize cases with
while use of ei tends to emphasize cases with small vii. Using
PRESS as a criterion for model selection will result in preference for
models that fit relatively well at remote rows of X. To correct for this,
Studentized versions of the predicted residuals and of PRESS can be
suggested. Not unexpectedly, these will get us back to ri and ti:
Alternative versions of PRESS may be defined as ~ r f
or Ctf in the same
spirit as the weighted jackknife suggested by Hinkley . See
Geisser and Eddy and Picard for other uses of these
residuals in model selection.
While the Studentized residuals do correct the residuals for equal
variance, the correlation structure of the residuals is not changed.
Clearly. e can be transformed to have a different correlation structure.
Since the distribution of e is singular, the obvious goal of transforming
so that the elements of the resulting vector are uncorrelated can be met
only if we are satisfied with a lower-dimensional vector. This, in turn,
has the serious drawback that the identification of residuals with cases
becomes blurred, and interpretation of these transformed residuals as
case statistics is generally not possible. However, for some special
purposes, such as formal tests for normality, change points, or
nonconstant variance, transformation to uncorrelated residuals has a
certain intuitive appeal.
DIAGNOSTIC METHODS USING RESIDUALS
Suppose that an n x (n - p') matrix C defines a linear transformation
6 = CTY. We will call G a vector of linear unbiased scalar (or LUS)
residuals if
(unbiased condition)
Var ( 6 ) = u21 (scalar covariance matrix condition)(2.2.28)
These conditions require only that CTX = 0 and CTC = I. The two
common methods of choosing C both require that p' cases be
nominated to have zero residuals. The choice of the nominated cases
may be arbitrary, so that the definition of the uncorrelated residuals is
not unique.
Suppose we partition eT = (e:, e:), XT = (X:, X:), CT = (C:, C : )
such that the subscript 1 corresponds to the p' cases nominated to have
zero residuals, and subscript 2 corresponds to the remaining n - p '
cases. We assume X, to be nonsingular. It follows from (2.2.27) and
Appendix A.2 that C2 must satisfy
C, can be chosen to be any factorization of the matrix in square
brackets in (2.2.29). C, is then determined uniquely from C: =
- c:x2 x; '.
BLUS residuals
Theil added the requirement of best, to get BLUS residuals, by
requiring ii to minimize E [ (Z: - e2 )T(T: - e2)]. Theil showed that
this is equivalent to using a spectral decomposition to find C,.
Computational methods are given by Theil and Farebrother
Using the BLUS residuals, Theil proposed a competitor to the
Durbin-Watson test for serial correlation; critical values
of Theil's test are given by Koerts and Abrahamse . These
two tests have been compared in several studies and are generally
comparable in power, although the Durbin-Watson statistic has
superior theoretical properties. Variants of Theil's method are given by
Durbin , Abrahamse and Louter , Abrahamse and Koerts
 , and Sims .
Huang and Bloch used G in place of e in testing for normality.
They point out that the independence of the BLUS residuals holds if
and only if the errors are normally distributed, and thus, under a non-
RESIDUALS A N D I N F L U E N C E I N REGRESSION
normal distribution for 8, the apparent advantages of using the BLUS
residuals disappears. Furthermore, they point out that the indepen-
dence of the BLUS residuals is lost if heteroscedasticity is present.
Thus, it should be no surprise that e appears to be more useful in
normality tests than G. Hedayat, Raktoe, and Telwar use the
BLUS residuals in a test for nonconstant variance.
Recursire residuals
To construct the recursive residuals 
it is necessary to first order the cases, typically by time. With the cases
ordered, the k-th recursive residual %, is defined as
where Pk-,
and Xk- , are computed using the first k - 1 cases only.
The term recursive is applied because Bk can be computed from - , by
use of an updating formula. Under (2.1.1) and normality, it is
straightforward to show that the Fk for k > p' are independent and
N (0, a'). Equivalent versions of (2.2.30) have been proposed as early as
Pizetti (1891). Algorithms for their construction are given by Brown et
al. and Farebrother .
The recursive residuals, which correspond to using a Choleski
factorization to choose C2 , are appropriate for examining
assumptions that depend on the order of the cases. Brown et al.
 consider two tests for a change point in the parameter vector bas
a function of k via cumulative sums of recursive residuals. Phillips and
Harvey use the recursive residuals in developing a test for serial
correlation. Tests for heteroscedasticity using recursive residuals are
discussed by Hedayat and Robson and Harvey and Phillips
2.3 Plotting methods
Residuals can be used in a variety of graphical and nongraphical
summaries to identify inappropriate assumptions. Generally, a
number of different plots will be required to extract the available
information.
DIAGNOSTIC METHODS USING RES1DUAI.S
Standard residual plots are those in which the ri or ei are plotted against
fitted values ii or other functions of X that are approximately
orthogonal to ri (exactly orthogonal to the ei). Anscombe gives
an interesting discussion of the motivation for these graphical pro-
cedures. The plots are commonly used to diagnose nonlinearity and
nonconstant error variance. Patterns, such as those in Fig. 2.3.l(b)--(d).
are indicative of these problems, since under a correctly specified model
the plots will appear haphazard, as in Fig. 2.3.1 (a).
Figure 2.3.1 Residual plots. (a) Null plot. (b) Nonconstant variance.
(c) Nonlinearity. (d) Nonlinearity. Source: Weisberg 
Historically, the ordinary residuals have been used most frequently
in standard residual plots. Recently, however, a number of authors.
including Andrews and Pregibon , have indicated a preference
for the Studentized residuals. The patterns in plots using the ri will not
be complicated by the nonconstant residual variances and will
RESIDUALS A N D INFLUENCE IN REGRESSION
generally be more revealing than those using ei. It is possible, for
example, for a plot using the e, to show a pattern similar to that in
Fig. 2.3.l(b) simply because the uii are not constant.
In simple regression, the plot of residuals against ji provides the
relevant information about the fit of the model that is available without
use of extra information such as time or spatial ordering of the cases.
In multiple regression, the proper choice of horizontal axis for this
plot is more problematic, as the two-dimensional plot is used to
represent a model in a p'-dimensional space. In essence, a
vector in p'-dimensional space is chosen and the data points are
projected onto that single vector. For example, in the fitted model j = 3
+ 2s, + 4s2, a plot of ri against x, will plot all points with the same
value of x, (regardless of x,) at the same position of the abscissa, while
a plot of ri versus ji treats all cases with the same value of 3 f 2x, + 4x2
as equivalent. The first of these two plots may be used to find model
inadequacies that are a function of xi alone, such as the need to add xf
to a model, or nonconstant variances of the form var (6,) = xlic2, i
= 1, 2, . . . , n, but will be inadequate for detecting an interaction
between x, and x2. Similarly, the plot of the residuals versus ji will be
useful in finding model inadequacies in the direction of the fitted values,
such as a variance pattern that is a monotonic function of the response.
For any n-vector 2, the vector VZ is in the column space of X. The
equivalence class of points plotted at the same place on the abscissa
consists of a (p - 1)-dimensional flat. The plot of residuals
against VZ will be most useful if the model acts in the same way
on all points in the equivalence class. The common choices for the
abscissa are VY = P, and, if Xj is the j-th column of X, VXj = Xi. Less
common, but equally useful, are plots against principal component
score vectors, which, except for a scale factor, are the columns of
the n x p matrix of left singular vectors in the singular value de-
composition of d The use of these corresponds to plotting in the
direction of the eigenvectors of 97 Z.
E X A M P L E 2.3.1.
CLOUDSEEDING NO. 3. Figure 2.3.2isa plot ofr,
versus ji for the cloud seeding data. This plot is clearly indicative of
some problem, since cases 1 and 15 are well separated from the others,
predicted rainfall is negative for two cases, and the general pattern of
the residuals appears to decrease as ji increases. It may show the need to
transform Y to correct possible nonlinearity and perhaps to eliminate
negative predicted rainfalls, or it may suggest other remedies such as
DIAGNOSTIC METHODS USING RESIDUALS
Fitted volues
Figure 2.3.2 ri versus fitted values, cloud seeding data
transforming predictors, or giving special attention to the cases that are
separated from the rest of the data.
Figure 2.3.3, a plot of r., versus S - Ne, suggests that the variance is a
decreasing function of S- N e since most of the large residuals
correspond to small values of S - Ne. In combination. the two plots
clearly suggest that the original model is inadequate. but the approprl-
ate remedial action is not clear.
Plots of residuals against VZ are often difficult to interpret because
informative patterns can be masked by the general scatter of points. As
an aid to using these plots for relatively large data sets, Cleveland and
Kleiner 11975) suggest superimposing robust reference lines. Let the
values plotted on the abscissa be denoted by a,, k = 1,2, . . . , n. with
the a, ordered from the smallest to largest, and let a,,. . . . . a,, be the
1 values of a with the smallest absolute deviation from a,. Let h,,.
j = 1, . . . , I, be the corresponding values of the ordinate (usually the
residuals or Studentized residuals). For each k, robust estimates of the
0.25, 0.50, and 0.75 quantiles of b,,, j = 1, . . . . 1 are plotted against a
RESIDUALS A N D INFLUENCE IN REGRESSION
Figure 2.3.3
ri versus S - Ne, cloud seeding data
robust estimate of the median of the akj . The window length I must
be chosen to balance resolution and stability, and is often chosen by
trial and error.
E X A M P L E 2.3.2.
OLD F A I T H F U L GEYSER. A geyser is a hot spring
that occasionally becomes unstable and erupts hot water and steam
into the air. One particular geyser, Old Faithful in Yellowstone
National Park, is particularly well known and is one of the major
tourist attractions in the United States. It erupts at an interval ofabout
40-100 min, with eruptions lasting from 1-6 min, to heights of near
35 m. National Park personnel predict eruption times based on the
length of the last eruption. Their predictions are based on the empirical
linear equation (minutes to the next eruption) = 30 + 10 x (duratiori of
current eruption in minutes). Because the physical mechanisms that
govern eruptions of the geyser are unknown, the prediction problem is
DIAGNOSTIC METHODS USING RESIDUALS
one of statistical modeling based on observed values of intervals and
durations only.
Figure 2.3.4 contains a scatter plot of y = interval versus
x = duration for 272 eruptions of Old Faithful in October, 1980. These
data were provided by Roderick A. Hutchinson, the Yellowstone Park
geologist. Following standard park procedure, intervals are measured
from the beginning of one eruption to the beginning of the next. The
figure indicates that a simple regression model is at least plausible for
this prediction problem, although the clustering of points into two
groups is clearly evident. Figure 2.3.5 gives a plot of r, versus x,. While
the clustering is clear, there is no obvious problem. However, if the
robust reference lines are superimposed, as in Fig. 2.3.6 (with 1 = 30),
slight curvature in the plot becomes apparent: extreme durations lead,
on the average, to predictions that are too long. With the reference lines
superimposed we recognize the possible need for a transformation of
this data.
Figure 2.3.4 y (interval to next eruption in minutes) versus x (duration of
current eruption to the nearest 0.1 min) for 272 eruptions of Old Faithful
Geyser, October, 1980. Source: Roderick A. Hutchinson, Yellowstone National
Figure 2.3.5
ri versus x, Old Faithful data
Figure 2.3.6 Enhanced residual plot, Old Faithful data, window width = 30
DIAGNOSTIC METHODS U S I N G RESIDUALS
E X A M P L E 2.3.3.
RESIDUAL.^ I N PLACE.
When the cases in a data
set have identifiable physical locations, useful information about a
model may be obtained by a semigraphical display obtained by plotting
the residuals in their physical locations. An example of this is given by
Daniel who discusses a classic 2' experiment on beans reported
by Yates . The experiment was carried out in blocks of 8. with two
3-factor and one 4-factor interaction confounded with blocks. Fitting a
model including one block elTect, four main effects and one 2-factor
interaction, Daniel obtained residuals, and plotted then1 in their
locations in the field (for a balanced design, all the rii are equal. so a plot
of the residuals is equivalent to a plot of the Studentized residuals), as
reproduced in Fig. 2.3.7. This plot indicates a region of apparent high
fertility that extends into all four blocks, and is therefore not removed
by the blocking effects. Daniel reanalyzed the data, using only blocks I
and I11 and found that the estimated residual variation is reduced by a
factor of 3.
Figure 2.3.7
Residuals in place. Source: Daniel . reprinted with
permission
When a constant term is not included in a model, plots of residuals
versus VZare complicated by the fact that the simple regression of e on
VZ is nonzero. If C, j, and ? are, respectively, the average of the
residuals, the ys, and the vector of averages of the xs, then 2 = .f - iTB
must be zero only if the constant is in the model. The slope of the
regression of e on VZ is
Thus, even a null plot will exhibit systematic features, especially if i is
far from zero.
RESIDUALS A N D INFLUENCE IN REGRESSION
ADDED VARIABLE PLOTS
For models with a constant, the standard plot of e versus VZ exploits
the orthogonality (or near orthogonality if the ti are used) between
plotted variables. Systematic nonlinear features of such plots suggest
model inadequacies, and may be useful when specific alternative
models are not available. However, they do suffer from the visual
difficulty that is often apparent in attempting to detect systematic
features of a swarm of points. This difficulty can be overcome by using
plots in which a systematic linear feature indicates an incorrect model.
To obtain these plots, we must choose a specific alternative for the
fitted model. From the alternative, a plot, and usually a test, can be
derived that compares the two models. These plots are often easy to
interpret and can be very useful.
Consider first an alternative model that differs from (2.1.1) by the
inclusion of a new explanatory variable Z. We hypothesize as an
alternative to (2.1.1) the model
An appropriate test comparing (2.3.2) to (2.1.1) is the F-test for $ = 0.
An equivalent plot is derived as follows. Defining as usual
V = X(XTX)-'XT, multiply both sides of (2.3.2) by I - V, to get
The left side of (2.3.3) is just the residual vector e for the model (2.1.1).
The first term on the right side is exactly zero. Taking expectations over
e in (2.3.3) gives
which suggests that a plot of e versus (I - V)Z will be linear, through
the origin. We call the plot of e versus (I - V)Z an added variable plot,
since it is designed to measure the effect of adding a variable to a model.
These plots have been discussed or illustrated by Draper and Smith
 , Anscombe , Mosteller and Tukey , Belsley,
Kuh and Welsch , and Weisberg .
In the regression of e on (I-V)Z, the estimated slope is
and the intercept is 0 if there is a constant in the model.
By the conditions given in Kruskal , the correct generalized least
squares estimate obtained using the covariance matrix implied in (2.3.3)
DIAGNOSTIC METHODS USING RESIDUALS
is identical to the ordinary least squares estimate given by (2.3.5). Using
the results near (2.1.5), it can be shown that 6 is identical to the least
squares estimate of 4 obtained from the regression of Y on both X and
Z. From this it follows immediately that the residuals in the added
variable plot are the same as the residuals for the regression of Y on
both X and Z.
Added variable plots are very useful for studying the role of a
variable Z if it enters linearly into a model. The general scatter of the
points gives an overall impression of the strength of the relationship.
Individual points that are well separated from the rest of the data give
heuristic information about the effects of outlying points on individual
coefficients, and may suggest cases for special study.
The added variable Z can represent either a constructed variable that
is defined by a specific alternative model, as will be discussed later in this
chapter, or one of the variables in the model. If U, is the projection
matrix on all the columns of X except X,, then the k added variable
plots of (I - Uk)Y versus (I - Uk)X, have been advocated by Belsley et
al. , who call them partial leverage regression plots.
Non-null behavior
When the appropriate model for the relationship between Y and (X, 2)
is more complicated than model (2.3.2), the usefulness of the added
variable plot depends on V. To see this, consider the model
Y = X/?+9Z'"+&
where Z") has i-th element
Power transformations are used in several places in this chapter, and
provide a rich and interesting class of nonlinear functions. Using a
linear Taylor series expansion about I = 1, z)" = zi + (I - l)z,log (z,),
so the model (2.3.6) is approximately
where L is an n-vector with i-th element zi log (zi). Multiplying by
(I - V) and taking expectations,
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
Thus the regression of e on (I - V ) Z may have any shape if V is chosen
appropriately and 4 # 0, 1 # 1. Similar results are obtained if Y is a
nonlinear function of X.
Cotnputatio~u. The added variable plots for each X, are potentially
expensive to compute since for each plot two sets of residuals must be
computed. However, Mosteller and Tukey and Velleman and
Welsch outline a method to obtain these plots in a relatively
simple way. For the variable X, Equation (2.3.5) implies that
where A, = (I - Uk)Xk/X:(I - Uk)Xk is the k-th column of an n x
p' matrix A. In matrix form, (2.3.10) is simply B = ATY. But,
since )= (XTX)-'XTY, it follows that AT = (XTX)-'XT, the
Moore--Penrose generalized inverse of X. Except for a scale factor,
( I - U,)X, is the k-th column of A: If aij is the (i, j)-th element of A,
the i-th element of (I - U,)X, is aik/C,ai. Given (I- U,)X, and
e = (1 - V)Y, the vector (I - Uk)Y is computed from the identity
which is proved by writing V as a sum of projections, V = U, + T,,
where Tk is the orthogonal projection on (I - U,)X,,. Then (I - U,)
Y = (1 - V ) Y +TkY, which upon simplifying gives (2.3.11).
As long as sufficient computer storage is available, the
Moore-Penrose inverse can be computed to obtain added variable
plots. However, if X is illconditioned, the Moore-Penrose inverse can
be numerically unstable. G. W. Stewart suggests that a stable algorithm can be based on the QR
decomposition . If X,, is the last column of X, Q,, is the
corresponding column of Q, and r,.,, is the indicated element of R, then
(I - U,.)X,. = rP.,.Qp.. (I - Uk)Xk can be computed for other columns
by using routines SQRDC, SQRSL, and SCHEX in LINPACK
 .
E X A M P L E 2.3.4.
J E T F I G H T E R S NO. 1. Stanley and Miller, in a
1979 RAND Corporation technical report, have attempted to build a
descriptive model of the role of various design and performance factors
in modeling technological innovation in jet fighter aircraft. Using data
on American jet fighters built since 1940, they used the date of the first
D I A G N O S T I C M E T H O D S lJSlNG RESIDUALS
flight as a stand-in for a measure of technology; presunlably. the level of
technology is increasing with time. In some of their work. they
considered the following variables:
FFD = first flight date, in months after January 1940:
SPR = specific power, proportional to power per unit weight;
RGF = flight range factor;
PLF = payload as a fraction of gross weight of aircraft;
SLF = sustained load factor;
CAR = 1 if aircraft can land on a carrier; 0 otherwise.
Exact definitions of all these quantities can be found in Stanley and
Miller . Between 1940 and 1979, 39 American jet fighters were
flown. Of these, 14 aircraft were modifications of earlier aircraft, and
for three others, the F-14A, F-15A, and F-16A, data are not available.
Data on the 22 remaining planes are given in Table 2.3.1. Following
Stanley and Miller we will fit models with FFD (or transformations of
it) as a linear function of the other variables.
Table 2.3.1 Jetfighter datn. Source: Stat~ley and Miller 
RESII>UALS A N D INFLUENCE IN REGRESSION
An issue in building a model for these data is the choice of appro-
priate scaling for the response and for the explanatory variables. The
use of FFD as the response suggests the unlikely assumption that
technological innovation is constant over time. It is perhaps Inore
reasonable to transform FFD so that the rate of change decreases with
time. since we are measuring innovation in one general technology. A
possible alternative scaling is the logarithm of FFD as a response, but
the value of log ( F F D ) will depend on the choice of origin for FFD. If
FFD is measured in months after January 1, 1900, then log ( F F D ) for
the range of first flight dates in the data would represent rates ofchange
that are nearly constant, while using January 1, 1940 as an origin will
allow greater variation. Following Miller and Stanley, we tentatively
adopt this as an origin both to allow for greater variation in the rate of
change and because 1940 represents a reasonable origin for the jet age.
In this example, we will define LFFD = log (FFD), and use natural
logarithms. We return to the problem of scaling FFD later.
The regression of LFFD on the five predictors is summarized in
Table 2.3.2. Three of the five variables are associated with large
1-values, and the coefficients for C A R and P L F are negative, indicating
that the ability to land on carriers, and the payload size adjusted for the
other variables, are negatively related to LFFD. The aircraft with the
largest vii is the F-1 1 1A with v,,, ,, = 0.496, although several of the vii
are of comparable magnitude. The F-111A also has the largest
Studentized residual, rZ2 = 2.337, with corresponding t,, = 2.77.
The added variable plots for SPR, RGF, and S L F are given as
Figs. 2.3.8-2.3.10. The apparent linear trends in the first two of these
Table 2.3.2
Fitted models for jet .fighter data
Case 22 (F-1 I 1 A) deleted
Residuals of SPR on RGf PLF SLF CAR
Figure 2.3.8 Added variable plot for SPR, jet fighter data
Residuals of RGFon SPR PLF SLF CAR
Figure 2.3.9 Added variable plot for RGF, jet fighter data
R E S I D U A L S AN[> I N F L U E N C E IN REGRESSION
Residuals of SLFon SPA RGF PLF CAR
Figure 2.3.10 Added variable plot for SLF, jet fighter data
figures suggest the usefulness of these variables as predictors, although
in each plot our attention is drawn to one case, the F-86A for SPR and
the F-1 11A for RGF. These cases may have an important role in
determining the corresponding coefficients.
Figure 2.3.10, the added variable plot for SLF, shows only a slight
linear trend, as reflected in the corresponding t S L ~
= 1.82 in Table 2.3.2.
However, the F-86A and F-1 1 1A are quite far from the trend line and
may indicate that the presence of these two aircraft actually suppresses
the usefulness of S L F . 0
Partial residual plots have been suggested as cornputationally con-
venient substitutes for the added variable plots. Recall that an added
variable plot is a plot of ( I - U,)Y = e -I- (I - u,)x,,!?~
(1 - U, )X,. The first component e of the ordinate is orthogonal to the
abscissa and represents scatter. The second component represents the
systematic part of an added variable plot.
D I A G N O S T I C M E T H O D S USING RESI1)UALS
Computationally, the most difficult part of an added variable plot is
obtaining U,. If this matrix is replaced by zero the result is a partial
residual plot of e + x,F, versus X,. Ezekiel used such a plot to
diagnose the need to transform an explanatory variable. As with the
added variable plot, the two terms that make up the ordinate are
orthogonal, the first term representing scatter and the second giving the
systemmatic component. Again, the slope of the regression in this plot
is /I,, and the residuals from the regression line are given by the
elements of e. This plot was called a partial residual plot by Larsen and
McCleary , and a residual plus component plot by Wood .
Although both the added variable plot and the partial residual plot
have the same slope and the same residuals, their appearance can be
markedly different. In the added variable plot, for example, the
estimated variance of the slope is
where R: is the square of the multiple correlation between X, and XI,
the matrix containing the other Xs. Apart from the multiplier
(n - pl)/(n - 2), the apparent estimated variance of /j, in the added
variable plot is the same as the estimated variance of D, from the full
regression. In the partial residual plot the apparent variance of /j, is
which ignores any effect due to fitting the other variables. If R: is large,
then (2.3.13) can be much smaller than (2.3.12), and the partial
residual plot will present an incorrect image of the strength of the
relationship between Y and X, (conditional on the other Xs). In fact, it
can be seen that the partial residual plot is a hybrid, reflecting the
systematic trend of X, adjusted for XI, but the scatter of X,
ignoring X,.
E X A M P L E 2.3.5.
JET FIGHTERS NO. 2. The
plots corresponding to the added variable plots for SPR, RGF, and
Figure 2.3.1 1 Partial residual plot for SPR, jet fighter data
Figure 2.3.12 Partial residual plot for RGF, jet fighter data
DiAGNOSTlC METHODS USING RESIDUALS
Figure 2.3.13 Partial residual plot for SLF, jet fighter data
SLF given previously are shown as Figs. 2.3.11-2.3.13. The two plots
for SPR (Figs. 2.3.8 and 2.3.1 1) are not too different, although the
overall impression of the partial residual plot is of a stronger
relationship than is shown in Fig. 2.3.8, and the F-86A is no longer an
extreme point. The two plots for RGF are very similar, and would lead
to the same conclusions. The two plots for SLF, however, are quite
different. In particular neither the F-111A nor the F-86A stand apart
from the rest of the data in Fig. 2.3.13, and the general swarm of points
is shifted right.0
Let y,, y,, . . . , y, denote n independent, univariate observations and
let F denote a cdf from a location/scale invariant family with mean 11
and variance at. Under the hypothesis that the yis are an identically
distributed sample from F, the regression of the vector of observed
order statistics uT = (u,,
u2, . . . , u,), U, = max (yi), on the vector of
R I : S I I ) U A L S ANI.) I N F L U E N C E I N R E G R E S S I O N
expected order statistics from the cdfF, of the standardized variate
( y -ii)/cr is linear ,
where the i-th element of the n-vector a is cri = E((i~~,-/i)/r~).
implies that a plot of u versus a can be used to check the appropriate-
ness of the hypothesized cdfF, with a substantially nonlinear plot
indicating an incorrect choice. Such plots are called probability plots
and have been in use since at least 1934 .
When the hypothesized distribution does not correspond to the
actual sampling distribution, the shape of the probability plot depends
on the 'difference' between the sampling distribution and the hypo-
thesized distribution. If the actual sampling distribution has relatively
short tails, then the probability plot will tend to be S-shapedLA long
tailed sampling distribution leads to/shaped
plots. Relatively skewed
sampling distributions result in J-shaped/or
inverted J-shapedf
plots. depending on the direction of the skew.
Probability plots can also be used as devices to find a few elements ofa
sample that differ from the others. For example, Daniel suggested using probability plots to assess the signifi-
cance of efTects in unreplicated factorial designs with all factors at two
levels. If the absolute values of the usual contrasts are plotled against
half-normal order statistics then the large or significant contrasts will
be plotted near the upper right corner of the plot, while the smaller or
nonsignificant contrasts will more or less fall on a line; see Zahn 
for details. The identical method can be used to detect outliers in
general: outlying elements of a sample will tend to fall toward the
extremes ofthe plot, while most of the points will fall on a line that does
not point toward the apparent outliers.
In general judging the adequacy of a probability plot requires
experience. For the normal distribution, Daniel and Wood and
Daniel provide many pages of training plots that may help the
analyst gain the necessary experience.
The construction of probability plots may be hindered by the
unavailability of exact values for expected order statistics. However,
DIAGNOSTIC METHODS USING RESIDUALS
adequate approximations can usually be constructed from F - '. For the
standard normal cdf 0, for example, ai = 0 - ' ( ( i - 3/8)/01+ 114))
provides an excellent approximation for n 2 5 .
Approximations for the half-normal distribution are given by Sparks
 , and for the gamma distribution by Roy, Gnanadesikan and
Srivastava . Wilk and Gnanadesikan coined
the term Q-Q plot (for quantile versus quantile) for these probability
plots to reflect the practical manner in which they are constructed.
In some problems, it may be useful to have a summary statistic for a
probability plot. An intuitively reasonable summary for symmetric
families is the squared correlation between the plotted quantities,
Small values of W' would give evidence against the assumed
distribution.
The statistic W' was suggested as a test for normality by Shapiro and
Francia ; a similar statistic
was suggested by Filliben . W' was originally suggested as an
approximation to the Shapiro and Wilk Wstatistic,
where 0 is the variance-covariance matrix of the order statistics from
the standard distribution . Weisberg pointed out
that for the normal distribution W and W' are essentially identical.
Both statistics have reasonable power against a wide class ofalternat-
ives. Critical values for the normal distribution are given by Shapiro
and Wilk and Shapiro and Francia , and have been widely
reprinted elsewhere. Prescott has studied the behavior of If' in
the presence of one or two outliers. Shapiro, Wilk and Chen and
Pearson, D'Agostino and Bowman compare various tests for
normality.
In regression, the probability plot and W (or W') are usually applied
to e or to the ri since E is unobservable. For example, normal plots of
residuals or Studentized residuals are a standard feature of most
regression packages. Unfortunately, normal plots and the correspond-
ing tests may not be effective when applied to residuals. Recall from
Equation (2.1.4) that ei = ~ ~ - C ~ v ~ ~ s ~ .
As long as the sjs have finite
RESIDUALS A N D INFLUENCE IN REGRESSION
variance, ZJtl,J~J
will tend toward normality and in some cases may
dominate E,. Thus, the el or the r, may exhibit a supernormality property
 and behave more
like a normal sample than would the E,. In small samples, the usefulness
of normal plots is unclear, and depends on n, p', and on V . In larger samples, however, if max (u,,) -t 0 (as required for i
asymptotic normality) W and W' applied to residuals is the same as
applying them to the unobserved errors . In
such cases, a normal plot of residuals may be interpreted in a way
equivalent to a normal plot of a univariate sample.
Atklnson , following Gentleman and Wilk , suggests
a method of interpreting probability plots of residuals, even in small
samples. The technique presented here is a straightforward extension of
At kinson's basic idea.
For a problem with (I - V) fixed, rn pseudo-random n-vectors
. . . , E, are generated from F (usually, F will be taken as standard
normal). The pseudo residuals e, = (I - V)E,, k = 1,2, . . . , m, are then
computed. Let the ordered elements of e, be denoted by e,,,, and, for
each i, let e:,,, 0 < q c 1, denote the q x 100 percentile of the empirical
distribution of {e,,,,, k = 1,2, . . . , m). Simultaneous probability plots
of the two n-vectors with elements (e;"i(") and (e:,;"'") describe an
envelope. roughly like a (1 -2a) x 100% simultaneous confidence
region. The probability plot of the data is then plotted along with the
corresponding envelope. If the observed residuals fall beyond or near
the boundary of the envelope, the assumption that E is sampled from F
1s called into doubt. If it is desired to use the envelope as an exact test,
further simulation may be necessary to determine the size. Atkinson
(198 1) uses a transformation of residuals in this plot, and chooses to use
F = half-normal distribution, but the ideas are the same regardless of
the transformation and choice of F.
E X A M P L E 2.3.6
C L O U D S E E D I N G NO. 4. Toillustrate probability
plots, we again use the cloud seeding data. Figures 2.3.14 and 2.3.15 are
normal probability plots for the variables S - Ne and P, respectively.
These plots are included for illustration only, since the sampling plan
outlined in Chapter 1 would not lead us to expect the predictors to
behave as a normally distributed sample. However, the plot for S - Ne
is approximately linear, as one would obtain from a normal sample.
The value of W' = 0.972 is well above the 10% point of its distribution
given normality. The plot for P is clearly not straight, indicating
Normal quantile
Figure 2.3.14
Normal probability plot for S - Ne. cloud seeding data
Normol quontile
Figure 2.3.15 Normal probability plot for P, cloud seeding data
RESIDUALS AN11 INFLUENCE IN R E G R E S S I O N
Normal quantile
Figure 2.3.16 Normal probability plot with simulated envelope for ri, cloud
seeding data
positive skew by its shape. The value of W' = 0.757 is much less than
point of its distribution given normality.
Figure 2.3.16 is a normal plot of Studentized residuals for the model
( 1.1.3). With so many parameters and only 24 cases, we cannot expect
this plot to exhibit non-normal behavior; the simulated envelope
in the plot can be expected to be useful here. Since the observed
plot is generally within the envelope, we have no evidence against
normality.
Transformations
The situations in which a transformation of the data might prove
worthwhile can be conveniently arranged in three classes. In the first,
the responses yi are independent and come from a known non-normal
family of distributions. A transformation is selected so that the
distribution of the transformed responses is sufficiently close to normal
to allow application of the appropriate normal theory methods. The
arcsin and square root variance stabilizing transformations for the
DlAGNOSTlC METHODS USING RESIDUALS
binomial and Poisson distributions are typical examples. The import-
ant point here is that the selection of the transformation is based on the
known distribution of the response variables.
In the second class, the expected responses Ey, are related to the
explanatory variables x,, . . . , x, by a known nonlinear function of the
parameters. A transformation is selected to linearize the response
function. If the distribution of the errors is sufficiently well behaved, the
transformed data can be analyzed using standard linear least squares.
For example, if theory suggests the relationship Ey = Po exp (/.I,
then it is reasonable to expect an approximately linear relationship
between log (y) and (x), log (Y) = log (Po) +PI x. It will, of course. be
important to perform various diagnostic checks on the transformed
model since there is no guarantee that the standard least squares
methods will be appropriate. If, for example, the errors E in the original
model have mean zero, constant variance, and are such that
y = /loexp(/llx)(l +E) then the centered errors in the transformed
model will also have mean zero and constant variance, yi = [log (Po)
+ E log (I + c)] + /l, x + [log (1 + c) - E log (1 + E ) 1. On !he other
hand, if the errors in the original model are additive, p = /lo
exp (/?, x)
+c, then the error variances in the transformed model will depend
In the final class, neither the distribution of the errors nor the
functional form of the relationship between Ey and the explanatory
variables is known precisely. This situation is perhaps the most dimcult
to handle since a specific single rationale for choosing a transforn~ation
is lacking. Generally, we would like a transformation to result in a
model with constant error variance, approximately normal errors, and
an easily interpreted and scientifically meaningful structure. One
method of proceeding in this situation is to specify a family p"' of
transformations indexed by a possibly vector-valued parameter i. and
then use the data to select a specific transformation that may result in a
model that has all the desirable properties.
Methods ofselectinga transformation in situations falling in the first
or second class are well known and good discussions can be found in
many standard references. For example, Scheffk 
discusses a general method of choosing variance stabilizing transform-
RESIDUALS A N D I N F L U E N C E IN REGRESSION
ations. Daniel and Wood give plots of a variety of
nonlinear forms that can be transformed to linear forms. In this section,
we concentrate on the third class of situations. We first present a
number of families of transformations and sketch a method of analysis
based on likelihood considerations. Several related graphical and
approximate methods are discussed later.
For a positive response variable y > 0, Box and Cox studied a
slight generalization of the family of monotonic power trattsformations
used earlier by Tukey ,
This family contains the usual log, square root, and inverse transform-
ations as special cases and is scaled to be continuous at I = 0. y(4 is
convex in y for A 2 1 and concave in y for 1 I
1, and is increasing in
both y and A. It will be useful for inducingapproximate symmetry when
the response is skewed. One effect of the log transformation, for
example, is to lighten one tail of the distribution. Generally, (2.4.1) will
be sensible in situations where the origin occurs naturally and the
response is skewed and positive. Since most robust methods of
estimation are dependent on symmetry, (2.4.1) might be used prior to
the application of such methods.
If the origin is artificial or negative responses occur, added flexibility
is provided by the extended power family,
Here, y + i., > 0. In some situations, it may be sufficient to substitute a
convenient value for A, and then proceed using (2.4.1) in combination
with the shifted response y + A,.
John and Draper propose the family of modul~ts
rrtlri.~fi~rt?~ntiotls
DIAGNOSTIC METHODS USING RESIDUALS
for obtaining approximate normality from symmetric long-tailed
distributions. This family is monotonic, continuous at i.
applicable in the presence of negative responses. When the responses
are all positive the modulus family reduces to a special case of the
extended power family (2.4.2). Basically, (2.4.3) applies the same power
transformation to both tails of a distribution symmetric about zero. If
desirable, an arbitrary point of symmetry can be included by adding a
parameter A, as in (2.4.2). If A < 0, then y 
which contains the usual logit transformation (A = 0) as a special case.
If the responsesare concentrated near 0 orb, this family will behave like
the power family.
SELECTING A TRANSFORMATION
In their original paper, Box and Cox discuss both likelihood and
Bayesian methods for selecting a particular transformation from the
chosen family. Following this account, the development of the specific
methods for any of the transformations families discussed above is
straightforward. Here we consider only likelihood based methods.
It isassumed that for each A,y(")isa monotonic function ofgand that
for some unknown A the vector of transformed responses Y ''J
can be written as
where thequantitites on the right are consistent with previous notation
and, in addition, the elements of E are independent and (approximately)
normally distributed with mean zero and constant variance a'.
RESIDUALS A N D INFLUENCE I N REGRESSION
probability density of the untransformed observations is
where J is the Jacobian of the transformation
For fixed E., (2.4.6) is the standard normal likelihood and thus the log
likelihood maximized over /l
and a', apart from an unimportant
constant, is
(A) = - f n log [RSS (A; Y )/n] + log ( J )
where RSS denotes the residual sum of squares from a fit using the
transformed responses,
Equivalently, the maximized log likelihood can be written as
L,,, ( A ) = -
$ 1 1 log [RSS (A, Z)/n]
where the n-vector Z has elements
In this latter form, the correction for change of scale is apparent. If
more than one model is to be considered, the analyses are conveniently
studied using the normalized transformation zj", so the residual sum of
squares for each A are on the same scale and can thus be compared. The
normalized transformation should also provide better computational
accuracy, particularly for large A.
For an arbitrary collection of n positive scalars a,, a,, . . . , a,, let
g(a) denote the geometric mean function
g(a) = (fi a,)'"'
The normalized transformation for the extenkd power family is then
DIAGNOSTIC METHODS USING RESIDUALS
which gives the corresponding transformation for the power family by
setting 1, = 0. The normalized transformations for the modulus and
folded-power families are
s i g n ( ~ i ) ~ ( l ~ l +
l)log(~yil+
( h - ' g [ ~ ( b - ~ ) l l o g [ ~ i / ( b - ~ i ) l , j. = 0
respectively.
The maximum likelihood estimate of 13. can be obtained by maximiz-
ing (2.4.7) or (2.4.8), or by finding the solution to dLma,(i)/di. = 0.
Alternatively, when I is a scalar, Box and Cox suggest reading X from a
plot of Lma,(R) against 13. for a few selected values of 2. Unless special
software is available, such plots will require one regression for each
value of 1 chosen. The accuracy of the estimate of i.
obtained in this way
will usually be acceptable since in practice it is desirable to round 3. to a
convenient or theoretically justifiable value.
An approximate (1 -a) x 100':; confidence region for i. is given by
the set of all A* satisfying
where X2(a,
V ) is the (1 -a) x 100 percentile of a chi-squared distri-
bution with degrees of freedom ,$equal to the number ofcomponents in
A. When 13. is a scalar such confidence regions are easily construcled
from the plot of Lma,(E.) against i.
Invariance
Before turning to examples, a few general comments may remove some
of the concerns about this procedure that are likely to arise in practice.
We first comment on invariance under rescaling the responses and then
briefly discuss normality, the choice of a model, and methods of
inference.
From (2.4.11) it is easily seen that for the family of folded-power
transformations the estimate lwill be unchanged under rescaling of the
responses, yi S cyi, c > 0. Thus, without loss of generality. the
responses may be scaled so that b = 1. If X contains a column of Is, the
extended power family will be invariant under rescaling in the sense
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
that (;., ,;,)
$(A,, ~ 2 , ) . If X does not contain a column of Is, this
family is not invariant under rescaling when 1, # 0. Schlesselman
 discusses this problem and suggests modifications of the power
family that yield scale invariant estimates when regression is through
the origin. The transformations obtained from the modulus farnily are
not invariant under resealing, a characteristic that is likely to be
annoying in practice. However, Shih suggests a generalized two-
parameter modulus transformation that is scale invariant.
The Box-Cox procedure for choosing a transformation is based on the
assumption that Y '"is normally distributed. It isclear, however, that in
general this assumption cannot be true, although it may hold in certain
special cases (i. = 0 in the power family). Hernandez and Johnson
 investigate the consequences of this inconsistency for the power
farnily. Their results suggest that asympotically A and the least squares
estimates of /?and aZ based on the transformed data arechosen to make
the distribution of the transformed data as closeas possible to a normal
distribution, as measured by Kullback-Leibler divergence. They
emphasize that appropriate diagnostic checks should always be applied
to the transformed data since an adequate approximation to normality
is not guaranteed by this procedure.
Choice of model and scaling the predictors
The role of X in selecting a transformation for Y can be crucial since the
likelihood procedure tries to achieve EY'" = X/l in addition to
normality and constant variance. The indicated transformation for one
S-structure may not be the same as that for another and the selection of
S niay well be the rnost important step. Generally, X should be selected
so that the resulting model can be interpreted without great difficulty, is
flexible enough to describe important possibilities, and is scientifically
meaningful.
Box and Cox suggest the following technique as an aid to under-
standing the importance of selected columns of X in determining a
transformation. Partition PT = (/?I,/?:) where /I2 isq x 1 and for fixed I
let l.,,,,(i.1~, = 0) denote the maximized log likelihood for the model
with /3* = 0. Then
DIAGNOSTIC METHODS USING RESIDUALS
where F ( A ; Z) is the usual F-ratio for H: 8, = 0 in terms of the
normalized response zlA'. If both Lma,(l / 8, = 0) and Lma,(i.) are
plotted against A on the same graph then the difference between the
heights of the two curves at a selected value of E. is a monotonic function
of F ( I ; Z). Large differences indicate that inclusion of B, may yield an
improved fit. If the maxima of the likelihoods occur at substantially
different values of I then the transformation under B2 = 0 may be
attempting to compensate for inadequacies in the reduced model.
Once an appropriate transformation has been selected, the analyst
must choose between conditional and unconditional methods of
inference for the transformed data. In the conditional approach the fact
that the data are used to select a transformation is ignored and the
analysis proceeds as if the appropriate scale were known a priori. In
contrast, unconditional methods include 1 as an unknown parameter
and allow for the appropriate modification of confidence statements.
Historically, conditional methods of inference seem to dominate the
literature on transformations. Bickel and Doksum provide a
comprehensive account of the unconditional approach and demon-
strate that the unconditional variances of parameter estimates can be
much larger than those from the conditional approach. If. for example,
the power family is used to select a transformation of a simple random
sample and A = 0, then 
The second term on the right is the amount that the variance is inflated
due to estimation of A. Hinkley and Runger provide a number of
compelling arguments in favor of the conditional approach. They
comment that unconditional confidence statements must logically take
a rather useless form. For example, an unconditional confidence state-
ment based on the average of a transformed simple random sample
might read: 'On some unknown scale 1, which is probably around ;, a
95 "/, confidence interval for EY'~' is j investigate the variance inflation due to
estimating A when prediction of future observations is the primary goal
and the data are back-transformed so that the predictions are always
made in the original scale. They conclude that, while there is some
RESIIIUALS A N D I N F L U E N C E 1N R E G R E S S I O N
inflation in this problem, it is generally not severe or important. For
further discussion, see Box and Cox .
In this monograph we adopt the conditional approach.
E X A M P L E 2.4.1.
T R E E D A T A N O . 1.
Toprovideafirstillustration
of the use of the Box-Cox procedure, we use the power family in
combination with the tree data from the Minitab Student Hundbook
 . The data, given in Table 2.4.1,
consist of measurements on the volume Vol, height H, and diameter D
Table 2.4.1
7ree datu. Source: Ryan et al. 
D = Diameter
H = Height
Yo/ = Volun~e
DIAGNOSTIC METHODS USING RESIDUALS
at 4.5 ft above ground level for a sample of 3l'black cherry trees in the
Allegheny National Forest, Pennsylvania. The data were collected to
provide a basis for determiningan easy way ofestimating the volume of
a tree (and eventually the amount of timber in a specified area of the
forest) using its height and diameter. Since the volume of a cone or
cylinder is not a linear function of diameter, a transformation of I'ol is
likely to result in a fit superior to that provided by the untransformed
Generally, a straightforward method of proceeding is to consider the
simple additive model for the transformed response, here (I.'ol)(')
and ?I. For illustration, we consider also a second model (I'ol)'" on H
since it is not unreasonable to suppose that the area of a cross
section of the tree rather than its diameter was reported. As a common
reference for these two models, we include the third and final model
(Val)") on H, Dl and D2 which was investigated by Ryan et al. . We refer to these as Models 1, 2, and 3, respectively.
With 1 = 1, a preliminary inspection of the plots of the Studentized
residuals ri against the fitted values, H and D for each of the three
Figure 2.4.1 ri versus D for Model 1, tree data
Figure 2.4.2
ri versus H for Model 3, tree data
,/' Model 1,
= 0.307 /,@/
Model 2, A -0.662
Figure 2.4.3 L,,,(A)
versus 1, tree data. Horizontal lines correspond to
asymptotic 95 % confidence intervals
DIAGNOSTIC METHODS USING RESIDUALS
models confirms that transformation is likely to be worthwhile. For
Model 1 the plot of ri versus D given in Fig. 2.4.1 shows a clear
nonlinear trend, while for Model 3 the plot of ri versus H given in
Fig. 2.4.2 strongly suggests that the variability increases with H. Other
plots yield similar conclusions, although some are a bit ambiguous.
Plots of L,,,(I)
against 1 and the approximate 95 > confidence
intervals from (2.4.12) for each of the three models are given in
Fig. 2.4.3. The maximum likelihood estimate of i. indicates a different
transformation for each model. For Model 3, = -0.066 and the suggested transformation is
(Val)''' = log (Vol), while the suggested transformations for Models 1
and 2 are 2 = 113 and 213, respectively. Comparing Models 1 and 3 we
see that the transformation 1 = 113 is compatible with both likeli-
hoods. Also, if 1 = 113 is used to transform Vol in Model 3, then the
term in D' is unnecessary and if A = 0 is used to transform Model 3,
then doesco contribute to the fit. (From (2.4.13), the F-statistics for D~
are F(0; 2) = 11.9 and F (113; Z) = 0.03.) Based on this analysis. there
is little reason to prefer Model 3 over the simpler ( ~01)"'~'
on D and H.
It is reassuring that the variables in the latter model are dimensionally
compatible, a condition often overlooked in practice.
A comparison between Models 2 and 3 can be carried out in a
manner analogous to that given above. The essential difference is that
the transformation suggested by Model 2 does not seem compatible
with the likelihood for Model 3. In this comparison, Model 3 may be
preferable.
The residual mean squares in terms of Z'" for Model 1 with i = 113,
Model 2 with 1 = 213, and Model 3 with 1 = 0 are 4.84,5.62, and 4.68,
respectively. Based on this and the previous analysis, Model 1 with
1 = 113 is our preference fromamong thoseconsidered. Unfortunately.
this transformation does not seem to correct all of the deficiencies
noted earlier. The transformation successfully induces additivity, and
the scatterplot of ri versus H given in Fig. 2.4.4 indicates that the
variance structure has been improved,although case 31 now stands out.
This example is intended to illustrate the use of the Box-Cox
procedure and the kinds of results that can be expected. Certainly,
other reasonable models can be formulated. For example. the relation
between the volume, height, and diameter of a cylinder or cone,
Vol cc D Z H , suggests an additive model with all variables replaced by
their logarithms. Transformations of the explanatory variables will be
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
Figure 2.4.4 r i versus H. for Model 1 with I = 1/3, tree data
considered later in this section. For further discussion of transform-
ations in this data set, see Atkinson .0
DIAGNOSTIC METHODS
Andrews (197la) demonstrates that the likelihood method for choos-
ing a transformation is sensitive to outlying responses. Since the scale is
subject to question prior to the application of this methodology,
diagnostic procedures applied to the untransformed data may not yield
reliable conclusions. An outlier in the untransformed data, for example,
may be brought into line by a transformation. Carroll proposed
a robust method obtained by replacing the likelihood function by an
objective function that is less sensitive to outlying responses. Although
Carroll's method is superior to the likelihood method in terms of
robustness properties, it is still sensitive to outliers. Diagnostic support
for the likelihood method is clearly important.
DIAGNOSTIC METHODS USING RESIDUALS
In this section we discuss two additional methods for assessing the
need to transform the responses. These methods, due to Atkinson
 and Andrews , can be based on new explanatory
variables constructed from the original data and have graphical
counterparts that are useful for identifying anomalies. Because of this
diagnostic ability and ease of calculation, these methods should prove
valuable by themselves or as support for a likelihood analysis.
Atkinson's method
Atkinson's method is based on the score statistic rD(i.,) for the
hypothesis A = A,. The score statistic does not require iteration and can
be obtained using standard regression routines. To see how this is done.
let Z(Lo) = (zF))
where Z'" may correspond to any of the single parameter transform-
ation families discussed previously. Apart from an unimportant sign
change, the score statistic is equal to the usual t-statistic for the
hypothesis 4 = 0 in the model
Asymptotically, the null distribution of tD(Ro) is standard normal. but
its distribution in small samples is intractable since both Z("@l
are random variables with nonstandard distributions.
In this approach to the calculation of r , (A,), Gi'.'J is regarded as a new
explanatory variable which Box terms a constructed variable.
The corresponding model (2.4.14) can be viewed as an approximation
obtained by expanding Z'" in a Taylor series about i.,
 .
In this expansion, the coefficient of the constructed variable
4 = A, -?and thus the least squares estimate 4 of cf, provides a quick
estimate A of A,
x = / i , - $
RESIDUALS AND INFLUENCE IN REGRESSION
Estimates obtained in this way will often be good approximations to 2,
but Atkinson (1 982) demonstrates by example that adequate agreement
cannot be guaranteed in general. Nonetheless, in the absence of special
software 2 may prove useful.
Another adjunct to this method is the added variable plot for the
constructed variable G$' Ideally, this plot should show a consistent
and clear linear trend, indicating that the evidence for the transform-
ation is spread evenly throughout the data. Outliers in an added
variable plot may correspond to cases which are distorting the evidence
for a transformation and thus require special attention. Substantial
curvature may be an indication that a modification of the transform-
ation family would permit a closer representation. Suppose, for
example. that the folded power transformation family (2.4.4) yields a
plot with a strong and consistent linear trend. The added variable plot
for the power family (2.4.1) will likely show strong curvature since the
constructed variables for the power and folded-power families are not
linearly related.
Alldrews' rt~ethod
Like Atkinson's method, Andrews' method is based on a test of the
hypothesis I = A,. The test statistic is constructed by expanding YcA)
about E.,.
Since I"'.'= X ~ + E ,
This model is similar to the model (2.4.14) used in the construction of
the score statistic. However, (2.4.14) is based on the normalized
transformed responses z?) whereas (2.4.16) is based on yjAo). In effect,
Andrews' approach ignores the Jacobian of the transformation.
The statistic for Andrews' test is equal to the t-statistic for the
hypothesis i., - 1, = 0 in the model
where ~i!~"is equal to G!?e)evaluated at the fitted values from the null
model Y'"' = X ~ + E .
It follows immediately from the work of
DIAGNOSTIC METHODS USING RESIDUALS
Milliken and Graybill that the ?-statistic has a standard
t-distribution with n - p' - 1 degrees of freedom. That is, Andrews' test
As in Atkinson's method, the least squares estimate of i., - i.
(2.4.17) can be used to obtain a quick estimate of i.
and the added
variable plot for the constructed variable G~!o)
should be inspected for
unusual features.
Applicatiorr to the power family
It is informative to compare the constructed variables for the Andrews
and Atkinson procedures for the power family in combination with the
hypothesis of no transformation (1, = I). In this situation it is easily
verified that
G:" = [ji log (fi) - ji + 11,
where $i is the i-th fitted value from Y = X ~ + E ,
The associated test statistics depend on these constructed variables
only through the residuals (I - V)GU1 from the regression of G"' on
X. If X contains a constant column, (I - V)l = 0 and the constructed
variables simplify to
G? = [ji log ( f i ) I
= (h log Cyi/g (Y)] - J'i)
Since (I - V) 9 = 0, it is clear that the approximation of GI1' obtained
by substituting Gi for yi is equivalent to G:'. Thus, in this situation G;)
may be regarded as an approximate version of Gk').
Although Andrews'method yields an exact test, there is evidence that
this method has some loss of power relative to Atkinson's method
 . Andrews' method also has certain robustness pro-
perties that are not shared by Atkinson's method. With replication. for
example, all cases in a single cell will have the same fitted value and
consequently methods based on 9 will be less sensitive to a single
outlier than those based on Y .
TukeyS test
Tukey's well-known single degree of freedom for nonadditivity is
obtained using the constructed variable G = (.$)
obtained under the
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
hypothesis of no transformation (I, = 1). Andrews and
Atkinson discuss the relationship between Tukey's test and their
respective methods. A discussion of Tukey's test applied to two-way
tables is given in Section 2.5.
E X A M P L E 2.4.2.
T R E E DATA NO. 2. The score statistics t,(l) and
the corresponding quick estimates 1 for each of the three models used
for the tree data of Example 2.4.1 are given in Table 2.4.2. For
comparison, the likelihood estimates are also given. In each situation,
the need for a transformation is indicated by the score statistic and the
agreement between 2 and 1 seems adequate.
Table 2.4.2 Trat~sj'orn~atiot~
statistics, tree data
Case 31 deleted
Figure 2.4.5 contains added variable plots for the constructed
variables (2.4.19) in each model. In the plotscorresponding to Models 1
and 2, case 31 stands out as a possible outlier and thus may be havingan
undue effect on the analysis. In the plot for Model 3, case 31 is not as
noticeable. The effects of case 31 can be seen by removing it from the
data and recomputing 2, 2, and t D ( l ) . These values are also given in
Table 2.4.2. For each model the agreement between 1 and 2 is still quite
good and the score statistics indicate that transformations are still
needed. Without case 31, however, the suggested transformations can
change. For model 2, the suggested transformation is 1 = 213 for the
full data and 1 = 112 for the reduced data. Either transformation may
yield an adequate model since, as further analysis will show, they are
compatible with the likelihood based on the full and reduced data.0
E X A M P L E 2.4.3.
JET F I G H T E R S NO. 3. In Example 2.3.4, rescaling
of FFD to log (FFD), with F F D measured in months after January 1940
was done on logical grounds. We now consider the choice of scale for
F F D more systematically.
Figure 2.4.6 gives a scatter plot of ri versus fitted values for the
regression of FFD on the other variables in addition to a constant. This
- - - - - - X - -
. . . . . . . . . . . . . . . . . . . . .
Constructed variable
, - - - - - - - - - x - - x - - - - - - - - - - - - - - - - - - - - - -
0 Constructed
5 variable 10
(Legend overleaf)
RESIDUALS A N D INFLUENCE IN REGRESSION
figure is a paragon of ambiguity. It allows a variety of interpretations,
spread increasing to the right, a slight downward bow in the plot, an
outlier in the F-1 l lA, or no problem at all, depending on the skill and
the preconceptions of the investigator. Finding a pattern in a scatter of
points may be a difficult task, and often renders plots such as this one
nearly useless.
The solid curve in Figure 2.4.7 is a plot of L,,,
(A) versus A for the
family of power transformations. The likelihood estimate of A is
>. = -0.024 and the asymptotic 95% confidence interval excludes
= 1; the log transformation is clearly suggested. A scatterplot of ri
versus fitted values for the log transformed data is given in Fig. 2.4.8. As
before, this plot does not provide a clear indication of a deficiency in the
model, although the F-11 1A still stands out.
The score statistic for the hypothesis A, = 1 confirms the need to
transform, t,(l) = -3.88, although the quick estimate of A seems
unacceptably far from X , X = - 0.54. The added variable plot for C'," is
given in Fig. 2.4.9. In this plot, one case, the F-111 A, is well separated
- - - - - - - - - - - - - - - - - - - - . - - > k x - - - - -
Constructed variable
Figure 2.4.5 Added variable plots for the score statistic, tree data. (a) Model
1. (b) Model 2. (c) Model 3
l l l l ' l l l l ' l l l l ' l l l l ' l l l l ' l l l l ' l l l l l l l l l L
DlAGNOSTIC METHODS USING RESIDUALS
Fitted values
Figure 2.4.6 ri versus fitted values, jet fighter data, response = FFD
from the rest arid it appears that our conclusions may change if it were
deleted. Figure 2.4.10 is the constructed variable plot for GI" after
deletion of the F-111A. The linear feature of Fig. 2.4.9 is now
completely absent, and the need to transform is less clear. Without
the F-111 A, t,(l) = -0.05. Inferences based on the likelihood method
are also very sensitive to the presence of this case. As shown by the
dashed curve in Fig. 2.4.7, 2 is close to 1 when the F-11lA is deleted.
Although the evidence for the need to transform F F D is weak and
depends heavily on the F-111 A, the log transformation may be sensible
for two reasons. First, as previously stated, F F D is a stand-in for
technological level and L F F D is more palatable than FFD. Second,
while the F-111A does seem to be different, it is the most recent aircraft
in the data, and for that reason we may wish to modify a model to
provide a better fit to it than we would for a plane developed 20 years
RESIDUALS A N D INFLUENCE IN REGRESSION
_ _ C _ _ _ _ _ - - - - - - - - _ _
/.--.-----
F-111 A deleted,
Figure 2.4.7
L,,, (I) versus I, jet fighter data
Box and Tidwell suggest a general procedure to aid in the
selection of transfor~nations for the explanatory variables. A useful
version of their procedure begins with the assumption that the response
yi can be written as
where xe'denotes the transformation of the j-th explanatory variable
and the cis are (approximately) normal with zero mean and constant
variance 0'. Any of the single parameter transformation families
discussed previously in this section may be used for x!?'. (Extensions to
multiple parameter transformation families are immediate.) Of course,
we may also have x!?' = xij, i = 1, 2, . . . , n, for selected j.
As an alternative to the use of nonlinear methods, inferences about
the ijs can be based on an approximation to (2.4.20) obtained by
1)IAGNOSTIC METHOIIS U S I N G RESIDUALS
Fitted value
Figure 2.4.8
ri versus fitted values, jet fighter data, response = LFFD
expanding xfy) about the hypothesized values Aoj, j =. 1,2, . . . . p,
In this model, the transformation parameter Aj is related to the slope of
the added variable plot for the constructed variable gljo~'. A linear trend
in such a plot may be taken as an indication that
the absence of
a linear trend indicates that either Aj = A,, or 11, = 0. As before. these
plots can also be used to identify outlying cases that may be distorting
the evidence for a transformation.
The approximate model (2.4.21) is still nonlinear in the parameters,
but a quick estimate 2, of Aj is
- - - - - x x - x - - - - - - - - - - - - - - - - - - - - - - - - - -
Figure 2.4.9 Added variable plot for the score statistic, jet fighter data
Constructed variable
- - - - - - - - - - - - - - - X - - - - - - - - - - - - - - - - - - - - -
Constructed variable
Figure 2.4.10 Added variable plot for the score statistic, jet fighter data with
the F-1 I 1A deleted
DIAGNOSTIC METHODS USING RESIDUALS
where Jj is the least squares estimate of pj(Aj - LOj) from (2.4.21) and Bj
is the least squares estimate of /lj from the null model
yi = Po + C/?jxpJ'+&i
Further iteration using the Ijs as starting values may be used to find
estimates which further reduce the residual sum of squares for the
original model (2.4.20). The quick estimate in combination with the
added variable plot will usually suffice for diagnosing the need to
transform. For the method to be effective, however, Bj must have a
relatively small standard error. Substantial collinearity among the
columns of X, for example, can result in unreasonable results.
particularly for the quick estimates.
For illustration, consider the family of power transformations
(2.4:l). The constructed variable is
g l A ~ ) = lo [x" log (x)] - [xAo - 11
which is equivalent to
y ( l ~ l = xAo log (x)
since the projection of (g(A,)) onto the orthogonal complement of the
space spanned by the remaining columns (variables) of X is all that
matters. When the hypothesis is that of no transformation (Ao = 1) the
constructed variable is simply g'l' = x log (x) which can be easily
computed with nearly any regression program.
E X A M P L E 2.4.4.
TREE DATA NO. 3. We use the tree data de-
scribed in Example 2.4.1 to illustrate the use of the Box-Tidwell
procedure. Figure 2.4.11 gives the added variable plots for the
constructed variables D log (D) and H log (H). The estimated coef-
ficients with their estimated standard errors for the regressions of Vol
on (D, H) and on (D, H, D log (D), H log (H)) are given in Table 2.4.3.
The plot for D log (D) in Fig. 2.4.1 1 shows a clear linear trend and
thus indicates the need to transform diameter. From Table 2.4.3, the
corresponding quick estimate of the power is ID
= 1 + 7.20414.708
= 2.53. In contrast, the plot for H log(H) shows no linear trend and
thus a transformation of H is probably unnecessary. The suggested
model, Vol on D'.'
and H, is not far from the model Vol on D2 and H
that was used in Example 2.4.1. In fact, the latter model might be
preferred based on ease of interpretation. Recall from Example 2.4.1,
- - X - - - - % - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Constructed variable
- - - % - % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Constructed variable
Figure 2.4.1 1
Added variable plots for Box-Tidwell method, tree data. (a)
D log (D).
(b) H log (H)
DIAGNOSTIC METHODS USING RESIDUALS
Table 2.4.3
Regressiot~ s~rnltnariesjbr two n~odels, tree dirri~
however, that Vol on D~ and H can be refined further by transforming
Vol. However, for Vol on DZe5
and H, the score statistic r,(l) = - 1.44
suggests that a transformation of Yo1 may not provide much
improvement.
In some problems, iteration may provide substantially improved
estimates of the Ljs for those variables requiring a transformation. In
this example, the iterated estimate of 1, remains close to 2.5.
Finally, the Box-Tidwell procedure applied to the model (V01)l'~ on
D and H that was suggested in Example 2.4.1 may be used to argue that
transformations of D and H after Vol are not likely to result in much
additional improvement. Such sequential procedures should not be
confused with methods for the simultaneous estimation of transform-
ations for the response and explanatory variables. 0
E X A M P L E 2.4.5.
CLOUD S E E D I N G NO. 5. In
method of selecting a transformation for the responses, the scales of the
explanatory variables are held fixed. This may not be appropriate for
the cloud seeding data since the response Yand prewetness P are both
measures of amount of rainfall. It seems sensible that these variables
should be measured in the same scale.
To investigate the need for transforming Yand P simultaneously, we
use the power family in combination with the model,
'where X, is the 24 x 9 matrix of explanatory variables excluding
prewetness and action x prewetness, P'" is the 24-vector of trans-
formed prewetness values, A is a 24 x 24 diagonal matrix with i-th
diagonal element equal to 1 if the i-th day was seeded and 0 otherwise,
and E is N(0,a21). Following the discussion in Section 2.4.2. the log
likelihood maximized over p and a2 for fixed I is
(A) = - - log {Z'aT [I - V'"] Z'"/ll]
RESIDUALS A N D INFLUENCE IN REGRESSION
where V'" is the usual projection matrix for model (2.4.23). The
maximum likelihood estimate 1 of A can be read from a plot of L,,,(A)
versus i. for a few selected values of A. For each plotted point a new
value of P"' and thus a new value of V'" has to be computed.
Figure 2.4.12 gives a plot of L,,,
(A) versus A along with the
associated 95 "/, asymptotic confidence interval. The maximum likeli-
hood estimate is 2 = 0.401 and A = 1 is well outside the confidence
interval. Since Yand P are both measures of the volume of rainfall, the
cube root transformation seems a sensible choice.
Figure 2.4.1 2
L,,, (A) versus A, cloud seeding data
A diagnostic plot for this procedure can be obtained by using model
(2.4.14) in combination with the Box-Tidwell method. Model (2.4.14)
with i,, = 1 can be rewritten as
Z"'= X,/Y1 +P5P'A)++15AP'*)+(1
Next, expanding PtA) about A '= 1 as in (2.4.21) and rearranging terms
Z'" = XB + (1. - 1) [/I5 Gbl' + /I1 AG,"' - G'"
DIAGNOSTIC METHODS USING RESIDUALS
where X is the matrix containing all untransformed explanatory
variables and
This form suggests that an added variable plot for the constructed
where fi5 and fi, are the least squares estimates of p5 and p, ,
regression of Z") on X, may be a useful diagnostic.
Figure 2.4.13 gives the added variable plot for the constructed
variable G. There seems to be a strong linear trend in this plot, but cases
1 and 15 clearly stand apart and may be controlling our impression of
the plot in addition to the results of the likelihood analysis. The least
squares slope of the added variable plot is - 0.762 which is an estimate
of I - 1. Thus a quick estimate of 1, is 2 = 1 - 0.762 = 0.238 which is
not too far from the cube root transformation suggested previously.
Constructed variable
Figure 2.4.13 Added variable plot for (2.4.24), cloud seeding data
RESIDUALS AND INFLUENCE IN REGRESSION
When cases 1 and 15 are removed, the evidence for a transformation
disappears. The dashed curve in Fig. 2.4.12 is a plot of L,, (A) versus I
for the reduced data. The maximum likelihood estimate is now 1 = 1.24
and A = 1 is well within the 95 % asymptotic confidence interval. In the
absence of additional information, the results of any analysis of these
data, regardless of the transformation used, should be interpreted with
caution. In future examples using the cloud seeding data, we will use the
cube root transformation.
2.5 Residual analysis in two-way tables
The linear model for the unreplicated two-way table is an example of
the kind of model that could be studied using the diagnostic methods
discussed earlier in this chapter. However, since the appearance of
Tukey's one degree of freedom test of additivity, a body of
methods that take advantage of the special structure of two-way tables
has developed. These methods, which can often be generalized to
higher-dimensional layouts, merit special study as examples of the ways
in which additional information can be used in diagnostic methods. We
survey some of these methods here.
Let yij denote the response in row i and column j of an r x c table, and
let pij = Eyij, p = ji.., ui = pi. - ji.. and Pj = jiaj - ji... Then the usual
additive model can be written as
where the errors elj are uncorrelated and have mean zero and constant
variance a'. Methods for detecting outliers relative to this model have
been investigated by Gentleman and Wilk , Daniel ,
Draper and John , Gentleman , and Galpin and Hawkins
 among others. Barnett and Lewis give an informative
discussion. Generally, the residuals eij = (yij -
J a j + Ye.) from a fit
of the additive model are reliable indicators of a single outlying cell. If,
for example, a single outlying value of magnitude 8 occurs in cell (1,l)
Eell = 8(r - 1) (c - l)/rc,
E l - 8 - ) / c
Eeil=-8(c-l)/rc,
DIAGNOSTIC METHODS USING RESIDUALS
Eeij = Blrc,
Thus, the residual corresponding to the outlying cell will have the
largest absolute expectation provided r 2 3, c 2 3. Since the residual
variances are constant for this model, there is no essential distinction to
be made between the ordinary residuals and the Studentized residuals.
A formal, normal theory test for a single outlier can be based on the
maximum normed residual
This statistic is equivalent to maxi(til obtained from the mean shift
outlier model described in Section 2.2.2. The 1 %, 5 % and 10 % points
of M N R for r = 3(1)10, c = 3(1)10, from Galpin and Hawkins ,
are reproduced in Table 2.5.1.
When two or more outlying values are present the residuals will often
lack noticeable peculiarities since the effects of multiple outliers can
filter through the entire table of residuals in complicated ways
 . Gentleman dis-
cusses methods for finding the k most likely outliers; that is, the k
observations whose removal provides the greatest reduction in the
residual sum of squares. This is equivalent to finding the k observations
that maximize the multiple case Studentized residual.
When nonadditivity is suspected, a useful initial representation of the
response is
where ,u, g, fij, and cij are as previously defined and yij = ,uij - pi. - p.j
$. p., . Of course, the usual additive model is obtained if yij = 0 for all i
and j. An equivalent condition for additivity is that all two-by-two
contrasts of the form pij - pilj - pij, f piIr be equal to zero. Johnson
and Graybill exploit this fact to develop a method for
estimating a2 in the presence of partial nonadditivity.
A variety of models and tests for nonadditivity can be obtained by
imposing additional structure on the interaction terms yij. The model
associated with Tukey's test is perhaps the best known and is
Table 2.5.1 Critical values for M N R = maximum normed residual in two-way tables. Starred values are exact. Source: Galpin and
Hawkins , reprinted with permission
RESIDUALS A N D INFLUENCE IN REGRESSION
obtained by setting
Y i j = 4ailJj
for all i and j. This model may be viewed as a way of modeling a linear-
by-linear interaction in latent variables associated with rows and,
columns and is sometimes called Tukey's concurrence model. Tukey's
I df test of additivity is obtained using a standard procedure for
converting a nonlinear model to a linear model : Replace ai and /Ij in y i j with their least squares estimates 12, and ISj
from the additive model (2.5.1) and then construct the usual F-statistic
F , for the hypothesis 4 = 0. Under the null hypothesis and normality,
this statistic has an F-distribution with 1 and (r - 1) (c - 1) - 1 df.
Graphical aids useful in interpreting this test can be constructed using
the methodology of'Section 2.3. The power of Tukey's test and its
robustness in non-normal situations have been investigated by Ghosh
and Sharma and Yates .
Mandel suggested two alternative structures for the inter-
action terms:
y i j = aibj
for all i and j. The models associated with (2.5.6) and (2.5.7) are called
the row and column regression models, respectively. These models can
also be motivated by appealing to the noti n of latent variables
associated with rows (2.5.7)gr columns (2.5.6) and, relative to (2.5.5),
are more flexible approaches to nonadditivity. In the row model, a test
of additivity is obtained by replacing Pj in (2.5.6) with its least squares
estimate from (2.5.1) and then constructing the usual F-statistic F,,,
for the hypothesis a , = a, = . . . = a, = 0. Under the null hypothesis
and normality, this statistic has an F-distribution with r- 1 and
(r - 1 ) (c - 2) df. The analogous test for the column model is con-
structed in the same way.
It is important to remember that these tests for nonadditivity are
obtained by approximating a nonlinear model with a linear model that
can be handled using standard techniques. Under the alternative
hypotheses. the statistics F, and F,,,
do not have noncentral
I;-distributions, as would usually be the case in standard applications.
In the presence of nonadditivity, the residual mean square resulting
DIAGNOSTIC METHODS USING RESIDUALS
from a fit of the linearized version of the model is positively biased. This
bias can be severe if 4 or XciZ is large. Hence, these methods should not
be regarded as being more than relatively straightforward ways of
detecting nonadditivity. If nonadditivity is found. it may be wise to
abandon the linearized version of the model in favor of more
appropriate methods of analysis. For example, the data might be
transformed to restore additivity.
In theapproaches ofTukey and Mandel, it is necessary to assume the
presence of main effects (ai # 0. /Ij $I 0) for the interactions to be
present. This and the specific structures assumed for yij place an often
unwarranted limitation on the types of nonadditivity that will be
detected by these techniques. Johnson and Graybill proposed
7.. = &.u.
where Ciwi = C j u j = 0 and C w f = C uf = 1 , as a more general
structure for detecting nonadditivity. Essentially, this assumes that the
interaction yij is a quadratic function of latent variables that need not
be related to the main effects. This form would be appropriate if the
interactions do not occur systematically across the entire table. but do
occur systematically in a subset of the full table or in only an isolated
cell. For example, a single outlier in cell ( 1 , 1 ) corresponds to
W T = ( w i ) = k , ( r - 1 , - I , - 1 , . . . , - 1 ) and U T = ( u i ) = k , ( c - 1 ,
- 1, - 1, . . . , - 1) (see Equation (2.5.2)), where k , and k , are con-
stants chosen to insure that W T W = U T U = 1 . As a second illustra-
tion, consider a nonadditive table in which the subtables formed by the
r - 1 rowsand the last r -srowsareadditive. the nonadditivity
being due solely to the difference between the subtables. Let j
for i = 1 , 2 , . . . , s , j = 1 , 2 , . . . ,cand p z i j = pij for i = s + I . . . . , r
and j = 1,2, . . . , c. Then rvi = k,(r - s) for i = 1, 2. . . . . s and wi =
for i = s + l , . . . , r, and U T = k , ( p l , j - ~ i l , , - f i z , j + f i z , , ) .
Generally, (2.5.8) can model any alternative situation in which the
matrix with elements Eeij is of rank 1.
Since the interaction terms are not functions of the main elTects, the
method used previously for constructing an easy test of additivity (6
= 0) does not apply. Instead, Johnson and Graybill derive the
maximum likelihood estimators under the assumption of normality.
Let E = ( e i j ) be the r x c matrix of residuals from the additive model.
Then the maximum likelihood estimators of the parameters in the
RESIDUALS A N D INFLUENCE IN REGRESSION
Johnson-Graybill model are
Si = yi.-y.,
p . = .F.,-j,.
b Z = largest eigenvalue of ETE
\^V = normalized eigenvector of EET associated with give the upper 1 "/,, 5 % , and 10% points of the null
distribution of A. These are reproduced in Table 2.5.2.
In addition to providing a reasonably flexible test, the
Johnson-Graybill approach provides a useful method for diagnosing
more specific forms of nonadditivity. Plots of Gi
versus ii or lij versus
P j may suggest that the column or row regression models respectively,
or perhaps Tukey's model, is appropriate. If w and fi each contain a
single relatively large value then this may be taken as an indication ofan
outlier in the cell corresponding to the coordinates of the large values.
The signs of the elements of w serve to identify a decomposition by
rows of the full table into two subtables that may be more nearly
additive. If the elements of the same sign are of the same order of
magnitude then this might be taken as an indication that the subtables
are additive. At the very least, such subtables require further inspection
when nonadditivity is present. Similar comments apply to 0.
For further discussion of the Johnson-Graybill approach and
extentions to situations in which the rank of the matrix of the expected
residuals is greater than 1, see Hegemann and Johnson and
Mandel ( 197 1). Bradu and Gabriel present a graphical technique
as an aid to determining an appropriate model. Hegemann and
Johnson compare the power of Tukey's test to that based on
the Johnson-Graybill model. Their general conclusion is that if the row
and column effects are large and the structure yij = q5aiPj is appropri-
DIAGNOSTIC METtlODS USING RESIDUALS
ate then Tukey's test will have the greater power. Otherwise, the
Johnson-Graybill test is preferred.
E X A M P L E 2.5.1
A G R I C U L T U R A L E X P E R I M E N T . We illustrate the
methods suggested in this section using data on a two-way classifi-
cation design from Carter, Collier, and Davis . This data set was
used by Johnson and Graybill to illustrate the use of their test.
The data, which are part of a larger experiment to determine the
effectiveness of blast furnace slags as agricultural liming materials on
three soil types, are presented in Table 2.5.3(a). The response is yield of
corn in bushels per acre.
As a base, we first consider the fit obtained from the additive
model (2.5.1). The ANOVA table and the normed residuals.
eij/(Ci Cje$)li2, are presented in Tables 2.5.3(c) and 2.5.3(b). respect-
ively. The results in Table 2.5.3(c) suggest that the average soil effects
are significant while the average treatment effects are not. The usual
estimate of a2 from the additive model is 79.0. The pattern of the signs
of the normed residuals in Table 2.5.3(b) might be taken as an
indication that the additive nod el is not appropriate. The MNR occurs
in cell (5.3) and has the value 0.603 which has a p-value less than 0.05.
Evidently, there is reason to suspect that the observat~nn in cell (5.3)
does not conform to the assumed model; that is, either the model or thc
observation is wrong.
We next fit the Johnson-Graybill model. The maximum likelihood
estimates of the interaction parameters are
J2 = 943.02
wT = (-0.476, -0.337, 0.086, 0.040, 0.767, - 0.212. 0.131)
fjT = ( - 0.206, - 0.581, 0.787)
The maximum likelihood estimate of n2 is 0.21. This is a biased estimate
of a2. Johnson and Graybill proposed an alternative estimator 8' of a2
that will be unbiased when additivity holds,
where the expectation is taken under the hypothesis 5 = 0. Tables of
~($~/a~)areavailable
from Mandel . If6 # 0 then theestimate is
no longer unbiased. For the problem at hand E ( ~ ~ / u ' )
= 8.94 and
Table 2.5.2
Upper pcrcentuye poinrsjbr null disrriburion of'A (Equation (2.5.9) ). Sturr~.d vulur.s are exucf. Source: Johnson und
Grujhill . reprinted with permission
RESIDUALS A N D I N F L U E N C E IN REGRESSION
Table 2.5.3 Agricultural data. Source: Carter et al. . (a) Data.
(b) Normed residuals. (c) Analysis of variance
(h) Normed residual.~
-- - - -- - -
7 rrarment
(c) Analysis of variance
Treatments
6' = 1.43. Both estimates of a' are considerably smaller than that
obtained from the additive model. The likelihood ratio test statistic for
6 = 0 has the value A = 0.9954, with the corresponding p-value less
than 0.01.
In an effort to understand the precise nature of the nonadditivity, we
turn to an inspection of w and U. First, plots of Gi versus 2i and Gj
versus fij do not display a clear linear trend and thus neither Tukey's
model nor the two versions of Mandel'sapproach is likely to providean
adequate explanation. The corresponding F-tests confirm this obser-
vation. as the three F-statistics are all less than 1. Second, an inspection
of VV reveals that treatments a, b, and f seem to form an additive
subtable; the elements of w corresponding to these treatments are all
negative and of the same order of magnitude. However, the remaining
positive elements are not of the same order of magnitude; the element
corresponding to treatment e is 19 times as large as the element
corresponding to treatment d. The interpretation is that, while the
subset formed by treatments c, d, e, and g may be more nearly additive
than [he full table, it does not seem to form an additive subset.
DIAGNOSTIC METHODS USING RESIDUALS
Similarly, inspection of u suggests that the first two columns form an
additive subtable. Inspecting both vectors simultaneously with a view
towards detecting a single outlier isolates treatment e in the third
column as a possible outlying cell. This again implicates cell (5.3).
At this point a number of options for further analysis are available.
One might, for example, replace the suspected outlier with a pseudo-
value and reanalyze the data. However, this as well as many other
techniques requires the specification of a model and at this point an
appropriate model is unknown. A more useful procedure is to delete
treatment e entirely and reanalyze the data for additivity.
With treatment e deleted, the estimate of u2 based on the additive
model is 30.0 and the F-tests corresponding to the interactive
components in the Tukey and Mandel models are again nonsignificant.
Fitting the Johnson-Graybill model to the reduced data set yields the
following estimates
Both estimates of 0' have increased, and A still has a p-value of less than
0.01 indicating that some nonadditivity remains. Inspection of w
reveals that treatments a, b, and f again form an additive subtable and
that treatments c, d, and g form a subtable that is more nearly additive
than when treatment e was included. The interpretation of u is the
same as previously given. In short, it appears that the nonadditivity
present in the reduced data set is due to the difference between the sets
of treatment {a, b, f j and {c, d, g ) in the third column. In retrospect.
much the same conclusions might have been reached from an
inspection of a plot of the data such as that given in Fig. 2.5.1. The
response lines for treatments a, b, and fare nearly parallel as are those
for (c, d, g}, while the response line for treatment e is anomalous. Of
course, hindsight is usually more accurate than foresight and such
visual inspections become difficult in larger tables.
A separate analysis of each set of treatments suggests that the data
within a set are additive: for both treatment sets the tests of 6 = 0 have
corresponding p-values greater than 0.05. Further, under the additive
model, the estimates of u2 from the treatment sets {a, b, f} and {c, d, g}
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
Figure 2.5.1 Agricultural data
are 9.23 and 1.01, respectively. These estimates are, of course, much
smaller than the original estimate from the additive model. The ratio of
the estimates, each being based on 4 df is 9.14 and this is approximately
the 2.5 % point of the appropriate F-distribution. This suggests that
either nonadditivity is still present in treatment set {a, b, f) or the
variances of the treatment sets are different.
To this point, the analysis suggests that the nonadditivity in the data
is due primarily to the differences between the treatment subtables
{a, b, f}, {c, d, g}, and {e). Depending on interest, the analysis of the
treatment effects could be carried on in a variety of ways.
In addition to this analysis of the Johnson-Graybill model, a
transformation to induce additivity could prove useful. Indeed, the
entire Johnson-Graybill approach might have been overlooked in
favor of the transformation methods of Section 2.4. As illustrated
below, however, this may often be unwise since not all nonadditivity
can be removed by a transformation.
A plot of L,,,(A)
for the power family is given as Fig. 2.5.2. The
maximum likelihood estimate is 1 = 0.497 and ,I = 1 (no transform-
DIAGNOSTIC METHODS USING RESIDUALS
Figure 2.5.2
L,,,(l) versus 1, agricultural data
ation) is near one end of the asymptotic 95 % confidence interval. There
is only mild evidence of the need to transform. The score statistic is
t,(l) = 1.64 and the corresponding quick estimate is
= 0.087 which
suggests the log transformation. (In this example. the agreement
between ): and
does not seem adequate.) Based on the transformed
data with 1 = 0.497, MNR = 0.582 for cell (5, 3) with a p-value near
0.05, and A = 0.9828 which has a p-value of less than 0.01.
While the transformed data are still nonadditive, it is possible that
the results of the likelihood analysis are being distorted by the outlying
cell. This suspicion is reinforced by the added variable plots for the
score statistic given in Fig. 2.5.3. Another application of the likelihood
method, this time without treatment e, gives i^ = 0.397 which is
consistent with the cube root transformation. For cube root trans-
formed data without treatment e, A = 0.9680. Again. substantial
nonadditivity remains.
The exploratory approach used in the previous example can, if
necessary, be formalized. Marasinghe and Johnson provide
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
Constructed variable
Figure 2.5.3 Added variable plot for the score statistic, agricultural data
likelihood ratio test statistics and associated critical values for the
hypothesesHW =O,GU = 0,andHW = GU = OwhereHandGare
full rank matrices of contrasts. Besides providing formal tests, this
material can help avoid the problems of overinterpretation that are
inherent in any exploratory analysis.
Assessment of influence
'To arrive inductively at laws of this kind, where one quantity depends on or
varies with another, all that is required is a series ofcareful and exact measures
in every different state of the dat~rm and quaesirum. Here, however, the
mathematical form of the law being of the highest importance, the greatest
attention must be given to the extreme cases as well as to all those points where
the one quantity changes rapidly with a small change of the other.'
HERSCHEL. Op. ~ j l .
The diagnostic methods presented in the last chapter are useful for
finding general inadequacies in a model. A related question that cannot
be easily addressed by those methods is that of stability, or the study of
thevariation in the results ofan analysis when the problem formulation
(see Fig. 1.2.1) is modified. Ifa case is deleted, for example, results based
on the reduced data set can be quite different from those based on the
complete data, as was suggested by many of the examples in Chapter 2.
We call the study of the dependence of conclusions and inferences on
various aspects of a problem formulation the study of influence.
The basic idea in influence analysis is quite simple. We introduce
small perturbations in the problem formulation, and then monitor how
the perturbations change the outcome of the analysis. The important
questions in designing methods for influence analysis are the choices of
the perturbation scheme, the particular aspect of an analysis to
monitor, and the method of measurement. The possible answers for
these separate questions can lead to a variety of different diagnostics.
For example, diagnostics resulting from perturbation schemes applied
to the data case by case can be quite different from those resulting from
perturbation schemes applied to assumptions such as normality of
In this chapter, we consider only one perturbation scheme in which
the data are modified by deletion of cases, either one at a time or in
groups. Case deletion diagnostics have found the greatest acceptance.
and have been applied in many problems besides linear least squares
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
regression. We will also limit our study to aspects of the analysis that
can be summarized by the sample influence curve, to be described here
at some length. Other approaches to the study of influence are
described in later chapters.
Motivation
Not all cases in a set of data play an equal role in determiningestimates,
tests, and other statistics. For linear least squares, the results of the last
chapter suggest that cases with vii near I or with large Studentized
residuals will play a larger role. In some problems, the character of the
regression may be determined by only a few cases while most of the
data is essentially ignored. An extreme example of this is given in
Fig. 3.1.1 for simple regression. If the one point separated from the
others is moved, downweighted, or completely removed from the data,
the resulting analysis may change substantially, as illustrated by the
two regression lines computed with and without the separated point.
While the change in the line can be anticipated from inspection of the
Figure 3.1.1 A simple regression scatter plot. -
regression of y on x, all
data. R 2 = 0.90. - - - - regression with the separated case removed, R2 < 0.01
ASSESSMENT OF I N F L U E N C E
scatterplot, the change in other summaries such as R' can be startling.
For the complete data in Fig. 3.1.1, R Z = 0.90, while if the one
separated case is removed R Z is less than 0.01 .
Table 3.1.1 contains a further example of a somewhat different
character. In fitting the model E y = 8, +/l,x, + /j,sz,case 4 may be
considered an outlier because of its large Studentized residual, but it
will have only modest influence on the estimates of the ps. Deletion of
case 6, with v,, = 1, will result in a rank deficient model. so this case has
a large influence. This example is deceptively simple, but the same
conditions can occur in much larger problems if the role of case 6 is
taken over by a small set of cases, and if the structure of x, and .u, is
made less obvious by a nonsingular linear transformation.
Table 3.1.1
A hypotheticc~l e.uartiple
The ability to find influential cases can benefit the analyst in at least
two ways. First, the study of influence yields information concerning
reliability of conclusions and their dependence on the assumed model.
For example, the usefulness of the complete data regression in Fig. 3.1.1
is highly dependent on the validity of the separated case. Alternatively.
if deletion of an influential case from a data set changes the sign of an
estimated parameter, relevant inference concerning that parameter
may be in doubt. Second, we shall see that cases in the p-dimensional
observation space that are far removed from other cases will tend to
have, on the average, a relatively large influence on the analysis. This, in
turn, may indicate areas in the observation space with inadequate
coverage for reliable estimation and prediction.
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
The techniques developed here are not intended to provide rules for
the rejection of data, as influential cases are not necessarily undesirable.
Often. in fact, they can provide more important information than most
other cases.
The emphasis in this chapter is on detecting influential cases rather
than on how to deal with them once they are found, since final
judgments must necessarily depend on context, making global recom-
mendations impossible. Some of the possible actions can be given,
however. If the influential cases correspond to gross measurement
errors. recording or keypunching errors, or inappropriate experimental
conditions, then they should be deleted or, if possible, corrected. If an
influential case cannot be traced to conditions that justify its removal
and the model is known to be approximately correct, a formal outlier
test might be useful, although such tests cannot be expected to be
powerful. Collecting more data or reporting the results of separate
analyses with and without the cases in question are two additional
possibilities that are often appropriate. Finally, in situations where
predictions are important it may be possible to circumvent partially the
effects of influential cases by isolating stable regions, or regions where
the influence is minimal and unimportant.
In the next three sections of this chapter we review some of the results
concerning the influence curve. Sample versions of the influence curve
provide justification for the basic tools used for finding influential
3.2 The influence curve
Let T, be a vector-valued statistic of length k based on an independent
and identically distributed sample z, , z,, . . . , z, from the cdf F defined
on Rm. Of interest is the assessment of the change in T,, when some
specific aspect of the problem is slightly changed. A first step in such an
assessment is to find a statistical functional Tthat maps (a subset of) the
set of all cdf's onto Rk such that, if E is the empirical cdf based on
z,. z,, . . . . z,, then 7'(F) = T,. If such a functional exists, then we can
study the properties of 7, by examining the behavior of T(F) or T(P)
when F or E is perturbed.
As a simple example, consider In = k = 1 and T,, = n- ' Czi = L The
corresponding statistical functional is
ASSESSMENT O F I N F L U E N C E
so clearly T ( F ) = SzdG = f. This estimator would be called robust
if 'small' changes in F or
do not produce wild fluctuations in I . and. in particular.
various finite sample versions derived from it. For the most part, our
approach to the influence curve is heuristic; for rigorous treatments.
interested readers are urged to consult recent books by Huber 
and Serfling (198 1).
Let 6, denote the cdf giving mass 1 to z in Rm. The kector-valued
influence curve ICI,F(~)
of 7' at F is defined pointwise by
T[(1 - E)F + ~ 8 , ] - 7'(F)
IC,.,(z) = lim
provided the limit exists for all z in Rm. Thus, the influence curve is just
the ordinary right-hand derivative, evaluated at E = 0. of ]'[ and expanded upon by Reeds is in
determining asymptotic properties of an estimator. Hampel and
Andrews et al. use influence curves to compare estimators and to
suggest robust modifications of existing estimation techniques. For
example, M-estimators are modified versions of maximum likelihood
estimates that have desirable properties for the corresponding
influence curves. The main use in this work is anticipated by Devlin.
Gnanadesikan and Kettenring (1 975). Pregibon (1 979. 198 I). Cook
and Weisberg ,and Hinkley : The influencecurve is used to
monitor the influence of individual cases on estimates.
The following introductory example illustrates the use of the
influence curve and suggests specific procedures for special purposes.
E X A M P L E 3.2.1.
T H E S A M P L E AVERAGE.
The influence curve for
/L = 7'(F) = jzdF(with k = rn = 1) can be computed directly from
(3.2.11 to be
(1 - ~ ) j i + ~ z - j l
IC (2) = lim -
( 3 . 2 2 )
RESIDUALS A N D I N F L U E N C E I N REGRESSION
An undesirable property of the sample average is that its influence
curve is unbounded; that is, small changes in F can produce large
changes in the estimator.
The influence curve can also be used in a number of ways to see how
individual cases affect the sample average. Suppose that a single
additional case z were added, giving a sample of size n + 1 and'the new
sample cdf F, = nF/(n + 1) + ( l / ( n + I))&. It easily follows that
where 2, = T(F, ). For a fixed sample size n, 2, - Z increases linearly
as z deviates from i; This gives the influence of a single future case on
the current sample average Z and only indirectly reflects the influence of
zi. i = 1, 2, . . . , n on 2. Equation (3.2.3) is related to the sensitivity
curves suggested by Tukey .
The influence of the i-th case zi on Z may be determined by removing
zi from the sample and proceeding as before,
where f(i,
denotes the sample average computed without the i-th case.
This describes a collection of n influence curves obtained by deleting
each case in turn. The influence of zi on 5 is obtained by evaluating the
i-th curve at z = zi. This results in the n case statistics
which can be expressed more informatively by writing (zi - .T(i,) in terms
of the full sample average,
z - zci, = (zi - q / ( n - 1)
Thus, the influence of a single case depends on the sample size and the
full sample residuals. Any case with a sufficiently large residual will be
influential for the sample average.
3.3 The influence curve in the linear model
The first step in finding the influence curve for the least squares
estimator of /3 in model (2.1.1) is to construct the appropriate
ASSESSMENT O F INFLUENCE
functional T. Following Hinkley , let the (p' + I)-vector (xT, 1,)
have a joint cdf F with
By allowing x to havedesign measure (3.3.1) will also describe problems
with X fixed. The functional corresponding to the least squares
estimator of fi is
7'(F) = C- ' (F)y ( F )
assuming, ofcourse, that Cis nonsingular. Next, let 6 , = b,,.,., be thecdf
that places mass 1 at (xT, y). The p'-dimensional influence curve as a
function of (xT, y) is defined pointwise by (3.2.1). An explicit formula is
obtained by writing
C((1 -E)F+EB,) = ( 1 - E ) ( C ( F ) + - x x T )
-E)F + ~ 6 , ) = ( 1 - & ) y ( F ) + ~ y x
From (3.3.3) updating C- ' (F) to C-'
( ( 1 - E)F + ~ 6 , )
is equivalent to
adding a new case at x with weight ~ / ( 1
-6). Using Appendix A.2,
Substituting for T ( ( 1 - E)F +&6,) in the definition of the influence
curve, simplifying, and taking the limit gives
If interest centers on a set of q independent linear combinations
of the elements of fi, then it is more appropriate to consider the
influence curve for these combinations. Let I) = ZB, where Z is a
q x p' rank q matrix. It is easily shown that the influence curve for
JI = Z / l = Z T ( F ) is
As with the influence curve for the sample average, the influence
curve for linear least squares regression is unbounded in each
R E S I D U A L S A N D l N F L U E N C E I N R E G R E S S I O N
component as - xTT(F) becomes large. This observation has led to
the development of robust regression methods that generally bound
influence by downweighting cases with large residuals. In addition,
however. the componentwise influence can grow large, even if
- xT7'(F) is small, if x is far from E F ( x ) and substantially in a direction
of an eigenvector corresponding to a small eigenvalue of C(F).'Robust
regression methods may also be highly influenced by such cases, as
discussed in Chapter 5.
For the influence curve to provide a useful diagnostic procedure in
regression, (3.3.6) must be modified by replacing (x, y) by (xi, yi),
i = 1, 2. . . . , 11, and by replacing parameters by statistics. Although
(3.3.6) is a useful theoretical diagnostic, as Hampel , Mallows
 . and others have pointed out, it describes an estimation
technique with respect to a theoretical sampling population F. In any
finite sample situation, more information relevant to the specific
problem can be obtained by removing dependence upon F and using an
asymptotically equivalent finite sample version, like those in
Example 3.2.1, that corresponds directly to the observed data.
3.4 Sample versions of the influence curve
Several finite sample versions of the influence curve that depend on an
observed sample have been suggested. Two of these, which shall be
called the empirical influence curve (EIC) and the sample influence
curve (SIC), have received the greatest attention, and will be discussed
most completely here; both are discussed by Mallows . They will
be presented as a continuation of the previous section on least squares
estimation of p, but the ideas are general and the application to other
situations should be clear.
In general, the EIC is obtained by substituting the sample cdf
in the influence curve. For linear models, using (3.3.5) and setting
B = T ( P ) gives
EIC(x, y) = n(XTX)- ' x ( y - xTb)
EICi = EIC(xi, y,) = , ~ ( X ~ X ) - l x ~ e ~
where, as usual, ei = y, - xf 8. The EIC is appealing on several grounds,
as it appears to be an exact analogy to the influence curve. It measures
ASSESSMENT O F INFLUENCE
the effects of an infinitesimal perturbation off at xi, and corresponds
to the infinitesimal jackknife method of Jaeckel . The EIC pretends that an infinitely large sample has been used to
obtain P, and it measures the instantaneous rate of change in the
estimator as a single case at x is added to the data.
A second sample version of the influence curve can be constructed to
display the influence of the i-th case on the computed estimate of /I.
general idea is to substitute the sample cdf with the i-th case deleted for
F in the influence curve and then evaluate the resulting EIC at the i-th
case. This is analogous to the treatment in Example 3.2.1.
Let E,,, denote the sample cdf with the i-th case deleted. For least
squares estimators of 8, substitution of F,i, for F in (3.3.5) yields an
empirical influence curve with the i-th case deleted,
EIC(i)(x, Y ) = (n - l ) ( X ; ) x ( i ) ) - ' X ( Y - xT&il )
= T ( E , ~ , )
X$,X,,,/(n - 1 ) = ~ x x ~ ~ F , ~ ,
This represents n EICs, one for each i = 1,2. . . . , n. The influence of
the i-th case is determined by evaluating (3.4.3) at (xi, yi),
Using the relationships in Appendix A.2, this can be more informatively
expressed in terms of the full sample,
The interpretation of EIC,,, is analogous to that for EIC. It should be
remembered, however, that EIC,,, is the result of the evaluation of n
separate influence curves.
Both the EIC and EIC(,, are constructed under the fiction that
infinitely large samples have been used to obtain
i = 1,2, . . . , n. The sample size n in (3.4.2) and (3.4.5) appears as a
result of the covariance structure and does not necessarily reflect the
effects of a finite sample. When investigating the influence of individual
cases on computed statistics, a more explicit dependence on 11 is
desirable, or else important finite sample characteristics can be
obscured. A more desirable sample version of the influence curve can be
obtained by setting F = and taking E = - l / ( n - 1 ) in the definition
R E S I D U A L S AN11 I N F L U E N C E 1N REGRESSION
of the influence curve (3.2.1). Evaluating at z: = (x?, y,)and E = - l/(n
- I), we find (1 - E)C + 126, = 17'(,,. The sample influence curve is then
SIC, = - (n - 1) (T(F(~)) - ~ ( p ) )
= (n- 1) (B- Dl,))
- (n - 1) (XTX)- 'xiei
which is proportional to the change in the estimate of/? when a case is
The essential difference between these three sample versions of the
influence curve appears in the power of the (1 -uii) term in the
denominators, while the numerators are essentially the same when
evaluated at the sample points. Recall that remote rows of X will tend to
have 1 - vii small. The EICli) will be most sensitive to cases with v,, large,
while ElC will be least sensitive. The SIC lies between these versions in
terms of relative weight given to vii.
Any of these sample versions of the influence curve for /? may be
transformed to a sample influence curve for II. = Z/l by multiplying on
the left by Z; see (3.3.6).
An alternative and perhaps more immediately revealing derivation of
EICi, EIC,,,, and SIC, can be obtained from a related perturbation
scheme . Let all cases have
error variance a2, except for case i with var (ei) = 02/wi, wi > 0. Then,
using Appendix A.2, the weighted least squares estimator of /? as a
function of wi can be written as
Differentiating (3.4.7) with respect to wi yields
The EIC,, apart from the multiplier n, is found by evaluating A@(w,) at
\ci = I and thus describes the rate of change in the estimator as wi
deviates from 1. Similarly, the EIC,,, is found by evaluating A/?(wi) as
wi + 0. and it measures the rate of change in the estimator as the i-th
case is deleted. The SIC is a compromise between ElC and EIC,,, since
is the average gradient over the whole interval.
ASSESSMENT O F INFLUENCE
E X A M P L E 3.4.1.
CLOUD SEEDING NO. 6. Figure 3.4.1 contains a
graph of the estimate of/I,, from the cloud seeding model (2.4.23) with
A = 113 as the weight for case 2 is varied from 0 to 1. The comments
made above concerning the three empirical influence curves are clear.
The EIC corresponds to the derivative at \vi = I , which seems too small.
while EIC,,,, the derivative as ,tii + 0, seems too large. The SIC, which
corresponds to the slope of the line joining j(O) and $(I), appears to
provide a more satisfactory summary of this curve.0
Figure 3.4.1 jj14(\i!2)
versus \r2. cloud seeding data. Note: s.e. f i?,&
0.177, s.e. (j,,
(0)) = 0.324
E X A M P L E 3.4.2.
P A R T I A L I;-TESTS. Partial F-tests for the hypoth-
esis that the individual coefficients of pare zero are commonly used to
simplify a linear model. When using this procedure, it is not uncommon
to find that retention of a particular coefficient depends on the presence
of a single case. This behavior seems particularly prevalent when the
model contains polynomial terms. The influence of individual cases on
the partial F-tests can be seen from the SIC for the associated F-
statistics .
RESIIIUALS A N D I N F L U E N C E IN REGRESSION
Let Pk denote the k-th component of band define
z k = fiklci J(bk)
where bk is the k-th diagonal element of (XTX)-'. The partial
F-statistic for the hypothesis that fik = 0 is Fk = s:. Further, let fikci,,
T,,,,, b,,,,, ci,f.,, and F,,,, denote the analogous quantities based on the
data set without the i-th case.
Characteristics of the SICi = (n- 1) [Fk - F,,,,] are most easily seen
by expressing F,,,,. as a function of Fk. We consider the three
components compr~sing F,,,, separately: using Appendix A.2,
6;) = [ ( n - p1)c3* - e;/(1 - vii)]/(n - p' - 1)
Cki = d:(XTX)-'xi
and dk is a unit vector of length p' with 1 in the k-th position. After
substituting these three forms into
a little algebra will verify that [; - ,, ( 3 L ) l i 2 p
(,I - p l - 1)rf
' - n - p - r ) [l + p2vii/(l - vii)]
where p denotes the correlation between fik and xTb, and ri is the i-th
Studentized residual.
Recall that vil/(l - vii) will be relatively large for remote points. The
term (n - p' - 1)rf/(11- p' - rf) = tf will be large when the i-th case is
an outlier, and under the null hypothesis it has an F(1, n-p'- 1)
distribution.
I t seems clear from inspection of Equation (3.4.10) that aln~ost
anything can happen to the partial F-statistics when a case is removed.
Two general observations seem particularly interesting, however:
Suppose that the deleted case appears to be an outlier (r? is large) and
that 11(1.~,;(
I - rii))'IZ is negligible; empirical investigations indicate
that typically p is not negligible by itself. Then,
ASSESSMENT O F I N F L U E N C E
Deleting a case with rf > 1 in a dense region will tend to increase all
partial F-statistics.
Next, consider the deletion of a point that fits the model quite well
(r? I 1). Then,
and we can generally expect all partial F-statistics greater than one to
decrease when a conforming point which has vii large is deleted.0
3.5 Applications of the sample influence curve
The sample influence curve defined at (3.4.6) has natural appeal as the
basis for diagnostic techniques that locate influential cases. We recall
again its basic properties: It is computed from observed data and apart
from constants it is interpreted as the change in a statistic when a case is
deleted. Also, for many problems including linear least squares
regression, the SIC, or approximations thereof, can be easily computed.
We shall see that the sample influence curve has other desirable
properties derived from geometrical considerations and from exten-
sions to the study of the influence of groups of cases.
In the remainder of this chapter, methods for finding influential cases
are developed from the sample influence curve; methods based on the
EIC or EIC(,, can be developed similarly. To be most useful, such
methods should allow the cases to be ordered on the basis of influence.
For linear least squares, the SIC for j? is a p'-dimensional vector and
there is no natural ordering of multidimensional vectors. Even in the
case p' = 2 where a scatterplot of the SIC can be constructed and
inspected, there is no natural way to construct a complete ordering of
the points on the basis of influence. It is necessary. therefore. to use a
norm to characterize influence and order cases. A norm may be
regarded asa function which maps the SIC into R1.
Ofcourse, there is a
natural ordering (less than) for points in R'. The choice of a norm to
characterize the SIC is a crucial part of the study of influence.
Norms can be usefully defined from properties of a model. We call
such norms rxternc~l. Alternatively, they can be defined without
reference to the model by considering the t~ values of the sample
influence curve as a multivariate sample,and applying an intrrrll~l norm.
After a study of characterizing norms for the influence of a single case.
we turn to multiple case influence measures, which are straightforward
RESIDUALS A N D INFLUENCE I N REGRESSION
generalizations of the one-at-a-time statistics. Other norms for in-
fluence are discussed in Chapters 4 and 5.
In linear least squares regression, the sample influence curve for /? is
(n - 1)- SICi = @ - &,
Since SIC, is a p'-vector, it is useful to consider norms D,(M, c)
determined by a symmetric, positive (semi-)definite p' x p' matrix M
and a positive scale factor c:
Contours of constant Di(M, c) are ellipsoids of dimension equal to the
rank of M. The contours may be viewed as being centered at $ or &,,
both interpretations being used in what follows.
This general norm has a useful interpretation in terms of linear
combinations of the elements of 8. Let z denote an arbitrary p' x 1
vector, k = ((n - 1 ) 2 ~ ) - 1 and, assuming that M is positive definite, let
As a function of z, the SIC for zT/3 is
SIC, (z) = (n - l)zT(@ - &,)
max [S1Ci(z)12 = D, (M, c)
The maximum is attained in the direction of M()- &,).Thus, Di(M, c)
can be interpreted as the maximum over z of the squared sample
influence curves for zTjY when z is constrained to lie within the ellipsoid
B. Of course, the ordering over i of these maxima will not change if
k > 0 is allowed to be arbitrary, but independent of i.
Clearly, the character of Di(M, c) is determined by M and c, which
may be chosen to reflect specific concerns. In what follows, we discuss
both internal and external norms. The inner-product matrix M is
nonstochastic for external norms in linear least squares. For internal
ASSESSMENT OF INFLUENCE
norms, which are developed in Section 3.5.2. M is stochastic even for
linear least squares.
A form of Di(M, c) that reveals the effects of varying M and c is
obtained by using Appendix A.2 to express &, in terms of the full
eixT(XTX)-' M(XTX)-' xiei
Di(M, c) =
c(1 - Vii)2
where ri is the i-th Studentized residual and Pi(M) is defined implicitly
in this expression. By the nature of the regression problem, M and c
should be chosen to make Di(M, c) invariant under changes of scale and
nonsingular linear transformations of the rows of X. In particular, c
should be chosen so that S2/c is scale free. While there are many ways to
achieve this, two stand out as obvious candidates: Choose c = kc?2 or
kc?:,, where k > 0 is a known constant that does not depend on X.The
former choice was suggested by Cook and Cook and
Weisberg , while the latter choice has been suggested by Belsley et
al. and Atkinson . If c =
then r,2e2/c = rf/k.
On the other hand, when c = kc?;, it follows from (2.2.8) that r,2c?2/c
= tflk where, as before, t: is the i-th externally Studentized residual.
For either of these choices for c, the stochastic part of Di(M, c)
depends only on r;. Since the null distribution of rf does not depend on
X, or on the actual values of the parameters, it is reasonable to ask how
the influence of the i-th case can be altered when the fit, as measured at
the i-th case by r;, is fixed. With rf fixed, it is clear from (3.5.3) that
influence is a monotonicallj increasing function of Pi(M). If Pi(M) is
large, the observed value of r? must be small for the case to be
uninfluential. However, Er? = 1 under a correct model and thus cases
with large Pi(M) will typically be influential.
If M is nonstochastic, then so is Pi(M), and its magnitude depends on
the location of x: relative to the distance measures determined by the
inner-product matrices (XTX)- ' M(xTX)- ' and (XTX)- '. It can be
expected that cases with large vii will have Pi(M) large also. However.
this need not necessarily follow since by choice of M the numerator of
Pi(M) can be small even if vii is large.
We view Pi(M) as the potential, relative to M, for the i-th case to be
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
influential. Potential is important since it can be used to describe and
detect configurations of the rows of X that are likely to produce highly
irillucntial cases.
M = XTX, r = p'G2
Although the class of invariant norms is large, one stands out by appeal
to usual confidence ellipsoid arguments. A (1 -a) x 100% confi-
dence ellipsoid for
based on B is given by the set of all
- fiT(XTx) (,?* - B) I F ( I - a; p: 11 - p')
This ellipsoid is centered at 8, with contours determined by the
eipenvalues and eigenvectors of (XTX);
is a scale factor used to
assign proper values to contours. Reference to (3.5.4) suggests setting
M = XTX and c - ~ ' 8 ~
in (3.5.2), to give
This measure, first proposed by Cook , gives the squared
distance from 8 to &, relative to the fixed geometry of XTX. By
exploiting the similarity to (3.5.4), values of Di(XTX, p'G2) can be
converted to a familiar probability scale by comparing computed
values to the F(p', n - p') distribution. For example, if Di(XTX, p'b2)
equals the 0.50 value of the corresponding F distribution, then deletion
of the i-th case would move the estimate of ,? to the edge of a 50%
confidence ellipsoid relative to B. However, Di(XTX, p'S2) is not
distributed as F; this comparison is used only for converting Di to a
familiar scale .
Figure 3.5.1 illustrates the measure Di(XTX, ~ ' 8 ~ )
for a problem with
p' = 2 and no intercept. The figure is derived from a linear approxi-
mation to a nonlinear problem to be discussed in Example 5.1.1. The
elliptical contours correspond to Di(XTX, ~ ' 8 ' ) = constant. Although
contours of constant influence are elliptical, the (fllcn, flzci,) often tend
to have nonelliptical scatter. In the figure, they generally fall along a
curve, with the exception of the one clearly influential case in the lower-
left corner.
Alternatively. Di can be rewritten as
ASSESSMENT OF I N F L U E N C E
Figure 3.5.1
fi,(i, in a model with no intercept. + indicates the full
data estimate (B,, P2). Ellipses are contours of constant Di with values shown
where P(,, = x&,. For problems where prediction is of more interest
than estimation, Di may be viewed as the usual Euclidean distance
and P(,,. Clearly, any norm in the p'-dimensional estimation
space may be regarded as a norm in the n-dimensional observation
space providcd that M is of the form M = XTBX.
A computationally convenient and revealing form for Di is obtained
by substituting M = XTX into (3.5.3) 
Apart from the constant p', Di is the product of a random term r f and
the potential Pi(XTX) = vii/(l - vii), which is a monotonic function of
oii. For linear least squares, computation and examination of the tii, has
become common practice .
and this is sensible if M = XTX is used to define a norm. ?'he potential
itself can be given several interesting interpretations. Cook 
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
noted that vii/(l - uii) = var (ji)/var (e,). Weisberg pointedlout
so the potential is a distance relative to the ellipsoids defined by
(X;,X,,,)-'. Huber noted the relationship ji = (l'-~~~)x:&~,
+ riiyi so that potential can be interpreted as a function of the relative
weight of yi in determining ii.
Finally, oii/(l - uii) is proportional to the
total change in the variance of prediction at x,, . . . , x, when xi is
The i-th case will be called influential ifDi is large; the exact definition
of large will depend on the problem, but Di greater than I, correspond-
ing to distances between fi and &, beyond a 50% confidence region,
usually provides a basis for comparison.
E X A M P L E 3.5.1.
coMsrNATroNs OF r!, uii. Suppose that in a
data set with p' = 3 and ti = 100, four pairs of (el, vii) occur as given in
Table 3.5.1. For each of these four cases Di = 3.0, so deletion-ofany one
of the four would move the estimate of
to the edge of a 95%
confidence region about fi, and each would be called highly influential.
However, the reasons for the influence in the four cases are not the
same. Cases 3 and 4appear to be outliers given the extreme values for ri,
while for the other two cases the influence is apparent because of the
potential; the large values of oii indicate that these cases are relatively
far removed from the bulk of the data.0
E X A M P L E 3.5.2.
R A T DATA. In an experiment to investigate
the amount of a drug retained in the liver of a rat, 19 rats were
Table 3.5.1 Residuals and vii for jolcr hypothetical cases
ASSESSMENT OF INFLUENCE
randomly selected, weighed, placed under light ether anesthesia and
given an oral dose of the drug. The dose an animal received was
determined as approximately 40 mg of the drug per kilogram of body
weight, since liver weight is known to be strongly related to body
weight and it was felt that large livers would absorb more of a given
dose than smaller livers. After a fixed length of time each rat was
sacrificed, the liver weighed, and the percent of the dose in the liver
determined.
The experimental hypothesis was that, for the method of determin-
ing the dose, there is no relationship between the percentage of the dose
in the liver (Y)
and the body weight (X
,), liver weight (X,),
and relative
dose (X,).
The data and sample correlations are given in Tables 3.5.2 and
3.5.3(a). As had been expected, the sample correlations between the
response and the explanatory variables are all small, and none of the
simple regressions of dose on any of the explanatory variables is
significant, all having t-values less than 1 as shown in Table 3.5.3(b).
However, the regression of Yon X,.
X , , and X 3 gives a different and
Table 3.5.2 Rat data. Source: IVeisherg 
X,-Body weight
X2-Liver weight
X,-Relafire dose
R E S I D U A L S AN11 I N F L U E N C E I N REGRESSION
Table 3.5.3 Rat doto. (u) Sunlple correlrrfions.
-Body weight (g)
S,-Liver weight (g)
S,-Relative dose
Body weight
Liver weight
Table 3.5.3 Rot dufu. (h) Krqrrssiott ,sirnlt,litry, 1-rrulues in pr~ret~rhese.~
Model including
Coeficietlr
(XI Xzr X3) -
/I, (rat weight)
[I, (liver weight)
/I3 (dose)
contradictory result: two of the explanatory variables, X , and X 3 , have
significant t-tests, with p < 0.05 in both cases, indicating that the two
measurements combined are apparently useful indicators of I: If X , is
dropped from the model, the same phenomenon appears. The analysis
so far might lead to the conclusion that a combination of dose and rat
weight is associated with the response.
Figure 3.5.2 gives plots of r , vi,, and Di against case number for the
model Yon X I , X,. X,. The ri do not display any unusual features as
they are all less than 2, without obvious trends or patterns. However,
inspecting the D,, we locate a possible cause: case 3 has D3 = 0.93; no
other case has Di bigger than 0.27, suggesting that case 3 alone may have
large enough influence to induce the anomaly. The value of u3, = 0.85
indicates that the problem with thiscase is that the vector x3 is different
from the others.
When case 3 is deleted, and the model is refit, the t-values for the
coefficients of X,, X , , and X 3 are all substantially less than 1 in
absolute value, so the anomalous result of a significant pair of
regressors can be attributed to case 3 alone. Of course, this could have
been anticipated from the discussion given in Example 3.4.2.
A S S E S S M E N T OF I N F L U E N C E
The reason for the influence of case 3 must now be studied.
Inspection of the data indicates that this rat,with weight 190g, was re-
ported to have received a full dose of 1.00,which was a larger dose than
it should have received according to the rule for assigning doses (for
example, rat 8 with a weight of 195 g received a lower dose of 0.98). A
number of causes for the result found in the first analysis are possible:
(I) the dose or weight recorded for case 3 is in error or (2) the
regression fit in the second analysis is not appropriate except in the
Case number
Case number
Figure 3.5.2. Rat data: plots against case number. (a) ri. (b) rii (cont'd
RESIDUALS A N D INFLUENCE IN REGRESSION
Case number
Figure 3.5.2 Rat data: plot against case number. (c) Di
region defined by the 18 points excluding case 3. This has many
implications concerning the experiment. It is possible that the combi-
nation of dose and rat weight chosen was fortuitous, and that the lack
of relationship found would not persist for any other combinations of
them, since inclusion of a data point apparently taken under different
conditions leads to a different conclusion. This suggests the need for
collection of additional data, with dose determined by some rule other
than a constant proportion of weight.0
Alterr~ativefull rank choices for M, c
The choice of (M, c) determines the geometric character of the norm.
The class of (M, c) for which Di(M, c) is invariant under linear
transformations is large, but the examples considered in Fig. 3.5.3
depict four obvious choices corresponding to pf-dimensional elliptical
Figure 3.5.3(a) shows the measure Di = Di(XTX, ~ ' 6 ' )
that has been
previously considered. All points on the ellipsoid drawn have the same
value for the characterizing norm. Measures using M = X;,X,,,
(Figs. 3.5.3(b) and (c)) can be usefully viewed as corresponding to
measuring the distance from
to fl relative to the ellipsoid defined
without the i-th case and centered at
As illustrated, the resulting
ellipsoids need not all be of the same shape, and thus direct comparison
ASSESSMENT OF I N F L U E N C E
Figure 3.5.3 Graphical comparison of four norms: (a) Di(XTX, ~'6'). (b)
Di(X$, XX,~,,
~'3'). (4 Di(X$,Xti,, p'ci;,). (d) Di(XTX, p's;,)
of the norm from case-to-case is questionable. From Fig. 3.5.3(b), for
example, deletion of case 2 appears to lead to more nearly circular
contours than did deletion of case 1 and, while D,(XTX, p'Z2)
= D, (XTX, p'b2), the relationship between Dl (X:, , X,,,, p'ZZ) and
D, (X,T,,X,,l, p'b2) is uncertain, as either may be larger.
In Fig. 3.5.3(b), c = p'b2 while c = p'Z;, in Fig. 3.5.3(c). These two
figures look alike and they have the same contours of constant value.
but the values assigned to the contours are different, as the scale factors
RESII>UAl,S AN11 I N F L U E N C E IN REGRESSION
are different in each figure. This, too, has the effect of making
comparisons between cases more difficult.
The fourth measure, graphed in Fig. 3.5.3(d), can be viewed again as
the distance from
to Busing ellipsoids determined by the full sample
M = XTX, but applying different scale factors for each i, so compar-
ability of the values of the norm is again unclear. The measure
D,(XTX, 6;,) has been called (DFFIW)' by Belsley et a/. .
Atkinson discusses [Di(XTX, p'$i,/(n - p'))]"'.
Other differences between these norms can be seen by examining
their algebraic forms, as listed in Table 3.5.4. Atkinson suggests using
6;) in place of ciZ since this will give more emphasis to outlying cases
(1: > r: when r: > 1). Belsley et a/. replace 5' with 6;) in order to make
the denominator statistically independent of the numerator under
normal theory. We prefer measures based on a fixed geometry where M
and c- do not depend on i since such measures provide an unambiguous
ordering of cases. In addition, 2'
could be replaced by a robust
estimator in order to reduce the effects of outlying cases on the
est~mated scale.
Table 3.5.4 Nortned itlfluetlce nleclsures. Source: Cook ond kveishrrg 
Reduced jbrm
Lower-dimetrsional norms
If M is chosen to have rank q < p', contours of constant Di(M, c) are
q-dimensional ellipsoids. In particular, if Z is a q x p' rank q matrix such
ASSESSMENT O F INFLUENCE
that * = Z/l is of interest, then the norm with
is an invariant norm corresponding to q-dimensional ellipsoidal
confidence contours for * based on 4 = z#.
Suppose a subset of the elements of b, say the last q, is of interest.
Partition X = (XI, X,), where X, is n x q. If Z = (0, I,),
Substituting this choice of M into the general form (3.5.3) for Di(M, c)
and simplifying yields
where U = X, (X:Xl)- ' X:,
and W* = V - U projects onto the
columns of (I - U)X2. The uii can be obtained from the projection
matrix for XI,
and the wz obtained by subtraction. The potential Pi(M)
for this measure is w:/(l - vii) which will tend to be relatively small if
the i-th row of (I - U)X2 is sufficiently close to zero or to the sample
average if the constant is not in XI.
Two special cases of lower-dimensional norms are of some interest.
If we set q = 1, then the measure concentrates on a single coefficient of
the parameter vector. If c = 6;, is used to replace c = t2,
the resulting
measure is called DFBETASij by Belsley el a/. . The potential
when q = 1 will be small if the i-th residual from the regression of X, on
X1 is small. In the general situation with q = 1, suppose $ = zTfl. If
c = 6', the norm becomes 
Di(M, c) = p ' ~ i p 2
(X'S, zT#)
where p(.,.) is the correlation. The maximum plDi of this norm for
fixed i occurs at z = xi.
If q = p, c = p6', and the intercept is excluded, then
RESIDUALS A N D INFLUENCE IN REGRESSION
When the intercept is not of interest, this last measure may be
preferable to the more usual Di = Di(XTX, p'r?'). We will continue to
use Di since modification of results for (3.5.13) is straightforward.
When xi = 8, uii = l/n, and measure (3.5.13) is zero. Relative to this
measure, observations at X have no influence.
E X A M P L E 3.5.3.
C L O U D S E E D I N G NO. 7. For the cloud seeding
data, the coefficients for the seeding effect and interactions are of
primary interest, so the choice of
is suggested. The distance measure based on ellipsoids for @, can be
computed from (3.5.10). For the data using Y"I3' and PU'j', the values
of Di and Dr are given in Table 3.5.5 (the other columns in this table
refer to a later example). The ordering of cases on influence is similar for
the two measures. Case 2 is the most influential, but D, is over three
times the size of ~ r . 0
Predictions
The diagnostics considered thus far measure the influence of individual
cases in terms of their effects on the estimation of selected linear
combinations of the elements of 8. The general measure Di(M, c),
however, is applicable in situations where prediction rather than
estimation is the primary goal.
Let X, be a q x p' matrix and suppose we wish to predict the q-vector
of future values
YJ = XfP+s,
where sf is independent of the vector of errors E in (2.1.1) and
Var ( E , ) = 0'1. A point prediction for Yf is
= X,S, and
~ a r ( E , - Y,) = O ~ X , ( X ~ X ) - ' X : + aZ1
A (1 -a) x 100% normal theory prediction region for Y, is given by
the collection of all q-vectors Y* such that
(E, - Y * ) T [ ~ f
( x T x ) - Ix; + I] - (PI - Y*) I F(l -a;q,n-p')
The sample influence curve for the point prediction 9, is propor-
tional to (2, - q,,i,) = XI ( p - &,).This in combination with (3.5.16)
Table 3.5.5
Injuence statistics, cloud seeding data
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
suggests the norm Di(M, c) with c = q62 and
If a single prediction is of interest then y = 1, X, = xJ and
where o, = xJ (XTX)- ' x and p2 is defined following (3.5.1 1). Thus,
the norm of the sample influence curve for a single prediction is simply
the analogous norm (3.5.1 1) for estimation, reduced by the factor
i1 /( 1 + o,
). Clearly, pJDi provides an upper bound for predictive as well
as estimative influence.
A drawback to the use of Di(M, c) for prediction is the requirement
that X, be specified a priori. If a model is to be used primarily for
prediction. X, may not be known during thedevelopment of the model.
A possible solution to this problem is to construct X, by choosing
points that in some sense cover the region of interest. Coverage could
be reflected both in terms of the location of the points and their density.
From this point of view, a useful default is the choice X ,
= X; that is,
consider the predictions at the cases used to construct the model. Then,
M = XTII+V]-'X = f ( X T X )
since [I + V] - ' = 1 - V. When X, = X,
Di ( M , C) = -
A second possible solution to the problem of an unspecified XI is to
set q = 1 and, for each i, choose x, to maximize (3.5.18). Let
v j i = xT(X"X)-'x, and rewrite (3.5.18) as
Di ( M , C) =
V i i ) (1 + v l )
Thus. maximizing Di(M, c) by choice of x, is equivalent to maximizing
o;,/(l + r , ) (see Appendix A.3). It follows that
and therefore
v.. - l/(n + 1)
max IDi (M, c)] = r: "
For large ti, this is essentially plDi.
ASSESSMENT OF INFLUENCE
In the linear least squares problem, use of an external norm that
corresponds to using confidence contours to order the values of the
sample influence curve is straightforward and appealing. These norms
are based on fixed metrics that do not depend on the observed behavior
of the sample versions of the influence curve. Of course. they do depend
on the expected behavior of the data in so far as (XTX)- ' or the related
inner-product matrices accurately reflect the variance of B. In contrast,
internal norms are based on a matrix that derives from the observed
values of the appropriate sample version of the influence curve. Internal
norms may be constructed to be robust with respect tB variations in the
model or methods of analysis that would necessitate different external
norms. If, for example, the model were altered to have Var(e)
= a2W- ', where W is known, then to be consistent with previous
rationale the inner-product matrix X T X for an external norm should be
changed to XT WX.
We present two methods for internal scaling. In the first. the 11 values
B-&, are treated as an unstructured p'-dimensional sample. and a
multivariate outlier technique is used to order the values. Other
methods for ordering a multivariate sample are given in Andrews
 , Gnanadesikan , and Barnett and Lewis .
In the second, we consider the norms Di(M, c), where M, and c are
chosen through use of the jackknife method.
Ordering trsing a multioariate outlier statistic
One method that is particularly well suited for study of the n values of
the SIC is Wilks' criterion for detecting a single outlier in a
multivariate sample. Let b,, b,, . . . , b, be p'-vectors, and define
6 = n-' C bi and A = C (bi - 6 ) (bi - 6)T. Wilks'criterion selects bi as a
possible outlier if i mitlimizes
Since 16) is proportional to the square of the volume of a
p'-dimensional ellipsoid, minimizing this ratio is equivalent to choosing
bi to minimize the volume remaining after bi is deleted, so in some sense
bi must be far from the other vectors b,, j # i.
The results on determinants in Appendix A.2 can be used to simplify
the ratio (3.5.22). One finds that minimizing (3.5.22) is equivalent to
RESIDUALS A N D INFLUENCE IN REGRESSION
maximizing the distance
over i = 1,2, . . . , n. For linear least squares, explicit formulae for the
6, can be obtained for any of the empirical versions of the influence
function discussed earlier in this chapter. It is useful here to discbss the
EIC and the SIC separately.
For the empirical influence curve (3.4.2), it is sufficient to take
bi = (XTX)- 'xiei and thus 6 = 0. The inner product matrix, say A,, is
The quantity (n/(n -pl))Ao is a robust estimate of Var (B) obtained
using the weighted jackknife method proposed by Hinkley .
Substituting into (3.5.23), the corresponding normed measure is
The statistic 64 can be computed by first defining W to be an n x n
diagonal matrix with diagonals e?, j = 1, . . . , n. The 64 are then the
diagonal elements of W1lZX(XTWX)-lXTW1lZ,
the projection on the
columns of WIiZX.
For the sample influence curve defined at (3.4.6), we can take
b, = (XTX)- 'xiei/(l - qi). Since 6 is not zero, the form (3.5.23) does
not simplify. The cross product matrix, say A,, is
where nf = Cxjej/(l - ojj). The matrix (n - l)Al/n is the estimate of
Var (fl) obtained from the usual, unweighted jackknife .
Although the corresponding internally scaled measure 6 f can be
computed exactly, some desirable algebraic simplification is possible if
the usually small correction for the center Z is neglected. Setting Z = 0,,1~\
and substituting into (3.5.23), the resulting measure is
ASSESSMENT OF INFLUENCE
As with the EIC, this measure can be computed as the diagonal
elements of the projection on the columns of W 1 1 2 X , where the t~ x n
diagonal matrix W has diagonal elements eFl(1 - t ~ ~ , ) ~ .
Jackknfe method
The jackknife can be used to provide an alternative internal scaling
method for empirical influence curves. In the most frequently used
version of the jackknife, estimates are obtained by averaging n analyses,
each obtained by deleting one case at a time from the data . In many problems, jackknife estimates of
parameters and variances have desirable properties. For example,
Hinkley suggests nA,/(n -p') as an alternative estimator of
~ a r ( 8 )
that is robust against nonconstant error variances. This. in
turn, suggests the use of Ji E D i ( A , ', pln/(n - p ' ) ) as an alternative
to Di = D i ( X T X , ~ ' 6 ~ ) .
The interpretation of J i is the same as that of
Dl, except that the metric should now be more robust. The statistic J,,
for i = 1,2, . . . , n, is given by
n - p ' e f x : [ C e f xjxjT]-'xi
Ji = Di(A; I , pln/(n - p')) = --
( 1 - vii12
( 1 - vii12
J i provides an interesting compromise between 6: and 6:. In addition,
the interpretation of Ji as a robust version of Di has some appeal. A
drawback of J,, at least for linear least squares. is that its computation
will generally requirea second pass through thedata to obtain (if,
Di is computed directly from ri and tiii.
EXAMPLE 3.5.4. CLOUD S E E D I N G NO. 8. Table 3.5.5 lists the
values of SP, S f , and Ji, as well as Di and D r as discussed in Example
3.5.3 for the cloud seeding data in the cube root scale. The statistics
show reasonable agreement, although J , = 62.189 and 6: = 1.000 are
remarkably large, stressing the role of case 2 more clearly. Also. J , ,
= 2.326 suggests further interest in case 18. The EIC measure 6: pays
less attention to the vii, and the ii: are large for cases with large rz.
The study of influence can be augmented by a number of graphical
displays. The most elementary are plots of the statistics ri, vii, and
R E S I D U A L S AN[> I N F L U E N C E IN REGRESSION
D, (M, c ) against case number. As illustrated earlier in Example 3.5.2,
these plots provide a quick method of finding cases with large residuals,
high potential, and high influence. They will be especially effective if the
sample size is too large to make examination of lists of statistics useful,
or if the ordering of cases is meaningful.
Atkinson (198 1) has suggested that influence for an entire sample can
be assessed by a display of the [Di(M, c ) ] ' ' ~ in a half-normal plot, with
a simulated envelope added as described in Section 2.3.4. High
influence cases will appear as isolated points at the far right of the
graph. If no cases are influential this plot should be approximately
straight. If part of the plot falls outside the simulated envelope, then
some evidence is given that the assumptions used to compute the
envelope. usually normality, independence, and constant variance, do
not apply.
A third graphical aid for the assessment of influence is the added
variable plot discussed in Section 2.3.2. Using the notation of that
section, the added variable plot is a graph of the residuals obtained
when X, is deleted from the model, (I - U,)Y, against the residuals
from the regression of Xk on the other Xs, (I - U,)X,. In some ways
these plots can be interpreted as a plot of y versus x in simple linear
regression. Individual or groups of cases that stand apart from the rest
of the cases should be investigated further. Their influence on the
coefficient in question can be determined by deleting them, either
individually or in groups, and recomputing the regression. Often, it will
be found that such cases are influential.
While these plots are undoubtedly useful in trying to understand
influence, they must be interpreted and used with some care since their
use does not correspond to any standard case-by-case diagnostic
method. When any case is omitted from the data, the projection matrix
U, changes and the entire character of the plot can change. In addition,
these plots can fail to identify highly influential cases. If the i-th
diagonal element of U, is large, the corresponding elements of (I
- Uk)Y and (I - U,)X, will tend to be small and thus the plotted point
may not exhibit unusual characteristics, while the corresponding case
could substantially influence fl,.
E X A M P L E 3.5.5.
C L O U D S E E D I N G NO. 9. Figure 3.5.4(a) is a half
normal plot of Df l2 for the cloud seeding data in the cube root scale.
The relatively wide envelope at the right of the plot suggests that an
influential case is very likely given the particular array of Xs in this data;
one such influential case is observed. Aside from this one point, the plot
Half-normal quantile
Half-normal quantile
Figure 3.5.4
Half-normal plot of D!I2, cloud seeding data. (a) Y, P, AP
transformed via cube root transformation. (b) Untransformed data
RESII>UALS A N D I N F L U E N C E IN REGRESSION
is essentially straight, suggesting that the model may be adequate. In
contrast, the plot given in Fig. 3.5.4(b) for the untransformed cloud
seeding data is generally curved. Atkinson would take this as
evidence of the need to transform the data. I-J
E X A M P L E 3.5.6.
JET FIGHTERS NO. 4. Added variable plots for
the jet fighter data are given in Figs. 2.3.8-2.3.10. In the plot for S L F ,
Fig. 2.3.10. it appears that the F-l 1 IA suppresses the usefulness of this
variable since, if this case were deleted, the remaining cases would
appear to show a slight systematic trend. When the point correspond-
ing to the F-1 I 1A is removed from the data in Fig. 2.3.10, but the
residuals are not recomputed, the slope increases from 0.0837 to 0.1 156.
Figure 3.5.5 is the correct added variable plot for S L F , with the
residuals recomputed after the F-1 1 1A is deleted from the original data.
The slope fitted here is 0.1386, so just deleting the F-11 I A from
Fig. 2.3.10 results in an underestimate of the slope. Furthermore, the
spread in Fig. 3.5.5 and 2.3.10 is markedly different, and the two plots
Residuals o f SLF on SPR RGF PLF CAR
Figure 3.5.5
Added variable plot for S L F computed without the F-1 1 IA, jet
fighter data
ASSESSMENT OF I N F L U E N C E
suggest different conclusions concerning SLF. From Fig. 3.5.5. SLF is
more clearly important.
3.6 Multiple cases
Both the derivation of the influence curves. and the diagnostic
procedures developed from them, have concentrated on the effects of
individual cases on estimates. For theoretical use of influence curves to
study estimators, it can be expected that a study of pointwise influence
will suffice. Additionally, in many practical data analytic problems.
consideration of cases one at a time will provide the analyst with most
of the information needed concerning the influence of cases on the
fitted model. However, it can happen that a group of cases will be
influential en bloc, but this influence can go undetected when cases are
examined individually. This is illustrated with Fig. 3.6.1. If point C or D
is deleted, the fitted regression will change very little. If both are deleted.
the estimates of parameters may be very different. Conversely. if A or B
is deleted the fitted line will change but if both are deleted. the fitted line
will stay about the same.
~i&re 3.6.1
Illustration of joint
influence. Source: Cook
Weisberg 
The generalization of the influence curve and its empirical versions to
multiple case problems is straightforward. Let I be an m-vector of
indices of selected cases, IT = (i,, i,, . . . , i,),
1 2 i, 5 n, and continue
the earlier notation so that the subscript '(I)' means 'with the m cases
indexed by I deleted,' while 'I' without parentheses will mean that only
the cases indexed by I are remaining. For linear least squares. one
obvious generalization of the sample influence curve is
RESIDUALS A N D INFLUENCE IN REGRESSION
There are (1) possible sets of cases at which the sample influence curve
can be evaluated.
The (externally) normed measure Dl (XTX, p'8') is
as given by Cook and Weisberg . The other externally normed
measures discussed in the last section are similarly defined for multiple
cases. The geometric interpretation of these measures is identical to
that for m = 1. An influential subset for estimating fi will correspond
to a large Dl.
As might be expected Dl can be expressed in multidimensional
analogues of the ri and vii. The results are obtained by first expressing
b ,
The inverse in (3.6.2) is computed using the basic formula in Appendix
A.2 to give
jo, = [(xTX)-I +(XTX)-' X:(I-VI)-'
XI(XTX)-'][XTY -X;ryl]
= B-(XTX)-I X : [ - ( I - V I ) - l ~ I ~ + ( I + ( I - V I ) - l VI)Yl]
Since (I - V1)- ' = 1 + (I - Vl)- V,,
Finally, substituting into (3.6.1) leads to the form
This result can be better understood by using the spectral de-
composition V, = T A P , where r, with columns y,, is an m x m
orthogonal matrix of eigenvectors, and A is an m x m diagonal matrix
of eigenvalues, 0 I 1, S . . . I
ASSESSMENT OF INFLUENCE
If A, = 1, the inverse in (3.6.6) does not exist, the data remaining after
the cases indexed by I are removed are rank deficient, and a unique
estimator ),I,does not exist. When A, = 1, we set Dl = + a.
IfA, < l,a
scalar version of (3.6.6) is given by
where, for 1 = l,2,. . . , m
Under normality, the h: are identically distributed. The form (3.6.7) for
Dl is directly comparable to Di, except Dl is given as a sum over m
orthogonal directions of squared residuals times fixed components,
while for Di, m = 1.
Other norms
The other choices for norms discussed in Section 3.5.1 can also be
generalized to the case m > 1 with little difficulty. In particular, if a
lower-dimensional norm corresponding to t,b = Z/l is of interest, then
Dr r Dl (M, c), with M, c defined by (3.5.8), provides the appropriate
norm. One can show that qDf < p'D, for all I and @, so if Dl is
negligible, so is Dr. In the special case where Z = (0, I,), and (XI, X,) is
the conforming partition of X, Df becomes
q t 2 @ = eT(1- Vl)- ' (V, - U1)(I - V,)- ' el
where U, is the appropriate principal minor of X1 (X:X1)-' XI.
The internally scaled norm for SIC, can be obtained by following the
derivation in Section 3.5.2. In practice, computation of this norm for
m > 1 is likely to be impractical because of the need to compute
(I - V1)-' (or its eigenvalues) for all possible subsets of size m.
Form = 1, potential has been defined as esserltially the fixed part of the
characterizing measure Di(M, c). Since each of the fixed parts of the
measures given in Table 3.5.4 is a monotonic function of uii, these
norms provide equivalent information on potential and oii is a
reasonable summary. When m > 1, the notion of potential is more
elusive since Dl(M, c) will not conveniently factor into fixed and
R E S l D U A L S A N D l N F L U E N C E IN REGRESSION
random parts. However, useful insights can be obtained from an
investigation of Dl.
Dividing and multiplying the right side of expression (3.6.7) for Dl by
X hi gives
This form can be simplified in two ways. First, by definition
which is the generalization of rf given at (2.2.19). Next, define
q: = hf/Ch,2. Under normality, each q,? follows a Beta distribution
with parameters 112 and (m - 1)/2; their joint distribution is Dirichlet.
We can therefore write
where Q, = CqfIl/(l -I,). This form corresponds closely to that for
m = 1, since Dl is factored into r:, which measures the degree to which
(YI, XI) is an outlying set, and a potential-like term that has random
components that are independent of the parameters in the model.
Several observations concerning QI can be made by simultaneously
considering {q:} and the eigenvalues of V1. First,
This interval is nonstochastic. If Am is small, the cases indexed by I will
have little potential regardless of the observed values of {qf}. For
example, if each vii, i E I, is small, it follows that QI must be small since
tr (V,) = Xuii. On the other hand, if I, is large, the cases must
have high potential. Since 1, I min (vii), a necessary condition for I, to
be large is all vii must be large; that is, each case individually must have
high potential.
For example, suppose m = 2 and I = (i, j), with v = vii = vjj(xi and
sj lie on the same elliptical contour). If, in addition, vij = 0, then
Vl = [:I, ),, = A, = v and Dl = Di + Dj. In this very special example,
the potential for this pair is large or small according to the size of v.
If 1r1 > p'. at most p' of the eigenvalues of VI are nonzero since V, has
ASSESSMENT OF INFLUENCE
rank of at most p'. Hence, for m 2 p', we may write
and the potential interval (3.6.13) has a lower bound of zero.
In situations where A, is small but A, is large, Ql depends on the
values of (q:}. When m = 2, qf is distributed as Beta (4, f), which has a
U-shaped density with most of its probability massed near 0 and 1. Q ,
will therefore tend to fall near one of the extremes of (3.6.13), and el will
tend to fall along one of the eigenvectors of Vl. When m > 2, the density
of each q; is reverse J-shaped, with mode at zero. Thus when
m > p', Q1 will tend to be small since each qf will be small on the
For any m 2 2 and under a correctly specified linear model, the
expected potential is
When the cases indexed by I form an outlying set under the shift
model Y = XP + Dc$ + E, 22r:/02 has a noncentral chi-squared distri-
bution with noncentrality parameter
One can show that, under this model, the joint distribution of (qf}
depends on u = o-'(I -A)'l2TTc$. With m = 2, one can show that
Clearly, outliers can occur in ways which force the potential to be large
E X A M P L E 3.6.1. m = 2. For illustration, consider the situation in
which m = 2 and oii = v for i~ I. Let p denote the correlation between
the residuals indexed by r.Then A , = o - (1 - v)lp(, 1, = r + (1 - a)lpl
and the associated
eigenvectors are
(1, sign ( p ) ) / J 2
(1, -sign ( p ) ) / J2, respectively. If I p l or (1 -
11) is small, the potential
will be essentially deterministic and equal to v/(l - o), which may be
RESIDUALS A N D INFLUENCE IN REGRESSION
large if the points in question are remote. This situation is illustrated
for p = p' = 2 by points x, and x, in Fig. 3.6.2. The points x, and x,
lie along the axes of the ellipse xT(XTX)- ' X = v and thus I p ) = 0. A
configuration for which 1/11 may be large is illustrated by points x,
and x3 = - x , . For these points, p = v/(l - u) and thus Ll = 0 and
i, = 21: and QI depends on the orientation of e, relative' to the
eigenvectors (1, I)/ J2 and (1, - I)/ J2. If the elements of e, are of
opposite sign and approximately equal in absolute value, QI will be near
its maximum, 20/(1- 2v). On the other hand, if the elements of el are
approximately equal, Q, will be near zero. Similar comments apply to a
replicated pair where p = - v/(l - v). El
Figure 3.6.2 Contour of constant v , ~
Clearly, for potential to be large the maximum eigenvalue of V, must
be large. As illustrated in Example 3.6.1, this will occur form = 2 if the
residual correlation (Appendix A.3) between the two cases is large in
absolute value. However, the associated interpretation depends on the
sign of this correlation as well as its absolute value. If the correlation is
large and negative, then the two cases are probably near each other and
may be judged simultaneously. If the correlation is large and positive,
ASSESSMENT O F INFLUENCE
the cases will lie on opposite edges of the sampled region and
simultaneous judgments of such cases may not be desirable.
A complete characterization of potential relative to Dl requires
knowing I,/(1 -A,), I = 1,2, . . . , m. However Dl depends on
1,/(1 -I,) only through Q, which is statistically independent of &lr:.
Therearea variety of ways to summarize this information on potential.
The interval [A,/(l -A1), ).,/(I
-).,)I, the expected potential (3.6.14)
and the tnaximum potrtitiul ),,/(I -i,) (or just i.,) are reasonable
candidates. We believe that the maximum potential is the most
desirable single number summary since it characterizes configurations
of the rows of X that can lead to highly influential groups of cases for
reasons that are independent of the fit.
Using (3.6.4), the multiple case norm DI(M, c) can be written in a
form which allows a general definition of maximum potential:
= r : $ Q l ( ~ )
ef (I-V,)-'X,(XTX)-I M(XTX)-I X:(I--VI)-lel
e:(I - V1)-'el
As in the case when m = 1, we consider only two choices for c: Choose
c = kG2 or kG;, where k > 0 is a constant that does not depend on X or
Y. With M fixed and c chosen as above, we define the maximum
potential relative to M as max,,[Q,(M)]. From the definition of
Q, (M), it follows that
maxe1[QI(M)] = Imax[(I - VI)-112XI(XTX)-
I M (XTX) I x
where I,,, [A] denotes the maximum eigenvalue of the matrix A. For
M = XTX, this reduces to ,4,/(1 - I,) as before. For the measure with
M = X:,X(I,, the maximum potential is I,.
Thus. the choices
M = XTX and M = X:,X,I, provide essentially the same information
about the maximum potential of a particular configuration of the rows
of X to be influential.
Alternative measures of potetrt ill1
A fixed measure of potential can be defined by appealing to the volume
argument analogous to that used for Wilks' statistic (3.5.22). In this
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
formulation, the potential at XI is measured by
High potential or remote sets are indicated by small values of this
measure, which is based on the internal dispersion of the xiTs in much
the same way as the internal norms are based on the dispersion of the
sample versions of the influence curve.
The measure (3.6.20) appears in a factorization of a statistic for
detecting influential cases by Andrews and Pregibon (see
Chapter 4). It was advocated as a generalized 'leverage' measure by
Draper and John .
The type of potential being measured by (3.6.20) seems to differ
fundamentally from that measured by the expected or maximum
potential. These latter measures judge the potential of a set of cases in
the determination of @, while (3.6.20) measures the degree to which XI is
isolated from the remaining rows of X in the p'-dimensional space
defined by the explanatory variables. As pointed out by Draper and
John , cases which have high potential according to (3.6.20) need
not have high potential in theestimation ofp. In reference to thesecond
situation in Example 3.6.1, for example, I I - V, ( = (1 - Al)(l - A,)
= ( 1 - 0)(1 - 21.) = 1 - 211. If o is large the pair of points will be judged
to have high potential according to (3.6.20). However, if e : ~
(1, 1 ) the
points will have no potential and thus no influence on 8.
Although (3.6.20) is not directly relevant to an investigation of the
cases that influence ), the information it provides may be useful in
other phases of an analysis. If, for example, it were possible to design
for the collection of additional data, knowing which of the present
points are remote in the factor space would certainly be helpful.
E S A M P L E 3.6.2.
A D A P T I V E S C O R E D A T A N O . ~ . Toillustratesome
of the previous commeilts on potential we consider two pairs of cases
from the data given by Mickey er rrl. . The model is simple linear
regression and the 11 = 21 cases are plotted in Fig. 2.2.1. As indicated in
the plot. cases 2. 18, and 19 are in question.
Table 3.6.1 gives the case statistics Di, r:, and oii for i = 2, 18, and 19.
Case 19 appears to be an outlier from the assumed model. As shown in
Example 2.2.3, the p-value associated with case 19 is between 0.0409
and 0.0425. Although case 19 appears as an outlier, it has relatively little
influence. Removal of this observation would move fl to the edge of a
ASSESSMENT O F I N F L U E N C E
Table 3.6.1 Selecred case stmtistics, aduprille score darc~
20 "/,confidence ellipse. Case 18, on the other hand, fits the model quite
well, but is influential because of the associated high potential L.,,,,,
= 0.652. Removal of this observation would move B to the edge of a
48 'j/, confidence ellipse. Case 2, as well as the remaining cases in the
data, would probably go unnoticed when inspecting individual case
statistics.
Consider next the highly influential pair (2, 18). D,,.,,, = 6.37.
Removal of this pair would move B to the edge of a 99.2 ?, confidence
ellipse. However, this does not appear to be an outlying pair since
t ( , , , , , = 2.01. This pair must, therefore, be influential because of the
associated potential. In fact, QI = 3.50, which lies near one end of the
interval [0.012,3.85] computed from (3.6.13). Ql depends on the
eigenvalues (A, = 0.012 and A, = 0.794) of V,,.,,, and on the chance
orientation of ec2,,,, relative to the associated eigenvectors. The
observed value of Ql is large for this pair since e:,, ,,, = (-9.57,
- 5.54) is in the direction of the eigenvector associated with i.,,
-4.53). However, since the lower endpoint of the potential range is
small, Q, for this pair does not necessarily have to be large. If e, and
had been of opposite sign. QI might have been small enough to make the
pair uninfluential. The fact that e2 and e 1 8 have the same sign and thus
lie on the same side of the fitted model could be an indication that the
model is incorrect; possibly there are outliers present or a quadratic
term is needed.
In contrast to the previous situation, the pair ( 1 8, 19) is uninfluential.
D( ,,,,,, = 0.15, but may be outlying, t:18,19) = 6.30. Of course. the
possibility that this is an outlying pair is due in part to the presence of
case 19. The observed Q, = 0.037 is very near the lower end of the
potential range [0.036, 2.0251. This value is small because e:,,. 19, =
(- 5.54, 30.28) lies in the direction of the eigenvector corresponding to
A , , ( - 5.54, 33.77).
Four possible summary measures of potential are given in
Table 3.6.2 for three pairs of cases. From the information in the first
RESlD1IALS A N D INFLUENCE I N R E G R E S S I O N
Table 3.6.2
Meustires ojporet~tial, m = 2, adaplive score data
ttr[V,(I - V,)- '1
~ / ( 1 -a)
three columns of this table, it may be difficult to form firm judgments
about the potential of the new pair (1 1, 18).
COMPUTING Dl
One goal in examining subsets of m > 1 cases is to find groups of cases
that, while not individually influential, are influential en bloc. Finding
influential subsets which include smaller influential subsets may add
little information because the observed influence of the subset will be
due. in part, to the influence of the smaller subset. Conversely, finding
an uninfluential subset that includes one or more cases that are singly or
jointly influential would not decrease the interest in those cases. Thus,
good candidates for inclusion in subsets will have small distance values
for m = 1, but they may well have relatively large values of vii or r;.
Alternatively, it may be desirable to consider the possibility that the
individual cases in an influential subset are related (for example, by time
or location). In this situation, good candidates for inclusion in subsets
will include influential cases.
E X A M P L E 3.6.3.
C L O U D S E E D I N G NO. 10. Theaboveremarkscan
be illustrated by reference to Fig. 3.6.3, which contains a semigraphical
summary of A,,,, t:, and D,
for nt = 2 in combination with thecube root
model for the cloud seeding data. In the display, rows and columns
correspond to case indices; thus, for example, the symbol in row 5,
column 8 represents the values of the statistics for the pair I = (5, 8).
The computed values have been divided into groups so that the more
ink used in printing the symbol, the larger the value. The displays
illustrate that: (1) subsets with high potential consist of case 2 and any
other case, case 18 and any other case and the pair (3,20);
(2) pairs for which t : is largecontain cases 7 or 24;and (3) the influential
pairs consist ofcase 2 and most others, (3, 20) and (7, 18). It is clear that
case 2 should be considered as being highly influential, and little is
gained by viewing it as one of a pair.
ASSESSMENT OF I N F L U E N C E
...... :I..
I IIIMIYIII
n t i ~ n ~ ~ n ~ ~ m
.I .:..... ...... :n.n
.I. .. :... ...... :I..
.I:. ..... :.....:1.:
:I... :... .. :...:I..
.I.:.: ......... :1..
........ :I..
:a,..... ....... :I..
...... :I..
. w . . : . . . . .
..... :I..
.#........ ..... :I..
.I...:....
..... :I..
.I........
... ..:I..
.#........ .... .:I..
.I........
..... :1..
.I........
............... I::
I W I I I I W I I I M I I M I I I I#
.H .............. :I .
.mil.:..... ...... :1.
.I........
...... :I..
.I .............. :I..
. a : . . . . . . .
:... ..:u.:
.I ............. ::I..
.......... :...
.......... I...
..............
. . . . . . . . . . :...
. . . . . . . . . . :...
..............
..............
..... .:. ..
.......... :...
.. I...:...
...... :...
. . . . . . . . . . . . .
. . . . . . . . . . . . .
..........
1 ........
I 11111111
.I .......
.I). .. :...
.I).. .....
.I... ....
. I . : . . ...
.I ....... .
' . M .......
.I........
.I ........
I . . . . . . . .
..........
.n ........
: n . . . . . . . .
 :..
.I ........
I N . . .'....
.I........
I ........
.I....:...
I ........
...... :..,
IINI.1I)111
..........
..........
..........
....... I..
..........
..........
..........
..........
..........
..........
..........
Figure 3.6.3 Semigraphical display of i.,,,, t:.and Dl. cloud seed~ng data
Among the pairs whose removal does not result In a rank defic~ent
model, the most likely outlying pair is (7, 15) with t:,, ,,, = 13.82. the
most influential pair is (2, 5) with D,,,,, = 16.48, and the pairs with the
highest potential are (2, 15) and (2, 5) with A,,,
equal to 0.9834 and
0.9822, respectively.
The only additional information obtained by an examination of all
pairs is for (3,20). For this pair, i,,,,, = I. and its deletlon leads to a rank
deficient model. These cases require special handling. and. to accorn-
modate them, deletion ofa variable ( E A ) from the model is deslrable.0
Form = 2 and n not too large, semigraphical displays like Fig. 3.6.3
can be used to present the information about pairs ofcases. However. ~f
m > 2 or n is large, this summary becomes impractical. and better
computational and display methods are needed.
Ifsufficient computer memory is available to store the res~dual vector
and all of the elements of V, an eficient algorithm for finding mult~ple
case outliers can be based on the Furnival and Wilson method
for subset selection. However, an equivalent algor~thrn for finding
subsets with large Dl is not immediately apparent. since altering a
subset by addition, deletion, or substitution of a case can result in
substantial changes in the eigenstructure of Vl. and hence in the ~alue
of D,. Even so, complete storage of V is usually impractical and real~st~c
R E S I D U A L S A N D I N F L U E N C E I N K E G R E S S i O N
techniques for finding influential subsets should use only the izsiduals
and the diagonal entries of V. Using only these, upper bounds for Dl
can be derived, and only if these are sufficiently large must D, be
computed exactly.
For the first upper bound, since A,/(l -
2 A1/(l - A,)z, 1 = 1,
2, . . . , m, D, can be bounded by
p ' t 2 (1 -Arn)
or. since T TT = I,
For this to be useful, A, must be replaced by an approximation that
can be computed without forming V1. Assuming tr (VI) = trace of VI
to be less than one, the simplest approximation is A, I tr (VI). Thus,
or equivalently,
The upper bound in (3.6.23) depends only on the single case statistics
and provides a potentially different upper bound for each I. For any
subset with tr (VI) 2 1, a better approximation to A, is required, which
requires forming VI. If m is small (2 or 3) exact computation of Dl is
probably as efficient as approximating A,.
For fixed m, let T = maxl(CiEl vii) and RZ = max,(C,,,e~), where I
varies over all subsets of size m under consideration. Two upper bounds
for the right side of (3.6.23) are then
ASSESSMENT OF INFLUENCE
and, if T < 1,
These last two may be combined to give
Clearly, (3.6.23) 1 (3.6.24) 1 (3.6.26), and (3.6.23) 1 (3.6.25) 1 (3.6.26).
An algorithm for finding all relevant subsets with fixed m can be
based on these approximations. First, influential subsets of size smaller
than m may be eliminated if desired. Then, the remaining trii and ef are
ordered, largest to smallest. The four inequalities can then be applied to
subsets with tr (V,) < 1 in the order (3.6.26), then (3.6.24), or (3.6.25),
and finally (3.6.23). Exact computation is required if (3.6.23) is too big.
By considering subsets according to the ordered lists of oii and ef, the
subsets that are more likely to be influential are considered first, and
once one of the bounds is sufficiently small, no further subsets made up
of cases lower in the lists need to be considered. Generally, this method
will be useful in data sets with n large relative to p', where tr (V,) will
usually be less than 1. In smaller data sets, relatively more subsets must
be considered. Cook and Weisberg discuss examples, for m = 2
and m = 3, and for two data sets, one with 11 = 21, p = 8 and the other
with n = 125, p = 4. The results of a simple algorithm are summarized
in Table 3.6.3. The number of subsets is less than the total number of
possible subsets because cases influential in subsets of a smaller size
were not considered as m was increased. While in data set 1 little is
Table 3.6.3 Computations using bounds. Source: Cook and Weisberg 
Data set 1
Data set 2
n = 2 1 , p = 8
n = 1 2 5 , p = 4
number of subsets considered
number of applications of inequalities
number of D, computed
number of subsets considered
number of applications of inequalities
number of D, computed
RESIDUALS A N D INFLUENCE IN REGRESSION
gained by use of the inequalities, in data set 2, with large n, substantial
decrease in computation is apparent.
EXAMPLE 3.6.4.
DRILL DATA. In this example, we consider a data
set obtained from an experiment to characterize the performance of a
certain type of drill bit over a range of drilling conditions. For each
experimental run, the work piece and drill were placed at opposite ends
of a .lathe and the values of the following design variables were set:
S = speed of rotation of the work piece in surface feet per minute;
F = feed rate in inches per revolution (rate at which the drill passes
through the work piece);
D = diameter of the drill bit.
The rate of rotation of the drill bit was held constant throughout the
experiment. The response variable Y is the axial load (thrust) on the
drill bit during the drilling process.
The experimental runs were originally arranged in a completely
randomized composite design, but the experiment was prematurely
terminated for reasons that are unimportant in this analysis. The data,
as provided by M. R. Delozier of Kennametal, Inc., Latrobe,
Pennsylvania, are given in Table 3.6.4. The coarseness of the responses
is due to rounding in the measurement technique; the responses for
each combination of S, F and D are from replicate runs. The portion
of the design that was completed is shown graphically in Fig. 3.6.4. The
size of each point is proportional to the number of replicates at that
Since the possibility that the response is a nonlinear function of the
explanatory variables cannot be discounted, we tentatively adopt the
second-order response surface model
Figure 3.6.5 gives plots of L,,,(I) versus I for the power family of
transformations for the second-order model (3.6.27) and for the first-
order subset model Y = B, + /3, S + p2 F + B3 D + &. Evidently a trans-
formation can improve the fit of the second-order model, but will not
result in a significantly improved fit for the subset model. The
magnitude of differences between the ordinates of the two curves shows
that including the cross product and quadratic terms does improve the
fit regardless of the transformation selected. While the likelihood
analysis clearly suggests that some transformation is necessary, it
ASSESSMENT OF INFLUENCE
Table 3.6.4 Drill .data. Source: M . R. Delozier
provides little help for deciding on a particular choice since the
asymptotic 95% confidence interval contains most of the power
transformations used in practice. In this analysis we use the logarithmic
transformation LY, since it is near the maximum likelihood estimate
and has been found to be appropriate in past analyses of similar data.
Transformations of the design variables will not be considered in this
The mean squares for lack of fit and pure error from the second-
order model using LY are 0.0779 and 0.0114, respectively, and the
Figure 3.6.4 Design for the drill data; locations of selected cases are indicated
full model: h = -0.314
3.6.5 L,,,(l).versus i., drill data
ASSESSMENT O F INFLUENCE
corresponding F-statistic is equal to 6.8 with 5 and 16 df. Even with
the logarithmic transformation, the fit of the model does not seem
adequate. Similar results are found for other transformations con-
tained in the 95 % confidence interval given in Fig. 3.6.5. Rather than
attempting to build a more complicated model, we next consider
various diagnostics applied to the second-order model.
The added variable plot of the constructed variable for the power
family is given in Fig. 3.6.6. No single case seems to be greatly
influencing the transformation, although cases 5, 9, and 31 form a
group in the upper-left corner and may be jointly influential. Figure
3.6.7 gives a scatter plot of the Studentized residuals for the data with
LYas response versus the fitted values. Aside from showing that
cases 5,9,28, and 31 have absolute Studentized residuals larger than 2.
this plot is of little help. Case 9 has the largest Studentized residual. and
t , = 3.36; the Bonferroni p-value is 0.097. When the mean square for
pureerror is used to estimate IS', r, = 4.26. With this substitution, r ,
has a nominal t(l6)-distribution since case 9 is not replicated. The
corresponding p-value using the Bonferroni inequality is 0.019.
Constructed variable
Figure 3.6.6 Added variable plot for the score statistic, drill data
RESIDUALS A N D INFLUENCE IN REGRESSION
Fitted value
Figure 3.6.7 ri versus fitted values, drill data
Index plots of uii and Di are given in Fig. 3.6.8. Cases 9 and 31
have the largest potential and the largest influence, v,, ,
3 1 = 0.550, D9 = 1.49 and D 3 , = 0.63. In view of the relative
positions of cases 9 and 31 in Fig. 3.6.4, the high potential for these
cases should not be surprising. A probability plot of the Studentized
residuals gives no reason to doubt the assumption of normality.
At this point we delete case 9, examine the case statistics for the
reduced data, delete the most influential case, and continue sequentially
in this manner until the least squares fit seems well behaved. A summary
of this process, which ended with the deletion of cases 6,9, and 28, is
given in Table 3.6.5. From Fig. 3.6.4, cases 6 and 9 lie on the F axis on
opposite sides of the origin. Evidently, the second-order model is
unable to describe the observed thrust along this axis, particularly
outside of the central cube. Case 28 is one of two replicates on the
upper, back, right corner of the cube. The response for case 28 is
apparently much too small, judging from the fit of the model and the
response at the second replicate.
ASSESSMENT O F INFLUENCE
Case number
Case number
Figure 3.6.8 Index plots, drill data. (a) vii. (b) Di
As a check on the above sequential procedure, we computed t :and D,
for all possible I with m = 2 and 3. Table 3.6.6 gives the four largest
values of ttand D,
for m = 2 and m = 3. The most likely outlying triplet
contains cases 6, 9, and 28, as identified previously. The agreement
between sequential and simultaneous methods cannot, of course. be
guaranteed in general. The Bonferroni p-value for I = (6.9.28) is 0.003.
The most influential triplet is I = (9, 12, 31) with D,,. ,,.,,, = 10.84.
Table 3.6.5 Drill data. (a) Regression summaries
Case 9 deleted
Cases 9, 28 deleted
Cases 6, 9, 28 deleted Cases 9, 12, 31 deleted
Intercept - 6.7
* F for lack of fit 
ASSESSMENT OF INFLUENCE
Table 3.6.5 Drill data. (b) Case statistics
Case 9 deleted
Cases 9. 28 deleted
* df for nominal t, using available orthogonal pure error.
Table 3.6.6
Selected case statistics for m = 2 and m = 3,
drill data
Removal of this triplet will displace
to the edge of a 99.9997 %
confidence region. The least squares fit of the second-order model
without this influential triplet is summarized in the final columns of
Table 3.6.5 (a).
Of the five points identified in this analysis, four (6.9, 12, 31) are
single replicates on the D and F axes and two (6,9) of these four are
contained in the outlying triplet. Any analysis of these data will be
strongly dependent on the validity of these four cases and, unless the
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
precise form of the model is known, conclusions will be tentative at
best. At this point, little can be gained by further analysis of these data,
since conclusions must depend so heavily on the four unreplicated
points. Useful statements concerning the relationships between the
variables will require more experimental runs.
Box and Draper proposeadesign criterion that wiil help avoid
the ambiguity inherent in this analysis: To minimize the effects of a
small proportion of outlying responses on the fitted values, choose a
design to minimize the dispersion of the viis, Z (vii - O)'/n. For fixed n
and p', this is equivalent to choosing a design to minimize C oiZi since
C = p ' / t ~ is fixed. The design points in this example give min(vii)
= 0.104. max (oii) = 0.663, fi = 0.323 and C (vii - 0)~/11 = 0.0247. One
way that this design can be improved is to move 6 of the 9 center points
to replicate the previously unreplicated points, giving min (vii) = 0.190,
max ( t s i i ) = 0.382, O = 0.323 and C (vii - O)'/n = 0.003. Generally, it is
necessary to replicate the remote points in a composite design to gain
some robustness against out1iers.O
Alternative approaches to
'The path by which we rise to knowledge must be made smooth and beaten in
its lower steps, and often ascended and descended. before we can scale our \ray
to any eminence. much less climb to the summit.'
~ ~ E R S C I ~ F L . .
The diagnostic statistics presented in the last chapter share a common
heritage: they all depend on the same perturbation scheme. namely case
deletion, and they all use a sample influence curve to monitor changes
in the resulting analysis. These methods seem to have found wide
acceptance because of their intuitive appeal and computational simp-
licity. Other approaches to the problem of assessing influence can be
developed by altering either the method of perturbation, or by
changing the aspect of the analysis that is monitored. In this chapter we
look at several methods that do not depend directly on the influence
curve, but do use case deletion perturbation schemes. There are both
advantages and perils in these other approaches. A principal danger is
the possibility of designinga measure that has no firm theoretical basis;
a useful measure must refer to some specific part of the analysis and the
values of the derived statistics must be monotonic measures of what is
meant by influence. The main advantage in other approaches is the
possibility of monitoring factors other than changes in the location
estimates. The methods based on the sample influence curve. for
example, are largely insensitive to changes in estimated scale; other
methods can take an alternate view.
We consider three approaches to influence that generally meet the
requirements of the last paragraph. The first of these compares the
volume of confidence ellipsoids based on full and reduced samples,
thereby directly including changes in estimated scale in the measure.
The second related measure is due to Andrews and Pregibon ( 1 978) and
can be thought of as a general omnibus diagnostic. although it is
R E S I D U A L S A N D I N F L U E N C E I N RECiRESSION
weakly dependent on the structure of the regression problem. We then
turn to a Bayesian predictivist procedure in which predictive distri-
butions of future observations are compared. This method is more
comprehensive than the others, corllbining several aspects of the
analysis into a single measure. After a comparison of influence
measures, we briefly discuss methods that can be used to calibrate the
various influence measures.
4.1 Volume of confidence ellipsoids
One possible measure of the uncertainty in estimating a vector of
parameters is the volume of a corresponding confidence ellipsoid
 . This volume is also related to various
measures of design optimality with smaller volumes corresponding to
more informative designs. A reasonable measure of influence that
responds to this uncertainty or information is the change in volume
when a subset of cases is removed. Computation of this measure is
straightforward, since the volume of an ellipsoid is proportional to the
inverse square root of the determinant of the appropriate cross product
To obtain a general measure, reorder X so that the last q l
columns of X correspond to the coefficients of interest and partition
X = (XI, X,) with X, n x q. Similarly, define C = (0, I,), so JI = Cj? is
the coefficient vector of interest. A ( I - a) x 100 ';/, confidence ellipsoid
for + based on 6 = CP is
If a subset of nl cases indexed by I is deleted, then the corresponding
ellipsoid based on GI,, = c$,,,
The volumes of the two ellipsoids are
I/ol(l (+)) K (~C?~F,)~'~IC(X~X)-
l,'ol(cP,,,(+)) a (q6:,,~y)q121C
(X&X(I,)- CTIIi2
where we adopt the shorthand F, = F (1 -a; q, n - p') and F; =
ALTERNATlVE APPROACHES TO l N F L U E N C E
F(l -cr;q,n -p' -m). The logarithm of the ratio of (4.1.4)
to (4.1.3)
= flog IC(XJ,X,I,)-
IC(X%)-'CTI
The results in Appendix A.2 can be used to simplify (4.1.5);
(n - p l - r:)/(n -p' - m)and the ratio ofdeterminants can be shown to
where U = XI (X:Xl )- ' X:is the projection on the columns of X that
are not of direct interest, and U, and V, are m x m submatrices of U and
V, respectively. Combining these results into (4.1.5)
and simplifying
VR,(JI) = -4log)l - ~ , ] + $ l ~ g \ l - U , )
For m = 1, two choices for q are of general interest. First, if q = pi.
then C = I, 11-V,I = 1 -vii, 11-U,I = 1 and (4.1.6)
Apart from the ratio of F-values, this is equivalent to the statistic
COVRATIO given in Belsley et al. .
Alternatively, if the intercept
is ignored then C = (0, I,), 1 I - V, 1 = 1 - vii, )I - U, ) = 1 - 1 / 1 1 , and
(4.1.6) becomes
This form is recommended for general use in situations when the origin
lies well outside the region of applicability of the model. This will
happen often when the explanatory variables are not centered.
RESIDUALS A N D INFLUENCE IN REGRESSION
The log volume measure can be positive or negative. A negative
measure means that deletion of the cases indexed by I decreases volume
and hence increases precision. This will occur form = 1 if r; is large but
vii is small. A positive value of this ratio implies a larger volume for the
reduced data, and less precision. This will occur in general for m = 1
whenever oii is large. The volume measure seems to balance the effects
of the residual and the potential, and these in turn pull the measure in
opposite directions.
4.2 The Andrews and Pregibon diagnostic
A distinct alternative method for detecting influential cases in linear
regression was suggested by Andrews and Pregibon . Initially,
consider the effects of an outlier in Y and an outlying row of X
separately. First, the deletion ofa case corresponding to an outlier in Y
will tend to result in a marked reduction in the residual sum of squares.
The residual sum of squares, therefore, is a diagnostic for detecting
influential cases arising because of an outlier in Y. Second, as seen in
Section 4.1, the influence ofa row of X is at least in part reflected by the
change in IXTXI when the row is deleted. IS IX'XI changes substantially
when xi isdeleted, then thecorresponding case (y,, x:) will have a large
influence on ) or, minimally, ~ar()).
Andrews and Pregibon suggest that these separate diagnostics based
on change in the residual sum of squares and I XTX I be combined into a
single diagnostic based on the change in (n - p ' ) ~ ? ~
x I XTX I resulting
from the deletion of one or more cases. Specifically, they suggest the
as a measure of the collective influence of the cases indexed by I.
A form for R , which allows additional insight into its behavior can be
obtained as follows. Let X * = (X, Y), the matrix of explanatory
variables augmented with Y. From Appendix A.2,
IX*TX*I = IXTXIIYTY-YTX(XTX)-'XTYI
= ( n - P')C?~JX'XJ
ALTERNATIVE APPROACHES TO INFLUENCE
'Thus, (4.2.1) can be represented as
Several immediate observations can be made from this form. First, R, is
a unitless measure. Second, R; 'IZ - 1 corresponds to the proportional
change in the volume of an ellipsoid generated by X*T X* when the
cases indexed by I are deleted. Small values of R, correspond to
influential cases. Finally, R, is invariant under permutations of the
columns of X* and thus the vector of responses Y is not given special
recognition. For this reason, R,does not make full use of the structure
ofthe regression problem. If there is interest in particular aspects of the
problem, then it may be desirable to use other measures that reflect
those interests directly. On the other hand, R, may serve effectively as
an omnibus measure of influence.
Under normality, (n -p' - m)e;,/(n - P ' ) & ~ follows a Beta distri-
bution with parameters (n - p' - m)/2 and 4 2 , so R , is proportional
to a Beta random variable and reference values based on moments can
be easily calculated.
For comparative purposes, it is convenient to take minus one half the
logarithm of R,, which is
AP, = -$log(R,) = -4loglI-V,I+$log
This statistic will now be large for influential cases, and can be
compared to the analogous volume ratio based on a p'-dimensional
ellipsoid (4.1.7). The two statistics differ primarily by signs and relative
weights of the two terms, and by a factor of - l/(n - p' - r:) in the
second logarithm. If (n - p') is large enough to ignore this last factor,
these statistics use the same information but combine it differently.
The determination of R I for all subsets of m cases can be a
formidable computational task. Andrews and Pregibon (1 978) discuss
strategies for approaching this problem.
4.3 Predictive influence
In this section, we present a Bayesian method for assessing the influence
of cases on the prediction of future observations. The method,
developed by Johnson and Geisser , uses Kullback-Leibler
divergences to measure the difference between predictive densities
based on full and reduced data sets. The discussion here is restricted to
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
the linear model (2.1.1), although the technique is quite general and
applicable in many other situations. We first assume that a2 is known
and later extend the methodology to the more common situation in
which a 2 is unknown. The former situation is easier to study since the
corresponding analytic details are relatively uncomplicated.
K U L L B A C K - L E I B L E R D I V E R G E N C E S A N D P R E D I C T I V E
DENSlTlES W I T H fJ2 K N O W N
Let Y denote an 11-vector of random variables that can be represented
by the linear model (2.1.1) and assume that the errors E follow an
rt-dimensional normal distribution with mean 0 and covariance
0'1. N,(O, a21). Given the observed value y of Y, we suppose that the
goal is to predict a q-dimensional vector Y, of future observations that
are represented by the linear model
where E, is Nq (0, u2 I ), XJ is a q x p' known matrix of explanatory
variables and /? is the same as that in (2.1.1).
The predictive density for Y, given y, X, X,, and a2, is a standard
Bayesian tool for inference about Y, . Predictive densities are free of unknown para-
meters by construction. The mean and median of the predictive density
are obvious choices for point predictions while the spread and shape of
the predictive density reflect the uncertainty of prediction. To obtain
the predictive density, it is first necessary to find the posterior density of
the unknown parameter p.
Let f(.lp, C) denote the density for a N,(p, C) random vector.
Following Johnson and Geisser, we assume the improper prior p ( j ) d j
x d p for p. The posterior density p(j?ly) for p given Y = y is
The corresponding predictive density for Y, given y, X, X,, and a 2 is
As implied by the notation, this predictive density is N,(X,),CT~[I
+ X,(XTX)- ' Xj]) and is obtained by averaging the sampling density
of the future observations with respect to the posterior distribution of
A L T E R N A T I V E A P P R O A C H E S TO I N F L U E N C E
A useful property of the predictive density (4.3.2) is that it will
converge almost surely to the sampling density of Y, .
The influence of a collection of cases I on prediction can be
determined by comparing the predictive density based on the full data
to the corresponding density obtained after removing the cases in
question. From (4.3.2), the predictive density for the reduced data is
N,(x/ )(I,, aZII + Xf (X~,X,,,)-
'XJ]). Influence is reflected by chan-
ges in both the location and shape of the predictive density. Of course,
one way to compare these densities and thus assess influence is to
compare the locations and scales separately. This quite naturally leads
to developments similar to those in Chapter 3 and Section 4.1.
A comprehensive method for comparing predictive densities can be
based on the Kullback-Leibler measure of divergence, defined as
follows. Let g,, i = 1,2, be densities and let E, be the expectation
operator with respect to yi. The Kullback-Leibler divergence measure
d(gl, g,) is defined by
d(g1, g,) = El Clog(ll1 lg,)l = J log(y,lg,)8, (.u)dx
This measure will be positive if 8, and y, are different and will equal
zeroifg, = g,. Iff, = N,(p,, C,)and f, = N,(p2, C2),assumingthat
C1, C, are positive definite, it is not hard to verify that
The first term on the right of (4.3.4) corresponds to the distance
between centers of f, andj; relative to contours ofconstant density for
S,. The second term compares the volumes of ellipsoids based on the
two distributions and it will be zero only if the volumes are equal. The
third term, tr(C, C; I), may be conveniently viewed as a 'remainder'
that compares theeigenstructure ofC, to that of C,. For example. if X,
and C, commute and thus have the same eigenvectors, then tr( C, C; ' )
is simply the sum of the ratios of the eigenvalues of C, to the
corresponding eigenvalues of C,.
The predictive distributions for the full and reduced data sets
= N,(x,~, aZII + X,(XTX)-' X:])
and j',,, = N,(x, B ,,,, a2[I
+ Xf (X;,X,,,)-' XJ]), respectively. The Kullback-Leibler divergence
RESIDUALS A N D INFLUENCE IN REGRESSION
measure can be computed in two ways, depending on which of these
distributions is associated with j; and which with f, in (4.3.3). From
(4.3.4), we see that distance between centers is computed relative to &,
which suggests associating S, with the full data predictive density. We
adopt this idea and, following Johnson and Geisser, we call d(j;,,, f ) a
predictive influence function (PIF).
E X A M P L E 4.3.1.
P R E D I C T I V E I N F L U E N C E W H E N q = 1, ni = 1,a2
K N O W N .
Let x; = X,,vf = xT(XTX)-'x, and uif = xT(XTX)-'xi.
The predictive density / based on the full data for a single future
prediction at x, is N (x; j, a2(1 + v,)) and the corresponding density
Ai, based on the reduced data is N {xj:
a2[1 + vr + v$/(l - vii)]}.
Using (4.3.4) and after a little algebra, the PIF d( f(i,,f; x,) for a
single prediction at xf can be written as
where p$ = u$/uiiuf is the squared correlation between x:#and
Thus, the behavior of this PIF depends on Di(XTX, a'), p$, u,, and uii.
With a2 replaced by c2, the first term on the right of (4.3.5) is the same
as that obtained from a comparison of point predictions in the
frequentist approach discussed in Section 3.5; see (3.5.18) and the
subsequent discussion.
The second and third terms on the right of (4.3.5) depend only on
1 + pj, t~~~~l,/[(l
+ 0,) (1 - uii)], the ratio of the variance associated with
hi, to that 0f.f Since this ratio is always 2 1, the variance of the
predictive distribution cannot decrease when a case is deleted and a2 is
known. The change in variance will tend to be large when vii is large and
To use a PIF, it is first necessary to specify X,, the matrix containing
the points in the factor space that correspond to future predictions.
This is clearly a disadvantage since X, will not normally be known
during the development of the model. To overcome this problem and
thus make the PIFs more available for use as routine diagnostics,
Johnson and Geisser suggest using X in place of Xf. When
we will write d, for d$,,, f ) where f,,, = N,(x~,,,,
u2[1 + X(X;,X,,,)-'XT]) and f = N,(Xjl, a2(I + V)).
ALTERNATIVE APPROACHES TO INFLUENCE
To obtain a relatively simple form for dl, we substitute into (4.3.4)
term by term. First, the change in centers is
Thus, the distance between p l and p2 is measured by a member of the
class of norms of the SIC, D,(M, c), with a2 in place of 5*. Also, this
form is closely related to the influence curve for prediction obtained as
a result of a frequentist approach.
Next, the change in volume is measured by
Since V is a rank p' symmetric, idempotent matrix, the eigenvalues of
I + V are 2 with multiplicity p' and 1 with multiplicity II -p' and
11 + VI = 2P'. Next, using Appendix A.2 to evaluate the partitioned
form of II+X(X$,X,,,)-'XTI that results from the partition XT
= (X:,, X a , it follows that
JI+X(X;,X(I))-lXTI = 2p'II++VI(I-VI)-11
Combining terms, the change in volume can be obtained from the
determinant of a single m x m matrix,
that depends only on the eigenvalues of V,.
The final term of dl is
tr [I:, C;']
= tr [(I + X ( X ~ , X I I , ) - '
XT) (I + V ) - ' 1
= tr [(I + X(X$,X,,,)-' XT) ( I -$V)]
which again depends on the eigenvalues of V,. Finally, combining the
last three results, dl can be expressed as
dl = D,(XTX,4a2)-$logII+$V,(I-Vl)-'I+$
tr[V,(I-V,)-'1
The PIFd, depends on only el and V,. The predictive approach.
therefore, utilizes the same building blocks as the previous approaches.
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
The main difference is in how the predictive approach combines this
information to produce one overall measure of influence.
The form for dl given in (4.3.9) is perhaps the most useful for the
purposes of computation since all quantities are calculated from the
full data. For interpretation, however, the identity
is useful: tr [VI(I - VI)-'1 is proportional to the sum of the variances
oft he estimated values, based on the reduced data, at the cases indexed
by I. In addition, under a correct linear model,
E [DI(XTX, 4a2)] = $ tr [V,(I - VI)-'1
which is the average squared distance between the centers of the
ellipsoids associated with the predictive densities based on the full and
reduced data and is proportional to the expected potential discussed in
Section 3.6.1.
PREDICTIVE I N F L U E N C E FUNCTIONS, c2 U N K N O W N
When a2 is unknown, the predictive densities are multivariate Student
densities rather than multivariate normal. Let S,(v, p, X) denote an
n-dimensional Student density with v degrees of freedom, location
parameter p and dispersion matrix Z. Assuming the joint prior
p(8, 02)djldo2 cc o-2 djlda2 and setting XJ = X, the predictive
densities based on the full and reduced data sets are
~,(n-p', x ) , ~ ~ ( I + v ) )
s,(n - P' - m, xB,~), &:I)(]
X(X;,X{~))-' xT)),
respectively. Unfortunately, the PIFs based on these densities are
complicated and difficult to study. Johnson and Geisser use
normal densities to approximate the predictive Student densities, and
then develop the corresponding approximate PIFs along the lines
indicated above.
For v > 2, the covariance matrix for a multivariate Student random
variahle is [r/(v - 2)] C. It is reasonable to use
[I - pi - 2
ALTERNATIVE APPROACHES TO INFLUENCE
densities to approximate the predictive densities based on the full and
reduced data, respectively. The approximate PIF 2, can now be
developed by following the steps in the a2 known case. The terms that
measure the change in volume and eigenstructure depend on the ratio
d$,/d2 and thus on r:.
The approximate PIF may be written as
Apart from constants, the difference between dl and dl is in the
presence of k, in the former measure. Since kl is a decreasing function of
r:, it will be small when the cases indexed by I do not conforn~ to the
assumed model.
The special case m = 1 is informative,
Thus, di depends only on n, p', r;, and vii, and is a monotonically
increasing function of uii when n, p', and r: are fixed. With a, pl.oii fixed.
2, is a convex function of r:, and, if vii is small, the minimum of 4
occur with r; > 0. As a practical matter, the fact that di is not always
monotonic in r; may not be important, since the minimum will occur
for a very small value of ri.
If the Kullback-Leibler divergence is computed with the roles of the
full and reduced densities interchanged, the resulting measure is
somewhat morecomplicated. In particular, the part of the measure that
compares centers uses a metric that is different for each choice of i, and
thus is not directly comparable from case to case; see Johnson and
Geisser for further details.
RESIDUALS A N D INFLUENCE IN REGRESSION
An alternative to choosing X, = X
In Example 4.3.1, we discussed the PIF for a single prediction at xr
when m = l and aZ is assumed known. Here, we discuss the cor-
responding results for a2 unknown. Notation, unless otherwise
defined, follows that in Example 4.3.1. The predictive densities for the
full and reduced data sets are Student densities which may be ap-
proximated by normal densities as before. Let ai(x,) be the approxi-
mate PIF obtained using the normal approximation.
Using (4.3.4) and the result of Example 4.3.1, it can be verified that
pl(n-p' - 2)
2di ( x , ) =
+ k i p $ - -
1 -vii 1 + v ,
as before. The difference between (4.3.12) and the analogous expression
in Example 4.3.1 is in the presence of ki.
As indicated previously, the usefulness of ai(x,) as a routine
diagnostic is limited because of the requirement that x, be specified a
priori. Indeed, this limitation was the motivation behind Johnson and
Geisser's suggestion to use X, = X for routine checking. A potential
problem with this approach, however, is that ai(x,) may be large for
some points X r that are not adequately reflected by the diagnostic
resulting from setting X , = X. This can be overcome by using
(7: = max [ d i ( x , )] with the maximum taken over all possible values of
xf. so that, for each i, the PlF is evaluated at the point x/* where the
influence is maximized. This is the same as one of the approaches
used in the discussion of the frequentist approach to prediction given in
Section 3.5. If d r is small then it can safely be concluded that the i-th
case is uninfluential for any single prediction. The same conclusion
does not necessarily follow when ai is small, since there may exist points
for which d i ( x , ) is relatively large. 1f
is large then predictions around
x ; will be seriously influenced by the i-th case. Further investigation
may be necessary to determine the stability of predictions in other
I t is easily verified that a i ( x f ) is monotonically increasing in
pj, [1',/(1 + v,)] and that it depends on x , only through this term.
ALTERNATIVE APPROACHES TO INFLUENCE
Thus, to maximize d i ( x J ) by choice of xJ it is sufficient to maximize
P$ [ v r / ( i + v / ) ] . From Appendix A.3 it follows that
Substitution into (4.3.12) yields
[vii - l / ( n + I)] ] + ki - log (k,) - 1
The first term, which measures location differences, is proportional to
(3.5.21), the analogous measure from the frequentist approach. The
remaining terms are similar to those in di, but are adjusted to give
differential weights to the various components. For example, each of
the final three terms of d: is l / n times the corresponding term in 2,.
Each of the remaining terms in d: can be obtained from the
corresponding term in di by replacing i[vii/(l - u i i ) ] with Loii - 1/
(n+ 1)]/(1 - vii). These relationships suggest that di may be relatively
more sensitive to the removal of cases with large values of r,? while d:
will be more sensitive to cases with large vii.
A comparison of influence measures
Thus far, we have considered no less than four distinct types of
diagnostic statistics to assess influence, each with many variations. A
comparison of the various.measures can be useful. As representatives of
the normed influence curves, we will consider for m = I,
D, = Di(XTX, ~ ' 6 ~ )
and D: = Di(XTX, pi$;,). To represent the volume
ratios, we use VR: defined by (4.1.7) and the logarithm of the
Andrews-Pregibon measure AP, defined by (4.2.4). Finally, two
measures based on the Bayesian predictivist approach, di defined by
(4.3.11) and 2: defined by (4.3.14), will be compared. The major
omissions from this list are the measures that require specification of a
set of combinations of coefficients of interest for study and the
internally scaled measures. These latter measures may have different
behavior than the overall measures, depending on the structure of a
specific problem.
When cases are considered one at a time, all of these influence
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
measures are functions of r;, vii, and the constants, n, p', and, for the
volume ratio, a ratio of percentage points of F. Thus, all the statistics
use the same building blocks but combine the information differently.
The behavior of these statistics can be studied by comparing them for
various combinations of 11, p', vii, and r:. Figure 4.4.1 contains plots of
all six measures versus rii for 11 = 50, p' = 5, and a different value of rf
in each plot, r: = 0, 1 , 4, and 9, respectively. Since the statistics have
different calibrations, we compare the qualitative shapes of the curves
rather than their vi~lues.
When r f = 0, $- &, = 0, and both Di and D: are exactly 0 for all
values of oii. The other measures do not have this property, and all
become larger for vii large. The Andrews-Pregibon measure and the
volume measures behave like a constant times log (1 - uii), while the
predictive measures respond only to much.larger values of uii. For
r l = 1. the two distance measures Di and D: are identical and require
moderately large values of oii to exhibit influence. The volume and
Andrews-Pregibon measures are not sensitive to the increase from
r,' = O to riz = 1 and exhibit essentially the same behavior as in
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
Figure 4.4.1 Several influence measures. (a) r; = 0 (b) r: = 1 (c) r: = 4
(d) r: = 9
Fig. 4.4.1 (a). The predictive measures di* and di are quite similar to the
distance measure Di.
As ri increases from 1, the qualitative judgements made when r: = 1
continue to be valid but are more clearly displayed in Fig. 4.4.l(c). The
measures (I,*, di, Di. and D; all behave like Di, while the volume measure
and the Andrews-Pregibon measure behave similarly. For the volume
measures in Fig. 4.4.1 (c) and (d), if oii is sufficiently small V R ; is negative,
and it becomes positive as rrii increases. For example, V R ; is about - 0.5
at llii = 0.02 in Fig. 4.4.l(d), and increases to 0 at about vii = 0.65 and
then becomes positive. In this figure the trade-off between r? and vii in
the volume measure is clear.
In summary, for m = 1, the measures form two classes: those that
respond to rf and tlii essentially as Di does (Di, Di,di, d,*),and those that
are relatively insensitive to vii(APi, VR;). The former measures appear
to provide an appropriate balance between potential and residuals. At
least for ttt = 1, Di, the easiest of these to compute and to interpret,
seems preferable.
A L T E R N A T I V E A P P R O A C H E S T O I N F L U E N C E
For m > 1, the comparison between the ~nfluence measures 1s much
more complicated, but some general comments are in order. Flrst. the
volume measures VR; and AP, depend only on (11 - p' - r:)/(11 - p'), the
eigenvalues of V,, and constants. If 11 - p' is large. these measures are
relatively insensitive to r:. The measures Dl. D',, dl,and df" all hate a lead
term like D, and hence they behave similarly. These depend not only on
rfand the eigenvalues of V,, but also on the orientation of the vector el
ofresiduals in an appropriate geometry. Thus, two groups of cases wlth
identical r:and eigenvalues of V1 need not have the same influence as
measured by Dl. For these measures, then, the notion of an influential
subset is more complicated, and the discussion of potent~al in
Section 3.6.1 is relevant.
Draper and John conducted a detailed exarninatlon of the
relative merits of APl and Dl. In addition to showing that A P l may
isolate cases that are not outliers or influential for parameter estim-
ation, they show by example that the reverse may also happen: The
Andrews-Pregibon statistic cannot be guaranteed to locate outliers or
cases that are influential for B. They recommend the study of rf
(essentially their Q,), 11 - VII. and Dl. In the larger class of statistics
discussed here, it is clear that their advice is sound, although thelr
choice of potential measures I I - V,I may be replaced by one of those
discussed in Section 3.6.1.
E X A M P L E 4.4.1.
CLOUD S E E D I N G NO. 11. Table 4.4.1 lists
several of the influence statistics discussed in this chapter for the cloud
seeding data; see also Table 3.5.5. The subset IC/ is chosen as in
Example 3.5.3. The important observation from this table is that the
ordering of cases on influence is different for the various statistics.
Computation of them all can lead to confusing conclusions. A more
reasonable approach is to adopt one of the measures - possibly Di - as
the standard and use additional measures as called for by specltic
concerns.O
Calibration
The various influence measures discussed in this and the previous
chapter each provide a way of ordering individual or groups of cases
based on their impact on a selected characteristic of the analysis.
Experience with a given measure will provide additional insight that
can be useful for an understanding of the importance of its magnitude.
Beyond this, however, there are only a few methods of calibration
KfiSI1)UAI.S AN11 1NE'L.UENCE I N R E G R E S S I O N
Table 4.4.1
More i~lfilrrrli-e ~~tet~stires,
clotit1 seetlirlg d i ~ r c ~
available. As mentioned previously, many of the measures of the form
D,(M, c) can be monotonically transformed to a more familiar scale
that does not depend on 11 and p' by comparing DI(M, c) to the
percentage points of the appropriate F-distribution. For example, the
knowledge that the removal of' case 1 would move the least squares
estimate of /I
to the edge of a 95 0/, confidence region while the removal
of case 2 would move the same estimate to the edge of a 5 "/, region is
surely more useful than just knowing that case 1 is more influential
than case 2. In addition, half-normal plots with a simulated envelope
(see Sections 2.3.4 and 3.5.3) can be used in combination with any of the
influence measures to help avoid problems of overinterpretation. These
techniques are intended as aids to interpretation and not as found-
ations for accept-reject rules or p-values.
Dempster and Gasko-Green suggest methods for sequen-
tially removing individual cases and determining conditional p-values
A L T E R N A T I V E APPROACHES TO I N F L U E N C E
that can be used to formulate stopping criteria. Their methods are
based on the repeated application of a selection rule to determine the
most discrepant case at each stage. The class of available selection rules
is large and includes many of the influence measures discussed
previously. Belsley et ul. discuss other methods such as the use of
gaps for determining the cases that require further attention.
Assessment of influence in other
'In the study of nature. we must not, therefore, be scrupulous as to how we
reach to a knowledge of such general facts: provided only we vcrify them
carefully when once detected. we must be content to seize them wherever they
are to be found.'
l1ERSCIIEL, Op. c ~ I .
Most of the methods for the analysis of influence presented in earlier
chapters depend on the elegance of the linear least squares regression
problem. The use of the sample influence curve to measure influence is
aided by the algebraic updating formulae in Appendix A.2 that allow
computations to be done from full sample statistics; interpretation of
normed influence measures is made clear by appeal to elliptical
contidence regions that characterize linear least squares regression.
As mentioned in Section 3.4, the extension of the sample versions of
the influence curve to other problems is conceptually straightforward.
As a practical matter, however, the use of these ideas can be expensive
since exact updating formulae are generally lacking. To compute
sample influence curve for a parameter 8, for example, values of
B,i,, i = l,2, . . . , n, in addition to the complete data estimate are
needed, and each of these may require iteration. In addition, the
definition of a residual and the choice of a norm can be troubling.
Norms of the sample influence curve based on elliptical contours will
not always be appropriate.
In this chapter we discuss ways in which diagnostics for linear least
squares regression might be extended to more complex situations. In
the next section we present a general definition of residuals and suggest
an extended version of the viis. A general approach to influence,
including likelihood-based measures, is discussed in Section 5.2. A
relatively inexpensive approximation of the sample influence curve is
suggested and this in turn leads to the problem ofjudging the accuracy
ASSESSMENT OF I N F L U E N C E IN OTHER P R O B L E M S
of the approximations. Sections 5.3-5.5 contain discussions of non-
linear least squares, logistic regression and robust regression, respect-
ively. We comment briefly on several other problems in Section 5.6.
The general purpose of this chapter is to suggest ideas rather than
specific diagnostics. Except perhaps for logistic regression. the specific
methods presented have not been studied in great detail and more work
is required before definite recommendations can be given.
A general definition of residuals
Cox and Snell define residuals for a fairly general class of models
and suggest a method for determining their first two moments. This in
turn leads to a generalization of some of the diagnostics for linear least
squares regression to more complex models.
Assume that the i-th response yi is a known function g, of an
unknown parameter vector 0 and an unobservable error ci.
The errors ci are assumed to be continuous, independent and identically
distributed with acompletely known distribution, so location and scale
parameters are not distinguished. This formulation excludes some
standard models such as time series and components of variance
problems where the response may depend on the errors in a more
complicated way.
Assuming a unique solution for E ~ , (5.1.1) may be re-expressed in the
ti=hi(yi,O),
i = 1 , 2 , . . . , 11
Cox and Snell define the i-th residual ii by
where 8is the maximum likelihood estimate of 0. We call E , a rrlarimu,n
likelihood residual (Cox and Snell call it a crude residual).
Suppose, for example, that (5.1.1) is the usual linear model written as
yi = X : ~ + U E ~ ,
are independent, identically distributed
normal random variables with E(ti) = 0 and var (E,) = 1. If
eT = (PT, 4,
ti = ()li - xTj)/&
= ei/(Cef / t l ) ' I 2
RESIDUALS A N D INFLUENCE IN RECiRESSlON
and ii is a standardized version of the ordinary residual e,. (In this
chapter, 22 is the maximum likelihood estimator of oZ.)
In general, moments of the maximum likelihood residuals cannot be
obtained explicitly. Useful approximations to E(ii) and E(i;) can,
however, be obtained from a quadratic expansion of (5.1.3) about 0,
ii 2 ci + (6 - o ) ~ H ~ ( o )
+ 4 (8 - o ) ~ H ~ ( o )
where Gi(0) is a q-vector with elements ahi(yi, O)/aOj, and ~ ~ ( 0 )
q x q matrix with elements d2hi(yi, O)/aOjaOk, both evaluated at 0.
Expressions for EPi, var (Pi), and cov ($ Pj) in terms of ki(e), Hi(0), the
expected information matrix, the score vector and the second-order
bias of the ML estimator 8 are given by Cox and Snell; see also Cox and
Hinkley . These expressions take the form
E(.$) = E(ci)+ai
var (ii) = var (ci) - cii
cov (i,, ij) = cij
In all but the simplest situations, the determination of the ais and cijs
will require a considerable amount of tedious algebra.
In the usual linear regression model, the expressions in (5.1.5) are
exact. One finds that E(q) = ai = 0, var (ci) = 1, cii = n(vii - pl/n)/
(n - p') and cij = - nvij/(n - p'), i # j. Generally, we expect that the cijs
can be viewed as extensions of the oijs and used as diagnostics in an
analogous manner.
Using (5.1.5) a Studentized version 6: of the ML residuals can be
defined so that E(&f) = E(ci) and var (6:)
= var (ci) to order l/n. The
motivation for this is analogous to that for the ris in linear regression:
provide a better reflection of the cis and plots can be interpreted
without the complications caused by nonconstant means and variances.
E X A M P L E 5.1.1.
L E U K E M I A DATA NO. 1. Leukemia is a type of
cancer characterized by an excess of white blood cells. At diagnosis, the
count of white blood cells provides a useful measure of the patient's
initial condition, more severe conditions being reflected by higher
counts. Feigl and Zelen discuss the use of the white blood cell
count as an explanatory variable in models to predict survival time after
diagnosis.
Feigl and Zelen report the survival times in weeks and the
white blood cell counts for a sample of 33 patients who died of acute
leukemia. In addition, each patient was classified as AG positive or AG
Table 5.1.1
Leukeniia data, y = survival time in weeks, W C = white blood cell count, and related staristics for 17 parients
diagnosed as AG positive. Source: Feigl and Zelen 
D! ,
where yi is the survival time for the i-th patient, ei, . . . . E, are
independent, standard exponential random variables, and if xi is the
(base 10) logarithm of the i-th white blood cell count, xi = xi - f'.
The log likelihood L(O,, 02) is easily found to be
02) = - n log (0,) - x y i exp(- 02xi)/U,
and the expected information matrix is
Selecled contours of constant L(O,, U2) are plotted in Fig. 5.1.1 (the
points plotted in this figure will be discussed later). The maximum
likelihood estimates 6, = 51.109 and 6, = - 1.1 10 were determined
using Newton's method.
The ML residuals defined by
= yi exp ( - 6,xi)/0,
are given in Table 5.1.1. Case 17 has the largest residual, 2, ,
= 3.47. If
are treated as a sample from a standard exponential distribution,
the residual for case 17 is not large, since the probability that the largest
order statistic exceeds 3.47 is 0.42. Of course, the ML residuals do not
have constant expectation or variance, and it is possible that a
Studentized version would be more revealing.
For the M L residuals defined by (5.1.8) Cox and Snell provide the
approximate moments.
E(&) = 1 +-++(xi C.Y~
- x,? Cxf )/(x,xf )"
var (ii) = 1 - -- + (xiC.~3 - 3.:
) / ( C X ~ ) ~
ASSESSMENT OF INFLUENCE IN O T H E R P R O B L E M S
Figure 5.1.1
Likelihood coltours for leukernla data '+' lndlqtes
OT = (51.109, - 1.1 10) with L(O)= -83 88.' x ' ~nd~cate
el,, '1.' ~nd~catesO,',-,
With the summations fixed, both E ( i , ) and var(i,) are quadratlc
functions of x, with maxima occurring at the values of u, that are closest
to Cx:/2Cxf and Cx:/6Cx;, respectively The values of u, and c,, are
given in Table 5.1.1. As expected, the values of c,, are largest at the
extremes with c,, = 0.77 the maximum. In analogy with linear
regression, case 2 may have a substantial influence on the M L estimates.
Cox and Snell construct a Studentized version E: of the ML residuals
by using (5.1.9) and (5.1.10) in combination with the transformatlons
E: = {i,/(l --l,))'+k~.
Assuming that 6: has a standard exponent~al
distribution, it can be shown that appropriate transformat~ons are
givenbyl,=-0.21c,i-1.43a,andk,=~(2a,+c,,),r=1.2 , . . . , 11.
The values of ci* are also given in Table 5.1.1. The largest difference
between 2, and E: occurs at case 17, cr, = 4.18. The chance that 4.18
would be exceeded in a sample of 17 from a standard exponent~al
distribution is 0.23 so that there is still no reason to suspect case 17 as an
The plots of the Studentized M L residuals versus the expected order
statistics from a standard exponential distribution and the plot of E:
versus xi give no reason to question distributional assumptions, or to
diagnose general failure of tile model. Overall, the solution seems well
behaved to this point.0
5.2 A general approach to influence
For ease of presentation, we shall continue to use the model described
at (5.1.1), although the ideas to be discussed in this section
are applicable to other paradigms as well. In particular, it is no
longer necessary that the errors be expressible in the form given
at (5.1.2).
Measures of the influenceofthe i-th case on the MLestimate 6can be
based on the sample influence curve SICi cc 6 - 6(i,, where 4,) denotes
the ML estimate of Ocomputed without the i-th case. While this idea is
straightforward, it may be computationally expensive to implement
since n + 1 ML estimates are needed, each of which may require
iteration. When faced with this expense, it may be useful to consider a
quadratic approximation of L,,,, the log likelihood obtained after
deleting the i-th case:
(6) + (0 - ~ ) T L ( , ) (6) 4- +(o - ~ ) T L ( ~ )
(6) (0 - a)
where L,,,(O^)
is the gradient vector with j-th element aL,,(0)/aOj
evaluated at 0 = 6 and Lfi,(6) has u, k)-th element a2L(,,(0)/aOj aO,,
evaluated at 0 = 6. If - ~ ( ~ ~ ( 6 )
is positive definite, the quadratic
approxin~ation is n~aximized at
We refer to 6:i, as a one-step approximation to 6,,, since it is the sameas
would be obtained by a single step of Newton's method using O^ as
starting values to maximize L,,,(O) .
If 8,,, is not too different from 6, and L,,,(O) is locally quadratic, the
one-step estimator should be close to the fully iterated value. For cases
that are influential, 6- 6,i, is 'large', the accuracy of the one-step
estimator is likely to be lower, but an accurateapproximation to 6,[) will
not he needed as long as 6 - b:,, is sufficiently 'large' to draw our
attention for [t~rttler consideration.
A S S E S S M E N T O F I N F L U E N C E I N OTHER PROBLEhZS
In the linear least squares problem, elliptical norms of the sample
influence curve provide a sufficiently rich class of rnetrics for ordering
cases on influence. In more general problems, this class can be overly
restrictive, especially if elliptical confidence contours are not appropri-
ate. If we let t ( 0 ) be a function of the q-vector 0, then a general measure
can be viewed as any function m(r (e), t(8,,,)) that maps into the
positive real line. Most of the alternative methods for assessing
influence given in Chapter 4, for example, can be expressed as members
of this general class. However, since m(t(8), t(b(,,)) is not in general a
function of the sample influence curve, the theoretical foundations for
influence measures derived from the influence curve may be lacking.
Before any alternative measure is to be adopted. its logical foundation
must be carefully studied.
An important example of the general measure is derived from the use
of contours of the log likelihood function to order cases based on
influence. Let L(0) be the log likelihood based on the complete data. We
define a likelihood dista~~ce
or, using the one-step estimator,
This is easily seen to be in the general class with t (0) = L(8), and LDi is
not necessarily a function of just the sample influence curve for 9.
The measures LDi and LD! may also be interpreted in terms of the
asymptotic confidence region 
where xZ(a; q ) is the upper a point of the chi-squared distribution with
q df, and q is the dimension of 0. LDi can therefore be calibrated by
comparison to the z 2 ( q ) distribution.
lfthe log likelihood contours are approximately elliptical, LD, can be
usefully approximated by Taylor expansion of L(8,,,) around 8.
~ ( 6 ( , , )
+ (O(,) - 8)TL(8) + i(8(,)
- 8 ) T ( ~ ( 8 ) )
(8(,) - b)
and, since t ( 8 ) = 0,
LDi 2 (aci, - 8)T ( - ~ ( 8 ) )
(8(i) - 8 )
A different approximation can be obtained by replacing the observed
information - ~ ( 8 )
in (5.2.5) by the expected information matrix.
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
evaluated at 6. Either of these approximations, however, can be
seriously misleading if contours of L(8) are markedly nonelliptical.
The likelihood distance can be easily modified to accommodate
situations in which a subset 8 , of 8 is of special interest. Let
OT = (O:, 8:) and 8?;, = (a:,,,, 8:,i)). An asymptotic confidence region
for 8 , is given by
where 9 , is tlie dimension of 8, and
denotes the log likelihood maximized over the parameter space for 8,
with 8 , fixed . The asymptotic
confidence region measure of the displacement of 8; when the i-th case
is deleted is now
LDi(91l92) = 2 [ ~ ( 8 ) -
~ ( f i , , i ) ,
82(8;,i)))l
= 2 ( ; ~ ( 8 )
- max [ ~ ( 8 , , , , ,
with a similar measure obtained if one-step estimators replace fully
iterated ones. This measure is compared to the x2(q1) distribution for
calibration.
As an illustration, consider again the usual linear model Y = XB
+ as, with the ei assumed independent, identically distributed N(0, 1).
If a = a, is known, it is easy to verify that
If a2 is unknown but is of special interest, (5.2.6) provides the desired
measure with 8, = 8 , 0, = a'. One finds
L((B, a2 ((B)) = - - log [27ra2 (B)] - -
where a 2 ( 8 ) = C ( y j - xf B)'/n. Setting B = &,,
ASSESSMENT OF INFLUENCE IN OTHER PROBLEMS
Since LDi(Bla2) is a monotonically increasing function of D, it is
equivalent to Di. Finally, the likelihood distance for (/I, a2) is found to
LDi(/I, a2) = n log (i?$,/C2) + ( y i - x T & ~ ) ) ~ / ~ $ )
Interestingly, this expression is guaranteed to be monotonically
increasing in tf only if the model contains a constant. For fixed n, p', and
vii, LDi(B, a ) is minimized at tf = (n - p' - 1 ) ( 1 - nvii)/(n - 1 ) which
may be positive if regression is through the origin.
E X A M P L E 5.2.1.
L E U K E M I A DATA N O . 2. The individual points
plotted in Fig. 5.1.1 represent 0;) = (8,(i),
&)), i = 1 , 2, . . . , 17, for
the leukemia data discussed in Example 5.1.1. Only case 17 deviates far
from the full sample ML estimate: 6:, ,, = (41.920, - 2.184), while the
full sample ML estimates are 8, = 51.109 and 8, = - 1.1 10. The
likelihood distance measure for case 17 is LD,, = 9.89. Comparing this
value to the percentage points of a x2(2) distribution indicates that the
removal of case 17 will displace 8 to the edge of a 99 % asymptotic
confidence region.
The values of LD,, i = 1, 2, . . . , 17, are given in Table 5.1.1. The
second largest value of L D , LD,, = LD,, = 0.35, indicates minimal
movement so that case 17 is the only individually influential case.
Recall from Example 5.1.1 that case 17 has the largest ML residual,
but there was insufficient evidence to reject it as an outlier. The
influence of case 17 seems to be due to its large ML residual in
combination with the relatively large value of cii. An inspection of the
original data reveals that case 17 corresponds to a patient with a very
large white blood cell count (100000) who survived for a relatively long
time. Feigl and Zelen mention that high white blood cell counts
are unreliable so a measurement error in x,, may be contributing to the
influence of case 17. In any event, conclusions based on such data
should be viewed skeptically.
In the preceding discussion, the fully iterated estimates 8(i) were used,
but the one-step estimates computed from (5.2.2) would have served as
well. When superimposed on Fig. 5.1.1, the one-step estimates for i
= 1,2, . . . , 16 are nearly indistinguishable from the cloud of points
around the maximum of the log likelihood. The only noticeable
R E S I I > U A L S AN11 I N F L U E N C E I N R E G R E S S I O N
disagreement occurs at case 17. The one-step estimate 8:, ,, falls at the
point indicated by a 'star' in the lower left-hand corner of Fig. 5.1.1.
Since 6:,,, is farther from 8 than 6(,,,, L D f , would still be large.
Finally, the quadratic approximation to LDi given at (5.2.5) would
probably work well in this example since the log likelihood contours
are nearly elliptical; the approximating elliptical contouk and the one-
step estimators were given in Fig. 3.5.1. However, the elliptical
approximation is not always applicable since even in this example it is
possible to transform the parameters to get clearly nonelliptical
contours for the log likelihood function.[ll
5.3 Nonlinear least squares
The nonlinear regression model is given by
where ,/'(xi, 0 ) is a scalar-valued function that is nonlinear in the
q-vector of unknown parameters 0, and the cj are independent and
identically distributed N(0, 1). For this problem, the maximum like-
lihood estimate 8 of 0 can be obtained by minimizing the residual sum
of squares,
The problem of determining 8 can be treated as a special case of the
general unconstrained maximization problem, although special
methods that use the fact that G (0) is quadratic are often appropriate;
see Kennedy and Gentle .
The problem of assessing influence in the nonlinear least squares
problem can be approached using the general methods outlined earlier
in Sections 5.1 and 5.2. In particular, one-step estimators 8;i, of the
vectors 8,,, that minimize the objective functions
( y j - f'(xj,
can be found by application of the result given by Equation (5.2.2).
However. particularly interesting results can be obtained if we allow a
further approximation. We suppose that, in a neighborhood about 8,
,l'(s,. 0) is approximately linear,
f ( x j . 0) z . f ( x j , 0) + iT(0 - 6)
where 2; is the j-th row of the t~ x q Jacobian matrix Z.
If theapproximation (5.3.4)issubstituted into G,i, (0) defined by (5.3.31.
the resulting objective function is minimized at
where e is the )I-vector with elements e, = yj-j'(xj. 8). This form
corresponds to that obtained by using a single step of the
Gauss-Newton method .
The last equation is simplified, with the aid of Appendix A.2 and the
fact that z T e = 0 to give a more usual form. Defining i.,,
=if (zTz)-lii,
we find 
When this particular algorithm is used to produce the one-step
estimators, the nonlinear least squares problem is essentially replaced
by a linear one, with the role of X taken by Z. Most of the diagnostics
and residual analyses for linear least squares may be expected to apply
at least approximately in nonlinear least squares. In particular. an
approximate Studentized residual is
where G2 = G (8)ln. An elliptical norm of the sample influence curve is
qr?2) = (6 - 6 ( i , ) T ( ~ T ~ )
(8 - 8(i,)/(162
When 6(,, is replaced by the one-step approximation 8,'i,. this norm
In this and the following two sections, we continue to use Di(.. . ) to
denote an elliptical norm. The parameter under consideration should
be clear from context. One step versions will be denoted by D f
The use of elliptical norms for influence, whether based on one-step
or fully iterated estimates, may be inappropriate for some nonlinear
R E S I D U A L S A N D I N F L U E N C E IN REGRESSION
problems if G(0) has markedly nonelliptical contours. In many
problems, elliptical confidence regions can be badly biased . and the bias may depend on the parameterization chosen for the
model .
The problem of choosing a parameterization can have important
emects on the analysis of influence.
Alternative norms for 6 - 6,',, or f? - f?(,, that are less dependent on
the shape of contours of G(0) can be suggested, but these will require
considerably more computation. The first of these norms is derived
from the form for IIi given by (3.5.6) as a norm of the change in the
vector of fitted values. For the nonlinear regression problem, this
(j(xj, 6) -J(x~, 6;,,)1~
When j'(xj, 0) is exactly linear in 0, Di and FD! are proportional;
otherwise, they may be quite different. When the parameterization of
the model is at issue, FD! may be the preferred statistic since it depends
on the parameterization only through approximation of &,,,. If i$, is .
used in place of 8/,,, FD, is invariant under choice of parameterization.
Finally, we consider measures derived directly from log likelihood
displacement. With reference to the (q + 1)-dimensional contours for
(0. u2). the measure is
where G,,,(O) = G(0) - (y, -/(xi, 0))'. When 0 alone is considered, the
resulting measure from (5.2.6) is
LD ! (0 1 u2) = n log [G (&:,,)/G discusses a set
of artificial data with t~ = 24, and for which the appropriate model is
[exp ( - 02xj) - exp(- 0, xi)] + asj
ASSESSMENT OF I N F L U E N C E I N O T H E R P R O B L E M S
The data are given in Columns 2 and 3 of Table 5.3.1. The rernalning
columns of the table give ei, ?,, Ci,, D~(z'z, 2?), FDli, and the two
likelihood distances computed from one-step estimates given the
maximum likelihood estimate bT = . From these statls-
tics, case 9 appears as a candidate for a possible outlier, and 11 is clearly
influential in this problem by any of the measures, assunllng that the
one-step approximation is adequate. To explore the adequacy of the
approximation, we have computed the fully iterated estin~ators 0,,, for
each i, using the modified Gauss-Newton algorithms with 6 as the
starting value. No more than three iterations were required to obta~n
about four-digit accuracy on 8,,,. The correspondence between 8,,, and
was very good, with the largest deviation occurring for case 9.
Figure 5.3.1 is a contour plot of G(0) for this problem, with the fully
iterated 6(i, added to the plot. In addition 8:,)
is indicated.
Figure 5.3.1 Contour plot of
), Duncan's data. '+' indicates br =
 , where G(0) = 0.
79. The points plotted are B,i,. The point
at the '*' is 8&)
Table 5.3.1 Duncan's data and related statistics. Source: Duncan 
LD) (8, oZ)
LD! (8102)
* Given as 0.25610 by Duncan
ASSESSMENT OF INFLUENCE IN OTHER PROBLEMS
At least for this one problem, we have found that the one-step
influence measures provide the same qualitative information as the
fully iterated ones and an influential case is clearly identified. In some
problems where the G(0) surface is less well behaved, we should expect
that the one-step procedures will not work as well. Further research
and experience with these methods is required.0
Logistic regression and generalized linear models
Although the logistic regression model does not fall in the general
framework for residuals given in Section 5.1, the results for the
assessment of influence given in Section 5.2 can be applied. We first
consider influence assessment, and then present several alternatives for
defining residuals.
Consider a sample yT = ( y , , y2, . . . , y,,) of independent random
variables such that yj is binomially distributed B(nj, pj) with nj known
and pi unknown. The logistic regression model specifies the
. relationship
qj = logit ( p j ) = log [ p j / ( l - pj)] = xf 8,
j = 1 2 . . . , n
where x , , x,, . . . , x, are p'-vectors of explanatory variables and 8 is an
unknown parameter vector. In such models, estimation of Pis typically
a major concern.
The log likelihood for 11 = X/? is
L(tt) = L(XB) = C [~jxjTB-aj(xjTB) + b j ( y j ) l
where aj(z) = nj log [ l + exp (z)] and bj(z) = log
likelihood estimate /? of /? is often found using ~ekton's method.
Once )is obtained, a one-step estimator &,of $ti, can be found using
the general results of Section 5.2. Following Pregibon , but using
different notation, define $j = exp (xf'P)/[l -I- exp ( x J P ) ] and let w be
an n x n diagonal matrix with j-th diagonal n j j j ( 1 - Cj). Also, let 5 be an
n-vector with j-th element ij = yj - n j j j . One can show that
L ( ) = x 5 ;
L,,) (6) = - (x; w ( ~ ) x,,, )
so that, using (5.2.2) and Appendix A.2,
Bti) = B + ( X B ~
x(i))- ' XTo)5(i)
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
q = W ' I ~ X
I xr\i/ l12. Pregibon discusses the accuracy of this
one-step approximation and concludes that componentwise the ap-
proximation tends to underestimate the fully iterated value, but that
this may be unimportant for identifying influential cases.
Measures for the differences b- B,,! or B- Bi, can be derived using
elliptical approximations, likelihood displacement, or changes in fitted-
value vectors as discussed in the last two sections. Following Pregibon
 , however, we will consider only the first of these,
D! (XTWX, p') = -
p' }lipi (1 -pi)
( I - vii)2
to characterize influence for logistic regression (Pregibon's measure c!
differs from (5.4.5) only by the factor p'
in the denominator).
Comparison of (5.4.5) to Di suggests that i;/[nifii(l - fii)(l - ;,,)I and
ijii may be interpreted and used in the same way as r: and vii in linear
regression.
Residuals for logistic regression can be defined in many ways.
Equation (5.4.5) and the analogy with linear least squares suggests the
quantities
xi = &/[niji(l -pi)] 'I2
Landwehr, Pregibon and Shoemaker and Pregibon use
an alternative set of residuals based on individual components
of the log likelihood ratio or deviance statistic, dev = -~[L(xB)
- L(logit (yipi))], where L (logit (yi/ni)) is the log likelihood obtained
when each rli is estimated by logit (yi/ni). The deviance has an
asymptotic x2 (,I - p') distribution. Components of deviance are de-
dev, = 3- ,/2[li(logit (yi/ni)) - li(x7 fi)lii2
where li (q) = yiq - ai (q) + bi (yi) is the log likelihood based on the i-th
case only, and the plus sign is used if logit(yi/n) > x f B and the minus
sign is used otherwise. Clearly, dev = ~ d e u f .
Landwehr et al. 
advocate the use of deoi in graphical procedures.
Finally. Cox and Snell suggest a somewhat more complicated
set of residuals based on a transformation to normality proposed by
Blom . Let
+ ( ~ ) = J ~ f - ' / ~ ( l - t ) - " ~ d t ,
The quantity b(u)/#(i) is the incomplete beta function 1,(2/3,2/3). The
ASSESSMENT OF INFLUENCE IN OTHER P R O B L E M S
i-th residual is then
Cox and Snell state that this set of residuals has essentially normal
behavior, even for ni as small as 5 and pi = 0.04. Estimates of the
variances of these residuals are given by Cox and Snell.
E X A M P L E 5.4.1.
LEUKEMIA DATA NO. 3. The data for all 23
patients are given in Table 5.4.1 in a form appropriate for fitting logistic
Table 5.4.1
Leiiket~liu tltrto$)r logistic regression. Sot~rce: Feigl
arid Zeler~ 
RESIIIUALS A N D I N F L U E N C E IN REGRESSION
models. We take the response y to be the number of patients surviving
at least 52 weeks for each combination of WBC = white blood cell
count, and AG = 1 for AG positive patients and AG = 0 for AG
negative patients. The five patients with WBC = 100000 are collapsed
into two groups, one (case 15) consisting of the three AG positives (with
one survivor) and one (case 30) consisting of two AG negatives (with no
survivors),
The usual summary statistics obtained from fitting the model
logit ( p j ) = Po + P, WBC + /I2
are given in Table 5.4.2(a). The deviance has the value 27.24 with 27 df.
There is no indication from this summary that the model is grossly
inadequate.
Table 5.4.2 Logistic regressiotl summaries, Leukemia data
((1) F ~ l l l dura
(b) Otle cnse removed
Asymp. s.e.
Asymp, s.e.
Index plots of the xi, the diagonal elements of
and D! (XTWX, p')
are given in Figs. 5.4.1-5.4.3, respectively. Clearly, case 15 is unusual
and may be seriously influencing the fit. From Fig. 5.4.1, x 1 5 is not
unusually large and thus the influence of case 15 is apparently due to its
relative position in the factor space. Case 15 consists of the results for
t ~ , , = 3 AG positive patients with WBC = 100000. The fact that one of
these patients survived for a relatively long time is surely contributing
to the influence of this case.
To understand the role of case 15, we could refit the model after
removing either all three patients in case 15 or just the suspect patient
(patient 17 in Table 5.1.1). For these data, both alternatives lead to
essentially the same revised fit. Table 5.4.2(b) summarizes the fitted
model after the removal of patient 17 or, equivaiently, modifying case
15 hy setting y , = 0 and t l l = 2. The summaries for the full and
A S S E S S M E N T O F I N F L U E N C E I N O T H E R P R O B L E M S
Case number
Figure 5.4.1 xi versus case number, leukemia data
Case num bet
Figure 5.4.2
Cii versus case number. leukemia data
reduced data in Table 5.4.2. are clearly quite different. This difference is
further illustrated in Fig. 5.4.4 which gives plots of the fitted survival
probabilities versus W'BC and AG for the full and reduced data.
Surprisingly, the removal of patient 17 increases the estimated
probability of survival for patients with small values of WBC. The
influence of case 15 is certainly overwhelming.
Case number
Figure 5.4.3 D! (XT w X, p') versus case number, leukemia data
Figure 5.4.4 The fitted probability of survival as a function of AG and WBC,
leukemia data
ASSESSMENT OF INFLUENCE IN OTHER PROBLEMS
The influence of patient 17 is of course dependent on the assumed
form of the model. One reasonable alternative to model (5.4.8) is
obtained by adding the interaction variable WBC x AG to allow for the
possibility that the slopes for the AG positive and AG negative groups
may differ. For these data, however, theaddition of the interaction term
does not lead to a significantly improved fit. For the full data the
asymptotic t-value for WBC x AG is 0.88 and case 15 is still the only
influential case. After the removal of case 15, the 1-value is 0.38 and no
single case is seriously influential.
As another alternative, we could transform WBC via a log transforrn-
ation, as was done in Example 5.1.1. When this alternative is pursued,
the importance of case 15 is lessened. For example, D:, (XT
= 0.47, and the fitted models with and without patient 17 are not as
different, as illustrated in Fig. 5.4.5. This reiterates the lesson that the
influence of a case can be changed by transforrnation.0
- *.. '. --
Solid lines:
- Dashed lines: Case 15 modified
3 4 5 6 7 8 9
4 5 6 7 8 9
Figure 5.4.5 The fitted probability of survival as a function of II'BC, using
log ( WBC) as a predictor, leukemia data
R E S I t l U A L S A N D I N F L U E N C E I N R E G R E S S I O N
Logistic regression is one member of the class of generalized linear
models described by Nelder and Wedderburn ; see also
Wedderburn . With the appropriate modifications, many of
the results for the logistic model can be applied in the larger class.
Let Y, , Y2, . . . . . Yn denote tr independent random variables such that
1: has density
zi) = exp[yai-ai(ai)+bi(y)]
Further, assume that a one-to-one function k can be specified such that
where xi is a p'-vector of observable variables, and pis an unobservable
parameter vector. For logistic regression, ai = log [pi/(l -pi)],
k(z) = z, and the other quantities are defined after (5.4.2). The function
k is called a link fur~ctiot~ since it provides the link between the
parameters ai and the linear regression function. It is often useful to
formulate the link function in terms of E(J.~).
The log likelihood for qi = xfB based on the i-th case only is simply
where Ai(ili) = (li(k(rli)). The corresponding score and observed in-
formation are
s, (qi) = ii(tli) = yiL(tli) - ~ ~ ( 4 ' ~ )
(5.4.1 I )
\vi(tli) = - i;(qi) = - yik(qi) + ~ ~ ( 4 ' ~ ) (5.4.12)
respectively. For logistic regression, si = yi - !tipi and wi = 11,p,.(l -pi),
i = 1.2 ,.... 11.
The log likelihood for p based on all 11 cases is
and the corresponding maximum likelihood estimate gof psatisfies the
system of equations
where S is the tr-vector with elements Si = .si(x:B) defined at (5.4.1 1).
Methods of inference, computations and the uniqueness of the
estimators are discussed in Nelder and Wedderburn and
Wedderburn . Here, we assume that the maximum
likelihood estimate is unique.
ASSESSMENT O F I N F L U E N C E IN O T H E R PROBLEMS
in general, the diagnostic methods developed for logistic regression
also can be used for generalized linear models characterized by
(5.4.9) and (5.4.10). In particular, (5.4.4) and (5.4.5) apply with
w = diag [wi(x~$)], and i = (si(x~$)),
where \vi and si are defined at
(5.4.12) and (5.4.1 I), respectively. One possible general extension of the
Studentized residual r f suggested by this procedure is F: =
s*?/Gi(l - Cii).
The i? arise also in connection with an extension of the normal
theory mean shift outlier model, as outlined in Section 2.2.2, when
applied to generalized linear models. One way to describe the
possibility that the i-th case is an outlier is to let
where dj = 1 ifj = i and 0 otherwise. This form might be appropriate
for the leukemia data, for example, because high white blood cell
counts are unreliable. I t is easily verified that the maximum likelihood
estimator of B under (5.4.13) is equal to &,, the maximum likelihood
estimator of fi obtained from the original model after deletion of the i-
th case. The maximum likelihood estimator of q5 will satisfy si(x:&,
+ q5) = 0.
In general, r*: is a modified version of the score test statistic for the hypothesis 4 = 0 obtained by substitut-
ing the observed information matrix for the expected information
matrix. For models with k(qi) = 0, the observed and expected inform-
ation matrices are the same, and i2 is the score test statistic. This
happens, for example, in logistic regression.
E X A M P L E 5.4.2.
L E U K E M I A DATA NO. 4. The log likelihood
based on the i-th case for the AG positive cases in the leukemia data can
be written as
li (qi) = - yiexp [ - log (0, ) - 0, .xi] - [log (0, ) + O2 .xi]
which is of the form given at (5.4.10) with k(qi) = - exp ( - qi), Ai(tli)
= qi, bi(yi) = 0, and qi = xf
with pT = [log (0, ), O,]. The cor-
responding score and observed information are
Thus, w = diag[yiexp ( - x ~ j ) ] , and ii = yiexp ( - x:])
- 1. Values
of D!(X~WX, p'), Gii and ii are given in the last three columns of
R E S I D U A L S A N D I N F L U E N C E IN R E G R E S S I O N
Table 5.1.1 for the AG positive cases. The information contained in
these three statistics is similar to that given by the E,? and cii obtained in
Section 5.1, and the fully iterated influence measure LDi obtained in
Section 5.2 (values for these statistics are also given in Table 5.1.1). The
two distance measures LD, and D! ( x ~ w x , ~ ' ) show very good
agreement, both clearly identifying case 17 as influential:simi1arly, the
ci* and the ti are closely related, with large c,? corresponding to large ti
and small c: corresponding to large negative Fi.Theagreement between
the cii and the Cii is not as strong as between the other statistics. Thus,
the cii and the Cii do not appear to contain the same information.0
5.5 Robust regression
In the usual linear regression model Y = X j? + E , a robust estimate pof
/? is obtained by minimizing
C P [ - X' B 1/51
with respect to /I,
where p is a suitably selected loss function and 8 is a
robust scale estimate that may be determined previously or simul-
taneously to achieve scale invariance. Estimators that minimize (5.5.1)
are called M-estimators, a shorthand for maximum likelihood type
estimators. For a discussion of robust regression methods see, for
example, Huber and Hogg .
Robust regression is designed to reduce or bound the influence of
outlying responses that often occur when sampling from a symmetric
long-tailed distribution. A number of authors, including Huber ,
caution that robust regression may be ineffective in the presence of
remote points in the factor space. Robust estimates can be as sensitive
as least squares estimates to such points and it is for this reason that
measures of case influence are needed in robust regression.
Many of the methods discussed in Chapter 3 for measuring the
influence of the i-th case can be applied to robust regression without
change. For example, let all cases have error variance a2 except for case i
which has var ( c i ) = ( T ~ / M ~ , ,
\vi > 0, and let W = diag(wj), wj = 1 for all
,i # i. Then the influence of case i can be assessed by applying (5.5.1) to
the transformed model
and monitoring the behavior of the corresponding robust estimates
(byi ) as \$ti is varied.
ASSESSMENT OF INFLUENCE IN OTHER PROBLEMS
E X A M P L E 5.5.1.
C L O U D S E E D I N G NO. 12. In this example, we
illustrate the preceding remarks by using the loss function modified according to (5.5.2) with
i = 2, to the cloud seeding data. The robust estimates B ( r c 1 2 ). 0 < by2
I 1, were obtained via an iterative algorithm based on Newton's
method as described in Huber and Holland and Welsch
 . The value of w2 was stepped from w2 = 1 to 0; at each step the
last value of (fl,6) was used as starting value.
Figure 5.5.1 contains a plot of the pl4(w2) component of j ( w 2 )
against w,. The diagonal line is added for reference; the approximate
Figure 5.5.1
p,4(w2) versus bc,, cloud seeding data. Standard error at ,s, = 1
is approximately 0.07
R E S I D U A L S A N D I N F L U E N C E 1N R E G R E S S I O N
standard error at w, = 1 is about 0.07. Clearly, P14 (w, ) is insensitive to
perturbations near w, = 1, but is highly sensitive to perturbations as
\\I, -+ 0. This plot should be compared to the corresponding plot for
least squares estimation given in Fig. 3.4.1. In this example the behavior
of the least squares and robust estimators are remarkably, similar.
The sample influence curve (3.4.6) for robust regression is
where p,,, = $(wl -+ 0) is the robust estimate of /?computed without
the i-th case. Various useful norms of the sample influence curve can be
obtained by following the rationale used in the linear least squares
problem. One possible norm of the sample influence curve is
Di (XT X, p'k) where k is a scalar chosen so that k (XTX)- corresponds
to an estimate of the asymptotic covariance matrix of $ .
A second norm can be based on the iteratively reweighted least
squares approach to computation of 8. Let Ci = ( y i - x T P ) / 3 , $ ( z )
= dp(z)/dz and let
= diag {$(4)/4}.
The norm is then given by
Dl (XT $% X, p'G2). Generally, it is difficult to recommend a specific
norm of this type since the best way to estimate thecovariance matrix of
is apparently unknown.
As indicated previously in this chapter, computation of the sample
influence curve will most likely be expensive. We consider again the
possibility of using a one-step estimate &, in place of &,. Let
$' = d$ (z)/dz, 0
= diag (q',) where cji = (I' (Ci) and let ci = qi xT
( X T Q X )- x,. Then a single step of Newton's method using the fully
iterated, complete data estimates ($,8) for starting values gives
provided, of course. that the relevant quantities are well defined (for
example, Cii f. 1). For linear least squares, the one-step estimator is
exact and reduces to
as shown at (3.4.6). The correspondence between least squares and
robust estimators should be clear from a comparison of (5.5.5) and
(5.5.6). In particular, the residuals ei in (5.5.6) have been replaced by the
Winsorized residuals cS$(Ci) and oii has been replaced by Cii.
ASSESSMENT OF I N F L U E N C E I N O T H E R PROBLEMS
A precise characterization of the accuracy of this one-step approxi-
mation (5.5.5) is unavailable, but the following observations may help.
First, for estimators with redescending $-functions, such as Andrews'
 sine estimator, (XT Q,,)
X(i,) need not be positive definite and
the one-step 'estimator f,'i, cannot be guaranteed to decrease the
objective function. The one-step estimators can be expected to be more
satisfactory for monotone $-functions.
Second, it is not difficult to verify that if $ is piecewise linear (for
example, the t+h function corresponding to (5.5.3)) and if the classifi-
cation of Zj according to the pieces of $ is the same as the classification
of the one-step residuals c?; = (yj - xf ):,,)/8
for all j # i. then p,',,
= &,,. More generally, the accuracy of the one-step approximation
seems to depend on the differences 1 Cj - e'f 1.
Assuming a sufficiently accurate one-step approximation. the effects
of remote points in the factor space on robust estimators can be
illustrated by using Huber's loss function:
xT(xTQx)-lxi,
( ~ ( x T Q x ) - ~
If - c 5 2, I
c, the influence of the i-th case can be greater than that for
least squares since vii I
Cii, i = 1, 2, . . . , n. Similarly when 16?,1 > c the
influence of the i-th case will generally be less than that for least squares.
Consider, for example, the situation in which ti = 0 but qj = 1 for all
j + i. If i?, > c, then
(XTX)-I xi
(n-l)CP-P:i)l = (n-1)
which is the SIC, for least squares reduced by the factor c/e',. Also,
f o r j f i
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
E X A M P L E 5.5.2.
CLOUDSEEDING NO. 13. Toillustratetheuseof
the one-step approximation, we use the cloud seeding data. The full
sample estimate for Huber's method was obtained with the least
squares estimate as starting values, with Huber's proposal 2 as
the iterative computing method, and using 8 = median absolute
residua1/0.6745 to estimate scale. Fifteen iterations were required to get
a solution such that the maximum proportional change in any
coeficient from the last iteration was less than 0.01.One-step measures
p'bZ) and 'fully iterated' Di(XT%x, ~ ' 8 ~ )
based on 10
iterations were then computed.
Table 5.5.1 lists the two measures for the five cases with the largest
values of Di(XT w X,
With theexception of the clearly influential
case 2. agreement between the two measures is adequate, and even case
2 is clearly identified by the one-step measure. Overall, 9 of 24 cases are
underestimated using the one-step estimate, but none seriously.
Table 5.5.1 Fioe lurgesr it~uetrce measures, i.loud
seedir~g dnrn
D! (XTW X, p ' d 2 )
D ~ sine estimator
and for several other data sets. While the results for the sine estimate
applied to these data generally agree with the results for the Huber
estimate, in other problems we have found the agreement to be much
worse. More work is needed to understand the one-step distance
measures and their usefulness when applied to the robust estimators.0
5.6 Other problems
In this section we give brief accounts of some of the other problems for
which influence has been studied, including the correlation coeficient,
ciiscriniinant analysis, and linear regression with incomplete data.
ASSESSMENT OF I N F L U E N C E IN OTHER P R O B L E M S
Suppose that X I , X, are normal random variables with means / 1 , , 11,.
variances a:, a:,
and covariance as
where gj = (xi - pj)/aj, j = 1.2. The empirical influence curve for a
sample ( x , ~ ,
xZi), i = 1,2, . . . , )I, is obtained by substituting the
corresponding sample cdf
in place of F,
where Aj = (xj- Sj)/sj, Sj = Xi.uji/,~ and s3 = Xi (xji - . T ~ ) ~ / , I ,
j = 1 , 2,
and jj is the usual estimator of p. The sample influence curve is given by
Both sample versions of the influence curve for p can provide useful
information on the effects of single cases in determining 6. In small
samples, however, where efficient calculation and methods of display
are not a serious issue, the SIC seems preferable, as it has a
straightforward interpretation and is perhaps the most directly rele-
vant. Devlin et a/. suggest the SIC for use in detecting outliers that
substantially affect b.
For the usual estimator b, the SIC can be studied by expressing b,,, as
a function of i, and other full sample statistics. One finds
where rji = (xji - Zi)/[sj(l - 1/tl)"2]
is the i-th Studentized residual.
i = 1.2, . . . , 11, for the j-th marginal sample, j = 1. 2. Since rji is a
monotonic function of the normal theory test that the i-th case is a
marginal outlier under the mean-shift model, the denominator of
(5.6.4) will be small if either xli or xzi appears to be an outlier when
judged against the respective marginal samples. A marginal outlier will
have a substantial influence on 6.
The numerator in (5.6.4) measures the joint effect of ( x , ~ ,
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
depends on the location of (.uri, x,;) relative to (.f,, 2,). If, for example.
j, > 0 and .u, ; < ?(, and xZi > i2 (or xi, > 2 , and xi, < 2,) then
jl - jl(i, < 0 .
In large samples, an approximation to the sample influence curve
nray be sufficient. As a first-order approximation, expand the denomi-
nator of (5.6.4) in (r:i/n, r i i / n ) in a linear Taylor expansion*about (0,O).
For large 11, (5.6.3) becomes
which is essentially the empirical influence curve evaluated at ( x l i , xZi).
Devlin et 01. suggest a graphical technique based on this
approximation. Let
EIC(.u,, .u2) = ( 1 - fi2)u, U ,
The advantage of this form is that the contours of constant influence
are hyperbolae. Devlin et suggest superimposing selected
contours of the EIC on scatter plots of (u,, u,) and, then reading the
approximate influence directly from the plot.
As seen previously, procedures based on'. the EIC should be
reasonable approximations to the SIC as long as n is large and the r i are
small to moderate. If rji is large (the case is well removed from the
centroid), Devlin et nl. suggested that the EIC will usually
underestimate the SIC. For this reason, their graphical procedure is
perhaps best used as an initial screen. If a case is found to be influential
it may be necessary to conduct a more precise investigation using the
Campbell has considered the use of the influence curve as an aid
in detecting outliers in two population normal discriminant analysis.
He derives the theoretical and sample influence curves for the usual
summary statistics, namely the Mahalanobis D2, the vector of dis-
ASSESSMENT OF I N F L U E N C E IN OTHER PROBLEMS
criminant means, and the vector of discriminant coefficients . Assuminga perturbation in
the first population, the influencecurve for D2 evaluated at a point x is a
quadratic function of the difference between the discriminant scores at
x and at the mean of the first population. The sample influence curve, in
which estimates replace parameters, corresponds to a [),,-like measure.
since there is no component of the influence curve to correspond to a
residual. Thus, influential cases for D2 are those that are more distant
(in an appropriate metric) from the mean of the other population.
Campbell also discusses a function of the influence curve for D2 and the
other statistics that can be useful in graphical methods for the study of
outlying cases.
L I N E A R REGRESSION W I T H INCOMPLETE DATA
Suppose we wish to fit a linear model of the type discussed earlier in this
monograph but values for some of the variables are not observed. We
call this a regression problem with incomplete data. Many writers have
addressed the problem of estimation of parameters with incomplete
data, often assuming that the unobserved data are 'missing at random'
 , and that the observed data follow a multivariate normal
distribution . Computational methods to
find the maximum likelihood estimates of parameters of the con-
ditional distribution of the response, given the predictors, have been
given by Orchard and Woodbury , Dempster, Laird and Rubin
 and Hocking and Marx , among others.
In all of this literature, little or no attention has been paid to the
problem of analyzing residuals and assessing influence. Shih has
made first steps in this direction. He defines residuals by essentially
using the general approach of Cox and Snell outlined in Section 5.1. If
the EM-algorithm of Dempster er al. is used for the computations a
very elegant result is obtained. At convergence of the algorithm, fill-in
values for unobserved values are estimated, and the residuals can then
be computed in the usual way based on the filled-in data. Studentized
residuals, however, are not as easy to obtain, as the likelihood function,
which is needed for the methods of Cox and Snell, is relatively
complicated. Shih has also considered the use of one-step estimators.
also using the EM algorithm, of the sample influence curve.
Generally, the maximum likelihood residuals seem to be superior to
the competitors, such as the residuals computed only from the fully
RESIDUALS A N D INFLUENCE IN REGRESSlON
observed cases. However, much more experience with these residuals is
required for them to be well understood.
For influence analysis, one can show that incomplete cases will
generally not be influential. In addition, the extent to which one-step
approximations are useful seems to depend on the covariance structure
of the data, and the pattern of the incomplete data.
Weighted least squares
The weighted least squares model is given by
where all quantities are as defined near (2.1.1), except that Var (6)
= a2W- and W isa known n x 11 diagonal matrix with wii > O.The w,,
are often called case weights. Although weighted least squares esti-
mators can be computed directly, it is usual to transform to an
unweighted least squares problem, and solve this simpler version.
Multiplying both sides of (A.l.l) on the left by W'I2,
or, if Y* = WLi2Y, X* = W112X, and E* = W1I2e,
Y* = X*/?+E*
Since Var(&*) = c21, it follows immediately that fi = (X*TX*)-lX*TY*.
Computationally, then, 8 can be obtained by multiplying j.,
and each element of xi, including the constant, by w ~ , ' , ' ~ , and solving
the resulting unweighted least squares problem. Using this
transformation, the residuals are e* = Y* - ~ * f i
= w1I2 (Y - ~ g ) .
while the correct residuals for the model (A.l.l) are e = Y - xP. The
elements of e* are sometimes called weighted residuals, and of course e
= W-'I2e*. Studentized residuals are identical under either formu-
lation. Distance measures are also the same under both formulations.
provided, of course, that the correct norm is used. For model (A.l.l).
the appropriate norm is Di(XTWX, p'c?2), which is equivalent to
Di(X*'X*, PI&'),
the correct norm for (A.1.2).
RESlDLlALS A N D INFLUENCE IN REGRESSION
A.2 Updating formulae
Let A be a p' x p' rank p' symmetric matrix, and suppose that a
and b are q x p' rank q matrices. Then, provided that the inverses
This remarkable formula shows how to modify the inverse of the
corrected cross product matrix when one or more rows of a matrix are
deleted or added. The most important special case is that of deleting a
single row xT from X. Setting A = XTX,
a = - xT, b = x:,
A version of this formula was given by Gauss (1821), and in several
papers about 1950 . Bingham used this basic
formula in a wide variety of applications in regression. A discussion of
the history of this type of updating, and generalizations of it, is given by
Henderson and Searle .
A closely related result concerns the determinant of a partitioned
q x q matrix Z, where
and A and D are nonsingular. Then,
This result is attributed to J. Schur by Henderson and Searle . It
can be used to establish several useful updating and downdating
formulae. For example, let A = XTX, B = Xf, C = X,, and D = I,,
where the use of I as a subscript is as in Section 3.6. Then
A.3 Residual correlations
Let vij = x:(XTX)-'xj be the (i, j)-th element of V and define pi, to be
the correlation between the i-th and the j-th residuals,
If xi = xj, then
p = - v../(l - v..)
The residual correlation for replicated rows of X is thus always negative
and will be large only if the corresponding vii is large. However,
where c > 1 is the number of replicates of xi. For replicated points,
therefore, large negative correlations (pi] < - 3) can occur only if xi is
replicated twice.
To investigate the general causes of a large value for p:, we shall fix x j
and choose xi to maximize p$ . The required calculations
are facilitated by first writing p$ in terms of explicit quadratic forms in
Vkl(i) = x:(x;) X(i) )- 'x,
Using (A.2.1),
v k ~ = vkl(i) - ~ki(i)vIi(i)/(l + vii(i)).
vkl(i) = Vkl + vkivli/(l - vii).
These expressions show how to update and downdate the elements
V i j = vij(i)/(l + vii(i))
0.. = 0.. . -v2../(1 + t
V i i = vii(i) /(I + vii(i)).
Finally, p$ may be expressed as,
RESIIIUALS A N D I N F L U E N C E I N K E G R E S S I O N
This form is convenient since vii(i, and v;,~, are quadratic forms in xi and
the corresponding inner product matrix (X$,X(,,)- ' does not depend on
xi by construction. Since x j is to be held fixed, ujjci) is a constant. Thus, to
maximize p$ by choice of xi it is sufficient to maximize
If the model contains a constant term, as will usually be the case, the
first term of xi is constrained to be 1 and the maximum of f(xi) must be
taken with respect to the last p components of xi. Assume that the
independent variables are measured around the sample averages in the
reduced data set, and let x: = (1, x:) and
and (A.3.9) can be usefully re-expressed as
The largest possible value for p:j will obviously depend on the subset
of RP over which the maximum is taken. If the model contains
functionally related terms (for example, x and x2) the appropriate
subset may be complex and will depend on the model. Here, we
consider the unconstrained maximum over RP by first considering
subsets of the form G(k) = { ~ ~ l x : ( % , X ( ~ ) ) -
' x i = k, k > 0) and then
maximizing over k. The effect of this is that for some models the derived
maximum may not be attainable.
Using the Cauchy-Schwarz inequality, it can be verified that
max [j'(xi)] =
+(x:(%) ?qij)-
' x ~ ) " ~
which is attained at
xi = xjk'/(xJ ( X i ) q i , ) - ' x j ) '
The global maximum can now be obtained by finding the value of k
which maximizes (A.3.11). This value is k* = t l 2 x T ( G , Xti,)- ' x )
Substituting k* into (A.3.1 I ) and the resulting expression into (A.2.81
and simplifying yields the final result,
max (pi:.) = tljjci,-- - --
which is attained at xi = nxj.
These results show that for pc to be large either xi or x, must he a
remote point. Otherwise, vjj,i, and thus (A.3.12) will be small. A second
requirement for a high correlation is that one point must be (ap-
proximately) a positive scalar multiple of the other, xi Z dxj where
d > 0. With x j fixed, the value of xi which maximizes p$ is xi = 11.r~.
Moreover, since the right side of (A.3.1 I) is monotonically increasing in
k*, in any fixed data set the correlation between a remote pair
of points which are (approximate) replicates will tend to be large.
Finally, when n is large, high correlations will also occur when
xi r - dxj. When xi = - x j the cases lie on the opposite edges of the
sampled region and pi, > 0.
Bibliography
Abrahamse, A. P. J. and Koerts, J. . A comparison between the power of
the Durbin- Watson test and the power of the BLUS test. J. Anter. Srarist.
Assoc.. 64, 938-49.
Abrahamse, A. P. J. and Koerts, J. . New estimators of disturbances in
regression analysis. J. Anrer. Srarisr. Assoc., 66, 71 - 74.
Abrahamse. A. P. J, and Louter, A. S. (197 1). One new test for autocorrelation
in least squares regression. Biometrika, 58, 53-60.
Aitchison. J. and Dunsrnore, 1. . Srarisrical Predicrion Analysis.
Cambridge: Cambridge University Press.
Aitkin, M. and Wilson, G. T. . Mixture models, outliers and the EM
algorithm. Technomerrics, 22, 325-3 1.
Allen. D. M. . The relationship between variable selection and data
augumentation and a method for prediction. Technometrics, 16, 125-27.
Anderson, R. L., Allen, D. M., and Cady, F. . Selection of predictor
variables in multiple linear regression, in Bancroft, T. A. (ed.)., Sraristical
Papers in Honor of George W. Snedecor. Ames: Iowa State Press.
Andrews, D. F. . A note on the selection of data transformations.
Biomerrika, 58, 249-54.
Andrews, D. F. . Significance testing based on residuals. Biometrika,
58, 139-48.
Andrews, D. F. . Plots of high-dimensional data. Biomerrics, 28, 124-
Andrews, D. F. . A robust method for multiple linear regression.
Terhnometrics, 16, 523-3 1.
Andrews, D. F., Bickel-P., Hampel, F., Huber, P., Rogers, W. H., and Tukey,
J. W. . Robrrsr Esrimares of Locarion. Princeton, N.J.: Princeton.
Andrews. D. F. and Pregibon. D. . Finding outliers that matter. J. Roy.
Srarisr. Soc.. Ser. B., 40, 85-93.
Anscornbe. F. J. . Examination of residuals. Proc. Fourth Berkeley
Synip.. 1, 1-36.
Anscornbe, F. J. . Topics in the investigation of linear relations fitted by
the method of least squares (with discussion). J. Roy. Stnrisr. Soc., Ser. B,
Anscombe, F. J. . Graphs in statistical analysis. Amer. Sfatisfician, 27,
B I B L I O G R A P H Y
Anscombe. F. J. and Tukey, J. W. . The examination and analysts of
residuals. Technometrics, 5, 14 1-60.
Atkinson, A. C. . Testing transformations to normality. J Roy. Statist.
Soc.. Ser, B., 35, 473-79.
Atkinson, A. C. . Robustness, transformations and two graphtcal
displays for outlying and influential observations in regression. Brontetrika.
68, 13-20.
Atkinson, A. C. . Regression diagnostics, transformations and con-
structed variables (with discussion). J. ROJ. Statist. Sot-.. Ser. B. 44, 1-36.
Bailey, B. . Tables of the Bonferroni [-statistic. J. An~er. Statrst. Assoc .
72, 469-78.
Barnett, V. and Lewis, T. . Outliers in Sta!istrcal Data. Chichester:
Bartlett, M. . An inverse matrix adjustment arising in discriminant
analysis. Ann. Math. Statist., 22, 107-1 11.
Bates, D. and Watts, D. . Relative curvature measures of nonlinearity
(with discussion). J. Roy. Statist. Soc., Ser. B. 22, 41 -88.
Bates, D. and Watts, D. . Parameter transformations for improved
confidence regions in nonlinear least squares. Ann. Statist., 9, 1 1 52-67.
Beale, E. M. L. . Confidence regions in nonlinear regression (with
discussion). J. Roy. Statist. Soc.. Ser. B. 22, 41 -88.
Beckman, R. and Trussell, H. . The distribution of an arbitrary
Studentized residual and the effects of updating in multiple regression. J
Amer. Statist. Assoc., 69, 199-20 1.
Behnken, D. W. and Draper, N. R. . Residuals and their variance
patterns. Technometrics, 14, 102- 11.
Belsley, D. A., Kuh, E., and Welsch. R. E. . Regresrion Diagnostics New
York: Wiley.
Bickel, P. and Doksum, K. . An analysis of transfo:mations revisited.
J. Amer. Statisr. Assoc., 76, 296-31 1.
Bingham, C. . Some identities useful in the analysis of residuals from
linear regression. University of Minnesota, School of Statistics. Technical
Report No. 300.
Bliss, C. I. . The method of probits. Science (Lancaster). 79, 38-39.
Blom, G. . Transformations of the binomial, negative binomtal. Po~sson
and chi-squared distributions. Biometrika. 41, 302-16.
Blom, G. . Statistical Estimates and Transjornled Befa Varrates. New
York: Wiley.
Box, G. E. P. . Strategy ofscientific model building, in Launer, R. L. and
Wilkinson, G. N., (eds.), Robwmess in Statistics. New York: Academic
Box, G. E. P. . Sampling and Bayes inference in scientific modeling
and robustness (with discussion). J. Roy. Statist. Soc . Ser. A, 143,383-430
Box, G. E. P. and Cox, D. R. . An analysis of transformations (with
discussion). J. Roy. Statist. Soc.. Sc~r. B. 26, 21 1-46.
Box, G. E. P. and Cox, D. R. . An analysis of transformattons revisited.
rebutted. J. Amer. Statist. Assoc., 77, 209-10.
Box, G. E. P. and Draper, N. R. . Robust designs. Biometrika, 62,347-52.
RESIDUALS AND INFLUENCE I N REGRESSION
Box, G. E. P. and Tiao, G. C. . A Bayesian approach to some outlier
problems. Biometrika, 55, 119-29.
Box, G. E. P. and Tidwell, P. W. . Transformations of the independent
variables. Technometrics, 4, 53 1 -50.
Bradu, D. and Gabriel, R. . The biplot as a diagnostic tool for models of
two-way tables. Technometrics, 20, 47-67.
Braham, R. . Field experimentation in weather modification (with
discussion). J. Amer. Statist. Assoc., 74, 57-67.
Brown, R. L., Durbin, J., and Evans, J. M. . Techniques for testing the
constancy of regression relationships (with discussion). J. Roy. Statist. Soc.,
Ser. B., 37, 149-63.
Campbell, N. A. . The influence function as an aid in outlier detection in
discriminant analysis. Applied Statistics, 27, 251 -58.
Carroll, K. . A robust method for testing transformations to achieve
approximate normality. J. Roy. Statist. Soc., Ser. B, 42, 71-78.
Carroll, R. and Ruppert, D. . On prediction and the power transform-
ation family. Biometrika, 68, 609-16.
Carter, 0.
R., Collier, B. L., and Davis, F. L. . Blast furnace slags as
agricultural liming materials. Agronomy Journal, 43, 430-33.
Cleveland, W. S. . LOWESS: A program for smoothing scatterplots by
robust locally weighted regression. Amer. Statistician, 35, 54.
Cleveland, W. S. and Kleiner, B. . A graphical technique for enhancing
scatterplots with moving statistics. Technometrics, 17, 447-54.
Cook, R. D. . Detection of influential observations in linear regression.
Technometrics, 19, 15-18.
Cook, R. D. . Letter to the editor. Technometrics, 19, 348.
Cook, R. D. . Influential observations in linear regression. J. Amer.
Statist. Assoc., 74, 169-74.
Cook, R. D., Holschuh, N., and Weisberg, S. . A note on an alternative
outlier model. J. Roy. Statist. Soc., Ser. B, 44, (in press).
Cook, R. D. and Prescott, P. . Approximate significance levels for
detecting outliers in linear regression. Technometrics, 23, 59-63.
Cook, R. D. and Weisberg, S. . Characterizations of an empirical
influence function
for detecting influential cases in
regression.
Technometrics, 22, 495-508.
Cook, R. D. and Weisberg, S. . Criticism and influence in regression. In
Leinhardt, S. (ed.), Sociological Methodology 1982, Chapter 8. San
Francisco: Jossey-Bass.
Cox, D. R. and Hinkley, D. V. . Theoretical Statistics. London:
Chapman and Hall.
Cox, D. R. and Snell, E. J. . A general definition OF residuals. J. Roy.
Statist. Soc., Ser. B, 30, 248-75.
Daniel, C. . Use of half normal plots in interpreting two-level
experiments. Technometrics, 4, 3 1 1 4
Daniel, C. . Applications o/Stutistics to Industrial Experimentation. New
York: Wiley.
Daniel, C. . Patterns in residuals in the two-way layout. Technometrics,
20, 385-95.
B I B L I O G R A P H Y
Daniel, C. and Wood, F. . Fitting Equations to Data, 2nd Edn. New
York: Wiley.
David, H. A. . Order Statistics, 2nd Edn. New York: Wiley.
Davies, R. B. and Hutton, B. . The effects of errors in the independent
variables in linear regression. Biometrika, 62, 383-91.
Dempster, A. P. and Gasko-Green, M. . New tools for residual analysis.
Ann. Statist., 9, 945-59.
Dempster, A. P., Laird, N., and Rubin, D. . Maximum likelihood from
incomplete data via the EM algorithm (with discussion). J. Roy. Statist.
SOC., Ser. B, 39, 1-38.
Devlin, S. J., Gnanadesikan, R., and Kettenring, J. R. . Robust
estimation and outlier detection with correlation coefficients. Biometrika,
62, 53 1 -46.
Dixon, W. J. . Analysis of extreme values. Ann. Math. Statist., 21,488-
Dongarra, J., Bunch, J. R., Moler, C. B., and Stewart, G. W. . The
LlNPACK Users' Guide. Philadelphia: SIAM.
Draper, N. R. and John, J. A. . Testing for three or fewer outliers in two-
way tables. Technometrics, 22, 9-16.
Draper, N. R. and John, J. A. . Influential observations and outliers in
regression. Technometrics, 23, 21 -26.
Draper, N. R. and Smith, H. . Applied Regression Analysis. New York:
Draper, N. R. and Smith, H. . Applied Regression Analysis, 2nd Edn.
New York: Wiley.
Duncan, G. T. . An empirical study ofjackknife constructed confidence
regions in nonlinear regression. Technometrics, 20, 123-29.
Durbin, J. . An alternative to the bounds test for testing serial
correlation in least squares regression. Economerrica, 38, 422-29.
Durbin, J. and Watson, G. S. . Testing for serial correlation in least
squares regression I. Biometrika, 37, 409-28.
Durbin, J. and Watson, G. S. . Testing for serial correlation in least
squares regression 11. Biometrika, 38, 159-78.
Durbin, J. and Watson, G. S. . Testing for serial correlation in least
squares regression 111. Biometrika, 58, 1-19.
Ellenberg, J. H. . The joint distribution of the standardized least squares
residuals from a general linear regression. J. Amer. Statist. Assoc., 68,941-
Ellenberg, J. H. . Testing for a single outlier from a general linear
regression model. Biornetrics, 32, 637-45.
Ezekiel, M. . A method of handling curvilinear correlation for any
number of variables. J. Amer. Statist. Assoc., 19, 431 -53.
Ezekiel, M. . Methods of correlation analysis. New York:
Ezekiel, M. and Fox, K. (1 958). Methods ofcorrelation and regression analysis.
New York: Wiley.
Farebrother, R. W. . BLUS residuals, Algorithm AS104. Applied
Statistics, 25, 317-19.
R E S I D U A L S A N D I N F L U E N C E I N R E G R E S S I O N
Farebrother. R. W. . Recursive residuals -a remark on algorithm A75:
Basic procedures for large, sparse or weighted least squares problems.
Applied S/ari.c/ics. 25, 323 -24.
Feipl. P. and Zelen, M. . Estimation of exponential probabilities with
concomitant information. Biometrics, 21, 826-38.
Filliben. J. J. . The probability plot correlation coefficient test for
normality. Technomctrics, 17, 1 1 1 -17, correction, 17, 520.
Fisk. P. R. . Discussion ofa paper by Brown, Durbin, and Evans. J. Roy.
Soc-.. Scr. B, 37, 164-66.
Fox. T., Hinkley, D. and Larntz, K., . Jackknifing in nonlinear
regression. Tcchnomerrics, 22, 29-33.
Furnival. G , and Wilson, R. . Regression by leaps and bounds.
Tt~chtiontetrics. 16, 499-5 1 1.
Galpin. J. and Hawkins, D. . Rejection of a single outlier in two or three
way tables. Technonterrics, 23, 65-70.
Gauss. C. F. ( 1 821. collected works 1873). Theoria Combinationis Ohser-
l.arinrl~in~
Errorihlrs Aiittintis 0htto.uiae. Werke 4. Section 35, Gottingen,
Geisser. S. . Bayesian estimation in multivariate analysis. Ann. Math.
Srarisr., 36, 150-59.
Geisser, S. . The inferential use of predictive distributions, in Godambe,
V. and Sprott, D. (eds.), Foundations oj'Stalistical Inji~rence. Toronto: Holt,
Rinehart and Winston, pp. 456-66.
Geisser. S. and Eddy. W. F. . A predictive approach to model selection.
J. .41?1tlr. Stc~tist. A.rsoc., 74, 153-60.
Gentleman. J . F. . Moving statistics for enhanced scatterplots. Applied
Srarisrics. 27, 354-58.
Gentleman. J. . Finding the k most likely outliers in two-way tables.
Techttomerrics, 22, 591 -600.
Gentleman. J. F. and Wilk, M. B. . Detecting outliers in a two-way
table: I. Statistical behavior of residuals. Technometrics, 17, 1-14.
Gentleman. J. F. and Wilk, M. B. . Detecting outliers, 11:
Supplementing the direct analysis of residuals. Biometrics, 31, 387-410.
Ghosh, M. and Sharma, D. . Power of Tukey's test for nonadditivity. J.
Roy. Sratisr. Soc., Ser. B, 25, 213-19.
Gnanadesikan, R. . Me~hods /or Statistical Analysis oJ' Multivariale
dara. New York: Wiley.
Grubhs. F. E. . Sample criteria for testing outlying observations. Ann.
Alnrh. Srorisr.. 21, 27-58.
Hamilton. D.. Watts. D., and Bates, D. . Accounting for intrinsic
nonlinearity in nonlinear regression parameter inference regions. Ann.
Storisr.. 10, 386393.
Hampel. F. . Contributions to the theory of robust estimation.
Llnpublished Ph.D. dissertation, Univ. of California, Berkeley.
Hampel, F. . The influence curve and its role in robust estimation. J.
Anter. Srarisr. Assoc., 69, 383-93.
Harvey. A. C. and Phillips, G. D. A. . A comparison of the power of
some tests for heteroscedasticity in the general linear model. J.
Econonrerrics, 2, 307 -3 16.