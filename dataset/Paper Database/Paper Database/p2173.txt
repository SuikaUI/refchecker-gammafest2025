Off-grid Direction of Arrival Estimation Using Sparse Bayesian
Zai Yang, Lihua Xie∗, Fellow, IEEE, and Cishen Zhang†
Direction of arrival (DOA) estimation is a classical problem in signal processing with many practical applications. Its research has recently been advanced owing to the development of methods based on sparse signal
reconstruction. While these methods have shown advantages over conventional ones, there are still difﬁculties in
practical situations where true DOAs are not on the discretized sampling grid. To deal with such an off-grid DOA
estimation problem, this paper studies an off-grid model that takes into account effects of the off-grid DOAs and
has a smaller modeling error. An iterative algorithm is developed based on the off-grid model from a Bayesian
perspective while joint sparsity among different snapshots is exploited by assuming a Laplace prior for signals
at all snapshots. The new approach applies to both single snapshot and multi-snapshot cases. Numerical simulations show that the proposed algorithm has improved accuracy in terms of mean squared estimation error. The
algorithm can maintain high estimation accuracy even under a very coarse sampling grid.
Introduction
Source localization using sensor arrays has been an active research area for decades. This paper focuses on the
narrowband far-ﬁeld source case where the wave front is assumed to be planar and the angle/direction information
is to be estimated, known as the direction of arrival (DOA) estimation problem. MUSIC is the most successful
method among conventional DOA estimation techniques. It has been proven to be a realization of the maximum
likelihood method in the case of a large number of snapshots and uncorrelated source signals . The research on
DOA estimation has been advanced in recent years owing to the development of methods based on sparse signal
reconstruction (SSR) or compressed sensing (CS) . In these methods, e.g., ℓ1-SVD , a ﬁxed sampling grid is
selected ﬁrstly that serves as the set of all candidates of DOA estimates. Then by assuming that all true (unknown)
DOAs are exactly on the selected grid, an SSR problem can be formulated where the DOAs of interest constitute the
support of the sparse signal to be recovered.
In the case of a single measurement vector (SMV, or a single snapshot), ℓ1 optimization is a favorable
approach to the sparse signal recovery due to its guaranteed recovery accuracy. It has been proven in that a
sparse signal can be accurately recovered under a so-called restricted isometry property (RIP) condition that requires
all columns of the measurement matrix be highly incoherent. In the case of multiple measurement vectors (MMV),
the sparse signals at all snapshots share the same support. It has been shown in that such joint sparsity can be
exploited to improve the averaged recovery success probability under a similar incoherent matrix condition. Sparse
Bayesian inference/learning (SBI) is another popular method for the sparse signal recovery in CS. In SBI,
the signal recovery problem is formulated from a Bayesian perspective while the sparsity information is exploited
by assuming a sparse prior for the signal of interest. As an example, a Laplace signal prior leads to a maximum
∗Z. Yang and L. Xie are with EXQUISITUS, Centre for E-City, School of Electrical and Electronic Engineering, Nanyang Technological
University, 639798, Singapore (e-mail: ; ).
†C. Zhang is with the Faculty of Engineering and Industrial Sciences, Swinburne University of Technology, Hawthorn VIC 3122, Australia
(e-mail: ).
 
a posteriori (MAP) optimal estimate that coincides with an optimal solution to the ℓ1 optimization . In the
MMV case, the joint sparsity among different (uncorrelated) snapshots is utilized by assuming the same sparse prior
for the signals at all snapshots . Correlations between snapshots have also been studied in a recent paper .
One merit of SBI is its ﬂexibility in modeling sparse signals that can not only promote the sparsity of its solution,
e.g., in , but also exploit the possible structure of the signal to be recovered, e.g., in . Since the Bayesian
inference is a probabilistic method and based on heuristics to some extent, one shortcoming of SBI is that it offers
fewer guarantees on the signal recovery accuracy as compared with, for example, ℓ1 optimization.
Recent advancements in array signal processing include compressive (CS-) MUSIC and subspace-augmented
(SA-) MUSIC . They are combinations of the conventional MUSIC technique and recent CS methods with guaranteed support recovery performance and can outperform MUSIC and standard CS approaches. Though existing
CS-based approaches have shown their improvements in DOA estimation, e.g., their success in the case of limited
snapshots, there are still difﬁculties in practical situations where the true DOAs are not on the sampling grid. On
one hand, a dense sampling grid is necessary for accurate DOA estimation to reduce the gap between the true DOA
and its nearest grid point since the estimated DOAs are constrained on the grid. On the other hand, a dense sampling
grid leads to a highly coherent matrix that violates the condition for the sparse signal recovery. We refer to the
model adopted in the standard CS methods as an on-grid model hereafter in the sense that the estimated DOAs are
constrained on the ﬁxed grid.
An off-grid model for DOA estimation is studied in where the estimated DOAs are no longer constrained
in the sampling grid set. The model takes into account the basis mismatch in the measurement matrix caused by the
off-grid DOAs. It has been shown in that the sparse total least squares (STLS) solver proposed in can yield
an MAP optimal estimate if the matrix perturbation caused by the basis mismatch is Gaussian. However, we show in
this paper that the Gaussian condition cannot be satisﬁed in the off-grid DOA estimation problem and hence a new
solver is needed.
In this paper, we propose a Bayesian algorithm for DOA estimation based on the off-grid model that applies to
both SMV and MMV cases. The off-grid distance (the distance from the true DOA to the nearest grid point) that lies
in a bounded interval is assumed to be uniformly distributed (noninformative) rather than Gaussian as in . We
refer to our algorithm as off-grid sparse Bayesian inference (OGSBI) in the body of the paper. We then incorporate
in our algorithm an idea in , using the singular value decomposition (SVD) to reduce the computational workload
of the signal recovery process and the sensitivity to noise, namely, OGSBI-SVD. Our approach is a spectral-like
method with the value of the spectrum at each DOA being the estimated source power from such a direction. We
show by numerical simulations that the proposed method has a smaller mean squared error (MSE) in comparison
with ℓ1-SVD . Indeed, the proposed method can exceed a lower bound of MSE that is shared among all on-grid
model based methods including CS-MUSIC and SA-MUSIC. It is also shown that the proposed method is more
accurate and faster than STLS. Moreover, the proposed method can estimate DOAs accurately even under a coarse
sampling grid.
Notations used in this paper are as follows. Bold-face letters are reserved for vectors and matrices. x, xT and xH
denote complex conjugate, transpose and conjugate transpose of a vector x, respectively. ∥·∥1, ∥·∥2 and ∥·∥F denote
the ℓ1 norm, ℓ2 norm and Frobenious norm, respectively. |A|, Tr {A} are the determinant and trace of a matrix A
respectively. xj is the jth entry of a vector x. Aj, Aj and Aij are the jth column, jth row and (i, j)th entry of a
matrix A, respectively. diag (A) denotes a column vector composed of the diagonal elements of a matrix A, and
diag (x) is a diagonal matrix with x being its diagonal elements. x′(θ) is the derivative of x(θ) with respect to θ. ℜ
and ℑtake the real and imaginary parts of a complex variable respectively. x ⊙y is the Hadamard (element-wise)
product of x and y. bx denotes an estimate of x.
The rest of the paper is organized as follows. Section 2 studies the off-grid model used in this paper. Section 3
introduces the proposed OGSBI and OGSBI-SVD algorithms. Section 4 presents our simulation results. Section 5
concludes this paper.
Off-grid DOA Estimation Model
Consider K narrowband far-ﬁeld sources sk(t), k = 1, · · · , K, impinging on an array of M omnidirectional sensors
from directions θk, k = 1, · · · , K. Time delays at different sensors can be represented by simple phase shifts,
leading to the observation model :
y(t) = A(θ)s(t) + e(t),
t = 1, · · · , T,
where y(t) = [y1(t), · · · , yM(t)]T , θ = [θ1, · · · , θK]T , s(t) = [s1(t), · · · , sK(t)]T , e(t) = [e1(t), · · · , eM(t)]T ,
and ym(t) and em(t), m = 1, · · · , M, are the output and measurement noise of the mth sensor at time t respectively.
The matrix A(θ) = [a (θ1) , · · · , a (θK)] is an array manifold matrix and a (θk) is called steering vector of the
kth source. The entry am (θk) contains the delay information of the kth source to the mth sensor. In this paper,
we assume that the number of sources K is already known. Readers are referred to a preprint version for
discussions on the case of unknown K. So, the goal is to ﬁnd the unknown DOAs θ given K, y(t) and the mapping
θ →A(θ). In the following we re-derive the off-grid model proposed in using linear approximation and further
show its relationship with the on-grid one.
˜θ1, · · · , ˜θN
be a ﬁxed sampling grid in the DOA range [0, π], where N denotes the grid number
and typically satisﬁes N ≫M > K. Without loss of generality, let ˜θ be a uniform grid with a grid interval
r = ˜θ2 −˜θ1 ∝N−1. Suppose θk /∈
˜θ1, · · · , ˜θN
for some k ∈{1, · · · , K} and that ˜θnk, nk ∈{1, · · · , N}, is the
nearest grid point to θk. We approximate the steering vector a (θk) using linearization:
. Denote A =
, · · · , a
, · · · , b
, β = [β1, · · · , βN]T ∈
N and Φ (β) = A + Bdiag (β), where for n = 1, · · · , N,
βn = θk −˜θnk,
xn(t) = snk(t),
if n = nk for any k ∈{1, · · · , K} ;
xn(t) = 0,
otherwise,
with nk ∈{1, · · · , N} and ˜θnk being the nearest grid to a source θk, k ∈{1, · · · , K}. By absorbing the approximation error into the measurement noise the observation model in (1) can be written into
y(t) = Φ (β) x(t) + e(t),
t = 1, · · · , T,
which is the off-grid model to be used in this paper. This model will be empirically validated in Subsection 4.1 by
showing that the total noise (approximation error plus measurement noise) follows the Gaussian distribution with
high probability if the measurement noise is Gaussian.
It should be noted that the off-grid model in (4) is closely related to the on-grid one that can be obtained by
setting β = 0 in (4) (Φ (0) = A). In fact, the off-grid model can be considered as the ﬁrst order approximation of
the true observation model while the on-grid one is the zeroth order approximation. As a result, the off-grid model
has a much smaller modeling error than the on-grid one. Such an advantage is twofold. First, by adopting the same
sampling grid the off-grid model results in higher accuracy, especially in the case of a low measurement noise where
the modeling error is the dominant modeling uncertainty. Second, a coarser sampling grid can be adopted in the
off-grid model to achieve a considerably reduced computational workload with a comparable modeling accuracy.
To estimate the DOAs θ we need to ﬁnd not only the support of the sparse signals x(t), t = 1, · · · , T, but also
the off-grid difference β. In this paper, we formulate the problem based on a Bayesian perspective and develop an
iterative algorithm to jointly estimate x(t), t = 1, · · · , T, and β in the following section.
OGSBI: Off-grid Sparse Bayesian Inference
We consider complex-valued signals throughout the paper since the matrix Φ (β) is complex-valued. We derive our
algorithm in the MMV case. The SMV is a special case by simply setting T = 1. Denote Y = [y(1), · · · , y(T)],
X = [x(1), · · · , x(T)] and E = [e(1), · · · , e(T)]. The off-grid DOA estimation model in (4) becomes
Y = Φ (β) X + E
with Φ (β) = A + Bdiag (β), Y , E ∈CM×T , X ∈CN×T , A, B, Φ (β) ∈CM×N and β ∈
matrix X of interest is jointly sparse (or row-sparse), i.e., all columns of X are sparse and share the same support.
Sparse Bayesian Formulation
Noise model
Under an assumption of white (circular symmetric) complex Gaussian noises, we have
p (E|α0) =
 e(t)|0, α−1
where α0 = σ−2 denotes the noise precision with σ2 being the noise variance, the probability density function
(PDF) of a (circular symmetric) complex Gaussian distributed random variable u ∼CN (µ, Σ) with mean µ and
covariance Σ is 
CN (u|µ, Σ) =
πN |Σ| exp
−(u −µ)H Σ−1 (u −µ)
Then we have
p (Y |X, α0, β) =
 y(t)|Φ (β) x(t), α−1
In this paper we assume that the noise precision α0 is unknown. A Gamma hyperprior is assumed for α0 since
it is a conjugate prior of the Gaussian distribution:
p (α0; c, d) = Γ (α0|c, d)
where Γ (α0|c, d) = [Γ (c)]−1 dcα0c−1 exp {−dα0} with Γ (·) being the Gamma function. We set c, d →0 as
in to obtain a broad hyperprior.
Sparse signal model
A sparse prior is needed for the jointly sparse matrix X of interest. We assume that the signals among snapshots are
independent and adopt the two-stage hierarchical prior: p (X; ρ) =
p (X|α) p (α; ρ) dα, where ρ > 0, α ∈RN,
Λ = diag (α) and
CN (x(t)|0, Λ) ,
p (α; ρ) =
Γ (αn|1, ρ) .
It is easy to show that all columns of X are independent and share the same prior. According to , for t =
1, · · · , T both ℜ{x(t)} and ℑ{x(t)} are Laplace distributed and share the same PDF that is strongly peaked at the
origin. As a result, the two-stage hierarchical prior is a sparse prior that favors most rows of X being zeros.
Off-grid distance model
We assume a uniform prior for β:
The prior is noninformative in the sense that the only information of β we use is its boundedness.
By combining the stages of the hierarchical Bayesian model, the joint PDF is
p (X, Y , α0, α, β) = p (Y |X, α0, β) p (X|α) p (α) p (α0) p (β)
with the distributions on the right hand side as deﬁned by (8), (10), (11), (9) and (12) respectively.
Bayesian Inference
An evidence procedure is exploited to perform the Bayesian inference since the exact posterior distribution
p (X, α0, α, β|Y ) cannot be explicitly calculated. Similar approaches have been used in standard Bayesian CS
methods . First it is easy to show that the posterior distribution of X is a complex Gaussian distribution:
p (X|Y , α0, α, β) =
CN (x(t)|µ(t), Σ)
µ(t) = α0ΣΦHy(t),
t = 1, · · · , T,
 α0ΦHΦ + Λ−1−1 .
Calculations of Σ and µ(t), t = 1, · · · , T, need estimates of the hyperparameters α0, α and β. In an evidence
procedure, they are estimated using an MAP estimate that maximizes p (α0, α, β|Y ). It can be easily observed
that to maximize p (α0, α, β|Y ) is equivalent to maximizing the joint PDF p (Y , α0, α, β) = p (α0, α, β|Y ) p (Y )
since p (Y ) is independent of the hyperparameters. An expectation-maximization (EM) algorithm is implemented
that treats X as a hidden variable and turns to maximizing E {log p (X, Y , α0, α, β)}, where p (X, Y , α0, α, β)
is given in (13) and E {·} denotes an expectation with respect to the posterior of X as given in (14) using the current
estimates of the hyperparameters.
Denote U = [µ(1), · · · , µ(T)] = α0ΣΦHY , X = X/
T, Y = Y /
T and ρ = ρ/T.
Following a similar procedure as in , it is easy to obtain the following updates of α and α0:
n = 1, · · · , N,
M + (c −1)/T
2 + Σnn, E
= ∥Y −ΦU∥2
n=1 γn with γn = 1 −α−1
For β, its estimate maximizes E {log p (Y |X, α0, β) p (β)} by (13) and thus minimizes
∥y(t) −(A + Bdiag (β)) x(t)∥2
∥y(t) −(A + Bdiag (β)) µ(t)∥2
(A + Bdiag (β)) Σ (A + Bdiag (β))Ho
= βT P β −2vT β + C
where C is a constant term independent of β, P is a positive semi-deﬁnite matrix and
 U · UH + Σ
BH (y(t) −Aµ(t))
The detailed derivation of (19) is provided in Appendix for simplicity of exposition. As a result, we have
βnew = arg
βT P β −2vT β
Remark 1 Though an explicit expression of βnew cannot be given, by recognizing that β is jointly sparse with x,
the dimension of β can be reduced to K in the computation and hence βnew can be efﬁciently calculated. We provide
the details in Subsection 3.5.
The proposed OGSBI algorithm is implemented as follows. After initializations of the hyperparameters α, α0
and β, we calculate Σ and µ(t), t = 1, · · · , T, using the current values of the hyperparameters according to (16)
and (15) respectively. Then we update α, α0 and β according to (17), (18) and (22) respectively. The process is
repeated until some convergence criterion is satisﬁed. We note that OGSBI has guaranteed convergence since the
function p (α0, α, β|Y ) is guaranteed to increase at each iteration by the property of EM algorithm .
In this subsection we recall a subspace-based idea in that uses the SVD of the measurement matrix Y = USV H
to reduce the computation of the signal reconstruction process and the sensitivity to the measurement noise. Then
we incorporate it into our OGSBI algorithm. Consider the noise-free case where Y = ΦX with K ≤T. We
have Rank (Y ) ≤Rank (X) ≤K. Let V = [V 1 V 2], where V 1 and V 2 are matrices that consist of the ﬁrst K
and the rest T −K columns of V respectively. Then we have that Y SV = Y V 1 ∈CM×K preserves all signal
information. In a general case where noises exist, by the SVD we have Y V = [Y SV Y V 2], where the ﬁrst part
Y SV preserves most signal information and is to be used in the following signal recovery process while the second
part is abandoned. Denote XSV = XV 1 and ESV = EV 1. Then we have
Y SV = ΦXSV + ESV .
In (23), Y SV , XSV and ESV can be viewed as the new matrices of sensor measurements, source signals and
measurement noises respectively. The joint sparsity still holds in XSV . We do not exploit possible correlations that
exist between columns of XSV (and in ESV ), i.e., we still assume that XSV (and ESV ) have independent columns.1
It is then straightforward to apply the proposed OGSBI algorithm to estimate XSV , β and then the DOAs. We use
OGSBI-SVD to refer to the resulting algorithm.
Based on implementation details to be introduced in Subsection 3.5, it can be shown that OGSBI-SVD has a
computational complexity of order O
per iteration while that for OGSBI is O
iteration. An additional computational workload of order O
 M2T, MT 2
is for the SVD of Y in OGSBI-
SVD. Since it is empirically found that OGSBI-SVD converges much faster than OGSBI, the whole computational
workload of OGSBI-SVD is less than that of OGSBI in general.2
Source Power and DOA Estimation
We use the estimated source powers from different directions to form a spectrum of the proposed algorithm. In the
following we derive a formula to estimate the source powers. We take OGSBI-SVD as an example. The case of
OGSBI is similar with some modiﬁcations . Let c
X = XSV V H
1 be an estimate of the signal X. Then consider
X row by row and we have c
1 , bΣnnV 1V H
where we use bU and bΣ to denote the ﬁnal estimates
of the mean and covariance of XSV respectively. We use the expectation as an estimate of the power ℘n from
direction ˜θn (with a modiﬁcation of βn):
b℘n = E {℘n} = 1
bΣnnV 1V H
Like other spectral-based methods, the DOAs are estimated using the locations of the highest peaks of the
spectrum. Suppose that the grid indices of the highest K peaks of b℘are bnk, k = 1, · · · , K. The estimated K DOAs
will be bθk = ˜θbnk + bβbnk, k = 1, · · · , K.
Implementation Details
This subsection presents some details of our implementations of OGSBI and OGSBI-SVD. At each iteration of
OGSBI or OGSBI-SVD, an N × N matrix inversion is required when updating Σ according to (16). By M < N
the Woodbury matrix identity is applied to give Σ = Λ −ΛΦHC−1ΦΛ with C = α−1
0 I + ΦΛΦH ∈CM×M.
By the fact that β is jointly sparse with x(t) whose K nonzero entries correspond to the locations of the K
sources, we calculate only entries of β that correspond to locations of the maximum K entries of α and set others to
zeros. As a result, β, P and v can be truncated into dimension of K or K × K. We still use β, P and v hereafter to
denote their truncated versions for simplicity. By (22) and
βT P β −2vT β
= 2 (P β −v) we have βnew = ˇβ
if P is invertible and ˇβ = P −1v ∈
K. Otherwise, we update β elementwise, i.e., at each step we update
one βn by ﬁxing up the other entries of β. For n = 1, · · · , K, ﬁrst we let
ˇβn = vn −(P n)T
1The correlations between columns of the signal matrix (XSV in our case) have recently been studied in .
2A possible exception happens in the case of T ≫N where the computation for the SVD is quite heavy. A modiﬁed approach in such
a case is to partition Y ﬁrstly into blocks with each of about N columns, then operate the SVD on each block and keep the resulting signal
subspaces, and ﬁnally do another SVD on the new signal matrix composed of all signal subspaces. A model similar to (23) can be cast.
where u−n is u without the nth entry for a vector u. Then by constraining βn ∈
2r, if ˇβn < −1
otherwise.
It is easy to show that the objective function is guaranteed to decrease at each step with βn deﬁned in (26).
We terminate OGSBI and OGSBI-SVD if ∥αi+1−αi∥2
< τ or the maximum number of iterations is reached,
where τ is a user-deﬁned tolerance and the superscript i refers to the iteration.
Numerical simulations
In this section, we present our numerical results for the DOA estimation. A standard uniform linear array (ULA) of
M = 8 sensors is considered. The origin is set at the middle point of the ULA to reduce the approximation error in
(2). So we have Amn = exp
and Bmn = −jπ
sin ˜θn · Amn, m = 1, · · · , M,
n = 1, · · · , N, with j = √−1. A uniform sampling grid {0◦, r, 2r, · · · , 180◦} is considered with r being the grid
interval. The number of snapshots is set to T = 200 in the case of MMV. We consider only OGSBI-SVD in the MMV
case since it is empirically observed to converge faster and be more accurate in comparison with OGSBI. In OGSBI-
SVD, we set ρ = 0.01 and c = d = 1×10−4. We initialize α0 =
t=1 V ar{(Y SV )t}, α =
AH (Y SV )t
and β = 0, where |·| applies elementwise. We set τ = 10−3 and the maximum number of iterations to 1000. We
note that the proposed algorithm is insensitive to the initializations of α0, α and β, as well as to ρ if ρ is not too
large. As reported in , the estimate of α0 can be inaccurate in some cases. But we have observed minimal effects
on the result of DOA estimation. Readers are referred to for detailed discussions. All experiments are carried
out in Matlab v.7.7.0 on a PC with a Windows XP system and a 3GHz CPU. Matlab codes have been made available
online at 
Comparison with ℓ1-SVD
We take ℓ1-SVD in as a representative of on-grid model based methods and compare OGSBI-SVD with it in
terms of mean squared error (MSE) and computational time with respect to the grid interval r and SNR. In our
experiment, we consider SNR = 10 and 0dB, and r = 0.5◦, 1◦, 2◦and 4◦. In each trial, K = 2 sources θ1, θ2
are uniformly generated within intervals [58◦, 62◦] and [86◦, 90◦] respectively. Before presenting our comparison
results, we show using Kolmogorov-Smirnov test that the total noise (measurement noise plus approximation error)
in (4) is Gaussian distributed with a rate of at least 94% in all scenarios. This empirically validates the off-grid model.
For each combination (SNR, r), the MSE is averaged over R = 200 trials: MSE =
where the superscript i refers to the ith trial. It should be noted that there exists a lower bound for the MSE of
ℓ1-SVD regardless of the SNR since the best DOA estimate that ℓ1-SVD can obtain is the grid point nearest to the
true DOA. In fact, the lower bound is shared among all on-grid model based methods including CS-MUSIC, SA-
MUSIC and the algorithm in . By assuming that the true DOA is uniformly distributed, the lower bound can be
easily calculated as LB = r2/12.3 Fig. 1 presents our experimental results. In all scenarios under consideration,
OGSBI-SVD has more accurate DOA estimation than ℓ1-SVD. Moreover, OGSBI-SVD can exceed the lower bound
for ℓ1-SVD in most scenarios. The phenomenon is signiﬁcant in the case of a higher SNR or a coarser sampling grid
where the on-grid model has a poor performance on describing the true observation model while it is overcome to a
large extent by the off-grid model used in this paper.
3The presented lower bound is, in fact, the expectation in the case of limited trials. The variance approaches zero as the number of trials
gets large.
Grid interval (degrees)
Lower bound for L1−SVD
L1−SVD, SNR = 10dB
L1−SVD, SNR = 0dB
OGSBI−SVD, SNR = 10dB
OGSBI−SVD, SNR = 0dB
Figure 1: MSEs of OGSBI-SVD and ℓ1-SVD. The lower bound is for ℓ1-SVD regardless of the SNR.
Table 1: Averaged Time Consumptions of ℓ1-SVD and OGSBI-SVD With Respect to SNR and r. Time unit: sec.
SNR = 10dB
Table 1 presents the averaged CPU times of OGSBI-SVD and ℓ1-SVD (excluding the SVD process that takes
about 0.003s in our case) with respect to SNR and r.4 For both OGSBI-SVD and ℓ1-SVD, their CPU times decrease
as the grid gets coarser. OGSBI-SVD is faster than ℓ1-SVD at r = 2◦and 4◦. One drawback of the proposed method
is that it is slow in the case of a dense sampling grid. In practice, we recommend to use a coarser grid with r = 2◦
for the proposed algorithm since it can give an accurate yet fast DOA estimation.
(1) We choose ℓ1 optimization for comparison because it is typically known to have better theoretical guarantee
for sparse recovery, though simpler solvers in CS, e.g., OMP , may succeed in our setting where the two
sources are well separated and have lower computational cost in such low dimensional problems. Readers are
referred to for more simulation results of ℓ1 optimization and the proposed method in the case of closely
spaced sources.
(2) Another advantage of the proposed algorithm is its smaller DOA estimation bias in comparison with that of
ℓ1-SVD .
4The code of ℓ1-SVD is provided by the author of . We note that its speed can be accelerated using state-of-the-art algorithms for CS.
Table 2: Averaged MSEs and CPU Times of STLS and OGSBI in the SMV Case With Respect to r.
Time (sec)
Table 3: Averaged MSEs of OGSBI-SVD in the Presence of Outliers. κ denotes a ratio by which outliers are
augmented.
Comparison with STLS
The off-grid model has recently been used in for DOA estimation. In , a sparse total least-squares (STLS)
approach is proposed. In the SMV case, STLS seeks to solve the nonconvex optimization problem
2 + ∥y −(A + Bdiag (β)) x∥2
2 + λ ∥x∥1
where x is the sparse source signal of interest, y is the noisy measurement, A, B and β are the same as deﬁned
in the off-grid model, and λ > 0 is a regularization parameter. From the Bayesian perspective, this is equivalent to
seeking for an MAP solution of (x, β) by assuming that the measurement noise is white Gaussian, x is Laplacian
and β is Gaussian. It is noted that the last assumption for β cannot properly capture the property of β. A local
minima of the problem in (27) is achieved in by an alternating approach, i.e., alternatively solving x with a
ﬁxed β, which requires a solution to an ℓ1-regularized least square problem, and solving β with a ﬁxed x, which
requires a solution to an N dimensional linear system. As argued in the SVD used in OGSBI-SVD can alleviate
the sensitivity to the measurement noise in the MMV case that is not used in . To make a fair comparison, we
consider only the SMV case when comparing our method with STLS though a similar problem can be cast for STLS
in the MMV case. In our implementation of OGSBI, we initialize α0 =
V ar{y} and α =
. The rest
settings are the same as those for OGSBI-SVD in the MMV case.
In our experiment, we consider two DOAs from 63.2◦and 90.3◦with SNR = 20dB. We consider r = 2◦and 4◦
for both OGSBI and STLS. The parameter λ in (27) is tuned to our best such that STLS achieves the smallest error.
Table 2 presents the averaged MSEs and CPU times of STLS and OGSBI over R = 200 trials. OGSBI obtains more
accurate DOA estimations than STLS in both the scenarios with remarkably less computational times. We also note
that it is possible to accelerate STLS using state-of-the-art algorithms for CS.
Sensitivity to Measurement Outliers
The SVD procedure in OGSBI-SVD is related to the principal component analysis (PCA). As is known that the
standard PCA is sensitive to outliers. Even a single corrupted measurement can deteriorate the quality of the approximation. In this subsection we carry out experiments to check whether the proposed OGSBI-SVD is sensitive to
measurement outliers due to the SVD. The experimental setup is similar to that in Subsection 4.1 but with SNR = ∞.
After acquiring the noiseless measurements, we randomly choose 3 out of the MT = 1600 measurements, multiply
by a constant ratio κ and then save as the outliers. Beside the case of no outliers (ratio = 1) we consider ﬁve other
cases where κ is set to 5, 10, 20, 50 and 100 respectively. Table 3 presents our simulation results of the MSEs. It can
be seen that the estimation accuracy of OGSBI-SVD can degrade signiﬁcantly even with about 0.2% measurements
being corrupted due to the sensitivity of the SVD.
Note that the corrupted measurement matrix Y due to the outliers is a sum of a low-rank matrix (noiseless
measurement matrix of rank K) and a sparse matrix (outliers). A robust PCA technique has recently been proposed
in that can recover the original low-rank matrix from the sparse outliers. So, it is possible to combine the robust
PCA technique in with the proposed OGSBI-SVD to improve its robustness to outliers, which, however, is
beyond the scope of this paper.
Conclusion
In this paper, we studied the off-grid DOA estimation model ﬁrstly proposed in for reducing the modeling error
due to discretization of a continuous range. We proposed an algorithm based on the off-grid model from a Bayesian
perspective that is applicable to both single snapshot and multi-snapshot cases. A subspace-based idea was used
to reduce the computational complexity of the signal recovery process and the sensitivity to noise. We illustrated
by simulations that the proposed approach outperforms standard CS methods whose performance is limited by the
underlying standard on-grid model. It is also more accurate than the algorithm in based on the off-grid model.
One drawback of the proposed algorithm is its slow speed in the case of a dense sampling grid though a coarser
grid can be adopted to obtain an accurate yet fast DOA estimation. A future work is to develop fast versions of our
algorithm. After this work, we have shown in that ℓ1 optimization also works for the off-grid DOA estimation
problem, where performance guarantees are also provided under some conditions.
Appendix: Derivation of (19)
Denote ∆= diag (β). Eq. (19) is based on the following two equalities:
∥y −(A + B∆) µ∥2
= ∥(y −Aµ) −B · diag (µ) · β∥2
diag (µ) BH (y −Aµ)
(A + B∆) Σ (A + B∆)Ho
 T β + βT 
where C1, C2 are constants independent of β, and the equality
diagH (u) Q · diag (v) · RT
= uH (Q ⊙R) v
for vectors u, v and matrices Q, R with proper dimensions is used. Note that βT Sβ ∈R for a positive semi-deﬁnite
matrix S with proper dimension and thus βT Sβ = ℜ
= βT · ℜS · β since β is real-valued. Then (19) is
obtained by observing that both BHB ⊙µµH and Σ ⊙BHB are positive semi-deﬁnite.