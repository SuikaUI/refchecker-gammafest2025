JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
CNN: Single-label to Multi-label
Yunchao Wei, Wei Xia, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao, Senior Member, IEEE
Shuicheng Yan, Senior Member, IEEE
Abstract—Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classiﬁcation
tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying
object layouts and insufﬁcient multi-label training images. In this work, we propose a ﬂexible deep CNN infrastructure, called
Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a
shared CNN is connected with each hypothesis, and ﬁnally the CNN output results from different hypotheses are aggregated with
max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this ﬂexible deep CNN infrastructure
include: 1) no ground-truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly
noisy and/or redundant hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may be well pre-trained with a
large-scale single-label image dataset, e.g. ImageNet; and 5) it may naturally output multi-label prediction results. Experimental
results on Pascal VOC2007 and VOC2012 multi-label image datasets well demonstrate the superiority of the proposed HCP
infrastructure over other state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3% after the fusion with
our complementary result in based on hand-crafted features on the VOC2012 dataset, which signiﬁcantly outperforms the
state-of-the-arts with a large margin of more than 7%.
Index Terms—Deep Learning, CNN, Multi-label Classiﬁcation
INTRODUCTION
INGLE-label image classiﬁcation, which aims to
assign a label from a predeﬁned set to an image,
has been extensively studied during the past few
years , , . For image representation and
classiﬁcation, conventional approaches utilize carefully designed hand-crafted features, e.g., SIFT ,
along with the bag-of-words coding scheme, followed
by the feature pooling , , and classic
classiﬁers, such as Support Vector Machine (SVM) 
and random forests . Recently, in contrast to the
hand-crafted features, learnt image features with deep
network structures have shown their great potential
in various vision recognition tasks , , ,
 . Among these architectures, one of the greatest breakthroughs in image classiﬁcation is the deep
convolutional neural network (CNN) , which has
achieved the state-of-the-art performance (with 10%
gain over the previous methods based on handcrafted features) in the large-scale single-label object
recognition task, i.e., ImageNet Large Scale Visual
Recognition Challenge (ILSVRC) with more than
one million images from 1,000 object categories.
Multi-label image classiﬁcation is however a more
general and practical problem, since the majority of
Yunchao Wei is with Department of Electrical and Computer Engineering,
National University of Singapore, and also with the Institute of Information Science, Beijing Jiaotong University, e-mail: .
Yao Zhao is with the Institute of Information Science, Beijing Jiaotong
University, Beijing 100044, China.
Bingbing Ni is with the Advanced Digital Sciences Center, Singapore.
Wei Xia, Junshi Huang, Jian Dong and Shuicheng Yan are with Department of Electrical and Computer Engineering, National University
of Singapore.
real-world images are with more than one objects
of different categories. Many methods , , 
have been proposed to address this more challenging
problem. The success of CNN on single-label image
classiﬁcation also sheds some light on the multilabel image classiﬁcation problem. However, the CNN
model cannot be trivially extended to cope with the
multi-label image classiﬁcation problem in an interpretable manner, mainly due to the following reasons.
Firstly, the implicit assumption that foreground objects are roughly aligned, which is usually true for
single-label images, does not always hold for multilabel images. Such alignment facilitates the design of
the convolution and pooling infrastructure of CNN
for single-label image classiﬁcation. However, for a
typical multi-label image, different categories of objects are located at various positions with different
scales and poses. For example, as shown in Figure 1,
for single-label images, the foreground objects are
roughly aligned, while for multi-label images, even
with the same label, i.e., horse and person, the spatial arrangements of the horse and person instances
vary largely among different images. Secondly, the
interaction between different objects in multi-label
images, like partial visibility and occlusion, also poses
a great challenge. Therefore, directly applying the
original CNN structure for multi-label image classiﬁcation is not feasible. Thirdly, due to the tremendous
parameters to be learned for CNN, a large number of
training images are required for the model training.
Furthermore, from single-label to multi-label (with n
category labels) image classiﬁcation, the label space
has been expanded from n to 2n, thus more training
 
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
Single-label images from ImageNet
Multi-label images from Pascal VOC
Fig. 1. Some examples from ImageNet and Pascal
VOC 2007 . The foreground objects in single-label
images are usually roughly aligned. However, the assumption of object alighment is not valid for multi-label
images. Also note the partial visibility and occlusion
between objects in the multi-label images.
data is required to cover the whole label space. For
single-label images, it is practically easy to collect
and annotate the images. However, the burden of
collection and annotation for a large scale multi-label
image dataset is generally extremely high.
To address these issues and take full advantage
of CNN for multi-label image classiﬁcation, in this
paper, we propose a ﬂexible deep CNN structure,
called Hypotheses-CNN-Pooling (HCP). HCP takes
an arbitrary number of object segment hypotheses as
the inputs, which may be generated by the sate-of-theart objectiveness detection techniques, e.g., binarized
normed gradients (BING) , and then a shared CNN
is connected with each hypothesis. Finally the CNN
output results from different hypotheses are aggregated by max pooling to give the ultimate multilabel predictions. Particularly, the proposed HCP infrastructure possesses the following characteristics:
ground-truth
information
required for training on the multi-label image dataset.
Different from previous works , , ,
ground-truth
information
annotation.
Since bounding box annotation is much more
costly than labelling, the annotation burden is
signiﬁcantly reduced. Therefore, the proposed
HCP has a better generalization ability when
transferred to new multi-label image datasets.
• The proposed HCP infrastructure is robust to the
noisy and/or redundant hypotheses. To suppress the
possibly noisy hypotheses, a cross-hypothesis
max-pooling operation is carried out to fuse the
outputs from the shared CNN into an integrative
prediction. With max pooling, the high predictive
scores from those hypotheses containing objects
are reserved and the noisy ones are ignored.
Therefore, as long as one hypothesis contains the
object of interest, the noise can be suppressed
after the cross-hypothesis pooling. Redundant
hypotheses can also be well addressed by max
• No explicit hypothesis label is required for training.
state-of-the-art
utilize the hypothesis label for training. They
ﬁrst compute the Intersection-over-Union (IoU)
overlap between hypotheses and ground-truth
bounding boxes, and then assign the hypothesis
with the label of the ground-truth bounding box
if their overlap is above a threshold. In contrast,
the proposed HCP takes an arbitrary number of
hypotheses as the inputs without any explicit
hypothesis labels.
• The shared CNN can be well pre-trained with a
large-scale single-label image dataset. To address
the problem of insufﬁcient multi-label training
images, based on the Hypotheses-CNN-Pooling
architecture, the shared CNN can be ﬁrst well
pre-trained
large-scale
single-label
dataset, e.g., ImageNet, and then ﬁne-tuned on
the target multi-label dataset.
intrinsically
multi-label
prediction results. HCP produces a normalized
probability distribution over the labels after the
softmax layer, and the the predicted probability
values are intrinsically the ﬁnal classiﬁcation
corresponding
categories.
Extensive experiments on two challenging multilabel image datasets, Pascal VOC 2007 and VOC 2012,
well demonstrate the superiority of the proposed
HCP infrastructure over other state-of-the-arts. The
rest of the paper is organized as follows. We brieﬂy
review the related work of multi-label classiﬁcation
in Section 2. Section 3 presents the details of the
HCP for image classiﬁcation. Finally the experimental
results and conclusions are provided in Section 4 and
Section 5, respectively.
RELATED WORK
During the past few years, many works on various
multi-label image classiﬁcation models have been conducted. These models are generally based on two
types of frameworks: bag-of-words (BoW) , ,
 , , and deep learning , , .
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
Hypotheses
extraction
dog，person，sheep
Shared convolutional neural network
Scores for individual
hypothesis
Fig. 2. An illustration of the infrastructure of the proposed HCP. For a given multi-label image, a set of input
hypotheses to the shared CNN is selected based on the proposals generated by the state-of-the-art objectness
detection techniques, e.g., BING . The shared CNN has a similar network structure to except for the
layer fc8, where c is the category number of the target multi-label dataset. We feed the selected hypotheses into
the shared CNN and fuse the outputs into a c-dimensional prediction vector with cross-hypothesis max-pooling
operation. The shared CNN is ﬁrstly pre-trained on the single-label image dataset, e.g., ImageNet and then ﬁnetuned with the multi-label images based on the squared loss function. Finally, we retrain the whole HCP to further
ﬁne-tune the parameters for multi-label image classiﬁcation.
Bag-of-Words Based Models
A traditional BoW model is composed of multiple
modules, e.g., feature representation, classiﬁcation
and context modelling. For feature representation,
the main components include hand-crafted feature
extraction, feature coding and feature pooling, which
generate global representations for images. Speciﬁcally, hand-crafted features, such as SIFT , Histogram of Oriented Gradients and Local Binary
Patterns are ﬁrstly extracted on dense grids or
sparse interest points and then quantized by different
coding schemes, e.g., Vector Quantization , Sparse
Coding and Gaussian Mixture Models . These
encoded features are ﬁnally pooled by feature aggregation methods, such as Spatial Pyramid Matching
(SPM) , to form the image-level representation. For
classiﬁcation, conventional models, such as SVM 
and random forests , are utilized. Beyond conventional modelling methods, many recent works ,
 , , , have demonstrated that the usage
of context information, e.g., spatial location of object
and background scene from the global view, can
considerably improve the performance of multi-label
classiﬁcation and object detection.
Although these works have made great progress
in visual recognition tasks, the involved hand-crafted
features are not always optimal for particular tasks.
Recently, in contrast to hand-crafted features, learnt
features with deep learning structures have shown
great potential for various vision recognition tasks,
which will be introduced in the following subsection.
Deep Learning Based Models
Deep learning tries to model the high-level abstractions of visual data by using architectures composed of multiple non-linear transformations. Specifically, deep convolutional neural network (CNN) 
has demonstrated an extraordinary ability for image classiﬁcation , , , , on singlelabel datasets such as CIFAR-10/100 and ImageNet .
architectures
adopted to address multi-label problems. Gong et
al. studied and compared several multi-label
loss functions for the multi-label annotation problem
based on a similar network structure to . However,
due to the large number of parameters to be learned
for CNN, an effective model requires lots of training
samples. Therefore, training a task-speciﬁc convolutional neural network is not applicable on datasets
with limited numbers of training samples.
Fortunately, some recent works , , , ,
 , have demonstrated that CNN models pretrained on large datasets with data diversity, e.g.,
ImageNet, can be transferred to extract CNN features
for other image datasets without enough training
data. Pierre et al. and Razavian et al. proposed
a CNN feature-SVM pipeline for multi-label classiﬁcation. Speciﬁcally, global images from a multi-label
dataset are directly fed into the CNN, which is pretrained on ImageNet, to get CNN activations as the
off-the-shelf features for classiﬁcation. However, different from the single-label image, objects in a typical
multi-label image are generally less-aligned, and also
often with partial visibility and occlusion as shown
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
in Figure 1. Therefore, global CNN features are not
optimal to multi-label problems. Recently, Oquab et
al. and Girshick et al. presented two proposalbased methods for multi-label classiﬁcation and detection. Although considerable improvements have been
made by these two approaches, these methods highly
depend on the ground-truth bounding boxes, which
may limit their generalization ability when transferred
to a new multi-label dataset without any bounding
box information.
In contrast, the proposed HCP infrastructure in this
paper requires no ground-truth bounding box information for training and is robust to the possibly noisy
and/or redundant hypotheses. Different from ,
 , no explicit hypothesis label is required during
the training process. Besides, we propose a hypothesis
selection method to select a small number of highquality hypotheses (10 for each image) for training,
which is much less than the number used in 
(128 for each image), thus the training process is
signiﬁcantly sped up.
HYPOTHESES-CNN-POOLING
Figure 2 shows the architecture of the proposed
Hypotheses-CNN-Pooling (HCP) deep network. We
apply the state-of-the-art objectness detection technique, i.e., BING , to produce a set of candidate
object windows. A much smaller number of candidate windows are then selected as hypotheses by the
proposed hypotheses extraction method. The selected
hypotheses are fed into a shared convolutional neural
network (CNN). The conﬁdence vectors from the input hypotheses are combined through a fusion layer
with max pooling operation, to generate the ultimate
multi-label predictions. In speciﬁc, the shared CNN
is ﬁrst pre-trained on a large-scale single-label image
dataset, i.e., ImageNet and then ﬁne-tuned on the
target multi-label dataset, e.g., Pascal VOC, by using
the entire image as the input. After that, we retrain
the proposed HCP with a squared loss function for
the ﬁnal prediction.
Hypotheses Extraction
HCP takes an arbitrary number of object segment
hypotheses as the inputs to the shared CNN and
fuses the prediction of each hypothesis with the max
pooling operation to get the ultimate multi-label predictions. Therefore, the performance of the proposed
HCP largely depends on the quality of the extracted
hypotheses. Nevertheless, designing an effective hypotheses extraction approach is challenging, which
should satisfy the following criteria:
High object detection recall rate: The proposed HCP
is based on the assumption that the input hypotheses
can cover all single objects of the given multi-label
image, which requires a high detection recall rate.
(a) Source image. (b) Hypothesis bounding
boxes generated by BING. Different colors indicate
different clusters, which are produced by normalized
cut. (c) Hypotheses directly generated by the bounding
boxes. (d) Hypotheses generated by the proposed HS
Small number of hypotheses: Since all hypotheses
of a given multi-label image need to be fed into the
shared CNN simultaneously, more hypotheses cost
more computational time and need more powerful
hardware (e.g., RAM and GPU). Thus a small hypothesis number is required for an effective hypotheses
extraction approach.
High computational efﬁciency: As the ﬁrst step of
the proposed HCP, the efﬁciency of hypotheses extraction will signiﬁcantly inﬂuence the performance
of the whole framework. With high computational
efﬁciency, HCP can be easily integrated into real-time
applications.
In summary, a good hypothesis generating algorithm should generate as few hypotheses as possible
in an efﬁcient way and meanwhile achieve as high
recall rate as possible.
During the past few years, many methods ,
 , , , , have been proposed to tackle
the hypotheses detection problem. , , are
based on salient object detection, which try to detect
the most attention-grabbing (salient) object in a given
image. However, these methods are not applicable
to HCP, since saliency based methods are usually
applied to a single-label scheme while HCP is a multilabel scheme. , , are based on objectness proposal (hypothesis), which generate a set of hypotheses
to cover all independent objects in a given image. Due
to the large number of proposals, such methods are
usually quite time-consuming, which will affect the
real-time performance of HCP.
Most recently, Cheng et al. proposed a surprisingly simple and powerful feature called binarized
normed gradients (BING) to ﬁnd object candidates by
using objectness scores. This method is faster than most popular alternatives , , and has a high object detection
recall rate (96.2% with 1,000 hypotheses). Although
the number of hypotheses (i.e., 1,000) is very small
compared with a common sliding window paradigm,
it is still very large for HCP.
To address this problem, we propose a hypotheses
selection (HS) method to select hypotheses from the
proposals extracted by BING. A set of hypothesis
bounding boxes are produced by BING for a given
image, denoted by H = {h1, h2, ..., hn}, where n is
the hypothesis number. An n × n afﬁnity matrix W is
constructed, where Wij (i, j <= n) is the IoU scores
between bi and bj, which can be deﬁned as
Wij = |hi ∩hj|
where |·| is used to measure the number of pixels.
The normalized cut algorithm is then adopted to
group the hypothesis bounding boxes into m clusters.
As shown in Figure 3(b), different colors indicate
different clusters. We empirically ﬁlter out those hypotheses with small areas or with high height/width
(or width/height) ratios, as those shown in Figure 3(c)
with red bounding boxes. For each cluster, we pick
out the top k hypotheses with higher predictive
scores generated by BING and resize them into square
shapes. As a result, mk hypotheses, which are much
fewer than those directly generated by BING, will be
selected as the inputs of HCP for each image.
Initialization of HCP
In the proposed HCP, the architecture of the shared
CNN is similar to the network described in . The
shared CNN contains ﬁve convolutional layers and
three fully-connected layers with 60 million parameters. Therefore, without enough training images, it
is very difﬁcult to obtain an effective HCP model
for multi-label classiﬁcation. However, to collect and
annotate a large-scale multi-label dataset is generally
unaffordable. Fortunately, a large-scale single-label
image dataset, i.e., ImageNet, can be used to pre-train
the shared CNN for parameter initialization, since
each image of multi-label is ﬁrstly cropped into many
hypotheses and each hypothesis is assumed to contain
at most one object based on the architecture of HCP.
However, directly using the parameters pre-trained
by ImageNet to initialize the shared CNN is not
appropriate, due to the following reasons. Firstly, both
the data amount and the object categories between
ImageNet and the target muliti-label dataset are usually different. Secondly, there exist very diverse and
complicated interactions among the objects in a multilabel image, which makes multi-label classiﬁcation
more challenging than single-label classiﬁcation. To
better initialize the shared CNN, based on the pretrained parameters by ImageNet, ﬁne-tuning is enforced to adjust the parameters.
Single-label Images
(e.g. ImageNet)
Multi-label Images
(e.g. Pascal VOC)
Pre-training on single-label image set
Image-fine-tuning on multi-label image set
Parameters transferring
The initialization of HCP is divided into two
steps. The shared CNN is ﬁrst pre-trained on a singlelabel image set, e.g., ImageNet and then ﬁne-tuned
on the target multi-label image set using the entire
image as input. Parameters pre-trained on ImageNet
are directly transferred for ﬁne-tuning except for the
last fully-connected layer, since the category numbers
between these two datasets are different.
As shown in Figure 4, the initialization process of
HCP is divided into two steps.
Step1: Pre-training on single-label image set. We
use the ImageNet to pre-train the shared CNN.
Given an image, we ﬁrst resize it into 256×256 pixels.
Then, we extract random 227×227 patches (and their
horizontal reﬂections) from the given image and train
our network based on these extracted patches. Each
extracted patch is pre-processed by subtracting the
image mean, and fed into the ﬁrst convolutional layer
of the CNN. Indicated by , the output of the last
fully-connected layer is fed into a 1,000-way softmax
layer with the multinomial logistic regression as the
loss function, to produce a probability distribution
over the 1,000 classes. For all layers, we use the recti-
ﬁed linear units (ReLU) as the nonlinear activation
function. We train the network by using stochastic
gradient descent with a momentum of 0.9 and weight
decay of 0.0005. To overcome overﬁtting, each of
the ﬁrst two fully-connected layers is followed by a
drop-out operation with a drop-out ratio of 0.5. The
learning rate is initialized as 0.01 for all layers and
reduced to one tenth of the current rate after every 20
epoches (90 epoches in all).
Step2: Image-ﬁne-tuning on multi-label image set.
To adapt the pre-trained model on ImageNet to HCP,
the entire images from a multi-label image set, e.g.,
Pascal VOC, are then utilized to further adjust the
parameters. The image-ﬁne-tuning (I-FT) process is
similar with the pre-training except for several details
listed as follows.
Each image is resized into 256×256 pixels without
cropping. Since the category number of Pascal VOC
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
is not equal to that of ImageNet, the output of the
last fully-connected layer is fed into a c-way softmax
which produces a probability distribution over the c
class labels. Different from the pre-training, squared
loss is used during I-FT. Suppose there are N images
in the multi-label image set, and yi = [yi1, yi2, · · · , yic]
is the label vector of the ith image. yij = 1 (j =
1, · · · , c) if the image is annotated with class j, and
otherwise yij = 0. The ground-truth probability vector
of the ith image is deﬁned as ˆpi = yi/||yi||1 and the
predictive probability vector is pi = [pi1, pi2, · · · , pic].
And then the cost function to be minimized is deﬁned
(pik −ˆpik)2.
During the I-FT process, as shown in Figure 4, the parameters of the ﬁrst seven layers are initialized by the
parameters pre-trained on ImageNet and the parameters of the last fully-connected layer are randomly
initialized with a Gaussian distribution G(µ, σ)(µ =
0, σ = 0.01). The learning rates of the convolutional
layers, the ﬁrst two fully-connected layers and the
last fully-connected layer are initialized as 0.001, 0.002
and 0.01 at the beginning, respectively. We executed
60 epoches in total and decreased the learning rate
to one tenth of the current rate of each layer after 20
epoches (momentum=0.9, weight decay=0.0005).
By setting the different learning rates for different
layers, the updating rates for the parameters from
different layers also vary. The ﬁrst few convolutional
layers mainly extract some low-level invariant representations, thus the parameters are quite consistent
from the pre-trained dataset to the target dataset,
which is achieved by a very low learning rate (i.e.,
0.001). Nevertheless, in the ﬁnal layers of the network,
especially the last fully-connected layer, which are
speciﬁcally adapted to the new target dataset, a much
higher learning rate is required to guarantee a fast
convergence to the new optimum. Therefore, the parameters can better adapt to the new dataset without
clobbering the pre-trained initialization. It should be
noted that the I-FT is a critical step of HCP. We tried
without this step and found that the performance on
VOC 2007 dropped dramatically.
Hypotheses-ﬁne-tuning
All the l = mk hypotheses are fed into the shared
CNN, which has been initialized as elaborated in Section 3.2. For each hypothesis, a c-dimensional vector
can be computed as the output of the shared CNN.
Indeed, the proposed HCP is based on the assumption
that each hypothesis contains at most one object and
all the possible objects are covered by some subset
of the extracted hypotheses. Therefore, the number
of hypotheses should be large enough to cover all
possible diversiﬁed objects. However, with more hypotheses, noise (hypotheses covering no object) will
inevitably increase.
To suppress the possibly noisy hypotheses, a crosshypothesis max-pooling is carried out to fuse the outputs into one integrative prediction. Suppose vi(i =
1, ..., l) is the output vector of the ith hypothesis from
the shared CNN and v(j)
(j = 1, . . . , c) is the jth
component of vi. The cross-hypothesis max-pooling
in the fusion layer can be formulated as
v(j) = max(v(j)
2 , . . . , v(j)
where v(j) can be considered as the predicted value
for the jth category of the given image.
The cross-hypothesis max-pooling is a crucial step
for the whole HCP framework to be robust to the
noise. If one hypothesis contains an object, the output
vector will have a high response (i.e., large value)
on the jth component, meaning a high conﬁdence for
the corresponding jth category. With cross-hypothesis
max-pooling, large predicted values corresponding to
objects of interest will be reserved, while the values
from the noisy hypotheses will be ignored.
During the hypotheses-ﬁne-tuning (H-FT) process,
the output of the fusion layer is fed into a c-way
softmax layer with the squared loss as the cost function, which is deﬁned as Eq. (2). Similar as I-FT, we
also adopt a discriminating learning rate scheme for
different layers. Speciﬁcally, we execute 60 epoches in
total and empirically set the learning rates of the convolutional layers, the ﬁrst two fully-connected layers
and the last fully-connected layer as 0.0001, 0.0002,
0.001 at the beginning, respectively. We decrease the
learning rates to one tenth of the current ones after
every 20 epoches. The momentum and the weight
decay are set as 0.9 and 0.0005, which are the same as
in the I-FT step.
Multi-label Classiﬁcation for Test Image
Based on the trained HCP model, the multi-label
classiﬁcation of a given image can be summarized as
follows. We ﬁrstly generate the input hypotheses of
the given image based on the proposed HS method.
Then, for each hypothesis, a c-dimensional predictive
result can be obtained by the shared CNN. Finally, we
utilize the cross-hypothesis max-pooling operation to
produce the ﬁnal prediction. As shown in Fig. 5, the
second row and the third row indicate the generated
hypotheses and the corresponding outputs from the
shared CNN. For each object independent hypothesis,
there is a high response on the corresponding category (e.g., for the ﬁrst hypothesis, the response on
car is very high). After cross-hypothesis max-pooling
operation, as indicated by the last row in Fig. 5, the
high responses (i.e., car, horse and person), which can
be considered as the predicted labels, are reserved.
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
Generate hypotheses
Input the shared CNN
Cross-hypothesis max-pooling
Fig. 5. An illustration of the proposed HCP for a VOC
2007 test image. The second row indicates the generated hypotheses. The third row indicate the predicted
results for the input hypotheses. The last row is predicted result for the test image after cross-hypothesis
max-pooling operation.
EXPERIMENTAL RESULTS
In this section, we present the experiments to validate
the effectiveness of our proposed Hypotheses-CNN-
Pooling (HCP) framework for multi-label image classiﬁcation.
Datasets and Settings
We evaluate the proposed HCP on the PASCAL Visual Object Classes Challenge (VOC) datasets ,
which are widely used as the benchmark for multilabel classiﬁcation. In this paper, PASCAL VOC 2007
and VOC 2012 are employed for experiments. These
two datasets, which contain 9,963 and 22,531 images
respectively, are divided into train, val and test subsets.
We conduct our experiments on the trainval/test splits
 . The evaluation metric is Average Precision (AP)
and mean of AP (mAP) complying with the PASCAL
challenge protocols.
Instead of using two GPUs as in , we conduct
experiments on one NVIDIA GTX Titan GPU with
6GB memory and all our training algorithms are
based on the code provided by Jia et al. . The
initialization of the shared CNN is based on the
parameters pre-trained on the 1,000 classes and 1.2
million images of ILSVRC-2012.
We compare the proposed HCP with the state-ofthe-art approaches. Speciﬁcally, the competing algorithms are generally divided into two types: those
based on hand-crafted features and those based on
learnt features.
contextual combination method of localization
and classiﬁcation to improve the performance
for both. Speciﬁcally, for classiﬁcation, image
representation is built on the traditional feature
extraction-coding-pooling pipeline, and object
localization is built on sliding-widow approaches.
Furthermore, the localization is employed to
enhance the classiﬁcation performance.
• FV : The Fisher Vector representation of
images can be considered as an extension of the
bag-of-words. Some well-motivated strategies,
e.g., L2 normalization, power normalization and
spatial pyramids, are adopted over the original
Fisher Vector to boost the classiﬁcation accuracy.
• NUS: In , Chen et al. presented an Ambiguityguided Mixture Model (AMM) to seamlessly
integrate external context features and object
features for general classiﬁcation, and then the
contextualized
iteratively and mutually boost the performance
classiﬁcation
Guided Subcategory (AGS) mining approach,
which can be seamlessly integrated into an
effective subcategory-aware object classiﬁcation
framework,
classiﬁcation
performance.
version of the above two, NUS-PSL received
the winner prizes of the classiﬁcation task in
PASCAL VOC 2010-2012.
• CNN-SVM : OverFeat , which obtained
competitive
performance
classiﬁcation task of ILSVRC 2013, was released
extractor.
Razavian et al. employed OverFeat, which is
pre-trained on ImageNet, to get CNN activations
as the off-the-shelf features. The state-of-the-art
classiﬁcation result on PASCAL VOC 2007 was
achieved by using linear SVM classiﬁers over
dimensional
representation
extracted from the 22nd layer of OverFeat.
• I-FT: The structure of the shared CNN follows
that of Krizhevsky et al. . The shared CNN
was ﬁrst pre-trained on ImageNet, and then
the last fully-connected layer was modiﬁed into
4096×20, and the shared CNN was re-trained
with squared loss function on PASCAL VOC for
multi-label classiﬁcation.
• PRE-1000C and PRE-1512 : Oquab et al. pro-
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
loggerhead
American lobster
cheeseburger
bell pepper
cottontail
Fig. 6. Exemplar images with ground-truth bounding
boxes from the detection dataset of ILSVRC 2013.
posed to transfer image representations learned
with CNN on ImageNet to other visual recognition tasks with limited training data. The network has exactly the same architecture as in .
Firstly, the network is pre-trained on ImageNet.
Then the ﬁrst seven layers of CNN are ﬁxed
with the pre-trained parameters and the last
fully-connected layer is replaced by two adaptation layers. Finally, the adaptation layers are
trained with images from the target PASCAL
VOC dataset. PRE-1000C and PRE-1512 mean
the transferred parameters are pre-trained on the
original ImageNet dataset with 1000 categories
and the augmented one with 1512 categories,
respectively. For PRE-1000C, 1.2 million images
from ILSVRC-2012 are employed to pre-train the
CNN, while for PRE-1512, 512 additional ImageNet classes (e.g., furniture, motor vehicle, bicycle etc.) are augmented to increase the semantic
overlap with categories in PASCAL VOC. To
accommodate the larger number of classes, the
dimensions of the ﬁrst two fully-connected layers
are increased from 4,096 to 6,144.
Hypotheses Extraction
In , BING1 has shown a good generalization ability on the images containing object categories that
are not used for training. Speciﬁcally, trained a
linear SVM using 6 object categories (i.e., the ﬁrst 6
categories in VOC dataset according to alpha order)
and the remaining 14 categories were used for testing.
The experimental results in demonstrate that the
transferred model almost has the same performance
1. 
with that using all categories (all the 20 categories in
PASCAL VOC) for training.
Since the proposed HCP is independent of the
ground-truth bounding box, no object location information can be used for training. Inspired by the
generalization ability test in , the detection dataset
of ILSVRC 2013 is used as augmented data for BING
training. It contains 395,909 training images with
ground-truth bounding box annotation from 200 categories. To validate the generalization ability of the
proposed framework for other multi-label datasets,
the categories as well as their subcategories which
are semantically overlapping with the PASCAL VOC
categoires are removed2.
For a fair comparison, we follow and only use
randomly 13,894 images (instead of all images) from
the detection dataset of ILSVRC 2013 for model training. Some selected samples are illustrated in Figure 6,
from which we can see that there are big differences
between objects in PASCAL VOC and the selected
ImageNet samples. After training, the learnt BING-
ImageNet model is used to produce hypotheses for
VOC 2007 and VOC 2012. We test the object detection
rate with 1000 proposals on VOC 2007, which is only
0.3% lower than the reported result (i.e., 96.2%) in .
Considering the computational time and the limitation of hardware, we proposed a hypotheses selection
(HS) method to ﬁlter the input hypotheses produced
by BING-ImageNet. As elaborated in Section 3.1, we
cluster the extracted proposals into 10 clusters based
on their bounding box overlapping information by
normalized-cut . The hypotheses are ﬁltered out,
which have smaller areas than 900 pixels or larger
height/width (or width/height) ratios than 4. Some
exemplar hypotheses extracted by the proposed HS
method are shown in Figure 7. We sort the hypotheses
for each cluster based on the predicted objectness
scores and show the ﬁrst ﬁve hypotheses.
During the training step, for each training image,
the top k hypotheses from each cluster are selected
and fed into the shared CNN. We experimentally vary
k = 1, 2, 3, 4, 5 to train the proposed HCP on VOC
2007 and observe that the performance changes only
slightly on the testing dataset. Therefore, we set k = 1
(i.e., 10 hypotheses for each image) for both VOC 2007
and VOC 2012 during the training stage to reduce
the training time. To achieve high object recall rate,
500 hypotheses (i.e., k = 50) are extracted from each
test image during the testing stage. On VOC 2012, the
hypotheses-ﬁne-tuning step takes roughly 20 hours.
For each testing image, about 1.5 second is cost.
2. The removed categories include bicycle, bird, water bottle, bus,
car, domestic cat, chair, table, dog, horse, motorcycle, person, ﬂower pot,
sheep, sofa, train, tv or monitor, wine bottle, watercraft, unicycle, cattle
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
dog, horse, person
cat, person, plant,
cat, chair, table,
plant, sofa
Fig. 7. Exemplar hypotheses extracted by the proposed HS method. For each image, the ground-truth bounding
boxes are shown on the left and the corresponding hypotheses are shown on the right. C1-C10 are the 10
clusters produced by normalized-cut and hypotheses in the same cluster share similar location information.
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
squared loss
Fig. 8. The changing trend of the squared loss during
the I-FT step on VOC 2007.
Fig. 9. The changing trend of mAP scores during the
I-FT step on VOC 2007. The mAP converges fast to
74.4% after almost 15 epoches on test dataset.
Initialization of HCP
As discussed in Section 3.2, the initialization process of HCP consists of two steps: Pre-training and
Image-ﬁne-tuning (I-FT). Since the structure setting
of the shared-CNN is almost consistent with the pretrained model implemented by , we apply the pretrained model on ImageNet by to initialize the
convolutional layers and the ﬁrst two fully-connected
layers of the shared CNN. For I-FT, we use images
from PASCAL VOC to re-train the shared CNN. As
shown in Figure 4, the pre-trained parameters for
the ﬁrst seven layers are transferred to initialize the
CNN for ﬁne-tuning. The last fully-connected layer
with 4096×20 parameters is randomly initialized with
Gaussian distribution.
Actually, similar as Gong et al. , the class labels of a given image can also be predicted by the
ﬁne-tuned model at the I-FT stage. Figure 8 shows
the changing trends of the squared loss of different
epoches on VOC 2007 during I-FT. The corresponding
change of mAP score on the test dataset is shown in
Figure 9. We can see that the mAP score based on the
image-ﬁne-tuned model can achieve 74.4% on VOC
2007, which is more competitive than the scheme of
CNN features with SVM classiﬁer .
Image Classiﬁcation Results
Image Classiﬁcation on VOC 2007: Table 1 reports
our experimental results compared with the state-ofthe-arts on VOC 2007. The upper part of Table 1 shows
the methods not using ground-truth bounding box
information for training, while the lower part of the
table shows the methods with that information. Besides, CNN-SVM, I-FT, HCP-1000C, HCP-2000C and
PRE-1000C are methods using additional images for
training from an extra dataset, i.e., ImageNet, and
the other methods only utilize PASCAL VOC data
for training. In speciﬁc, HCP-1000C indicates that the
initialized parameters of the shared CNN are pretrained on the 1.2 million images from 1000 categories
of ILSVRC-2012. Similar as , for HCP-2000C, we
augment the ILSVRC-2012 training set with additional
1,000 ImageNet classes (about 0.8 million images)
to improve the semantic overlap with classes in the
Pascal VOC dataset.
From the experimental results, we can see that the
CNN based methods which utilize additional images from ImageNet have a 2.6%∼13.9% improvement
compared with the state-of-the-art methods based on
hand-crafted features, i.e., 71.3% . By utilizing the
ground-truth bounding box information, a remarkable
improvement can be achieved for both deep learning based methods (PRE-1000C vs. CNN-SVM and
I-FT) and hand-crafted feature based methods (AGS
and AMM vs. INRIA and FV). However, bounding
box annotation is quite costly. Therefore, approaches
requiring ground-truth bounding boxes cannot be
transferred to the datasets without such annotation.
From Table 1, it can be seen that the proposed HCP
has a signiﬁcant improvement compared with the
state-of-the-art performance even without bounding
box annotation i.e., 81.5% vs. 77.7% (HCP-1000C vs.
PRE-1000C ). Compared with HCP-1000C, 3.7%
improvement can be achieved by HCP-2000C. Since
the proposed HCP requires no bounding box annotation, the proposed method has a much stronger
generalization ability to new multi-label datasets.
Figure 10 shows the predicted scores of images for
different categories on the VOC 2007 testing dataset
using models from different ﬁne-tuning epoches. For
each histogram3, orange bars indicate predicted scores
of the ground-truth categories. We show the predictive scores at the 1st and the 60th epoch during I-
FT and H-FT stages. For the ﬁrst row, it can be
seen that the predictive score for the train category
gradually increases. Besides, for the third row, it can
be seen that there are three ground-truth categories
3. For each histogram, categories from left to right are plane, bike,
bird, boat, bottle, bus, car, cat, chair, cow, table, dog, horse, motor, person,
plant, sheep, sofa, train and tv.
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
Classiﬁcation results (AP in %) comparison for state-of-the-art approaches on VOC 2007 (trainval/test). The
upper part shows methods not using ground-truth bounding box information for training, while the lower part
shows methods with that information. * indicates methods using additional data (i.e., ImageNet) for training.
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
69.3 56.2 66.6
68.1 83.4 53.6 58.3 51.1 62.2 45.2
54.3 75.8 62.1 63.5
64.8 52.8 70.6
64.1 77.5 55.5 55.6 41.8 56.3 41.7
56.6 79.7 51.5 58.3
CNN-SVM* 88.5
81.0 83.5 82.0
72.5 85.3 81.6 59.9 58.5 66.5 77.8
62.6 87.4 71.8 73.9
84.7 87.5 81.8
73.0 86.4 84.8 51.8 63.9 67.9 82.7 84. 0
54.1 89.5 65.8 74.4
HCP-1000C*
90.1 92.8 89.9
80.0 91.7 91.6 57.7 77.8 70.9 89.3
62.7 94.4 78.3 81.5
HCP-2000C*
96.0 92.1 93.7 93.4 58.7 84.0 93.4 92.0 62.8 89.1 76.3 91.4 95.0 87.8
93.1 69.9 90.3 68.0 96.8 80.6 85.2
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
83.0 58.4 76.1
77.5 88.8 69.1 62.2 61.8 64.2 51.3
67.7 86.3 70.9 71.1
81.5 65.0 71.4
76.2 87.2 68.5 63.8 55.8 65.8 55.6
69.7 83.6 77.0 71.3
PRE-1000C* 88.5
81.5 87.9 82.0
75.5 90.1 87.2 61.6 75.7 67.3 85.5
58.0 90.4 77.9 77.7
Classiﬁcation results (AP in %) comparison for state-of-the-art approaches on VOC 2012 (trainval/test). The
upper part shows methods not using ground-truth bounding box information for training, while the lower part
shows methods with that information. * indicates methods using additional data (i.e., ImageNet) for training.
plane bike bird boat bottle bus
cat chair cow table dog horsemotorpersonplantsheep sofa train
94.6 74.3 87.8 80.2 50.1 82.0 73.7 90.1 60.6 69.9 62.7 86.9 78.7
77.5 49.3 88.5 69.2 74.7
LeCun-ICML* 
96.0 77.1 88.4 85.5 55.8 85.8 78.6 91.2 65.0 74.4 67.7 87.8 86.0
83.6 61.1 91.8 76.1 79.0
HCP-1000C*
97.7 83.0 93.2 87.2 59.6 88.2 81.9 94.7 66.9 81.6 68.0 93.0 88.2
85.1 55.4 93.0 77.2 81.7
HCP-2000C*
97.5 84.3 93.0 89.4 62.5 90.2 84.6 94.8 69.7 90.2 74.1 93.4 93.7
90.3 61.8 94.4 78.0 84.2
plane bike bird boat bottle bus
cat chair cow table dog horsemotorpersonplantsheep sofa train
NUS-PSL 
97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5
79.2 73.4 94.5 80.7 82.2
PRE-1000C* 
93.5 78.4 87.7 80.9 57.3 85.0 81.6 89.4 66.9 73.8 62.0 89.5 83.2
79.0 54.3 88.0 78.3 78.7
PRE-1512* 
94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4
86.6 62.3 91.1 79.8 82.8
HCP-2000C+NUS-PSL* 98.9 91.894.892.4 72.6 95.091.897.485.292.983.196.0 96.6 96.1
68.4 92.0 79.697.388.5 90.3
in the given image, i.e., car, horse, person. It should be
noted that the car category is not detected during ﬁnetuning while it is successfully recovered in HCP. This
may be because the proposed HCP is a hypotheses
based method and both foreground (i.e., horse, person)
and background (i.e., car) objects can be equivalently
treated. However, during the ﬁne-tuning stage, the
entire image is treated as the input, which may lead
to ignorance of some background categories.
Image Classiﬁcation on VOC 2012: Table 2 reports
our experimental results compared with the state-ofthe-arts on VOC 2012. LeCun et al. reported the
classiﬁcation results on VOC 2012, which achieved
the state-of-the-art performance without using any
bounding box annotation. Compared with , the
proposed HCP-1000C has an improvement of 2.7%.
Both pre-trained on the ImageNet dataset with 1,000
classes, HCP-1000C gives a more competitive result
compared with PRE-1000C (81.7% vs. 78.7%).
From Table 2, it can be seen that the proposed HCP-
1000C is not as competitive as NUS-PSL and PRE-
1512 . This can be explained as follows. For NUS-
PSL, which got the winner prize of the classiﬁcation
task in PASCAL VOC 2012, model fusion from both
detection and classiﬁcation is employed to generate
the integrative result, while the proposed HCP-1000C
is based on a single model without any fusion. For
PRE-1512, 512 extra ImageNet classes are selected for
CNN pre-training. In addition, the selected classes
have intensive semantic overlap with PASCAL VOC,
including hoofed mammal, furniture, motor vehicle, public
transport, bicycle. Therefore, the greater improvement
of PRE-1512 compared with HCP-1000C is reasonable.
By augmenting another 1,000 classes, our proposed
HCP-2000C can achieve an improvement of 1.4% compared with PRE-1512.
Finally, the comparison in terms of rigid and articulated categories among NUS-PSL, PRE-1512 and
HCP-2000C is shown in Table 3, from which it can be
seen that the hand-crafted feature based scheme, i.e.,
NUS-PSL, outperforms almost all CNN feature based
schemes for rigid categories, including plane, bike, boat,
bottle, bus, car, chair, table, motor, sofa, train, tv, while
for articulated categories, CNN feature based schemes
seem to be more powerful. Based on theses results, it
can be observed that there is strong complementarity between hand-crafted feature based schemes and
CNN feature based schemes. To verify this assumption, a late fusion between the predicted scores of
NUS-PSL (also from the authors of this paper) and
JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014
I-FT-1 epoch
I-FT-60 epoch
H-FT-1 epoch
H-FT-60 epoch
horse, person
car, horse, person
bike, bus, car,
bottle, chair, table,
person, plant
Fig. 10. Samples of predicted scores on the VOC 2007 testing dataset using models from different ﬁne-tuning
epochs (i.e., I-FT-1st epoch, I-FT-60th epoch, H-FT-1st epoch, and H-FT-60th epoch).
HCP is executed to make an enhanced prediction
for VOC 2012. Incredibly, the mAP score on VOC
2012 can surge to 90.3% as shown in Table 2, which
demonstrates the great complementarity between the
traditional framework and the deep networks.
CONCLUSIONS
In this paper, we presented a novel Hypotheses-
CNN-Pooling (HCP) framework to address the multilabel image classiﬁcation problem. Based on the proposed HCP, CNN pre-trained on large-scale singlelabel image datasets, e.g., ImageNet, can be successfully transferred to tackle the multi-label problem. In
addition, the proposed HCP requires no bounding box
annotation for training, and thus can easily adapt to
new multi-label datasets. We evaluated our method
on VOC 2007 and VOC 2012, and veriﬁed that significant improvement can be made by HCP compared
with the state-of-the-arts. Furthermore, it is proved
that late fusion between outputs of CNN and handcrafted feature schemes can incredibly enhance the
classiﬁcation performance.