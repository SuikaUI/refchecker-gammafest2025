Fi-GNN: Modeling Feature Interactions via Graph Neural
Networks for CTR Prediction
Institute of Information Engineering,
Chinese Academy of Sciences
University of Chinese Academy of
 
Institute of Automation, Chinese
Academy of Sciences
University of Chinese Academy of
 
Institute of Automation and
Artificial Intelligence Research,
Chinese Academy of Sciences
 
Xiaoyu Zhang
Institute of Information Engineering,
Chinese Academy of Sciences
 
Liang Wang
Institute of Automation, Chinese
Academy of Sciences
University of Chinese Academy of
 
Click-through rate (CTR) prediction is an essential task in web
applications such as online advertising and recommender systems,
whose features are usually in multi-field form. The key of this
task is to model feature interactions among different feature fields.
Recently proposed deep learning based models follow a general
paradigm: raw sparse input multi-filed features are first mapped
into dense field embedding vectors, and then simply concatenated
together to feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions.
However, the simple unstructured combination of feature fields will
inevitably limit the capability to model sophisticated interactions
among different fields in a sufficiently flexible and explicit fashion.
In this work, we propose to represent the multi-field features
in a graph structure intuitively, where each node corresponds to
a feature field and different fields can interact through edges. The
task of modeling feature interactions can be thus converted to modeling node interactions on the corresponding graph. To this end, we
design a novel model Feature Interaction Graph Neural Networks
(Fi-GNN). Taking advantage of the strong representative power
of graphs, our proposed model can not only model sophisticated
feature interactions in a flexible and explicit fashion, but also provide good model explanations for CTR prediction. Experimental
results on two real-world datasets show its superiority over the
state-of-the-arts.
The first two authors Zekun Li and Zeyu Cui contribute to this work equally. Shu Wu
and Xiaoyu Zhang are both corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
CIKM ’19, November 3–7, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6976-3/19/11...$15.00
 
CCS CONCEPTS
• Information systems →Recommender systems; Business intelligence; • Applied computing →Online shopping.
Feature interactions, Graph neural networks, CTR prediction, Recommender system
ACM Reference Format:
Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-
GNN: Modeling Feature Interactions via Graph Neural Networks for CTR
Prediction. In The 28th ACM International Conference on Information and
Knowledge Management (CIKM ’19), November 3–7, 2019, Beijing, China.
ACM, New York, NY, USA, 10 pages. 
INTRODUCTION
The goal of click-through rate prediction is to predict the probabilities of users clicking ads or items, which is critical to many web
applications such as online advertising and recommender systems.
Modeling sophisticated feature interactions plays a central role in
the success of CTR prediction. Distinct from continuous features
which can be naturally found in images and audios, the features
for web applications are mostly in multi-field categorical form. For
example, the four-fields categorical features for movies may be: (1)
Language = {English, Chinese, Japanese, ... }, (2) Genre = {action,
fiction, ... }, (3) Director = {Ang Lee, Christopher Nolan, ... }, and
(4) Starring = {Bruce Lee, Leonardo DiCaprio, ... } (noted that there
are much more feature fields in real applications). These multi-field
categorical features are usually converted to sparse one-hot encoding vectors, and then embedded to dense real-value vectors, which
can be used to model feature interactions.
Factorization machine (FM) is a well-known model proposed
to learn second-order feature interactions from vector inner products. Field-aware factorization machine (FFM) further considers
the field information and introduces field-aware embedding. Regrettably, these FM-based models can only model second-order interaction and the linearity modeling limits its representative power.
Recently, many deep learning based models have been proposed
 
to learn high-order feature interactions, which follow a general
paradigm: simply concatenate the field embedding vectors together
and feed them into DNN or other specifically designed models to
learn interactions. For example, Factorisation-machine supported
Neural Networks (FNN) , Neural Factorization Machine (NFM)
 , Wide&Deep and DeepFM utilize DNN to model interactions. However, these model based on DNN learn high-order feature
interactions in a bit-wise, implicit fashion, which lacks good model
explanations. Some models try to learn high order interactions
explicitly by introducing specifically designed networks. For example, Deep&Cross introduces Cross Network (CrossNet) and
xDeepFM introduces Compressed Interaction Network (CIN).
Nevertheless, we argue that they are still not sufficiently effective and explicit, since they still follow the general paradigm of
combining feature fields together to model their interactions. The
simple unstructured combination will inevitably limit the capability
to model sophisticated interactions among different feature fields
in a flexible and explicit fashion.
In this work, we take the structure of multi-field features into
consideration. Specifically, we represent the multi-field features
in a graph structure named feature graph. Intuitively, each node
in the graph corresponds to a feature field and different fields can
interact through edges. The task of modeling sophisticated interactions among feature fields can be thus converted to modeling
node interactions on the feature graph. To this end, we design a
novel model Feature interaction Graph Neural Networks (Fi-GNN)
based on Graph Neural Networks (GNN), which is able to model
sophisticated node (feature) interactions in a flexible and explicit
fashion. In Fi-GNN, the nodes will interact by communicating the
node states with neighbors and update themselves in a recurrent
fashion. At every time step, the model interact with neighbors at
one hop deeper. Therefore, the number of interaction steps equals
to the order of feature interactions. Moreover, the edge weights
reflecting importances of different feature interactions and node
weights reflecting importances of each feature field on the final
CTR prediction can be learnt by Fi-GNN, which can provide good
explanations. Overall, our proposed model can model sophisticated
feature interactions in an explicit, flexible fashion and also provide
good model explanations.
Our contributions can be summarized in threefold:
• We point out the limitation of the existing works which
consider multi-field features as an unstructured combination
of feature fields. To this end, we propose to represent the
multi-field features in a graph structure for the first time.
• We design a novel model Feature Interaction Graph Neural Networks (Fi-GNN) to model sophisticated interactions
among feature fields on the graph-structured features in a
more flexible and explicit fashion.
• Extensive experiments on two real-world datasets show that
our proposed method can not only outperform the state-ofthe-arts but also provide good model explanations.
The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 provides an elaborative description
of our proposed method. The extensive experiments and detailed
analysis are presented in Section 4, followed by the conclusion in
Section 5.
RELATED WORK
In this section, we briefly review the existing models that model
feature interactions for CTR prediction and graph neural networks.
Feature Interaction in CTR Prediction
Modeling feature interactions is the key to success of CTR prediction and therefore extensively studied in the literature. LR is a
linear approach, which can only models the first-order interaction
on the linear combination of raw individual features. FM learns
second-order feature interactions from vector inner products. Afterwards, different variants of FM have been proposed. Field-aware
factorization machine (FFM) considers the field information and
introduces field-aware embedding. AFM considers the weight
of different second-order feature interactions. However, these approaches can only model second-order interaction which is not
sufficient.
With the success of DNN in various fields, researchers start to
use it to learn high-order feature interactions due to its deeper
structures and nonlinear activation functions. The general paradigm is to concatenate the field embedding vectors together and
feed them into DNN to learn the high-order feature interactions.
 utilizes convolutional networks to model feature interactions.
Factorisation-machine supported Neural Networks (FNNs) uses
the pre-trained factorization machines for field embedding before
applying DNN. Product-based Neural Network (PNN) models
both second-order and high-order interactions by introducing a
product layer between field embedding layer and DNN layer. Similarly, Neural Factorization Machine (NFM) has a Bi-Interaction
Pooling layer between embedding layer and DNN layer to model
second-order interactions, but the followed operation is summation
instead of concatenation as in PNN. Some works on another line
try to model the second-order and high-order interactions jointly
via a hybrid architectures. The Wide&Deep and DeepFM 
contain a wide part to model the low-order interaction and a deep
part to model the high-order interaction. However, all these approaches leveraging DNN learn the high-order feature interactions
in an implicit, bit-wise way and therefore lack good model explainability. Recently, some work try to learn feature interactions in an
explicit fashion via specifically designed networks. Deep&Cross
 introduces a CrossNet which takes outer product of features
at the bit level. On the contrary, xDeepFM introduces a CIN to
take outer product at the vector level. Nevertheless, they still don’t
solve the most fundamental problem, that is to concatenate the field
embedding vectors together. The simple unstructured combination
of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a flexible and explicit
fashion. To this end, we proposed to represent the multi-field features in a graph structure, where each node represents a field and
different feature fields can interact through the edges. Accordingly,
we can model the flexible interactions among different feature fields
on the graphs.
Graph Neural Networks
Graph is a kind of data structure which models a set of objects
(nodes) and their relationships (edges). Recently, researches of analyzing graphs with machine learning have been receiving more
ܐଵ       ܐଶ              ܐଷ                  ܐସ
Field-aware Embedding Layer
Multi-head Self-attention Layer
1 0 … 0 0 1 … 0 0 1 … 0 0 1 … 0
Field 1 Field 2 Field 3 Field 4
Feature Graph
Graph Neural Network Layer - ૚
Graph Neural Network Layer - ૛
Graph Neural Network Layer - ࢀ
Attentional Scoring Layer
Logistic Loss
Figure 1: Overview of our proposed method. The input raw
multi-field feature vector is first converted to field embedding vectors via an embedding layer and represented as a
feature graph, which is then feed into Fi-GNN to model feature interactions. An attention layer is applied on the output of Fi-GNN to predict the click through rate ˆy. Details of
embedding layer and Fi-GNN are illustrated in Figure 2 and
Figure 3 respectively.
and more attention because of the great representative power of
graphs. Early works usually convert graph-structured data into
sequence-structured data to deal with. Inspired by word2vec ,
the work proposed an unsupervised DeepWalk algorithm to
learn node embedding in graph based on random walks. After
that, proposed a network embedding algorithm LINE, which
preserve the first- and second-order structural information. proposed node2vec which introduces a biased random walk. However,
these methods can be computationally expensive and non-optimal
for large graphs.
Graph neural networks (GNN) are designed to tackle these problems, which are deep learning based methods that operate on the
graph domain. The concept of GNN is first proposed by . Generally, nodes in GNNs interact with neighbors by aggregating information from neighborhoods and updating their hidden states. There
have been many variants of GNN with various kinds of aggregators
and updaters proposed these days. Here we only present some representative and classical methods. Gated Graph Neural Networks
(GGNN) uses GRU as updater. Graph Convolutional Networks (GCN) considers the spectral structure of graphs and
utilizes the convolutional aggregator. GraphSAGE considers the
spatial information. It introduces three kinds of aggregators: mean
aggregator, LSTM aggregator and Pooling aggregator. Graph attention network (GAT) incorporates the attention mechanism
into the propagation step. There are some surveys which
provide more elaborative introduction of various kinds of GNN
Due to convincing performance and high interpretability, GNN
has been a widely applied graph analysis method. Recently, there
are many application of GNN like neural machine translation ,
semantic segmentation , image classification , situation
recognition , recommendation , script event prediction ,
fashion analysis . GNN is suitable for modeling node interactions on graph-structured features intrinsically. In this work,
we proposed a model Fi-GNN based on GGNN to model feature
interactions on the graph-structured features for CTR prediction.
OUR PROPOSED METHOD
We first formulate the problem and then introduce the overview
of our proposed method, followed by the elaborate detail of each
component.
Problem Formulation
Suppose the training dataset consists of m-fields categorical features (m is the number of feature fields) and the associated labels
y ∈{0, 1} which indicate user click behaviors. The task of CTR
prediction is to predict ˆy for the input m-fields features, which
estimates the probability of a user clicking. The key of the task is to
model the sophisticated interactions among different feature fields.
Figure 1 is the overview of our proposed method (m=4). The input
sparse m-field feature vector is first mapped into sparse one-hot
embedding vectors and then embedded to dense field embedding
vectors via the embedding layer and the multi-head self-attention
layer. The field embedding vectors are then represented as a feature
graph, where each node corresponds to a feature field and different
feature fields can interact through edges. The task of modeling
interaction can be thus converted to modeling node interactions
on the feature graph. Therefore, the feature graph is feed into our
proposed Fi-GNN to model node interactions. An attention scoring
layer is applied on the output of Fi-GNN to estimate the clickthrough rate ˆy. In the following, we will introduce the details of
our proposed method.
Embedding Layer
The multi-field categorical feature x is usually sparse and of huge
dimension. Following previous works , we represent each field as a one-hot encoding vector and then embed it to a
dense vector, noted as field embedding vector. Let us consider the
example in Section 1, a movie {Language: English, Genre: fiction,
Director: Christopher Nolan, Starring: Leonardo DiCaprio } is first
transformed into a high-dimensional sparse features via one-hot
[1, 0, ..., 0]
, [0, 1, ..., 0]
, [0, 1, ..., 0]
, [0, 1, ..., 0]
A field-aware embedding layer is then applied upon the one-hot
vectors to embed them to low dimensional, dense real-value field
embedding vectors as shown in Figure ??. Likewise, the field embedding vectors of m-field feature can be obtained:
E = [e1, e2, e3, ..., em] ,
where ei ∈Rd denotes the embedding vector of fieldi andd denotes
the dimension of field embedding vectors.
Multi-head Self-attention Layer
Transformer is prevalent in NLP and has achieved great success in many tasks. At the core of Transformer, the multi-head
self-attention mechanism is able to model complicated dependencies between word pairs in multiple semantic subspaces. In the
literature of CTR prediction, we take advantage of the multi-head
self-attention mechanism to capture the complex dependencies
between feature field pairs, i.e, pairwise feature interactions, in
different semantic subspaces.
Following , given the feature embeddings E, we obtain the
feature representation of features that cover the pairwise interactions of an attention head i via scaled dot-product:
Hi = softmaxi(QKT
E, K = W(K)
E, V = W(V )
The matrices W(Q)
∈Rdi×d, W(K)
∈Rdi×d, W(V )
∈Rdi×d are
three weight parameters for attention head i, di is the dimension
size of head i, and Hi ∈Rm×di .
Then we combine the learnt feature representations of each
head to preserve the pairwise feature interactions in each semantic
H1 = ReLU(H1 ⊕H2 ⊕· · · ⊕Hh),
where ⊕denotes the concatenation operation and h denotes the
number of attention heads. The learnt feature representations H1 ∈
Rm×d′ are used for the initial node states of the graph neural network, where d′ = Íh
Feature Graph
Distinguished from the previous works which simply concatenate
the field embedding vectors together and feed them into designed
models to learn feature interactions, we represent them in a graph
structure. In particular, We represent each input multi-field feature
as a feature graph G = (N, E), where each nodeni ∈N corresponds
to a feature field i and different fields can interact through the
edges, so that |N| = m. Since each two fields ought to interact, it is
a weighted fully connected graph while the edge weights reflect
importances of different feature interactions. Accordingly, the task
of modeling feature interactions can be converted to modeling node
interactions on the feature graph.
Feature Interaction Graph Neural Network
Fi-GNN is designed to model node interactions on the feature graph,
which is based on GGNN . It is able to model the interactions
in a flexible and explicit fashion.
Preliminaries. In Fi-GNN, each nodeni is associated with a hidden
state vector ht
i and the state of graph is composed of these node
3, ..., htm
where t denote the interaction step. The learnt feature representations by the multi-head self-attention layer are used for their initial
node states H1. As shown in Figure 2, the nodes interact and update
their states in a recurrent fashion. At each interaction step, the
nodes aggregate the transformed state information with neighbors,
Figure 2: Framework of Fi-GNN. The nodes interact with
neighbors and update their states in a recurrent fashion. At
each interaction step, each node will first aggregate transformed state information from neighbors and then update
its state according to the aggregated information and history
via GRU and residual connection.
and then update their node states according to the aggregated information and history via GRU and residual connection. Next, we
will introduce the details of Fi-GNN elaborately.
State Aggregation. At interaction step t, each node will aggregate
the state information from neighbors. Formally, the aggregated
information of node ni is sum of its neighbors’ transformed state
information,
A[nj,ni]Wpht−1
where Wp is the transformation function. A ∈Rm×m is the adjacency matrix containing the edge weights. For example, A[nj,ni] is
the weight of edge from node nj to ni, which can reflect the importance of their interaction. Apparently, the transformation function
and adjacency matrix decide on the node interactions. Since the interaction on each edge ought to differ, we aim to achieve edge-wise
interaction, which requires a unique weight and transformation
function for each edge.
(1) Attentional Edge Weights. The adjacency matrix in the conventional GNN models is usually in the binary form, i.e., only contains 0 and 1. It can only reflect the connected relation of nodes
but fails to reflect the importances of their relations. In order to
infer the importances of interactions between different nodes, we
propose to learn the edge weights via an attention mechanism. In
particular, the weight of edge from node ni to node nj is calculated
with their initial node states, i.e., the corresponding field embedding
vectors. Formally,
w(ni,nj) =
exp(LeakyRelu(Ww
k exp(LeakyRelu(Ww [ei || ek])),
where Ww ∈R2d′ is a weight matrix, || is the concatenation operation. The softmax function is utilized to make weights easily
comparable across different nodes. Therefore, the adjacency matrix
A[ni,nj] =
w(ni,nj), if i , j,
Since the edge weights reflects the importances of different interaction, Fi-GNN can provide good explanations on the relation
of different feature fields of input instance, which will be further
discussed in Section 4.5.
(2) Edge-wise Transformation. As discussed before, a fixed
transformed function on all the edges is unable to model the flexible
interactions and a unique transformation for each edge is essential.
Nevertheless, our graph is complete graph with a huge number of
edges. Simply assigning a unique transformation weight to each
edge will consuming too much parameter space and running time.
To reduce the time and space complexity and also achieve edge-wise
transformation, we assign an output matrix Wi
out and an input
in to each node ni similar with . As shown in Figure
2, when node ni sends its state information to node nj, the state
information will first be transformed by its output matrix Wi
and then transformed by node nj’s input matrix Wj
in before nj
receives it. The transformation function of edge ni →nj from node
ni to node nj thus could be written as,
Likewise, the transformation function of edge nj →ni from node
nj to node nj is
Accordingly, the Equation 1 could be rewritten as,
A[nj,ni]Wj
In this way, the number of parameters is proportional to the number of nodes rather than numerous edges, which greatly reduces
the space and time complexity and meanwhile achieves edge-wise
interaction.
State Update. After aggregating state information, the nodes will
update the state vectors via GRU and residual connections.
(1) State update via GRU. In traditional GGNN, the state vector of node ni is updated via GRU based on the aggregated state
information at
i and its state at last step. Formally,
i = GRU (ht−1
It can be formalized in detail as:
i = σ(Wzat
i + Uzht−1
i = σ(Wr at
i + Ur ht−1
i = tanh(What
where, Wz, Wr , Wh, bz, br , bh are weights and biases of the updating function Gated Recurrent Unit (GRU) . zt
i are update
gate vector and reset gate vector, respectively.
(2) State update via Residual Connections. Previous works
 have proved that it’s effective to combine the low-order
and high-order interactions together. We thus introduce extra residual connections to update note states along with GRU, which can
facilitate low-order feature reuse and gradients back-propagation.
Therefore, the Eq. (7) can be rewritten as,
i = GRU (ht−1
Attentional Scoring Layer
After T propagation steps, we can obtain the node states
2 , ..., hTm
Since the nodes have interacted with their T-order neighbors, the
T-order feature interactions is modeled. We need a graph-level
output to predict CTR.
Attentional Node Weights The final state of each field node has
captured the global information. In other words, these field nodes
are neighborhood-aware. Here we predict a score on the final state
of each field respectively and sum them up with an attention mechanism which measures their influences on the overall prediction.
Formally, the prediction score of each node ni and its attentional
node weight can be estimated via two multiple layers perceptions
respectively as,
ˆyi = MLP1(hp
ai = MLP2(hp
The overall prediction is a summation of all nodes:
Note that it is actually same as the work . Intuitively, MLP1 is
used to model the prediction score of each field aware of the global
information and MLP2 is used to model the weights of each field
(i.e., importance of fields’ influence on the overall prediction).
Our loss function is Log loss, which is defined as follows:
(yiloд(ˆyi) + (1 −yi)loд(1 −ˆyi)),
where N is the total number of training samples and i indexes the
training samples. The parameters are updated via minimizing the
Log Loss using RMSProp . Most CTR datasets have unbalanced
proportion of positive and negative samples, which will mislead the
predictions. To balance the proportion, we randomly select equal
number of positive and negative samples in each batch during
training process.
Parameter Space. The parameter needed to be learnt mainly
consists of the parameters correlated to nodes and the perception
networks in attention mechanism. For each node ni, we have an
input matrix Wi
in and an output matrix Wi
out to transform state
information. Totally we have 2m matrices, which are proportional
to the number of nodes m. Besides, the multi-head self-attention
layer contains the following weight matrices
for each head, and the number of parameters of the entire layer
is (3dd′ + hdd′). In addition, we have two matrices of perception
networks in the self-attention mechanism and also parameters in
GRU. Overall, there are O(2m + hdd′) matrices.
Model Analysis
Comparison with Previous CTR Models. As discussed
before, the previous deep learning based CTR models model highorder interactions in a general paradigm: raw sparse input multifiled features are first mapped into dense field embedding vectors,
then simply concatenated together and feed into deep neural networks (DNN) or other specifically designed networks to learn highorder feature interactions. The simple unstructured combination of
feature fields inevitably limits the capability to model sophisticated
interactions among different fields in a sufficiently flexible and explicit fashion. In this way, the interaction between different fields
is conducted in a fixed fashion, no matter how sophisticated the
used network is. In addition, they lack good model explanation.
Since we represent the multi-field features in a graph structure,
our proposed model Fi-GNN is able to model interactions among
different fields in the form of node interactions. Compared with the
previous CTR models, Fi-GNN can model the sophisticated feature
interaction via flexible edge-wise interaction function, which is
more effective and explicit. Moreover, the edge weights reflecting
importance of different interactions can be learnt in Fi-GNN, which
provides good model explanations for CTR prediction. In fact, if
the edge weight is all 1 and the transformation matrix on each edge
is same, our model Fi-GNN collapses into FM. Taking advantage
of the great power of GNN, we can apply flexible interactions on
different feature fields.
Comparison with Previous GNN Models. Our proposed
model Fi-GNN is designed based on GGNN, upon which we mainly
make two improvements: (1) we achieve edge-wise interaction via
attentional edge weights and edge-wise transformation; (2) we
introduce an extra residual connection along with GRU to update
states, which can help regain the low-order information.
As discussed before, the node interaction on each edge in GNN
depends on the edge weight and the transformation function on the
edge. The conventional GGNN uses binary edge weights which fails
to reflect the importance of the relations, and a fixed transformation
function on all the edges. In contrast, our proposed Fi-GNN can
model edge-wise interactions via attention edge weights and edgewise transformation functions. When the interaction order is high,
the node states tend to be smooth, i.e., the states of all the nodes
tend to be similar. The residual connections can help identity the
nodes by adding initial node states.
Table 1: Statistics of evaluation datasets.
#Instances
#Features (sparse)
45,840,617
40,428,967
EXPERIMENTS
In this section, we conduct extensive experiments to answer the
following questions:
RQ1 How does our proposed Fi-GNN perform in modeling highorder feature interactions compared with the state-of-the-art
RQ2 Does our proposed Fi-GNN perform better than original
GGNN in modeling high-order feature interactions?
RQ3 What are the influences of different model configurations?
RQ4 What are the relations between features of different fields?
Is our proposed model explainable?
We first present some fundamental experimental settings before
answering these questions.
Experiment Setup
Datasets. We evaluate our proposed models on the following
two datasets, whose statistics are summarized in Table 1.
1. Criteo1. This is a famous industry benchmark dataset for
CTR prediction, which has 45 million users’ click records in 39
anonymous feature fields on displayed ads. Given a user and the
page he is visiting, the goal is to predict the probability that he will
click on a given ad.
2. Avazu2. This dataset contains users’ click behaviors on displayed mobile ads. There are 23 feature fields including user/device
features and ad attributes. The fields are partial anonymous.
For the two datasets, we remove the infrequent features appearing in less than 10, 5 times respectively and treat them as a single
feature “<unknown>”. Since the numerical features may have large
variance, we normalize numerical values by transforming a value
z to loд2(z) if z > 2, which is proposed by the winner of Criteo
Competition3. The instances are randomly split in 8:1:1 for training,
validation and testing.
Evaluation Metrics. We use the following two metrics for
model evaluation: AUC (Area Under the ROC curve) and Logloss
(cross entropy).
AUC measures the probability that a positive instance will be
ranked higher than a randomly chosen negative one. A higher AUC
indicates a better performance.
Logloss measures the distance between the predicted score and
the true label for each instance. A lower Logloss indicates a better
performance.
Relative Improvement (RI). It should be noted that a small
improvement with respect to AUC is regarded significant for realworld CTR tasks . In order to estimate the relative
improvement of our model achieves over the compared models, we
here measure RI-AUC and RI-Logloss, which can be formulated
RI-X = |X(model) −X(base)|
where |x| returns the absolute value of x, X can be either AUC or
Logloss, model refers to our proposed model and base refers to the
compared model.
Baselines. As described in Section 2.1, the early approaches
can be categorized into three types: (A) Logistic Regression (LR)
which models first-order interaction; (B) Factorization Machine
(FM) based linear models which model second-order interactions;
(C) Deep learning based models which model high-order interactions on the concatenated field embedding vectors.
We select the following representative methods of three types
to compare with ours.
LR (A) models first-order interaction on the linear combination
of raw individual features.
FM (B) models second-order feature interactions from vector
inner products.
1 
2 
3 
Table 2: Performance Comparison of Different methods. The best performance on each dataset and metric are highlighted.
Further analysis is provided in Section 4.2.
Model Type
RI-Logloss
RI-Logloss
First-order
Second-order
High-order
DeepCrossing 
CrossNet 
Fi-GNN (ours)
AFM (B) is a extent of FM, which considers the weight
of different second-order feature interactions by using attention
mechanism. It is one of the state-of-the-art models that model
second-order feature interactions.
DeepCrossing (C) utilizes DNN with residual connections
to learn high-order feature interactions in an implicit fashion.
NFM (C) utilizes a Bi-Interaction Pooling layer to model
the second-order interactions, and then feeds the concatenated
second-order combinatorial features into DNNs to model highorder interactions.
CrossNet (Deep&Cross) (C) is the core of Deep&Cross
model, which tries to model feature interactions explicitly by taking
outer product of concatenated feature vector at the bit-wise level.
CIN (xDeepFM) (C) is the core of xDeepFM model, which
takes outer product of stacked feature matrix at vector-wise level.
Implementation Details. We implement our method using
Tensorflow4. The optimal hyper-parameters are determined by
the grid search strategy. Implementation of baselines follows .
Dimension of field embedding vectors is 16 and batch size is 1024
for all methods. DeepCrossing has four feed-forward layers, each
with 100 hidden units. NFM has one hidden layer of size 200 on
top of Bi-Interaction layer as recommended in the paper . There
are three interaction layers for both CrossNet and CIN. All the
experiments were conducted over a sever equipped with 8 NVIDIA
Titan X GPUs.
Model Comparison (RQ1)
The performance of different methods is summarized in Table 2,
from which we can obtain the following observations:
(1) LR achieves the worst performance among these baselines,
which proves that the individual features is insufficient in
CTR prediction.
(2) FM and AFM, which model second-order feature interactions,
outperform LR on all datasets, indicating that it’s effective
to model pair-wise interaction between feature fields. In
addition, AFM achieves better performance than FM, which
proves the effectiveness of attention on different interactions.
(3) The methods modeling high-order interaction mostly outperform the methods that model second-order interactions.
4The code is released at 
This indicates the second-order feature interactions is not
sufficient.
(4) DeepCrossing outperforms NFM, proving the effectiveness
of residual connections in CTR prediction.
(5) Our proposed Fi-GNN achieves best performance among all
these methods on two datasets. Considering the fact that previous improvements with respect to AUC at 0.001-level are
regarded significant for CTR prediction task, our proposed
method shows great superiority over these state-of-the-arts
especially on Criteo dataset, owing to the great representative power of graph structure and the effectiveness of GNN
on modeling node interactions.
(6) Compared with these baselines, the relative improvement
of our model achieves on Criteo dataset is higher than that
on Avazu dataset. This might be attributed to that there are
more feature fields in Criteo dataset, which can take more
advantage of the representative power of graph structure.
Ablation Study (RQ2)
Our proposed model Fi-GNN is based on GGNN, upon which we
mainly make two improvements: (1) we achieve edge-wise node
interactions via attentional edge weights and edge-wise transformation; (2) we introduce extra residual connections to update state
along with GRU. To evaluate the effectiveness of the two improvements on modeling node interactions, we conduct ablation study
and compare the following three variants of Fi-GNN:
Fi-GNN(-E/R): Fi-GNN without the two above mentioned improvements: edge-wise node interactions (E) and residual connections (R).
Fi-GNN(-E): Fi-GNN without edge-wise interactions (E).
Fi-GNN(-R): Fi-GNN without residual connections (R), which
is also GGNN with edge-wise interactions.
The performance comparison is shown in Figure 3(a), from which
we can obtain the following observations:
(1) Compared with FiGNNïĳŇthe performance of Fi-GNN(-E)
drops by a large margin, suggesting that it’s crucial to model
the edge-wise interaction. Fi-GNN(-E) achieves better performance than Fi-GNN(-E/R), proving that the residual connections can indeed provide useful information.
(2) The full model Fi-GNN outperforms the three variants, indicating that the two improvements we make, i.e., residual
Fi-GNN(-E/R)
Fi-GNN(-E)
Fi-GNN(-R)
Fi-GNN(-E/R)
Fi-GNN(-E)
Fi-GNN(-R)
(a) edge-wise interaction (E) and residual connections (R)
Fi-GNN(-W/T)
Fi-GNN(-W)
Fi-GNN(-T)
Fi-GNN(-W/T)
Fi-GNN(-W)
Fi-GNN(-T)
(b) attentional edge weight (W) and edge-wise transformation (T)
Figure 3: Two groups of ablation studies on Fi-GNN.
(a) State Dimensionality
(b) Interaction Step
Figure 4: AUC performance with different state dimensionality D (left) and interaction step T (right) on Criteo and
Avazu dataset.
connections and edge-wise interactions, can jointly boost
the performance.
We take two measures to achieve edge-wise node interactions in
Fi-GNN: attentional edge weight (W) and edge-wise transformation
(T). To further investigate where dose the great improvement come
from, we conduct another ablation study and compare the following
three variants of Fi-GNN:
Fi-GNN(-W/T): Fi-GNN without self-adaptive adjacency matrix
(W) and edge-wise transformation (T), i.e., uses binary adjacency
matrix (all the edge weights are 1) and a shared transformation
matrix on all the edges. It is also Fi-GNN-(E),
Fi-GNN(-W): FI-GNN without attentional edge weights, i.e.,
uses binary adjacency matrix.
Fi-GNN(-T): FI-GNN without edge-wise transformation, i.e.,
uses a shared transformation on all the edges.
The performance comparison is shown in Figure 3(a). We can
see that Fi-GNN(-T) and Fi-GNN(-W) both outperform Fi-GNN(-
W/T), which proves their effectiveness. Nevertheless, Fi-GNN(-W)
achieves greater improvements than Fi-GNN(-T), suggesting that
the edge-wise transformation is more effective than attentional edge
weights in modeling edge-wise interaction. This is quite reasonable
since the transformation matrix oughts to have stronger influence
on interactions than a scalar attentional edge weight. In addition, Fi-
GNN achieves the best performance demonstrates that it’s crucial
to take both the two measures to model edge-wise interaction.
Hyper-Parameter Study (RQ3)
Influence of different state dimensionality. We first investigate how the performance changes w.r.t. the dimension of the
node states d′, which is also the output size of the initial multihead self-attention layer. The results on Criteo and Avazu datasets
are shown in Figure 4(a). On Avazu dataset, the performance first
increases and then begins to decrease when the dimension size
reaches 32, which indicates that state size of 32 has been represented enough information and the model is overfitted when too
many parameters are used. Nevertheless, on Criteo dataset, the performance peaks with the dimension size of 64, which is reasonable
since the dataset is more complexed which needs larger dimension
size to carry out enough information.
Influence of different interaction steps. We are interested in what the optimal highest order of feature interactions is.
Our proposed Fi-GNN can answer the question, since the interaction
step T equals to the highest order of feature interaction. Therefore,
we conduct experiments on how the performance changes w.r.t. the
highest order of feature interaction, i.e., the interaction step T. The
results on Criteo and Avazu datasets are shown in Figure 4(b). On
Avazu datasets, we can see that the performance increases along
with the increasing of T until it reaches 2, after that the performance starts to decrease. By contrast, the performance peaks when
T = 3 on Criteo dataset. This finding suggests 2-order and 3-order
interactions are enough for Avazu and Criteo dataset, respectively.
It is reasonable since the Avazu and Criteo datasets have 23 and
39 feature fields, respectively. Thus the Criteo dataset needs more
interaction steps for the field nodes to fully interact with other
nodes in the feature graphs.
Model Explanation (RQ4)
In this section, we will answer the question that can Fi-GNN provide explanations. We apply attention mechanisms on the edges
and nodes in the feature graphs and obtain attentional edge weights
and attentional node weights respectively, which can provide explanations from different aspects.
Attentional Edge weights. The attentional edge weight
reflects the importance of interaction between the two connected
field nodes, which can also reflect the relation of the two feature fields. Higher the weight is, stronger the relation is. Figure 5
presents the heat map of the globally averaged adjacency matrix
Figure 5: Heat map of attentional edge weights at the globallevel on Avazu, which reflects the importance of relations
between different feature fields.
of all the samples in Avazu dataset, which can reflect the relations
between different fields in a global level. Since they are some anonymous feature fields, we only show the remaining 13 feature fields
with real meanings.
As can be seen, some feature fields tend to have a strong relations
with others, such as site_category and site_id. This makes sense
since the two feature field both corresponds to the website where
the impressions are put on. They contain the main contextual information of impressions. Hour is another feature which have close
relations with others. It is reasonable since Avazu focuses on mobile
scene, where user surfing online at any time of a day. The surfing
time has strong influence on other advertising features. On the
other hand, device_ip and device_id seem to have weak relations
with other feature fields. This may due to that they nearly equal to
user identity, which is relatively fixed and hard to be influenced by
other features.
Attentional Node weights. The attentional node weights
reflect the importances of feature fields’ influence on the overall
prediction score. Figure 6 presents the heat map of global-level and
case-level attentional node weights. The leftmost is an globally averaged one of all the samples in Avazu dataset. The left four are randomly selected, whose predicted scores are [0.97, 0.12, 0.91, 0.99],
and labels are respectively. At the global level, we can see
that the feature field app_category have the strongest influence on
the clicking behaviors. It is reasonable since Avazu focuses on mobile scene, where the app is the most important factor. At the case
level, we observe that the final clicking behavior mainly depends
on one critical feature field in most cases.
CONCLUSIONS
In this paper, we point out the limitations of the previous CTR
models which consider multi-field features as an unstructured combination of feature fields. To overcome these limitations, we propose
to represent the multi-field features in a graph structure for the
global average
Figure 6: Heat map of attentional node weights at both
global- and case-level on Avazu, which reflects the importance of different feature fields on the final prediction.
first time, where each node corresponds to a feature field and different fields can interact through edges. Therefore, modeling feature
interactions can be converted to modeling node interaction on the
graph. To this end, we design a novel model Fi-GNN which is able to
model sophisticated interactions among feature fields in a flexible
and explicit fashion. Overall, we propose a new paradigm of CTR
prediction: represent multi-field features in a graph structure and
convert the task of modeling feature interactions to modeling node
interactions on graphs, which may motivate the future work in this
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of
China (61772528, 61871378) and National Key Research and Development Program (2016YFB1001000, 2018YFB1402600).