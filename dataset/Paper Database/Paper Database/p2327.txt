Sparse Kernel Approximations for Efﬁcient Classiﬁcation and Detection
Andrea Vedaldi
Andrew Zisserman
Dept. of Engineering Science, University of Oxford, UK
{vedaldi,az}@robots.ox.ac.uk
Efﬁcient learning with non-linear kernels is often based
on extracting features from the data that “linearise” the
While most constructions aim at obtaining lowdimensional and dense features, in this work we explore
high-dimensional and sparse ones. We give a method to
compute sparse features for arbitrary kernels, re-deriving
as a special case a popular map for the intersection kernel
and extending it to arbitrary additive kernels. We show that
bundle optimisation methods can handle efﬁciently these
sparse features in learning. As an application, we show that
product quantisation can be interpreted as a sparse feature
encoding, and use this to signiﬁcantly accelerate learning
with this technique. We demonstrate these ideas on image
classiﬁcation with Fisher kernels and object detection with
deformable part models on the challenging PASCAL VOC
data, obtaining ﬁve to ten-fold speed-ups as well as reducing memory use by an order of magnitude.
1. Introduction
Recent advances in large scale convex optimisation have
extended the applicability of linear Support Vector Machines (SVMs) to very large datasets as well as complex
regression problem with structured outputs .
However, there exist no known general method that can
achieve a similar efﬁciency when working directly with
non-linear kernels, and these are often necessary to achieve
optimal accuracy. This explains the increasing interest in
techniques that can efﬁciently reduce non-linear kernels to
linear ones . These techniques have
been shown to obtain state-of-the-art performance especially in the large scale setting .
In more detail, the efﬁciency of linear SVMs depends
on their simple form f(x; w) = ⟨w, x⟩as the inner product of an input data vector x ∈Rd and a parameter vector w ∈Rd. By contrast, non-linear SVMs f(x; α) =
i=1 αiK(xi, x) are expanded in term of evaluations of a
non-linear kernel function K(x, x′) and are much slower
to compute as well as train.
However, since all kernels
K(x, y) can be represented as inner products up to a transformation Ψ(x) of the data, i.e. K(x, y) = ⟨Ψ(x), Ψ(y)⟩,
in principle one can reduce any non-linear SVM to a much
faster linear one. Unfortunately, this usually requires features Ψ(x) that are inﬁnite dimensional and/or difﬁcult
to compute.
This motivates the interest in feature maps
ˆΨ(x) ∈RD that approximate a given kernel, i.e. K(x, y) ≈
⟨ˆΨ(x), ˆΨ(y)⟩, while being ﬁnite dimensional and computationally cheap.
In order to represent the non-linearity, the dimension D
of the feature ˆΨ(x) is usually larger than the dimension d
of the input data x. While most authors have focused on
reducing D (see Sect. 6), a large dimensionality may not
be an issue provided that the features are sparse . The
questions then become: which kernels can be represented
by sparse features? How to compute them? And what advantages or disadvantages do these representations offer?
Our ﬁrst contribution is to derive a general and simple
construction for sparse approximate features ˆΨ(x) for any
kernel K(x, x′) (Sect. 2.1). The theory relates sparse features and existing dense ones geometrically (Sect. 6). It also
includes as a special case the intersection kernel map of
Maji and Berg and generalises it to arbitrary additive
kernels (Sect. 2.1).
Our second contribution is a fast learning method for the
new sparse representations. In contrast to their dense counterparts, the sparse features approximate a kernel by a nondiagonal inner product (Sect. 2.1). We introduce a bundle
optimisation method that learns efﬁciently with sparse
data and non-diagonal regularisers (Sect. 4). This algorithm is the key in leveraging the sparsity of the features not
just as a way of using less memory, but also as a way to
improve learning speed.
Our third contribution is to apply the theory to product
quantisation (PQ) . PQ is a data compression method
for large scale learning that was used, for example, by
the best entry in the 2011 edition of the ImageNet challenge . We show that PQ can be interpreted as a
sparse feature map (Sect. 3) and that this allows an algorithm to learn on the compressed data directly, with a ﬁve to
ten fold speedup on the standard approach (Sect. 5.1) in ad-
dition to the usual substantial reduction in storage. We also
apply PQ for the ﬁrst time to deformable part models ,
demonstrating similar beneﬁts (Sect. 5.2).
2. Kernels and approximate feature maps
Learning a linear SVM amounts to ﬁtting a linear function f(x; w) = ⟨w, x⟩to labels y1, . . . , yn ∈{−1, +1} at
points x1, . . . , xn ∈Rd, i.e. ﬁnding a parameter vector w
such that yi ≈f(xi; w) for all i. This is usually formulated
as the convex optimisation problem
2 ∥w∥2 + 1
Li(⟨w, xi⟩)
where Li(z) is the hinge loss max{0, 1−yiz}. Solvers such
as can be used to optimise (1) very quickly, in
time linear in the data size n and dimension d.
Linear SVMs are fast but not very expressive.
Fortunately, the space of representable functions f(x; w) =
⟨w, Ψ(x)⟩can be easily extended by encoding the data by
a non-linear map Ψ(x) ∈RD. This usually entails augmenting the data dimensionality, i.e. D > d, and therefore
increasing by a factor D/d the space and time complexity
of the linear SVM. Using a sparse encoding may help in
reducing this cost.
Speed, however, is not the only concern in the design of
a good encoding. Arbitrarily increasing the class of representable functions f(x; w) may in fact reduce the performance on the test data due to over-ﬁtting. Fitting the
data must be traded off with the regularity of the function f(x; w), as measured by the term ∥w∥2 in the SVM
objective (1).
The meaning of ∥w∥2 in term of statistical properties of the function f(x; w) is determined by
Ψ(x) itself, or, more fundamentally, by the kernel function
K(x, x′) = ⟨Ψ(x), Ψ(x′)⟩. In fact, any two encodings that
generate the same kernel learn the same functions. Moreover, all PD functions K(x, x′) are the inner product ⟨·, ·⟩H
of an encoding , or feature map, Ψ : X →H in a suitable inner-product space H, called feature space (Fig. 1a):
K(x, x′) = ⟨Ψ(x), Ψ(x′)⟩H.
Since a kernel directly captures the statistical properties of
the SVM, it is often desirable to derive an encoding from a
given kernel rather than designing it directly. While there
exist general methods for constructing a feature space H
for any kernel [20, pag. 33], in calculations one needs to
express the features as ﬁnite vectors of coordinates, which
is harder to obtain, and even impossible if H is inﬁnite dimensional. This motivates looking for approximate but ﬁnite dimensional features, which is the subject of the next
2.1. Approximate feature maps
The goal is to approximate a given kernel K by a feature map ˆΨ : X →RD that is ﬁnite dimensional, possibly
sparse, and efﬁcient to compute. Approximating the kernel means that ˆK(x, x′) = ⟨ˆΨ(x), ˆΨ(x′)⟩must be close, in
some sense, to K(x, x′). We give ﬁrst a standard construction for dense approximate features and then we modify it
to yield sparse ones. It may be helpful to refer to Fig. 1 for
a geometric interpretation.
Dense approximations. A common feature construction
technique is to approximate the exact feature space H of
the kernel K by a subspace H′ ⊂H that is (i) ﬁnite dimensional and (ii) has a computable coordinate representation. A simple way to do so is to deﬁne H′ as the span
of D feature vectors Ψ(zi) of representative data points
z1, . . . , zD ∈X (Fig. 1b), i.e.:
Ψ(zi)Φi : Φ ∈RD
Then, for each feature Ψ(x) ∈H, one deﬁnes an approximation ˆΨ(x) ∈H′. If the goal is to preserve the kernel
between any two points x, x′ ∈X, the Cauchy-Schwartz
(⟨ˆΨ(x), Ψ(x′)⟩H −⟨Ψ(x), Ψ(x′)⟩H)2
≤∥ˆΨ(x) −Ψ(x)∥2
H · ∥Ψ(x′)∥2
suggests to deﬁne ˆΨ(x) as the minimiser the norm of the
residual ∥ˆΨ(x) −Ψ(x)∥H. This is the same as deﬁning
ˆΨ(x) to be the orthogonal projection of the exact feature
vector Ψ(x) on the subspace H′.
The coordinates Φ(x) of the approximation ˆΨ(x) can be
computed by minimising the residual analytically:
Φ(x) = argmin
Let KXX′ denote the Gram (kernel) matrix calculated at
points X = (x1, . . . , xm) and X′ = (x′
1, . . . , x′
Kij = K(xi, x′
j). Moreover, let Z = (z1, . . . , zD) be
the sequence of the D representative points. Rewriting the
residual (4) in matrix form as Kxx −2KxZΦ + Φ⊤KZZΦ,
differentiating w.r.t. Φ, and equating to zero yields the coordinate Φ(x) of point x
pseudoinverse
kernel matrix may not be full rank).
The functions
R are called coordinate functions.
(a) exact feature space
(d) Nyström's approx.
(b) dense approx.
(c) sparse approx.
Figure 1: Geometric interpretation. (a) The exact feature space, reconstructing the kernel with no approximation. A distribution p(x) on the input data maps to a corresponding density in this space (ellipsoid). (b) The dense feature approximation
is obtained by projecting the exact feature vector Ψ(x) to the span H′ of representatives Ψ(z1), . . . , Ψ(zD). (c) The sparse
feature approximation uses a different subset ¯Z of representatives for each encoded point x (in this case ¯Z = {z1}). (d)
Nystr¨om’s approximation (Sect. 6) deﬁnes H′ to be the PCA subspace of the the data distribution (a); the coordinates Φ1(x),
Φ2(x) are expressed relative to an orthonormal PCA basis obtained from an eigenvalue problem.
approximated
i=1 Ψ(zi)Φi(x), which generates the kernel ⟨ˆΨ(x), ˆΨ(x′)⟩H
i,j=1 Φi(x)K(zi, zi)Φj(x′),
ˆK(x, x′) = Φ(x)⊤KZZΦ(x′) .
This expresses the approximated kernel ˆK as the nondiagonal inner product in RD given by the PD matrix KZZ.
An equivalent diagonal representation can be obtained by a
change of coordinates. For example, let KZZ = V ⊤Λ2V
be the eigen-decomposition of the kernel matrix and deﬁne
¯Φ(x) = ΛV Φ(x). Then ˆK(x, x′) = ¯Φ(x)⊤¯Φ(x′).
Both the coordinate vectors Φ(x) and ¯Φ(x) are dense in
general because the feature vectors Ψ(x) are approximated
as combinations of all the representative points Z.
Sparse approximations. The key idea to obtain a sparse
feature map is to note that each given feature vector Ψ(x)
can often be approximated well by a small subset of the
D representative points z1, . . . , zD, for example its neighbours (Fig. 1c). Formally, one can restrict the approximation (4) to use only P < D representative points by solving
The constraint ∥Φ∥0 ≤P allows at most P representative
points ¯Z ⊂Z to have non-zero coefﬁcients. These coef-
ﬁcients are still given by the formula (5) by replacing Z
with ¯Z. Denoting by Φ(x; ¯Z) the resulting sparse coordinate vector, the problem (7) is reduced to ﬁnding the best
subset ¯Zx :
Ψ(zi)Φi(x; ¯Z)
This is the feature space equivalent of the sparse encoding/reconstruction problem, and as such can be approximately solved by popular techniques such as matching pursuit, orthogonal matching pursuit, and Lasso. In certain applications it is also possible to specify ¯Zx directly, as in the
example below. The idea of choosing the representative as
a function of x can be found in the context of Gaussian processes in .
While the vectors Φ(x; P) are sparse by construction,
the kernel approximation is still given by (6), which involves the multiplication by the full matrix KZZ. Switching
to the diagonal representation ¯Φ(x) = ΛV Φ(x; P) as before eliminates KZZ but results in a dense coordinate vector. Therefore the key to efﬁcient learning with sparse features is ﬁnding an algorithm that can work efﬁciently with
sparse data and a non-diagonal inner product.
Example: sparse maps for additive kernels.
An additive kernel K(x, x′) on Rd decomposes as a sum
j=1 k(xj, x′
j) of 1D kernels k(x, x′). Examples include
the χ2 kernel k(x, x′) = 2xx′/(x + x′), the Hellinger’s
xx′, and the intersection kernel min{x, x′}, which
have been shown to perform particularly well for histogram
data (bag-of-visual visual words, spatial histograms,
colour histograms, HOG, etc.).
Since a feature for the additive kernel K(x, x′) can be
obtained by stacking the features for each of the components k(xi, x′
i), it sufﬁces to derive the latter. To construct
a sparse feature Φ(x) for the 1D kernel k(x, x′) one can
(i) sample D representative points zi uniformly on R and
(ii) project each point x ∈R to the two adjacent representatives ¯Z = {zi, zi+1} such that zi ≤x < zi+1. Consider for example the intersection kernel min{x, x′} and,
without loss of generality, the set of representative points
Z = {0, 1, 2, . . . , D −1}. Evaluating Φ(x) selects the two
points ¯Z = {i, i + 1}, i = ⌊x⌋and sets the respective coef-
ﬁcients in Φ(x) using (5), i.e.
 i + 1 −x
This equation linearly distributes x to zi and zi+1. The resulting feature Φ(x) is identical to the sparse intersection
kernel map of , which can therefore be interpreted as
a sparse projection method, optimal in the sense of (4). In
addition to allowing for many variants (e.g. using more than
two representative points), the theory extends this construction to arbitrary additive kernels, including χ2, for which we
give the ﬁrst sparse feature approximation:
2(1 + 2i)x
(i + x)(1 + i + x)
 i + 1 −x
The next section investigates a completely different application of the theory, namely product quantisation.
3. Product quantisation as a sparse feature
This section applies the general theory of Sect. 2.1 to
Product Quantisation (PQ) . Consider a dataset of
high-dimensional descriptors xi ∈Rd (e.g. spatial histograms , Fisher vectors ). Storing one descriptor
xi requires bd bits, where b is the average number of bits
per component, 32 if IEEE single precision math is used.
The latter is just a particular way of encoding independently
each descriptor component in space of 232 elements. PQ
encodes the data more efﬁciently by considering groups of
components instead. In detail, PQ (i) partitions the vector xi
into M blocks of G = d/M components, (ii) quantises each
block into a codebook of 2bG elements by using k-means on
sample data, and (iii) stores for each block just the index of
the corresponding codeword, for a total of bGM = bd bits.
PQ reduces the memory required to store the dataset by
a factor 32/b. In practice, this factor can be an order of
magnitude or more , making the technique very useful
for large-scale learning, where the uncompressed image descriptors can occupy terabytes. In particular, if compression
allows for the entire data to be stored in central memory,
this dramatically accelerates learning compared to accessing data from disk. According to , however, the PQ
compression offers no intrinsic speed beneﬁt because the
data must be uncompressed on the ﬂy in the solver, as the
latter must still operate on the original vectors xi. We show
here that, by interpreting PQ as a sparse feature encoding,
it is instead possible to learn directly on the PQ compressed
data, achieving a signiﬁcant speed-up in combination with
appropriate solvers (Sect. 4).
In particular, consider learning a linear SVM (1) on PQ
compressed data. Assume for simplicity that there is only
one PQ block (M = 1), as the extension to M > 1 is immediate by stacking. PQ approximates each data point x by
the closest element zi in a codebook Z = (z1, . . . , zD) ∈
RG×D, where D = 2bG is the number of codewords. This
is the same approximation given by the sparse feature construction (7) by imposing that the feature Φ(x) has exactly
one non-zero component (P = 1) and that the latter is equal
to 1. The latter condition ensures that only the index of
the non-zero component must be stored and not its value as
The sparse PQ feature map Φ(x) in combination with
the inner product KZZ results in the approximated kernel
(Sect. 2.1)
ˆK(x, x′) = Φ(x)⊤KZZΦ(x) = (Φ(x)Z)⊤(ZΦ(x′))
where ZΦ(x) returns the codeword zi that PQ uses to
approximate x. Therefore decompressing the data in the
solver as in results in exactly the same approximate
kernel as the PQ features, and the two learning methods are
equivalent (Sect. 2). Handling non-linear (e.g. additive) kernels is similar, except that PQ must be modiﬁed to use the
metric induced by the kernel, rather than the Euclidean one.
The next section introduces a solver that can accelerate
learning by using the PQ sparse features.
4. Efﬁcient learning with bundle methods
Modifying the linear SVM learning problem (1) to use
the sparse feature Φ(x) of Sect. 2.1 and the corresponding
non-diagonal inner product ⟨·, ·⟩KZZ yields the objective
2 w⊤KZZw + 1
Li(w⊤KZZΦ(xi)). (10)
To exploit the sparsity of the data Φ(x) in the calculation
of the loss (second term), one can lump together the dense
factors in v = KZZw and consider the equivalent objective
Li(v⊤Φ(xi)).
To handle efﬁciently non-diagonal regulariser K†
ZZ as well
as the the sparse data we propose to modify the cuttingplane/bundle solver of . This starts by collecting all
the individual loss terms in (11) into a single loss function
L(v⊤Φ(X)) = 1
Li(v⊤Φ(xi))
where Φ(X) = [Φ(x1), . . . , Φ(xn)], and then solving the
convex optimisation problem
ξ ≥L(v⊤Φ(X)).
This is called one-slack formulation because ξ is a single
scalar slack variable capturing the loss averaged over all example data points. The idea is then to construct a sequence
of approximated solutions v1, . . . , vt, . . . corresponding to
a progressively more reﬁned piecewise linear lower approximation of the loss function
i=1,...,t bt −⟨at, v⟩≤L(v⊤Φ(X)),
where (at, bt) are the parameters of the plane added at iteration t. Each plane is tangent to the loss at L(v⊤Φ(X)) at
at = −Φ(X)∇L(v⊤
t Φ(X))+v⊤
Note that the planes are under-estimators of the loss due to
the convexity of the latter; if the loss is not differentiable,
then one simply takes a sub-gradient instead of the gradient
Given the current solution vt, the next one vt+1 is found
as the minimiser of the convex problem
ξ ≥bi −⟨ai, v⟩,
i = 1, 2, . . . , t.
The dual of this problem is given by
+,∥α∥1=1 b⊤α −1
2λα⊤(A⊤KZZA)α
where α are the dual variables, A = [a1, . . . , at], b =
[b1, . . . , bt]⊤and v = KZZα/λ at the optimum (strong
duality condition). The size of the dual problem is equal to
the iteration index t and unrelated to the number of training
samples n. Since typically t ≪n, this explains the efﬁciency of the method. Note also that the term A⊤KZZA
involves multiplying by the kernel matrix KZZ instead of
the pseudo-inverse K†
ZZ that appears in the primal (11).
Assuming that the algorithm converges at t ≪n, as typical, the cost of each iteration is O(Pn + D2), where P is
the number of non-zero components in the data. O(Pn) operations are required to generate the new plane (at, bt) by
scanning the dataset, and O(D2) operations are required to
compute the product ˜at = KZZat.
By comparison, using the equivalent dense features ¯Φ(x)
with diagonal regulariser (Sect. 2.1) requires O(Dn) operations. Hence the sparse features use D/P times less memory and are faster provided that Pn + D2 < Dn. While the
latter is always true for sufﬁciently large n (since P ≪D
by construction), we see next how this complexity can be
signiﬁcantly reduced by exploiting the structure of KZZ.
Exploiting the structure of KZZ. The bottleneck in the
bundle algorithm is the multiplication KZZai. For example, for PQ with d/G blocks and D = 2bG codewords
per block, this multiplication requires O(dD2/G) operations (for all the blocks).
Fortunately, this calculation
can be signiﬁcantly accelerated by using the factorisation
KZZai = Z⊤(Zai), as this requires only O(dD) operations. With this improvement, using the sparse PQ features
with the bundle solver (which we call delayed expansion)
uses a fraction
d2bG + dn/G
of the operations that would be required by expanding
points on the ﬂy (immediate expansion) as in , or by
using directly dense features. Since the last term becomes
rapidly negligible for a large number of data points n, this
algorithm is roughly G times faster, where G is the size of
the PQ blocks.
As a second example, consider the intersection kernel
approximation (8). In this case the kernel matrix KZZ has
KZZ = V ⊤V,
Since the products V v and V ⊤(V v) are cumulative sums,
these can be computed in time O(D) rather than O(D2).
Comparison with stochastic gradient descent. Stochastic gradient methods such as PEGASOS are sometimes preferred to bundle methods because they offer similar convergence speed but simpler implementation. For example, apply PEGASOS to the optimisation of (11),
which results in the simple update equation
ZZv + δiΦ(xi)
where ηt is the learning rate and δi = ˙Li(v⊤Φ(xi)) is the
derivative of the loss. However, since K†
ZZ is full, the SGD
update is not sparse. While this can be alleviated by optimisations such as the use of mini-batches , it is still
a signiﬁcant bottleneck, especially if the multiplication by
KZZ requires O(D2) operations as in the general case.
To summarise, the bundle solver performs a dense operation only after each complete pass over the data, and
therefore seems more suitable to handle sparse representations with non-isotropic regularisers.1 The next section
demonstrates applications to the efﬁcient learning of image
classiﬁers and object detectors.
5. Experiments
5.1. PQ for image classiﬁcation
This section evaluates the PQ compression technique on
the PASCAL VOC 2007 classiﬁcation challenge . The
task is to classify twenty object categories and the performance is measured as mean Average Precision (mAP). Each
image is represented by the Fisher encoding xi obtained as
described in : ﬁrst, dense SIFT features are extracted
every three pixels (using the vl_phow function of )
and compressed from dimension 128 to 80 by using PCA;
then a Gaussian mixture model with 256 components is ﬁtted to sample PCA-SIFT descriptors and used to generate
a Fisher encoding of the image for 3 × 1 spatial subdivisions , for a total of 2 × 256 × 80 × 3 = 40,960
dimensions. The dataset totals about 5,000 training images
and occupies about 2GB of memory.
The baseline system works as well as (about 59%
mAP) and can be improved slightly by increasing the number of spatial subdivisions (up to about 62% in our experiments). This is a very solid baseline which, combined with
PQ, let top the ImageNet classiﬁcation challenge in
2011 . In fact, PQ allows for a tenfold reduction of
memory usage (from 2GB to about 100MB in our case,
see Fig. 2) with minimal impact on the classiﬁcation performance (less than 1% mAP).
Our sparse PQ features (delayed expansion) further improve this excellent system by a 5–10 fold speedup in
training, compared to decompressing the data on the ﬂy
(immediate expansion) . As predicted by (16), the gain
is larger for larger block sizes G, which also results in better accuracy. As the number of codewords 2bG increases,
the term 2bG/n starts to dominate and the speedup becomes
smaller (16). This indicates that our technique is particularly useful for a very large number n of training samples.
5.2. PQ for object detection
This section proposes a new application area of PQ,
namely deformable parts models (DPM) for object category detection. A DPM is a collection of spring-connected
parts whose appearance is described by HOG templates.
Matching a part to an image location requires multiplying
the part template by the HOG descriptor extracted at that
location. The latter is a collection of w ×h HOG cells, each
1A second problem not discussed in is the choice of a learning rate.
While ηt = 1/(λt) is optimal for a SVM with isotropic regulariser λ 
(which is also the difference between PEGASOS and standard SGD), handling (11) is trickier. For example, K†
ZZ for the intersection kernel (17)
does not have full rank, meaning that the objective function is not even
strongly convex. In practice, we found this tuning to be delicate.
of which is a d-dimensional feature vector (where d = 32
in ), so this costs O(whd) operations. Detecting with the
model requires matching all parts at all locations and scales,
a costly multi-dimensional convolution operation, and is the
main bottleneck in testing . Learning a DPM is even
more expensive, as this entails testing multiple times on the
training data in order to identify the negative support vectors (mining of hard negatives) .
PQ can be used to encode HOG cells with a single codeword index (Sect. 3).
To test this idea, we reimplemented from scratch, using a bundle solver rather
than the original SGD method . This solver has the advantages discussed above (Sect. 5.1) and was found to be as
fast as SGD in the standard (uncompressed) case. The code
is run on a machine with twelve cores, parallelising operations such as mining for the hard negative examples. Other
reﬁnements from , such as bounding box regression and
contextual rescoring, are not used in our experiments.
Space saving. Space is used to store the hard negative examples (about 1GB) and the pre-computed HOG features
for all training images (about 14GB). For D = 256 and
D = 512 PQ codewords a modest drop in detection accuracy (about 3% mAP) was observed. However, encoding
each HOG cell with log D bits rather the 32 × 32 = 1,024
required by IEEE single precision ﬂoats gives a 113 fold
reduction of storage for D = 512. Compared to using
the common trick of remapping the feature components to
bytes, as done in and in Fig. 3 for the baseline results, PQ still results in a 28-fold saving. In order to further
simplify our implementation, we simply used 32 bits to encode each HOG codeword index (for D = 512 this wastes
32 −9 = 23 bits). Even so, the observed storage reduction
is more than tenfold, from a dozen GBs to less than one.
Time saving. Testing can be up to d = 32 times faster
with PQ because convolving a part ﬁlter requires now just
wh operations rather than whd (because each HOG cell is
represented by a P = 1 sparse vector). Yet each of the
wh operations involves a non-local memory access. Moreover, our sparse convolution code is not as optimised as the
routine that we use for the dense case (which uses a fast
BLAS implementation). Overall, the sparse convolution is
“just” twice as fast than the standard method (Fig. 3). This
speedup is orthogonal to others such as and can be
combined with them. Since testing is an integral part of
training, and in fact is its bottleneck, this speedup transfers
to training too. Finally, the bundle solver beneﬁts from using PQ, as indicated by (16), and is three times faster than
using the uncompressed data.
6. Related methods
This section summarises existing methods for the construction of approximate feature maps for kernels.
Bit rate (compression factor)
Accuracy (AP %)
0.12 (256)
0.17 (192)
0.25 (128)
pq−immediate (G = 1)
pq−immediate (G = 4)
pq−immediate (G = 6)
pq−immediate (G = 8)
pq−delayed (G = 1)
pq−delayed (G = 4)
pq−delayed (G = 6)
pq−delayed (G = 8)
Bit rate (compression factor)
0.12 (256)
0.17 (192)
0.25 (128)
Bit rate (compression factor)
0.12 (256)
0.17 (192)
0.25 (128)
Figure 2: PQ for classiﬁcation. Plain linear SVM (heavy black horizontal line), PQ SVM with immediate expansion (solid
lines, ) and our PQ SVM with delayed expansion (dashed lines, Sect. 3). From the left to right: mAP classiﬁcation
accuracy on PASCAL VOC 2007, memory, and learning time. PQ allows for a substantial compression (> 40 times) with
minimal accuracy loss (1% mAP). Delayed expansion results in a signiﬁcant speed-up (up to 10 times) compared to the
usual immediate expansion method . In principle, delayed expansion is G times faster than the plain (uncompressed)
SVM too, but this is not observed due to implementation details (the standard SVM learning code uses an optimised BLAS
library, while our delayed expansion code does not). Crucially, however, the plain solver cannot be used efﬁciently if the
uncompressed data does not ﬁt in central memory .
aerop bicyc bird boat bottl
cat chair cow dinin
dog horse motor perso potte sheep sofa train tvmon mean
58.1 10.0 12.7 21.2 52.1 55.0 20.1
14.8 25.6 43.8
neg. mining [min]
solver [min]
neg. mined [GB]
1.09 1.27 1.05 1.06 1.05 1.05 1.10
1.03 1.13 1.11
13.6 15.0 13.2 12.5 13.6 17.8 15.8
13.1 13.7 14.7
9.5 10.4 16.4 47.6 52.0 16.0
12.7 19.7 41.7
neg. mining [min]
solver [min]
neg. mined [GB]
0.04 0.05 0.04 0.04 0.04 0.04 0.05
0.04 0.04 0.04
Figure 3: PQ for detection. Space and time complexity of learning deformable parts model on the PASCAL VOC
2007 data for the twenty classes: the detector accuracy as AP on the test data, the time required to mine the hard negative
examples, the time spent in the bundle solver, the space required to store the hard negatives, and the space required to stored
the pre-extracted HOG features for the training data. Top: standard method. Bottom: sparse features from PQ.
Nystr¨om’s (PCA) approximations. Most feature constructions are variants of Nystr¨om’s approximation , whose
geometry is also related to the one of our sparse features
(Fig. 1). Given a data distribution p(x), this approximation seeks directly the feature map Φ : X →RD that
minimises the average reconstruction error of the kernel
This is given by projecting the exact feature Ψ(x)
on the top D principal components of the weighted kernel
K(x, x′)p(x)p(x′) (Fig. 1d).The corresponding coordinate functions (Sect. 2.1) Φi(x) = κiui(x) are proportional to the eigenfunctions ui(x) of the kernel with the D
largest eigenvalues κ2
Nystr¨om’s construction is often considered a theoretical
tool as the density p(x) is not known analytically. An exception is that, by assuming a simple form of p(x), use
this construction to derive closed form features for kernels
such as χ2 and intersection.
Alternatively, the distribution p(x) can be approximated
empirically by the training samples X = (x1, . . . , xn)
(Fig. 1.c).
Finding the kernel eigenfunctions reduces to
a n × n discrete eigenproblem, but encoding a new point
x is quite slow as it requires computing the projections
K(x, x1), . . . , K(x, xn). This approximation is equivalent
to computing the dense feature map (5) with representative
points Z = X followed by PCA. It can be accelerated signiﬁcantly for additive kernels because in this case the
coordinate functions have a scalar argument and can be precomputed.
Sampling. Other popular techniques are based on sampling
ideas , but the resulting features can be high dimensional and slow to compute in practice (an exception are
once more the additive kernels ).
7. Summary
We have presented a general method to construct sparse
approximate feature maps for arbitrary kernels, relating
it to dense constructions based on Nystr¨om’s approximation. These representations are based on non-diagonal inner
products which can be handled efﬁciently by bundle optimisation methods. We proposed two applications of the theory: the encoding of additive kernels and accelerating learning with product quantisation. The latter technique is able
to reduce signiﬁcantly the memory needed for large scale
learning in classiﬁcation and detection as well as accelerating optimisation and inference, with only a minor impact on
Acknowledgments. This work was supported by the Oxford Violette and Samuel Glasstone Research Fellowships
in Science and the ERC grant VisRec no. 228180.