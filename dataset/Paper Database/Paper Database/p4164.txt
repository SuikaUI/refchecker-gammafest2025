Vol.:(0123456789)
Machine Learning 111:243â€“272
 
Boosting Poisson regression models withÂ telematics car
driving data
GuangyuanÂ Gao1Â Â· HeÂ Wang2Â Â· MarioÂ V.Â WÃ¼thrich3
Received: 9 May 2020 / Revised: 1 February 2021 / Accepted: 9 February 2021 /
Published online: 21 March 2021
Â© The Author(s) 2021
With the emergence of telematics car driving data, insurance companies have started to
boost classical actuarial regression models for claim frequency prediction with telematics car driving information. In this paper, we propose two data-driven neural network
approaches that process telematics car driving data to complement classical actuarial pricing with a driving behavior risk factor from telematics data. Our neural networks simultaneously accommodate feature engineering and regression modeling which allows us to
integrate telematics car driving data in a one-step approach into the claim frequency regression models. We conclude from our numerical analysis that both classical actuarial risk
factors and telematics car driving data are necessary to receive the best predictive models.
This emphasizes that these two sources of information interact and complement each other.
Keywordsâ€‚ Densely connected feed-forward neural networkÂ Â· Convolutional neural
networkÂ Â· Combined actuarial neural networkÂ Â· Claims frequency modelingÂ Â· Telematics
car driving dataÂ Â· Poisson regressionÂ Â· Generalized linear modelÂ Â· Regression treeÂ Â·
Telematics heatmap
1â€‚ Introduction
Product development and car insurance pricing is an important task of actuarial modeling.
Generalized linear models (GLMs) are state-of-the-art for car insurance pricing. To overcome some of the shortcomings of GLMs other regression models are also used, e.g., generalized additive models (GAMs) and regression trees are promoted in Henckaerts etÂ al.
Editors: Tim Verdonck, Bart Baesens, MarÃ­a Ã“skarsdÃ³ttir and Seppe vanden Broucke.
* Mario V. WÃ¼thrich
 
Center forÂ Applied Statistics andÂ School ofÂ Statistics, Renmin University ofÂ China, 100872Â Beijing,
Department ofÂ Finance andÂ Ying Shang Nan Ke Actuarial Science Center, Southern University
ofÂ Science andÂ Technology, 518055Â Shenzhen, China
Department ofÂ Mathematics, ETH Zurich, RiskLab, 8092Â Zurich, Switzerland
Machine Learning 111:243â€“272
 , boosting models are used in Henckaerts etÂ al. , Lee and Yang etÂ al.
 , or neural networks are proposed in Ferrario etÂ al. . Many pricing approaches
have in common that they are solely based on classical policyholder information such as
age of driver, type of car, age of car, vehicle power, etc. This classical policyholder information is called a priori information because it is available at the conclusion of contract,
see Verschuren . Increasingly, posterior information about individual policyholder
behavior and insurance claims is collected, and this a posteriori information is incorporated
in insurance policy renewals. In car insurance, a posteriori information is often encoded
in a bonus-malus system (BMS) which scores past claims history and directly affects the
insurance prices of policy renewals by a multiplicative factor. One stream of literature on
BMS studies optimal design, efficiency and economic questions related to BMS, see Loimaranta , De Pril , Lemaire , Denuit etÂ al. , Brouhns etÂ al. 
and Ãgoston and Gyetvai . A second stream of literature rather addresses the question of how an existing BMS can be used to improve the predictive power for forecasting future claims since a BMS reveals past policyholder behavior, see e.g.,Â  Boucher and
Inoussa , Boucher and Pigeon and Verschuren .
With the emergence of telematics car driving data one can even go one step further,
namely, one does not only have a discrete claim indicator variable (which runs into the
BMS), but insurers receive continuously personalized car driving information about their
policyholders. This continuous telematics data may reveal that a specific young driver has a
cautious driving style, while a matured driver may still have a wild and aggressive driving
behavior. Telematics car driving data will encode such differences. Our goal is to explore
first steps on how such information can be extracted from telematics car driving data. This
is far from being trivial because telematics car driving data comes with all its challenges
such as big data (we typically have TBs of data that needs to be processed), data error, etc.
Our telematics data records speed and acceleration in all directions second by second
from the start of the engine to the switch off of the engine. Current literature proposes
three different ways to perform feature engineering on such type of data: (a) Weidner
etÂ al.Â  extract covariates from time series of telematics data using discrete Fourier transforms; (b) Huang and Meng , Paefgen etÂ al. , Sun etÂ al. and
Verbelen etÂ al. do not directly consider telematics car driving data in time series
structure, but rather calculate scores such as average speed, 90%-quantile of acceleration
rates, or proportions of driving distances on different types of roads; (c) Gao etÂ al. ,
Gao and WÃ¼thrich extract covariates from speed-acceleration heatmaps using principal components analysis (PCA) and neural network architectures. These papers have in
common that they first extract several potential risk factors from telematics data, and then
select the useful ones in a second step using a regression model for claim frequency modeling. This procedure has limitations because it assumes that the extracted feature information is the relevant one, i.e.,Â  it involves judgment in a first step similar to manual feature
engineering, which may not be optimal for subsequent steps.
We mention other literature on telematics data which extracts specific information.
Ayuso etÂ al. , Boucher etÂ al. and Lemaire etÂ al. study risk
exposures such as driving distances or time exposures. Such information is interesting for
two reasons. Firstly, an appropriate exposure acts as an offset in regression modeling and,
henceforth, does not need explicit modeling. Secondly, new insurance products are developed where prices are calculated on a pay-as-you-drive (PAYD) basis. Such products may
also be interesting from an environmental point of view because driving less makes insurance less costly. Denuit etÂ al. propose a credibility model to incorporate posterior
information of driving behavior, this is in the spirit of BMS. Richman discusses
Machine Learning 111:243â€“272
possible ways to analyze telematics data. Ho etÂ al. , Hung etÂ al. and Kamble
etÂ al. study telematics data and driving cycles to understand vehicular emissions,
energy consumption and impacts on traffic in different cities of the world. The techniques
used for these studies rely on similar tools as proposed in Gao etÂ al. .
In this paper, we use the speed-acceleration heatmap construction proposed in Gao etÂ al.
 as a representation of driving behavior. However, in contrast to this former paper,
we do not use a two-step approach by first scoring heatmaps and then using these scores in
a regression analysis. In this paper, we conduct both feature engineering of the speed-acceleration heatmap and claim frequency regression simultaneously, using a densely connected
feed-forward neural network and a convolutional neural network, respectively. That is, the
networks learn a feature representation of the speed-acceleration heatmap that is directly
used in a Poisson regression model. We start from a densely connected feed-forward neural
network because this is the most basic neural network. Secondly, we challenge the previous approach with a convolutional neural network. Our data has a spatial structure, and it is
known that convolutional neural networks can deal very successfully with spatial objects;
we refer to Goodfellow etÂ al. for a general discussion of neural networks and to
Chollet and Allaire for an introduction to the neural network package Keras used
in this paper. The fundamental difference between the two types of neural networks is that
dense layers learn global patterns on the input space, while convolution layers learn local
patterns. Compared to dense layers, convolution layers have relatively fewer parameters
since they apply the same convolutional operation to different local regions of the input
space. Their main property is translation invariance which allows convolution layers to
find similar patterns at different locations of the input space, see Wiatowski and BÃ¶lcskei
 , Zhang etÂ al. and Zhang etÂ al. . It is often said that deep learning
models are black boxes, but this is not necessarily true for convolutional neural networks.
Convolutional neural network can be interpreted and we discover patterns in the speedacceleration heatmaps which explain the most relevant drivers of higher claim frequencies.
In this paper, we are going to compete three different set-ups: (1) Poisson GLM based
on classical actuarial risk factors (covariates); (2) Poisson neural network regression model
based on telematics data, and (3) a combination of (1) and (2) in the spirit of WÃ¼thrich
and Merz . From our numerical analysis we conclude that (3) is the most powerful
approach. Firstly, not surprisingly, Poisson GLMs based on classical risk factors can be
enhanced by telematics information, and secondly, telematics information is not sufficient
because classical actuarial risk factors may reveal under which circumstances the telematics data has been generated. Thus, both sources of information interact which makes them
a more powerful predictive tool.
1.1â€‚ Organization
SectionÂ 2 introduces our GLM approach for claim frequency modeling using classical risk
factors. SectionÂ 3 describes our telematics car driving data and it establishes two neural
networks for claim frequency modeling using speed-acceleration heatmaps. SectionÂ 4 considers a combined actuarial neural network for claim frequency modeling using both the
classical risk factors and the speed-acceleration heatmaps. Moreover, the convolutional
neural network solution is interpreted to explain how a driving behavior risk factor is constructed from speed-acceleration heatmaps. SectionÂ 5 concludes with our main findings.
Machine Learning 111:243â€“272
2â€‚ Claims frequency modeling using classical risk factors
Our data considers compulsory motor third-party liability (MTPL) insurance of n = 973
cars in China. Each insurance policy has the same maximal coverage of CNY 122,Â 000.
These MTPL insurance policies have been active within the time period from 01/01/2014
to 31/05/2017, and we have full reporting information up to 29/06/2017. A preliminary
analysis indicates that more than 99% of all claims are reported within one month after
the accident date. For this reason (and because information is missing), we neglect late
reported claims after 29/06/2017; we expect that such late reported claims only marginally
influence our analysis. For insurance policies i = 1, â€¦ , nâ€Š, we denote the response variable
of claim counts by Yi âˆˆâ„•0â€Š, the exposure of the effective policy duration by ei > 0 (also
called years-at-risk), and the set of classical risk factors by xiâ€Š. We assume that the claim
counts can be described by the following regression model
where íœ†âˆ¶X â†’â„+ is a regression function mapping the covariates xi âˆˆX to expected frequencies íœ†(xi) âˆˆâ„+â€Š. The general aim is to choose regression functions íœ†(â‹…) such that they
describe the systematic effects in the claim counts as accurately as possible. This choice
involves pre-processing of covariates xi âˆˆX (and choices of appropriate covariate spaces
Xâ€Š), which is part of regression modeling.
Before giving a descriptive analysis of our data, we give the main summary statistics.
The total exposure over the entire portfolio is âˆ‘n
i=1 ei = 2, 177 years-at-risk, i.e.,Â  several
policies run over multiple years, the average exposure being âˆ‘n
i=1 eiâˆ•n = 2.24 years-at-risk.
FigureÂ 1 (top-left) shows a histogram of the aggregate exposures with policies partitioned
w.r.t.Â  the exposure lengths eiâ€Š. The overall (homogeneous) claim frequency estimate is
i=1 ei = 0.24â€Š; this average is consistent with the market benchmark in China
but it is much higher than typically in Europe and North America. In China, MTPL policies cover both physical injuries and property damages of third party regardless whether
the policyholders is at fault or not, only the corresponding maximal coverage differs. This
explains why the frequency is comparably higher than in other regions of the world. FigureÂ 1 (top-right) shows the claim counts Yi on each policy. Most policies do not suffer any
claims, and very few policies have more than 3 claims.
2.1â€‚ Feature engineering
Our first modeling attempt for regression problem (2.1) is to choose a Poisson generalized linear model (GLM) with log-link function (being the canonical link for the Poisson
GLM). This choice implies that covariates xi impact the regression function linearly on the
canonical scale, which, in turn, requires covariate pre-processing so that we receive reasonable regression models. We start by describing the available covariate information. We
consider five covariate components: Chinese region (â€Šíš›íšíšíš’íš˜íš—iâ€Š), driverâ€™s gender (â€Šíšíšíš—íšíšíš›iâ€Š),
driverâ€™s age (â€Šíšíš›íš’íšŸíšíš›_íšŠíšíšiâ€Š), carâ€™s age (â€ŠíšŒíšŠíš›_íšŠíšíšiâ€Š) and average driving hours in (0,Â  80]
km/h per week (â€ŠíšŠíšŸíš_íš‘íš˜íšíš›íšœiâ€Š), for all insurance policies i = 1, â€¦ , nâ€Š. Our preliminary data
cleaning ensures that the main driver of a car does not change over the entire observation
period and we concatenate policy renewals of the same driver over this observation period.
Thus, we can follow the same driver for at most 3 years and 5 months from 01/01/2014 to
31/05/2017; the resulting exposures ei are shown in Fig.Â 1 (top-left). We remark that we
for i = 1, â€¦ , n,
Machine Learning 111:243â€“272
exclude several other covariates such as number of seats, car brand or price of car since a
preliminary analysis indicates that those covariates are less significant for claims frequency
prediction, at least for our small insurance portfolio, otherwise we may run into issues of
over-fitting.
We are going to provide empirical statistics for these five covariates in Fig.Â 1 (middle
and bottom rows). The distribution of the exposures is shown on the left axis (in black and
gray bars), and the empirical frequencies are shown on the right axis (in blue color) with
dotted blue lines giving estimated 2 standard deviation confidence bounds. We take the
logarithm of the empirical claim frequencies because for small exposures they are volatile;
moreover, the y-scales are identical on each row.
Fig.â€¯1â€‚ â€‰Histogram of the distribution of exposures (left axis) and the corresponding logarithm of the empirical claim frequencies (right axis and in blue color, where appropriate): policies partitioned w.r.t.Â  exposures
ei (top-left), claim counts Yi (top-right), regions (middle-left), gender (middle-right), driverâ€™s age (bottomleft), carâ€™s age (bottom-middle), and average driving time (bottom-right); each row has identical y-scale
Machine Learning 111:243â€“272
2.1.1â€‚ Categorical variables: Chinese regions andÂ driverâ€™s gender
In a preliminary step, we have merged 11 Chinese regions containing only very small exposures, resulting in four different regions {í™·íšíš‹íšíš’, í™¾íšíš‘íšíš›íšœ, íš‚íš‘íšŠíš—íšíš‘íšŠíš’, íš‰íš‘íšíš“íš’íšŠíš—íš}â€Š. From
Fig.Â  1 (middle-left) we observe that Hebei province has a substantially lower empirical frequency than the other 3 Chinese regions. Concerning gender, male drivers have a
slightly lower empirical frequency, see Fig.Â 1 (middle-right), however, the difference not
being significant on a 95% confidence level.
2.1.2â€‚ Driverâ€™s age
Typically, a suitable regression function for car claim frequency modeling is non-monotone in the driverâ€™s age variable. Therefore, the driverâ€™s age covariate needs pre-processing.
There are two different ways to do so, either we use categorical coding by building age
groups or we use a different functional form for driverâ€™s age, for instance, a natural cubic
spline leading to a GAM. For the moment, we consider categorical coding of the driverâ€™s
age variable. We explore a marginal Poisson regression tree using covariate íšíš›íš’íšŸíšíš›_íšŠíšíši
as explanatory variable and exposure ei as offset for building age groups of homogeneous
claims frequency. This Poisson regression tree suggests to build 5 age groups to receive
sufficient homogeneity within each age group (still keeping sufficient exposures in all age
groups), see Fig.Â 2. We observe that the smallest group contains 13% of all policies and the
biggest group 29% of all policies. The resulting age groups are given in TableÂ 1.
We note that we will further merge age groups in Sect.Â  2.2. We have also explored
incorporating the driverâ€™s age variable as a continuous covariate in a GAM. The predictive
performance of the latter has been similar to the GLM with categorical coding but using a
Fig.â€¯2â€‚ â€‰Marginal Poisson regression tree for claim frequencies w.r.t.Â  driverâ€™s age
Machine Learning 111:243â€“272
more complex regression function, therefore, we work with the simpler age grouping version given in TableÂ 1.
2.1.3â€‚ Carâ€™s age
For cars aged less than 3 years, the claim frequencies are similar. For cars aged more than
3 years, the logarithm of claim frequency has a linearly increasing shape, see Fig.Â 1 (bottom-middle). Therefore, we pre-process the carâ€™s age to create a new explanatory variables,
íšŒíšŠíš›_íšŠíšíšâ†¦íšŒíšŠíš›= max(0, íšŒíšŠíš›_íšŠíšíšâˆ’3).
2.1.4â€‚ Average driving time perÂ week
As emphasized in Ayuso etÂ al. , the total driving time is important covariate information. At this stage, it is debatable whether total driving time is a classical or
a telematics covariate because this information is only available if suitable devices are
installed in the cars. We remark that we follow insurance policies over multiple years, but
only for the most recent periods there is telematics data available. For this reason, we typically have a longer observation period of claims history on insurance policies than of corresponding telematics data. As a compromise we calculate average driving time per week
from the time periods where telematics data is available. An implicit assumption is that
the calculated average driving time per week using the more recent periods of telematics
data is a good approximation for the entire observation period of insurance exposure. Thus,
we create a variable â€˜average driving time per weekâ€™ (in hours), we cap this variable at 21
hours per week (to avoid outliers) and we only account for the time in speeds (0,Â 80]km/h,
since this corresponds to the telematics heatmaps discussed below. From Fig.Â 1 (bottomright), we observe a linear relationship between the logarithm of claim frequency and the
average driving hours per week. Note that one can also use the average driving distance per
week instead, both variables are measures of driving intensity.
2.2â€‚ A generalized linear model forÂ claims frequency
The five classical risk factors have been pre-processed as described above to provide
The variable gender is binary. The age of car variable car and the average driving time
per week ave_hours are continuous. The variable region and the age group variable
age_group are incorporated by using dummy coding. Henceforth, we have 10-dimensional covariate space X âŠ‚â„10 that can be directly used in a Poisson GLM with canonical
xi = 111:243â€“272
We split our entire portfolio into a learning data set and a test data set in a stratified
way w.r.t.Â  empirical claim frequencies. The learning data set contains 780 cars with a total
exposure of 1,Â 756 years-at-risk, and the test data set contains 193 cars with a total exposure of 421 years-at-risk. We calibrate our models on the learning data set and evaluate its
out-of-sample predictive performance on the test set. In later sections on neural network
regression models, we will further split the learning data set into training data set D1 and
validation data set D2â€Š. The validation data set is used to determine hyper-parameters while
training the models with the training data set. Denote the index sets of training, validation
and test data sets by D1, D2, D3â€Š, respectively. We summarize the partition used throughout
this document in TableÂ 2.
2.2.1â€‚ Poisson generalized linear model
We start with the Poisson GLM. The basic assumption is that the underlying expected
claims frequency of íœ† has a multiplicative structure in the covariate components xi providing regression function on the canonical scale (under log-link)
We choose íš–íšŠíš•íš driver, íš–íš’íšíšíš•íš_1 age group and íš‰íš‘íšíš“íš’íšŠíš—íš region as reference levels for
dummy coding. The coefficients are estimated by minimizing the Poisson deviance loss on
the learning set D1 âˆªD2:
where íœƒ= (í›½0, (í›¼r)r, í›¾íšíšíš–íšŠíš•íš, (í›¿ag)ag, í›½1, í›½2)
âˆˆâ„11 collects all GLM regression parameters.
The results show that several region coefficients and age group coefficients are not significant. Including these regions and age groups may lead to over-fitting. We perform the
step-wise variable selection on the full model (2.2) in the backward direction given by the
Akaikeâ€™s Information Criterion (AIC). It turns out that we should merge the regions of
íš‚íš‘íšŠíš—íšíš‘íšŠíš’ and í™¾íšíš‘íšíš›íšœ with the reference region of íš‰íš‘íšíš“íš’íšŠíš—íšâ€Š, i.e.,Â  only Hebei region is
significantly different. Moreover, we merge age groups íš¢íš˜íšíš—íš_íŸ·â€Š, íš¢íš˜íšíš—íš_íŸ¸ and íš–íšŠíšíšíš›íšíš
with the reference age group of íš–íš’íšíšíš•íš_íŸ·â€Š. The final Poisson GLM is as follows, under
modified covariate space X âŠ‚â„5,
xi â†¦log íœ†(xi) = í›½0 + í›¼íš›íšíšíš’íš˜íš—i + í›¾íšíšíš—íšíšíš›i + í›¿íšŠíšíš_íšíš›íš˜íšíš™i + í›½1íšŒíšŠíš›i + í›½2íšŠíšŸíš_íš‘íš˜íšíš›íšœi.
íœƒâ†¦L(íœƒ;D1 âˆªD2) =
eiíœ†(xi) âˆ’Yi âˆ’Yi log (eiíœ†(xi)) + Yi log Yi,
xi â†¦log íœ†(xi) = log íœ†(xi;íœƒ) = í›½0 + í›¼í™·íšíš‹íšíš’íŸ™í™·íšíš‹íšíš’(íš›íšíšíš’íš˜íš—i) + í›¾íšíšíš–íšŠíš•íšíŸ™íšíšíš–íšŠíš•íš(íšíšíš—íšíšíš›i)+
í›¿íš–íš’íšíšíš•íš_íŸ¸íŸ™íš–íš’íšíšíš•íš_íŸ¸(íšŠíšíš_íšíš›íš˜íšíš™i) + í›½1íšŒíšŠíš›i + í›½2íšŠíšŸíš_íš‘íš˜íšíš›íšœi,
Tableâ€¯2â€‚ â€‰Stratified partition w.r.t.
empirical claim frequencies
Training data D1
Validation data D2
Test data D3
Machine Learning 111:243â€“272
with regression parameter íœƒ= (í›½0, í›¼í™·íšíš‹íšíš’, í›¾íšíšíš–íšŠíš•íš, í›¿íš–íš’íšíšíš•íš_íŸ¸, í›½1, í›½2)
âˆˆâ„6â€Š. Its maximum
likelihood estimator (MLE) Ì‚íœƒD1âˆªD2 on the learning data D1 âˆªD2 and the associated p-values are shown in TableÂ 3.
The car drivers in Hebei region have a significantly lower claim frequency than those
in Zhejiang, Shanghai and other regions. Female drivers have a slight higher claims frequency than male drivers, but the difference is not statistically significant on a 5% level.
Drivers at ages have a lower claims frequency than those at other ages. For cars
older than 3 years a log-linear functional form is supported, and the claim frequency is
increasing log-linearly with the average driving hours per week.
2.2.2â€‚ Outâ€‘ofâ€‘sample test error
The predictive performance of the estimated claim frequency model (2.4) is evaluated by
the out-of-sample Poisson deviance loss on the test data D3 (called test error):
where Ì‚íœƒD1âˆªD2 denotes the MLE based on learning set D1 âˆªD2â€Š. Preference is given to the
model with the smaller test error. The test error of model (2.4) is 1.1230 and the in-sample
learning error received through (2.3) is 1.0205. For comparison, we establish the homogeneous model
The test error of the homogeneous model (without any covariates and systematic effects) is
1.1703 and the learning error is 1.0717, thus, clearly bigger than in model (2.4). The latter
model is our benchmark for all subsequent derivations.
3â€‚ Claims frequency modeling using telematics data
We establish a first predictive model that is based on telematics car driving data, only.
The portfolio used is identical to the one of Sect.2, we also refer to TableÂ 2. Each driver
i = 1, â€¦ , n is described by a so-called speed-acceleration (v-a) heatmap Ziâ€Š, and its explicit
L(Ì‚íœƒD1âˆªD2;D3) =
eiíœ†(xi;Ì‚íœƒD1âˆªD2) âˆ’Yi âˆ’Yi log
eiíœ†(xi;Ì‚íœƒD1âˆªD2)
+ Yi log Yi,
ğœ†(â‹…) â‰¡Ì„ğœ†D1âˆªD2 =
iâˆˆD1âˆªD2 Yi
iâˆˆD1âˆªD2 ei
Tableâ€¯3â€‚ â€‰MLEÌ‚íœƒD1âˆªD2 for íœƒâˆˆâ„6
of model (2.4)
Parameters
Standard error
< 2 Ã— 10âˆ’16
Machine Learning 111:243â€“272
construction from telematics car driving data is described in detail in Sect.3.2. The general
Poisson regression setting is given as follows
where ei > 0 is the exposure of car driver i, ğœ†i > 0 is a given prior estimate of claim frequency for car driver i, and Zi â†¦ğœŒ(Zi) > 0 is a telematics driving behavior risk factor.
In Sect.Â 3.2, we construct the v-a heatmap Ziâ€Š; in SectionÂ 3.3, we apply a densely connected feed-forward neural network to estimate the driving behavior risk factor íœŒ(Zi)â€Š; and
in Sect.Â 3.4, we challenge the densely connected feed-forward neural network by a convolutional neural network.
3.1â€‚ Telematics car driving data
We discuss our telematics data in this section. Our telematics data is collected from three
independent sensors: GPS signal, instrumental panel and three-axis accelerometer. The
GPS location is transmitted second by second, and from these GPS locations we can calculate average speed and acceleration every second. The instrumental panel provides the
speed of the car every second as shown to the driver, and from this we can also calculate the average acceleration every second. Finally, the accelerometer records acceleration
in all three directions. Unfortunately, the precision of these measurements may be poor
because these devices need to be recalibrated regularly, and from similar telematics projects it is known that this recalibration might cause some difficulties, for instance, it might
be influenced by the fact whether a car is parked at a steep road or in a flat plane during
recalibration. We also observe difficulties in our data with the accelerometer device, i.e.,
the GPS signal and the instrumental panel being in line, but the accelerometer showing
different measurements. For this analysis we have decided to rely on the information from
the instrumental panel because it sometimes happens that the GPS signal gets lost (or is
imprecise) when, for instance, driving through a tunnel.
3.2â€‚ Speedâ€‘acceleration heatmaps
We compress individual telematics car driving information into a so-called speed-acceleration (v-a) heatmap for each driver i. These v-a heatmaps describe how drivers accelerate
and brake at different speeds. As highlighted in Gao and WÃ¼thrich , telematics car
driving data easily results in big data of several TBs. This makes it necessary to compress
this data appropriately to make it useful for statistical modeling. This data compression is
performed as in Gao etÂ al. , basically, telematics data is aggregated in a suitable way.
In Gao etÂ al. we have studied the speed of convergence of such aggregations and we
have seen that it takes roughly three months of data until the aggregation has converged.
For this reason, all subsequent analysis will be based on the three months of driving experience from 01/05/2016 to 31/07/2016. This is the time period when we have a maximal
number of cars with telematics data. Remark that we did not find seasonality in the heatmap constructions, and even if there was, we judge all drivers on common ground because
we choose the identical time period.
Denote the v-a rectangle by R = (0, 80]km/hÃ—[âˆ’2, 2]m/s2â€Š. Speed is truncated within
(0,Â 80]km/h since we want to remove the idle phase and there are not sufficiently many
observations above 80km/h to receive stable heatmaps. Acceleration is capped within
âˆ¼Poisson(eiÌƒíœ†iíœŒ(Zi)),
for i = 1, â€¦ , n,
Machine Learning 111:243â€“272
[âˆ’2, 2]m/s2 since there are not sufficiently many observation outside of this interval. Note
that these accelerations and decelerations are moderate, Weidner etÂ al. and
Sun etÂ al. work with bigger values.
The acceleration interval [âˆ’2, 2]m/s2 is divided into 6 equally spaced sub-intervals
j = 1, â€¦ , 6â€Š, and the speed interval (0,Â 80]km/h is divided into 16 equally spaced sub-intervals k = 1, â€¦ , 16â€Š; see Fig.Â 3. For each speed sub-interval k = 1, â€¦ , 16â€Š, the acceleration
pattern of driver i is defined as the (probability) distribution of accelerations in that speed
sub-interval:
where ti,j,k is the total time spent in acceleration sub-interval j for given speed sub-interval k
of driver i. We have normalization âˆ‘6
j=1 zi,j,k = 1â€Š, thus, for every k we receive probabilities
of a categorical distribution.
The driving behavior of every car driver i is represented by a 6 Ã— 16 matrix, called v-a
We plot the v-a heatmaps of the two selected drivers 44 and 191 (belonging to the test data
D3â€Š) in Fig.Â 4. It shows that driver 191 accelerates and brakes much more intensely than
driver 44. In Sect.Â 4, we show that driver 191 has the largest driving behavior risk factor
and driver 44 has the smallest one in our test data D3â€Š, i.e., these are the two extreme cases
It is standard in image recognition that inputs to convolutional neural networks are threedimensional arrays consisting of heightÃ—widthÃ—channels. Therefore, we transform (by a
slight abuse of notation) the v-a heatmap to a three-dimensional array Zi âˆˆ 6Ã—16Ã—1.
j=1 ti,j,k
Zi = (zi,j,k)j=1âˆ¶6,k=1âˆ¶16 âˆˆ 6Ã—16.
Speed(km/h)
Acceleration(m/s^2)
Fig.â€¯3â€‚ â€‰Partition of R = 111:243â€“272
3.3â€‚ A densely connected neural network forÂ claim frequency prediction
The heatmap Zi has dimension of 6 Ã— 16 Ã— 1â€Š, and usually we need feature engineering of Zi
before using it in a claim frequency model, this is the approach taken in Gao etÂ al. .
Instead of manually feature engineering in a two-step modeling approach, we apply a
densely connected neural network to do both feature engineering and regression modeling
simultaneously.
3.3.1â€‚ Densely connected feedâ€‘forward neural network architecture
We design a densely connected feed-forward neural network with two hidden layers. The
Keras code is provided in â€œAppendix Aâ€, and the architecture is shown in Listing 1. We
choose the number of neurons in each of the two hidden layers as m1 = 30 and m2 = 10â€Š.
Each row of Listing 1 shows the layer name, the type of layer (in the bracket), the dimension of the layer, the number of parameters and the preceding layer name(s) it is connected
Fig.â€¯4â€‚ â€‰v-a heatmaps of drivers 44 and 191 belonging to the test data D3
Machine Learning 111:243â€“272
We have two input layers, one flatten layer, three dense layers, two dropout layers and
one multiply layer. More specifically, the following layers are connected sequentially to
form the densely connected feed-forward neural network denoted by dnn.
The flatten layer Each element of Zi is on unit scale , therefore, we input
Zi directly to the neural network without any pre-processing. We flatten the array
Zi to a m0 = 6 â‹…16 = 96 dimensional vector z1
i,j)j=1âˆ¶m0 through the flatten layer
heatmap_flat:
This layer discards the spatial structure. There is no parameter in this layer involved.
The first dense layer The m0-dimensional vector z1
i is projected to a m1-dimensional
space through the first dense hidden layer heatmap_dense1:
We use the hyperbolic tangent activation; other activation functions lead to similar results.
There are (m0 + 1)m1 parameters íœƒ2
l,j âˆˆâ„ in this layer.
The first dropout layer A dropout layer heatmap_drop1 with dropout rate d1 is
inserted between the first dense layer and the second dense layer. The dropout layer does
íœ™1 âˆ¶ 6Ã—16Ã—1 â†’ m0,
Zi â†¦íœ™1(Zi) = z1
i,1, â€¦ , z1
íœ™2 âˆ¶ m0 â†’(âˆ’1, 1)m1,
i,1, â€¦ , z2
i,j = tanh
j = 1, â€¦ , m1.
Machine Learning 111:243â€“272
not affect the neural network architecture, and it does not involve any model parameters.
The dropout layer only affects the calibration, in particular, it prevents from over-fitting
because in each gradient descent step neurons are removed independently and randomly
with the given dropout rate d1â€Š. We tune the dropout rate d1 = 0.1 according to the validation error; see model calibration in Sect.Â 3.3.2.
The second dense layer The m1-dimensional vector z2
i is projected to a m2-dimensional
space through the second dense layer heatmap_dense2:
There are (m1 + 1)m2 parameters íœƒ3
l,j âˆˆâ„ in this layer. We tune the number of neurons
The second dropout layer A dropout layer heatmap_drop2 with dropping rate d2 is
inserted between the second dense layer and the third dense layer. The explanation is similar to the first dropout layer. We tune the dropout rate d2 = 0.1.
The third dense layer The m2-dimensional vector z3
i is projected to a 1-dimensional
space through the third dense layer heatmap_factor:
We call z4
i the driving behavior risk factor of driver i. We use the exponential activation
function since the driving behavior risk factor must be positive, see (3.1), and because the
log-link is the canonical link in the Poisson GLM. There are m2 + 1 parameters íœƒ4
this layer. Altogether, the above 6 layers define a mapping (composition) from the heatmap
to the driving behavior risk factor in (3.1):
The multiply layer Finally, we multiply z4
i with exposure ei > 0 and prior claim frequency
estimate Ìƒíœ†i to get the output of the neural network (layer response) eiÌƒíœ†iz4
i = eiÌƒíœ†iíœŒdnn(Zi).
Note that the term ei Ìƒğœ†i goes into the neural network through the input layer vol, and it acts
as an offset in the regression model.
3.3.2â€‚ Densely connected feedâ€‘forward neural network model calibration
We choose the average claim frequency from the homogeneous model (2.6) as prior estimates, i.e., ğœ†i = Ì„ğœ†D1âˆªD2â€Š. Denote the vector of all neural network parameters by íœƒâ€Š. We apply
the adam version of the gradient decent method to iteratively find a good parameter estimate for íœƒâ€Š. As objective function we choose the Poisson deviance loss having in-sample
training error on D1:
íœ™3 âˆ¶(âˆ’1, 1)m1 â†’(âˆ’1, 1)m2,
i,1, â€¦ , z3
i,j = tanh
j = 1, â€¦ , m2.
íœ™4 âˆ¶(âˆ’1, 1)m2 â†’â„+,
íœŒdnn âˆ¶ 6Ã—16Ã—1 â†’â„+,
Zi â†¦íœŒdnn(Zi) = (íœ™4â—¦íœ™3â—¦íœ™2â—¦íœ™1)(Zi) = z4
eiÌƒíœ†iíœŒdnn(Zi;íœƒ) âˆ’Yi âˆ’Yi log
eiÌƒíœ†iíœŒdnn(Zi;íœƒ)
+ Yi log Yi.
Machine Learning 111:243â€“272
We emphasize that our training data set D1 is small. Therefore, we perform steepest gradient descent on D1â€Š, and not any stochastic gradient descent method.
FigureÂ 5 shows the steepest gradient descent performance íœƒâ†¦L(íœƒ;D1) on the training data D1 (red color) and the corresponding out-of-sample validation losses íœƒâ†¦L(íœƒ;D2)
on the validation data D2 .1 The model starts to over-fit after roughly 80 iterations (note
that the training error is not monotone decreasing due to the presence of dropouts). Using
the callback_early_stopping option we retrieve the parameters with the lowest validation
loss after not improving in a sequence of 5 iterations. It takes 4 seconds to complete calibration on a 1.3 GHz dual Intel Core i5 computer. Note that the initial values of the parameters in the third dense layer heatmap_factor are set to 0; see Keras code in â€œAppendix
Aâ€. Hence, gradient descent calibration of the neural network starts from the homogeneous model ei Ì„ğœ†D1âˆªD2â€Š. The hyper-parameters m1, m2, d1, d2 are selected as 30,Â 10,Â 0.1,Â 0.1 by
monitoring the validation error.
TableÂ 4 provides the results in column â€˜dnnâ€™. The densely connected feed-forward neural
network with driving behavior risk factor íœŒdnn(Zi) outperforms the GLM out-of-sample and
we observe a test error decrease from 1.1230 to 1.1035. Thus, for this data partition the v-a
heatmap Zi has better predictive power than the 5 classical risk factors xi (in our GLM).
Fig.â€¯5â€‚ â€‰Steepest gradient descent calibration of the densely connected neural network (dnn)
Tableâ€¯4â€‚ â€‰Learning error and test error of the models studied
Homogeneous
Learning error
Test error
Reduction in test error
1â€‚ Note that internally Keras drops all (constant) terms that are not relevant for the gradient descent algorithm. For this reason, the y-axis of Fig.Â 5 does not match Poisson deviance losses because the constant
Yi log Yi âˆ’Yi
and correspondingly for D2 are missing, and the scaling factor of 2 is not
considered.
Machine Learning 111:243â€“272
3.4â€‚ Convolutional neural network forÂ claim frequency prediction
It is natural to replace the densely connected feed-forward neural network to process the
v-a heatmap Zi by a convolutional neural network. In the former approach we apply a flatten layer íœ™1 in the first step, this implies that we lose the topological structure of the v-a
heatmap. On the other hand, convolutional neural networks are designed to handle spatial
3.4.1â€‚ The architecture
We design a convolutional neural network with two convolution layers. The first convolution layer has a 6 Ã— 1 convolution window with q filters and stride 1. The second convolution layer has a 1 Ã— 2 convolution window with 1 filter and stride 2. These two convolution
layers are motivated by fact that they allow for interpretation of our results, see the following description of these two convolution layers and their interpretation in Sect.Â 4.2. The
Keras code is provided in â€œAppendix Aâ€; the architecture is shown in Listing 2, where we
choose q = 2.
An obvious difference between Listings 1 and 2 is the number of parameters. The convolutional neural network has 28 parameters while the densely connected feed-forward
neural network has 3,Â 231 parameters. This is because the convolution windows are small
and their parameters are shared in different locations of the heatmap. For this reason, we do
not use dropout layers here. We have two input layers, two convolution layers, one flatten
layer, one dense layer and one multiply layer. More specifically, the following layers are
connected sequentially to form the convolutional neural network.
The first convolution layer We let a 6 Ã— 1 convolution window moving along the speed
direction of the v-a heatmap with stride 1 to extract q acceleration patterns in each speed
Machine Learning 111:243â€“272
sub-interval (5(k âˆ’1), 5k] km/h for k = 1, â€¦ , 16â€Š; see Fig.Â 3. The layer heatmap_conv1
defines the following mapping:
We call z1
i,j,k,l as the l-th acceleration pattern in the speed sub-interval (5(k âˆ’1), 5k] km/h
for driver i, extracted by the first convolution layer. There are 7q parameters íœƒ1
this layer. Note that we use the same symbols for activations z and parameters íœƒ as in the
densely connected feed-forward neural network. When necessary, we will clarify to avoid
confusion.
The second convolution layer A 1 Ã— 2 convolution window is moving along the horizontal direction of the v-a heatmap with stride 2 to extract one acceleration pattern in the
speed sub-interval (10(k âˆ’1), 10k] km/h for k = 1, â€¦ , 8â€Š; see Fig.Â 3. The layer heatmap_
conv2 defines the following mapping:
We call z2
i,j,k,l as the acceleration pattern in the speed sub-interval (10(k âˆ’1), 10k] km/h for
driver i, extracted by the second convolution layer. There are 2(q + 1) parameters íœƒ2
in this layer. One may increase the filter numbers of this layer and add more convolution
layers behind this layer, but they always provide a similar predictive performance.
The flatten layer We flatten the array Z2 into a 8-dimensional vector through the layer
heatmap_flat:
There is no parameter in the flatten layer. The purpose of this layer is to transform the array
into a vector so it can be used as the input of the dense layer followed.
The dense layer The 8-dimensional vector z3 is projected to a 1-dimensional space
through the dense layer heatmap_factor:
íœ“1 âˆ¶ 6Ã—16Ã—1 â†’(âˆ’1, 1)1Ã—16Ã—q,
Zi â†¦íœ“1(Zi) = Z1
i,j,k,l)j=1,k=1âˆ¶16,l=1âˆ¶q,
i,j,k,l = tanh
t,lzi,t,k,1
j = 1, k = 1, â€¦ , 16, l = 1, â€¦ , q.
íœ“2 âˆ¶(âˆ’1, 1)1Ã—16Ã—q â†’(âˆ’1, 1)1Ã—8Ã—1,
i,j,k,l)j=1,k=1âˆ¶8,l=1,
i,j,k,l = tanh
i,j,2kâˆ’1,s + íœƒ2
j = 1, k = 1, â€¦ , 8, l = 1.
íœ“3 âˆ¶(âˆ’1, 1)1Ã—8Ã—1 â†’(âˆ’1, 1)8,
i,1, â€¦ , z3
i,j,k,l = z2
k = 1, â€¦ , 8.
íœ“4 âˆ¶(âˆ’1, 1)8 â†’â„+,
Machine Learning 111:243â€“272
We call z4
i the driving behavior risk factor of driver i. There are 9 parameters íœƒ4
layer. For the same reasons as above, we use the exponential activation function. Altogether, the above 4 layers define the mapping from the heatmaps to the driving behavior
risk factor in (3.1):
The multiply layer Finally we multiply z4
i with the exposure ei > 0 and the prior
claims frequency estimate Ìƒíœ†i to get the output of the neural network (layer response)
i = eiÌƒíœ†iíœŒcnn(Zi).
3.4.2â€‚ Convolutional neural network model calibration
The calibration of the convolutional neural network is similar to the calibration of the
densely connected feed-forward neural network in Sect.Â 3.3.2. Again, we choose the average claims frequency of the homogeneous model (2.6) as the prior estimates. The initial
values of the parameters in the dense layer are set to 0; see keras code in â€œAppendix Aâ€.
So we start calibrating from the homogeneous model ei Ì„ğœ†D1âˆªD2â€Š. The algorithm converges
fast as shown in Fig.Â 6, it only takes 12 seconds. The hyper-parameter q is selected as 2 by
monitoring the validation error.
The results are given in TableÂ 4. Using the convolutional neural network with driving
behavior risk factors, we improve the test error of the GLM (1.1075 versus 1.1230), but the
out-of-sample performance is slightly worse compared to the densely connected network.
However, we have a slight preference for the convolutional network approach because it
keeps the number of parameters involved on a small scale.
4â€‚ Boosting classical risk factors withÂ telematics data
Equation (3.1) introduces a way of incorporating classical risk factors into neural networks.
We choose the estimated claims frequency from GLM (2.4) as prior claim frequency
estimates, i.e., Ìƒíœ†i = íœ†(xi;Ì‚íœƒ) = Ì‚íœ†(xi)â€Š. This is in the spirit of the combined actuarial neural
network (CANN) approach proposed in WÃ¼thrich and Merz , which in our context
boosts the GLM frequencies Ì‚íœ†(xi) with telematics driving risk factors íœŒ(Zi).
íœŒcnn âˆ¶ 6Ã—16Ã—1 â†’â„+,
Zi â†¦íœŒcnn(Zi) =
íœ“4â—¦íœ“3â—¦íœ“2â—¦íœ“1)
Fig.â€¯6â€‚ â€‰Steepest gradient descent calibration of the convolutional neural network (cnn)
Machine Learning 111:243â€“272
4.1â€‚ Telematics neural network boosted Poisson generalized linear model
CANN boosts the Poisson GLM (2.4) with either the densely connected neural network of
Listing 1 (dnn) or the convolutional neural network of Listing 2 (cnn): For i = 1, â€¦ , n we
where eiÌ‚íœ†(xi) is the GLM estimated expected number of claims of driver i with classical
risk factors xiâ€Š, see (2.4). The architectures of the networks in (4.1) and (4.2) are chosen
identical to the ones in Sects.3.3.2 and 3.4.2. Also the network calibration goes along the
same lines as above, the gradient descent results are shown in Figs.Â 7 and 8.
Note that both gradient descent calibrations start from the Poisson GLM (2.4) by the
way we initialize the algorithm: the starting points in Figs.Â 7 and 8 are below those in
Figs.Â 5 and 6 because we already start from a reasonably good GLM, and the convergence
rates in Figs.Â 7 and 8 are very fast. Note that we keep the GLM parameters Ì‚ğœƒ of the classical covariates during the neural network calibration for two reasons: firstly, this keeps the
âˆ¼Poisson(eiÌ‚íœ†(xi)íœŒdnn(Zi)),
âˆ¼Poisson(eiÌ‚íœ†(xi)íœŒcnn(Zi)),
Fig.â€¯7â€‚ â€‰Calibration of densely connected neural network boosted Poisson GLM (dnn + glm)
Fig.â€¯8â€‚ â€‰Calibration of convolutional neural network boosted Poisson GLM (cnn + glm)
Machine Learning 111:243â€“272
interpretation of the GLM prediction and gives a stable neural network calibration; secondly, the interpretation of íœŒ is intuitively obtained as a modification of the GLM prediction Ì‚ğœ†â€Š. In this sense, we boost the GLM by integrating the GLM prediction as an offset into
the networks. If more data would be available one could think of also further training Ì‚ğœƒâ€Š,
this would allow for more general interactions between the classical covariates and telematics information.
4.1.1â€‚ Comparison ofÂ driving behavior risk factors
We start by comparing the driving behavior risk factors íœŒdnn(Zi) and íœŒcnn(Zi) on the test
data i âˆˆD3 in Fig.Â 9.
From the graphs in Fig.Â 9 we conclude that both networks provide almost identical driving risk factors íœŒdnn(Zi) â‰ˆíœŒcnn(Zi) in (0.4,Â 1.6). In fact, the more complex densely connected feed-forward neural network can be replaced by a simpler convolutional neural network having only 28 parameters. In Fig.Â 4, we show the v-a heatmaps with the minimal
and maximal driving behavior risk factors, i.e., high acceleration and braking obviously
triggers a higher frequency in our example.
4.1.2â€‚ Comparison ofÂ different models
TableÂ 4 and Fig.Â 10 (left) compare the different models. The v-a heatmap boosted Poisson
GLMs (dnn + glm and cnn + glm) have the best out-of-sample predictive performance.
Thus, we conclude that v-a heatmaps Zi contain information beyond classical actuarial
covariates xiâ€Š, and on the other hand these v-a heatmaps Zi do not fully replace classical
actuarial covariates xiâ€Š. The former statement is clear because we believe that v-a heatmaps
Zi best describe driving styles. However, also the latter makes sense because v-a heatmaps
Zi will interact with road conditions, car type, etc., which may be reflected in region,
Fig.â€¯9â€‚ â€‰Comparison of driving
behavior risk factors íœŒdnn(Zi) and
íœŒcnn(Zi) on test data i âˆˆD3
Machine Learning 111:243â€“272
car and other classical actuarial covariates. That is, classical actuarial covariates may
indicate under which circumstances the telematics data has been collected.
4.2â€‚ Interpretation ofÂ theÂ convolutional neural network results
We explain how the acceleration pattern z3
i,1,k,1 in the speed sub-interval
(10(k âˆ’1), 10k] km/h is constructed. In â€œAppendix Bâ€, we approximate z3
i,k by a linear combination of z0
i,j,2kâˆ’1,1 and z0
i,j,2k,1â€Š, namely,
where íœ”j = íœƒ2
j,2 is interpreted as the weight of each acceleration sub-interval (2 âˆ’2jâˆ•3, 2 âˆ’2(j âˆ’1)âˆ•3] m/s2â€Š, j = 1, â€¦ , 6â€Š. We plot these weights íœ”j
in Fig.Â 10 (middle).
The signs of the weights íœ”j for hard accelerating and hard braking are opposite to
those for smooth driving. We interpret z3
i,k as the relative frequency of smooth driving in
the speed sub-interval (10(k âˆ’1), 10k] km/h for k = 1, â€¦ , 8â€Š. The absolute values of the
weights for hard braking are larger than those for hard accelerating. It seems that hard
braking plays a more important role than hard accelerating in the acceleration pattern z3
We draw the pair plots of z3
i,k on the test data i âˆˆD3 in Fig.Â 11. It shows that acceleration patterns z3
i,k in neighboring speed intervals are quite similar, with high correlations of
around 0.9. Acceleration patterns z3
i,k in (10,Â 40] km/h tend be smaller than those at other
speeds, indicating that drivers tend to hard accelerate and brake in (10,Â 40] km/h more
often than in other speed intervals (which, of course, makes perfect sense).
Acceleration patterns z3
i,k in different speed intervals are combined to obtain the driving
behavior risk factor through the (last) dense layer, see (3.14). We interpret the parameters
k)k=1âˆ¶8 in this last dense layer as the weights for each speed sub-interval (10(k âˆ’1), 10k]
km/h for k = 1, â€¦ , 8â€Š. We plot (íœƒ4
k)k=1âˆ¶8 in Fig.Â 10 (right). The absolute values of íœƒ4
decreasing with speeds. It seems that the acceleration pattern z3
i,k in low speeds plays a
more important role in constructing the (overall) driving behavior risk factor than at high
speeds. Of course, also this makes sense because frequent claim counts often happen at low
speeds, say, in urban area.
zi,j,2kâˆ’1,1 + zi,j,2k,1
for k = 1, â€¦ , 8,
Fig.â€¯10â€‚ â€‰(left) Reduction in test error of all models studied (compared to the homogeneous model); (middle)
weights íœ”j for each acceleration sub-interval; (right) weights íœƒ4
k for each speed sub-interval
Machine Learning 111:243â€“272
4.3â€‚ Sensitivity test
We have a comparably small portfolio of n = 973 car drivers. It is necessary to conduct a
sensitivity test to see whether the above conclusions are still valid for different trainingvalidation-test partitions. We further partition the training set D1 into 3 disjoint data sets
of approximately the same size D1,1, D1,2, D1,3â€Š, and design four training-validation-test
partitions as shown in TableÂ 5. Note that partition 0 is the data split used in the previous
For each training-validation-test partition, we fit all six models of TableÂ 4. For each
model, we calculate the reduction in test error compared to the homogeneous model. For
the convolutional neural network boosted GLM (4.2), we calculate the weights for each
acceleration sub-interval and for each speed sub-interval. All the results are shown in
Fig.Â 12, and they should be compared to Fig.Â 10.
Similar reduction in test errors over all partitions of TableÂ 5 for both neural networks
reconfirm that both architectures have similar predictive power. Moreover, in general, it is
beneficial to combine telematics information with classical risk factors because potential
interaction may lead to better predictive models. However, the previous statement that v-a
heatmaps have better predictive power than the 5 classical risk factors (in a GLM) is not
confirmed by partitions 2 and 4. For partitions 1 and 4 both the signs of íœ” and íœƒ4 switch,
leaving the sign of the driving behavior risk factor z4
i unchanged. The patterns in the middle column of Fig.Â 12 are similar to those in Fig.Â 10, indicating that the hard braking plays
a more important role than the hard acceleration. The previous statement of the importance
of low speeds are supported by partitions 1 and 2, while partitions 3 and 4 do not violate
this statement.
Finally, we perform another data split with a different seed leading to another five training-validation-test partitions. The results for the five partitions are presented in Fig.Â 13. We
conclude with the same findings as those from Fig.Â 12.
5â€‚ Conclusions
We propose two neural networks, a densely connected feed-forward neural network and
a convolutional neural network, for extracting driver risk information from telematics car
driving data represented by v-a heatmaps. The neural networks simultaneously perform
feature engineering and regression modeling. Both neural network approaches have a similar predictive performance in our example, however, the convolutional one uses much less
parameters. Unlike the black box of the densely connected feed-forward neural network,
the convolutional neural network can be interpreted and we explain how the driving behavior risk factor is constructed by the convolutional neural network.
Tableâ€¯5â€‚ â€‰Training-validation-test
partitions; partition 0 is the split
used in the previous sections
Partitions
Training set (60%)
Validation
Test set (20%)
D1 = D1,1 âˆªD1,2 âˆªD1,3
D1 = D1,1 âˆªD1,2 âˆªD1,3
D1,1 âˆªD2 âˆªD3
D1,2 âˆªD2 âˆªD3
D1,3 âˆªD2 âˆªD3
Machine Learning 111:243â€“272
Specific locations in v-a heatmaps have their meanings, and we need to make sure that
the convolution window has a right size and moves in a sensible way to capture these
meanings. Our design of the convolutional neural network targets at detecting similar
acceleration patterns in different speed intervals.
As byproducts of our empirical data analysis, we conclude that both the classical risk
factors and driving behavior risk factor are needed for claims frequency modeling, and hard
braking in low speeds contributes the most to the driving behavior risk factor. Thus, telematics data contains driving style information beyond classical risk factors that is relevant
for claim frequency prediction, and on the other hand, classical risk factors as, for instance,
regional information may explain certain driving patterns. Letting these two ingredients
interact will lead to better predictive models.
Appendix A: Keras code
Machine Learning 111:243â€“272
Appendix B: Approximation ofÂ theÂ acceleration pattern z3
Following equations (3.10) and (3.12), and approximating the hyperbolic tangent function
by its Taylorâ€™s expansion of order 1, tanh(x) â‰ˆxâ€Š, we have the following approximation:
Machine Learning 111:243â€“272
where the last approximation follows from zi,j,2kâˆ’1,1 â‰ˆzi,j,2k,1 â‰ˆ(zi,j,2kâˆ’1,1 + zi,j,2k,1)âˆ•2â€Š.
This is because the acceleration patterns in the two neighboring speed intervals should be
Appendix C: Figures
See Figs.Â 11, 12 and 13.
i,1,2kâˆ’1,l + íœƒ2
i,1,2kâˆ’1,1 + íœƒ2
i,1,2kâˆ’1,2 + íœƒ2
i,1,2k,1 + íœƒ2
j,1zi,j,2kâˆ’1,1
j,2zi,j,2kâˆ’1,1
j,1zi,j,2k,1
j,2zi,j,2k,1
j,2)zi,j,2kâˆ’1,1 +
j,2)zi,j,2k,1
zi,j,2kâˆ’1,1 + zi,j,2k,1
zi,j,2kâˆ’1,1 + zi,j,2k,1
Machine Learning 111:243â€“272
Fig.â€¯11â€‚ â€‰Pair plots of acceleration patterns z3
i,k in each speed sub-interval (10(k âˆ’1), 10k] km/h on the test
data i âˆˆD3
Machine Learning 111:243â€“272
Fig.â€¯12â€‚ â€‰Each row is for a data partition as shown in TableÂ 5: (left) reduction in test error for glm (2.4), dnn
(Listing 1), cnn (Listing 2), dnn + glm (4.1) and cnn + glm (4.2) compared to the homogeneous model
(2.6); (middle) weights íœ”j for each acceleration sub-interval; (right) weights íœƒ4
k for each speed sub-interval
Machine Learning 111:243â€“272
Fig.â€¯13â€‚ â€‰Another data split using a different seed: (left) reduction in test error for glm (2.4), dnn (Listing 1),
cnn (Listing 2), dnn + glm (4.1) and cnn + glm (4.2) compared to the homogeneous model (2.6); (middle)
weights íœ”j for each acceleration sub-interval; (right) weights íœƒ4
k for each speed sub-interval
Machine Learning 111:243â€“272
Acknowledgementsâ€‚ Guangyuan Gao gratefully acknowledges financial support from the National Natural
Science Foundation of China (71901207).
Fundingâ€‚ Open access funding provided by Swiss Federal Institute of Technology Zurich.
Open Accessâ€‚ This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article
are included in the articleâ€™s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the articleâ€™s Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit