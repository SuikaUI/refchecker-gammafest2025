Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443–4458
July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics
: A Benchmark to Evaluate Rationalized NLP Models
Jay DeYoung⋆Ψ, Sarthak Jain⋆Ψ, Nazneen Fatema Rajani⋆Φ, Eric LehmanΨ,
Caiming XiongΦ, Richard SocherΦ, and Byron C. WallaceΨ
⋆Equal contribution.
ΨKhoury College of Computer Sciences, Northeastern University
ΦSalesforce Research, Palo Alto, CA, 94301
State-of-the-art models in NLP are now predominantly based on deep neural networks
that are opaque in terms of how they come
to make predictions.
This limitation has
increased interest in designing more interpretable deep models for NLP that reveal the
‘reasoning’ behind model outputs. But work
in this direction has been conducted on different datasets and tasks with correspondingly
unique aims and metrics; this makes it difﬁcult
to track progress. We propose the Evaluating
Rationales And Simple English Reasoning
) benchmark to advance research
on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for
which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how
well the rationales provided by models align
with human rationales, and also how faithful
these rationales are (i.e., the degree to which
provided rationales inﬂuenced the corresponding predictions). Our hope is that releasing this
benchmark facilitates progress on designing
more interpretable NLP systems. The benchmark, code, and documentation are available
at 
Introduction
Interest has recently grown in designing NLP systems that can reveal why models make speciﬁc
predictions. But work in this direction has been
conducted on different datasets and using different
metrics to quantify performance; this has made it
difﬁcult to compare methods and track progress.
We aim to address this issue by releasing a standardized benchmark of datasets — repurposed and
augmented from pre-existing corpora, spanning a
range of NLP tasks — and associated metrics for
measuring different properties of rationales. We refer to this as the Evaluating Rationales And Simple
English Reasoning (ERASER
) benchmark.
Commonsense Explanations (CoS-E)
Where do you ﬁnd the most amount of leafs?
(a) Compost pile (b) Flowers (c) Forest (d) Field (e) Ground
Movie Reviews
In this movie, … Plots to take over the world. The acting is
great! The soundtrack is run-of-the-mill, but the action more
than makes up for it
(a) Positive (b) Negative
Evidence Inference
Article Patients for this trial were recruited … Compared with
0.9% saline, 120 mg of inhaled nebulized furosemide had no
eﬀect on breathlessness during exercise.
(a) Sig. decreased (b) No sig. diﬀerence (c) Sig. increased
Prompt With respect to breathlessness, what is the reported
diﬀerence between patients receiving placebo and those
receiving furosemide?
H A man in an orange vest leans over a pickup truck
P A man is touching a truck
(a) Entailment (b) Contradiction (c) Neutral
Figure 1: Examples of instances, labels, and rationales
illustrative of four (out of seven) datasets included in
ERASER. The ‘erased’ snippets are rationales.
In curating and releasing ERASER we take inspiration from the stickiness of the GLUE and SuperGLUE 
benchmarks for evaluating progress in natural language understanding tasks, which have driven rapid
progress on models for general language representation learning. We believe the still somewhat
nascent subﬁeld of interpretable NLP stands to beneﬁt similarly from an analogous collection of standardized datasets and tasks; we hope these will
aid the design of standardized metrics to measure
different properties of ‘interpretability’, and we
propose a set of such metrics as a starting point.
Interpretability is a broad topic with many possible realizations . In ERASER we focus speciﬁcally on
rationales, i.e., snippets that support outputs. All
datasets in ERASER include such rationales, explicitly marked by human annotators. By deﬁnition,
rationales should be sufﬁcient to make predictions,
but they may not be comprehensive. Therefore, for
some datasets, we have also collected comprehensive rationales (in which all evidence supporting
an output has been marked) on test instances.
The ‘quality’ of extracted rationales will depend
on their intended use. Therefore, we propose an
initial set of metrics to evaluate rationales that
are meant to measure different varieties of ‘interpretability’. Broadly, this includes measures of
agreement with human-provided rationales, and assessments of faithfulness. The latter aim to capture
the extent to which rationales provided by a model
in fact informed its predictions. We believe these
provide a reasonable start, but view the problem of
designing metrics for evaluating rationales — especially for measuring faithfulness — as a topic for
further research that ERASER can facilitate. And
while we will provide a ‘leaderboard’, this is better
viewed as a ‘results board’; we do not privilege
any one metric. Instead, ERASER permits comparison between models that provide rationales with
respect to different criteria of interest.
We implement baseline models and report their
performance across the corpora in ERASER. We
ﬁnd that no single ‘off-the-shelf’ architecture is
readily adaptable to datasets with very different
instance lengths and associated rationale snippets
(Section 3). This highlights a need for new models
that can consume potentially lengthy inputs and
adaptively provide rationales at a task-appropriate
level of granularity. ERASER provides a resource
to develop such models.
In sum, we introduce the ERASER benchmark
(www.eraserbenchmark.com), a uniﬁed set of diverse NLP datasets (these are repurposed and augmented from existing corpora,1 including sentiment analysis, Natural Language Inference, and
QA tasks, among others) in a standardized format featuring human rationales for decisions, along
with starter code and tools, baseline models, and
standardized (initial) metrics for rationales.
Related Work
Interpretability in NLP is a large, fast-growing
area; we do not attempt to provide a comprehensive
overview here. Instead we focus on directions particularly relevant to ERASER, i.e., prior work on
models that provide rationales for their predictions.
Learning to explain. In ERASER we assume that
1We ask users of the benchmark to cite all original papers,
and provide a BibTeX entry for doing so on the website.
rationales (marked by humans) are provided during
training. However, such direct supervision will not
always be available, motivating work on methods
that can explain (or “rationalize”) model predictions using only instance-level supervision.
In the context of modern neural models for text
classiﬁcation, one might use variants of attention
 to extract rationales. Attention mechanisms learn to assign soft weights to
(usually contextualized) token representations, and
so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions
 . This likely
owes to encoders entangling inputs, complicating
the interpretation of attention weights on inputs
over contextualized representations of the same.2
By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the
classiﬁer, by construction providing faithful explanations. Recent work has proposed hard attention
mechanisms as a means of providing explanations.
Lei et al. proposed instantiating two models
with their own parameters; one to extract rationales,
and one that consumes these to make a prediction.
They trained these models jointly via REINFORCE
 style optimization.
Recently, Jain et al. proposed a variant
of this two-model setup that uses heuristic feature
scores to derive pseudo-labels on tokens comprising rationales; one model can then be used to perform hard extraction in this way, while a second
(independent) model can make predictions on the
basis of these. Elsewhere, Chang et al. 
introduced the notion of classwise rationales that
explains support for different output classes using
a game theoretic framework. Finally, other recent
work has proposed using a differentiable binary
mask over inputs, which also avoids recourse to
REINFORCE .
Post-hoc explanation. Another strand of interpretability work considers post-hoc explanation
methods, which seek to explain why a model made
a speciﬁc prediction for a given input. Commonly
2Interestingly, Zhong et al. ﬁnd that attention sometimes provides plausible but not faithful rationales. Elsewhere,
Pruthi et al. show that one can easily learn to deceive
via attention weights. These ﬁndings highlight that one should
be mindful of the criteria one wants rationales to fulﬁll.
these take the form of token-level importance
scores. Gradient-based explanations are a standard
example . These enjoy a clear semantics (describing
how perturbing inputs locally affects outputs), but
may nonetheless exhibit counterintuitive behaviors
 .
Gradients of course assume model differentiability. Other methods do not require any model
properties. Examples include LIME and Alvarez-Melis and Jaakkola ;
these methods approximate model behavior locally by having it repeatedly make predictions over
perturbed inputs and ﬁtting a simple, explainable
model over the outputs.
Acquiring rationales. Aside from interpretability
considerations, collecting rationales from annotators may afford greater efﬁciency in terms of model
performance realized given a ﬁxed amount of annotator effort . In particular,
recent work by McDonnell et al. has
observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort.
Combining rationale annotation with active learning is another promising direction
 .
Learning from rationales. Work on learning from
rationales marked by annotators for text classiﬁcation dates back over a decade .
Earlier efforts proposed extending standard discriminative models like Support Vector Machines
(SVMs) with regularization terms that penalized
parameter estimates which disagreed with provided
rationales .
Other efforts have attempted to specify generative
models of rationales .
More recent work has aimed to exploit rationales in training neural text classiﬁers. Zhang et al.
 proposed a rationale-augmented Convolutional Neural Network (CNN) for text classiﬁcation, explicitly trained to identify sentences supporting categorizations. Strout et al. showed that
providing this model with rationales during training yields predicted rationales that are preferred
by humans (compared to rationales produced without explicit supervision). Other work has proposed
‘pipeline’ approaches in which independent models are trained to perform rationale extraction and
classiﬁcation on the basis of these, respectively
 , assuming
Size (train/dev/test)
Evidence Inference
7958 / 972 / 959
6363 / 1491 / 2817
Movie Reviews
1600 / 200 / 200
97957 / 6122 / 6111
24029 / 3214 / 4848
8733 / 1092 / 1092
911938 / 16449 / 16429
Table 1: Overview of datasets in the ERASER benchmark. Tokens is the average number of tokens in each
document. Comprehensive rationales mean that all supporting evidence is marked; !denotes cases where this
is (more or less) true by default; ◇, ◆are datasets for
which we have collected comprehensive rationales for
either a subset or all of the test datasets, respectively.
Additional information can be found in Appendix A.
explicit training data is available for the former.
Rajani et al. ﬁne-tuned a Transformerbased language model on
free-text rationales provided by humans, with an
objective of generating open-ended explanations to
improve performance on downstream tasks.
Evaluating rationales. Work on evaluating rationales has often compared these to human judgments , or elicited other human evaluations of explanations . There has also been work on
visual evaluations of saliency maps .
Measuring agreement between extracted and
human rationales (or collecting subjective assessments of them) assesses the plausibility of rationales, but such approaches do not establish whether
the model actually relied on these particular rationales to make a prediction. We refer to rationales
that correspond to the inputs most relied upon to
come to a disposition as faithful.
Most automatic evaluations of faithfulness measure the impact of perturbing or erasing words or
tokens identiﬁed as important on model output . We build upon these methods in Section
4. Finally, we note that a recent article urges the
community to evaluate faithfulness on a continuous
scale of acceptability, rather than viewing this as a
binary proposition .
Datasets in ERASER
For all datasets in ERASER we distribute both reference labels and rationales marked by humans
as supporting these in a standardized format. We
delineate train, validation, and test splits for all
corpora (see Appendix A for processing details).
We ensure that these splits comprise disjoint sets
of source documents to avoid contamination.3 We
have made the decision to distribute the test sets
publicly,4 in part because we do not view the ‘correct’ metrics to use as settled. We plan to acquire
additional human annotations on held-out portions
of some of the included corpora so as to offer hidden test set evaluation opportunities in the future.
Evidence inference .
dataset of full-text articles describing randomized
controlled trials (RCTs).
The task is to infer
whether a given intervention is reported to either
signiﬁcantly increase, signiﬁcantly decrease, or
have no signiﬁcant effect on a speciﬁed outcome, as
compared to a comparator of interest. Rationales
have been marked as supporting these inferences.
As the original annotations are not necessarily exhaustive, we collected exhaustive rationale annotations on a subset of the validation and test data.5
BoolQ . This corpus consists
of passages selected from Wikipedia, and yes/no
questions generated from these passages. As the
original Wikipedia article versions used were not
maintained, we have made a best-effort attempt to
recover these, and then ﬁnd within them the passages answering the corresponding questions. For
public release, we acquired comprehensive annotations on a subset of documents in our test set.5
Movie Reviews . Includes positive/negative sentiment labels on movie
reviews. Original rationale annotations were not
necessarily comprehensive; we thus collected comprehensive rationales on the ﬁnal two folds of the
original dataset .5 In contrast
to most other datasets, the rationale annotations
here are span level as opposed to sentence level.
FEVER . Short for Fact Extraction and VERiﬁcation; entails verifying claims
from textual sources. Speciﬁcally, each claim is to
be classiﬁed as supported, refuted or not enough
information with reference to a collection of source
3Except for BoolQ, wherein source documents in the original train and validation set were not disjoint and we preserve
this structure in our dataset. Questions, of course, are disjoint.
4Consequently, for datasets that have been part of previous benchmarks with other aims (namely, GLUE/superGLUE)
but which we have re-purposed for work on rationales in
ERASER, e.g., BoolQ , we have carved out
for release test sets from the original validation sets.
5Annotation details are in Appendix B.
texts. We take a subset of this dataset, including
only supported and refuted claims.
MultiRC . A reading comprehension dataset composed of questions with
multiple correct answers that by construction depend on information from multiple sentences. Here
each rationale is associated with a question, while
answers are independent of one another. We convert each rationale/question/answer triplet into an
instance within our dataset. Each answer candidate
then has a label of True or False.
Commonsense Explanations (CoS-E) .
This corpus comprises multiplechoice questions and answers from along with supporting rationales. The rationales in this case come in the form both of highlighted (extracted) supporting snippets and freetext, open-ended descriptions of reasoning. Given
our focus on extractive rationales, ERASER includes only the former for now. Following Talmor
et al. , we repartition the training and validation sets to provide a canonical test split.
e-SNLI . This dataset augments the SNLI corpus with
rationales marked in the premise and/or hypothesis
(and natural language explanations, which we do
not use). For entailment pairs, annotators were required to highlight at least one word in the premise.
For contradiction pairs, annotators had to highlight
at least one word in both the premise and the hypothesis; for neutral pairs, they were only allowed
to highlight words in the hypothesis.
Human Agreement We report human agreement
over extracted rationales for multiple annotators
and documents in Table 2. All datasets have a high
Cohen κ ; with substantial or better
agreement.
In ERASER models are evaluated both for their
predictive performance and with respect to the rationales that they extract. For the former, we rely
on the established metrics for the respective tasks.
Here we describe the metrics we propose to evaluate the quality of extracted rationales. We do
not claim that these are necessarily the best metrics for evaluating rationales, however. Indeed, we
hope the release of ERASER will spur additional
research into how best to measure the quality of
model explanations in the context of NLP.
#Annotators/doc
#Documents
Evidence Inference
0.618 ± 0.194
0.617 ± 0.227
0.647 ± 0.260
0.726 ± 0.217
Movie Reviews
0.712 ± 0.135
0.799 ± 0.138
0.693 ± 0.153
0.989 ± 0.102
0.854 ± 0.196
0.871 ± 0.197
0.931 ± 0.205
0.855 ± 0.198
0.728 ± 0.268
0.749 ± 0.265
0.695 ± 0.284
0.910 ± 0.259
0.619 ± 0.308
0.654 ± 0.317
0.626 ± 0.319
0.792 ± 0.371
0.743 ± 0.162
0.799 ± 0.130
0.812 ± 0.154
0.853 ± 0.124
Table 2: Human agreement with respect to rationales. For Movie Reviews and BoolQ we calculate the mean
agreement of individual annotators with the majority vote per token, over the two-three annotators we hired via
Upwork and Amazon Turk, respectively. The e-SNLI dataset already comprised three annotators; for this we
calculate mean agreement between individuals and the majority. For CoS-E, MultiRC, and FEVER, members of
our team annotated a subset to use a comparison to the (majority of, where appropriate) existing rationales. We
collected comprehensive rationales for Evidence Inference from Medical Doctors; as they have a high amount of
expertise, we would expect agreement to be high, but have not collected redundant comprehensive annotations.
Agreement with human rationales
The simplest means of evaluating extracted rationales is to measure how well they agree with those
marked by humans. We consider two classes of
metrics, appropriate for models that perform discrete and ‘soft’ selection, respectively.
For the discrete case, measuring exact matches
between predicted and reference rationales is likely
too harsh.6 We thus consider more relaxed measures.
These include Intersection-Over-Union
(IOU), borrowed from computer vision , which permits credit assignment
for partial matches. We deﬁne IOU on a token level:
for two spans, it is the size of the overlap of the
tokens they cover divided by the size of their union.
We count a prediction as a match if it overlaps with
any of the ground truth rationales by more than
some threshold (here, 0.5). We use these partial
matches to calculate an F1 score. We also measure
token-level precision and recall, and use these to
derive token-level F1 scores.
Metrics for continuous or soft token scoring
models consider token rankings, rewarding models
for assigning higher scores to marked tokens. In
particular, we take the Area Under the Precision-
Recall curve (AUPRC) constructed by sweeping a
threshold over token scores. We deﬁne additional
metrics for soft scoring models below.
In general, the rationales we have for tasks are
sufﬁcient to make judgments, but not necessarily
comprehensive. However, for some datasets we
have explicitly collected comprehensive rationales
for at least a subset of the test set. Therefore, on
these datasets recall evaluates comprehensiveness
directly (it does so only noisily on other datasets).
6Consider that an extra token destroys the match but not
usually the meaning
We highlight which corpora contain comprehensive
rationales in the test set in Table 3.
Measuring faithfulness
As discussed above, a model may provide rationales that are plausible (agreeable to humans) but
that it did not rely on for its output. In many settings one may want rationales that actually explain
model predictions, i.e., rationales extracted for an
instance in this case ought to have meaningfully in-
ﬂuenced its prediction for the same. We call these
faithful rationales. How best to measure rationale
faithfulness is an open question. In this ﬁrst version
of ERASER we propose simple metrics motivated
by prior work .
In particular, following Yu et al. we deﬁne
metrics intended to measure the comprehensiveness
(were all features needed to make a prediction selected?) and sufﬁciency (do the extracted rationales
contain enough signal to come to a disposition?) of
rationales, respectively.
Comprehensiveness.
To calculate rationale
comprehensiveness we create contrast examples : We construct a contrast example for xi, ˜xi, which is xi with the predicted rationales ri removed. Assuming a classiﬁcation setting, let m(xi)j be the original prediction
provided by a model m for the predicted class j.
Then we consider the predicted probability from
the model for the same class once the supporting
rationales are stripped. Intuitively, the model ought
to be less conﬁdent in its prediction once rationales
are removed from xi. We can measure this as:
comprehensiveness = m(xi)j −m(xi/ri)j
A high score here implies that the rationales were
indeed inﬂuential in the prediction, while a low
score suggests that they were not. A negative value
Where do you ﬁnd the most amount of leafs?
Where do you ﬁnd the most amount of leafs?
(a) Compost pile
(b) Flowers
(c) Forest
(e) Ground
(a) Compost pile
(b) Flowers
(c) Forest
(e) Ground
ˆp(Forest|xi)
Where do you ﬁnd the most amount of leafs?
(a) Compost pile
(b) Flowers
(c) Forest
(e) Ground
Comprehensiveness
Figure 2: Illustration of faithfulness scoring metrics, comprehensiveness and sufﬁciency, on the Commonsense
Explanations (CoS-E) dataset. For the former, erasing the tokens comprising the provided rationale (˜xi) ought to
decrease model conﬁdence in the output ‘Forest’. For the latter, the model should be able to come to a similar
disposition regarding ‘Forest’ using only the rationales ri.
here means that the model became more conﬁdent
in its prediction after the rationales were removed;
this would seem counter-intuitive if the rationales
were indeed the reason for its prediction.
Sufﬁciency. This captures the degree to which
the snippets within the extracted rationales are adequate for a model to make a prediction.
sufﬁciency = m(xi)j −m(ri)j
These metrics are illustrated in Figure 2.
As deﬁned, the above measures have assumed
discrete rationales ri. We would also like to evaluate the faithfulness of continuous importance
scores assigned to tokens by models. Here we
adopt a simple approach for this. We convert soft
scores over features si provided by a model into
discrete rationales ri by taking the top−kd values,
where kd is a threshold for dataset d. We set kd to
the average rationale length provided by humans
for dataset d (see Table 4). Intuitively, this says:
How much does the model prediction change if we
remove a number of tokens equal to what humans
use (on average for this dataset) in order of the
importance scores assigned to these by the model.
Once we have discretized the soft scores into rationales in this way, we compute the faithfulness
scores as per Equations 1 and 2.
This approach is conceptually simple. It is also
computationally cheap to evaluate, in contrast to
measures that require per-token measurements, e.g.,
importance score correlations with ‘leave-one-out’
scores , or counting how
many ‘important’ tokens need to be erased before
a prediction ﬂips . However, the necessity of discretizing continuous scores
forces us to pick a particular threshold k.
We can also consider the behavior of these measures as a function of k, inspired by the measurements proposed in Samek et al. in the context of evaluating saliency maps for image classi-
ﬁcation. They suggested ranking pixel regions by
importance and then measuring the change in output as they are removed in rank order. Our datasets
comprise documents and rationales with quite different lengths; to make this measure comparable
across datasets, we construct bins designating the
number of tokens to be deleted. Denoting the tokens up to and including bin k for instance i by rik,
we deﬁne an aggregate comprehensiveness measure:
m(xi)j −m(xi/rik)j)
This is deﬁned for sufﬁciency analogously. Here
we group tokens into k = 5 bins by grouping them
into the top 1%, 5%, 10%, 20% and 50% of tokens, with respect to the corresponding importance
score. We refer to these metrics as “Area Over the
Perturbation Curve” (AOPC).7
These AOPC sufﬁciency and comprehensiveness
measures score a particular token ordering under
a model. As a point of reference, we also report
these when random scores are assigned to tokens.
7Our AOPC metrics are similar in concept to ROAR
 except that we re-use an existing model
as opposed to retraining for each fraction.
Baseline Models
Our focus in this work is primarily on the ERASER
benchmark itself, rather than on any particular
model(s). But to establish a starting point for future
work, we evaluate several baseline models across
the corpora in ERASER.8 We broadly classify these
into models that assign ‘soft’ (continuous) scores
to tokens, and those that perform a ‘hard’ (discrete)
selection over inputs. We additionally consider
models speciﬁcally designed to select individual
tokens (and very short sequences) as rationales, as
compared to longer snippets. All of our implementations are in PyTorch and are
available in the ERASER repository.9
All datasets in ERASER comprise inputs, rationales, and labels. But they differ considerably in
document and rationale lengths (Table A). This motivated use of different models for datasets, appropriate to their sizes and rationale granularities. We
hope that this benchmark motivates design of models that provide rationales that can ﬂexibly adapt to
varying input lengths and expected rationale granularities. Indeed, only with such models can we
perform comparisons across all datasets.
Hard selection
Models that perform hard selection may be viewed
as comprising two independent modules: an encoder which is responsible for extracting snippets
of inputs, and a decoder that makes a prediction
based only on the text provided by the encoder. We
consider two variants of such models.
Lei et al. . In this model, an encoder induces a binary mask over inputs x, z. The decoder
accepts the tokens in x unmasked by z to make a
prediction ˆy. These modules are trained jointly via
REINFORCE style estimation,
minimizing the loss over expected binary vectors
z yielded from the encoder. One of the advantages
of this approach is that it need not have access to
marked rationales; it can learn to rationalize on the
basis of instance labels alone. However, given that
we do have rationales in the training data, we experiment with a variant in which we train the encoder
explicitly using rationale-level annotations.
In our implementation of Lei et al. , we
drop in two independent BERT 
or GloVe base modules
8This is not intended to be comprehensive.
9 
eraserbenchmark
with bidirectional LSTMs on top to induce contextualized representations of tokens for the encoder and decoder,
respectively. The encoder generates a scalar (denoting the probability of selecting that token) for
each LSTM hidden state using a feedfoward layer
and sigmoid. In the variant using human rationales
during training, we minimize cross entropy loss
over rationale predictions. The ﬁnal loss is then
a composite of classiﬁcation loss, regularizers on
rationales , and loss over rationale
predictions, when available.
Pipeline models.
These are simple models in
which we ﬁrst train the encoder to extract rationales, and then train the decoder to perform prediction using only rationales. No parameters are
shared between the two models.
Here we ﬁrst consider a simple pipeline that ﬁrst
segments inputs into sentences. It passes these,
one at a time, through a Gated Recurrent Unit
(GRU) , to yield hidden representations that we compose via an attentive decoding
layer . This aggregate representation is then passed to a classiﬁcation module
which predicts whether the corresponding sentence
is a rationale (or not). A second model, using effectively the same architecture but parameterized independently, consumes the outputs (rationales) from
the ﬁrst to make predictions. This simple model is
described at length in prior work . We further consider a ‘BERT-to-BERT’
pipeline, where we replace each stage with a BERT
module for prediction .
In pipeline models, we train each stage independently. The rationale identiﬁcation stage is trained
using approximate sentence boundaries from our
source annotations, with randomly sampled negative examples at each epoch. The classiﬁcation
stage uses the same positive rationales as the identiﬁcation stage, a type of teacher forcing (details in Appendix C).
Soft selection
We consider a model that passes tokens through
BERT to induce contextualized representations that are then passed to a bidirectional LSTM . The hidden representations from the LSTM
are collapsed into a single vector using additive
attention . The LSTM layer
allows us to bypass the 512 word limit imposed by
Evidence Inference
Lei et al. 
Lei et al. (u)
Lehman et al. 
Bert-To-Bert
Lei et al. 
Lei et al. (u)
Lehman et al. 
Bert-To-Bert
Movie Reviews
Lei et al. 
Lei et al. (u)
Lehman et al. 
Bert-To-Bert
Lei et al. 
Lei et al. (u)
Lehman et al. 
Bert-To-Bert
Lei et al. 
Lei et al. (u)
Lehman et al. 
Bert-To-Bert
Lei et al. 
Lei et al. (u)
Bert-To-Bert
Lei et al. 
Lei et al. (u)
Bert-To-Bert
Table 3: Performance of models that perform hard rationale selection. All models are supervised at the rationale level except for those marked with (u), which learn
only from instance-level supervision; † denotes cases in
which rationale training degenerated due to the REIN-
FORCE style training. Perf. is accuracy (CoS-E) or
macro-averaged F1 (others). Bert-To-Bert for CoS-E
and e-SNLI uses a token classiﬁcation objective. Bert-
To-Bert CoS-E uses the highest scoring answer.
BERT; when we exceed this, we effectively start
encoding a ‘new’ sequence (setting the positional
index to 0) via BERT. The hope is that the LSTM
learns to compensate for this. Evidence Inference
and BoolQ comprise very long (>1000 token) inputs; we were unable to run BERT over these. We
instead resorted to swapping GloVe 300d embeddings in place of BERT
representations for tokens. spans.
To soft score features we consider: Simple gradients, attention induced over contextualized representations, and LIME .
Evidence Inference
GloVe + LSTM - Attention
GloVe + LSTM - Gradient
GloVe + LSTM - Lime
GloVe + LSTM - Random
GloVe + LSTM - Attention
GloVe + LSTM - Gradient
GloVe + LSTM - Lime
GloVe + LSTM - Random
BERT+LSTM - Attention
BERT+LSTM - Gradient
BERT+LSTM - Lime
BERT+LSTM - Random
BERT+LSTM - Attention
BERT+LSTM - Gradient
BERT+LSTM - Lime
BERT+LSTM - Random
BERT+LSTM - Attention
BERT+LSTM - Gradient
BERT+LSTM - Lime
BERT+LSTM - Random
BERT+LSTM - Attention
BERT+LSTM - Gradient
BERT+LSTM - Lime
BERT+LSTM - Random
BERT+LSTM - Attention
BERT+LSTM - Gradient
BERT+LSTM - Lime
BERT+LSTM - Random
Table 4: Metrics for ‘soft’ scoring models. Perf. is accuracy (CoS-E) or F1 (others). Comprehensiveness and
sufﬁciency are in terms of AOPC (Eq. 3). ‘Random’
assigns random scores to tokens to induce orderings;
these are averages over 10 runs.
Evaluation
Here we present initial results for the baseline models discussed in Section 5, with respect to the metrics proposed in Section 4. We present results in
two parts, reﬂecting the two classes of rationales
discussed above: ‘Hard’ approaches that perform
discrete selection of snippets, and ‘soft’ methods
that assign continuous importance scores to tokens.
In Table 3 we evaluate models that perform discrete selection of rationales. We view these as inherently faithful, because by construction we know
which snippets the decoder used to make a prediction.10 Therefore, for these methods we report
only metrics that measure agreement with human
annotations.
10This assumes independent encoders and decoders.
Due to computational constraints, we were unable to run our BERT-based implementation of Lei
et al. over larger corpora. Conversely, the
simple pipeline of Lehman et al. assumes
a setting in which rationale are sentences, and so
is not appropriate for datasets in which rationales
tend to comprise only very short spans. Again, in
our view this highlights the need for models that
can rationalize at varying levels of granularity, depending on what is appropriate.
We observe that for the “rationalizing” model
of Lei et al. , exploiting rationale-level supervision often (though not always) improves agreement with human-provided rationales, as in prior
work . Interestingly, this does not seem strongly correlated
with predictive performance.
Lei et al. outperforms the simple pipeline
model when using a BERT encoder. Further, Lei
et al. outperforms the ‘BERT-to-BERT’
pipeline on the comparable datasets for the ﬁnal
prediction tasks. This may be an artifact of the
amount of text each model can select: ‘BERT-to-
BERT’ is limited to sentences, while Lei et al.
 can select any subset of the text. Designing
extraction models that learn to adaptively select
contiguous rationales of appropriate length for a
given task seems a potentially promising direction.
In Table 4 we report metrics for models that
assign continuous importance scores to individual tokens. For these models we again measure
downstream (task) performance (macro F1 or accuracy). Here the models are actually the same,
and so downstream performance is equivalent. To
assess the quality of token scores with respect to
human annotations, we report the Area Under the
Precision Recall Curve (AUPRC).
These scoring functions assign only soft scores
to inputs (and may still use all inputs to come to
a particular prediction), so we report the metrics
intended to measure faithfulness deﬁned above:
comprehensiveness and sufﬁciency, averaged over
‘bins’ of tokens ordered by importance scores. To
provide a point of reference for these metrics —
which depend on the underlying model — we report results when rationales are randomly selected
(averaged over 10 runs).
Both simple gradient and LIME-based scoring
yield more comprehensive rationales than attention
weights, consistent with prior work . Attention
fares better in terms of AUPRC — suggesting better agreement with human rationales — which is
also in line with prior ﬁndings that it may provide
plausible, but not faithful, explanation . Interestingly, LIME does particularly well
across these tasks in terms of faithfulness.
From the ‘Random’ results that we conclude
models with overall poor performance on their ﬁnal tasks tend to have an overall poor ordering, with
marginal differences in comprehensiveness and suf-
ﬁciency between them. For models that with high
sufﬁciency scores: Movies, FEVER, CoS-E, and e-
SNLI, we ﬁnd that random removal is particularly
damaging to performance, indicating poor absolute
ranking; whereas those with high comprehensiveness are sensitive to rationale length.
Conclusions and Future Directions
We have introduced a new publicly available resource: the Evaluating Rationales And Simple English Reasoning (ERASER) benchmark. This comprises seven datasets, all of which include both
instance level labels and corresponding supporting
snippets (‘rationales’) marked by human annotators.
We have augmented many of these datasets with
additional annotations, and converted them into a
standard format comprising inputs, rationales, and
outputs. ERASER is intended to facilitate progress
on explainable models for NLP.
We proposed several metrics intended to measure the quality of rationales extracted by models,
both in terms of agreement with human annotations, and in terms of ‘faithfulness’. We believe
these metrics provide reasonable means of comparison of speciﬁc aspects of interpretability, but we
view the problem of measuring faithfulness, in particular, a topic ripe for additional research (which
ERASER can facilitate).
Our hope is that ERASER enables future work
on designing more interpretable NLP models, and
comparing their relative strengths across a variety of tasks, datasets, and desired criteria. It also
serves as an ideal starting point for several future
directions such as better evaluation metrics for interpretability, causal analysis of NLP models and
datasets of rationales in other languages.
Acknowledgements
We thank the anonymous ACL reviewers.
This work was supported in part by the NSF (CA-
REER award 1750978), and by the Army Research
Ofﬁce (W911NF1810328).