High Conﬁdence Off-Policy Evaluation
Philip S. Thomas1,2
Georgios Theocharous1
Mohammad Ghavamzadeh1,3
1Adobe Research, 2University of Massachusetts Amherst, 3INRIA Lille
{phithoma,theochar,ghavamza}@adobe.com
Many reinforcement learning algorithms use trajectories collected from the execution of one or more policies to propose a new policy. Because execution of a
bad policy can be costly or dangerous, techniques for
evaluating the performance of the new policy without
requiring its execution have been of recent interest in industry. Such off-policy evaluation methods, which estimate the performance of a policy using trajectories collected from the execution of other policies, heretofore
have not provided conﬁdences regarding the accuracy
of their estimates. In this paper we propose an off-policy
method for computing a lower conﬁdence bound on the
expected return of a policy.
Introduction
In this paper we show how trajectories generated by some
policies, called behavior policies, can be used to compute
the conﬁdence that the expected return of a different policy,
called the evaluation policy, will exceed some lower bound.
This high conﬁdence off-policy evaluation mechanism has
immense implications in industry, where execution of a new
policy can be costly or dangerous if it performs worse than
the policy that is currently being used. There are many applications that are hindered by such safety concerns, including news recommendation systems , patient
diagnosis systems , neuroprosthetic control , automatic Propofol administration , and lifetime value optimization in marketing systems . In our experiments we show how our algorithm can
be applied to a digital marketing problem where conﬁdence
bounds are necessary to motivate the potentially risky gamble of executing a new policy.
Although the off-policy evaluation problem has been
solved efﬁciently in the multi-arm bandit case , it is still an open question for sequential decision
problems. Existing methods for estimating the performance
of the evaluation policy using trajectories from behavior
policies do not provide conﬁdence bounds .
Copyright © 2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Our approach is straightforward—for each trajectory,
we use importance sampling to generate an importance
weighted return, which is an unbiased estimate of the expected return of the evaluation policy . These importance weighted returns can be used
by a concentration inequality to get a con-
ﬁdence bound on the expected return. The primary challenge of this approach is that the importance weighted returns have high variance and a large possible range, both of
which loosen the bounds produced by existing concentration
inequalities. We therefore derive a novel modiﬁcation of an
existing concentration inequality that makes it particularly
well suited to this setting.
In the rest of this paper we explain how to generate the
unbiased estimates, describe existing concentration inequalities, derive our new concentration inequality, and provide
supporting empirical studies that show both the necessity of
high conﬁdence off-policy evaluation methods and the viability of our approach.
Preliminaries
Let S and A denote the state and action spaces, rt ∈
[rmin, rmax] be the bounded reward at time t, and γ ∈ 
be a discount factor.1 We denote by π(a|s, θ) the probability (density) of taking action a in state s when using policy parameters θ ∈Rnθ, where nθ is a positive integer—
the dimension of the policy parameter space. A trajectory of
length T is an ordered set of states (observations), actions,
and rewards: τ = {s1, a1, r1, s2, a2, r2, . . . , sT , aT , rT }.
We deﬁne the (normalized and discounted) return of a trajectory to be
t=1 γt−1rt
where R−and R+ are upper and lower bounds on
t=1 γt−1rt. In the absence of domain-speciﬁc knowledge,
the loose bounds R−= rmin(1 −γT )/(1 −γ) and R+ =
rmax(1 −γT )/(1 −γ) can be used. Let
ρ(θ) := E [R(τ)|θ]
1Although we use MDP notation, by replacing states with observations, our results carry over to POMDPs with reactive policies.
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence
denote the performance of policy parameters θ, i.e., the expected discounted return when using policy parameters θ.
We assume that all trajectories are of length at most T.
We assume that we are given a data set, D, that consists
of n trajectories, {τi}n
i=1, each labeled by the policy parameters that generated them, {θi}n
i=1, i.e.,2
(τi, θi) : i ∈{1, . . . , n}, τi generated using θi
Note that {θi}n
i=1 are behavior polices—those that generated the batch of data (trajectories). Finally, we denote by
θ the evaluation policy—the one that should be evaluated
using the data set D. Although some trajectories in D may
have been generated using the evaluation policy, we are particularly interested in the setting where some or all of the behavior policies are different from the evaluation policy. As
described in the introduction, our goal is to present a mechanism that takes a conﬁdence, (1 −δ) ∈ , as input and
returns a corresponding lower bound, ρ−∈ , for the
performance of the evaluation policy, ρ(θ). The mechanism
should also be able to take a lower bound, ρ−∈ , as
input and return the conﬁdence, 1 −δ, that ρ−is a lower
bound on ρ(θ).
Generating Unbiased Estimates of ρ(θ)
Our approach relies on our ability to take an element
(τ, θi) ∈D, i.e., a trajectory τ generated by a behavior policy θi, and compute an unbiased estimate, ˆρ(θ, τ, θi), of the
performance of the evaluation policy ρ(θ). We use importance sampling to generate
these unbiased estimates:3
ˆρ(θ, τ, θi) = R(τ) Pr(τ|θ)
Pr(τ|θi) := R(τ)
π(at|st, θ)
π(at|st, θi)
importance weight
where Pr(τ|θ) is the probability that trajectory τ is generated by following policy θ. Note that we do not need to
require π(a|s, θi) > 0 for all s and a in (1), since division by zero can never occur in this equation. This is because at would have never been chosen in trajectory τi if
π(at|st, θi) = 0.
For each θi, ˆρ(θ, τ, θi) is a random variable that can be
sampled by generating a trajectory, τ, using policy parameters θi, and then using (1). If π(a|s, θ) = 0 for all s and a
where π(a|s, θi) = 0, then importance sampling is unbiased,
i.e., E[ˆρ(θ, τ, θi)] = ρ(θ). However, it is important to consider what happens when this is not the case—when there is
one or more state-action pair, s, a, where π(a|s, θ) > 0 but
π(a|s, θi) = 0. In this case ˆρ(θ, τ, θi) is a biased estimator
because it does not have access to data that can be used to
evaluate the outcome of taking action a in state s. For simplicity, we avoid this by assuming that if π(a|s, θi) = 0, then
2Note that θi denotes the parameter vector of the ith trajectory
and not the ith element of θ.
3Per-decision importance sampling is another unbiased estimator that could be used in place of
ordinary importance sampling. Here we use ordinary importance
sampling due to its simplicity.
Figure 1: Empirical estimate of the probability density function (PDF) of ˆρ(θ, τ, θi) on a simpliﬁed version of the
mountain-car problem with T =
20. The behavior policy, θi, corresponds to a suboptimal policy and the evaluation policy, θ, is selected along the natural
policy gradient from θi. The PDF is estimated from 100,000
trajectories. It is important to note that while the tightest upper bound on ˆρ(θ, τ, θi) is approximately 109.4, the largest
observed importance weighted return is only around 316.
The sample mean is about 0.191 ≈10−0.72. Note that
the horizontal axis is scaled logarithmically. Also, the upper bound on ˆρ(θ, τ, θi) was computed using a brute force
search for the s, a pair that maximizes π(a|s, θ)/π(a|s, θi).
Without domain-speciﬁc knowledge, this state-action pair
could occur at every time step and could result in a return
of 1, making the largest possible importance weighted return (π(a|s, θ)/π(a|s, θi))T .
π(a|s, θ) = 0. In the appendix we explain this in more detail
and show that the lower bound that we propose holds even
without this assumption.
Since the smallest possible return is zero and the importance weights are nonnegative, the importance weighted returns, ˆρ(θ, τ, θi), are bounded from below by zero. However,
when the action selected in a state by the behavior policy
has low probability under the behavior policy but high probability under the evaluation policy, i.e., π(at|st, θi) is small
and π(at|st, θ) is large, then the corresponding importance
weighted return, ˆρ(θ, τ, θi), might be large. So, the random
variables ˆρ(θ, τ, θi) are bounded from below by zero, have
expected value in , and may have a large upper bound.
This means that ˆρ(θ, τ, θi) often has a very long tail, as
shown in Fig. 1. Thus, the primary challenge of our endeavor
is to measure and account for this large range and high variance to produce a tight bound on ρ(θ).
Concentration Inequality
We consider three concentration inequalities that provide
probability bounds on how a random variable deviates from
its expectation. Let X1, . . . , Xn be n independent realvalued bounded random variables such that for each i ∈
{1, . . . , n}, we have Pr (Xi ∈[0, bi]) = 1 and E[Xi] = µ.
In the context or our problem, the Xi correspond to the importance weighted returns, ˆρ(θ, τ, θi), the uniform mean, µ,
is ρ(θ), and bi is an upper bound on ˆρ(θ, τ, θi). Recall from
the last section that bi can be exceedingly large—about 109.4
in the example of Fig. 1. In the following, for simplicity, we
assume that all the random variables have the same upper
Chernoff-Hoeffding (CH) inequality: This bound indicates that with probability at least 1 −δ, we have
Maurer & Pontil’s empirical Bernstein (MPeB) inequality : This bound replaces the true (unknown in our setting) variance in Bernstein’s inequality with the sample variance. The MPeB inequality states that with probability at least 1 −δ, we have4
Xi −7b ln(2/δ)
3(n −1) −1
(Xi −Xj)2.
Anderson (AM) inequality: The Anderson inequality is based on the Dvoretzky-Kiefer-Wolfowitz
inequality , and the
variant we use here is with the optimal constants found by
Massart . The AM inequality states that with probability at least 1 −δ, we have
(zi+1 −zi) min
where z1, . . . , zn are the samples of the random variables
X1, X2, . . . , Xn, sorted such that z1 ≤z2 ≤. . . ≤zn,
and z0 = 0. Unlike the CH and MPeB bounds, which hold
for independent random variables, the AM inequality only
holds for independent and identically distributed (i.i.d.) random variables, i.e., X1, . . . , Xn should also be identically
distributed. In the context of our problem, this means that
the AM bound can only be used when all of the trajectories
in D were generated by a single behavior policy.
Notice that the effect of the range, b, is decreased in MPeB
relative to CH, since in MPeB the range is divided by n,
whereas in CH it is divided by √n. While CH is based on
the sample mean and MPeB is based on the sample mean
and variance, AM takes into account the entire sample cumulative distribution function. This allows AM to only depend on the largest observed sample and not b. This can be
a signiﬁcant improvement in situations like the example of
Fig. 1, where the largest observed sample is about 316, while
b is approximately 109.4. However, despite AM’s desirable
property that it does not depend on the range of the random
variable, it is not suitable for our problem for two reasons: 1)
It tends to be looser than MPeB for random variables without
long tails, due to its inherent reliance on the Kolmogorov-
Smirnov statistic . 2) As
discussed earlier, unlike CH and MPeB, AM can be applied
only if the random variables are i.i.d., and to the best of our
knowledge, it is not obvious how to extend it to the setting
in which the random variables are only independent and not
4To obtain this from Maurer and Pontil’s Thm. 11, we ﬁrst normalize X and then apply Thm. 11 with 1 −X instead of X.
identically distributed (in the context of our problem, this is
when D is generated by multiple behavior policies).
Our goal in the rest of this section is to extend MPeB so
that it is useful for our policy evaluation problem, i.e., so
that it is independent of the range of the random variables
and able to handle random variables that have different
ranges but the same mean. This results in a new concentration inequality that combines the desirable properties of
MPeB (general tightness and applicability to random variables that are not identically distributed) with those of AM
(no direct dependence on the range of the random variables).
In the context of our policy evaluation problem, it also removes the need to determine a tight upper bound on the
largest possible importance weighted return, which may require expert consideration of domain-speciﬁc properties.
Our new bound is an extension of MPeB that relies on
two key insights: 1) removing the upper tail of a distribution can only lower its expected value, and 2) MPeB can be
generalized to handle random variables with different ranges
if it is simultaneously specialized to random variables with
the same mean. We prove our new concentration inequality in Thm. 1. To prove this theorem, we collapse the tail of
the distribution of the random variables, normalize the random variables so that the MPeB inequality can be applied,
and then use MPeB to generate a lower-bound from which
we extract a lower-bound on the mean of the original random variables. Our approach for collapsing the tails of the
distributions and then bounding the means of the new distributions is similar to bounding the truncated mean and is a
form of Winsorization . Later
we will discuss how the threshold values, ci, can be selected
automatically from the data.
Theorem 1. Let X1, . . . , Xn be n independent realvalued bounded random variables such that for each i ∈
{1, . . . , n}, we have Pr (0 ≤Xi) = 1, E[Xi] ≤µ, and
the ﬁxed real-valued threshold ci > 0. Let δ > 0 and
Yi := min{Xi, ci}. Then with probability at least 1 −δ,
empirical mean
7n ln(2/δ)
term that goes to zero as 1/n as n →∞
term that goes to zero as 1/√n as n →∞
Proof. We deﬁne n independent random variables, Z =
i=1, as Zi := Yi
ci . Thus, we have
Since E[Yi] ≤E[Xi] ≤µ, we may write
E[ ¯Z] = 1
Notice that the Zi random variables, and therefore also the
(1 −Zi) random variables, are n independent random variables with values in . So, using Thm. 11 of Maurer and
Pontil , with probability at least 1 −δ, we have
E[1 −¯Z] ≤1 −¯Z +
2Vn(1 −Z) ln(2/δ)
+ 7 ln(2/δ)
3(n −1) , (5)
where the empirical variance, Vn(1 −Z), is deﬁned as
Vn(1 −Z) :=
 (1 −Zi) −(1 −Zj)
The claim follows by replacing ¯Z, E[ ¯Z], and Vn(1 −Z) in
(5) with (3), (4), and (6).
Remark 1: Notice that if Pr (Xi ≤bi) = 1 and ci = bi
for all i, then Thm. 1 degenerates to Thm. 11 of Maurer and
Pontil .
Remark 2: Thm. 1 allows us to take evaluation policy
parameters θ, a set of trajectories, D, generated by several
behavior policies, and a conﬁdence level, (1 −δ), as input,
and return a probabilistic lower bound on the performance
of this policy, ρ(θ) = µ. The lower bound is the right-handside (RHS) of (2).
Remark 3: Despite the nested sum, P
i,j, the RHS of (2)
can be evaluated in linear time (a single pass over the samples), since we may write
(Ai −Aj)2 = 2n
and so (2) may be rewritten as
ci −7n ln(2/δ)
t2 ln(2/δ)
Remark 4: Whereas in Remark 1 we used a conﬁdence, 1−
δ, to compute a lower bound on ρ(θ) = µ, the bound can
also be inverted to produce a conﬁdence from a lower bound,
Then our conﬁdence that µ ≥µ−is
1 −min{1, 2 exp
if ζ is real and positive,
otherwise.
Remark 5: In order to use the result of Thm. 1 in our policy
evaluation application, we must select the values of the ci,
i.e., the thresholds beyond which the distributions of the Xi
are collapsed. To simplify this procedure, we select a single
c > 0 and set ci = c for all i. When c is too large, it loosens
the bound just like a large range b does. On the other hand,
when c is too small, it decreases the expected values of the
Yi, which also loosens the bound. The optimal c must properly balance this trade-off between the range and mean of
the Yi. Fig. 2 illustrates this trade-off for the mountain car
problem described in Fig. 1.
Figure 2: The lower bound µ−from Thm. 1 when using different values of c on the 100,000 trajectories used to generate Fig. 1. The optimal value of c is around 100, which
equates to collapsing the tail of the distribution in Fig. 1 at
100. The curve continues below the horizontal axis down to
−129,703 for c = 109.4, i.e., the upper bound on ˆρ(θ, τ, θi).
Thm. 1 requires the thresholds, ci, to be ﬁxed—i.e., they
should not be computed using realizations of any Xi. So, we
partition the data set, D, into two sets, Dpre and Dpost. Dpre
is used to estimate the optimal threshold, c, and Dpost is used
to compute the lower bound (the RHS of (2)). The optimal
value of c is the one that results in the largest lower bound,
i.e., maximizes the RHS of (2). Note that the RHS of (2)
depends on the sample mean and sample variance. We will
use the sample mean and variance of Dpre to predict what the
sample mean and variance in the RHS of (2) would be if we
use Dpost and a speciﬁc value of c. We then select a c∗that
maximizes this prediction, i.e.,
c∗∈arg max
prediction of Dpost’s sample mean
7c ln(2/δ)
3(npost −1)
npre(npre −1)
prediction of Dpost’s sample variance
Recall that Yi := min{Xi, ci}, so all the three terms in (7)
depend on c. Once an estimate of the optimal threshold, c∗,
has been formed from Dpre, Thm. 1 is applied using c∗and
the samples in Dpost. From our preliminary experiments we
found that using 1/20 of the samples in Dpre and the remaining 19/20 in Dpost works well. In our application we know the
−5,831,000
Table 1: A comparison of 95% conﬁdence lower bounds on
ρ(θ). The 100,000 trajectories and evaluation policy are the
same as in Fig. 1 and 2. The sample mean (average importance weighted return) is 0.191.
true mean is in , so we require c∗≥1. When some of
the random variables are identically distributed, we ensure
that they are divided with 1/20 in Dpre and 19/20 in Dpost.5
Experiments
Mountain Car
We used the mountain car data from Fig. 1 to compare the
lower bounds found when using Thm. 1, CH, MPeB, and
AM. The results are provided in Table 1. These results re-
ﬂect our previous discussion—the large possible range of
ˆρ(θ, τ, θi) causes CH to perform poorly since it scales with
b/√n. MPeB is the next worst since it scales with b/(n−1).
AM performs reasonably well since it depends on the largest
observed sample, z100000 ≈316, rather than b ≈109.4.
However, as expected, Thm. 1 performs the best because it
combines the tightness of MPeB with AM’s lack of dependence on b.
Digital Marketing using Real-World Data
Adobe Marketing Cloud is a powerful set of tools that allows
companies to fully leverage digital marketing using both automated and manual solutions. It has been deployed widely
across the internet, with approximately seven out of every
ten dollars transacted on the web passing through one of
Adobe’s products. Adobe Target, one of the six core components of Adobe Marketing Cloud, allows for automated userspeciﬁc targeting of advertisements and campaigns. When a
user requests a webpage that contains an advertisement, the
decision of which advertisement to show is computed based
on a vector containing all of the known features of the user.
This problem tends to be treated as a bandit problem,
where an agent treats each advertisement as a possible action and attempts to maximize the probability that the user
clicks on the advertisement. Although this greedy approach
has been successful, it does not necessarily also maximize
the total number of clicks from each user over his or her
lifetime. It has been shown that more far-sighted reinforcement learning approaches to this problem can improve signiﬁcantly upon bandit solutions (Theocharous and Hallak
In order to avoid the large costs associated with deployment of a bad policy, in this application it is imperative
that new policies proposed by RL algorithms are thoroughly
evaluated prior to execution. Because off-policy evaluation
methods are known to have high variance, estimates of performance without associated conﬁdences are not sufﬁcient.
5In our policy evaluation application, the importance weighted
returns from two trajectories are identically distributed if the trajectories were generated by the same behavior policy.
However, our high-conﬁdence off-policy evaluation method
can provide sufﬁcient evidence supporting the deployment
of a new policy to warrant its execution.
For our second case study we used real data, captured with
permission from the website of a Fortune 50 company that
receives hundreds of thousands of visitors per day and which
uses Adobe Target, to train a simulator using a proprietary
in-house system identiﬁcation tool at Adobe. The simulator
produces a vector of 31 real-valued features that provide a
compressed representation of all of the available information
about a user. The advertisements are clustered into two highlevel classes that the agent must select between. After the
agent selects an advertisement, the user either clicks (reward
of +1) or does not click (reward of 0) and the feature vector
describing the user is updated. We selected T = 20 and γ =
This is a particularly challenging problem because the reward signal is sparse—if each action is selected with probability 0.5 always, only about 0.38% of the transitions are
rewarding, since users usually do not click on the advertisements. This means that most trajectories provide no feedback. Also, whether a user clicks or not is close to random,
so returns have relatively high variance.
We generated data using an initial baseline policy and
then evaluated a new policy proposed by an in-house reinforcement learning algorithm. Fig. 3 shows the 95% conﬁdence lower bound produced using different numbers of trajectories and various concentration inequalities. As in the
mountain car example, Thm. 1 signiﬁcantly outperforms
previously existing concentration inequalities.
Fig. 4, gives our conﬁdence for every possible lower
bound. This characterizes the risk associated with deployment of the new policy since it gives a conﬁdence bound for
every possible outcome—it bounds the probability of different levels of degradation in performance relative to the behavior policy as well as the probability of different amounts
of improvement. Fig. 4 is an exceptionally compelling argument for deployment of the new policy in place of the
behavior policy.
Conclusion and Future Work
We have presented a technique that can instill the user of an
RL algorithm with conﬁdence that a newly proposed policy
will perform well, without requiring the new policy to actually be executed. This is accomplished by providing conﬁdence bounds for various levels of performance degradation
and improvement. Our ability to compute tight conﬁdence
bounds comes from a novel adaptation of an existing concentration inequality to make it particularly well suited to
this application. Speciﬁcally, it can handle random variables
that are not identically distributed and our experiments suggest that it is tight even for distributions with heavy upper
Our scheme for automatically selecting the threshold parameter, c, is ad hoc. This could be improved, especially
by a method for adaptively determining how many samples
should be used to select c. Second, our creation of a practical lower bound on the expected return of a policy might
10,000,000
20,000,000
Lower Bound
Number of Trajectories
Figure 3: 95% conﬁdence lower bound (unnormalized) on
ρ(θ) using trajectories generated using the simulator described in the text. The behavior policy’s true expected return is approximately 0.0765 (per step click probability of
0.0765/T ≈.38%) and is plotted as “Behavior”. The evaluation policy’s true expected return is approximately 0.086
(per step click probability of ≈.43%). These two estimates
of the policies’ performances were computed by deploying
them in the simulator for 1 million trajectories and computing the average return. The curves are averaged over ten trials, and the largest standard deviation is 0.002. CH never
achieved a lower bound above −22, and so it is not visible
in this plot. Notice that, using Thm. 1, we are able to guarantee that the evaluation policy is an improvement upon the
behavior policy with at least 95% conﬁdence using only 2
million trajectories (recall that the website has several hundred thousand visitors per day).
allow for the transfer of bandit algorithms to the sequential
decision making setting.
In this appendix we show that, for any (τ, θi)
ˆρ(θ, τ, θi) is a random variable whose expectation is a lower
bound on the performance of the evaluation policy, ρ(θ). Let
Y and Z be the sets of trajectories, τ, such that Pr(τ|θ) ̸= 0
and Pr(τ|θi) ̸= 0, respectively, and let Yc be the complement of Y. Our claim comes from the following series of
(in)equalities:
ˆρ(θ, τ, θi)
R(τ) Pr(τ|θ)
R(τ) Pr(τ|θ) dτ
R(τ) Pr(τ|θ) dτ +
R(τ) Pr(τ|θ) dτ
Y∩ZcR(τ) Pr(τ|θ) dτ
R(τ) Pr(τ|θ) dτ = Eτ∼θ
where Eτ∼θ denotes the expected value when the trajectories, τ, are generated using policy parameters θ. In (8),
Figure 4: Our conﬁdence that ρ(θ) is at least the (unnormalized) lower bound speciﬁed on the horizontal axis, as computed using Thm. 1. As in Fig. 3, the initial behavior policy’s
expected return is approximately 0.0765 and the evaluation
policy’s true (unknown in practice) expected return is approximately 0.086, both of which are marked on the horizontal axis. First we used 2 million trajectories from the behavior policy to compute lower bounds on the performance
of the evaluation policy for every possible conﬁdence. The
result of this experiment is shown by the solid black line.
The 95% conﬁdence lower bound was 0.077. Although this
means guaranteed improvement with high conﬁdence, it is
only a minor improvement that may not warrant the high
overhead costs associated with deploying a new policy. We
therefore collected an additional 3 million trajectories using
the initial behavior policy and recomputed the lower bounds
using all 5 million trajectories. The result of this experiment
is shown by the dashed blue line, where the 95% conﬁdence
lower bound is 0.0803. We then deployed the evaluation policy and collected 1 million on-policy trajectories and recomputed the lower bounds once again. The result of this experiment using all 6 million trajectories is shown by the dotted
red line. Using all 6 million trajectories resulted in a 95%
conﬁdence lower bound of 0.81, which supports continued
deployment of the evaluation policy.
(a) This integral is zero because, from the deﬁnition of Y,
we have Pr(τ|θ) = 0, for each τ ∈Yc ∩Z.
(b) This inequality holds because R(τ) ≥0.
It is clear from (8) that if the support of Pr(τ|θ) (the evaluation policy) is a subset of the support of Pr(τ|θi) (the behavior policy), then
Y∩Zc R(τ) Pr(τ|θ)dτ = 0, and as a
result, Eτ∼θi
ˆρ(θ, τ, θi)
= ρ(θ), which means ˆρ(θ, τ, θi) is
an unbiased estimate of ρ(θ). However, if this is not the case,
i.e., Eτ∼θi
ˆρ(θ, τ, θi)
≤ρ(θ), our results are still valid,
because later in the paper we will ﬁnd a lower-bound on
ˆρ(θ, τ, θi)
, which would also be a lower bound on
ρ(θ) (our quantity of interest).