Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136–141,
Baltimore, Maryland USA, June 26–27, 2014. c⃝2014 Association for Computational Linguistics
The DCU-ICTCAS MT system at WMT 2014 on German-English
Translation Task
Liangyou Li∗, Xiaofeng Wu∗, Santiago Cort´es Va´ıllo∗
Jun Xie†, Andy Way∗, Qun Liu∗†
∗CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
† Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,xiaofengwu,scortes,away,qliu}@computing.dcu.ie
 
This paper describes the DCU submission to WMT 2014 on German-English
translation task. Our system uses phrasebased translation model with several popular techniques,
including Lexicalized
Reordering Model, Operation Sequence
Model and Language Model interpolation.
Our ﬁnal submission is the result of system combination on several systems which
have different pre-processing and alignments.
Introduction
On the German-English translation task of WMT
2014, we submitted a system which is built with
Moses phrase-based model .
For system training,
we use all provided
German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models , a 9gram Operation Sequence Model and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to
produce the ﬁnal outputs.
Our phrase-based systems are tuned with k-best
MIRA on development
set. We set the maximum iteration to be 25.
The Language Models in our systems are
trained with SRILM . We trained
Filtered Out (%)
Monolingual (English)
Table 1: Results of language detection: percentage
of ﬁltered out sentences
a 5-gram model with Kneser-Ney discounting
 .
In the next sections, we will describe our system
in detail. In section 2, we will explain our preprocessing steps on corpus. Then in section 3, we
will describe some techniques we have tried for
this task and the experiment results. In section 4,
our ﬁnal conﬁguration for submitted system will
be presented. And we conclude in the last section.
Pre-processing
We use all the training data for German-English
translation, including Europarl, News Commentary and Common Crawl. The ﬁrst thing we noticed is that some Non-German and Non-English
sentences are included in our training data. So we
apply Language Detection for both
monolingual and bilingual corpora.
For monolingual data (only including English sentences in
our task), we ﬁlter out sentences which are detected as other language with probability more than
And for bilingual data, A sentence
pair is ﬁltered out if the language detector detects a different language with probability more than
0.999995 on either the source or the target. The
ﬁltering results are given in Table 1.
In our experiment, German compound words are splitted based on frequency . In addition, for both monolingual
and bilingual data, we apply tokenization, normalizing punctuation and truecasing using Moses
scripts. For parallel training data, we also ﬁlter out
sentence pairs containing more than 80 tokens on
either side and sentence pairs whose length ratio
between source and target side is larger than 3.
Techniques
In our preliminary experiments, we take newstest
2013 as our test data and newstest 2008-2012 as
our development data.
In total, we have more
than 10,000 sentences for tuning. The tuning step
would be very time-consuming if we use them all. So in this section, we use Feature Decay Algorithm (FDA) to select
2000 sentences as our development set. Table 2
shows that system performance does not increase
with larger tuning set and the system using only
2K sentences selected by FDA is better than the
baseline tuned with all the development data.
In this section, alignment model is trained
by MGIZA++ with
grow-diag-final-and heuristic function.
And other settings are mostly default values in
Lexicalized Reordering Model
German and English have different word order
which brings a challenge in German-English machine translation. In our system, we adopt three
Lexicalized Reordering Models (LRMs) for addressing this problem. They are word-based LRM
(wLRM), phrase-based LRM (pLRM) and hierarchal LRM (hLRM).
These three models have different effect on the
translation. Word-based and phrase-based LRMs
are focus on local reordering phenomenon, while
hierarchical LRM could be applied into longer reordering problem. Figure 1 shows the differences
 . And Table 3 shows
effectiveness of different LRMs.
wbe-msd-bidirectional-fe,
phrase-msd-bidirectional-fe
hier-mslr-bidirectional-fe to specify
these three LRMs. From Table 2, we could see
that LRMs signiﬁcantly improves the translation.
Occurrence of a swap according to
the three orientation models: word-based, phrasebased, and hierarchical. Black squares represent word alignments, and gray squares represent blocks identiﬁed by phrase-extract. In (a), block
bi = (ei, fai) is recognized as a swap according to
all three models. In (b), bi is not recognized as a
swap by the word-based model. In (c), bi is recognized as a swap only by the hierarchical model.
 
Operation Sequence Model
The Operation Sequence Model (OSM) explains the translation procedure as
a linear sequence of operations which generates
source and target sentences in parallel. Durrani
et al. deﬁned four translation operations:
Generate(X,Y), Continue Source Concept, Generate Source Only (X) and Generate Identical, as
well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward. These operations are described as follows.
• Generate(X,Y) make the words in Y and the
ﬁrst word in X added to target and source
string respectively.
• Continue Source Concept adds the word in
the queue from Generate(X,Y) to the source
• Generate Source Only (X) puts X in the
source string at the current position.
• Generate Identical generates the same word
for both sides.
• Insert Gap inserts a gap in the source side for
future use.
• Jump Back (W) makes the position for translation be the Wth closest gap to the current
• Jump Forward moves the position to the index after the right-most source word.
Tuning Set
newstest 2013
+LM Interpolation
+Factored Model
+Sparse Feature
+TM Combination
+OSM Interpolation
Table 2: Preliminary results on tuning set and test set . All scores on test set are casesensitive BLEU[%] scores. And scores on tuning set are case-insensitive BLEU[%] directly from tuning
result. Baseline uses all the data from newstest 2008-2012 for tuning.
Tuning Set (uncased)
newstest 2013
Baseline+FDA
Table 3: System BLEU[%] scores when different LRMs are adopted.
The probability of an operation sequence O =
(o1o2 · · · oJ) is:
p(oj|oj−n+1 · · · oj−1)
where n indicates the number of previous operations used.
In this paper we train a 9-gram OSM on training data and integrate this model directly into loglinear framework (OSM is now available to use
in Moses). Our experiment shows OSM improves
our system by about 0.8 BLEU (see Table 2).
Language Model Interpolation
In our baseline, Language Model (LM) is trained
on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword ﬁfth edition
(only taking partial data with size of 1.6G), English side of UN corpus and English side of 109
French-English corpus.
Instead of training a single model on all data, we interpolate language
models trained on each subset by tuning
weights to minimize perplexity of language model
measured on the target side of development set.
In our experiment, after interpolation, the language model doesn’t get a much lower perplexity,
but it slightly improves the system, as shown in
Other Tries
In addition to the techniques mentioned above, we
also try some other approaches. Unfortunately all of these methods described in this section are
non-effective in our experiments. The results are
shown in Table 2.
• Factored Model :
We tried to integrate a target POS factored
model into our system with a 9-gram POS
language model to address the problem of
word selection and word order.
But experiment doesn’t show improvement.
English POS is from Stanford POS Tagger
 .
• Translation Model Combination: In this experiment, we try to use the method of to combine phrase tables or reordering tables from different subsets of data
to minimize perplexity measured on development set. We try to split the training data in
two ways. One is according to data source,
resulting in three subsets: Europarl, News
Commentary and Common Crawl. Another
one is to use data selection. We use FDA to
select 200K sentence pairs as in-domain data
and the rest as out-domain data. Unfortunately both experiments failed. In Table 2, we only report results of phrase table combination
on FDA-based data sets.
• OSM Interpolation: Since OSM in our system could be taken as a special language
model, we try to use the idea of interpolation
similar with language model to make OSM
adapted to some data. Training data are splitted into two subsets with FDA. We train
9-gram OSM on each subsets and interpolate
them according to OSM trained on the development set.
• Sparse Features:
For each source phrase,
there is usually more than one corresponding
translation option. Each different translation
may be optimal in different contexts. Thus
in our systems, similar to 
which proposed a Maximum Entropy-based
rule selection for the hierarchical phrasebased model, features which describe the
context of phrases, are designed to select the
right translation. But different with , we use sparse features to model the context.
And instead of using syntactic POS, we adopt independent POS-like
features: cluster ID of word. In our experiment mkcls was used to cluster words into 50
groups. And all features are generalized to
cluster ID.
Submission
Based on our preliminary experiments in the section above, we use LRMs, OSM and LM interpolation in our ﬁnal system for newstest 2014.
But as we ﬁnd that Language Models trained on
UN corpus and 109 French-English corpus have
a very high perplexity and in order to speed up
the translation by reducing the model size, in this
section, we interpolate only three language models from monolingual data provided, English Gigaword ﬁfth edition and target side of training data.
In addition, we also try some different methods for
ﬁnal submission. And the results are shown in Table 4.
• Development Set Selection: Instead of using
FDA which is dependent on test set, we use
the method of to select tuning set from newstest 2008-2013 for
the ﬁnal system. We only keep 2K sentences
which have more than 30 words and higher
BLEU score. The experiment result is shown
in Table 4 ( The system is indicated as Baseline).
• Pre-processing:
In our preliminary experiments,
sentences are tokenized without
changing hyphen. Thus we build another system where all the hyphens are tokenized aggressively.
• SyMGIZA++: Better alignment could lead to
better translation. So we carry out some experiments on SyMGIZA++ aligner , which modiﬁes the
original IBM/GIZA++ word alignment models to allow to update the symmetrized models between chosen iterations of the original
training algorithms. Experiment shows this
new alignment improves translation quality.
• Multi-alignment Selection: We also try to use
multi-alignment selection 
to generate a ”better” alignment from three
alignmens: MGIZA++ with function growdiag-ﬁnal-and, SyMGIZA++ with function
grow-diag-ﬁnal-and and fast alignment . Although this method show
comparable or better result on development
set, it fails on test set.
Since we build a few systems with different
setting on Moses phrase-based model, a straightforward thinking is to obtain the better translation from several different translation systems. So
we use system combination on the 1-best outputs of three systems (indicated with ∗in table 4). And this results in our
best system so far, as shown in Table 4. In our ﬁnal
submission, this result is taken as primary.
Conclusion
This paper describes our submitted system to
WMT 2014 in detail.
This system is based on
Tuning Set
newstest 2014
+SyMGIZA++∗
+Multi-Alignment Selection
+Hyphen-Splitted
+SyMGIZA++∗
+Multi-Alignment Selection
System Combination
Table 4: Experiment results on newstest 2014. We report case-sensitive BLEU[%] score on test set and
case-insensitive BLEU[%] on tuning set which is directly from tuning result. Baseline is the phrase-based
system with LRMs, OSM and LM interpolation on smaller datasets, tuned with selected development set.
Systems indicated with ∗are used for system combination.
Moses phrase-based model, and integrates Lexicalized Reordering Models, Operation Sequence
Model and Language Model interpolation.
Also system combination is used on several systems which have different pre-processing and alignment.
Acknowledgments
This work is supported by EC Marie-Curie initial
training Network EXPERT (EXPloiting Empirical appRoaches to Translation) project (http:
//expert-itn.eu). Thanks to Johannes Leveling for his help on German compound splitting.
And thanks to Jia Xu and Jian Zhang for their advice and help on this paper and experiments.