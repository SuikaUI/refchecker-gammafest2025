Supervised Contrastive Learning
Prannay Khosla ∗
Google Research
Piotr Teterwak ∗†
Boston University
Chen Wang †
Aaron Sarna ‡
Google Research
Yonglong Tian †
Phillip Isola †
Aaron Maschinot
Google Research
Google Research
Dilip Krishnan
Google Research
Contrastive learning applied to self-supervised representation learning has seen
a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches
subsume or signiﬁcantly outperform traditional contrastive losses such as triplet,
max-margin and the N-pairs loss. In this work, we extend the self-supervised
batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class
are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the
supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture.
We show consistent outperformance over cross-entropy on other datasets and two
ResNet variants. The loss shows beneﬁts for robustness to natural corruptions,
and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement and reference TensorFlow
code is released at 1.
Introduction
Figure 1: Our SupCon loss consistently outperforms cross-entropy with standard data augmentations. We show top-1 accuracy for the ImageNet
dataset, on ResNet-50, ResNet-101 and ResNet-
200, and compare against AutoAugment , RandAugment and CutMix .
The cross-entropy loss is the most widely used loss
function for supervised learning of deep classiﬁcation models.
A number of works have explored
shortcomings of this loss, such as lack of robustness
to noisy labels and the possibility of poor
margins , leading to reduced generalization
performance. However, in practice, most proposed
alternatives have not worked better for large-scale
datasets, such as ImageNet , as evidenced by the
continued use of cross-entropy to achieve state of the
art results .
∗Equal contribution.
†Work done while at Google Research.
‡Corresponding author: 
1PyTorch implementation: 
34th Conference on Neural Information Processing Systems , Vancouver, Canada.
 
Figure 2: Supervised vs. self-supervised contrastive losses: The self-supervised contrastive loss (left, Eq. 1)
contrasts a single positive for each anchor (i.e., an augmented version of the same image) against a set of
negatives consisting of the entire remainder of the batch. The supervised contrastive loss (right) considered
in this paper (Eq. 2), however, contrasts the set of all samples from the same class as positives against the
negatives from the remainder of the batch. As demonstrated by the photo of the black and white puppy, taking
class label information into account results in an embedding space where elements of the same class are more
closely aligned than in the self-supervised case.
In recent years, a resurgence of work in contrastive learning has led to major advances in selfsupervised representation learning . The common idea in these works is the
following: pull together an anchor and a “positive” sample in embedding space, and push apart the
anchor from many “negative” samples. Since no labels are available, a positive pair often consists of
data augmentations of the sample, and negative pairs are formed by the anchor and randomly chosen
samples from the minibatch. This is depicted in Fig. 2 (left). In , connections are made of
the contrastive loss to maximization of mutual information between different views of the data.
In this work, we propose a loss for supervised learning that builds on the contrastive self-supervised
literature by leveraging label information. Normalized embeddings from the same class are pulled
closer together than embeddings from different classes. Our technical novelty in this work is to
consider many positives per anchor in addition to many negatives (as opposed to self-supervised
contrastive learning which uses only a single positive). These positives are drawn from samples
of the same class as the anchor, rather than being data augmentations of the anchor, as done in
self-supervised learning. While this is a simple extension to the self-supervised setup, it is nonobvious how to setup the loss function correctly, and we analyze two alternatives. Fig. 2 (right) and
Fig. 1 (Supplementary) provide a visual explanation of our proposed loss. Our loss can be seen as
a generalization of both the triplet and N-pair losses ; the former uses only one positive
and one negative sample per anchor, and the latter uses one positive and many negatives. The use of
many positives and many negatives for each anchor allows us to achieve state of the art performance
without the need for hard negative mining, which can be difﬁcult to tune properly. To the best of
our knowledge, this is the ﬁrst contrastive loss to consistently perform better than cross-entropy on
large-scale classiﬁcation problems. Furthermore, it provides a unifying loss function that can be
used for either self-supervised or supervised learning.
Our resulting loss, SupCon, is simple to implement and stable to train, as our empirical results show.
It achieves excellent top-1 accuracy on the ImageNet dataset on the ResNet-50 and ResNet-200
architectures . On ResNet-200 , we achieve a top-1 accuracy of 81.4%, which is a 0.8%
improvement over the state of the art cross-entropy loss on the same architecture (see Fig. 1).
The gain in top-1 accuracy is accompanied by increased robustness as measured on the ImageNet-C
dataset . Our main contributions are summarized below:
1. We propose a novel extension to the contrastive loss function that allows for multiple positives
per anchor, thus adapting contrastive learning to the fully supervised setting. Analytically and
empirically, we show that a na¨ıve extension performs much worse than our proposed version.
2. We show that our loss provides consistent boosts in top-1 accuracy for a number of datasets. It is
also more robust to natural corruptions.
3. We demonstrate analytically that the gradient of our loss function encourages learning from hard
positives and hard negatives.
4. We show empirically that our loss is less sensitive than cross-entropy to a range of hyperparameters.
Related Work
Our work draws on existing literature in self-supervised representation learning, metric learning
and supervised learning. Here we focus on the most relevant papers. The cross-entropy loss was
introduced as a powerful loss function to train deep networks . The key idea is simple
and intuitive: each class is assigned a target (usually 1-hot) vector. However, it is unclear why
these target labels should be the optimal ones and some work has tried to identify better target label
vectors, e.g. . A number of papers have studied other drawbacks of the cross-entropy loss,
such as sensitivity to noisy labels , presence of adversarial examples , and poor
margins . Alternative losses have been proposed, but the most effective ideas in practice have
been approaches that change the reference label distribution, such as label smoothing , data
augmentations such as Mixup and CutMix , and knowledge distillation .
Powerful self-supervised representation learning approaches based on deep learning models have
recently been developed in the natural language domain . In the image domain, pixelpredictive approaches have also been used to learn embeddings . These methods
try to predict missing parts of the input signal. However, a more effective approach has been to
replace a dense per-pixel predictive loss, with a loss in lower-dimensional representation space. The
state of the art family of models for self-supervised representation learning using this paradigm are
collected under the umbrella of contrastive learning . In these works,
the losses are inspired by noise contrastive estimation or N-pair losses . Typically, the
loss is applied at the last layer of a deep network. At test time, the embeddings from a previous
layer are utilized for downstream transfer tasks, ﬁne tuning or direct retrieval tasks. introduces
the approximation of only back-propagating through part of the loss, and also the approximation of
using stale representations in the form of a memory bank.
Closely related to contrastive learning is the family of losses based on metric distance learning or
triplets . These losses have been used to learn powerful representations, often in supervised settings, where labels are used to guide the choice of positive and negative pairs. The key
distinction between triplet losses and contrastive losses is the number of positive and negative pairs
per data point; triplet losses use exactly one positive and one negative pair per anchor. In the supervised metric learning setting, the positive pair is chosen from the same class and the negative pair
is chosen from other classes, nearly always requiring hard-negative mining for good performance
 . Self-supervised contrastive losses similarly use just one positive pair for each anchor sample,
selected using either co-occurrence or data augmentation . The major difference is
that many negative pairs are used for each anchor. These are usually chosen uniformly at random
using some form of weak knowledge, such as patches from other images, or frames from other randomly chosen videos, relying on the assumption that this approach yields a very low probability of
false negatives.
Resembling our supervised contrastive approach is the soft-nearest neighbors loss introduced in 
and used in . Like , we improve upon by normalizing the embeddings and replacing
euclidean distance with inner products. We further improve on by the increased use of data
augmentation, a disposable contrastive head and two-stage training (contrastive followed by crossentropy), and crucially, changing the form of the loss function to signiﬁcantly improve results (see
Section 3). also uses a closely related loss formulation to ours to entangle representations at
intermediate layers by maximizing the loss. Most similar to our method is the Compact Clustering
via Label Propagation (CCLP) regularizer in Kamnitsas et. al. . While CCLP focuses mostly
on the semi-supervised case, in the fully supervised case the regularizer reduces to almost exactly
our loss formulation. Important practical differences include our normalization of the contrastive
embedding onto the unit sphere, tuning of a temperature parameter in the contrastive objective, and
stronger augmentation. Additionally, Kamnitsas et. al. use the contrastive embedding as an input to
a classiﬁcation head, which is trained jointly with the CCLP regularizer, while SupCon employs a
two stage training and discards the contrastive head. Lastly, the scale of experiments in Kamnitsas
et. al. is much smaller than in this work. Merging the ﬁndings of our paper and CCLP is a promising
direction for semi-supervised learning research.
Our method is structurally similar to that used in for self-supervised contrastive learning,
with modiﬁcations for supervised classiﬁcation. Given an input batch of data, we ﬁrst apply data
augmentation twice to obtain two copies of the batch. Both copies are forward propagated through
the encoder network to obtain a 2048-dimensional normalized embedding. During training, this
representation is further propagated through a projection network that is discarded at inference time.
The supervised contrastive loss is computed on the outputs of the projection network. To use the
trained model for classiﬁcation, we train a linear classiﬁer on top of the frozen representations using
a cross-entropy loss. Fig. 1 in the Supplementary material provides a visual explanation.
Representation Learning Framework
The main components of our framework are:
• Data Augmentation module, Aug(·). For each input sample, x, we generate two random augmentations, ˜x = Aug(x), each of which represents a different view of the data and contains some
subset of the information in the original sample. Sec. 4 gives details of the augmentations.
• Encoder Network, Enc(·), which maps x to a representation vector, r = Enc(x) ∈RDE. Both
augmented samples are separately input to the same encoder, resulting in a pair of representation
vectors. r is normalized to the unit hypersphere in RDE . Consistent with the ﬁndings of , our analysis and experiments show that this
normalization improves top-1 accuracy.
• Projection Network, Proj(·), which maps r to a vector z = Proj(r) ∈RDP . We instantiate
Proj(·) as either a multi-layer perceptron with a single hidden layer of size 2048 and output
vector of size DP = 128 or just a single linear layer of size DP = 128; we leave to future work
the investigation of optimal Proj(·) architectures. We again normalize the output of this network
to lie on the unit hypersphere, which enables using an inner product to measure distances in the
projection space. As in self-supervised contrastive learning , we discard Proj(·) at the end
of contrastive training. As a result, our inference-time models contain exactly the same number of
parameters as a cross-entropy model using the same encoder, Enc(·).
Contrastive Loss Functions
Given this framework, we now look at the family of contrastive losses, starting from the selfsupervised domain and analyzing the options for adapting it to the supervised domain, showing that
one formulation is superior. For a set of N randomly sampled sample/label pairs, {xk, yk}k=1...N,
the corresponding batch used for training consists of 2N pairs, {˜xℓ, ˜yℓ}ℓ=1...2N, where ˜x2k and
˜x2k−1 are two random augmentations (a.k.a., “views”) of xk (k = 1...N) and ˜y2k−1 = ˜y2k = yk.
For the remainder of this paper, we will refer to a set of N samples as a “batch” and the set of 2N
augmented samples as a “multiviewed batch”.
Self-Supervised Contrastive Loss
Within a multiviewed batch, let i ∈I ≡{1...2N} be the index of an arbitrary augmented sample,
and let j(i) be the index of the other augmented sample originating from the same source sample.
In self-supervised contrastive learning (e.g., ), the loss takes the following form.
 zi • zj(i)/τ
exp (zi • za/τ)
Here, zℓ= Proj(Enc(˜xℓ)) ∈RDP , the • symbol denotes the inner (dot) product, τ ∈R+ is a
scalar temperature parameter, and A(i) ≡I \ {i}. The index i is called the anchor, index j(i) is
called the positive, and the other 2(N −1) indices ({k ∈A(i) \ {j(i)}) are called the negatives.
Note that for each anchor i, there is 1 positive pair and 2N −2 negative pairs. The denominator has
a total of 2N −1 terms (the positive and negatives).
Supervised Contrastive Losses
For supervised learning, the contrastive loss in Eq. 1 is incapable of handling the case where, due to
the presence of labels, more than one sample is known to belong to the same class. Generalization
to an arbitrary numbers of positives, though, leads to a choice between multiple possible functions.
Eqs. 2 and 3 present the two most straightforward ways to generalize Eq. 1 to incorporate supervision.
exp (zi • zp/τ)
exp (zi • za/τ)
exp (zi • zp/τ)
exp (zi • za/τ)
Here, P(i) ≡{p ∈A(i) : ˜yp = ˜yi} is the set of indices of all positives in the multiviewed batch
distinct from i, and |P(i)| is its cardinality. In Eq. 2, the summation over positives is located outside
of the log (Lsup
out) while in Eq. 3, the summation is located inside of the log (Lsup
in ). Both losses have
the following desirable properties:
• Generalization to an arbitrary number of positives. The major structural change of Eqs. 2
and 3 over Eq. 1 is that now, for any anchor, all positives in a multiviewed batch (i.e., the
augmentation-based sample as well as any of the remaining samples with the same label) contribute to the numerator. For randomly-generated batches whose size is large with respect to the
number of classes, multiple additional terms will be present (on average, N/C, where C is the
number of classes). The supervised losses encourage the encoder to give closely aligned representations to all entries from the same class, resulting in a more robust clustering of the representation
space than that generated from Eq. 1, as is supported by our experiments in Sec. 4.
• Contrastive power increases with more negatives. Eqs. 2 and 3 both preserve the summation
over negatives in the contrastive denominator of Eq. 1. This form is largely motivated by noise
contrastive estimation and N-pair losses , wherein the ability to discriminate between
signal and noise (negatives) is improved by adding more examples of negatives. This property is
important for representation learning via self-supervised contrastive learning, with many papers
showing increased performance with increasing number of negatives .
• Intrinsic ability to perform hard positive/negative mining. When used with normalized representations, the loss in Eq. 1 induces a gradient structure that gives rise to implicit hard positive/negative mining. The gradient contributions from hard positives/negatives (i.e., ones against
which continuing to contrast the anchor greatly beneﬁts the encoder) are large while those for easy
positives/negatives (i.e., ones against which continuing to contrast the anchor only weakly beneﬁts
the encoder) are small. Furthermore, for hard positives, the effect increases (asymptotically) as
the number of negatives does. Eqs. 2 and 3 both preserve this useful property and generalize it
to all positives. This implicit property allows the contrastive loss to sidestep the need for explicit
hard mining, which is a delicate but critical part of many losses, such as triplet loss . We note
that this implicit property applies to both supervised and self-supervised contrastive losses, but our
derivation is the ﬁrst to clearly show this property. We provide a full derivation of this property
from the loss gradient in the Supplementary material.
Table 1: ImageNet Top-1 classiﬁcation
accuracy for supervised contrastive
losses on ResNet-50 for a batch size of
The two loss formulations are not, however, equivalent. Because log is a concave function, Jensen’s Inequality implies that Lsup
out. One would thus expect Lsup
the superior supervised loss function (since it upper-bounds
in ). This conclusion is also supported analytically. Table 1
compares the ImageNet top-1 classiﬁcation accuracy using
out and Lsup
in for different batch sizes (N) on the ResNet-50
 architecture. The Lsup
out supervised loss achieves signiﬁcantly higher performance than Lsup
in . We conjecture that this is due to the gradient of Lsup
structure less optimal for training than that of Lsup
out. For Lsup
out, the positives normalization factor
(i.e., 1/|P(i)|) serves to remove bias present in the positives in a multiviewed batch contributing to
the loss. However, though Lsup
also contains the same normalization factor, it is located inside of
the log. It thus contributes only an additive constant to the overall loss, which does not affect the
gradient. Without any normalization effects, the gradients of Lsup
are more susceptible to bias in
the positives, leading to sub-optimal training.
An analysis of the gradients themselves supports this conclusion. As shown in the Supplementary,
the gradient for either Lsup
out,i or Lsup
in,i with respect to the embedding zi has the following form.
zp(Pip −Xip) +
Here, N(i) ≡{n ∈A(i) : ˜yn ̸= ˜yi} is the set of indices of all negatives in the multiviewed batch,
and Pix ≡exp (zi • zx/τ) / P
a∈A(i) exp (zi • za/τ). The difference between the gradients for the
two losses is in Xip.
exp(zi•zp/τ)
exp(zi•zp′/τ)
If each zp is set to the (less biased) mean positive representation vector, z, Xin
ip reduces to Xout
exp (zi • z/τ)
exp (zi • z/τ) =
exp (zi • z/τ)
|P(i)| · exp (zi • z/τ) =
|P(i)| = Xout
From the form of ∂Lsup
/∂zi, we conclude that the stabilization due to using the mean of positives
beneﬁts training. Throughout the rest of the paper, we consider only Lsup
Connection to Triplet Loss and N-pairs Loss
Supervised contrastive learning is closely related to the triplet loss , one of the widely-used loss
functions for supervised learning. In the Supplementary, we show that the triplet loss is a special
case of the contrastive loss when one positive and one negative are used. When more than one
negative is used, we show that the SupCon loss becomes equivalent to the N-pairs loss .
Experiments
We evaluate our SupCon loss (Lsup
out, Eq. 2) by measuring classiﬁcation accuracy on a number
of common image classiﬁcation benchmarks including CIFAR-10 and CIFAR-100 and ImageNet . We also benchmark our ImageNet models on robustness to common image corruptions
 and show how performance varies with changes to hyperparameters and reduced data. For
the encoder network (Enc(·)) we experimented with three commonly used encoder architectures:
ResNet-50, ResNet-101, and ResNet-200 . The normalized activations of the ﬁnal pooling
layer are used as the representation vector. We experimented with four different
implementations of the Aug(·) data augmentation module: AutoAugment ; RandAugment ;
SimAugment , and Stacked RandAugment (see details of our SimAugment and Stacked
RandAugment implementations in the Supplementary). AutoAugment outperforms all other data
augmentation strategies on ResNet-50 for both SupCon and cross-entropy. Stacked RandAugment
performed best for ResNet-200 for both loss functions. We provide more details in the Supplementary.
Classiﬁcation Accuracy
Table 2 shows that SupCon generalizes better than cross-entropy, margin classiﬁers (with use of
labels) and unsupervised contrastive learning techniques on CIFAR-10, CIFAR-100 and ImageNet
datasets. Table 3 shows results for ResNet-50 and ResNet-200 (we use ResNet-v1 ) for ImageNet. We achieve a new state of the art accuracy of 78.7% on ResNet-50 with AutoAugment (for
Cross-Entropy
Max-Margin 
Table 2: Top-1 classiﬁcation accuracy on ResNet-50 for various datasets. We compare cross-entropy
training, unsupervised representation learning (SimCLR ), max-margin classiﬁers and SupCon (ours).
We re-implemented and tuned hyperparameters for all baseline numbers except margin classiﬁers where we
report published results. Note that the CIFAR-10 and CIFAR-100 results are from our PyTorch implementation
and ImageNet from our TensorFlow implementation.
Architecture
Augmentation
Cross-Entropy (baseline)
MixUp 
Cross-Entropy (baseline)
CutMix 
Cross-Entropy (baseline)
AutoAugment 
Cross-Entropy (our impl.)
AutoAugment 
AutoAugment 
Cross-Entropy (baseline)
ResNet-200
AutoAugment 
Cross-Entropy (our impl.)
ResNet-200
Stacked RandAugment 
ResNet-200
Stacked RandAugment 
ResNet-101
Stacked RandAugment 
Table 3: Top-1/Top-5 accuracy results on ImageNet for AutoAugment with ResNet-50 and for Stacked
RandAugment with ResNet-101 and ResNet-200. The baseline numbers are taken from the referenced
papers, and we also re-implement cross-entropy.
comparison, a number of the other top-performing methods are shown in Fig. 1). Note that we also
achieve a slight improvement over CutMix , which is considered to be a state of the art data
augmentation strategy. Incorporating data augmentation strategies such as CutMix and MixUp
 into contrastive learning could potentially improve results further.
We also experimented with memory based alternatives . On ImageNet, with a memory size
of 8192 (requiring only the storage of 128-dimensional vectors), a batch size of 256, and SGD
optimizer, running on 8 Nvidia V100 GPUs, SupCon is able to achieve 79.1% top-1 accuracy on
ResNet-50. This is in fact slightly better than the 78.7% accuracy with 6144 batch size (and no
memory); and with signiﬁcantly reduced compute and memory footprint.
Since SupCon uses 2 views per sample, its batch sizes are effectively twice the cross-entropy equivalent. We therefore also experimented with the cross-entropy ResNet-50 baselines using a batch size
of 12,288. These only achieved 77.5% top-1 accuracy. We additionally experimented with increasing the number of training epochs for cross-entropy all the way to 1400, but this actually decreased
accuracy (77.0%).
We tested the N-pairs loss in our framework with a batch size of 6144. N-pairs achieves only
57.4% top-1 accuracy on ImageNet. We believe this is due to multiple factors missing from N-pairs
loss compared to supervised contrastive: the use of multiple views; lower temperature; and many
more positives. We show some results of the impact of the number of positives per anchor in the
Supplementary (Sec. 6), and the N-pairs result is inline with them. We also note that the original
N-pairs paper has already shown the outperformance of N-pairs loss to triplet loss.
Robustness to Image Corruptions and Reduced Training Data
Deep neural networks lack robustness to out of distribution data or natural corruptions such as noise,
blur and JPEG compression. The benchmark ImageNet-C dataset is used to measure trained
model performance on such corruptions. In Fig. 3(left), we compare the supervised contrastive
models to cross-entropy using the Mean Corruption Error (mCE) and Relative Mean Corruption Error metrics . Both metrics measure average degradation in performance compared to ImageNet
test set, averaged over all possible corruptions and severity levels. Relative mCE is a better metric
when we compare models with different Top-1 accuracy, while mCE is a better measure of absolute
robustness to corruptions. The SupCon models have lower mCE values across different corruptions,
Architecture
rel. mCE mCE
Cross-Entropy
AlexNet 
(baselines)
VGG-19+BN 
ResNet-18 
Cross-Entropy
(our implementation)
ResNet-200
Supervised Contrastive
ResNet-200
Figure 3: Training with supervised contrastive loss makes models more robust to corruptions in images. Left:
Robustness as measured by Mean Corruption Error (mCE) and relative mCE over the ImageNet-C dataset
 (lower is better). Right: Mean Accuracy as a function of corruption severity averaged over all various
corruptions. (higher is better).
Figure 4: Accuracy of cross-entropy and supervised contrastive loss as a function of hyperparameters and
training data size, all measured on ImageNet with a ResNet-50 encoder. (From left to right) (a): Standard
boxplot showing Top-1 accuracy vs changes in augmentation, optimizer and learning rates. (b): Top-1 accuracy
as a function of batch size shows both losses beneﬁt from larger batch sizes while Supervised Contrastive has
higher Top-1 accuracy even when trained with smaller batch sizes. (c): Top-1 accuracy as a function of SupCon
pretraining epochs. (d): Top-1 accuracy as a function of temperature during pretraining stage for SupCon.
Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD
Caltech-101 Flowers Mean
SimCLR-50 88.20
73.20 89.20
76.86 92.35
74.60 93.47
76.76 93.48
SupCon-200
74.26 93.12
Table 4: Transfer learning results. Numbers are mAP for VOC2007 ; mean-per-class accuracy for Aircraft,
Pets, Caltech, and Flowers; and top-1 accuracy for all other datasets.
showing increased robustness. We also see from Fig. 3(right) that SupCon models demonstrate
lesser degradation in accuracy with increasing corruption severity.
Hyperparameter Stability
We experimented with hyperparameter stability by changing augmentations, optimizers and learning
rates one at a time from the best combination for each of the methodologies. In Fig. 4(a), we
compare the top-1 accuracy of SupCon loss against cross-entropy across changes in augmentations
(RandAugment , AutoAugment , SimAugment , Stacked RandAugment ); optimizers
(LARS, SGD with Momentum and RMSProp); and learning rates. We observe signiﬁcantly lower
variance in the output of the contrastive loss. Note that batch sizes for cross-entropy and supervised
contrastive are the same, thus ruling out any batch-size effects. In Fig. 4(b), sweeping batch size
and holding all other hyperparameters constant results in consistently better top-1 accuracy of the
supervised contrastive loss.
Transfer Learning
We evaluate the learned representation for ﬁne-tuning on 12 natural image datasets, following the
protocol in Chen et.al. . SupCon is on par with cross-entropy and self-supervised contrastive loss
on transfer learning performance when trained on the same architecture (Table 4). Our results are
consistent with the ﬁndings in and : while better ImageNet models are correlated with better
transfer performance, the dominant factor is architecture. Understanding the connection between
training objective and transfer performance is left to future work.
Training Details
The SupCon loss was trained for 700 epochs during pretraining for ResNet-200 and 350 epochs for
smaller models. Fig. 4(c) shows accuracy as a function of SupCon training epochs for a ResNet50,
demonstrating that even 200 epochs is likely sufﬁcient for most purposes.
An (optional) additional step of training a linear classiﬁer is used to compute top-1 accuracy. This is
not needed if the purpose is to use representations for transfer learning tasks or retrieval. The second
stage needs as few as 10 epochs of additional training. Note that in practice the linear classiﬁer
can be trained jointly with the encoder and projection networks by blocking gradient propagation
from the linear classiﬁer back to the encoder, and achieve roughly the same results without requiring
two-stage training. We chose not to do that here to help isolate the effects of the SupCon loss.
We trained our models with batch sizes of up to 6144, although batch sizes of 2048 sufﬁce for most
purposes for both SupCon and cross-entropy losses (as shown in Fig. 4(b)). We associate some of
the performance increase with batch size to the effect on the gradient due to hard positives increasing
with an increasing number of negatives (see the Supplementary for details). We report metrics for
experiments with batch size 6144 for ResNet-50 and batch size 4096 for ResNet-200 (due to the
larger network size, a smaller batch size is necessary). We observed that for a ﬁxed batch size it was
possible to train with SupCon using larger learning rates than what was required by cross-entropy to
achieve similar performance.
All our results used a temperature of τ = 0.1. Smaller temperature beneﬁts training more than
higher ones, but extremely low temperatures are harder to train due to numerical instability. Fig.
4(d) shows the effect of temperature on Top-1 performance of supervised contrastive learning. As
we can see from Eq. 4, the gradient scales inversely with choice of temperature τ; therefore we
rescale the loss by τ during training for stability.
We experimented with standard optimizers such as LARS , RMSProp and SGD
with momentum in different permutations for the initial pre-training step and training of the dense layer.
While SGD with momentum works best for training ResNets with
cross-entropy, we get the best performance for SupCon on ImageNet by using LARS for
pre-training and RMSProp to training the linear layer.
For CIFAR10 and CIFAR100 SGD
with momentum performed best.
Additional results for combinations of optimizers are
provided in the Supplementary.
Reference code is released at