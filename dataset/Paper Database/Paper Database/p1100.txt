Machine learning & artiﬁcial intelligence in the quantum domain
Vedran Dunjko
Institute for Theoretical Physics, University of Innsbruck, Innsbruck 6020, Austria
Max Planck Institute of Quantum Optics, Garching 85748, Germany
Email: 
Hans J. Briegel
Institute for Theoretical Physics, University of Innsbruck Innsbruck 6020, Austria
Department of Philosophy, University of Konstanz, Konstanz 78457, Germany
Email: 
Abstract. Quantum information technologies, on the one side, and intelligent learning
systems, on the other, are both emergent technologies that will likely have a transforming
impact on our society in the future. The respective underlying ﬁelds of basic research –
quantum information (QI) versus machine learning and artiﬁcial intelligence (AI) – have
their own speciﬁc questions and challenges, which have hitherto been investigated largely
independently. However, in a growing body of recent work, researchers have been probing the question to what extent these ﬁelds can indeed learn and beneﬁt from each other.
QML explores the interaction between quantum computing and machine learning, investigating how results and techniques from one ﬁeld can be used to solve the problems of
the other. In recent time, we have witnessed signiﬁcant breakthroughs in both directions
of inﬂuence. For instance, quantum computing is ﬁnding a vital application in providing
speed-ups for machine learning problems, critical in our “big data” world. Conversely,
machine learning already permeates many cutting-edge technologies, and may become
instrumental in advanced quantum technologies. Aside from quantum speed-up in data
analysis, or classical machine learning optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning
tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works
exploring the use of artiﬁcial intelligence for the very design of quantum experiments,
and for performing parts of genuine research autonomously, have reported their ﬁrst
successes. Beyond the topics of mutual enhancement – exploring what ML/AI can do
for quantum physics, and vice versa – researchers have also broached the fundamental
issue of quantum generalizations of learning and AI concepts. This deals with questions
of the very meaning of learning and intelligence in a world that is fully described by
quantum mechanics. In this review, we describe the main ideas, recent developments,
and progress in a broad spectrum of research investigating machine learning and artiﬁcial
intelligence in the quantum domain.
I. Introduction
A. Quantum mechanics, computation and information processing
B. Artiﬁcial intelligence and machine learning
1. Learning from data: machine learning
2. Learning from interaction: reinforcement learning
3. Intermediary learning settings
4. Putting it all together: the agent-environment paradigm
C. Miscellanea
 
II. Classical background
A. Methods of machine learning
1. Artiﬁcial neural networks and deep learning
2. Support Vector Machines
3. Other models
B. Mathematical theories of supervised and inductive learning
1. Computational learning theory
2. VC theory
C. Basic methods and theory of reinforcement learning
III. Quantum mechanics, learning, and AI
IV. Machine learning applied to (quantum) physics
A. Hamiltonian estimation and metrology
1. Hamiltonian estimation
2. Phase estimation settings
3. Generalized Hamiltonian estimation settings
B. Design of target evolutions
1. Oﬀ-line design
2. On-line design
C. Controlling quantum experiments, and machine-assisted research
1. Controlling complex processes
2. Learning how to experiment
D. Machine learning in condensed-matter and many-body physics
V. Quantum generalizations of machine learning concepts
A. Quantum generalizations: machine learning of quantum data
1. State discrimination, state classiﬁcation, and machine learning of quantum data
2. Computational learning perspectives: quantum states as concepts
B. (Quantum) learning and quantum processes
VI. Quantum enhancements for machine learning
A. Learning eﬃciency improvements: sample complexity
1. Quantum PAC learning
2. Learning from membership queries
B. Improvements in learning capacity
1. Capacity from amplitude encoding
2. Capacity via quantized Hopﬁeld networks
C. Run-time improvements: computational complexity
1. Speed-up via adiabatic optimization
2. Speed-ups in circuit architectures
VII. Quantum learning agents, and elements of quantum AI
A. Quantum learning via interaction
B. Quantum agent-environment paradigm for reinforcement learning
1. AE-based classiﬁcation of quantum ML
C. Towards quantum artiﬁcial intelligence
VIII. Outlook
Acknowledgements
References
I. INTRODUCTION
Quantum theory has inﬂuenced most branches of physical sciences. This inﬂuence ranges from minor
corrections, to profound overhauls, particularly in ﬁelds dealing with suﬃciently small scales. In
the second half of the last century, it became apparent that genuine quantum eﬀects can also be
exploited in engineering-type tasks, where such eﬀects enable features which are superior to those
achievable using purely classical systems. The ﬁrst wave of such engineering gave us, for example,
the laser, transistors, and nuclear magnetic resonance devices. The second wave, which gained
momentum in the ’80s, constitutes a broad-scale, albeit not fully systematic, investigation of the
potential of utilizing quantum eﬀects for various types of tasks which, at the base of it, deal with
the processing of information. This includes the research areas of cryptography, computing, sensing
and metrology, all of which now share the common language of quantum information science. Often,
the research into such interdisciplinary programs was exceptionally fruitful. For instance, quantum
computation, communication, cryptography and metrology are now mature, well-established and
impactful research ﬁelds which have, arguably, revolutionized the way we think about information
and its processing. In recent years, it has become apparent that the exchange of ideas between
quantum information processing and the ﬁelds of artiﬁcial intelligence and machine learning has
its own genuine questions and promises. Although such lines of research are only now receiving a
broader recognition, the very ﬁrst ideas were present already at the early days of QC, and we have
made an eﬀort to fairly acknowledge such visionary works.
In this review we aim to capture research at the interplay between machine learning, artiﬁcial
intelligence and quantum mechanics in its broad scope, with a reader with a physics background in
mind. To this end, we dedicate comparatively large amount of space to classical machine learning
and artiﬁcial intelligence topics, which are often sacriﬁced in physics-oriented literature, while
keeping the quantum information aspects concise.
The structure of the paper is as follows. In the remainder of this introductory section I, we give
quick overviews of the relevant basic concepts of the ﬁelds quantum information processing, and of
machine learning and artiﬁcial intelligence. We ﬁnish oﬀthe introduction with a glossary of useful
terms, list of abbreviations, and comments on notation. Subsequently, in section II we delve deeper
into chosen methods, technical details, and the theoretical background of the classical theories.
The selection of topics here is not necessarily balanced, from a classical perspective. We place
emphasis on elements which either appear in subsequent quantum proposals, which can sometimes
be somewhat exotic, or on aspects which can help put the relevance of the quantum results into
proper context. Section III brieﬂy summarizes the topics covered in the quantum part of the review.
Sections IV - VII cover the four main topics we survey, and constitute the central body of the paper.
We ﬁnish with a an outlook section VIII.
Remark: The overall objective of this survey is to give a broad, “birds-eye” account of the
topics which contribute to the development of various aspects of the interplay between quantum
information sciences, and machine learning and artiﬁcial intelligence. Consequently, this survey
does not necessarily present all the developments in a fully balanced fashion. Certain topics, which
are in their very early stages of investigation, yet important for the nascent research area, were
given perhaps a disproportional level of attention, compared to more developed themes. This is, for
instance, particularly evident in section VII, which aims to address the topics of quantum artiﬁcial
intelligence, beyond mainstream data analysis applications of machine learning. This topic is relevant
for a broad perspective on the emerging ﬁeld, however it has only been broached by very few authors,
works, including the authors of this review and collaborators. The more extensively explored topics
of, e.g., quantum algorithms for machine learning and data mining, quantum computational learning
theory, or quantum neural networks, have been addressed in more focused recent reviews the pure state of a quantum system is given by a unit vector |ψ⟩in a complex Hilbert
space, 2) closed system pure state evolution is generated by a Hamiltonian H, speciﬁed by the linear
Schr¨odinger equation H |ψ⟩= iℏ∂
∂t |ψ⟩, 3) the structure of composite systems is given by the tensor
product, and 4) projective measurements (observables) are speciﬁed by, ideally, non-degenerate
Hermitian operators, and the measurement process changes the description of the observed system
from state |ψ⟩to an eigenstate |φ⟩, with probability given by the Born rule p(φ) = |⟨ψ |φ⟩|2 . While the full theory still requires the handling of subsystems and classical
ignorance1, already the few mathematical axioms of pure state closed system theory give rise to many
quintessentially quantum phenomena, like superpositions, no-cloning, entanglement, and others,
most of which stem from just the linearity of the theory. Many of these properties re-deﬁne how
researchers in quantum information perceive what information is, but also have a critical functional
role in say quantum enhanced cryptography, communication, sensing and other applications. One
of the most fascinating consequences of quantum theory are, arguably, captured by the ﬁeld of
quantum information processing (QIP), and in particular quantum computing (QC), which is most
relevant for our purposes.
QC has revolutionized the theories and implementations of computation. This ﬁeld originated from
the observations by Manin and Feynman that the calculation of
certain properties of quantum systems, as they evolve in time, may be intractable, while the quantum
systems themselves, in a manner of speaking, do perform that hard computation by merely evolving.
Since these early ideas, QC has proliferated, and indeed the existence of quantum advantages which
1 This requires more general and richer formalism of density operators, and leads to generalized measurements,
completely positive evolutions, etc.
are oﬀered by scalable universal quantum computers have been demonstrated in many settings.
Perhaps most famously, quantum computers have been shown to have the capacity to eﬃciently
solve algebraic computational problems, which are believed to be intractable for classical computers.
This includes the famous problems of factoring large integers computing the discrete logarithms
 , but also many others such as Pell equation solving, some non-Abelian hidden subgroup
problems, and others, see e.g. for a review. Related
to this, nowadays we also have access to a growing collection of quantum algorithms2 for various
linear algebra tasks, as given in e.g. , which may oﬀer speed-ups.
Processing block 1
Processing block 2
Processing block 3
Processing block k
FIG. 1 Oracular computation and
query complexity: a (quantum) algorithm solves a problem by intermittently calling a black-box subroutine, deﬁned only via its inputoutput relations. Query complexity
of an algorithm is the number of
calls to the oracle, the algorithm
will perform.
Quantum computers can also oﬀer improvements in many optimization and simulation tasks, for instance, computing certain properties
of partition functions , simulated annealing , solving semideﬁnite programs
 , performing approximate optimization
 , and, naturally, in the tasks of simulating quantum systems .
Advantages can also be achieved in terms of the eﬃcient use of
sub-routines and databases. This is studied using oracular models
of computation, where the quantity of interest is the number of calls
to an oracle, a black-box object with a well-deﬁned set of inputoutput relations which, abstractly, stands in for a database, subroutine, or any other information processing resource. The canonical
example of a quantum advantage in this setting is the Grover’s search
 algorithm which achieves the, provably optimal,
quadratic improvement in unordered search (where the oracle is
the database). Similar results have been achieved in a plethora
of other scenarios, such as spatial search , search over structures ),
NAND and more general boolean tree evaluation
problems , as well as more recent “cheat sheet” technique results leading to better-than-quadratic improvements. Taken a bit more broadly, oracular
models of computation can also be used to model communication tasks, where the goal is to
reduce communication complexity (i.e. the number of communication rounds) for some information
exchange protocols . Quantum computers can also be used for solving sampling
problems. In sampling problems the task is to produce a sample according to an (implicitly) deﬁned
distribution, and they are important for both optimization and (certain instances of) algebraic
For instance, Markov Chain Monte Carlo methods, arguably the most proliﬁc set of computational
methods in natural sciences, are designed to solve sampling tasks, which in turn, can be often
2 In this review it makes sense to point out that the term “quantum algorithm” is a bit of a misnomer, as what
we really mean is “an algorithm for a quantum computer”. An algorithm – an abstraction – cannot per se be
“quantum”, and the term quantum algorithm could also have meant e.g.“algorithm for describing or simulating
quantum processes”. Nonetheless, this term, in the sense of “algorithm for a quantum computer” is commonplace in
QIP, and we use it in this sense as well. The concept of “quantum machine learning” is, however, still ambiguous in
this sense, and depending on the authors, can easily mean “quantum algorithm for ML“, or “ML applied to QIP”.
3 Optimization and computation tasks can be trivially regarded as special cases of sampling tasks, where the target
distribution is (suﬃciently) localized at the solution.
used to solve other types of problems. For instance, in statistical physics, the capacity to sample
from Gibbs distributions is often the key tool to compute properties of the partition function. A
broad class of quantum approaches to sampling problems focuses on quantum enhancements of
such Markov Chain methods . Sampling tasks
have been receiving an ever increasing amount of attention in the QIP community, as we will
comment on shortly. Quantum computers are typically formalized in one of a few standard models
of computation, many of which are, computationally speaking, equally powerful4. Even if the models
are computationally equivalent, they are conceptually diﬀerent. Consequently, some are better
suited, or more natural, for a given class of applications. Historically, the ﬁrst formal model, the
quantum Turing machine , was preferred for theoretical and computability-related
considerations. The quantum circuit model is standard for algebraic
problems. The measurement-based quantum computing (MBQC) model is, arguably, best-suited for graph-related problems ,
multi-party tasks and distributed computation and blind quantum
computation . Topological quantum computation 
was an inspiration for certain knot-theoretic algorithms , and is closely
related to algorithms for topological error-correction and fault tolerance. The adiabatic quantum
computation model is constructed with the task of ground-state preparation in
mind, and is thus well-suited for optimization problems .
List of models
applications
(BQP-complete)
(not exlusive)
algorithms
distributed computing
Topological
knot-theoretic problems
optimization problems
List of models
applications
(restricted)
computing trace of unitary
Linear Optics
Shallow Random Q. Circuits sampling
Commuting Q. Circuits
RestrictedAdiabatic
optimization tasks
FIG. 2 Computational models
Research into QIP also produced examples of
interesting restricted models of computation:
models which are in all likelihood not universal for eﬃcient QC, however can still solve
tasks which seem hard for classical machines.
Recently, there has been an increasing interest in such models, speciﬁcally the linear
optics model, the so-called low-depth random
circuits model and the commuting quantum
circuits model5. In it was shown that the linear optics
model can eﬃciently produce samples from
a distribution speciﬁed by the permanents of
certain matrices, and it was proven (barring
certain plausible mathematical conjectures)
that classical computers cannot reproduce
the samples from the same distribution in
polynomial time. Similar claims have been made for low-depth random circuits and commuting quantum circuits, which comprise only commuting gates
 . Critically, these restricted models can be
4 Various notions of “equally powerful” are usually expressed in terms of algorithmic reductions. In QIP, typically,
the computational model B is said to be at least as powerful as the computational model A, if any algorithm
of complexity O(f(n)) (where f(n) is some scaling function, e.g. “polynomial” or “exponential”), deﬁned for
model A, can be eﬃciently (usually this means in polynomial time) translated to an algorithm for B, which solves
the same problem, and whose computational complexity is O(poly(f(n))). Two models are then equivalent if A
is as powerful as B and B is as powerful as A. Which speciﬁc reduction complexity we care about (polynomial,
linear, etc.) depends on the setting: e.g. for factoring polynomial reductions suﬃce, since there seems to be an
exponential separation between classical and quantum computation. In contrast, for search, the reductions need to
be sub-quadratic to maintain a quantum speed-up, since only a quadratic improvement is achievable.
5 Other restricted models exist, such as the one clean qubit model (DQC1) where the input comprises only one qubit
in a pure state, and others are maximally mixed. This model can be used to compute a function – the normalized
trace of a unitary speciﬁed by a quantum circuit – which seems to be hard for classical devices.
realized to suﬃcient size, as to allow for a demonstration of computations which the most powerful
classical computers that are currently available cannot achieve, with near-term technologies. This
milestone, referred to as quantum supremacy , and has been getting
a signiﬁcant amount of attention in recent times. Another highly active ﬁeld in QIP concentrates on
(analogue) quantum simulations, with applications in quantum optics, condensed matter systems,
and quantum many-body physics . Many, if not most of the above mentioned
aspects of quantum computation are ﬁnding a role in quantum machine learning applications.
Next, we brieﬂy review basic concepts from the classical theories of artiﬁcial intelligence and machine
B. Artiﬁcial intelligence and machine learning
Executive summary: The ﬁeld of artiﬁcial intelligence incorporates various methods, which
are predominantly focused on solving problems which are hard for computers, yet seemingly easy for humans. Perhaps the most important class of such tasks pertain to learning
problems. Various algorithmic aspects of learning problems are tackled by the ﬁeld of machine
learning, which evolved from the study of pattern recognition in the context of AI. Modern
machine learning addresses a variety of learning scenarios, dealing with learning from data, e.g.
supervised (data classiﬁcation), and unsupervised (data clustering) learning, or from interaction,
e.g. reinforcement learning. Modern AI states, as its ultimate goal, the design of an intelligent
agent which learns and thrives in unknown environments. Artiﬁcial agents that are intelligent in
a general, human sense must have the capacity to tackle all the individual problems addressed by
machine learning and other more specialized branches of AI. They will consequently require a
complex combination of techniques.
In its broadest scope, the modern ﬁeld of artiﬁcial intelligence (AI) encompasses a wide variety
of sub-ﬁelds. Most of these sub-ﬁelds deal with the understanding and abstracting of aspects of
various human capacities which we would describe as intelligent, and attempt to realize the same
capacities in machines. The term “AI” was coined at Dartmouth College conferences in the 1956
 , which were organized to develop ideas about machines that can think,
and the conferences are often cited as the birthplace of the ﬁeld. The conferences were aimed to
“ﬁnd how to make machines use language, form abstractions and concepts, solve kinds of problems
now reserved for humans, and improve themselves” 6. The history of the ﬁeld has been turbulent,
with strong opinions on how AI should be achieved. For instance, over the course of its ﬁrst 30
years, the ﬁeld has crystalized into two main competing and opposite viewpoints on how AI may be realized: computationalism – holding that that the mind functions
by performing purely formal operations on symbols, in the manner of a Turing machine, see e.g.
 ), and connectionism – which models mental and behavioral phenomena
as the emergent processes of interconnected networks of simple units, mimicking the biological
brain, see e.g. ). Aspects of these two viewpoints still inﬂuence approaches to AI.
Irrespective of the underlying philosophy, for the larger part of the history of AI, the realization of
“genuine AI” was, purportedly perpetually “a few years away” – a feature often attributed also to
6 Paraphrased from .
quantum computers by critics of the ﬁeld. In the case of AI, such runaway optimism had a severe
calamitous eﬀect on the ﬁeld, in multiple instances, especially in the context of funding (leading
to periods now dubbed “winters of AI”). By the late 90s, the reputation of the ﬁeld was low, and,
even in hindsight, there was no consensus on the reasons why AI failed to produce human-level
intelligence. Such factors played a vital role in the fragmentation of the ﬁeld into various sub-ﬁelds
which focused on specialized tasks, often appearing under diﬀerent names.
A particularly inﬂuential perspective of AI, often called nouvelle or embodied AI, was advocated
by Brooks, which posited that intelligence emerges from (simple) embodied systems which learn
through interaction with their environments . In contrast to standard approaches
of the time, Nouvelle AI insists on learning, rather than having properties pre-programmed, and
on the embodiment of AI entities, as opposed to abstract entities like chess playing programs.
To a physicist, this perspective that intelligence is embodied is reminiscent to the viewpoint that
information is physical, which had been “the rallying cry of quantum information theory” . Such embodied approaches are particularly relevant in robotics where the key issues involve
perception (the capacity of the machine to interpret the external world using its sensors, which
includes computer vision, machine hearing and touch), motion and navigation (critical in e.g.
automated cars). Related to human-computer interfaces, AI also incorporates the ﬁeld of natural
language processing which includes language understanding – the capacity of the machine to
derive meaning from natural language, and language generation – the ability of the machine to
convey information in a natural language.
TSP example: ﬁnding the
shortest route visiting the largest cities
in Germany.
Other general aspects of AI pertain to a few well-studied capacities of intelligent entities . For
instance, automated planning is related to decision theory7
and, broadly speaking, addresses the task of identifying strategies (i.e. sequences of actions) which need to be performed in
order to achieve a goal, while minimizing (a speciﬁed) cost.
Already the simple class of so-called oﬀ-line planning tasks,
where the task, cost function, and the set of possible actions are
known beforehand, contains genuinely hard problems, e.g. it
include, as a special case, the NP-complete8 travelling salesman
problem (TSP); for illustration see Fig. 3 9 .
In modern times, TSP itself would no longer be considered a
genuine AI problem, but it is serves to illustrate how already
very specialized, simple sub-sub-tasks of AI may be hard. More
general planning problems also include on-line variants, where
not everything is known beforehand (e.g. TSP but where the
“map” may fail to include all the available roads, and one simply
has to actually travel to ﬁnd good strategies). On-line planning
overlaps with reinforcement learning, discussed later in this
section. Closely related to planning is the capacity of intelligent
entities for problem solving. In technical literature, problems
7 Not to be confused with decision problems, studied in algorithmic complexity.
8 Roughly speaking, NP is the class of decision (yes, no) problems whose solutions can be eﬃciently veriﬁed by a
classical computer in polynomial time. NP-complete problems are the hardest problems in NP in the sense that
any other NP problem can be reduced to an NP complete problem via polynomial-time reductions. Note that the
exact solutions to NP-compete problems are believed to be intractable even for quantum computers.
9 Figure 3 has been modiﬁed from 
solving is distinguished from planning by a lack of additional structure in the problem, usually
assumed in planning – in other words, problem solving is more general and typically more broadly
deﬁned than planning. The lack of structure in general problem solving establishes a clear connection
to (also unstructured) searching and optimization: in the setting of no additional information or
structure, problem solving is the search for the solution to a precisely speciﬁed problem. While
general problem solving can be, theoretically, achieved by a general search algorithm (which can
still be subdivided into classes such as depth-ﬁrst, breath-ﬁrst, depth-limited search etc.), more
often there is structure to the problem, in which case an informed search strategies – often called
heuristic search strategies – will be more eﬃcient . Human intelligence,
to no small extent, relies on our knowledge. We can accumulate knowledge, reason over it, and use
it to come to the best decisions, for instance in the context of problem solving and planning. An
aspect of AI tries to formalize such logical reasoning, knowledge accumulation and knowledge
representation, often relying on formal logic, most often ﬁrst order logic.
A particularly important class of problems central to AI, and related to knowledge acquisition,
involve the capacity of the machine to learn through experience. This feature was emphasized
already in the early days of AI, and the derived ﬁeld of machine learning (ML) now stands as
arguably the most successful aspect (or spin-oﬀ) of AI, which we will address in more detail.
1. Learning from data: machine learning
Linear classifier
Supervised learning
Unsupervised learning
Supervised (in this case, best linear classiﬁer) and unsupervised learning (here clustering into two most likely groups
and outliers) illustrated.
Stemming from the traditions of pattern recognition, such as recognizing
handwritten text, and statistical learning theory (which places ML ideas
in a rigorous mathematical framework), ML, broadly speaking, explores the construction of algorithms
that can learn from, and make predictions about data.
Traditionally,
ML deals with two main learning settings: supervised and unsupervised
learning, which are closely related to
data analysis and data mining-type
tasks . A broader perspective on the ﬁeld also includes reinforcement learning , which is closely
related to learning as is realized by biological intelligent entities. We shall discuss reinforcement
learning separately.
In broad terms, supervised learning deals with learning-by-example: given a certain number of
labeled points (so-called training set) {(xi, yi)}i where xi denote data points, e.g. N−dimensional
vectors, and yi denote labels (e.g. binary variables, or real values), the task is to infer a “labeling
rule” xi 7→yi which allows us to guess the labels of previously unseen data, that is, beyond the
training set. Formally speaking, we deal with the task of inferring the conditional probability
distribution P(Y = y|X = x) (more speciﬁcally, generating a labeling function which, perhaps
probabilistically, assigns labels to points) based on a certain number of samples from the joint
distribution P(X, Y ). For example, we could be inferring whether a particular DNA sequence belongs
to an individual who is likely to develop diabetes. Such an inference can be based on the datasets
of patients whose DNA sequences had been recorded, along with the information on whether they
actually developed diabetes. In this example, the variable Y (diabetes status) is binary, and the
assignment of labels is not deterministic, as diabetes also depends on environmental factors. Another
example could include two real variables, where x is the height from which an object is dropped,
and y the duration of the fall. In this example, both variables are real-valued, and (in vacuum)
the labeling relation will be essentially deterministic. In unsupervised learning, the algorithm is
provided just with the data points without labels. Broadly speaking, the goal here is to identify
the underlying distribution, or structure, and other informative features in the dataset. In other
words, the task is to infer properties of the distribution P(X = x), based on a certain number of
samples, relative to a user-speciﬁed guideline or rule. Standard examples of unsupervised learning
are clustering tasks, where data-points are supposed be grouped in a manner which minimizes
within-group mean-distance, while maximizing the distance between the groups. Note that the
group membership can be thought of as a label, thus this also corresponds to a labeling task, but
lacks “supervision”: examples of correct labelings. In basic examples of such tasks, the number of
expected clusters is given by the user, but this too can be automatically optimized.
Other types of unsupervised problems include feature extraction and dimensionality reduction,
critical in combatting the so-called curse of dimensionality. The curse of dimensionality refers
to problems which stem from the fact that the raw representations of real-life data often occupy
very high dimensional spaces. For instance, a standard resolution one-second video-clip at standard
refresh frequency, capturing events which are extended in time maps to a vector in ∼108 dimensional
space10, even though the relevant information it carries (say a licence-plate number of a speeding
car ﬁlmed) may be signiﬁcantly smaller. More generally, intuitively it is clear that, since geometric
volume scales exponentially with the dimension of the space it is in, the number of points needed
to capture (or learn) general features of an n−dimensional object will also scale exponentially.
In other words, learning in high dimensional spaces is exponentially diﬃcult. Hence, a means of
dimensionality reduction, from raw representation space (e.g. moving car clips), to the relevant
feature space (e.g. licence-plate numbers) is a necessity in any real-life scenario.
These approaches the data-points to a space of signiﬁcantly reduced dimension, while attempting
to maintain the main features – the relevant information – of the structure of the data. A typical
example of a dimensionality example technique is e.g. principal component analysis. In practice, such
algorithms also constitute an important step in data pre-processing for other types of learning and
analysis. Furthermore, this setting also includes generative models (related to density estimation),
where new samples from an unknown distribution are generated, based on few exact samples.
As humanity is amassing data at an exponential rate it becomes
ever more relevant to extract genuinely useful information in an automated fashion. In modern
world ubiquitous big data analysis and data mining are the central applications of supervised and
unsupervised learning.
10 Each frame is cca. 106 dimensional, as each pixel constitutes one dimension, multiplied with 30 frames required for
the one-second clip.
2. Learning from interaction: reinforcement learning
Reinforcement learning (RL) is, traditionally, the
third canonical category of ML. Partially caused by the relatively recent prevalence of (un)supervised
methods in the contexts of the pervasive data mining and big data analysis topics, many modern
textbooks on ML focus on these methods. RL strategies have mostly remained reserved for robotics
and AI communities. Lately, however, the surge of interest in adaptive and autonomous devices,
robotics, and AI have increased the prominence of RL methods.
One recent celebrated result which relies on the extensive use of standard ML and RL techniques
in conjunction is that of AlphaGo , a learning system which mastered the
game of Go, and achieved, arguably, superhuman performance, easily defeating the best human
players. This result is notable for multiple reasons, including the fact that it illustrates the potential of learning machines over special-purpose solvers in the context of AI problems: while
specialized devices which relied on programming over learning (such as Deep Blue) could surpass human performance in chess, they failed to do the same for the more complicated game of
Go, which has a notably larger space of strategies. The learning system AlphaGo achieved this
many years ahead of typical predictions. The distinction between RL and other data-learning
ML methods is particularly relevant from a quantum information perspective, which will be addressed in more detail in section VII.B. RL constitutes a broad learning setting, formulated within
the general agent-environment paradigm (AE paradigm) of AI .
Here, we do not deal with a static database, but rather an interactive task environment. The
learning agent (or, a learning algorithm) learns through the interaction with the task environment.
Environment
Environment
FIG. 5 An agent interacts with an environment by exchanging percepts and actions. In
RL rewards can be issued. Basic environments
are formalized by Markov Decision Processes
(inset in Environment). Environments are reminiscent to oracles, see 1, in that the agent only
has access to the input-output relations. Further, ﬁgures of merit for learning often count
the number of interaction steps, which is analogous to the concept of query complexity.
As an illustration, one can imagine a robot, acting on
its environment, and perceiving it via its sensors – the
percepts being, say, snapshots made by its visual system, and actions being, say, movements of the robot –
as depicted in Fig. 5 . The AE formalism is, however,
more general and abstract. It is also unrestrictive as it
can also express supervised and unsupervised settings.
In RL, it is typically assumed that the goal of the process is manifest in a reward function, which, roughly
speaking, rewards the agent, whenever the agents behavior was correct (in which case we are dealing with
positive reinforcement, but other variants of operant
conditioning are also used11). This model of learning
seems to cover pretty well how most biological agents
(i.e. animals) learn: one can illustrate this through
the process of training a dog to do a trick by giving it
treats whenever it performs well. As mentioned earlier,
RL is all about learning how to perform the “correct”
sequence of actions, given the received percepts, which is an aspect of planning, in a setting which is
fully on-line: the only way to learn about the environment is by interacting with it.
11 More generally, we can distinguish four modes of such operant conditioning: positive reinforcement (reward when
correct), negative reinforcement (removal of negative reward when correct), positive punishment (negative reward
when incorrect) and negative punishment (removal of reward when incorrect).
3. Intermediary learning settings
While supervised, unsupervised and reinforcement learning constitute the three broad categories
of learning, there are many variations and intermediary settings. For instance, semi-supervised
learning interpolates between unsupervised and supervised settings, where the number of labeled
instances is very small compared to the total available training set. Nonetheless, even a small number
of labeled examples have been shown to improve the bare unsupervised performance , or, from an opposite perspective, unlabeled data can help with classiﬁcation when facing
a small quantity of labeled examples. In active supervised learning, the learning algorithm can
further query the human user, or supervisor, for the labels of particular points which would improve
the algorithm’s performance. This setting can only be realized when it is operatively possible
for the user to correctly label all the points, and may yield advantages when this exact labeling
process is expensive. Further, in supervised settings, one can consider so-called inductive learning
algorithms which output a classiﬁer function, based on the training data, which can be used to
label all possible points. A classiﬁer is simply a function which assigns labels to the points in the
domain of the data. In contrast, in transductive learning settings, the
points that need to be labeled later are known beforehand – in other words, the classiﬁer function is
only required to be deﬁned on a-priori known points. Next, a supervised algorithm can perform lazy
learning, meaning that the whole labeled dataset is kept in memory in order to label unknown
points (which can then be added), or eager learning, in which case, the (total) classiﬁer function
is output (and the training set is no longer explicitly required) . Typical examples
of eager learning are linear classiﬁers, such as basic support vector machines, described in the next
section, whereas lazy learning is exempliﬁed by e.g. nearest-neighbour methods12 . Our last example,
online learning , can be understood as either an extension of eager supervised
learning, or a special case of RL. Online learning generalizes standard supervised learning, in the
sense that the training data is provided sequentially to the learner, and used to, incrementally,
update the classifying function. In some variants, the algorithm is asked to classify each point,
and is given the correct response afterward, and the performance is based on the guesses. The
match/mismatch of the guess and the actual label can also be understood as a reward, in which
case online learning becomes a restricted case of RL.
4. Putting it all together: the agent-environment paradigm
The aforementioned specialized learning scenarios can be phrased in a unifying language, which also
enables us to discuss how specialized tasks ﬁt in the objective of realizing true AI.
In modern take on AI , the central concept of the theory is that of an
agent. An agent is an entity which is deﬁned relative to its environment, and which has the capacity
to act, that is, do something.
In computer science terminology the requirements for something to be an agent (or for something to
act) are minimal, and essentially everything can be considered an agent – for instance, all non-trivial
computer programs are also agents.
12 For example, in k−nearest neighbour classiﬁcation, the training set is split into disjoint subsets speciﬁed by the
shared labels. Given a new point which is to be classiﬁed, the algorithm identiﬁes k nearest neighbour points from
the data set to the new point. The label of the new point is decided by the majority label of these neighbours. The
labeling process thus needs to refer to the entire training set.
AI concerns itself with agents which do more – for instance they also perceive their environment,
interact with it, and learn from experience. AI is nowadays deﬁned13 as the ﬁeld which is aimed at
designing intelligent agents , which are autonomous, perceive their world
using sensors, act on it using actuators, and choose their activities as to achieve certain goals – a
property which is also called rationality in literature.
Environment
FIG. 6 Basic agent-environment
Agents only exist relative to an environment (more speciﬁcally a
task environment), with which they interact, constituting the overall
AE paradigm, illustrated in Fig. 6. While it is convenient to picture
robots when thinking about agents, they can also be more abstract
and virtual, as is the case with computer programs “living” in the
internet14. In this sense, any learning algorithm for any of the
more specialized learning settings can also be viewed as a restricted
learning agent, operating in a special type of an environment, e.g. a
supervised learning environment may be deﬁned by a training phase,
where the environment produces examples for the learning agent,
followed by a testing phase, where the environment evaluates the
agent, and ﬁnally the application phase, where the trained and veriﬁed model is actually used. The
same also obviously holds for more interactive learning scenarios such as the reinforcement-driven
mode of learning – RL – we brieﬂy illustrated in section I.B.2, is natively phrased in the AE
paradigm. In other words, all machine learning models and settings can be phrased within the broad
AE paradigm.
Although the ﬁeld of AI is fragmented into research branches with focus on isolated, speciﬁc goals,
the ultimate motivation of the ﬁeld remains the same: the design of true, general AI, sometimes
referred to as artiﬁcial general intelligence (AGI)15, that is, the design of a “truly intelligent” agent
 .
The topic of what ingredients are needed to build AGI is diﬃcult, and without a consensus.
One perspective focuses on the behavioral aspects of agents. In literature, many features of intelligent
behavior are captured by characterizing more speciﬁc types of agents: simple reﬂex agents, modelbased reﬂex agents, goal-based agents, utility-based agents, etc. Each type captures an aspect of
intelligent behavior, much like the fragments of the ﬁeld of ML, understood as a subﬁeld of AI,
capture speciﬁc types of problems intelligent agents should handle. For our purposes, the most
important, overarching aspect of intelligent agents is the capacity to learn16, and we will emphasize
learning agents in particular.
The AE paradigm is particularly well suited for such an operational perspective, as it abstracts from
the internal structure of agents, and focuses on behavior and input-output relations.
More precisely, the perspective on AI presented in this review is relatively simple. a) AI pertains
to agents which behave intelligently in their environments, and, b) the central aspect of intelligent
behaviour is that of learning.
13 Over the course of its history, AI had many deﬁnitions, many of which invoke the notion of an agent, while some
older, deﬁnitions talk about machines, or programs which “think”, “have minds” and so on (Russell and Norvig,
As clariﬁed, the ﬁeld of AI has fragmented, and many of the sub-ﬁelds deal with speciﬁc computational problems,
and the development of computational methodologies useful in AI related problems, for instance ML (i.e. its
supervised and unsupervised variants). In such sub-ﬁelds with a more pragmatic computational perspective, the
notion of agents is not used as often.
14 The subtle topics of such virtual, yet embodied agents is touched again later in section VII.A.
15 The ﬁeld of AGI, under this label, emerged in mid 2000s, and the term is used to distinguish the objective of
realizing intelligent agents from the research focusing more specialized tasks, which are nowadays all labeled AI.
AGI is also referred to as strong AI, or, sometimes full AI.
16 A similar viewpoint, that essentially all AI problems/features map to a learning scenario, is also advocated in
 .
While we, unsurprisingly, do not more precisely specify what intelligent behaviour entails, already
this simple perspective on AI has non-trivial consequences. The ﬁrst is that intelligence can be
ascertained from the interaction history between the agent and its environment alone. Such a
viewpoint on AI is also closely related to behavior-based AI and the ideas behind the Turing test
 ; it is in line with an embodied viewpoint on AI (see embodied AI in section I.B) and
it has inﬂuenced certain approaches towards quantum AI, touched in section VII.C. The second is
that the development of better ML and other types of relevant algorithms does constitute genuine
progress towards AI, conditioned only on the fact that such algorithms can be coherently combined
into a whole agent. It is however important to note that to actually achieve this integration may be
far from trivial.
In contrast to such strictly behavioral and operational points of view, an alternative approach
towards whole agents (or complete intelligent agents) focuses on agent architectures and cognitive
architectures . In this approach to AI the emphasis is equally placed not
only on intelligent behaviour, but also on forming a theory about the structure of the (human) mind.
One of the main goals of a cognitive architecture is to design a comprehensive computational model
which encapsulates various results stemming from research in cognitive psychology. The aspects
which are predominantly focused on understanding human cognition are, however, not central for
our take on AI.
We discuss this further in section VII.C.
C. Miscellanea
a. Abbreviations and acronyms
First occurrence
AE paradigm agent-environment paradigm
artiﬁcial general intelligence
artiﬁcial intelligence
artiﬁcial neural network
Bayesian experimental design
Boltzmann machine
bounded-error quantum polynomial time
content-addressable memory
computational learning theory
density matrix exponentiation
one clean qubit model
Hopﬁeld network
measurement-based quantum computation
Markov decision process
machine learning
neural network
non-deterministic polynomial time
PAC learning probably approximately correct learning
principal component analysis
partially observable Markov decision process II.C
projective simulation
quantum computation
quantum information processing
quadratic unconstrained binary optimization VI.C.1
reinforcement learning
reﬂective PS
support vector machine
b. Notation
Throughout this review paper, we have strived to use the notation speciﬁed in the
reviewed works. To avoid a notational chaos, we, however keep the notation consistent within
subsections – this means that, within one subsection, we adhere to the notation used in the majority
of works if inconsistencies arise.
II. CLASSICAL BACKGROUND
The main purpose of this section is to provide the background regarding classical ML and AI
techniques and concepts which are either addressed in quantum proposals we discuss in the following
sections or important for the proper positioning of the quantum proposal in the broader learning
context. The concepts and models of this section include common models found in classical literature,
but also certain more exotic models, which have been addressed in modern quantum ML literature.
While this section contains most of the classical background needed to understand the basic ideas of
the quantum ML literature, to tame the length of this section, certain very specialized classical ML
ideas are presented on-the-ﬂy during the upcoming reviews.
We ﬁrst provide the basics concepts related to common ML models, emphasizing neural networks in
II.A.1 and support vector machines in II.A.2. Following this, in II.A.3, we also brieﬂy describe a
larger collection of algorithmic methods, and ideas arising in the context of ML, including regression
models, k−means/medians, decision trees, but also more general optimization and linear algebra
methods which are now commonplace in ML. Beyond the more pragmatic aspects of model design for
learning problems, in subsection II.B we provide the main ideas of the mathematical foundations of
computational learning theory, which discuss learnability – i.e. the conditions under which learning
is possible at all – computational learning theory and the theory of Vapnik and Chervonenkis – which
rigorously investigate the bounds on learning eﬃciency for various supervised settings. Subsection
II.C covers the basic concepts and methods of RL.
A. Methods of machine learning
Executive summary: Two particularly famous models in machine learning are artiﬁcial
neural networks – inspired by biological brains, and support vector machines – arguably
the best understood supervised learning model. Neural networks come in many ﬂavours, all
of which model parallel information processing of a network of simple computational units,
neurons. Feed-forward networks (without loops) are typically used for supervised learning. Most
of the popular deep learning approaches ﬁt in this paradigm. Recurrent networks have loops
– this allows e.g. feeding information from outputs of a (sub)-network back to its own input .
Examples include Hopﬁeld networks, which can be used as content-addressable memories,
and Boltzmann machines, typically used for unsupervised learning. These networks are related
Ising-type models, at zero, or ﬁnite temperatures, respectively – this sets the grounds for some of
the proposals for quantization. Support vector machines classify data in an Euclidean space, by
identifying best separating hyperplanes, which allows for a comparatively simple theory. The
linearity of this model is a feature making it amenable to quantum processing. The power of
hyperplane classiﬁcation can be improved by using kernels which, intuitively, map the data to
higher dimensional spaces, in a non-linear way. ML naturally goes beyond these two models, and
includes regression (data ﬁtting) methods and many other specialized algorithms.
Since the early days of the ﬁelds of AI and ML, there have been many proposals on how to achieve
the ﬂavours of learning we described above. In what follows we will describe two popular models for
ML, speciﬁcally artiﬁcial neural networks, and support vector machines. We highlight that many
other models exist, and indeed, in many ﬁelds other learning methods (e.g. regression methods), are
more commonly used. A selection of such other models is brieﬂy mentioned thereafter, along with
examples of techiques which overlap with ML topics in a broader sense, such as matrix decomposition
techniques, and which can be used for e.g. unsupervised learning.
Our choice of emphasis is, in part, again motivated by later quantum approaches, and by features of
the models which are particularly well-suited for cross-overs with quantum computing.
1. Artiﬁcial neural networks and deep learning
Artiﬁcial neural networks (artiﬁcial NNs, or just NNs) are a biologically inspired approach to tackling
learning problems. Originating in 1943 , the basic component of NNs
is the artiﬁcial neuron (AN), which is, abstractly speaking, a real-valued function AN : Rk →R
parametrized by a vector of real, non-negative weights (wi)i = w ∈Rk, and the activation function
φ : R →R, given with
, with x = (xi)i ∈Rk.
For the particular choice when the activation function is the threshold function φθ(x) = 1 if
x > θ ∈R+ and φθ(x) = 0 otherwise, the AN is called a perceptron , and has
been studied extensively. Already such simple perceptrons performing classiﬁcation into subspaces
speciﬁed by the hyperplane with the normal vector w, and oﬀ-set θ (c.f. support vector machines
later in this section).
Note, in ML terminology, a distinction should be made between artiﬁcial neurons (ANs) and
perceptrons – perceptrons are special cases of ANs, with the ﬁxed activation function – the step
function –, and a speciﬁed update or training rule. ANs in modern times use various activation
functions (often the diﬀerentiable sigmoid functions), and can use diﬀerent learning rules. For our
purposes, this distinction will not matter.The training of such a classiﬁer/AN for supervised learning
purposes consists in optimizing the parameters w and θ as to correctly label the training set – there
are various ﬁgures of merit particular approaches care about, and various algorithms that perform
such an optimization, which are not relevant at this point. By combining ANs in a network we
obtain NNs (if ANs are perceptrons, we usually talk about multi-layered perceptrons). While single
perceptrons, or single-layered perceptrons can realize only linear classiﬁcation, already a three-layered
network suﬃces to approximate any continuous real-valued function (precision depending on the
number of neurons in the inner, so-called hidden, layer). Cybenko was the ﬁrst
to prove this for sigmoid activation functions, whereas Hornik generalized this to show that the
same holds for all non-constant, monotonically increasing and bounded activation functions soon thereafter. This shows that if suﬃciently many neurons are available, a three-layered
ANN can be trained to learn any dataset, in principle17. Although this result seems very positive, it
comes with the price of a large model complexity, which we discuss in section II.B.218. In recent
times, it has become apparent that using multiple, sequential, hidden feed-forward layers (instead of
one large), i.e. deep neural networks (deep NNs), may have additional beneﬁts. First, they may
reduce the number of parameters . Second, the sequential nature of processing
of information from layer to layer can be understood as a feature abstraction mechanism (each layer
processes the input a bit, highlighting relevant features which are processed further). This increases
the interpretability of the model (intuitively, the capacity for high level explanations of the
model’s performance) , which is perhaps best illustrated in so-called convolutional
(deep) NNs, whose structure is inspired by the visual cortex. One of the main practical disadvantages
of such deep networks is the computational cost and computational instabilities in training ), and also the size of the dataset which
has to be large . With modern technology and datasets, both obstacles are
becoming less prohibitive, which has lead to a minor revolution in the ﬁeld of ML.
Not all ANNs are feed-forward: recurrent neural networks (recurrent NNs) allow for the
backpropagation of signals. Particular examples of such networks are so called Hopﬁeld networks
(HNs), and Boltzmann machines (BMs), which are often used for diﬀerent purposes than feedforward networks. In HNs, we deal with one layer, where the outputs of all the neurons serve as
inputs to the same layer. The network is initialized by assigning binary values (traditionally, −1
and 1 are used, for reasons of convenience) to the neurons (more precisely, some neurons are set
to ﬁre, and some not), which are then processed by the network, leading to a new conﬁguration.
This update can be synchronous (the output values are ”frozen” and all the second-round values are
computed simultaneously) or asynchronous (the update is done one neuron at a time in a random
order). The connections in the network are represented by a matrix of weights (wij)ij, specifying
the connection strength between the ith and the jth neuron. The neurons are perceptrons, with a
threshold activation function, given with the local threshold vector (θi)i. Such a dynamical system,
under a few mild assumptions , converges to a conﬁguration (i.e. bit-string) which
(locally) minimizes the energy functional
with s = (si)i, si ∈{−1, 1}, that is, the Ising model. In general, this model has many local minima,
which depend on the weights wij, and the thresholds, which are often set to zero. Hopﬁeld provided
a simple algorithm ),
which enables one to “program” the minima – in other words, given a set of bitstrings S (more
precisely, strings of signs +1/ −1), one can ﬁnd the matrix wij such that exactly those strings S are
local minima of the resulting functional E. Such programmed minima are then called stored patterns.
Furthermore, Hopﬁeld’s algorithm achieved this in a manner which is local (the weights wij depend
only on the ith and jth bits of the targeted strings, allowing parallelizability), incremental (one can
modify the matrix wij to add a new string without having to keep the old strings in memory), and
immediate. Immediateness means that the computation of the weight matrix is not a limiting, but
ﬁnite process. Violating e.g. incrementality would lead to a lazy algorithm (see section I.B.3), which
can be sub-optimal in terms of memory requirements, but often also computational complexity19. It
was shown that the minima of such a trained network are also attractive ﬁxed-points, with a ﬁnite
basin of attraction. This means that if a trained network is fed a new string, and let run, it will
(eventually) converge to a pattern which is closest to it (the distance measure that is used depends on
the learning rule, but typically it is the Hamming distance, i.e. number of entries where the strings
disagree). Such a system then forms an associative memory, also called a content-addressable
memory (CAM). CAMs can be used for supervised learning (the “labels” are the stored patterns),
and conversely, supervised learning machinery can be used for CAM20. An important feature of
HNs is their capacity: how many distinct patterns it can store21. For the Hebbian update rule this
19 The lazy algorithm may have to process all the patterns/data-points the number of which may be large and/or
20 For this, one simply needs to add a look-up table connecting labels to ﬁxed patterns.
21 Reliable storage entails that previously stored patterns will be also recovered without change (i.e they are energetic
local minima of Eq. (2), but also that there is a basin of attraction – a ball around the stored patterns with respect
to a distance measure (most commonly the Hamming distance) for which the dynamical process of the network
converges to the stored pattern. An issue with capacities is the occurrence of spurious patterns: local minima with
a non-trivial basin of attraction which were not stored.
number scales as O(n/ log(n)), where n is the number of neurons, which Storkey 
improved to O(n/
log(n)). In the meantime, more eﬃcient learning algorithms have been invented
 . Aside from applications as CAMs, due to the representation in terms of
the energy functional in Eq. (2), and the fact that the running of HNs minimize it, they have also
been considered for the tasks of optimization early on . The operative
isomorphism between Hopﬁeld networks and the Ising model, technically, holds only in the case of a
zero-temperature system. Boltzmann machines generalize this. Here, the value of the ith neuron
is set to −1 or 1 (called “oﬀ” and “on” in literature, respectively) with probability
p(i = −1) = (1 + exp (−β∆Ei))−1 , with ∆Ei =
wij sj + θi,
where ∆Ei is the energy diﬀerence of the conﬁguration with ith neuron being on or oﬀ, assuming
the connections w are symmetric, and β is the inverse temperature of the system. In the limit of
inﬁnite running time, the network’s conﬁguration is given by the (input-state invariant) Boltzmann
distribution over the conﬁgurations, which depends on the weights w, local thresholds (weights)
θ and the temperature. BMs are typically used in a generative fashions, to model, and sample
from, (conditional) probability distributions. In the simplest variant, the training of the network
attempts to ensure that the limiting distribution of the network matches the observed frequencies
in the dataset. This is achieved by the tuning of the parameters w and θ. The structure of the
network dictates how complicated a distribution can be represented. To capture more complicated
distributions, over say k dimensional data, the BMs have N > k neurons. k of them will be denoted
as visible units, and the remainder are called hidden units, and they capture latent, not directly
observable, variables of the system which generated the dataset, and which we are in fact modelling.
Training such networks consists in a gradient ascent of the log-likelihood of observing the training
data, in the parameter space. While this seems conceptually simple, it is computationally intractable,
in part as it requires accurate estimates of probabilities of equilibrium distributions, which are hard
to obtain. In practice, this is somewhat mitigated by using restricted BMs, where the hidden
and visible units form the partition of a bi-partite graph (so only connections between hidden and
visible units exist). (Restricted) BMs have a large spectrum of uses, including providing generative
models – producing new samples from the estimated distribution, as classiﬁers – via conditioned
generation, as feature extractors – a form of unsupervised clustering, and as building blocks of
deep architectures . However, their utility is mostly limited by the cost
of training – for instance, the cost of obtaining equilibrium Gibbs distributions, or by the errors
stemming from heuristic training methods such as contrastive divergence .
2. Support Vector Machines
Support Vector Machines (SVMs) form a family of perhaps best understood approaches for solving
classiﬁcation problems. The basic idea behind SVMs is that a natural way to classify points based
on a dataset {xi, yi}i, for binary labels yi ∈{−1, 1}, is to generate a hyperplane separating the
negative instances from the positive ones. Such observations are not new, and indeed, perceptrons,
brieﬂy discussed in the previous section, perform the same function.
Such a hyperplane can then be used to classify all points. Naturally, not all sets of points allow
this (those that do are called linearly separable), but SVMs are further generalized to deal with
sets which are not linearly separable using so-called kernels. Kernels, eﬀectively, realize non-linear
mappings of the original dataset to higher dimensions where they may become separable, depending
on a few technical conditions 22), and by allowing a certain degree of misclassiﬁcation, which leads
to so-called “soft-margin” SVMs.
Maximum margin
Maximum margin
hyperplane
Basic example of an SVM,
trained on a linearly separable dataset.
Even in the case the dataset is linearly separable, there will still
be many hyperplanes doing the job. This leads to various variants
of SVMs, but the basic variant identiﬁes a hyperplane which: a)
correctly splits the training points, and b) maximizes the so-called
margin: the distance of the hyperplane to the nearest point (see
The distance of choice is most often the geometric Euclidean
distance, which leads to so-called maximum margin classiﬁers.
In high-dimensional spaces, in general the maximization of the
margin ends in a situation where there are multiple +1 and −1
instances of training data points which are equally far from the
hyperplane.
These points are called support vectors.
ﬁnding of a maximum margin classiﬁer corresponds to ﬁnding a
normal vector w and oﬀset b of the separating hyperplane, which
corresponds to the optimization problem
w∗= argminw,b
such that yi(w.xi + b) ≥1.
The formulation above is actually derived from the basic problem by noting that we may arbitrarily
and simultaneously scale the pair (w, b) without changing the hyperplane. Therefore, we may always
choose a scaling such that the realized margin is 1, in which case, the margin corresponds to ∥w∥−1,
which simply maps a maximization problem to a minimization problem as above. The square
ensures the problem is stated as a standard quadratic programming problem. This problem is often
expressed in its Lagrange dual form, which reduces to
1, . . . α∗
N) = argminα1...αN
αiαjyiyjxi.xj
such that αi ≥0 and
where the solution of the original problem is given by
In other words, we have expressed w∗in the basis of the data-vectors, and the data-vectors xi for
which the corresponding coeﬃcient αi is non-zero are precisely the support vectors. The oﬀset b∗is
22 Indeed, this can be supported by hard theory, see Cover’s Theorem .
easily computed having access to one support vector of, say, an instance +1, denoted x+, by solving
w∗.x+ + b∗= 1.
The class of a new point z can also be computed directly using the support vectors via the following
expression
yiαixi.z + b∗
The dual representation of the optimization problem is convenient when dealing with kernels. As
mentioned, a way of dealing with data which is not linearly separable, is to ﬁrst map all the points into
a higher-dimensional space via a non-linear function φ : Rm →Rn, where m < n is the dimensionality
of the datapoints. As we can see, in the dual formulation, the data-points only appear in terms of
inner products xi.xj. This leads to the notion of the kernel function k which, intuitively, measures the
similarity of the points in the larger space, typically deﬁned with k(xi, xj) = φ(xi)τφ(xj). In other
words, to train the SVM according to a non-trivial kernel k, induced by the non-linear mapping φ, the
optimization line Eq. (6) will be replaced with argminα1...αN
i,j αiαjyiyjk(xi, xj)
The oﬀset is computed analogously, using just one application of φ. The evaluation of a new point
is given in the same way with z 7→sign (P
i yiαik(xi, z) + b∗) . In other words, the data-points need
not be explicitly mapped via φ, as long as the map-inducing inner product k(·, ·) can be computed
more eﬀectively. The choice of the kernel is critical in the performance of the classiﬁer, and the
ﬁnding of good kernels is non-trivial and often solved by trial-and-error.
While increasing the dimension of the extended space (co-domain of φ) may make data-points more
linearly separable (i.e. fewer mismatches for the optimal classiﬁer), in practice they will not be fully
separable (and furthermore, increasing the kernel dimension comes with a cost which we elaborate
on later). To resolve this, SVMs allow for misclassiﬁcation, with various options for measuring the
“amount” of misclassiﬁcation, inducing a penalty function. A typical approach to this is to introduce
so-called “slack variables” ξi ≥0 to the original optimization task, so:
w∗= argminw,b
such that yi(w.xi + b) ≥1 −ξi.
If the value ξi of the optimal solution is between 0 and 1, the point i is correctly classiﬁed, but
is within the margin, and ξi > 1 denotes a misclassiﬁcation. The (hyper)parameter C controls
the relative importance we place on minimizing the margin norm, versus the importance we place
on misclassiﬁcation. Interestingly, the dual formulation of the above problem is near-identical
to the hard-margin setting discussed thus far, with the small diﬀerence that the parameters αi
are now additionally constrained with αi ≤C in Eq. (7). SVMs, as described above, have been
extensively studied from the perspective of computational learning theory, and have been connected
to other learning models. In particular, their generalization performance, which, roughly speaking,
characterizes how well a trained model23 will perform beyond the training set can be analyzed.
This is the most important feature of a classifying algorithm. We will brieﬂy discuss generalization
23 In ML, the term model is often overloaded. Most often it refers to a classiﬁcation system which has been trained
on a dataset, and in that sense it “models” the actual labeling function. Often, however, it will also refer to a class
of learning algorithms (e.g. the SVM learning model).
performance in section II.B.2. We end this short review of SVMs by considering a non-standard
variant, which is interesting for our purposes as it has been beneﬁcially quantized. SVMs as described
are trained by ﬁnding the maximal margin hyperplane. Another model, called least-squares SVM
(LS-SVM) takes a regression (i.e. data-ﬁtting) approach to the problem, and ﬁnds a hyperplane
which, essentially, minimizes the least square distance of the vector of labels, and the vector of
distances from the hyperplane, where the ith entry of the vector is given with (w.xi + b). This is
eﬀected by a small modiﬁcation of the soft-margin formulation:
LS = argminw,b
such that yi(w.xi + b) = 1 −ξi,
where the only two diﬀerences are that the constraints are now equalities, and the slack variables
are squared in the optimization expression. This seemingly innocuous change causes diﬀerences
in performance, but also in the training. The dual formulation of the latter optimization problem
reduces to a linear system of equations:
1N Ω+ γ−1I
where 1 is an “all ones” vector, Y is the vector of labels yi, b is the oﬀset, γ is a parameter depending
on C. α is the vector of the Lagrange multipliers yielding the solution. This vector again stems from
the dual problem which we omitted due to space constraints, and which can be found in . Finally, Ωis the matrix collecting the (mapped) “inner products” of the
training vectors so Ωi,j = k(xi, xj), where k is a kernel function, in the simplest case, just the inner
product. The training of LS-SVMs is thus simpler (and particularly convenient from a quantum
algorithms perspective), but the theoretical understanding of the model, and its relationship to the
well-understood SVMs, is still a matter of study, with few known results (see e.g. (Ye and Xiong,
3. Other models
While NNs and SVMs constitute two popular approaches for ML tasks (in particular, supervised
learning), many other models exist, suitable for a variety of ML problems. Here we very brieﬂy
list and describe some of such models which have also appeared in the context of quantum ML.
While classiﬁcation typically assigns discrete labels to points, in the case when the labeling function
has a continuous domain (say the segment ) we are dealing with function approximation tasks,
often dealt with by using regression techniques. Typical examples here include linear regression,
which approximate the relationship of points and labels with a linear function, most often minimizing
the least-squares error. More broadly, such techniques are closely related to data-ﬁtting, that
is, ﬁtting the parameters of a parametrized function such as to best ﬁt observed (training) data.
The k-nearest neighbour algorithm is an intuitive classiﬁcation algorithm which given a new point
considers the k nearest training points (with respect to a metric of choice), and assigns the label by
the majority vote (if used for classiﬁcation), or by averaging (in the case of regression, i.e. continuous
label values). The mutually related k-means and k-medians algorithms are typically used for
clustering: the k speciﬁes the number of clusters, and the algorithm deﬁnes them in a manner which
minimizes the within-cluster distance to the mean (or median) point.
Another method for classiﬁcation and regression optimizes decision trees, where each dimension,
or entry (or more generally a feature24) of the new data point inﬂuences a move on a decision tree.
The depth of the tree is the length of the vector (or number of features), and the degree of each
node depends on the possible number of distinct features/levels per entry25. The vertices of the
tree specify an arbitrary feature of interest, which can inﬂuence the classiﬁcation result, but most
often they consider the overlaps with geometrical regions of the data-point space. Decision trees are
in principle maximally expressive (can represent any labeling function), but very diﬃcult to train
without constraints.
More generally, classiﬁcation tasks can be treated as the problem of ﬁnding a hypothesis h :
Data →Labels (in ML, the term hypothesis is essentially synonymous to the term classiﬁer, also
called a learner) which is from some family H, which minimizes error (or loss) under some loss
function. For instance, the hypotheses realized by SVMs are given by the hyperplanes (in the
kernel space), and in neural nets they are parametrized by the parameters of the nets: geometry,
thresholds, activation functions, etc. Additional to loss terms, the minimization of which is called
empirical risk minimization, ML applications beneﬁt from adding an additional component to
the objective function: the regularization term, the purpose of which is to penalize complex
functions, which could otherwise lead to poor generalization performance, see section II.B.2. The
choices of loss functions, regularization terms, and classes of hypotheses lead to diﬀerent particular
models, and training corresponds to optimization problems given by the choice of the loss function
and the hypothesis (function) family. Furthermore, it has been shown that essentially any learning
algorithm which requires only convex optimization for training leads to poor performance under
noise. Thus non-convex optimization is necessary for optimal learning ).
An important class of meta-algorithms for classiﬁcation problems are boosting algorithms. The
basic idea behind boosting algorithms is the highly non-trivial observation, ﬁrst proven via the
seminal AdaBoost algorithm , that multiple weak classiﬁers, which
perform better than random on distinct parts of the input space, can be combined into an overall
better classiﬁer. More precisely, given a set of (weak) hypotheses/classiﬁers {hj}, hj : Rn →{−1, 1},
under certain technical conditions, there exists a set of weights {wi}, wi ∈R, such that the composite
classiﬁer of the form hcw(x) = sign(P
i wihi(x)) performs better. Interestingly, a single (weak)
learning model can be used to generate the weak hypotheses needed for the construction of a better
composite classiﬁer – one which, in principle, can achieve arbitrary high success probabilities, i.e. a
strong learner. The ﬁrst step of this process is achieved by altering the frequencies at which the
training labeled data-points appear, one can eﬀectively alter the distributions over the data (in a
black-box setting, these can be obtained by e.g. rejection sampling methods). The training of one
and the same model on such diﬀerentially distributed datasets can generate distinct weak learners,
which emphasize distinct parts of the input space. Once such distinct hypotheses are generated,
optimization of the weight wi of the composite model is performed. In other words, weak learning
models can be boosted26.
24 Features, however, have a more generic meaning in the context of ML. A data vector is a vector of features, where
what a feature is depends on the context. For instance, features can be simply values at particular positions, or
more global properties: e.g. a feature of data vectors depicting an image may be “contains a circle”, and all vectors
corresponding to pictures with circles have it. Even more generically, features pertain to observable properties of
the objects the data-points represent (“observable” here simply means that the property can be manifested in the
data vector).
25 For instance, we can classify humans, parrots, bats and turtles, by binary features can fly and is mamal. E.g.
choosing the root can fly leads to the branch can fly = no with two leaves decided by is mamal = yes pinpointing
the human, whereas is mamal = no would specify the turtle. Parrots and bats would be distinguished by the same
feature in the can fly = yes subtree.
26 It should be mentioned that the above description only serves to illustrate the intuition behind boosting ideas.
In practice, various boosting methods have distinct steps, e.g. they may perform the required optimizations in
diﬀering orders, using training phases in parallel etc. which is beyond the needs of this review.
Aside from the broad classes of approaches to solve various ML tasks, ML is also often conﬂated
with speciﬁc computational tools which are used to solve them. A prominent example of this is
the development of algorithms for optimization problems, especially those arising in the training of
standard learning models. This includes e.g. particle swarm optimization, genetic and evolutionary
algorithms, and even variants of stochastic gradient descent. ML also relies on other methods including linear algebra tools, e.g. matrix decomposition methods, such as singular value decomposition,
QR-, LU- and other decompositions, derived methods such as principal component analysis , and
various techniques from the ﬁeld of signal analysis (Fourier, Wavelet, Cosine, and other transforms).
The latter set of techniques serves to reduce the eﬀective dimension of the data set, and helps combat
the curse of dimensionality. The optimization, linear algebra, and signal processing techniques, and
their interplay with quantum information is an independent body of research with enough material
to deserve a separate review, and we will only reﬂect on these methods when needed.
B. Mathematical theories of supervised and inductive learning
Executive summary: Aside from proposing learning models, such as NNs or SVMs, learning
theory also provides formal tools to identify the limits of learnability. No free lunch theorems
provide sobering arguments that na¨ıve notions of “optimal” learning models cannot be obtained,
and that all learning must rely on some prior assumptions. Computational learning theory
relies on ideas from computational complexity theory, to formalize many settings of supervised
learning, such as the task of approximating or identifying an unknown (boolean) function – a
concept –which is just the binary labeling function. The main question of the theory is the
quantiﬁcation of the number of invocations of the black-box – i.e. of the function (or of the oracle
providing examples of the function’s values on selected inputs) – needed to reliably approximate
the (partially) unknown concept to desired accuracy. In other words, computational learning
theory considers the sample complexity bounds for various learning settings, specifying the
concept families and type of access. The theory of Vapnik and Chervonenkis, or simply VC
theory, stems from the tradition of statistical learning. One of the key goals of the theory is
to provide theoretical guarantees on generalization performance. This is what is asked for
in the following question: given a learning machine trained on a dataset of size N, stemming
from some process, with a measured empirical risk (error on the training set) of some value
R, what can be said about its future performance on other data-points which may stem from
the same process? One of the key results of VC theory is that this can be answered, with the
help of a third parameter – the model complexity of the learning machine. Model complexity,
intuitively, captures how complicated functions the learner can learn: the more complicated
the model, the higher chance of “overﬁtting”, and consequently, the weaker the guarantees
on performance beyond the training set. Good learning models can control their model
complexity, leading to a learning principle of structural risk minimization. The art of ML is
a juggling act, balancing sample complexity, model complexity, and the computational
complexity of the learning algorithm27.
27 While the dichotomies between sample complexity and computational complexity are often considered in literature,
the authors have ﬁrst heard the trichotomic setting, including model complexity from . Examples
of such balancing, and its failures can be observed in sections V.A.2, and VI.A.1.
Although modern increase of interests in ML and AI are mostly due to applications, aspects of ML
and AI do have strong theoretical backgrounds. Here we focus on such foundational results which
clarify what learning is, and which investigate the questions of what learning limits are. We will
very brieﬂy sketch some of the basic ideas.
The ﬁrst collection of results, called No Free Lunch theorems place seemingly pessimistic bounds on
the conditions under which learning is at all possible . No Free Lunch theorems
are, essentially, a mathematical formalization of Hume’s famous problem of induction , which deals with the justiﬁcation of inductive reasoning. One example of
inductive reasoning occurs during generalization. Hume points out that, without a-priori assumptions,
concluding any property concerning a class of objects based on any number of observations28 is not
In a similar vein, learning based on experience cannot be justiﬁed without further assumptions:
expecting that a sequence of events leads to the same outcome as it did in the past, is only justiﬁed
if we assume a uniformity of nature. The problems of generalization and of uniformity can be
formulated in the context of supervised learning and RL, with (not uncontroversial) consequences (c.f.
(NFL)). For instance, one of the implications is that the expected performance of any two learning
algorithms beyond the training set must be equal, if one uniformly averages over all possible labeling
functions, and analogous statements hold for RL settings – in other words, without assumptions on
environments/datasets, the expected performance of any two learning models will be essentially the
same, and two learning models cannot be meaningfully compared in terms of performance without
making statements about the task environments in question. In practice, we, however, always have
some assumptions on the dataset and environment: for instance the principle of parsimony (i.e.
Occam’s razor), asserting that simpler explanations tend to be correct, prevalent in science, suﬃces
to break the symmetries required for NFLs to hold in their strongest form .
No review of theoretical foundations of learning theory should circumvent the works of Valiant, and
the general computational learning theory (CLT), which stems from a computer science tradition,
initiated by Valiant , and the related VC theory of Vapnik and Chervonenkis,
developed from a statistical viewpoint . We present the basic ideas of these theories
in no particular order.
1. Computational learning theory
CLT can be understood as a rigorous formalization of supervised learning, and which stems from
a computational complexity theory tradition. The most famous model in CLT is that of probably
approximately correct (PAC) learning. We will explain the basic notions of PAC learning on a simple
example: optical character recognition. Consider the task of training an algorithm to decide whether
a given image (given as a black and white bitmap) of a letter corresponds to the letter “A”, by
supplying a set of examples and counterexamples: a collection of images. Each image x can be
encoded as a binary vector in {0, 1}n (where n =height×width of the image).
Assuming that there exists an univocally correct assignment of label 0 (not “A”) and 1 to each image
implies there exists a characteristic function f : {0, 1}n →{0, 1} which discerns letters A from other
28 An exception to this would be the uninteresting case when the class was ﬁnite and all instances had been observed.
images. Such an underlying characteristic function (or, equivalently, the subset of bitstrings for
which it attains value ”1”) is, in computational learning theory, called a concept. Any (supervised)
learning algorithm will ﬁrst be supplied with a collection of N examples (xi, f((xi))i. In some
variants of PAC learning, it is assumed that the data-points (x) are drawn from some distribution
D attaining values in {0, 1}n. Intuitively, this distribution can model the fact that in practice, the
examples that are given to the learner stem from its interaction with the world, which speciﬁes
what kinds of “A”s we are more likely to see29. PAC learning typically assumes inductive settings,
meaning that the learning algorithm, given a sample set SN (comprising N identically independently
distributed samples from D) outputs a hypothesis h : {0, 1}n →{0, 1} which is, intuitively, the
algorithms “best guess” for the actual concept f. The quality of the guess is measured by the total
error (also known as loss, or regret),
errD(hSN ) =
P(D = x)|hSN (x) −f(x)|,
averaged according to the same (training) distribution D, where hSN is the hypothesis the (deterministic) learning algorithm outputs given the training set SN. Intuitively, the larger the training
set is (N), the smaller the error will be, but this also depends on the actual examples (and thus SN
and D). PAC theory concerns itself with probably (δ), approximately (ϵ) correct learning, i.e. with
the following expression:
PSN∼DN [errD(hSN ) ≤ϵ] ≥1 −δ,
where S ∼D means S was drawn according to the distribution D. The above expression is a
statement certifying that the learning algorithm, having been trained on the dataset sampled from
D, will, except with probability δ, have a total error below ϵ. We say a concept f is (ϵ, δ)-learnable,
under distribution D, if there exists a learning algorithm, and an N, such that Eq. (16) holds, and
simply learnable, if it is (ϵ, δ)-learnable for all (ϵ, δ) choices. The functional dependence of N on
(ϵ, δ) (and on the concept and distribution D) is called the sample complexity. In PAC learning,
we are predominantly concerned with identifying tractable problems, so a concept/distribution pair
f, D is PAC-learnable if there exists an algorithm for which the sample complexity is polynomial in
ϵ−1 and δ−1. These basic ideas are generalized in many ways. First, in the case the algorithm cannot
output all possible hypotheses, but only a restricted set H (e.g. the hypothesis space is smaller
than the total concept space), we can look for the best case solution by substituting the actual
concept f with the optimal choice h∗∈H which minimizes the error in (15), in all the expressions
above. Second, we are typically not interested in just distinguishing the letter “A” from all other
letters, but rather recognizing all letters. In this sense, we typically deal with a concept class (e.g.
“letters”), which is a set of concepts, and it is (PAC) learnable if there exists an algorithm for which
each of the concepts in the class are (PAC) learnable. If, furthermore, the same algorithm also
learns for all distributions D, then the class is said to be (distribution-free) learnable.
CLT contains other models, generalizing PAC. For instance, concepts may be noisy or stochastic. In
the agnostic learning model, the labeled examples (x, y) are sampled from a distribution D over
{0, 1}n × {0, 1}, which also models probabilitstic concepts30. Further, in agnostic learning, we deﬁne
29 For instance, in modern devices, the devices are (mostly) trained for the handwriting of the owner, which will
most of the time be distinct from other persons handwritings, although the device should in principle handle any
(reasonable) handwriting.
30 Note that we recover the standard PAC setting once the conditional probability distribution of PD(y|x) where the
values of the ﬁrst n bits (data-points) are ﬁxed, is Kronecker-delta – i.e. the label is deterministic.
a set of concepts C ⊆{c|c : {0, 1}n →{0, 1}}, and given D, we can identify the best deterministic
approximation of D in the set C, given with optC = minc∈CerrD(c). The goal of learning is to
produce a hypothesis h ∈C which performs not much worse than the best approximation optC, in
the PAC sense – the algorithm is a (ϵ, δ)−agnostic learner for D and C, if given access to samples
from D it outputs a hypothesis h ∈C such that errD(c) ≤ϵ + optC, except with probability δ.
Another common model in CLT is, the exact learning from membership queries model
 , which is, intuitively, related to active supervised learning (see section I.B.3). Here,
we have access to an oracle, a black-box, which outputs the concept value f(x) when queried with an
example x. The basic setting is exact, meaning we are required to output a hypothesis which makes
no errors whatsoever, however with a bounded probability (say 3/4). In other words, this is PAC
learning where ϵ = 0, but we get to choose which examples we are given, adaptively, and δ is bounded
away from 1/2. The ﬁgure of merit usually considered in this setting is query complexity, which
denotes the number of calls to the oracle the learning algorithm uses, and is for most intents and
purposes synonymous to sample complexity31. This, in spirit, corresponds to an active supervised
learning setting.
Much of PAC learning deals with identifying examples of interesting concept classes which are
learnable (or proving that relevant classes are not), but other more general results exist connecting
this learning framework. For instance, we can ask whether we can achieve a ﬁnite-sampling universal
learning algorithm: that is, an algorithm that can learn any concept, under any distribution using
some ﬁxed number of samples N. The No Free Lunch theorems we mentioned previously imply
that this is not possible: for each learning algorithm (and ϵ, δ), and any N there is a setting
(concept/distribution) which requires more than N samples to achieve (ϵ, δ)-learning.
Typically, the criterion for a problem to be learnable assumes that there exists a classiﬁer whose
performance is essentially arbitrarily good – that is, it assumes the classiﬁer is strong. The boosting
result in ML, already touched upon in section II.A.3, shows that settling on weak classiﬁers, which
perform only slightly better than random classiﬁcation, does not generate a diﬀerent concept of
learnability .
Classical CLT theory has also been generalized to deal with concepts with continuous ranges. In
particular, so called p-concepts have range in . The generalization of
the entire CLT to deal with such continuous-valued concepts is not without problems, but nonetheless,
some of the central results, for instance quantities which are analogs of the VC-dimension, and
analogous theorems relating this to generalization performance, can still be provided for an overview given in the context of the learning of quantum states discussed in section
Computational learning theory is closely related to the statistical learning theory of Vapnik and
Chervonenkis (VC theory) which we discuss next.
2. VC theory
The statistical learning formalism of Vapnik and Chervonenkis was developed over the course of
more than 30 years, and in this review we are forced to present just a chosen aspect of the total
31 When the oracle allows non-trivial inputs, one typically talks about query complexity. Sample complexity deals
with the question of “how many samples” which suggest the setting where the oracle only produces outputs, without
taking inputs. The distinction is not relevant for our purposes and is more often a matter of convention of the
research line.
theory, which deals with generalization performance guarantees. In the previous paragraph on
PAC learning, we have introduced the concept of total error, which we will refer to as (total)
risk. It is deﬁned as the average over all the data points, which is, for a hypothesis h, given with
R(h) = error(h) = P
x P(D = x)|h(x) −f(x)| (we are switching notation to maintain consistency
with literature of diﬀering communities). However, this quantity cannot be evaluated in practice, as
in practice we only have access to the training data. This leads us to the notion of the empirical
risk given with
|h(x) −f(x)|,
where SN is the training set drawn independently from the underlying distribution D.
The quantity ˆR(h) is intuitive and directly measurable. However, the problem of ﬁnding learning
models which optimize empirical risk alone is not in it self interesting as it is trivially resolved with
a look-up table. From a learning perspective, the more interesting and relevant quantity is the
performance beyond the training set, which is contained in the unmeasurable R(h), and indeed the
task of inductive supervised learning is identifying h which minimizes R(h), given only the ﬁnite
training set SN. Intuitively, the hypothesis h which minimizes the empirical risk should also be our
best bet for the hypothesis which minimizes R(h), but this can only make sense if our hypothesis
family is somehow constrained, at least to a family of total functions: again, a look-up table has
zero empirical risk, yet says nothing about what to do beyond. One of the key contributions of VC
theory is to establish a rigorous relationship between the observable quantity ˆR(h) – the empirical
risk, the quantity we actually wish to bound R(h) – the total risk, and the family of hypotheses our
learning algorithm can realize. Intuitively, if the function family is too ﬂexible (as is the case with
just look-up tables) a perfect ﬁt on the examples says little. In contrast, having a very restrictive
set of hypotheses, say just one (which is independent from the dataset/concept and the generating
distribution), suggest that the empirical risk is a fair estimate of the total risk (however bad it
may be), as nothing has been tailored for the training set. This brings us to the notion of the
model complexity of the learning model, which has a few formalizations, and here we focus on
the Vapnik-Chervonenkis dimension of the model (VC dimension)32.
The VC-dimension is an integer number assigned to a set of hypotheses H ⊆{h|h : S →{0, 1}},
(e.g. the possible classiﬁcation functions our learning algorithm can even in principle be trained
to realize), where S can be, for instance, the set of bitstrings {0, 1}n, or, more generally, say real
vectors in Rn. In the context of basic SVMs, the set of hypotheses are “all hyperplanes”33. Consider
now a subset Ck of k points in Rn in a general position34. These points can attain binary labels
in 2k diﬀerent ways. The hypothesis family H is said to shatter the set C, if for any labeling ℓ
of the set Ck, there exists a hypothesis h ∈H which correctly labels the set Ck according to ℓ. In
other words, using functions from H we can learn any labeling function on the set Ck of k points
in a general position perfectly. The VC dimension of H is then the largest kmax such that there
exists the set Ckmax of points in general position which is shattered (perfectly “labelable” for any
labeling) by H. For instance, for n = 2, “rays” shatter three points but not 4 (imagine vertices
of a square where diagonally opposite vertices share the same label), and in n = N, “hyperplanes”
32 Another popular measure of model complexity is e.g. Rademacher complexity .
33 Naturally, a non-trivial kernel function enriches the set of hypotheses realized by SVMs.
34 General position implies that no sub-set of points is co-planar beyond what is necessary, i.e. points in SRn are in
general position if no hyperplane in Rn contains more than n points in S.
shatter N + 1 points. While it is beguiling to think that the VC dimension corresponds to the
number of free parameters specifying the hypothesis family, this is not the case35. The VC theorem
(in one of its variants) then states that the empirical risk matches total risk,
up to a deviation which decays in the number of samples, but grows in the VC-dimension of the
model, more formally:
ˆR(hSN ) −R(hSN ) ≤ϵ
d (log(2N/d) + 1)
where d is the VC-dimension of the model, N number of samples, and hSN is the hypothesis output
by the model, given the training set SN, which is sampled from the underlying distribution D.
The underlying distribution D implicitly appears also in the total risk R. Note that the chosen
acceptable probability of incorrectly bounding the true error, that is, probability δ, contributes only
logarithmically to the misestimation bound ϵ, whereas the VC dimension and the number of samples
contribute (mutually inversely) linearly to the square of ϵ.
The VC theorem suggests that the ideal learning algorithm would have a low VC dimension (allowing
a good estimate of the relationship of the empirical and total risk), while at the same time, performing
well on the training set. This leads to a learning principle called structural risk minimization.
Consider a parametrized learning model (say parametrized by an integer l ∈l) such that each l
induces a hypothesis family Hl, each more expressive then the previous, so Hl ⊆Hl+1. Structural
risk minimization (contrasted to empirical risk minimization which just minimizes empirical risk)
takes into account that in order to have (a guarantee on) good generalization performance we need to
have both good observed performance (i.e. low empirical risk) and low model complexity. High model
complexity induces the risk stemming from the structure of the problem, manifested in common
issues such as data overﬁtting. In practice, this is achieved by considering (meta-)parametrized
models, like {Hl},where we minimize a combination of l (inﬂuencing the VC-dimension) and the
empirical risk associated to Hl. In practice, this is realized by adding a regularization term to the
training optimization, so generically the (unregularized) learning process resulting in argminh∈H ˆR(h)
is updated to argminhl∈Hl
ˆR(h) + reg(l)
, where reg(·) penalizes the complexity of the hypothesis
family, or just the given hypothesis.
VC dimension is also a vital concept in PAC learning, connecting the two frameworks. Note ﬁrst
that a concept class C, which is a set of concepts is also a legitimate set of hypotheses, and thus
has a well-deﬁned VC dimension dC. Then, the sample complexity of (ϵ, δ)−(PAC)-learning of C is
given with O
 (dC + ln 1/δ)ϵ−1
Many of the above results can also be applied in the contexts of unsupervised learning, however
the theory of unsupervised (or structure learning), is mostly concerned with the understanding of
particular methodologies, the topic of which is beyond this review paper.
35 The canonical counterexample is the family speciﬁed by the partition of the real plane, halved by the graph of the
two-parametric function hα,β(x) = α sin(βx), which can be proven to shatter any ﬁnite number of points in n = 2.
The fact that the number of parameters of a function does not fully capture the complexity of the function should
not be surprising as any (continuous) function over k + n variables (parameters + dimension) can be encoded as a
function over 1 + n variables.
C. Basic methods and theory of reinforcement learning
Executive summary: While RL, in all generality, studies learning in and from interactive task
environments, perhaps the best understood models consider more restricted settings. Environments can often be characterized by Markov Decision Processes, i.e. they states, which
can be observed by the agent. The agent can cause transitions from states to states, by its
actions, but the rules of transitions are not known beforehand. Some of the transitions are
rewarded. The agent learns which actions to perform, given that the environment is in some
state, such that it receives the highest value of rewards (expected return), either in a ﬁxed
time frame (ﬁnite-horizon) or over (asymptotically) long time periods, where future rewards
are geometrically depreciated (inﬁnite-horizon). Such models can be solved by estimating
action-value functions, which assign expected return to actions given states, for which the
agent must explore the space of strategies, but other methods exist. In more general models,
the state of the environment need not be fully observable, and such settings are signiﬁcantly
harder to solve. RL settings can also be tackled by models from the so-called Projective
Simulation framework for the design of learning agents, inspired by physical stochastic processes.
While comparatively new, this model is of particular interest as it had been designed with the
possibilities of beneﬁcial quantization in mind. Interactive learning methods include models
beyond textbook RL, including partially observable settings, which require generalization and
more. Such extensions, e.g. generalization, typically require techniques from non-interactive
learning scenarios, but also lead to agents with an ever increasing level of autonomy. In this
sense, RL forms a bridge between ML and general AI models.
Broadly speaking, RL deals with the problem of learning how to optimally behave in unknown
environments. In the basic textbook formalism, we deal with a task environment, which is
speciﬁed by a Markov decision process (MDP). MDPs are labeled, directed graphs with additional
structures, comprising a discrete and ﬁnite sets of states S = {si} and actions A = {ai}, which
denote the possible states of the environment, and the actions the learning agent can perform on it,
respectively.
FIG. 8 A three state, two-action MDP.
The choice of the actions of the agent change the state
of the environment, in a manner which is speciﬁc to the
environment (MDP), and which may be probabilistic. This
is captured by a transition rule P(s|s′, a), denoting the
probability of the environment ending up in the state
s, if the action a had been performed in the state s′.
Technically, this can be viewed as a collection of actionspeciﬁc Markov transition matrices {P a}a∈A that the
learner can apply on the environment by performing an
These describe the dynamics of the environment conditioned on the actions of the agent.
The ﬁnal component specifying the environment is a reward function
R : S × A × S →Λ, where Λ is a set of rewards, often
binary. In other words, the environment rewards certain
transitions36. At each time instance, the action of the
learner is speciﬁed by a policy: a conditional probability distribution π(a|s), specifying the probability of the agent outputting the action a provided it is in the state s. Given an MDP, intuitively
the goal is ﬁnding good policies, i.e. those which yield high rewards. This can be formalized in
many non-equivalent ways.
Given a policy π and some initial state s we can e.g. deﬁne ﬁnite-horizon expected total reward
after N interaction steps with Rs
i=1 ri, where ri is the expected reward under policy π at
time-step i, in the given environment, and assuming we started from the state s. If the environment is
ﬁnite and strongly connected37, the ﬁnite-horizon rewards diverge as the horizon N grows. However,
by adding a geometrically depreciating factor (rate γ) we obtain an always bounded expression
Rγ(π) = P∞
i=1 γiri, called the inﬁnite horizon expected reward (parametrized by γ), which is
more commonly studied in literature. The expected rewards in ﬁnite or inﬁnite horizons form the
typical ﬁgures of merit in solving MDP problems, which come in two ﬂavors. First, in decision
theory, or planning (in the context of AI), the typical goal is ﬁnding the policy πopt which optimizes
the (in)ﬁnite horizon reward in a given MDP, formally: given the (full or partial) speciﬁcation of
the MDP M, solve πopt = argmaxπRN/γ(π), where R is the expected reward in ﬁnite (for N steps)
or inﬁnite horizon (for a given depreciation γ) settings, respectively. Such problems can be solved
by dynamic and linear programming. In RL , the speciﬁcation of the
environment (the MDP), in contrast, is not given, but rather can be explored by interacting with it
dynamically. The agent can perform an action, and receive the subsequent state (and perhaps a
reward). The ultimate goal here comes in two related (but conceptually diﬀerent) ﬂavours. One is to
design an agent which will over time learn the optimal policy πopt, meaning the policy can be read
out from the memory of the agent/program. Slightly diﬀerently, we wish an agent which will, over
time gradually alter its behaviour (policy) as to act according to the optimal policy. While
in theory these two are closely related, e.g. in robotics these are quite diﬀerent as the reward rate
before convergence (perfect learning) also matters 38. First of all, we point out that RL problems as
given above can be solved reliably whenever the MDP is ﬁnite and strongly connected: a trivial
solution is to stick to a random policy until a reliable tomography of the environment can be done,
after which the problem is resolved via dynamic programming 39. Often, environments actually have
additional structure, so-called initial and terminal states: if the agent reaches the terminal state, it
is “teleported” to the ﬁxed initial state. Such structure is called episodic, and can be used as a
means of ensuring the strong connectivity of the MDP.
One way of obtaining solutions is by tracking so-called value functions Vπ(s) : S →R which assign
expected reward under policy π assuming we start from state s; this is done recursively: the value
of the current state is the current reward plus the averaged value of the subsequent state (averaged
under the stochastic transition rule of the environment P(s|a, s′)).
Optimal policies optimize
these functions, and this too is achieved sequentially by modifying the policy as to maximize the
value functions. This, however, assumes the knowledge of the transition rule P(s|a, s′). In further
36 Rewards can also be probabilistic. This can be modelled by explicitly allowing stochastic reward functions, or by
extending the state space, to include rewarding and non-rewarding instances of states (note, the reward depends on
current state, action and the reached state) in which case the probability of the reward is encoded in the transition
probabilities.
37 In this context this means that the underlying MDP has ﬁnite return times for all states, that is, there is a ﬁnite
probability of going back to the initial state from any state for some sequence of actions.
38 These two ﬂavours are closely related to the notions of on-policy and oﬀ-policy learning. These labels typically
pertain to how the estimates of the optimal policy are internally updated, which may be in accordance to the
actual current policy and actions of the agent, or independently from the executed action, respectively. For more
details see e.g. .
39 If the environment is not strongly connected, this is not possible: for instance the ﬁrst move of the learner may lead
to “good” or “bad” regions from which there is no way out, in which case optimal behaviour cannot be obtained
with certainty.
development of the theory, it was shown that tracking action-value functions Qπ(s, a), given by
Qπ(s, a) =
P(s′|a, s)(Λ(s, a, s′) + γVπ(s′))
assigning the value not only to the state, but the subsequent action as well can be modiﬁed into an
online learning algorithm40. In particular, the Q-values can be continuously estimated by weighted
averaging the current reward (at timestep t) for an action-value, and the estimate of the highest
possible Q-value of the subsequent action-value:
Qt+1(st, at) = Qt(st, at)
learning rate
learned value
Qt(st+1, a)
estimate of optimal
future value
−Qt(st, at)
Note that having access to the optimal Q-values suﬃces to ﬁnd the optimal policy: given a state,
simply pick an action with the highest Q-value, but the algorithm above says nothing about which
policy the agent should employ while learning. In it was shown that the
algorithm, speciﬁed by the update rule of Eq. 21, called Q-learning indeed converges to optimal Q
values as long as the agent employs any ﬁxed policy which has non-zero probabilities for all actions
given any state (the parameter αt, which is a function of time, has to satisfy certain conditions, and
γ should be the γ of the targeted ﬁgure of merit Rγ)41.
In essence, this result suﬃces for solving the ﬁrst ﬂavour of RL, where the optimal policy is “learned”
by the agent in the limit, but, in principle, never actually used. The convergence of the Q-learning
update to the optimal Q-values, and consequently to the optimal behaviour, has been proven for all
learning agents using Greedy-in-the-limit, inﬁnite exploration (GLIE) policies. As the name suggests,
such policies, in the asymptotic limit perform actions with the highest value estimated42.
At the same time, inﬁnite exploration means that, in the limit all state/action combinations will be
tried out inﬁnitely many times ensuring true optimal action values are found, and that the local
minima are avoided. In general, the optimal trade oﬀbetween these two competing properties,
the exploration of the learning space, and the exploitation of obtained knowledge is quintessential
for RL. There are many other RL algorithms which are based on state value, or action-value
optimizations, such as SARSA43, various value iteration methods, temporal diﬀerence methods etc.
 . In more recent times, progress has been achieved by using parametrized
approximations of state-action-value-functions – a cross-breed between function approximation and
reinforcement learning – which reduces the search space of available Q-functions. Here, the results
40 This rule is inspired by the the Bellman optimality equation, Q∗(s, a) := E[R(s, a)] + γE[maxa′Q ∗(s′, a′)], where
the expected values are taken over the randomness MDP transition rule and the reward function, which has as the
solution – the ﬁxed point – the optimal Q−value function. This equation can be used when the speciﬁcation of the
environment is fully known. Note that the optimal Q-values can be found without actually explicitly identifying an
optimal policy.
41 Q-learning is an example of an oﬀ-policy algorithm as the estimate of the future value in Eq. 21 is not evaluated
relative to the actual policy of the agent (indeed, it is not necessarily even deﬁned), but rather relative to the
so-called “greedy-policy”, which takes the action with the maximal value estimate (note the estimate appears with
a maximization term).
42 To avoid any confusion, we have introduced the concept policy to refer to the conditional probability distributions
specifying what the agent will do given a state. However, the same term is often overloaded to also refer to
the speciﬁcation of the eﬀective policy an agent will use given some state/time-step. For instance, “ϵ−greedy
policies” refer to behaviour in which, given a state, the the agent outputs the action with the highest corresponding
Q−value – i.e. acts greedily – with probability 1 −ϵ, and produces a random action otherwise. Clearly, this rule
speciﬁes a policy at any given time step, given the current Q-value table of the agent. One can also think of
time-dependent policies, which mean that the policy also explicitly depends on the time-step. An example of a such
a time-dependant and a (slowly converging) GLIE policy is an ϵ−greedy policy, where ϵ = ϵ(t) = 1/t is a function
of the time-step, converging to zero.
43 SARSA is the acronym for state-action-reward-state-action.
which combine deep learning for value function approximation with RL have been particularly
successful and the same approach also underpins the AlphaGo 
system. This brings us to a diﬀerent class of methods which do not optimize state, or action-value
functions, but rather learn complete policies, often by performing an estimate of gradient descent,
or other means of direct optimization in policy space. This is feasible whenever the policies
are speciﬁed indirectly, by a comparably small number of parameters, and can in some cases be
faster .
The methods we discussed thus far consider special cases of environments, where the environment
is Markovian, or, related to this, fully observable. The most common generalization of this are
so-called partially observable MDPs (POMDP), where the underlying MDP structure is extended
to include a set of observations O and a stochastic function deﬁned with the conditional probability
distribution PP OMDP (o ∈O|s ∈S, a ∈A). The set of states of the environment are no longer
directly accessible to the agent, but rather the agent perceives the observations from the set O,
which indirectly and, in general, stochastically depend on the actual unobservable environmental
state, as given by the distribution PP OMDP , and the action the agent took last. POMDPs are
expressive enough to capture many real world problems, and are thus a common world model in AI,
but are signiﬁcantly more diﬃcult to deal with compared to MDPs 44.
FIG. 9 Illustration of the structure of the
episodic and compositional memory in PS,
comprising clips (episodes) and probabilistic transitions. The actuator of the agent
performs the action. Adapted from .
As mentioned, the setting of POMDPs moves us one step
closer to arbitrary environment settings, which is the
domain of artiﬁcial (general) intelligence45.
The context of AGI is often closely related to modern view
on robotics, where the structure of what can be observed,
and what actions are possible stems not only from the
nature of the environment, but also (bodily) constraints of
the agent: e.g. a robot is equipped with sensors, specifying
and limiting what the robot can observe or perceive, and
actuators, constraining the possible actions. In such an
agent-centric viewpoint, we typically talk about the set
of percepts – signals that the agent can perceive – which
may correspond to full states, or partial observations,
depending on the agent-environment setting – and the set of actions46.
This latter viewpoint, that the percept/action structure stems from the physical constitution of
the agent and the environment, which we will refer to as an embodied perspective, was one
of the starting points of the development of the projective simulation (PS) model for AI. PS is a
physics-inspired model for AI which can be used for solving RL tasks. The centerpiece of the model
is the so-called Episodic and Compositional Memory (ECM), which is a stochastic network of clips,
see Fig. 9.
44 For instance, the problem of ﬁnding optimal inﬁnite-horizon policies, which was solvable via dynamical programming
in the fully observable (MDP) case becomes, in general, uncomputable.
45 To comment a bit on how RL methods and tasks may be generalized towards general AI, one can consider learning
scenarios where one has to combine standard data-learning ML to handle the realistic percept space (which is
eﬀectively inﬁnite) with RL techniques. An example of this as was done e.g. in the famous AlphaGo system . Further, one could also consider more general types of interaction, beyond the strict turn-based
metronomic model. For instance in active reinforcement learning, the interaction occurs relative to an external
clock, which intertwines computational complexity and learning eﬃciency of the agent (see section VII.A). Further,
the interaction may occur in fully continuous time. This setting is also not typically studied in the basic theory of
AI, but occurs in the closely related problem of control theory , which may be more
familiar to physicists. Such generalizations are at the cutting edge of research, also in the classical realm, and also
beyond the scope of this paper.
46 In this sense, a particular agent/robot, may perceive the full state of the environment in some environments (making
the percepts identical to states), whereas in other environments, the sensors fail to observe everything, in which
case the percepts correspond to observations.
Clips are representations of short autobiographical episodes, i.e. memories of the agent. Using
the compositional aspects of the memory, which allows for a rudimentary notion of creativity, the
agent can also combine actual memories to generate ﬁctitious, conceivable clips which need not have
actually occurred. More formally, clips can be deﬁned recursively as either memorized percepts or
actions, or otherwise structures (e.g. sequences) of clips. Given a current percept, the PS agent calls
its ECM network to perform a stochastic random walk over its clip space (the structure of which
depends on the history of the agent) projecting itself into conceivable situations, before committing
to an action. Aspects of this model have been beneﬁcially quantized, and also used both in quantum
experiments and in robotics and we will focus more on this model in section VII.A.
a. Learning eﬃciency and learnability for RL
As mentioned in the introduction to this section, No
Free Lunch theorems also apply to RL, and any statement about learning requires us to restrict the
space of possible environments. For instance, “ﬁnite-space, time-independent MDPs” is a restriction
which allows perfect learning relative to some of the standard ﬁgures of merit, as was ﬁrst proven by
the Q-learning algorithm. Beyond learnability, in more recent times, notions of sample complexity
for RL tasks have also been explored, addressing the problem from diﬀerent perspectives. The theory
of sample complexity for RL settings is signiﬁcantly more involved than for supervised learning,
although the very basic desiderata remain the same: how many interaction steps are needed before
the agent learns. Learning can naturally mean many things, but most often what is meant is that
the agent learns the optimal policy. Unlike supervised learning, RL has the additional temporal
dimension in the deﬁnitions of optimality (e.g. ﬁnite or inﬁnite horizons), leading to an even broader
space of options one can explore. Further details on this important ﬁeld of research are beyond
the scope of this review, and we refer the interested reader to e.g. the thesis of Kakade which also does a good job of reviewing some of the early works, and ﬁnds sample complexity
bounds for RL for many basic settings, or e.g. 
for some of the newer results.
III. QUANTUM MECHANICS, LEARNING, AND AI
Quantum mechanics has already had profound eﬀect on the ﬁelds of computation and information
processing. However, its impact on AI and learning has, up until very recently, been modest.
Although the ﬁelds of ML and AI have a strong connection to theory of computation, these ﬁelds
are still diﬀerent, and not all progress in (quantum) computation implies qualitative progress in
AI. For instance, although it has been more than 20 years, still the arguably most celebrated result
in QC is that of Shor’s factoring algorithm , which, on the face of it, has no impact
on AI47. Nonetheless, other, less famous results may have application to various aspects of AI and
learning. The ﬁeld of QIP has thus, from its early stages had a careful and tentative interplay with
various aspects of AI, although it is only recently that this line of research has received a broader
attention. Roughly speaking, we can identify four main directions covering the interplay between
ML/AI summarized in in Fig. 10.
47 In fact, this is not entirely true – certain proofs of separation between PAC learnability in the quantum and classical
model assume hardness of factoring of certain integers (see section VI.A.2).
Applications of ML in quantum physics
(1) Estimation and metrology
(2) Quantum control and gate design
(3) Controlling quantum experiments,
machine-assisted research
(4) Condensed matter and many body physics
Quantum enhancements for ML
(1) Quantum perceptrons and neural networks
(2) Quantum computational learning theory
(3) Quantum enhancement of learning capacity
(4) Quantum computational algorithmic speedups for learning
Quantum generalizations of ML-type tasks
(1) Quantum generalizations:
learning of quantum data
(2) (Quantum) learning of quantum processes
Quantum learning agents and elements of quantum AI
(1) Quantum-enhanced learning through
interaction
(2) Quantum agent-environment paradigm
(3) Towards quantum AI
FIG. 10 Table of topics investigating the overlaps between quantum physics, machine learning, and AI.
Historically speaking, the ﬁrst contacts between aspects of QIP and learning theory occurred
in terms of the direct application of statistics and statistical learning in light of the quantum
theory, which forms the ﬁrst line: classical machine learning applied in quantum theory
and experiment reviewed in section IV. In this ﬁrst topic, ML techniques are applied to data
stemming from quantum experiments. The second topic, in contrast, machine learning over genuinely
quantum data: quantum generalization of machine learning-type tasks, discussed in section
V. This brings us to the topic which has been receiving substantial interest in recent times: can
quantum computers genuinely help in machine learning problems, addressed in section
VI. The ﬁnal topic we will investigate considers aspects of QIP which extend beyond machine
learning (taken in a narrow sense), such as generalizations of RL, and which can be understood as
stepping-stones towards quantum AI. This is reﬂected upon in section VII.C
It is worthwhile to note that there are many possible natural classiﬁcations of the comprehensive ﬁeld
we discuss in this review. Our chosen classiﬁcation is motivated by two subtly diﬀering perspectives
on the classiﬁcation of quantum ML, discussed further in section VII.B.1.
IV. MACHINE LEARNING APPLIED TO (QUANTUM) PHYSICS
In this section we review works and ideas where ML methods have been either directly utilized, or
have otherwise been instrumental for QIP results. To do so, we are however, facing the ungrateful
task of specifying the boundaries of what is considered a ML method. In recent times, partially due
to its successes, ML has become a desirable key word, and consequently an umbrella term for a broad
spectrum of techniques. This includes algorithms for solving genuine learning problems, but also
methods and techniques designed for indirectly related problems. From such an all-encompassing
viewpoint, ML also includes aspects of (parametric) statistical learning, the solving of black-box
(or derivative-free) optimization problems, but also the solving of hard optimization problems in
general48.
As we do not presume to establish hard boundaries, we adopt a more inclusive perspective. The
collection of all works which utilize such methods, which could conceivably ﬁt in broad-scope ML, for
QIP applications cannot be covered in one review. Consequently, we place emphasis on pioneering
works, and works where the authors themselves advertise the ML ﬂavour of used methodologies,
thereby emphasizing the potential of such ML/QIP interdisciplinary endeavors.
The use of ML in the context of QIP, understood as above, has been considerable, with an eﬀective
explosion of related works in the last few years. ML has been shown to be eﬀective in a great
variety of QIP related problems: in quantum signal processing, quantum metrology, Hamiltonian
estimation, and in problems of quantum control. In recent times, the scope of applications has been
signiﬁcantly extended, ML and involved techniques have also been applied to combatting noise in
the process of performing quantum computations, problems in condensed-matter and many-body
physics, and in the design of novel quantum optical experiments. Such results suggest that advanced
ML/AI techniques will play an integral role in quantum labs of the future, and in particular, in the
construction of advanced quantum devices and, eventually, quantum computers. In a complementary
direction, QIP applications have also engaged many of the methods of ML, showing that QIP may
also become a promising proving ground for cutting edge ML research.
Contacts between statistical learning theory (as a part of the theoretical foundations of ML) and
quantum theory come naturally due to the statistical foundations of quantum theory. Already the
very early theories of quantum signal processing , probabilistic aspects of quantum
theory and quantum state estimation , and early works 
which would lead to modern quantum metrology included statistical
analyses which establish tentative grounds for more advanced ML/QIP interplay. Related early
works further emphasize the applicability of statistical methods, in particular maximum likelihood
estimation, to quantum tomographic scenarios, such as the tasks of state estimation ,
the estimation of quantum processes and measurements and the reconstruction of quantum processes from incomplete tomographic data 49. The works of this type generically focus on physical scenarios where clean analytic theory
can be applied. However, in particular in experimental, or noisy (thus, realistic) settings, many
of the assumptions, which are crucial for the pure analytic treatment, fail. This leads to the ﬁrst
category of ML applications to QIP we consider.
48 Certain optimization problems, such as online optimization problems where information is revealed incrementally,
and decisions are made before all information is available, are more clearly related to “quintessential” ML problems
such as supervised, unsupervised, or reinforcement learning.
49 Interestingly, such techniques allow for the identiﬁcation of optimal approximations of unphysical processes which
can be used to shed light on the properties of quantum operations.
A. Hamiltonian estimation and metrology
Executive summary: Metrological scenarios can involve complex measurement strategies,
where, e.g., the measurements which need to be performed may depend on previous outcomes.
Further, the physical system under analysis may be controlled with the help of additional
parameters – so-called controls – which can be sequentially modiﬁed, leading to a more complicated
space of possibilities. ML techniques can help us ﬁnd optima in such a complex space of
strategies, under various constraints, which are often pragmatically and experimentally
motivated constraints.
The identifying of properties of physical systems, be it dynamic properties of evolutions (e.g. process
tomography), or properties of the states of given systems (e.g. state tomography), is a fundamental
task. Such tasks are resolved by various (classical) metrological theories and methods, which can
identify optimal strategies, characterize error bounds, and which have also been quite generally
exported to the quantum realm. For instance, quantum metrology studies the estimation of the
parameters of quantum systems, and, generally, identiﬁes optimal measurement strategies, for
their estimation. Further, quantum metrology places particular emphasis on scenarios where genuine
quantum phenomena – a category of phenomena associated to and sometimes even deﬁned by
the need for complex, and diﬃcult-to-implement quantum devices for their realization – yield an
advantage over simpler, classical strategies.
The speciﬁcation of optimal strategies, in general, constitute the problem of planning50, for
which various ML techniques can be employed. The ﬁrst examples of ML applications for ﬁnding
measurement strategies originate from the problem of phase estimation, a special case of Hamiltonian
estimation. Interestingly, already this simple case, provides a fruitful playground for ML techniques:
analytically optimal measurement strategies are relatively easy to ﬁnd, but are experimentally
unfeasible. In turn, if we limit ourselves to a set of “simple measurements”, near-optimal results are
possible, but they require diﬃcult-to-optimize adaptive strategies – the type of problem ML is good
for. Hamiltonian estimation problems have also been tackled in more general settings, invoking more
complex machinery. We ﬁrst brieﬂy describe basic Hamiltonian estimation settings and metrological
concepts. Then we will delve deeper in these results combining ML with metrology problems.
1. Hamiltonian estimation
The generic scenarios of Hamiltonian estimation, a common instance of metrology in the quantum
domain, consider a quantum system governed by a (partially unknown) Hamiltonian within a speciﬁed
family H(θ), where θ = (θ1, . . . , θn), is a set of parameters θ. Roughly speaking, Hamiltonian
estimation deals with the task of identifying the optimal methods (and the performance thereof) for
estimating the Hamiltonian parameters.
This amounts to optimizing the choice of initial states (probe states), which will evolve under
the Hamiltonian, and the choice of the subsequent measurements, which uncover the eﬀect the
Hamiltonian had, and thus, indirectly, the parameter values51. This proliﬁc research area considers
50 More speciﬁcally, most metrology settings problems constitute instances of oﬀ-line planning, and thus not RL, as
the “environment speciﬁcation” is fully speciﬁed – in other words, there is no need to actually run an experiment,
and the optimal strategies can be found oﬀ-line. See section I.B for more detail.
51 Technically, the estimation also involves the use of a suitable estimator function, but these details will not matter.
many restrictions, variations and generalizations of this task. For instance, one may assume settings
in which we either have control over the Hamiltonian evolution time t, or it is ﬁxed so that t = t0,
which are typically referred to as frequency, and phase estimation, respectively. Further, the eﬃciency
of the process can be measured in multiple ways. In a frequentist approach, one is predominantly
interested in estimation strategies which, roughly speaking, allow for the best scaling of precision of
the estimate, as a function of the number of measurements. The quantity of interest is the so-called
quantum Fisher information, which bounds and quantiﬁes the scaling. Intuitively, in this setting,
also called the local regime, many repetitions of measurements are typically assumed. Alternatively,
in the Bayesian, or single-shot, regime the prior information, which is given as a distribution over
the parameter to be estimated, and its update to the posterior distribution given a measurement
strategy and outcome, are central objects . The objective
here is the identiﬁcation of preparation/measurement strategies which optimally reduce the average
variance of the posterior distribution, which is computed via Bayes’ theorem.
One of the key interests in this problem is that the utilization of, arguably, genuine quantum features,
such as entanglement, squeezing etc. in the structure of the probe states and measurements may
lead to provably more eﬃcient estimation than is possible by so-called classical strategies for many
natural estimation problems. Such quantum-enhancements are potentially of immense practical
relevance . The identiﬁcation of optimal scenarios has been achieved in
certain “clean” theoretical scenarios, which are, however, often unrealistic or impractical. It is in
this context that ML-ﬂavoured optimization, and other ML approaches can help.
2. Phase estimation settings
Interesting estimation problems, from a ML perspective, can already be found in the simple
examples of a phase shift in an optical interferometer, where one of the arms of an otherwise
balanced interferometer contains a phase shift of θ. Early on, it was shown that given an optimal
probe state, with mean photon number N, and an optimal (so-called canonical) measurement, the
asymptotic phase uncertainty can decay as N −1 52 , known as the
Heisenberg limit. In contrast, the restriction to “simple measurement strategies” (as characterized
by the authors) , involving only photon number measurements in the two output arms, achieve a
quadratically weaker scaling of
N −1, referred to as the standard quantum limit. This was proven
in more general terms: the optimal measurements cannot be achieved by the classical post-processing
of photon number measurements of the output arms, but constitute an involved, experimentally
unfeasible POVM . However in it was shown
how this can be circumvented by using “simple measurements”, provided they can be altered in
run-time. Each measurement consists of a photon number measurement of the output arms, and
is parametrized by an additional, controllable phase shift of φ in the free arm – equivalently, the
unknown phase can be tweaked by a chosen φ. The optimal measurement process is an adaptive
strategy: an entangled N-photon state is prepared ), the photons are
sequentially injected into the interferometer, and photon numbers are measured. At each step, the
measurement performed is modiﬁed by choosing a diﬀering phase shift φ, which depends on previous
measurement outcomes. In , an explicit strategy was
52 This is often also expressed in terms of the variance (∆θ)2, so as N−2, rather than the standard deviation.
given, which achieves the Heisenberg scaling of the optimal order O(1/N). However, for N > 4 it
was shown this strategy is not strictly optimal.
This type of planning is hard as it reduces to the solving of non-convex optimization problems53.
The ﬁeld of ML deals with such planning problems as well, and thus many optimization techniques
have been developed for this purpose. The applications of such ML techniques, speciﬁcally particle
swarm optimization were ﬁrst suggested in pioneering works ,
and later in . In subsequent work, perhaps more well-known methods
of diﬀerential evolution have been demonstrated to be superior and more computationally eﬃcient
 .
3. Generalized Hamiltonian estimation settings
ML techniques can also be employed in signiﬁcantly more general settings of quantum process
estimation. More general Hamiltonian estimation settings consider a partially controlled evolution
given by HC(θ), where C is a collection of control parameters of the system. This is a reasonable
setting in e.g. the production of quantum devices, which have controls (C), but whose actual
performance (dependant on θ) needs to be conﬁrmed.
Further, since production devices are
seldom identical, it is beneﬁcial to even further generalize this setting, by allowing the unknown
parameters θ to be only probabilistically characterized. More precisely, they are probabilistically
dependent on another set of hyperparameters ζ = (ζ1, . . . , ζk), such that the parameters θ are
distributed according to a known conditional probability distribution P(θ|ζ). This generalized task
of estimating the hyperparameters ζ thus allows the treatment of systems with inherent stochastic
noise, when the inﬂuence of noise is understood (given by P(θ|ζ)). Such very general scenarios are
addressed in , relying on classical learning techniques of Bayesian experimental
design (BED) , combined with Monte Carlo methods. The details of this method
are beyond the scope of this review, but, roughly speaking, BED assumes a Bayesian perspective
on the experiments of the type described above. The estimation methods of the general problem
(ignoring the hyperparameters and noise, for simplicity, although the same techniques apply) realize
a conditional probability distribution P(d|θ; C) where d corresponds to experimental data, i.e.
measurement outcomes collected in the experiment. Assuming some prior distribution over hidden
parameters (P(θ)), the posterior distribution, given experimental outcomes, is given via Bayes
theorem by
P(θ|d; C) = P(d|θ; C)P(θ)
The evaluation of above is already non trivial, predominantly as the normalization factor P(d|C)
includes an integration over the parameter space. Further, of particular interest are scenarios where
an experiment is iterated many times. In this case, analogously to the adaptive setting for metrology
discussed above, it is beneﬁcial to tune the control parameters C dependent on the outcomes.
BED , tackles such adaptive settings, by selecting the subsequent control parameters
C as to maximize a utility function54, for each update step. The Bayes updates consist of the
53 The non-convexity stems from the fact that the eﬀective input state at each stage depends on previous measurements
performed. As the entire interferometer set-up can be viewed as a one-subsystem measurement, the conditional
states also depend on unknown parameters, and these are used in the subsequent stages of the protocol .
54 The utility function is an object stemming from decision theory and, in the case of BED it measures how well the
experiment improves our inferences. It is typically deﬁned by the prior-posterior gain of information as measured
by the Shannon entropy, although there are other possibilities.
computing of P(θ|d1, . . . , dl−1dk) ∝P(dk|θ)P(θ|d1, . . . , dl−1) at each step. The evaluation of the
normalization factor P(d|C) is, however, also non-trivial, as it includes an integration over the
parameter space. In this integration is tackled via numerical integration
techniques, namely sequential Monte Carlo, yielding a novel technique for robust Hamiltonian
estimation.
The robust Hamiltonian estimation method was subsequently expanded to use access to trusted
quantum simulators, which forms a more powerful and eﬃcient estimation scheme 55, which was also shown to be robust to moderate noise and imperfections in the trusted
simulators . A restricted version of the method of estimation with simulators
was experimentally realized in . More recently, connected to the methods
of robust Hamiltonian estimation, Bayesian and sequential Monte Carlo based estimation have
further been combined with particle swarm optimization techniques . There
the goal was to achieve reliable coupling strength and frequency estimation in simple decohering
systems, corresponding to realistic physical models. More speciﬁcally, the studied problem is the
estimation of ﬁeld-atom coupling terms, and the mode frequency term, in the Jaynes-Cummings
model. The controlled parameters are the local qubit ﬁeld strength, measurements are done via
swap spectroscopy.
Aside from using ML to perform partial process tomography of controlled quantum systems, ML
can also help in the genuine problems of quantum control, speciﬁcally, the design of target quantum
gates. This forms the subsequent topic.
B. Design of target evolutions
Executive summary: One of the main tasks quantum information is the design of target
quantum evolutions, including quantum gate design. This task can be tackled by quantum
control which studies controlled physical systems where certain parameters can be adjusted
during system evolution, or by using extended systems, and unmodulated dynamics. Here, the
underlying problem is an optimization problem, that is, the problem of ﬁnding optimal control
functions or extended system parameters, of a system which is otherwise fully speciﬁed. Under
realistic constraints these optimization tasks are often non-convex, thus hard for conventional
optimizers, yet amenable to advanced ML technologies. Target evolution design problems
can also be tackled by using feed-back from the actual experimental system, leading to the use of
on-line optimization methods and RL.
From a QIP perspective, one of the most important tasks is the design of elementary quantum gates,
needed for quantum computation. The paradigmatic approach to this is via quantum control, which
aims to identify how control ﬁelds of physical systems need to be adapted in time, to achieve desired
evolutions. The designing of target evolutions can also be achieved in other settings, e.g. by using
larger systems, and unmodulated dynamics. In both cases, ML optimization techniques can be used
to design optimal strategies, oﬀline. However, target evolutions can also be achieved in run-time,
by interacting with a tunable physical system, and without the need for the complete description of
55 This addition partially circumvents the computation of the likelihood function P(d|θ; C) which requires the
simulation of the quantum system, and is in fact, in general intractable.
the system. We ﬁrst consider oﬀ-line settings, and brieﬂy comment on the latter on-line settings
thereafter.
1. Oﬀ-line design
The paradigmatic setting in quantum control considers a Hamiltonian with a controllable (c) and
a drift part (dr), e.g. H(C(t)) = Hdr + C(t)Hc. The free part is modulated via a (real-valued)
control ﬁeld C(t). The resulting time-integrated operator U = U[C(t)] ∝exp
0 dtH(C(t))
over some ﬁnite time T, is a function of the chosen ﬁeld function C(t). The typical goal is to specify
the control ﬁeld C(t) which maximizes the transition probability from some initial state |0⟩to a
ﬁnal state |φ⟩, thus ﬁnd argmaxC| ⟨φ| U[C(t)] |0⟩| 56. Generically, the mappings C(t) 7→U[C(t)]
are highly involved, but nonetheless, empirically it was shown that greedy optimization approaches
provide optimal solutions (which is the reason why greedy approaches dominate in practice). This
empirical observation was later elucidated theoretically , suggesting that in
generic systems local minima do not exist, which leads to easy optimization for a more up-to-date account). This is good news for experiments, but also suggests
that quantum control has no need for advanced ML techniques. However, as is often the case with
claims of such generality, the underlying subtle assumptions are fragile which can often be broken.
In particular, greedy algorithms for optimizing the control problem as above can fail, even in the
low dimensional case, if we simply place rather reasonable constraints on the control function and
parameters. Already for 3-level and 2-qubit systems with constraints on the allowed evolution time
t, and the precision of the linearization of the time-dependent control parameters57, it is possible
to construct examples where greedy approaches fail, yet global (derivative-free) approaches, in
particular diﬀerential evolution, succeed .
Another example of hard oﬀ-line control concerns the design of high ﬁdelity single-shot three-qubit
gates58, which is in addressed using a specialized novel optimization
algorithm the authors called subspace-selective self-adaptive diﬀerential evolution (SuSSADE) .
An interesting alternative approach to gate design is by utilizing larger systems. Speciﬁcally designed
larger systems can naturally implement desired evolutions on a subsystem, without the need of
time-dependent control ). In other
words, local gates are realized despite the fact that the global dynamics is unmodulated. The
non-trivial task of constructing such global dynamics, for the Toﬀoli gate, is in 
tackled by a method which relies stochastic gradient descent, and draws from supervised learning
techniques.
2. On-line design
Complementary to oﬀ-line methods, here we assume access to an actual quantum experiment, and the
identiﬁcation of optimal strategies relies on on-line feedback. In these cases, the quantum experiment
56 An example of such additional ﬁelds would be controlled laser ﬁelds in ion trap experiments, and the ﬁeld function
C speciﬁes how the laser ﬁeld strengths are modulated over time.
57 It is assumed that the ﬁeld function C(t) describing parameter values as functions of time is step-wise constant,
split in K segments. The larger the value K is, the better is the approximation of a smooth function which would
arguably be better suited for greedy approaches.
58 This includes the Toﬀoli (and Fredkin) gate which is of particular interest as it forms a universal gate set together
with the simple single-qubit Hadamard transform (if ancillas qubits are used).
need not be fully speciﬁed beforehand. Further, the required methodologies lean towards on-line
planning and RL, rather than optimization. In the case optimization is required, the parameters of
optimization are diﬀerent due to experimental constraints, see for an extensive
treatment of the topic.
The connections between on-line methods which use feedback from experiments to “steer” systems
to desired evolutions, have been connected to ML in early works . These exploratory works deal with generic control problems via experimental
feedback, and have, especially at the time, remained mostly unnoticed by the community. In more
recent times, feedback-based learning and optimization has received more attention. For instance in
 the authors have explored the applicability of a modiﬁed Q-learning algorithm
for RL (see section II.C) on canonical control problems. Further, the potential of RL methods had
been discussed in the context of optimal parameter estimation, but also typical optimal control
scenarios in . In the latter work, the authors also provide a concise
yet extensive overview of related topics, and outline a perspective which uniﬁes various aspects of ML
and RL in an approach to resolve hard quantum measurement and control problems. In , RL based on PS updates was analyzed in the context of general control-and-feedback
problems. Finally, ideas of uniﬁed computational platforms for quantum control, albeit without
explicit emphasis on ML techniques had been previously provided in .
In the next section, we further coarse-grain our perspective, and consider scenarios where ML
techniques control various gates, and more complex processes, and even help us learn how to do
interesting experiments.
C. Controlling quantum experiments, and machine-assisted research
Executive summary: ML and RL techniques can help us control complex quantum systems, devices, and even quantum laboratories. Furthermore, almost as a by-product, they
may also help us to learn more about the physical systems and processes studied
in an experiment. Examples include adaptive control systems (agents) which learn how to
control quantum devices, e.g. how to preserve the memory of a quantum computer, combat
noise processes, generate entangled quantum states, and target evolutions of interest.
In the process of learning such optimal behaviours even simple artiﬁcial agents also learn, in an
implicit, embodied embodied, sense, about the underlying physics, which can be used
by us to obtain novel insights. In other words artiﬁcial learning agents can genuinely
help us do research.
The prospects of utilizing ML and AI in quantum experiments have been investigated also for
“higher-level” experimental design problems. Here one considers automated machines that control
complex processes which e.g. specify the execution of longer sequences of simple gates, or the
execution of quantum computations. Moreover, it has been suggested that learning machines can
be used for, and integrated into, the very design of quantum experiments, thereby helping us in
conducting genuine research. We ﬁrst present two results where ML and RL methods have been
utilized to control more complex processes (e.g. generate sequences of quantum gates to preserve
memory), and consider the perspectives of machines genuinely helping in research thereafter.
1. Controlling complex processes
The simplest example of involved ML machinery used to generate control of slightly more complex
systems was done in the context of is the problem of dynamical decoupling for quantum memories.
In this scenario, a quantum memory is modelled as a system coupled to a bath (with a local
Hamiltonian for the system (HS) and the bath HB), and decoherence is realized by a coupling
term HSB; the local unitary errors are captured by HS. The evolution of the total Hamiltonian
Hnoise = HS + HB + HSB would destroy the contents of the memory, but this can be mitigated
by adding a controllable local term HC acting on the system alone59. Certain optimal choices
of the control Hamiltonian HC are known. For instance, we can consider the scenario where HC
is modulated such that it implements instantaneous60 Pauli-X and Pauli-Y unitary operations,
sequentially, at intervals ∆t. As this interval, which is also the time of the decoherence-causing free
evolution, approaches zero, so ∆t →0, this process is known to ensure perfect memory. However, the
moment the setting is made more realistic, allowing ﬁnite ∆t times, the space of optimal sequences
becomes complicated. In particular, optimal sequences start depending on ∆t, the form of the noise
Hamiltonian, and total evolution time.
Probe play
Measurement angles φ
PS Learning agent
Adapted measurements
φ’ performed
FIG. 11 The learning agent learns how to
correctly perform MBQC measurements in
an unknown ﬁeld.
To identify optimal sequences, in ,
the authors employ recurrent NNs, which are trained as a
generative model – meaning they are trained to generate
sequences which minimize ﬁnal noise. The entire sequences
of pulses (Pauli gates) which the networks generated were
shown to outperform well-known sequences.
In a substantially diﬀerent setting, where interaction necessarily arises, the authors studied how AI/ML techniques
can be used to make quantum protocols themselves adaptive. Speciﬁcally, the authors applied RL methods based
on PS (see section
VII.A) to the task of protecting quantum computation
from local stray ﬁelds . In MBQC
 , the
computation is driven by performing adaptive single-qubit
projective measurements on a large entangled resource
state, such as the cluster state . In a scenario where the resource
state is exposed to a stray ﬁeld, each qubit undergoes a local rotation. To mitigate this, in , the authors introduce learning agent, which “plays” with a local probe qubit, initialized
in say the +1 eigenstate of σx, denoted |+⟩, learning how to compensate for the unknown ﬁeld.
In essence, given a measurement, the agent chooses a diﬀerent measurement, obtaining a reward
whenever a +1 outcome is observed. The agent is thus trained to compensate for the unknown ﬁeld,
and serves as an “interpreter” between desired measurements and the measurements which should
be performed in the given setting (i.e. in the given ﬁeld with given frequency of measurements
(∆t)), see Fig. 11. The problem of mitigating such ﬁxed stray ﬁelds could naturally be solved
59 For the sake of intuition, a frequent application of X gates, referred to as bang-bang control, on a system which is
freely evolving with respect to σz eﬀectively ﬂips the direction of rotation of the system Hamiltonian, eﬀectively
undoing its action.
60 By instantaneous we mean that it is assumed that the implementation requires no evolution time, e.g. by using
inﬁnite ﬁeld strengths.
with non-adaptive methods where we use the knowledge about the system to solve our problem, by
e.g. measuring the ﬁeld and adapting accordingly, or by using fault-tolerant constructions. From a
learning perspective, such direct methods have a few shortcomings which may be worth presenting
for didactic purposes. Fault tolerant methods are clearly wasteful, as they fail to gain utilize any
knowledge about the noise processes. In contrast, ﬁeld estimation methods learn too much, and
assume a model of the world. To clarify the latter, to compensate the measured ﬁeld, we need
to use quantum mechanics, speciﬁcally the Born rule. In contrast, RL approach is model-free: the
Born rule plays no part, and “correct behavior” is learned, and established exclusively based on
experience. This is conceptually diﬀerent, but also operatively critical, as model-free approaches
allow for more autonomy and ﬂexibility (i.e. the same machinery can be used in more settings
without intervention)61. Regarding learning too much, one of the basic principles of statistical
learning posits that “when solving a problem of interest, one should not solve a more general problem
as an intermediate step” , which is intuitive. The problem of the presented setting
is “how to adapt the measurement settings,” and not “characterize the stray ﬁelds”. While in the
present context, the information-theoretic content of the two questions may be the same, it should
easy to imagine that if more complex ﬁelds are considered, full process characterization contains a
lot more information than needed to optimally adapt the local measurements. The approaches of
 can further be generalized to utilize information from stabilizer measurements
 , or similarly outcomes of syndrome measurements when codes are utilized
 , (instead of probe states) to similar ends. Addressing somewhat related
problems, but using supervised learning methods, the authors in have shown
how to compensate for qubit decoherence (stochastic evolution) also in experiments .
2. Learning how to experiment
One of the ﬁrst examples of applications of RL in QIP appears in the context of experimental
photonics, where one of the current challenges lies in the generation of highly entangled, high
dimensional, multi-party states. Such states are generated on optical tables, the conﬁguration
of which, to generate complex quantum states, can be counter-intuitive and unsystematic. The
searching for conﬁgurations which are interesting can be mapped to a RL problem, where a learning
agent is rewarded whenever it generates an interesting state (in a simulation). In a precursor work
 , the authors used a feedback-assisted search algorithm to identify previously
unknown conﬁgurations which generate novel highly entangled states. This demonstrated that
the design of novel quantum experiments can also be automatized, which can signiﬁcantly aid in
research. This idea given in the context of optical tables, has subsequently been combined with
earlier proposals to employ AI agents in quantum information protocols and as ”lab robots” in
future quantum laboratories . This led to the application of more advanced RL
techniques, based on the PS framework, for the tasks of understanding the Hilbert space accessible
with optical tables, and the autonomous machine-discovery of useful optical gadgets . Related to the topic of learning new insight from experimenting machines, in the authors consider the problem of preparing target states by means of chosen pulses
61 Indeed, the authors also show that correct behavior can be established when additional unknown parameters are
introduced, like time-and-space dependent ﬁelds for results), where hand-crafted methods
would fail.
implementing (a restricted set) of rotations. This is a standard control task, and authors show that
RL achieves respectable and sometimes near-optimal results. However, for our purposes, the most
relevant aspects of this work pertain to the fact that the authors also illustrate how of ML/RL
techniques can be used to obtain new insights in quantum experiments, and non-equilibrium physics,
by circumventing human intuition which can be ﬂawed. Interestingly, the authors also demonstrate
the reverse, i.e. how physics insights can help elucidate learning problems62.
D. Machine learning in condensed-matter and many-body physics
Executive summary: One of the quintessential problems of many-body physics is the identiﬁcation of phases of matter. A popular overlap between ML and this branch of physics
demonstrates that supervised and unsupervised systems can be trained to classify diﬀerent phases.
More interestingly, unsupervised learning can be used to detect phases, and even discover
order parameters – possibly genuinely leading to novel physical insights. Another important
overlap considers the representational power of (generalized) neural networks, to characterize interesting families of quantum systems. Both suggest a deeper link between
certain learning models, on the one side, and physical systems, on the other side, the scope of
which is currently an important research topic.
ML techniques have, over the course of last 20 years, become an indispensable toolset of many
natural sciences which deal with highly complex systems. These include biology (speciﬁcally genetics,
genomics, proteomics, and the general ﬁeld of computational biology) ,
medicine (e.g. in epidemiology, disease development, etc.) ,
chemistry , high energy and particle physics . Unsurprisingly,
they have also permeated various aspects of condensed matter and many-body physics. Early
examples of this were proposed in the context of quantum chemistry and density functional theory
 , or for the approximation
of the Green’s function of the single-site Anderson impurity model . The
interest in connections between NNs and many-body and condensed matter physics has undergone
immense growth since. Some of the results which we cover next deviate from the primary topic of
this review, those concerning the overlaps of QIP and ML. However, since QIP, condensed matter,
and many-body physics share signiﬁcant overlaps we feel it is important to at least brieﬂy ﬂesh out
the basic ideas.
One of the basic lines of research in this area deals with the learning of phases of matter, and the
detection of phase transitions in physical systems. A canonical example is the discrimination of
samples of conﬁgurations stemming from diﬀerent phases of matter, e.g. Ising model conﬁgurations
of thermal states below, or above the critical temperature. This problem has been tackled using
principal component analysis and nearest neighbour unsupervised learning techniques 
 ).
Such methods also have the potential to, beyond just detecting
phases, actually identify order parameters – in the above case, magnetization. More
complicated discrimination problems, e.g. discriminating Coulomb phases, have been resolved
62 For instance, the authors investigate the strategies explored by the learning agent, and identify spin-glass like phase
transition in the space of protocols as a function of the protocol duration. This highlights the diﬃculty of the
learning problem.
using basic feed-forward networks, and convolutional NNs were trained to detect topological phases,
 , but also phases in fermionic systems on cubic lattices . Neural networks have also been combined with quantum Monte Carlo methods , and with unsupervised methods ), in both cases to improve classiﬁcation performance in various systems. It is notable that all
these methods prove quite successful in “learning” phases, without any information of the system
Hamiltonian. While the focus in this ﬁeld had mostly been on neural network architectures, other
supervised methods, speciﬁcally kernel methods (e.g. SVMs) had been used for the same purpose
 . Kernel methods may be in some cases advantageous as they can have a
higher interpretability: it is often easier to understand the reason behind the optimal model in the
cases of kernel methods, rather than NNs, which also means that learning about the underlying
physics may be easier in the cases of kernel methods. Note that this will most likely be challenged
by deep NN approaches in years to come.
A partial explanation behind the success of neuronal approaches for classifying phases of matter may
lie in their form. Speciﬁcally, they may have the capacity to encode important properties of physical
systems both in the classical in quantum case. This motivates the second line of research we mention
in this context. BMs, even in their restricted variant, are known to have the capacity to encode
complicated distributions. In the same sense, restricted BMs, extended to accept complex weights
(i.e. the weights wij in Eqs. (2) and (3)) encode quantum states, and the hidden layer captures
correlations, both classical and quantum (entanglement). In it was shown
that this approach describes equilibrium and dynamical properties of many prototypical systems
accurately: that is, restricted BMs form a useful ansatz for interesting quantum states (called
neural-network quantum states (NQS)), where the number of neurons in the hidden layer controls
the size of the representable subset of the Hilbert space. This is analogous to how, for instance,
the bond dimension controls the scope of the matrix product state ansatz .
This property can also be exploited in order to achieve eﬃcient quantum state tomography63 . In subsequent works, the authors have also analyzed the structure of entanglement
of NQS states , and have provided analytic proofs of the representation power
of deep restricted BMs, proving they can e.g. represent ground states of any k-local Hamiltonians
with polynomial-size gaps . It is worthwhile to note that representational
powers of standard variational representations (e.g. that of the variational renormalization group)
had previously been contrasted to those of deep NNs , with the goal of
elucidating the success of deep networks. Related to this, the Tensor Network formalism has been used for the eﬃcient description of deep
convolutional arithmetic circuits, establishing also a formal connection between quantum many-body
states and deep learning . Very recently, the intersection between ML and
many-body quantum physics have also inspired research into ML-motivated entanglement witnesses
and classiﬁers , and also into furthering the connections
between ML and many-body physics, speciﬁcally, entanglement theory. These recent results have
positioned NNs as one of the most exciting new techniques to be applied in the context of both
condensed-matter and many-body physics. Additionally, they also show the potential of the converse
direction of inﬂuence – the application of mathematical formalism of many-body physics for the
deepening of our understanding of complex learning models.
63 This method can be thought of as eﬀectively by assigning a prior stating that the analyzed state is well approximated
V. QUANTUM GENERALIZATIONS OF MACHINE LEARNING CONCEPTS
The onset of quantum theory necessitated a change in how we describe physical systems, but
also a change in our understanding of what information is64. Quantum information is a more
general concept, and QIP exploits the genuine quantum features for more eﬃcient processing (using
quantum computers) and more eﬃcient communication. Such quintessential quantum properties,
such as the fact that even pure states cannot be perfectly copied , are
often argued to be at the heart of many quantum applications, such as cryptography. Similarly,
quintessential information processing operations are more general in the quantum world: closed
quantum systems can undergo arbitrary unitary evolutions, whereas the corresponding classical
closed-system evolutions correspond to the (ﬁnite) group of permutations65.
The majority of
ML literature deals with learning from, and about data – that is, classical information. This
section examines the question of what ML looks like, when the data (and perhaps its processing) is
fundamentally quantum. We will ﬁrst explore quantum generalizations of supervised learning, where
the “data-points” are now genuine quantum states. This generates a plethora of scenarios which are
indistinguishable in the classical case (e.g. having one or two copies of the same example is not the
same!). Next, we will consider another quantum generalization of learning, where quantum states are
used to represent the generalizations of unknown concepts in CLT – thus we talk about the learning
of quantum states. Following this we will present some results on quantum generalizations of
POMDP’s which could lead to quantum-generalized reinforcement learning (although this actually
just generalizes the mathematical structure).
A. Quantum generalizations: machine learning of quantum data
Executive summary: A signiﬁcant fraction of the ﬁeld of ML deals with data analysis, classi-
ﬁcation, clustering, etc. QIP generalizes standard notions of data, to include quantum states.
The processing of quantum information comes with restrictions (e.g. no-cloning or no-deleting),
but also new processing options. This section addresses the question of how conventional
ML concepts can be extended to the quantum domain, mostly focusing on aspects of
supervised learning and learnability of quantum systems, but also concepts underlying RL.
One of the basic problems of ML is that of supervised learning, where a training set D = {(xi, yi)}i is
used to infer a labeling rule mapping data points to labels xi
→yi (see section I.B for more details).
More generally, supervised learning deals with classiﬁcation of classical data. In the tradition of
QIP, data can also be quantum – that is, all quantum states carry, or rather represent, (quantum)
information. What can be done with datasets of the type {(ρi, yi)}i, where ρi is a quantum state?
Colloquially it is often said that one of the critical distinction between classical and quantum data is
that quantum data cannot be copied. In other words, having one instance of an example, by notation
abuse denoted (ρi ⊗yi), is not generally as useful as having two copies (ρi ⊗yi)⊗2. In contrast in
the case of classiﬁcation with functional labeling rules, this is the same. The closest classical analog
64 Arguably, in the light of the physicalistic viewpoint on the nature of information, which posits that “Information is
[ultimately] physical”.
65 Classical evolutions are guaranteed to transform computational basis states (the “classical states”) to computational
basis states, and closed-system implies the dynamics must be reversible, leaving only permutations.
of dealing with quantum data is the case where labelings are not deterministic, or equivalently,
where the conditional distribution P(label|datapoint) is not extremal (Dirac). This is the case of
classiﬁcation (or learning) of random variables, or probabilistic concepts, where the task is to produce
the best guess label, specifying the random process which “most likely” produced the datapoint66.
In this case, having access to two examples in the training phase which are independently sampled
from the same distribution is not the same as having two copies of one and the same individual
sample–these are perfectly correlated and carry no new information67. To obtain full information
about a distribution, or random variable, one in principle needs inﬁnitely many samples. Similarly,
in the quantum case, having inﬁnitely many copies of the same quantum state ρ is operatively
equivalent to having a classical description of the given state.
Despite similarities, quantum information is still diﬀerent from mere stochastic data. The precursors
of ML-type classiﬁcation tasks can be identiﬁed in the theories of quantum state discrimination,
which we brieﬂy comment on ﬁrst. Next, we review some early works dealing with “quantum pattern
matching” which spans various generalizations of supervised settings, and ﬁrst works which explicitly
propose the study of quantum-generalized machine learning. Next, we discuss more general results,
which characterize inductive learning in quantum settings. Finally, we present a CLT perspective on
learning with quantum data, which addresses the learnability of quantum states.
1. State discrimination, state classiﬁcation, and machine learning of quantum data
a. State discrimination
The entry point to this topic can again be traced to seminal works of
Helstrom and Holevo as the problems of state discrimination can
be rephrased as variants of supervised learning problems. In typical state discrimination settings,
the task is the identifying of a given quantum state (given as an instance of a quantum system
prepared in that state), under the promise that it belongs to a (typically ﬁnite) set {ρi}i, where the
set is fully classically speciﬁed. Recall, state estimation, in contrast, typically assumes continuous
parametrized families, and the task is the estimation of the parameter. In this sense, discrimination
is a discretized estimation problem68, and the problems of identifying optimal measurements (under
various ﬁgures of merit), and success bounds have been considered extensively and continuously
throughout the history of QIP .
Remark: Traditional quantum state discrimination can be rephrased as degenerate supervised
learning setting for quantum states. Here, the space of “data-points” is restricted to a ﬁnite (or
parametrized) family {ρi}i, and the training set contains an eﬀective inﬁnite number of examples
D = {(ρi, i)⊗∞}; naturally, this notation is just a short-hand for having the complete classical
description of the quantum states 69. In what follows we will sometimes write ρ⊗∞to denote a
quantum system containing the classical description of the density matrix ρ.
66 Note that in this setting we do not have the descriptions of the stochastic processes given a-priory – they are to be
inferred from the training examples.
67 In this sense, no-cloning theorem also applies to classical information: an unknown random variable cannot be
cloned. In QIP language this simply means that no-cloning theorem applies to diagonal density matrices, i.e.
ρ ̸→ρ ⊗ρ, even when ρ is promised to be diagonal.
68 Intuitively, estimation is to discrimination, what regression is to classiﬁcation in the ML world.
69 From an operative, and information content perspective, having inﬁnitely many copies is equivalent to having a
full classical description: inﬁnite copies are suﬃcient and necessary for perfect tomography – yielding the exact
classical description – whereas having an exact classical description is suﬃcient and necessary for generating an
unbounded copy number.
b. Quantum template matching – classical templates
A variant of discrimination, or class assignment
task, which is one of the ﬁrst instances of works which establish explicit connections with ML and
discrimination-type problems, is “template matching” . In this pioneering work,
the authors consider discrimination problems where the input states ψ may not correspond to the
(known) template states {ρi}i, and the correct matching label is determined by the largest the
Uhlmann ﬁdelity. More precisely, the task is deﬁned as follows: given a classically speciﬁed family
of template states {ρi}i, given M copies of a quantum input ψ⊗M, output the label icorr deﬁned
with icorr = argmaxiTr
. In this original work, the authors focused on two-class cases,
with pure state inputs, and identify fully quantum, and semi-classical strategies for this problem.
“Fully quantum strategies” identify the optimal POVM. Semi-classical strategies impose a restriction
of measurement strategies to separable measurements, or perform state estimation on the input, a
type of “quantum feature extraction”.
c. Quantum template matching – quantum templates.
In a generalization of the work in , the authors in consider the case where instead of having access to
the classical descriptions of the template states {ρi}i, we are given access to a certain number K
of copies. In other words, we are given access to a quantum system in the state N
.. Setting
K →∞, recovers the case with classical templates. This generalized setting introduces many
complications, which do not exist in the “more classical” case with classical templates. For instance,
classifying measurements now must “use up” copies of template states, as they too cannot be cloned.
The authors identify various ﬂavors of semi-classical strategies for this problem. For instance, if the
template states are ﬁrst estimated, we are facing the scenario of classical templates (albeit with
error). The classical template setting itself allows semiclassical strategies, where all systems are
ﬁrst estimated, and it allows coherent strategies. The authors ﬁnd optimal solutions for K = 1,
and show that there exists a fully quantum procedure that is strictly superior to straightforward
semiclassical extensions.
Remark: Quantum template matching problems can be understood as quantum-generalized supervised learning, where the training set is of the form {(ρ⊗K
, i)i}, data beyond the training set
comes from the family
(number of copies is known), and the classes are deﬁned via minimal
distance, as measured by the Uhlmann ﬁdelity. The case K →∞approaches the special case of
classical templates. Restricting the states ψ to the set of template states (restricted template
matching), and setting M = 1 recovers standard state discrimination.
d. Other known optimality results for (restricted) template matching
For the restricted matching case,
where the input is promised to be from the template set, the optimal solutions for the two-class
setting, minimum error ﬁgure of merit, and uniform priors of inputs, have been found in for the qubit case. In the authors
found optimal solutions for the unambiguous discrimination case70. An asymptotically optimal
strategy restricted matching with ﬁnite templates K < ∞, for arbitrary priors, and mixed qubit
states was later found in . This work also provides a solid introduction
70 In unambiguous discrimination, the device is allowed to output an ambiguous “I do not know” outcome, but is not
allowed to err in the case it does output an outcome. The goal is to minimize the probability of the ambiguous
to the topic, a review of quantum analogies for statistical learning, and emphasizes connections to
ML methodologies and concepts.
Later, in the authors introduced and compared all three strategies: classical
estimate-and-discriminate, classical optimal, and quantum strategy, for the restricted template
matching case with ﬁnite templates. Recall, the adjective “classical” here denotes that the training
states are fully measured out as the ﬁrst step – the quantum set is converted to classical information,
meaning that no quantum memory is further required, and that the learning can be truly inductive.
A surprising result is that the intuitive estimate-and-discriminate strategy, which reduces supervised
classiﬁcation to optimal estimation coupled with a (standard) quantum state discrimination problem,
is not optimal for learning. Another measurement provides not only better performance, but matches
the optimal quantum strategy exactly (as opposed to asymptotically). Interestingly, the results
of and opposite claims for essentially the same
setting: no separation, vs. separation between coherent (fully quantum) and semi-classical strategies,
respectively. This discrepancy is caused by diﬀerences in the chosen ﬁgures of merit, and a diﬀerent
deﬁnition of asymptotic optimality , and serves as an eﬀective reminder of the subtle
nature of quantum learning. Optimal strategies had been subsequently explored in other settings
as well, e.g. when the data-set comprises coherent states , and or in the cases
where an error margin is in an otherwise unambiguous setting .
e. Quantum generalizations of (un)supervised learning
The works of the previous paragraph consider
particular families of generalizations of supervised learning problems. The ﬁrst attempts to classify
and characterize what ML could look like in a quantum world from a more general perspective
was, however, ﬁrst explicitly done in . There, the basic object introduced is the
database of labeled quantum or classical objects, i.e. DK
n = {(|ψi⟩⊗i , yi)}n
71, which may come
in copies. Such a database can, in general then be processed to solve various types of tasks, using
classical or quantum processing. The authors propose to characterize quantum learning scenarios
in terms of classes, denoted Lcontext
. Here context may denote we are dealing with classical or
quantum data and whether the learning algorithm is relying on quantum capabilities or not. The
goal speciﬁes the learning task or goal (perhaps in very broad terms). Examples include Lc
corresponds to standard classical ML, and Lq
c, which could mean we use a quantum computer to
analyze classical data. The example of template matching classical templates (K = ∞) considered earlier in this section would be denoted Lc
q, and the generalization with ﬁnite
template numbers K < ∞would ﬁt in L⊗K
. While the formalism above suggests focus on supervised
settings, the authors also suggest that datasets could be inputs for (unsupervised) clustering. The
authors further study quantum algorithms for determining closeness of quantum states72, which
could be the basic building block of quantum clustering algorithms, and also compute certain error
bounds for special cases of classiﬁcation (state discrimination) using well known results of Helstrom
 . Similar ideas were used in for the purpose of deﬁnition
of a quantum decision tree algorithm for data classiﬁcation in the quantum regime.
The strong connection between quantum-generalized learning theory sketched out in and the classical73 theory of Helstrom was more deeply explored in (Gambs,
71 Such a dataset can be stored in, or instantiated by, a 2-n partite quantum system, prepared in the state
i=1 |ψi⟩⊗Ki |yi⟩.
72 These are based on the SWAP-test (see section VI.C.2), in terms of Uhlmann ﬁdelity
73 Here we mean classical in the sense of “being a classic”, rather than pertaining to classical systems.
2008). There, the author computed the lower bounds of sample complexity – in this case the minimal
number of copies K – needed to solve a few types of classiﬁcation problems. For this purpose the
author introduced a few techniques which reduce ML-type classiﬁcation problems to the settings
where theory of could be directly applied. These types of results contribute to the
establishing of a deeper connection between problems of ML and techniques of QIP.
f. Quantum inductive learning
Recall that inductive, eager learning, produces a best guess classiﬁer
which can be applied to the entire domain of data-points, based on the training set. But, already
the results of discussed in paragraph on template matching with quantum
templates, point to problems with this concept in the quantum realm – the optimal classiﬁer may
require a copy of the quantum data-points to perform classiﬁcation, which seemingly prohibits
unlimited use. The perspectives of such quantum generalizations of supervised learning in its
inductive form, were recently addressed from a broad perspective . Recall
that inductive learning algorithms, intuitively, use only the training set to specify a hypothesis (the
estimation of the true labeling function). In contrast, in transductive learning, the learner is also
given the data points the labels of which are unknown. These unlabeled points may correspond
to the cross-validation test set, or the actual target data. Even though the labels are unknown,
they carry additional information of the complete dataset which can be helpful in identifying the
correct labeling rule 74. Another distinction is that transductive algorithms need only label the given
points, whereas inductive algorithms need to specify a classiﬁer, i.e., a labeling function, deﬁned on
the entire space of possible points. In , the authors notice that the property
of an algorithm being inductive corresponds to a non-signaling property75, using which they can
prove that “being inductive” (i.e. being “no signalling”) is equivalent to having an algorithm which
outputs a classiﬁer h based on the training set alone, which is then applied to every training instance.
A third equivalent characterization of inductive learning is that the training and testing cleanly
separate as phases. While these observations are quite intuitive in the classical case, they are in
fact problematic in the quantum world. Speciﬁcally, if the training examples are quantum objects,
quantum no-cloning, in general, prohibits the applying of a hypothesis function (candidate labeling
function) h arbitrarily many times. This is easy to see since each instance of h must depend on the
quantum data in some non-trivial way, if we are dealing with a learning algorithm. Multiple copies
of h would then require multiple copies of (at least parts of) the quantum data.
A possible implication of this would be that, in the quantum realm, inductive learning cannot be
cleanly separated into training and testing. Nonetheless, the authors show that the no-signalling
criterion, for certain symmetric measures of performance, implies that a separation is, asymptotically,
possible. Speciﬁcally, the authors show that for any quantum inductive no-signalling algorithm A
there exists another, perhaps diﬀerent algorithm A′ which does separate in a training and testing
phase and which, asymptotically, attains the same performance . Such a
protocol A′, essentially, utilizes a semi-classical strategy. In other words, for inductive settings,
classical intuition survives, despite no-cloning theorems.
74 For instance, a transductive algorithm may use unsupervised clustering techniques to assign labels, as the whole set
is given in advance.
75 The outcome of the entire learning and evaluation process can be viewed as a probability distribution P(y) =
P(y1 . . . yk|x1 . . . xk; A), where A is the training set, x1, . . . xk are the points of the test state and y1 . . . yk the
respective labels the algorithm assigns with the probability P(y). No signaling implies that the marginal distribution
for the kth test element P(yk) only depends on xk and the training set, but not on other test points {xl}l̸=k.
2. Computational learning perspectives: quantum states as concepts
The previous subsections addressed the topics of classiﬁcation of quantum states, based on quantum
database examples. The overall theory, however, relies on the assumption that there exists a labeling
rule, which generates such examples, and what is learned is the labeling rule. This rule is also
known as concept, in CLT (e.g. PAC learning, see section II.B.1 for details). A reasonable suﬃcient
criterion is, if one can predict the probabilities of outcomes of any two-outcome measurements on
this state, as this already suﬃces for a full tomographic reconstruction. What would “the learning
of quantum states” mean, from this perspective? What does it mean to “know a quantum state”? A
natural criterion is that one “knows” a quantum state, if one can predict the measurement outcome
probabilities of any given measurement. In , the author addressed the question
of the learnability of quantum states in the sense above, where the role of a concept is played by
a given quantum state, and “knowing” the concept then equates to the possibility of predicting
the outcome probability of a given measurement and its outcome. One immediate distinction from
conventional CLT, discussed in II.B.1, is that the concept range is no longer binary. However, as as
we clariﬁed, classical CLT theory has generalizations with continuous ranges. In particular, so called
p-concepts have range in , and quantities which are analogs of the
VC-dimension, and analogous theorems relating this to generalization performance, exist for the
p-concept case as well ). Explicitly, the basic elements of such the generalized
theory are: domain of concepts X, a sample x ∈X and the p-concept f : X → . These abstract
objects are mapped to central objects of quantum information theory as follows:
the domain of concepts is the set of two-outcome quantum measurement, and a sample is a POVM
element Π76 (in short: x ↔Π); the p-concept to be learned is a quantum state ψ and the evaluation
of the concept/hypothesis on the sample corresponds to the probability Tr[Πψ] ∈ of observing
the measurement outcome associated with Π when the state ψ is measured.
To connect the data classiﬁcation-based perspectives of supervised learning to the CLT perspective
above, note that in the given quantum state CLT this framework, the quantum concept – quantum
state – “classiﬁes” quantum POVM elements (the eﬀects) according to the probability of observing
that eﬀect. The training set elements for this model are of the form (Π, Tr(ρΠ)), with 0 ≤Π ≤1.
In the spirit of CLT, the concept class “quantum states”, is said to be learnable under some
distribution D over two-outcome generalized measurement elements (Π), if for every concept –
quantum state ρ – there exists an algorithm with access to examples of the form (Π, Tr(ρΠ)), where
Π is drawn according to D, which outputs a hypothesis h which (approximately) correctly predicts
the label Tr(ρΠ′) with high probability, when Π′ is drawn from D. Note that the role of a hypothesis
here can simply be played by a “best guess” classical description of the quantum state ρ. The key
result of is that quantum states are learnable with sample complexity scaling only
linearly in the number of qubits77, that is logarithmically in the dimension of the density matrix. In
operative terms, if Alice wishes to send an n qubit quantum state to Bob who will perform on it a
two-outcome measurement (and Alice does not know which), she can achieve near-ideal performance
by sending (O(n)) classical bits78, which has clear practical but also theoretical importance. In some
sense, these results can also be thought of as a generalized variant of Holevo bound theorems (Holevo,
76 More precisely Π is a positive-semideﬁnite operator such that 1 −Π is positive-semideﬁnite as well.
77 The dependencies on the allowed inverse error and inverse allowed failure probability are polynomial and polylogarithmic, respectively.
78 Here we assume Alice can locally generate her states at will. A classical strategy (using classical channels) is thus
always possible, by having Alice send the outcomes of full state tomography (or equiv. the classical description of
the state), but this requires the using of O(2n) bits already for pure states.
1982), limiting how much information can be stored and retrieved in the case of quantum systems.
This latter result has thus far been more inﬂuential in the contexts of tomography than quantum
machine learning, despite being quite a fundamental result in quantum learning theory. However,
for fully practical purposes. The results above come with a caveat. The learning of quantum state is
eﬃcient in sample complexity (e.g. number of measurements one needs to perform), however, the
computational complexity of the reconstruction of the hypothesis is, in fact, likely exponential
in the qubit number. Very recently, the eﬃciency of also the reconstruction algorithms for the
learning of stabilizer states was shown in .
B. (Quantum) learning and quantum processes
Executive summary: The notion of quantum learning has been used in literature to refer to
the studying of various aspects of “learning about” quantum systems. Beyond the learning of
quantum states, one can also consider the learning of quantum evolutions. Here “knowing”
is operatively deﬁned as having the capacity to implement the given unitary at a later point – this
is similar to how “knowning” in computational learning theory implies we can apply the concept
function at a later point. Finally, as learning can pertain to learning in interactive environments –
RL – one can consider the quantum generalizations of such settings. One of the ﬁrst results in
this direction formulates a quantum generalization of POMDPs. Note as POMDPs form
the mathematical basis of RL, the quantum-generalized mathematical object – quantum POMDP,
may form a basis of quantum-generalized RL.
a. Learning of quantum processes
The concept of learning is quite diﬀuse and “quantum learning”
has been used in literature quite often, and not every instance corresponds to generalizations of
“classical learning” in a machine or statistical learning sense. Nonetheless, some such works further
illustrate the distinctions between the approaches one can employ with access to classical (quantum)
tools, while learning about classical or quantum objects.
Learning unitaries For instance “quantum learning of unitary operations” has been used to refer
to the task of optimal storing and retrieval of unknown unitary operations, which is a two stage
process. In the storing phase, one is given access to a few uses of some unitary U. In the retrieval
phase, one is asked to approximate the state U |ψ⟩, given one or few instances of a (previously
fully unknown) state |ψ⟩. Like in the case of quantum template states (see section V.A.1), we can
distinguish semi-classical prepare-and-measure strategies (where U is estimated and represented as
classical information), and quantum strategies, where the unitaries are applied on some resource
state, which is used together with the input state |ψ⟩in the retrieval stage. There is no simple
universal answer to the question of optimal strategies. In , the authors have shown
that, under reasonable assumptions, the surprising result that optimal strategies are semi-classical.
In contrast, in the same question was asked for generalized measurements, and
the opposite was shown: optimal strategies require quantum memory. See e.g. 
for some recent results on probabilistic unitary storage and retrieval, which can be understood as
genuinely quantum learning 79 of quantum operations.
Learning measurements The problem of identifying which measurement apparatus one is facing
has ﬁrst been in comparatively fewer works, see e.g. for a more recent
example. Related to this, we encounter a more learning-theoretical perspective on the topic of
learning measurements. In the comprehensive paper (which can serve as a
review of parts of quantum ML in its own right), the authors explore the question of the learnability
of quantum measurements. This can be thought of as the dual of the task of learning quantum
states discussed previously in this section. Here, the examples are of the form (ρ, Tr(ρE)), and it is
the measurement that is ﬁxed. In this work, the authors compute a number of complexity measures,
which are closely related to the VC dimension (see section II.B.1), for which sample complexity
bounds are known. From such complexity bounds one can, for instance, rigorously answer various
relevant operative questions, such as, how many random quantum probe states we need to prepare on
average, to accurately estimate a quantum measurement. Complementing the standard estimation
problems, here we do not compute the optimal strategy, but eﬀectively gauge the information gain of
a randomized strategy. These measures are computed for the family of hypotheses/concepts which
can be obtained by either ﬁxing the POVM element (thus learning the quantum measurement), or
by ﬁxing the state ), and clearly illustrate the power of ML
theory when applied in QIP context.
b. Foundations of quantum-generalized RL
The majority of quantum generalizations of machine
learning concepts ﬁt neatly in the domain of supervised learning, however, with few notable
exceptions. In particular, in , the authors introduce a quantum generalization of
partially observable Markov decision processes (POMDP), discussed in section II.C. For convenience
of the reader we give a brief recap of these objects. A fully observable MDP is a formalization of
task environments: the environment can be in any number of states S the agent can observe. An
action a ∈A of the agent triggers a transition of the state of the environment – the transition can be
stochastic, and is speciﬁed by a Markov transition matrix P a.80 Additionally, beyond the dynamics,
each MDP comes with a reward function R : S ×A×S →Λ, which rewards certain state-action-state
transitions. In POMDP, the agent does not see the actual state of the environment, but rather just
observations o ∈O, which are (stochastic) functions of the environmental state81. Although the
exact environmental state of the environment is not directly accessible to the agent, given the full
speciﬁcation of the system, the agent can still assign a probability distribution over the state space
given an interaction history. This is called a belief state, and, can be represented as a mixed state
(mixing the “classical” actual environmental states), which is diagonal in the POMDP state basis.
The quantum generalization promotes the environment belief state to any quantum state deﬁned
on the Hilbert space spanned by the orthonormal basis {|s⟩|s ∈S}. The dynamic of the quantum
POMDP are deﬁned by actions which correspond to quantum instruments (superoperators) the
agent can apply: to each action a, we associate the set of Krauss operators {Ka
o }o∈O, which satisfy
o = 1. If the agent performs the action a, and observes the observation o, the state of
the environment is mapped as ρ →Ka
†], where Tr[Ka
†] is the probability
79 Quantum in that that which is learned is encoded in a quantum state.
80 In other words, for any environment state s, producing an action a causes a transition to some state s′ with
probability ⃗s′τP a⃗s, where states are represented as canonical vectors.
81 In general, the observations output can also depend on the previous action of the agent.
of observing that outcome. Finally, rewards are deﬁned via the expected values of action-speciﬁc
positive operators Ra, so Tr[Raρ], given the state ρ. In , the authors have studied
this model from the computational perspective of the hardness of identifying the best strategies
for the agent, contrasting this setting to classical settings, and proving separations. In particular,
the complexity of deciding policy existence for ﬁnite horizons82, are the same for the quantum
and classical cases83. However, a separation can be found with respect to the goal reachability
problem, which asks whether there exists a policy (of any length) which, with probability 1, reaches
some target state. This separation is maximal – this problem is decidable in the classical case,
yet undecidable in the quantum case. While this particular separation may not have immediate
consequences for quantum learning, it suggests that there may be other (dramatic) separations, with
more immediate relevance.
VI. QUANTUM ENHANCEMENTS FOR MACHINE LEARNING
One of the most advertised aspects of quantum ML deals with the question of whether quantum
eﬀects can help us solve classical learning tasks more eﬃciently, ideally mirroring the successes
of quantum computation. The very ﬁrst attempts to apply quantum information techniques to
ML problems were made even before the seminal works of Shor and Grover . Notable examples include the pioneering research into quantum neural networks and quantum
perceptrons , and also in the potential of quantum computational
learning theory . The topic of quantum neural networks (quantum
NNs) has had sustained growth and development since these early days, exploring various types of
questions regarding the interplay of quantum mechanics and neural networks. Most of the research in
this area is not directly targeted at algorithmic improvements, hence will be only brieﬂy mentioned
here. A fraction of the research into quantum NNs, which was disproportionately more active in the
early days, considered the speculative topics of the function of quantum eﬀects in neural networks,
both artiﬁcial and biological . Parts of this research line has focused
concrete models, such as the eﬀect of transverse ﬁelds in HNs ,
and decoherence in models of biological nets , which, it is argued, would destroy
any potential quantum eﬀect. A second topic which permeates the research in quantum NNs is
concerned with the fundamental question of a meaningful quantization of standard feed-forward
neural networks. The key question here is ﬁnding the best way to reconcile the linear nature of
quantum theory, and the necessity for non-linearities in the activation function of a neural network
(see section II.A.1), and identifying suitable physical systems to implement such a scheme. Early
ideas here included giving up on non-linearities per se, and considering networks of unitaries which
substitute layers of neurons . Another approach exploits non-linearities which
stem from measurements and post-selection ). The same
issue is addressed by Behrman et al. by using a continuous mechanical
system where the non-linearity is achieved by coupling the system with an environment 84, in the
model system of quantum dots. The purely foundational research into implementations of such
networks, and analysis of their quantum mechanical features, has been and is continuing to be an
82 That is, given a full speciﬁcation of the setting, decide whether there exist a policy for the agent which achieves a
cumulative reward above some value, in a certain number of states.
83 This decision problem is undecidable in the inﬁnite horizon case, already for the classical problem, and thus trivially
undecidable in the quantum case as well.
84 Similar ideas were also discussed by Peruˇs in .
active ﬁeld of research ). For more information on this topic we refer
the reader to more specialized reviews .
Unlike the research into quantum NNs, which has a foundational ﬂavor, majority of works studying
quantum eﬀects for classical ML problems are speciﬁcally focused on identifying improvements..
First examples of quantum advantages in this context were provided in the context of quantum
computational learning theory, which is the topic of the ﬁrst subsection below. In the second
subsection we will survey research suggesting the possibilities of improvement of the capacity of
associative memories. The last subsection deals with proposals which address computational
run-time improvements of classical learning algorithms, the ﬁrst of which came out already in
the early 2000s. Here we will diﬀerentiate approaches which focus on quantum improvements in the
training phase of a classiﬁer by means of quantum optimization (mostly focused on exploiting
near-term technologies, and restricted devices), and approaches which build algorithms based on,
roughly speaking, quantum parallelism and “quantum linear algebra” – which typically assume
universal quantum computers, and often “pre-ﬁlled” database. It should be noted that the majority
of research in quantum ML is focused precisely on this last aspect, and the results here are already
quite numerous. We can thus aﬀord to present only a chosen selection of results.
A. Learning eﬃciency improvements: sample complexity
Executive summary: The ﬁrst results showing the separation between quantum and classical
computers were obtained in the context of oracles, and for sample complexity – even the famous
Grover’s search algorithm constitutes such a result. Similarly, CLT deals with the learning,
i.e., the identiﬁcation or the approximation of concepts, which are also nothing but oracles.
Thus, quantum oracular computation settings and learning theory share the same underlying
framework, which is investigated and exploited in this formal topic. To talk about quantum
CLT, and improvements, or bounds, on sample complexity, the classical concept oracles are thus
upgraded to quantum concept oracles, which output quantum states, and/or allow access
in superposition.
As elaborated in section II.B.1, CLT deals with the problem of learning concepts, typically abstracted
as boolean functions of bit-strings of length n so c : {0, 1}n →{0, 1}, from input-output relations
alone. For intuitive purposes it is helpful to think of the task of optical character recognition (OCR),
where we are given a bitmap image (black-and-white scan) of some size n = N × M, and a concept
may be say “everything which represents the letter A”, more precisely, the concept, specifying which
bitmaps correspond to the bitmaps of the letter “A”. Further, we are most often interesting in a
learning performance for a set of concepts: a concept class C = {c|c : {0, 1}n →{0, 1}} – in the
context of the running example of OCR, we care about algorithms which are capable of recognising
all letters, and not just “A”.
The three typical settings studied in literature are the PAC model, exact learning from membership
queries, and the agnostic model, see section II.B.1. These models diﬀer in the type of access to the
concept oracle which is allowed. In the PAC model, the oracle outputs labeled examples according
to some speciﬁed distribution, analogous to basic supervised learning. In the membership queries
model, the learner gets to choose the examples, and this is similar to active supervised learning.
In the agnostic model, the concept is “noisy”, i.e. forms a stochastic function, which is natural
in supervised settings (the joint datapoint-label distribution P(x, y) need not be functional), for
details we refer the reader to section II.B.1.
All three models have been treated from a quantum perspective, and whether or not quantum
advantages are obtainable greatly depends on the details of the settings. Here we give a very succinct
overview of the main results, partially following the structure of the recent survey on the topic by
Arunachalam and de Wolf .
1. Quantum PAC learning
The ﬁrst quantum generalization of PAC learning was presented in ,
where the quantum example oracle was deﬁned to output coherent superpositions
pD(x) |x, c(x)⟩,
for a given distribution D over the data points x, for a concept c. Recall, classical PAC oracles
output a sample pair (x, c(x)), where x is drawn from D, which can be understood as copies
of the mixed state P
x pD(x) |x, c(x)⟩⟨x, c(x)|, with pD(x) = P(D = x). The quantum oracle
reduces to the standard oracle if the quantum example is measured in the standard (computational)
basis. This ﬁrst pioneering work showed that quantum algorithms, with access to such a quantumgeneralized oracle can provide more eﬃcient learning of certain concept classes. The authors have
considered the concept class of DNF formulas, under the uniform distribution: here the concepts
are s-term formulae in disjunctive normal form. In other words, each concept c is of the form
j, where xI is a substring of x associated to I, which is a subset of the indices of
cardinality at most s, and (xI)′
j is a variable or its negation (a literal). An example of a DNF is of
the form (x1 ∧x3 ∧¬x6) ∨(x4 ∧¬x8 ∧x1) · · · , where parentheses (terms) only contain variables or
their negations in conjunction (ANDs, ∧), whereas all the parentheses are in disjunction (ORs, ∨).
The uniform DNF learning problem (for n variables, and poly(n) terms) is not known to be
eﬃciently PAC learnable, but, in it was proven to be eﬃciently
quantum PAC learnable. The choice of this learning problem was not accidental: DNF learning
is known to be learnable in the membership query model, which is described in detail in the next
section. The corresponding classical algorithm which learns DNF in the membership query model
directly inspired the quantum variant in the PAC case 85. If the underlying distribution over the
concept domain is uniform, other concept classes can be learned with a quantum speed-up as well,
speciﬁcally, so called k-juntas: n-bit binary functions which depend only on k < n bits. In , Atıcı and Servedio have shown that there exists a quantum algorithm for learning
k-juntas using O(k log(k)/ϵ) uniform quantum examples, O(2k) uniform classical examples, and
O(n k log(k)/ϵ+2klog(1/ϵ)) time. Note the improvement in this case is not in query complexity, but
rather in the classical processing, which, for the best known classical algorithm has complexity at
least O(n2k/3) for further details).
Diverging from perfect PAC settings, in , the authors considered the learning of
linear boolean functions86 under the uniform distribution over the examples. The twist in this work
is the assumption of noise87 which allows for evidence of a classical quantum learnability separation.
85 To provide the minimal amount of intuition, the best classical algorithm for the membership query model, heavily
depends on Fourier transforms (DFT) of certain sets – the authors then use the fact that FT can be eﬃciently
implemented on the amplitudes of the states generated by the quantum oracle using quantum computers. We refer
the reader to for further details.
86 The learning of such functions is in QIP circles also known as the (non-recursive) Bernstein-Vazirani problem
deﬁned ﬁrst in .
87 However, the meaning of noise is not exactly the same in the classical and quantum case.
a. Distribution-free PAC
While the assumption of the uniform distribution D constitutes a convenient
theoretical setting, in reality, most often we have few guarantees on the underlying distribution
of the examples. For this reason PAC learning often refers to distribution-free learning, meaning,
learning under the worst case distribution D. Perhaps surprisingly, it was recently shown that
the quantum PAC learning model oﬀers no advantages, in terms of sample complexity, over the
classical model. Speciﬁcally, in the authors show that if C is a
concept class of VC dimension d + 1, then for every (non-negative) δ ≤1/2 and ϵ ≤1/20, every
(ϵ, δ)-quantum PAC learner requires Ω(d/ϵ + log(d−1)/ϵ) samples. The same number of samples,
however, is also known to suﬃce for a classical PAC learner (for any ϵ and δ).
A similar result, showing no separation between quantum and classical agnostic learning was also
proven in 88 .
b. Quantum predictive PAC learning
Standard PAC learning settings do not allow exponential
separations between classical and quantum sample complexity of learning, and consequently the
notion of learnable concepts is the same in the classical and the quantum case. This changes if we
consider weaker learning settings, or rather, a weaker meaning of what it means to learn. The PAC
learning setting assumes that the learning algorithm outputs a hypothesis h with a low error with
high conﬁdence. In the classical case, there is no distinction between expecting that the hypothesis
h can be applied once, or any arbitrary number of times. However, in the quantum case, where the
examples from the oracle may be quantum states, this changes, and inductive learning in general may
not be possible in all settings, see section V. In , the author considers a quantum
PAC settings where only one (or polynomially few) evaluations of the hypothesis are required, called
the Predictive Quantum (PQ) model89. In this setting the author identiﬁes a relational concept class
(i.e. each data point may have many correct labels) which is not (polynomially) learnable in the
classical case, but is PQ learnable under a standard quantum oracle, under the uniform distribution.
The basic idea is to use quantum states, obtained by processing quantum examples, for each of
the testing instances – in other words, the “implementation” of the hypothesis contains a quantum
state obtained from the oracle. This quantum state cannot be eﬃciently estimated, but can be
eﬃciently obtained using the PQ oracle. The concept class, and the labeling process are inspired by
a distributed computation problem for which an exponential classical-quantum separation had been
identiﬁed earlier in . This work provides another noteworthy example of
the intimate connection between various aspects of QIP – in this case, quantum communication
complexity theory – and quantum learning.
2. Learning from membership queries
In the model of exact learning from membership queries, the learner can choose the elements from
the concept domain it wishes labeled (similar to active learning), however, the task is to identify the
concept exactly (no error), except with probability δ < 1/390 Learning from membership queries
88 The notions of eﬃciency and sample complexity in the agnostic model are analogous to those in the PAC model,
as is the quantum oracle which provides the coherent samples P
pD(x, y) |x, y⟩. See section II.B.1 for more
89 In a manner of speaking, to learn a concept, in the PAC sense, implies we can apply what we have learned arbitrarily
many times. In PQ it suﬃces that the learner be capable of applying what it had learned just once once, to be
considered successful. It however follows that if the number of examples is polynomial, PQ learnability also implies
that the veriﬁcation of learning can be successfully executed polynomially many times as well.
90 As usual, success probability which is polynomially bounded away from 1/2 would also do.
has, in the quantum domain, usually been called oracle identiﬁcation. While quantum improvements
in this context are possible, in , the authors show that they are at
most low-degree polynomial improvements in the most general cases. More precisely, if a concept
class C over n −bits has classical and quantum membership query complexities D(C) and Q(C),
respectively, then D(C) = O(nQ(C)3)91 – in other words, improvements in sample complexity
can be at most polynomial. Polynomial relationships have also been established for worst-case
exact learning sample complexitites (so-called (N, M)-query complexity), see and
 . The above result is in spirit similar to earlier results in , where it was shown quantum query complexity cannot provide a better than polynomial
improvement over classical results, unless structural promises on the oracle are imposed.
The results so far considered are standard, comparatively simple generalizations of classical learning
settings, leading to somewhat restricted improvements in sample complexity.
More dramatic
improvements are possible if computational (time) complexity is taken into account, or if slightly
non-standard generalizations of the learning model are considered. Note, we are not explicitly
bringing computational complexity separations into the picture. Rather, under the assumption that
certain computation problems are hard for the learner, we obtain a sample complexity separation.
In particular, already in the authors have constructed several classes of
Boolean functions in the distribution-free model whose eﬃcient learning (in the sample complexity
sense) implies the capacity of factoring of so-called Blum integers - a task not known to be solvable
classically, but solvable on a quantum computer92. Using this observations, Servedio and Gortler
have demonstrated classes which are eﬃciently quantum PAC learnable, and classes which are
eﬃciently learnable in the quantum membership query model, but which not eﬃciently learnable in
the corresponding classical models, unless Blum integers 93 can be eﬃciently factored on a classical
computer .
91 This simple formulation of the claim of was presented in (Arunachalam and de Wolf,
92 These ideas exploit the connections between asymmetric cryptography and learning. In asymmetric cryptography,
a message can be decrypted easily using a public key, but the decryption is computationally hard, unless one has
a private key. To exemplify public key can be a Blum integer, whereas the private key one of the factors. The
data-points are essentially the encryptions of integers k E(k, N), for a public key N. The concept is deﬁned by the
least signiﬁcant bit of k, which, provably, is not easier to obtain with bounded error than the decryption itself –
which is computationally hard. A successful eﬃcient learner of such a concept could factor Blum integers.The full
proposal has further details we omit for simplicity.
93 The integer n is a Blum integer if it is a product of two distinct prime numbers p and q, which are congruent to 3
mod 4 (i.e. both can be written in the form 4t + 3, for a non-negative integer t.).
B. Improvements in learning capacity
Executive summary: The observation that a complete description of quantum systems typically
requires the speciﬁcation of exponentially many complex-valued amplitudes has lead to the idea
that those same amplitudes could be used to store data using only logarithmically few systems.
While this idea fails for most applications, it has inspired some of the ﬁrst proposals to use
quantum systems for the dramatic improvement of the capacities of associative, or
content-addressable memories. More likely quantum upgrades of CAM memories, however,
may come from a substantially diﬀerent direction – which explores methods of extracting
information from HNs – used as CAM memories – and which is inspired by quantum adiabatic
computing to realize a recall process which is similar yet diﬀerent from standard recall methods.
The quantum methods may yield advantages by outputting superpositions of data, and
it has been suggested they also utilize the memory more eﬃciently, leading to increased
capacities.
The pioneering investigations in the areas between CLT, NNs and QIP, have challenged the classical
sample complexity bounds. Soon thereafter (and likely independently), the ﬁrst proposals suggesting
quantum improvements in the context of space complexity emerged – speciﬁcally the eﬃciency of
associative memories. Recall, associative, or content-addressable memory (abbreviated CAM) is a
storage device which can be loaded with patterns, typically a subset of n-bit bit-strings P = {xi}i,
xi ∈{0, 1}n, which are then, unlike in the case of standard RAM-type memories, not recovered by
address but by content similarity: given an input string y ∈{0, 1}n, the memory should return y
if it is one of the stored patterns (i.e. y ∈P), or a stored pattern which is “closest” to y, with
respect to some distance, typically the Hamming distance. Deterministic perfect storage of any set
of patterns clearly requires O(n × 2n) bits (there are in total 2n distinct patterns each requiring n
bits), and the interesting aspects of CAMs begin when the requirements are somewhat relaxed. We
can identify roughly two basic groups of ideas which were suggested to lead to improved capacities.
The ﬁrst group, sketched next, relies directly on the structure of the Hilbert space, whereas the
second group of ideas stems from the quantization of a well-understood architecture for a CAM
memory system: the Hopﬁeld network.
1. Capacity from amplitude encoding
In some of the ﬁrst works it was suggested that
the proverbial “exponential-sized” Hilbert space describing systems of qubits may allow exponential
improvements: intuitively even exponentially numerous pattern sets P can be “stored” in a quantum
state of only n qubits: |ψP ⟩= |P|−1
x∈P |x⟩. These early works suggested creative ideas on
how such a memory could be used to recover patterns (e.g. via modiﬁed amplitude ampliﬁcation),
albeit, often suﬀering from lack of scalability, and other quite fundamental issues to yield complete
proposals94, and thus we will not dig into details. We will, however, point out that these works may
be interpreted to propose some of the ﬁrst examples of “amplitude encoding” of classical data, which
94 For a discussion on some of the shortcomings see e.g. , and we also refer the
reader to more recent reviews for further details and analysis of the potential application of
such memories to pattern recognition problems.
is heavily used in modern approaches to quantum ML. In particular, the stored memory of a CAM
can always be represented as a single bit-string (b(0···0), b(0···1) . . . , b(1...1)) of length 2n (each bit in
the bit-string is indexed by a pattern, and its value encodes if it is stored or not). This data-vector
(in this case binary, but this is not critical) is thus encoded into amplitudes of a quantum state of
an exponentially smaller number of qubits: b = (b(0···0), b(0···1) . . . , b(1...1)) →P
x∈{0,1}n bx |x⟩(up
to normalization).
2. Capacity via quantized Hopﬁeld networks
A diﬀerent approach to increasing the capacities of CAM memories arises from the “quantization”
of diﬀerent aspects of classical HNs, which constitute well-understood classical CAM systems.
a. Hopﬁeld networks as a content-addressable memory
Recall, a HN is a recurrent NN characterized
by a set of n neurons, whose connectivity is given by a (typically symmetric) real matrix of weights
W = (wij)ij and a vector of (real) local thresholds {θi}n
i=1. In the context of CAM memories, the
matrix W encodes the stored patterns, which are in this setting best represented as sequences of
signs, so x ∈{1, −1}n. The retrieval, given an input pattern y ∈{1, −1}n, is realized by setting the
kth neuron sk to the kth value of the input pattern yk, followed by the “running of the network”
according to standard perceptron rules: each neuron k computes its subsequent value by checking
if its inbound weighted sum is above the local threshold: sk ←sign(P
l wklsl −θk) (assuming
sign(0) = +1)95. As discussed previously, under moderate assumptions the described dynamical
system converges to local attractive points, which also correspond to the energy minima of the Ising
functional
Such a system still allows signiﬁcant freedom in the rule specifying the matrix W, given a set of
patterns to be stored: intuitively, we need to “program” the minima of E (choosing the appropriate
W will suﬃce, as the local thresholds can be set to zero) to be the target patterns, ideally without
storing too many unwanted, so-called spurious, patterns. This, and other properties of a useful
storing rule, that is, rule which speciﬁes W given the patterns, are given as follows :
a) locality: an update of a particular connection should depend only on the information available to
the neurons on either side of the connection96; b) incrementality: the rule should allow the updating
of the matrix W to store an additional pattern based only on the new pattern and W itself 97 c)
immediateness: the rule should not require a limiting computational process for the evaluation of the
weight matrix (rather, it should be a simple computation of few steps). The most critical property
of a useful rule is that it d) results in a CAM with a non-trivial capacity: it should be capable of
storing and retrieving some number of patters, with controllable error (which includes few spurious
patterns, for instance).
95 The updates can be synchronous, meaning all neurons update their values at the same time, or asynchronous, in
which case usually a random order is assigned. In most analyses, and here, asynchronous updates are assumed.
96 Locality matters as the lack of it prohibits parallelizable architectures.
97 In particular, it should not be necessary to have external memory storing e.g. all stored patters, which would render
HN-based CAM memories undesirably non-adaptive and inﬂexible.
The historically ﬁrst rule, the Hebbian rule, satisﬁes all the conditions above and is given by a simple
recurrence relation: for the set of patterns {xk}k the weight matrix is given with wij = P
j is the jth sign of the kth pattern, and M is the number of patterns). The capacity of
HN’s under standard recall and Hebbian updates has been investigated from various perspectives,
and in the context of absolute capacity (the asymptotic ratio of the number of patterns that can be
stored without error to the number of neurons, as the network size tends to inﬁnity), it is known to
scale as O(
2ln(n)). A well known result in the ﬁeld improves on this to the capacity of O , while maintaining all
the desired properties. Here, we should emphasize that, in broad terms, the capacity is typically
(sub)-linear in n. Better results, however, can be achieved in the classical settings if some of the
assumptions a) −c) are dropped, but this is undesirable.
b. Quantization of Hopﬁeld-based CAMs
In early works , the
authors have considered fuzzy and probabilistic learning rules, and have broadly argued that a) such
probabilistic rules correspond to a quantum deliberation process and that b) the resulting CAMs
can have signiﬁcantly larger capacities. However, more rigorous (and fully worked out) results were
shown more recently, by combining HNs with ideas from adiabatic QC.
The ﬁrst idea, presented in connects HNs and quantum annealing. Recall
that the HN can be characterized by the Ising functional E(s) = −1
ij wijsisj (see Eq. 2),
where the stored patterns correspond to local minima, and where we have, without the loss of
generality, assumed that the local thresholds are zero. The classical recall corresponds to the
problem of ﬁnding local minima closest to the input pattern y. However, an alternative system,
with similar features, is obtained if the input pattern is added in place of the local thresholds:
E(s, y) = −1
ij wijsisj −Γ P
i yisi. Intuitively, this lowers the energy landscape of the system
speciﬁcally around the input pattern conﬁguration. But then, the stored pattern (previous local
minimum) which is closest to the input pattern is the most likely candidate for a global minimum.
Further, the problem of ﬁnding such conﬁgurations can now be tackled via quantum annealing: we
deﬁne the quantum “memory Hamiltonian” naturally as Hmem = −1
j , and the HN
Hamiltonian, given input y with Hp = Hmem + ΓHinp, where the input Hamiltonian is given with
i . The quantum recall is obtained by the adiabatic evolution via the Hamiltonian
trajectory H(t) = Λ(t)Hinit + Hp, where Λ(0) is large enough that Hinit dominates, and Λ(1) = 0.
The system is initialized in the ground state of the (arbitrary and simple) Hamiltonian Hinit, and if
the evolution in t is slow enough to satisfy the criteria of the adiabatic theorem, the system ends in
the ground state of Hp. This proposal exchanged local optimization (classical retrieval) for global
optimization. While this is generally a bad idea98, what is gained is a quantum formulation of the
problem which can be run on adiabatic architectures, and also the fact that this system can return
quantum superpositions of recalled patterns, if multiple stored patterns are approximately equally
close to the input, which can be an advantage . However, the system above
does not behave exactly the same as the classical recall network, which was further investigated
98 Generically, local optimization is easier than global, and in the context of the Ising system, global optimization is
known to be NP-hard.
in subsequent work analysing the sensitivity of the quantum recall
under various classical learning rules. Further, in the authors have provided
an extensive analysis of the capacity of the Hebb-based HN, but under quantum annealing recall
as proposed in showing, surprisingly, that this model yields exponential
storage capacity, under the assumption of random memories. This result stands in apparent stark
contrast to standard classical capacities reported in textbooks99.
Regarding near-term implementability, in the authors have investigated the
suitability of the Chimera graph-based architectures of D-Wave programmable quantum annealing
device for quantum recall HN tasks, showing potential for demonstrable quantum improvements in
near-term devices.
C. Run-time improvements: computational complexity
Executive summary: The theory of quantum algorithms has provided examples of computational speed ups for decision problems, various functional problems, oracular problems,
sampling tasks, and optimization problems. This section presents quantum algorithms which
provide speed-ups for learning-type problems. The two main classes of approaches diﬀer
in the underlying computational architecture – a large class of algorithms relies on quantum
annealers, which may not be universal for QC, but may natively solve certain sub-tasks important in the context of ML. These approaches then have an increased likelihood of being
realizable with near-term devices. In contrast, the second class of approaches assumes
universal quantum computers, and often data prepared and accessible in quantum database, but
oﬀers up to exponential improvements. Here we distinguish between quantum amplitude
ampliﬁcation and amplitude encoding approaches, which, with very few exceptions, cover
all quantum algorithms for supervised and unsupervised learning.
The most proliﬁc research area within quantum ML in the last few years has focused on identifying
ML algorithms, or their computationally intensive subroutines, which may be sped up using quantum
computers. While there are multiple natural ways to classify the performed research, an appealing
ﬁrst-order delineation follows the types of quantum computational architectures assumed100. Here
we can identify research which is focused on using quantum annealing architectures, which are
experimentally well justiﬁed and even commercially available in recent times (mostly in terms of the
D-Wave system set-ups). In most of such research, the annealing architecture will be utilized to
perform a classically hard optimization problem usually emerging in the training phases of many
classical algorithms. An involved part of such approaches will often be a meaningful rephrasing of
such ML optimization to a form which an annealing architecture can (likely) handle. While the
overall supervised task comprises multiple computational elements, it is only the optimization that
will be treated by a quantum system in these proposals.
The second approach to speeding up ML algorithms assumes universal quantum computation
capabilities. Here, the obtained algorithms are typically expressed in terms of quantum circuits.
99 At this point it should be mentioned that recently exponential capacities of HNs have been proposed for fully
classical systems, by considering diﬀerent learning rules , which also
tolerate moderate noise. The relationship and potential advantages of the quantum proposals remains to be
elucidated.
100 Other classiﬁcation criteria could be according to tasks, i.e. supervised vs. unsupervised vs. generative models etc.,
or depending on the underlying quantum algorithms used, e.g. amplitude ampliﬁcation, or equation solving.
For most proposals in this research line, to guarantee actual speed-ups, there will be additional
assumptions. For instance, most proposals can only guarantee improvements if the data, which
is to be analyzed, is already present in a type of a quantum oracle or a quantum memory, and,
more generally, that certain quantum states, which depend on the data, can be prepared eﬃciently.
The overhead of initializing such a memory in the ﬁrst place is not counted, but this may not
unreasonable as in practice, the same database is most often used for a great number of analyses.
Other assumptions may also be placed on the structure of the dataset itself, such as low condition
numbers of certain matrices containing the data .
1. Speed-up via adiabatic optimization
Quantum optimization techniques play an increasingly important role in quantum ML. Here, we can
roughly distinguish two ﬂavours of approaches, which diﬀer in what computationally diﬃcult aspect
of training of a classical model is tackled by adiabatic methods. In the (historically) ﬁrst approach,
we deal with clear-cut optimization in the context of binary classiﬁers, and more speciﬁcally, boosting
(see II.A.3). Since, it has been shown that annealers can also help by generating samples from
hard-to-simulate distributions. We will mostly focus on the historically ﬁrst approaches, and only
brieﬂy mention the other more recent results.
a. Optimization for boosing
The representative line of research, which also initiated the development
of this topic of quantum-enhanced ML based on adiabatic quantum computation, focuses on a
particular family of optimization problems called quadratic unconstrained optimization (QUBO)
problems of the form
1, . . . , x∗
n) = argmin(x1,...,xn)
Jijxixj, xk ∈{0, 1}
speciﬁed by a real matrix J. QUBO problems are equivalent to the problem of identifying lowest
energy states of the Ising functional101 E(s) = −1
ij Jijsisj + P
i θisi, provided we make no
assumptions on the underlying lattice. Modern annealing architectures provide means for tackling
the problem of ﬁnding such ground states using adiabatic quantum computation. Typically we are
dealing with systems which can implement the tunable Hamiltonian of the form
H(t) = −A(t)
where A, B are smooth positive functions such that A(0) ≫B(0) and B(1) ≫A(1), that is, by
tuning t suﬃciently slowly, we can perform adiabatic preparation of the ground state of the Ising
Hamiltonian Htarget, thereby solving the optimization problem. In practice, the parameters Jij
cannot be chosen fully freely in D-Wave architectures), and also the realized interaction strenght values have a
limited precision and accuracy , but we will ignore this for
the moment. In general, ﬁnding ground states of the Ising model is functional NP-hard102, which is
likely beyond the reach of quantum computers. However, annealing architectures still may have
many advantages, for instance it is believed they may still provide speed ups in all, or at least
average instances, and/or that they may provide good heuristic methods, and hopefully near optimal
solutions103.
In other words, any aspect of optimization occurring in ML algorithms which has an eﬃcient
mapping to (non-trivial) instances of QUBO problems, speciﬁcally those which can be realized by
experimental set-ups, is a valid candidate for quantum improvements. Such optimization problems
have been identiﬁed in a number of contexts, mostly dealing with training binary classiﬁers, thus
belong to the class of supervised learning problems. The ﬁrst setting considers the problem of
building optimal classiﬁers from linear combinations of simple hypothesis functions, which minimize
empirical error, while controlling the model complexity through a so-called regularization term. This
is the common optimization setting of boosting (see II.A.3), and, with appropriate mathematical
gymnastics and few assumptions, it can be reduced to a QUBO problem.
The overarching setting of this line of works can be expressed in the context of training a binary
classiﬁer by combining weaker hypotheses. For this setting, consider a dataset D = {xi, yi}M
xi ∈Rn, yi ∈{−1, 1}, and a set of hypotheses {hj}K
j=1, hj : Rn →{−1, 1}. For a given weight vector
w ∈Rn we deﬁne the composite classiﬁer of the form hcw(x) = sign(P
k wkhk(x)).
The training of the composite classiﬁer is achieved by the optimization of the vector w as to minimize
misclassiﬁcation on the training set, and as to decrease the risk of overtraining. The misclassiﬁcation
cost is speciﬁed via a loss function L, which depends on the dataset, and the hypothesis set in the
boosting context. The overtraining risk, which tames the complexity of the model, is controlled by
a so-called regularization term R. Formally we are solving
argminw L(w; D) + R(w).
This constitutes the standard boosting frameworks exactly, but is also closely related to the training
of certain SVMs, i.e. hyperplane classiﬁers104. In other words, quantum optimization techniques
which work for boosting setting can also help for hyperplane classiﬁcation.
There are a few well-justiﬁed choices for L and R, leading to classiﬁers with diﬀerent properties.
Often, best choices (the deﬁnition of which depends on the context) lead to hard optimization , and some of those can be reduced to QUBOs, but not straightforwardly.
In the pioneering paper on the topic , Neven and co-authors consider the boosting
setting. The regularization term is chosen to be proportional to the 0-norm, which counts the number
of non-zero entries, that is, R(w, λ) = λ∥w∥0. The parameter λ controls the relative importance of
regularization in the overall optimization task. A common choice for the loss function would be the
0-1 loss function L0−1, optimal in some settings, given with L0−1(w) = PM
j=1 Θ (−yj
k wkhk(xj))
(where Θ is the step function), which simply counts the number of misclassiﬁcations. This choice
102 Finding ground states is not a decision problem, so, technically it is not correct to state it is NP-hard. The class
functional NP (FNP) is the extension of the NP class to functional (relational) problems.
103 Indeed, one of the features of adiabatic models in general is that they provide an elegant means for (generically)
providing approximate solutions, by simply performing the annealing process faster than prescribed by the adiabatic
104 If we allow the hypotheses hj to attain continuous real values, then by setting hj to be the projection on the
jth component of the input vector, so hj(x) = xj, then the combined classiﬁer attains attains the inner-productthreshold form hcw(x) = sign(wτx) which contains hyperplane classiﬁers – the only component missing is the
hyperplane oﬀset b which incorporated into the weight vector by increasing the dimension by 1.
is reasonably well motivated in terms of performance, and is likely to be computationally hard.
With appropriate discretization of the weights w, which the authors argue likely does not hurt
performance, the above forms a solid candidate for a general adiabatic approach. However, it does
not ﬁt the QUBO structure (which has only quadratic terms), and hence cannot be tackled using
existing architectures. To achieve the desired QUBO structure the authors impose two modiﬁcations:
they opt for a quadratic loss function L2(w) = PM
j=1 |yj −P
k wkhk(xj)|2, and restrict the weights
to binary (although this can be circumvented to an extent). Such a system is also tested using
numerical experiments. In a follow-up paper , the same team has generalized
the initial proposal to accommodate another practical issue: problem size. Available architectures
allow optimization over a few thousand variables, whereas in practice the number of hypotheses one
optimizes over (K) may be signiﬁcantly larger. To resolve this, the authors show how to break a
large optimization problem into more manageable chunks while maintaining (experimentally veriﬁed)
good performance. These ideas were also tested in an actual physical architecture , and combined and reﬁned in a more general, iterative algorithm in ,
tested also using actual quantum architectures.
While L0−1 loss functions were known to be good choices, they were not the norm in practice as
they lead to non-convex optimization – so convex functions were preferred. However, in 2010 it
became increasingly clear that convex functions are provably bad choices. For instance, in the
seminal paper Long and Servedio105, showed that boosting with convex
optimization completely fails in noisy settings. Motivated by this in , the
authors re-investigate D-Wave type architectures, and identify a reduction which allows a non-convex
optimization. Expressed in the hyperplane classiﬁcation setting (as explained, this is equivalent
to the boosting setting in structure), they identify a reduction which (indirectly) implements a
non-convex function lq(x) = min{(1 −q)2, (max(0, 1 −x))2}. This function is called the q-loss
function, where q is a real parameter. The implementation of the q-loss function allows for the
realization of optimization relative to the total loss of the form Lq(w, b; D) = P
j lq(yj(wτx + b)).
The resulting regularization term is in this case proportional to the 2-norm of w, instead of the
0-norm as in the previous examples, which may be sub-optimal. Nonetheless, the above forms
a prime example where quantum architectures lead to ML settings which would not have been
explored in the classical case (the loss Lq is unlikely to appear naturally in many settings) yet are
well motivated, as a) the function is non-convex and thus has the potential to circumvent all the
no-go results for convex functions, and b) the optimization process can be realized in a physical
system. The authors perform a number of numerical experiments demonstrating the advantages of
this choice of a non-convex loss function when analysing noisy data, which is certainly promising. In
later work , it was also suggested that combinations of loss-regularization which
are realizable in quantum architectures can also be used for so-called totally corrective boosting
with cardinality penalization, which is believed to be classically intractable.
The details of this go beyond the scope of this review, but we can at least provide a ﬂavour of
the problem. In corrective boosting, the algorithm updates the weights w essentially one step at
a time. In totally corrective boosting, at the tth step of the boosting algorithm optimization, t
entries of w are updated simultaneously. This is known to lead to better regularized solutions, but
the optimization is harder. Cardinality penalization pertains to using explicitly the 0-norm for the
regularization (discussed earlier), rather than the more common 1-norm. This, too, leads to harder
105 Servedio also, incidentally, provided some of the earliest results in quantum computational learning theory, discussed
in previous sections.
optimization which may be treated using an annealing architecture. In , the
authors signiﬁcantly generalized the scope of loss functions which can be embedded into quantum
architectures, by observing that any polynomial unconstrained binary optimization can, with small
overhead, be mapped onto a (slightly larger) QUBO problem. This, in particular, opens up the
possibility of implementing odd-degree polynomials which are non-convex and can approximate the
0-1 loss function. This approach introduced new classes of unusual yet promising loss functions.
b. Applications of quantum boosting
Building on the “quantum boosting” architecture described
above, in , the authors explore the possibility of (aside from boosting)
realizing anomaly detection, speciﬁcally envisioned in the computationally challenging problem of
software veriﬁcation and validation106. In the proposed learning step the authors use quantum
optimization (boosting) to learn the characteristics of the program being tested. In the novel testing
step the authors modify the target Hamiltonian as to lower the energy of the states which encode
input-outputs where the real and ideal software diﬀer. These can then be prepared in superposition
(i.e. they can prepare a state which is a superposition over the inputs where the P will produce an
erroneous output) similarly to the previously mentioned proposals in the context of adiabatic recall
of superpositions in HN .
c. Beyond boosting
Beyond the problems of boosting, annealers have been shown to be useful for
the training of so-called Bayesian Network Structure Learning problems , as
their training can also be reduced to QUBOs. Further, annealing architectures can also be used the
training of deep neural networks, relying on sampling, rather than optimization. A notable approach
to this is based on the fact that the training of deep networks usually relies on the use of a so-called
generative deep belief network, which are, essentially, restricted BMs with multiple layers107.
The training of deep belief networks, in turn, is the computational bottleneck, as i requires the
sampling of hard-to-generate distributions, which may be more eﬃciently prepared using annealing
architectures, see e.g. . Further. novel ideas introducing fully quantum
BM-like models have been proposed . Further, in recent work which builds on the ﬂexible construction in , the authors have
shown how to achieve programmable adiabatic architectures, which allows running algorithms where
the weights themselves are in superposition. This possibility is also sure to inspire novel QML ideas.
Moving on from BMs, in recent work , the authors have also shown how
suitable annealing architectures may be useful to speed-up the performing of probabilistic inference
in so-called Markov logic networks108. This task involves the estimation of partition functions of
arising from statistical models, concretely Markov random ﬁelds, which include the Ising model as a
special case. Quantum annealing may speed up this sub-task.
More generally, general, the ideas that restricted, even simple, quantum systems which may be
realizable with current technologies, could implement information processing elements useful for
106 A software is represented as a map P from input to output spaces, here speciﬁed as subset of the space of pairs
(xinput, xoutput). An implemented map (software) P is diﬀerentiated from the ideal software ˆP by the mismatches
in the deﬁning pairs.
107 In other words, they are slightly less restricted BMs, with multiple layers and no within-layer connectivity.
108 Markov logic networks combine ﬁrst-order logic as used for knowledge representation and reasoning, and statistical modelling – essentially, the world is described via ﬁrst-order sentences (a
knowledge base), which gives rise to a graphical statistical model (a Markov random ﬁeld), where correlations stem
from the relations in the knowledge base.
supervised learning are beginning to be explored in setting beyond annealers. For instance, in
 , a simple interferometric circuit is used for the eﬃcient evaluation of distances
between data-vectors, useful for classiﬁcation and clustering. A more complete account of these
recent ideas is beyond the scope of this review.
2. Speed-ups in circuit architectures
One of the most important applications of ML in recent times has been in the context of data
mining, and analyzing so-called big data. The most impressive improvements in this context have
been achieved by proposing specialized quantum algorithms which solve particular ML problems.
Such algorithms assume the availability of full-blown quantum computers, and have been tentatively
probed since early 2000s. In recent times, however, we have witnessed a large inﬂux of ideas. Unlike
the situation we have seen in the context of quantum annealing, where an optimization subroutine
alone was run on a quantum system, in most of the approaches of this section, the entire algorithm,
and even the dataset may be quantized.
The ideas for quantum-enhancements for ML can roughly be classiﬁed into two groups: a) approaches
which rely on Grover’s search and amplitude ampliﬁcation to obtain up-to-quadratic speed-ups,
and, b) approaches which encode relevant information into quantum amplitudes, and which have a
potential for even exponential improvements. The second group of approaches forms perhaps the
most developed research line in quantum ML, and collects a plethora quantum tools – most notably
quantum linear algebra, utilized in quantum ML proposals.
a. Speed-ups by amplitude ampliﬁcation
In , it was noticed that the training of
support vector machines may be a hard optimization task, with no obviously better approaches than
brute-force search. In turn, for such cases of optimization with no structure, QIP oﬀers at least a
quadratic relief, in the guise of variants of Grover’s search algorithm or its application
to minimum ﬁnding . This idea predates, and is, in spirit, similar to some of
the early adiabatic-based proposals of the previous subsection, but the methodology is substantially
diﬀerent. The potential of quadratic improvements stemming from Grover-like search mechanisms
was explored more extensively in , in the context of unsupervised learning
tasks. There the authors assume access to a black-box oracle which computes a distance measure
between any two data-points. Using this, combined with amplitude ampliﬁcation techniques ), the authors achieve up to quadratic improvements
in key subroutines used in clustering (unsupervised learning) tasks. Speciﬁcally, improvements
are obtained in algorithms performing minimum spanning tree clustering, divisive clustering and
k-medians clustering109. Additionally, the authors also show that quantum eﬀects allow for a better
parallelization of clustering tasks, by constructing a distributed version of Grover’s search. This
construction may be particularly relevant as large databases can often be distributed.
More recently, in the author considers the problem of training deep (more than
two-layered) BMs. As we mentioned earlier, one of the bottlenecks of exactly training BMs stems
109 In minimum tree clustering, data is represented as a weighted graph (weight being the distance), and a minimum
weight spanning tree is found. k clusters are identiﬁed by simply removing the k −1- highest weight edges. Divisive
clustering is an iterative method which splits sets into two subsets according to a chosen criterion, and this process
is iterated. k−median clustering identiﬁes clusters which minimize the cumulative within-cluster distances to the
median point of the cluster.
from the fact that it requires the estimation of probabilities of certain equilibrium distributions.
Computing this analytically is typically not possible (it is as hard as computing partition functions),
and sampling approaches are costly as it requires attaining the equilibrium distribution and many
iterations to reliably estimate small values. This is often circumvented by using proxy solutions
(e.g. relying on contrastive divergence) to train approximately, but it is known that these methods
are inferior to exact training. In , a quantum algorithm is devised which
prepares coherent encodings of the target distributions, relying on quantum amplitude ampliﬁcation,
often attaining quadratic improvements in the number of training points, and even exponential
improvements in the number of neurons, in some regimes. Quadratic improvements have also been
obtained in pure data mining contexts, speciﬁcally in association rules mining ,
which, roughly speaking identiﬁes correlations between objects in large databases 110. As our ﬁnal
example in the class of quantum algorithms relying on amplitude ampliﬁcation we mention the
algorithm for the training perceptrons . Here, quantum amplitude ampliﬁcation
was used to quadratically speed up training, but, interestingly, also to quadratically reduce the error
probability. Since perceptrons constitute special cases of SVMs, this result is similar in motivation to
the much older proposal , but relies on more modern and involved techniques.
b. Precursors of amplitude encoding
In an early pioneering, and often overlooked, work , Sch¨utzhold proposed an interesting application of QC on pattern recognition problems, which
addresses many ideas which have only been investigated, and re-invented, by the community relatively
recently. The author considers the problem of identifying “patterns” in images, speciﬁed by N × M
black-and-white bitmaps, characterized by a function f : {1, . . . , N} × {1, . . . , M} →{0, 1} (which
technically coincides with a concept in CLT see II.B.1), specifying the color-value f(x, y) ∈{0, 1} of a
pixel at coordinate (x, y). The function f is given as a quantum oracle |x⟩|y⟩|b⟩
→|x⟩|y⟩|b ⊕f(x, y)⟩.
The oracle is used in quantum parallel (applied to a superposition of all coordinates), and conditioned
on the bit-value function being 1 (this process succeeds with constant probability, whenever the
density of points is constant,) leading to the state |ψ⟩= N P
x,y s.t.f(x,y)=1 |x⟩|y⟩, where N is
a normalization factor. Note, this state is proportional to the vectorized bitmap image itself,
when given in the computational basis. Next, the author points out that “patterns” – repeating
macroscopic features – can often be detected by applying discrete Fourier transform to the image
vector, which has classical complexity O(NM log(NM)). However, the quantum Fourier transform
(QFT) can be applied to the state |ψ⟩utilizing exponentially fewer gates. The author proceeds
to show that the measurements of the QFT transformed state may yield useful information, such
as pattern localization. This work is innovative in a few aspects. First, the author utilized the
encoding of data-points (here strings of binary values) into amplitudes by using a quantum memory,
in a manner which is related to the applications in the context of content-addressable memories
discussed in VI.B.1. It should be pointed out, however, that in the present application of amplitude
encoding, non-binary amplitudes have clear meaning (in say grayscale images), although this is
not explicitly discussed by the author. Second, in contrast to all previous proposals, the author
shows the potential for a quantiﬁable exponential computational complexity improvement for a
family of tasks. However, this is all contingent on having access of the pre-ﬁlled database (Uf) the
110 To exemplify the logic behind association rules mining, in the typical context of shopping, if shopping item (list
element) B occurs in nearly every shopping list in which shopping item A occurs as well, one concludes that the
person buying A is also likely to buy B. This is captured by the rule denoted B ⇒A.
loading of which would nullify any advantage. Aside from the fact that this may be considered
a one-oﬀoverhead, Sch¨utzhold discusses physical means of loading data from optical images in a
quantum-parallel approach, which may be eﬀectively eﬃcient.
c. Amplitude encoding: linear algebra tools
The very basic idea of amplitude encoding is to treat
states of N−level quantum systems, as data vectors themselves. More precisely given a data-vector
x ∈Rn, the amplitude encoding would constitute the normalized quantum state |x⟩= P
i xi |i⟩/||x||,
where it is often also assumed that norm of the vector ∥x∥can always be accessed.
Note that N−dimensional data-points are encoded into amplitudes of n ∈O(log(N)) qubits.
Any polynomial circuit applied to the n-qubit register encoding the data thus constitutes only a
polylogarithmic computation relative to the data-vector size, and this is at the basis of all exponential
improvements , discussed in the previous section)111.
These ideas have lead to a research area which could be called “quantum linear algebra” (QLA),
that is, a collection of algorithms which solve certain linear algebra problems, by directly encoding
numerical vectors into state vectors. These quantum sub-routines have then been used to speed up
numerous ML algorithms, some of which we describe later in this section. QLA includes algorithms
for matrix inversion and principal component analysis , and
many others. For didactic purposes, we will ﬁrst give the simplest example which performs the
estimation of inner products in logarithmic time.
Tool 1: inner product evaluation Given access to boxes which prepare quantum states |ψ⟩and |φ⟩,
the overlap |⟨φ |ψ⟩|2 can be estimated to precision ϵ using O(1/ϵ) copies, using the so-called the
swap-test.
The swap test applies a controlled-SWAP gate onto the state |ψ⟩|φ⟩, where
the control qubit is set to the uniform superposition |+⟩. The probability of “succeeding”, i.e.
observing |+⟩on the control after the circuit is given with (1+|⟨φ |ψ⟩|2)/2, and this can be estimated
by iteration (a more eﬃcient option using quantum phase estimation is also possible). If the states
|ψ⟩and |φ⟩encode unit-length data vectors, the success value encodes their inner product up to sign.
Norms, and phases can also be estimated by minor tweaks to this basic idea – in particular, actual
norms of the amplitude-encoded states will be accessible in a separate oracle, and used in algorithms.
The sample complexity of this process depends only on precision, whereas the gate complexity is
proportional to O(log(N)) as that many qubits need to be control-swapped and measured.
The swap test also works as expected if the reduced states are mixed, and the overall state is
product. This method of computing inner products, relative to classical vector multiplication, oﬀers
an exponential improvement with respect to N (if calls to devices which generate |ψ⟩and |φ⟩take
O(1)), at the cost of signiﬁcantly worse scaling with respect to errors, as classical algorithms have
typical error scaling with the logarithm of inverse error, O(log(1/ϵ)). However, in context of ML
problems, this is can constitute an excellent compromise.
Tool 2: quantum linear system solving Perhaps the most inﬂuential technique for quantum enhanced
algorithms for ML is based on one of the quintessential problems of linear algebra: solving systems of
111 In a related work , the authors investigate the learning capacity, of “small” quantum
systems, and identify certain limitations in the context of Bayesian learning, based on Grover optimality bounds.
Here, “small” pertains to systems of logarithmic size, encoding information in amplitudes. This work thus probes
the potential of space complexity improvements for quantum-enhanced learning, related to early ideas discussed in
equations. In their seminal paper , the authors have proposed the ﬁrst algorithm
for “quantum linear system” (QLS) solving, which performs the following. Consider an N × N
linear system Ax = b, where κ and d are the condition number112, and sparsity of the Hermitian
system matrix A113. Given (quantum) oracles giving positions and values of non-zero elements
of A, ) and an oracle which prepares the quantum state |b⟩which is the amplitude encoding
of b (up to norm), the algorithm in prepares the quantum state |x⟩which is
ϵ−close to the amplitude encoding of the solution vector x. The run-time of the ﬁrst algorithm is
˜O(κ2d2 log(N)/ϵ). Note, the complexity scales proportionally to the logarithm of the system size.
Note that any classical algorithm must scale at least with N, and this oﬀers room for exponential
improvements. The original proposal in relies on Hamiltonian simulation
(implementing exp(iAt),) upon which phase estimation is applied. Once phases are estimated,
inversely proportional amplitudes – that is, the inverses of the eigenvalues of A – are imprinted via a
measurement. It has also been noted that certain standard matrix pre-conditioning techniques can
also be applicable in the QLS scheme . The linear scaling in the error in these
proposals stems from the phase estimation subroutine. In more recent work ,
the authors also rely on best Hamiltonian simulation techniques, but forego the expensive phase
estimation. Roughly speaking, they (probabilistically) implement a linear combination of unitaries
of the form P
k αkexp(ikAt) upon the input state. This constitutes a polynomial in the unitaries
which can be made to approximate the inverse operator A−1 (in a measurement-accessible subspace)
more eﬃciently. This, combined with other numerous optimizations, yields a ﬁnal algorithm with
complexity ˜O(κdpolylog(N/ϵ)), which is essentially optimal.
It is important to note that the
apparently exponentially more eﬃcient schemes above do not trivially imply provable computational
improvements, even if we assume free access to all oracles. For instance, one of the issues is that
the quantum algorithm outputs a quantum state, from which classical values can only be accessed
by sampling. This process for the reconstruction of the complete output vector would kill any
improvements. On the other hand, certain functions of the amplitudes can be computed eﬃciently,
the computation of which may still require O(N) steps classically, yielding the desired exponential
improvement. Thus this algorithm will be most useful as a sub-routine, an intermediary step of
bigger algorithms, such as those for quantum machine learning.
Tool 3: density matrix exponentiation Density matrix exponentiation (DME) is a remarkably simple
idea, with few subtleties, and, arguably, profound consequences. Consider an N-dimensional density
matrix ρ. Now, from a mathematics perspective, ρ is nothing but a semideﬁnite positive matrix,
although it is also commonly used to denote the quantum state of a quantum system – and these
two are subtly diﬀerent concepts. In the ﬁrst reading, where ρ is a matrix (we will denote it [ρ]
to avoid confusion), [ρ] is also a valid description of a physical Hamiltonian, with time-integrated
unitary evolution exp(−i[ρ]t). Could one approximate exp(−i[ρ]t), having access to quantum systems
prepared in the state ρ? Given suﬃciently many copies (ρ⊗n), the obvious answer is yes – one could
use full state tomography to reconstruct [ρ], to arbitrary precision, and then execute the unitary
using say Hamiltonian simulation (eﬃciency notwithstanding). In , the authors
show a signiﬁcantly simpler method: given any input state σ, and one copy of ρ, the quantum state
σ′ = TrB[exp(−i∆tS)(σA ⊗ρB) exp(i∆tS)],
112 Here, the condition number of the matrix A is given by the quotient of the largest and smallest singular value of A.
113 The assumption that A is Hermitian is non-restrictive, as an oracle for any sparse matrix A can be modiﬁed to
yield an oracle for the symmetrized matrix A′ = |0⟩⟨1| ⊗A† + |1⟩⟨0| ⊗A.
where S is the Hermitian operator corresponding to the quantum SWAP gate, approximates the
desired time evolution to ﬁrst order, for small ∆t: σ′ = σ −i∆t[ρ, σ] + O(∆t2). If this process is
iterated, by using fresh copies of ρ, we obtain that the target state σρ = exp(−iρt)σ exp(iρt) can
be approximated to precision ϵ, by setting ∆t to O(ϵ/t) and using O(t2/ϵ) copies of the state ρ.
DME is, in some sense, a generalization of the process of using SWAP-tests between two quantum
states, to simulate aspects of a measurement speciﬁed by one of the quantum states. One immediate
consequence of this result is in the context of Hamiltonian simulation, which can now be eﬃciently
realized (with no dependency on the sparsity of the Hamiltonian), whenever one can prepare
quantum systems in a state which is represented by the matrix of the Hamiltonian. In particular,
this can be realized using qRAM stored descriptions of the Hamiltonian, whenever the Hamiltonian
itself is of low rank. More generally, this also implies, e.g. that QLS algorithms can also be eﬃciently
executed when the system matrix is not sparse, but rather dominated by few principal components,
i.e. close to a low rank matrix114.
Remark: Algorithms for QLS, inner product evaluation, quantum PCA, and consequently, almost
all quantum algorithms listed in the remainder of this section also assume “pre-loaded databases”,
which allow accessing of information in quantum parallel, and/or the accessing or eﬃcient preparation
of amplitude encoded states. The problem of parallel access, or even the storing of quantum states
has been addressed and mostly resolved using so-called quantum random access memory (qRAM)
architectures 115. The same qRAM structures can be also used to realize
oracles utilized in the approaches based on quantum search. However, having access to quantum
databases pre-ﬁlled with classical data does a-priori not imply that quantum amplitude encoded
states can also be generated eﬃciently, which is, at least implicilty, assumed in most works below. For
a separate discussion on the cost of some of similar assumptions, we refer the reader to , the authors have also considered continuous variable variants of qRAM, QLS
and DME, which immediately lead to continuous variables implementations of all the quantum tools
and most quantum-enhanced ML algorithms listed below.
Regression algorithms One of the ﬁrst proposals for quantum enhancements tackled linear regression
114 Since a density operator is normalized, the eigenvalues of data-matrices are rescaled by the dimension of the system.
If the eigenvalues are close to uniform, they are rendered exponentially small in the qubit number. This then
requires exponential precision in DME, which would oﬀ-set any speed-ups. However, if the spectrum is dominated
by a constant number of terms, the precision required, and overall complexity, is again independent from the
dimension, allowing overall eﬃcient algorithms.
115 qRAM realizes the following mapping: |addr⟩|b⟩
|addr⟩|b ⊕daddr⟩, where daddr represents the data stored
at the address addr (the ⊕represents modulo addition, as usual), which is the reversible variant of conventional
RAM memories. In , it was shown a qRAM can be constructed such that its internal
processing scales logarithmically in the number of memory cells.
problems, speciﬁcally, least squares ﬁtting, and relied on QLS. In least squares ﬁtting, we are given
N M-dimensional real datapoints paired with real labels, so (xi, yi)N
i=1, xi = (xj
i)j ∈RM, y = (yi)i ∈
RN. In regression y is called the response variable (also regressant or dependant variable), whereas
the datapoints xi are called predictors (or regressors or explanatory variables), and the goal of
least-squares linear regression is to establish the best linear model, that is β = (βj)j ∈RM given
argminβ∥Xβ −y∥2,
where the data matrix X collects the data-points xi as rows. In other words, linear regression
assumes a linear relationship between the predictors and the response variables. It is well-established
that the solution to the above least-squares problem is given with β = X+y, where X+ is the
Moore-Penrose pseudoinverse of the data-matrix, which is, in the case that X†X is invertible, given
with X+ = (X†X)−1X†. The basic idea in is to apply X† onto the initial vector
|y⟩which amplitude-encodes the response variables, obtaining a state proportional to X† |y⟩. This
can be done e.g. by modifying the original QLS algorithm to imprint not the
inverses of eigenvalues but the eigenvalues themselves. Following this, the task of applying (X†X)−1
(onto the generated state proportional to X† |y⟩) is interpreted as an equation-solving problem for
the system (X†X)β = X†y.
The end result is a quantum state |β⟩proportional to the solution vector β, in time O(κ4d3 log(N)/ϵ),
where κ, d and ϵ are the condition number, the sparsity of the “symmetrized” data matrix X†X,
and the error, respectively. Again, we have in general few guarantees on the behaviour of κ, and an
obvious restriction on the sparsity d of the data-matrix. However, whenever both are O(polylog(N)),
we have a potential116 for exponential improvements. This algorithm is not obviously useful for
actually ﬁnding the solution vector β, as it is encoded in a quantum state. Nonetheless, it is useful
for estimating the quality of ﬁt: essentially by applying X onto |β⟩we obtain the resulting prediction
of y, which can be compared to the actual response variable vector via a swap test eﬃciently117.
These basic ideas for quantum linear regression have since been extended in a few works. In an
extensive, and complementary work , the authors rely on the powerful technique of
“qubitization” , and optimize the goal of actually producing the best-ﬁt
parameters β. By necessity, the complexity of their algorithm is proportional to the number of
data-points M, but is logarithmic in the data dimension N, and quite eﬃcient in other relevant
parameters. In , the authors follow the ideas of more closely,
and achieve the same results as in the original work also when the data matrix is not sparse, but
rather low-rank. Further, they improve on the complexities by using other state-of-the-art methods.
This latter work critically relies on the technique of DME.
Clustering algorithms In , amplitude encoding and inner product estimation are
used to estimate the distance ∥u −¯v∥between a given data vector u and the average of a collection
of data points (centroid) ¯v = P
i vi/M for M datapoints {vi}i, in time which is logarithmic in both
116 In this section we often talk about the “potential” for exponential speed-ups because some of the algorithms as
given do not solve classical computational problems for which classical lower bounds are known. Consider the
conditions which have to be satisﬁed for the QLS algorithm to oﬀer exponential speed-ups. First, we need to be
dealing with problems where the preparation of the initial state and qRAM memory can be done in O(polylog(N)).
Next, the problem condition number must be O(polylog(N)) as well. Assuming all this is satisﬁed, we are still not
done: the algorithm generates a quantum state. As classical algorithms do not output quantum states, we cannot
talk about quantum speed-ups. The quantum state can be measured, outputting at most O(polylog(N)) (more
would kill exponential speed-ups due to printout alone) bits which are functions of the quantum state. However, the
hardness of computing these output bits, given all the initial assumptions is clearly not obvious, needs to be proven.
117 In the paper, the authors take care to appropriately symmetrize all the matrices in a manner we discussed in a
previous footnote, but for clarity, we ignore this technical step.
the vector length N, and number of points M. Using this as a building block, the authors also
show an algorithm for k-means classiﬁcation/clustering (where the computing of the distances to
the centroid is the main cost), achieving an overall complexity O(M log(MN)/ϵ), which may even
further be improved in some cases. Here, it is assumed that amplitude-encoded state vectors, and
their normalization values, are accessible via an oracle, or that they can be eﬃciently implemented
from a qRAM storing all the values. Similar techniques, combined with coherent quantum phase
estimation, and Grover-based optimization, have been also used for the problem of k-nearest
neighbour algorithms for supervised and unsupervised learning .
Quantum Principal Component Analysis The ideas of DME were in the same paper immediately applied to a quantum version of principal component analysis (PCA). PCA
constitutes one of the most standard unsupervised learning techniques, useful for dimensionality
reduction but, naturally, has a large scope of applications beyond ML. In quantum PCA, for a
quantum state ρ one applies quantum phase estimation of the unitary exp(−i[ρ]) using DME, applied
onto the state ρ itself. In the ideal case of absolute precision, given the spectral decomposition
i λi |λi⟩⟨λi| , this process generates the state P
i λi |λi⟩⟨λi| ⊗| ˜λi⟩⟨˜λi|, where ˜λi denotes the
numerical estimation of the eigenvalue λi, corresponding to the eigenvector |λi⟩. Sampling from
this state recovers both the (larger) eigenvalues, and the corresponding quantum states, which
are amplitude-encoding the eigenvectors, which may be used in further quantum algorithms. The
recovery of high-value eigenvalues and eigenvectors constitutes the essence of classical PCA as well.
Quantum Support Vector Machines One of the most inﬂuential papers in quantum-enhanced ML
relies on QLS and DME for for the task of quantizing support vector machine algorithms. For the
basic ideas behind SVMs see section II.A.2.
We focus our attention to the problem of training SVMs, as given by the optimization task in its
dual form, in Eq. (6), repeated here for convenience:
1, . . . α∗
N) = argminα1...αN
αiαjyiyjxi.xj, such that αi ≥0 and
The solution of the desired SVM is then easily computed by w∗= P
As a warm-up result, in the authors point out that using quantum evaluation
of inner products, appearing in Eq. (30), already can lead to exponential speed-ups, with respect
to the data-vector dimension N. The quantum algorithm complexity is, however, still polynomial
in the number of datapoints M, and the error dependence is now linear (as the error of the inner
product estimation is linear). The authors proceed to show that full exponential improvements
can be possible (with respect to N and M both), however for the special case of least-squares
SVMs. Given the background discussions we have already done with respect to DME and QLS, the
basic idea is here easy to explain. Recall that the problem of training least-squares SVMs reduces
to a linear program, speciﬁcally a least-squares minimization. As we have seen previously, such
minimization reduces to equation solving, which was given by the system in Eq. (14), which we
repeat here:
1N Ω+ γ−1I
Here, 1 is an “all ones” vector, Y is the vector of labels yi, α is the vector of the Lagrange multipliers
yielding the solution, b is the oﬀset, γ is a parameter depending on the hyperparameter C, and Ω
is the matrix collecting the (mapped) inner products of the training vectors so Ωi,j = xi.xj. The
key technical aspects of demonstrate how the system above is realized
in a manner suitable for QLS. To give a ﬂavour of the approach, we will simply point out that
the system sub-matrix Ωis proportional to the reduced density matrix of the quantum state
i |xi| |i⟩1 |xi⟩2 , obtained after tracing out the subsystem 2. This state can, under some constraints,
be eﬃciently realized with access to qRAM encoding the data-points. Following this, DME enables
the application of QLS where the system matrix has a block proportional to Ω, up to technical
details we omit for brevity. The overall quantum algorithm generates the quantum state proportional
to |ψout⟩∝b |0⟩+ PM
i=1 αi |i⟩, encoding the oﬀset and the multipliers. The multipliers need not be
extracted from this state by sampling. Instead any new point can be classiﬁed by (1) generating an
amplitude-encoded state of the input, and (2) estimating the inner product between this state and
out⟩∝b |0⟩|0⟩+ PM
i=1 αi|xi| |i⟩|xi⟩, which is obtained by calling the quantum data oracle using
|ψout⟩. This process has an overall complexity of O(κ3
effϵ−3 log(MN)), where κeff depends on the
eigenstructure of the data matrix. Whenever this term is polylogarithmic in data size, we have a
potential for exponential improvements.
Gaussian process regression In the authors demonstrate how QLS can be used
to dramatically improve Gaussian process regression (GPR), a powerful supervised learning method.
GPR can be thought of as a stochastic generalization of standard regression: given a training set
{xi, yi}, it models the latent function (which assigns labels y to data-points), assuming Gaussian
noise on the labels f(x) = y + ϵ where ϵ encodes independent and identically distributed More
precisely, GPR is a process in which an initial distribution over possible latent functions is reﬁned
by taking into account the training set points, using Bayesian inference. Consequently, the output
of GPR is, roughly speaking, a distribution over models f which are consistent with the observed
data (the training set). While the descriptions of such a distribution may be large, in computational
terms, to predict the value of a new point x∗, in GPR, one needs to compute two numbers: a linear
predictor (also referred to as the predictive mean, or simply mean), and the variance of the predictor,
which are speciﬁc to x∗. These numbers characterize the distribution of the predicted value y∗by
the GPR model which is consistent with the training data. Further, it turns out, both values can
be computed using modiﬁed QLS algorithms. The fact that this ﬁnal output size is independent
from the dataset size, combined with QLS, provides possibilities for exponential speed-ups in terms
of data size. This, naturally holds, provided the data is available in qRAM, as is the case in most
algorithms of this section. It should be mentioned that the authors take meticulous care of listing
out all the “hidden costs”, (and the working out intermediary algorithms) in the ﬁnal tally of the
computational complexity.
Geometric and topological data analysis All the algorithms we presented in this subsection thus far
critically depend on having access to “pre-loaded” databases – the loading itself would introduce
a linear dependence on the database size, whereas the inner-product, QLS and DME algorithms
provide potential for just logarithmic dependence. However, this can be circumvented in the cases
where the data-points in the quantum database can be eﬃciently computed individually. This
is reminiscent of the fact that most applications of Grover’s algorithm have a step in which the
Grover oracle is eﬃciently computed. In ML applications, this can occur if the classical algorithm
requires, as a computational step, a combinatorial exploration of the (comparatively small) dataset.
Then, the quantum algorithm can generate the combinatorially larger space in quantum parallel –
thereby eﬃciently computing the eﬀective quantum database. The ﬁrst example where this was
achieved was presented in , in context of topological and geometric data analysis.
These techniques are very promising in the context of ML, as topological features of data do not
depend on the metric of choice, and thus capture the truly robust, features of the data. The notion
of topological features (in the ML world of discrete data points) are given by those properties
which exist when data is observed at diﬀerent spatial resolutions. Such persistent features are thus
robust and less likely to be artefacts of noise, or choice of parameters, and are mathematically
formalized through so-called persistent homology. A particular family of features of interest are
the number of connected components, holes, voids (or cavities). These numbers, which are deﬁned
for simplicial complexes (roughly, a closed set of simplices), are called Betti numbers. To extract
such features from data, one must thus construct nested families of simplical complexes from the
data, and compute the corresponding features captured by the Betti numbers. However, there
are combinatorially many simplices one should consider, and which should be analyzed, and one
can roughly think of each possible simplex as data-points which need further analysis. However,
they are eﬃciently generated from a small set – essentially the collection of the pair-wise distances
between datapoints. The authors show how to generate quantum states which encode the simplexes
in logarithmically few qubits, and further show that from this representation, the Betti numbers can
be eﬃciently estimated. Iterating this at various resolutions allows the identiﬁcation of persistent
features. As usual, full exponential improvements happen under some assumptions on the data, and
here they are manifest in the capacity to eﬃciently construct the simplical states – in particular,
having the total number of simplices in the complex be exponentially large would suﬃce, although
it is not clear when this is the case, see . This proposal provides evidence that
quantum ML methods based on amplitude encoding may, at least in some cases, yield exponential
speed-ups even if data is not pre-stored in a qRAM or an analogous system.
As mentioned a large component of modern approaches to quantum-enhanced ML, relies on quantum
linear algebra techniques, and any progress in this area may lead to new quantum ML algorithms.
A promising recent examples of this were given in terms of algorithms for quantum gradient descent
 , which could e.g. lead to novel quantum
methods for training neural networks.
VII. QUANTUM LEARNING AGENTS, AND ELEMENTS OF QUANTUM AI
The topics discussed thus far in this review, with few exceptions, deal with the relationship between
physics, mostly QIP, and traditional ML techniques which allow us to better understand data, or the
process which generates it. In this section, we go one step beyond data analysis and optimization
techniques and address the relationship between QIP and more general learning scenarios, or even
between QIP and AI. As mentioned, in more general learning or AI discussions, we typically talk
about agents, interacting with their environments, which may be, or more often fail to be, intelligent.
In our view, by far the most important aspect of any intelligent agent, is its capacity to learn from
its interactions with its environment. However, general intelligent agents learn in environments
which are complex and changeable. Further, the environments are susceptible to being changed
by the agent itself, which is the crux of e.g. learning by experiments. All this delineates general
learning frameworks, which begin with RL, from more restricted settings of data-driven ML.
In this section, we will consider physics-oriented approaches to learning via interaction, speciﬁcally
the PS model, and then focus on quantum-enhancements in the context of RL118. Following this, we
118 Although RL is a particularly mathematically clean model for learning by interaction, it is worthwhile to note
will discuss an approach for considering the most general learning scenarios, where the agent, the
environment and their interaction, are treated quantum-mechanically: this constitutes a quantum
generalization of the broad AE framework, underlying modern AI. We will ﬁnish oﬀbrieﬂy discussing
other results from QIP which may play a role in the future of QAI, which do not directly deal with
learning, but which may still play a role in the future of QAI.
A. Quantum learning via interaction
Executive summary: The ﬁrst proposal which addressed the speciﬁcation of learning agents,
designed with the possibility of quantum processing of episodic memory in mind, was the
model of Projective Simulation PS. The results on quantum improvements of agents which learn
by interacting with classical environments have mostly been given within this framework. The
PS agent deliberates by eﬀectively projecting itself into conceivable situations, using its memory,
which organizes its episodic experiences in a stochastic network. Such an agent can solve basic
RL problems, meta-learn, and solve problems with aspects of generalization. The deliberation
is a stochastic diﬀusion process, allowing for a few routes for quantization. Using quantum
random walks, quadratic speed-ups can be obtained.
The applications of QIP to reinforcement and other interactive learning problems has been comparatively less studied, when compared to quantum enhancements in supervised and unsupervised
problems. One of the ﬁrst proposals which provides a coherent view on learning agents from a
physics perspective was that of Projective Simulation (abbrv. PS) .
We ﬁrst provide a detailed description the PS model, and review the few other works related to
this topic at the end of the section. PS is a ﬂexible framework for the design of learning agents
motivated both from psychology and physics, and inﬂuenced by modern views on robotics. One of
the principal reasons why we focus on this model is that it provides a natural route to quantization,
which will be discussed presently. However already the classical features of the model reveal an
underlying physical perspective which may be of interest for the reader, and which we brieﬂy expose
The PS viewpoint on (quantum) agents is conceived around a few basic principles. First, in the PS
view, the agent is a physical, or rather, an embodied entity, existing relative to its environment,
rather than a mathematical abstraction119. Note, this does not prohibit computer programs to be
agents: while the print-out of the code is not an agent, the executed instantiation of the code, the
running program, so to speak, has its own well-deﬁned virtual interfaces, which delineate it from,
and allow interaction with other programs in its virtual world – in this sense, that program too is
embodied. Second, the interfaces of the agent are given by its sensors, collecting the environmental
input, and the actuators, enabling the agent to act on the environment. Third, the learning is
learning from experience, and, the interfaces of the agent constrain the elementary experiences of the
agent to be collections from the sets of percepts S = {si}i which the agent can perceive and actions
A = {ai}i. At this point we remark that the basic model assumes discretized time, and sensory
it is not fully general – for instance learning in real environments always involves supervised and other learning
paradigms to control the size of the exploration space, but also various other techniques which occur when we try
to model settings in continuous, or otherwise not turn-based fashion.
119 For instance, the Q-learning algorithm (see section II.C) is typically deﬁned without an embodied agent-environment
context. Naturally, we can easily promote this particular abstract model to an agent, by deﬁning an agent which
internally runs the Q-learning algorithm.
space, which is consistent with actual realizations, although this could be generalized. Fourth, a
(good) learning agent’s behaviour – that is, the choice of actions, given certain percepts – is based
on its cumulative experience, accumulated in the agent’s memory, which is structured. This brings
us to the central concept of the PS framework, which is the memory of the agent: the episodic and
compositional memory (ECM).
The ECM is a structured network of units of experience which are called clips or episodes. A clip,
denoted ci, can represent120 an individual percept or action, so ci ∈S ∪A – and indeed there is no
other external type appearing in the PS framework. However, experiences may be more complex
(such as an autobiographical episodic memory, similar to short video-clips, where we remember
a temporally extended sequence of actions and percepts that we experienced). This brings us to
the following recursive deﬁnition: a clip is either a percept, an action, or a structure over clips.
FIG. 12 a) The agent learns to associate symbols to one of the
two movements. b) the internal PS network requires only action
and percept clips, arranged in two layers, with connections only
from actions to percepts.
The “smiling” edges are rewarded.
Adapted from .
Typical examples of structured clips
are percept-action (s1, a1, . . . , sk, ak)
sequences describing what happened,
a k−length history of interaction between the agent and environment. Another example are simple
sets of percepts (s1 or s2 . . .), which
will be later used to generalize knowledge. The overall ECM is a network of
clips (that is, a labeled directed graph,
where the vertices are the clips), where
the edges organize the agent’s previous experiences, and has a functional purpose explained momentarily. Fifth, learning agent must act: that is, there has to be a deﬁned deliberation mechanism,
which given a current percept, the state of memory, i.e. the current ECM network, the agent,
probabilistically decides on (or rather “falls into”) the next action and performs it. Finally, sixth,
a learning agent must learn, that is, the ECM network must change under experiences and this
occurs in two modes: by (1) changing the weights of the edges, and (2) the topology of the network,
through the addition of deletion of clips. The above six principles describe the basic blueprint
behind PS agents. The construction of a particular agent will require us to further specify certain
components, which we will exemplify using the simplest example: a reinforcement learning PS agent,
capable of solving the so-called invasion game. In the invasion game, the agent Fig 12 is facing an
attacker, who must be blocked by appropriately moving to the left or right. These two options form
the actions of the agent.
The attacker presents a symbol, say a left- or right- pointing arrow, to signal what its next move
will be. Initially, the percepts have no meaning for the agent, and indeed the attacker can alter the
meaning in time. The basic scenario here is, in RL terms a contextual two-armed bandit problem
 , where the agent gets rewarded when it correctly couples the two
percepts to the two actions.
The basic PS agent that can solve this is speciﬁed as follows. The action and percept spaces are the
two moves, and two signals, so A = {−, +} (left and right move), and S = {←, →}, respectively. The
clips set is just the union of the two sets. The connections are directed edges from percepts to actions,
120 Representation means that we, strictly speaking, distinguish actual percepts, from the memorized percepts, and the
same for actions. This distinction is however not crucial for the purposes of this exposition.
weighted with real values, called h−values, hij ≥1, which form the h−matrix. The deliberation is
realized by a random walk in the memory space, governed proportionally to the h−matrix: that is
the probability of transition from percept s to action a is given with p(a|s) =
a′ hs,a′ . In other
words, the column-wise normalized h−matrix speciﬁes the stochastic transition matrix of the PS
model, in the Markov chain sense. Finally, the learning is manifest in the tuning of the h−values,
via an update rule, which is in its most basic form given with:
ht+1(cj, ci) = ht(cj, ci) + δcj,ciλ,
where t, t + 1 denote consecutive time steps, λ denotes the reward received in the last step, and
δcj,ci is 1 if and only if the ci to cj transition occurred in the previous step. Simply stated, used
edges get rewards. The h−value ht(ci, cj) associated to the edges connecting clips ci, cj, when the
time step t is clear from context we will simply denote hij.
One can easily see that the above rule constitutes a simple RL mechanism, and that it will indeed
over time lead to a winning strategy in the invasion game; since only the correctly paired transitions
get rewards, they are taken more and more frequently. However, these h−values in this simple
process diverge, which also makes re-learning, in the eventuality the rules of the game change, more
diﬃcult with time. To manage this, one typically introduces a decay, or dissipation, parameter γ
leading to the rule:
ht+1(cj, ci) = ht(cj, ci) −γ(ht(cj, ci) −1) + δcj,ciλ.
The dissipation is applied at each time step.
Note that since the dissipating term diminishes the values of ht(cj, ci) by an amount proportional
to the deviation of these values from 1, where 1 is the initial value. The above rule leads to the unit
value h = 1 when there are no rewards, and a limiting upper value of 1 + λ/γ, when every move is
FIG. 13 Basic learning curves for PS with non-zero γ
in the invasion game with a rules switch at time step
250. Adapted from .
This limits maximal eﬃciency to 1−(2+λ/γ)−1,
but, as a trade-oﬀ, leads to much faster relearning. This is illustrated in Fig. 13.
The update rules can get a bit more involved, in
the setting of delayed rewards. For instance, in a
maze, or so called grid-world settings, illustrated
in Fig. 14, it is a sequence of actions that leads
to a reward. In other words, the ﬁnal reward
must “propagate” to all relevant percept-action
edges which were involved in the winning move
In the basic PS model, this is done via a socalled glow mechanism: to each edge in the
ECM, a glow value gij is assigned in addition to
the hij−value. It is set to 1 whenever the edge
is used, and decays with the rate η ∈ , that
ij = (1 −η)gt−1
ij . The h−value update rule
is appended to reward all “glowing” edges, proportional to the glow value, whenever a reward
is issued:
ht+1(cj, ci) = ht(cj, ci) −γ(ht(cj, ci) −1) + gt(cj, ci)λ.
In other words, all the edges which contributed to the ﬁnal reward get a fraction, in proportion to
how recently they were used. This parallels the intuition that the more recent actions relative to
the rewarded move played a larger role in getting rewarded.
The expression in Eq. 33 has functional similarities to the Q-learning action-value update rule in
Eq. 21. However, the learning dynamics is diﬀerent, and the expressions are conceptually diﬀerent
– Q-learning updates estimate bounded Q-values, whereas the PS is not a state-value estimation
method, but rather a purely reward-driven system.
The PS framework allows other constructions as well. In , the
authors also introduced emoticons – edge-speciﬁc ﬂags, which capture aspects of intuition. These
can be used to speed-up re-learning via a reﬂection mechanism, where a random walk can be iterated
multiple times, until a desired – ﬂagged – set of actions is hit, see 
for more detail. Further in this direction, the deliberation of the agent can be based not on a hitting
process – where the agent performs the ﬁrst action it hits – but rather on a mixing process. In
the latter case, the ECM is a collection Markov chains, and the correct action is sampled from the
stationary distribution over the ECM. This model is referred to as the reﬂective PS (rPS) model, see
Fig. 15. Common to all models, however, is that the deliberation process is governed by a stochastic
walk, speciﬁed by the ECM.
FIG. 14 The environment is essentially a
grid, where each site has an individual percept, the moves dictate the movements of
the agent (say up, down, left, right), and
certain sites are blocked oﬀ– walls. The
agent explores this world looking for the
rewarded site. When the exit is found, a
reward is given and the agent is reset to
the same initial position.
Adapted from
 .
Regarding performance, the basic PS structure, with a
two-layered network encoding percepts and actions – which
matches standard tabular RL approaches – was extensively
analysed and benchmarked against other models . However, the questions
that are emphasized in PS literature diverge from questions
of performance in RL tasks, in two directions. First, the
authors are interested in the capacities of the PS model
beyond textbook RL.
For instance, in it was shown that
the action composition aspects of the ECM allow the
agent to perform better in some benchmarking scenarios, which had a natural application for example in the
context of protecting MBQC from unitary noise , and in the context of ﬁnding novel quantum
experiments , elaborated on in section IV.C. Further, by utilizing the capacity of ECM to
encode larger and multiple networks, we can also address
problems which require generalization – inferring correct behaviour by percept similarity –
but also design agents which autonomously optimize their
own meta-parameters, such as γ and η in the PS model. That is, the agents can meta-learn . These problems go beyond the basic RL framework, and the PS framework is ﬂexible
enough to also allow the incorporation of other learning models – e.g. neural networks could be
used to perform dimensionality reduction (which could allow for broader generalization capabilities),
or even to directly optimize the ECM itself. The PS model has been combined with such additional
learning machinery in an application to robotics and haptic skill learning .
However, there is an advantage into keeping the underlying PS dynamics homogenous, that is,
essentially solely based on random walks over the PS network, in that if oﬀers a few natural routes
to quantization. This is the second direction of foundational research in PS. For instance, in the authors expressed the entire classical PS deliberation dynamics as
a incoherent part of a Liouvillean dynamics (master equation for the quantum density operator),
which also included some coherent part (Hamiltonian-driven unitary dynamics). This approach may
yield advantages both in deliberation time and also expands the space of internal policies the agent
can realize.
Another perspective on the quantization of the PS model was developed in the framerowk of
discrete-time quantum walks. In , the authors have exploited the paradigm
of Szegedy-style quantum walks, to improve quadratically deliberation times of rPS agents. The
Szegedy approach to random walks can be used to specify a unitary random walk
operator UP , for a given transition matrix P 121, whose spectral properties are intimately related to
those of P itself. We refer the reader to the original references for the exact speciﬁcation of UP , and
just point out that UP can be eﬃciently constructed via a simple circuit depending on P, or given
black-box access to entries of P.
Assume P corresponds to an irreducible and aperiodic (guaranteeing a unique stationary distribution),
and also time-reversible (meaning it satisﬁes detailed balance conditions) Markov chain. Let π = (πi)i
be the unique stationary distribution of P, and δ the spectral gap of P 122, and |π⟩= P
be the coherent encoding of the distribution π. Then we have that a) UP |π⟩= |π⟩, and b) the
eigenstates {λi} of P and eigenphases θi of UP are related by λi = cos(θi)123. This is important
as the spectral properties, speciﬁcally the spectral gap δ more-or-less tightly ﬁxes the mixing
time – that is the number of applications of P needed to obtain the stationary distribution –
to ˜O(1/δ), by the famous Aldous bounds . This quantity will later bound the
complexity of classical agents. In contrast, for UP , we have that its non-zero eigenphases θ are
not smaller than ˜O(1/
δ). This quadratic diﬀerence between the inverse spectral eigenvalue gap
in the classical case, and the eigenphase gap in the quantum case is at the crux of all speed-ups.
QrPS representation of network,
and its steady state over non-action (red)
and action (blue) clips.
In , it was shown how the above
properties of UP can be used to construct a quantum
operator R(π) ≈1 −2 |π⟩⟨π| , which exponentially ef-
ﬁciently approximates the reﬂection over the encoding
of the stationary distribution |π⟩. The basic idea in the
construction of R(π) is to apply phase estimation onto UP
with precision high enough to detect non-zero phases, impose a global phase on all states with a non zero detected
phase, and undo the process.
Due to the quadratic relationship between the inverse
spectral gap, and the smallest eigenphase, this can be
achieved in time ˜O(1/
δ). That is, we can reﬂect over
121 By transition matrix, we mean an entry-wise non-negative matrix, with columns adding to unity.
122 The spectral gap is deﬁned with δ = 1 −|λ2|, where λ2 is, in norm, the second largest eigenvalue.
123 In full detail, these relations hold whenever the MC is lazy (all states transition back to themselves with probability
at least 1/2 ), ensuring that all the eigenvalues are non-negative, which can be ensured by adding the identity
transition with probability 1/2. This slows down mixing and hitting processes by an irrelevant factor of 2.
the (coherent encoding of the) stationary distribution, whereas obtaining it by classical mixing
takes ˜O(1/δ) applications of the classical walk operator. In this was used to
obtain quadratically accelerated deliberation times for the rPS agent. In the rPS model, the ECM
network has a special structure, enforced by the update rules. In particular, for each percept s we
can consider the subnetwork ECMs, which collects all the clips one can reach starting from s. By
construction, it contains all the action clips, but also other, intermediary clips. The corresponding
Markov chain Ps, governing the dynamics of ECMs, is, by construction, irreducible, aperiodic and
time-reversible. In the deliberation process, given percept s, the deliberation process mixes the
corresponding Markov chain Ps, and outputs the reached clip, provided it is an action clip, and
repeats the process otherwise.
Computationally speaking, we are facing the problem of outputting a single sample, clip c, drawn
according to the conditional probability distribution p(c) = πc/ϵ if c ∈A and p(c) = 0 otherwise.
Here ϵ is the total weight of all action clips in π. The classical computational complexity of this
task is given by the product of ˜O(1/δ) – which is the mixing cost, and O(1/ϵ) which is the average
time needed to actually hit an action clip. Using the Szegedy quantum walk techniques, based on
constructing the reﬂector R(π), followed by an amplitude ampliﬁcation algorithm to “project” onto
the action space, we obtain a quadratically better complexity of ˜O(1/
δ) × O(1/√ϵ). In full detail,
this is achievable if we can generate one copy of the coherent encoding of the stationary distribution
eﬃciently at each step, and in the context of the rPS this can be done in many cases as was shown
in , and further generalized in and .
The proposal in was the ﬁrst example of a provable quantum speed-up in
the context of RL 124, and was followed up by a proposal for an experimental demonstration
 , which identiﬁed a possibility of a modular implementation based on coherent
controlization – the process of adding control to almost unknown unitaries.
It is worth-while to note that further progress in algorithms for quantum walks and quantum Markov
chain theory has the potential to lead to quantum improvements of the PS model. This to an
extent mirrors the situation in quantum machine learning, where new algorithms for quantum linear
algebra may lead to quantum speed-ups of other supervised and unsupervised algorithms.
Computational speed-ups of deliberation processes in learning scenarios are certainly important,
but in strict RL paradigm, such internal processing does not matter, and the learning eﬃciency
depends only on the number of interaction steps needed to achieve high quality performance. Since
the rPS and its quantum analog, the so-called quantum rPS agent are, by deﬁnition, behaviorally
equivalent (i.e. they perform the same action with the same probability, given identical histories),
their learning eﬃciency is the same. The same, however, holds in the context of all the supervised
learning algorithms we discussed in previous sections, where the speed-ups were in the context of
computational complexity. In contrast, quantum CLT learning results did demonstrate improvements
in sample complexity, as discussed in section VI.A.
While formally distinct, computational and sample complexity can become more closely related the
moment the learning settings are made more realistic. For instance, if the training of a given SVM
requires the solution of a BQP complete problem125, classical machines will most likely be able to
124 We point out that the ﬁrst ideas suggesting that quantum eﬀects could be useful had been previously suggested in
 .
125 BQP stands for bounded-error quantum polynomial, and collects decision problems which can be solved with
bounded error using a quantum computer. Complete problems of a given class are, in a sense, the hardest problems
in that class, as all other are reducible to the complete instances using weaker reductions. In particular, it is not
believed BQP complete problems are solvable on a classical computer, whereas all decision problems solvable by
classical computers do belong to the class BQP.
run classiﬁcation instances which are uselessly small. In contrast, a quantum computer could run
such a quantum-enhanced learner. The same observation motivates most of research into quantum
annealers for ML, see section VI.C.1.
In , similar ideas were more precisely formalized in the context of active
reinforcement learning, where the interaction is occurring relative to some external real time. This
is critical, for instance, in settings where the environment changes relative to this real time, which is
always the case in reality. If the deliberation time is slow relative to this change, the agent perceives
a “blurred”, time-averaged environment where one cannot learn. In contrast, a faster agent will have
time to learn before the environment changes – and this makes a qualitative diﬀerence between the
two agents. In the next section we will show how actual learning eﬃciency, in the rigid metronomic
turn-based setting can also be improved, under stronger assumptions.
As mentioned, works which directly apply quantum techniques to RL, or other interactive modes of
learning, are comparatively few in numbers, despite the ever growing importance of RL. These results
still constitute quite isolated approaches, and we brieﬂy review two recent papers. In the authors design a RL algorithm based on a deep Boltzmann machine, and combine
this with quantum annealing methods for training such machines to achieve a possible speed-up.This
work combines multiple interesting ideas, and may be particularly relevant in the light of recent
advances in quantum annealing architectures. In , the authors demonstrated certain
building blocks of larger quantum RL agents in systems of superconducting qubits.
B. Quantum agent-environment paradigm for reinforcement learning
Executive summary: To characterize the ultimate scope and limits of learning agents in
quantum environments, one must ﬁrst establish a framework for quantum agents, quantum
environments and their interaction: a quantum AE paradigm. Such a paradigm should
maintain the correct classical limit, and preserve the critical conceptual components – in particular
the history of the agent-environment interaction, which is non-trivial in the quantum case. With
such a paradigm in place the potential of quantum enhancements of classical agents is explored,
and it is shown that quantum eﬀects, under certain assumptions, can help near-generically
improve the learning eﬃciency of agents. A by-product of the quantum AE paradigm is
a classiﬁcation of learning settings, which is diﬀerent and complementary to the classiﬁcation
stemming from a supervised learning perspective.
The topics of learning agents acting in quantum environments, and the more general questions of
the how agent-environment interactions should be deﬁned, have to this day only been broached
in few works by the authors of this review and other co-authors. As these topics may form the
general principles underlying the upcoming ﬁeld of quantum AI, we take liberty to present them to
substantial detail.
Motivated by the pragmatic question of the potential of quantum enhancements in general learning
settings, in it was suggested that the ﬁrst step should be the identiﬁcation of a
quantum generalization of the AE paradigm, which underlies both RL and AI. This is comparatively
easy to do in ﬁnite-sized, discrete space settings.
a. Quantum agent-environment paradigm
The (abstract) AE paradigm, roughly illustrated in Fig.
6, can be understood as a two-party communication scenario, the quantum descriptions of which
are well-understood in QIP. In particular, the two players – here the agent, and the environment
– are modelled as (inﬁnite) sequences of unitary maps {Ei
A}i, and {Ei
E}i, respectively. They both
have private memory registers RA and RE, with matching Hilbert spaces HA, and HE, and to
enable precise speciﬁcation of how they communicate (and to cleanly delineate the two players),
the register of the communication channel, RC, is introduced, and it is the register which is alone
accessible to both players – that is, the maps of the agent act on HA ⊗HC and of the environment
on HE ⊗HC126. The two players then interact by sequentially applying their respective maps in
turn (see Fig. 16).
To further tailor this fully general setting for the AE paradigm purposes, the percept and action sets
are promoted to sets of orthonormal vectors {|s⟩|s ∈S} and {|a⟩|a ∈A}, which are also mutually
orthogonal. These are referred to as classical states. The Hilbert space of the channel is spanned by
these two sets, so HC = span{|x⟩| x ∈S ∪A}.
This also captures the notion that the agent/environment only performs one action, or issues
one percept, per turn. Without loss of generality, we can also assume that the state-spaces of
the agent’s and environment’s registers are also spanned by sequences of percepts and actions.
It is without loss of generality assumed that the reward status is encoded in the percept space.
RL: Tested agent-environment interaction
suitable for RL. In general, each map of the tester U T
acts on a fresh subsystem of the register RT , which is
not under the control of the agent, nor of the environment. The crossed wires represent multiple systems.
DL: The simpler setting of standard quantum machine
learning, where the environmental map is without internal memory, presented in the same framework.
It should be mentioned that the quantum AE
paradigm also includes all other quantum ML
settings as a special case. For instance, most
quantum-enhanced ML algorithms assume access to quantum database, a quantum memory,
and this setting is illustrated in Fig. 16, part
DL. Since the quantum database is without loss
of generality a unitary map, it requires no additional memory of its own, nor does it change
over interaction steps.
At this point, the classical AE paradigm can
be recovered when the maps of the agent and
environment are restricted to “classical maps”,
which, roughly speaking do not generate superpositions of classical states, nor entanglement
when applied to classical states.
Further, we now obtain a natural classiﬁcation
of generalized AE settings: CC, CQ, QC and QQ, depending on whether the agent or the environment
are classical (C) or quantum (Q). We will come back to this classiﬁcation in section VII.B.1.
The performance of a learning agent, beyond internal processing time, is a function of the history of
interaction, which is a distribution over percept-action sequences (of a given ﬁnite length) which
can occur between a given agent and environment. Any genuine learning-related ﬁgure of merit, for
instance, the probability of a reward at a given time-step (eﬃciency), or number of steps needed
before the eﬃciency is above a threshold (learning speed) is a function of the interaction history. In
126 Other delineations are possible, where the agent and environment have individually deﬁned interfaces – a part of E
accesible to A and a part of A accessible to E – leading to a four-partite system, but we will not be considering this
here .
the classical case, the history can simply be read out by a classical-basis measurement of the register
HC, as the local state of the communication register is diagonal in this basis, and not entangled to
the other systems – meaning the measurement does not perturb, i.e. commutes with the interaction.
In the quantum case this is not, in general, the case. To recover a robust notion of a history (needed
for gauging of the learning), a more detailed description of measurement is used, which captures
weaker measurements as well: an additional system, a tester is added, which interchangeably couples
to the HC register, and can copy full or partial information to a separate register. Formally, this is
a sequence of controlled maps, relative to the classical basis, controlled by the states on HC and
acting on a separate register, as illustrated in Fig. 16. The tester can copy the full information,
when the maps are a generalized controlled-NOT gate – in which case it is called a classical tester –
or even do nothing, in which case the interaction is untested. The restriction of the tester to maps
which are controlled with respect to the classical basis guarantees that a classical interaction will
never be perturbed by its presence. With this basic framework in place, the authors show a couple
of basic theorems characterizing when any quantum separations in learning-related ﬁgures of merit
of can be expected at all. The notion of quantum separations here are the same as in the context
of oracular computation, or quantum PAC theory: a separation means no classical agent could
achieve the same performance. The authors prove basic expected theorems: quantum improvements
(separations) require a genuine quantum interaction, and, further, full classical testing prohibits this.
Further, they show that for any speciﬁcation of a classical environment, there exists a “quantum
implementation” – a sequence of maps {Ei
E}i – which is consistent with the classical speciﬁcation,
and prohibits any quantum improvements.
The interactions for the classical (A) and
quantum-enhanced classical agent (Aq). In Steps 1
and 2, Aq uses quantum access to an oracularized
environment Eq
oracle to obtain a rewarding sequence
hr. Step 3: Aq simulates the agent A, and ‘trains’ the
simulation to produce the rewarding sequence. In Step
4, Aq uses the pre-trained agent for the remainder of
the now classically tested interaction, with the classical
environment E. Adapted from .
b. Provable quantum improvements in RL
However, if the above no-go scenarios are relaxed,
much can be achieved.
The authors provide a structure of task environments (roughly
speaking, maze-type problems), speciﬁcation of
quantum-accessible realizations of these environments, and a sporadic tester (which leaves
a part of the interaction untested), for which
classical learning agents can often be quantumenhanced. The idea has a few steps, which we
only very brieﬂy sketch out. As a ﬁrst step, the
environments considered are deterministic and
strictly episodic – this means the task is reset
after some M steps. Since the environments
are deterministic, whether or not rewards are
given depends only on the sequence of actions,
as the interlacing percepts are uniquely speciﬁed. Since everything is reset after M steps
there are no correlations in the memory of the
environment between the blocks, i.e. episodes.
This allows for the speciﬁcation of a quantum version of the same environment, which can be
accessed in superpositions and which takes blocks of actions and returns the same sequence plus
a reward status – moreover, it can be realized such that it is self-inverse127. With access to such
an object, a quantum agent can actually Grover-search for an example of a winning sequence. To
convert this exploration advantage to a learning advantage, the set of agents and environments is
restricted to pairs which are “luck-favoring”, i.e. those where better performance in the past implies
improved performance in the future, relative to a desired ﬁgure of merit. Under these conditions, any
learning agent which is luck-favoring relative to a given environment can be quantum enhanced by
ﬁrst using quantum access to quadratically faster ﬁnd the ﬁrst winning instance, which is then used
to “pre-train” the agent in question. The overall quantum-enhanced agent, provably outperforms
the basic classical agent. The construction is illustrated in Fig. 17. These results can be generalized
to a broader class of environments. Although these results form the ﬁrst examples of quantum
improvements in learning ﬁgures of merit in RL contexts, the assumptions of having access to
“quantized” environments of the type used–in essence, the amount of quantum control the agent
is assumed to have– are quite restrictive from a practical perspective. The questions of minimal
requirements, and the questions of the scope of improvements possible are still unresolved.
1. AE-based classiﬁcation of quantum ML
The AE paradigm is typically encountered in the contexts of RL, robotics, and more general AI
settings, while it is less common in ML communities. Nonetheless, conventional ML scenarios can
naturally be embedded in this paradigm, since it is, ultimately, mostly unrestrictive. For instance,
supervised learning can be thought of as an interaction with an environment which is, for a certain
number of steps, an eﬀective database (or the underlying process, generating the data), providing
training examples. After a certain number of steps, the environment starts providing unlabeled datapoints, and the agent responds with the labels. If we further assume the environment additionally
responds with the correct label to whatever the agent sent, when the data-point/percept was from
the training set, we can straightforwardly read out the empirical risk (training set error) from the
history. Since the quantization of the AE paradigm naturally leads to four settings – CC, CQ, QC
and QQ – depending on whether the agent, or environment, or both are fully quantum systems, we
can classify all of the results in quantum ML into one of the four groups. Such coarse grained division
places standard ML in CC, results on using ML to control quantum systems in CQ, quantum speed
ups in ML algorithms (without a quantum database, as is the case in annealing approaches) in QC,
and quantum ML/RL where the environments, databases or oracles are quantum-accessible are QQ.
This classiﬁcation is closely related to the classiﬁcation introduced in , which
uses the Lcontext
, notation, where “context” may denote we are dealing with classical or quantum
data and/or learner, and “goal” speciﬁes the learning task (see section V.A.1 for more details).
The QAE-based separation is not, however, identical to it: for instance classical learning tasks
may require quantum or classical access – this distinguishes the examples of quantum speed-ups in
internal processing in ML which require a quantum database, and those which do not. In operative
terms, this separation makes sense as the database must be pre-ﬁlled at some point, and if this is
included we obtain a QC setting (which now may fail to be eﬃcient in terms of communication
complexity). On the other hand, the Lcontext
systematics does a nice job separating classical ML,
from quantum generalizations of the same, discussed in section V. This mismatch also illustrates
127 This realization is possible under a couple of technical assumptions, for details see .
the diﬃculties one encounters if a suﬃciently coarse-grained classiﬁcation of the quantum ML ﬁeld
is required. The classiﬁcation criteria of this ﬁeld, and also aspects of QAI in this review have
been inspired by both the AE-induced criteria (perhaps natural from a physics perspective), and
the Lcontext
classiﬁcation (which is more objective driven, and natural from a computer science
perspective).
C. Towards quantum artiﬁcial intelligence
Executive summary: Can quantum computers help us build (quantum) artiﬁcial intelligence?
The answer to this question cannot be simpler than the answer to the to the deep, and largely
open, question of what intelligence is in the ﬁrst place. Nonetheless, at least for very pragmatic
readings of AI, early research directions into what QAI may be in the future can be
identiﬁed. We have seen that quantum machine learning enhancements and generalizations cover
data analysis and pattern matching aspects. Quantum reinforcement learning demonstrates how
interactive learning can be quantum-enhanced. General QC can help with various planning,
reasoning, and similar symbol manipulation tasks intelligent agents seem to be good at. Finally,
the quantum AE paradigm provides a framework for the design and evaluation of whole quantum
agents, built also from quantum-enhanced subroutines. These conceptual components form a
basis for a behaviour-based theory of quantum-enhanced intelligent agents.
AI is quite a loaded concept, in a manner in which ML is not. The question of how genuine AI can be
realized is likely to be as diﬃcult as the more basic question of what intelligence is at all, which has
been puzzling philosophers and scientists for centuries. Starting a broad discussion of when quantum
AI will be reached, and what will be like, is thus clearly ill-advised. We can nonetheless provide
a few less controversial observations. The ﬁrst observation is the fact that the overall concept of
quantum AI might have multiple meanings. First, it may pertain to a generalization of the very
notions of intelligence, in the sense section V discusses how classical learning concepts generalize to
include genuinely quantum extensions. A second, and a perhaps more pragmatic reading of quantum
AI, may ask whether quantum eﬀects can be utilized to generate more intelligent agents, where the
notion of intelligence itself is not generalized: quantum-enhanced artiﬁcial intelligence. We
will focus on this latter reading for the remainder of this review, as quantum generalization of basic
learning concepts on its own, just as the notion of intelligence on its own, seem complicated enough.
To comment on the question of quantum-enhanced AI, we ﬁrst remind the reader that the conceptual
debates in AI often have two perspectives. The ultimately pragmatic perspective is concerned only
with behavior in relevant situations. This is perhaps best captured by Alan Turing, who suggested
that it may be irrelevant what intelligence is, if it can be recognized, by virtue of similarity to a
“prototype” of intelligence – a human 128. Another perspective tends to try to capture
cognitive architectures, such as SOAR developed by John Laird, Allen Newell, and Paul Rosenbloom
 . Cognitive architectures try to identify the components needed to build intelligent
agents, capable of many tasks, and thus also care about how the intelligence is implemented. They
often also serve as models of human cognition, and are both theories of what cognition is, and how
128 Interestingly, the Turing test assumes that humans are good supervised learners of the concept of “intelligent
agents”, all the while being incapable of specifying the classiﬁer – the deﬁnition of intelligence – explicitly.
to implement it. A third perspective comes from the practitioners of AI who often believe that
AI will be a complicated combination of various methods and techniques including learning and
specialized algorithms, but are also sympathetic to the Turing test as the deﬁnitional method. A
simple reading of this third perspective is particularly appearing, as it allows us to all but equate
computation, ML and AI. Consequently all quantum machine learning algorithms, and even broader,
even most quantum algorithms already constitute progress in quantum AI. Aspects of such reading
can be found in a few works on the topic 129.
The current status of the broad ﬁeld of quantum ML and related research is showing signs of activity
with respect to all of the three aspects mentioned. The substantial activity in the context of ML
improvements, in all aspects presented, is certainly ﬁlling the toolbox of methods which one day may
play a role in the complicated designs of quantum AI practitioners. In this category, a relevant role
may also be played by various algorithms which may help in planning, pruning, reasoning via symbol
manipulation, and other tasks AI practice and theory encounters. Many possible quantum algorithms
which may be relevant come to mind. Examples include the algorithm for performing Bayesian
inference , algorithms for quadratic and super-polynomial improvements in NANDand boolean-tree evaluations, which are important in evaluation of optimal strategies in two-player
games 130 . Further, even more exotic
ideas, such as quantum game theory , may be relevant. Regarding approaches
to quantum artiﬁcial general intelligence, and, related, to quantum cognitive architectures, while
no proposals explicitly address this possibility, the framework of PS oﬀers suﬃcient ﬂexibility and
structure that it may be considered a good starting point. Further, this framework is intended
to keep a homogenous structure, which may lead to more straightforward global quantization, in
comparison to models which are built out of inhomogeneous blocks – already in classical systems,
the performance of system combined out of inhomogeneous units may lead to diﬃcult-to-control
behaviour, and it stands to reason that quantum devices may have a more diﬃcult time to be
synchronized. It should be mentioned that recently there have been works providing a broad
framework describing how composite large quantum systems can be precisely treated . Finally, from the ultimate pragmatic perspective, the quantum AE paradigm presented
can oﬀer a starting point for a quantum-generalized Turing test for QAI, as the Turing test itself
ﬁts in the paradigm: the environment is the administrator of the test, and the agent is the machine
trying to convince the environment it is intelligent. Although, momentarily, the only suitable referees
for such a test are classical devices – humans – it may be conceivable they, too, may ﬁnd quantum
gadgets useful to better ascertain the nature of the candidate 131. However, at this point it is prudent
to remind ourselves and the reader, that all the above considerations are still highly speculative,
and that the research into genuine AI has barely broken ground.
VIII. OUTLOOK
In this review, we have presented overviews of various lines of research that connect the ﬁelds of
quantum information and quantum computation, on the one side, and machine learning and artiﬁcial
129 It should be mentioned that some of the early discussions on quantum AI also consider the possibilities that human
brains utilize some form of quantum processing, which may be at the crux of human intelligence. Such claims are
still highly hypothetical, and not reviewed in this work.
130 See for a simple explanation.
131 This is reminiscent to the problem of quantum veriﬁcation, where quantum Turing test is a term used for the test
which eﬃciently decides whether the Agent is a genuine quantum device/computer 
intelligence, on the other side. Most of the work in this new area of research is still largely theoretical
and conceptual, and there are, for example, hardly any dedicated experiments demonstrating how
quantum mechanics can be exploited for ML and AI. However, there are a number of theoretical
proposals and also ﬁrst experimental works
showing how these ideas can be implemented in the laboratory 132. At the same time it is clear that certain quantum
technologies, which have been developed in the context in QIP and QC, can be readily applied to
quantum learning, to the extent that learning agents or algorithms employ elements of quantum
information processing in their very design. Similarly, it is clear, and there are by now several
examples, how techniques from classical machine learning can be fruitfully employed in data analysis
and the design of experiments in quantum many-body physics (see section IV.D). One may ask
about the long-term impact of the exchange of concepts and techniques between QM and ML/AI.
Which implications will this exchange have on the development of the individual ﬁelds, and what is
the broader perspective of these individual activities leading towards a new ﬁeld of research, with
its own questions and promises? Indeed, returning the focus back to the topics of this review, we
can highlight one overarching question encapsulating the collective eﬀort of the presented research:
⇒What are the potential, and the limitations of an interaction between quantum physics,
and ML and AI?
From a purely theoretical perspective, we can learn from analogies with the ﬁelds of communication,
computation, or sensing. QIP has shown that to understand the limits of such information processing
disciplines, both in pragmatic and conceptual sense, one must consider the full extent of quantum
theory. Consequently, we should expect that the limits of learning, and of intelligence can also only
be fully answered in this broader context. In this sense, the topics discussed in sections V already
point to the rich and complex theory describing what learning may be, when even information itself
is a quantum object, and aspects of the section VII.C point to how a general theory of quantum
learning may be phrased133. The motivation of phrasing such a general theory may be fundamental,
but it also may have more pragmatic consequences. In fact, arguments can be made that the ﬁeld
of quantum machine learning and the future ﬁeld of quantum AI may constitute one of the
most important research ﬁelds to emerge in recent times. A part of the reason behind
such a bold claim stems from the obvious potential of both directions of inﬂuence between the two
constituent sides of quantum learning (and quantum AI). For instance, the potential of quantum
enhancements for ML is profound. In a society where data is generated at geometric rate134, and
where its understanding may help us combat global problems, the potential of faster, better analyses
cannot be overestimated. In contrast, ML and AI technologies are becoming indispensable tools in
all high technologies, but they are also showing potential to help us do research in a novel, better
way. A more subtle reason supporting optimism lies in positive feedback loops between ML, AI and
QIP which are becoming apparent, and which is moreover, speciﬁc to these two disciplines. To begin
with, we can claim that QC, once realized, will play an integral part in future AI systems, on general
grounds. This can be deduced from even a cursory overview of the history of AI, which reveals that
qualitative improvements in computing and information technologies result in progress in AI tasks,
132 These complement the experimental work based on superconducting quantum annealers , which is closely related to one of the approaches to QML.
133 The question of whether information may be quantum, and whether we can talk about “quantum knowledge” as
an outside observer broaches the completely fundamental questions of interpretations of quantum mechanics: for
instance a Quantum Bayesianist would likely reject such a third-person perspective on learning.
134 
which is also intuitive. In simple terms, state-of-the-art in AI will always rely on state-of-the-art
in computing. In contrast, ML and AI technologies are becoming indispensable tools in all high
technologies.
The perfect match between ML, AI and QIP, however may have deeper foundations. In
particular,
→advancements in ML/AI may help with critical steps in the building of quantum computers.
In recent times, it has become ever more apparent that learning methods may make the diﬀerence
between a given technology being realizable or being eﬀectively impossible – beyond obvious examples,
for instance direct computational approaches to build a human-level Go-playing software had failed,
whereas AlphaGo , a fundamentally learning AI technology, achieved this complex
goal. QC may in fact end up being such a technology, where exquisite fast, and adaptive control –
realized by an autonomous smart laboratory perhaps, helps mitigate the hurdles towards quantum
computers. However, cutting edge research discussed in sections IV.C and IV.D suggest that ML
and AI techniques could help at an even deeper level, by helping us discover novel physics which
may be the missing link for full blown quantum technologies. Thus ML and AI may be what we
need to build quantum computers.
Another observation, which is hinted at increasing frequency in the community, and which fully
entwines ML, AI and QIP, is that
→AI/ML applications may be the best reasons to build quantum computers.
Quantum computers have been proven to dramatically outperform their classical counterparts only
on a handful of (often obscure) problems. Perhaps the best applications of quantum computers that
have enticed investors until recently were quantum simulation and quantum cryptology (i.e. using
QC to break encryption), which may have been simply insuﬃcient to stimulate broad-scale public
investments. In contrast ML and AI-type tasks may be regarded as the “killer applications” QC has
been waiting for. However, not only are ML and AI applications well motivated – in recent times,
arguments have been put forward that ML-type applications may be uniquely suited to be tackled
by quantum technologies. For instance, ML-type applications deal with massive parallel processing
of high dimensional data – quantum computers seem to be good for this. Further, while most
simulation and numerics tasks require data stability, which is incompatible with the noise modern
days quantum devices undergo, ML applications always work with noisy data. This means that such
an analysis makes sense only if it is robust to noise to start with, which is the often unspoken fact
of ML: the important features are the robust features. Under such laxer set of constraints on the
desired information processing, various current day technologies, such as quantum annealing methods
may become a possible solution. The two main ﬂavours, or directions of inﬂuence, in quantum
ML thus have a natural synergistic eﬀect further motivating that despite their quite fundamental
diﬀerences, they should be investigated in close collaboration. Naturally, at the moment, each
individual sub-ﬁeld of quantum ML comes with its own set of open problems, key issues which
need to be resolved before any credible verdict on the future of quantum ML can be made. Most
ﬁt in one of the two quintessential categories of research into quantum-enhanced topic: a) what
are the limits/how much of an edge over best classical solutions can be achieved, and b) could the
proposals be implemented in practice in any reasonable term. For most of the topics discussed, both
questions above remain widely open. For instance, regarding quantum-enhancements using universal
computation, only a few models have been beneﬁcially quantized, and the exact problem they solve,
even in theory, is not matching the best established methods used in practice. Regarding the second
facet, the most impressive improvements (barring isolated exceptions) can be achieved only under
a signiﬁcant number of assumptions, such as quantum databases, and certain suitable properties
the structure of the data-sets135. Beyond particular issues which were occasionally pointed out in
various parts of this review, we will forego providing an extensive list of speciﬁc open questions for
each of the research lines, and refer the interested reader to the more specialized reviews for more
detail .
This leads us to the ﬁnal topic of speculation of this outlook section: whether QC will truly be
instrumental in the construction of genuine artiﬁcial (general) intelligence. On one hand, there
is no doubt that quantum computers could help in heavily computational problems one typically
encounters in, e.g., ML. In so far as AI reduces to sets of ML tasks, quantum computing may
help. But AI is more than a sum of such speciﬁc-task-solving parts. Moreover, human brains are
(usually) taken as a reference for systems capable of generating intelligent behaviour. Yet there is
little, and no non-controversial, reason to believe genuine quantum eﬀects play any critical part
in their performance (rather, there is ample reasons to dismiss the relevance of quantum eﬀects).
In other words, quantum computers may not be necessary for general AI. The extent to which
quantum mechanics has something to say about general AI will be subject of research in years to
come. Nonetheless, already now, we can set aside any doubt that quantum computers and AI can
help each other, to an extent which will not be disregarded.
ACKNOWLEDGEMENTS
The authors are grateful to Walter Boyajian, Jens Clausen, Joseph Fitzsimons, Nicolai Friis, Alexey
A. Melnikov, Davide Orsucci, Hendrik Poulsen Nautrup, Patrick Rebentrost, Katja Ried, Maria
Schuld, Gael Sent´ıs, Omar Shehab, Sebastian Stabinger, Jordi Tura i Brugu´es, Petter Wittek and
Sabine W¨olk for helpful comments to various parts of the manuscript.