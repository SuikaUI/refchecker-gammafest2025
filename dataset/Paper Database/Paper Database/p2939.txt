Proceedings of SemEval-2016, pages 1–18,
San Diego, California, June 16-17, 2016. c⃝2016 Association for Computational Linguistics
SemEval-2016 Task 4: Sentiment Analysis in Twitter
Preslav Nakov♣, Alan Ritter♦, Sara Rosenthal♥, Fabrizio Sebastiani♣∗, Veselin Stoyanov♠
♣Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar
♦Department of Computer Science and Engineering, The Ohio State University, USA
♥IBM Watson Health Research, USA
♠Johns Hopkins University, USA
This paper discusses the fourth year of
the ”Sentiment Analysis in Twitter Task”.
SemEval-2016 Task 4 comprises ﬁve subtasks, three of which represent a signiﬁcant
departure from previous editions.
two subtasks are reruns from prior years and
ask to predict the overall sentiment, and the
sentiment towards a topic in a tweet.
three new subtasks focus on two variants of
the basic “sentiment classiﬁcation in Twitter”
task. The ﬁrst variant adopts a ﬁve-point scale,
which confers an ordinal character to the classiﬁcation task.
The second variant focuses
on the correct estimation of the prevalence of
each class of interest, a task which has been
called quantiﬁcation in the supervised learning literature. The task continues to be very
popular, attracting a total of 43 teams.
Introduction
Sentiment classiﬁcation is the task of detecting
whether a textual item (e.g., a product review, a
blog post, an editorial, etc.)
expresses a POSI-
TIVE or a NEGATIVE opinion in general or about
a given entity, e.g., a product, a person, a political
party, or a policy. Sentiment classiﬁcation has become a ubiquitous enabling technology in the Twittersphere. Classifying tweets according to sentiment
has many applications in political science, social sciences, market research, and many others .
∗Fabrizio Sebastiani is currently on leave from Consiglio
Nazionale delle Ricerche, Italy.
As a testament to the prominence of research on
sentiment analysis in Twitter, the tweet sentiment
classiﬁcation (TSC) task has attracted the highest
number of participants in the last three SemEval
campaigns .
Previous editions of the SemEval task involved
binary (POSITIVE vs. NEGATIVE) or single-label
multi-class classiﬁcation (SLMC) when a NEU-
TRAL1 class is added (POSITIVE vs. NEGATIVE vs.
NEUTRAL). SemEval-2016 Task 4 represents a signiﬁcant departure from these previous editions. Although two of the subtasks (Subtasks A and B) are
reincarnations of previous editions (SLMC classiﬁcation for Subtask A, binary classiﬁcation for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks
C and D) and in combination (Subtask E):
Ordinal Classiﬁcation
We replace the two- or three-point scale with a ﬁvepoint scale {HIGHLYPOSITIVE, POSITIVE, NEU-
TRAL, NEGATIVE, HIGHLYNEGATIVE}, which is
now ubiquitous in the corporate world where human ratings are involved: e.g., Amazon, TripAdvisor, and Yelp, all use a ﬁve-point scale for rating sentiment towards products, hotels, and restaurants.
Moving from a categorical two/three-point scale
to an ordered ﬁve-point scale means, in machine
learning terms, moving from binary to ordinal classiﬁcation (a.k.a. ordinal regression).
1We merged OBJECTIVE under NEUTRAL, as previous attempts to have annotators distinguish between the two have consistently resulted in very low inter-annotator agreement.
Quantiﬁcation
We replace classiﬁcation with quantiﬁcation, i.e.,
supervised class prevalence estimation. With regard
to Twitter, hardly anyone is interested in whether a
speciﬁc person has a positive or a negative view of
the topic. Rather, applications look at estimating the
prevalence of positive and negative tweets about a
given topic. Most (if not all) tweet sentiment classiﬁcation studies conducted within political science
 , economics
 , social
science , and market research
 ,
use Twitter with an interest in aggregate data and not
in individual classiﬁcations.
Estimating prevalences (more generally, estimating the distribution of the classes in a set of unlabelled items) by leveraging training data is called
quantiﬁcation in data mining and related ﬁelds. Previous work has argued that quantiﬁcation is not a
mere byproduct of classiﬁcation, since (a) a good
classiﬁer is not necessarily a good quantiﬁer, and
vice versa, see, e.g., ; (b) quantiﬁcation requires evaluation measures different from
classiﬁcation.
Quantiﬁcation-speciﬁc learning approaches have been proposed over the years; Sections 2 and 5 of contain
several pointers to such literature.
Note that, in Subtasks B to E, tweets come labelled with the topic they are about and participants need not classify whether a tweet is about a
given topic. A topic can be anything that people express opinions about; for example, a product (e.g.,
iPhone6), a political candidate (e.g., Hillary Clinton), a policy (e.g., Obamacare), an event (e.g., the
Pope’s visit to Palestine), etc.
The rest of the paper is structured as follows. In
Section 2, we give a general overview of SemEval-
2016 Task 4 and the ﬁve subtasks. Section 3 focuses
on the datasets, and on the data generation procedure. In Section 4, we describe in detail the evaluation measures for each subtask. Section 5 discusses
the results of the evaluation and the techniques and
tools that the top-ranked participants used. Section 6
concludes, discussing the lessons learned and some
possible ideas for a followup at SemEval-2017.
Task Deﬁnition
SemEval-2016 Task 4 consists of ﬁve subtasks:
1. Subtask A: Given a tweet, predict whether it is
of positive, negative, or neutral sentiment.
2. Subtask B: Given a tweet known to be about a
given topic, predict whether it conveys a positive or a negative sentiment towards the topic.
3. Subtask C: Given a tweet known to be about a
given topic, estimate the sentiment it conveys
towards the topic on a ﬁve-point scale ranging from HIGHLYNEGATIVE to HIGHLYPOS-
4. Subtask D: Given a set of tweets known to be
about a given topic, estimate the distribution
of the tweets in the POSITIVE and NEGATIVE
5. Subtask E: Given a set of tweets known to be
about a given topic, estimate the distribution
of the tweets across the ﬁve classes of a ﬁvepoint scale, ranging from HIGHLYNEGATIVE
to HIGHLYPOSITIVE.
Subtask A is a rerun – it was present in all three previous editions of the task. In the 2013-2015 editions,
it was known as Subtask B.2 We ran it again this year
because it was the most popular subtask in the three
previous task editions. It was the most popular subtask this year as well – see Section 5.
Subtask B is a variant of SemEval-2015 Task 10
Subtask C , with POSITIVE, NEUTRAL, and NEGATIVE
as the classiﬁcation labels.
Subtask E is similar to SemEval-2015 Task 10
Subtask D, which consisted of the following problem: Given a set of messages on a given topic from
the same period of time, classify the overall sentiment towards the topic in these messages as strongly
positive, weakly positive, neutral, weakly negative,
or strongly negative.
Note that in SemEval-2015
Task 10 Subtask D, exactly one of the ﬁve classes
had to be chosen, while in our Subtask E, a distribution across the ﬁve classes has to be estimated.
2Note that we retired the expression-level subtask A, which
was present in SemEval 2013–2015 .
As per the above discussion, Subtasks B to E
are new. Conceptually, they form a 2×2 matrix, as
shown in Table 1, where the rows indicate the goal
of the task (classiﬁcation vs. quantiﬁcation) and the
columns indicate the granularity of the task (twovs. ﬁve-point scale).
Granularity
Five-point
Classiﬁcation
Quantiﬁcation
Table 1: A 2×2 matrix summarizing the similarities and the
differences between Subtasks B-E.
In this section, we describe the process of collection
and annotation of the training, development and testing tweets for all ﬁve subtasks. We dub this dataset
the Tweet 2016 dataset in order to distinguish it from
datasets generated in previous editions of the task.
Tweet Collection
We provided the datasets from the previous editions3
(see Table 2) of this task for training and development. In addition we
created new training and testing datasets.
Twitter2013-train
Twitter2013-dev
Twitter2013-test
SMS2013-test
Twitter2014-test
Twitter2014-sarcasm
LiveJournal2014-test
Twitter2015-test
Table 2: Statistics about data from the 2013-2015 editions of
the SemEval task on Sentiment Analysis in Twitter, which could
be used for training and development for SemEval-2016 Task 4.
3For Subtask A, we did not allow training on the testing
datasets from 2013–2015, as we used them for progress testing.
We employed the following annotation procedure. As in previous years, we ﬁrst gathered tweets
that express sentiment about popular topics.
this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned named entity
recognition system . The collected tweets were greatly skewed towards the neutral class. In order to reduce the class imbalance, we
removed those that contained no sentiment-bearing
words. We used SentiWordNet 3.0 as a repository of sentiment words. Any
word listed in SentiWordNet 3.0 with at least one
sense having a positive or a negative sentiment score
greater than 0.3 was considered sentiment-bearing.4
The training and development tweets were collected from July to October 2015. The test tweets
were collected from October to December 2015. We
used the public streaming Twitter API to download
the tweets.5
We then manually ﬁltered the resulting tweets to
obtain a set of 200 meaningful topics with at least
100 tweets each (after ﬁltering out near-duplicates).
We excluded topics that were incomprehensible,
ambiguous (e.g., Barcelona, which is the name both
of a city and of a sports team), or too general
(e.g., Paris, which is the name of a big city). We
then discarded tweets that were just mentioning the
topic but were not really about the topic.
Note that the topics in the training and in the test
sets do not overlap, i.e., the test set consists of tweets
about topics different from the topics the training
and development tweets are about.
Annotation
The 2016 data consisted of four parts:
(for training models), DEV (for tuning models),
DEVTEST (for development-time evaluation), and
TEST (for the ofﬁcial evaluation). The ﬁrst three
datasets were annotated using Amazon’s Mechanical Turk, while the TEST dataset was annotated on
CrowdFlower.
4Filtering based on an existing lexicon does bias the dataset
to some degree; however, the text still contains sentiment expressions outside those in the lexicon.
5We distributed the datasets to the task participants in a
similar way: we only released the annotations and the tweet
IDs, and the participants had to download the actual tweets
by themselves via the API, for which we provided a script:
 download
Instructions: Given a Twitter message and a topic, identify whether the message is highly positive, positive, neutral,
negative, or highly negative (a) in general and (b) with respect to the provided topic. If a tweet is sarcastic, please
select the checkbox “The tweet is sarcastic”. Please read the examples and the invalid responses before beginning
if this is the ﬁrst time you are working on this HIT.
Figure 1: The instructions provided to the Mechanical Turk annotators, followed by a screenshot.
Annotation with Amazon’s Mechanical Turk.
Human Intelligence Task (HIT) consisted of providing all required annotations for a given tweet message. In order to qualify to work on our HITs, a
Mechanical Turk annotator (a.k.a. “Turker”) had to
have an approval rate greater than 95% and to have
completed at least 50 approved HITs.
was carried out by ﬁve Turkers and consisted of ﬁve
tweets to be annotated. A Turker had to indicate the
overall polarity of the tweet message (on a ﬁve-point
scale) as well as the overall polarity of the message
towards the given target topic (again, on a ﬁve-point
scale). The annotation instructions along with an example are shown in Figure 1. We made available to
the Turkers several additional examples, which are
shown in Table 3.
We rejected HITs with the following problems:
• one or more responses do not have the overall
sentiment marked;
• one or more responses do not have the sentiment towards the topic marked;
• one or more responses appear to be randomly
Annotation with CrowdFlower.
We annotated
the TEST data using CrowdFlower, as it allows better quality control of the annotations across a number of dimensions. Most importantly, it allows us to
ﬁnd and exclude unreliable annotators based on hidden tests, which we created starting with the highestconﬁdence and highest-agreement annotations from
Mechanical Turk. We added some more tests manually. Otherwise, we setup the annotation task giving exactly the same instructions and examples as in
Mechanical Turk.
Consolidation of annotations.
In previous years,
we used majority voting to select the true label (and
discarded cases where a majority had not emerged,
which amounted to about 50% of the tweets). As this
year we have a ﬁve-point scale, where the expected
agreement is lower, we used a two-step procedure. If
three out of the ﬁve annotators agreed on a label, we
accepted the label. Otherwise, we ﬁrst mapped the
categorical labels to the integer values −2, −1, 0, 1,
2. Then we calculated the average, and ﬁnally we
mapped that average to the closest integer value. In
order to counter-balance the tendency of the average
to stay away from −2 and 2, and also to prefer 0, we
did not use rounding at ±0.5 and ±1.5, but at ±0.4
and ±1.4 instead.
To give the reader an idea about the degree of
agreement, we will look at the TEST dataset as an
example. It included 20,632 tweets. For 2,760, all
ﬁve annotators assigned the same value, and for another 9,944 there was a majority value. For the remaining 7,928 cases, we had to perform averaging
as described above.
The consolidated statistics from the ﬁve annotators on a three-point scale for Subtask A are shown
in Table 4. Note that, for consistency, we annotated
the data for Subtask A on a ﬁve-point scale, which
we then converted to a three-point scale.
The topic annotations on a two-point scale for
Subtasks B and D are shown in Table 5, while those
on a ﬁve-point scale for Subtasks C and E are in Table 6. Note that, as for Subtask A, the two-point
scale annotation counts for Subtasks B and D derive from summing the POSITIVEs with the HIGH-
LYPOSITIVEs, and the NEGATIVEs with the HIGH-
LYNEGATIVEs from Table 6; moreover, this time we
also remove the NEUTRALs.
Overall Sentiment
Topic Sentiment
Why would you still wear shorts when it’s this cold?! I
love how Britain see’s a bit of sun and they’re like ’OOOH
LET’S STRIP!
Britain: NEGATIVE
Saturday without Leeds United is like Sunday dinner it
doesn’t feel normal at all (Ryan)
Leeds United: HIGHLYPOSITIVE
Who are you tomorrow? Will you make me smile or just
bring me sorrow? #HottieOfTheWeek Demi Lovato
Demi Lovato: POSITIVE
Table 3: List of example tweets and annotations that were provided to the annotators.
Table 4: 2016 data statistics (Subtask A).
Table 5: 2016 data statistics (Subtasks B and D).
HIGHLYPOSITIVE
HIGHLYNEGATIVE
Table 6: 2016 data statistics (Subtasks C and E).
As we use the same test tweets for all subtasks,
the submission of results by participating teams was
subdivided in two stages: (i) participants had to submit results for Subtasks A, C, E, and (ii) only after
the submission deadline for A, C, E had passed, we
distributed to participants the unlabelled test data for
Subtasks B and D.
Otherwise, since for Subtasks B and D we ﬁlter
out the NEUTRALs, we would have leaked information about which the NEUTRALs are, and this information could have been used in Subtasks C and E.
Finally, as the same tweets can be selected for different topics, we ended up with some duplicates; arguably, these are true duplicates for Subtask A only,
as for the other subtasks the topics still differ. This
includes 25 duplicates in TRAIN, 3 in DEV, 2 in DE-
VTEST, and 116 in TEST. There is a larger number
in TEST, as TEST is about twice as large as TRAIN,
DEV, and DEVTEST combined. This is because we
wanted a large TEST set with 100 topics and 200
tweets per topic on average for Subtasks C and E.
Evaluation Measures
This section discuss the evaluation measures for the
ﬁve subtasks of our SemEval-2016 Task 4. A document describing the evaluation measures in detail6
 , and a scoring software implementing all the ﬁve “ofﬁcial” measures, were made
available to the participants via the task website together with the training data.7
For Subtasks B to E, the datasets are each subdivided into a number of “topics”, and the subtask
needs to be carried out independently for each topic.
As a result, each of the evaluation measures will be
“macroaveraged” across the topics, i.e., we compute
the measure individually for each topic, and we then
average the results across the topics.
6 
7An earlier version of the scoring script contained a bug,
to the effect that for Subtask B it was computing F P N
not ρP N. This was detected only after the submissions were
closed, which means that participants to Subtask B who used the
scoring system (and not their own implementation of ρP N) for
parameter optimization, may have been penalized in the ranking
as a result.
Subtask A: Message polarity classiﬁcation
Subtask A is a single-label multi-class (SLMC) classiﬁcation task. Each tweet must be classiﬁed as belonging to exactly one of the following three classes
C={POSITIVE, NEUTRAL, NEGATIVE}.
We adopt the same evaluation measure as the
2013-2015 editions of this subtask, F PN
1 is the F1 score for the POSITIVE class:
1 = 2πP ρP
Here, πP and ρP denote precision and recall for the
POSITIVE class, respectively:
PP + PU + PN
PP + UP + NP
where PP, UP, NP, PU, PN are the cells of the
confusion matrix shown in Table 7.
Gold Standard
POSITIVE NEUTRAL NEGATIVE
Table 7: The confusion matrix for Subtask A. Cell XY stands
for “the number of tweets that the classiﬁer labeled X and the
gold standard labells as Y ”. P, U, N stand for POSITIVE,
NEUTRAL, NEGATIVE, respectively.
is deﬁned analogously, and the measure we
ﬁnally adopt is F PN
as from Equation 1.
Subtask B: Tweet classiﬁcation according
to a two-point scale
Subtask B is a binary classiﬁcation task. Each tweet
must be classiﬁed as either POSITIVE or NEGATIVE.
For this subtask we adopt macroaveraged recall:
2(ρP + ρN)
In the above formula, ρP and ρN are the positive and the negative class recall, respectively. Note
that U terms are entirely missing in Equation 5; this
is because we do not have the NEUTRAL class for
SemEval-2016 Task 4, subtask A.
ρPN ranges in , where a value of 1 is
achieved only by the perfect classiﬁer (i.e., the classiﬁer that correctly classiﬁes all items), a value of 0
is achieved only by the perverse classiﬁer (the classiﬁer that misclassiﬁes all items), while 0.5 is both
(i) the value obtained by a trivial classiﬁer (i.e., the
classiﬁer that assigns all tweets to the same class –
be it POSITIVE or NEGATIVE), and (ii) the expected
value of a random classiﬁer. The advantage of ρPN
over “standard” accuracy is that it is more robust to
class imbalance. The accuracy of the majority-class
classiﬁer is the relative frequency (aka “prevalence”)
of the majority class, that may be much higher than
0.5 if the test set is imbalanced. Standard F1 is also
sensitive to class imbalance for the same reason. Another advantage of ρPN over F1 is that ρPN is invariant with respect to switching POSITIVE with NEG-
ATIVE, while F1 is not. See for
more details on ρPN.
As we noted before, the training dataset, the development dataset, and the test dataset are each subdivided into a number of topics, and Subtask B needs
to be carried out independently for each topic. As a
result, the evaluation measures discussed in this section are computed individually for each topic, and
the results are then averaged across topics to yield
the ﬁnal score.
Subtask C: Tweet classiﬁcation according
to a ﬁve-point scale
Subtask C is an ordinal classiﬁcation (OC –
also known as ordinal regression) task, in which
each tweet must be classiﬁed into exactly one
of the classes in C={HIGHLYPOSITIVE,
HIGHLYNEGA-
TIVE}, represented in our dataset by numbers in
{+2,+1,0,−1,−2}, with a total order deﬁned on C.
The essential difference between SLMC (see Section 4.1 above) and OC is that not all mistakes weigh
equally in the latter. For example, misclassifying a
HIGHLYNEGATIVE example as HIGHLYPOSITIVE
is a bigger mistake than misclassifying it as NEGA-
TIVE or NEUTRAL.
As our evaluation measure, we use macroaveraged mean absolute error (MAEM):
MAEM(h, Te) = 1
|h(xi)−yi|
where yi denotes the true label of item xi, h(xi)
is its predicted label, Tej denotes the set of test
documents whose true class is cj, |h(xi) −yi| denotes the “distance” between classes h(xi) and yi
(e.g., the distance between HIGHLYPOSITIVE and
NEGATIVE is 3), and the “M” superscript indicates
“macroaveraging”.
The advantage of MAEM over “standard” mean
absolute error, which is deﬁned as:
MAEµ(h, Te) =
|h(xi) −yi|
is that it is robust to class imbalance (which is useful, given the imbalanced nature of our dataset). On
perfectly balanced datasets MAEM and MAEµ are
equivalent.
Unlike the measures discussed in Sections 4.1 and
4.2, MAEM is a measure of error, and not accuracy,
and thus lower values are better. See for more detail on MAEM.
Similarly to Subtask B, Subtask C needs to be carried out independently for each topic. As a result,
MAEM is computed individually for each topic,
and the results are then averaged across all topics
to yield the ﬁnal score.
Subtask D: Tweet quantiﬁcation according
to a two-point scale
Subtask D also assumes a binary quantiﬁcation
setup, in which each tweet is classiﬁed as POSITIVE
or NEGATIVE. The task is to compute an estimate
ˆp(cj) of the relative frequency (in the test set) of
each of the classes.
The difference between binary classiﬁcation (as
from Section 4.2) and binary quantiﬁcation is that
errors of different polarity (e.g., a false positive and
a false negative for the same class) can compensate
each other in the latter. Quantiﬁcation is thus a more
lenient task since a perfect classiﬁer is also a perfect
quantiﬁer, but a perfect quantiﬁer is not necessarily
a perfect classiﬁer.
We adopt normalized cross-entropy, better known
as Kullback-Leibler Divergence (KLD). KLD was
proposed as a quantiﬁcation measure in , and is deﬁned as follows:
KLD(ˆp, p, C) =
p(cj) loge
KLD is a measure of the error made in estimating
a true distribution p over a set C of classes by means
of a predicted distribution ˆp. Like MAEM in Section 4.3, KLD is a measure of error, which means
that lower values are better. KLD ranges between 0
(best) and +∞(worst).
Note that the upper bound of KLD is not ﬁnite
since Equation 7 has predicted prevalences, and not
true prevalences, at the denominator: that is, by
making a predicted prevalence ˆp(cj) inﬁnitely small
we can make KLD inﬁnitely large. To solve this
problem, in computing KLD we smooth both p(cj)
and ˆp(cj) via additive smoothing, i.e.,
p(cj)) + ϵ · |C|
= p(cj) + ϵ
1 + ϵ · |C|
where ps(cj) denotes the smoothed version of p(cj)
and the denominator is just a normalizer (same for
the ˆps(cj)’s); the quantity ϵ =
2·|Te| is used as a
smoothing factor, where Te denotes the test set.
The smoothed versions of p(cj) and ˆp(cj) are
used in place of their original versions in Equation 7;
as a result, KLD is always deﬁned and still returns
a value of 0 when p and ˆp coincide.
KLD is computed individually for each topic,
and the results are averaged to yield the ﬁnal score.
Subtask E: Tweet quantiﬁcation according
to a ﬁve-point scale
Subtask E is an ordinal quantiﬁcation (OQ) task,
in which (as in OC) each tweet belongs exactly to
one of the classes in C={HIGHLYPOSITIVE, POSI-
TIVE, NEUTRAL, NEGATIVE, HIGHLYNEGATIVE},
where there is a total order on C. As in binary quantiﬁcation, the task is to compute an estimate ˆp(cj) of
the relative frequency p(cj) in the test tweets of all
the classes cj ∈C.
The measure we adopt for OQ is the Earth
Mover’s Distance ), a
measure well-known in the ﬁeld of computer vision.
EMD is currently the only known measure for ordinal quantiﬁcation.
It is deﬁned for the general
case in which a distance d(c′, c′′) is deﬁned for each
c′, c′′ ∈C. When there is a total order on the classes
in C and d(ci, ci+1) = 1 for all i ∈{1, ..., (C −1)}
(as in our application), the Earth Mover’s Distance
is deﬁned as
EMD(ˆp, p) =
and can be computed in |C| steps from the estimated
and true class prevalences.
Like KLD in Section 4.4, EMD is a measure
of error, so lower values are better; EMD ranges
between 0 (best) and |C| −1 (worst). See for more details on EMD.
As before, EMD is computed individually for
each topic, and the results are then averaged across
all topics to yield the ﬁnal score.
Participants and Results
A total of 43 teams (see Table 15 at the end of the
paper) participated in SemEval-2016 Task 4, representing 25 countries; the country with the highest
participation was China (5 teams), followed by Italy,
Spain, and USA (4 teams each). The subtask with
the highest participation was Subtask A (34 teams),
followed by Subtask B (19 teams), Subtask D (14
teams), Subtask C (11 teams), and Subtask E (10
It was not surprising that Subtask A proved to
be the most popular – it was a rerun from previous
years; conversely, none among Subtasks B to E had
previously been offered in precisely the same form.
Quantiﬁcation-related subtasks (D and E) generated
24 participations altogether, while subtasks with an
ordinal nature (C and E) attracted 21 participations.
Only three teams participated in all ﬁve subtasks;
conversely, no less than 23 teams took part in one
subtask only (with a few exceptions, Subtask A).
Many teams that participated in more than one subtask used essentially the same system for all of them,
with little tuning to the speciﬁcs of each subtask.
Few trends stand out among the participating systems. In terms of the supervised learning methods
used, there is a clear dominance of methods based on
deep learning, including convolutional neural networks and recurrent neural networks (and, in particular, long short-term memory networks); the software libraries for deep learning most frequently used
by the participants are Theano and Keras.
Conversely, kernel machines seem to be less frequently
used than in the past, and the use of learning methods other than the ones mentioned above is scarce.
The use of distant supervision is ubiquitous; this
is natural, since there is an abundance of freely available tweets labelled according to sentiment (possibly with silver labels only, e.g., emoticons), and it
is intuitive that their use as additional training data
could be helpful. Another ubiquitous technique is
the use of word embeddings, usually generated via
either word2vec or GloVe
 ; most authors seem to
use general-purpose, pre-trained embeddings, while
some authors also use customized word embeddings, trained either on the Tweet 2016 dataset or
on tweet datasets of some sort.
Nothing radically new seems to have emerged
with respect to text preprocessing; as in previous
editions of this task, participants use a mix of by
now obvious techniques, such as negation scope detection, elongation normalization, detection of ampliﬁers and diminishers, plus the usual extraction
of word n-grams, character n-grams, and POS ngrams.
The use of sentiment lexicons (alone or
in combination with each other; general-purpose or
Twitter-speciﬁc) is obviously still frequent.
In the next ﬁve subsections, we discuss the results of the participating systems in the ﬁve subtasks, focusing on the techniques and tools that the
top-ranked participants have used. We also focus on
how the participants tailored (if at all) their approach
to the speciﬁc subtask. When discussing a speciﬁc
subtask, we will adopt the convention of adding to
a team name a subscript which indicates the position in the ranking for that subtask that the team obtained; e.g., when discussing Subtask E, “Finki2” indicates team “Finki, which placed 2nd in the ranking
for Subtask E”. The papers describing the participants’ approach are quoted in Table 15.
Subtask A: Message polarity classiﬁcation
Table 8 ranks the systems submitted by the 34 teams
who participated in Subtask A “Message Polarity
Classiﬁcation” in terms of the ofﬁcial measure F PN
We further show the result for two other measures,
ρPN (the measure that we adopted for Subtask B)
and accuracy (Acc =
TP+TN+FP+FN ). We also
report the result for a baseline classiﬁer that assigns
to each tweet the POSITIVE class. For Subtask A
evaluated using F PN
, this is the equivalent of the
majority class classiﬁer for (binary or SLMC) classiﬁcation evaluated via vanilla accuracy, i.e., this is
the “smartest” among the trivial policies that attempt
to maximize F PN
SwissCheese
SENSEI-LIF
aueb.twitter.sentiment 0.6055
0.61611 0.6355
0.59816 0.52824
10 ECNU (*)
0.58510 0.61710 0.57116
11 NTNUSentEval
0.58311 0.6198
0.58012 0.59218 0.54520
0.58012 0.6198
0.57614 0.60515 0.59611
0.57614 0.60713 0.58414
0.57516 0.61512 0.58513
0.57417 0.57919 0.53723
0.57118 0.60713 0.6393
19 DIEGOLab16 (*)
0.55419 0.59317 0.54919
0.53920 0.55721 0.51826
0.50521 0.56020 0.54122
22 DSIC-ELIRF
0.50222 0.51125 0.51327
0.49923 0.53722 0.57215
0.49923 0.51624 0.54321
25 ISTI-CNR
0.49425 0.52923 0.56717
0.47826 0.49627 0.45231
27 Tweester
0.45527 0.50326 0.52325
28 Minions
0.41528 0.48528 0.55618
29 Aicyber
0.40229 0.45729 0.50628
0.40130 0.43830 0.48029
31 VCU-TSA
0.37231 0.39032 0.38232
32 SentimentalITists
0.33932 0.42431 0.48029
0.33033 0.33334 0.29834
34 CICBUAPnlp
0.30334 0.37733 0.37433
Table 8: Results for Subtask A “Message Polarity Classiﬁcation” on the Tweet 2016 dataset. The systems are ordered by
their F P N
score. In each column the rankings according to the
corresponding measure are indicated with a subscript. Teams
marked as “(*)” are late submitters, i.e., their original submission was deemed irregular by the organizers, and a revised submission was entered after the deadline.
All 34 participating systems were able to outperform the baseline on all three measures, with the exception of one system that scored below the baseline on Acc. The top-scoring team (SwissCheese1)
used an ensemble of convolutional neural networks,
differing in their choice of ﬁlter shapes, pooling
shapes and usage of hidden layers. Word embeddings generated via word2vec were also used, and
the neural networks were trained by using distant
supervision.
Out of the 10 top-ranked teams, 5
teams (SwissCheese1, SENSEI-LIF2, UNIMELB3,
INESC-ID4, INSIGHT-18) used deep NNs of some
sort, and 7 teams (SwissCheese1, SENSEI-LIF2,
UNIMELB3, INESC-ID4, aueb.twitter.sentiment5,
INSIGHT-18)
generalpurpose or task-speciﬁc word embeddings, generated via word2vec or GloVe.
Historical results.
We also tested the participating systems on the test sets from the three previous editions of this subtask. Participants were not
allowed to use these test sets for training.
Results (measured on F PN
) are reported in Table 9.
The top-performing systems on Tweet 2016 are also
top-ranked on the test datasets from previous years.
There is a general pattern: the top-ranked system in
year x outperforms the top-ranked system in year
(x −1) on the ofﬁcial dataset of year (x −1). Topranked systems tend to use approaches that are universally strong, even when tested on out-of-domain
test sets such as SMS, LiveJournal, or sarcastic
tweets .
It is unclear where improvements
come from:
(a) the additional training data that
we made available this year , thus effectively doubling the amount of training data, or
(b) because of advancement of learning methods.
We further look at the top scores achieved by any
system in the period 2013–2016.
The results are
shown in Table 10. Interestingly, the results for a
test set improve in the second year it is used (i.e., the
year after it was used as an ofﬁcial test set) by 1–3
points absolute, but then do not improve further and
stay stable, or can even decrease a bit. This might be
due to participants optimizing their systems primarily on the test set from the preceding year.
sarcasm Journal
SwissCheese
SENSEI-LIF
aueb.twitter.sentiment 0.6667
0.60216 0.58212 0.64415 0.39123
0.59516 0.5938
0.61015 0.54016 0.64513 0.45010
10 ECNU (*)
0.60611 0.58510
11 NTNUSentEval
0.62311 0.6411
0.65110 0.42713
0.59913 0.58311
0.58919 0.50920 0.58720 0.38624
0.59317 0.58012
0.64210 0.5968
0.59814 0.58012
0.61612 0.57514 0.64811 0.39920
0.61710 0.57614
0.56521 0.51119 0.61419 0.36027
0.59715 0.57614
0.57913 0.64712 0.40718
0.60312 0.57516
0.59817 0.46523 0.64513 0.40519
0.59218 0.58511 0.62717 0.38125
0.58618 0.57118
19 DIEGOLab16 (*)
0.61114 0.50621 0.61818 0.4975
0.58419 0.55419
0.61213 0.52417 0.63916 0.4687
0.58419 0.53920
0.56720 0.56215 0.55623 0.39521
0.53121 0.50521
22 DSIC-ELIRF
0.49425 0.40426 0.54626 0.34229
0.53121 0.50222
0.49026 0.44324 0.54725 0.37226
0.50225 0.49923
0.46228 0.40825 0.51428 0.31033
0.49326 0.49923
25 ISTI-CNR
0.53822 0.49222 0.57221 0.32730
0.50824 0.49425
0.51823 0.31529 0.57122 0.32032
0.51723 0.47826
27 Tweester
0.50624 0.34028 0.52927 0.5403
0.47928 0.45527
28 Minions
0.48927 0.52118 0.55424 0.42016
0.48127 0.41528
29 Aicyber
0.41829 0.36127 0.45729 0.32631
0.43229 0.40229
0.39430 0.31030 0.41531 0.35228
0.41331 0.40130
31 VCU-TSA
0.38331 0.30731 0.44430 0.42514
0.41630 0.37231
32 SentimentalITists
0.33933 0.23833 0.39332 0.28834
0.34333 0.33932
0.35532 0.28432 0.39332 0.43012
0.37732 0.33033
34 CICBUAPnlp
0.19334 0.19334 0.33534 0.39322
0.30334 0.30334
Table 9: Historical results for Subtask A “Message Polarity Classiﬁcation”. The systems are ordered by their score on the Tweet
2016 dataset; the rankings on the individual datasets are indicated with a subscript. The meaning of “(*)” is as in Table 8.
Subtask B: Tweet classiﬁcation according
to a two-point scale
Table 11 ranks the 19 teams who participated in
Subtask B “Tweet classiﬁcation according to a twopoint scale” in terms of the ofﬁcial measure ρPN.
Two other measures are reported, F PN
(the measure adopted for Subtask A) and accuracy (Acc). We
also report the result of a baseline that assigns to
each tweet the positive class. This is the “smartest”
among the trivial policies that attempt to maximize
ρPN. This baseline always returns ρPN = 0.500.
Note however that this is also (i) the value returned by the classiﬁer that assigns to each tweet the
negative class, and (ii) the expected value returned
by the random classiﬁer; for more details see , where ρPN is called K.
The top-scoring team (Tweester1) used a combination of convolutional neural networks, topic
modeling, and word embeddings generated via
Similar to Subtask A, the main trend
among all participants is the widespread use of deep
learning techniques.
Tweet SMS Tweet
Tweet Tweet
sarcasm Journal
Best in 2016 0.723 0.641 0.744
Best in 2015 0.728 0.685 0.744
Best in 2014 0.721 0.703 0.710
Best in 2013 0.690 0.685
Table 10: Historical results for the best systems for Subtask A “Message Polarity Classiﬁcation” over the years 2013–2016.
0.72010 0.76217
thecerealkiller
0.72010 0.7487
11 pkudblab
0.68911 0.71611 0.8327
0.67912 0.70812 0.8346
13 ISTI-CNR
0.67113 0.69013 0.81111
14 SwissCheese
0.64814 0.67414 0.82010
15 SentimentalITists 0.62415 0.64315 0.80213
0.61816 0.61017 0.71218
0.61617 0.63316 0.79215
0.52218 0.50218 0.57719
19 VCU-TSA
0.50219 0.44819 0.77516
Table 11: Results for Subtask B “Tweet classiﬁcation according
to a two-point scale” on the Tweet 2016 dataset. The systems
are ordered by their ρP N score (higher is better). The meaning
of “(*)” is as in Table 8.
top-ranked
participating
5 teams (Tweester1,
15, UNIMELB7, Finki10) used convolutional neural
networks; 3 teams (thecerealkiller3, UNIMELB7,
Finki10) submitted systems using recurrent neural networks;
and 7 teams (Tweester1, LYS2,
INSIGHT-15, UNIMELB7, Finki10) incorporated in
their participating systems either general-purpose
or task-speciﬁc word embeddings (generated via
toolkits such as GloVe or word2vec).
Conversely, the use of classiﬁers such as support
vector machines, which were dominant until a few
years ago, seems to have decreased, with only one
team (TwiSE8) in the top 10 using them.
Subtask C: Tweet classiﬁcation according
to a ﬁve-point scale
Table 12 ranks the 11 teams who participated in Subtask C “Tweet classiﬁcation according to a ﬁve-point
scale” in terms of the ofﬁcial measure MAEM; we
also show MAEµ (see Equation 6). We also report
the result of a baseline system that assigns to each
tweet the middle class (i.e., NEUTRAL); for ordinal classiﬁcation evaluated via MAEM, this is the
majority-class classiﬁer for (binary or SLMC) classiﬁcation evaluated via vanilla accuracy, i.e., this is
 the “smartest” among the
trivial policies that attempt to maximize MAEM.
SentimentalITists 1.1489
11 pkudblab
Table 12: Results for Subtask C “Tweet classiﬁcation according to a ﬁve-point scale” on the Tweet 2016 dataset. The systems are ordered by their MAEM score (lower is better). The
meaning of “(*)” is as in Table 8.
The top-scoring team (TwiSE1) used a singlelabel multi-class classiﬁer to classify the tweets according to their overall polarity. In particular, they
used logistic regression that minimizes the multinomial loss across the classes, with weights to cope
with class imbalance.
Note that they ignored the
given topics altogether.
0.15012 1.83812
10 ECNU (*)
0.12110 0.14811 1.1719
11 ISTI-CNR
0.12711 0.1479
12 SwissCheese 0.19112 0.1479
0.26113 0.27413 2.97313
0.39914 0.33614 3.93014
Table 13: Results for Subtask D “Tweet quantiﬁcation according to a two-point scale” on the Tweet 2016 dataset. The systems are ordered by their KLD score (lower is better). The
meaning of “(*)” is as in Table 8.
Only 2 of the 11 participating teams tuned their
systems to exploit the ordinal (as opposed to binary,
or single-label multi-class) nature of this subtask.
The two teams who did exploit the ordinal nature
of the problem are PUT3, which uses an ensemble
of ordinal regression approaches, and ISTI-CNR7,
which uses a tree-based approach to ordinal regression.
All other teams used general-purpose approaches for single-label multi-class classiﬁcation,
in many cases relying (as for Subtask B) on convolutional neural networks, recurrent neural networks,
and word embeddings.
Subtask D: Tweet quantiﬁcation according
to a two-point scale
Table 13 ranks the 14 teams who participated in Subtask D “Tweet quantiﬁcation according to a twopoint scale” on the ofﬁcial measure KLD.
other measures are reported, absolute error (AE):
AE(p, ˆp, C) = 1
|ˆp(c) −p(c)|
and relative absolute error (RAE):
RAE(p, ˆp, C) = 1
|ˆp(c) −p(c)|
where the notation is the same as in Equation 7.
We also report the result of a “maximum likelihood” baseline system (dubbed Baseline1). This
system assigns to each test topic the distribution of
the training tweets (the union of TRAIN, DEV, DE-
VTEST) across the classes. This is the “smartest”
among the trivial policies that attempt to maximize
KLD. We also report the result of a further (less
smart) baseline system (dubbed Baseline2), i.e., one
that assigns a prevalence of 1 to the majority class
(which happens to be the POSITIVE class) and a
prevalence of 0 to the other class.
The top-scoring team (Finki1) adopts an approach
based on “classify and count”, a classiﬁcationoriented (instead of quantiﬁcation-oriented) approach, using recurrent and convolutional neural
networks, and GloVe word embeddings.
Indeed, only 5 of the 14 participating teams tuned
their systems to the fact that it deals with quantiﬁcation (as opposed to classiﬁcation).
teams who do rely on quantiﬁcation-oriented approaches, teams LYS2 and HSENN14 used an existing structured prediction method that directly optimizes KLD; teams QCRI5 and ISTI-CNR11 use
existing probabilistic quantiﬁcation methods; team
NRU-HSE7 uses an existing iterative quantiﬁcation
method based on cost-sensitive learning. Interestingly, team TwiSE2 uses a “classify and count”
approach after comparing it with a quantiﬁcationoriented method (similar to the one used by teams
LYS2 and HSENN14) on the development set, and
concluding that the former works better than the latter. All other teams used “classify and count” approaches, mostly based on convolutional neural networks and word embeddings.
Subtask E: Tweet quantiﬁcation according
to a ﬁve-point scale
Table 14 lists the results obtained by the 10 participating teams on Subtask E “Tweet quantiﬁcation according to a ﬁve-point scale”. We also report the
result of a “maximum likelihood” baseline system
(dubbed Baseline1), i.e., one that assigns to each test
topic the same distribution, namely the distribution
of the training tweets (the union of TRAIN, DEV,
DEVTEST) across the classes; this is the “smartest”
among the trivial policies (i.e., those that do not require any genuine work) that attempt to maximize
We further report the result of less smart baseline system (dubbed Baseline2) – one that assigns
a prevalence of 1 to the majority class (which coincides with the POSITIVE class) and a prevalence of
0 to all other classes.
INSIGHT-1 0.3668
Table 14: Results for Subtask E “Tweet quantiﬁcation according to a ﬁve-point scale” on the Tweet 2016 dataset. The systems are ordered by their EMD score (lower is better). The
meaning of “(*)” is as in Table 8.
Only 3 of the 10 participants tuned their systems
to the speciﬁc characteristics of this subtask, i.e., to
the fact that it deals with quantiﬁcation (as opposed
to classiﬁcation) and to the fact that it has an ordinal
(as opposed to binary) nature.
In particular, the top-scoring team (QCRI1) used
a novel algorithm explicitly designed for ordinal
quantiﬁcation, that leverages an ordinal hierarchy of
binary probabilistic quantiﬁers.
Team NRU-HSE4 uses an existing quantiﬁcation approach based on cost-sensitive learning, and
adapted it to the ordinal case.
Team ISTI-CNR6 instead used a novel adaptation
to quantiﬁcation of a tree-based approach to ordinal
regression.
Teams LYS7 and HSENN9 also used an existing
quantiﬁcation approach, but did not exploit the ordinal nature of the problem.
The other teams mostly used approaches based on
“classify and count” (see Section 5.4), and viewed
the problem as single-label multi-class (instead of
ordinal) classiﬁcation; some of these teams (notably,
team Finki2) obtained very good results, which testiﬁes to the quality of the (general-purpose) features
and learning algorithm they used.
Conclusion and Future Work
We described SemEval-2016 Task 4 “Sentiment
Analysis in Twitter”, which included ﬁve subtasks
including three that represent a signiﬁcant departure
from previous editions. The three new subtasks focused, individually or in combination, on two variants of the basic “sentiment classiﬁcation in Twitter”
task that had not been previously explored within
SemEval. The ﬁrst variant adopts a ﬁve-point scale,
which confers an ordinal character to the classiﬁcation task. The second variant focuses on the correct
estimation of the prevalence of each class of interest,
a task which has been called quantiﬁcation in the
supervised learning literature. In contrast, previous
years’ subtasks have focused on the correct labeling
of individual tweets. As in previous years , the 2016 task was very popular and attracted
a total of 43 teams.
A general trend that emerges from SemEval-2016
Task 4 is that most teams who were ranked at the top
in the various subtasks used deep learning, including convolutional NNs, recurrent NNs, and (generalpurpose or task-speciﬁc) word embeddings. In many
cases, the use of these techniques allowed the teams
using them to obtain good scores even without tuning their system to the speciﬁcs of the subtask at
hand, e.g., even without exploiting the ordinal nature of the subtask – for Subtasks C and E – or
the quantiﬁcation-related nature of the subtask – for
Subtasks D and E. Conversely, several teams that
have indeed tuned their system to the speciﬁcs of
the subtask at hand, but have not used deep learning
techniques, have performed less satisfactorily. This
is a further conﬁrmation of the power of deep learning techniques for tweet sentiment analysis.
Concerning Subtasks D and E, if quantiﬁcationbased subtasks are proposed again, we think it might
be a good idea to generate, for each test topic ti,
multiple “artiﬁcial” test topics t1
i , ..., where class
prevalences are altered with respect to the ones of ti
by means of selectively removing from ti tweets belonging to a certain class. In this way, the evaluation
can take into consideration (i) class prevalences in
the test set and (ii) levels of distribution drift (i.e., of
the divergence of the test distribution from the training distribution) that are not present in the “naturally
occurring” data.
By varying the amount of removed tweets at will,
one may obtain many test topics, thus augmenting
the magnitude of the experimentation at will while at
the same time keeping constant the amount of manual annotation needed.
In terms of possible follow-ups of this task, it
might be interesting to have a subtask whose goal
is to distinguish tweets that are NEUTRAL about
the topic (i.e., do not express any opinion about
the topic) from tweets that express a FAIR opinion (i.e., lukewarm, intermediate between POSITIVE
and NEGATIVE) about the topic.
Another possibility is to have a multi-lingual
tweet sentiment classiﬁcation subtask, where training examples are provided for the same topic for two
languages (e.g., English and Arabic), and where participants can improve their performance on one language by leveraging the training examples for the
other language via transfer learning. Alternatively, it
might be interesting to include a cross-lingual tweet
sentiment classiﬁcation subtask, where training examples are provided for a given language (e.g., English) but not for the other (e.g., Arabic); the second
language could be also a surprise language, which
could be announced at the last moment.