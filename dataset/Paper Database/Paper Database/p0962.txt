Moment Tensor Potentials: a class of systematically
improvable interatomic potentials
Alexander V. Shapeev∗
July 26, 2016
Density functional theory oﬀers a very accurate way of computing materials properties
from ﬁrst principles. However, it is too expensive for modelling large-scale molecular systems
whose properties are, in contrast, computed using interatomic potentials.
The present paper considers, from a mathematical point of view, the problem of constructing interatomic potentials that approximate a given quantum-mechanical interaction model.
In particular, a new class of systematically improvable potentials is proposed, analyzed, and
tested on an existing quantum-mechanical database.
Introduction
Molecular modelling is an increasingly popular tool in biology, chemistry, physics, and materials
science . The success of molecular modelling largely depends on the accuracy and eﬃciency of
calculating the interatomic forces. The two major approaches to computing interatomic forces are
(1) quantum mechanics (QM) calculations , and (2) using empirical interatomic potentials.
In the ﬁrst approach, ﬁrst the electronic structure is computed and then the forces on the atoms
(more precisely, on their nuclei) are deduced. QM calculations are therefore rather computationally
demanding, but can yield a high quantitative accuracy. On the other hand, the accuracy and
transferability of empirical interatomic potentials is limited, but they are orders of magnitude less
computationally demanding. With interatomic potentials, typically, the forces on atoms derive
from the energy of interaction of each atom with its atomic neighborhood (typically consisting of
tens to hundreds of atoms).
This hence makes it very attractive to design a combined approach whose eﬃciency would be
comparable to interatomic potentials, yet the accuracy is similar to the one obtained with ab initio
simulations. Moreover, the scaling of computational complexity of using the interatomic potentials
is O(N), where N is the number of atoms, whereas, for instance, standard Kohn-Sham density
functional theory calculations scale like O(N 3)—which implies that the need in such combined
approaches is even higher for large numbers of atoms.
One way of achieving this is designing accurate interatomic potentials.
With this goal in
mind, we categorize all potentials into two groups, following the regression analysis terminology
of . The ﬁrst group is the parametric potentials with a ﬁxed number of numerical or functional parameters. All empirical potentials known to the author are parametric, which is their
∗Skolkovo Institute of Science and Technology, Skolkovo Innovation Center, Building 3, Moscow 143026 Russia.
Email: .
 
disadvantage—their accuracy cannot be systematically improved. As a result, if the class of problems is suﬃciently large and the required accuracy is suﬃciently high (say, close to that of ab
initio calculations), then the standard parametric potentials are not suﬃcient and one needs to
employ nonparametric potentials. In theory, the accuracy of these potentials can be systematically
improved. In practice, however, increasing the accuracy is done at a cost of higher computational
complexity (which is nevertheless orders of magnitude less than that of the QM calculations), and
the accuracy is still limited by that of the available QM model the potential is ﬁtted to. Also, it
should be noted that, typically, such ﬁtted potentials do not perform well for the situations that
they were not ﬁtted to (i.e., they exhibit little or no transferability).
Each nonparametric potential has two major components: (1) a representation (also referred
to as “descriptors”) of atomic environments, and (2) a regression model which is a function of
the representation. It should be emphasized that ﬁnding a good representation of atomic environments is not a trivial task, as the representation is required to satisfy certain restrictions, namely,
invariance with respect to Euclidean transformations (translations, rotations, and reﬂections) and
permutation of chemically equivalent atoms, smoothness with respect to distant atoms coming to
and leaving the boundary of the atomic environment, as well as completeness (for more details see
the discussion in ). The latter restriction means that the representation should contain
all the information to unambiguously reconstruct the atomic environment.
In this context, one common approach to constructing nonparametric potentials, called the
neural networks potentials (NNP) , is using artiﬁcial neural networks as the regression model
and a family of descriptors ﬁrst proposed by Behler and Parrinello . These descriptors have a
form of radial functions applied to and summed over atomic distances to the central atom of the
environment, augmented with similar functions with angular dependence summed over all pairs of
atoms. Another approach called Gaussian Approximation Potentials (GAP) uses Gaussian
process regression and a cleverly designed set of descriptors that are evaluated by expanding
smoothened density ﬁeld of atoms into a spherical harmonics basis .
Other recent approaches include which employs the bispectrum components of the atomic
density, as proposed in an earlier version of GAP , and uses a linear regression model to ﬁt
the QM data. Also, uses Gaussian process regression of a force-based model rather than the
potential energy-based model. Additionally, in a recent work the authors put forward an
approach of regressing the interatomic interaction energy together with the electron density.
It is worthwhile to mention a related ﬁeld of research, namely the development of non-reactive
interatomic potentials. Such potentials assume a ﬁxed underlying atomistic structure (ﬁxed interatomic bonds or ﬁxed lattice sites that atoms are bound to). The works developing nonparametric
versions of such potentials include .
In the present paper we propose a new approach to constructing nonparametric potentials
based on linear regression and invariant polynomials. The main feature of this approach is that
the proposed form of the potential can provably approximate any regular function satisfying all the
needed symmetries (see Theorems 3.1 and 3.2), while being computationally eﬃcient. The building
block of the proposed potentials is what we call the moment tensors—similar to inertia tensors of
atomic environments. Hence we call this approach the Moment Tensor Potentials (MTP).
The manuscript is structured as follows. Section 2 gives an overview of interatomic potentials. Section 3 introduces MTP and then formulates and proves the two main theorems. The
practical implementation of MTP is discussed in Section 4. Section 5 reports the accuracy and
computational eﬃciency (i.e., the CPU time) of MTP and compares MTP with GAP.
Interatomic Potentials
Consider a system consisting of Ntot atoms with positions x ∈(Rd)Ntot (more precisely, with
positions of their nuclei to be x), where d is the number of physical dimensions, taken as d = 3
in most of applications. These atoms have a certain energy of interaction, Eq(x), typically given
by some QM model that we aim to approximate. For simplicity, we assume that all atoms are
chemically equivalent.
We make the assumption that Eq(x) can be well approximated by a sum of energies of atomic
environments of the individual atoms. We denote the atomic environment of atom k by Dxk and
let it equal to a tuple
Dxk := (xi −xk)1≤i≤Ntot, 0<|xi−xk|≤Rcut,
where Rcut is the cut-oﬀradius, in practical calculations typically taken to be between 5 and 10˚A.
In other words, Dxk is the collection of vectors joining atom k with all other atoms located at
distance Rcut or closer. Thus, we are looking for an approximant of the form
where V is called the interatomic potential. This assumption is true in most systems with shortrange interactions (as opposed to, e.g., Coulomb interaction in charged or polarized systems), refer
to recent works for rigorous proofs of this statement for simple QM models.
Mathematically, since Dxk can be a tuple of any size (in practice limited by the maximal
density of atoms), V can be understood as a family of functions each having a diﬀerent number
of arguments. For convenience, however, we will still refer to V as a “function”.
The function V = V (u1, . . . , un) is required to satisfy the following restrictions:
(R1) Permutation invariance:
V (u1, . . . , un) = V (uσ1, . . . , uσn)
for any σ ∈Sn,
where Sn denotes the set of permutations of (1, . . . , n).
(R2) Rotation and reﬂection invariance:
V (u1, . . . , un) = V (Qu1, . . . , Qun)
for any Q ∈O(d),
where O(d) is the orthogonal group in Rd. For simplicity in what follows we will denote
Qu := (Qu1, . . . , Qun).
(R3) Smoothness with respect to the number of atoms (more precisely, with respect to atoms
leaving and entering the interaction neighborhood):
V : Rd×n →R is a smooth function
V (u1, . . . , un) = V (u1, . . . , un, un+1)
whenever |un+1| ≥Rcut.
In most of the works performing practical calculations, including , the
interatomic potentials are chosen such that the energy and forces are continuous. There
are, however, exceptions including that require the second derivatives of energy to be
continuous, and proposing exponential decay of the potential with no strict cut-oﬀ
Note that E is translation symmetric by deﬁnition.
Empirical Interatomic Potentials
For the purpose of illustration, we will brieﬂy present two popular classes of empirical interatomic
potentials. The ﬁrst class is pair potentials (also known as two-body potentials),
V pair(u) =
where by u we denote the collection of the relative coordinates, u = (ui)n
i=1. It is clear that this
potential satisﬁes (R1) and (R2), while if we take ϕ(r) = 0 for r ≥Rcut (as is done in most of
practical calculations) then it also satisﬁes (R3). The second potential is the Embedded Atom
Model (EAM),
V eam(u) =
ϕ(|ui|) + F
where if ϕ(r) and ρ(r) are chosen to vanish for r ≥Rcut then it also satisﬁes (R1)–(R3).
Both potentials can be viewed to have descriptors of the form
where fν are chosen such that their linear combination can approximate any smooth function
(e.g., fν(r) = rν(Rcut −r)2 for r ≤Rcut and fν(r) = 0 for r > Rcut, where ν = 0, 1, . . .; the term
(Rcut −r)2 ensures continuous energy and forces). Then, we can approximate ϕ(r) ≈P
(the sum is taken over some ﬁnite set of ν), and hence
V pair(u) ≈
Likewise one can approximate V eam with the exception that it will not be, in general, a linear
function of Rν(u).
This set of descriptors is not complete, because it is based only on the distances to the central
atom, but is insensitive to bond angles.
Non-empirical Interatomic Potentials
Next, we review a number of non-empirical interatomic potentials proposed recently as a more
accurate alternative to the empirical ones.
We start with the NNPs . In addition to the descriptors given by (2.3) they also employ
the descriptors of the form
f(|ui|, |uj|, ui · uj).
(Note that NN can be, in principle, used with any set of descriptors. If these descriptors are
complete then such NNP is systematically improvable.) Typically, 50 to 100 descriptors of the
form (2.3) and (2.4) are chosen as an input to a NN, while its output yields the function V .
In practice, this approach gives convincing results, however, it is an open question whether or not
these descriptors are complete.
The GAP uses a diﬀerent idea, consisting of: (1) forming a smoothened atomic density
(2) approximating it through spherical harmonics, and (3) constructing functionals applied to the
spherical harmonics coeﬃcients that satisfy all needed symmetries. One can prove mathematically
that this approach can approximate any regular symmetric function (i.e., a function that satisﬁes
(R1)–(R3)). However, expanding functions in a spherical harmonics basis can be computationally
expensive.
Several other non-empirical potentials have been proposed recently. A method using a representation based on expanding the atomic density function in spherical harmonics together with
linear regression was used in . In the authors use Gaussian process regression to train a
model that predicts forces on atoms directly (as opposed to a model that ﬁts the energy).
Moment Tensor Potentials
Representation with Invariant Polynomials
In the present paper we propose an alternative approach based on invariant polynomials. The idea
is that any given potential V ∗(u), if it is smooth enough, can be approximated by a polynomial
p(u) ≈V ∗(u)—we will analyze the error of such an approximation in Section 3.2. The approximant
can always be chosen symmetric. Indeed, given p(u), one can consider the symmetrized polynomial
psym(u1, . . . , un) = 1
p(uσ1, . . . , uσn),
and then V ∗(u) ≈psym(u). Similarly, one can symmetrize psym with respect to rotations and
reﬂections. Hence, theoretically, one can construct a basis of such polynomials bν(u) and choose
V ∗(u) ≈V (u) :=
This approach is implemented for small systems of up to ten atoms , however, generalizations
of this approach require a more eﬃcient way of generating the invariant polynomials. The main
diﬃculty is related to the fact that the number of permutations in a system of n atoms is n! which
grows too fast in order to, for instance, calculate the right hand side of (3.1).
In the present paper we propose a basis for the set of all polynomials invariant with respect
to permutation, rotation, and reﬂection.
The main feature of the proposed basis is that the
computational complexity of computing these polynomials scales like O(n). Moreover, one can
easily construct such bases to also satisfy the (R3) property (refer to Section 4), making it a
promising candidate for eﬃcient nonparametric interatomic potentials.
The M polynomials
The building blocks of the basis functions for the approximation (representation) of V = V (u) are
the “moment” polynomials M = M•,•(u) deﬁned as follows.
For integer µ, ν ≥0 we let
Mµ,ν(u) :=
where w⊗ν := w ⊗. . . ⊗w is the Kronecker product of ν copies of the vector w ∈Rd. Thus,
Mµ,ν(u) ∈(Rd)ν (i.e., an ν-dimensional tensor) for each u. Computing Mµ,ν(u) requires linear
time in n, but exponential in ν. This means that if the maximal value of ν is bounded then
computing Mµ,ν can be done eﬃciently.
There is a mechanical interpretation of Mµ,ν(u). Consider µ = 0, then M0,0 simply gives the
number of atoms at the distance of Rcut or less (this can also be understood as the “mass” of these
atoms), M0,1 is the center of mass of such atoms (scaled by the mass), M0,2 is the tensor of second
moments of inertia, etc. For µ > 0, Mµ,ν can be interpreted as weighted moments of inertia, with
i-th atom’s weight being |ui|2µ.
The basis polynomials Bα
The basis polynomials are indexed by k ∈N, k ≤n, where our deﬁnition for N is
N = {0, 1, . . .},
and a k × k symmetric matrix α of integers αi,j ≥0 (i, j ∈{1, . . . , k}). For such matrices, by α′
we deﬁne the sum of the oﬀ-diagonal elements of i-th row,
Next, we deﬁne a contraction operator (product) of tensors T (i) ∈(Rd)α′
β(1,i)...β(i−1,i)β(i,i+1)...β(i,k),
where each β is a collection of multiindices β =
1≤i<j≤k, and each multiindex β(i,j) has the
following form,
, . . . , β(i,j)
∈{1, . . . , d}αi,j
1 ≤i < j ≤k.
To deﬁne this contraction rigorously, we let
M := {(i, j) ∈{1, . . . , m}2 : i < j},
B := ({1, . . . , d}αij)(i,j)∈M
and, expanding the multiindex notation, we write
...β(i−1,i)
...β(i,i+1)
Hence αij can be interpreted as how many dimensions are contracted between T (i) and T (j).
Finally, we let
and call it a basis function (for a given α). Here Bα(u) is, essentially, a k-body function. We note
that Bα(u) ≡Bβ(u) if there exists a permutation σ ∈Sk such that αij = βσiσj for all i and j.
For illustrative purposes we work out three examples of diﬀerent Bα. First, let us take α =
. This implies k = 2 (since this is a 2 × 2 matrix) and α′
2 = 1. Hence Bα is the
scalar product of Mµ1,1 and Mµ2,1, indeed:
= Mµ1,1 · Mµ2,1,
denotes the β(1,2)
-th component of the vector Mµ1,1.
As the next example, we take α =
2 = 2, and hence Bα is the
Frobenius product of the two matrices, Mµ1,2 and Mµ2,2:
= Mµ1,1 :Mµ2,1.
In the last example, we take α =
. Then Bα is the following contraction of the
matrix Mµ1,2, and two vectors, Mµ2,1 and Mµ2,1:
 Mµ1,2Mµ2,1
Representability
We are now ready to formulate the main result, namely that the linear combinations of Bα span
all permutation and rotation invariant polynomials. Let us denote the set of all polynomials of n
vector-valued variables by P, the set of permutation invariant polynomials by Pperm ⊂P and the
set of rotation invariant polynomials by Prot ⊂P.
Theorem 3.1. The polynomials Bα form a spanning set of the linear space Prot ∩Pperm ⊂P, in
the sense that any p ∈Prot ∩Pperm can be represented by a (ﬁnite) linear combination of Bα (but
this combination is, in general, not unique).
We postpone the proof of this result to Section 3.3, after we illustrate in Section 3.2 that a
QM model can be eﬃciently approximated with polynomials.
Approximation Error Estimate
In this section we present an error analysis of ﬁtting the prototypical tight-binding QM model as
proposed in with the polynomials Bα(u). We note that we directly take the site energy
V q(u) constructed in (rather than starting from the total energy Eq(x)) and ﬁx n to be
constant throughout this analysis. The latter assumption needs some comment. Letting n to be
constant is essentially equivalent to assuming ﬁnite Rcut, provided that the atoms cannot come
too close to each other. We note that making such assumption is not an insigniﬁcant limitation
of the present analysis, however, we ﬁnd lifting this limitation not at all trivial.
The QM model is deﬁned as follows. We let, for convenience, u0 := 0 and deﬁne the Hamiltonian matrix
i, j ∈{0, . . . , n} and i ̸= j,
i = j ∈{0, . . . , n},
where ϕ : Rd →R is an empirically chosen function referred to as the hopping integral .
Then we apply the function f q to the matrix H (refer to for details on matrix functions)
and deﬁne V q as the (0, 0)-th element of this matrix:
V q(u) := (f q(H))0,0,
f q(ϵ) := ϵ
1 + eϵ/(kBT)−1
where kB > 0 is the Boltzmann constant, and T > 0 is the electronic temperature, and we take
the chemical potential to be zero.
Next, we let R > 0 and δ0 > 0, allow ui to vary in Vδ0, where
Vδ := {ζ ∈C : Re(ζ) ∈[−R −δ, R + δ]d, Im(ζ) ∈[−δ, δ]d},
and assume that ϕ(v) is analytically extended to Vδ0. We note that in many models ϕ(ζ) has
a singularity at ζ = 0 (for example, ϕ(ζ) = β0 exp(−q|ζ|) for some β0, q > 0 [14, Equation
(7.24)]), therefore by assuming analytical extensibility of ϕ onto Vδ0 we implicitly assume some
approximation of such irregular ϕ with a function that is regular around ζ = 0. For example, one
could use the Hermite functions basis, e−|ζ|2/2Hn1(ζ1)Hn2(ζ2) . . . Hnd(ζd) , to approximate ϕ(ζ)
away from ζ = 0.
Theorem 3.2. There exist C > 0 and ρ > 1, both depending only on n, δ0, Mδ0, and kBT, such
that for any m ∈N there exists pm ∈Pperm ∩Prot of degree m such that
u: maxi |ui|≤R
|V q(u) −pm(u)| < Cρ−m.
The proof of this theorem is based on the following result by Hackbusch and Khoromskij :
Proposition 3.3. Let Eρ := {ζ ∈C : |z −1| + |z + 1| ≤ρ + ρ−1}, for some ρ > 1,
:= [−1, 1]j−1 × Eρ × [−1, 1]n−j−1,
f = f(z1, . . . , zn) deﬁned on the union of all E(j)
Mρ(f) := max
Let p = p(z1, . . . , zn) be the polynomial interpolant of f of degree m on the Chebyshev-Gauss-
Lobatto nodes in [−1, 1]n. Then
maxi |zi| ≤1
|f(z) −p(z)| ≤2(1 + 2π−1 log(n))mMρ(f) ρ−m
An illustration of Eρ and Vδ is given in Figure 1.
Proof of Theorem 3.2. The plan of the proof is to, after an auxiliary step (Step 1), successively
obtain bounds on ϕ (Step 2), on H(u) (Step 3), and on the interpolating polynomial (Step 4),
and then symmetrize this interpolating polynomial (Step 5).
Step 1. First, we deﬁne Mδ := supζ∈Vδ ϕ(ζ) and note that by the Cauchy integral formula one can
= (2R + 8δ)M2δ
∀z : |z| ≤δ.
This is valid for any δ ≤δ0/2.
Step 2. Next we let δ ∈(0, δ0/2], which will be ﬁxed later, and note that if z ∈Vδ then |Im(ϕ(z))| ≤
Im(ϕ(Re(z))) + M ′
δ0Im(z) ≤M ′
δ0δ thanks to the intermediate value theorem.
Figure 1: Illustration of Vδ (black square) and Eρ (blue ellipse) on the complex plane for R = 1,
δ = 0.4, ρ = 1.4. Both regions contain [-1,1] (red line) and for this choice of parameters Eρ ⊂Vδ.
Step 3. Next, following Proposition 3.3, deﬁne
× Vδ × Vn−i−1
Hence note that for u ∈Uδ, H(u) is symmetric (possibly non-Hermitian) with at most 2n elements
being non-real, which makes it easy to estimate using the Frobenius norm:
∥Im(H(u))∥≤
By a spectrum perturbation argument (namely, the Bauer-Fike theorem ) we have the corresponding bound on the spectrum:
|Im(Sp(H(u)))| ≤
The real part of the spectrum can be estimated directly through the norm:
|Re(Sp(H(u)))| ≤∥H(u)∥≤nMδ0.
Step 4. We next bound V q(u) for u ∈Uδ. If needed we decrease δ such that
and use the following representation :
V q(u) = −1
 (H(u) −zI)−1
where we take γ := ∂Ω, where Ω= {z ∈C : |Re(z)| ≤nMδ0 + π
3kBT, |Im(z)| ≤2π
3 kBT}. The
choice of the region Ωis such that any point z ∈γ is separated from any eigenvalue λ of H(u)
by the distance |z −λ| ≥π
3kBT, and at the same time f q(z) is regular on Ω. This allows us to
estimate, for z ∈γ,
∥(H(u) −zI)−1∥≤
|V q(u)| ≤sup
|f q(z)| |γ|
where C is a generic constant that depends only on n, δ0, Mδ0, and kBT. It now remains to notice
that there exists ρ > 1 that depends on δ such that
{z ∈C : |z/R −1| + |z/R + 1| ≤ρ + ρ−1} ⊂Vδ.
Hence Proposition 3.3 applies to the function f(z) = V q(Rz) and yields the interpolating polynomial ˜pm(u) such that
u: maxi |ui|≤R
|V q(u) −˜pm(u)| ≤
u: maxi |ui|∞≤R
|V q(u) −˜pm(u)| ≤Cρ−m.
By construction ˜pm ∈Pperm.
Indeed, the function f(z) = V q(Rz) is symmetric with respect
to permutation of variables, and so is the Chebyshev-Gauss-Lobatto interpolations nodes on the
domain [−1, 1]n. Hence, uniqueness of interpolation yields permutation symmetry of ˜pm.
Step 5. We deﬁne
Q∈O(d) ˜pm(Qu)dQ
∈Pperm ∩Prot,
(where dQ denotes the Haar measure) and thanks to the rotation invariance of V q we recover
It is worthwhile noting that the integration with respect to rotation was used only as a technical
tool in the proof. If we directly constructed an approximation V (u) = P
α cαBα(u) to V q(u), rather
than using the Chebyshev-Gauss-Lobatto interpolation as an intermediate step, then V (u) would
be rotationally invariant by construction.
Proof of Theorem 3.1
We start with stating the First Fundamental Theorem for the orthogonal group O(d) :
Theorem 3.4. p ∈P is rotation invariant if and only if it can be represented as a polynomial of
n(n + 1)/2 scalar variables of the form rij(u) := ui · uj, where 1 ≤i ≤j ≤n.
We hence can identify a polynomial p = p(u) ∈Prot with the respective polynomial q = q(r) ∈
Q, where Q is the set of polynomials of n(n + 1) scalar variables r = (rij)1≤i,j ≤n (we lift the
requirement i ≤j for the ease of notation in what follows).
In order to proceed, we introduce the notation of composition of tuples:
ab := (ab1, . . . , abm)
for b = (b1, . . . , bm) ⊂{1, . . . , n}m and a = (a1, . . . , an).
Hence deﬁne
Qperm := {q ∈Q : q(r) ≡q(rσσ) ∀σ ∈Sn}
that corresponds to Pperm, where we likewise let rσσ := (rσiσj)1≤i,j ≤n.
We next formulate a rather intuitive result, essentially stating that Qperm can be spanned by
symmetrizing all monomials of r.
Lemma 3.5.
Qperm = Span
(rσiσj)αij : m ∈N, m ≤n, α ∈Nm×m
Proof. If q ∈Qperm then
q(rσσ) = 1
q(r) = q(r).
It remains to apply this identity to all monomials q(r) = Qm
to obtain the stated
The next step towards proving the main result is the following lemma, where we denote N :=
{1, . . . , n}:
Lemma 3.6. For m ∈N, m ≤n, and α ∈Nm×m,
(uγi · uγj)αij.
Proof. Before commencing with the proof, we note that we will use the distributive law of addition
and multiplication in the following form:
β(1,i)...β(i−1,i)β(i,i+1)...β(i,m)(u) =
We recall the notations α′
i and B, introduced in (3.2) and (3.3) respectively, and hence express
β(1,i)...β(i−1,i)β(i,i+1)...β(i,m)
uγi,β(j,i)
uγi,β(i,j)
 uγi,β(i,j)
 uγj,β(i,j)
β∈{1,...,d}αij
(uγi,β)(uγj,β)
 uγi · uγj
 uγi · uγj
Proof of Theorem 3.1. In view of the previous lemma, we can denote
˜Qperm = Span
(rγiγj)αij : m ∈N, m ≤n, α ∈Nm×m
and, applying the earlier lemmas, formulate the statement of Theorem 3.1 as ˜Qperm = Qperm. The
latter is an immediate corollary of the following more specialized result.
Lemma 3.7. For m ∈N, m ≤n, denote
perm := Span
(rσiσj)αij : α ∈Nm×m
perm := Span
(rγiγj)αij : α ∈Nm×m
Then ˜Q(m)
perm = Q(m)
Before we prove this lemma, we give two auxiliary results.
Lemma 3.8. We equip N m with the lexicographical order and hence denote by Γ := {γ ∈N m :
γ = min{σγ : σ ∈Sn}} the set of representatives of equivalence classes {σγ : σ ∈Sn}, where σγ
is understood as composition of tuples (3.6). Also let Cγ = #{σγ : σ ∈Sn}, where # denotes the
number of elements in a set. Then
(rγiγj)αij =
(rσγiσγj )αij.
Proof. Any σ ∈Sn induces a one-to-one mapping γ 7→σγ on N m. Hence
(rγiγj)αij =
(rσγiσγj )αij
(rγiγj)αij =
(rσγiσγj )αij.
It remains to group up the terms for which σγ is the same.
The next auxiliary result is proved by a trivial combinatorial argument, essentially expressing
that all elements of Γ other than (1, . . . , m) ∈Γ have repeated values.
Proposition 3.9. Let m ≥1. Then Γ = {(1, . . . , m)} ∪Γ′, where Γ′ := {γ ∈Γ : maxi γi ≤
Proof of Lemma 3.7. We argue by induction over m. For m = 0 the statement is obvious since
perm = ˜Q(0)
perm = Span{1}, therefore we only need to prove the induction step.
Now, for m ∈N, m ≤n, we choose an arbitrary α and let
(rσjσk)αjk ∈Qperm,
C{1,...,m}
(rγjγk)αjk ∈˜Qperm.
These q(r) and ˜q(r) span Q(m)
perm and ˜Q(m)
perm, respectively. We aim to show that q(r) −˜q(r) ∈
perm . This will prove the result considering that, by deﬁnition, Q(m−1)
Indeed, in view of the two previous results, recall the deﬁnition Γ′ := {γ ∈Γ : maxi γi ≤m−1},
˜q(r) −q(r) =
C{1,...,m}
(rσγiσγj )αij −
(rσiσj)αij
C{1,...,m}
(rσγiσγj )αij.
Next, we denote
γi=k, γj=ℓ
and hence express
˜q(r) −q(r) =
C{1,...,m}
(rσkσℓ)α(γ)
Note that the upper limit of both products is m −1 thanks to the deﬁnition of Γ′ (and, of course,
Proposition 3.9). Also, note that we used the fact that, from the deﬁnition of α(γ), α(γ)
whenever k > ℓ. Since P
ℓ=i (rσkσℓ)α(γ)
kℓby deﬁnition belongs to Q(m−1)
for each γ,
we ﬁnally derive that
˜q(r) −q(r) ∈Q(m−1)
This concludes the proof of the induction step.
Practical Implementation
The representation of the interatomic potential through polynomials, outlined in Section 3.1, does
not satisfy the (R3) property needed for the practical implementation. Hence, as the next step
we modify the interatomic potential to satisfy this property. After this, in Section 4.1 we discuss
the steps needed to compute Bα(u), and in Section 4.2 we describe the algorithms we used for
ﬁtting the potentials.
First, notice that for a ﬁxed ν, a linear combination of moment tensors Mµ,ν(u) is a polynomial
of |ui|2 multiplied by u⊗ν
i . The space of polynomials of |ui|2 can be substituted with any other
space of functions (which, for generality, can be made dependent on ν), provided that they can
represent any regular function of |u|, i.e.,
Mµ,ν(u) :=
fµ,ν(|ui|)u⊗ν
where, e.g., fµ,ν(r) = r−µ−νfcut(r) or fµ(r) = e−kµrfcut(r), kµ > 0 is some sequence of real numbers,
and fcut(r) is some cutoﬀfunction such that fcut(r) = 0 for r ≥Rcut. Here fµ,ν(r) plays essentially
the same role as the “radial symmetry functions” in the NNP or the radial basis functions in
 . We then let
(cf. (3.4)) and deﬁne the interatomic potential by
cα ˜Bα(u),
where A is a set of matrix-valued indices ﬁxed a priori and cα is the set of coeﬃcients found in
the training stage. For the rest of the paper we will omit tildes in ˜
M•,• and ˜B•.
Computing the Energy and Forces
Next, we discuss the steps needed to compute the interatomic potential and its derivatives for a
given neighborhood u. The computation consists of two parts, the precomputation (oﬄine) step
and the evaluation (online) step. The precomputation step accepts the set A of values of α as an
input and generates the data for the next step, which is the eﬃcient calculation of Bα(u) for a
given neighborhood u.
Before we proceed, we make two observations.
1. The elements of the tensors Mµ,ν are the “moments” of the form
mµ,β(u) :=
fµ,ν(|ui|)
where β is a multiindex such that |β| = ν. The higher the dimension is, the more repeated
moments each Mµ,ν contains (e.g., each matrix Mµ,2 has 9 elements, out of which at most 6
may be diﬀerent due to the symmetricity of Mµ,2).
2. The scalar functions Bα consist of products of Mµ,ν (which means that the elements of Bα
are linear combinations of products of mµ,β). Diﬀerentiating products of two terms is easier
than products of three or more terms. Hence it will be helpful to have a representation of
Bα as a product of two tensors.
To that end, we extend the deﬁnition of the product
by allowing the result to be a
tensor of nonzero dimension:
β(1,1)...β(k,k) :=
β(i,1)...β(i,k),
where each T (i) is a tensor of dimension Pk
j=1 αij (here we let αij = αji), B is deﬁned in
(3.3), and we make a convention that β(i,j) := β(j,i) for i > j.
Hence we deﬁne
ˆB¯α,α(u) :=
j=1 αij(u),
parametrized by k ∈N, ¯α ∈Nk and a symmetric matrix α ∈Nk×k. ˆB¯α,α(u) is a tensor of
dimension Pk
i=1 αii. Clearly if ¯αi = αii for all i then ˆB¯α,α = Bα, which makes the collection
of tensors ˆB¯α,α a generalization of Bα.
Next, consider ˆB¯α,α(u) for some ¯α ∈Nk and α ∈Nk×k, ﬁx 1 ≤I ≤k, and denote ¯β =
(¯α1, . . . , ¯αI), ¯γ = (¯αI+1, . . . , ¯αk),
αI+1,I+1 + PI
i=1 αi,I+1
αI+2,I+2 + PI
i=1 αi,I+2
One can then express the elements of ˆB¯α,α(u) through products of elements of ˆB¯β,β(u) and
ˆB¯γ,γ(u). Note that by reordering the rows and columns of α and β, one can generate many
diﬀerent ways of representing ˆB¯α,α(u). When doing computations, one should exercise this
freedom in such a way that the resulting tensors are of minimal dimension so that the total
computation cost is reduced.
Finally, note that even if ˆB¯α,α(u) was scalar-valued, ˆB¯β,β(u) and ˆB¯γ,γ(u) do not have to
be scalar-valued—this motivates the need to introduce the tensor-valued basis functions
ˆB¯α,α(u).
Precomputation
The precomputation step is hence as follows (we keep the argument u of, e.g., in Bα(u) to match
the above notation, however, the particular values of u never enter the precomputation step)
• Starting with the set of tensors Bα(u) (indexed by α ∈A), establish their correspondence
to ˆB¯α,α(u) and recursively represent each ˆB¯α,α(u) through some ˆB¯β,β(u) and ˆB¯γ,γ(u) as
described above. In each case, out of all such products choose the one with the minimal sum
of the number of dimensions of these two tensors.
• Enumerate all the elements of all the tensors ˆB¯α,α(u) as bi(u) (i is the ordinal number of the
corresponding element).
• Represent each bi(u) as either bi(u) = mµi,βi(u) or bi(u) = PJi
j=1 cjbℓj(u)bkj(u).
• Output the resulting
(1) correspondence of Bα(u) to bi(u),
(2) µi and βi, and
(3) tuples of (i, c, ℓ, k) corresponding to bi(u) = PJi
j=1 cjbℓj(u)bkj(u).
Evaluation
The evaluation step, as written out below, accepts u as an input and evaluates Bα(u) using the
precomputed data (described above).
1. For a given u, calculate all mµi,βi(u).
2. Then calculate all other bi(u) using the tuples of (i, c, ℓ, k).
3. Finally, pick those bi(u) that correspond to scalar Bα(u) (as opposed to non-scalar ˆB¯α,α).
It remains to form the linear combination of Bα(u) with the coeﬃcients obtained from a linear
regression (training), and then sum up all these linear combinations for all atomic environments
to obtain the interatomic interaction energy of a given atomistic system. The forces are computed
by reverse-mode diﬀerentiation of the energy with respect to the atomic positions .
Once the set A of values of α is ﬁxed, we need to determine the coeﬃcients cα. This is done with
the regularized (to avoid overﬁtting ) linear regression in the following way.
Let a database of atomic conﬁgurations X = {x(k) : k = 1, . . . , K}, where x(k) is of size N (k),
be given together with their reference energies and forces, Eq(x(k)) = E(k) and −∇Eq(x(k)) = f (k).
We form an overdetermined system of linear equations on cα,
i ) = E(k),
i ) = −f (k)
(cf. (2.1) and (2.2)), which we write in the matrix form Xc = g.
These equations may be
ill-conditioned, hence a regularization must be used. We tried three versions of regularization,
namely the ℓp regularization with p ∈{0, 1, 2}, all described below. All three can be written as
subject to
where t is the regularization parameter and ∥c∥ℓ0 is deﬁned as the number of nonzero entires in c.
For p ≥1 this can be equivalently rewritten as
∥Xc −g∥2 + γ∥c∥2
where γ is an alternative regularization parameter.
ℓ2 regularization
For the solution of the overdetermined linear equations Xc = g we take
c = (XTX + γ diag(XTX))−1XTg,
where diag(A) denotes the diagonal matrix whose diagonal elements are the same as in A. The
penalization matrix was chosen as diag(XTX) instead of the identity matrix so that its scaling
with respect to the database size and the scale of the basis functions Bα, is compatible with that
of the covariance matrix XTX. The regularization parameter γ was determined from the 16-fold
cross-validation scheme as described in the next paragraph.
To perform the 16-fold cross-validation, we split the database X evenly into 16 non-overlapping
databases ˜X1, . . . , ˜X16. We then train 16 diﬀerent models, each on the database X \ ˜Xi and ﬁnd
the RMS error when tested on ˜Xi. The parameter γ is then chosen such that the cross-validation
RMS error averaged over these 16 models is minimal.
ℓ0 regularization
The advantage of the ℓ0 regularization is that it produces sparse solutions c, whereas the ℓ2
regularization does not. We note that the ℓ1 regularization also produces sparse solutions ,
but our numerical experiments show that the ℓ0 regularization produces signiﬁcantly more sparse
solutions (however, at a cost of a larger precomputation time).
We thus solve a sequence of problems, parametrized by an integer parameter Nnz (number of
non-zeros) as follows:
∥Xc −g∥subject to ∥c∥ℓ0 = Nnz,
and choose the minimal Nnz such that ∥Xc −g∥reaches the accuracy goal.
To describe the algorithm, it is convenient to rewrite the problem as follows. Let A be the set
of all indices α (earlier denoted as A).
ﬁnd A ⊂A such that min
∥Xc −g∥is minimal,
subject to |A| = Nnz and cA\A = 0.
This is essentially a compressed sensing problem . In order to solve it we take a standard
greedy algorithm (similar to the matching pursuit from the compressed sensing literature) and
turn it into a genetic algorithm by adding the local search and crossover steps. The main variable
in this algorithm is the family (population) of the sets A which is denoted by A. The cap on the
population size is set to be Ncap > 1. The algorithm is as follows.
1. Let A = {∅} as the solution for Nnz = 0.
2. For each A ∈A ﬁnd i /∈A, such that c corresponding to A∪{i} is the best (i.e., minimizing
∥Xc −g∥). Then replace A with A ∪{i}.
3. “Crossover”: If |A| > 1 then do the following. For each pair of sets, A, A′ ∈A, divide
randomly these sets into two nonintersecting subsets, A = A1 ∪A2 and A′ = A′
generate new sets A1 ∪A′
1 ∪A2. To generate such splittings of the sets, ﬁrst sample
uniformly an integer m ∈{1, . . . , |A\A′|} and then form A2 and A′
2 by uniformly sampling m
distinct elements from A and A′ respectively. Then replace all the sets in A with the newly
generated sets. (Note that if |A| = Ncap then up to Ncap(Ncap −1) sets will be generated—2
sets from each of
4. “Local search”:
4.1. For each A ∈A ﬁnd j ∈A such that c corresponding to A \ {j} is the best.
4.2. Then ﬁnd i /∈A \ {j} such that c corresponding to (A \ {j}) ∪{i} is the best.
4.3. If i ̸= j then:
4.3a. include (A \ {j}) ∪{i} into A,
4.3b. if |A| > Ncap then exclude A from A, and
4.3c. go to step 4.1.
5. Remove all but Ncap best sets in A.
6. Repeat steps 2–5 until the accuracy goal on ∥Xc −g∥is reached. Then take the best set
A ∈A and compute the corresponding c.
We note that whenever A is ﬁxed then ﬁnding c corresponding to A is easy:
c = ((XTX)AA)−1(XTg)A, where •A and •AA are the operations of extracting a subset of rows
and columns corresponding to A ⊂A.
Numerical Experiments
Our next goal is to understand how MTP performs compared to other interatomic potentials.
Unfortunately, there are no existing works performing quantitative comparison between diﬀerent
machine learning potentials in terms of their accuracy and computational eﬃciency. In the present
work we compare the performance of MTP with that of GAP for tungsten on the QM
database published at www.libatoms.org together with the GAP code. We test MTP by ﬁtting
it on the database of 9 693 conﬁgurations of tungsten, with nearly 150 000 individual atomic
environments, and compare it to the analogously ﬁtted GAP, namely, the one tagged as GAP6
in or as the iterative-SOAP-GAP or I-S-GAP in . We note that this is a database of the
Kohn-Sham DFT calculations (as opposed to a tight-binding model used in the analysis) for an
electronic temperature of 1000◦K. We did not prove algebraic convergence of MTP to this model,
however we will observe it numerically.
We choose Rcut = 4.9˚A, and also set the minimal distance parameter to be Rmin := 1.9˚A. For
the radial functions we choose
ˆfµ,ν(r) :=
r−ν−2rµ(Rcut −r)2
and then for each ν we orthonormalize them on the interval [Rmin, Rcut] with the weight r2ν(r −
Rmin)(Rcut −r). This procedure yields us the functions fµ,ν used with (4.1). (This procedure is
equivalent to ﬁrst orthonormalizing the functions r−2rµ(Rcut−r)2 with the weight (r−Rmin)(Rcut−
r), the same weight as for the Chebyshev polynomials, and then multiplying by r−ν.) Here r−ν
compensates for r⊗ν, r−2 prioritizes closer atoms to more distant onces, and (Rcut −r)2 ensures a
smooth cut-oﬀ.
Convergence with the Number of Basis Functions
Even though theoretically it is proved that MTP is systematically improvable, the actual accuracy
depends on the size of the regression problem to be solved. We therefore ﬁrst study how fast the
MTP ﬁt converges with the number of basis functions used.
Theorem 3.2 suggests that the ﬁtting error decreases exponentially with the polynomial degree.
At the same time, a crude upper bound on the dimension of the space of polynomials of degree m
is (n + 1)m, where n is the maximal number of atoms in an atomic neighborhood. This suggests
an algebraic decay of the ﬁtting error with the number of basis functions.
We hence study convergence of the error of ﬁtting of potentials based on two choices of the sets
of α. The ﬁrst choice of the set of α, AN, is to limit the corresponding degree of Bα for α ∈AN
AN = {α : deg(Bα) ≤N}.
Since fµ,ν are no longer polynomials, we make a convention that the degree of fµ,ν is µ + 1 (while
the degree of the constant function f(u) := 1 is zero). Interestingly, it was found that a slightly
better choice is
N = {α : deg(Bα) ≤N + 8(#α)},
where #α is the length of α. In fact, A′
N corresponds to AN if we make another convention that
deg(fµ,ν) = µ + 5.
Figure 2 displays the RMS ﬁtting error in forces as a function of the size of the set AN. One can
observe an algebraic convergence, which indicates that Theorem 3.2 is valid for the Kohn-Sham
DFT also. The observed rate is (#A)−0.227, where the exponent −0.227 is not a universal constant
associated with MTP, but depends on the database chosen. A preasymptotic regime with a faster
algebraic convergence rate can be seen for small #A.
æ æ æ æ æ æ æ æ æ
0.3 HðAL-0.227
Figure 2: RMS ﬁtting error in forces as a function of the size of AN and A′
An algebraic
convergence can be observed.
Performance Tests
We next test the performance of MTP and compare it to GAP in terms of ﬁtting error and
computation (CPU) time. We choose two versions of MTP diﬀerent from each other by the set A.
The ﬁrst MTP, denoted by MTP1, is generated by limiting deg(Bα)+8(#α) ≤62 and additionally limiting (#α) ≤4, µ ≤5, and ν ≤4. MTP2 is generated by limiting deg(Bα) + 8(#α) ≤52,
additionally limiting (#α) ≤5, µ ≤3, and ν ≤5, and applying the ℓ0 regularization algorithm
with Ncap = 4, as described in Section 4.2.2, to extract 760 basis functions (so as to make the
accuracy of GAP and MTP2 same, as discussed in the next paragraph).
Potential:
CPU time/atom [ms]:
134.2 ±2.6
basis functions:
Fit errors:
force RMS error [eV/˚A]:
Cross-validation errors:
regularization parameter γ:
force RMS error[eV/˚A]:
Table 1: Eﬃciency and accuracy of MTP as compared to GAP. The mean CPU time is given
for a single-core computation on the laptop Intel i7-2675QM CPU, together with the standard
deviation as computed on a number of independent runs. The last section of the table reports the
force RMS errors for the 16-fold cross-validation.
The data from the conducted eﬃciency and accuracy tests are summarized in Table 1. The
RMS force (more precisely, RMS of all force components over all non-single-atom conﬁgurations)
is 1.505 eV/˚A, which is used to compute the relative RMS error. The errors relative to this RMS
force are also presented in the table. The GAP error is calculated based on the data from .
The CPU times do not include the initialization (precomputation) or constructing the atomistic
neighborhoods.
One can see that MTP1 has about the same number of ﬁtting parameters as GAP, while its
ﬁtting accuracy is about 1.5 times better and the computation time is 40 times smaller. MTP2 was
constructed such that its ﬁtting accuracy is the same as GAP, but it uses much less parameters
for ﬁtting and its computation is more than two orders of magnitude faster.
Also included in the table is the 16-fold cross-validation error. It shows that MTP2 is not
overﬁtted on the given database, whereas MTP1 needs regularization to avoid overﬁtting. We
anticipate, however, that by signiﬁcantly increasing the database size, the cross-validation error of
MTP1 would go down and reach the current ﬁtting error of MTP1, since the ﬁtting error follows
closely the algebraic decay (see Figure 2) and is not expected to deteriorate with increasing the
database size.
Conclusion
The present paper considers the problem of approximating a QM interaction model with interatomic potentials from a mathematical point of view. In particular, (1) a new class of nonparametric (i.e., systematically improvable) potentials satisfying all the required symmetries has been
proposed and advantages in terms of accuracy and performance over the existing schemes have
been discussed and (2) an algebraic convergence of ﬁtting with these potentials in a simple setting
has been proved and it was then conﬁrmed with the numerical experiments.
This work is done under the assumption that all atoms are chemically equivalent. A straightforward extension to multicomponent systems would be to let the radial functions depend not
only on the positions of atoms xi, but also on the types of atoms ti. Thus, the expression for the
moments for atom i could be
fµ,ν(|xj −xi|, ti, tj)(xj −xi)⊗ν,
where the summation is over the neighborhood of atom i. We leave exploring this path to future
publications.
Acknowledgement
The author is grateful to G´abor Cs´anyi for igniting the interest in this topic and for valuable
discussions as a part of our on-going collaboration. Also, the author thanks Albert Bart´ok-P´artay
for his help with implementation of MTP in the QUIP software1 which was used to compare the
performance of MTP with GAP. Finally, the author thanks the anonymous referees for many
suggestions that lead to improvement of the paper.