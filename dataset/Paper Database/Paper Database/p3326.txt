Robust Object Tracking with
Online Multiple Instance Learning
Boris Babenko, Student Member, IEEE, Ming-Hsuan Yang, Senior Member, IEEE, and
Serge Belongie, Member, IEEE
Abstract—In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other
information. Recently, a class of tracking techniques called “tracking by detection” has been shown to give promising results at
real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This
classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight
inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift.
In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and
can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking
that achieves superior results with real-time performance. We present thorough experimental results (both qualitative and quantitative)
on a number of challenging video clips.
Index Terms—Visual Tracking, multiple instance learning, online boosting.
INTRODUCTION
BJECT tracking is a well-studied problem in computer
vision and has many practical applications. The
problem and its difficulty depend on several factors, such
as the amount of prior knowledge about the target object and
the number and type of parameters being tracked (e.g.,
location, scale, detailed contour). Although there has been
some success with building trackers for specific object classes
(e.g., faces , humans , mice , rigid objects ), tracking
generic objects has remained challenging because an object
can drastically change appearance when deforming, rotating
out of plane, or when the illumination of the scene changes.
A typical tracking system consists of three components:
1) an appearance model, which can evaluate the likelihood
that the object of interest is at some particular location, 2) a
motion model, which relates the locations of the object over
time, and 3) a search strategy for finding the most likely
location in the current frame. The contributions of this paper
deal with the first of these three components; we refer the
reader to for a thorough review of the other components.
Although many tracking methods employ static appearance
models that are either defined manually or trained using
only the first frame , , , , , , these methods are
often unable to cope with significant appearance changes.
Such challenges are particularly difficult when there is
limited a priori knowledge about the object of interest. In
this scenario, it has been shown that an adaptive appearance
model, which evolves during the tracking process as the
appearance of the object changes, is the key to good
performance , , . Training adaptive appearance
models, however, is itself a difficult task, with many
questions yet to be answered. Such models often involve
many parameters that must be tuned to get good performance (e.g., “forgetting factors” that control how fast the
appearance model can change), and can suffer from drift
problems when an object undergoes partial occlusion.
In this paper, we focus on the problem of tracking an
arbitrary object with no prior knowledge other than its
location in the first video frame (sometimes referred to as
“model-free” tracking). Our goal is to develop a more
robust way of updating an adaptive appearance model; we
would like our system to be able to handle partial
occlusions without significant drift and for it to work well
with minimal parameter tuning. To do this, we turn to a
discriminative learning paradigm called Multiple Instance
Learning (MIL) that can handle ambiguities in the
training data. This technique has found recent success in
other computer vision areas, such as object detection ,
 and object recognition , , .
We will focus on the problem of tracking the location and
scale of a single object, using a rectangular bounding box to
approximate these parameters. It is plausible that the ideas
presented here can be applied to other types of tracking
problems like tracking multiple objects (e.g., ), tracking
contours (e.g., , ), or tracking deformable objects
(e.g., ), but this is outside the scope of our work.
The remainder of this paper is organized as follows: In
Section 2, we review the current state of the art in adaptive
appearance models; in Section 3, we introduce our tracking
algorithm; in Section 4, we present qualitative and
quantitative results of our tracker on a number of challenging video clips. We conclude in Section 5.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
. B. Babenko and S. Belongie are with the Department of Computer Science
and Engineering, University of California, San Diego, 9500 Gilman Dr.,
La Jolla, CA 92093-0404. E-mail: {bbabenko, sjb}@cs.ucsd.edu.
. M.-H. Yang is with the Department of Computer Science, University of
California, Merced, CA 95344. E-mail: .
Manuscript received 16 Feb. 2010; revised 1 July 2010; accepted 5 Aug. 2010;
published online 13 Dec. 2010.
Recommended for acceptance by H. Bischof.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number
TPAMI-2010-02-0100.
Digital Object Identifier no. 10.1109/TPAMI.2010.226.
0162-8828/11/$26.00  2011 IEEE
Published by the IEEE Computer Society
ADAPTIVE APPEARANCE MODELS
An important choice in the design of appearance models is
whether to model only the object , or both the object
and the background , , , , , , . Many
of the latter approaches have shown that training a model to
separate the object from the background via a discriminative
classifier can often achieve superior results. These methods
are closely related to object detection—an area that has seen
great progress in the last decade—and are referred to as
“tracking-by-detection” or “tracking by repeated recognition” . In particular, the recent advances in face detection
 have inspired some successful real-time tracking
algorithms , .
A major challenge that is often not discussed in the
literature is how to choose positive and negative examples
when updating the adaptive appearance model. Most
commonly this is done by taking the current tracker
location as one positive example and sampling the
neighborhood around the tracker location for negatives. If
the tracker location is not precise, however, the appearance
model ends up getting updated with a suboptimal positive
example. Over time this can degrade the model and can
cause drift. On the other hand, if multiple positive examples
are used (taken from a small neighborhood around the
current tracker location), the model can become confused
and its discriminative power can suffer (cf. Figs. 1a, 1b).
Alternatively, Grabner et al. recently proposed a semisupervised approach where labeled examples come from
the first frame only and subsequent training examples are
left unlabeled. This method is particularly well suited for
scenarios where the object leaves the field of view
completely, but it throws away a lot of useful information
by not taking advantage of the problem domain (e.g., it is
safe to assume small interframe motion).
Object detection faces issues similar to those described
above in that it is difficult for a human labeler to be
consistent with respect to how the positive examples are
cropped. In fact, Viola et al. argue that object detection
has inherent ambiguities that cause difficulty for traditional
supervised learning methods. For this reason they suggest
the use of a MIL approach for object detection. We give
a more formal definition of MIL in Section 3.2, but the basic
idea of this learning paradigm is that during training,
examples are presented in sets (often called “bags”) and
labels are provided for the bags rather than individual
instances. If a bag is labeled positive, it is assumed to
contain at least one positive instance; otherwise the bag is
negative. For example, in the context of object detection, a
positive bag could contain a few possible bounding boxes
around each labeled object (e.g., a human labeler clicks on
the center of the object and the algorithm crops several
rectangles around that point). Therefore, the ambiguity is
passed on to the learning algorithm, which now has to
figure out which instance in each positive bag is the most
“correct.” Although one could argue that this learning
problem is more difficult in the sense that less information
is provided to the learner, in some ways it is actually easier
because the learner is allowed some flexibility in finding a
decision boundary. Viola et al. present convincing results
showing that a face detector trained with weaker labeling
(just the center of the face) and a MIL algorithm outperforms a state of the art supervised algorithm trained with
explicit bounding boxes.
In this paper, we make an analogous argument to that of
Viola et al. and propose to use a MIL-based appearance
model for object tracking (cf. Fig. 1c). In fact, in the object
tracking domain there is even more ambiguity than in object
detection because the tracker has no human input and has to
bootstrap itself. Therefore, we expect the benefits of a MIL
approach to be even more significant than in the object
detection problem. In order to incorporate MIL into a
tracker, an online MIL algorithm is required. The algorithm
we propose is based on boosting and is related to the
MILBoost algorithm as well as the Online AdaBoost
algorithm (to our knowledge this is the first online MIL
algorithm in the literature). We present empirical results on
challenging video sequences which show that using an
online MIL-based appearance model can lead to more robust
and stable tracking than existing methods in the literature.
TRACKING WITH ONLINE MIL
In this section, we introduce our tracking algorithm, MIL-
Track, which uses a MIL-based appearance model. We begin
with an overview of our tracking system which includes a
description of the motion model we use. Next, we review the
MIL problem and briefly describe the MILBoost algorithm
 . We then review online boosting , and present a
novel boosting-based algorithm for online MIL. Finally, we
review various implementation details.
System Overview and Motion Model
The basic flow of the tracking system we implemented in
this work is illustrated in Fig. 2 and summarized in
Algorithm 1. Our image representation consists of a set of
Haar-like features that are computed for each image patch
 , ; this is discussed in more detail in Section 3.6. The
appearance model is composed of a discriminative classifier
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
Fig. 1. Updating a discriminative appearance model: (a) Using a
single positive image patch to update a traditional discriminative
classifier. The positive image patch chosen does not capture the object
perfectly. (b) Using several positive image patches to update a
traditional discriminative classifier. This can make it difficult for the
classifier to learn a tight decision boundary. (c) Using one positive bag
consisting of several image patches to update a MIL classifier. See
Section 4 for empirical results of these three strategies.
which is able to return pðy ¼ 1jxÞ (we will use pðyjxÞ as
shorthand), where x is an image patch (or the representation
of an image patch in feature space) and y is a binary variable
indicating the presence of the object of interest in that image
patch. At every time step t, our tracker maintains the object
location ‘
t . Let ‘ðxÞ denote the location of image patch x (for
now, let’s assume this consists of only the ðx; yÞ coordinates
of the patch center and that scale is fixed; below, we
consider tracking scale as well). For each new frame, we
crop out a set of image patches Xs ¼ fx : k‘ðxÞ  ‘
that are within some search radius s of the current tracker
location, and compute pðyjxÞ for all x 2 Xs. We then use a
greedy strategy to update the tracker location:
t ¼ ‘ arg max
In other words, we do not maintain a distribution of the
target’s location at every frame, and our motion model is such
that the location of the tracker at time t is equally likely to
appear within a radius s of the tracker location at time ðt  1Þ:
otherwise:
This could be extended with something more sophisticated,
such as a particle filter, as is done in , , ;
however, we again emphasize that our focus is on the
appearance model.
Algorithm 1. MILtrack
Input: Video frame number k
1: Crop out a set of image patches, Xs ¼ fx : k‘ðxÞ 
t1k < sg and compute feature vectors.
2: Use MIL classifier to estimate pðy ¼ 1jxÞ for x 2 Xs.
3: Update tracker location ‘
t ¼ ‘ðargmaxx2Xs pðyjxÞÞ.
4: Crop out two sets of image patches Xr ¼ fx : k‘ðxÞ 
t k < rg and Xr; ¼ fx : r < k‘ðxÞ  ‘
5: Update MIL appearance model with one positive bag
Xr and jXr;j negative bags, each containing a single
image patch from the set Xr;.
Once the tracker location is updated, we proceed to
update the appearance model. We crop out a set of patches
Xr ¼ fx : k‘ðxÞ  ‘
t k < rg, where r < s is a scalar radius
(measured in pixels), and label this bag positive (recall that
in MIL we train the algorithm with labeled bags). In
contrast, if a standard learning algorithm were used, there
would be two options: Set r ¼ 1 and use this as a single
positive instance or set r > 1 and label all these instances
positive. For negatives we crop out patches from an annular
region Xr; ¼ fx : r < k‘ðxÞ  ‘
t k < g, where r is same as
before and  is another scalar. Since this generates a
potentially large set, we then take a random subset of these
image patches and label them negative. We place each
negative example into its own negative bag, though placing
them all into one negative bag yields the same result.
Incorporating scale tracking into this system is straightforward. First, we define an extra parameter  to be the scale
space step size. When searching for the location of the object
in a new frame, we crop out image patches from the image
at the current scale ‘s
t, as well as one scale step larger and
smaller, ‘s
t  ; once we find the location with the maximum
response, we update the current state (both position and
scale) accordingly. When updating the appearance model,
we have the option of cropping training image patches only
from the current scale or from the neighboring scales as
well; in our current implementation we do the former.
It is important to note that tracking in scale-space is a
double-edged sword. In some ways the problem becomes
more difficult because the parameter space becomes larger,
and consequently there is more room for error. However,
tracking this additional parameter may mean that the image
patcheswecropoutarebetteraligned,makingiteasierforour
classifier to learn the correct appearance. In our experiments,
we have noticed both behaviors—sometimes adding scale
tracking helps and other times it hurts performance.
Details on how all of the above parameters were set are
in Section 4, although we use the same parameters
throughout all of the experiments. We continue with a
more detailed review of MIL.
Multiple Instance Learning
Traditional discriminative learning algorithms for training a
binary classifier that estimates pðyjxÞ require a training data
set of the form fðx1; y1Þ; . . . ; ðxn; ynÞg, where xi is an instance
(in our case a feature vector computed for an image patch)
andyi 2 f0; 1gisabinarylabel.InMultipleInstanceLearning,
training data has the form fðX1; y1Þ; . . . ; ðXn; ynÞg, where a
bag Xi ¼ fxi1; . . . ; ximg and yi is a bag label. The bag labels
are defined as:
where yij are the instance labels, which are not known
during training. In other words, a bag is considered positive
if it contains at least one positive instance. Numerous
algorithms have been proposed for solving the MIL
problem , , . The algorithm that is most closely
related to our work is the MILBoost algorithm proposed by
Viola et al. in . MILBoost uses the gradient boosting
framework to train a boosting classifier that maximizes
the log likelihood of bags:
ðlog pðyijXiÞÞ:
Notice that the likelihood is defined over bags and not
instances because instance labels are unknown during
BABENKO ET AL.: ROBUST OBJECT TRACKING WITH ONLINE MULTIPLE INSTANCE LEARNING
Fig. 2. Tracking by detection with a greedy motion model: An
illustration of how most tracking by detection systems work.
training, and yet the goal is to train an instance classifier
that estimates pðyjxÞ. We therefore need to express pðyijXiÞ,
the probability of a bag being positive, in terms of its
instances. In , the Noisy-OR (NOR) model is adopted for
doing this:
pðyijXiÞ ¼ 1 
ð1  pðyijxijÞÞ;
although other models could be swapped in (e.g., ). The
equation above has the desired property that if one of the
instances in a bag has a high probability, the bag probability
will be high as well. As mentioned in , with this
formulation, the likelihood is the same whether we put all of
the negative instances in one bag or if we put each in its own
bag. Intuitively this makes sense because no matter how we
arrange things, we know that every instance in a negative
bag is negative. We refer the reader to for further details
on MILBoost. Finally, we note that MILBoost is a batch
algorithm (meaning it needs the entire training data set at
once) and cannot be trained in an online manner as we need
in our tracking application. Nevertheless, we adopt the loss
function in (4) and the bag probability model in (5) when we
develop our online MIL algorithm in Section 3.4.
Online Boosting
Our algorithm for online MIL is based on the boosting
framework and is related to the work on Online
AdaBoost and its adaptation in . The goal of
boosting is to combine many weak classifiers hðxÞ (usually
decision stumps) into an additive strong classifier:
where k are scalar weights. There have been many boosting
algorithms proposed to learn this model in batch mode ,
 ; typically this is done in a greedy manner where the
weak classifiers are trained sequentially. After each weak
classifier is trained, the training examples are reweighted
such that examples that were previously misclassified
receive more weight. If each weak classifier is a decision
stump, then it chooses one feature that has the most
discriminative power for the entire weighted training set.
In this case, boosting can be viewed as performing feature
selection, choosing a total of K features which is generally
much smaller than the size of the entire feature pool. This
has proven particularly useful in computer vision because it
creates classifiers that are efficient at run time .
In , Oza develops an online variant of the popular
AdaBoost algorithm , which minimizes the exponential
loss function. This variant requires that all h can be trained
in an online manner. The basic flow of Oza’s algorithm is as
follows: For an incoming example x, each hk is updated
sequentially and the weight of example x is adjusted after
each update. Since the formulas for the example weights
and classifier weights in AdaBoost depend only on the error
of the weak classifiers, Oza proposes keeping a running
average of the error of each hk, which allows the algorithm
to estimate both the example weight and the classifier
weights in an online manner.
In Oza’s framework, if every h is restricted to be a
decision stump, the algorithm has no way of choosing the
most discriminative feature because the entire training set is
never available at one time. Therefore, the features for each
hk must be picked a priori. This is a potential problem for
computer vision applications since they often rely on the
feature selection property of boosting. Grabner et al. 
proposed an extension of Oza’s algorithm which performs
feature selection by maintaining a pool of M > K candidate
weak stump classifiers h. When a new example is passed in,
all of the candidate weak classifiers are updated in parallel.
Then, the algorithm sequentially chooses K weak classifiers
from this pool by keeping running averages of errors for
each, as in , and updates the weights of h accordingly.
We employ a similar feature selection technique in our
Online MIL algorithm, although the criteria for choosing
weak classifiers is different.
Online Multiple Instance Boosting
The algorithms in and rely on the special properties
of the exponential loss function of AdaBoost, and therefore
cannot be readily adapted to the MIL problem. We now
present our novel online boosting algorithm for MIL. As in
 , we take a statistical view of boosting, where the
algorithm is trying to optimize a specific objective
function J. In this view, the weak classifiers are chosen
sequentially to optimize the following criteria:
ðhk; kÞ ¼ arg max
JðHk1 þ hÞ;
where Hk1 is the strong classifier made up of the first ðk  1Þ
weak classifiers and H is the set of all possible weak
classifiers. In batch boosting algorithms, the objective
function J is computed over the entire training data set.
In our case, for the current video frame we are given a
training data set fðX1; y1Þ; ðX2; y2Þ . . .g, where Xi ¼ fxi1;
xi2 . . .g. We would like to update our classifier to maximize
log likelihood of this data (4). We model the instance
probability as
pðyjxÞ ¼ 
where ðxÞ ¼
1þex is the sigmoid function; the bag probabilities pðyjXÞ are modeled using the NOR model in (5). To
simplifythe problem,we absorbthe scalarweights t into the
weak classifiers by allowing them to return real values
rather than binary.
At all times our algorithm maintains a pool of M > K
candidate weak stump classifiers h. To update the classifier,
we first update all weak classifiers in parallel, similar to .
Note that although instances are in bags, the weak classifiers
in a MIL algorithm are instance classifiers and therefore
require instance labels yij. Since these are unavailable, we
pass in the bag label yi for all instances xij to the weak
training procedure. We then choose K weak classifiers h
from the candidate pool sequentially by maximizing the log
likelihood of bags:
hk ¼ arg max
h2fh1;...;hMg
LðHk1 þ hÞ:
See Algorithm 2 for the pseudocode of Online MILBoost
and Fig. 3 for an illustration of tracking with this algorithm.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
Algorithm 2. Online MILBoost (OMB)
Input: Data set fXi; yigN
i¼1, where Xi ¼ fxi1; xi2; . . .g; yi 2
1: Update all M weak classifiers in the pool with data
2: Initialize Hij ¼ 0 for all i; j
3: for k ¼ 1 to K do
for m ¼ 1 to M do
ij ¼ ðHij þ hmðxijÞÞ
iðyi logðpm
i Þ þ ð1  yiÞ logð1  pm
m ¼ argmaxmLm
hkðxÞ hmðxÞ
Hij ¼ Hij þ hkðxÞ
12: end for
Classifier
Discussion
There are a couple of important issues to point out about
this algorithm. First, we acknowledge the fact that training
the weak classifiers with positive labels for all instances in
the positive bags is suboptimal because some of the
instances in the positive bags may actually not be “correct.”
The algorithm makes up for this when it is choosing the
weak classifiers h based on the bag likelihood loss function.
We have also experimented using online GradientBoost 
to compute weights (via the gradient of the loss function)
for all instances, but found this to make little difference in
accuracy while making the system slower. Second, if we
compare (7) and (9), we see that the latter has a much more
restricted choice of weak classifiers. This approximation
does not seem to degrade the performance of the classifier
in practice, as noted in . Finally, we note that the
likelihood being optimized in (9) is computed only on the
current examples. Thus, it has the potential of overfitting to
current examples and not retaining information about
previously seen data. This is averted by using online weak
classifiers that do retain information about previously seen
data, which balances out the overall algorithm between
fitting the current data and retaining history.
Implementation Details
3.6.1 Weak Classifiers
Recall that we require weak classifiers h that can be updated
online. In our system, each weak classifier hk is composed of
a Haar-like feature fk and four parameters ð1; 1; 0; 0Þ that
are estimated online. The classifiers return the log odds ratio:
hkðxÞ ¼ log pt
y ¼ 1jfkðxÞ
y ¼ 0jfkðxÞ
where ptðftðxÞjy ¼ 1Þ  N ð1; 1Þ and similarly for y ¼ 0.
We let pðy ¼ 1Þ ¼ pðy ¼ 0Þ and use Bayes rule to compute the
above equation. When the weak classifier receives new data
fðx1; y1Þ; . . . ; ðxn; ynÞg, we use the following update rules:
1 1 þ ð1  Þ 1
1 1 þ ð1  Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðfkðxiÞ  1Þ2
where 0 <  < 1 is a learning rate parameter. The update
rules for 0 and 0 are similarly defined.
3.6.2 Image Features
We represent each image patch as a vector of Haar-like
features , which are randomly generated, similarly to .
BABENKO ET AL.: ROBUST OBJECT TRACKING WITH ONLINE MULTIPLE INSTANCE LEARNING
Fig. 3. An illustration of how using MIL for tracking can deal with occlusions. Frame 1: Consider a simple case where the classifier is allowed to only
pick one feature from the pool. The first frame is labeled. One positive patch and several negative patches (not shown) are extracted and the
classifiers are initialized. Both OAB and MIL result in identical classifiers—both choose feature #1 because it responds well with the mouth of the
face (feature #3 would have performed well also, but suppose #1 is slightly better). Frame 2: In the second frame there is some occlusion. In
particular, the mouth is occluded, and the classifier trained in the previous step does not perform well. Thus, the most probable image patch is no
longer centered on the object. OAB uses just this patch to update; MIL uses this patch along with its neighbors. Note that MIL includes the correct
image patch in the positive bag. Frame 3: When updating, the classifiers try to pick the feature that best discriminates the current example as well
the ones previously seen. OAB has trouble with this because the current and previous positive examples are too different. It chooses a bad feature.
MIL is able to pick the feature that discriminates the eyes of the face because one of the examples in the positive bag was correctly cropped (even
though the mouth was occluded). MIL is therefore able to successfully classify future frames. Note that if we assign positive labels to all of the image
patches in the MIL bag and use these to train OAB, it would still have trouble picking a good feature.
Each feature consists of two to four rectangles, and each
rectangle has a real valued weight. The feature value is then
a weighted sum of the pixels in all the rectangles. These
features can be computed efficiently using the integral
image trick described in .
EXPERIMENTS
We tested our MILTrack system on several challenging
video sequences. For comparison, we implemented a tracker
based on the Online AdaBoost (OAB) algorithm described in
 . We plugged this learning algorithm into our system
and used the same features and motion model as for
MILTrack (See Section 3.1). We acknowledge the fact that
our implementation of the OAB tracker achieves worse
performance than is reported in ; this could be because
we are using simpler features or because our parameters
were not tuned per video sequence. However, our study is
still valid for comparison because only the learning
algorithm changes between our implementation of the
OAB tracker and MILTrack, and everything else is kept
constant. This allows us to isolate the appearance model to
make sure that it is the cause of the performance difference.
One of the goals of this work is to demonstrate that using
MIL results in a more robust and stable tracker. For this
reason all algorithm parameters were fixed for all the
experiments. This holds for all algorithms we tested. For
MILTrack and OAB the parameters were set as follows: The
search radius s is set to 35 pixels. For MILTrack, we sample
positives in each frame using a radius r ¼ 4 (we found that
the algorithm is fairly robust for a range of values). This
generates a total of 45 image patches comprising one positive
bag (for clarity, we call this MILTrack(45)). For the OAB
tracker we tried two variations. In the first variation we set
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
Tracking Object Location: Average Center Location Errors (pixels)
Bold green font indicates best performance, red italics font indicates second best.
Fig. 4. Tracking object location: location error plots. See text for details.
r ¼ 1, generating only one positive example per frame (we
call this OAB(1)); in the second variation we set r ¼ 4 as we do
in MILTrack (although in this case, each of the 45 image
patches is labeled positive); we call this OAB(45). The reason
we experimented with these two versions was to show that
the superior performance of MILTrack is not simply due to
the fact that we extract multiple positive examples per frame.
In fact, as we will see shortly, when multiple positive
examples are used for the OAB tracker, its performance
degrades1 (cf. Table 1 and Fig. 4). The scalar  for sampling
negative examples was set to 50, and we randomly sample
65 negative image patches from the set Xr; (though, during
initialization with the first frame, we sample 1,000 patches).
The learning rate  for the weak classifiers is set to 0.85.
Finally, the number of candidate weak classifiers M was set to
250 andthe number of chosen weak classifiers K was set to50.
To gauge absolute performance we also compare our
results to three other algorithms, using code provided by
the respective authors. The first of these is the SemiBoost
tracker ;2 as mentioned earlier, this method uses label
information from the first frame only, and then updates the
appearance model via online semi-supervised learning in
subsequent frames. This makes it particularly robust to
scenarios where the object leaves the scene completely.
However, the model relies strongly on the prior classifier
(trained using the first frame). We found that on clips
exhibiting significant appearance changes this algorithm
often lost the object. The second algorithm is FragTrack .3
This algorithm uses a static appearance model based on
BABENKO ET AL.: ROBUST OBJECT TRACKING WITH ONLINE MULTIPLE INSTANCE LEARNING
Tracking Object Location: Precision at a Fixed Threshold of 20
Bold green font indicates best performance, red italics font indicates second best.
Fig. 5. Tracking Object Location: Precision plots. See text for details.
1. We also experimented with the LogitBoost loss function (as in ),
which penalizes noisy examples less harshly, and although it worked better
than OAB, it did not outperform MILTrack. We omit the detailed results
due to space constraints.
2. Code available at 
download.htm.
3. Code available at 
fragtrack.htm.
integral histograms, which have been shown to be very
efficient. The appearance model is part based, which makes
it robust to occlusions. For both algorithms, we use the
default parameters provided by the authors for all of our
experiments. For experiments where we track both location
and scale we compare to IVT , setting the parameters
such that only location and scale are tracked (rather than a
full set of affine parameters). For the trackers that involve
randomness, all results are averaged over five runs.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
Fig. 6. Tracking Object Location: Screenshots of tracking results, highlighting instances of out-of-plane rotation, occluding clutter, scale and
illumination change. For the sake of clarity we only show three trackers per video clip. (a) Sylvester. (b) David indoors. (c) Cola can.
The system was implemented in C++ (code and data
available on our project website: 
project/tracking-online-multiple-instance-learning), and
runs at about 25 frames per second (FPS).
Evaluation Methodology
Evaluating a tracking algorithm is itself a challenge.
Qualitative comparison on test video clips is most common;
quantitative comparison typically involves plotting the
center location error versus frame number. Since these plots
can be difficult to interpret, it is useful to summarize
performance by computing the mean error over all the
frames of the video. However, this value sometimes fails to
correctly capture tracker performance. For example, if a
tracker tracks an object closely for most of the video, but loses
track completely on the last several frames, the mean location
error may be higher than a tracker that sticks with the object
though not as precisely. The preference between these two
behaviors inevitably depends on the final application.
For the above reasons, in addition to presenting screen
shots and location error analysis, we include precision plots,
similar to the analysis in and suggested in . These plots
show the percentage of frames for which the estimated object
location was within some threshold distance of the ground
truth. To summarize these plots, we chose threshold 20 and
report the precision at this point in the curve (e.g., this is the
percent of frames for which the tracker was less than 20 pixels
off from the ground truth); this threshold roughly corresponds to at least a 50 percent overlap between the tracker
bounding box and the ground truth. Note that we could have
used the PASCAL overlap criteria throughout our
evaluation; however, this would require us to label full
bounding boxes (which is more time consuming), and would
make it difficult to compare trackers that do and do not
return estimated scale. Finally, note that when multiple trails
were done, we computed error for each trial and averaged the
errors rather than averaging the tracker outputs and
computing error.
Tracking Object Location
We perform our experiments on three publicly available
video sequences, as well as six of our own. For all
sequences, we labeled the ground truth center of the object
for every five frames, and interpolated the location in the
other frames (with the exception of the “Occluded Face”
sequence, for which the authors of provided ground
truth). All video frames were converted to gray scale prior
to processing.
The quantitative results are summarized in Tables 1 and
2, and plots are shown in Figs. 4 and 5; Figs. 6, 7, and 8 show
screen captures for some of the clips. Below is a more
detailed discussion of the video sequences.
4.2.1 Sylvester and David Indoors
These two video sequences have been used in several
recent tracking papers , , , and they present
challenging lighting, scale, and pose changes. Our algorithm achieves the best performance (tying FragTrack on
the “Sylvester” sequence).
4.2.2 Occluded Face, Occluded Face 2
In the “Occluded Face” sequence, which comes from the
authors of , FragTrack performs the best because it is
specifically designed to handle occlusions via a part-based
model. However, on our similar, but more challenging clip,
“Occluded Face 2”, FragTrack performs poorly because it
cannot handle appearance changes well (e.g., when the
subject puts a hat on, or turns his face). This highlights the
advantages of using an adaptive appearance model.
4.2.3 Cola Can, Surfer
The cola can sequence contains a specular object, which
adds some difficulty. The “Surfer” clip was downloaded
from Youtube; this clip would be easier to track if color
information were used,4 but since we use grayscale images
BABENKO ET AL.: ROBUST OBJECT TRACKING WITH ONLINE MULTIPLE INSTANCE LEARNING
Fig. 7. Tracking Object Location: Screenshots of tracking results,
highlighting instances of out-of-plane rotation, occluding clutter, scale,
and illumination change. For the sake of clarity, we only show three
trackers per video clip. (a) Occluded face. (b) Occluded face 2.
(c) Surfer.
4. It would be straightforward to extend our system to use color—e.g.,
compute Haar features over color channels.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
Fig. 8. Tracking Object Location: Screenshots of tracking results, highlighting instances of out-of-plane rotation, occluding clutter, scale, and
illumination change. For the Tiger 2 clip, we also include close-up shots of the object to highlight the wide range of appearance changes. For the
sake of clarity, we only show three trackers per video clip. (a) Tiger 2. (b) Coupon book.
Fig. 9. Tracking Object Location and Scale: Average center location errors. See text for details.
Tracking Object Location and Scale: Location Mean Error
Bold green font indicates best performance, red italics font indicates second best.
for all experiments this clip is fairly challenging. Both
MILTrack and the SemiBoost tracker perform well on these
clips (cf. Fig. 5).
4.2.4 Tiger 1, Tiger 2
These sequences exhibit many challenges and contain
frequent occlusions and fast motion (which causes motion
blur). The two sequences show the toy tiger in many different
poses, and include out of plane rotations (cf. Fig. 8a). Our
algorithm outperforms the others, often by a large margin.
4.2.5 Coupon Book
This clip illustrates a problem that arises when the tracker
relies too heavily on the first frame. The appearance of the
BABENKO ET AL.: ROBUST OBJECT TRACKING WITH ONLINE MULTIPLE INSTANCE LEARNING
Fig. 10. Tracking Object Location and Scale: Screenshots showing results for tracking both location and scale of objects. Note that the localization
is much more precise when scale is one of the tracked parameters. (a) David indoors. (b) Snack bar. (c) Tea box.
coupon book is changed after about 50 frames by folding
one of its pages; then an “imposter” coupon book is
introduced to distract the trackers. MILTrack successfully
tracks the correct coupon book, while FragTrack and the
SemiBoost tracker are confused by the impostor object.
Tracking Object Location and Scale
Here we present results for both location and scale tracking.
Scale tracking is independent of the appearance model, so
our implementation of scale tracking for MILTrack is easily
carried over to the OAB tracker. Note that the quantitative
results we present are still based on object center location
only; we do not measure error of scale estimation. This
allows us to compare results of trackers that estimate scale
and those with a fixed scale. Furthermore, gathering ground
truth for object center is less time consuming than for a full
bounding box.
4.3.1 David Indoor
This is the same clip that we studied in the previous
section. Here we see a big advantage of using scale
tracking—MILTrack with scale performs better than MIL-
Track without scale, and it performs better than OAB(1)
with scale. However, the IVT tracker achieves the best
result on this video clip. We believe IVT is particularly well
suited to faces since it uses a subspace PCA appearance
model. We will see in the next experiments that IVT does
not work well in other scenarios.
Quantitative results are summarized in Tables 3 and 4,
and plots are shown in Figs. 9 and 11; Fig. 10 shows screen
4.3.2 Snack Bar
In this clip the goal is to track an object that changes in scale
and moves against a background that is very similar in
texture. We see that the IVT tracker fails in this case when
the object is turned upside down. The IVT tracker uses a
generative model, rather than discriminative, so it does not
take into account the negative examples from the image.
Because the background is so similar to the object of interest
in this video clip, IVT ultimately loses the object and snaps
to some part of the background. As before, we see that
MILTrack with scale performs better than MILTrack without scale and OAB(1) with scale; overall MILTrack achieves
the best performance on this clip.
4.3.3 Tea Box
This clip again shows the shortcomings of IVT—the clip
shows a box of tea which is moved around and rotated
(exposing new faces of the box). IVT fails when these out of
plane rotations take place (see Fig. 10c, frame #240 and
beyond). Though the center location error is similar for both
version of MILTrack (Fig. 9 ), we can see the version that
includes scale search results in more satisfactory results
(e.g., frame #134).
DISCUSSION/CONCLUSIONS
In this paper, we presented a novel way of updating an
adaptive appearance model of a tracking system. We
argued that using Multiple Instance Learning to train the
appearance classifier results in more robust tracking and
presented an online boosting algorithm for MIL. We
presented empirical results on many challenging video
clips where we measured quantitative performance of our
tracker compared to a number of competing state of the art
algorithms; these results show that our tracker is, on
average, the most robust with respect to partial occlusions,
and various appearance changes.
There are still some interesting unanswered questions
about adaptive appearance models. Although our method
results in more robust tracking, it cannot completely avoid
the types of problems that adaptive appearance trackers
suffer from. In particular, if an object is completely occluded
for a long period of time or if the object leaves the scene
completely, any tracker with an adaptive appearance model
will inevitably start learning from incorrect examples and
lose track of the object. Some interesting work exploring
ways to deal with this issue has been presented in and
more recently in . These methods attempt to combine a
pretrained object detector with an adaptively trained
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
AUGUST 2011
Fig. 11. Tracking Object Location & Scale: Precisions plots. See text for details.
Tracking Object Location and Scale: Precision at a Fixed Threshold of 20
Bold green font indicates best performance, red italics font indicates second best.
tracker. One interesting avenue for future work would be to
combine these ideas with the ones presented in this paper.
Another challenge is to track articulated objects which
cannot be easily delineated with a bounding box. These
types of objects may require a part-based approach, such as
the recent methods in object detection , .
Finally, online algorithms for Multiple Instance Learning
could be useful in areas outside of visual tracking. Work on
better algorithms and theoretical analysis relating offline/
batch MIL and online MIL is already under way (e.g., ),
and we suspect more is to come.
ACKNOWLEDGMENTS
The authors would like to thank Kristin Branson, Piotr Dolla´r,
DavidRoss,andtheanonymousreviewersforvaluableinput.
This research has been supported by US National Science
Foundation (NSF) CAREER Grant #0448615, NSF IGERT
Grant DGE-0333451, and US Office of Naval Research Grant
#N00014-08-1-0638. Ming-Hsuan Yang is supported in part
by a University of California Merced faculty start-up fund
and a Google faculty award. Part of this work was performed
while Boris Babenko and Ming-Hsuan Yang were at the
Honda Research Institute, USA.