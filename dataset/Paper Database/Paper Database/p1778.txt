Rotate to Attend: Convolutional Triplet Attention Module
Diganta Misra *
 
Trikay Nalamada *
Indian Institute of Technology, Guwahati
 
Ajay Uppili Arasanipalai *
University of Illinois, Urbana Champaign
 
National University of Singapore
 
capability
interdependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly
used in a variety of computer vision tasks recently.
this paper, we investigate light-weight but effective attention mechanisms and present triplet attention, a novel
method for computing attention weights by capturing crossdimension interaction using a three-branch structure. For
an input tensor, triplet attention builds inter-dimensional
dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial
information with negligible computational overhead. Our
method is simple as well as efﬁcient and can be easily
plugged into classic backbone networks as an add-on module.
We demonstrate the effectiveness of our method on
various challenging tasks including image classiﬁcation on
ImageNet-1k and object detection on MSCOCO and PAS-
CAL VOC datasets. Furthermore, we provide extensive insight into the performance of triplet attention by visually
inspecting the GradCAM and GradCAM++ results. The
empirical evaluation of our method supports our intuition
on the importance of capturing dependencies across dimensions when computing attention weights. Code for this
paper can be publicly accessed at 
com/LandskapeAI/triplet-attention
1. Introduction
Over the years of computer vision research, convolutional neural network architectures of increasing depth
have demonstrated major success in many computer vision tasks .
Numerous recent work
 have proposed using either channel at-
*Equal Contribution
Transformation
Transformation
Transformation
Figure 1. Abstract representation of triplet attention with three
branches capturing cross-dimension interaction. Given the input
tensor, triplet attention captures inter-dimensional dependencies
by rotating the input tensor followed by residual transformation.
tention, or spatial attention, or both to improve the performance of these neural networks. These attention mechanisms have the capabilities of improving the feature representations generated by standard convolutional layers by explicitly building dependencies among channels or weighted
spatial mask for spatial attention.
The intuition behind
learning attention weights is to allow the network to have
the ability to learn where to attend and further focus on the
target objects.
One of the most prominent methods is the squeezeand-excitation networks (SENet) . Squeeze and Excite
(SE) module computes channel attentions and provides incremental performance gains at a considerably low cost.
SENet was succeeded by Convolutional Block Attention
Module (CBAM) and Bottleneck Attention Module
(BAM) , both of which stressed on providing robust
representative attentions by incorporating spatial attention
along with channel attention.
They provided substantial
performance gains over their squeeze-and-excite counter-
 
part at a small computational overhead.
Different from the aforementioned attention approaches
that require a number of extra learnable parameters, the
foundation backbone of this paper is to investigate the way
of building cheap but effective attentions while maintaining similar or providing better performance.
In particular, we aim to stress on the importance of capturing crossdimension interaction while computing attention weights to
provide rich feature representations. We take inspiration
from the method of computing attention in CBAM 
which successfully demonstrated the importance of capturing spatial attention along with channel attention.
CBAM, the channel attention is computed in a similar way
as that of SENet except for the usage of global average
pooling (GAP) and global max pooling (GMP) while the
spatial attention is generated by simply reducing the input to
a single channel output to obtain the attention weights. We
observe that the channel attention method within CBAM
 although providing signiﬁcant performance improvements does not account for cross-dimension interaction
which we showcase to have a favorable impact on the performance when captured.
Additionally, CBAM incorporates dimensionality reduction while computing channel attention which is redundant to capture non-linear local dependencies between channels.
Based on the above observation, in this paper, we propose triplet attention which accounts for cross-dimension
interaction in an efﬁcient way. Triplet attention comprises
of three branches each responsible for capturing crossdimension between the spatial dimensions and channel dimension of the input. Given an input tensor with shape
(C × H × W), each branch is responsible for aggregating cross-dimensional interactive features between either
the spatial dimension H or W and the channel dimension
C. We achieve this by simply permuting the input tensors
in each branch and then passing the tensor through a Z-pool,
followed by a convolutional layer with kernel size of k × k.
The attention weights are then generated by a sigmoid activation layer and then is applied on the permuted input tensor
before permuting it back into the original input shape.
Compared to previous channel attention mechanisms
 , our approach offers two advantages.
First, our method helps in capturing rich discriminative
feature representations at a negligible computational overhead which we further empirically verify by visualizing the
Grad-CAM and Grad-CAM++ results. Second, unlike our predecessors, our method stresses the importance
of cross-dimension interaction with no dimensionality reduction, thus eliminating indirect correspondence between
channels and weights.
We showcase this way of computing attention in parallel across branches while accounting for cross-dimension
dependencies is extremely effective and cheap in computational terms.
For instance, for ResNet-50 with
25.557M parameters and 4.122 GFLOPs, our proposed
plug-in triplet attention results in an increase of parameters
by 4.8K and GFLOPs by 4.7e-2 respectively while providing a 2.28% improvement in Top-1 accuracy. We evaluate our method on ImageNet-1k classiﬁcation and object detection on PASCAL VOC and MS COCO 
while also providing extensive insight into the effectiveness
of our method by visualizing the Grad-CAM and Grad-
CAM++ outputs respectively.
2. Related Work
Attention in human perception relates to the process of
selectively concentrating on parts of the given information
while ignoring the rest.
This mechanism helps in reﬁning perceived information while retaining the context of it.
Over the last few years, several researched methods have
proposed to efﬁciently incorporate this attention mechanism
in deep convolution neural network (CNN) architectures to
improve performance on large-scale vision tasks. In the following part of this section, we will review some attention
mechanisms that are strongly related to this work.
Residual Attention Network proposes a trunk-andmask encoder-decoder style module to generate robust
three-dimensional attention maps. Due to the direct generation of 3D attention maps, the method is quite computationally complex as compared to the recently proposed methods
to compute attention. This was followed by the introduction
of Squeeze-and-Excitation Networks (SENet) which
as debated by many was the ﬁrst to successfully implement an efﬁcient way of computing channel attention while
providing signiﬁcant performance improvements. The aim
of SENet was to model the cross-channel relationships in
feature maps by learning per-channel modulation weights.
Succeeding SENet, Convolutional Block Attention Module
(CBAM) was proposed, in which they enrich the attention maps by adding max pooled features for the channel
attention along with an added spatial attention component.
This combination of spatial attention and channel attention
demonstrated substantial improvement in performance as
compared to SENet. More recently, Double Attention Networks (A2-Nets) introduced a novel relation function
for Non-Local (NL) blocks. NL blocks were introduced to capture long range dependencies via non-local operations and were designed to be lightweight and easy to
use in any architecture. Global Second order Pooling Networks (GSoP-Net) uses second-order pooling for richer
feature aggregation. The key idea is to gather important features from the entire input space using second order pooling
and subsequently distributing them to make it easier for further layers to recognize and propagate. Global-Context Networks (GC-Net) propose a novel NL-block integrated
with a SE block in which they aimed to combine contextual
Global Avg Pool
C/r × 1 × 1
C/r × 1 × 1
Conv + ReLU
Channel Pool
C/r × 1 × 1
Batch Norm
HW × 1 × 1
C/r × 1 × 1
C/r × 1 × 1
Batch Norm
Batch Norm
Channel Pool
Batch Norm
Figure 2. Comparisons with different attention modules: (a) Squeeze Excitation (SE) Module; (b) Convolutional Block Attention
Module (CBAM); (c) Global Context (GC) Module; (d) triplet attention (ours). The feature maps are denoted as feature dimensions, e.g.
C × H × W denotes a feature map with channel number C, height H and width W. ⊗represents matrix multiplication, ⊙denotes
broadcast element wise multiplication and ⊕denotes broadcast element-wise addition.
representations with channel weighting more efﬁciently. Instead of simple downsampling by GAP as in the case of
SENet , GC-Net uses a set of complex permutationbased operations to reduce the feature maps before passing
it to the SE block.
Attention mechanisms have also been successfully used
for image segmentation and ﬁne grained image classiﬁcation. Criss-Cross Networks (CCNet) and SPNet 
present novel attention blocks to capture rich contextual information using intersecting strips. Xiao et al. propose
a pipeline integrated with one bottom-up and two top-down
attention for ﬁne grained image classiﬁcation. Cao et al. 
introduce the ’Look and Think Twice’ mechanism which is
based on a computational feedback process inspired from
the human visual cortex which helps in capturing visual attention on target objects even in distorted background conditions.
Most of the above methods have signiﬁcant shortcomings which we address in our method. Our triplet attention module aims to capture cross-dimension interaction
and thus is able to provide signiﬁcant performance gains at a
justiﬁed negligible computational overhead as compared to
the above described methods where none of them account
for cross-dimension interaction while allowing some form
of dimensionality reduction which is unnecessary to capture
cross-channel interaction.
3. Proposed Method
In this section, we ﬁrst revisit CBAM and analytically diagnose the efﬁciency of the shared MLP structure within the channel attention module of CBAM. Subsequently, we propose our triplet attention module where we
demonstrate the importance of cross-dimension dependencies and further compare the complexity of our method with
other standard attention mechanisms. Finally, we conclude
by showcasing how to adapt triplet attention into standard
deep CNN architectures for different challenging tasks in
the domain of computer vision.
3.1. Revisiting Channel Attention in CBAM
We ﬁrst revisit the channel attention module used in
CBAM in this subsection. Let χ ∈RC×H×W be the
output of a convolutional layer and the subsequent input to
the channel attention module of CBAM where C, H and W
represent the channels of the tenor or the number of ﬁlters,
height, and width of the spatial feature maps, respectively.
The channel attention in CBAM can be represented by the
following equation:
ω = σ(f(W0,W1)(g(χ)) + f(W0,W1)(δ(χ)))
where ω ∈RC×1×1 represent the learnt channel attention
weights which are then applied to the input χ, g(χ) is the
global average pooling (GAP) function as formulated as follows:
and δ(χ) represents the global max pooling (GMP) function
written as:
δ(χ) = max
The above two pooling functions make up the two methods of spatial feature aggregation in CBAM. Symbol σ
represents the sigmoid activation function.
f(W0,W1)(g(χ)) and f(W0,W1)(δ(χ)) are two transformations.
Thus, after expanding f(W0,W1)(g(χ)) and
f(W0,W1)(δ(χ)), we have the following form of ω:
ω = σ(W1ReLU(W0g(χ)) + W1ReLU(W0δ(χ))) (4)
Input Tensor
Triplet Attention
Permutation
Figure 3. Illustration of the proposed triplet attention which has three branches. The top branch is responsible for computing attention
weights across the channel dimension C and the spatial dimension W. Similarly, the middle branch is responsible for channel dimension
C and spatial dimension H. The ﬁnal branch at the bottom is used to capture spatial dependencies (H and W). In the ﬁrst two branches, we
adopt rotation operation to build connections between the channel dimension and either one of the spatial dimension. Finally, the weights
are aggregated by simple averaging. More details can be found in Sec. 3.2
where ReLU represents the Rectiﬁed Linear Unit and W0
and W1 are weight matrices, the size of which are deﬁned
to be C × C
r ×C, respectively. Here, r represents the
reduction ratio in the bottleneck of the MLP network which
is responsible for dimensionality reduction. Larger r results
in lower computational complexity and vice versa. To note,
the weights of the MLP: W0 and W1 are shared in CBAM
for both the inputs: g(χ) and δ(χ). In Eq. (4), the channel descriptors are projected into a lower dimensional space
and then maps them back which causes loss in inter-channel
relation because of the indirect weight-channel correspondence.
3.2. Triplet Attention
As demonstrated in Sec. 1, the goal of this paper is to
investigate how to model cheap but effective channel attention while not involving any dimensionality reduction. In
this subsection, unlike CBAM and SENet , which
require a certain number of learnable parameters to build
inter-dependencies among channels, we present an almost
parameter-free attention mechanism to model channel attention and spatial attention, namely triplet attention.
Overview: The diagram of the proposed triplet attention
can be found in Fig. 3. As the name implies, triplet attention is made up of three parallel branches, two of which are
responsible for capturing cross-dimension interaction between the channel dimension C and either the spatial dimension H or W. The remaining ﬁnal branch is similar
to CBAM , used to build spatial attention. The outputs
from all three branches are aggregated using simple averaging. In the following, before speciﬁcally describing the
proposed triplet attention, we ﬁrst introduce the intuition of
building cross-dimension interaction.
Cross-Dimension Interaction: Traditional ways of computing channel attention involve computing a singular
weight, often a scalar for each channel in the input tensor
and then scaling these feature maps uniformly using the
singular weight. Though this process of computing channel attention has been proven to be extremely lightweight
and quite successful, there is a signiﬁcant missing piece in
considering this method. Usually, to compute these singular weights for channels, the input tensor is spatially decomposed to one pixel per channel by performing global average
pooling. This results in a major loss of spatial information
and thus the inter-dependence between the channel dimension and the spatial dimension is absent when computing
attention on these single pixel channels. CBAM introduced spatial attention as a complementary module to the
channel attention. In simple terms, the spatial attention tells
’where in the channel to focus’ and the channel attention
tells ’what channel to focus on’. However, the shortcoming in this process is that the channel attention and spatial
attention are segregated and computed independent of each
other. Thus, any relationship between the two is not considered. Motivated by the way of building spatial attention, we
present the concept of cross dimension interaction, which
addresses this shortcoming by capturing the interaction between the spatial dimensions and the channel dimension of
the input tensor. We introduce cross-dimension interaction
in triplet attention by dedicating three branches to capture
dependencies between the (C, H), (C, W) and (H, W) dimensions of the input tensor respectively.
Z-pool: The Z-pool layer here is responsible for reducing
the zeroth dimension of the tensor to two by concatenating
the average pooled and max pooled features across that dimension. This enables the layer to preserve a rich representation of the actual tensor while simultaneously shrinking
its depth to make further computation lightweight. Mathe-
matically, it can be represented by the following equation:
Z-pool(χ) = [MaxPool0d(χ), AvgPool0d(χ)],
where 0d is the 0th-dimension across which the max and
average pooling operations take place. For instance, the Z-
Pool of a tensor of shape (C × H × W) results in a tensor
of shape (2 × H × W).
Triplet Attention: Given the above deﬁned operations, we
deﬁne triplet attention as a three branched module which
takes in an input tensor and outputs a reﬁned tensor of the
same shape. Given an input tensor χ ∈RC×H×W , we ﬁrst
pass it to each of the three branches in the proposed triplet
attention module. In the ﬁrst branch, we build interactions
between the height dimension and the channel dimension.
To achieve so, the input χ is rotated 90◦anti-clockwise
along the H axis. This rotated tensor denoted as ˆ
the shape (W × H × C).
χ1 is then passed through Zpool and is subsequently reduced to ˆ
1 which is of shape
(2 × H × C). ˆ
1 is then passed through a standard convolutional layer of kernel size k × k followed by a batch normalization layer, which provides the intermediate output of
dimensions (1 × H × C). The resultant attention weights
are then generated by passing the tensor through a sigmoid
activation layer (σ). The attention weights generated are
subsequently applied to ˆ
χ1 and then rotated 90◦clockwise
along the H axis to retain the original input shape of χ.
Similarly, in the second branch, we rotate χ 90◦anticlockwise along the W axis. The rotated tensor ˆ
represented with dimension of (H × C × W) and is passed
through a Z-pool layer. Thus, the tensor is reduced to ˆ
the shape (2 × C × W). ˆ
2 is passed through a standard
convolutional layer deﬁned by kernel size k × k followed
by a batch normalization layer which outputs a tensor of
the shape (1 × C × W). The attention weights are then
obtained by passing this tensor through a sigmoid activation
layer (σ) which are then simply applied on ˆ
χ2 and the output
is subsequently rotated 90◦clockwise along the W axis to
retain the same shape as input χ.
For the ﬁnal branch, the channels of the input tensor
χ are reduced to two by Z-pool. This reduced tensor ˆ
of shape (2 × H × W) is then passed through a standard
convolution layer deﬁned by kernel size k followed by a
batch normalization layer. The output is passed through sigmoid activation layer (σ) to generate the attention weights
of shape (1 × H × W) which are then applied to the input
χ. The reﬁned tensors of shape (C × H × W) generated
by each of the three branches are then aggregated by simple
averaging.
Summarizing, the process to obtain the reﬁned attentionapplied tensor y from triplet attention for an input tensor
χ ∈RC×H×W can be represented by the following equa-
Attention Mechanism
Parameters
Overhead (ResNet-50)
2C2/r + 2k2
C/r(3C + 2k2C/r + 1)
Triplet Attention
Table 1. Comparisons of various attention modules based on their
parameter complexity and overhead using ResNet-50 backbone.
2))+χσ(ψ3( ˆ
χ3))), (6)
where σ represents the sigmoid activation function; ψ1, ψ2
and ψ3 represent the standard two-dimensional convolutional layers deﬁned by kernel size k in the three branches
of triplet attention. Simplifying Eq.(6), y becomes:
χ2ω2 + χω3) = 1
3(y1 + y2 + y3),
where ω1, ω2 and ω3 are the three cross-dimensional attention weights computed in triplet attention. The y1 and y2 in
Eq. (7) represents the 90◦clockwise rotation to retain the
original input shape of (C × H × W).
Complexity Analysis: In Tab. 1, we empirically verify
the parameter efﬁciency of triplet attention as compared
to other standard attention mechanisms. C represents the
number of input channels to the layer, r represents the reduction ratio used in the bottleneck of the MLP while computing the channel attention and the kernel size used for
2D convolution is represented by k; k ≪C. We show
that the parameter overhead brought along by different attention layers is much higher as compared to our method.
We calculate the overhead on a ResNet-50 by adding
the attention layers in each block while ﬁxing r to be 16. k
was ﬁxed at 7 for CBAM and triplet attention while for
BAM k was set to be 3. The reason for the lower overhead cost for BAM as compared to CBAM, GC and SE
 is because unlike the latter mentioned attention layers
being used in every block, BAM was used only three times
across the architecture in total according to the default setting for BAM.
4. Experiments
In this section, we provide the details for experiments
and results that demonstrate the performance and efﬁciency
of triplet attention, and compare it with previously proposed
attention mechanisms on several challenging computer vision tasks like ImageNet-1k classiﬁcation and object detection on PASCAL VOC and MS COCO datasets
using standard network architectures like ResNet-50 
and MobileNetV2 . To further validate our results, we
Parameters
ResNet 
ResNet-101
SENet 
Triplet Attention (Ours)
SENet 
GSoP-Net1 
A2-Nets 
Triplet Attention (Ours)
SENet 
ResNet-101
Triplet Attention (Ours)
MobileNetV2 
MobileNetV2
SENet 
Triplet Attention (Ours)
Table 2. Single-crop error rate (%) on the ImageNet validation set and complexity comparisons in terms of network parameters (in millions)
and ﬂoating point operations per second (FLOPs). Other than reporting results on heavy-weight ResNets, we also show results based on
light-weight mobile networks. With a negligible increase of learnable parameters, our approach works much better than the baselines and
is also comparable to the state-of-the-art methods that need large additional parameters and computations, like GSoP-Net1 .
provide the Grad-CAM and Grad-CAM++ results
for sample images to showcase the ability of triplet attention
to capture more deterministic feature-rich representations.
All ImageNet models were trained using 8 Nvidia Tesla
V100 GPUs, and all object detection models were trained
with 4 Nvidia Tesla P100 GPUs. We did not observe any
substantial difference in total wall time between the baseline
models and those augmented with triplet attention.
4.1. ImageNet
To train our ResNet based models, we add triplet
attention layers at the end of each bottleneck block. We
follow the exact training conﬁguration as for consistent and fair comparison with other methods. Similarly,
we follow the approach of to train our MobileNetV2based architecture.
Our results for the validated architectures are shown in
Tab. 2. Triplet attention is able to match or outperform
other similar techniques, while simultaneously introducing
the fewest number of additional model parameters.
A ResNet50-based model augmented with triplet attention achieves a 2.04% improvement in top-1 error rate on
ImageNet while only increasing the number of parameters by approximately 0.02% and increasing the FLOPs by
≈1%. The only comparable model that outperforms triplet
attention is GSoP-Net, which provides a 0.5% gain over
triplet attention at the cost of 10.7% more parameters and
53.6% more FLOPs.
We observe similar trend in performance in the smaller
ResNet-18 model where triplet attention provides a 0.5%
improvement in top-1 error rate while only increasing the
parametric complexity by 0.02%.
For ResNet-101 based models, triplet attention outperforms both vanilla and SENet variants by 0.66% and 0.41%,
Parameters
ResNet-50 
Faster R-CNN 
ResNet-101 
SENet-50 
ResNet-50 + CBAM 
ResNet-50 + Triplet Attention (Ours)
ResNet-50 
RetinaNet 
SENet-50 
ResNet-50 + CBAM 
ResNet-50 + Triplet Attention (Ours)
ResNet-50 
Mask RCNN 
SENet-50 
ResNet-50 + 1 NL block 
GCNet 
ResNet-50 + Triplet Attention (Ours)
Table 3. Object detection mAP(%) on the MS COCO validation set. Triplet Attention results in higher performance gain with minimal
computational overhead.
respectively. While SRM and CBAM were able to obtain marginally better results than triplet attention, our approach is still the lightest in terms of parameters.
With MobileNetV2, triplet attention provides a 0.98%
improvement in top-1 error rate on ImageNet while only
increasing parameters by approximately 0.03%. We also
observed that CBAM hurts model performance in case of a
MobileNetV2 where it drops accuracy by 1.71%. The experimental results demonstrate that the proposed triplet attention works well for both heavy and light-weight models
with a negligible increase in parameters and computations.
In the following subsection and supplementary materials,
we will show the effectiveness of our triplet attention module when applied to other vision tasks, like object detection,
instance segmentation, and human key-point detection.
4.2. PASCAL VOC
ResNet-50 
Faster R-CNN 
ResNet-50 + CBAM 
ResNet-50 + TA (Ours)
Table 4. Object detection mAP(%) on the PASCAL VOC 2012
test set. Triplet attention results in providing signiﬁcant improvement in performance with negligible overhead as compared to it’s
counterparts. TA represents Triplet Attention.
For object detection, we utilize our pre-trained ResNet-
50 model described in Sec. 4.1 in conjunction with Faster
R-CNN with FPN on the Pascal VOC dataset .
We adopt default training conﬁguration for the detectron2
toolkit to train a baseline ResNet-50 and ResNet-
50 with CBAM . For all models, we train on the 2007
and 2012 versions of the training set and validate on the
2007 validation set as described in .
The results can be found in Tab. 4. When compared to
the baseline model and its corresponding CBAM variant,
our triplet attention module is able to produce a distinct improvement in AP score, beating the baseline ResNet50 by
6.9%, and CBAM by 2.6% while having a backbone that
consumes fewer FLOPs and parameters.
4.3. MS COCO
As in Sec.
4.2, using the ImageNet models augmented with triplet attention as backbones, we train Faster-
RCNN , Mask-RCNN , and RetinaNet models to apply our attention module to object detection tasks
on the COCO dataset .
We use the training procedure described in ,implemented in the mmdetection framework , to ensure a fair test. Our results for the
COCO dataset results are summarized in Tab. 3. We observe that triplet attention outperforms most of the similar
layers, achieving a higher AP score in multiple categories.
Across all architectures, adding a triplet attention module
improves the AP score by over 2 points in AP over the
baseline model while using the same ImageNet backbone
described in Sec. 4.1 that adds a negligible computational
overhead. The improvement in performance observed in the
experiments showcase the beneﬁt of our cross-dimension
interaction strategy in triplet attention.
4.4. Ablation Study on Branches
We further validate the importance of cross-dimension
interaction by conducting ablation experiments to observe
Parameters
Top-1 Accuracy (%)
ResNet-32 
ResNet-32 + TA (channel off)
ResNet-32 + TA (spatial off)
ResNet-32 + TA (full)
VGG-16 + BN 
VGG-16 + BN + TA (channel off)
VGG-16 + BN + TA (spatial off)
VGG-16 + BN + TA (full)
MobileNet-v2 
MobileNet-v2 + TA (channel off)
MobileNet-v2 + TA (spatial off)
MobileNet-v2 + TA (full)
Table 5. Effect of different branches in triplet attention on performance in CIFAR-10 classiﬁcation.
the impact of the branches in the triplet attention module. In
Tab. 5, spatial off indicates that the third branch, where the
input tensor is not permuted, is switched off, and channel
off indicates that the two branches, which involve permutations of the input tensor, are switched off. As shown, the
results support our intuition with triplet attention having all
three branches switched on, denoted as full, to be performing consistently better than the vanilla version and its two
counterparts.
4.5. Grad-CAM Visualization
We hypothesize that the cross-dimensional interaction
provided by triplet attention helps the network learn more
meaningful internal representations of the image. To validate this claim, we provide sample visualizations from the
Grad-CAM and Grad-CAM++ techniques, which
visualize the gradients of the top-class prediction with respect to the input image as a colored overlay. As shown in
Fig. 4, triplet attention is able to capture tighter and more
relevant bounds on images from the ImageNet dataset .
In certain cases, when using triplet attention, a ResNet50 is
able to identify classes that the baseline model fails at predicting correctly. More Grad-CAM based results are presented in the supplementary section.
The visualizations support our understanding of the intrinsic capability of triplet attention to capture richer and
more discriminative contextual information for a particular
target class. This property of triplet attention is extremely
favorable and helpful in improving the performance of deep
neural network architectures as compared to their baseline
counterparts.
5. Conclusion
In this work, we propose a novel attention layer, triplet
attention, which captures the importance of features across
dimensions in a tensor. Triplet attention uses an efﬁcient attention computation method that does not have any information bottlenecks. Our experiments demonstrate that triplet
GradCAM ++
G.T. - Husky
G.T. - Warplane
Vanilla ResNet-50
Predicted Label – Husky
Confidence Score – 56.89%
ResNet-50 + CBAM
Predicted Label – Husky
Confidence Score – 65.02%
ResNet-50 + Triplet Attention
Predicted Label – Husky
Confidence Score – 82.21%
Vanilla ResNet-50
Predicted Label – projectile, missile
Confidence Score – 46.51%
: Incorrect Prediction
ResNet-50 + CBAM
Predicted Label – Warplane
Confidence Score – 82.49%
ResNet-50 + Triplet Attention
Predicted Label – Warplane
Confidence Score – 90.16%
Figure 4. Visualization of Grad-CAM and Grad-CAM++ results. The results were obtained for two random samples from
the ImageNet validation set and were compared for a baseline
ResNet-50, ResNet-50 + CBAM and a ResNet-50 + triplet attention. Ground truth (G.T) labels for the images are provided below
the original samples and the networks prediction and conﬁdence
scores are provided in the corresponding boxes.
attention improves the baseline performance of architectures like ResNet and MobileNet on tasks like image classiﬁcation on ImageNet and object detection on MS COCO,
while only introducing a minimal computational overhead.
We expect that other novel and robust techniques of capturing cross-dimension dependencies when computing attention may improve upon our results while reducing cost.
In the future, we plan to investigate the effects of adding
triplet attention to more sophisticated architectures like Ef-
ﬁcientNets and extend our intuition in the domain of
3D vision.
6. Acknowledgements
The authors would like to offer sincere gratitude to everyone who supported during the timeline of this project including Himanshu Arora from Montreal Institute for Learning Algorithms (MILA), Jaegul Choo and Sanghun Jung
from Korea Advanced Institute of Science and Technology
(KAIST). This work utilizes resources supported by the National Science Foundation’s Major Research Instrumentation program , grant #1725729, as well as the University of Illinois at Urbana-Champaign.