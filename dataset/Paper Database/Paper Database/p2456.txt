Government Information Quarterly 39 101666
Available online 30 December 2021
0740-624X/© 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( 
The perils and pitfalls of explainable AI: Strategies for explaining
algorithmic decision-making
Hans de Bruijn, Martijn Warnier, Marijn Janssen *
Delft University of Technology, Delft, the Netherlands
A R T I C L E I N F O
Artificial intelligence
Algorithms
Computational intelligence
Data-driven decision
Socio-tech
Transparency
Accountability
E-government
A B S T R A C T
Governments look at explainable artificial intelligence's (XAI) potential to tackle the criticisms of the opaqueness
of algorithmic decision-making with AI. Although XAI is appealing as a solution for automated decisions, the
wicked nature of the challenges governments face complicates the use of XAI. Wickedness means that the facts
that define a problem are ambiguous and that there is no consensus on the normative criteria for solving this
problem. In such a situation, the use of algorithms can result in distrust. Whereas there is much research
advancing XAI technology, the focus of this paper is on strategies for explainability. Three illustrative cases are
used to show that explainable, data-driven decisions are often not perceived as objective by the public. The
context might raise strong incentives to contest and distrust the explanation of AI, and as a consequence, fierce
resistance from society is encountered. To overcome the inherent problems of XAI, decisions-specific strategies
are proposed to lead to societal acceptance of AI-based decisions. We suggest strategies to embrace explainable
decisions and processes, co-create decisions with societal actors, move away from an instrumental to an insti­
tutional approach, use competing and value-sensitive algorithms, and mobilize the tacit knowledge of
professionals
1. Introduction
Public organizations increasingly use artificial intelligence (AI) for
automating and supporting their decision-making, and there has been a
steady increase in publications on this topic or automate existing ones. The deployment of new electronic
services would likely increase government effectiveness and efficiency
 . But also other public values like
accountability, transparency, equality, privacy and security, sustain­
ability, and interoperability should be given attention when designing
AI for public use .
The use of AI encounters many challenges .
The decisions made by autonomous computational algorithms can
severely impact both individuals and organizations and influence the power balance between govern­
ments, businesses, and citizens. As algorithms become increasingly
autonomous and invisible, it is becoming harder to see and explain them
 . An algorithmic society might be too opaque to
be accountable for its behavior .
What decisions computational algorithms make, based on what infor­
mation and how they make these decisions, should be explained to the
XAI has the potential to explain the working of AI to the general
public. Although there has been much research on AI in the public sector
context, explainable artificial intelligence (XAI) has been given less
attention. XAI is a field based on the idea that advice given by expert
systems would be more acceptable to humans if the advice could be
explained to them . XAI contrasts with opaque, black-box approaches that often
cannot explain where a decision comes from or how it is justified. The
more complex AI models are built, the more accurate these are, but the
explainability of their working might be lost . In this
paper, XAI is defined as the extent to which AI outcomes are insightful for
the general public.
Explainability is an intuitively appealing concept but is hard to
realize. Belle and Papantonis provide four suggestions for
creating explainability, including explanation by simplification,
describing the contribution of each feature to the decisions, explaining
* Corresponding author.
E-mail address: (M. Janssen).
Contents lists available at ScienceDirect
Government Information Quarterly
journal homepage: www.elsevier.com/locate/govinf
 
Received 7 March 2021; Received in revised form 5 December 2021; Accepted 19 December 2021
Government Information Quarterly 39 101666
an instance instead of in general, and using graphical visualization
methods for explanations. At the same time, they also discuss the
complexity of realizing such suggestions. Simplifications might not be
correct, features can be interrelated, local explanations can fail to pro­
vide the complete picture, and graphical visualization requires as­
sumptions about data that might not necessarily be true.
Explainability is assumed to create transparency and trust in AI.
Although trust might be affected in different ways than expected, situ­
ational factors also affect trust . Trans­
parency can both increase or decrease trust . In a similar vein, XAI might either increase or decrease trust.
Hence the nature of explainability should be understood better, and
strategies are needed for creating trust in XAI.
In this paper, we derive the challenges of XAI and develop strategies
for overcoming these challenges. This paper is structured as follows. In
Section 2, we provide an overview of the concept of XAI followed by
three illustrative case studies that demonstrate the challenges of XAI in
Section 3. Section 4 explains why XAI is so challenging to realize and use
in government. In Section 5, the relationships between XAI, trans­
parency and trust are presented, followed by a proposal for strategies on
how to deal with explainability in algorithmic decision-making. Finally,
in Section 6, conclusions are drawn.
2. Explainable AI literature
The XAI research field has its origin in the early 90s in the field of
expert systems . Pioneers such as Swartout and Moore
reasoned that advice-giving by expert systems would be more acceptable
to humans if the expert system could explain why it gave a particular
advice . Expert systems
are based on a large collection of rules that try to capture the knowledge
of an expert. Hence the name ‘expert system’ is used. Rules are typically
described in the form of implications, from which new conclusions can
be derived if specific premises hold. An explanation consists of a trace of
the application of rules with conforming conclusions and premises. An
explanation looks like “the system came to this diagnosis because it
applied these rules in this order to these initial symptoms, thereby
concluding that the patient has this sickness”. Such explanations, named
trace explanations, were the first type of explanations.
At a later stage, more explanation types emerged, including justifi­
cation, strategy, and terminological explanations . In essence, these techniques are trace explanations enriched with
more domain knowledge. In this way, explanations become easier to
interpret. For example, by explaining domain-specific terminology or
adding justifications to the rules used by the expert system. Different
explanation types are typically combined to explain the outcome of
expert systems. Similar techniques are used for explaining the behavior
of other systems, including training systems , medical
support systems , legal
support systems , and educational systems
 .
All the above systems have in common is that they are based on an
underlying symbolic representation. Although intended for processing by
machines, symbolic systems use languages (symbols), which are un­
derstandable by humans and that humans can use to verify the
reasoning. For decision-making, the logic to arrive at a decision is
simulating human reasoning, for instance, through the rules “if .. then ..
“. Such rules are often in the form of computer code built and understood
by people. Furthermore, while an expert may understand symbolic
systems' logic, the logic might not be easy to understand for non-experts
 . Therefore,
the focus of XAI is often on interpreting whether the results are correct.
Hence, instead of XAI, sometimes the term interpretable machine learning
is used to explain and present model behavior in understandable terms
to humans .
In contrast to symbolic systems, non-symbolic systems, including
popular machine learning models such as deep learning, arrive at de­
cisions by connecting the inputs with the outputs . Such systems produce the correct answer without
executing the logic to arrive at this answer. The logic cannot be grasped
directly by humans. These systems are not directly programmed and
constructed by humans, but rather are sophisticated statistical models
that ‘learn’ by being trained on large amounts of data using machine
learning techniques, such as neural networks or deep learning . The internal representation of these non-symbolic AI systems
does not contain a collection of human-readable rules but instead a
collection of non-linear correlations. Whereas the logic in symbolic
systems is verifiable by experts, non-symbolic systems are more
cumbersome, as translation steps are required to make them under­
standable to humans. Therefore, only post-hoc analysis can be con­
ducted to verify the results. Many of these algorithms are continuously
trained with new data and learn from their own decisions . The continuous training or self-learning of algorithms
will also influence the explanations. Hence, explanations also need to be
continuously updated.
XAI envisions the construction of a symbolic, human-understandable
model automatically from the non-symbolic, statistical machine-learned
model. This would make the model interpretable, and hence, it would be
relatively straightforward to explain the outcomes of the system. There
is a large body of academic literature on the interpretation and expla­
nation of machine-learned models . Techniques used for explaining machine-learned models range
from relatively straightforward sensitivity analysis to highly complex machine learning techniques, such as Taylor
decomposition 
and layer-wise relevance propagation . Advanced
models built to explain machine learning models' outcomes deploy
machine learning techniques themselves. The explanations generated by
such models are typically not easy to understand and require interpre­
tation by experts . As a consequence, the explanations,
at least to some extent, are open to interpretation. The more complex the
situation, the more challenging it is to explain the results.
Some researchers have distinguished between model-centric expla­
nations (an explanation of the AI model itself, as focused on above) for
general information-sharing and broader accountability purposes; and
more subject-centric explanations (explanations of how a particular
decision has impacted a particular individual or group). An example of
the latter are so-called recourse algorithms that look at the possible
harm caused by decisions and reverse such decisions. If harmful out­
comes are generated in a range of counterfactual scenarios, then these
need to be reversed. Such an algorithm generates candidate changes to
the variables that would reverse an algorithm's decision . In this way, the harmful outcomes of AI-
based decisions are removed. Note that, while a promising research
direction, such techniques still need to be developed further in order to
be useful in practice and deployed on a large scale.
Based on the state of the art literature, we can conclude that XAI is a
promising research field that is quickly developing, but that there are
not yet widely available techniques that can be easily deployed to pro­
vide unambiguous explanations of the underlying AI models. This is
especially true for the non-symbolic machine learning-based models,
that remain hard to explain and interpret, especially to non-experts.
Due to the outlined challenges to understanding an explanation, the
term “meaningful” is sometimes included in practice . Meaningful refers to systems providing explanations that are
understandable to individual users. As there are many types of users,
different explanations might be required. What meaningful explain­
ability looks like will likely depend on the complexity of the context in
which AI will be used, the type of data used, the intent and purpose for
its use, and whom it should be explained to. To understand this prospect
better, three illustrative case studies are presented in the next section.
H. de Bruijn et al.
Government Information Quarterly 39 101666
3. The complexity of XAI in practice
The three cases were selected because they are illustrative of
different failures to explain AI-based decisions. In this paper, we take
three prominent cases situated in three different countries and use them
to analyze and illustrate the challenges of XAI in the next section. The
three cases are selected because they are illustrative of the failure to
explain AI-based decisions.
3.1. Risks analysis of fraud in the social domain in the Netherlands
System Risk Identification (SyRI) is a fraud detection system based
on the integration of personal data from several databases controlled
and maintained by public agencies in the Netherlands . This project dismantles the traditional silos in
which multiple agencies store data. Almost any kind of data was allowed
to be shared for the broad goal of detecting fraud using AI. By connecting
all governmental data about citizens, potentially anybody became the
subject of this type of analysis. Due to SyRI, entire neighborhoods were
identified as potentially fraudulent. Black-boxed AI models were used to
determine who does or does not come into view of the enforcement and
investigation services, e.g., neighborhoods with expensive cars and low
incomes. Although the motive of fraud detection is not contested by
society, the use of AI to carry out such detection can be problematic.
Even though explaining how data and algorithms are used to select
certain groups is possible, society contested the approach. Anybody can
be of interest to the investigators, which violates the principle that
people are considered innocent until proven guilty.
3.2. Immigration in the UK
In the UK, AI is used to make decisions on whether immigrants are
allowed to enter the country . Although the final de­
cision remains in the hand of the civil servants, AI is used to determine
which cases require more scrutiny. Questions started to be asked about
how AI affects the immigration policy and the rights of immigrants. One
of the reasons for such questions is the fear that people belonging to
different AI-created groups will be treated in different ways. This could
result in “fast lanes” that would lead to “speedy boarding for white
people” . Although this might not be viewed as a
racial bias by some, the idea of having persons added to a group in which
their chance of getting a visa differs from that of other groups is against
the expectation of non-discrimination and equal treatment. This chal­
lenges the public values of equal opportunity and non-discrimination.
The counter-argument used is that AI does not make decisions, and
the final decision is in the hands of the human caseworkers. Although
such caseworkers should consider the outcomes of AI as a suggestion,
their own critical thinking might be affected or reduced. They might not
even have the abilities or be given the freedom to contest the outcomes
of the AI-generated decision.
3.3. Re-offending of criminals
In the USA, thousands of court cases are fed into algorithms to pre­
dict whether a defendant will commit a new crime or fail to return to
court. As with the previous cases, the projects were initiated by the
government, however in this case the tool was developed by a com­
mercial company with the aim to give judges the most objective infor­
mation available to make decisions about prisoners' risk of re-offending
 . Defendants are given a risk score that is presented to the
judge. Judges use these risk scores to make pre-trial decisions on de­
fendants. Some persons have praised the system for ensuring that
dangerous persons are kept in jail, whereas harmless persons go free.
Besides, the system does not use race, gender, employment, or living
place to avoid decision bias based on race, gender, or appearance and to
arrive at more objective outcomes. Nevertheless, a study showed that
the system predicted that black defendants pose a higher risk of recid­
ivism . Although race is not
included in the data fed to the AI system, racial bias is introduced by
utilizing other data sources. Similar to the previous case, the system was
also criticized for substituting and removing the judges' critical thinking
and nudging them towards biased recommendations. The judges can
ignore the AI-generated recommendations if they consider them wrong,
but they should justify their decision to deviate instead of that the AI-
generated decision is motivated. In addition, the ability of the AI sys­
tem to learn from its own decisions has both pros and cons. By learning
from their own mistakes, these mistakes can be prevented in the future.
At the same time, this raises the question if the AI system is sufficiently
mature for taking into production and use. Finally, the algorithm used in
this case is proprietary and not open for scrutiny by the public.
All three cases aim to address societal problems, reduce civil servant
bias, and make more objective decisions by using data and algorithms.
Despite that the working of the algorithms could be explained, up to a
point, in the first two cases, AI may result in outcomes that do not meet
the commonly accepted public values of non-discrimination, equal
treatment and, judges' independence. Accurate predicting is never fully
possible using these types of algorithms. Even if the results can be
explained, the outcomes can be contested and mistrusted. Although the
decisions might be perceived as objective, they are objected by society.
4. Challenges of XAI
XAI is an intuitively appealing concept, as explanations are some­
thing that is desired. However, society's norms and values can be
translated differently and what is acceptable differs between societies.
Even within a single society, norms might be different. That is why
deliberation is deemed to be necessary for policy-making . Policy-making is a process that focuses on reaching a consensus
about the norms. Although the reasons why explainable AI does not
fulfill its intended purpose are intertwined, we describe these reasons
using the following seven main challenges of XAI summarized in
Table 1. The challenges are multi-faceted and intertwined.
The first objection against XAI is obvious. XAI often focuses on
ensuring that solutions are understood by the public . XAI assumes a certain level of expertise
of the public, however, many persons will simply lack that expertise and
cannot assess whether an AI-based decision is fair or just . Tools and instruments can be used to abstract and explain de­
cisions, but this results in a further deviation of how the actual AI-based
decision is taken, and the simplification might be contestable 101666
Papantonis, 2021). Besides, XAI is not about making a single decision
but about a large number of decisions. The emerging ‘fast lane’ in the
immigration case study shows that although a single decision might be
accepted, but the overall outcome is contested. Whereas an individual
case can be explained, the overall effect cannot.
Second, explaining AI-based decisions is not a neutral process. For
example, in the re-offending case, data about use race, gender,
employment, and living place are left out to avoid bias, racial profiling,
and reinforcing historical discrimination. Nevertheless, this resulted in
false objectivity as these characteristics can be reflected by other vari­
ables. Decisions are based on a complex interaction between data and
possibly multiple algorithms, and in the process of deriving the con­
ceptual ‘translation’ that a broader audience can understand, the
translator also makes inherently disputable (biased) choices. For
instance, political preferences might be reflected by the explanation. In
turn, others might arrive at alternative explanations, and the explana­
tion given might be contested. Furthermore, no single explanation
would do the work for all decisions made by an algorithm.
The third reason is that the algorithm is all too often not static. Al­
gorithms learn from their own decisions and by incorporating new data
 . Hence algorithms are dynamic and change
over time. The learning part can be contested. In the re-offending case,
the argument was that the AI system would learn and improve from its
mistakes. At the same time, this argument suggests that AI does not work
properly and makes mistakes. This raises the question of whether it is
fair to use the AI system at all. The more dynamic an algorithm's context
is, the more challenging XAI will be. These dynamics result in today's
explanation becoming obsolete tomorrow. In addition, it might even be
unclear for experts what exactly changed. You cannot explain what you
do not know.
The fourth reason is that algorithms interfere with each other. A
decision made by an algorithm is influenced by the, inherently biased,
data collected from different sources , by the other (learning) algorithms , and by the context where the algorithm is
deployed . This is shown, for example, in the case of
risk analysis in the social domain. Adding more data and using a variety
of algorithms hinders explainability and might introduce bias or racial
profiling, as happened in the re-offending case. Furthermore, the social
domain typically depends on the tacit knowledge of the situation at
hand. Tacit knowledge is not codified and therefore cannot be taken into
account by an algorithm. The variety of algorithms hampers explain­
ability – for the same reason as under the third objection: the inference is
neither knowable nor explainable.
Fifth, algorithmic decisions might have different consequences for
different individuals. In some cases, XAI will only be meaningful if the
AI-based decision can be explained at an individual level. Put differ­
ently, the explanation of AI is very much context-dependent . This makes it hard to explain the working of the algorithms
in general and to explain their different outcomes. Transparency might
be meaningless if algorithms make decisions that influence an in­
dividual's life . Furthermore, equal treatment is an
important public value, but individuals are different and are treated
differently. Individualized XAI means that the four previous objections
manifest in almost infinite variations. Take the re-offending case. If a
defendant wants to know how the algorithm makes a decision, it is hard
to compare that decision with that of others , as their situation differs.
The logic to compare with other cases is unclear. Furthermore, seg­
mentation between criminals and non-criminals is fair for society, but a
segmentation between low and high incomes is not considered fair. In
the risk analysis in the social domain, the United Nations (UN) com­
mented on the use of AI for the segmentation stating that treating poor
people differently from the rich is not acceptable . Yet, at
the heart of many AI applications is some kind of segmentation
algorithm.
The sixth reason is that algorithms are often used to address so-called
wicked problems in which traditional rule-based systems are not
appropriate or easy to use, and algorithms search for the underlying,
non-visible patterns . Problems are wicked
because the relevant facts are ambiguous, and the actors involved
disagree on the question of how to value a problem morally. Wickedness
implies that there is no single right problem definition and no single
right solution. Explaining using the traditional cause-effect relationship
is not applicable to wicked problems and, as
such, cannot be used for explaining them. Furthermore, there might be
many solutions to deal with these types of problems, and the algorithms
might only provide one possible solution. Again, this results in con­
testing the outcomes. Furthermore, these ‘solutions’ typically change
over time for wicked problems . Per definition, it
is impossible to explain something ambiguous.
Finally, The ability to explain the causality behind an AI-based de­
cision does not mean that the AI system actually uses this causality and
that the actual relationship between inputs and outputs might be
different. In all three cases, the AI systems were black-boxed systems
whose inner working was opaque. Even if we could ‘explain’ an algo­
rithm – how do we know (can we verify) that this is the same one that is
actually used? This can only be verified in practice by having regular
audits and inspecting how the algorithm works. Different explanations
might be needed for the same algorithms over time. For the public,
changing explanations over time is neither understandable nor
acceptable.
5. Trust in explainable AI
XAI should be conducive to the trust and societal acceptance of AI-
based decisions. XAI should result in legitimate (unbiased) decisions
and trust in the effectiveness and fairness of such decisions. XAI is aimed
at creating transparent and clear decision-making processes and de­
cisions, which in turn should result in trust in decisions. However, the
relationship between transparency and trust is often more complicated
than assumed . Transparency might in­
fluence trust in such a way that consumers do not purchase, donors do
not give, shareholders do not invest, or governments are not trusted
 . As such, even the best XAI might not result in the
desired trust. People who do not know how AI works will not have
sufficient confidence in it and thus will not use or approve its outcomes.
In addition to these limitations of XAI, there is the societal context in
which algorithms are used. This context can also have a major impact on
the explainability of AI. Finally, the possible societal impact influences
the explainability. The content and impact can be used for classifying the
situations for deploying XAI, as shown in Fig. 1.
• The context of using algorithms might be less or more politicized. Politi­
cization refers to the level of possible conflict and the chance of
having contested outcomes. Contested outcomes refer to issues in
which the public has opposing opinions, like abortion and immi­
gration, often dependent on their political preference. Therefore, the
decisions made by AI are also politicized and show the preference of
those in power.
- The impact of an AI-based decision might also be high or low. For
example, AI-based decisions on law enforcement priorities are
potentially high-impact decisions, determining whether citizens will
be faced with the police. Other decisions, like a fine for speeding,
might have a low impact.
When having high levels of politicization and high impact (Quadrant
II), the public will be very critical of endeavors to explain AI – the AI-
based decision has a high impact and is operating in a highly politi­
cized context. That means that XAI will likely be conflict-generating, and
parties will have a different view on what constitutes the right decision
and if a decision is perceived as fair. The latter might depend on their
political preference. The public will be interested in the AI, underlying
H. de Bruijn et al.
Government Information Quarterly 39 101666
the decision that affects them, and will likely distrust the explanation.
They will always find an argument for their distrust, given the limita­
tions of XAI listed in the preceding section and their political
preferences.
The opposite situation can be found when there is a low level of
politicization and low impact (Quadrant III). Likely, this means that
people will not be interested in an explanation of the AI-based decision.
Decisions simply do not matter and have limited consequences. These
types of decisions might be relatively simple, and black-boxed AI solu­
tions might not be needed. Rule-based systems that are easy to explain
are sufficient, and XAI might not be needed at all.
When there is a highly politicized environment, even an AI-based
decision with a low impact can be perceived as problematic (Quadrant
IV). The public might be suspicious of every decision and question the
outcomes. Due to the contestable nature of the decisions, the decisions
might be perceived as having a high impact, which results in a shift
towards high interest and low trust in AI. The more politicization, the
higher the chance that a decision will be perceived as having a high
impact. Hence, this type of decision might shift to the second quadrant.
Finally, in a low politicized context having a high impact (Quadrant
I), AI-based decision outcomes will probably be accepted more easily
than in a highly politicized context. However, a difference between
decisions having a positive impact (e.g., a firm receiving a subsidy, an
immigrant-receiving a residence permit) and a negative impact (e.g., the
subsidy or the permit are denied) need to be made. If a high-impact
decision is negative, the person affected negatively might be inclined
to file a complaint and even politicize the decision by stating that there is
something wrong with the subsidy policy or that the system is broken. In
summary, when the impact of a decision is high, negative decisions carry
incentives for politicization.
In summary, XAI is always applied in a specific context – and this
context might have strong incentives to contest and distrust the expla­
nation of AI, irrespective of XAI's quality. If the level of politicization is
high, the trust in XAI will probably be low, even for low-impact de­
cisions. If the impact of a decision is high, there might be incentives to
politicize the issue and to challenge XAI. All three cases presented in this
paper can be positioned in the high politicized and high impact quadrant
(II), although their initial position might be in other quadrants. Due to
the outcomes' possible negative or high impact, their position was
shifted to the high politicized and impact quadrant. Strategies for trusted
XAI need to take into account the high politicized and high impact
6. Strategies for trusted XAI
XAI has fundamental limitations for use in complex situations and
will not always provide an acceptable explanation or improve trust.
Hence we derive a series of strategies for policy-makers that might
contribute to more legitimacy and trust . The strategies are presented in a ‘from, to’ frame to show the shift
in emphasis needed. The strategies can be both an alternative, or an
addition, to XAI. The strategies suggest a shift away from an instru­
mentation approach focused on the AI algorithm towards an approach
focused on creating legitimacy and trust in decisions. There is no one-to-
one connection between the challenges and the strategies. The strategies
do not solve the challenges necessarily, as much is dependent on how
these are realized. Furthermore, the strategies should be used in concert.
6.1. Strategy 1: From explaining AI to explaining decisions produced
We could shift our attention from explaining AI to explaining the
decision supported by AI. A decision might be fully or partially based on
AI, but in any case, decision-makers should be able to explain why a
decision has been made. When decision-makers have this burden of
proof, there might be an incentive to scrutinize the algorithms used or
deviate from AI-based decision-making critically. It might make them
decide not to rely on AI or on a particular type of AI exclusively.
Take the example of re-offending criminals. A judge must always
argue which considerations have led to the verdict - in a way that is
transparent to the litigating parties and to society. This obligation to
explain the final decision can be an incentive to reflect on the role of the
algorithm critically. After all, the mere fact that an algorithm has a
specific outcome is not sufficient to argue a decision - either what the
algorithm does is properly explained, or the outcome of the algorithm is
wholly or partially ignored.
6.2. Strategy 2: From designing algorithms to negotiated algorithms
In some cases, algorithms can be more authoritative if they are not
designed by experts only. Instead, an approach of co-creation with the
public and interested parties can be taken. The main choices algorithms
are based upon can be discussed and published. The parties involved can
try to find consensus about, for example, the variables that are taken into
account by an algorithm or the scope of an algorithm – what decisions an
Fig. 1. Contextualizing XAI.
H. de Bruijn et al.
Government Information Quarterly 39 101666
algorithm should make and should not make and how humans should
remain in control. A process like this results in negotiated algorithms in
which every stakeholder has its say, and a consensus needs to be
6.3. Strategy 3: From explainable algorithms to explainable processes
Closely related to the idea that algorithms can be explained is that
the design process also can be explained. Transparency then refers to
questions like who will be involved, who will have what role, what are
the main issues that will be debated, how will parties deal with dissensus
and uncertainties, how will they make their decisions. Not all discus­
sions need to be documented in detail, but only the relevant processes
that lead to decisions and the argumentation why decisions were taken.
The attention shifts from making algorithms explainable to making the
process of creating algorithms explainable.
The case study on re-offending criminals can serve as an example. A
great deal of scientific expertise is available on crime, recidivism, con­
ditions under which recidivism occurs, bias in detection and sentencing,
etc. When various experts are involved in the development of an algo­
rithm, this can contribute to a “negotiated” algorithm (strategy 2) and
more awareness of the limitations and risks of an algorithm. A metaphor
for the latter is the instruction leaflet for a medicine. Experts can be
involved in developing algorithms and drawing up a leaflet for an al­
gorithm: an overview of the risks and instructions on how the algorithm
should or should not be used. If an algorithm is negotiated, the next step
might be to design the process of negotiations (strategy 3). The trans­
parency of this process can contribute to the acceptance of the negoti­
ated algorithm.
6.4. Strategy 4: From an instrumental to an institutional approach
The value of XAI is often translated into tools or instruments that are
conducive to more transparency. However, transparency also requires
an institutional approach – the development of ‘rules of the game’ for
dealing with AI. Institution-building can comprise setting up organiza­
tional structures that facilitate the development of these rules of the
game. Examples are the establishment of regulators with authority to
scrutinize and audit algorithms and to develop regulation or, within
organizations, review committees that are positioned as countervailing
powers of developers and users of algorithms.
Consider the case of risk analysis of fraud with social benefits. The
world of social security has a well-developed institutional structure.
There are, for example, professional benefit agencies, client interest
groups, scientists and experts, and there is social advocacy. The higher
the degree of institutionalization, the easier it is to design an institu­
tional structure that can be used as a countervailing power in the design
and use of algorithms. A continuous critical look from this counter­
vailing power can be conducive to the right use of algorithms.
6.5. Strategy 5: From monopolistic algorithms and datasets to competing
algorithms and datasets
Using a metaphor from the world of economics, organizations often
employ monopolistic algorithms and monopolistic datasets to a lesser
extent. They develop one algorithm or one family of algorithms and use
these algorithms to base their decisions upon. The transparency of AI-
based decision-making can be enhanced by deliberately using
competing algorithms and datasets. Only if competing algorithms that
are trained on independently collected datasets result in more or less the
same decision, it might be reasonable to assume that this is a correct
decision. If competing algorithms provide different decisions, a human
decision-maker should take over. Although simultaneous failure of
independently-built and -operating algorithms is less likely, a false
feeling of trust might be created as multiple algorithms and data might
all be wrong. Suppose courts use competing algorithms and these
different algorithms result in different outcomes. This will probably be
conducive to a critical reflection on the algorithms.
6.6. Strategy 6: From algorithms to value-sensitive algorithms
AI-based decision-making can reinforce deeply rooted biases, and
therefore result in morally wrong decisions. When designing algorithms,
the parties involved can take certain key values into account. One should
aim to design the algorithm in such a way that data that might result in
biases or discrimination (e.g., age, gender, race) is ignored and verify
whether these undesired variables have an impact on the proposed de­
cisions of the algorithm . Furthermore, tests can be
conducted if humans are treated in the same manner. This ‘value sen­
sitive’ design of algorithms incentivizes the parties
involved to be transparent about what values they want to safeguard and
how these values are guaranteed.
6.7. Strategy 7: From algorithms replacing professional decision-making
to professionals challenging algorithmic decision-making
There is a classic tension between analytical decision-making based
on facts and figures, and intuitive decision-making of professionals
based on their tacit knowledge. Both types of decision-making have their
strengths and weaknesses. There is the risk that with the emergence of
AI, intuitive decision-making will be replaced by predominantly
analytical decision-making. Also, if professionals are replaced, then
their tacit knowledge will be lost. They often have deep insight into the
nature of societal problems and what should be taken into account. It
might be a strategy to make AI-based decisions and ask professionals to
make decisions based on their tacit knowledge. Decisions made by AI
and humans can be compared for reliability and accuracy, and facilitate
mutual learning.
In the migration example, the important question is which cases
require more scrutiny than others. For the migration case, this strategy
would mean that the algorithm and professionals will assess a number of
cases. The question is then whether the judgments diverge - where they
do, further analysis may reveal why this is the case, and lessons may be
drawn for the design and use of algorithms.
There is little guidance below on which strategies should be put
together and which shouldn't. The essence of the challenges presented in
Table 1 is that the complexity and ambiguity of algorithms make it
impossible to explain them in a simple and unambiguous manner. There
is no single best strategy for dealing with the challenges, as such a
portfolio of strategies should be employed. If, for example, there is a
negotiated algorithm (strategy 2), there are institutional checks and
balances (strategy 4), and there are regular opportunities for pro­
fessionals to challenge algorithmic decision-making (strategy 7), then
the combination of these strategies can contribute to greater trust in
algorithms. Some of the strategies might complement each other, like
strategy 2 and 6, in which citizen representatives are used to sensitize
algorithm design to fundamental values. Strategy 1 and 3 look at
different aspects, with the former focusing on decisions but not design,
and the latter focusing on the design process. Each of the strategies has
its own shortcomings and might not solve the challenge of XAI. The
strategies might be time-consuming and resource-intensive. Also, the
strategies might not result in the desired outcomes. Nevertheless, the
portfolio of strategies can contribute to the legitimacy of the use of al­
gorithms. Obviously, the XAI strategies may not be effective for the
problems they were not designed to tackle. The strategies might not be
able to tackle the problem in cases where XAI might not be a good so­
lution for automating decision-making. Diagnosing the situation at hand
should always be the first step before jumping into solutions.
7. Discussion and conclusions
XAI faces many challenges when used for consequential decision-
H. de Bruijn et al.
Government Information Quarterly 39 101666
making by governments. Often XAI is approached as a technical problem
in which better algorithms can facilitate explanation. This view neglects
the wicked nature of the problems for which XAI is used. What might
initially look like a simple problem for a public body to explain the
working of an algorithm to the public is often far more complicated. The
public lacks the expertise, explanations might be contested, explana­
tions might change over time or differ per case, various data sources and
algorithms are used, the working of the algorithms does not reflect the
explanation, and the problems are ambiguous and can be tackled in
different ways. Apart from these challenges, the socio-political context
in which XAI is used might create strong incentives to distrust the ex­
planations of AI, irrespective of the quality of the explanation. In
particular, if there are high levels of politicization and the decisions'
impact is high. Surprisingly, data-driven AI is often employed for
automating highly politicized situations in which the decisions have a
high impact, as these might be hard to automate in another way.
The challenges show the need to broaden the view on XAI beyond
merely taking an instrumental view. XAI should be approached as a
socio-technical challenge in which both technology and social aspects
are addressed in concert. The focus should be on the impact and on
creating trust and not only on overcoming the opaqueness. The strate­
gies include shifts from 1) explaining AI to explaining decisions 2) from
designing algorithms to negotiating algorithms, 3) from explainable
algorithms to explainable processes, 4) from an instrumental to an
institutional approach, 5) from monopolistic algorithms and datasets to
competing algorithms and datasets, 6) from algorithms to value-
sensitive algorithms and 7) from algorithms replacing professional
decision-making to professionals challenging algorithmic decision-
making. As the challenges are multi-faceted and interrelated, a combi­
nation of strategies will be typically needed.
In further research, each strategy can be expanded and tested in
practice. Strategies for creating legitimacy and trust in XAI are derived
for use by policy-makers, but they can be used as a future research
agenda on XAI. This research is explorative in nature and consequently
has several limitations. The work is based on three illustrative case
studies. In other case studies, new challenges might appear. The stra­
tegies might not be exhaustive and can be extended in further research.
Also, the strategies are not tested in practice and we did not investigate
how the strategies can be employed to realize successful outcomes. The
XAI research agenda should be broadened and become more than
opening an algorithmic black box. The actual use of XAI should be
analyzed, its positive and negative impact evaluated, and socio-
technical aspects captured.
In this paper, the focus was primarily on the algorithms. Outside the
scope of the paper is the malfunctioning of the software infrastructure,
human mistakes, the use of (non-)trusted technology and open-source
software. These elements can influence the XAI outcomes and should
be tackled as well. Finally, we recommend investigating the relationship
between XAI, openness, transparency, accountability and explainability.
These concepts are interrelated and can be unraveled in further
Marijn Janssen is a Full Professor in ICT & Governance in the
Technology, Policy and Management Faculty of Delft University of
Technology. His research is focused on ICT-architecting and design
science in situations in which multiple public and private organizations
need to collaborate, in which ICT plays an enabling role, there are
various ways to proceed, and socio-technical solutions are constrained
by organizational realities and political wishes. Marijn was nominated in
2018 and 2019 by Apolitical as one of the 100 most influential people in
the Digital Government worldwide 
government-world100. He has published over 600 refereed publica­
tions, his google h-score is 72, having over 20 K citations. More infor­
mation: www.tbm.tudelft.nl/marijnj.
Declaration of Competing Interest
The authors declare that they have no known competing financial
interests or personal relationships that could have appeared to influence
the work reported in this paper.
Acknowledgments
This research was partially supported by TAILOR, a project funded
by EU Horizon 2020 research and innovation programme under GA No
952215. A preliminary version of this paper was published in the Dutch