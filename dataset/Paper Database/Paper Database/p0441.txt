Focus on i2b2
Obesity NLP Challenge
Viewpoint Paper 
Recognizing Obesity and Comorbidities in Sparse Data
ÖZLEM UZUNER, PHD
A b s t r a c t
In order to survey, facilitate, and evaluate studies of medical language processing on clinical
narratives, i2b2 (Informatics for Integrating Biology to the Bedside) organized its second challenge and workshop.
This challenge focused on automatically extracting information on obesity and ﬁfteen of its most common
comorbidities from patient discharge summaries. For each patient, obesity and any of the comorbidities could be
Present, Absent, or Questionable (i.e., possible) in the patient, or Unmentioned in the discharge summary of the
patient. i2b2 provided data for, and invited the development of, automated systems that can classify obesity and
its comorbidities into these four classes based on individual discharge summaries. This article refers to obesity and
comorbidities as diseases. It refers to the categories Present, Absent, Questionable, and Unmentioned as classes. The
task of classifying obesity and its comorbidities is called the Obesity Challenge.
The data released by i2b2 was annotated for textual judgments reﬂecting the explicitly reported information on
diseases, and intuitive judgments reﬂecting medical professionals’ reading of the information presented in
discharge summaries. There were very few examples of some disease classes in the data. The Obesity Challenge
paid particular attention to the performance of systems on these less well-represented classes.
A total of 30 teams participated in the Obesity Challenge. Each team was allowed to submit two sets of up to
three system runs for evaluation, resulting in a total of 136 submissions. The submissions represented a
combination of rule-based and machine learning approaches.
Evaluation of system runs shows that the best predictions of textual judgments come from systems that ﬁlter the
potentially noisy portions of the narratives, project dictionaries of disease names onto the remaining text, apply
negation extraction, and process the text through rules. Information on disease-related concepts, such as
symptoms and medications, and general medical knowledge help systems infer intuitive judgments on the
 J Am Med Inform Assoc. 2009;16:561–570. DOI 10.1197/jamia.M3115.
Introduction
Narrative patient records allow doctors to write precise
notes. The narratives do not contain controlled vocabularies,
and thus allow doctors ﬂexibility of expression.1 However,
the narratives also make information contained inaccessible
to automated clinical systems. Natural language processing
(NLP) and medical language processing (MLP) focus on
technologies that can extract structured information from
narratives.2
The Obesity Challenge was motivated by the clinical need
for technologies that can help counter the current obesity
epidemic.3 Its goal was to systematically evaluate NLP and
MLP systems. Run as a shared task, the challenge was
organized as a part of an i2b2 (Informatics for Integrating
Biology to the Bedside) “Driving Biology Project.” A total of
30 teams participated in the Obesity Challenge and met at a
workshop cosponsored by the American Medical Informatics Association. This paper provides an overview of the
challenge, describes the data and the evaluation metrics,
reviews the best performing systems, and identiﬁes directions for future MLP research.
Related Work
Systematic, head-to-head evaluations of technology can help
advance state of the art and guide future research.4 Shared
tasks provide a way of conducting such evaluations. They
provide the participants with a common set of training
documents annotated with the ground truth for a particular
task and evaluate all participants on the same held-out set.
Outside the medical domain, shared tasks have included the
Message Understanding Conference5 and the Text Retrieval
Evaluation Conferences (TREC),6 organized by the National
Afﬁliations of the author: University at Albany, SUNY, Albany, NY;
Middle East Technical University, Northern Cyprus Campus, Kalkanli, Guzelyurt, Mersin 10, Turkey.
This work was supported in part by the NIH Road Map for Medical Research Grants U54LM008748. Institutional Review Board
approval has been granted for the studies presented in this manuscript. The author thanks all participating teams for their contributions to the challenge, and AMIA for its support in organizing the
workshop that accompanied the challenge.
Correspondence: Özlem Uzuner, College of Computing Information, University at Albany, SUNY, Draper 114A, 135 Western Ave,
Albany, NY, 12222; e-mail:  .
Received for review: 12/22/08; Accepted for publication: 04/07/09.
Journal of the American Medical Informatics Association
July / August 2009
Institute of Standards and Technology.7 Shared tasks for
biomedicine have included BioCreAtIvE8 and TREC Genomics.9
In 2006, we organized the ﬁrst MLP shared task on clinical
narratives.10 This task focused on two challenges involving
discharge summaries: automatic de-identiﬁcation of personal health information (the De-identiﬁcation Challenge)11
and automatic evaluation of the smoking status of patients
(the Smoking Challenge).12 These shared tasks were followed by a similar effort of the University of Cincinnati
Computational Medicine Center.13 The Obesity Challenge
continued i2b2’s efforts to make existing clinical records
available to the research community. Extracting information
about obesity and comorbidities from narrative discharge
summaries was the focus of this challenge.
Challenge Task: Recognition of Obesity
and Comorbidities
To deﬁne the Obesity Challenge task, two experts from the
Massachusetts General Hospital Weight Center studied 50
(25 each) random pilot discharge summaries from the Partners HealthCare Research Patient Data Repository. The
experts identiﬁed ﬁfteen frequently occurring obesity comorbidities: asthma, atherosclerotic cardiovascular disease
(CAD), congestive heart failure (CHF), depression, diabetes
mellitus (DM), gallstones/cholecystectomy, gastroesophageal reﬂux disease (GERD), gout, hypercholesterolemia,
hypertension
hypertriglyceridemia,
obstructive
sleep apnea (OSA), osteoarthritis (OA), peripheral vascular
disease (PVD), and venous insufﬁciency. They determined
the Obesity Challenge task as automatic classiﬁcation of
obesity and the above comorbidities, referred to as diseases,
as Present, Absent, or Questionable in a patient, or Unmentioned in the discharge summary of the patient. We deﬁne
these classes as follows:
1. Present: the patient has/had the disease.
2. Absent: the patient does/did not have the disease.
3. Questionable: the patient may have the disease.
4. Unmentioned: the disease is not mentioned in the discharge summary.
We expect that the technologies developed in response to
the challenge will be useful for indexing, classifying, and
summarizing obesity-related facts found in discharge summaries. All relevant Institutional Review Boards approved
the i2b2 Obesity Challenge.
Obesity Challenge Data
Data Draw and De-identiﬁcation
Obesity Challenge data consisted of 1237 discharge summaries from the Partners HealthCare Research Patient Data
Repository. These data were chosen from the discharge
summaries of patients who were overweight or diabetic and
had been hospitalized for obesity or diabetes sometime since
12/1/04. Some of the selected summaries included no
mention of the stems “obes” and “diabet”, others included at
least one mention of these stems.
De-identiﬁcation was performed semi-automatically. All
private health information was replaced with synthetic
identiﬁers.11
Annotation
The data for the challenge were annotated by two obesity
experts from the Massachusetts General Hospital Weight
Center. The experts were given a textual task, which asked
them to classify each disease (see list of diseases above) as
Present, Absent, Questionable, or Unmentioned based on
explicitly documented information in the discharge summaries, e.g., the statement “the patient is obese”. The experts
were also given an intuitive task, which asked them to
classify each disease as Present, Absent, or Questionable by
applying their intuition and judgment to information in the
discharge summaries, e.g., the statement “the patient weighs
230 lbs and is 5 ft 2 inches”. We refer to the textual task
annotations as textual judgments and the intuitive task annotations as intuitive judgments.
Given the tasks, the experts agreed that:
• Textual judgments would require no reasoning.
• Intuitive judgments would generally agree with a textual
Present, Absent, or Questionable judgment. The focus of
the intuitive task would be on diseases marked Unmentioned.
• A textual judgment of Unmentioned, in the absence of
information from the discharge summary supporting an
inference about the disease, would translate to an intuitive judgment of Absent
• Information that would allow inference of diseases would
include mentions of examination and test results, e.g.,
blood pressure or blood sugar measurements, physical
characteristics, e.g., body mass index, and the medication
and diseases discussed in the discharge summary.
Agreement and Tie-breaking
The two experts independently annotated our 1237 discharge summaries. The kappa () agreement14 between the
two annotators on each disease is shown in Table 1. The
lowest  on textual judgments was 0.71. For 12 diseases,  on
textual judgments was above 0.8; for four diseases,  on
Table 1 y Kappa Agreement on Textual and Intuitive
Comorbidity (Disease)
Textual Kappa
Intuitive Kappa
Atherosclerotic CV disease
Congestive heart failure (CHF)
Depression
Diabetes mellitus (DM)
Gallstones/cholecystectomy
Hypercholesterolemia
Hypertension (HTN)
Hypertriglyceridemia
Obstructive sleep apnea (OSA)
Osteoarthritis (OA)
Peripheral vascular disease
Venous insufﬁciency
CV  cardiovascular; GERD  gastroesophageal reﬂux disease.
Uzuner, Recognizing Obesity and Comorbidities in Sparse Data
textual judgments was between 0.71 and 0.79. The lowest 
on intuitive judgments was 0.44. For seven diseases,  on
intuitive judgments was above 0.8; for six of the diseases, 
on intuitive judgments was between 0.6 and 0.79. Although
the  values are open to interpretation,15  of 0.8 is widely
used as the threshold for “almost perfect agreement”, 
values of 0.6–0.79 indicate “substantial agreement”14. Please
see the online supplement at for a description of agreement calculation and extended analysis of
agreement.
After annotation, a resident from the Massachusetts General Hospital resolved the disagreements in textual judgments. Majority vote among the three annotators determined the ground truth for the textual task. In the absence
of a third obesity expert who could resolve the disagreements in intuitive judgments, only judgments agreed on
by the two obesity experts were used as the ground truth
for the intuitive task. Table 2 shows the correspondence
between the ground truth textual and intuitive judgments. Most textual Present judgments map to intuitive
Present judgments. Similar observations hold for the
other classes.
Final Data
Table 3 and Table 4 show data distribution into training and
test sets per disease. The distributions are non-uniform. In
studying datasets with unbalanced class distributions, it is
easier to focus on the better populated classes and ignore
the less well-represented ones due to their limited contribution to overall performance. In our case, the less wellrepresented classes indicate the possibility or absence of a
disease in a patient. Accurate recognition of these classes
allows their inclusion in structured knowledge bases that
can support future clinical decisions. Please refer to the
online supplement at for Table 5 and
baseline results on these data.
We evaluated system performances using micro- and
macro-averaged precision (P), recall (R), and F-measure
(F1). Given the emphasis of the Obesity Challenge on the
less well-represented classes, we used macro-averaged
F-measure as the primary metric for evaluation. Microaveraged F-measure maintained a global perspective on
the results.
Evaluation Metrics
For each disease, the macro-averaged metrics represent the
arithmetic mean of the precision, recall, and F-measure on the
Present, Absent, Questionable, and Unmentioned classes that
are observed in the ground truth for that disease (see Eqs 1, 2
and 3). The macro-averaged precision, recall, and F-measure of
the system are obtained from the precision, recall, and Fmeasure on the classes observed in the ground truth for all
diseases. In these formulae, M is the number of classes.
Table 2 y Distribution of Classes between Textual and Intuitive Ground Truth
Intuitive Present
Intuitive Absent
Questionable
No Intuitive Class
(No Agreement)
Textual Present
Textual Absent
Textual Questionable
Textual Unmentioned
No Textual Class (No Agreement)
Table 3 y Distribution of Textual Judgments into Training and Test Sets
Questionable
Unmentioned
Depression
Gallstones
Hypercholesterolemia
Hypertriglyceridemia
Venous insufﬁciency
CAD  coronary artery disease; CHF  congestive heart failure; DM  diabetes mellitus; GERD  gastroesophageal reﬂux disease; HTN 
hypertension; OSA  obstructive sleep apnea; OA  osteo arthritis; PVD  peripheral vascular disease.
Journal of the American Medical Informatics Association
July / August 2009
Equation 1—Macro-averaged Precision Pmacro
Pmacro  
Equation 2—Macro-averaged Recall (Rmacro)
Rmacro  
Equation 3—Macro-averaged F-measure (F1macro)
F1macro  
Macro-averages give equal weight to each class, including
rare ones.16 As a result, two systems that make the same raw
number of mistakes can end up with two different macroaveraged scores.
Equation 4 and Equation 5 show the formulae for computing
micro-averaged precision and recall from true positives
(TP), false positives (FP), and false negatives (FN) for each
class.16,17 In these formulae, M is the number of classes.
Micro-averaged F-measure is the harmonic mean of microaveraged precision and recall (Eq 6). Micro-averages give
equal weight to each sample regardless of its class. They are
dominated by those classes with the greatest number of
Equation 4—Micro-averaged Precision (Pmicro)
Equation 5—Micro-averaged Recall (Rmicro)
Equation 6—Micro-averaged F-measure (F1micro)
2  Pmicro  Rmicro
Pmicro  Rmicro
Signiﬁcance Test
We determined the signiﬁcance of the difference of the
systems’ performance using the Z test on two proportions.18,19 We used a two-tailed test with a Z value of  1.645
and conﬁdence level of 0.9.20
Obesity Challenge Submissions
A total of 30 teams participated in the Obesity Challenge
(see Table 6). Training data were released in March 2008.
Test data were released in June 2008. Each team submitted
up to three system runs for predicting textual judgments
and three for predicting intuitive judgments on test data.
We received a total of 68 textual and 68 intuitive system
runs.21–46 To obtain textual task results, we ranked each
team on its best performing textual system run. To assess
the intuitive task, we ranked each team on its best
performing intuitive system run. We review the top ten
textual and intuitive systems in ranked order below.
Top Ten Textual Systems
Of the top ten textual systems, Yang et al.,22 Solt et al.,42
Ware et al.,28 Childs et al.,24 Mishra et al.43 Szarvas et al.,21
and Deshazo et al.26 ﬁltered the narrative summaries from
information indirectly related to the patient and marked
negations and uncertainty through methods that resembled
NegEx47 or ConText.48 In addition:
Yang et al. used a precompiled dictionary of disease,
symptom, treatment, and medication terms. They looked for
Table 4 y Distribution of Intuitive Judgments into Training and Test Sets
Questionable
Depression
Gallstones
Hypercholesterolemia
Hypertriglyceridemia
Venous insufﬁciency
CAD  coronary artery disease; CHF  congestive heart failure; DM  diabetes mellitus; GERD  gastroesophageal reﬂux disease; HTN 
hypertension; OSA  obstructive sleep apnea; OA  osteo arthritis; PVD  peripheral vascular disease.
Uzuner, Recognizing Obesity and Comorbidities in Sparse Data
sentences with either exact or approximate matches. For
documents that contained more than one sentence about a
disease, they determined the class for that disease based on
a weighted combination of the evidence in sentences.22
Solt et al. stripped the documents of personal identiﬁers,
expanded abbreviations, and split discharge summaries into
sections. To mark a disease as Present, they developed a
rule-based classiﬁer with disease names, synonyms, spelling
variants, and semantically related terms. They partitioned
text using contextual clues that indicate negative or uncertain statements and fed the partitions into a series of binary
classiﬁers that determined whether each disease was Questionable, Absent, or Present, in that order. Diseases that
failed to receive any of these three labels were labeled
Unmentioned.42
Ware et al. used regular expressions with a set of diseaserelated keywords and their synonyms. They assumed that
keywords not marked as negated, historical, or associated
with a relative would indicate a disease is present.28
Childs et al. used the rule-based Rocket AeroText information extraction system49 with keywords, their synonyms,
and patterns generated by medical experts. They weighed
and combined the evidence for each class of each disease.24
Mishra et al. marked the text with a set of disease-related
keywords compiled by analyzing the training set. They
determined the total number of positive, negative, and
uncertain assertions for each disease in a discharge summary. The class with the highest number of assertions
related to the disease labeled the disease. Ties were broken
in favor of positive assertions.43
Szarvas et al. used term frequency and conditional probability
in the Present class to preselect the most common terms that
could aid classiﬁcation. They supplemented this list with
spelling variants and infrequent terms. The resulting dictionaries, along with disease contexts and document structure,
formed the backbone of their rule-based system.21
Savova et al.25 and Patrick et al.44 deviated from the pattern
of text ﬁltering and negation extraction. Savova et al. com-
Table 6 y Participating Teams
Afﬁliations
Ambert et al.
Oregon Health and Science University
United States
Barrett et al.
University of Victoria
Illinois State University
United States
Childs et al.
Lockheed Martin and SAGE Analytica
United States
DeShazo et al.
University of Washington
United States
Frunza et al.
University of Ottawa
Grabar et al.
LIPN–UMR 7030, Université Paris 13—CNRS
Centre de Recherche des Cordeliers
Université Paris Descartes
HEGP AP-HP
California State University, San Marcos
United States
Nara Institute of Science and Technology
Harkema et al.
University of Pittsburgh
United States
Jazayeri et al.
University of Alberta
Lan et al.
National University of Singapore
Institute of Infocomm Research
MacNamee et al.
Dublin Institute of Technology
Mata et al.
Universidad de Huelva
University of Edinburgh
University of Minnesota
United States
Boston University
United States
University of Utah
United States
Mishra et al.
Centers for Disease Control and Prevention
National Center for Public Health informatics
United States
Neves et al.
Centro Nacional de Biotecnología
Universidad Complutense de Madrid
Patrick et al.
University of Sydney
University of Minnesota, Duluth
United States
Peshkin et al.
United States
Alias-I, Inc.
Savova et al.
Mayo Clinic
United States
Solt et al.
Budapest University of Technology and Economics
TextMiner, Ltd, Budapest
Szarvas et al.
University of Szeged
Ware et al.
United States
West Virginia University
Yang et al.
University of Manchester
Journal of the American Medical Informatics Association
July / August 2009
bined an information extraction system, a maximum entropy classiﬁer, and an SVM. They evaluated these approaches, and determined the best one for each disease on
each of the textual and intuitive tasks. They then allowed the
identiﬁed best method to judge a disease for a task.25
Patrick et al. used a combination of rules and a decision-tree
classiﬁer with features that included signs, symptoms, and
medication names related to each disease. They also leveraged the correlations between diseases.44
DeShazo et al. analyzed 300 of the discharge summaries,
annotating them for information that supported ground
truth textual judgments. They employed a rule base to
propagate the information supporting ground truth judgments to the rest of the corpus.26
Top Ten Intuitive Systems
Most intuitive systems beneﬁted from the output of the
textual systems. Solt et al.42 Szarvas et al.21 and Childs et
al.24 determined a default mapping between textual and
intuitive judgments and used it as the starting point. The top
four intuitive systems employed rule-bases that incorporated “disease-speciﬁc, non-preventive medications and
their brand names”, disease-related procedures, and symptoms highly correlated with diseases,42 “numeric expressions corresponding to measurements”21, and medication
names.24,28
Different from the top four, Ambert et al. took a machine
learning approach to the intuitive task. They combined
hot-spot ﬁltering with error-correcting output codes. They
identiﬁed words that demonstrated high information gain
with respect to each disease, extracted the text within a
100-character window of these words, marked the negations, and vectorized the extracted text. Of the created
vectors, “the ones that were absent any non-zero features”
were automatically labeled Absent. The rest were labeled
using error-correcting output codes that weighted each class
inversely proportionally to its size.45
Meystre extracted sections and sentences from each discharge summary using regular expressions and rules. In
these excerpts, he disambiguated acronyms and extracted
concept identiﬁers from the Uniﬁed Medical Language
System (UMLS).50 He supplemented the identiﬁed concepts
with medications and biomarker values that could indicate a
disease. He determined intuitive labels using NegEx and
ConText.46
Yang et al. based their intuitive predictions on evidence
sentences containing information about symptoms, clinical
measurements, and medications. They processed the sentences using clinical information, so the symptoms more
directly related to a disease were more heavily weighted.
The evidence sentences were considered to mark the presence of a disease unless a negation extractor marked them as
negative or uncertain. In diseases with multiple evidence
sentences, the information was combined.22
DeShazo et al. used SVMs for their intuitive system. This
system used features derived from the text by the rule-based
classiﬁer they developed for the textual task.26
Table 8 y Signiﬁcance Tests on the Top Ten Textual Systems
Yang et al.
Solt et al.
Ware et al.
Childs et al.
Mishra et al.
Szarvas et al.
Savova et al.
Patrick et al.
Jazayeri et al.
Sorted by macro-averaged F-measure. marks pairs Not signiﬁcantly different in macro-averaged F-measure. *marks pairs Not signiﬁcantly
different in micro-averaged F-measure. †System utilized external annotators. Only the upper diagonal is marked.
Table 7 y Micro- and Macro-averaged Results on Textual Judgments, Sorted by Macro-averaged F-Measure
Macro-Averaged
Micro-Averaged
Yang et al.
Solt et al.
Ware et al.
Childs et al.
Mishra et al.
Szarvas et al.
Savova et al.
Patrick et al.
*Jazayeri et al.
†DeShazo et al.
Best F-measures are in bold. †System utilized external annotators.
*System description not available.
Uzuner, Recognizing Obesity and Comorbidities in Sparse Data
Matthews evaluated as features stemmed word tokens,
bigrams, trigrams, UMLS semantic types of concepts, and
negation as extracted by NegEx. He identiﬁed the most
useful features for each class and applied Bayesian networks
to classify diseases.33
Obesity Challenge Results
The results for the textual task are shown in Table 7 and in
Table 8. Table 7 shows that the best macro-averaged
F-measure on the textual task was 0.8052; the best microaveraged F-measure was 0.9773. Table 8 shows that the
macro-averaged performance difference between the top
two systems is not statistically signiﬁcant. The top three
systems are not signiﬁcantly different in their micro-averaged F-measures. Table 9 and Table 10 show the top ten
intuitive systems, as ranked by the macro-averaged Fmeasure. The best macro-averaged F-measure on the intuitive task is 0.6745; the best micro-averaged F-measure is
0.9654. Table 10 shows that the top three systems are not
statistically different in either macro- or micro-averaged
F-measures.
Table 11 shows that the top ten systems on the textual task
had F-measures ranging from 0.92 to 0.97 on Present class.
Their F-measures range from 0.97 to 0.99 on the Unmentioned class. On the Absent class, the F-measures range from
0.39 to 0.66; on the Questionable class, the F-measures range
from 0 to 0.62. Table 12 shows that seven out of the top ten
systems produced a zero F-measure on the Questionable
class on the intuitive task. The best F-measure for this class
is 0.12. The performance of the top ten systems on the
Present class range from 0.92 to 0.95, while the top ten
systems on the Absent class performed in a range from 0.97
Discussion
Rule-based approaches played a signiﬁcant role in the top
ten systems in the textual task. Machine learning approaches
contributed to the top ten systems in the intuitive task but
were less dominant in the textual task.
Given the similar approaches taken by the top ten textual
systems, we expect that their performance differences
resulted from the accuracy of their negation extraction
modules and the completeness of their dictionaries. The
approaches taken by the intuitive systems were more varied.
In general, clinical information, world knowledge, and information from the textual task beneﬁted the top ten intuitive systems. A subset of the top ten textual and intuitive
systems took advantage of medical experts, indicating the
value of engaging medical professionals in system development.
A subset of the top ten textual and intuitive systems encodes
expert knowledge in the form of hand-crafted rules and
patterns, generated either through direct interactions with
domain experts or through (laypersons’) observations on the
ground truth created by domain experts. “Expert knowledge is a combination of a theoretical understanding of the
problem and a collection of heuristic problem-solving rules
that experience has shown to be effective in the domain”51.
Table 9 y Micro- and Macro-averaged Results on Intuitive Judgments, Sorted by Macro-averaged F-Measure
Macro-Averaged
Micro-Averaged
Solt et al.
Szarvas et al.
Childs et al.
Ware et al.
Ambert et al.
Yang et al.
†DeShazo et al.
Jazayeri et al.
Best F-measures are in bold. †System utilized external annotators.
Table 10 y Signiﬁcance Tests on the Top Ten Intuitive Systems
Solt et al.
Szarvas et al.
Childs et al.
Ware et al.
Ambert et al.
Yang et al.
†DeShazo et al.
Sorted by macro-averaged F-measure. marks pairs Not signiﬁcantly different in macro-averaged F-measure. *marks pairs Not signiﬁcantly
different in micro-averaged F-measure. †System utilized external annotators. Only the upper diagonal is marked.
Journal of the American Medical Informatics Association
July / August 2009
However, such knowledge is limited to a closed-domain,
narrowly deﬁned task. Expert systems based on this knowledge, e.g., the hand-crafted systems developed for the
Obesity Challenge, perform well when tested within the
domain of their focus; however, they require some work to
be adapted to new tasks and domains.
Despite the limitations on their generalizeability, MLP systems that can address the Obesity Challenge with nearhuman-level performance were developed within a three
month period. Although starting from an existing system
was preferred for the development of some systems,
e.g.,24,46 most, including two of the best systems22,42 developed for the Obesity Challenge, were built from scratch.
The main complexity and difﬁculty of the Obesity Challenge, in
contrast to past challenges12,13 and most mainstream MLP work,
came from the focus on less well-represented classes. The worst
macro-averaged F-measures on the challenge were 0.2237 and
0.3358, in the textual and intuitive tasks respectively.
In particular, the textual Questionable class contained some
discharge summaries that were incorrectly classiﬁed by all
system runs. One such summary, marked Questionable for
GERD, stated “The patient was continued on her PPI for
GERD prophylaxis.  required increasing her dosage of
Nexium secondary to GERD-like symptoms.”
Similarly, for the textual Absent class, no system runs could
correctly predict the judgment for CAD in a discharge
summary which stated, “no history of cancer or heart
disease.” In general, textual Absent judgment required careful study of the context where diseases are mentioned. For
example, recognizing the absence of diabetes when a patient
“had no further insulin requirement and was not a diabetic”
requires correct interpretation of this text. Only a subset of
the submitted system runs correctly classiﬁed this case.
The Present class was easier to predict. For example, all
systems correctly labeled a discharge summary which stated
“adult onset diabetes mellitus”. However, even the Present
class was not straightforward when the discharge summary
failed to mention the disease by name. For example, a
discharge summary about “ventral hernia” and “atrial ﬁbrillation” that did not mention “coronary artery disease” or
“cardiovascular disease” was judged Present for CAD. Only
a subset of the submitted system runs predicted this textual
judgment. Prediction of textual Present judgments was even
more difﬁcult in summaries using biomarkers or other
related information to describe a disease. For example, none
of the system runs submitted to the i2b2 challenge could
correctly predict the ground truth judgment for obesity on
the discharge summary that stated “The patient’s admission
weight was 106.2 kg. Her discharge weight was 100.7
kilograms”, and “weight should be monitored daily.”
The textual Unmentioned class was the easiest to predict.
Most of these judgments were classiﬁed correctly by almost
all the submitted system runs. Those textual Unmentioned
judgments that could not be predicted correctly demonstrate
peculiarities of data. For example, author’s reading of the
statement “The patient was an obese male” indicates a
textual label of Present for obesity and disagrees with the
ground truth label of Unmentioned.
Table 11 y Top Ten Textual Systems on Individual Classes (Aggregate Over All Diseases)
Questionable
Unmentioned
Yang et al.
Solt et al.
Ware et al.
Childs et al.
Mishra et al.
Szarvas et al.
Savova et al.
Patrick et al.
Jazayeri et al.
†DeShazo et al.
Best F-measures per class are in bold. Sorted by macro-averaged F-measure. †System utilized external annotators.
Table 12 y Top Ten Intuitive Systems on Individual Classes (Aggregate over All Diseases)
Questionable
Solt et al.
Szarvas et al.
Childs et al.
Ware et al.
Ambert et al.
Yang et al.
†DeShazo et al.
Jazayeri et al.
Best F-measures per class are in bold. Sorted by macro-averaged F-measure. †System utilized external annotators.
Uzuner, Recognizing Obesity and Comorbidities in Sparse Data
Given the characteristics of the data and the observations on
performance on the less well-represented classes, removing
the emphasis from these classes would have made the Obesity Challenge much more mainstream and much more
straightforward, but not trivial. Eighty-ﬁve percent of the
systems in the intuitive task and 93% of the systems in the
textual task achieved micro-averaged F-measures above 0.8.
Two of the best performing systems from the Obesity
Challenge are open source and can either be downloaded for
local installations or utilized online.52,53
Conclusions and Implications for Future Research
The Obesity Challenge demonstrates the difﬁculty of differentiating textual judgments from intuitive ones. The overlap
in information used by automated systems for identifying
textual and intuitive judgments and the author’s observations on the Obesity Challenge data indicate that textual
judgments of domain experts may differ from textual judgments of lay persons. In other words, the annotators’ domain knowledge may have led them to consider some
inferred information as explicit. As a result, some judgments
that could be considered intuitive by lay persons were found
among the textual judgments.54
However, even with unclear boundaries between textual
and intuitive judgments, the automated systems built by lay
persons effectively extracted much useful information from
discharge summaries. These systems performed best on the
most factual and objective pieces of information. They
experienced more difﬁculty arriving at conclusions only
medical experts could infer. Most of the factual and objective
pieces of information were identiﬁed by simple rule-based
systems armed with dictionaries of terms and negation
extraction modules. Machine learning approaches that studied the patterns in the textual judgments provided a beginning to correctly predicting intuitive judgments. We should
emphasize that the relative performance of the systems is
likely to change if we have much larger corpora for both
training and testing. The unavailability of such corpora is
likely to be the largest bottleneck for future progress in MLP.
References y
1. Van Ginneken A, De Wilde M, Van Mulligen E, Stam H. Can
data representation and interface demands be reconciled? Approach in orca. AMIA Annu Symp Proc; 1997:779–83.
2. Friedman C, Alderson PO, Austin JH, Cimino JJ, Johnson SB. A
general natural-language text processor for clinical radiology.
J Am Med Inform Assoc 1994;1(2):161–74.
3. Christakis NA, Fowler JH. The spread of obesity in a large social
network over 32 years. N Engl J Med 2007 Jul 26;357(4):370–9.
4. Friedman C, Hripcsak G, Shablinsky I. An evaluation of natural
language processing methodologies. AMIA Annu Symp Proc;
1998:855–9.
5. Grishman R, Sundheim B. Message Understanding Conference-6: A brief history. 16th Conference on Computational
Linguistics, COLING 1996, pp 466–71.
6. Sparck Jones K. Reﬂections on TREC. Inf Proc Manag 1995;31(3):
7. NIST Available at: Accessed: Aug 7, 2008.
8. Hirschman L, Yeh A, Blaschke C, Valencia A. Overview of
BioCreAtIvE: Critical assessment of information extraction for
biology. BMC Bioinform 2005;6(S1).
9. Hersh W, Bhupatiraju RT, Corley S. Enhancing access to the
Bibliome: The TREC genomics track. Medinfo 2004;11(2):773–7.
10. Uzuner Ö, Szolovits P, Kohane I. i2b2 workshop on natural
language processing challenges for clinical discharge summaries. Available at: Accessed: Oct 21,
11. Uzuner Ö, Luo Y, Szolovits P. Evaluating the State-of-the-art in
automatic de-identiﬁcation. J Am Med Inform Assoc 2007;14(5):
12. Uzuner Ö, Goldstein I, Luo Y, Kohane I. Identifying patient
smoking status from medical discharge summaries. J Am Med
Inform Assoc 2008;15(1):14–24.
13. Pestian JP, Brew C, Matykiewicz P, et al. A shared task involving multi-label classiﬁcation of clinical free text. BioNLP, 2007,
pp 97–104.
14. Cohen J. A coefﬁcient of agreement for nominal scales. Educ
Psychol Meas 1960;20(1):37–46.
15. Hripcsak G, Heitjan DF. Measuring agreement in medical informatics reliability studies. J Biomed Inform 2002;35(2):99–110.
16. Yang Y, Liu X. A Re-examination of text categorization methods. Proceedings of the ACM SIGIR Conference on Research
and Development in Inf Retrieval, pp 42–9, 1999.
17. Ozgur A, Ozgur L, Gungor T. Text categorization with classbased and corpus-based keyword selection. Lecture Notes in
Computer Science. Springer-Verlag. 2005;(3733):607–616.
18. Osborn CE. Statistical Applications for Health Information
Management, 2nd edn, Boston: Jones & Bartlett Publishers, 2005.
19. Z-test for Two Proportions. Available at: 
dimensionresearch.com/resources/calculators/ztest.html. Accessed: Jun 19, 2008.
20. Chinchor N. The Statistical Signiﬁcance of the MUC-4 Results,
McLean, VA: 4th Conference Mess Understand, 1992, pp 30–50.
21. Szarvas G, Farkas R, Almási A, et al. Semi-automated construction of decision rules to predict morbidities from clinical texts.
J Am Med Inform Assoc 2009;16:601–5.
22. Yang H, Spasic I, Keane JA, Nenadic G. A text mining approach
to the prediction of a disease status from clinical discharge
summaries. J Am Med Inform Assoc 2009;16:596–600.
23. Guillen R. Identifying obesity and Co-morbidities from medical
records. Proceedings of the i2b2 Workshop on Challenges in
Natural Language Processing for Clinical Data, 2008.
24. Childs LC, Taylor RJ, Simonsen L, et al. Description of a
rule-based system for the i2b2 challenge in natural language
processing for clinical data. J Am Med Inform Assoc 2009:571–5.
25. Savova G, Clark C, Zheng J, et al. The Mayo/MITRE system for
discovery of obesity and its comorbidities. Proceedings of the
i2b2 Workshop on Challenges in Natural Language Processing
for Clinical Data, 2008.
26. DeShazo JP, Turner AM. Hands-on NLP: An interactive and
user-centered system to classify discharge summaries for obesity and Related Co-morbidities. Proceedings of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical
Data, 2008.
27. Califf ME. Combining rules and naïve bayes for disease classi-
ﬁcation. Proceedings of the i2b2 Workshop on Challenges in
Natural Language Processing for Clinical Data, 2008.
28. Ware H, Mullett CJ, Jagannathan V. Natural language processing framework to assess clinical conditions. J Am Med Inform
Assoc 2009:585–9.
29. Hara K. Classifying narrative patient records without any
external resources. Proceedings of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, 2008.
30. Grabar N, Hamon T, Dart T. Term variation and semantics for
document classiﬁcation and detection of obesity and its Comorbidities cases. Proceedings of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, 2008.
31. Harkema H, Piwowar H, Amizadeh S, et al. A baseline system
for the i2b2 obesity challenge. Proceedings of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical
Data, 2008.
Journal of the American Medical Informatics Association
July / August 2009
32. MacNamee B, Kelleher JD, Delany SJ. Medical language processing for patient diagnosis using text classiﬁcation and negation
labelling. Proceedings of the i2b2 Workshop on Challenges in
Natural Language Processing for Clinical Data, 2008.
33. Matthews MP. Bayesian networks and the i2b2 obesity challenge. Proceedings of the i2b2 Workshop on Challenges in
Natural Language Processing for Clinical Data, 2008.
34. Frunza O, Inkpen D. Representation and classiﬁcation techniques for clinical data focused on obesity and its co-morbidities. Proceedings of the i2b2 Workshop on Challenges in Natural
Language Processing for Clinical Data, 2008.
35. Peshkin L, Cano C, Carpenter B, Baldwin B. Regularized logistic
regression for clinical record processing. Proceedings of the i2b2
Workshop on Challenges in Natural Language Processing for
Clinical Data, 2008.
36. Ho B, Nytrø Ø, Bassøe CF. NLP obesity challenge: Using clinical
markers for EHR classiﬁcation. Proceedings of the i2b2 Workshop on
Challenges in Natural Language Processing for Clinical Data, 2008.
37. Mata J, Maña MJ, Bermúdez JM, Cruz NP, Jiménez P. Handling
negation in classiﬁcation of clinical texts. Proceedings of the i2b2
Workshop on Challenges in Natural Language Processing for
Clinical Data, 2008.
38. McInnes BT. Using CuiTools to identify obesity and its Co-morbidities
in discharge summaries. Proceedings of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, 2008.
39. Neves M, Carazo JM, Pascual-Montano A. Botero: A SVM
classiﬁer for clinical text in the obesity domain. Proceedings of
the i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, 2008.
40. Pedersen T. Learning high precision rules to make predictions
of morbidities in discharge summaries. Proceedings of the i2b2
Workshop on Challenges in Natural Language Processing for
Clinical Data, 2008.
41. Barrett N, Weber-Jahnke J. An introduction to MLP driven by the
i2b2 challenge. Proceedings of the i2b2 Workshop on Challenges in
Natural Language Processing for Clinical Data, 2008.
42. Solt I, Tikk D, Gál V, Kardkovács ZT. Semantic classiﬁcation of
diseases in discharge summaries using a context-aware rulebased classiﬁer. J Am Med Inform Assoc 2009;16:580–4.
43. Mishra N, Cummo D, Arnzen J, Bonander J. A rule-based
approach for identifying obesity and its co-morbidities in medical discharge summaries. J Am Med Inform Assoc 2009:576–9.
44. Patrick J, Asgari P. A brief summary about the approach and
explanation of the attributes of the developed system for i2b2
challenge. Proceedings of the i2b2 Workshop on Challenges in
Natural Language Processing for Clinical Data, 2008.
45. Ambert KH, Cohen AM. A system for classifying disease
comorbidity status from medical discharge summaries using
automated hotspot and negated concept detection. J Am Med
Inform Assoc 2009;16:590–5.
46. Meystre SM. Detecting patients suffering from obesity and
common comorbidities by analyzing narrative clinical text.
Proceedings of the i2b2 Workshop on Challenges in Natural
Language Processing for Clinical Data, 2008.
47. Chapman WW, Bridewell W, Hanbury P, et al. Algorithm for
identifying negated ﬁndings and diseases in discharge summaries. J Biomed Inform 2001;34:301–10.
48. Chapman W, Chu D, Dowling JN. ConText: An algorithm for
identifying contextual features from clinical text. BioNLP 2007,
49. AeroText information extraction system. Available at: http://
www.rocketsoftware.com/products/rocket-aerotext. Accessed:
Apr 1, 2009.
50. Uniﬁed Medical Language System (UMLS). Available at:
 Accessed: Nov 21,
51. Luger GF. Artiﬁcial Intelligence: Structures and strategies for
Complex Problem Solving, 6th edn, Boston: Addison-Wesley,
Pearson Education, 2009.
52. Solt I. Automatic semantic annotation of medical discharge
summaries. Available at: 
pl. Accessed: Apr 2, 2009.
53. Szarvas G. Automatic obesity-related morbidity identiﬁer.
Available at: 
pageobesity. Accessed: Apr 2, 2009.
54. Miller RA. Reference standards in evaluating system performance. J Am Med Inform Assoc 2002;9(1):87–8.
Uzuner, Recognizing Obesity and Comorbidities in Sparse Data