Deep Learning Identity-Preserving Face Space
Zhenyao Zhu1,∗
Ping Luo1,3,∗
Xiaogang Wang2
Xiaoou Tang1,3,†
1Department of Information Engineering, The Chinese University of Hong Kong
2Department of Electronic Engineering, The Chinese University of Hong Kong
3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
 
 
 
 
Face recognition with large pose and illumination variations is a challenging problem in computer vision. This paper addresses this challenge by proposing a new learningbased face representation:
the face identity-preserving
(FIP) features.
Unlike conventional face descriptors,
the FIP features can signiﬁcantly reduce intra-identity
variances, while maintaining discriminativeness between
identities. Moreover, the FIP features extracted from an
image under any pose and illumination can be used to
reconstruct its face image in the canonical view.
property makes it possible to improve the performance of
traditional descriptors, such as LBP and Gabor ,
which can be extracted from our reconstructed images in
the canonical view to eliminate variations.
In order to
learn the FIP features, we carefully design a deep network
that combines the feature extraction layers and the reconstruction layer. The former encodes a face image into the
FIP features, while the latter transforms them to an image
in the canonical view. Extensive experiments on the large
MultiPIE face database demonstrate that it signiﬁcantly
outperforms the state-of-the-art face recognition methods.
1. Introduction
In many practical applications, the pose and illumination
changes become the bottleneck for face recognition .
Many existing works have been proposed to account for
such variations. The pose-invariant methods can be generally separated into two categories: 2D-based 
and 3D-based .
In the ﬁrst category, poses are
either handled by 2D image matching or by encoding a
test image using some bases or exemplars. For example,
∗indicates equal contribution.
†This work is supported by the General Research Fund sponsored by
the Research Grants Council of the Kong Kong SAR (Project No. CUHK
416312 and CUHK 416510) and Guangdong Innovative Research Team
Program (No.201001D0104648280).
Figure 1. Three face images under different poses and illuminations
of two identities are shown in (a).
The FIP features extracted from
these images are also visualized. The FIP features of the same identity
are similar, although the original images are captured in different poses
and illuminations. These examples indicate that FIP features are sparse
and identity-preserving (blue indicates zero value).
(b) shows some
images of two identities, including the original image (left) and the
reconstructed image in the canonical view (right) from the FIP features.
The reconstructed images remove the pose and illumination variations and
retain the intrinsic face structures of the identities. Best viewed in color.
Carlos et al.
 used stereo matching to compute the
similarity between two faces. Li et al. represented
a test face as a linear combination of training images, and
utilized the linear regression coefﬁcients as features for face
recognition.
3D-based methods usually capture 3D face
data or estimate 3D models from 2D input, and try to match
them to a 2D probe face image.
Such methods make it
possible to synthesize any view of the probe face, which
makes them generally more robust to pose variation. For
instance, Li et al. ﬁrst generated a virtual view for the
probe face by using a set of 3D displacement ﬁelds sampled
from a 3D face database, and then matched the synthesized
face with the gallery faces. Similarly, Asthana et al. 
matched the 3D model to a 2D image using the view-based
active appearance model.
The illumination-invariant methods typically
Figure 2. The LBP (a), LE (b), CRBM (c), and FIP (d) features of 50
identities, each of which has 6 images in different poses and illuminations
are projected into two dimensions using Multidimensional scaling (MDS).
Images of the same identity are visualized in the same color. It shows that
FIP has the best representative power. Best viewed in color.
make assumptions about how illumination affects the face
images, and use these assumptions to model and remove
the illumination effect. For example, Wagner et al. 
designed a projector-based system to capture images of
each subject in the gallery under a few illuminations, which
can be linearly combined to generate images under arbitrary
illuminations. With this augmented gallery, they adopted
sparse coding to perform face recognition.
The above methods have certain limitations.
For example, capturing 3D data requires additional cost and
resources . Inferring 3D models from 2D data is an illposed problem . As the statistical illumination models
 are often summarized from controlled environment,
they cannot be well generalized in practical applications.
In this paper, unlike previous works that either build
physical models or make statistical assumptions,
propose a novel face representation, the face identitypreserving (FIP) features, which are directly extracted
from face images with arbitrary poses and illuminations.
This new representation can signiﬁcantly remove pose and
illumination variations, while maintaining the discriminativeness across identities, as shown in Fig.1 (a).
Furthermore, unlike traditional face descriptors, e.g. LBP ,
Gabor , and LE , which cannot recover the original
images, the FIP features can reconstruct face images in the
frontal pose and with neutral illumination (we call it the
canonical view) of the same identity, as shown in Fig.1 (b).
With this attractive property, the conventional descriptors
and learning algorithms can utilize our reconstructed face
images in the canonical view as input so as to eliminate the
negative effects from poses and illuminations.
Speciﬁcally, we present a new deep network to learn
the FIP features.
It utilizes face images with arbitrary
pose and illumination variations of an identity as input,
and reconstructs a face in the canonical view of the same
identity as the target (see Fig.3). First, input images are
encoded through feature extraction layers, which have three
locally connected layers and two pooling layers stacked
alternately. Each layer captures face features at a different
As shown in Fig.3, the ﬁrst locally connected
layer outputs 32 feature maps.
Each map has a large
number of high responses outside the face region, which
mainly capture pose information, and some high responses
inside the face region, which capture face structures (red
indicates large response and blue indicates no response).
On the output feature maps of the second locally connected
layer, high responses outside the face region have been
signiﬁcantly reduced, which indicates that it discards most
pose variations while retain the face structures. The third
locally connected layer outputs the FIP features, which is
sparse and identity-preserving.
Second, the FIP features recover the face image in the
canonical view using a fully-connected reconstruction layer.
As there are large amount of parameters, our network is
hard to train using tranditional training methods .
We propose a new training strategy, which contains two
steps: parameter initialization and parameter update. First,
we initialize the parameters based on the least square
dictionary learning. We then update all the parameters by
back-propagating the summed squared reconstruction error
between the reconstructed image and the ground truth.
Existing deep learning methods for face recognition
are generally in two categories: (1) unsupervised learning
features with deep models and then using discriminative
methods (e.g. SVM) for classiﬁcation ; (2)
directly using class labels as supervision of deep models
 . In the ﬁrst category, features related to identity,
poses, and lightings are coupled when learned by deep
models. It is too late to rely on SVM to separate them later.
Our supervised model makes it possible to discard pose and
lighting features from the very bottom layer. In the second
category, a ‘0/1’ class label is a much weaker supervision,
compared with ours using a face image (with thousands
of pixels) of the canonical view as supervision.
require the deep model to fully reconstruct the face in the
canonical view rather than simply predicting class labels,
and this strong regularization is more effective to avoid
overﬁtting.
This design is suitable for face recognition,
where a canonical view exists. Different from convolutional
neural networks whose ﬁlters share weights, our ﬁlers
are localized and do not share weights since we assume
different face regions should employ different features.
This work makes three key contributions. (1) We propose a new deep network that combines the feature extraction layers and the reconstruction layer. Its architecture is
carefully designed to learn the FIP features. These features
can eliminate the poses and illumination variations, and
n2=24×24×32
n2=24×24×32
5×5 Locally
Connected and
Feature Extraction Layers
Reconstruction Layer
5×5 Locally
Connected and
5×5 Locally
n1=48×48×32
Figure 3. Architecture of the deep network. It combines the feature extraction layers and reconstruction layer. The feature extraction layers include three
locally connected layers and two pooling layers. They encode an input face x0 into FIP features x3. x1, x2 are the output feature maps of the ﬁrst and
second locally connected layers. FIP features can be used to recover the face image y in the canonical view. y is the ground truth. Best viewed in color.
maintain discriminativeness between different identities.
(2) Unlike conventional face descriptors, the FIP features
can be used to reconstruct a face image in the canonical
view. We also demonstrate signiﬁcant improvement of the
existing methods, when they are applied on our reconstructed face images. (3) Unlike existing works that need to know
the pose of a probe face, so as to build models for different
poses speciﬁcally, our method can extract the FIP features
without knowing information on pose and illumination.
The FIP features outperform the state-of-the-art methods,
including both 2D-based and 3D-based methods, on the
MultiPIE database .
2. Related Work
This section reviews related works on learning-based
face descriptors and deep models for feature learning.
Learning-based descriptors. Cao et al. devised an
unsupervised feature learning method (LE) with randomprojection trees and PCA trees, and adopted PCA to gain
a compact face descriptor. Zhang et al. extended 
by introducing an inter-modality encoding method, which
can match face images in two modalities, e.g. photos and
sketches, signiﬁcantly outperforming traditional methods
 . There are studies that learn the ﬁlters and patterns
for the existing handcrafted descriptors. For example, Guo
 proposed a supervised learning approach with
the Fisher separation criterion to learn the patterns of LBP
 . Zhen et al. adopted a strategy similar to LDA
to learn the ﬁlters of LBP. Our FIP features are learned
with a multi-layer deep model in a supervised manner, and
have more discriminative and representative power than
the above works.
We illustrate the feature space of FIP
compared with LE and LBP in Fig.2 (a), (b) and (d),
respectively, which show that the FIP space better maintains
both the intra-identity consistency and the inter-identity
discriminativeness.
Deep models.
The deep models learn representations
by stacking many hidden layers, which are layer-wisely
trained in an unsupervised manner. For example, the deep
belief networks (DBN) and deep Boltzmann machine
 (DBM) stack many layers of restricted Boltzmann
machines (RBM) and can extract different levels of features.
Recently, Huang et al. introduced the convolutional
restricted Boltzmann machine (CRBM), which incorporates
local ﬁlters into RBM. Their learned ﬁlters can preserve the
local structures of data. Sun et al. proposed a hybrid Convolutional Neural Network-Restricted Boltzmann
Machine (CNN-RBM) model to learn relational features
for comparing face similarity.
Unlike DBN and DBM
employ fully connected layers, our deep network combines
both locally and fully connected layers, which enables it to
extract both the local and global information. The locally
connected architecture of our deep network is similar to
CRBM , but we learn the network with a supervised
scheme and the FIP features are required to recover the
frontal face image. Therefore, this method is more robust
to pose and illumination variations, as shown in Fig.2 (d).
3. Network Architecture
Fig.3 shows the architecture of our deep model. The
input is a face image x0 under an arbitrary pose and
illumination, and the output is a frontal face image under
neutral illumination y. They both have n0 = 96 × 96 =
9216 dimensions. The feature extraction layers have three
locally connected layers and two pooling layers, which
encode x0 into FIP features x3.
In the ﬁrst layer, x0 is transformed to 32 feature maps
through a weight matrix W 1 that contains 32 sub-matrices
W 1 = [W 1
2 ; . . . ; W 1
∈Rn0,n01, each of
which is sparse to retain the locally connected structure
 . Intuitively, each row of W 1
i represents a small ﬁlter
centered at a pixel of x0, so that all of the elements in this
row equal zeros except for the elements belonging to the
ﬁlter. As our weights are not shared, the non-zero values of
these rows are not the same2. Therefore, the weight matrix
W 1 results in 32 feature maps {x1
i=1, each of which has
n0 dimensions. Then, a matrix V 1, where Vij ∈{0, 1}
encodes the 2D topography of the pooling layer , downsamples each of these feature map to 48 × 48 in order to
reduce the number of parameters need to be learned and
obtain more robust features. Each x1
i can be computed as3
i = V 1σ(W 1
where σ(x) = max(0, x) is the rectiﬁed linear function
 that is feature-intensity-invariant.
So it is robust to
shape and illumination variations. x1 can be obtained by
concatenating all the x1
i ∈R48×48 together, obtaining a
large feature map in n1 = 48 × 48 × 32 dimensions.
In the second layer, each x1
i is transformed to x2
i 32 submatrices {W 2
i ∈R48×48,48×48,
i is down-sampled using V 2 to 24×24 dimensions.
Eq.2 means that each small feature map in the ﬁrst layer is
multiplied by 32 sub-matrices and then summed together.
Here, each sub-matrix has sparse structure as discussed
above. We can reformulate Eq.2 into a matrix form
x2 = V 2σ(W 2x1),
where W 2 = [W 2′
1 ; . . . ; W 2′
32], ∀W 2′
∈R48×48,n1 and
1; . . . ; x1
32] ∈Rn1, respectively. W 2′
obtained by repeating W 2
i for 32 times.
Thus, x2 has
n2 = 24 × 24 × 32 dimensions.
In the third layer, x2 is transformed to x3, i.e. the FIP
features, similar to the second layer, but without pooling.
1In our notation, X ∈Ra,b means X is a two dimensional matrix
with a rows and b columns. x ∈Ra×b means x is a vector with a × b
dimensions. Also, [x; y] means that we concatenate vectors or matrices
x and y column-wisely, while [xy] means that we concatenate x and y
row-wisely.
2For the convolutional neural network such as , the non-zero values
are the same for each row.
3Note that in the conventional deep model , there is a bias term b, so
that the output is σ(Wx + b). Since Wx + b can be written as f
drop the bias term b for simpliﬁcation.
Thus, x3 is the same size as x2.
x3 = σ(W 3x2),
where W 3 = [W 3
1 ; . . . ; W 3
i ∈R24×24,n2 and x2 =
1; . . . ; x2
32] ∈Rn2, respectively.
Finally, the reconstruction layer transforms the FIP
features x3 to the frontal face image y, through a weight
matrix W 4 ∈Rn0,n2,
y = σ(W 4x3).
4. Training
Training our deep network requires estimating all the
weight matrices {W i} as introduced above, which is challenging because of the millions of parameters. Therefore,
we ﬁrst initialize the weights and then update them all. V 1
and V 2 are manually deﬁned and ﬁxed.
4.1. Parameter Initialization
We cannot employ RBMs to unsupervised pre-train
the weight matrices, because our input/output data are in
different spaces. Therefore, we devise a supervised method
based on the least square dictionary learning. As shown in
Fig.3, X3 = {x3
i=1 are a set of FIP features and Y =
i=1 are a set of target images, where m denotes the
number of training examples. Our objective is to minimize
the reconstruction error
W 1,W 2,W 3,W 4 ∥Y −σ(W 4X3) ∥2
where ∥· ∥F is the Frobenius norm. Optimizing Eq.6 is
not trivial because of its nonlinearity.
However, we can
initialize the weight matrices layer-wisely as
W 1 ∥Y −OW 1X0 ∥2
W 2 ∥Y −PW 2X1 ∥2
W 3 ∥Y −QW 3X2 ∥2
W 4 ∥Y −W 4X3 ∥2
In Eq.7, X0 = {x0
i=1 is a set of input images.
has been introduced in Sec.3, so that W 1X0 results in 32
feature maps for each input. O is a ﬁxed binary matrix
that sums together the pixels in the same position of these
feature maps, which makes OW 1X0 at the same size as Y .
In Eq.8, X1 = {x1
i=1 is a set of outputs of the ﬁrst locally
connected layer before pooling and P is also a ﬁxed binary
matrix, which sums together the corresponding pixels and
rescales the results to the same size as Y . Q, X2 in Eq.9 are
deﬁned in the same way.
Intuitively, we ﬁrst directly use X0 to approximate
Y with a linear transform W 1 without pooling.
W 1 has been initialized, X1 = V 1σ(W 1X0) is used
to approximate Y again with another linear transform,
We repeat this process until all the matrices have
been initialized.
A similar strategy has been adopted
by , which learns different levels of representations
with a convolutional architecture. All of the above equations have closed-form solutions.
For example, W 0 =
(OT O)−1(OT Y X0T )(X0X0T )−1. The other matrices can
be computed in the same way.
4.2. Parameter Update
We update all the weight matrices after the initialization
by minimizing the loss function of reconstruction error
E(X0; W) =∥Y −Y ∥2
where W = {W 1, . . . , W 4}. X0 = {x0
i }, Y = {yi},
and Y = {yi} are a set of input images, a set of target
images, and a set of reconstructed images, respectively. We
update W using the stochastic gradient descent, in which
the update rule of W i, i = 1 . . . 4, in the k-th iteration is
∆k+1 = 0.9 · ∆k −0.004 · ϵ·W i
k+1 = ∆k+1 + W i
where ∆is the momentum variable , ϵ is the learning
= xi−1(ei)T is the derivative, which is
computed as the outer product of the back-propagation error
ei and the feature of the previous layer xi−1. In our deep
network, there are three different expressions of ei. First,
for the transformation layer, e4 is computed based on the
derivative of the linear rectiﬁed function 
 [y −y]j,
j = [W 4x3]j. [·]j denotes the j-th element of a
Similarly, back-propagation error for e3 is computed as
[W 4T e4]j,
j = [W 3x2]j.
We compute e1 and e2 in the same way as e3 since they
both adopt the same activation function. There is a slight
difference due to down-sampling. For these two layers, we
must up-sample the corresponding back-propagation error e
so that it has the same dimensions as the input feature. This
strategy has been introduced in . We need to enforce the
weight matrices to have locally connected structures after
each gradient step as introduced in . We implement this
by setting the corresponding matrix elements to zeros, if
there supposed to be no connections.
5. Experiments
We conduct two sets of experiments. Sec.5.1 compares
with state-of-the-art methods and learning-based descriptors. Sec.5.2 demonstrates that classical face recognition
methods can be signiﬁcantly improved when applied on our
reconstructed face images in the canonical view.
To extensively evaluate our method under
different poses and illuminations, we select the MultiPIE
face database , which contains 754,204 images of 337
identities.
Each identity has images captured under 15
poses and 20 illuminations. These images were captured
in four sessions during different periods. Like the previous
methods , we evaluate our algorithm on a subset
of the MultiPIE database, where each identity has images
from all the four sections under seven poses from yaw
angles −45◦∼+45◦, and 20 illuminations marked as ID
00-19 in MultiPIE. This subset has 128,940 images.
5.1. Face Recognition
The existing works conduct experiments on MultiPIE
with three different settings: Setting-I was introduced in
 ; Setting-II and Setting-III were introduced in
 . We describe these settings below.
Setting-I and Setting-II only adopt images with different poses, but with neutral illumination marked as ID 07.
They evaluate robustness to pose variations. For Setting-I,
the images of the ﬁrst 200 identities in all the four sessions
are chosen for training, and the images of the remaining
137 identities for test. During test, one frontal image (i.e.
0◦) of each identity in the test set is selected to the gallery,
so there are 137 gallery images in total. The remaining
images from −45◦∼+45◦except 0◦are selected as
probes. For Setting-II, only the images in session one are
used, which only has 249 identities.
The images of the
ﬁrst 100 identities are for training, and the images of the
remaining 149 identities for test. During test, one frontal
image of each identity in the test set is selected in the
gallery. The remaining images from −45◦∼+45◦except
0◦are selected as probes.
Setting-III also adopts images in session one for training
and test, but it utilizes the images under all the 7 poses
and 20 illuminations.
This is to evaluate the robustness
when both pose and illumination variations are present. The
selection of probes and gallery are the same as Setting-II.
We evaluate both the FIP features and the reconstructed
images using the above three settings.
Face images are
roughly aligned according to the positions of eyes, and
rescaled to 96×96. They are converted to grayscale images.
The mean value over the training set is subtracted from
each pixel.
For each identity, we use the images with
6 poses ranging from −45◦∼+45◦except 0◦, and 19
illuminations marked as ID 00-19 except 07, as input to
train our deep network.
The reconstruction target is the
−45◦−30◦−15◦+15◦+30◦+45◦Avg
FA-EGFC 
SA-EGFC 
CRBM +LDA 80.3
100.0 98.5
100.0 99.3
Table 1. Recognition rates under Setting-I. The ﬁrst and the second
highest rates are highlighted. “✓” indicates the method needs to know
the pose; “×”, otherwise.
−45◦−30◦−15◦+15◦+30◦+45◦Avg
CRBM +ℓ2
Table 2. Recognition rates under Setting-I. The proposed features are
compared with LE and CRBM using only the ℓ2 distance for face
recognition. The ﬁrst and the second highest rates are highlighted. “✓”
indicates the method needs to know the pose; “×”, otherwise.
image captured in 0◦under neutral illumination (ID 07).
In the test stage, in order to better demonstrate the proposed
methods, we directly adopt the FIP and the reconstructed
images (denoted as RL) as features for face recognition.
Results of Setting-I
In this setting, we show superior results in Table 1, where
the FIP and RL features are compared with four methods,
including LGBP , VAAM , FA-EGFC , and SA-
EGFC , and two learning-based descriptors, including
LE and CRBM .
As discussed in Sec.1, LGBP
is a 2D-based method, while VAAM, FA-EGFC, and SA-
EGFC used 3D face models. We apply LDA on LE, CRBM,
FIP, and RL to obtain compact features. Note that LGBP,
VAAM, and SA-EGFC need to know the pose of a probe,
which means that they build different models to account
for different poses speciﬁcally. We do not need to know
the pose of the probe, since our deep network can extract
FIP features and reconstruct the face image in the canonical
view given a probe under any pose and any illumination.
This is one of our advantages over existing methods.
Several observations can be made from Table 1. First,
RL performs best on the averaged recognition rates and ﬁve
poses. The improvement is larger for larger pose variations.
It is interesting to note that RL even outperforms all the
3D-based models, which veriﬁes that our reconstructed face
images in the canonical view are of high quality and robust
to pose changes. Fig.4 shows several reconstructed images,
indicating that RL can effectively remove the variations
of poses and illuminations, while still retains the intrinsic
shapes and structures of the identities.
−45◦−30◦−15◦+15◦+30◦+45◦Avg
100.0 100.0 97.0
100.0 100.0 98.6
Table 3. Recognition rates of RL+LDA compared with Li under
Setting-II. “✓” indicates the method needs to know the pose; “×”,
otherwise.
Recognition Rates on Different Poses
−45◦−30◦−15◦+15◦+30◦+45◦Avg
Recognition Rates on Different Illuminations
Table 4. Recognition rates of RL+LDA compared with Li under
Setting-III. “✓” indicates the method needs to know the pose; “×”,
otherwise.
Second, FIP features are better than the two learningbased descriptors and the other three methods except SA-
EGFC, which used the 3D model and required the pose of
the probe. We further report the results of FIP compared
with LE and CRBM using only ℓ2 distance in Table 2 .
The RL and FIP outperform the above two learning based
features, especially when large pose variations are present.
Third, although FIP does not exceed RL, its still a
valuable representation, because it has the sparse property
and can reconstruct RL efﬁciently and losslessly.
Results of Setting-II and Setting-III
Li et al. evaluated on these two settings and reported
the state-of-the-art results.
Setting-II covers only pose
variations and Setting-III covers both pose and illumination
variations.
For Setting-II, the results of RL+LDA compared with
 are reported in Table 3, which shows that RL obtains
the best results on all the poses. Note that the poses of
probes in are assumed to be given, which means they
trained a different model for each pose separately.
did not report detailed recognition rates when the poses of
the probes are unknown, except for describing a 20-30%
decline of the overall recognition rate.
For Setting-III, RL+LDA is compared with on
images with both pose and illumination variations. Table
4 reports that our approach achieves better results on all
the poses and illuminations. The recognition rate under a
pose is the averaged result over all the possible illuminations. Similarly, the recognition rate under one illumination
condition is the averaged result of all the possible poses. We
observe that the performance of RL+LDA under different
illuminations is close because RL can well remove the
effect of different types of illuminations.
5.2. Improve Classical Face Recognition Methods
In this section, we will show that the conventional feature
extraction and dimension reduction methods in the face
recognition literature, such as LBP , Gabor , PCA
 , LDA , and Sparse Coding (SC) , can achieve
signiﬁcant improvement when they adopt our reconstructed
images as input.
We conduct three experiments using the training/testing
data of Setting-I. First, we show the advantage of our
reconstructed images in the canonical view over the original
images. Second, we show the improvements of Gabor when
it is extracted on our reconstructed images. Third, we show
that LBP can be improved as well.
In the ﬁrst experiment, ℓ2 distance, SC, PCA, LDA, and
PCA+LDA are directly applied on the raw pixels of the
original images and our reconstructed images, respectively.
The recognition rates are reported in Fig.5(a), where the
results on the original images and the reconstructed images
are illustrated as solid bars (front) and hollow bars (back).
We observe that each of the above methods can be improved
at least 30% on average. They can achieve relatively high
performance on different poses, because our reconstruction
layer can successfully recover the frontal face image. For
example, the recognition rates of SC on different poses
using the original images are 20.9%, 43.6%, 65.0%, 66.1%,
38.3%, and 26.9%, respectively, while 92.7%, 97.1%,
97.8%, 98.5%, 97.8%, and 81.8%, respectively, using the
reconstructed images.
In the second experiment, we extract Gabor features on
both the original images and reconstructed images.
observe large improvements by using the reconstructed
images. Speciﬁcally, for each image in 96 × 96, we evenly
select 11 × 10 keypoints and apply 40 Gabor kernels (5
scales × 8 orientations) on each of these keypoints. We
again use the ℓ2 distance, PCA, LDA, and PCA+LDA for
face recognition. The results are shown in Fig.5(b).
In the third experiment, we extract LBP features on both
original images and reconstructed images. Speciﬁcally, we
divide each 96 × 96 image into 12 × 12 cells, and the 59
uniform binary patterns are computed in each cell.
then adopt the χ2 distance, PCA, LDA, and PCA+LDA for
face recognition. Fig.5(c) shows that LBP combined with
all these methods can also be signiﬁcantly improved. For
instance, the averaged recognition rate of LBP+χ2 using the
original images is 75.9%, and the corresponding accuracy
on our reconstructed images, i.e. RL+LBP+χ2, is 96.5%,
which is better than 94.9% of RL+ℓ2 in Table 2.
Recognition Rate(%)
Probe Pose(°)
Sparse Coding
(a) Pixels
Recognition Rate(%)
Probe Pose(°)
Gabor+PCA+LDA
(b) Gabor descriptors
Recognition Rate(%)
Probe Pose(°)
LBP+PCA+LDA
(c) LBP descriptors
Figure 5. The conventional face recognition methods can be improved
when they are applied on our reconstructed images. The results of three
descriptors (pixel intensity, Gabor, and LBP) and four face recognition
methods (ℓ2 or χ2 distance, sparse coding (SC), PCA, and LDA) are
reported in (a), (b) and (c), respectively.
The hollow bars are the
performance of these methods applied on our reconstructed images, while
the solid bars are on the original images.
6. Conclusion
We have proposed identity-preserving features for face
recognition.
The FIP features are not only robust to
pose and illumination variations, but can also be used to
reconstruct face images in the canonical view.
learned using a deep model that contains feature extraction
layers and a reconstruction layer. We show that FIP features
outperform the state-of-the-art face recognition methods.
We have aslo improved classical face recognition methods
by applying them on our reconstructed face images. In the
future work, we will extend the framework to deal with
robust face recognition in other difﬁcult conditions such
as expression change and face sketch recognition ,
and will combine FIP features with more classic face
recognition approaches to further improve the performance
 .