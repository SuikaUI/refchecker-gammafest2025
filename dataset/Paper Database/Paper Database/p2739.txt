ReﬁneNet: Multi-Path Reﬁnement Networks for
High-Resolution Semantic Segmentation
Guosheng Lin1,2, Anton Milan1, Chunhua Shen1,2, Ian Reid1,2
1The University of Adelaide,
2Australian Centre for Robotic Vision
{guosheng.lin;anton.milan;chunhua.shen;ian.reid}@adelaide.edu.au
very deep convolutional neural networks
(CNNs) have shown outstanding performance in object
recognition and have also been the ﬁrst choice for dense
classiﬁcation problems such as semantic segmentation.
However, repeated subsampling operations like pooling or
convolution striding in deep CNNs lead to a signiﬁcant decrease in the initial image resolution.
Here, we present
ReﬁneNet, a generic multi-path reﬁnement network that
explicitly exploits all the information available along the
down-sampling process to enable high-resolution prediction using long-range residual connections. In this way,
the deeper layers that capture high-level semantic features
can be directly reﬁned using ﬁne-grained features from earlier convolutions. The individual components of ReﬁneNet
employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which
captures rich background context in an efﬁcient manner. We
carry out comprehensive experiments and set new stateof-the-art results on seven public datasets. In particular,
we achieve an intersection-over-union score of 83.4 on the
challenging PASCAL VOC 2012 dataset, which is the best
reported result to date.
1. Introduction
Semantic segmentation is a crucial component in image
understanding. The task here is to assign a unique label (or
category) to every single pixel in the image, which can be
considered as a dense classiﬁcation problem. The related
problem of so-called object parsing can usually be cast as
semantic segmentation. Recently, deep learning methods,
and in particular convolutional neural networks (CNNs),
e.g., VGG , Residual Net , have shown remarkable results in recognition tasks. However, these approaches
exhibit clear limitations when it comes to dense prediction
in tasks like dense depth or normal estimation 
Figure 1. Example results of our method on the task of object parsing (left) and semantic segmentation (right).
and semantic segmentation . Multiple stages of spatial pooling and convolution strides reduce the ﬁnal image
prediction typically by a factor of 32 in each dimension,
thereby losing much of the ﬁner image structure.
One way to address this limitation is to learn deconvolutional ﬁlters as an up-sampling operation to generate high-resolution feature maps. The deconvolution operations are not able to recover the low-level visual features
which are lost after the down-sampling operation in the convolution forward stage. Therefore, they are unable to output
accurate high-resolution prediction. Low-level visual information is essential for accurate prediction on the boundaries or details. The method DeepLab recently proposed by
Chen et al. employs atrous (or dilated) convolutions to
account for larger receptive ﬁelds without downscaling the
image. DeepLab is widely applied and represents state-ofthe-art performance on semantic segmentation. This strategy, although successful, has at least two limitations. First,
it needs to perform convolutions on a large number of detailed (high-resolution) feature maps that usually have highdimensional features, which are computational expensive.
Moreover, a large number of high-dimensional and highresolution feature maps also require huge GPU memory resources, especially in the training stage. This hampers the
computation of high-resolution predictions and usually limits the output size to 1/8 of the original input. Second, dilated convolutions introduce a coarse sub-sampling of features, which potentially leads to a loss of important details.
 
Another type of methods exploits features from intermediate layers for generating high-resolution prediction, e.g.,
the FCN method in and Hypercolumns in . The intuition behind these works is that features from middle layers are expected to describe mid-level representations for
object parts, while retaining spatial information. This information is though to be complementary to the features from
early convolution layers which encode low-level spatial visual information like edges, corners, circles, etc., and also
complementary to high-level features from deeper layers
which encode high-level semantic information, including
object- or category-level evidence, but which lack strong
spatial information.
We argue that features from all levels are helpful for semantic segmentation. High-level semantic features helps
the category recognition of image regions, while low-level
visual features help to generate sharp, detailed boundaries
for high-resolution prediction. How to effectively exploit
middle layer features remains an open question and deserves more attentions. To this end we propose a novel network architecture which effectively exploits multi-level features for generating high-resolution predictions. Our main
contributions are as follows:
1. We propose a multi-path reﬁnement network (Re-
ﬁneNet) which exploits features at multiple levels
of abstraction for high-resolution semantic segmentation. ReﬁneNet reﬁnes low-resolution (coarse) semantic features with ﬁne-grained low-level features in a
recursive manner to generate high-resolution semantic feature maps. Our model is ﬂexible in that it can be
cascaded and modiﬁed in various ways.
2. Our cascaded ReﬁneNets can be effectively trained
end-to-end, which is crucial for best prediction performance. More speciﬁcally, all components in Re-
ﬁneNet employ residual connections with identity mappings , such that gradients can be directly
propagated through short-range and long-range residual connections allowing for both effective and efﬁcient end-to-end training.
3. We propose a new network component we call
“chained residual pooling” which is able to capture
background context from a large image region. It does
so by efﬁciently pooling features with multiple window sizes and fusing them together with residual connections and learnable weights.
4. The proposed ReﬁneNet achieves new state-of-theart performance on 7 public datasets, including PAS-
CAL VOC 2012, PASCAL-Context, NYUDv2, SUN-
RGBD, Cityscapes, ADE20K, and the object parsing
Person-Parts dataset. In particular, we achieve an IoU
score of 83.4 on the PASCAL VOC 2012 dataset, outperforming the currently best approach DeepLab by a
large margin.
To facilitate future research, we release both source code
and trained models for our ReﬁneNet.1
1.1. Related Work
CNNs become the most successful methods for semantic segmentation in recent years.
The early methods in
 are region-proposal-based methods which classify
region proposals to generate segmentation results. Recently
fully convolution network (FCNNs) based based methods
 show effective feature generation and end-toend training, and thus become the most popular choice for
semantic segmentation. FCNNs have also been widely applied in other dense-prediction tasks, e.g., depth estimation
 , image restoration , image super-resolution
The proposed method here is also based on fully
convolution-style networks.
FCNN based methods usually have the limitation of lowresolution prediction. There are a number of proposed techniques which addressed this limitation and aim to generate
high-resolution predictions. The atrous convolution based
approach DeepLab-CRF in directly output a middleresolution score map then applies the dense CRF method
 to reﬁne boundaries by leveraging color contrast information. CRF-RNN extends this approach by implementing recurrent layers for end-to-end learning of the
dense CRF and FCNN. Deconvolution methods 
learn deconvolution layers to up-sample the low-resolution
predictions.
The depth estimation method employs
super-pixel pooling to output high-resolution prediction.
There are several existing methods which exploit middle layer features for segmentation. The FCN method in
 adds prediction layers to middle layers to generate
prediction scores at multiple resolutions.
They average
the multi-resolution scores to generate the ﬁnal prediction
mask. Their system is trained in a stage-wise manner rather
than end-to-end training. The method Hypercolumn 
merges features from middle layers and learns dense classiﬁcation layers. Their method employs stage-wise training
instead of end-to-end training. The method Seg-Net and
U-Net apply skip-connections in the deconvolution architecture to exploit the features from middle layers.
Although there are a few existing work, how to effectively exploit middle layer features remains an open question. We propose a novel network architecture, ReﬁneNet,
to address this question. The network architecture of Re-
ﬁneNet is clearly different from existing methods.
ﬁneNet consists of a number of specially designed components which are able to reﬁne the coarse high-level semantic
1Our source code will be available at 
guosheng/refinenet
features by exploiting low-level visual features. In particular, ReﬁneNet employs short-range and long-range residual
connections with identity mappings which enable effective
end-to-end training of the whole system, and thus help to
archive good performance. Comprehensive empirical results clearly verify the effectiveness of our novel network
architecture for exploiting middle layer features.
2. Background
Before presenting our approach, we ﬁrst review the
structure of fully convolutional networks for semantic segmentation in more detail and also discuss the recent
dilated convolution technique which is speciﬁcally designed to generate high-resolution predictions.
Very deep CNNs have shown outstanding performance
on object recognition problems.
Speciﬁcally, the recently proposed Residual Net (ResNet) has shown
step-change improvements over earlier architectures, and
ResNet models pre-trained for ImageNet recognition tasks
are publicly available. Because of this, in the following we
adopt ResNet as our fundamental building block for semantic segmentation. Note, however, that replacing it with any
other deep network is straightforward.
Since semantic segmentation can be cast as a dense classiﬁcation problem, the ResNet model can be easily modiﬁed
for this task. This is achieved by replacing the single label
prediction layer with a dense prediction layer that outputs
the classiﬁcation conﬁdence for each class at every pixel.
This approach is illustrated in Fig. 2(a). As can be seen, during the forward pass in ResNet, the resolution of the feature
maps (layer outputs) is decreased, while the feature depth,
i.e. the number of feature maps per layer (or channels) is
increased. The former is caused by striding during convolutional and pooling operations.
The ResNet layers can be naturally divided into 4 blocks
according to the resolution of the output feature maps, as
shown in Fig. 2(a). Typically, the stride is set to 2, thus reducing the feature map resolution to one half when passing
from one block to the next. This sequential sub-sampling
has two effects: ﬁrst it increases the receptive ﬁeld of convolutions at deeper levels, enabling the ﬁlters to capture
more global and contextual information which is essential
for high quality classiﬁcation; second it is necessary to keep
the training efﬁcient and tractable because each layer comprises a large number of ﬁlters and therefore produces an
output which has a corresponding number of channels, thus
there is a trade-off between the number of channels and resolution of the feature maps. Typically the ﬁnal feature map
output ends up being 32 times smaller in each spatial dimension than the original image (but with 1000s of channels).
This low-resolution feature map loses important visual details captured by early low-level ﬁlters, resulting in a rather
coarse segmentation map. This issue is a well-known limitation of deep CNN-based segmentation methods.
An alternative approach to avoid lowering the resolution while retaining a large receptive ﬁeld is to use dilated (atrous) convolution. This method introduced in ,
has the state-of-the-art performance on semantic segmentation. The sub-sampling operations are removed (the stride
is changed from 2 to 1), and all convolution layers after
the ﬁrst block use dilated convolution. Such a dilated convolution (effectively a sub-sampled convolution kernel) has
the effect of increasing the receptive ﬁeld size of the ﬁlters without increasing the number of weights that must be
learned (see illustration in Fig. 2(b)). Even so, there is a
signiﬁcant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of
feature maps at higher resolution. For example, if we retain
all channels in all layers to be at least 1/4 of the original image resolution, and consider a typical number of ﬁlter channels to be 1024, then we can see that the memory capacity
of even high-end GPUs is quickly swamped by very deep
networks. In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than
1/8 size of the original rather than 1/4, when using a deep
In contrast to dilated convolution methods, in this paper
we propose a means to enjoy both the memory and computational beneﬁts of deresolving, while still able to produce
effective and efﬁcient high-resolution segmentation prediction, as described in the following section.
3. Proposed Method
We propose a new framework that provides multiple
paths over which information from different resolutions and
via potentially long-range connections, is assimilated using
a generic building block, the ReﬁneNet. Fig. 2(c) shows
one possible arrangement of the building blocks to achieve
our goal of high resolution semantic segmentation. We begin by describing the multi-path reﬁnement arrangement
in Sec. 3.1 followed by a detailed description of each Re-
ﬁneNet block in Sec. 3.2.
3.1. Multi-Path Reﬁnement
As noted previously, we aim to exploit multi-level features for high-resolution prediction with long-range residual connections. ReﬁneNet provides a generic means to
fuse coarse high-level semantic features with ﬁner-grained
low-level features to generate high-resolution semantic feature maps. A crucial aspect of the design ensures that the
gradient can be effortlessly propagated backwards through
the network all the way to early low-level layers over longrange residual connections, ensuring that the entire network
can be trained end-to-end.
For our standard multi-path architecture, we divide the
pre-trained ResNet (trained with ImageNet) into 4 blocks
Prediction
Dilated convolutions
Multi-Path
Figure 2. Comparison of fully convolutional approaches for dense classiﬁcation. Standard multi-layer CNNs, such as ResNet (a) suffer
from downscaling of the feature maps, thereby losing ﬁne structures along the way. Dilated convolutions (b) remedy this shortcoming
by introducing atrous ﬁlters, but are computationally expensive to train and quickly reach memory limits even on modern GPUs. Our
proposed architecture that we call ReﬁneNet (c) exploits various levels of detail at different stages of convolutions and fuses them to obtain
a high-resolution prediction without the need to maintain large intermediate feature maps. The details of the ReﬁneNet block are outlined
in Sec. 3 and illustrated in Fig 3.
Chained Residual
Adaptive Conv.
Multi-path input
Multi-resolution Fusion
Chained Residual Pooling
Output Conv.
RCU: Residual Conv Unit
Multi-resolution Fusion
Figure 3. The individual components of our multi-path reﬁnement network architecture ReﬁneNet. Components in ReﬁneNet employ
residual connections with identity mappings. In this way, gradients can be directly propagated within ReﬁneNet via local residual connections, and also directly propagate to the input paths via long-range residual connections, and thus we achieve effective end-to-end training
of the whole system.
according to the resolutions of the feature maps, and employ
a 4-cascaded architecture with 4 ReﬁneNet units, each of
which directly connects to the output of one ResNet block
as well as to the preceding ReﬁneNet block in the cascade.
Note, however, that such a design is not unique. In fact,
our ﬂexible architecture allows for a simple exploration of
different variants. For example, a ReﬁneNet block can accept input from multiple ResNet blocks. We will analyse
a 2-cascaded version, a single-block approach as well as a
2-scale 7-path architecture later in Sec. 4.3.
We denote ReﬁneNet-m as the ReﬁneNet block that connects to the output of block-m in ResNet. In practice, each
ResNet output is passed through one convolutional layer to
adapt the dimensionality. Although all ReﬁneNets share the
same internal architecture, their parameters are not tied, allowing for a more ﬂexible adaptation for individual levels
of detail. Following the illustration in Fig. 2(c) bottom up,
we start from the last block in ResNet, and connect the output of ResNet block-4 to ReﬁneNet-4. Here, there is only
one input for ReﬁneNet-4, and ReﬁneNet-4 serves as an extra set of convolutions which adapt the pre-trained ResNet
weights to the task at hand, in our case, semantic segmentation. In the next stage, the output of ReﬁneNet-4 and the
ResNet block-3 are fed to ReﬁneNet-3 as 2-path inputs. The
goal of ReﬁneNet-3 is to use the high-resolution features
from ResNet block-3 to reﬁne the low-resolution feature
map output by ReﬁneNet-4 in the previous stage. Similarly,
ReﬁneNet-2 and ReﬁneNet-1 repeat this stage-wise reﬁnement by fusing high-level information from the later layers
and high-resolution but low-level features from the earlier
ones. As the last step, the ﬁnal high-resolution feature maps
are fed to a dense soft-max layer to make the ﬁnal prediction in the form of a dense score map. This score map is
then up-sampled to match the original image using bilinear
interpolation.
The entire network can be efﬁciently trained end-to-end.
It is important to note that we introduce long-range residual connections between the blocks in ResNet and the Re-
ﬁneNet modules. During the forward pass, these long-range
residual connections convey the low-level features that encode visual details for reﬁning the coarse high-level feature
maps. In the training step, the long-range residual connections allow direct gradient propagation to early convolution
layers, which helps effective end-to-end training.
3.2. ReﬁneNet
The architecture of one ReﬁneNet block is illustrated in
Fig. 3(a). In the multi-path overview shown in Fig 2(c),
ReﬁneNet-1 has one input path, while all other ReﬁneNet
blocks have two inputs. Note, however, that our architecture
is generic and each Reﬁne block can be easily modiﬁed to
accept an arbitrary number of feature maps with arbitrary
resolutions and depths.
Residual convolution unit.
The ﬁrst part of each Re-
ﬁneNet block consists of an adaptive convolution set that
mainly ﬁne-tunes the pretrained ResNet weights for our
task. To that end, each input path is passed sequentially
through two residual convolution units (RCU), which is a
simpliﬁed version of the convolution unit in the original
ResNet , where the batch-normalization layers are removed (cf. Fig. 3(b)). The ﬁlter number for each input path
is set to 512 for ReﬁneNet-4 and 256 for the remaining ones
in our experiments.
Multi-resolution fusion. All path inputs are then fused into
a high-resolution feature map by the multi-resolution fusion
block, depicted in Fig. 3(c). This block ﬁrst applies convolutions for input adaptation, which generate feature maps
of the same feature dimension (the smallest one among the
inputs), and then up-samples all (smaller) feature maps to
the largest resolution of the inputs.
Finally, all features
maps are fused by summation. The input adaptation in this
block also helps to re-scale the feature values appropriately
along different paths, which is important for the subsequent
sum-fusion. If there is only one input path (e.g., the case
of ReﬁneNet-4 in Fig. 2(c)), the input path will directly go
through this block without changes.
Chained residual pooling. The output feature map then
goes through the chained residual pooling block, schematically depicted in Fig. 3(d). The proposed chained residual
pooling aims to capture background context from a large
image region. It is able to efﬁciently pool features with
multiple window sizes and fuse them together using learnable weights.
In particular, this component is built as a
chain of multiple pooling blocks, each consisting of one
max-pooling layer and one convolution layer. One pooling block takes the output of the previous pooling block as
input. Therefore, the current pooling block is able to re-use
the result from the previous pooling operation and thus access the features from a large region without using a large
pooling window. If not further speciﬁed, we use two pooling blocks each with stride 1 in our experiments.
The output feature maps of all pooling blocks are fused
together with the input feature map through summation of
residual connections. Note that, our choice to employ residual connections also persists in this building block, which
once again facilitates gradient propagation during training.
In one pooling block, each pooling operation is followed by
convolutions which serve as a weighting layer for the summation fusion. It is expected that this convolution layer will
learn to accommodate the importance of the pooling block
during the training process.
Output convolutions.
The ﬁnal step of each ReﬁneNet
block is another residual convolution unit (RCU). This results in a sequence of three RCUs between each block. To
reﬂect this behavior in the last ReﬁneNet-1 block, we place
two additional RCUs before the ﬁnal softmax prediction
step. The goal here is to employ non-linearity operations
on the multi-path fused feature maps to generate features
for further processing or for ﬁnal prediction. The feature
dimension remains the same after going through this block.
3.3. Identity Mappings in ReﬁneNet
Note that all convolutional components of the ReﬁneNet
have been carefully constructed inspired by the idea behind
residual connections and follow the rule of identity mapping . This enables effective backward propagation of
the gradient through ReﬁneNet and facilitates end-to-end
learning of cascaded multi-path reﬁnement networks.
Employing residual connections with identity mappings
allows the gradient to be directly propagated from one block
to any other blocks, as was recently shown by . This
concept encourages to maintain a clean information path
for shortcut connections, so that these connections are not
“blocked” by any non-linear layers or components. Instead,
non-linear operations are placed on branches of the main
information path. We follow this guideline for developing
the individual components in ReﬁneNet, including all convolution units. It is this particular strategy that allows the
multi-cascaded ReﬁneNet to be trained effectively. Note
that we include one non-linear activation layer (ReLU) in
the chained residual pooling block. We observed that this
ReLU is important for the effectiveness of subsequent pooling operations and it also makes the model less sensitive to
changes in the learning rate. We observed that one single
ReLU in each ReﬁneNet block does not noticeably reduce
the effectiveness of gradient ﬂow.
We have both short-range and long-range residual connections in ReﬁneNet. Short-range residual connections refer to local shot-cut connections in one RCU or the residual
pooling component, while long-range residual connections
refer to the connection between ReﬁneNet modules and the
ResNet blocks. With long-range residual connections, the
gradient can be directly propagated to early convolution layers in ResNet and thus enables end-to-end training of all
network components.
The fusion block fuses the information of multiple shortcut paths, which can be considered as performing summation fusion of multiple residual connections with necessary
dimension or resolution adaptation. In this aspect, the role
of the multi-resolution fusion block here is analogous to
the role of the “summation” fusion in a conventional residual convolution unit in ResNet. There are certain layers in
ReﬁneNet, and in particular within the fusion block, that
perform linear feature transformation operations, like linear
feature dimension reduction or bilinear up-sampling. These
layers are placed on the shortcut paths, which is similar to
the case in ResNet . As in in ResNet, when a shortcut
connection crosses two blocks, it will include a convolution
layer in the shortcut path for linear feature dimension adaptation, which ensures that the feature dimension matches the
subsequent summation in the next block. Since only linear
transformation are employed in these layers, gradients still
can be propagated through these layers effectively.
4. Experiments
To show the effectiveness of our approach, we carry
out comprehensive experiments on seven public datasets,
which include six popular datasets for semantic segmentation on indoors and outdoors scenes , and one dataset for object parsing called
Person-Part. The segmentation quality is measured by the
intersection-over-union (IoU) score , the pixel accuracy
Table 1. Object parsing results on the Person-Part dataset. Our
method achieves the best performance (bold).
Attention 
LG-LSTM 
Graph-LSTM 
DeepLab 
DeepLab-v2 (Res101) 
ReﬁneNet-Res101 (ours)
Table 2. Ablation experiments on NYUDv2 and Person-Part.
Initialization
Chained pool.
Person-Parts
ResNet-101
ResNet-101
ResNet-152
and the mean accuracy over all classes. As commonly
done in the literature, we apply simple data augmentation
during training. Speciﬁcally, we perform random scaling
(ranging from 0.7 to 1.3), random cropping and horizontal
ﬂipping of the images. If not further speciﬁed, we apply
test-time multi-scale evaluation, which is a common practice in segmentation methods . For multi-scale evaluation, we average the predictions on the same image across
different scales for the ﬁnal prediction. We also present an
ablation experiment to inspect the impact of various components and an alternative 2-cascaded version of our model.
Our system is built on MatConvNet .
4.1. Object Parsing
We ﬁrst present our results on the task of object parsing,
which consists of recognizing and segmenting object parts.
We carry out experiments on the Person-Part dataset 
which provides pixel-level labels for six person parts including Head, Torso, Upper/Lower Arms and Upper/Lower
Legs. The rest of each image is considered background.
There are training 1717 images and 1818 test images. We
use four pooling blocks in our chained residual pooling for
this dataset.
We compare our results to a number of state-of-the-art
methods, listed in Table 1.
The results clearly demonstrate the improvement over previous works. In particular,
we signiﬁcantly outperform the the recent DeepLab-v2 approach which is based on dilated convolutions for highresolution segmentation, using the same ResNet as initialization. In Table 2, we present an ablation experiment to
quantify the inﬂuence of the following components: Network depth, chained residual pooling and multi-scale evaluation (Msc Eva), as described earlier.
This experiment
shows that each of these three factors can improve the overall performance. Qualitative examples of our object parsing
on this dataset are shown in Fig.4.
(a) Test Image
(b) Ground Truth
(c) Prediction
Figure 4. Our prediction examples on Person-Parts dataset.
4.2. Semantic Segmentation
We now describe our experiments on dense semantic
labeling on six public benchmarks and show that our Re-
ﬁneNet outperforms previous methods on all datasets.
The NYUDv2 dataset consists of 1449
RGB-D images showing interior scenes. We use the segmentation labels provided in , in which all labels are
mapped to 40 classes. We use the standard training/test split
with 795 and 654 images, respectively. We train our models
only on RGB images without using the depth information.
Quantitative results are shown in Table 3. Our ReﬁneNet
achieves new state-of-the-art result on the NYUDv2 dataset.
Similar to the object parsing task above, we also perform
ablation experiments on the NYUDv2 dataset to evaluate
the effect of different settings. The results are presented in
Table 2. Once again, this study demonstrates the beneﬁts of
adding the proposed chained residual pooling component
and deeper networks, both of which consistently improve
Table 3. Segmentation results on NYUDv2 (40 classes).
training data
pixel acc.
Gupta et al. 
FCN-32s 
FCN-HHA 
Context 
ReﬁneNet-Res152
Table 4. Segmentation results on the Cityscapes test set.
method achieves the best performance.
FCN-8s 
Dilation10 
Context 
LRR-4x 
DeepLab 
DeepLab-v2(Res101) 
ReﬁneNet-Res101 (ours)
the performance as measured by IoU.
PASCAL VOC 2012 is a well-known segmentation
dataset which includes 20 object categories and one background class.
This dataset is split into a training set, a
validation set and a test set, with 1464, 1449 and 1456
images each.
Since the test set labels are not publicly
available, all reported results have been obtained from the
VOC evaluation server.
Following the common convention , the training set is augmented by additional annotated VOC images provided in as well as
with the training data from the MS COCO dataset . We
compare our ReﬁneNet on the PASCAL VOC 2012 test set
with a number of competitive methods, showing superior
performance. We use dense CRF method in for further
reﬁnement for this dataset, which gives marginal improvement of 0.1% on the validation set. Since dense CRF only
brings very minor improvement on our high-resolution prediction, we do not apply it on other datasets.
The detailed results for each category and the mean IoU
scores are shown in Table 5. We achieve an IoU score of
83.4, which is the best reported result on this challenging
dataset to date.2 We outperform competing methods in almost all categories. In particular, we signiﬁcantly outperform the method DeepLab-v2 which is the currently
best known dilation convolution method and uses the same
ResNet-101 network as initialization. Selected prediction
examples are shown in Fig. 5.
Cityscapes is a very recent dataset on street scene images from 50 different European cities. This dataset provides ﬁne-grained pixel-level annotations of roads, cars,
pedestrians, bicycles, sky, etc. The provided training set
has 2975 images and the validation set has 500 images. In
2The result link to the VOC evaluation server:
 
robots.ox.ac.uk:8080/anonymous/B3XPSK.html
Table 5. Results on the PASCAL VOC 2012 test set (IoU scores). Our ReﬁneNet archives the best performance (IoU 83.4).
FCN-8s 
DeconvNet 
CRF-RNN 
BoxSup 
Context 
DeepLab 
DeepLab2-Res101 
CSupelec-Res101 
ReﬁneNet-Res101
ReﬁneNet-Res152
(a) Test Image
(b) Ground Truth
(c) Prediction
Figure 5. Our prediction examples on VOC 2012 dataset.
total, 19 classes are considered for training and evaluation.
The test set ground-truth is withheld by the organizers, and
we evaluate our method on the their evaluation server. The
test results are shown in Table 4. In this challenging setting, our architecture again outperforms previous methods.
A few test images along with ground truth and our predicted
semantic maps are shown in Fig. 6.
(a) Test Image
(b) Ground Truth
(c) Prediction
Figure 6. Our prediction examples on Cityscapes dataset.
PASCAL-Context.
The PASCAL-Context dataset
provides the segmentation labels of the whole scene for the
PASCAL VOC images.
We use the segmentation labels
which contain 60 classes (59 object categories plus background) for evaluation as well as the provided training/test
splits. The training set contains 4998 images and the test
set has 5105 images. Results are shown in Table 6. Even
without additional training data and with the same underlying ResNet architecture with 101 layers, we outperform the
previous state-of-the-art achieved by DeepLab.
SUN-RGBD is a segmentation dataset that contains
around 10, 000 RGB-D indoor images and provides pixel
labeling masks for 37 classes. Results are shown in Table
7. Our method outperforms all existing methods by a large
margin across all evaluation metrics, even though we do not
make use of the depth information for training.
Table 6. Segmentation results on PASCAL-Context dataset (60
classes). Our method performs the best. We only use the VOC
training images.
Extra train data
FCN-8s 
BoxSup 
HO-CRF 
Context 
DeepLab-v2(Res101) 
COCO (∼100K)
ReﬁneNet-Res101 (ours)
ReﬁneNet-Res152 (ours)
Table 7. Segmentation results on SUN-RGBD dataset (37 classes).
We compare to a number of recent methods. Our ReﬁneNet signiﬁcantly outperforms the existing methods.
Train data
Pixel acc.
Liu et al. 
Ren et al. 
Kendall et al. 
Context 
ReﬁneNet-Res101
ReﬁneNet-Res152
Table 8. Segmentation results on the ADE20K dataset (150
classes) val set. our method achieves the best performance.
FCN-8s 
SegNet 
DilatedNet 
Cascaded-SegNet 
Cascaded-DilatedNet 
ReﬁneNet-Res101 (ours)
ReﬁneNet-Res152 (ours)
Table 9. Evaluations of 4 variants of cascaded ReﬁneNet: single ReﬁneNet, 2-cascaded ReﬁneNet, 4-cascaded ReﬁneNet, 4cascaded ReﬁneNet with 2-scale ResNet on the NYUDv2 dataset.
We use the 4-cascaded version as our main architecture throughout
all experiments in the paper because this turns out to be the best
compromise between accuracy and efﬁciency.
Initialization
single ReﬁneNet
2-cascaded ReﬁneNet
4-cascaded ReﬁneNet
4-cascaded 2-scale ReﬁneNet
ADE20K MIT is a newly released dataset for scene
parsing which provides dense labels of 150 classes on more
than 20K scene images.
The categories include a large
variety of objects (e.g., person, car, etc.) and stuff (e.g.,
sky, road, etc.). The provided validation set consisting of
2000 images is used for quantitative evaluation. Results are
shown in Table 8. Our method clearly outperforms the baseline methods described in .
4.3. Variants of cascaded ReﬁneNet
As discussed earlier, our ReﬁneNet is ﬂexible in that it
can be cascaded in various manners for generating various
architectures. Here, we discuss several variants of our Re-
ﬁneNet. Speciﬁcally, we present the architectures of using a
single ReﬁneNet, a 2-cascaded ReﬁneNet and a 4-cascaded
ReﬁneNet with 2-scale ResNet.
The architectures of all
three variants are illustrated in Fig. 7. The architecture of 4cascaded ReﬁneNet is already presented in Fig. 2(c). Please
note that this 4-cascaded ReﬁneNet model is the one used
in all other experiments.
The single ReﬁneNet model is the simplest variant of
our network.
It consists of only one single ReﬁneNet
block, which takes all four inputs from the four blocks of
ResNet and fuses all-resolution feature maps in a single process. The 2-cascaded version is similar our main model (4cascaded) from Fig. 2(c), but employs only two ReﬁneNet
modules instead of four. The bottom one, ReﬁneNet-2, has
two inputs from ResNet blocks 3 and 4, and the other one
has three inputs, two coming from the remaining ResNet
blocks and one from ReﬁneNet-2. For the 2-scale model in
Fig. 7(c), we use 2 scales of the image as input and respectively 2 ResNets to generate feature maps; the input image
is scaled to a factor of 1.2 and 0.6 and fed into 2 independent
The evaluation results of these variants on the NYUD
dataset are shown in Table 9. This experiment demonstrates
that the 4-cascaded version yields better performance than
the 2-cascaded and 1-cascaded version, and using 2-scale
image input with 2 ResNet is better than using 1-scale input.
This is expected due to the larger capacity of the network.
However, it also results in longer training times. Hence, we
resort to using the single-scale 4-cascaded version as the
standard architecture in all our experiments.
5. Conclusion
We have presented ReﬁneNet, a novel multi-path reﬁnement network for semantic segmentation and object parsing. The cascaded architecture is able to effectively combine high-level semantics and low-level features to produce
high-resolution segmentation maps.
Our design choices
are inspired by the idea of identity mapping which facilitates gradient propagation across long-range connections
and thus enables effective end-to-end learning. We outperform all previous works on seven public benchmarks, setting a new mark for the state of the art in semantic labeling.
Acknowledgments
This research was supported by the
Australian Research Council through the Australian Centre for Robotic Vision (CE140100016).
C. Shen’s participation was supported by an ARC Future Fellowship
(FT120100969). I. Reid’s participation was supported by
Single ReﬁneNet
2-cascaded ReﬁneNet
4-cascaded 2-scale ReﬁneNet
1.2x input
0.6x input
Prediction
Prediction
Prediction
Figure 7. Illustration of 3 variants of our network architecture: (a) single ReﬁneNet, (b) 2-cascaded ReﬁneNet and (c) 4-cascaded ReﬁneNet
with 2-scale ResNet. Note that our proposed ReﬁneNet block can seamlessly handle different numbers of inputs of arbitrary resolutions
and dimensions without any modiﬁcation.
an ARC Laureate Fellowship (FL130100102).