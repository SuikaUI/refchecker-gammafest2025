HAL Id: hal-01351226
 
Submitted on 3 Aug 2016
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Quality Assessment of Wikipedia Articles without
Feature Engineering
Quang-Vinh Dang, Claudia-Lavinia Ignat
To cite this version:
Quang-Vinh Dang, Claudia-Lavinia Ignat. Quality Assessment of Wikipedia Articles without Feature
Engineering. Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, Jun
2016, Newark, United States. pp.27-30, ￿10.1145/2910896.2910917￿. ￿hal-01351226￿
Quality Assessment of Wikipedia Articles without Feature
Engineering
Quang - Vinh Dang
Université de Lorraine, LORIA, F-54506
Inria, F-54600
CNRS, LORIA, F-54506
 
Claudia - Lavinia Ignat
Inria, F-54600
Université de Lorraine, LORIA, F-54506
CNRS, LORIA, F-54506
 
As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of
attention during the last decade. Most research eﬀorts focused on classiﬁcation of Wikipedia articles quality by using
a diﬀerent feature set. However, so far, no “golden feature
set” was proposed.
In this paper, we present a novel approach for classifying Wikipedia articles by analysing their
content rather than by considering a feature set. Our approach uses recent techniques in natural language processing
and deep learning, and achieved a comparable result with
the state-of-the-art.
quality assessment, Wikipedia, feature engineering, document representation, deep learning
INTRODUCTION
Internet has opened the border of traditional libraries:
nowadays everyone can participate and contribute to a common human knowledge repository. Wikipedia is a great example of a knowledge resource receiving contribution from
a huge number of authors. At the time of writing, there are
more than ﬁve millions articles in English Wikipedia, and
38 million articles in all languages1, and the size of English
Wikipedia is over 60 times compared with Britannica2.
However, due to the huge number of contributors3 and
articles, the quality of Wikipedia articles is not equally distributed . Several research works claimed that the quality of centralized human knowledge resources such as books
or Britannica are higher than Wikipedia .
1 of
Wikipedia as on 5 - Jan - 2016.
2 
comparisons as on 5 - Jan - 2016.
3According
 
Wikipedians, there are more than 100,000 regular Wikipedia
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
JCDL ’16, June 19-23, 2016, Newark, NJ, USA
c⃝2016 ACM. ISBN 978-1-4503-4229-2/16/06...$15.00
DOI: 
In order to improve the quality of Wikipedia, an eﬀective method is needed for quality assessment of its articles.
Wikipedia deﬁnes quality classes for its articles, including
FA, A, GA, B, C, Start, Stub where FA is the highest quality class and Stub the lowest quality class4.
Assigning the correct quality class for each Wikipedia article is an important task, as authors and reviewers can be
notiﬁed to pay more attention for improving the low quality articles, and search engines could promote high quality
class articles as query result.
However, the high velocity
of changes on Wikipedia makes impossible a manual quality assessment of articles by human experts. Therefore, it
is important to design an automatic approach for quality
assessment of Wikipedia articles.
Existing approaches on this topic 
are all based on deﬁning a feature set that is believed to
describe in the best way the quality of a Wikipedia article.
Certain approaches claim that longer articles are of a better
quality, some others consider that discussions and interactions among authors and reviewers of an article increase the
quality of an article and others consider that the quality of
an article is determined by contributions of highly respected
There is no standard rule for selecting features, which is
considered as one of the most diﬃcult tasks in machine learning. Moreover, feature selection is language dependent. In
this paper, we present a new approach that avoids feature
engineering and that determines the quality of an article
based on its content. We build a deep neural network model
where the input is the full content of the Wikipedia articles,
and the output is the quality class of the articles. The same
approach can be deﬁned for diﬀerent language data sets.
We start by presenting related works in quality assessment of Wikipedia articles.
We then present our classiﬁcation model including article representation and the deep
neural networks technique that we used for the classiﬁcation.
We then describe the evaluation we performed and
we compare our results with state-of-the-art techniques. Finally, we present our concluding remarks and we provide
some directions for future work.
RELATED WORKS
Even though existing research works on automatic quality assessment of Wikipedia articles use a diﬀerent feature
set, they can be classiﬁed into two main families: one is
analyzing the edit history of an article (for instance, who
4The description of each quality class is available at https:
//en.wikipedia.org/wiki/Template:Grading scheme.
contributed to the article and the type of their modiﬁcations) and the second one is analyzing the article itself (for
instance, its length, number of images, presence of an information box).
Belonging to the ﬁrst family of approaches, measures
the quality of Wikipedia articles based on author authority.
Using a similar idea, applied authors’ h-index to measure the quality of articles on Wikipedia. In , the authors
used both metrics of article’s content and authors’ authority to measure the quality of Wikipedia articles. However,
this research work used a manual evaluation by volunteering
students which is not very reliable for verifying the classi-
ﬁcation. Moreover, the accuracy obtained is not very high.
 analyzed the edit network around a Wikipedia article to
retrieve the information about the quality of that article. 
presented a model that analysed the collaboration between
authors and reviewers on Wikipedia to measure the quality
of articles.
On the other hand, as the most simple approach, proposed to use simple word count to evaluate the quality of
Wikipedia. Dalip et al analyzed the eﬀect of diﬀerent
feature sets including structure, length, style, review, network and readability in a regression model for measuring
the quality of Wikipedia articles and they discussed about a
minimal feature set . More recently, used a machine
learning model for quality prediction of Wikipedia articles
including format-based features such as the number of headings of level 2 of a particular article. Based on the work of
 , Wikimedia Foundation built an online service called
ORES to classify the quality class of Wikipedia articles ,
using a set of 24 features for English Wikipedia. This set of
features is slightly diﬀerent for other languages Wikipedia.
Each research work selected and used a diﬀerent feature
set to measure and classify the quality of Wikipedia articles.
However, feature selection is mostly based on the heuristic of
researchers and so far, there is no “gold - standard” feature
set to classify and measure the quality of Wikipedia articles.
In this paper, we claim that the quality of a Wikipedia article should depend on its own content. Certain features can
be derived from the article content. Using the full content
of Wikipedia articles as the input of training model should
avoid missing an important feature that was not manually
recognized.
We use the technique Doc2Vec to represent Wikipedia
articles and a deep neural network to classify their quality.
Deep learning is an emerging research ﬁeld today and, to
our knowledge, our work is the ﬁrst one that applied deep
learning for assessing quality of Wikipedia articles. Our approach provides a novel point of view to Wikipedia quality
classiﬁcation.
CLASSIFICATION MODEL
In this section, we present how to design and feed the
content of Wikipedia articles into a neural network.
Article representation
Most machine learning algorithms including neural networks require the input to be represented as a ﬁxed-length
feature vector. As Wikipedia articles have diﬀerent lengths,
we need an approach that maps Wikipedia articles to ﬁxedlength feature vectors. The most common ﬁxed-length vector representation for documents is the bag-of-words where
a document is represented as the bag of its words. However,
this approach disregards semantics and even word order.
In this paper, we applied the unsupervised learning algorithm called Paragraph Vector, recently known as Doc2Vec
 that learns vector representations for variable-length
pieces of texts and overcomes the disadvantages of bag-ofwords by taking into account the order and semantics of
words. In this approach every word and every paragraph
are mapped to a unique vector.
The paragraph vector is
concatenated with several word vectors from the paragraph
and trained in order to predict the next word in a text window.
While word vectors are shared among paragraphs,
paragraph vectors are unique among paragraphs.
We applied the Doc2Vec approach where each Wikipedia
document corresponds to a paragraph in the above description. While the generated word vectors are not further used,
the document vector is given as input for our deep neural
Deep neural networks
Deep learning has been successfully applied for several
text classiﬁcation tasks such as Reuters news or sentiment
analysis .
Neural networks, or artiﬁcial neural networks (ANN), are
machine learning models inspired by biological neural networks for the estimation of generally unknown functions that
depend on a large number of parameters. Neural networks
are typically organized in layers made up of a number of
interconnected nodes which contain an activation function.
Patterns are presented to the network via the input layer,
which communicates to one or more hidden layers. The hidden layers perform the actual processing via a system of
weighted connections. The hidden layers then transmit the
answer to an output layer. A deep neural network (DNN)
 is deﬁned as an artiﬁcial neural network with multiple
hidden layers that allows learning abstraction from data.
In our approach, we used a DNN with four hidden layers
to learn and classify the representation vectors of Wikipedia
articles computed by Doc2Vec.
IMPLEMENTATION AND RESULTS
Implementation
We used the data set contained in around 30,000 English
Wikipedia articles which are classiﬁed to six quality classes
FA, GA, B, C, Start, Stub already by Wikipedia reviewers.
The data set is provided by Wikimedia Foundations5. We
separated the data set to training and testing set with the
ratio 80/20, similarly to and ORES .
We transformed all Wikipedia articles on both training
and testing set to Doc2Vec vectors by using the library gensim67. The output of the ﬁrst phase is the collection of vectors for Wikipedia articles in the training and testing set.
Therefore, we have a dataset of 30,000 same length vectors.
In the second phase, we trained the DNN model on the training set by using tensorﬂow 8, the deep learning library from
5The data set is available at 
public-datasets/enwiki/.
6 
7Our hypothesis is that the labeled articles in the training
set and the unlabeled articles in the testing set are all completed and available.
8 
Google. Our DNN has four hidden layers, with 2000, 1000,
500 and 200 neurons respectively9. The number of neurons
is selected as a rule of thumb. The ﬁnal task is to apply the
trained DNN model on the testing vector set, and compare
the predicting quality labels with correct values assigned by
human judgements.
Currently, no standard methodology exists for constructing an optimal neural network with the right number of layers and number of neurons for each layer. An optimal neural network can be built uniquely empirically . However,
randomly choosing a structure for a deep neural network is
not a good solution as it leads to performances of a random
guess, i.e. a low accuracy of 16.7%.
The predictions obtained by our model are displayed by
the confusion matrix in Table 1. The training loss graph in
Fig. 1 illustrates the training loss value, i.e. the diﬀerence
between predicted quality labels and their correct values in
the training set, as a function of the number of iterations
during the training phase. The graph shows that no local
minima is found when the number of training steps reaches
a high value (25,000), as the decreasing trend is observed
throughout the entire training process.
As the data set we used is balanced, i.e. the number of
articles in each class is very close, the accuracy metric is
suitable to evaluate the classiﬁcation. Accuracy is deﬁned
as a ratio between the number of correct predictions and
the total number of articles in the testing data set.
accuracy of our DNN classiﬁer is 55%.
We compare our approach with other popular classiﬁcation approaches on the same data set. Using the 24 features
of ORES as the feature set, k-Nearest Neighbor (k-NN) ,
Classiﬁcation And Regression Tree (CART) and Random
Forest(RF) implemented by ORES, achieved the accuracy of
51%, 48% and 60% respectively. Using the feature set composed of 11 features presented in which is a subset of
the 24 features set used in ORES, Random Forest algorithm
achieved the accuracy of 58%. The performance of classiﬁers
is summarized in Table 2.
The accuracy of DNN is higher than the one obtained by
the k-NN and CART approaches.
The lower accuracy of
DNN classiﬁer with respect to the RF approach can be explained by the parameter of Doc2Vec transformation. Due
to our computation power, the size of the vectors was limited to 500, which may lead to the consequence that Doc2Vec
vectors did not capture all the structure of the Wikipedia
articles. Moreover, the low accuracy is also due to our unoptimized DNN, as no standard way exists for constructing a
DNN. We can see the improvement from to ORES when
more features are added.
To our knowledge, Wikimedia ORES API, which is based
on the work of is the only existing approach for
classiﬁcation into all six quality classes. Other works only
classiﬁed between a subset of classes, such as between FA
and Start with an accuracy of 84%, or between FA-GA
as a class and the set of (B, C, Start, Stub) as another
class with an accuracy of 84%, or between FA-GA and
C-Start with an accuracy of 66%. For these binary classiﬁcation tasks, the DNN approach achieved a very high accuracy compared with previous approaches: 99% to classify
9The implementation is available at 
vinhqdang/doc2vec dnn wikipedia
Confusion matrix of classifying quality classes.
Gray cells are correct predictions. Rows (italic) are actual
quality class. Columns are predicted values of the model.
For instance, there are 778 articles correctly predicted as
FA, and 160 articles which are GA and are predicted as FA.
Doc2Vec & DNN
Warncke et al. 
Wikimedia ORES 
Table 2: Accuracy scores of diﬀerent classiﬁers on English
Wikipedia.
between FA and Start, 86% to classify between FA-GA and
the other classes and 90% to classify between FA-GA and
We observe that the quality class of a Wikipedia article
could be determined by only analyzing its content, so the approach of training the prediction model based on the content
and not on feature sets is a promising and interesting approach to be improved in the future. As Doc2Vec approach
is language independent we expect that our approach can
be generally applied to any language Wikipedia.
CONCLUSIONS AND FUTURE WORKS
Feature selection is one of the most diﬃcult task in machine learning.
Existing research works proposed diﬀerent feature sets for measuring quality of Wikipedia articles.
Each feature set has its own pros and cons, and there is no
“golden feature set”. As feature selection process is mostly
a manual work, we may never know what feature set is the
best for assessing quality of Wikipedia articles.
In this paper, we presented an approach to avoid feature selection process.
Our approach follows the process
of Wikipedia reviewers: ﬁrst they read the article and then
decide what quality class this article should belong to. Using
this approach, no feature selection is required to describe a
Wikipedia article. We achieved very high accuracy scores
for classiﬁcation into binary quality classes and an accuracy
score comparable with the state-of-the-art Wikimedia ORES
service for classiﬁcation between all quality classes.
As a future work we plan to improve performances of our
approach by optimizing the deep neural network’s structure.
ACKNOWLEDGMENTS
Experiments presented in this paper were carried out using the Grid’5000 testbed, supported by a scientiﬁc inter-
Figure 1: DNN training loss
est group hosted by Inria and including CNRS, RENATER
and several Universities as well as other organizations (see