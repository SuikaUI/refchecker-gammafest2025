INFORMATION THEORETIC CLUSTERING
FOR UNSUPERVISED DOMAIN-ADAPTATION
Subhadeep Dey Srikanth Madikeri Petr Motlicek
Idiap-RR-09-2016
APRIL 2016
Centre du Parc, Rue Marconi 19, P.O. Box 592, CH - 1920 Martigny
T +41 27 721 77 11 F +41 27 721 77 12 www.idiap.ch
INFORMATION THEORETIC CLUSTERING FOR UNSUPERVISED DOMAIN-ADAPTATION
Subhadeep Dey1,2, Srikanth Madikeri1 and Petr Motlicek1
1Idiap Research Institute, Martigny, Switzerland
2Ecole Polytechnique F´ed´erale de Lausanne, Lausanne, Switzerland
{subhadeep.dey, srikanth.madikeri, petr.motlicek}@idiap.ch
The aim of the domain-adaptation task for speaker veriﬁcation is
to exploit unlabelled target domain data by using the labelled source
domain data effectively. The i-vector based Probabilistic Linear Discriminant Analysis (PLDA) framework approaches this task by clustering the target domain data and using each cluster as a unique
speaker to estimate PLDA model parameters. These parameters are
then combined with the PLDA parameters from the source domain.
Typically, agglomerative clustering with cosine distance measure is
used. In tasks such as speaker diarization that also require unsupervised clustering of speakers, information-theoretic clustering measures have been shown to be effective. In this paper, we employ
the Information Bottleneck (IB) clustering technique to ﬁnd speaker
clusters in the target domain data. This is achieved by optimizing the
IB criterion that minimizes the information loss during the clustering process. The greedy optimization of the IB criterion involves agglomerative clustering using the Jensen-Shannon divergence as the
distance metric. Our experiments in the domain-adaptation task indicate that the proposed system outperforms the baseline by about
14% relative in terms of equal error rate.
Index Terms— Speaker veriﬁcation, Domain adaptation, Information theoretic measures, PLDA model.
1. INTRODUCTION
The i-vector based PLDA system requires large collection of labelled
data (speaker labels) to deliver state-of-the-art performance .
However in realistic applications, it might be too expensive to provide labelled speaker data for every domain of interest. For instance,
we consider the task of speaker veriﬁcation on recordings from social media (e.g. Youtube, Facebook). In such a problem, a large
amount of unlabelled data from the target (e.g. Youtube) domain
may be provided while the system is built using telephone corpora
speaker-labelled recordings obtained from the Switchboard
(SWB) database ).
The workshop on domain-adaptation1 for
speaker veriﬁcation targeted this problem by creating two development datasets, namely labelled data from Switchboard database and
unlabelled NIST Speaker Recognition Evaluation (SRE) data from
SRE04 to SRE08.
In the literature, many approaches have been explored to adapt
the speaker recognition system to target domain data. Most of these
approaches focus on reducing the mismatch at the model level. Particularly, in the i-vector PLDA based speaker veriﬁcations systems,
adapting the PLDA parameters has been shown to be successful . Another recent work at the model level uses a collection of
1 
Fig. 1. Traditional framework of the speaker veriﬁcation system.
whitening matrices for (length) normalizing the i-vectors . However, the whiteners are estimated from the source domain. Alternatively, the inter-dataset variation is compensated at the i-vector level
using techniques such as Nuisance Attribute Projection (NAP) .
Among all the above mentioned approaches, the target domain
data is best used when adapting PLDA model parameters. To adapt
these parameters, the target domain data is clustered and each cluster
is assumed to represent a unique speaker. These speaker labels are
then used to adapt the source domain PLDA parameters to the target
domain .
Speaker clustering on the unlabelled data is clearly a critical process that determines the success of model adaptation. A similar task
where this problem occurs is speaker diarization, which labels the
speaker segments in a speech recording in an unsupervised fashion.
Commonly used model-based techniques use the Hidden Markov
Model/Gaussian Mixture Model (HMM/GMM) to model the
entire conversation. Alternatively, a non-parametric method using
the Information Bottleneck (IB) method has been used . Unlike
HMM/GMM that clusters GMMs estimated from long segments of
the audio, the IB method divides input speech into relatively short
(∼2.5 s) segments which are then iteratively merged using IB criterion to obtain the ﬁnal speaker segments. The IB criterion minimizes
the information loss in the clustering process while simultaneously
ﬁnding compact representation of the data. In this paper, we explore
IB method to perform speaker clustering on the unlabelled target
domain data to be subsequently applied in speaker veriﬁcation. In
this work, we apply IB to cluster i-vectors to obtain speaker labels
from the unlabelled (target domain) data. The distance metric for IB
clustering uses Jensen-Shannon (JS) divergence which is shown to
outperform conventional metrics.
The paper is organized as follows: Sections 2 and 3 describe the
framework for the domain-adaptation task and the baseline speaker
veriﬁcation system, respectively. Section 4 presents the proposed
information theoretic approach and Section 5 describes the experimental section. Finally, the paper is concluded in Section 6.
2. DOMAIN-ADAPTATION FRAMEWORK
As shown in Figure 1, the training process of the traditional (i.e., ivector PLDA) speaker veriﬁcation module can be divided into two
• Unsupervised Phase: In this phase the parameters of the
Universal Background Model (UBM), Total variability matrix (T) and the Whitening matrix (W) are estimated. With
reference to domain-adaptation, it was observed in that
using the target domain data to estimate parameters of the
UBM and T-matrix does not improve the performance of the
speaker veriﬁcation system, however computing the whitening matrix on the target domain data signiﬁcantly improves
the performance.
• Supervised Phase: It consists of training the PLDA model
parameters which requires speaker labels and multiple occurrences of the speaker.
3. BASELINE SPEAKER VERIFICATION SYSTEM
In simpliﬁed PLDA model , an i-vector (x) is decomposed into
speaker factor and residual as follows:
x = µ + Vy + ϵ,
where the matrix V represents inter-class variability, y is the latent
variable which follows a Gaussian distribution with zero mean (0)
and identity covariance matrix (E); the residual ϵ follows the Gaussian distribution with zero mean (0) and full covariance matrix (Σ),
µ is the mean of the i-vectors. The distance between two i-vectors
(x1 and x2) is computed as:
S(x1, x2) = p(x1, x2|Hs)
p(x1, x2|Hd),
where the hypothesis Hs is that the two i-vectors share the same
speaker latent variable (y) and the hypothesis Hd is that the two ivectors do not share the same latent variable. A closed form solution
of Equation 2 can be found in . In the PLDA model, an i-vector
follows a Gaussian distribution as N(µ, Φ + Σ), where Φ = V Vt.
The adaptation procedure, as described in , of the source (Vout
and Σout) and target (Vin and Σin) domain PLDA model are given
by the following equations:
Σ = αΣout + (1 −α)Σin,
Φ = α(VoutV′
out) + (1 −α)(VinV′
where (Σ, Φ) are adapted parameters of the new PLDA model and
the parameter α (∈ ) balances the contribution of the source and
target domain PLDA model .
To estimate the PLDA model parameters Vin and Σin on the
target domain data, speaker labels are required. Thus, the data is
clustered and each cluster is assigned to a unique speaker label. For
instance, agglomerative clustering of i-vectors using a simple cosine
distance metric is shown to provide good speaker labels for successful PLDA adaptation and is used as the baseline system.
4. INFORMATION BOTTLENECK METHOD FOR
SPEAKER CLUSTERING
In the IB framework, the input variable U is associated with a relevance variable (R), which signiﬁes some information that needs to
Fig. 2. Framework of the proposed system.
be preserved during clustering. For instance in speech, this information can be related to acoustic classes, speaker identity, etc. The
IB ﬁnds K clusters C = {C1, C2, · · · , CK} of the input data U
such that (i) it is the most compact representation of the data, and
(ii) it preserves most of the information in the relevance variables after clustering. Mathematically, this is equivalent to maximizing the
following objective function (F):
F = I (R, C) −1
β I (U, C) ,
where I is the Mutual Information (MI) between two random variables. The ﬁrst term in the right hand side of Equation 4 signiﬁes the
amount of information preserved after clustering while the second
MI term restricts the compactness of the clusters. The Lagrangian
parameter (β) is used to control the trade-off between these two
To optimize the IB criterion in Equation 4, a greedy technique
may be used. This translates to agglomerative clustering of the data.
At each iteration, two clusters are merged such that the value of the
objective function increases. This is given by:
∆F = (p(Ci) + p(Cj))di,j,
where di,j is the combination of Jensen-Shannon (JS) divergence between two distributions:
di,j = JS(p(R|Ci), p(R|Cj))−1
β JS((p(U|Ci), p(U|Cj)), (6)
where the function JS is the Jensen-Shannon divergence measure
between two probability distributions. The JS divergence between
two probability distributions is the sum of the Kullback-Leibler divergences between the individual distributions and the average distribution .
As mentioned earlier, when applied to speaker diarization, the
clusters are speaker segments (usually of length 2.5s). The relevance
variables are posterior vectors obtained for each speech frame from
a GMM estimated on the audio recording being diarized. Each element in the posterior vector is the posterior probability of a GMM
component.
4.1. IB clustering algorithm
The IB based clustering algorithm, as used in speaker diarization
task, is summarized below:
• Input: (a) data, (b) posterior probability of the relevance variable with respect to the input variable U, (c) trade-off parameter (β).
• Output: Clusters C = {C1, C2, · · · , CK}, where K is the
number of desired clusters.
• Initialize the algorithm with each data point as its own cluster.
• Compute ∆F(Ci, Cj) for all possible combinations of the
clusters Ci and Cj.
• For l=1, · · · , L (L is maximum number of clusters):
– Merge the two closest clusters with the maximum ∆F.
– Recompute the ∆F between the new cluster and all
other clusters.
The maximum number of clusters can be ﬁxed in multiple ways.
For instance, in speaker diarization a normalized MI based criterion
is used. This is convenient as the MI terms are already computed.
4.2. IB for speaker clustering
In this section we adapt the IB clustering approach to obtain speaker
labels on target domain data in the i-vector PLDA framework. The
posteriors of relevance variables, input to the IB clustering algorithm
as described above, need to be deﬁned. A natural extension of the
speaker diarization system would be to use posteriors of Gaussian
components from the UBM. However, discarding completely the
feature vectors of a recording and using only the posteriors of Gaussian components (from UBM) for speaker clustering task can be suboptimal. A better way to compute the posterior of the relevance variables is to use i-vector representation accounting for both the posteriors of Gaussian components (from UBM) and features vectors of
an utterance. We build on the success of clustering i-vectors in 
by deriving relevance variables based on i-vector cosine distances.
We present two approaches based on the discussion above:
1. Average zeroth order statistics:
Let O = { o1, o2, · · · , oT } be an utterance with T number of
feature vectors. Let us suppose that an UBM is also trained on
these feature vectors with qth Gaussian component of UBM
represented by Bq. Use these Gaussian components of the
UBM (Bq) as the relevance variables and the posterior probability of the relevance variables is computed by:
p(Bq|O) = 1
The probability p(Bq|ot) can be computed from the parameters of the UBM. The quantity p(Bq|O) is referred to as the
average zeroth order statistics. The posterior probabilities of
the relevance variables are computed for each of the utterances of target domain data. The IB clustering as described
in Section 4.1 is used to obtain speaker labels with (a) input
data being the utterances of target domain data (O), and (b)
posterior probability of relevance variable as the average zeroth order statistics (p(Bq|O)).
2. Dot product:
The dot-product between i-vectors can be converted to probability scale and is used directly as the posterior probability
of the relevance variable as described below:
• Let A be the cosine distance measure matrix between
the i-vectors in the target domain, where (m, n)th element of matrix A is given by:
Am,n = 1 −
||xm||||xn||,
Table 1. Performance of the i-vector based PLDA system on NIST-
SRE 2010 male evaluation set by estimating the parameters in different datasets.
UBM and T-matrix
which is the cosine distance between the i-vectors xm
• Convert each of these entries of matrix A into posterior
probability of the relevance variable (pm,n) using softmax function as given by the following equation:
n expAm,n .
The quantity pm,n, referred to as posterior probability of the relevance variables, is computed for all the
i-vectors. The IB clustering as described in Section 4.1
is used to obtain speaker labels with, (a) input data being the i-vectors of target domain data (x), and (b) the
posterior probability of relevance variable as the quantity pm,n.
In this paper, we hypothesize that the speaker vectors in the
PLDA space are more discriminative and thus will result in better
clustering than the original i-vector space. The i-vector projected
in PLDA space (PLDA-vector) is expressed by the following equation:
ˆy = (E + VtΣV)−1VtΣ−1(x −µ).
The projected vector in the PLDA space is obtained for each
i-vector and we refer to them as PLDA-vector.
5. EXPERIMENTS
As deﬁned in the the domain-adaptation challenge protocol2, the
Switchboard (SWB) and NIST Speaker Recognition Evaluation
(SRE) datasets are used for system development. The Switchboard
dataset is the source domain data and the domain adaptation protocol dictates that labels of this data are known. The SRE dataset is
referred to as the target domain dataset as it matches the evaluation
condition and labels of this dataset are unknown. The Switchboard
data consists of 33,039 utterances with 3,114 speakers (both male
and female). The SRE dataset contains 36,470 utterances drawn
from the speaker evaluations dataset (SRE04 to SRE 08) with 3,500
speakers (male and female combined), out of which 13,628 utterances belong to the male set. The evaluation set is drawn from SRE
2010 evaluation condition 5, which is telephone enrolment and telephone test; the evaluation set consists of 3,465 target and 175,873
non-target male trials. In this paper, we report our results on the
male set in terms of Equal Error Rate (EER) and minimum Decision
Cost Function (minDCF) .
From Table 1, we observe that training the PLDA model with
only the source domain data (SWB) results in 3.8% EER, whereas
training the PLDA model with a labelled SRE development data
gives 2.1%. Ideally, we want to reach the performance close to 2.1%
EER (and 0.193 minDCF) using the unlabelled target domain data.
2 
Table 2. Performance of the baseline systems (speaker clustering
based on cosine distance and PLDA scores) on NIST-SRE 2010 male
evaluation set after domain adaptation.
1 – i-vector
2 – PLDA-vector
3 – PLDA scores
5.1. Baseline system (Speaker clustering based on cosine distance and PLDA scores)
The performance of various baseline systems, namely (i) i-vectors
with cosine distance (System 1), (ii) PLDA-vector with cosine distance (System 2), and (iii) PLDA Scores (System 3) is shown in
Table 2. For System 1, the unlabelled target domain i-vectors are
used for agglomerative clustering with cosine distance as metric.
For System 2, the PLDA-vectors are obtained using Equation
and these PLDA-vectors are used for agglomerative clustering with
cosine distance as the distance metric. As shown in Table 2, the
performance of System 2 is marginally better than the ﬁrst system
in terms of minDCF. The best performance is obtained with 1,000
clusters for both the systems, which is close to the actual number
of speakers in the target domain dataset (1,115 speakers). The best
performance is obtained with interpolation parameter α ∈[0.2, 0.3]
suggesting that the contribution of the source domain parameters is
greater than target domain PLDA parameters. System 3 was developed as follows: the distance between i-vectors was computed as in
Equation 2 by using the source domain PLDA model parameters.
This distance metric (PLDA score) is used for agglomerative clustering and the clustered output is used to train target domain PLDA
model. The best performance is obtained with a α value of 0.2 and
the performance of the system is better than System 1 and 2 in terms
of minDCF.
5.2. Speaker clustering based on IB algorithm
The input to IB clustering algorithm as explained in Section 4, the
posterior probabilities of the relevance variable, is critical in obtaining speaker labels. We explore three choices of posterior probabilities for speaker clustering:
• Average zeroth order statistics: As hitherto explained, we
use average zeroth order statistics of an utterance as posterior
probability of the relevance variable for clustering. From Table 3 (ﬁrst row), we observe that it performs better than the
baseline systems in terms of EER and minDCF. Thus the average zeroth order statistics of an utterance carries sufﬁcient
speaker discriminative information for IB clustering as evident from the results obtained.
• i-vector dot product: As described in Section 4, we convert
the cosine distances between i-vectors to posterior probability, and IB clustering is used to obtain speaker labels. This
system performs worse than the zeroth order statistics based
system but still outperforms the baseline system in terms of
minDCF. Although i-vectors are obtained from the average
zeroth order statistics and feature vectors, the i-vector dot
product system still performs worse than the average zeroth
order statistics system on using IB method. Thus, the poste-
Table 3. Performance of the proposed system (speaker clustering
based on IB algorithm) on NIST-SRE 2010 male evaluation set. All
systems perform better than the baseline systems in Table 2.
Relevance Variable
Average zeroth order statistics
i-vector dot product
PLDA-vector dot product
rior probability of the relevance variable obtained by converting the dot product scores, as explained in Section 4, looses
speaker discriminative information for clustering.
• PLDA-vector dot product: First we estimate the parameters
of the source domain PLDA model. The i-vectors are projected in the source domain PLDA space to obtain PLDAvectors as given by Equation 9. The posterior probability
is computed as described in Section 4 by replacing the dot
product between two i-vectors by dot product between two
PLDA-vectors and IB clustering is used. It can be observed
from Table 3 (third row) that this system performs the best
in terms of EER and minDCF and is able to bridge the gap
in performance up to 1.3% EER (from 3.8% to 2.5%). The
PLDA-vectors are computed using a discriminative classiﬁer
(PLDA model) and hence IB method exploits the posterior
probability of relevance variable using PLDA-vectors in an
effective way.
6. CONCLUSIONS AND FUTURE WORK
We observed that training the PLDA model using labelled target
domain data results in 45% reduction in EER compared to using
only labelled source domain data (from absolute 3.8% to 2.1%).
While assuming that the labels of target domain data is not known,
we proposed to explore agglomerative clustering with different distance metrics to obtain speaker labels. The baseline system uses dot
product distance metric for agglomerative clustering and it provides
performance of 2.9% EER. Furthermore, we explored IB clustering
technique (based on JS divergence metric) for obtaining speakers labels and found that it provides 14% relative improvement over the
baseline system (from absolute 2.9% to 2.5%). We observed that the
i-vector dot product system with IB clustering is the worst performing system which could be the effect of uncalibrated scores. Thus
in future, we plan to perform calibration of scores obtained from
dot product and subsequently derive the posterior probability of the
relevance variables.
7. ACKNOWLEGEMENT
This work was supported by the project EU FP7 project EU Speaker
Identiﬁcation Integrated Project (SIIP).
8. REFERENCES
 Najim Dehak, Patrick J. Kenny, Reda Dehak, Pierre Dumouchel, and Pierre Ouellet,
“Front end factor analysis for
speaker veriﬁcation,” IEEE Transactions on Audio, Speech and
Language Processing, 2010.
 J.J. Godfrey, E.C. Holliman, and J. McDaniel, “Switchboard:
telephone speech corpus for research and development,”
Acoustics, Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, Mar 1992, vol. 1, pp.
517–520 vol.1.
 Daniel Garcia Romero and Alan McCree, “Supervised domain
adaptation for i-vector based speaker recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 2014,
pp. 4047–4051.
 Daniel Garcia Romero, Alan McCree, Stephen Shum, Niko
Brummer, and Carlos Vaquero, “Unsupervised domain adaptation for i-vector speaker recognition,”
in Proceedings of
Odyssey, Joensuu, Finland, 2014., 2014, pp. 260–264.
 Stephen H. Shum, Douglas A. Reynolds, Daniel Garcia-
Romero, and Alan McCree,
“Unsupervised clustering approaches for domain adaptation in speaker recognition,”
Proceedings of Odyssey, Joensuu, Finland, 2014., 2014, pp.
 Elliot Singer, Douglas Reynolds, et al.,
“Domain mismatch compensation for speaker recognition using a library of
whiteners,” Signal Processing Letters, IEEE, vol. 22, no. 11,
pp. 2000–2003, 2015.
 Hagai Aronowitz, “Inter dataset variability compensation for
speaker recognition,”
in IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 2014, pp. 4002–4006.
 J. Ajmera and C. Wooters, “A robust speaker clustering algorithm,” in Automatic Speech Recognition and Understanding,
2003. ASRU ’03. 2003 IEEE Workshop on, Nov 2003, pp. 411–
 J. Ajmera,
H. Bourlard,
I. Lapidot,
and I. McCowan,
“Unknown-multiple speaker clustering using hmm,”
PROCEEDINGS OF ICSLP-2002, 2002, pp. 573–576.
 Deepu Vijayasenan, Fabio Valente, and Herv´e Bourlard, “An
information theoretic combination of MFCC and TDOA features for speaker diarization,” IEEE Transactions on Audio,
Speech & Language Processing, vol. 19, no. 2, pp. 431–438,
 Daniel Garcia Romero and Carol Y. Espy Wilson, “Analysis of
ivector length normalization in speaker recognition systems,”
in INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association, Florence, Italy,
August 27 to 31, 2011, 2011, pp. 249–252.
 Niko Br¨ummer and Edward de Villiers, “The speaker partitioning problem,” in Odyssey 2010: The Speaker and Language
Recognition Workshop, Brno, Czech Republic, June 28 - July
1, 2010, 2010, p. 34.
 “Jensen-Shannon-Divergence,”
 
wikipedia.org/wiki/Jensen-Shannon_
divergence/.
 Thomas M Cover and Joy A Thomas, Elements of information
theory, John Wiley & Sons, 2012.
 “NIST Speaker Evaluation Recognition 2010,” http://
www.nist.gov/itl/iad/mig/sre10.cfm.