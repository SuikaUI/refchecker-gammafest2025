Published as a conference paper at ICLR 2018
PROGRESSIVE GROWING OF GANS FOR IMPROVED
QUALITY, STABILITY, AND VARIATION
Tero Karras
{tkarras,taila,slaine,jlehtinen}@nvidia.com
Samuli Laine
Jaakko Lehtinen
NVIDIA and Aalto University
We describe a new training methodology for generative adversarial networks. The
key idea is to grow both the generator and discriminator progressively: starting
from a low resolution, we add new layers that model increasingly ﬁne details as
training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at
10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10.
Additionally, we describe several implementation details that are important for
discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image
quality and variation. As an additional contribution, we construct a higher-quality
version of the CELEBA dataset.
INTRODUCTION
Generative methods that produce novel samples from high-dimensional data distributions, such as
images, are ﬁnding widespread use, for example in speech synthesis ,
image-to-image translation , and image inpainting . Currently the most prominent approaches are autoregressive models
 , variational autoencoders (VAE) , and generative adversarial networks (GAN) . Currently they all have signiﬁcant
strengths and weaknesses. Autoregressive models – such as PixelCNN – produce sharp images but
are slow to evaluate and do not have a latent representation as they directly model the conditional
distribution over pixels, potentially limiting their applicability. VAEs are easy to train but tend
to produce blurry results due to restrictions in the model, although recent work is improving this
 . GANs produce sharp images, albeit only in fairly small resolutions and with
somewhat limited variation, and the training continues to be unstable despite recent progress . Hybrid methods
combine various strengths of the three, but so far lag behind GANs in image quality .
Typically, a GAN consists of two networks: generator and discriminator (aka critic). The generator
produces a sample, e.g., an image, from a latent code, and the distribution of these images should
ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer
a function that tells whether that is the case, a discriminator network is trained to do the assessment,
and since networks are differentiable, we also get a gradient we can use to steer both networks to
the right direction. Typically, the generator is of main interest – the discriminator is an adaptive loss
function that gets discarded once the generator has been trained.
There are multiple potential problems with this formulation. When we measure the distance between
the training distribution and the generated distribution, the gradients can point to more or less random
directions if the distributions do not have substantial overlap, i.e., are too easy to tell apart . Originally, Jensen-Shannon divergence was used as a distance metric , and recently that formulation has been improved and a number of
more stable alternatives have been proposed, including least squares , absolute
deviation with margin , and Wasserstein distance . Our contributions are largely orthogonal to this ongoing discussion, and we primarily
use the improved Wasserstein loss, but also experiment with least-squares loss.
The generation of high-resolution images is difﬁcult because higher resolution makes it easier to tell
the generated images apart from training images , thus drastically amplifying
the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory
constraints, further compromising training stability. Our key insight is that we can grow both the
generator and discriminator progressively, starting from easier low-resolution images, and add new
layers that introduce higher-resolution details as the training progresses. This greatly speeds up
training and improves stability in high resolutions, as we will discuss in Section 2.
The GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff
between image quality and variation, but that view has been recently challenged .
The degree of preserved variation is currently receiving attention and various methods have been
suggested for measuring it, including inception score , multi-scale structural
similarity (MS-SSIM) , birthday paradox , and explicit tests for the number of discrete modes discovered . We will
describe our method for encouraging variation in Section 3, and propose a new metric for evaluating
the quality and variation in Section 5.
Section 4.1 discusses a subtle modiﬁcation to the initialization of networks, leading to a more balanced learning speed for different layers. Furthermore, we observe that mode collapses traditionally
plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly
they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy
competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section 4.2).
We evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets.
We improve
the best published inception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a
higher quality version of the CELEBA dataset that allows experimentation with output resolutions up to 1024 × 1024 pixels.
This dataset and our full implementation are available at
 trained networks can be
found at along
with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at 
PROGRESSIVE GROWING OF GANS
Our primary contribution is a training methodology for GANs where we start with low-resolution
images, and then progressively increase the resolution by adding layers to the networks as visualized
in Figure 1. This incremental nature allows the training to ﬁrst discover large-scale structure of the
image distribution and then shift attention to increasingly ﬁner scale detail, instead of having to learn
all scales simultaneously.
We use generator and discriminator networks that are mirror images of each other and always grow
in synchrony. All existing layers in both networks remain trainable throughout the training process.
When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure 2.
This avoids sudden shocks to the already well-trained, smaller-resolution layers. Appendix A describes structure of the generator and discriminator in detail, along with other training parameters.
We observe that the progressive training has several beneﬁts. Early on, the generation of smaller
images is substantially more stable because there is less class information and fewer modes . By increasing the resolution little by little we are continuously asking a much simpler
question compared to the end goal of discovering a mapping from latent vectors to e.g. 10242
images. This approach has conceptual similarity to recent work by Chen & Koltun . In
practice it stabilizes the training sufﬁciently for us to reliably synthesize megapixel-scale images
using WGAN-GP loss and even LSGAN loss .
Published as a conference paper at ICLR 2018
Training progresses
Figure 1: Our training starts with both the generator (G) and discriminator (D) having a low spatial resolution of 4×4 pixels. As the training advances, we incrementally add layers to G and D,
thus increasing the spatial resolution of the generated images. All existing layers remain trainable
throughout the process. Here N × N refers to convolutional layers operating on N × N spatial
resolution. This allows stable synthesis in high resolutions and also speeds up training considerably.
One the right we show six example images generated using progressive growing at 1024 × 1024.
Another beneﬁt is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2–6 times
faster, depending on the ﬁnal output resolution.
The idea of growing GANs progressively is related to the work of Wang et al. , who use multiple discriminators that operate on different spatial resolutions. That work in turn is motivated by
Durugkar et al. who use one generator and multiple discriminators concurrently, and Ghosh
et al. who do the opposite with multiple generators and one discriminator.
Hierarchical
GANs deﬁne a generator and discriminator for each level of an image pyramid. These methods build on the same observation as our work
– that the complex mapping from latents to high-resolution images is easier to learn in steps – but
the crucial difference is that we have only a single GAN instead of a hierarchy of them. In contrast
to early work on adaptively growing networks, e.g., growing neural gas and neuro
evolution of augmenting topologies that grow networks greedily,
we simply defer the introduction of pre-conﬁgured layers. In that sense our approach resembles
layer-wise training of autoencoders .
INCREASING VARIATION USING MINIBATCH STANDARD DEVIATION
GANs have a tendency to capture only a subset of the variation found in training data, and Salimans
et al. suggest “minibatch discrimination” as a solution. They compute feature statistics not
only from individual images but also across the minibatch, thus encouraging the minibatches of
generated and training images to show similar statistics. This is implemented by adding a minibatch
layer towards the end of the discriminator, where the layer learns a large tensor that projects the
input activation to an array of statistics. A separate set of statistics is produced for each example in a
minibatch and it is concatenated to the layer’s output, so that the discriminator can use the statistics
internally. We simplify this approach drastically while also improving the variation.
Our simpliﬁed solution has neither learnable parameters nor new hyperparameters. We ﬁrst compute
the standard deviation for each feature in each spatial location over the minibatch. We then average
these estimates over all features and spatial locations to arrive at a single value. We replicate the
value and concatenate it to all spatial locations and over the minibatch, yielding one additional (constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it
best to insert it towards the end (see Appendix A.1 for details). We experimented with a richer set
of statistics, but were not able to improve the variation further. In parallel work, Lin et al. 
provide theoretical insights about the beneﬁts of showing multiple images to the discriminator.
Published as a conference paper at ICLR 2018
Figure 2: When doubling the resolution of the generator (G) and discriminator (D) we fade in the
new layers smoothly. This example illustrates the transition from 16 × 16 images (a) to 32 × 32
images (c). During the transition (b) we treat the layers that operate on the higher resolution like a
residual block, whose weight α increases linearly from 0 to 1. Here 2× and 0.5× refer to doubling
and halving the image resolution using nearest neighbor ﬁltering and average pooling, respectively.
The toRGB represents a layer that projects feature vectors to RGB colors and fromRGB does
the reverse; both use 1 × 1 convolutions. When training the discriminator, we feed in real images
that are downscaled to match the current resolution of the network. During a resolution transition,
we interpolate between two resolutions of the real images, similarly to how the generator output
combines two resolutions.
Alternative solutions to the variation problem include unrolling the discriminator 
to regularize its updates, and a “repelling regularizer” that adds a new loss term
to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. The
multiple generators of Ghosh et al. also serve a similar goal. We acknowledge that these
solutions may increase the variation even more than our solution – or possibly be orthogonal to it –
but leave a detailed comparison to a later time.
NORMALIZATION IN GENERATOR AND DISCRIMINATOR
GANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between
the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization in the generator, and
often also in the discriminator. These normalization methods were originally introduced to eliminate covariate shift. However, we have not observed that to be an issue in GANs, and thus believe
that the actual need in GANs is constraining signal magnitudes and competition. We use a different
approach that consists of two ingredients, neither of which include learnable parameters.
EQUALIZED LEARNING RATE
We deviate from the current trend of careful weight initialization, and instead use a trivial N(0, 1)
initialization and then explicitly scale the weights at runtime. To be precise, we set ˆwi = wi/c,
where wi are the weights and c is the per-layer normalization constant from He’s initializer . The beneﬁt of doing this dynamically instead of during initialization is somewhat
subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent
methods such as RMSProp and Adam . These
methods normalize a gradient update by its estimated standard deviation, thus making the update
independent of the scale of the parameter. As a result, if some parameters have a larger dynamic
range than others, they will take longer to adjust. This is a scenario modern initializers cause, and
thus it is possible that a learning rate is both too large and too small at the same time. Our approach
ensures that the dynamic range, and thus the learning speed, is the same for all weights. A similar
reasoning was independently used by van Laarhoven .
Published as a conference paper at ICLR 2018
PIXELWISE FEATURE VECTOR NORMALIZATION IN GENERATOR
To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the
generator after each convolutional layer. We do this using a variant of “local response normalization” , conﬁgured as bx,y = ax,y/
x,y)2 + ϵ, where ϵ = 10−8,
N is the number of feature maps, and ax,y and bx,y are the original and normalized feature vector
in pixel (x, y), respectively. We ﬁnd it surprising that this heavy-handed constraint does not seem to
harm the generator in any way, and indeed with most datasets it does not change the results much,
but it prevents the escalation of signal magnitudes very effectively when needed.
MULTI-SCALE STATISTICAL SIMILARITY FOR ASSESSING GAN RESULTS
In order to compare the results of one GAN to another, one needs to investigate a large number of
images, which can be tedious, difﬁcult, and subjective. Thus it is desirable to rely on automated
methods that compute some indicative metric from large image collections. We noticed that existing
methods such as MS-SSIM ﬁnd large-scale mode collapses reliably but fail to
react to smaller effects such as loss of variation in colors or textures, and they also do not directly
assess image quality in terms of similarity to the training set.
We build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid representations of generated and target images, starting at a low-pass
resolution of 16 × 16 pixels. As per standard practice, the pyramid progressively doubles until the
full resolution is reached, each successive level encoding the difference to an up-sampled version of
the previous level.
A single Laplacian pyramid level corresponds to a speciﬁc spatial frequency band. We randomly
sample 16384 images and extract 128 descriptors from each level in the Laplacian pyramid, giving
us 221 (2.1M) descriptors per level. Each descriptor is a 7 × 7 pixel neighborhood with 3 color
channels, denoted by x ∈R7×7×3 = R147. We denote the patches from level l of the training set
and generated set as {xl
i=1 and {yl
i=1, respectively. We ﬁrst normalize {xl
i} and {yl
i} w.r.t. the
mean and standard deviation of each color channel, and then estimate the statistical similarity by
computing their sliced Wasserstein distance SWD({xl
i}), an efﬁciently computable randomized approximation to earthmovers distance, using 512 projections .
Intuitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation
at this spatial resolution. In particular, the distance between the patch sets extracted from the lowestresolution 16 × 16 images indicate similarity in large-scale image structures, while the ﬁnest-level
patches encode information about pixel-level attributes such as sharpness of edges and noise.
EXPERIMENTS
In this section we discuss a set of experiments that we conducted to evaluate the quality of
our results.
Please refer to Appendix A for detailed description of our network structures
and training conﬁgurations.
We also invite the reader to consult the accompanying video
( for additional result images and latent space interpolations.
In this section we will distinguish between the network structure (e.g., convolutional layers, resizing), training conﬁguration (various normalization layers, minibatch-related operations), and training loss (WGAN-GP, LSGAN).
IMPORTANCE OF INDIVIDUAL CONTRIBUTIONS IN TERMS OF STATISTICAL SIMILARITY
We will ﬁrst use the sliced Wasserstein distance (SWD) and multi-scale structural similarity (MS-
SSIM) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-theart loss function (WGAN-GP) and training conﬁguration in an unsupervised
setting using CELEBA and LSUN BEDROOM datasets in 1282
Published as a conference paper at ICLR 2018
LSUN BEDROOM
Training conﬁguration
Sliced Wasserstein distance ×103
Sliced Wasserstein distance ×103
(a) Gulrajani et al. 
11.97 10.51
8.03 14.48 11.25
(b) + Progressive growing
(c) + Small minibatch
75.42 41.33 41.62 26.57 46.23
72.73 40.16 42.75 42.46 49.52
(d) + Revised training parameters
4.71 11.84
(e∗) + Minibatch discrimination
6.04 16.29
5.32 11.88
Minibatch stddev
+ Equalized learning rate
(g) + Pixelwise normalization
(h) Converged
Table 1: Sliced Wasserstein distance (SWD) between the generated and training images (Section 5)
and multi-scale structural similarity (MS-SSIM) among the generated images for several training
setups at 128 × 128. For SWD, each column represents one level of the Laplacian pyramid, and the
last one gives an average of the four distances.
(h) Converged
Figure 3: (a) – (g) CELEBA examples corresponding to rows in Table 1. These are intentionally
non-converged. (h) Our converged result. Notice that some images show aliasing and some are not
sharp – this is a ﬂaw of the dataset, which the model learns to replicate faithfully.
resolution. CELEBA is particularly well suited for such comparison because the training images
contain noticeable artifacts (aliasing, compression, blur) that are difﬁcult for the generator to reproduce faithfully. In this test we amplify the differences between training conﬁgurations by choosing a
relatively low-capacity network structure (Appendix A.2) and terminating the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged.
Table 1 lists the numerical values for SWD and MS-SSIM in several training conﬁgurations, where
our individual contributions are cumulatively enabled one by one on top of the baseline . The MS-SSIM numbers were averaged from 10000 pairs of generated images, and
SWD was calculated as described in Section 5. Generated CELEBA images from these conﬁgurations are shown in Figure 3. Due to space constraints, the ﬁgure shows only a small number of
examples for each row of the table, but a signiﬁcantly broader set is available in Appendix H. Intuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in
colors, textures, and viewpoints. However, this is not captured by MS-SSIM: we can immediately
see that conﬁguration (h) generates signiﬁcantly better images than conﬁguration (a), but MS-SSIM
remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does indicate a clear improvement.
The ﬁrst training conﬁguration (a) corresponds to Gulrajani et al. , featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables
progressive growing of the networks, which results in sharper and more believable output images.
SWD correctly ﬁnds the distribution of generated images to be more similar to the training set.
Our primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in
(c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which
is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix A.2). As an
intermediate test (e∗), we enable minibatch discrimination , which somewhat
surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation.
In contrast, our minibatch standard deviation (e) improves the average SWD scores and images. We
then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD
Published as a conference paper at ICLR 2018
Sliced Wasserstein distance ×103
Training time in hours
Sliced Wasserstein distance ×103
Training time in hours
Millions of real images shown
Training time in hours
Fixed layers
Progressive growing
Figure 4: Effect of progressive growing on training speed and convergence. The timings were
measured on a single-GPU setup using NVIDIA Tesla P100. (a) Statistical similarity with respect
to wall clock time for Gulrajani et al. using CELEBA at 128 × 128 resolution. Each graph
represents sliced Wasserstein distance on one level of the Laplacian pyramid, and the vertical line
indicates the point where we stop the training in Table 1. (b) Same graph with progressive growing
enabled. The dashed vertical lines indicate points where we double the resolution of G and D. (c)
Effect of progressive growing on the raw training speed in 1024 × 1024 resolution.
and subjective visual quality. Finally, in (h) we use a non-crippled network and longer training – we
feel the quality of the generated images is at least comparable to the best published results so far.
CONVERGENCE AND TRAINING SPEED
Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image
throughput. The ﬁrst two plots correspond to the training conﬁguration of Gulrajani et al. 
without and with progressive growing. We observe that the progressive variant offers two main beneﬁts: it converges to a considerably better optimum and also reduces the total training time by about
a factor of two. The improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity. Without progressive growing, all
layers of the generator and discriminator are tasked with simultaneously ﬁnding succinct intermediate representations for both the large-scale variation and the small-scale detail. With progressive
growing, however, the existing low-resolution layers are likely to have already converged early on,
so the networks are only tasked with reﬁning the representations by increasingly smaller-scale effects as new layers are introduced. Indeed, we see in Figure 4(b) that the largest-scale statistical
similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the
rest of the training. The smaller-scale curves (32, 64, 128) level off one by one as the resolution is
increased, but the convergence of each curve is equally consistent. With non-progressive training in
Figure 4(a), each scale of the SWD metric converges roughly in unison, as could be expected.
The speedup from progressive growing increases as the output resolution grows. Figure 4(c) shows
training progress, measured in number of real images shown to the discriminator, as a function of
training time when the training progresses all the way to 10242 resolution. We see that progressive
growing gains a signiﬁcant head start because the networks are shallow and quick to evaluate at
the beginning. Once the full resolution is reached, the image throughput is equal between the two
methods. The plot shows that the progressive variant reaches approximately 6.4 million images in
96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours
to reach the same point. In this case, the progressive growing offers roughly a 5.4× speedup.
HIGH-RESOLUTION IMAGE GENERATION USING CELEBA-HQ DATASET
To meaningfully demonstrate our results at high output resolutions, we need a sufﬁciently varied
high-quality dataset. However, virtually all publicly available datasets previously used in GAN
literature are limited to relatively low resolutions ranging from 322 to 4802. To this end, we created
a high-quality version of the CELEBA dataset consisting of 30000 of the images at 1024 × 1024
resolution. We refer to Appendix C for further details about the generation of this dataset.
Published as a conference paper at ICLR 2018
Figure 5: 1024 × 1024 images generated using the CELEBA-HQ dataset. See Appendix F for a
larger set of results, and the accompanying video for latent space interpolations.
Mao et al. (128 × 128)
Gulrajani et al. (128 × 128)
Our (256 × 256)
Figure 6: Visual quality comparison in LSUN BEDROOM; pictures copied from the cited articles.
Our contributions allow us to deal with high output resolutions in a robust and efﬁcient fashion.
Figure 5 shows selected 1024 × 1024 images produced by our network. While megapixel GAN
results have been shown before in another dataset , our results are vastly more
varied and of higher perceptual quality. Please refer to Appendix F for a larger set of result images
as well as the nearest neighbors found from the training data. The accompanying video shows latent
space interpolations and visualizes the progressive training. The interpolation works so that we ﬁrst
randomize a latent code for each frame (512 components sampled individually from N(0, 1)), then
blur the latents across time with a Gaussian (σ = 45 frames @ 60Hz), and ﬁnally normalize each
vector to lie on a hypersphere.
We trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed
qualitative differences between the results of consecutive training iterations. Our implementation
used an adaptive minibatch size depending on the current output resolution so that the available
memory budget was optimally utilized.
In order to demonstrate that our contributions are largely orthogonal to the choice of a loss function,
we also trained the same network using LSGAN loss instead of WGAN-GP loss. Figure 1 shows six
examples of 10242 images produced using our method using LSGAN. Further details of this setup
are given in Appendix B.
Published as a conference paper at ICLR 2018
POTTEDPLANT
CHURCHOUTDOOR
Figure 7: Selection of 256 × 256 images generated from different LSUN categories.
LSUN RESULTS
Figure 6 shows a purely visual comparison between our solution and earlier results in LSUN BED-
ROOM. Figure 7 gives selected examples from seven very different LSUN categories at 2562. A
larger, non-curated set of results from all 30 LSUN categories is available in Appendix G, and the
video demonstrates interpolations. We are not aware of earlier results in most of these categories,
and while some categories work better than others, we feel that the overall quality is high.
CIFAR10 INCEPTION SCORES
The best inception scores for CIFAR10 (10 categories of 32 × 32 RGB images) we are aware of
are 7.90 for unsupervised and 8.87 for label conditioned setups . The large
difference between the two numbers is primarily caused by “ghosts” that necessarily appear between
classes in the unsupervised setting, while label conditioning can remove many such transitions.
When all of our contributions are enabled, we get 8.80 in the unsupervised setting. Appendix D
shows a representative set of generated images along with a more comprehensive list of results
from earlier methods. The network and training setup were the same as for CELEBA, progression limited to 32 × 32 of course. The only customization was to the WGAN-GP’s regularization
term Eˆx∼Pˆx[(||∇ˆxD(ˆx)||2 −γ)2/γ2]. Gulrajani et al. used γ = 1.0, which corresponds to
1-Lipschitz, but we noticed that it is in fact signiﬁcantly better to prefer fast transitions (γ = 750) to
minimize the ghosts. We have not tried this trick with other datasets.
DISCUSSION
While the quality of our results is generally high compared to earlier work on GANs, and the training
is stable in large resolutions, there is a long way to true photorealism. Semantic sensibility and understanding dataset-dependent constraints, such as certain objects being straight rather than curved,
leaves a lot to be desired. There is also room for improvement in the micro-structure of the images.
That said, we feel that convincing realism may now be within reach, especially in CELEBA-HQ.
Published as a conference paper at ICLR 2018
ACKNOWLEDGEMENTS
We would like to thank Mikael Honkavaara, Tero Kuosmanen, and Timi Hietanen for the compute
infrastructure. Dmitry Korobchenko and Richard Calderwood for efforts related to the CELEBA-HQ
dataset. Oskar Elek, Jacob Munkberg, and Jon Hasselgren for useful comments.