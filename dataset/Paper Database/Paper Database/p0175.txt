SegFormer: Simple and Efﬁcient Design for Semantic
Segmentation with Transformers
Enze Xie1, Wenhai Wang2, Zhiding Yu3, Anima Anandkumar3,4, Jose M. Alvarez3, Ping Luo1
1The University of Hong Kong 2Nanjing University 3NVIDIA 4Caltech
We present SegFormer, a simple, efﬁcient yet powerful semantic segmentation
framework which uniﬁes Transformers with lightweight multilayer perceptron
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises
a novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the interpolation of
positional codes which leads to decreased performance when the testing resolution
differs from training. 2) SegFormer avoids complex decoders. The proposed
MLP decoder aggregates information from different layers, and thus combining
both local attention and global attention to render powerful representations. We
show that this simple and lightweight design is the key to efﬁcient segmentation
on Transformers. We scale our approach up to obtain a series of models from
SegFormer-B0 to SegFormer-B5, reaching signiﬁcantly better performance and
efﬁciency than previous counterparts. For example, SegFormer-B4 achieves 50.3%
mIoU on ADE20K with 64M parameters, being 5× smaller and 2.2% better than
the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on
Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.
Code will be released at: github.com/NVlabs/SegFormer.
Introduction
ADE20K mIoU
Params (Millions)
Swin Transformer
SegFormer-B0
SegFormer-B2
DeeplabV3+/R101
HRNet-W48 + OCR
SegFormer-B4
HRNet-W48 + OCR
DeepLabV3+/R101
SegFormer-B5
Figure 1: Performance vs. model efﬁciency on ADE20K. All results
are reported with single model and single-scale inference. SegFormer
achieves a new state-of-the-art 51.0% mIoU while being signiﬁcantly
more efﬁcient than previous methods.
Semantic segmentation is a fundamental task in
computer vision and enables many downstream
applications. It is related to image classiﬁcation
since it produces per-pixel category prediction
instead of image-level prediction. This relationship is pointed out and systematically studied in
a seminal work , where the authors used fully
convolutional networks (FCNs) for semantic segmentation tasks. Since then, FCN has inspired
many follow-up works and has become a predominant design choice for dense prediction.
Since there is a strong relation between classi-
ﬁcation and semantic segmentation, many stateof-the-art semantic segmentation frameworks are
variants of popular architectures for image classi-
ﬁcation on ImageNet. Therefore, designing backbone architectures has remained an active area
in semantic segmentation. Indeed, starting from
early methods using VGGs , to the latest methods with signiﬁcantly deeper and more powerful
backbones , the evolution of backbones has dramatically pushed the performance boundary of
 
 
semantic segmentation. Besides backbone architectures, another line of work formulates semantic
segmentation as a structured prediction problem, and focuses on designing modules and operators,
which can effectively capture contextual information. A representative example in this area is dilated
convolution , which increases the receptive ﬁeld by “inﬂating” the kernel with holes.
Witnessing the great success in natural language processing (NLP), there has been a recent surge of
interest to introduce Transformers to vision tasks. Dosovitskiy et al. proposed vision Transformer
(ViT) for image classiﬁcation. Following the Transformer design in NLP, the authors split an image
into multiple linearly embedded patches and feed them into a standard Transformer with positional
embeddings (PE), leading to an impressive performance on ImageNet. In semantic segmentation,
Zheng et al. proposed SETR to demonstrate the feasibility of using Transformers in this task.
SETR adopts ViT as a backbone and incorporates several CNN decoders to enlarge feature resolution.
Despite the good performance, ViT has some limitations: 1) ViT outputs single-scale low-resolution
features instead of multi-scale ones. 2) It has high computation cost on large images. To address these
limitations, Wang et al. proposed a pyramid vision Transformer (PVT), a natural extension of ViT
with pyramid structures for dense prediction. PVT shows considerable improvements over the ResNet
counterpart on object detection and semantic segmentation. However, together with other emerging
methods such as Swin Transformer and Twins , these methods mainly consider the design of
the Transformer encoder, neglecting the contribution of the decoder for further improvements.
This paper introduces SegFormer, a cutting-edge Transformer framework for semantic segmentation
that jointly considers efﬁciency, accuracy, and robustness. In contrast to previous methods, our
framework redesigns both the encoder and the decoder. The key novelties of our approach are:
• A novel positional-encoding-free and hierarchical Transformer encoder.
• A lightweight All-MLP decoder design that yields a powerful representation without complex and
computationally demanding modules.
• As shown in Figure 1, SegFormer sets new a state-of-the-art in terms of efﬁciency, accuracy and
robustness in three publicly available semantic segmentation datasets.
First, the proposed encoder avoids interpolating positional codes when performing inference on
images with resolutions different from the training one. As a result, our encoder can easily adapt to
arbitrary test resolutions without impacting the performance. In addition, the hierarchical part enables
the encoder to generate both high-resolution ﬁne features and low-resolution coarse features, this is
in contrast to ViT that can only produce single low-resolution feature maps with ﬁxed resolutions.
Second, we propose a lightweight MLP decoder where the key idea is to take advantage of the
Transformer-induced features where the attentions of lower layers tend to stay local, whereas the
ones of the highest layers are highly non-local. By aggregating the information from different layers,
the MLP decoder combines both local and global attention. As a result, we obtain a simple and
straightforward decoder that renders powerful representations.
We demonstrate the advantages of SegFormer in terms of model size, run-time, and accuracy on three
publicly available datasets: ADE20K, Cityscapes, and COCO-Stuff. On Citysapces, our lightweight
model, SegFormer-B0, without accelerated implementations such as TensorRT, yields 71.9% mIoU
at 48 FPS, which, compared to ICNet , represents a relative improvement of 60% and 4.2% in
latency and performance, respectively. Our largest model, SegFormer-B5, yields 84.0% mIoU, which
represents a relative 1.8% mIoU improvement while being 5 × faster than SETR . On ADE20K,
this model sets a new state-of-the-art of 51.8% mIoU while being 4 × smaller than SETR. Moreover,
our approach is signiﬁcantly more robust to common corruptions and perturbations than existing
methods, therefore being suitable for safety-critical applications. Code will be publicly available.
Related Work
Semantic Segmentation. Semantic segmentation can be seen as an extension of image classiﬁcation
from image level to pixel level. In the deep learning era , FCN is the fundamental work of
semantic segmentation, which is a fully convolution network that performs pixel-to-pixel classiﬁcation
in an end-to-end manner. After that, researchers focused on improving FCN from different aspects
such as: enlarging the receptive ﬁeld ; reﬁning the contextual information [21–
Overlap Patch
Embeddings
Transformer
Transformer
Transformer
Transformer
Overlap Patch
Figure 2: The proposed SegFormer framework consists of two main modules: A hierarchical Transformer
encoder to extract coarse and ﬁne features; and a lightweight All-MLP decoder to directly fuse these multi-level
features and predict the semantic segmentation mask. “FFN” indicates feed-forward network.
29]; introducing boundary information ; designing various attention modules ; or
using AutoML technologies . These methods signiﬁcantly improve semantic segmentation
performance at the expense of introducing many empirical modules, making the resulting framework
computationally demanding and complicated. More recent methods have proved the effectiveness of
Transformer-based architectures for semantic segmentation . However, these methods are still
computationally demanding.
Transformer backbones. ViT is the ﬁrst work to prove that a pure Transformer can achieve
state-of-the-art performance in image classiﬁcation. ViT treats each image as a sequence of tokens and
then feeds them to multiple Transformer layers to make the classiﬁcation. Subsequently, DeiT 
further explores a data-efﬁcient training strategy and a distillation approach for ViT. More recent
methods such as T2T ViT , CPVT , TNT , CrossViT and LocalViT introduce
tailored changes to ViT to further improve image classiﬁcation performance.
Beyond classiﬁcation, PVT is the ﬁrst work to introduce a pyramid structure in Transformer,
demonstrating the potential of a pure Transformer backbone compared to CNN counterparts in
dense prediction tasks. After that, methods such as Swin , CvT , CoaT , LeViT and
Twins enhance the local continuity of features and remove ﬁxed size position embedding to
improve the performance of Transformers in dense prediction tasks.
Transformers for speciﬁc tasks. DETR is the ﬁrst work using Transformers to build an end-toend object detection framework without non-maximum suppression (NMS). Other works have also
used Transformers in a variety of tasks such as tracking , super-resolution , ReID ,
Colorization , Retrieval and multi-modal learning . For semantic segmentation,
SETR adopts ViT as a backbone to extract features, achieving impressive performance.
However, these Transformer-based methods have very low efﬁciency and, thus, difﬁcult to deploy in
real-time applications.
This section introduces SegFormer, our efﬁcient, robust, and powerful segmentation framework
without hand-crafted and computationally demanding modules. As depicted in Figure 2, SegFormer
consists of two main modules: (1) a hierarchical Transformer encoder to generate high-resolution
coarse features and low-resolution ﬁne features; and (2) a lightweight All-MLP decoder to fuse these
multi-level features to produce the ﬁnal semantic segmentation mask.
Given an image of size H × W × 3, we ﬁrst divide it into patches of size 4 × 4. Contrary to ViT
that uses patches of size 16 × 16, using smaller patches favors the dense prediction task. We then
use these patches as input to the hierarchical Transformer encoder to obtain multi-level features at
{1/4, 1/8, 1/16, 1/32} of the original image resolution. We then pass these multi-level features to the
All-MLP decoder to predict the segmentation mask at a H
4 × Ncls resolution, where Ncls is the
number of categories. In the rest of this section, we detail the proposed encoder and decoder designs
and summarize the main differences between our approach and SETR.
Hierarchical Transformer Encoder
We design a series of Mix Transformer encoders (MiT), MiT-B0 to MiT-B5, with the same architecture
but different sizes. MiT-B0 is our lightweight model for fast inference, while MiT-B5 is the largest
model for the best performance. Our design for MiT is partly inspired by ViT but tailored and
optimized for semantic segmentation.
Hierarchical Feature Representation. Unlike ViT that can only generate a single-resolution feature
map, the goal of this module is, given an input image, to generate CNN-like multi-level features.
These features provide high-resolution coarse features and low-resolution ﬁne-grained features that
usually boost the performance of semantic segmentation. More precisely, given an input image with
a resolution of H × W × 3, we perform patch merging to obtain a hierarchical feature map Fi with a
resolution of
2i+1 × Ci, where i ∈{1, 2, 3, 4}, and Ci+1 is larger than Ci.
Overlapped Patch Merging. Given an image patch, the patch merging process used in ViT, uniﬁes
a N × N × 3 patch into a 1 × 1 × C vector. This can easily be extended to unify a 2 × 2 × Ci
feature path into a 1 × 1 × Ci+1 vector to obtain hierarchical feature maps. Using this, we can shrink
our hierarchical features from F1 ( H
4 × C1) to F2 ( H
8 × C2), and then iterate for any
other feature map in the hierarchy. This process was initially designed to combine non-overlapping
image or feature patches. Therefore, it fails to preserve the local continuity around those patches.
Instead, we use an overlapping patch merging process. To this end, we deﬁne K, S, and P, where
K is the patch size, S is the stride between two adjacent patches, and P is the padding size. In our
experiments, we set K = 7, S = 4, P = 3 ,and K = 3, S = 2, P = 1 to perform overlapping patch
merging to produces features with the same size as the non-overlapping process.
Efﬁcient Self-Attention. The main computation bottleneck of the encoders is the self-attention layer.
In the original multi-head self-attention process, each of the heads Q, K, V have the same dimensions
N × C, where N = H × W is the length of the sequence, the self-attention is estimated as:
Attention(Q, K, V ) = Softmax( QKT
The computational complexity of this process is O(N 2), which is prohibitive for large image
resolutions. Instead, we use the sequence reduction process introduced in . This process uses a
reduction ratio R to reduce the length of the sequence of as follows:
ˆK = Reshape(N
R , C · R)(K)
K = Linear(C · R, C)( ˆK),
where K is the sequence to be reduced, Reshape( N
R , C · R)(K) refers to reshape K to the one with
shape of N
R × (C · R), and Linear(Cin, Cout)(·) refers to a linear layer taking a Cin-dimensional
tensor as input and generating a Cout-dimensional tensor as output. Therefore, the new K has
dimensions N
R × C. As a result, the complexity of the self-attention mechanism is reduced from
O(N 2) to O( N2
R ). In our experiments, we set R to from stage-1 to stage-4.
Mix-FFN. ViT uses positional encoding (PE) to introduce the location information. However, the
resolution of PE is ﬁxed. Therefore, when the test resolution is different from the training one, the
positional code needs to be interpolated and this often leads to dropped accuracy. To alleviate this
problem, CPVT uses 3 × 3 Conv together with the PE to implement a data-driven PE. We argue
that positional encoding is actually not necessary for semantic segmentation. Instead, we introduce
Mix-FFN which considers the effect of zero padding to leak location information , by directly
using a 3 × 3 Conv in the feed-forward network (FFN). Mix-FFN can be formulated as:
xout = MLP(GELU(Conv3×3(MLP(xin)))) + xin,
where xin is the feature from the self-attention module. Mix-FFN mixes a 3 × 3 convolution and
an MLP into each FFN. In our experiments, we will show that a 3 × 3 convolution is sufﬁcient to
provide positional information for Transformers. In particular, we use depth-wise convolutions for
reducing the number of parameters and improving efﬁciency.
Lightweight All-MLP Decoder
SegFormer incorporates a lightweight decoder consisting only of MLP layers and this avoiding the
hand-crafted and computationally demanding components typically used in other methods. The key
to enabling such a simple decoder is that our hierarchical Transformer encoder has a larger effective
receptive ﬁeld (ERF) than traditional CNN encoders.
The proposed All-MLP decoder consists of four main steps. First, multi-level features Fi from
the MiT encoder go through an MLP layer to unify the channel dimension. Then, in a second
step, features are up-sampled to 1/4th and concatenated together. Third, a MLP layer is adopted to
fuse the concatenated features F. Finally, another MLP layer takes the fused feature to predict the
segmentation mask M with a H
4 × Ncls resolution, where Ncls is the number of categories.
This lets us formulate the decoder as:
ˆFi = Linear(Ci, C)(Fi), ∀i
ˆFi = Upsample(W
4 )( ˆFi), ∀i
F = Linear(4C, C)(Concat( ˆFi)), ∀i
M = Linear(C, Ncls)(F),
where M refers to the predicted mask, and Linear(Cin, Cout)(·) refers to a linear layer with Cin and
Cout as input and output vector dimensions respectively.
DeepLabv3+
Figure 3: Effective Receptive Field (ERF) on Cityscapes (average over 100 images). Top row: Deeplabv3+. Bottom row: Seg-
Former. ERFs of the four stages and the decoder heads of both
architectures are visualized. Best viewed with zoom in.
Effective Receptive Field Analysis.
For semantic segmentation, maintaining large receptive ﬁeld to include context information has been a central issue . Here, we use effective receptive ﬁeld (ERF) as a
toolkit to visualize and interpret why
our MLP decoder design is so effective on Transformers. In Figure 3, we
visualize ERFs of the four encoder
stages and the decoder heads for both
DeepLabv3+ and SegFormer. We can
make the following observations:
• The ERF of DeepLabv3+ is relatively small even at Stage-4, the deepest stage.
• SegFormer’s encoder naturally produces local attentions which resemble convolutions at lower
stages, while able to output highly non-local attentions that effectively capture contexts at Stage-4.
• As shown with the zoom-in patches in Figure 3, the ERF of the MLP head (blue box) differs from
Stage-4 (red box) with a signiﬁcant stronger local attention besides the non-local attention.
The limited receptive ﬁeld in CNN requires one to resort to context modules such as ASPP 
that enlarge the receptive ﬁeld but inevitably become heavy. Our decoder design beneﬁts from the
non-local attention in Transformers and leads to a larger receptive ﬁeld without being complex. The
same decoder design, however, does not work well on CNN backbones since the overall receptive
ﬁeld is upper bounded by the limited one at Stage-4, and we will verify this later in Table 1d,
More importantly, our decoder design essentially takes advantage of a Transformer induced feature
that produces both highly local and non-local attention at the same time. By unifying them, our MLP
decoder renders complementary and powerful representations by adding few parameters. This is
another key reason that motivated our design. Taking the non-local attention from Stage-4 alone is
not enough to produce good results, as will be veriﬁed in Table 1d.
Relationship to SETR.
SegFormer contains multiple more efﬁcient and powerful designs compared with SETR :
• We only use ImageNet-1K for pre-training. ViT in SETR is pre-trained on larger ImageNet-22K.
• SegFormer’s encoder has a hierarchical architecture, which is smaller than ViT and can capture
both high-resolution coarse and low-resolution ﬁne features. In contrast, SETR’s ViT encoder can
only generate single low-resolution feature map.
• We remove Positional Embedding in encoder, while SETR uses ﬁxed shape Positional Embedding
which decreases the accuracy when the resolution at inference differs from the training ones.
• Our MLP decoder is more compact and less computationally demanding than the one in SETR.
This leads to a negligible computational overhead. In contrast, SETR requires heavy decoders with
multiple 3×3 convolutions.
Experiments
Experimental Settings
Datasets: We used three publicly available datasets: Cityscapes , ADE20K and COCO-
Stuff . ADE20K is a scene parsing dataset covering 150 ﬁne-grained semantic concepts consisting
of 20210 images. Cityscapes is a driving dataset for semantic segmentation consisting of 5000 ﬁneannotated high resolution images with 19 categories. COCO-Stuff covers 172 labels and consists of
164k images: 118k for training, 5k for validation, 20k for test-dev and 20k for the test-challenge.
Implementation details: We used the mmsegmentation1 codebase and train on a server with 8 Tesla
V100. We pre-train the encoder on the Imagenet-1K dataset and randomly initialize the decoder.
During training, we applied data augmentation through random resize with ratio 0.5-2.0, random
horizontal ﬂipping, and random cropping to 512 × 512, 1024×1024, 512 × 512 for ADE20K,
Cityscapes and COCO-Stuff, respectively. Following we set crop size to 640 × 640 on ADE20K
for our largest model B5. We trained the models using AdamW optimizer for 160K iterations on
ADE20K, Cityscapes, and 80K iterations on COCO-Stuff. Exceptionally, for the ablation studies, we
trained the models for 40K iterations. We used a batch size of 16 for ADE20K and COCO-Stuff, and
a batch size of 8 for Cityscapes. The learning rate was set to an initial value of 0.00006 and then used
a “poly” LR schedule with factor 1.0 by default. For simplicity, we did not adopt widely-used tricks
such as OHEM, auxiliary losses or class balance loss. During evaluation, we rescale the short side
of the image to training cropping size and keep the aspect ratio for ADE20K and COCO-Stuff. For
Cityscapes, we do inference using sliding window test by cropping 1024 × 1024 windows. We report
semantic segmentation performance using mean Intersection over Union (mIoU).
Ablation Studies
Inﬂuence of the size of model. We ﬁrst analyze the effect of increasing the size of the encoder on
the performance and model efﬁciency. Figure 1 shows the performance vs. model efﬁciency for
ADE20K as a function of the encoder size and, Table 1a summarizes the results for the three datasets.
The ﬁrst thing to observe here is the size of the decoder compared to the encoder. As shown, for
the lightweight model, the decoder has only 0.4M parameters. For MiT-B5 encoder, the decoder
only takes up to 4% of the total number of parameters in the model. In terms of performance, we
can observe that, overall, increasing the size of the encoder yields consistent improvements on all
the datasets. Our lightweight model, SegFormer-B0, is compact and efﬁcient while maintaining a
competitive performance, showing that our method is very convenient for real-time applications. On
the other hand, our SegFormer-B5, the largest model, achieves state-of-the-art results on all three
datasets, showing the potential of our Transformer encoder.
Inﬂuence of C, the MLP decoder channel dimension. We now analyze the inﬂuence of the channel
dimension C in the MLP decoder, see Section 3.2. In Table 1b we show performance, ﬂops, and
parameters as a function of this dimension. We can observe that setting C = 256 provides a
very competitive performance and computational cost. The performance increases as C increases;
however, it leads to larger and less efﬁcient models. Interestingly, this performance plateaus for
channel dimensions wider than 768. Given these results, we choose C = 256 for our real-time
models SegFormer-B0, B1 and C = 768 for the rest.
1 
Table 1: Ablation studies related to model size, encoder and decoder design.
(a) Accuracy, parameters and ﬂops as a function of the model size on the three datasets. “SS” and “MS” means single/multi-scale test.
Cityscapes
COCO-Stuff
Model Size
mIoU(SS/MS) ↑
mIoU(SS/MS) ↑
mIoU(SS) ↑
37.4 / 38.0
76.2 / 78.1
42.2 / 43.1
78.5 / 80.0
46.5 / 47.5
81.0 / 82.2
49.4 / 50.0
81.7 / 83.3
50.3 / 51.1
82.3 / 83.9
51.0 / 51.8
82.4 / 84.0
(b) Accuracy as a function of the MLP
dimension C in the decoder on ADE20K.
(c) Mix-FFN vs. positional encoding (PE) for
different test resolution on Cityscapes.
(d) Accuracy on ADE20K of CNN and
Transformer encoder with MLP decoder.
“S4” means stage-4 feature.
Flops ↓Params ↓mIoU ↑
ResNet50 (S1-4)
ResNet101 (S1-4)
ResNeXt101 (S1-4)
MiT-B2 (S4)
MiT-B2 (S1-4)
MiT-B3 (S1-4)
Table 2: Comparison to state of the art methods on ADE20K and Cityscapes. SegFormer has signiﬁcant
advantages on #Params, #Flops, #Speed and #Accuracy. Note that for SegFormer-B0 we scale the short side of
image to {1024, 768, 640, 512} to get speed-accuracy tradeoffs.
Cityscapes
MobileNetV2
ICNet 
PSPNet 
MobileNetV2
DeepLabV3+ 
MobileNetV2
SegFormer (Ours)
Non Real-Time
ResNet-101
EncNet 
ResNet-101
PSPNet 
ResNet-101
CCNet 
ResNet-101
DeeplabV3+ 
ResNet-101
OCRNet 
GSCNN 
WideResNet38
Axial-DeepLab 
AxialResNet-XL
Dynamic Routing 
Dynamic-L33-PSP
Auto-Deeplab 
NAS-F48-ASPP
SegFormer (Ours)
SegFormer (Ours)
Mix-FFN vs. Positional Encoder (PE). In this experiment, we analyze the effect of removing the
positional encoding in the Transformer encoder in favor of using the proposed Mix-FFN. To this
end, we train Transformer encoders with a positional encoding (PE) and the proposed Mix-FFN
and perform inference on Cityscapes with two different image resolutions: 768×768 using a sliding
window, and 1024×2048 using the whole image.
Table 1c shows the results for this experiment. As shown, for a given resolution, our approach using
Mix-FFN clearly outperforms using a positional encoding. Moreover, our approach is less sensitive
to differences in the test resolution: the accuracy drops 3.3% when using a positional encoding with a
lower resolution. In contrast, when we use the proposed Mix-FFN the performance drop is reduced
to only 0.7%. From these results, we can conclude using the proposed Mix-FFN produces better and
more robust encoders than those using positional encoding.
Effective receptive ﬁeld evaluation. In Section 3.2, we argued that our MLP decoder beneﬁts
from Transformers having a larger effective receptive ﬁeld compared to other CNN models. To
quantify this effect, in this experiment, we compare the performance of our MLP-decoder when
used with CNN-based encoders such as ResNet or ResNeXt. As shown in Table 1d, coupling our
MLP-decoder with a CNN-based encoder yields a signiﬁcantly lower accuracy compared to coupling
it with the proposed Transformer encoder. Intuitively, as a CNN has a smaller receptive ﬁeld than the
Transformer (see the analysis in Section 3.2), the MLP-decoder is not enough for global reasoning.
In contrast, coupling our Transformer encoder with the MLP decoder leads to the best performance.
Moreover, for Transformer encoder, it is necessary to combine low-level local features and high-level
non-local features instead of only high-level feature.
Comparison to state of the art methods
We now compare our results with existing approaches on the ADE20K , Cityscapes and
COCO-Stuff datasets.
ADE20K and Cityscapes: Table 2 summarizes our results including parameters, FLOPS, latency,
and accuracy for ADE20K and Cityscapes. In the top part of the table, we report real-time approaches
where we include state-of-the-art methods and our results using the MiT-B0 lightweight encoder. In
the bottom part, we focus on performance and report the results of our approach and related works
using stronger encoders.
As shown, on ADE20K, SegFormer-B0 yields 37.4% mIoU using only 3.8M parameters and 8.4G
FLOPs, outperforming all other real-time counterparts in terms of parameters, ﬂops, and latency. For
instance, compared to DeeplabV3+ (MobileNetV2), SegFormer-B0 is 7.4 FPS, which is faster and
keeps 3.4% better mIoU. Moreover, SegFormer-B5 outperforms all other approaches, including the
previous best SETR, and establishes a new state-of-the-art of 51.8%, which is 1.6% mIoU better than
SETR while being signiﬁcantly more efﬁcient.
Table 3: Comparison to state of the art methods on Cityscapes
test set. IM-1K, IM-22K, Coarse and MV refer to the ImageNet-1K,
ImageNet-22K, Cityscapes coarse set and Mapillary Vistas. SegFormer
outperforms the compared methods with equal or less extra data.
Extra Data
PSPNet 
ResNet-101
PSANet 
ResNet-101
CCNet 
ResNet-101
OCNet 
ResNet-101
Axial-DeepLab 
AxiaiResNet-XL
IM-22K, Coarse
As also shown in Table 2, our results also hold
on Cityscapes. SegFormer-B0 yields 15.2 FPS
and 76.2% mIoU (the shorter side of input image being 1024), which represents a 1.3% mIoU
improvement and a 2× speedup compared to
DeeplabV3+. Moreover, with the shorter side
of input image being 512, SegFormer-B0 runs
at 47.6 FPS and yields 71.9% mIoU, which is
17.3 FPS faster and 4.2% better than ICNet.
SegFormer-B5 archives the best IoU of 84.0%,
outperforming all existing methods by at least
1.8% mIoU, and it runs 5 × faster and 4 ×
smaller than SETR .
On Cityscapes test set, we follow the common setting and merge the validation images to the
train set and report results using Imagenet-1K pre-training and also using Mapillary Vistas .
As reported in Table 3, using only Cityscapes ﬁne data and Imagenet-1K pre-training, our method
achieves 82.2% mIoU outperforming all other methods including SETR, which uses ImageNet-22K
pre-training and the additional Cityscapes coarse data. Using Mapillary pre-training, our sets a
new state-of-the-art result of 83.1% mIoU. Figure 4 shows qualitative results on Cityscapes, where
SegFormer provides better details than SETR and smoother predictions than DeeplabV3+.
Table 4: Results on COCO-Stuff full dataset containing
all 164K images from COCO 2017 and covers 172 classes.
DeeplabV3+ 
OCRNet 
COCO-Stuff. Finally, we evaluate SegFormer on the
full COCO-Stuff dataset.
For comparison, as existing methods do not provide results on this dataset,
we reproduce the most representative methods such
as DeeplabV3+, OCRNet, and SETR. In this case, the
ﬂops on this dataset are the same as those reported for
ADE20K. As shown in Table 4, SegFormer-B5 reaches
46.7% mIoU with only 84.7M parameters, which is 0.9%
better and 4× smaller than SETR. In summary, these results demonstrate the superiority of SegFormer
in semantic segmentation in terms of accuracy, computation cost, and model size.
Robustness to natural corruptions
Model robustness is important for many safety-critical tasks such as autonomous driving . In this
experiment, we evaluate the robustness of SegFormer to common corruptions and perturbations. To
DeepLabv3+
Figure 4: Qualitative results on Cityscapes. Compared to SETR, our SegFormer predicts masks with substantially ﬁner details near object boundaries. Compared to DeeplabV3+, SegFormer reduces long-range errors as
highlighted in red. Best viewed in screen.
this end, we follow and generate Cityscapes-C, which expands the Cityscapes validation set with
16 types of algorithmically generated corruptions from noise, blur, weather and digital categories. We
compare our method to variants of DeeplabV3+ and other methods as reported in . The results
for this experiment are summarized in Table 5.
Our method signiﬁcantly outperforms previous methods, yielding a relative improvement of up to
588% on Gaussian Noise and up to 295% on snow weather. The results indicate the strong robustness
of SegFormer, which we envision to beneﬁt safety-critical applications where robustness is important.
Table 5: Main results on Cityscapes-C. “DLv3+”, “MBv2”, “R” and “X” refer to DeepLabv3+, MobileNetv2,
ResNet and Xception. The mIoUs of compared methods are reported from .
Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright Contr Satur JPEG Snow Spatt Fog Frost
DLv3+ (MBv2)
38.9 47.4 17.3
DLv3+ (R50)
42.0 55.9 22.8
DLv3+ (R101)
47.8 55.1 22.7
DLv3+ (X41)
46.6 57.6 20.6
DLv3+ (X65)
50.7 63.6 23.8
DLv3+ (X71)
50.4 64.1 20.2
30.5 27.3 11.0
31.6 37.6 19.7
DilatedNet
32.3 34.7 19.2
45.9 52.9 22.2
42.7 34.4 16.2
47.3 67.9 32.6
SegFormer-B5
68.4 78.5 49.9
Conclusion
In this paper, we present SegFormer, a simple, clean yet powerful semantic segmentation method
which contains a positional-encoding-free, hierarchical Transformer encoder and a lightweight All-
MLP decoder. It avoids common complex designs in previous methods, leading to both high efﬁciency
and performance. SegFormer not only achieves new state of the art results on common datasets,
but also shows strong zero-shot robustness. We hope our method can serve as a solid baseline for
semantic segmentation and motivate further research. One limitation is that although our smallest
3.7M parameters model is smaller than the known CNN’s model, it is unclear whether it can work
well in a chip of edge device with only 100k memory. We leave it for future work.
Acknowledgement
We thank Ding Liang, Zhe Chen and Yaojun Liu for insightful discussion without which this paper
would not be possible.
Details of MiT Series
In this section, we list some important hyper-parameters of our Mix Transformer (MiT) encoder. By
changing these parameters, we can easily scale up our encoder from B0 to B5.
In summary, the hyper-parameters of our MiT are listed as follows:
• Ki: the patch size of the overlapping patch embedding in Stage i;
• Si: the stride of the overlapping patch embedding in Stage i;
• Pi: the padding size of the overlapping patch embedding in Stage i;
• Ci: the channel number of the output of Stage i;
• Li: the number of encoder layers in Stage i;
• Ri: the reduction ratio of the Efﬁcient Self-Attention in Stage i;
• Ni: the head number of the Efﬁcient Self-Attention in Stage i;
• Ei: the expansion ratio of the feed-forward layer in Stage i;
Table 6 shows the detailed information of our MiT series. To facilitate efﬁcient discussion, we assign
the code name B0 to B5 for MiT encoder, where B0 is the smallest model designed for real-time,
while B5 is the largest model designed for high performance.
More Qualitative Results on Mask Predictions
In Figure 5, we present more qualitative results on Cityscapes, ADE20K and COCO-Stuff, compared
with SETR and DeepLabV3+.
Compared to SETR, our SegFormer predicts masks with signiﬁcantly ﬁner details near object
boundaries because our Transformer encoder can capture much higher resolution features than
SETR, which preserves more detailed texture information. Compared to DeepLabV3+, SegFormer
reduces long-range errors beneﬁt from the larger effective receptive ﬁeld of Transformer encoder than
More Visualization on Effective Receptive Field
In Figure 6, we select some representative images and effective receptive ﬁeld (ERF) of DeepLabV3+
and SegFormer. Beyond larger ERF, the ERF of SegFormer is more sensitive to the context of
the image. We see SegFormer’s ERF learned the pattern of roads, cars, and buildings, while
DeepLabV3+’s ERF shows a relatively ﬁxed pattern. The results also indicate that our Transformer
encoder has a stronger feature extraction ability than ConvNets.
More Comparison of DeeplabV3+ and SegFormer on Cityscapes-C
In this section, we detailed show the zero-shot robustness compared with SegFormer and DeepLabV3+.
Following , we test 3 severities for 4 kinds of “Noise” and 5 severities for the rest 12 kinds of
corruptions and perturbations.
As shown in Figure 7, with severity increase, DeepLabV3+ shows a considerable performance
degradation. In contrast, the performance of SegFormer is relatively stable. Moreover, SegFormer
has signiﬁcant advantages over DeepLabV3+ on all corruptions/perturbations and all severities,
demonstrating excellent zero-shot robustness.
Output Size
Layer Name
Mix Transformer
Overlapping
Patch Embedding
K1 = 7; S1 = 4; P1 = 3
Transformer
Overlapping
Patch Embedding
K2 = 3; S2 = 2; P2 = 1
Transformer
Overlapping
Patch Embedding
K3 = 3; S3 = 2; P3 = 1
Transformer
Overlapping
Patch Embedding
K4 = 3; S4 = 2; P4 = 1
Transformer
Table 6: Detailed settings of MiT series. Our design follows the principles of ResNet . (1) the
channel dimension increase while the spatial resolution shrink with the layer goes deeper. (2) Stage 3
is assigned to most of the computation cost.
Params (M)
Table 7: Mix Transformer Encoder
DeepLabV3+
Figure 5: Qualitative results on Cityscapes, ADE20K and COCO-Stuff. First row: Cityscapes. Second row:
ADE20K. Third row: COCO-Stuff. Zoom in for best view.
DeepLabv3+
DeepLabv3+
DeepLabv3+
Figure 6: Effective Receptive Field on Cityscapes. ERFs of the four stages and the decoder heads of both architectures are visualized.
Gaussian Noise
Shot Noise
Impluse Noise
Speckle Noise
Motion blur
Defocus Blur
Glass Blur
Gaussian Blur
Brightness
JPEG_compression
Figure 7: Comparison of zero shot robustness on Cityscapes-C between SegFormer and DeepLabV3+. Blue line is SegFormer and orange
line is DeepLabV3+. X-Axis means corrupt severity and Y-Axis is mIoU. Following , we test 3 severities for “Noise” and 5 severities for