Published as a conference paper at ICLR 2016
NEURAL NETWORKS WITH FEW MULTIPLICATIONS
Zhouhan Lin
Universit´e de Montr´eal
 
Matthieu Courbariaux
Universit´e de Montr´eal
 
Roland Memisevic
Universit´e de Montr´eal
 
Yoshua Bengio
Universit´e de Montr´eal
For most deep learning algorithms training is notoriously time consuming. Since
most of the computation in training neural networks is typically spent on ﬂoating
point multiplications, we investigate an approach to training that eliminates the
need for most of these. Our method consists of two parts: First we stochastically
binarize weights to convert multiplications involved in computing hidden states
to sign changes. Second, while back-propagating error derivatives, in addition to
binarizing the weights, we quantize the representations at each layer to convert the
remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does
not hurt classiﬁcation performance but can result in even better performance than
standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural networks.
INTRODUCTION
Training deep neural networks has long been computational demanding and time consuming. For
some state-of-the-art architectures, it can take weeks to get models trained .
Another problem is that the demand for memory can be huge. For example, many common models
in speech recognition or machine translation need 12 Gigabytes or more of storage . To deal with these issues it is common to train deep neural networks by resorting to GPU or
CPU clusters and to well designed parallelization strategies .
Most of the computation performed in training a neural network are ﬂoating point multiplications.
In this paper, we focus on eliminating most of these multiplications to reduce computation. Based
on our previous work , which eliminates multiplications in computing
hidden representations by binarizing weights, our method deals with both hidden state computations
and backward weight updates. Our approach has 2 components. In the forward pass, weights are
stochastically binarized using an approach we call binary connect or ternary connect, and for backpropagation of errors, we propose a new approach which we call quantized back propagation that
converts multiplications into bit-shifts. 1
RELATED WORK
Several approaches have been proposed in the past to simplify computations in neural networks.
Some of them try to restrict weight values to be an integer power of two, thus to reduce all the multiplications to be binary shifts . In this way, multiplications are eliminated in both training and testing time. The disadvantage is that model performance
can be severely reduced, and convergence of training can no longer be guaranteed.
approaches
 
BinaryConnect
 
Published as a conference paper at ICLR 2016
Kim & Paris introduces a completely Boolean network, which simpliﬁes the test time computation at an acceptable performance hit. The approach still requires a real-valued, full precision
training phase, however, so the beneﬁts of reducing computations does not apply to training. Similarly, Machado et al. manage to get acceptable accuracy on sparse representation classiﬁcation by replacing all ﬂoating-point multiplications by integer shifts. Bit-stream networks also provides a way of binarizing neural network connections, by substituting weight
connections with logical gates. Similar to that, Cheng et al. proves deep neural networks
with binary weights can be trained to distinguish between multiple classes with expectation back
propagation.
There are some other techniques, which focus on reducing the training complexity. For instance,
instead of reducing the precision of weights, Simard & Graf quantizes states, learning rates,
and gradients to powers of two. This approach manages to eliminate multiplications with negligible
performance reduction.
BINARY AND TERNARY CONNECT
BINARY CONNECT REVISITED
In Courbariaux et al. , we introduced a weight binarization technique which removes multiplications in the forward pass. We summarize this approach in this subsection, and introduce an
extension to it in the next.
Consider a neural network layer with N input and M output units. The forward computation is y =
h(Wx + b) where W and b are weights and biases, respectively, h is the activation function, and x
and y are the layer’s inputs and outputs. If we choose ReLU as h, there will be no multiplications
in computing the activation function, thus all multiplications reside in the matrix product Wx. For
each input vector x, NM ﬂoating point multiplications are needed.
Binary connect eliminates these multiplications by stochastically sampling weights to be −1 or 1.
Full precision weights ¯w are kept in memory as reference, and each time when y is needed, we
sample a stochastic weight matrix W according to ¯w. For each element of the sampled matrix W,
the probability of getting a 1 is proportional to how “close” its corresponding entry in ¯w is to 1. i.e.,
P(Wij = 1) = ¯wij + 1
; P(Wij = −1) = 1 −P(Wij = 1)
It is necessary to add some edge constraints to ¯w. To ensure that P(Wij = 1) lies in a reasonable
range, values in ¯w are forced to be a real value in the interval [-1, 1]. If during the updates any of
its value grows beyond that interval, we set it to be its corresponding edge values −1 or 1. That way
ﬂoating point multiplications become sign changes.
A remaining question concerns the use of multiplications in the random number generator involved
in the sampling process. Sampling an integer has to be faster than multiplication for the algorithm to
be worth it. To be precise, in most cases we are doing mini-batch learning and the sampling process
is performed only once for the whole mini-batch. Normally the batch size B varies up to several
hundreds. So, as long as one sampling process is signiﬁcantly faster than B times of multiplications,
it is still worth it. Fortunately, efﬁciently generating random numbers has been studied in Jeavons
et al. ; van Daalen et al. . Also, it is possible to get random numbers according to real
random processes, like CPU temperatures, etc. We are not going into the details of random number
generation as this is not the focus of this paper.
TERNARY CONNECT
The binary connect introduced in the former subsection allows weights to be −1 or 1. However, in
a trained neural network, it is common to observe that many learned weights are zero or close to
zero. Although the stochastic sampling process would allow the mean value of sampled weights to
be zero, this suggests that it may be beneﬁcial to explicitly allow weights to be zero.
To allow weights to be zero, some adjustments are needed for Eq. 1. We split the interval of [-1, 1],
within which the full precision weight value ¯
wij lies, into two sub-intervals: [−1, 0] and = ¯wij; P(Wij = 0) = 1 −¯wij
P(Wij = −1) = −¯wij; P(Wij = 0) = 1 + ¯wij
Like binary connect, ternary connect also eliminates all multiplications in the forward pass.
QUANTIZED BACK PROPAGATION
In the former section we described how multiplications can be eliminated from the forward pass. In
this section, we propose a way to eliminate multiplications from the backward pass.
Suppose the i-th layer of the network has N input and M output units, and consider an error signal
δ propagating downward from its output. The updates for weights and biases would be the outer
product of the layer’s input and the error signal:
′ (Wx + b)
′ (Wx + b)
where η is the learning rate, and x the input to the layer. The operator ⊙stands for element-wise
multiply. While propagating through the layers, the error signal δ needs to be updated, too. Its
update taking into account the next layer below takes the form:
′ (Wx + b)
There are 3 terms that appear repeatedly in Eqs. 4 to 6: δ, h
′ (Wx + b) and x. The latter two terms
introduce matrix outer products. To eliminate multiplications, we can quantize one of them to be an
integer power of 2, so that multiplications involving that term become binary shifts. The expression
′ (Wx + b) contains downﬂowing gradients, which are largely determined by the cost function and
network parameters, thus it is hard to bound its values. However, bounding the values is essential
for quantization because we need to supply a ﬁxed number of bits for each sampled value, and if
that value varies too much, we will need too many bits for the exponent. This, in turn, will result in
the need for more bits to store the sampled value and unnecessarily increase the required amount of
computation.
′ (Wx + b) is not a good choice for quantization, x is a better choice, because it is the
hidden representation at each layer, and we know roughly the distribution of each layer’s activation.
Our approach is therefore to eliminate multiplications in Eq. 4 by quantizing each entry in x to an
integer power of 2. That way the outer product in Eq. 4 becomes a series of bit shifts. Experimentally, we ﬁnd that allowing a maximum of 3 to 4 bits of shift is sufﬁcient to make the network
work well. This means that 3 bits are already enough to quantize x. As the ﬂoat32 format has 24
bits of mantissa, shifting (to the left or right) by 3 to 4 bits is completely tolerable. We refer to this
approach of back propagation as “quantized back propagation.”
If we choose ReLU as the activation function, and since we are reusing the (Wx + b) that was
computed during the forward pass, computing the term h
′ (Wx + b) involves no additional sampling
or multiplications. In addition, quantized back propagation eliminates the multiplications in the
outer product in Eq. 4. The only places where multiplications remain are the element-wise products.
In Eq. 5, multiplying by η and σ requires 2 × M multiplications, while in Eq. 4 we can reuse the
result of Eq. 5. To update δ would need another M multiplications, thus 3 × M multiplications
Published as a conference paper at ICLR 2016
are needed for all computations from Eqs. 4 through 6. Pseudo code in Algorithm 1 outlines how
quantized back propagation is conducted.
Algorithm 1 Quantized Back Propagation (QBP). C is the cost function. binarize(W) and clip(W)
stands for binarize and clip methods. L is the number of layers.
Require: a deep model with parameters W, b at each layer. Input data x, its corresponding targets
y, and learning rate η.
1: procedure QBP(model, x, y, η)
1. Forward propagation:
for each layer i in range(1, L) do
Wb ←binarize(W)
Compute activation ai according to its previous layer output ai−1, Wb and b.
2. Backward propagation:
Initialize output layer’s error signal δ =
for each layer i in range(L, 1) do
Compute ∆W and ∆b according to Eqs. 4 and 5.
Update W: W ←clip(W −∆W)
Update b: b ←b −∆b
∂ak−1 by updating δ according to Eq. 6.
Like in the forward pass, most of the multiplications are used in the weight updates. Compared
with standard back propagation, which would need 2MN +3M multiplications at least, the amount
of multiplications left is negligible in quantized back propagation. Our experiments in Section 5
show that this way of dramatically decreasing multiplications does not necessarily entail a loss in
performance.
EXPERIMENTS
We tried our approach on both fully connected networks and convolutional networks. Our implementation uses Theano . We experimented with 3 datasets: MNIST, CIFAR10,
and SVHN. In the following subsection we show the performance that these multiplier-light neural
networks can achieve. In the subsequent subsections we study some of their properties, such as
convergence and robustness, in more detail.
GENERAL PERFORMANCE
We tested different variations of our approach, and compare the results with Courbariaux et al.
 and full precision training (Table 1). All models are trained with stochastic gradient descent
(SGD) without momentum. We use batch normalization for all the models to accelerate learning.
At training time, binary (ternary) connect and quantized back propagation are used, while at test
time, we use the learned full resolution weights for the forward propagation. For each dataset, all
hyper-parameters are set to the same values for the different methods, except that the learning rate
is adapted independently for each one.
Table 1: Performances across different datasets
Full precision
Binary connect
Binary connect +
Quantized backprop
Ternary connect +
Quantized backprop
Published as a conference paper at ICLR 2016
The MNIST dataset has 50000 images for training and 10000 for testing. All
images are grey value images of size 28 × 28 pixels, falling into 10 classes corresponding to the
10 digits. The model we use is a fully connected network with 4 layers: 784-1024-1024-1024-10.
At the last layer we use the hinge loss as the cost. The training set is separated into two parts, one
of which is the training set with 40000 images and the other the validation set with 10000 images.
Training is conducted in a mini-batch way, with a batch size of 200.
With ternary connect, quantized backprop, and batch normalization, we reach an error rate of 1.15%.
This result is better than full precision training (also with batch normalization), which yields an error
rate 1.33%. If without batch normalization, the error rates rise to 1.48% and 1.67%, respectively. We
also explored the performance if we sample those weights during test time. With ternary connect
at test time, the same model (the one reaches 1.15% error rate) yields 1.49% error rate, which is
still fairly acceptable. Our experimental results show that despite removing most multiplications,
our approach yields a comparable (in fact, even slightly higher) performance than full precision
training. The performance improvement is likely due to the regularization effect implied by the
stochastic sampling.
Taking this network as a concrete example, the actual amount of multiplications in each case can
be estimated precisely. Multiplications in the forward pass is obvious, and for the backward pass
section 4 has already given an estimation. Now we estimate the amount of multiplications incurred
by batch normalization. Suppose we have a pre-hidden representation h with mini-batch size B on
a layer which has M output units (thus h should have shape B × M), then batch normalization can
be formalized as γ h−mean(h)
+ β. One need to compute the mean(h) over a mini-batch, which
takes M multiplications, and BM + 2M multiplication to compute the standard deviation std(h).
The fraction takes BM divisions, which should be equal to the same amount of multiplication.
Multiplying that by the γ parameter, adds another BM multiplications. So each batch normalization
layer takes an extra 3BM + 3M multiplications in the forward pass. The backward pass takes
roughly twice as many multiplications in addition, if we use SGD. These amount of multiplications
are the same no matter we use binarization or not. Bearing those in mind, the total amount of
multiplications invoked in a mini-batch update are shown in Table 2. The last column lists the ratio
of multiplications left, after applying ternary connect and quantized back propagation.
Table 2: Estimated number of multiplications in MNIST net
Full precision
Ternary connect +
Quantized backprop
without BN
1.7480 × 109
1.8492 × 106
1.7535 × 109
7.4245 × 106
CIFAR10 contains images of size 32 × 32 RGB pixels. Like for
MNIST, we split the dataset into 40000, 10000, and 10000 training-, validation-, and test-cases,
respectively. We apply our approach in a convolutional network for this dataset. The network has
6 convolution/pooling layers, 1 fully connected layer and 1 classiﬁcation layer. We use the hinge
loss for training, with a batch size of 100. We also tried using ternary connect at test time. On
the model trained by ternary connect and quantized back propagation, it yields 13.54% error rate.
Similar to what we observed in the fully connected network, binary (ternary) connect and quantized
back propagation yield a slightly higher performance than ordinary SGD.
The Street View House Numbers (SVHN) dataset contains RGB images of
house numbers. It contains more than 600,000 images in its extended training set, and roughly
26,000 images in its test set. We remove 6,000 images from the training set for validation. We use 7
layers of convolution/pooling, 1 fully connected layer, and 1 classiﬁcation layer. Batch size is also
Published as a conference paper at ICLR 2016
set to be 100. The performances we get is consistent with our results on CIFAR10. Extending the
ternary connect mechanism to its test time yields 2.99% error rate on this dataset. Again, it improves
over ordinary SGD by using binary (ternary) connect and quantized back propagation.
CONVERGENCE
Taking the convolutional networks on CIFAR10 as a test-bed, we now study the learning behaviour
in more detail. Figure 1 shows the performance of the model in terms of test set errors during
training. The ﬁgure shows that binarization makes the network converge slower than ordinary SGD,
but yields a better optimum after the algorithm converges. Compared with binary connect (red line),
adding quantization in the error propagation (yellow line) doesn’t hurt the model accuracy at all.
Moreover, having ternary connect combined with quantized back propagation (green line) surpasses
all the other three approaches.
Figure 1: Test set error rate at each epoch for ordinary back propagation, binary connect, binary
connect with quantized back propagation, and ternary connect with quantized back propagation.
Vertical axis is represented in logarithmic scale.
THE EFFECT OF BIT CLIPPING
In Section 4 we mentioned that quantization will be limited by the number of bits we use. The
maximum number of bits to shift determines the amount of memory needed, but it also determines
in what range a single weight update can vary. Figure 2 shows the model performance as a function
of the maximum allowed bit shifts. These experiments are conducted on the MNIST dataset, with
the aforementioned fully connected model. For each case of bit clipping, we repeat the experiment
for 10 times with different initial random instantiations.
The ﬁgure shows that the approach is not very sensible to the number of bits used. The maximum
allowed shift in the ﬁgure varies from 2 bits to 10 bits, and the performance remains roughly the
same. Even by restricting bit shifts to 2, the model can still learn successfully. The fact that the
performance is not very sensitive to the maximum of allowed bit shifts suggests that we do not need
to redeﬁne the number of bits used for quantizing x for different tasks, which would be an important
practical advantage.
The x to be quantized is not necessarily distributed symmetrically around 2. For example, Figure 3
shows the distribution of x at each layer in the middle of training. The maximum amount of shift
to the left does not need to be the same as that on the right. A more efﬁcient way is to use different
values for the maximum left shift and the maximum right shift. Bearing that in mind, we set it to 3
bits maximum to the right and 4 bits to the left.
Published as a conference paper at ICLR 2016
Figure 2: Model performance as a function of the maximum bit shifts allowed in quantized back
propagation. The dark blue line indicates mean error rate over 10 independent runs, while light blue
lines indicate their corresponding maximum and minimum error rates.
Figure 3: Histogram of representations at each layer while training a fully connected network for
MNIST. The ﬁgure represents a snap-shot in the middle of training. Each subﬁgure, from bottom
up, represents the histogram of hidden states from the ﬁrst layer to the last layer. The horizontal
axes stand for the exponent of the layers’ representations, i.e., log2 x.
CONCLUSION AND FUTURE WORK
We proposed a way to eliminate most of the ﬂoating point multiplications used during training a
feedforward neural network. This could make it possible to dramatically accelerate the training of
neural networks by using dedicated hardware implementations.
A somewhat surprising fact is that instead of damaging prediction accuracy the approach tends improve it, which is probably due to several facts. First is the regularization effect that the stochastic
sampling process entails. Noise injection brought by sampling the weight values can be viewed as a
regularizer, and that improves the model generalization. The second fact is low precision weight values. Basically, the generalization error bounds for neural nets depend on the weights precision. Low
precision prevents the optimizer from ﬁnding solutions that require a lot of precision, which correspond to very thin (high curvature) critical points, and these minima are more likely to correspond
to overﬁtted solutions then broad minima (there are more functions that are compatible with such
solutions, corresponding to a smaller description length and thus better generalization). Similarly,
Published as a conference paper at ICLR 2016
Neelakantan et al. adds noise into gradients, which makes the optimizer prefer large-basin
areas and forces it to ﬁnd broad minima. It also lowers the training loss and improves generalization.
Directions for future work include exploring actual implementations of this approach (for example,
using FPGA), seeking more efﬁcient ways of binarization, and the extension to recurrent neural
ACKNOWLEDGMENTS
The authors would like to thank the developers of Theano . We acknowledge the
support of the following agencies for research funding and computing support: Samsung, NSERC,
Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR.