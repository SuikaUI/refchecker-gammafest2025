Kernel Method for Nonlinear Granger Causality
Daniele Marinazzo, Mario Pellicoro, and Sebastiano Stramaglia
Dipartimento Interateneo di Fisica, Universita` di Bari, Italy
Istituto Nazionale di Fisica Nucleare, Sezione di Bari, Italy
TIRES-Center of Innovative Technologies for Signal Detection and Processing, Universita` di Bari, Italy
 
Important information on the structure of complex systems can be obtained by measuring to what extent
the individual components exchange information among each other. The linear Granger approach, to
detect cause-effect relationships between time series, has emerged in recent years as a leading statistical
technique to accomplish this task. Here we generalize Granger causality to the nonlinear case using the
theory of reproducing kernel Hilbert spaces. Our method performs linear Granger causality in the feature
space of suitable kernel functions, assuming arbitrary degree of nonlinearity. We develop a new strategy to
cope with the problem of overﬁtting, based on the geometry of reproducing kernel Hilbert spaces.
Applications to coupled chaotic maps and physiological data sets are presented.
DOI: 10.1103/PhysRevLett.100.144103
PACS numbers: 05.45.Tp, 05.10.a, 87.10.e
Experiments in many ﬁelds of science provide a time
series of simultaneously recorded variables. The analysis
of the synchronization between time series is an important tool to study communications between different
components of complex systems. In many systems, however, it is important not only to detect synchronized states,
but also to identify cause-effect (drive-response) relationships between components . Information-theoretic approaches to causality are based on the estimation of
entropy and mutual information , a hard numerical
problem when conditioning with respect to a large number
of variables is to be done. Another major approach to
analyzing causality between two time series has been
proposed by Granger : if the prediction error of the ﬁrst
time series is reduced by including measurements from the
second one in the linear regression model, then the second
time series is said to have a causal inﬂuence on the ﬁrst
one. This linear frame for measuring causality has been
widely applied in many ﬁelds, including rheochaos 
neurophysiology , economy , and climatology
 . The importance of Granger causality is the suggestion
to use prediction (and tools from learning theory in
particular) to measure the amount of information exchanged by two (sub)systems; it is worth mentioning that
also a measure of self-organization, rooted in optimal
predictors, has been recently proposed . Some attempts
to extend Granger causality to the nonlinear case have been
recently proposed . The main problem of all approaches is the detection of false causalities , which
may arise due to overﬁtting of the learning scheme.
The purpose of this work is to present a novel approach
that measures Granger causality of time series, assuming
an arbitrary degree of nonlinearity, while controlling over-
ﬁtting, and thus avoiding the problem of false causalities.
To this aim we exploit the properties of kernel machines,
the state of the art in learning models .
We start describing the connection between Granger
causality and information-theoretic approaches like the
transfer entropy TE in . Let fngn1;...;Nm be a time
series that may be approximated by a stationary Markov
pnjn1; . . . ; nm 
pnjn1; . . . ; nm1. We will use the shorthand notation
Xi  i; . . . ; im1>
and xi  im,
1; . . . ; N, and treat these quantities as N realizations of
the stochastic variables X and x. The minimizer of the
risk functional, Rf  R dXdxx  fX2pX; x, represents the best estimate of x, given X, and corresponds 
to the regression function fX  R dxpxjXx. Now, let
fngn1;...;Nm be another time series of simultaneously
acquired quantities, and denote Yi  i; . . . ; im1>.
The best estimate of x, given X and Y, is now gX; Y 
R dxpxjX; Yx. If the generalized Markov property holds,
pxjX; Y  pxjX;
then fX  gX; Y and the knowledge of Y does not
improve the prediction of x. TE is a measure of the
violation of (1); it follows that Granger causality implies a
nonzero transfer entropy.
Because of the ﬁniteness of N, the risk functional cannot
be evaluated; we consider the empirical risk ERf 
i1 xi  fXi2, and the search for the minimum of
ER is constrained in a suitable functional space, called
hypothesis space; the simplest choice is the space of all
linear functions, corresponding to linear regression. In the
following we propose a geometrical description of linear
Granger causality. For each  2 f1; . . . ; mg, the samples of
the th component of X form a vector u 2 <N; without
loss of generality we assume that each u has zero mean
and that x  x1; . . . ; xN> is normalized and zero mean.
We denote ~xi the value of the linear regression of x versus
X, evaluated at Xi. The vector ~x  ~x1; . . . ; ~xN> can be
PRL 100, 144103 
P H Y S I C A L
R E V I E W
L E T T E R S
week ending
11 APRIL 2008
0031-9007=08=100(14)=144103(4)
© 2008 The American Physical Society
obtained as follows. Let H <N
be the span of
u1; . . . ; um; then ~x is the projection of x on H. In other
words, calling P the projector on the space H, we have ~x 
Px. Moreover, the prediction error, given X, is x  jjx 
~xjj2  1  ~x>~x. Calling X the m
N matrix having vectors u as rows, H coincides with the range of the N
matrix K  X>X.
Using both X and Y, the values of the linear regression
form the vector ~x0  P0x, P0 being the projector on the
space H0 <N, spanned by the u1; . . . ; um and the components of Y v1; . . . ; vm (assumed to be zero mean). H0 is
the range of the matrix K0  Z>Z, where Z is the 2m
matrix with vectors u and v as rows. The prediction error
is now xy  jjx  ~x0jj2  1  ~x0>~x0. We now note that
H H0; hence, H0  H
H?. The last formula shows
geometrically the enlargement of the hypothesis space, due
to the inclusion of the Y variables. Calling P? the projector
on H?, we have xy  x  jjP?xjj2, and the linear
Granger causality index reads
Y ! X  x  xy
 jjP?xjj2
1  ~x>~x :
Linear Granger causality is usually assessed according to
well-known test statistics; see, e.g., . Instead of assessing the presence (or not) of causality by means of a single
statistical test, and in view of the nonlinear extension, we
introduce a causality index which by construction is not
affected by overﬁtting. We observe that H? is the range of
the matrix
~K  K0  PK0  K0P  PK0P. Hence the
natural choice of the orthonormal basis in H? is the set
of the eigenvectors, with nonvanishing eigenvalue, of ~K.
Calling t1; . . . ; tm these eigenvectors, we have jjP?xjj2 
i , where ri is the Pearson’s correlation coefﬁcient of
x and ti. To avoid false causalities, we ﬁrst evaluate, by
Student’s t test, the probability i that ri is due to chance,
assuming x and ti normal. Since we are dealing with
multiple comparison, we use the Bonferroni correction to
select the eigenvectors ti0, correlated with x, with expected
fraction of false positive equal to 0.05. Then we calculate a
new causality index by summing only over the fri0g which
pass the Bonferroni test, thus obtaining what we call
ﬁltered linear Granger causality index:
FY ! X 
1  ~x>~x :
Exchanging the roles of the two time series, we may
evaluate the causality index in the opposite direction
FX ! Y.
The formulation of linear Granger causality, above described, allows an efﬁcient generalization to the nonlinear
case using methods of the theory of reproducing kernel
Hilbert spaces . Let us ﬁrst deal with the problem of
predicting x using the knowledge of X. Given a kernel
representation
kX; X0 
aaaXaX0, we consider H, the range of the N
N Gram matrix K with elements Kij  kXi; Xj. As in the
linear case, we calculate ~x, the projection of x onto H.
Because of the spectral representation of k, ~x coincides
with the linear regression of x in the feature space spanned
a, the eigenfunctions of k; the regression is nonlinear in the original variables. We remark that H corresponds to the functional space where well-known methods,
like support vector machines and kernel ridge regression,
search for the regressor .
While using both X and Y to predict x, we append X and
Y variables to construct the Z variable with samples Zi 
XiYi>; then we evaluate the Gram matrix K0 with elements K0
ij  kZi; Zj. The regression values now form
vector ~x0 equal to the projection of x on H0, the range of
K0. In this work we consider two choices of the kernel (see
the discussion in ): the inhomogeneous polynomial
(IP) of integer order p: kpX; X0  1  X>X0p, and the
Gaussian: kX; X0  exp XX0>XX0
, whose complexity depends on the scale parameter .
First we consider the IP kernel. In this case the eigenfunctions a are all the monomials, in the input variables,
up to the pth degree. In this case H H0, and we can
proceed as in the linear case, decomposing H0  H
and calculating ~K  K0  PK0  K0P  PK0P. Along
the same lines as those described in the linear case, we
may construct the ﬁltered Granger causality taking into
account only the eigenvectors of
~K which pass the
Bonferroni test. We discuss some examples of application
of our method with IP kernel. First we consider two
unidirectionally coupled noisy logistic maps:
xn1  1  ax2n  s n;
yn1  1  e1  ay2n  e1  ax2n  s
g are unit variance Gaussianly distributed noise
terms (the parameter s determines their relevance), a 
1:8 and e 2 0; 1 represents the coupling x ! y. In the
noise-free case (s  0), a transition to complete synchronization occurs at e  0:37. Varying e and s, we have
considered runs of N iterations, after a transient of 103, and
evaluated F, using the kernel with p  2, in both directions. We ﬁnd that FY ! X is zero for all values of e, s,
and N. On the other hand FX ! Y is zero at e smaller
than a threshold ec; see Fig. 1. FX ! Y is zero also at
complete synchronization, as there is no information transfer in this regime. As noted in , the causal relation can
be inferred only when the coupling is not large enough to
let full synchronization emerge, or when the synchronized
state is frequently perturbed by internal or external noise
driving the system out of the synchronized state. Indeed, at
ﬁxed e > 0:37, FX ! Y is zero until s reaches a threshold sc. Both ec and sc scale as N0:5, as expected, see
As another simulated example, we consider two unidirectionally coupled Henon maps:
PRL 100, 144103 
P H Y S I C A L
R E V I E W
L E T T E R S
week ending
11 APRIL 2008
xn  1:4  x2
n1  0:3^xn1;
^xn  xn1;
yn  1:4  0:3^yn1  0:7y2
n1  0:3xn1yn1;
^yn  yn1;
and analyze the causality between time series fxg and fyg;
by construction, x is driving y. Using m  2 and IP kernel
with various values of p, on runs of length N  1000, we
correctly ﬁnd that the causality y ! x is always zero while
the causality x ! y is nonzero and maximal at p  2, the
interaction being quadratic (Table I).
A real example consists in rat electroencephalogram
(EEG) signals from right (R) and left (L) cortical intracranial electrodes, employed in the study of the pathophysiology of epilepsy and already analyzed in . We
analyze both the normal EEG signals and the EEG signals
from the same rat after unilateral lesion in the rostral pole
of the reticular thalamic nucleus, when spike discharges
are observed due to local synchronization of neurons activity in the neighborhood of the electrode at which the
signal was recorded. In Fig. 3 top, the indices F, for the
normal EEG signals of the rat, are depicted for p  1, 2, 3,
4. We ﬁnd zero causality in the direction L ! R for all p,
and a small causality R ! L only at p  1. After the
unilateral lesion (Fig. 3 bottom) we ﬁnd that causality R !
L is almost unchanged, while a relevant L ! R causality
now appears at p  1 and (smaller) at p  2. A more
conservative statistical procedure, in situations where the
value of p is not known a priori, is to apply, at each p, the
Bonferroni’s correction corresponding to the total number
of comparisons, in this case 91 (  2  9  25  55);
using this correction, the causality R ! L becomes zero
in both cases and for all p, while the causality L ! R
remains unchanged and equal to the values depicted in
Fig. 3. The results reported in are qualitatively consistent with our ﬁndings, indeed the same directions of
asymmetry are found in the two analyses, but our approach
allows us to make more sharp and precise statements about
the causality relationships between the two EEG signals:
the only statistically robust causality relationship is L ! R
after the lesion. Moreover, as the maximum of FL ! R
occurs at p  1, our analysis seems to suggest that in this
experiment the information transfer mechanism is essentially linear .
Turning to consider the Gaussian kernel, the condition
H H0 does not necessarily hold and some differences in
the approach are in order. In this case we call H the span of
the eigenvectors of K whose eigenvalue is not smaller than
max, where max is the largest eigenvalue of K and
small number (we use
 106). We calculate ~x  Px,
where P is the projector on H. After evaluating the Gram
matrix K0, the following matrix is considered:
where fwg are the eigenvectors of K0, and the sum is over
the eigenvalues f
ig not smaller than
times the largest
eigenvalue of K0. Then we evaluate ~K  K  PK 
KP  PKP, and denote P? the projector onto the range
sc(N)=0.55 N−0.5
ec(N)=2.47 N−0.5
Scaling of the critical values of e and s (see the text)
with N. ec also has a weak dependence on s.
Causalities x ! y for coupled Henon’s maps.
right−>left
left−>right
right−>left
left−>right
The ﬁltered causality indexes, for the rat EEG signals
before (top) and after the lesion (bottom), is displayed versus p,
the order of the inhomogeneous polynomial kernel.
The ﬁltered causality index, for the coupled maps, is
displayed versus e for three values of s. The inhomogeneous
polynomial kernel with p  2 is used, and m  1.
PRL 100, 144103 
P H Y S I C A L
R E V I E W
L E T T E R S
week ending
11 APRIL 2008
of ~K. The ﬁltered Granger causality index for Gaussian
kernels is then constructed as in the previous cases. As
another real example, we consider time series of heart rate
(H) and breath rate (B) of a sleeping human suffering from
sleep apnea . Using IP kernels, we ﬁnd unidirectional causality H ! B; its strength increases with the
order p of the kernel, from F  0:01 at p  1 to F 
0:03 at p  5. These ﬁndings conﬁrm the strongly nonlinear nature of the interaction between heart and respiration signals in sleep apnea syndrome , which is evident
also using the Gaussian kernel and varying , as depicted
in Fig. 4 top. No causality B ! H is found to be signi-
ﬁcative, while nonzero causality H ! B is found for 
1. Note that the causality index vanishes, by construction,
at small  and at large , because in both limits the kernel
matrix tends to be constant (0 and 1, respectively). In Fig. 4
bottom the bivariate time series is analyzed by means of the
transfer entropy . It is interesting to compare the two
approaches in this application. TE is nonzero in both
directions and shows a slightly stronger ﬂow of information H ! B. Our approach recognizes, as signiﬁcative,
only the causality H ! B, thus revealing unidirectional
drive-response relationship in the sleep apnea pathology.
In conclusion, we considered the problem of nonlinear
coherence of signals, in particular, the detection of driveresponse relationships. Exploiting the geometry of reproducing kernel Hilbert spaces we have introduced a ﬁltered
index which is able to measure cause-effect relationships
with an arbitrary amount of nonlinearity, and is not affected
by overﬁtting. The choice of the optimal value of m can be
done using the standard cross validation scheme or the
embedding dimension . Our method is equivalent to
performing a linear Granger causality in the feature space
of the kernel; hence, also in the nonlinear case, our approach continues to fulﬁll the good properties of linear
models. The framework of Granger causality assumes
stationarity of signals: further work should deal with the
effects of nonstationarities on nonlinear estimates of causalities .
 M. Lungarella, K. Ishiguro, Y. Kuniyoshi, and N. Otsu,
Int. J. Bifurcation Chaos Appl. Sci. Eng. 17, 903 .
 T. Schreiber, Phys. Rev. Lett. 85, 461 .
 M. Wiesenfeldt, U. Parlitz, and W. Lauterborn, Int. J.
Bifurcation Chaos Appl. Sci. Eng. 11, 2217 .
 S. Frenzel and B. Pompe, Phys. Rev. Lett. 99, 204101
 C. W. J. Granger, Econometrica 37, 424 .
 R. Ganapathy et al., Phys. Rev. E 75, 016211 .
 M. Kaminski, M. Z. Ding, W. A. Truccolo, and S. L.
Bressler, Biol. Cybern. 85, 145 ; K. J. Blinowska,
R. Kus, and M. Kaminski, Phys. Rev. E 70, 050902(R)
 G. Rodriguez, N. Rowe, J. Int. Money Finance 26, 1174
 U. Triacca, Theor. Appl. Climatol. 81, 133 .
 V. Vapnik, Statistical Learning Theory .
 C. R. Shalizi, K. L. Shalizi, and R. Haslinger, Phys. Rev.
Lett. 93, 118701 .
 Y. Chen et al., Phys. Lett. A 324, 26 ; P. F. Verdes,
Phys. Rev. E 72, 026222 ; D. Marinazzo et al.,
Phys. Rev. E 73, 066216 .
 M. Palus and M. Vejmelka, Phys. Rev. E 75, 056211
 ; H. Nalatore and M. Z. Ding, Phys. Rev. E 75,
031123 .
 J. Shawe-Taylor and N. Cristianini, Kernel Methods For
Pattern Analysis .
 N. Ancona and S. Stramaglia, Neural Comput. 18, 749
 R. Quian Quiroga et al., Phys. Rev. E 65, 041903 .
 Another possible explanation is that, due to a high level of
noise, nonlinear effects cannot be put in evidence in this
experiment.
 
 J. A. Jo et al., Ann. Biomed. Eng. 35, 1425 .
 H. Kantz and T. Schreiber, Nonlinear Time Series Analysis
 .
 M. Dhamala, G. Rangarajan, and M. Ding, Phys. Rev.
Lett. 100, 018701 .
breath−>heart
heart−>breath
breath−>heart
heart−>breath
(Top) The ﬁltered causality indexes, for the physionet
data set, are displayed versus , the width of the Gaussian
kernel. (Bottom) Transfer entropies versus r, the length scale.
PRL 100, 144103 
P H Y S I C A L
R E V I E W
L E T T E R S
week ending
11 APRIL 2008