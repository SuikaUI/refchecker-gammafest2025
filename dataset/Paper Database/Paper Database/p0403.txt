PEGASUS: Pre-training with Extracted Gap-sentences for
Abstractive Summarization
Jingqing Zhang * 1 Yao Zhao * 2 Mohammad Saleh 2 Peter J. Liu 2
Recent work pre-training Transformers with
self-supervised objectives on large text corpora
has shown great success when ﬁne-tuned on
downstream NLP tasks including text summarization.
However, pre-training objectives tailored for abstractive text summarization have
not been explored.
Furthermore there is a
lack of systematic evaluation across diverse domains. In this work, we propose pre-training
large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important
sentences are removed/masked from an input document and are generated together as one output
sequence from the remaining sentences, similar
to an extractive summary. We evaluated our best
PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured
by ROUGE scores. Our model also shows surprising performance on low-resource summarization,
surpassing previous state-of-the-art results on 6
datasets with only 1000 examples. Finally we
validated our results using human evaluation and
show that our model summaries achieve human
performance on multiple datasets.
*Equal contribution
1Data Science Institute,
College London, London, UK
2Brain Team, Google Research,
Correspondence
< >,
Yao Zhao < >,
Mohammad Saleh
< >, Peter J. Liu < >.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).
Figure 1: The base architecture of PEGASUS is a standard
Transformer encoder-decoder. Both GSG and MLM are
applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence
is masked with [MASK1] and used as target generation text
(GSG). The other two sentences remain in the input, but
some tokens are randomly masked by [MASK2] (MLM).
Introduction
Text summarization aims at generating accurate and concise
summaries from input document(s). In contrast to extractive
summarization which merely copies informative fragments
from the input, abstractive summarization may generate
novel words. A good abstractive summary covers principal
information in the input and is linguistically ﬂuent.
abstractive
summarization,
sequence-to-sequence
 has become a dominant framework
using encoder-decoder architectures based on RNNs
 
and more recently Transformers .
Most prior work on neural abstractive summarization
relied on large-scale, high-quality datasets of supervised
document-summary pairs and
achieved promising results . In recent years, there has
been increased interest in collecting new summarization
datasets that have more abstractive summaries , have longer documents, , utilize multiple documents , and are sourced from diverse domains ;
 
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
however, there has been little work on systematic evaluation
of models across these broad settings.
Contemporaneously, the adoption of Transformer models
 pre-trained using self-supervised objectives on large text corpora have improved performance on many NLP tasks
 .
Recent work leveraging such pre-training for Transformerbased sequence-to-sequence models has extended the success to text generation, including abstractive summarization.
In this work, we study pre-training objectives speciﬁcally
for abstractive text summarization and evaluate on 12 downstream datasets spanning news , science , short
stories , instructions , emails , patents , and legislative bills . We ﬁnd that masking whole sentences from a document and generating these gap-sentences from the rest of the
document works well as a pre-training objective for downstream summarization tasks. In particular, choosing putatively important sentences outperforms lead or randomly
selected ones. We hypothesize this objective is suitable for
abstractive summarization as it closely resembles the downstream task, encouraging whole-document understanding
and summary-like generation. We call this self-supervised
objective Gap Sentences Generation (GSG). Using GSG
to pre-train a Transformer encoder-decoder on large corpora of documents (Web and news articles) results in our
method, Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence models, or
With our best 568M parameter model trained on the recently introduced C4 corpus we equal
or exceed state-of-the-art on the 12 summarization tasks
we consider. We further push forward the state-of-the-art
using a newly collected text corpus comprised of news-like
articles we call HugeNews, including the highly competitive
XSum and CNN/DailyMail summarization datasets.
Large-scale document-summary datasets are rare and in
practice there is a mismatch between research datasets and
real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. We simulate this setting and show that our
model is able to adapt very quickly when ﬁne-tuning with
small numbers of supervised pairs, obtaining state-of-the-art
results in 6 datasets with only 1000 examples.
Qualitatively we observed high quality outputs from our
best models and validated this in human evaluation studies.
We found that PEGASUS summaries are at least as good as
reference summaries for the datasets we assessed – XSum,
CNN/DailyMail, and Reddit TIFU – even at low-levels of
supervision.
To summarize our contributions:
• We propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences.
• We evaluate the proposed pre-training objective on a
broad range of downstream summarization tasks, with
careful ablations to choose the best model settings,
which we use to train a 568M parameter PEGASUS
model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered.
• We show how good abstractive summarization performance can be achieved across broad domains with
very little supervision by ﬁne-tuning the PEGASUS
model and surpassing previous state-of-the-art results
on many tasks with as little as 1000 examples.
• We conducted human evaluation studies to validate our
experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail,
and Reddit TIFU.
Related Work
Dai & Le ; Ramachandran et al. used LM and
autoencoder pre-training on in-domain data to improve performance of RNN sequence models. However, the combination of pre-training with much larger external text corpora
(such as Wikipedia, books, or Web-pages) and Transformerbased sequence models has led to a dramatic improvement in
performance when ﬁne-tuned for both natural language understanding and text generation tasks . Most similar to our approach are Transformer
encoder-decoder models pre-trained on some masked input
pre-training objective.
 proposed masked sequence-tosequence generation that reconstructs a sentence fragment
given the remaining part of the sentence. A single sentence
fragment was randomly selected.
 proposed jointly training on
three types of language modeling tasks: unidirectional (leftto-right and right-to-left), bidirectional (word-level mask,
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
with next sentence prediction), and sequence-to-sequence
(word-level mask) prediction.
 generalized the text-to-text framework to a variety of NLP tasks and showed the advantage
of scaling up model size (to 11 billion parameters) and
pre-training corpus, introducing C4, a massive text corpus
derived from Common Crawl, which we also use in some
of our models. T5 was pre-trained with randomly corrupted
text spans of varying mask ratios and sizes of spans.
 introduced a denoising autoencoder to pre-train sequence-to-sequence models. BART
corrupted text with an arbitrary noising function and learned
to reconstruct the original text. For generation tasks, the
noising function was text inﬁlling which used single mask
tokens to mask random sampled spans of text.
In contrast to MASS, UniLM, BART and T5, the proposed
PEGASUS masks multiple whole sentences rather than
smaller continuous text spans. In our ﬁnal objective we
deterministically choose sentences based on importance,
rather than randomly. As in T5, PEGASUS does not reconstruct full input sequences, and only generates the masked
sentences as a single output sequence. In this work we focus
entirely on downstream summarization (generative) tasks
and do not evaluate on NLU classiﬁcation tasks.
There has been some work on the low-resource, summarization setting using the CNN/DailyMail dataset. Radford
et al. showed that a large Transformer language
model pre-trained on Web text could generate summaries
if prompted with ”TL;DR”, achieving a ROUGE-2 of 8.27
on CNN/DailyMail. Khandelwal et al. pre-trained a
Transformer language model on Wikipedia, and ﬁne-tuned
using 3000 examples, achieving 13.1 ROUGE-2.
Pre-training Objectives
We propose a new pre-training objective, GSG, in this
work, but for comparison, we also evaluate BERT’s maskedlanguage model objective, in isolation and in conjunction
Gap Sentences Generation (GSG)
We hypothesize that using a pre-training objective that more
closely resembles the downstream task leads to better and
faster ﬁne-tuning performance. Given our intended use for
abstractive summarization, our proposed pre-training objective involves generating summary-like text from an input
document. In order to leverage massive text corpora for pretraining, we design a sequence-to-sequence self-supervised
objective in the absence of abstactive summaries. A naive
option would be to pre-train as an extractive summarizer;
however, such a procedure would only train a model to copy
sentences, thus not suitable for abstractive summarization.
Inspired by recent success in masking words and contiguous
spans , we select and
mask whole sentences from documents, and concatenate the
gap-sentences into a pseudo-summary. The corresponding
position of each selected gap sentence is replaced by a mask
token [MASK1] to inform the model. Gap sentences ratio,
or GSR, refers to the number of selected gap sentences to
the total number of sentences in the document, which is
similar to mask rate in other works.
To even more closely approximate a summary, we select
sentences that appear to be important/principal to the document. The resulting objective has both the empirically
demonstrated beneﬁts of masking, and anticipates the form
of the downstream task.
We consider 3 primary strategies for selecting m gap sentences without replacement from a document, D = {xi}n,
comprised of n sentences:
Uniformly select m sentences at random.
Select the ﬁrst m sentences.
Select top-m scored sentences according to importance. As a proxy for importance we compute ROUGE1-
F1 between the sentence and the rest of the
document, si = rouge(xi, D \ {xi}), ∀i.
In this formulation sentences are scored independently (Ind)
and the top m selected. We also consider selecting them
sequentially (Seq) as in Nallapati et al. by greedily
maximizing the ROUGE1-F1 between selected sentences,
S ∪{xi}, and remaining sentences, D \ (S ∪{xi}) as in
Algorithm 1.
Algorithm 1 Sequential Sentence Selection
2: for j ←1 to m do
si := rouge
S ∪{xi}, D \ (S ∪{xi})
∀i s.t. xi /∈S
k := arg maxi{si}n
S := S ∪{xk}
6: end for
When calculating ROUGE1-F1, we also consider n-grams
as a set (Uniq) instead of double-counting identical n-grams
as in the original implementation (Orig). This results in
four variants of the principal sentence selection strategy,
choosing Ind/Seq and Orig/Uniq options.
An example containing lead, random and principal gap sentence selection strategies are shown in Figure 2.
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
INVITATION ONLY We are very excited to be co-hosting
a major drinks reception with our friends at Progress. This
event will sell out, so make sure to register at the link
Speakers include Rajesh Agrawal, the London
Deputy Mayor for Business, Alison McGovern, the Chair of
Progress, and Seema Malhotra MP. Huge thanks to the our
friends at the ACCA, who have supported this event. The
Labour Business Fringe at this year’s Labour Annual Conference is being co-sponsored by Labour in the City and the
Industry Forum. Speakers include John McDonnell, Shadow
Chancellor, and Rebecca Long-Bailey, the Shadow Chief
Secretary to the Treasury, and our own Chair, Kitty Ussher.
Attendance is free, and refreshments will be provided.
Figure 2: An example of sentences (from the C4 corpus)
selected by Random, Lead and Ind-Orig respectively. Best
viewed in color.
Masked Language Model (MLM)
Following BERT, we select 15% tokens in the input text, and
the selected tokens are (1) 80% of time replaced by a mask
token [MASK2], or (2) 10% of time replaced by a random
token, or (3) 10% of time unchanged. We apply MLM to
train the Transformer encoder as the sole pre-training objective or along with GSG. When MLM is the sole pre-training
objective, the Transformer decoder shares all parameters
with encoder when ﬁne-tuning on downstream tasks following Rothe et al. .
Figure 1 simultaneously shows how both GSG and MLM
are applied to the same example when used in conjunction.
However, we found that MLM does not improve downstream tasks at large number of pre-training steps (section
6.1.2), and chose not to include MLM in the ﬁnal model
PEGASUSLARGE (section 6.2).
Pre-training Corpus
For pre-training we considered two large text corpora:
• C4, or the Colossal and Cleaned version of Common
Crawl, introduced in Raffel et al. ; consists of
text from 350M Web-pages (750GB).
• HugeNews, a dataset of 1.5B articles (3.8TB) collected from news and news-like websites from 2013-
A whitelist of domains ranging from highquality news publishers to lower-quality sites such as
high-school newspapers, and blogs was curated and
used to seed a web-crawler. Heuristics were used to
identify news-like articles, and only the main article
text was extracted as plain text.
Downstream Tasks/Datasets
For downstream summarization, we only used public abstractive summarization datasets, and access them through
TensorFlow Summarization Datasets 1, which provides
publicly reproducible code for dataset processing and
train/validation/test splits. We used train/validation/test ratio of 80/10/10 if no split was provided, and 10% train split
as validation if there was no validation split.
XSum consists of 227k BBC articles
from 2010 to 2017 covering a wide variety of subjects along
with professionally written single-sentence summaries.
CNN/DailyMail dataset contains
93k articles from the CNN, and 220k articles the Daily Mail
newspapers. Both publishers supplement their articles with
bullet point summaries. We use the non-anonymized variant
used in See et al. .
NEWSROOM is a large dataset containing 1.3M article-summary pairs written by authors and
editors in the newsrooms of 38 major publications between
1998 and 2017.
Multi-News is a multi-document summarization dataset consisting of 56k pairs of news articles and their human-written summaries from the site
newser.com.
Gigaword contains 4M examples extracted from news articles (seven publishers) from the Gigaword corpus . The task is to generate the
headline from the ﬁrst sentence.
arXiv, PubMed are two long document
datasets of scientiﬁc publications from arXiv.org (113k) and
PubMed (215k). The task is to generate the abstract from
the paper body.
BIGPATENT consists of 1.3 million
U.S. patents along with human summaries under nine patent
classiﬁcation categories.
WikiHow is a large-scale dataset
of instructions from the online WikiHow.com website. Each
of 200k examples consists of multiple instruction-step paragraphs along with a summarizing sentence. The task is
to generate the concatenated summary-sentences from the
paragraphs.
Reddit TIFU contains 120K posts of
informal stories from the online discussion forum Reddit,
more speciﬁcally the TIFU sub-reddit from 2013-Jan to
2018-Mar. The sub-reddit posts strictly follow the rule of
writing a descriptive ”TL;DR” summary and has higher qual-
1 
catalog/overview
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
ity than (which used more subreddits)
based on our manual inspection. We uses the TIFU-long
subset (using TLDR as summaries) in the work.
AESLC consists of 18k email
bodies and their subjects from the Enron corpus , a collection of email messages of employees
in the Enron Corporation.
BillSum contains 23k US
Congressional bills and human-written reference summaries
from the 103rd-115th sessions of Congress. We
do not use the California test set which is out-of-distribution.
Following Grusky et al., the number of examples and extractive fragment coverage/density for all downstream datasets
is illustrated in Appendix A.
Experiments
In a similar strategy to Raffel et al. , to save time
and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of
12 datasets before scaling up pre-training with the best settings to the ﬁnal 568M parameters, PEGASUSLARGE. The
datasets (XSum, CNN/DailyMail, WikiHow and Reddit
TIFU) were chosen for diversity in abstractiveness, writing
style, and size.
PEGASUSBASE had L = 12, H = 768, F = 3072, A = 12
and PEGASUSLARGE had L = 16, H
4096, A = 16, where L denotes the number of layers for encoder and decoder (i.e.
Transformer blocks),
H for the hidden size, F for the feed-forward layer
size and A for the number of self-attention heads. We
pre-trained PEGASUSBASE with a batch size of 256 and
PEGASUSLARGE with a batch size of 8192. We refer to
PEGASUSBASE without pre-training as TransformerBASE.
We used sinusoidal positional encoding following Vaswani
et al. . For optimization, both pre-training and ﬁnetuning used Adafactor with square
root learning rate decay and dropout rate of 0.1.
We used greedy-decoding for studies in Section 6.1, and
used beam-search with a length-penalty, α, as in Wu et al.
 for the ﬁnal large model.
All experiments’ hyper parameters can be found in Appendix C and reported numbers are in Appendix D and
Ablations on PEGASUSBASE
We used PEGASUSBASE to evaluate choices of pre-training
corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly
available C4 corpus.
Note that the y-axis in Figures 3, 4, 5 are normalized by the
left-most bar using 1
RLbase ) where R1,
R2, RL are ROUGE F1 scores and R1base, R2base, RLbase
are the scores of the conﬁguration corresponding to the ﬁrst
With more pre-training steps, the model observed more documents in the pre-training corpus. A PEGASUSBASE model
trained for 500k (highest we tried) steps did not observe all
training examples on C4 nor HugeNews. Appendix B shows
the number of pre-training steps had an unsurprisingly positive impact on downstream dataset performance. We used
500k steps for the ablation studies and the large model.
PRE-TRAINING CORPUS
CNN/DailyMail
Reddit TIFU
Figure 3: Effect of pre-training corpus. PEGASUSBASE
pre-trained on C4 (350M Web-pages) and HugeNews (1.5B
news-like documents).
Figure 3 shows that pre-training on HugeNews was more
effective than C4 on the two news downstream datasets,
while the non-news informal datasets (WikiHow and Reddit
TIFU) prefer the pre-training on C4. This suggests pretraining models transfer more effectively to downstream
tasks when their domains are aligned better.
EFFECT OF PRE-TRAINING OBJECTIVES
We compared six variants of GSG (Lead, Random,
Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing
30% sentences as gap sentences. As shown in Figure 4a, Ind-
Orig achieved the best performance followed by Seq-Uniq.
Ind-Orig and Seq-Uniq were consistently better (or similar)
than Random and Lead across the four downstream datasets.
Lead had decent performance on the two news datasets but
was signiﬁcantly worse on the two non-news datasets, which
agrees ﬁndings of lead bias in news datasets . The results suggest choosing principal
sentences works best for downstream summarization tasks,
and we chose Ind-Orig for the PEGASUSLARGE.
A signiﬁcant hyper-parameter in GSG is the gap-sentences
ratio (GSR). A low GSR makes the pre-training less challenging and computationally efﬁcient. On the other hand,
choosing gap sentences at a high GSR loses contextual in-
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
CNN/DailyMail
Reddit TIFU
MLM solely
MLM & Ind-Orig
(a) Effect of pre-training objectives (30% GSR).
CNN/DailyMail
Reddit TIFU
(b) Effect of gap sentences ratio with GSG (Ind-Orig).
Figure 4: Effect of pre-training settings with PEGASUSBASE pre-trained on C4.
formation necessary to guide the generation. We compared
GSRs from 15% to 75%. For a fair comparison, the original
documents were truncated to have up to 400 words. The
maximum input length, Linput in the encoder and the maximum target length, Ltarget in the decoder were set as 512
Figure 4b shows that different downstream datasets had
slightly different optima. The best performance always had
GSR lower than 50%. The model with 15% gap sentences
achieved the highest ROUGE scores on CNN/DailyMail,
while XSum/Reddit TIFU and WikiHow did better with 30%
and 45% respectively. When scaling up to PEGASUSLARGE
(Section 6.2), we chose an effective GSR of 30%.
As mentioned, the MLM objective can either be applied solely or together with GSG. We jointly trained MLM
with GSG Ind-Orig (MLM & Ind-Orig), which masks 30%
sentences and extra 15% tokens in unselected sentences, as
shown in Figure 1. Figure 4a shows that the model pretrained with MLM alone performed signiﬁcantly worse and
MLM & Ind-Orig had similar performance as Random. Interestingly, when comparing MLM & Ind-Orig to Ind-Orig,
we empirically observed MLM improved ﬁne-tuning performance at early pre-training checkpoints (100k - 200k
steps), but inhibited further gains with more pre-training
steps (500k). Therefore, we chose not to include MLM in
PEGASUSLARGE.
EFFECT OF VOCABULARY
We compared two tokenization methods2:
Byte-pairencoding algorithm (BPE) , and SentencePiece Unigram algorithm (Unigram)
proposed in Kudo . We evaluated Unigram with different vocabulary sizes ranging from 32k to 256k. In these
experiments, models were pre-trained for 500k steps on
the C4 corpus with the Ind-Orig objective and 15% GSR.
As shown in Figure 5, BPE and Unigram were comparable on news datasets while Unigram outperformed BPE
2Implemented in 
CNN/DailyMail
Reddit TIFU
Unigram 32k
Unigram 64k
Unigram 96k
Unigram 128k
Unigram 256k
Figure 5: Effect of vocabulary with PEGASUSBASE trained
on C4 (15% GSR, Ind-Orig).
on non-news datasets, especially WikiHow.
and CNN/DailyMail, Unigram 96k achieved the highest
ROUGE scores. On WikiHow and Reddit TIFU, the best
conﬁgurations were Unigram 128k and 64k respectively.
Therefore, we used the overall best vocabulary option Unigram 96k in PEGASUSLARGE.
Larger Model Results
PEGASUSBASE,
PEGASUSLARGE had increased capacity from larger
hidden size (H : 768 →1024, F : 3072 →4096,
A : 12 →16), number of layers (L : 12 →16) and
traversed much more data, due to larger batch size
(B : 256 →8192) (same number of pre-training steps,
We adopted the best practices found in the
PEGASUSBASE ablation studies using the GSG (Ind-Orig)
pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUSLARGE had 568M
parameters.
To encourage the model to copy, which is an important aspect of the more extractive datasets, we left 20% of selected
sentences unchanged in the input instead of replacing with
[MASK1]. We increased the GSR to 45% to achieve a similar number of “gaps” as the optimal 30% found above.
We reported the performance of the models pre-trained
on HugeNews and C4 separately. We conducted a simple
hyper-parameter sweep of learning rate and length penalty,
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
Table 1: Results of PEGASUSLARGE and PEGASUSBASE on all downstream datasets compared with the previous SOTA,
which are fetched from . We only compared
with previous abstractive models except on BillSum which had extractive results only. BIGPATENT, arXiv, PubMed and
Multi-News datasets contain very long summaries and we truncate them to 256 tokens, in similar range compared to . Best ROUGE numbers on each dataset and
numbers within 0.15 of the best numbers are bolded.
TransformerBASE
PEGASUSBASE
Previous SOTA
PEGASUSLARGE
PEGASUSLARGE
(HugeNews)
30.83/10.83/24.41
39.79/16.58/31.70
45.14/22.27/37.25
45.20/22.06/36.99
47.21/24.56/39.25
CNN/DailyMail
38.27/15.03/35.48
41.79/18.81/38.93
44.16/21.28/40.90
43.90/21.20/40.76
44.17/21.47/41.11
40.28/27.93/36.52
42.38/30.06/38.52
39.91/28.38/36.87
45.07/33.39/41.28
45.15/33.51/41.33
Multi-News
34.36/5.42/15.75
42.24/13.27/21.44
43.47/14.89/17.41
46.74/17.95/24.26
47.52/18.72/24.91
35.70/16.75/32.83
36.91/17.66/34.08
39.14/19.92/36.57
38.75/19.96/36.14
39.12/19.86/36.24
32.48/10.53/23.86
36.58/15.64/30.01
28.53/9.23/26.54
43.06/19.71/34.80
41.35/18.51/33.42
Reddit TIFU
15.89/1.94/12.22
24.36/6.09/18.75
19.0/3.7/15.1
26.54/8.94/21.64
26.63/9.01/21.60
42.98/20.51/31.87
43.55/20.43/31.80
37.52/10.63/22.79
53.63/33.16/42.25
53.41/32.89/42.07
35.63/7.95/20.00
34.81/10.16/22.50
41.59/14.26/23.55
44.70/17.27/25.80
44.67/17.18/25.73
33.94/7.43/19.02
39.98/15.15/25.23
40.59/15.59/23.59
45.49/19.90/27.69
45.09/19.56/27.42
15.04/7.39/14.93
34.85/18.94/34.10
23.67/10.29/23.44
37.69/21.85/36.84
37.40/21.22/36.45
44.05/21.30/30.98
51.42/29.68/37.78
40.80/23.83/33.73
57.20/39.56/45.80
57.31/40.19/45.82
Table 2: A comparison of PEGASUSLARGE with other pretrained models on XSum, CNN/DailyMail and Gigaword. Best
ROUGE numbers and numbers within 0.15 of the best numbers are bolded.
CNN/DailyMail
BERTShare 
38.52/16.12/31.13
39.25/18.09/36.45
38.13/19.81/35.62
MASS 
39.75/17.24/31.95
42.12/19.50/39.01
38.73/19.71/35.96
UniLM 
43.33/20.21/40.51
38.45/19.45/35.75
BART 
45.14/22.27/37.25
44.16/21.28/40.90
T5 
43.52/21.55/40.69
PEGASUSLARGE (C4)
45.20/22.06/36.99
43.90/21.20/40.76
38.75/19.96/36.14
PEGASUSLARGE (HugeNews)
47.21/24.56/39.25
44.17/21.47/41.11
39.12/19.86/36.24
α, when ﬁne-tuning PEGASUSLARGE on each downstream
CNN/DailyMail,
Multi-News,
PATENT datasets contain input documents longer than
the maximum input length (Linput = 512 tokens) in pretraining. This would present a problem for position embeddings which would never be updated for longer input
lengths, but we conﬁrm the postulation that sinusoidal positional encodings generalize well
when ﬁne-tuning PEGASUSLARGE beyond the input lengths
observed in training up to Linput = 1024 tokens. Since
average input length in BIGPATENT, arXiv, PubMed and
Multi-News are well beyond 1024 tokens, further scaling up
Linput or applying a two-stage approach 
may improve performance even more, although this is outside the scope of this work.
Tables 1 and 2 show the performance improvements
of PEGASUSBASE and PEGASUSLARGE on downstream
datasets. While PEGASUSBASE exceeded current state-ofthe-art on many datasets, PEGASUSLARGE achieved better
than state-of-the-art results on all downstream datasets using
HugeNews, although C4 performed better on WikiHow.
The improvement from a Transformer model without pretraining (TransformerBASE) to PEGASUSLARGE was more
signiﬁcant on smaller datasets. For example, the ROUGE2-
F1 scores nearly tripled on AESLC and quintupled on Reddit TIFU. The large jumps in performance suggest that
small text summarization datasets beneﬁt the most from
pre-training. We further investigate low resource summarization in Section 6.3.
Zero and Low-Resource Summarization
In real-world practice, it is often difﬁcult to collect a large
number of supervised examples to train or ﬁne-tune a summarization model. To simulate the low-resource summarization setting, we picked the ﬁrst 10k (k = 1, 2, 3, 4) training
examples from each dataset to ﬁne-tune PEGASUSLARGE
(HugeNews) . We ﬁne-tuned the models up to 2000 steps
with batch size 256, learning rate 0.0005, and picked
the checkpoint with best validation performance. In Figure. 6, in 8 out of 12 datasets, with just 100 examples
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
CNN/DailyMail
Multi-News
Reddit TIFU
Figure 6: Fine-tuning with limited supervised examples. The solid lines are PEGASUSLARGE ﬁne-tuned on 0 (zero shot), 10,
100, 1k,10k examples. The dashed lines are TransformerBASE models, equivalent in capacity as PEGASUSBASE and trained
using the full supervised datasets, but with no pre-training. All numbers are reported in Appendix E.
Table 3: Human evaluation side-by-side results on Likert
(1-5) scale (higher is better). Scores are bolded if they are
not worse than human-level performance by p < 0.01.
CNN/DailyMail
Reddit TIFU
mean (p-value)
mean (p-value)
mean (p-value)
Experiment 1: pretrain comparison
Human-written
PEGASUSLARGE (HugeNews)
3.6 (0.0001)
PEGASUSLARGE (C4)
3.5 (0.009)
TransformerBASE
2.0 (3e-10)
2.9 (0.06)
1.4 (5e-23)
Experiment 2: low resource
Human-written
PEGASUSLARGE (HugeNews) 10 examples
3.4 (0.007)
2.6 (0.006)
PEGASUSLARGE (HugeNews) 100 examples
3.4 (0.08)
2.1 (4e-8)
PEGASUSLARGE (HugeNews) 1000 examples
3.6 (0.07)
2.7 (0.01)
PEGASUSLARGE (HugeNews) full supervision
2.8 (0.05)
PEGASUSLARGE could be ﬁne-tuned to generate summaries
at comparable quality to TransformerBASE trained on the
full supervised datasets ranging from 20k to 200k examples.
PEGASUSLARGE also beat previous state-of-the-art results
on 6 out of 12 datasets with only 1000 ﬁne-tuning examples.
On CNN/DailyMail, with half the number of parameters PEGASUSLARGE demonstrated much better zero-shot
(ROUGE2-F=13.28) performance than GPT-2 with 3000 examples.
Qualitative Observations and Human Evaluation
Overall, we observed high-linguistic quality (in terms of ﬂuency and coherence), closely emulating the style of groundtruth summaries. While some previous work suggested that
maximum likelihood training results in repetitive text in
model outputs we found this to be
rare in our outputs and did not require additional countermeasures to mitigate dis-ﬂuencies.
Although ROUGE clearly has its draw-backs , over-penalizing abstractive approaches compared to extractive ones and having no sense of linguistic quality, we found that choosing perplexity-optimized
models using aggregated ROUGE ) resulted in
qualitatively good models. Randomly sampled (by a program) model decodes across all datasets and a broad range
of ROUGE scores can be found in Appendix I.We found
that even low-ROUGE model summaries often were highquality, Figure G.1.
To assess how close PEGASUSLARGE is to human performance we conducted human evaluation experiments on
Amazon Mechanical Turk comparing model summaries with
(human) reference summaries given the input document.
The examples were drawn from the XSum, CNN/DailyMail,
and Reddit TIFU datasets; the ﬁrst two were chosen due to
their popularity in past work, and the third was chosen for
its signiﬁcant difference in style. Workers were asked to rate
the summaries on a 1-5 scale, with higher being better (full
experiment details provided in Appendix F) and a paired
t-test was used to assess whether scores were signiﬁcantly
different from human.
In the ﬁrst experiment, PEGASUSLARGE (HugeNews),
PEGASUSLARGE (C4), and TransformerBASE were compared with reference summaries; in the second experiment,
PEGASUSLARGE (HugeNews) ﬁne-tuned using 10, 100,
1000, and all supervised examples were compared with
references; the results are shown in Table 3. According to
the signiﬁcance level of p < 0.01, both PEGASUSLARGE
(HugeNews) and PEGASUSLARGE (C4) outputs were at
least as good as the reference summaries in all cases. Even
at low-levels of supervision PEGASUSLARGE (HugeNews)
was not measurably worse than human summaries on XSum
and CNN/DailyMail. In the Reddit TIFU case, however, perhaps due to its diverse writing styles, human performance
required full supervision.
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization
Test-set Overlap with Pre-training Corpus
The pre-training corpora are a large collection of documents
from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of
overlap between the pre-training corpus and downstream
datasets. We also studied if the pre-trained model was able
to exploit memorization to achieve higher performance on
the downstream datasets.
To measure the overlap, we calculated similarities between
all pairs of downstream test set targets and pre-training
documents. We use the ROUGE-2 recall as a similarity
measure (common 2-grams / test set targets 2-grams). It is
not necessarily exact match even if the similarity score is 1.0.
We ﬁltered all test set examples that have similarity to any
pre-training example above a threshold, and recalculated the
ROUGE scores on the remaining test set. In Figure 7, we
conducted this study on the pre-training corpus C4 and test
set of XSum, CNN/Dailymail, Reddit TIFU and WikiHow,
with a similarity threshold of 1.0 and 0.8. Results show
that only XSum has signiﬁcant amount of overlap 15% to
20%, and ﬁltering those examples does not change ROUGE
scores more than 1%. We also manually examined those
overlapped examples with similarity of 1.0, and found that
the models produce very different summaries compared to
the human written ones, suggesting that there was no clear
memorization.
post-filter remaining data
average change in ROUGE-1/2/L
CNN/DailyMail
CNN/DailyMail
Reddit TIFU
Reddit TIFU
filter similarity=1.0
filter similarity>0.8
Figure 7: Percentage of overlap between C4 and downstream test sets, and ROUGE score changes after removing
those overlapped examples in test sets.
Additional PEGASUSLARGE Improvements
Following our experiments on PEGASUSLARGE pre-trained
on C4 and HugeNews, we pre-trained a PEGASUSLARGE
model on both corpora and stochastically sampled important
sentences. The PEGASUSLARGE (mixed,stochastic) model
includes the changes: (1) The model was pre-trained on the
mixture of C4 and HugeNews weighted by their number
of examples. (2) The model dynamically chose gap sen-
Results (ROUGE-1/ROUGE-2/ROUGE-L F
scores) of PEGASUSLARGE (mixed, stochastic) on downstream datasets. ‡ We updated the BIGPATENT dataset to
preserve casing, some format cleanings are also changed.
CNN/DailyMail
47.60/24.83/39.64
44.16/21.56/41.30
45.98/34.20/42.18
Multi-News
47.65/18.75/24.95
39.65/20.47/36.76
46.39/22.12/38.41
Reddit TIFU
27.99/9.81/22.94
52.29/33.08/41.66 ‡
44.21/16.95/25.67
45.97/20.15/28.25
37.68/21.25/36.51
59.67/41.58/47.59
tences ratio uniformly between 15%-45%. (3) Importance
sentences were stochastically sampled with 20% uniform
noise on their scores. (4) The model was pre-trained for
1.5M steps instead of 500k steps, as we observed slower convergence of pre-training perplexity. (5) The SentencePiece
tokenizer was updated to encode the newline character. The
PEGASUSLARGE (mixed, stochastic) model achieved best
results on almost all downstream tasks, as shown in Table 4.
Conclusion
In this work, we proposed PEGASUS, a sequence-tosequence model with gap-sentences generation as a pretraining objective tailored for abstractive text summarization. We studied several gap-sentence selection methods
and identiﬁed principle sentence selection as the optimal
strategy. We demonstrated the effects of the pre-training
corpora, gap-sentences ratios, vocabulary sizes and scaled
up the best conﬁguration to achieve state-of-the-art results
on all 12 diverse downstream datasets considered. We also
showed that our model was able to adapt to unseen summarization datasets very quickly, achieving strong results in as
little as 1000 examples. We ﬁnally showed our model summaries achieved human performance on multiple datasets
using human evaluation.
Code and Model Checkpoints Release
The training code and instructions for using model checkpoints can be found at
 
Acknowledgments
We thank Anastassia Kornilova, Eva Sharma, Shashi
Narayan, Adam Roberts, Etienne Pot, and the Google News
team for assistance with datasets, and Carey Radebaugh,
David Grangier, Doug Eck, and Samy Bengio for reviewing
the manuscript.
PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization