Published as a conference paper at ICLR 2020
BERTSCORE: EVALUATING TEXT GENERATION WITH
Tianyi Zhang∗†‡⋄, Varsha Kishore∗‡, Felix Wu∗‡, Kilian Q. Weinberger†‡⋄, and Yoav Artzi‡§
‡Department of Computer Science and §Cornell Tech, Cornell University
{vk352, fw245, kilian}@cornell.edu
{yoav}@cs.cornell.edu
⋄ASAPP Inc.
 
We propose BERTSCORE, an automatic evaluation metric for text generation.
Analogously to common metrics, BERTSCORE computes a similarity score for
each token in the candidate sentence with each token in the reference sentence.
However, instead of exact matches, we compute token similarity using contextual
embeddings. We evaluate using the outputs of 363 machine translation and image
captioning systems. BERTSCORE correlates better with human judgments and
provides stronger model selection performance than existing metrics. Finally, we
use an adversarial paraphrase detection task to show that BERTSCORE is more
robust to challenging examples when compared to existing metrics.
INTRODUCTION
Automatic evaluation of natural language generation, for example in machine translation and caption
generation, requires comparing candidate sentences to annotated references. The goal is to evaluate
semantic equivalence. However, commonly used methods rely on surface-form similarity only. For
example, BLEU , the most common machine translation metric, simply counts
n-gram overlap between the candidate and the reference. While this provides a simple and general
measure, it fails to account for meaning-preserving lexical and compositional diversity.
In this paper, we introduce BERTSCORE, a language generation evaluation metric based on pretrained BERT contextual embeddings . BERTSCORE computes the similarity
of two sentences as a sum of cosine similarities between their tokens’ embeddings.
BERTSCORE addresses two common pitfalls in n-gram-based metrics .
First, such methods often fail to robustly match paraphrases. For example, given the reference people like foreign cars, BLEU and METEOR incorrectly give a higher score
to people like visiting places abroad compared to consumers prefer imported cars. This leads to
performance underestimation when semantically-correct phrases are penalized because they differ
from the surface form of the reference. In contrast to string matching (e.g., in BLEU) or matching
heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which
have been shown to be effective for paraphrase detection . Second, n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes . For example, given a small window of size two, BLEU will only mildly penalize
swapping of cause and effect clauses (e.g. A because B instead of B because A), especially when
the arguments A and B are long phrases. In contrast, contextualized embeddings are trained to
effectively capture distant dependencies and ordering.
We experiment with BERTSCORE on machine translation and image captioning tasks using the
outputs of 363 systems by correlating BERTSCORE and related metrics to available human judgments. Our experiments demonstrate that BERTSCORE correlates highly with human evaluations.
In machine translation, BERTSCORE shows stronger system-level and segment-level correlations
with human judgments than existing metrics on multiple common benchmarks and demonstrates
∗Equal contribution. † Work done at Cornell.
 
Published as a conference paper at ICLR 2020
strong model selection performance compared to BLEU.
We also show that BERTSCORE is
well-correlated with human annotators for image captioning, surpassing SPICE, a popular taskspeciﬁc metric .
Finally, we test the robustness of BERTSCORE on
the adversarial paraphrase dataset PAWS , and show that it is more robust to adversarial examples than other metrics.
The code for BERTSCORE is available at
 
PROBLEM STATEMENT AND PRIOR METRICS
Natural language text generation is commonly evaluated using annotated reference sentences. Given
a reference sentence x tokenized to k tokens ⟨x1, . . . , xk⟩and a candidate ˆx tokenized to l tokens
⟨ˆx1, . . . , ˆxl⟩, a generation evaluation metric is a function f(x, ˆx) ∈R. Better metrics have a higher
correlation with human judgments. Existing metrics can be broadly categorized into using n-gram
matching, edit distance, embedding matching, or learned functions.
n-GRAM MATCHING APPROACHES
The most commonly used metrics for generation count the number of n-grams that occur in the
reference x and candidate ˆx. The higher the n is, the more the metric is able to capture word order,
but it also becomes more restrictive and constrained to the exact form of the reference.
Formally, let Sn
ˆx be the lists of token n-grams (n ∈Z+) in the reference x and candidate
ˆx sentences. The number of matched n-grams is P
ˆx I[w ∈Sn
x], where I[·] is an indicator
function. The exact match precision (Exact-Pn) and recall (Exact-Rn) scores are:
Exact-Pn =
ˆx I[w ∈Sn
Exact-Rn =
Several popular metrics build upon one or both of these exact matching scores.
The most widely used metric in machine translation is BLEU , which
includes three modiﬁcations to Exact-Pn. First, each n-gram in the reference can be matched at
most once. Second, the number of exact matches is accumulated for all reference-candidate pairs in
the corpus and divided by the total number of n-grams in all candidate sentences. Finally, very short
candidates are discouraged using a brevity penalty. Typically, BLEU is computed for multiple values
of n (e.g. n = 1, 2, 3, 4) and the scores are averaged geometrically. A smoothed variant, SENT-
BLEU is computed at the sentence level. In contrast to BLEU, BERTSCORE is
not restricted to maximum n-gram length, but instead relies on contextualized embeddings that are
able to capture dependencies of potentially unbounded length.
METEOR computes Exact-P1 and Exact-R1 while allowing
backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases. For
example, running may match run if no exact match is possible. Non-exact matching uses an external
stemmer, a synonym lexicon, and a paraphrase table. METEOR 1.5 
weighs content and function words differently, and also applies importance weighting to different
matching types. The more recent METEOR++ 2.0 further incorporates a learned
external paraphrase resource. Because METEOR requires external resources, only ﬁve languages
are supported with the full feature set, and eleven are partially supported. Similar to METEOR,
BERTSCORE allows relaxed matches, but relies on BERT embeddings that are trained on large
amounts of raw text and are currently available for 104 languages. BERTSCORE also supports
importance weighting, which we estimate with simple corpus statistics.
Other Related Metrics
NIST is a revised version of BLEU that weighs each
n-gram differently and uses an alternative brevity penalty. ∆BLEU modiﬁes
multi-reference BLEU by including human annotated negative reference sentences. CHRF compares character n-grams in the reference and candidate sentences. CHRF++ extends CHRF to include word bigram matching. ROUGE is a commonly used
metric for summarization evaluation. ROUGE-n computes Exact-Rn (usually n = 1, 2),
while ROUGE-L is a variant of Exact-R1 with the numerator replaced by the length of the longest
common subsequence. CIDER is an image captioning metric that computes
Published as a conference paper at ICLR 2020
cosine similarity between tf–idf weighted n-grams. We adopt a similar approach to weigh tokens
differently. Finally, Chaganty et al. and Hashimoto et al. combine automatic metrics
with human judgments for text generation evaluation.
EDIT-DISTANCE-BASED METRICS
Several methods use word edit distance or word error rate , which quantify
similarity using the number of edit operations required to get from the candidate to the reference. TER normalizes edit distance by the number of reference words, and
ITER adds stem matching and better normalization. PER computes position independent error rate, CDER models block reordering as an edit operation. CHARACTER and EED operate
on the character level and achieve higher correlation with human judgements on some languages.
EMBEDDING-BASED METRICS
Word embeddings are learned dense token representations. MEANT 2.0 
uses word embeddings and shallow semantic parses to compute lexical and structural similarity.
YISI-1 is similar to MEANT 2.0, but makes the use of semantic parses optional.
Both methods use a relatively simple similarity computation, which inspires our approach, including
using greedy matching and experimenting with a similar importance
weighting to YISI-1. However, we use contextual embeddings, which capture the speciﬁc use of
a token in a sentence, and potentially capture sequence information. We do not use external tools
to generate linguistic structures, which makes our approach relatively simple and portable to new
languages. Instead of greedy matching, WMD , WMDO , and
SMS propose to use optimal matching based on earth mover’s distance . The tradeoff1 between greedy and optimal matching was studied by Rus & Lintean
 . Sharma et al. compute similarity with sentence-level representations. In contrast, our
token-level computation allows us to weigh tokens differently according to their importance.
LEARNED METRICS
Various metrics are trained to optimize correlation with human judgments. BEER uses a regression model based on character n-grams and word bigrams. BLEND uses regression to combine 29 existing metrics. RUSE combines three pre-trained sentence embedding models. All these methods require costly human judgments as supervision for each dataset, and risk poor generalization to new domains, even within a
known language and task . Cui et al. and Lowe et al. train a
neural model to predict if the input text is human-generated. This approach also has the risk of being
optimized to existing data and generalizing poorly to new data. In contrast, the model underlying
BERTSCORE is not optimized for any speciﬁc evaluation task.
Given a reference sentence x = ⟨x1, . . . , xk⟩and a candidate sentence ˆx = ⟨ˆx1, . . . , ˆxl⟩, we use
contextual embeddings to represent the tokens, and compute matching using cosine similarity, optionally weighted with inverse document frequency scores. Figure 1 illustrates the computation.
Token Representation
We use contextual embeddings to represent the tokens in the input sentences x and ˆx. In contrast to prior word embeddings , contextual embeddings, such as BERT and ELMO ,
can generate different vector representations for the same word in different sentences depending on
the surrounding words, which form the context of the target word. The models used to generate
these embeddings are most commonly trained using various language modeling objectives, such as
masked word prediction .
1We provide an ablation study of this design choice in Appendix C.
Published as a conference paper at ICLR 2020
the weather is
cold today
it is freezing today
Contextual
Pairwise Cosine
Similarity
RBERT = (0.713⇥1.27)+(0.515⇥7.94)+...
1.27+7.94+1.82+7.90+8.88
Importance Weighting
(Optional)
Maximum Similarity
Figure 1: Illustration of the computation of the recall metric RBERT. Given the reference x and
candidate ˆx, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy
matching in red, and include the optional idf importance weighting.
We experiment with different models (Section 4), using the tokenizer provided with each model.
Given a tokenized reference sentence x = ⟨x1, . . . , xk⟩, the embedding model generates a sequence of vectors ⟨x1, . . . , xk⟩. Similarly, the tokenized candidate ˆx = ⟨ˆx1, . . . , ˆxm⟩is mapped
to ⟨ˆx1, . . . , ˆxl⟩. The main model we use is BERT, which tokenizes the input text into a sequence
of word pieces , where unknown words are split into several commonly observed
sequences of characters. The representation for each word piece is computed with a Transformer
encoder by repeatedly applying self-attention and nonlinear transformations
in an alternating fashion. BERT embeddings have been shown to beneﬁt various NLP tasks .
Similarity Measure
The vector representation allows for a soft measure of similarity instead of
exact-string or heuristic matching. The cosine
similarity of a reference token xi and a candidate token ˆxj is
∥xi∥∥ˆxj∥. We use pre-normalized
vectors, which reduces this calculation to the inner product x⊤
i ˆxj. While this measure considers
tokens in isolation, the contextual embeddings contain information from the rest of the sentence.
The complete score matches each token in x to a token in ˆx to compute recall,
and each token in ˆx to a token in x to compute precision. We use greedy matching to maximize
the matching similarity score,2 where each token is matched to the most similar token in the other
sentence. We combine precision and recall to compute an F1 measure. For a reference x and
candidate ˆx, the recall, precision, and F1 scores are:
FBERT = 2 PBERT · RBERT
PBERT + RBERT
Importance Weighting
Previous work on similarity measures demonstrated that rare words can
be more indicative for sentence similarity than common words . BERTSCORE enables us to easily incorporate importance weighting. We experiment
with inverse document frequency (idf) scores computed from the test corpus. Given M reference
sentences {x(i)}M
i=1, the idf score of a word-piece token w is
idf(w) = −log 1
I[w ∈x(i)] ,
where I[·] is an indicator function. We do not use the full tf-idf measure because we process single
sentences, where the term frequency (tf) is likely 1. For example, recall with idf weighting is
xi∈x idf(xi) maxˆxj∈ˆx x⊤
xi∈x idf(xi)
Because we use reference sentences to compute idf, the idf scores remain the same for all systems
evaluated on a speciﬁc test set. We apply plus-one smoothing to handle unknown word pieces.
2We compare greedy matching with optimal assignment in Appendix C.
Published as a conference paper at ICLR 2020
Baseline Rescaling
Because we use pre-normalized vectors, our computed scores have the same
numerical range of cosine similarity (between −1 and 1). However, in practice we observe scores
in a more limited range, potentially because of the learned geometry of contextual embeddings.
While this characteristic does not impact BERTSCORE’s capability to rank text generation systems,
it makes the actual score less readable. We address this by rescaling BERTSCORE with respect
to its empirical lower bound b as a baseline. We compute b using Common Crawl monolingual
datasets.3 For each language and contextual embedding model, we create 1M candidate-reference
pairs by grouping two random sentences. Because of the random pairing and the corpus diversity,
each pair has very low lexical and semantic overlapping.4 We compute b by averaging BERTSCORE
computed on these sentence pairs. Equipped with baseline b, we rescale BERTSCORE linearly. For
example, the rescaled value ˆRBERT of RBERT is:
ˆRBERT = RBERT −b
After this operation ˆRBERT is typically between 0 and 1. We apply the same rescaling procedure
for PBERT and FBERT. This method does not affect the ranking ability and human correlation of
BERTSCORE, and is intended solely to increase the score readability.
EXPERIMENTAL SETUP
We evaluate our approach on machine translation and image captioning.
Contextual Embedding Models
We evaluate twelve pre-trained contextual embedding models,
including variants of BERT , RoBERTa , XLNet , and XLM . We present the best-performing models in Section 5.
We use the 24-layer RoBERTalarge model5 for English tasks, 12-layer BERTchinese model for Chinese tasks, and the 12-layer cased multilingual BERTmulti model for other languages.6 We show the
performance of all other models in Appendix F. Contextual embedding models generate embedding
representations at every layer in the encoder network. Past work has shown that intermediate layers
produce more effective representations for semantic tasks . We use the WMT16
dataset as a validation set to select the best layer of each model (Appendix B).
Machine Translation
Our main evaluation corpus is the WMT18 metric evaluation dataset , which contains predictions of 149 translation systems across 14 language pairs, gold
references, and two types of human judgment scores. Segment-level human judgments assign a score
to each reference-candidate pair. System-level human judgments associate each system with a single
score based on all pairs in the test set. WMT18 includes translations from English to Czech, German,
Estonian, Finnish, Russian, and Turkish, and from the same set of languages to English. We follow
the WMT18 standard practice and use absolute Pearson correlation |ρ| and Kendall rank correlation
τ to evaluate metric quality, and compute signiﬁcance with the Williams test for |ρ|
and bootstrap re-sampling for τ as suggested by Graham & Baldwin . We compute systemlevel scores by averaging BERTSCORE for every reference-candidate pair. We also experiment with
hybrid systems by randomly sampling one candidate sentence from one of the available systems for
each reference sentence . This enables system-level experiments with a higher
number of systems. Human judgments of each hybrid system are created by averaging the WMT18
segment-level human judgments for the corresponding sentences in the sampled data. We compare
BERTSCOREs to one canonical metric for each category introduced in Section 2, and include the
comparison with all other participating metrics from WMT18 in Appendix F.
In addition to the standard evaluation, we design model selection experiments. We use 10K hybrid
systems super-sampled from WMT18. We randomly select 100 out of 10K hybrid systems, and rank
them using the automatic metrics. We repeat this process 100K times. We report the percentage of
the metric ranking agreeing with the human ranking on the best system (Hits@1). In Tables 23-28,
3 
4BLEU computed on these pairs is around zero.
5We use the tokenizer provided with each model. For all Hugging Face models that use the GPT-2 tokenizer,
at the time of our experiments, the tokenizer adds a space to the beginning of each sentence.
6All the models used are from 
Published as a conference paper at ICLR 2020
FBERT (idf)
Table 1: Absolute Pearson correlations with system-level human judgments on WMT18. For each
language pair, the left number is the to-English correlation, and the right is the from-English. We
bold correlations of metrics not signiﬁcantly outperformed by any other metric under Williams Test
for that language pair and direction. The numbers in parenthesis are the number of systems used for
each language pair and direction.
FBERT (idf)
Table 2: Absolute Pearson correlations with system-level human judgments on WMT18. We use
10K hybrid super-sampled systems for each language pair and direction. For each language pair, the
left number is the to-English correlation, and the right is the from-English. Bolding criteria is the
same as in Table 1.
we include two additional measures to the model selection study: (a) the mean reciprocal rank of the
top metric-rated system according to the human ranking, and (b) the difference between the human
score of the top human-rated system and that of the top metric-rated system.
Additionally, we report the same study on the WMT17 and the WMT16 datasests in Appendix F.7 This adds 202 systems to our evaluation.
Image Captioning
We use the human judgments of twelve submission entries from the COCO
2015 Captioning Challenge. Each participating system generates a caption for each image in the
COCO validation set , and each image has approximately ﬁve reference captions.
Following Cui et al. , we compute the Pearson correlation with two system-level
metrics: the percentage of captions that are evaluated as better or equal to human captions (M1)
and the percentage of captions that are indistinguishable from human captions (M2). We compute
BERTSCORE with multiple references by scoring the candidate with each available reference and
returning the highest score. We compare with eight task-agnostic metrics: BLEU , METEOR , ROUGE-L , CIDER ,
BEER , EED , CHRF++ , and
CHARACTER . We also compare with two task-speciﬁc metrics: SPICE and LEIC . SPICE is computed using the similarity of scene graphs
parsed from the reference and candidate captions. LEIC is trained to predict if a caption is written
by a human given the image.
7For WMT16, we only conduct segment-level experiments on to-English pairs due to errors in the dataset.
Published as a conference paper at ICLR 2020
FBERT (idf)
Table 3: Model selection accuracies (Hits@1) on WMT18 hybrid systems. We report the average of
100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the highest numbers for
each language pair and direction.
(78k/ 20k)
-.029/.236
FBERT (idf)
Table 4: Kendall correlations with segment-level human judgments on WMT18. For each language
pair, the left number is the to-English correlation, and the right is the from-English. We bold correlations of metrics not signiﬁcantly outperformed by any other metric under bootstrap sampling for
that language pair and direction. The numbers in parenthesis are the number of candidate-reference
sentence pairs for each language pair and direction.
Machine Translation
Tables 1–3 show system-level correlation to human judgements, correlations on hybrid systems, and model selection performance. We observe that BERTSCORE is consistently a top performer. In to-English results, RUSE shows competitive
performance. However, RUSE is a supervised method trained on WMT16 and WMT15 human
judgment data. In cases where RUSE models were not made available, such as for our from-English
experiments, it is not possible to use RUSE without additional data and training. Table 4 shows
segment-level correlations. We see that BERTSCORE exhibits signiﬁcantly higher performance
compared to the other metrics. The large improvement over BLEU stands out, making BERTSCORE
particularly suitable to analyze speciﬁc examples, where SENTBLEU is less reliable. In Appendix A,
we provide qualitative examples to illustrate the segment-level performance difference between
SENTBLEU and BERTSCORE. At the segment-level, BERTSCORE even signiﬁcantly outperforms
RUSE. Overall, we ﬁnd that applying importance weighting using idf at times provides small bene-
ﬁt, but in other cases does not help. Understanding better when such importance weighting is likely
to help is an important direction for future work, and likely depends on the domain of the text and
the available test data. We continue without idf weighting for the rest of our experiments. While
recall RBERT, precision PBERT, and F1 FBERT alternate as the best measure in different setting, F1
FBERT performs reliably well across all the different settings. Our overall recommendation is therefore to use F1. We present additional results using the full set of 351 systems and evaluation metrics
in Tables 12–28 in the appendix, including for experiments with idf importance weighting, different
contextual embedding models, and model selection.
Image Captioning
Table 5 shows correlation results for the COCO Captioning Challenge.
BERTSCORE outperforms all task-agnostic baselines by large margins. Image captioning presents a
challenging evaluation scenario, and metrics based on strict n-gram matching, including BLEU and
ROUGE, show weak correlations with human judgments. idf importance weighting shows signiﬁ-
Published as a conference paper at ICLR 2020
RBERT (idf)
Pearson correlation on the
2015 COCO Captioning Challenge.
The M1 and M2 measures are described
in Section 4. LEIC uses images as additional inputs. Numbers with ∗are cited
from Cui et al. .
We bold the
highest correlations of task-speciﬁc and
task-agnostic metrics.
Trained on QQP
(supervised)
Trained on QQP
(supervised)
(Not trained
FBERT (idf)
Area under ROC curve (AUC) on QQP
and PAWSQQP datasets.
The scores of trained DecATT , DIIN ,
and ﬁne-tuned BERT are reported by Zhang et al.
 . Numbers with ∗are scores on the held-out test
set of QQP. We bold the highest correlations of taskspeciﬁc and task-agnostic metrics.
cant beneﬁt for this task, suggesting people attribute higher importance to content words. Finally,
LEIC , a trained metric that takes images as additional inputs and is optimized
speciﬁcally for the COCO data and this set of systems, outperforms all other methods.
Despite the use of a large pre-trained model, computing BERTSCORE is relatively fast. We
are able to process 192.5 candidate-reference pairs/second using a GTX-1080Ti GPU. The complete
WMT18 en-de test set, which includes 2,998 sentences, takes 15.6sec to process, compared to 5.4sec
with SacreBLEU , a common BLEU implementation. Given the sizes of commonly used
test and validation sets, the increase in processing time is relatively marginal, and BERTSCORE is
a good ﬁt for using during validation (e.g., for stopping) and testing, especially when compared to
the time costs of other development stages.
ROBUSTNESS ANALYSIS
We test the robustness of BERTSCORE using adversarial paraphrase classiﬁcation. We use the
Quora Question Pair corpus and the adversarial paraphrases from the Paraphrase Adversaries from Word Scrambling dataset . Both datasets contain pairs of sentences labeled to indicate whether they are paraphrases or not. Positive examples
in QQP are real duplicate questions, while negative examples are related, but different questions.
Sentence pairs in PAWS are generated through word swapping. For example, in PAWS, Flights from
New York to Florida may be changed to Flights from Florida to New York and a good classiﬁer
should identify that these two sentences are not paraphrases. PAWS includes two parts: PAWSQQP,
which is based on the QQP data, and PAWSWiki. We use the PAWSQQP development set which
contains 667 sentences. For the automatic metrics, we use no paraphrase detection training data.
We expect that pairs with higher scores are more likely to be paraphrases. To evaluate the automatic
metrics on QQA, we use the ﬁrst 5,000 sentences in the training set instead of the the test set because
the test labels are not available. We treat the ﬁrst sentence as the reference and the second sentence
as the candidate.
Table 6 reports the area under ROC curve (AUC) for existing models and automatic metrics. We
observe that supervised classiﬁers trained on QQP perform worse than random guess on PAWSQQP,
which shows these models predict the adversarial examples are more likely to be paraphrases. When
Published as a conference paper at ICLR 2020
adversarial examples are provided in training, state-of-the-art models like DIIN 
and ﬁne-tuned BERT are able to identify the adversarial examples but their performance still decreases signiﬁcantly from their performance on QQP. Most metrics have decent performance on
QQP, but show a signiﬁcant performance drop on PAWSQQP, almost down to chance performance.
This suggests these metrics fail to to distinguish the harder adversarial examples. In contrast, the
performance of BERTSCORE drops only slightly, showing more robustness than the other metrics.
DISCUSSION
We propose BERTSCORE, a new metric for evaluating generated text against gold standard references. BERTSCORE is purposely designed to be simple, task agnostic, and easy to use. Our analysis
illustrates how BERTSCORE resolves some of the limitations of commonly used metrics, especially
on challenging adversarial examples. We conduct extensive experiments with various conﬁguration
choices for BERTSCORE, including the contextual embedding model used and the use of importance weighting. Overall, our extensive experiments, including the ones in the appendix, show that
BERTSCORE achieves better correlation than common metrics, and is effective for model selection. However, there is no one conﬁguration of BERTSCORE that clearly outperforms all others.
While the differences between the top conﬁgurations are often small, it is important for the user to
be aware of the different trade-offs, and consider the domain and languages when selecting the exact
conﬁguration to use. In general, for machine translation evaluation, we suggest using FBERT, which
we ﬁnd the most reliable. For evaluating text generation in English, we recommend using the 24layer RoBERTalarge model to compute BERTSCORE. For non-English language, the multilingual
BERTmulti is a suitable choice although BERTSCORE computed with this model has less stable
performance on low-resource languages. We report the optimal hyperparameter for all models we
experimented with in Appendix B
Brieﬂy following our initial preprint publication, Zhao et al. published a concurrently developed method related to ours, but with a focus on integrating contextual word embeddings with earth
mover’s distance rather than our simple matching process. They also
propose various improvements compared to our use of contextualized embeddings. We study these
improvements in Appendix C and show that integrating them into BERTSCORE makes it equivalent
or better than the EMD-based approach. Largely though, the effect of the different improvements
on BERTSCORE is more modest compared to their method. Shortly after our initial publication,
YiSi-1 was updated to use BERT embeddings, showing improved performance . This
further corroborates our ﬁndings. Other recent related work includes training a model on top of
BERT to maximize the correlation with human judgments and evaluating generation with a BERT model ﬁne-tuned on paraphrasing . More recent work
shows the potential of using BERTSCORE for training a summarization system 
and for domain-speciﬁc evaluation using SciBERT to evaluate abstractive text
summarization .
In future work, we look forward to designing new task-speciﬁc metrics that use BERTSCORE as a
subroutine and accommodate task-speciﬁc needs, similar to how Wieting et al. suggests to use
semantic similarity for machine translation training. Because BERTSCORE is fully differentiable,
it also can be incorporated into a training procedure to compute a learning loss that reduces the
mismatch between optimization and evaluation objectives.
ACKNOWLEDGEMENT
This research is supported in part by grants from the National Science Foundation (III-1618134, III-
1526012, IIS1149882, IIS-1724282, TRIPODS-1740822, CAREER-1750499), the Ofﬁce of Naval
Research DOD (N00014-17-1-2175), and the Bill and Melinda Gates Foundation, SAP, Zillow,
Workday, and Facebook Research. We thank Graham Neubig and David Grangier for for their
insightful comments. We thank the Cornell NLP community including but not limited to Claire
Cardie, Tianze Shi, Alexandra Schoﬁeld, Gregory Yauney, and Rishi Bommasani. We thank Yin
Cui and Guandao Yang for their help with the COCO 2015 dataset.
Published as a conference paper at ICLR 2020