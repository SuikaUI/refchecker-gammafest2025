Similarity Estimation Techniques from Rounding
Algorithms
Moses S. Charikar
Dept. of Computer Science
Princeton University
35 Olden Street
Princeton, NJ 08544
 
A locality sensitive hashing scheme is a distribution on a
family F of hash functions operating on a collection of objects, such that for two objects x, y,
Prh∈F[h(x) = h(y)] = sim(x, y),
where sim(x, y) ∈ is some similarity function deﬁned
on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects
can be estimated from their compact sketches, and also
leads to eﬃcient algorithms for approximate nearest neighbor search and clustering.
Min-wise independent permutations provide an elegant construction of such a locality
sensitive hashing scheme for a collection of subsets with the
set similarity measure sim(A, B) = |A∩B|
We show that rounding algorithms for LPs and SDPs used
in the context of approximation algorithms can be viewed
as locality sensitive hashing schemes for several interesting
collections of objects. Based on this insight, we construct
new locality sensitive hashing schemes for:
1. A collection of vectors with the distance between ⃗u
and ⃗v measured by θ(⃗u,⃗v)/π, where θ(⃗u,⃗v) is the angle between ⃗u and ⃗v. This yields a sketching scheme
for estimating the cosine similarity measure between
two vectors, as well as a simple alternative to minwise
independent permutations for estimating set similarity.
2. A collection of distributions on n points in a metric
space, with distance between distributions measured
by the Earth Mover Distance (EMD), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space
such that, for distributions P and Q,
Eh∈F[d(h(P), h(Q))]
≤O(log n log log n) · EMD(P, Q).
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
STOC’02, May 19-21, 2002, Montreal, Quebec, Canada.
Copyright 2002 ACM 1-58113-495-9/02/0005 ...$5.00.
INTRODUCTION
The current information explosion has resulted in an increasing number of applications that need to deal with large
volumes of data. While traditional algorithm analysis assumes that the data ﬁts in main memory, it is unreasonable
to make such assumptions when dealing with massive data
sets such as data from phone calls collected by phone companies, multimedia data, web page repositories and so on.
This new setting has resulted in an increased interest in
algorithms that process the input data in restricted ways,
including sampling a few data points, making only a few
passes over the data, and constructing a succinct sketch of
the input which can then be eﬃciently processed.
There has been a lot of recent work on streaming algorithms, i.e.
algorithms that produce an output by making one pass (or a few passes) over the data while using a
limited amount of storage space and time.
To cite a few
examples, Alon et al considered the problem of estimating frequency moments and Guha et al considered the
problem of clustering points in a streaming fashion. Many
of these streaming algorithms need to represent important
aspects of the data they have seen so far in a small amount of
space; in other words they maintain a compact sketch of the
data that encapsulates the relevant properties of the data
set. Indeed, some of these techniques lead to sketching algorithms – algorithms that produce a compact sketch of a data
set so that various measurements on the original data set
can be estimated by eﬃcient computations on the compact
sketches. Building on the ideas of , Alon et al give algorithms for estimating join sizes. Gibbons and Matias 
give sketching algorithms producing so called synopsis data
structures for various problems including maintaining approximate histograms, hot lists and so on. Gilbert et al 
give algorithms to compute sketches for data streams so as
to estimate any linear projection of the data and use this to
get individual point and range estimates. Recently, Gilbert
et al gave eﬃcient algorithms for the dynamic maintenance of histograms. Their algorithm processes a stream of
updates and maintains a small sketch of the data from which
the optimal histogram representation can be approximated
very quickly.
In this work, we focus on sketching algorithms for estimating similarity, i.e. the construction of functions that produce
succinct sketches of objects in a collection, such that the
similarity of objects can be estimated eﬃciently from their
sketches. Here, similarity sim(x, y) is a function that maps
pairs of objects x, y to a number in , measuring the
degree of similarity between x and y. sim(x, y) = 1 corresponds to objects x, y that are identical while sim(x, y) = 0
corresponds to objects that are very diﬀerent.
Broder et al introduced the notion of min-wise
independent permutations, a technique for constructing such
sketching functions for a collection of sets. The similarity
measure considered there was
sim(A, B) = |A ∩B|
We note that this is exactly the Jaccard coeﬃcient of similarity used in information retrieval.
The min-wise independent permutation scheme allows the
construction of a distribution on hash functions h : 2U →U
Prh∈F[h(A) = h(B)] = sim(A, B).
Here F denotes the family of hash functions (with an associated probability distribution) operating on subsets of the
universe U. By choosing say t hash functions h1, . . . ht from
this family, a set S could be represented by the hash vector
(h1(S), . . . ht(S)). Now, the similarity between two sets can
be estimated by counting the number of matching coordinates in their corresponding hash vectors.1
The work of Broder et al was originally motivated by the
application of eliminating near-duplicate documents in the
Altavista index. Representing documents as sets of features
with similarity between sets determined as above, the hashing technique provided a simple method for estimating similarity of documents, thus allowing the original documents
to be discarded and reducing the input size signiﬁcantly.
In fact, the minwise independent permutations hashing
scheme is a particular instance of a locality sensitive hashing
scheme introduced by Indyk and Motwani in their work
on nearest neighbor search in high dimensions.
Definition 1. A locality sensitive hashing scheme is a
distribution on a family F of hash functions operating on a
collection of objects, such that for two objects x, y,
Prh∈F[h(x) = h(y)] = sim(x, y)
Here sim(x, y) is some similarity function deﬁned on the
collection of objects.
Given a hash function family F that satisﬁes (1), we will
say that F is a locality sensitive hash function family corresponding to similarity function sim(x, y). Indyk and Motwani showed that such a hashing scheme facilitates the construction of eﬃcient data structures for answering approximate nearest-neighbor queries on the collection of objects.
In particular, using the hashing scheme given by minwise
independent permutations results in eﬃcient data structures
for set similarity queries and leads to eﬃcient clustering algorithms. This was exploited later in several experimental
papers: Cohen et al for association-rule mining, Haveliwala et al for clustering web documents, Chen et al 
for selectivity estimation of boolean queries, Chen et al 
for twig queries, and Gionis et al for indexing set value
1One question left open in was the issue of compact representation of hash functions in this family; this was settled
by Indyk , who gave a construction of a small family of
minwise independent permutations.
attributes. All of this work used the hashing technique for
set similarity together with ideas from .
We note that the deﬁnition of locality sensitive hashing
used by is slightly diﬀerent, although in the same spirit
as our deﬁnition. Their deﬁnition involves parameters r1 >
r2 and p1 > p2. A family F is said to be (r1, r2, p1, p2)sensitive for a similarity measure sim(x, y) if Prh∈F[h(x) =
h(y)] ≥p1 when sim(x, y) ≥r1 and Prh∈F[h(x) = h(y)] ≤
p2 when sim(x, y) ≤r2. Despite the diﬀerence in the precise deﬁnition, we chose to retain the name locality sensitive
hashing in this work since the two notions are essentially the
same. Hash functions with closely related properties were
investigated earlier by Linial and Sasson and Indyk et
Our Results
In this paper, we explore constructions of locality sensitive hash functions for various other interesting similarity
functions.
The utility of such hash function schemes (for
nearest neighbor queries and clustering) crucially depends
on the fact that the similarity estimation is based on a test
of equality of the hash function values. We make an interesting connection between constructions of similarity preserving hash-functions and rounding procedures used in the design of approximation algorithms. We show that procedures
used for rounding fractional solutions from linear programs
and vector solutions to semideﬁnite programs can be used
to derive similarity preserving hash functions for interesting
classes of similarity functions.
In Section 2, we prove some necessary conditions on similarity measures sim(x, y) for the existence of locality sensitive hash functions satisfying (1). Using this, we show that
such locality sensitive hash functions do not exist for certain
commonly used similarity measures in information retrieval,
the Dice coeﬃcient and the Overlap coeﬃcient.
In seminal work, Goemans and Williamson introduced semideﬁnite programming relaxations as a tool for
approximation algorithms.
They used the random hyperplane rounding technique to round vector solutions for the
MAX-CUT problem. We will see in Section 3 that the random hyperplane technique naturally gives a family of hash
functions F for vectors such that
Prh∈F[h(⃗u) = h(⃗v)] = 1 −θ(⃗u,⃗v)
Here θ(⃗u,⃗v) refers to the angle between vectors ⃗u and ⃗v.
Note that the function 1 −θ
π is closely related to the function cos(θ). (In fact it is always within a factor 0.878 from it.
Moreover, cos(θ) can be estimated from an estimate of θ.)
Thus this similarity function is very closely related to the
cosine similarity measure, commonly used in information
retrieval.
(In fact, Indyk and Motwani describe how
the set similarity measure can be adapted to measure dot
product between binary vectors in d-dimensional Hamming
space. Their approach breaks up the data set into O(log d)
groups, each consisting of approximately the same weight.
Our approach, based on estimating the angle between vectors is more direct and is also more general since it applies
to general vectors.) We also note that the cosine between
vectors can be estimated from known techniques based on
random projections . However, the advantage of a
locality sensitive hashing based scheme is that this directly
yields techniques for nearest neighbor search for the cosine
similarity measure.
An attractive feature of the hash functions obtained from
the random hyperplane method is that the output is a single
bit; thus the output of t hash functions can be concatenated
very easily to produce a t-bit vector.2 Estimating similarity
between vectors amounts to measuring the Hamming distance between the corresponding t-bit hash vectors. We can
represent sets by their characteristic vectors and use this
locality sensitive hashing scheme for measuring similarity
between sets. This yields a slightly diﬀerent similarity measure for sets, one that is linearly proportional to the angle
between their characteristic vectors.
In Section 4, we present a locality sensitive hashing scheme
for a certain metric on distributions on points, called the
Earth Mover Distance. We are given a set of points L =
{l1, . . . ln}, with a distance function d(i, j) deﬁned on them.
A probability distribution P(X) (or distribution for short) is
a set of weights p1, . . . pn on the points such that pi ≥0 and
  pi = 1. (We will often refer to distribution P(X) as simply P, implicitly referring to an underlying set X of points.)
The Earth Mover Distance EMD(P, Q) between two distributions P and Q is deﬁned to be the cost of the min
cost matching that transforms one distribution to another.
(Imagine each distribution as placing a certain amount of
earth on each point. EMD(P, Q) measures the minimum
amount of work that must be done in transforming one distribution to the other.) This is a popular metric for images
and is used for image similarity, navigating image databases
and so on .
is to represent an image as a distribution on features with
an underlying distance metric on features (e.g. colors in a
color spectrum). Since the earth mover distance is expensive
to compute (requiring a solution to a minimum transportation problem), applications typically use an approximation
of the earth mover distance. (e.g. representing distributions
by their centroids).
We construct a hash function family for estimating the
earth mover distance. Our family is based on rounding algorithms for LP relaxations for the problem of classiﬁcation
with pairwise relationships studied by Kleinberg and Tardos , and further studied by Calinescu et al
Chekuri et al
 . Combining a new LP formulation described by Chekuri et al together with a rounding technique
of Kleinberg and Tardos, we show a construction of a hash
function family which approximates the earth mover distance to a factor of O(log n log log n). Each hash function in
this family maps a distribution on points L = {l1, . . . , ln}
to some point li in the set. For two distributions P(X) and
Q(X) on the set of points, our family of hash functions F
satisﬁes the property that:
Eh∈F[d(h(P), h(Q))]
≤O(log n log log n) · EMD(P, Q).
We also show an interesting fact about a rounding algorithm in Kleinberg and Tardos applying to the case
where the underlying metric on points is a uniform metric. In this case, we show that their rounding algorithm can
2In Section 2, we will show that we can convert any locality
sensitive hashing scheme to one that maps objects to {0, 1}
with a slight change in similarity measure.
However, the
modiﬁed hash functions convey less information, e.g.
collision probability for the modiﬁed hash function family is
at least 1/2 even for a pair of objects with original similarity
be viewed as a generalization of min-wise independent permutations extended to a continuous setting. Their rounding
procedure yields a locality sensitive hash function for vectors
whose coordinates are all non-negative. Given two vectors
⃗a = (a1, . . . an) and ⃗b = (b1, . . . bn), the similarity function
sim(⃗a,⃗b) =
 i min(ai, bi)
 i max(ai, bi).
(Note that when ⃗a and ⃗b are the characteristic vectors for
sets A and B, this expression reduces to the set similarity
measure for min-wise independent permutations.)
Applications of locality sensitive hash functions to solving
nearest neighbor queries typically reduce the problem to the
Hamming space. Indyk and Motwani give a data structure that solves the approximate nearest neighbor problem
on the Hamming space. Their construction is a reduction to
the so called PLEB (Point Location in Equal Balls) problem,
followed by a hashing technique concatenating the values of
several locality sensitive hash functions. We give a simple
technique that achieves the same performance as the Indyk
Motwani result in Section 5. The basic idea is as follows:
Given bit vectors consisting of d bits each, we choose a number of random permutations of the bits. For each random
permutation σ, we maintain a sorted order of the bit vectors,
in lexicographic order of the bits permuted by σ. To ﬁnd a
nearest neighbor for a query bit vector q we do the following: For each permutation σ, we perform a binary search on
the sorted order corresponding to σ to locate the bit vectors
closest to q (in the lexicographic order obtained by bits permuted by σ). Further, we search in each of the sorted orders
proceeding upwards and downwards from the location of q,
according to a certain rule. Of all the bit vectors examined,
we return the one that has the smallest Hamming distance
to the query vector. The performance bounds we can prove
for this simple scheme are identical to that proved by Indyk
and Motwani for their scheme.
EXISTENCEOF LOCALITY SENSITIVE
HASH FUNCTIONS
In this section, we discuss certain necessary properties for
the existence of locality sensitive hash function families for
given similarity measures.
Lemma 1. For any similarity function sim(x, y) that admits a locality sensitive hash function family as deﬁned in
(1), the distance function 1 −sim(x, y) satisﬁes triangle inequality.
Proof. Suppose there exists a locality sensitive hash function family such that
Prh∈F[h(x) = h(y)] = sim(x, y).
1 −sim(x, y) = Prh∈F[h(x) ̸= h(y)].
Let ∆h(x, y) be an indicator variable for the event h(x) ̸=
h(y). We claim that ∆h(x, y) satisﬁes the triangle inequality,
∆h(x, y) + ∆h(y, z) ≥∆h(x, z).
Since ∆h() takes values in the set {0, 1}, the only case
when the above inequality could be violated would be when
∆h(x, y) = ∆h(y, z) = 0. But in this case h(x) = h(y) and
h(y) = h(z). Thus, h(x) = h(z) implying that ∆h(x, z) = 0
and the inequality is satisﬁed. This proves the claim. Now,
1 −sim(x, y) = Eh∈F[∆h(x, y)]
Since ∆h(x, y) satisﬁes the triangle inequality, Eh∈F[∆h(x, y)]
must also satisfy the triangle inequality.
This proves the
This gives a very simple proof of the fact that for the
set similarity measure sim(A, B) =
|A∪B|, 1 −sim(A, B)
satisﬁes the triangle inequality. This follows from Lemma 1
and the fact that a set similarity measure admits a locality
sensitive hash function family, namely that given by minwise
independent permutations.
One could ask the question whether locality sensitive hash
functions satisfying the deﬁnition (1) exist for other commonly used set similarity measures in information retrieval.
For example, Dice’s coeﬃcient is deﬁned as
simDice(A, B) =
2(|A| + |B|)
The Overlap coeﬃcient is deﬁned as
simOvl(A, B) =
min(|A|, |B|)
We can use Lemma 1 to show that there is no such locality sensitive hash function family for Dice’s coeﬃcient and
the Overlap measure by showing that the corresponding distance function does not satisfy triangle inequality.
Consider the sets A = {a}, B = {b}, C = {a, b}. Then,
simDice(A, C) = 2
simDice(C, B) = 2
simDice(A, B)
1 −simDice(A, C)
1 −simDice(C, B)
1 −simDice(A, B)
Similarly, the values for the Overlap measure are as follows:
simOvl(A, C) = 1,
simOvl(C, B) = 1,
simOvl(A, B) = 0
1 −simOvl(A, C) + 1 −simOvl(C, B) < 1 −simOvl(A, B)
This shows that there is no locality sensitive hash function
family corresponding to Dice’s coeﬃcient and the Overlap
It is often convenient to have a hash function family that
maps objects to {0, 1}. In that case, the output of t diﬀerent
hash functions can simply be concatenated to obtain a t-bit
hash value for an object. In fact, we can always obtain such
a binary hash function family with a slight change in the
similarity measure. A similar result was used and proved by
Gionis et al . We include a proof for completeness.
Lemma 2. Given a locality sensitive hash function family
F corresponding to a similarity function sim(x, y), we can
obtain a locality sensitive hash function family F′ that maps
objects to {0, 1} and corresponds to the similarity function
1+sim(x,y)
Proof. Suppose we have a hash function family such
Prh∈F[h(x) = h(y)] = sim(x, y).
Let B be a pairwise independent family of hash functions
that operate on the domain of the functions in F and map elements in the domain to {0, 1}. Then Prb∈B[b(u) = b(v)] =
1/2 if u ̸= v and Prb∈B[b(u) = b(v)] = 1 if u = v. Consider
the hash function family obtained by composing a hash function from F with one from B. This maps objects to {0, 1}
and we claim that it has the required properties.
Prh∈F,b∈B[b(h(x)) = b(h(y))] = 1 + sim(x, y)
With probability sim(x, y), h(x) = h(y) and hence b(h(x) =
b(h(y)). With probability 1 −sim(x, y), h(x) ̸= h(y) and in
this case, Prb∈B[b(h(x) = b(h(y))] = 1
Pr[b(h(x)) = b(h(y))]
sim(x, y) + (1 −sim(x, y))/2
(1 + sim(x, y))/2.
This can be used to show a stronger condition for the
existence of a locality sensitive hash function family.
Lemma 3. For any similarity function sim(x, y) that admits a locality sensitive hash function family as deﬁned in
(1), the distance function 1 −sim(x, y) is isometrically embeddable in the Hamming cube.
Proof. Firstly, we apply Lemma 2 to construct a binary
locality sensitive hash function family corresponding to similarity function sim′(x, y) = (1 + sim(x, y))/2. Note that
such a binary hash function family gives an embedding of
objects into the Hamming cube (obtained by concatenating
the values of all the hash functions in the family). For object x, let v(x) be the element in the Hamming cube x is
mapped to. 1 −sim′(x, y) is simply the fraction of bits that
do not agree in v(x) and v(y), which is proportional to the
Hamming distance between v(x) and v(y). Thus this embedding is an isometric embedding of the distance function
1 −sim′(x, y) in the Hamming cube. But
1 −sim′(x, y) = 1 −(1 + sim(x, y))/2 = (1 −sim(x, y))/2.
This implies that 1 −sim(x, y) can be isometrically embedded in the Hamming cube.
We note that Lemma 3 has a weak converse, i.e. for a
similarity measure sim(x, y) any isometric embedding of the
distance function 1−sim(x, y) in the Hamming cube yields a
locality sensitive hash function family corresponding to the
similarity measure (α + sim(x, y))/(α + 1) for some α > 0.
RANDOM HYPERPLANE BASED HASH
FUNCTIONS FOR VECTORS
Given a collection of vectors in Rd, we consider the family
of hash functions deﬁned as follows: We choose a random
vector ⃗r from the d-dimensional Gaussian distribution (i.e.
each coordinate is drawn the 1-dimensional Gaussian distribution). Corresponding to this vector ⃗r, we deﬁne a hash
function h⃗r as follows:
if ⃗r · ⃗u ≥0
if ⃗r · ⃗u < 0
Then for vectors ⃗u and ⃗v,
Pr[h⃗r(⃗u) = h⃗r(⃗v)] = 1 −θ(⃗u,⃗v)
This was used by Goemans and Williamson in their
rounding scheme for the semideﬁnite programming relaxation of MAX-CUT.
Picking a random hyperplane amounts to choosing a normally distributed random variable for each dimension. Thus
even representing a hash function in this family could require
a large number of random bits. However, for n vectors, the
hash functions can be chosen by picking O(log2 n) random
bits, i.e. we can restrict the random hyperplanes to be in
a family of size 2O(log2 n). This follows from the techniques
in Indyk and Engebretsen et al , which in turn use
Nisan’s pseudorandom number generator for space bounded
computations . We omit the details since they are similar
to those in .
Using this random hyperplane based hash function, we obtain a hash function family for set similarity, for a slightly
diﬀerent measure of similarity of sets. Suppose sets are represented by their characteristic vectors. Then, applying the
above scheme gives a locality sensitive hashing scheme where
Pr[h(A) = h(B)]
Also, this hash function family facilitates easy incorporation
of element weights in the similarity calculation, since the
values of the coordinates of the characteristic vectors could
be real valued element weights. Later, in Section 4.1 we will
present another technique to deﬁne and estimate similarity
of weighted sets.
THE EARTH MOVER DISTANCE
Consider a set of points L = {l1, . . . ln} with a distance
function d(i, j) (assumed to be a metric).
A distribution
P(L) on L is a collection of non-negative weights (p1, . . . pn)
for points in X such that
  pi = 1. The distance between
two distributions P(L) and Q(L) is deﬁned to be the optimal
cost of the following minimum transportation problem:
fi,j · d(i, j)
Note that we deﬁne a somewhat restricted form of the
Earth Mover Distance. The general deﬁnition does not assume that the sum of the weights is identical for distributions P(L) and Q(L). This is useful for example in matching
a small image to a portion of a larger image.
We will construct a hash function family for estimating
the Earth Mover Distance based on rounding algorithms for
the problem of classiﬁcation with pairwise relationships, introduced by Kleinberg and Tardos . (A closely related
problem was also studied by Broder et al ). In designing
hash functions to estimate the Earth Mover Distance, we
will relax the deﬁnition of locality sensitive hashing (1) in
three ways.
1. Firstly, the quantity we are trying to estimate is a
distance measure, not a similarity measure in .
2. Secondly, we will allow the hash functions to map objects to points in a metric space and measure
E[d(h(x), h(y))].
(A locality sensitive hash function
for a similarity measure sim(x, y) can be viewed as
a scheme to estimate the distance 1 −sim(x, y) by
Prh∈F[h(x) ̸= h(y)]. This is equivalent to having a
uniform metric on the hash values).
3. Thirdly, our estimator for the Earth Mover Distance
will not be an unbiased estimator, i.e. our estimate
will approximate the Earth Mover Distance to within
a small factor.
We now describe the problem of classiﬁcation with pairwise relationships. Given a collection of objects V and labels
L = {l1, . . . , ln}, the goal is to assign labels to objects. The
cost of assigning label l to object u ∈V is c(u, l). Certain
pairs of objects (u, v) are related; such pairs form the edges
of a graph over V . Each edge e = (u, v) is associated with a
non-negative weight we. For edge e = (u, v), if u is assigned
label h(u) and v is assigned label h(v), then the cost paid is
wed(h(u), h(v)).
The problem is to come up with an assignment of labels
h : V →L, so as to minimize the cost of the labeling h given
c(v, h(v)) +
wed(h(u), h(v))
The approximation algorithms for this problem use an LP
to assign, for every u ∈V , a probability distribution over
labels in L (i.e. a set of non-negative weights that sum up
to 1). Given a distribution P over labels in L, the rounding algorithm of Kleinberg and Tardos gave a randomized
procedure for assigning label h(P) to P with the following
properties:
1. Given distribution P(L) = (p1, . . . pn),
Pr[h(P) = li] = pi.
2. Suppose P and Q are probability distributions over L.
E[d(h(P), h(Q))] ≤O(log n log log n) EMD(P, Q) (7)
We note that the second property (7) is not immediately
obvious from , since they do not describe LP relaxations
for general metrics.
Their LP relaxations are deﬁned for
Hierarchically well Separated Trees (HSTs). They convert
a general metric to such an HST using Bartal’s results on probabilistic approximation of metric spaces via tree
metrics. However, it follows from combining ideas in 
with those in Chekuri et al
 . Chekuri et al do in fact
give an LP relaxation for general metrics. The LP relaxation
does indeed produce distributions over labels for every object u ∈V . The fractional distance between two labelings is
expressed as the min cost transshipment between P and Q,
which is identical to the Earth Mover Distance EMD(P, Q).
Now, this fractional solution can be used in the rounding algorithm developed by Kleinberg and Tardos to obtain the
second property (7) claimed above. In fact, Chekuri et al
use this fact to claim that the gap of their LP relaxation is
at most O(log n log log n) (Theorem 5.1 in ).
We elaborate some more on why the property (7) holds.
Kleinberg and Tardos ﬁrst (probabilistically) approximate
the metric on L by an HST using .
This is a tree
with all vertices in the original metric at the leaves. The
pairwise distance between any two vertices does no decrease
and all pairwise distances are increased by a factor of at
most O(log n log log n) (in expectation). For this tree metric, they use an LP formulation which can be described as
follows. Suppose we have a rooted tree. For subtree T, let
ℓT denote the length of the edge that T hangs oﬀof, i.e. the
ﬁrst edge on the path from T to the root. Further, for distribution P on the vertices of the original metric, let P(T)
denote the total probability mass that P assigns to leaves in
T; Q(T) is similarly deﬁned. The distance between distributions P and Q is measured by
 T ℓT |P(T) −Q(T)|, where
the summation is computed over all subtrees T. The Kleinberg Tardos rounding scheme ensures that E[d(h(P), h(Q))]
is within a constant factor of
 T ℓT |P(T) −Q(T)|.
Suppose instead, we measured the distance between distributions by EMD(P, Q), deﬁned on the original metric. By
probabilistically approximating the original metric by a tree
metric T ′, the expected value of the distance EMDT ′(P, Q)
(on the tree metric T ′) is at most a factor of O(log n log log n)
times EMD(P, Q). This follows since all distances increase
by O(log n log log n) in expectation. Now note that the tree
distance measure used by Kleinberg and Tardos
 T ℓT |P(T)−
Q(T)| is a lower bound on (and in fact exactly equal to)
EMDT ′(P, Q). To see that this is a lower bound, note that
in the min cost transportation between P and Q on T ′,
the ﬂow on the edge leading upwards from subtree T must
be at least |P(T) −Q(T)|. Since the rounding scheme ensures that E[d(h(P), h(Q))] is within a constant factor of
 T ℓT |P(T) −Q(T)|, we have that
E[d(h(P), h(Q))]
O(1) EMDT ′(P, Q)
O(log n log log n) EMD(P, Q)
where the expectation is over the random choice of the HST
and the random choices made by the rounding procedure.
Theorem 1. The Kleinberg Tardos rounding scheme yields
a locality sensitive hashing scheme such that
E[d(h(P), h(Q))]
≤O(log n log log n) EMD(P, Q).
Proof. The upper bound on E[d(h(P), h(Q))] follows directly from the second property (7) of the rounding scheme
stated above.
We show that the lower bound follows from the ﬁrst property (6). Let yi,j be the joint probability that h(P) = li
and h(Q) = lj. Note that
 j yi,j = pi, since this is simply the probability that h(P) = li. Similarly
 i yi,j = qj,
since this is simply the probability that h(Q) = lj. Now, if
h(P) = li and h(Q) = lj, then d(h(P)h(Q)) = d(i, j). Hence
E[d(f(P), f(Q))] =
 i,j yi,j · d(i, j). Let us write down the
expected cost and the constraints on yi,j.
E[d(h(P), h(Q))]
yi,j · d(i, j)
Comparing this with the LP for EMD(P, Q), we see that
the values of fi,j = yi,j is a feasible solution to the LP (2) to
(5) and E[d(h(P), h(Q))] is exactly the value of this solution.
Since EMD(P, Q) is the minimum value of a feasible solution, it follows that EMD(P, Q) ≤E[d(h(P), h(Q))].
Calinescu et al
 study a variant of the classiﬁcation
problem with pairwise relationships called the 0-extension
problem. This is the version without assignment costs where
some objects are assigned labels apriori and this labeling
must be extended to the other objects (a generalization of
multiway cut). For this problem, they design a rounding
scheme to get a O(log n) approximation. Again, their technique does not explicitly use an LP that gives probability
distributions on labels. However in hindsight, their rounding scheme can be interpreted as a randomized procedure
for assigning labels to distributions such that
E[d(h(P), h(Q))] ≤O(log n) EMD(P, Q).
Thus their rounding scheme gives a tighter guarantee than
(7). However, they do not ensure (6). Thus the previous
proof showing that EMD(P, Q) ≤E[d(h(P), h(Q))] does
not apply.
In fact one can construct examples such that
EMD(P, Q) > 0, yet E[d(h(P), h(Q))] = 0.
Hence, the
resulting hash function family provides an upper bound on
EMD(P, Q) within a factor O(log n) but does not provide
a good lower bound.
We mention that the hashing scheme described provides
an approximation to the Earth Mover Distance where the
quality of the approximation is exactly the factor by which
the underlying metric can be probabilistically approximated
In particular, if the underlying metric itself is
an HST, this yields an estimate within a constant factor.
This could have applications in compactly representing distributions over hierarchical classes. For example, documents
can be assigned a probability distribution over classes in
the Open Directory Project (ODP) hierarchy.
This hierarchy could be thought of as an HST and documents can
be mapped to distributions over this HST. The distance between two distributions can be measured by the Earth Mover
Distance. In this case, the hashing scheme described gives a
way to estimate this distance measure to within a constant
Weighted Sets
We show that the Kleinberg Tardos rounding scheme
for the case of the uniform metric actually is an extension
of min-wise independent permutations to the weighted case.
First we recall the hashing scheme given by min-wise independent permutations.
Given a universe U, consider a
random permutation π of U. Assume that the elements of
U are totally ordered.
Given a subset A ⊆U, the hash
function hπ is deﬁned as follows:
hπ(A) = min{π(A)}
Then the property satisﬁed by this hash function family is
Prπ[hπ(A) = hπ(B)] = |A ∩B|
We now review the Kleinberg Tardos rounding scheme for
the uniform metric: Firstly, imagine that we pick an inﬁnite
sequence {(it, αt)}∞
t=1 where for each t, it is picked uniformly
and at random in {1, . . . n} and αt is picked uniformly and
at random in . Given a distribution P = (p1, . . . , pn),
the assignment of labels is done in phases. In the ith phase,
we check whether αi ≤pit. If this is the case and P has not
been assigned a label yet, it is assigned label it.
Now, we can think of these distributions as sets in R2 (see
Figure 1).
Figure 1: Viewing a distribution as a continuous set.
The set S(P) corresponding to distribution P consists of
the union of the rectangles [i−1, i]×[0, pi]. The elements of
the universe are [i −1, i] × α. [i −1, i] × α belongs to S(P)
iﬀα ≤pi. The notion of cardinality of union and intersection of sets is replaced by the area of the intersection and
union of two such sets in R2. Note that the Kleinberg Tardos rounding scheme can be interpreted as constructing a
permutation of the universe and assigning to a distribution
P, the value i such that (i, α) is the minimum in the permutation amongst all elements contained in S(P). Suppose
instead, we assign to P, the element (i, α) which is the minimum in the permutation of S(P). Let h be a hash function
derived from this scheme (a slight modiﬁcation of the one in
 ). Then,
Pr[h(P) = h(Q)] = |S(P) ∩S(Q)|
|S(P) ∪S(Q)| =
 i min(pi, qi)
 i max(pi, qi)
For the Kleinberg Tardos rounding scheme, the probability of collision is at least the probability of collision for the
modiﬁed scheme (since two objects hashed to (i, α1) and
(i, α2) respectively in the modiﬁed scheme would be both
mapped to i in the original scheme). Hence
PrKT [h(P) = h(Q)]
 i min(pi, qi)
 i max(pi, qi)
PrKT [h(P) ̸= h(Q)]
 i min(pi, qi)
 i max(pi, qi)
 i |pi −qi|
 i max(pi, qi) ≤
The last inequality follows from the fact that
1 in the Kleinberg Tardos setting.
This was exactly the
property used in to obtain a 2-approximation for the
uniform metric case.
Note that the hashing scheme given by (8) is a generalization of min-wise independent permutations to the weighted
setting where elements in sets are associated with weights
∈ . Min-wise independent permutations are a special
case of this scheme when the weights are {0, 1}. This scheme
could be useful in a setting where a weighted set similarity notion is desired.
We note that the original min-wise
independent permutations can be used in the setting of integer weights by simply duplicating elements according to
their weight. The present scheme would work for any nonnegative real weights.
APPROXIMATE NEAREST NEIGHBOR
SEARCH IN HAMMING SPACE.
Applications of locality sensitive hash functions to solving
nearest neighbor queries typically reduce the problem to the
Hamming space. Indyk and Motwani give a data structure that solves the approximate nearest neighbor problem
on the Hamming space Hd. Their construction is a reduction to the so called PLEB (Point Location in Equal Balls)
problem, followed by a hashing technique concatenating the
values of several locality sensitive hash functions.
( ). For any ϵ > 0, there exists an algorithm for ϵ-PLEB in Hd using O(dn + n1+1/(1+ϵ)) space
and O(n1/(1+ϵ)) hash function evaluations for each query.
We give a simple technique that achieves the same performance as the Indyk Motwani result:
Given bit vectors consisting of d bits each, we choose
N = O(n1/(1+ϵ)) random permutations of the bits. For each
random permutation σ, we maintain a sorted order Oσ of
the bit vectors, in lexicographic order of the bits permuted
by σ. Given a query bit vector q, we ﬁnd the approximate
nearest neighbor by doing the following: For each permutation σ, we perform a binary search on Oσ to locate the
two bit vectors closest to q (in the lexicographic order obtained by bits permuted by σ). We now search in each of
the sorted orders Oσ examining elements above and below
the position returned by the binary search in order of the
length of the longest preﬁx that matches q. This can be done
by maintaining two pointers for each sorted order Oσ (one
moves up and the other down). At each step we move one of
the pointers up or down corresponding to the element with
the longest matching preﬁx. (Here the length of the longest
matching preﬁx in Oσ is computed relative to q with its bits
permuted by σ). We examine 2N = O(n1/(1+ϵ)) bit vectors in this way. Of all the bit vectors examined, we return
the one that has the smallest Hamming distance to q. The
performance bounds we can prove for this simple scheme
are identical to that proved by Indyk and Motwani for their
scheme. An advantage of this scheme is that we do not need
a reduction to many instances of PLEB for diﬀerent values
of radius r, i.e. we solve the nearest neighbor problem simultaneously for all values of radius r using a single data
structure.
We outline the main ideas of the analysis. In fact, the
proof follows along similar lines to the proofs of Theorem
5 and Corollary 3 in . Suppose the nearest neighbor of
q is at a Hamming distance of r from q. Set p1 = 1 −r
p2 = 1 −r(1+ϵ)
and k = log1/p2 n. Let ρ = ln 1/p1
ln 1/p2 . Then
nρ = O(n1/(1+ϵ)). We can show that with constant probability, from amongst N = O(n1/(1+ϵ)) permutations, there
exists a permutation such that the nearest neighbor agrees
with p on the ﬁrst k coordinates in σ. Further, over all L
permutations, the number of bit vectors that are at Hamming distance of more than r(1+ϵ) from q and agree on the
ﬁrst k coordinates is at most 2N with constant probability.
This implies that for this permutation σ, one of the 2L bit
vectors near q in the ordering Oσ and examined by the algorithm will be a (1 + ϵ)-approximate nearest neighbor. The
probability calculations are similar to those in , and we
only sketch the main ideas.
For any point q′ at distance at least r(1 + ϵ) from q, the
probability that a random coordinate agrees with q is at
most p2. Thus the probability that the ﬁrst k coordinates
agree is at most pk
For the N permutations, the
expected number of such points that agree in the ﬁrst k
coordinates is at most N. The probability that this number is ≤2N is > 1/2. Further, for a random permutation
σ, the probability that the nearest neighbor agrees in k coordinates is pk
Hence the probability that there
exists one permutation amongst the N = nρ permutations
where the nearest neighbor agrees in k coordinates is at least
1 −(1 −n−ρ)nρ > 1/2. This establishes the correctness of
the procedure.
As we stated earlier, a nice property of this data structure
is that it automatically adjusts to the correct distance r
to the nearest neighbor, i.e.
we do not need to maintain
separate data structures for diﬀerent values of r.
CONCLUSIONS
We have demonstrated an interesting relationship between
rounding algorithms used for rounding fractional solutions
of LPs and vector solutions of SDPs on the one hand, and
the construction of locality sensitive hash functions for interesting classes of objects, on the other.
Rounding algorithms yield new constructions of locality
sensitive hash functions that were not known previously.
Conversely (at least in hindsight), locality sensitive hash
functions lead to rounding algorithms (as in the case of minwise independent permutations and the uniform metric case
in Kleinberg and Tardos ).
An interesting direction to pursue would be to investigate
the construction of sketching functions that allow one to estimate information theoretic measures of distance between
distributions such as the KL-divergence, commonly used in
statistical learning theory. Since the KL-divergence is neither symmetric nor satisﬁes triangle inequality, new ideas
would be required in order to design a sketch function to
approximate it. Such a sketch function, if one exists, would
be a very valuable tool in compactly representing complex
distributions.