Diffusion Models: A Comprehensive Survey of Methods and Applications
LING YANG, Peking University, China
ZHILONG ZHANG∗, Peking University, China
YANG SONG, OpenAI, USA
SHENDA HONG, Peking University, China
RUNSHENG XU, University of California, Los Angeles, USA
YUE ZHAO, Carnegie Mellon University, USA
WENTAO ZHANG, Peking University, China
BIN CUI, Peking University, China
MING-HSUAN YANG†, University of California at Merced, USA
Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many
applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly
expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood
estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative
models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer
vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This
survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing
to potential areas for further exploration. Github: 
CCS Concepts: • Computing methodologies →Computer vision tasks; Natural language generation; Machine learning approaches.
Additional Key Words and Phrases: Generative Models, Diffusion Models, Score-Based Generative Models, Stochastic Differential
ACM Reference Format:
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023.
Diffusion Models: A Comprehensive Survey of Methods and Applications. 1, 1 , 58 pages. 
∗Contributed equally.
†Wentao Zhang, Bin Cui, and Ming-Hsuan Yang are corresponding authors.
Authors’ addresses: Ling Yang, Peking University, China, ; Zhilong Zhang, Peking University, China, ;
Yang Song, OpenAI, USA, ; Shenda Hong, Peking University, China, ; Runsheng Xu, University of
California, Los Angeles, USA, ; Yue Zhao, Carnegie Mellon University, USA, ; Wentao Zhang, Peking University,
China, ; Bin Cui, Peking University, China, ; Ming-Hsuan Yang, University of California at Merced, USA,
 .
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from .
© 2023 Association for Computing Machinery.
Manuscript submitted to ACM
Manuscript submitted to ACM
 
Yang et al.
Introduction
Foundations of Diffusion Models
Denoising Diffusion Probabilistic Models (DDPMs)
Score-Based Generative Models (SGMs)
Stochastic Differential Equations (Score SDEs)
Diffusion Models with Efficient Sampling
Learning-Free Sampling
SDE Solvers
ODE solvers
Learning-Based Sampling
Optimized Discretization
Truncated Diffusion
Knowledge Distillation
Diffusion Models with Improved Likelihood
Noise Schedule Optimization
Reverse Variance Learning
Exact Likelihood Computation
Diffusion Models for Data with Special Structures
Discrete Data
Data with Invariant Structures
Data with Manifold Structures
Known Manifolds
Learned Manifolds
Connections with Other Generative Models
Large Language Models and Connections with Diffusion Models
Variational Autoencoders and Connections with Diffusion Models
Generative Adversarial Networks and Connections with Diffusion Models
Normalizing Flows and Connections with Diffusion Models
Autoregressive Models and Connections with Diffusion Models
Energy-based Models and Connections with Diffusion Models
Applications of Diffusion Models
Unconditional and Conditional Diffusion Models
Conditioning Mechanisms in Diffusion Models
Diffusion with DPO/RLHF
Condition Diffusion on Labels and Classifiers
Condition Diffusion on Texts, Images, and Semantic Maps
Condition Diffusion on Graphs
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Computer Vision
Image Super Resolution, Inpainting, Restoration, Translation, and Editing
Semantic Segmentation
Video Generation
Generating Data from Diffusion Models
Point Cloud Completion and Generation
Anomaly Detection
Natural Language Generation
Multi-Modal Generation
Text-to-Image Generation
Scene Graph-to-Image Generation
Text-to-3D Generation
Text-to-Motion Generation
Text-to-Video Generation
Text-to-Audio Generation
Temporal Data Modeling
Time Series Imputation
Time Series Forecasting
Waveform Signal Processing
Robust Learning
Interdisciplinary Applications
Drug Design and Life Science
Material Design
Medical Image Reconstruction
Future Directions
Revisiting Assumptions
Theoretical Understanding
Latent Representations
AIGC and Diffusion Foundation Models
Conclusion
References
INTRODUCTION
Diffusion models have emerged as the new state-of-the-art family of deep generative models.
They have broken the long-time dominance of generative adversarial networks (GANs) in the challenging task
of image synthesis and have also shown potential in a variety of domains, ranging from computer
vision , natural language processing
 , temporal data modeling , multi-modal modeling ,
Manuscript submitted to ACM
Yang et al.
Fig. 1. Taxonomy of diffusion models variants (in Sections 3 to 5), connections with other generative models (in Section 6), applications
of diffusion models (in Section 7), and future directions (in Section 8).
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
robust machine learning , to interdisciplinary applications in fields such as computational chemistry
 and medical image reconstruction .
Numerous methods have been developed to improve diffusion models, either by enhancing empirical performance or by extending the model’s capacity from a theoretical perspective . Over
the past two years, the body of research on diffusion models has grown significantly, making it increasingly challenging
for new researchers to stay abreast of the recent developments in the field. Additionally, the sheer volume of work can
obscure major trends and hinder further research progress. This survey aims to address these problems by providing a
comprehensive overview of the state of diffusion model research, categorizing various approaches, and highlighting
key advances. We hope this survey to serve as a helpful entry point for researchers new to the field while providing a
broader perspective for experienced researchers.
In this paper, we first explain the foundations of diffusion models (Section 2), providing a brief but self-contained
introduction to three predominant formulations: denoising diffusion probabilistic models (DDPMs) , scorebased generative models (SGMs) , and stochastic differential equations (Score SDEs) . Key to
all these approaches is to progressively perturb data with intensifying random noise (called the “diffusion” process),
then successively remove noise to generate new data samples. We clarify how they work under the same principle of
diffusion and explain how these three models are connected and can be reduced to one another.
Next, we present a taxonomy of recent research that maps out the field of diffusion models, categorizing it into three
key areas: efficient sampling (Section 3), improved likelihood estimation (Section 4), and methods for handling data with
special structures (Section 5), such as relational data, data with permutation/rotational invariance, and data residing on
manifolds. We further examine the models by breaking each category into more detailed sub-categories, as illustrated
in Fig. 1. In addition, we discuss the connections of diffusion models to other deep generative models (Section 6),
including variational autoencoders (VAEs) , generative adversarial networks (GANs) , normalizing flows
 , autoregressive models , and energy-based models (EBMs) . By combining these models
with diffusion models, researchers have the potential to achieve even stronger performance.
Following that, our survey reviews six major categories of application that diffusion models have been applied to in the
existing research (Section 7): computer vision, natural language process, temporal data modeling, multi-modal learning,
robust learning, and interdisciplinary applications. For each task, we provide a definition, describe how diffusion models
can be employed to address it and summarize relevant previous work. We conclude our paper (Sections 8 and 9) by
providing an outlook on possible future directions for this exciting new area of research.
FOUNDATIONS OF DIFFUSION MODELS
Diffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise,
then learn to reverse this process for sample generation. We present the intuition of diffusion models in Fig. 2. Current
research on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilistic
models (DDPMs) , score-based generative models (SGMs) , and stochastic differential equations
(Score SDEs) . We give a self-contained introduction to these three formulations in this section, while discussing
their connections with each other along the way.
Denoising Diffusion Probabilistic Models (DDPMs)
A denoising diffusion probabilistic model (DDPM) makes use of two Markov chains: a forward chain that
perturbs data to noise, and a reverse chain that converts noise back to data. The former is typically hand-designed with
Manuscript submitted to ACM
Yang et al.
Fig. 2. Diffusion models smoothly perturb data by adding noise, then reverse this process to generate new data from noise. Each
denoising step in the reverse process typically requires estimating the score function (see the illustrative figure on the right), which is
a gradient pointing to the directions of data with higher likelihood and less noise.
the goal to transform any data distribution into a simple prior distribution (e.g., standard Gaussian), while the latter
Markov chain reverses the former by learning transition kernels parameterized by deep neural networks. New data
points are subsequently generated by first sampling a random vector from the prior distribution, followed by ancestral
sampling through the reverse Markov chain .
Formally, given a data distribution x0 ∼𝑞(x0), the forward Markov process generates a sequence of random variables
x1, x2 . . . x𝑇with transition kernel 𝑞(x𝑡| x𝑡−1). Using the chain rule of probability and the Markov property, we can
factorize the joint distribution of x1, x2 . . . x𝑇conditioned on x0, denoted as 𝑞(x1, . . . , x𝑇| x0), into
𝑞(x1, . . . , x𝑇| x0) =
𝑞(x𝑡| x𝑡−1).
In DDPMs, we handcraft the transition kernel 𝑞(x𝑡| x𝑡−1) to incrementally transform the data distribution 𝑞(x0) into a
tractable prior distribution. One typical design for the transition kernel is Gaussian perturbation, and the most common
choice for the transition kernel is
𝑞(x𝑡| x𝑡−1) = N (x𝑡;
1 −𝛽𝑡x𝑡−1, 𝛽𝑡I),
where 𝛽𝑡∈(0, 1) is a hyperparameter chosen ahead of model training. We use this kernel to simply our discussion here,
although other types of kernels are also applicable in the same vein. As observed by Sohl-Dickstein et al. ,
this Gaussian transition kernel allows us to marginalize the joint distribution in Eq. (1) to obtain the analytical form of
𝑞(x𝑡| x0) for all 𝑡∈{0, 1, · · · ,𝑇}. Specifically, with 𝛼𝑡B 1 −𝛽𝑡and ¯𝛼𝑡B Î𝑡
𝑠=0 𝛼𝑠, we have
𝑞(x𝑡| x0) = N (x𝑡; √¯𝛼𝑡x0, (1 −¯𝛼𝑡)I).
Given x0, we can easily obtain a sample of x𝑡by sampling a Gaussian vector 𝝐∼N (0, I) and applying the transformation
x𝑡= √¯𝛼𝑡x0 + √1 −¯𝛼𝑡𝝐.
When ¯𝛼𝑇≈0, x𝑇is almost Gaussian in distribution, so we have 𝑞(x𝑇) B
𝑞(x𝑇| x0)𝑞(x0)dx0 ≈N (x𝑇; 0, I).
Intuitively speaking, this forward process slowly injects noise to data until all structures are lost. For generating
new data samples, DDPMs start by first generating an unstructured noise vector from the prior distribution (which is
typically trivial to obtain), then gradually remove noise therein by running a learnable Markov chain in the reverse
time direction. Specifically, the reverse Markov chain is parameterized by a prior distribution 𝑝(x𝑇) = N (x𝑇; 0, I) and a
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
learnable transition kernel 𝑝𝜃(x𝑡−1 | x𝑡). We choose the prior distribution 𝑝(x𝑇) = N (x𝑇; 0, I) because the forward
process is constructed such that 𝑞(x𝑇) ≈N (x𝑇; 0, I). The learnable transition kernel 𝑝𝜃(x𝑡−1 | x𝑡) takes the form of
𝑝𝜃(x𝑡−1 | x𝑡) = N (x𝑡−1; 𝜇𝜃(x𝑡,𝑡), Σ𝜃(x𝑡,𝑡))
where 𝜃denotes model parameters, and the mean 𝜇𝜃(x𝑡,𝑡) and variance Σ𝜃(x𝑡,𝑡) are parameterized by deep neural
networks. With this reverse Markov chain in hand, we can generate a data sample x0 by first sampling a noise vector
x𝑇∼𝑝(x𝑇), then iteratively sampling from the learnable transition kernel x𝑡−1 ∼𝑝𝜃(x𝑡−1 | x𝑡) until 𝑡= 1.
Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal
of the forward Markov chain. That is, we have to adjust the parameter 𝜃so that the joint distribution of the reverse
Markov chain 𝑝𝜃(x0, x1, · · · , x𝑇) B 𝑝(x𝑇) Î𝑇
𝑡=1 𝑝𝜃(x𝑡−1 | x𝑡) closely approximates that of the forward process
𝑞(x0, x1, · · · , x𝑇) B 𝑞(x0) Î𝑇
𝑡=1 𝑞(x𝑡| x𝑡−1) (Eq. (1)). This is achieved by minimizing the Kullback-Leibler (KL)
divergence between these two:
KL(𝑞(x0, x1, · · · , x𝑇) || 𝑝𝜃(x0, x1, · · · , x𝑇))
(𝑖)= −E𝑞(x0,x1,··· ,x𝑇) [log𝑝𝜃(x0, x1, · · · , x𝑇)] + const
= E𝑞(x0,x1,··· ,x𝑇)
−log𝑝(x𝑇) −
log 𝑝𝜃(x𝑡−1 | x𝑡)
𝑞(x𝑡| x𝑡−1)
B−𝐿VLB(x0)
≥E [−log𝑝𝜃(x0)] + const,
where (i) is from the definition of KL divergence, (ii) is from the fact that 𝑞(x0, x1, · · · , x𝑇) and 𝑝𝜃(x0, x1, · · · , x𝑇) are
both products of distributions, and (iii) is from Jensen’s inequality. The first term in Eq. (8) is the variational lower
bound (VLB) of the log-likelihood of the data x0, a common objective for training probabilistic generative models.
We use “const” to symbolize a constant that does not depend on the model parameter 𝜃and hence does not affect
optimization. The objective of DDPM training is to maximize the VLB (or equivalently, minimizing the negative VLB),
which is particularly easy to optimize because it is a sum of independent terms, and can thus be estimated efficiently by
Monte Carlo sampling and optimized effectively by stochastic optimization .
Ho et al. propose to reweight various terms in 𝐿VLB for better sample quality and noticed an important
equivalence between the resulting loss function and the training objective for noise-conditional score networks (NCSNs),
one type of score-based generative models, in Song and Ermon . The loss in takes the form of
E𝑡∼U⟦1,𝑇⟧,x0∼𝑞(x0),𝝐∼N(0,I)
𝜆(𝑡) ∥𝝐−𝝐𝜃(x𝑡,𝑡)∥2
where 𝜆(𝑡) is a positive weighting function, x𝑡is computed from x0 and 𝝐by Eq. (4), U⟦1,𝑇⟧is a uniform distribution
over the set {1, 2, · · · ,𝑇}, and 𝝐𝜃is a deep neural network with parameter 𝜃that predicts the noise vector 𝝐given x𝑡
and 𝑡. This objective reduces to Eq. (8) for a particular choice of the weighting function 𝜆(𝑡), and has the same form
as the loss of denoising score matching over multiple noise scales for training score-based generative models ,
another formulation of diffusion models to be discussed in the next section.
Manuscript submitted to ACM
Yang et al.
Score-Based Generative Models (SGMs)
At the core of score-based generative models is the concept of (Stein) score (a.k.a., score or score function) .
Given a probability density function 𝑝(x), its score function is defined as the gradient of the log probability density
∇x log𝑝(x). Unlike the commonly used Fisher score ∇𝜃log𝑝𝜃(x) in statistics, the Stein score considered here is a
function of the data x rather than the model parameter 𝜃. It is a vector field that points to directions along which the
probability density function has the largest growth rate.
The key idea of score-based generative models (SGMs) is to perturb data with a sequence of intensifying
Gaussian noise and jointly estimate the score functions for all noisy data distributions by training a deep neural
network model conditioned on noise levels (called a noise-conditional score network, NCSN, in ). Samples
are generated by chaining the score functions at decreasing noise levels with score-based sampling approaches,
including Langevin Monte Carlo , stochastic differential equations , ordinary differential
equations , and their various combinations . Training and sampling are completely decoupled
in the formulation of score-based generative models, so one can use a multitude of sampling techniques after the
estimation of score functions.
With similar notations in Section 2.1, we let 𝑞(x0) be the data distribution, and 0 < 𝜎1 < 𝜎2 < · · · < 𝜎𝑡< · · · < 𝜎𝑇be
a sequence of noise levels. A typical example of SGMs involves perturbing a data point x0 to x𝑡by the Gaussian noise
distribution 𝑞(x𝑡| x0) = N (x𝑡; x0, 𝜎2
𝑡𝐼). This yields a sequence of noisy data densities 𝑞(x1),𝑞(x2), · · · ,𝑞(x𝑇), where
𝑞(x𝑡)𝑞(x0)dx0. A noise-conditional score network is a deep neural network s𝜃(x,𝑡) trained to estimate
the score function ∇x𝑡log𝑞(x𝑡). Learning score functions from data (a.k.a., score estimate) has established techniques
such as score matching , denoising score matching , and sliced score matching , so we can
directly employ one of them to train our noise-conditional score networks from perturbed data points. For example,
with denoising score matching and similar notations in Eq. (10), the training objective is given by
E𝑡∼U⟦1,𝑇⟧,x0∼𝑞(x0),x𝑡∼𝑞(x𝑡|x0)
∇x𝑡log𝑞(x𝑡) −s𝜃(x𝑡,𝑡)
(𝑖)= E𝑡∼U⟦1,𝑇⟧,x0∼𝑞(x0),x𝑡∼𝑞(x𝑡|x0)
∇x𝑡log𝑞(x𝑡| x0) −s𝜃(x𝑡,𝑡)
= E𝑡∼U⟦1,𝑇⟧,x0∼𝑞(x0),x𝑡∼𝑞(x𝑡|x0)
−𝜎𝑡s𝜃(x𝑡,𝑡)
= E𝑡∼U⟦1,𝑇⟧,x0∼𝑞(x0),𝝐∼N(0,I)
𝜆(𝑡) ∥𝝐+ 𝜎𝑡s𝜃(x𝑡,𝑡)∥2
where (i) is derived by , (ii) is from the assumption that 𝑞(x𝑡| x0) = N(x𝑡; x0, 𝜎2
𝑡I), and (iii) is from the fact that
x𝑡= x0 + 𝜎𝑡𝝐. Again, we denote by 𝜆(𝑡) a positive weighting function, and “const” a constant that does not depend
on the trainable parameter 𝜃. Comparing Eq. (14) with Eq. (10), it is clear that the training objectives of DDPMs and
SGMs are equivalent, once we set 𝝐𝜃(x,𝑡) = −𝜎𝑡s𝜃(x,𝑡). Moreover, one can generalize the score matching with higher
order. High-order derivatives of data density provide additional local information about the data distribution. Meng et
al. proposes a generalized denoising score matching method to efficiently estimate the high-order score function.
The proposed model can improve the mixing speed of Langevin dynamics and thus the sampling efficiency of diffusion
For sample generation, SGMs leverage iterative approaches to produce samples from s𝜃(x,𝑇), s𝜃(x,𝑇−1), · · · , s𝜃(x, 0)
in succession. Many sampling approaches exist due to the decoupling of training and inference in SGMs, some of which
are discussed in the next section. Here we introduce the first sampling method for SGMs, called annealed Langevin
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
dynamics (ALD) . Let 𝑁be the number of iterations per time step and 𝑠𝑡> 0 be the step size. We first initialize
ALD with x(𝑁)
∼N (0, I), then apply Langevin Monte Carlo for 𝑡= 𝑇,𝑇−1, · · · , 1 one after the other. At each time
step 0 ≤𝑡< 𝑇, we start with x(0)
𝑡+1 , before iterating according to the following update rule for 𝑖= 0, 1, · · · , 𝑁−1:
𝝐(𝑖) ←N (0, I)
2𝑠𝑡s𝜃(x(𝑖)
𝑡,𝑡) + √𝑠𝑡𝝐(𝑖).
The theory of Langevin Monte Carlo guarantees that as 𝑠𝑡→0 and 𝑁→∞, x(𝑁)
becomes a valid sample from
the data distribution 𝑞(x0).
Stochastic Differential Equations (Score SDEs)
DDPMs and SGMs can be further generalized to the case of infinite time steps or noise levels, where the perturbation and
denoising processes are solutions to stochastic differential equations (SDEs). We call this formulation Score SDE ,
as it leverages SDEs for noise perturbation and sample generation, and the denoising process requires estimating score
functions of noisy data distributions.
Score SDEs perturb data to noise with a diffusion process governed by the following stochastic differential equation
(SDE) :
dx = f(x,𝑡)d𝑡+ 𝑔(𝑡)dw
where f(x,𝑡) and 𝑔(𝑡) are diffusion and drift functions of the SDE, and w is a standard Wiener process (a.k.a., Brownian
motion). The forward processes in DDPMs and SGMs are both discretizations of this SDE. As demonstrated in Song et
al. , for DDPMs, the corresponding SDE is:
where 𝛽( 𝑡
𝑇) = 𝑇𝛽𝑡as 𝑇goes to infinity; and for SGMs, the corresponding SDE is given by
where 𝜎( 𝑡
𝑇) = 𝜎𝑡as 𝑇goes to infinity. Here we use 𝑞𝑡(x) to denote the distribution of x𝑡in the forward process.
Crucially, for any diffusion process in the form of Eq. (15), Anderson shows that it can be reversed by solving the
following reverse-time SDE:
f(x,𝑡) −𝑔(𝑡)2∇x log𝑞𝑡(x)
d𝑡+ 𝑔(𝑡)d ¯w
where ¯w is a standard Wiener process when time flows backwards, and d𝑡denotes an infinitesimal negative time step.
The solution trajectories of this reverse SDE share the same marginal densities as those of the forward SDE, except that
they evolve in the opposite time direction . Intuitively, solutions to the reverse-time SDE are diffusion processes
that gradually convert noise to data. Moreover, Song et al. prove the existence of an ordinary differential
equation (ODE), namely the probability flow ODE, whose trajectories have the same marginals as the reverse-time SDE.
The probability flow ODE is given by:
2𝑔(𝑡)2∇x log𝑞𝑡(x)
Manuscript submitted to ACM
Yang et al.
Both the reverse-time SDE and the probability flow ODE allow sampling from the same data distribution as their
trajectories have the same marginals.
Once the score function at each time step t, ∇x log𝑞𝑡(x), is known, we unlock both the reverse-time SDE (Eq. (18))
and the probability flow ODE (Eq. (19)) and can subsequently generate samples by solving them with various numerical
techniques, such as annealed Langevin dynamics (cf ., Section 2.2), numerical SDE solvers , numerical
ODE solvers , and predictor-corrector methods (combination of MCMC and numerical ODE/SDE
solvers) . Like in SGMs, we parameterize a time-dependent score model s𝜃(x𝑡,𝑡) to estimate the score function by
generalizing the score matching objective in Eq. (14) to continuous time, leading to the following objective:
E𝑡∼U[0,𝑇],x0∼𝑞(x0),x𝑡∼𝑞(x𝑡|x0)
s𝜃(x𝑡,𝑡) −∇x𝑡log𝑞0𝑡(x𝑡| x0)
where U[0,𝑇] denotes the uniform distribution over [0,𝑇], and the remaining notations follow Eq. (14).
Subsequent research on diffusion models focuses on improving these classical approaches (DDPMs, SGMs, and Score
SDEs) from three major directions: faster and more efficient sampling, more accurate likelihood and density estimation,
and handling data with special structures (such as permutation invariance, manifold structures, and discrete data).
We survey each direction extensively in the next three sections (Sections 3 to 5). In Table 1, we list the three types of
diffusion models with more detailed categorization, corresponding articles and years, under continuous and discrete
time settings.
DIFFUSION MODELS WITH EFFICIENT SAMPLING
Generating samples from diffusion models typically demands iterative approaches that involve a large number of
evaluation steps. A great deal of recent work has focused on speeding up the sampling process while also improving
quality of the resulting samples. We classify these efficient sampling methods into two main categories: those that do
not involve learning (learning-free sampling) and those that require an additional learning process after the diffusion
model has been trained (learning-based sampling).
Learning-Free Sampling
Many samplers for diffusion models rely on discretizing either the reverse-time SDE present in Eq. (18) or the probability
flow ODE from Eq. (19). Since the cost of sampling increases proportionally with the number of discretized time steps,
many researchers have focused on developing discretization schemes that reduce the number of time steps while also
minimizing discretization errors.
SDE Solvers. The generation process of DDPM can be viewed as a particular discretization of the
reverse-time SDE. As discussed in Section 2.3, the forward process of DDPM discretizes the SDE in Eq. (16), whose
corresponding reverse SDE takes the form of
2𝛽(𝑡)(x𝑡−∇x𝑡log𝑞𝑡(x𝑡))d𝑡+
Song et al. show that the reverse Markov chain defined by Eq. (5) amounts to a numerical SDE solver for
Noise-Conditional Score Networks (NCSNs) and Critically-Damped Langevin Diffusion (CLD) both solve
the reverse-time SDE with inspirations from Langevin dynamics. In particular, NCSNs leverage annealed Langevin
dynamics (ALD, cf ., Section 2.2) to iteratively generate data while smoothly reducing noise level until the generated
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Table 1. Three types of diffusion models are listed with corresponding articles and years, under continuous and discrete settings.
Efficient Sampling
Learning-Free Sampling
SDE Solvers
Song et al. 
Continuous
Dockhorn et al. 
Continuous
Jolicoeur et al. 
Continuous
Jolicoeur et al. 
Continuous
Chuang et al. 
Continuous
Song et al. 
Continuous
Karras et al. 
Continuous
ODE Solvers
Liu et al. 
Continuous
Song et al. 
Continuous
Zhang et al. 
Continuous
Karras et al. 
Continuous
Lu et al. 
Continuous
Zhang et al. 
Continuous
Learning-Based Sampling
Optimized Discretization
Watson et al. 
Watson et al. 
Dockhorn et al. 
Continuous
Knowledge Distillation
Salimans et al. 
Luhman et al. 
Meng et al. 
Truncated Diffusion
Lyu et al. 
Zheng et al. 
Improved Likelihood
Noise Schedule Optimization
Noise Schedule Optimization
Nichol et al. 
Kingma et al. 
Huang et al. 
Yang et al. 
Reverse Variance Learning
Reverse Variance Learning
Bao et al. 
Nichol et al. 
Exact Likelihood Computation
Exact Likelihood Computation
Song et al. 
Continuous
Huang et al. 
Continuous
Song et al. 
Continuous
Lu et al. 
Continuous
Data with Special Structures
Manifold Structures
Learned Manifolds
Vahdat et al. 
Continuous
Yang et al. 
Ramesh et al. 
Rombach et al. 
Known Manifolds
Bortoli et al. 
Continuous
Huang et al. 
Continuous
Data with Invariant Structures
Data with Invariant Structures
Niu et al. 
Jo et al. 
Continuous
Shi et al. 
Continuous
Xu et al. 
Discrete Data
Discrete Data
Meng et al. 
liu et al. 
Continuous
Sohl et al. 
Austin et al. 
Xie et al. 
Gu et al. 
Campbell et al. 
Continuous
data distribution converges to the original data distribution. Although the sampling trajectories of ALD are not exact
solutions to the reverse-time SDE, they have the correct marginals and hence produce correct samples under the
assumption that Langevin dynamics converges to its equilibrium at every noise level. The method of ALD is further
improved by Consistent Annealed Sampling (CAS) , a score-based MCMC approach with better scaling of time
steps and added noise. Inspired by statistical mechanics, CLD proposes an augmented SDE with an auxiliary velocity
term resembling underdamped Langevin diffusion. To obtain the time reversal of the extended SDE, CLD only needs to
Manuscript submitted to ACM
Yang et al.
learn the score function of the conditional distribution of velocity given data, arguably easier than learning scores of
data directly. The added velocity term is reported to improve sampling speed as well as quality.
The reverse diffusion method proposed in discretizes the reverse-time SDE in the same way as the forward one.
For any one-step discretization of the forward SDE, one may write the general form below:
x𝑖+1 = x𝑖+ f𝑖(x𝑖) + g𝑖z𝑖,
𝑖= 0, 1, · · · , 𝑁−1
where z𝑖∼N (0, I), f𝑖and g𝑖are determined by drift/diffusion coefficients of the SDE and the discretization scheme.
Reverse diffusion proposes to discretize the reverse-time SDE similarly to the forward SDE, i.e.,
x𝑖= x𝑖+1 −f𝑖+1(x𝑖+1) + g𝑖+1g𝑡
𝑖+1s𝜃∗(x𝑖+1,𝑡𝑖+1) + g𝑖+1z𝑖
𝑖= 0, 1, · · · , 𝑁−1
where s𝜃∗(x𝑖,𝑡𝑖) is the trained noise-conditional score model. Song et al. prove that the reverse diffusion
method is a numerical SDE solver for the reverse-time SDE in Eq. (18). This process can be applied to any types of
forward SDEs, and empirical results indicate this sampler performs slightly better than DDPM for a particular
type of SDEs called the VP-SDE.
Jolicoeur-Martineau et al. develop an SDE solver with adaptive step sizes for faster generation. The step
size is controlled by comparing the output of a high-order SDE solver versus the output of an low-order SDE solver. At
each time step, the high- and low-order solvers generate new sample x′
high and x′
low from the previous sample x′𝑝𝑟𝑒𝑣
respectively. The step size is then adjusted by comparing the difference between the two samples. If x′
high and x′
are similar, the algorithm will return x′
high and then increase the step size. The similarity between x′
high and x′
measured by:
𝛿(x′, x′prev)
where 𝛿(x′
low, x′prev) B max(𝜖𝑎𝑏𝑠,𝜖𝑟𝑒𝑙max(| x′
low, | x′prev|)), and 𝜖𝑎𝑏𝑠and 𝜖𝑟𝑒𝑙are absolute and relative tolerances.
The predictor-corrector method proposed in solves the reverse SDE by combining numerical SDE solvers
(“predictor”) and iterative Markov chain Monte Carlo (MCMC) approaches (“corrector"). At each time step, the predictorcorrector method first employs a numerical SDE solver to produce a coarse sample, followed by a "corrector" that
corrects the sample’ marginal distribution with score-based MCMC. The resulting samples have the same time-marginals
as solution trajectories of the reverse-time SDE, i.e., they are equivalent in distribution at all time steps. Empirical
results demonstrate that adding a corrector based on Langevin Monte Carlo is more efficient than using an additional
predictor without correctors . Karras et al. further improve the Langevin dynamics corrector in 
by proposing a Langevin-like “churn” step of adding and removing noise, achieving new state-of-the-art sample quality
on datasets like CIFAR-10 and ImageNet-64 .
ODE solvers. A large body of works on faster diffusion samplers are based on solving the probability flow ODE
(Eq. (19)) introduced in Section 2.3. In contrast to SDE solvers, the trajectories of ODE solvers are deterministic and
thus not affected by stochastic fluctuations. These deterministic ODE solvers typically converge much faster than their
stochastic counterparts at the cost of slightly inferior sample quality.
Denoising Diffusion Implicit Models (DDIM) is one of the earliest work on accelerating diffusion model
sampling. The original motivation was to extend the original DDPM to non-Markovian case with the following Markov
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
𝑞(x1, . . . , x𝑇| x0) =
𝑞(x𝑡| x𝑡−1, x0)
𝑞𝜎(x𝑡−1 | x𝑡, x0) = N (x𝑡−1| ˜𝜇𝑡(x𝑡, x0), 𝜎2
˜𝜇𝑡(x𝑡, x0) B
1 −𝛼𝑡−1 −𝜎2
𝑡· x𝑡−√𝛼𝑡x0
This formulation encapsulates DDPM and DDIM as special cases, where DDPM corresponds to setting 𝜎2
DDIM corresponds to setting 𝜎2
𝑡= 0. DDIM learns a Markov chain to reverse this non-Markov perturbation process,
which is fully deterministic when 𝜎2
𝑡= 0. It is observed in that the DDIM sampling process amounts
to a special discretization scheme of the probability flow ODE. Inspired by an analysis of DDIM on a singleton dataset,
generalized Denoising Diffusion Implicit Models (gDDIM) proposes a modified parameterization of the score
network that enables deterministic sampling for more general diffusion processes, such as the one in Critically-Damped
Langevin Diffusion (CLD) . PNDM proposes a pseudo numerical method to generate sample along a specific
manifold in R𝑁. It uses numerical solver with nonlinear transfer part to solve differential equation on manifolds and
then generates sample, which encapsulates DDIM as a special case.
Through extensive experimental investigations, Karras et al. show that Heun’s 2𝑛𝑑order method 
provides an excellent trade off between sample quality and sampling speed. The higher-order solver leads to smaller
discretization error at the cost of one additional evaluation of the learned score function per time step. Heun’s method
generates samples of comparable, if not better quality than Euler’s method with fewer sampling steps.
Diffusion Exponential Integrator Sampler and DPM-solver leverage the semi-linear structure of probability
flow ODE to develop customized ODE solvers that are more efficient than general-purpose Runge-Kutta methods.
Specifically, the linear part of probability flow ODE can be analytically computed, while the non-linear part can be
solved with techniques similar to exponential integrators in the field of ODE solvers. These methods contain DDIM as
a first-order approximation. However, they also allow for higher order integrators, which can produce high-quality
samples in just 10 to 20 iterations—far fewer than the hundreds of iterations typically required by diffusion models
without accelerated sampling.
Learning-Based Sampling
Learning-based sampling is another efficient approach for diffusion models. By using partial steps or training a sampler
for the reverse process, this method achieves faster sampling speeds at the expense of slight degradation in sample
quality. Unlike learning-free approaches that use handcrafted steps, learning-based sampling typically involves selecting
steps by optimizing certain learning objectives.
Optimized Discretization. Given a pre-trained diffusion model, Watson et al. put forth a strategy
for finding the optimal discretization scheme by selecting the best 𝐾time steps to maximize the training objective for
DDPMs. Key to this approach is the observation that the DDPM objective can be broken down into a sum of individual
terms, making it well suited for dynamic programming. However, it is well known that the variational lower bound
used for DDPM training does not correlate directly with sample quality . A subsequent work, called Differentiable
Diffusion Sampler Search , addresses this issue by directly optimizing a common metric for sample quality called
the Kernel Inception Distance (KID) . This optimization is feasible with the help of reparameterization 
Manuscript submitted to ACM
Yang et al.
and gradient rematerialization. Based on truncated Taylor methods, Dockhorn et al. derive a second-order
solver for accelerating synthesis by training a additional head on top of the first-order score network.
Truncated Diffusion. One can improve sampling speed by truncating the forward and reverse diffusion processes
 . The key idea is to halt the forward diffusion process early on, after just a few steps, and to begin the
reverse denoising process with a non-Gaussian distribution. Samples from this distribution can be obtained efficiently
by diffusing samples from pre-trained generative models, such as variational autoencoders or generative
adversarial networks .
Knowledge Distillation. Approaches that use knowledge distillation can significantly improve the
sampling speed of diffusion models. Specifically, in Progressive Distillation , the authors propose distilling the full
sampling process into a faster sampler that requires only half as many steps. By parameterizing the new sampler as a
deep neural network, authors are able to train the sampler to match the input and output of the DDIM sampling process.
Repeating this procedure can further reduce sampling steps, although fewer steps can result in reduced sample quality.
To address this issue, the authors suggest new parameterizations for diffusion models and new weighting schemes for
the objective function.
DIFFUSION MODELS WITH IMPROVED LIKELIHOOD
As discussed in Section 2.1, the training objective for diffusion models is a (negative) variational lower bound (VLB)
on the log-likelihood. This bound, however, may not be tight in many cases , leading to potentially suboptimal
log-likelihoods from diffusion models. In this section, we survey recent works on likelihood maximization for diffusion
models. We focus on three types of methods: noise schedule optimization, reverse variance learning, and exact loglikelihood evaluation.
Noise Schedule Optimization
In the classical formulation of diffusion models, noise schedules in the forward process are handcrafted without trainable
parameters. By optimizing the forward noise schedule jointly with other parameters of diffusion models, one can further
maximize the VLB in order to achieve higher log-likelihood values .
The work of iDDPM demonstrates that a certain cosine noise schedule can improve log-likelihoods. Specifically,
the cosine noise schedule in their work takes the form of
ℎ(𝑡) = cos
where ¯𝛼𝑡and 𝛽𝑡are defined in Eqs. (2) and (3), and 𝑚is a hyperparameter to control the noise scale at 𝑡= 0. They also
propose a parameterization of the reverse variance with an interpolation between 𝛽𝑡and 1 −¯𝛼𝑡in the log domain.
In Variational Diffusion Models (VDMs) , authors propose to improve the likelihood of continuous-time
diffusion models by jointly training the noise schedule and other diffusion model parameters to maximize the VLB.
They parameterize the noise schedule using a monotonic neural network 𝛾𝜂(𝑡), and build the forward perturbation
process according to 𝜎2
𝑡= sigmoid(𝛾𝜂(𝑡)), 𝑞(x𝑡| x0) = N ( ¯𝛼𝑡x0, 𝜎2
𝑡I), and ¯𝛼𝑡=
𝑡). Moreover, authors prove
that the VLB for data point x can be simplified to a form that only depends on the signal-to-noise ratio R(𝑡) B ¯𝛼2
particular, the 𝐿𝑉𝐿𝐵can be decomposed to
𝐿𝑉𝐿𝐵= −Ex0 KL(𝑞(x𝑇|x0) || 𝑝(x𝑇)) + Ex0,x1 log𝑝(x0|x1) −𝐿𝐷,
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
where the first and second terms can be optimized directly in analogy to training variational autoencoders. The third
term can be further simplified to the following:
2Ex0,𝜖∼N(0,I)
∥x0 −˜x𝜃(x𝑣, 𝑣)∥2
where Rmax = 𝑅(1), Rmin = 𝑅(𝑇), x𝑣= ¯𝛼𝑣x0 + 𝜎𝑣𝜖denotes a noisy data point obtained by diffusing x0 with the forward
perturbation process until 𝑡= 𝑅−1(𝑣), and ˜x𝜃denotes the predicted noise-free data point by the diffusion model. As a
result, noise schedules do not affect the VLB as long as they share the same values at Rmin and Rmax, and will only
affect the variance of Monte Carlo estimators for VLB.
Another line of works propose to the modify diffusion trajectory through the integration of cross-modality
information. Specifically, the cross-modal information, denoted as 𝑟𝜙(𝑦,𝑥0), is extracted from any conditional input 𝑦
and original sample 𝑥0 with relational network 𝑟𝜙(·). And then it can be injected to the forward process as an additional
bias to adapt diffusion trajectory:
𝑞𝑡(𝑥𝑡|𝑥0,𝑦) = N (𝑥𝑡, √¯𝛼𝑡𝑥0 + 𝑘𝑡𝑟𝜙(𝑥0,𝑦), (1 −¯𝛼𝑡)𝐼)
where 𝑘𝑡is a non-negative scalar that control the magnitude of the bias term. It is important to note that with this
modification, the forward process ceases to be a Markovian chain. ContextDiff introduces a general framework
to jointly learn the cross-modal relational network 𝑟𝜙and the diffusion model, and derives the VLB and sampling
procedure for this modified diffusion process.
Reverse Variance Learning
The classical formulation of diffusion models assumes that Gaussian transition kernels in the reverse Markov chain
have fixed variance parameters. Recall that we formulated the reverse kernel as 𝑞𝜃(x𝑡−1 | x𝑡) = N (𝜇𝜃(x𝑡,𝑡), Σ𝜃(x𝑡,𝑡))
in Eq. (5) but often fixed the reverse variance Σ𝜃(x𝑡,𝑡) to 𝛽𝑡I. Many methods propose to train the reverse variances as
well to further maximize VLB and log-likelihood values.
In iDDPM , Nichol and Dhariwal propose to learn the reverse variances by parameterizing them with a form
of linear interpolation and training them using a hybrid objective. This results in higher log-likelihoods and faster
sampling without losing sample quality. In particular, they parameterize the reverse variance in Eq. (5) as:
Σ𝜃(x𝑡,𝑡) = exp(𝜃· log 𝛽𝑡+ (1 −𝜃) · log ˜𝛽𝑡),
where ˜𝛽𝑡B 1−¯𝛼𝑡−1
· 𝛽𝑡and 𝜃is jointly trained to maximize VLB. This simple parameterization avoids the instability of
estimating more complicated forms of Σ𝜃(x𝑡,𝑡) and is reported to improve likelihood values.
Analytic-DPM shows a remarkable result that the optimal reverse variance can be obtained from a pre-trained
score function, with the analytic form below:
Σ𝜃(x𝑡,𝑡) = 𝜎2
1 −𝛽𝑡E𝑞𝑡(x𝑡)
||∇x𝑡log𝑞𝑡(x𝑡)||2
As a result, given a pre-traied score model, we can estimate its first- and second-order moments to obtain the optimal
reverse variances. Plugging them into the VLB can lead to tighter VLBs and higher likelihood values.
Manuscript submitted to ACM
Yang et al.
Exact Likelihood Computation
In the Score SDE formulation, samples are generated by solving the following reverse SDE, where ∇x𝑡log𝑝𝜃(x𝑡,𝑡)
in Eq. (18) is replaced by the learned noise-conditional score model s𝜃(x𝑡,𝑡):
dx = 𝑓(x𝑡,𝑡) −𝑔(𝑡)2s𝜃(x𝑡,𝑡)d𝑡+ 𝑔(𝑡)dw.
Here we use 𝑝sde
to denote the distribution of samples generated by solving the above SDE. One can also generate data
by plugging the score model into the probability flow ODE in Eq. (19), which gives:
d𝑡= 𝑓(x𝑡,𝑡) −1
2𝑔2(𝑡)s𝜃(x𝑡,𝑡)
B ˜𝑓𝜃(x𝑡,𝑡)
Similarly, we use 𝑝ode
to denote the distribution of samples generated via solving this ODE. The theory of neural
ODEs and continuous normalizing flows indicates that 𝑝ode
can be computed accurately albeit with high
computational cost. For 𝑝sde
𝜃, several concurrent works demonstrate that there exists an efficiently
computable variational lower bound, and we can directly train our diffusion models to maximize 𝑝sde
using modified
diffusion losses.
Specifically, Song et al. prove that with a special weighting function (likelihood weighting), the objective
used for training score SDEs implicitly maximizes the expected value of 𝑝sde
on data. It is shown that
D𝐾𝐿(𝑞0 ∥𝑝sde
) ≤L(𝜃;𝑔(·)2) + D𝐾𝐿(𝑞𝑇∥𝜋),
where L(𝜃;𝑔(·)2) is the Score SDE objective in Eq. (20) with 𝜆(𝑡) = 𝑔(𝑡)2. Since D𝐾𝐿(𝑞0 ∥𝑝sde
) = −E𝑞0 log(𝑝sde
and D𝐾𝐿(𝑞𝑇∥𝜋) is a constant, training with L(𝜃;𝑔(·)2) amounts to minimizing −E𝑞0 log(𝑝sde
), the expected negative
log-likelihood on data. Moreover, Song et al. and Huang et al. provide the following bound for
′ (x) is defined by
2 ||𝑔(𝑡)s𝜃(x𝑡,𝑡)||2 + ∇· (𝑔(𝑡)2s𝜃(x𝑡,𝑡) −𝑓(xt), t)
𝑑𝑡−Ex𝑇[log𝑝sde
(x𝑇) | x0 = 𝑥]
The first part of Eq. (38) is reminiscent of implicit score matching and the whole bound can be efficiently estimated
with Monte Carlo methods.
Since the probability flow ODE is a special case of neural ODEs or continuous normalizing flows, we can use
well-established approaches in those fields to compute log𝑝ode
accurately. Specifically, we have
(x0) = log𝑝𝑇(x𝑇) +
∇· ˜𝑓𝜃(x𝑡,𝑡)d𝑡.
One can compute the one-dimensional integral above with numerical ODE solvers and the Skilling-Hutchinson trace
estimator . Unfortunately, this formula cannot be directly optimized to maximize 𝑝ode
on data, as it requires
calling expensive ODE solvers for each data point x0. To reduce the cost of directly maximizing 𝑝ode
with the above
formula, Song et al. propose to maximize the variational lower bound of 𝑝sde
as a proxy for maximizing
, giving rise to a family of diffusion models called ScoreFlows.
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Lu et al. further improve ScoreFlows by proposing to minimize not just the vanilla score matching loss
function, but also its higher order generalizations. They prove that log𝑝ode
can be bounded with the first, second,
and third-order score matching errors. Building upon this theoretical result, authors further propose efficient training
algorithms for minimizing high order score matching losses and reported improved 𝑝ode
DIFFUSION MODELS FOR DATA WITH SPECIAL STRUCTURES
While diffusion models have achieved great success for data domains like images and audio, they do not necessarily
translate seamlessly to other modalities. Many important data domains have special structures that must be taken into
account for diffusion models to function effectively. Difficulties may arise, for example, when models rely on score
functions that are only defined on continuous data domains, or when data reside on low dimensional manifolds. To
cope with these challenges, diffusion models have to be adapted in various ways.
Discrete Data
Most diffusion models are geared towards continuous data domains, because Gaussian noise perturbation as used in
DDPMs is not a natural fit for discrete data, and the score functions required by SGMs and Score SDEs are only defined
on continuous data domains. To overcome this difficulty, several works build on Sohl-Dickstein et al.
 to generate discrete data of high dimensions. Specifically, VQ-Diffusion replaces Gaussian noise with a
random walk on the discrete data space, or a random masking operation. The resulting transition kernel for the forward
process takes the form of
𝑞(x𝑡| x𝑡−1) = v⊤(x𝑡)Q𝑡v(x𝑡−1)
where v(x) is a one-hot column vector, and Q𝑡is the transition kernel of a lazy random walk. D3PM accommodates
discrete data in diffusion models by constructing the forward noising process with absorbing state kernels or discretized
Gaussian kernels. Campbell et al. present the first continuous-time framework for discrete diffusion models.
Leveraging Continuous Time Markov Chains, they are able to derive efficient samplers that outperform discrete
counterparts, while providing a theoretical analysis on the error between the sample distribution and the true data
distribution.
Concrete Score Matching (CSM) proposes a generalization of the score function for discrete random variables.
Concrete score is defined by the rate of change of the probabilities with respect to directional changes of the input,
which can be seen as a finite-difference approximation to the continuous (Stein) score. The concrete score can be
efficiently trained and applied to MCMC.
Based on the theory of stochastic calculus, Liu et al. proposes a framework for diffusion models to
generate data on constrained and structured domains, including discrete data as a special case. Using a fundamental
theorem in stochastic calculus, the Doob’s h-transform, one can constrain the data distribution on a specific area by
including a special force term in the reverse diffusion process. They use a parameterization of the force term with an
EM-based optimization algorithm. Furthermore, the loss function can be transformed to 𝐿2 loss using Girsanov theorem.
Data with Invariant Structures
Data in many important domains have invariant structures. For example, graphs are permutation invariant, and point
clouds are both translation and rotation invariant. In diffusion models, these invariances are often ignored, which can
Manuscript submitted to ACM
Yang et al.
lead to suboptimal performance. To address this issue, several works propose to endow diffusion models with
the ability to account for invariance in data.
Niu et al. first tackle the problem of permutation invariant graph generation with diffusion models. They
achieve this by using a permutation equivariant graph neural network , called EDP-GNN, to parameterize
the noise-conditional score model. GDSS further develops this idea by proposing a continuous-time graph diffusion
process. This process models both the joint distribution of nodes and edges through a system of stochastic differential
equations (SDEs), where message-passing operations are used to guarantee permutation invariance.
Similarly, Shi et al. and Xu et al. enable diffusion models to generate molecular conformations
that are invariant to both translation and rotation. For example, Xu et al. shows that Markov chains starting
with an invariant prior and evolving with equivariant Markov kernels can induce an invariant marginal distribution,
which can be used to enforce appropriate data invariance in molecular conformation generation. Formally, let T be a
rotation or translation operation. Given that 𝑝(x𝑇) = 𝑝(T (x𝑇)), 𝑝𝜃(x𝑡−1 | x𝑡) = 𝑝𝜃(T (x𝑡−1) | T (x𝑡)), Xu et al. 
 prove that the distribution of samples is guaranteed to be invariant to T, that is, 𝑝0(x) = 𝑝0(T (x)). As a result,
one can build a diffusion model that generates rotation and translation invariant molecular conformations as long as
the prior and transition kernels enjoy the same invariance.
Data with Manifold Structures
Data with manifold structures are ubiquitous in machine learning. As the manifold hypothesis posits, natural
data often reside on manifolds with lower intrinsic dimensionality. In addition, many data domains have well-known
manifold structures. For instance, climate and earth data naturally lie on the sphere because that is the shape of our
planet. Many works have focused on developing diffusion models for data on manifolds. We categorize them based on
whether the manifolds are known or learned, and introduce some representative works below.
Known Manifolds. Recent studies have extended the Score SDE formulation to various known manifolds. This
adaptation parallels the generalization of neural ODEs and continuous normalizing flows to Riemannian
manifolds . To train these models, researchers have also adapted score matching and score functions to
Riemannian manifolds.
The Riemannian Score-Based Generative Model (RSGM) accommodates a wide range of manifolds, including
spheres and toruses, provided they satisfy mild conditions. The RSGM demonstrates that it is possible to extend diffusion
models to compact Riemannian manifolds. The model also provides a formula for reversing diffusion on a manifold.
Taking an intrinsic view, the RSGM approximates the sampling process on Riemannian manifolds using a Geodesic
Random Walk. It is trained with a generalized denoising score matching objective.
In contrast, the Riemannian Diffusion Model (RDM) employs a variational framework to generalize the
continuous-time diffusion model to Riemannian manifolds. The RDM uses a variational lower bound (VLB) of the
log-likelihood as its loss function. The authors of the RDM model have shown that maximizing this VLB is equivalent
to minimizing a Riemannian score-matching loss. Unlike the RSGM, the RDM takes an extrinsic view, assuming that
the relevant Riemannian manifold is embedded in a higher dimensional Euclidean space.
Learned Manifolds. According to the manifold hypothesis , most natural data lies on manifolds with significantly reduced intrinsic dimensionality. Consequently, identifying these manifolds and training diffusion models
directly on them can be advantageous due to the lower data dimensionality. Many recent works have built on this
idea, starting by using an autoencoder to condense the data into a lower dimensional manifold, followed by training
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
diffusion models in this latent space. In these cases, the manifold is implicitly defined by the autoencoder and learned
through the reconstruction loss. In order to be successful, it is crucial to design a loss function that allows for the joint
training of the autoencoder and the diffusion models.
The Latent Score-Based Generative Model (LSGM) seeks to address the problem of joint training by pairing a
Score SDE diffusion model with a variational autoencoder (VAE) . In this configuration, the diffusion model is
responsible for learning the prior distribution. The authors of the LSGM propose a joint training objective that merges
the VAE’s evidence lower bound with the diffusion model’s score matching objective. This results in a new lower bound
for the data log-likelihood. By situating the diffusion model within the latent space, the LSGM achieves faster sample
generation than conventional diffusion models. Additionally, the LSGM can manage discrete data by converting it into
continuous latent codes.
Rather than jointly training the autoencoder and diffusion model, the Latent Diffusion Model (LDM) addresses
each component separately. First, an autoencoder is trained to produce a low-dimensional latent space. Then, the
diffusion model is trained to generate latent codes. DALLE-2 employs a similar strategy by training a diffusion
model on the CLIP image embedding space, followed by training a separate decoder to create images based on the CLIP
image embeddings.
Structure-guided Adversarial training of Diffusion Models (SADMs) , for the first time, propose to utilize the
structural information within the sample batch. Specifically, SADMs incorporate an adversarially-trained structural
discriminator to enforce the preservation of manifold structure among samples within each training batch. This approach
leverages the intrinsic data manifold to facilitate the generation of realistic samples, thereby significantly advancing the
capabilities of previous diffusion models in tasks such as image synthesis and cross-domain fine-tuning.
CONNECTIONS WITH OTHER GENERATIVE MODELS
In this section, we first introduce five other important classes of generative models and analyze their advantages and
limitations. Then we introduce how diffusion models are connected with them, and illustrate how these generative
models are promoted by incorporating diffusion models. The algorithms that integrate diffusion models with other
generative models are summarized in Table 2, and we also provide a schematic illustration in Fig. 3.
Large Language Models and Connections with Diffusion Models
Large Language Models (LLMs) have profoundly impacted the AI community, and showcased the
advanced language comprehension and reasoning abilities. Recent works begin to extend their impressive reasoning
abilities to visual generative tasks for overall generation planning. The collaboration between LLMs 
and diffusion models can significantly improve the text-image alignment as well as the quality of
generated images . For instance, RealCompo utilizes LLMs to enhance the compositional generation
of diffusion models by generating images grounded on bounding box layouts from the LLM. EditWorld composes
a set of LLMs and pretrained diffusion models to generate an image editing dataset that contains numerous instructions
with world knowledge . VideoTetris uses the LLM to decompose text prompts along temporal axis for guiding
video generation with smoother and more reasonable transitions. SemanticSDS and Trans4D extend the
planning ability of LLMs to facilitate more complex 3D and 4D diffusion generation. Notably, RPG leverages
the vision-language prior of multimodal LLMs to reason out complementary spatial layouts from text prompt, and
manipulates the object compositions for diffusion models in both text-guided image generation and editing process,
achieving SOTA performance in compositional synthesis scenarios and providing guidance for subsequent research.
Manuscript submitted to ACM
Yang et al.
Table 2. Diffusion models are incorporated into different generative models.
Large Language Model
Zhang et al. 
Yang et al. 
Yang et al. 
Tian et al. 
Yang et al. 
Zeng et al. 
Variational Auto-Encoder
Luo et al. 
Hunag et al. 
Vadhat et al. 
Generative Adversarial Network
Wang et al. 
Yang et al. 
Normalizing Flow
Zhang et al. 
Gong et al. 
kim et al. 
Wang et al. 
Yang et al. 
Autoregressive Model
Meng et al. 
Meng et al. 
Hoogeboom et al. 
Rasul et al. 
Energy-based Model
Gao et al. 
Yu et al. 
Variational Autoencoders and Connections with Diffusion Models
Variational Autoencoders aim to learn both an encoder and a decoder to map input data to values in
a continuous latent space. In these models, the embedding can be interpreted as a latent variable in a probabilistic
generative model, and a probabilistic decoder can be formulated by a parameterized likelihood function. In addition,
the data x is assumed to be generated by some unobserved latent variable z using conditional distribution 𝑝𝜃(x | z),
and 𝑞𝜙(z | x) is used to approximately inference z. To guarantee an effective inference, a variational Bayes approach is
used to maximize the evidence lower bound:
L(𝜙,𝜃; x) = E𝑞(z|x)
log𝑝𝜃(x, z) −log𝑞𝜙(z | x)
with L(𝜙,𝜃; x) ≤log𝑝𝜃(x). Provided that the parameterized likelihood function 𝑝𝜃(x | z) and the parameterized
posterior approximation 𝑞𝜙(z | x) can be computed in a point-wise way and are differentiable with their parameters,
the ELBO can be maximized with gradient descent. This formulation allows flexible choices of encoder and decoder
models. Typically, these models are represented by exponential family distributions whose parameters are generated by
multi-layer neural networks.
The DDPM can be conceptualized as a hierarchical Markovian VAE with a fixed encoder. Specifically, DDPM’s
forward process functions as the encoder, and this process is structured as a linear Gaussian model (as described by
Eq. (2)). The DDPM’s reverse process, on the other hand, corresponds to the decoder, which is shared across multiple
decoding steps. The latent variables within the decoder are all the same size as the sample data.
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Fig. 3. Illustrations of works incorporating diffusion models with other generative models, such as : LLM where a diffusion
model is guided by the LLM planning, VAE where a diffusion model is applied on a latent space, GAN where noise is
injected to the discriminator input, normalizing flow where noise is injected in both forward and backward processes in the
flow, autoregressive model where the training objective is similar to diffusion models, and EBM where a sequence of EBMs
is learned by diffusion recovery likelihood.
In a continuous-time setting, Song et al. , Huang et al. , and Kingma et al. 
demonstrate that the score matching objective may be approximated by the Evidence Lower Bound (ELBO) of a deep
hierarchical VAE. Consequently, optimizing a diffusion model can be seen as training an infinitely deep hierarchical
VAE—a finding that supports the common belief that Score SDE diffusion models can be interpreted as the continuous
limit of hierarchical VAEs.
The Latent Score-Based Generative Model (LSGM) furthers this line of research by illustrating that the ELBO
can be considered a specialized score matching objective in the context of latent space diffusion. Though the crossentropy term in the ELBO is intractable, it can be transformed into a tractable score matching objective by viewing the
score-based generative model as an infinitely deep VAE.
Generative Adversarial Networks and Connections with Diffusion Models
Generative Adversarial Networks (GANs) mainly consist of two models: a generator𝐺and a discriminator 𝐷.
These two models are typically constructed by neural networks but could be implemented in any form of a differentiable
system that maps input data from one space to another. The optimization of GANs can be viewed as a mini-max
optimization problem with value function 𝑉(𝐺, 𝐷):
𝐷Ex∼𝑝data(x) [log 𝐷(x)] + Ez∼𝑝z(z) [log(1 −𝐷(𝐺(z)))].
The generator 𝐺aims to generate new examples and implicitly model the data distribution. The discriminator 𝐷is
usually a binary classifier that is used to identify generated examples from true examples with maximally possible
accuracy. The optimization process ends at a saddle point that produces a minimum about the generator and a maximum
about the discriminator. Namely, the goal of GAN optimization is to achieve Nash equilibrium . At that point, the
generator can be considered that it has captured the accurate distribution of real examples.
Manuscript submitted to ACM
Yang et al.
One of the issues of GAN is the instability in the training process, which is mainly caused by the non-overlapping
between the distribution of input data and that of the generated data. One solution is to inject noise into the discriminator
input for widening the support of both the generator and discriminator distributions. Taking advantage of the flexible
diffusion model, Wang et al. inject noise to the discriminator with an adaptive noise schedule determined
by a diffusion model. On the other hand, GAN can facilitate sampling speed of diffusion models. Xiao et al. 
 show that slow sampling is caused by the Gaussian assumption in the denoising step, which is justified only for
small step sizes. As such, each denoising step is modeled by a conditional GAN, allowing larger step size. To ensure
the diffusion model captures authentic manifold structures in the data distribution, SADM advocates adversarial
training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real
manifold structures from the generated ones.
Normalizing Flows and Connections with Diffusion Models
Normalizing flows are generative models that generate tractable distributions to model high-dimensional
data . Normalizing flows can transform simple probability distribution into an extremely complex probability
distribution, which can be used in generative models, reinforcement learning, variational inference, and other fields.
Existing normalizing flows are constructed based on the change of variable formula . The trajectory in
normalizing flows is formulated by a differential equation. In the discrete-time setting, the mapping from data x to
latent z in normalizing flows is a composition of a sequence of bijections, taking the form of 𝐹= 𝐹𝑁◦𝐹𝑁−1 ◦. . . ◦𝐹1.
The trajectory {x1, x2, . . . x𝑁} in normalizing flows satisfies :
x𝑖= 𝐹𝑖(x𝑖−1,𝜃), x𝑖−1 = 𝐹−1
for all 𝑖≤𝑁.
Similar to the continuous setting, normalizing flows allow for the retrieval of the exact log-likelihood through a
change of variable formula. However, the bijection requirement limits the modeling of complex data in both practical
and theoretical contexts . Several works attempt to relax this bijection requirement . For example,
DiffFlow introduces a generative modeling algorithm that combines the benefits of both flow-based and diffusion
models. As a result, DiffFlow produces sharper boundaries than normalizing flow and learns more general distributions
with fewer discretization steps compared to diffusion probabilistic models. Implicit Nonlinear Diffusion Model (INDM)
 optimizes the pre-encoding process of latent diffusion, which first encodes the original data into the latent space
using normalizing flow, and then performs diffusion in the latent space. Using a non-linear diffusion process, INDM can
effectively improve the likelihood and the sampling speed.
To scale up the training of CNFs, recent works propose efficient simulation-free approaches by parameterizing a vector field which flows from noise samples to data samples. Lipman et al. propose Flow
Matching (FM) to train CNFs based on constructing explicit conditional probability paths between the noise distribution
and each data sample. Wang et al. conduct an in-depth analysis of the essence of rectification in rectified
flow and extend it to rectified diffusion. Besides, they identify that it is not straightness but first-order property
is the essential training target of rectified diffusion with theoretical derivations. Yang et al. further propose
Consistency Flow Matching , a novel FM method that explicitly enforces self-consistency in the velocity field.
Consistency Flow Matching directly defines straight flows starting from different times to the same endpoint,
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
imposing constraints on their velocity values:
L𝜃= 𝐸𝑡∼U𝐸𝑥𝑡,𝑥𝑡+Δ𝑡||𝑓𝜃(𝑡,𝑥𝑡) −𝑓𝜃−(𝑡+ Δ𝑡,𝑥𝑡+Δ𝑡)||2
2 + 𝛼||𝑣𝜃(𝑡,𝑥𝑡) −𝑣𝜃−(𝑡+ Δ𝑡,𝑥𝑡+Δ𝑡)||2
𝑓𝜃(𝑡,𝑥𝑡) = 𝑥𝑡+ (1 −𝑡) ∗𝑣𝜃(𝑡,𝑥𝑡),
where U is the uniform distribution on [0, 1 −Δ𝑡], 𝛼is a positive scalar, Δ𝑡denotes a time interval which is a small and
positive scalar. 𝜃−denotes the running average of past values of 𝜃using exponential moving average (EMA), 𝑥𝑡and
𝑥𝑡+Δ𝑡follows a pre-defined distribution which can be efficiently sampled, for example, VP-SDE or OT path .
In this way, Consistency Flow Matching innovatively bridges consistency models and flow matching models
through the novel concept of straight flows characterized by velocity consistency.
Autoregressive Models and Connections with Diffusion Models
Autoregressive Models (ARMs) work by decomposing the joint distribution of data into a product of conditional
distributions using the probability chain rule:
log𝑝(x1:𝑇) =
log𝑝(𝑥𝑡| x<𝑡)
where x<𝑡is a shorthand for 𝑥1,𝑥2, . . . ,𝑥𝑡−1 . Recent advances in deep learning have facilitated significant
progress for various data modalities , such as images , audio , and text .
Autoregressive models (ARMs) offer generative capabilities through the use of a single neural network. Sampling from
these models requires the same number of network calls as the data’s dimensionality. While ARMs are effective density
estimators, sampling is a continuous, time-consuming process—particularly for high-dimensional data.
The Autoregressive Diffusion Model (ARDM) , on the other hand, is capable of generating arbitrary-order data,
including order-agnostic autoregressive models and discrete diffusion models as special cases . Instead
of using causal masking on representations like ARMs, the ARDM is trained with an effective objective that mirrors
that of diffusion probabilistic models. At the testing stage, the ARDM is able to generate data in parallel—enabling its
application to a range of arbitrary-generation tasks.
Ment et al. incorporates randomized smoothing into autoregressive generative modeling, in order to
improve the sample quality. The original data distribution is smoothed by convolving it with a smooth distribution, e.g.,
a Gaussian or Laplacian kernel. The smoothed data distribution is learned by autoregressive model, and then the learned
distribution is denoised by either applying gradient-based denoising approach or introducing another conditional
autoregressive model. By choosing the level of smoothness appropriately, the proposed method can improve the sample
quality of existing autoregressive models while retaining reasonable likelihoods.
On the other hand, Autoregressive conditional score models (AR-CSM) proposes a score matching method
to model the conditional distribution of autoregressive model. The score function of conditional distribution, i.e.,
∇𝑥𝑡log𝑝(𝑥𝑡| x<𝑡), does not need to be normalized and thus one can use more flexible and advanced neural networks
in the model. Furthermore, the univariate conditional score function can be efficiently estimated, even though the
dimension of original data might be very high. For inference, AR-CSM uses Langevin dynamics that only need the
score function to sample from a density.
Manuscript submitted to ACM
Yang et al.
Energy-based Models and Connections with Diffusion Models
Energy-based Models (EBMs) can be viewed
as one generative version of discriminators , while can be learned from unlabeled input data. Let
x ∼𝑝data(x) denote a training example, and 𝑝𝜃(x) denote a probability density function that aims to approximates
𝑝data(x). An energy-based model is defined as:
exp(𝑓𝜃(x)),
exp(𝑓𝜃(x))𝑑x is the partition function, which is analytically intractable for high-dimensional x. For
images, 𝑓𝜃(x) is parameterized by a convolutional neural network with a scalar output. Salimans et al. 
compare both constrained score models and energy-based models for modeling the score of the data distribution, finding
that constrained score models, i.e., energy based models, can perform just as well as unconstrained models when using
a comparable model structure.
Although EBMs have a number of desirable properties, two challenges remain for modeling high-dimensional
data. First, learning EBMs by maximizing the likelihood requires MCMC method to generate samples from the model,
which can be very computationally expensive. Second, as demonstrated in , the energy potentials learned with
non-convergent MCMC are not stable, in the sense that samples from long-run Markov chains can be significantly
different from the observed samples, and thus it is difficult to evaluate the learned energy potentials. In a recent study,
Gao et al. present a diffusion recovery likelihood method to tractably learn samples from a sequence of
EBMs in the reverse process of the diffusion model. Each EBM is trained with recovery likelihood, which aims to
maximize the conditional probability of the data at a certain noise level, given their noisy versions at a higher noise
level. EBMs maximize the recovery likelihood because it is more tractable than marginal likelihood, as sampling from
the conditional distributions is much easier than sampling from the marginal distributions.
APPLICATIONS OF DIFFUSION MODELS
Diffusion models have recently been employed to address a variety of challenging real-world tasks due to their flexibility
and strength. We have grouped these applications into six different categories based on the task: computer vision,
natural language processing, temporal data modeling, multi-modal learning, robust learning, and interdisciplinary
applications. For each category, we provide a brief introduction to the task, followed by a detailed explanation of how
diffusion models have been applied to improve performance. Table 3 summarizes the various applications that have
made use of diffusion models.
Unconditional and Conditional Diffusion Models
Before we introduce the applications of diffusion models, we illustrate two basic application paradigms of diffusion
models, namely unconditional diffusion models and conditional diffusion models. As a generative model, the history
of diffusion models is very similar to VAE, GAN, flow models, and other generative models. They all first developed
unconditional generation, and then conditional generation followed closely. Unconditional generation is often used
to explore the upper limit of the performance of the generative model, while conditional generation is more about
application-level content because it can enable us to control the generation results according to our intentions. In addition
to promising generation quality and sample diversity, diffusion models are especially superior in their controllability.
The main algorithms of unconditional diffusion models have been well discussed in Sections 2 to 5, in next part, we
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Table 3. Summary of all the applications utilizing the diffusion models.
Computer Vision
 , , , , , , , , 
Super Resolution, Inpainting, Restoration, Translation, and Editing
 , , , , , 
Semantic Segmentation
 , , , 
Video Generation
 , , , , , , , , 
Point Cloud Completion and Generation
 , , , , 
Generating Data from Diffusion Models
 , , 
Natural Language Generation
Natural Language Generation
 , , , , , 
Temporal Data Modeling
Time Series Imputation
 , , , 
Time Series Forecasting
 , , 
Waveform Signal Processing
 , 
Multi-Modal Learning
Text-to-Image Generation
 , , , , , , , , , , , , 
Scene Graph-to-Image Generation
Text-to-3D/4D Generation
 , , , , , 
Text-to-Motion Generation
 , 
Text-to-Video Generation
 , , , , , , 
Text-to-Audio Generation
 , , , , , , 
Robust Learning
Robust Learning
 , , , , , 
Interdisciplinary Applications
Molecular Graph Modeling
 , , , , , , , 
Material Design
 , 
Medical Image Reconstruction
 , , , , , 
mainly discuss how conditional diffusion models are applied to different applications with different forms of conditions,
and choose some typical scenarios for demonstrations.
Conditioning Mechanisms in Diffusion Models. Utilizing different forms of conditions to guide the generation
directions of diffusion models are widely used, such as labels, classifiers, texts, images, semantic maps, graphs and
so on. However, some of the conditions are structural and complex, thus the methods to condition on them are
deserving discussion. There are mainly four kinds of conditioning mechanisms, including concatenation, gradient-based,
cross-attention and adaptive layer normalization (adaLN). The concatenation means diffusion models concatenate
informative guidance with intermediate denoised targets in diffusion process, such as label embedding and semantic
feature maps. The gradient-based mechanism incorporates task-related gradient into the diffusion sampling process for
controllable generation. For example, in image generation, one can train an auxiliary classifier on noisy images, and
then use gradients to guide the diffusion sampling process towards an arbitrary class label. The cross-attention performs
attentional message passing between the guidance and diffusion targets, which is usually conducted in a layer-wise
manner in denoising networks. The adaLN mechanism follows the widespread usage of adaptive normalization layers
 in GANs , Scalable Diffusion Models explores replacing standard layer norm layers in transformer-based
diffusion backbones with adaptive layer normalization. Instead of directly learning dimension-wise scale and shift
parameters, it regresses them from the sum of the time embedding and conditions.
Diffusion with DPO/RLHF. Building on the success of reinforcement learning from human feedback (RLHF)
in Large Language Models (LLMs) , numerous methods in diffusion models have attempted to use similar
approaches for model alignment . Some methods use a pretrained reward model or train a new one to guide the
generation process. For instance, ImageReward manually annotated a large dataset of human-preferred images
and trained a reward model to assess the alignment between images and human preferences. Some methods bypass the
training of a reward model and directly finetune diffusion models on human preference datasets . Diffusion-DPO
 reformulates Direct Preference Optimization (DPO) to account for a diffusion model’s notion of likelihood, utilizing
the evidence lower bound to derive a differentiable objective. Recently, Zhang et al. propose IterComp to
iteratively align the base diffusion model with composition-aware model preferences from the model gallery, consisting
of six powerful open-source diffusion models, effectively enhancing the performance of base model on conditional
Manuscript submitted to ACM
Yang et al.
diffusion generation. As demonstrated in Fig. 4, IterComp outperforms other three types of conditional diffusion
methods while achieving the best inference efficiency.
IterComp (Ours)
A glass sphere sculpture, concealed inside the sphere is a large Pirate Ship in a Lightning storm, large
waves, in the dark, moonlight filters through a nearby window, casting a serene glow.
InstanceDiffusion
The little prince standing on small earth in starry sky, With a bright red scarf and golden hair, he gazes
at the stars, capturing the essence of adventure. by saint exupery, crocheted style.
Text-controlled
LLM-controlled
Layout-controlled
Reward-controlled
5.63 s/Img
23.02 s/Img
15.57 s/Img
9.88 s/Img
A wise and intelligent little girl, her face illuminated by the lights of the night, embodies the universe's
mysteries. Created from constellations and galaxy nebulae, she holds the endless power of a quasar. Her
expression is insightful, as if she understands the depths of a black hole.
In the heart of an enchanted forest, a majestic tree stands illuminated by glowing mushrooms and tiny
lights. Its thick roots form a staircase leading to a cozy door, suggesting a hidden world within. The scene
is vibrant and magical, inviting wonder and exploration in this mystical woodland realm.
Fig. 4. Qualitative comparison between IterComp and three types of compositional generation methods: text-controlled,
LLM-controlled, and layout-controlled approaches. Colored text denotes the advantages of IterComp in generated images.
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Condition Diffusion on Labels and Classifiers. Conditioning diffusion process on the guidance of labels is a
straight way to add desired properties into generated samples. However, when labels are limited, it is difficult to enable
diffusion models to sufficiently capture the whole distribution of data. SGGM proposes a self-guided diffusion
process conditioning on the self-produced hierarchical label set, while You et al. demonstrate large-scale
diffusion models and semi-supervised learners benefit mutually with a few labels via dual pseudo training. Dhariwal
and Nichol proposes classifier guidance to boost the sample quality of a diffusion model by using an extra trained
classifier. Ho and Salimans jointly train a conditional and an unconditional diffusion model, and find that it is
possible to combine the resulting conditional and unconditional scores to obtain a trade-off between sample quality and
diversity similar to that obtained by using classifier guidance.
Condition Diffusion on Texts, Images, and Semantic Maps. Recent researches begin to condition diffusion process
on the guidance of more semantics, such as texts, images, and semantic maps, to better express rich semantics in
samples. DiffuSeq conditions on texts and proposes a seq-to-seq diffusion framework that helps with four NLP
tasks. SDEdit conditions on a styled images to make image-to-image translation, while LDM unifies these
semantic conditions with flexible latent diffusion. Kindly note that if conditions and diffusion targets are of different
modalities, pre-alignment is a practical way to strengthen the guided diffusion. unCLIP and ConPreDiff
 leverage CLIP latents in text-to-image generation, which have align the semantics between images and texts. RPG
 conditions on complementary rectangle and contour regions to enable compositional text-to-image generation
and complex text-guided image editing. ContextDiff proposes a universal forward-backward consistent diffusion
model for better conditioning on various input modalities.
Condition Diffusion on Graphs. Graph-structured data usually exhibits complex relations between nodes, thus
conditioning on graphs are extremely hard for diffusion models. SGDiff proposes the first diffusion model
specifically designed for scene graph to image generation with a novel masked contrastive pre-training. Such masked
pre-training paradigm is general and can be extended to any cross-modal diffusion architectures for both coarse- and finegrained guidance. Other graph-conditioned diffusion models are mainly studied for graph generation. Graphusion 
conditions on the latent clusters of graph dataset to generate new 2D graphs that greatly align with data distribution.
BindDM , IPDiff and IRDiff propose to condition on 3D protein graph to generate 3D molecules with
equivariant diffusion.
Computer Vision
Image Super Resolution, Inpainting, Restoration, Translation, and Editing. Generative models have been used to
tackle a variety of image restoration tasks including super-resolution, inpainting, and translation . Image super-resolution aims to restore high-resolution images from low-resolution inputs, while image
inpainting revolves around reconstructing missing or damaged regions in an image.
Several methods make use of diffusion models for these tasks. For example, Super-Resolution via Repeated Refinement
(SR3) uses DDPM to enable conditional image generation. SR3 conducts super-resolution through a stochastic,
iterative denoising process. The Cascaded Diffusion Model (CDM) consists of multiple diffusion models in
sequence, each generating images of increasing resolution. Both the SR3 and CDM directly apply the diffusion process
to input images, which leads to larger evaluation steps. In order to allow for the training of diffusion models with
limited computational resources, some methods have shifted the diffusion process to the latent space using
Manuscript submitted to ACM
Yang et al.
Fig. 5. Image super resolution results produced by LDM .
pre-trained autoencoders. The Latent Diffusion Model (LDM) streamlines the training and sampling processes for
denoising diffusion models without sacrificing quality.
For inpainting tasks, RePaint features an enhanced denoising strategy that uses resampling iterations to better
condition the image. ConPreDiff proposes a universal diffusion model based on context prediction to consistently
improve unconditional/conditional image generation and image inpainting (see Figure Fig. 6). Meanwhile, Palette 
employs conditional diffusion models to create a unified framework for four image generation tasks: colorization,
inpainting, uncropping, and JPEG restoration. Image translation focuses on synthesizing images with specific desired
Fig. 6. Image inpainting results produced by ConPreDiff .
styles . SDEdit uses a Stochastic Differential Equation (SDE) prior to improve fidelity. Specifically, it begins
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
by adding noise to the input image, then denoises the image through the SDE. Denoising Diffusion Restoration Models
(DDRM) takes advantage of a pre-trained denoising diffusion generative model for solving linear inverse problem,
and demonstrates DDRM’s versatility on several image datasets for super-resolution, deblurring, inpainting, and
colorization under various amounts of measurement noise. Please refer to Section 7.4.1 for more text-to-image
diffusion models.
Semantic Segmentation. Semantic segmentation aims to label each image pixel according to established object
categories. Generative pre-training can enhance the label utilization of semantic segmentation models, and recent
work has shown that representations learned through DDPM contain high-level semantic information that is useful
for segmentation tasks . The few-shot method that leverages these learned representations has outperformed
alternatives such as VDVAE and ALAE . Similarly, Decoder Denoising Pretraining (DDeP) integrates
diffusion models with denoising autoencoders and delivers promising results on label-efficient semantic segmentation. ODISE explores diffusion models for open-vocabulary segmentation tasks, and proposes a novel implicit
captioner to generate captions for images for better utilizing pre-trained large-scale text-to-image diffusion models.
Video Generation. Generating high-quality videos remains a challenge in the deep learning era due to the
complexity and spatio-temporal continuity of video frames . Recent research has turned to diffusion models to
improve the quality of generated videos . For example, the Flexible Diffusion Model (FDM) uses a generative
model to allow for the sampling of any arbitrary subset of video frames, given any other subset. The FDM also includes a
specialized architecture designed for this purpose. Additionally, the Residual Video Diffusion (RVD) model utilizes
an autoregressive, end-to-end optimized video diffusion model. It generates future frames by amending a deterministic
next-frame prediction, using a stochastic residual produced through an inverse diffusion process. Please refer to
Section 7.4.5 for more text-to-video diffusion models.
Generating Data from Diffusion Models. Synthesizing datasets from generative models can effectively advance
various tasks like classification . Recent works have begun to utilize diffusion models to achieve this goal
for vision tasks. For example, Trabucco et al. adopt diffusion models to make effective data augmentation for fewshot image classification. DistDiff proposes a training-free data expansion framework with a distribution-aware
diffusion model. It constructs hierarchical prototypes to approximate the real data distribution, and optimizes latent
data points in generation process with hierarchical energy guidance. InstructPix2Pix leverages two large pretrained
models (i.e., GPT-3 and Stable Diffusion) to generate a large dataset of input-goal-instruction triplet examples, and
trains an instruction-following image editing model on the dataset. To enable image editing to reflect chalenging world
knowledge and dynamics from both real physical world and virtual media, EditWorld , introduces a new task
named world-instructed image editing, as the data examples presented in Fig. 7. EditWorld proposes an innovative
compositional framework with a set of pretrained LLMs and Diffusion Models, illustrated in Fig. 8, to synthesize a
world-instructed training dataset for instruction-following image editing.
Point Cloud Completion and Generation. Point clouds are a critical form of 3D representation for capturing
real-world objects. However, scans often generate incomplete point clouds due to partial observation or self-occlusion.
Recent studies have applied diffusion models to address this challenge, using them to infer missing parts in order
to reconstruct complete shapes. This work has implications for many downstream tasks such as 3D reconstruction,
augmented reality, and scene understanding .
Manuscript submitted to ACM
Yang et al.
What happens if a
hole appears in
the balloon?
Turn the pocket
watch on its back.
Training Data Examples
What would it be like
if they got married
and 60 years have
Melting the
snowman with a
hairdryer.
Generated Results of EditWorld
Melting the
snowman with a
hairdryer.
Melting the
snowman with a
hairdryer.
Generated Results of InstructPix2Pix
Generated Results of MagicBrush
Fig. 7. Comparing EditWorld with InstructPix2Pix and MagicBrush.
(a) Text-to-Image Generation for Diverse Scenes
(b) Extracting Paired Data from Realistic Video Frames
Now you are an "textual prompt creator", Please provide
several examples based on real-world physical conditions,
each example should sequentially include an initial image
description, a final image description, image change
instructions, and keywords. Here's one example: The initial
image description is "{input_text}", the image change
instruction is "{instruct}", the final image description is
"{output_text}", and the keywords are "{keywords}". Please
present the examples in the format of "1. {input_text};
{instruct}; {output_text}; {keywords}\n2. ...".
Video-LLava
image pair
Provide the Instructions based on the provided Descriptions,
for example, the Description is {Description}, Then provide
the {instruct}.
“Describe this video”
Select Image Pairs
from Video
input text
output text
Description
Requirment
instructions
Instructions
Instructions
Combinational T2I Synthesis
ControlNet
IP-Adapter
image pair
Fig. 8. EditWorld generates a training dataset of world-instructed image editing from two different branches.
Luo et al. 2021 has taken the approach of treating point clouds as particles in a thermodynamic system, using
a heat bath to facilitate diffusion from the original distribution to a noise distribution. Meanwhile, the Point-Voxel
Diffusion (PVD) model joins denoising diffusion models with the pointvoxel representation of 3D shapes. The
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
q'(z|X(0))
Shape Latent
Fig. 9. The directed graphical model of the diffusion process for point clouds .
Point Diffusion-Refinement (PDR) model uses a conditional DDPM to generate a coarse completion from partial
observations; it also establishes a point-wise mapping between the generated point cloud and the ground truth.
Anomaly Detection. Anomaly detection is a critical and challenging problem in machine learning and
computer vision . Generative models have been shown to own a powerful mechanism for anomaly detection
 , modeling normal or healthy reference data. AnoDDPM utilizes DDPM to corrupt the input image
and reconstruct a healthy approximation of the image. These approaches may perform better than alternatives based
on adversarial training as they can better model smaller datasets with effective sampling and stable training schemes.
DDPM-CD incorporates large numbers of unsupervised remote sensing images into the training process through
DDPM. Changes of remote sensed images is detected by utilizing a pre-trained DDPM and applying the multi-scale
representations from the diffusion model decoder.
Natural Language Generation
Natural language processing aims to understand, model, and manage human languages from different sources such
as text or audio. Text generation has become one of the most critical and challenging tasks in natural language
processing . It aims to compose plausible and readable text in the human language given input data (e.g.,
a sequence and keywords) or random noise. Numerous approaches based on diffusion models have been developed
for text generation. Discrete Denoising Diffusion Probabilistic Models (D3PM) introduces diffusion-like generative
models for character-level text generation . It generalize the multinomial diffusion model through going
beyond corruption processes with uniform transition probabilities. Large autoregressive language models (LMs) is
able to generate high-quality text . To reliably deploy these LMs in real-world applications, the text
generation process is usually expected to be controllable. It means we need to generate text that can satisfy desired
requirements (e.g., topic, syntactic structure). Controlling the behavior of language models without re-training is a
major and important problem in text generation . Analog Bits generates the analog bits to represent the
discrete variables and further improves the sample quality with self-conditioning and asymmetric time intervals.
Although recent methods have achieved significant successes on controlling simple sentence attributes (e.g., sentiment) , there is little progress on complex, fine-grained controls (e.g., syntactic structure). In order to tackle
more complex controls, Diffusion-LM proposes a new language model based on continuous diffusion. Diffusion-
LM starts with a sequence of Gaussian noise vectors and incrementally denoises them into vectors corresponding to
words. The gradual denoising steps help produce hierarchical continuous latent representations. This hierarchical and
continuous latent variable can make it possible for simple, gradient-based methods to accomplish complex control.
Manuscript submitted to ACM
Yang et al.
Similarly, DiffuSeq also conducts diffusion process in latent space and proposes a new conditional diffusion model to
accomplish more challenging text-to-text generation tasks. Ssd-LM performs diffusion on the natural vocabulary
space instead of a learned latent space, allowing the model to incorporate classifier guidance and modular control
without any adaptation of off-the-shelf classifiers. CDCD proposes to model categorical data (including texts) with
diffusion models that are continuous both in time and input space, and designs a score interpolation technique for
optimization.
Multi-Modal Generation
Text-to-Image Generation. Vision-language models have attracted a lot of attention recently due to the number of
potential applications . Text-to-Image generation is the task of generating a corresponding image from a descriptive
text . Blended diffusion utilizes both pre-trained DDPM and CLIP models, and it proposes a
solution for region-based image editing for general purposes, which uses natural language guidance and is applicable
to real and diverse images. On the other hand, unCLIP (DALLE-2) proposes a two-stage approach, a prior model
that can generate a CLIP-based image embedding conditioned on a text caption, and a diffusion-based decoder that can
generate an image conditioned on the image embedding. Recently, Imagen proposes a text-to-image diffusion
model and a comprehensive benchmark for performance evaluation. It shows that Imagen performs well against the
state-of-the-art approaches including VQ-GAN+CLIP , Latent Diffusion Models , and DALL-E 2 . Inspired
Ours Imagen LDM
“ A waterfall is in a lush
rainforest teeming with
vibrant vegetation.”
“ A scientist in a
laboratory, surrounded
by equipment and notes.”
“ A woman is dancing
with a white background
wall featuring graffiti of a
“ A cozy fireplace with
crackling flames, a
hearth rug, and a
stack of logs nearby.”
“ An Italian espresso with
latte art in the shape of a
heart, accompanied by a
saucer and a spoon.”
Fig. 10. Synthesis examples demonstrating text-to-image capabilities of for various text prompts with LDM, Imagen, and
ContextDiff .
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
by the ability of guided diffusion models to generate photorealistic samples and the ability of text-to-image
models to handle free-form prompts, GLIDE applies guided diffusion to the application of text-conditioned
image synthesis. VQ-Diffusion proposes a vector-quantized diffusion model for text-to-image generation, and it
eliminates the unidirectional bias and avoids accumulative prediction errors. Versatile Diffusion proposes the first
unified multi-flow multimodal diffusion framework, which supports image-to-text, image-variation, text-to-image, and
text-variation, and can be further extended to other applications such as semantic-style disentanglement, image-text
dual-guided generation, latent image-to-text-to-image editing, and more. Following Versatile Diffusion, UniDiffuser
 proposes a unified diffusion model framework based on Transformer, which can fit multimodal data distributions
and simultaneously handle text-to-image, image-to-text, and joint image-text generation tasks. ConPreDiff for the
first time incorporates context prediction into text-to-image diffusion models, and significantly improves generation
performance without additional inference costs. ContextDiff proposes general contextualized diffusion model by
incorporating the cross-modal context encompassing interactions and alignments into forward and reverse processes.
A qualitative comparison between these models are presented in Fig. 10.
User Text (Base) Prompt:
A nobleman is standing
under a bright moon while
two owls are on a big oak.
Identify key phrases:
1、A noble man
2、A bright moon
3、Two owls
4、A big oak
Text-to-Text Recaptioning
1. A nobleman, regal and distinguished, with a
sharp gaze, dressed in a velvet doublet,
standing proudly in his ancestral castle
2. A bright moon, full and radiant, casting a silver
glow over a tranquil lake, serene and majestic
in the night sky.
3. Two owls, perched side by side, wise and
mysterious, with piercing eyes, on an ancient,
gnarled branch under the starlit sky
4. A big oak, towering and robust, its sprawling
branches a testament to centuries, leaves
whispering stories in the gentle breeze.
Rationale Generation
## Task objectives: Generate complementary
subregions based on the recaptioned subprompts
and assign them to subregions.
[Instructions + In-context examples]
[Question]:
Subprompts: 1. A nobleman, regal and …..
2. A bright moon, full and radiant ……
3. Two owls, perched side by side ……
4. A big oak, towering and robust ……
Let’s think step by step (trigger CoT reasoning)
Rationale: Here we have 4 subprompts, thus total
4 regions. First, we take the nobleman and his
castle as foreground and background, we place
them in lower left of the image. Next, bright moon
should in the sky so we assign the moon to the
higher left corner…… (Coarse grained area division)
A nobleman
Recaption-Plan-Generate (RPG) for
Text-to-Image Generation
Stage 1: Recaption
Stage 2: Plan
Stage 3: Generate
Base Prompt
Complementary Regional Diffusion
Weighted sum for each sampling step
Region-wise
Generation
Chain-of-Thought
Subregion Planning
Therefore, the region split ratio should be:
1,1,1;2,2,3 （Column Mode）
[Assign subprompts to subregions]
Region 0：A bright moon, full and radiant……
Region 1：Two owls, perched side by side ……
Region 2：A nobleman, regal and …..
Region 3：A big oak, towering and robust ……
CoT Reasoning
Fig. 11. Overview of RPG framework for text-to-image generation.
A new interesting line of diffusion model research is to leverage the pre-trained text-to-image diffusion model
for more complex or fine-grained control of synthesis results. DreamBooth presents the first technique that
tackles the new challenging problem of subject-driven generation, allowing users, from just a few casually captured
images of a subject, to recontextualize subjects, modify their properties, original art renditions, and more. Different
from those image diffusion models conditioned on text prompts, ControlNet attempts to control pre-trained
large diffusion models to support additional semantic maps, like edge maps, segmentation maps, keypoints, shape
normals, depths, etc. However, most methods often face challenges when handling complex text prompts involving
multiple objects with multiple attributes and relationships. To this end, RPG proposes a brand new training-free
text-to-image generation/editing framework harnessing the powerful chain-of-thought reasoning ability of multimodal
LLMs to enhance the compositionality of text-to-image diffusion models. This new RPG framework unifies both
text-guided image generation (in Fig. 11) and image editing (in Fig. 12) tasks in a closed-loop fashion. Notably, as
demonstrated in Fig. 13, RPG outperforms all SOTA methods, such as SDXL and DALL-E 3 , demonstrate its
Manuscript submitted to ACM
Yang et al.
User Text Prompt:
Six patterned mugs, arranged in
two columns, on a marble surface,
and a rose in the vase on the left.
Task Instruction +
In-context Examples +
User Text Prompt
Edited Image
Multi-Round Editing with RPG
Image-to-Text Recaptioning
## Task objectives: Analyze user text prompt to identify key
phrases and attributes, utilizing visual reasoning on the
image to detect inconsistency
[Task instruction+ In-context examples ]
[Question]
User text prompt: Six patterned mugs, arranged in two columns,
on a marble surface, and a rose in the vase on the left.
Task description: Analyze the prompt and recaption the image,
find inconsistency between the prompt and the recaption prompt,
make an edit plan, Let’s think step by step (trigger CoT reasoning):
Recaption the image: Eight patterned mugs, six mugs arranged in
two columns on a marble surface, and two roses in the two mugs
on the left lower corner.
Stage 1: Recaption
Editing Planning:
Rationale Generation：
1. More mugs staggered （delete）
2. More roses exist (delete)
3. There is no vase visible in the image (replace or add)
Edit plan:
According to the inconsistency,
(i) Firstly, we delete the mug on the right down corner along with its reflection image.
(ii) Next, we delete the rose in the first column from left to right along with its reflection image.
(iii) Finally, here is only one inconsistency, we should replace the mug in the left lower corner
with a vase along with its reflection image.
Stage 2: Plan
Stage 3: Generate
Another Loop or Exit
Precise Contours
Region-wise Editing
Chain-of-Thought
Inpainting
Source/Generated Image
Fig. 12. RPG can unify text-guided image generation and editing in a closed-loop approach.
superiority. Furthermore, RPG framework is user-friendly, and can generalize to different MLLM architectures and
diffusion backbones (e.g., ControlNet).
Scene Graph-to-Image Generation. Despite text-to-image generation models has made exciting progress from
natural language descriptions, they struggle to faithfully reproduce complex sentences with many objects and relationships. Generating images from scene graphs (SGs) is an important and challenging task for generative models .
Traditional methods mainly predict an image-like layout from SGs, then generate images based on the
layout. However, such intermediate representations would lose some semantics in SGs, and recent diffusion models
 are also unable to address this limitation. SGDiff proposes the first diffusion model specifically for image
generation from scene graphs (Fig. 14), and learns a continuous SG embedding to condition the latent diffusion model,
which has been globally and locally semantically-aligned between SGs and images by the designed masked contrastive
pre-training. SGDiff can generate images that better express the intensive and complex relations in SGs compared with
both non-diffusion and diffusion methods. However, high-quality paired SG-image datasets are scarce and small-scale,
how to leverage large-scale text-image datasets to augment the training or provide a semantic diffusion prior for better
initialization is still an open problem.
Text-to-3D Generation. 3D content generation has been in high demand for a wide range of
applications, including gaming, entertainment, and robotics simulation. Augmenting 3D content generation with natural
language could considerably help with both novices and experienced artists. DreamFusion adopts a pre-trained 2D
text-to-image diffusion model to perform text-to-3D synthesis. It optimizes a randomly-initialized 3D model (a Neural
Radiance Field, or NeRF) with a probability density distillation loss, which utilizes a 2D diffusion model as a prior for
optimization of a parametric image generator. To obtain fast and high-resolution optimization of NeRF, Magic3D 
proposes a two-stage diffusion framework built on cascaded low-resolution image diffusion prior and high-resolution
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Prompt: A beautiful landscape with a river in the middle, the left of the river is in the evening and in the winter with a big iceberg and a
small village while some people are skiing on the river and some people are skating, the right of the river is in the summer with a volcano
in the morning and a small village while some people are playing.
RPG (Ours)
Left Prompt: A Chinese general wearing a crown, with whiskers and golden Chinese style armor, standing with a majestic dragon head on
his chest, symbolizing his strength, wearing black and gold boots. His appearance exudes a sense of authority, wisdom, and an unyielding
spirit , embodying the ideal ancient Chinese hero.
Right Prompt: This painting is a quintessential example of ancient Chinese ink art , At the top of the painting , towering mountains shrouded
in mist rise majestically. The mountains‘ craggy peaks are sketched with fine , precise lines , typical of traditional Chinese ink art. A slender
swirling mists, meandering waterfall begins its descent here , its water appearing almost ethereal amidst the soft. In the middle section, the
waterfall cascades energetically , creating a dynamic contrast with the serene mountains above. Lush pine trees , rendered with graceful ,
flowing brush strokes , flank the waterfall. These trees appear to dance with the rhythm of the water , adding a vibrant life to the scene. At
the bottom , the waterfall concludes its journey in a tranquil pool. The water's surface is calm , reflecting the surrounding nature and the sky
above. Here , delicate flowers and small shrubs are depicted along the water's edge , symbolizing peace and harmony with nature.
RPG (Ours)
RPG (Ours)
Fig. 13. Compared to previous SOTA models, RPG exhibits a superior ability to express intricate and compositional
text prompts within generated images (colored text denotes critical part).
Manuscript submitted to ACM
Yang et al.
Fig. 14. SGDiff leverages masked contrastive pre-training for scene graph-based image diffusion generation.
latent diffusion prior. In order to achieve high-fidelity 3D creation, Make-It-3D optimizes a neural radiance field
by incorporating constraints from the reference image at the frontal view and diffusion prior at novel views, enhancing
the coarse model into textured point clouds and increasing realism with diffusion prior and high-quality textures from
the reference image. ProlificDreamer presents Variational Score Distillation (VSD), optimizing a distribution of
3D scenes based on textual prompts as random variables to closely align the distribution of rendered images from
all perspectives with a pretrained 2D diffusion model, using KL divergence as the measure. IPDreamer further
proposes a novel 3D object synthesis framework that enables users to create controllable and high-quality 3D objects
effortlessly. It excels in synthesizing a high-quality 3D object which can greatly align with a provided complex image
Modeling compositional 3D data distribution is a fundamental and critical task for generative models. Current
feed-forward methods are primarily capable of generating single objects and face challenges when creating
more complex scenes containing multiple objects due to limited training data. Recently, a series of learnable-layout
compositional methods have been proposed . These methods combine multiple object-ad-hoc
radiance fields and then optimize the positions of the radiance fields from external feedback. For example, Epstein
et al. propose learning a distribution of reasonable layouts based solely on the knowledge from a large pre-trained
text-to-image model. Vilesov et al. introduce an optimization method based on Monte-Carlo sampling and
physical constraints. However, these forms of layout guidance are relatively coarse and not expressive enough for
fine-grained control. Yang et al. address this problem by incorporating semantic embeddings that ensure
view consistency and distinctly differentiate objects into SDS processes (namely SemanticSDS ), which are flexible
and expressive for optimizing 3D scenes. As shown in Fig. 15, SemanticSDS can achieve superior precision and
quality in compositional text-to-3D generation over existing methods.
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
GraphDreamer
LucidDreamer
A rabbit sits atop a large, expensive watch with many shiny gears, made half of
iron and half of gold, eang a birthday cake that is in front of the rabbit
A car with the front right side made of cheese, the front le side made of sushi,
and the back made of LEGO.
A mannequin adorned with a dress made of feathers and moss stands at the center,
ﬂanked by a vase with a single blue tulip and another with blue roses.
SemanticSDS
Fig. 15. SemanticSDS achieves superior compositional text-to-3d generation results over state-of-the-art baselines, particularly
in generating multiple objects with diverse attibutes.
Text-to-Motion Generation. Human motion generation is a fundamental task in computer animation, with
applications covering from gaming to robotics . The generate motion is usually a sequence of human poses
Manuscript submitted to ACM
Yang et al.
represented by joint rotations and positions. Motion Diffusion Model (MDM) adapts a classifier-free diffusionbased generative model for the human motion generation, which is transformer-based, combining insights from motion
generation literature, and regularizes the model with geometric losses on the locations and velocities of the motion.
FLAME involves a transformer-based diffusion to better handle motion data, which manages variable-length
motions and well attend to free-form text. Notably, it can edit the parts of the motion, both frame-wise and joint-wise,
without any fine-tuning.
Text-to-Video Generation. Tremendous recent progress in text-to-image diffusion-based generation motivates
the development of text-to-video generation . Make-A-Video proposes to extend a diffusion-based
text-to-image model to text-to-video through a spatiotemporally factorized diffusion model. It leverages joint text-image
prior to bypass the need for paired text-video data, and further presents super-resolution strategies for high-definition,
high frame-rate text-to-video generation. Imagen Video generates high definition videos by designing a cascaded
video diffusion models, and transfers some findings that work well in the text-to-image setting to video generation,
including frozen T5 text encoder and classifier-free guidance. Tune-A-Video introduces one-shot video tuning for
text-to-video generation, which eliminates the burden of training with large-scale video datasets. It employs efficient
attention tuning and structural inversion to significantly enhance temporal consistency. Text2Video-Zero achieves
zero-shot text-to-video synthesis using a pretrained text-to-image diffusion model, ensuring temporal consistency
through motion dynamics in latent codes and cross-frame attention. Its goal is to enable affordable text-guided video
generation and editing without additional fine-tuning. FateZero is the first framework for temporal-consistent
zero-shot text-to-video editing using pre-trained text-to-image diffusion model. It fuses the attention maps in the DDIM
inversion and generation processes to maximally preserve the consistency of motion and structure during editing.
ContextDiff incorporates the cross-modal context information about the interactions between text condition and
video sample into forward and reverse processes, forming a forward-backward consistent video diffusion model for
text-to-video generation.
Most of text-to-video diffusion models are trained on fixed-size video datasets, and thus are often limited to generating
a relatively small number of frames, leading to significant degradation in quality when tasked with generating longer
videos. Several advancements have sought to overcome this limitation through various strategies.
Vlogger employs a masked diffusion model for conditional frame input facilitating longer video generation, and
StreamingT2V utilizes a ControlNet-like conditioning mechanism to enable auto-regressive video generation.
Recent VideoTetris introduces a Spatio-Temporal Compositional Diffusion method for handling scenes with
multiple objects and following progressive complex prompts (i.e., compositional text-to-video generation). Besides,
VideoTetris develops a new video data preprocessing method and a consistency regularization method called Reference
Frame Attention to improve auto-regressive long video generation through enhanced motion dynamics and prompt
semantics. Qualitative comparisons in Fig. 16 show that VideoTetris not only generates superior quality compositional
videos, but also produces high-quality long videos that align with compositional prompts while maintaining the best
consistency.
Text-to-Audio Generation. Text-to-audio generation is the task to transform normal language texts to voice
outputs . Grad-TTS presents a novel text-to-speech model with a score-based decoder and diffusion
models. It gradually transforms noise predicted by the encoder and is further aligned with text input by the method of
Monotonic Alignment Search . Grad-TTS2 improves Grad-TTS in an adaptive way. Diffsound presents
a non-autoregressive decoder based on the discrete diffusion model , which predicts all the mel-spectrogram
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
（a) Video Generation with Compositional Prompts
A heroic robot on the le, and a magical girl
on the right are saving the day.
A handsome young man is drinking coﬀee on a wooden table.
---------> (transi+ons to)
A handsome young man and a beau:ful young lady on his le, are drinking coﬀee on a wooden table.
VideoCrafter2
AnimateDiff
ModelScope
A cute brown dog and a sleepy cat
are napping in the sun.
StreamingT2V
A cute brown squirrel in Antarctica, on a pile of hazelnuts cinematic.
---------> (transitions to)
A cute brown squirrel and a cute white squirrel in Antarctica, on a pile of hazelnuts cinematic
StreamingT2V
(b) Long Video Genera9on with Progressive Composi9onal Prompts
Fig. 16. Comparing VideoTetris with open-sourced or commercial T2V models in short and long video generation.
Manuscript submitted to ACM
Yang et al.
Historical
Time Series
Prediction
References
𝒙𝒕−𝟏= 𝝁𝜽(𝒙𝒕 , 𝒙𝑹, 𝒙𝑯, 𝑰𝒔, 𝒕)
Main Framework
Historical
Time Series
Similarity
Feature Set
Pre-trained
References
Retrieval Mechanism
Fig. 17. Overview of retrieval-augmented diffusion models for time series forecasting (RATD ). The historical time series 𝑥𝐻is
inputted into the retrieval module to for the corresponding references 𝑥𝑅. After that, 𝑥𝐻is concatenated with the noise as the main
input for the model 𝜇𝜃. 𝑥𝑅will be utilized as the guidance for the denoising process.
tokens in every single step, and then refines the predicted tokens in the following steps. EdiTTS leverages the
score-based text-to-speech model to refine a mel-spectrogram prior that is coarsely modified. Instead of estimating the
gradient of data density, ProDiff parameterizes the denoising diffusion model by directly predicting the clean data.
Temporal Data Modeling
Time Series Imputation. Time series data are widely used with many important real-world applications . Nevertheless, time series usually contain missing values for multiple reasons, caused by mechanical or artificial
errors . Recent years, imputation methods have been greatly for both deterministic imputation 
and probabilistic imputation , including diffusion-based approaches. Conditional Score-based Diffusion models for
Imputation (CSDI) presents a novel time series imputation method that leverages score-based diffusion models.
Specifically, for the purpose of exploiting correlations within temporal data, it adopts the form of self-supervised training
to optimize diffusion models. Its application in some real-world datasets reveals its superiority over previous methods.
Controlled Stochastic Differential Equation (CSDE) proposes a novel probabilistic framework for modeling
stochastic dynamics with a neural-controlled stochastic differential equation. Structured State Space Diffusion (SSSD)
 integrates conditional diffusion models and structured state-space models to particularly capture long-term
dependencies in time series. It performs well in both time series imputation and forecasting tasks.
Time Series Forecasting. Time series forecasting is the task of forecasting or predicting the future value over a
period of time. Neural methods have recently become widely-used for solving the prediction problem with univariate
point forecasting methods or univariate probabilistic methods . In the multivariate setting, we also have
point forecasting methods as well as probabilistic methods, which explicitly model the data distribution using
Gaussian copulas , GANs , or normalizing flows . TimeGrad presents an autoregressive model for
forecasting multivariate probabilistic time series, which samples from the data distribution at each time step through
estimating its gradient. It utilizes diffusion probabilistic models, which are closely connected with score matching and
energy-based methods. Specifically, it learns gradients by optimizing a variational bound on the data likelihood and
transforms white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling
 during inference time. To handle complex time series forecasting, as illustrated in Fig. 17, Liu et al. for the
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
first time introduce Retrieval- Augmented Time series Diffusion (RATD) , allowing for greater utilization of the
dataset and providing meaningful guidance in the denoising process.
Waveform Signal Processing. In electronics, acoustics, and some related fields, the waveform of a signal is denoted
by the shape of its graph as a function of time, independent of its time and magnitude scales. WaveGrad introduces
a conditional model for waveform generation that estimates gradients of the data density. It receives a Gaussian white
noise signal as input and iteratively refines the signal with a gradient-based sampler. WaveGrad naturally trades
inference speed for sample quality by adjusting the number of refinement steps, and make a connection between nonautoregressive and autoregressive models with respect to audio quality. DiffWave presents a versatile and effective
diffusion probabilistic model for conditional or unconditional waveform generation. The model is non-autoregressive
and is efficiently trained by optimizing a variant of variational bound on the data likelihood. Moreover, it produces
high-fidelity audio in different waveform generation tasks, such as class-conditional generation and unconditional
generation.
Robust Learning
Robust learning is a class of defense methods that help learning networks that are robust to adversarial perturbations
or noises . While adversarial training is viewed as a standard defense method against
adversarial attacks for image classifiers, adversarial purification has shown significant performances as an alternative
defense method , which purifies attacked images into clean images with a standalone purification model. Given
an adversarial example, DiffPure diffuses it with a small amount of noise following a forward diffusion process
and then restores the clean image with a reverse generative process. Adaptive Denoising Purification (ADP) 
demonstrates that an EBM trained with denoising score matching can effectively purify attacked images within
just a few steps. It further proposes an effective randomized purification scheme, injecting random noises into images
before purification. Projected Gradient Descent (PGD) presents a novel stochastic diffusion-based pre-processing
robustification, which aims to be a model-agnostic adversarial defense and yield a high-quality denoised outcome. In
addition, some works propose to apply a guided diffusion process for advanced adversarial purification .
Interdisciplinary Applications
Drug Design and Life Science. Graph Neural Networks and corresponding representation
learning techniques have achieved great success in many areas, including modeling
molecules/proteins in various tasks ranging from property prediction to molecule/protein generation , where a molecule is naturally represented by a node-edge graph. On one hand, recent works propose to
pre-train GNN/transformer specifically for molecules/proteins with biomedical or physical insights ,
and achieve remarkable results. On the other hand, more works begin to utilize graph-based diffusion models for
enhancing molecule or protein generation. Torsional diffusion presents a new diffusion framework that makes
operations on the space of torsion angles with a diffusion process on the hyperspace and an extrinsic-to-intrinsic scoring
model. GeoDiff demonstrates that Markov chains evolving with equivariant Markov kernels can produce an
invariant distribution, and further design blocks for the Markov kernels to preserve the desirable equivariance property.
There are also other works incorporate the equivariance property into 3D molecule generation and protein
generation . Motivated by the classical force field methods for simulating molecular dynamics, ConfGF 
directly estimates the gradient fields of the log density of atomic coordinates in molecular conformation generation.
Manuscript submitted to ACM
Yang et al.
Fig. 18. IPDiff incorporates protein-ligand interactions into both forward and reverve processes of molecular
diffusion models.
Recently, given a target protein, the design of 3D small drug molecules that can closely bind to the target begins
to be promoted by diffusion models . IPDiff proposes a novel 3D molecular diffusion model for
structure-based drug design (SBDD). As illustrated in Fig. 18, the pocket-ligand interaction is explicitly considered in
both forward and reverse processes with the proposed prior-conditioning and prior-shifting mechanisms. Notably,
IPDiff beats all previous diffusion-based and autoregressive generation models regarding binding-related metrics and
molecular properties. BindDM proposes a hierarchical complex-subcomplex diffusion model for SBDD tasks,
which incorporates essential binding-adaptive subcomplex for 3D molecule diffusion generation. IRDiff proposes
an interaction-based retrieval-augmented 3D molecular diffusion model named IRDIFF for SBDD tasks. As deonstrated
in Fig. 19, this model guides 3D molecular generation using informative external target-aware references, designing
two novel augmentation mechanisms, i.e., retrieval augmentation and self augmentation, to incorporate essential
protein-molecule binding structures for target-aware molecular generation.
There are also studies that use diffusion models for protein generation, such as DiffAb. DiffAb proposes for
the first time a diffusion-based 3D antibody design framework that models both the sequence and structure of the
complementarity-determining regions (CDRs) that determine antibody complementarity. Experiments show that DiffAb
can be used for various antibody design tasks, such as jointly generating sequence-structure, designing CDRs with
fixed frameworks, and optimizing antibodies. SMCDiff proposes to first learn a distribution over diverse and
longer protein backbone structures via an E(3)-equivariant graph neural network, and then efficiently samples scaffolds
from this distribution given a motif. The generation results demonstrates the designed backbones is well aligned with
AlphaFold2-predicted structures.
Material Design. Solid state materials are the critical foundation of numerous key technologies . Crystal
Diffusion Variational Autoencoder (CDVAE) incorporates stability as an inductive bias by proposing a noise
conditional score network, which simultaneously utilizes permutation, translation, rotation, and periodic invariance
Manuscript submitted to ACM
Diffusion Models: A Comprehensive Survey of Methods and Applications
Fig. 19. IRDiff designs a interaction-based retrieval-augmented generation frameowrk for SBDD.
properties. Luo et al. model sequences and structures of complementarity-determining regions with
equivariant diffusion, and explicitly target specific antigen structures to generate antibodies at atomic resolution.
Medical Image Reconstruction. An inverse problem is to recover an unknown signal from observed measurements,
and it is an important problem in medical image reconstruction of Computed Tomography (CT) and Magnetic Resonance
Imaging (MRI) . Song et al. utilize a score-based generative model to reconstruct
an image consistent with both the prior and the observed measurements. Chung et al. train a continuous
time-dependent score function with denoising score matching, and iterate between the numerical SDE solver and data
consistency step for reconstruction at the evaluation stage. Peng et al. perform MR reconstruction by
gradually guiding the reverse-diffusion process given observed k-space signal, and propose a coarse-to-fine sampling
algorithm for efficient sampling.
FUTURE DIRECTIONS
Research on diffusion models is in its early stages, with much potential for improvement in both theoretical and
empirical aspects. As discussed in early sections, key research directions include efficient sampling and improved
likelihood, as well as exploring how diffusion models can handle special data structures, interface with other types of
generative models, and be tailored to a range of applications. In addition, we foresee that future research on diffusion
models will likely expand to the following avenues.
Revisiting Assumptions. Numerous typical assumptions in diffusion models need to be revisited and analyzed. For
example, the assumption that the forward process of diffusion models completely erases any information in data
and renders it equivalent to a prior distribution may not always hold. In reality, complete removal of information is
unachievable in finite time. It is of great interest to understand when to halt the forward noising process in order
to strike a balance between sampling efficiency and sample quality . Recent advances in Schrödinger bridges
Manuscript submitted to ACM
Yang et al.
and optimal transport provide promising alternative solutions, suggesting new formulations for
diffusion models that are capable of converging to a specified prior distribution in finite time.
Theoretical Understanding. Diffusion models have emerged as a powerful framework, notably as the only one that
can rival generative adversarial networks (GANs) in most applications without resorting to adversarial training. Key
to harnessing this potential is an understanding of why and when diffusion models are effective over alternatives
for specific tasks. It is important to identify which fundamental characteristics differentiate diffusion models from
other types of generative models, such as variational autoencoders, energy-based models, or autoregressive models.
Understanding these distinctions will help elucidate why diffusion models are capable of generating samples of excellent
quality while achieving top likelihood. Equally important is the need to develop theoretical guidance for selecting and
determining various hyperparameters of diffusion models systematically.
Latent Representations. Unlike variational autoencoders or generative adversarial networks, diffusion models are less
effective for providing good representations of data in their latent space. As a result, they cannot be easily used for
tasks such as manipulating data based on semantic representations. Furthermore, since the latent space in diffusion
models often possesses the same dimensionality as the data space, sampling efficiency is negatively affected and the
models may not learn the representation schemes well .
AIGC and Diffusion Foundation Models. From Stable Diffusion to ChatGPT, Artificial Intelligence Generated Content
(AIGC) has gained much attention in both academic and industrial circles. Generative Pre-Training is the core technique
in GPT-1/2/3/4 and (Visual) ChatGPT , which exhibits promising generation performance and
surprising emergent abilities equipped with Large Language Models (LLMs) and Visual Foundation Models
 . It is interesting to transfer the generative pre-training (decoder-only) from GPT series to diffusion model
class, evaluate the diffusion-based generation performance at scale, and analyse the emergent abilities of diffusion
foundation models. Furthermore, combining LLMs with diffusion models have been proved to be a new promising
direction .
CONCLUSION
We have provided a comprehensive look at diffusion models from various angles. We began with a self-contained
introduction to three fundamental formulations: DDPMs, SGMs, and Score SDEs. We then discussed recent efforts
to improve diffusion models, highlighting three major directions: sampling efficiency, likelihood maximization, and
new techniques for data with special structures. We also explored connections between diffusion models and other
generative models and outlined potential benefits of combining the two. A survey of applications across six domains
illustrated the wide-ranging potential of diffusion models. Finally, we outlined possible avenues for future research.