Incremental Learning Through Deep Adaptation
Amir Rosenfeld
John K. Tsotsos
Department of Electrical Engineering and Computer Science
York University, Toronto, ON, Canada
 , 
Abstract—Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of
those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in
the number of parameters for each added domain, typically as many as the original network. We propose a method called Deep
Adaptation Networks (DAN) that constrains newly learned ﬁlters to be linear combinations of existing ones. DANs precisely preserve
performance on the original domain, require a fraction (typically 13%, dependent on network architecture) of the number of parameters
compared to standard ﬁne-tuning procedures and converge in less cycles of training to a comparable or better level of performance.
When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with
negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations,
enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness
of our method on a range of image classiﬁcation tasks and explore different aspects of its behavior.
Index Terms—Incremental Learning, Transfer Learning, Domain Adaptation
INTRODUCTION
While deep neural networks continue to show remarkable performance gains in various areas such as image
classiﬁcation , semantic segmentation , object detection
 , speech recognition medical image analysis - and
many more - it is still the case that typically, a separate
model needs to be trained for each new task. Given two
tasks of a totally different modality or nature, such as
predicting the next word in a sequence of words versus
predicting the class of an object in an image, it stands to
reason that each would require a different architecture or
computation. A more restricted scenario - and the one we
aim to tackle in this work - is that of learning a representation which works well on several related domains. Such a
scenario was recently coined by Rebufﬁet al. as multipledomain learning (MDL) to set it aside from multi-task
learning - where different tasks are to be performed on the
same domain. An example of MDL is image-classiﬁcation
where the images may belong to different domains, such as
drawings, natural images, etc. In such a setting it is natural
to expect that solutions will:
Utilize the same computational pipeline
Require a modest increment in the number of required parameters for each added domain
Retain performance of already learned datasets
(avoid “catastrophic forgetting”)
Be learned incrementally, dropping the requirement
for joint training such as in cases where the training data for previously learned tasks is no longer
available.
Our goal is to enable a network to learn a set of related tasks
one by one while adhering to the above requirements. We
do so by augmenting a network learned for one task with
controller modules which utilize already learned representations for another. The parameters of the controller modules
are optimized to minimize a loss on a new task. The training
data for the original task is not required at this stage. The
network’s output on the original task data stays exactly as
it was; any number of controller modules may be added
to each layer so that a single network can simultaneously
encode multiple distinct domains, where the transition from
one domain to another can be done by setting a binary
switching variable or controlled automatically. The resultant
architecture is coined DAN, standing for Deep Adaptation
Networks. We demonstrate the effectiveness of our method
on the recently introduced Visual Decathlon Challenge 
whose task is to produce a classiﬁer to work well on ten
different image classiﬁcation datasets. Though adding only
13% of the number of original parameters for each newly
learned task (the speciﬁc number depends on the network
architecture), the average performance surpasses that of ﬁne
tuning all parameters - without the negative side effects
of doubling the number of parameters and catastrophic
forgetting. In this work, we focus on image classiﬁcation in
various datasets, hence in our experiments the word “task”
refers to a speciﬁc dataset/domain. The proposed method
is extensible to multi-task learning, such as a single network
that performs both image segmentation and classiﬁcation,
but this paper does not pursue this.
Our main contribution is the introduction of an improved alternative to transfer learning, which is as effective
as ﬁne-tuning all network parameters towards a new task,
precisely preserves old task performance, requires a fraction
(network dependent, typically 13%) of the cost in terms of
new weights and is able to switch between any number
of learned tasks. Experimental results verify the applicability of the method to a wide range of image-classiﬁcation
We introduce two variants of the method, a fullyparametrized version, whose merits are described above
 
Controller Module(
Convolution
Controller Module(
Convolution
Controller Module(
Convolution
Dataset Decider
Classifier 1
Classifier 2
Overview of proposed method. For newly learned domains, controller modules are attached to convolutions
of a base network, whose parameter are frozen. A switching variable α allows to switch the behavior of the network
between the original behaviour of the convolution and a re-parametrized one for the new domain. α can be determined
either manually or via a sub-network (“Dataset Decider”) which determines the source domain of the image, switching
accordingly between different sets of control parameters. α Also controls which of the classiﬁers to apply. Other layers
(e.g, non-linearities, batch-normalization, skip layers) not shown for presentation purposes. We visualize one added task,
though an arbitrary number of tasks can be added.
Fig. 2: Controller modules: ﬁlters of a convolutional layer of
a base network are modiﬁed by re-combining their weights
through a controller module, where a switching α variable
can choose between the original ﬁlters Fl and newly created
l . We show a controller module for a single added
task, but any number of controllers can be added, with α
then being a vector instead of scalar.
and one with far fewer parameters, which signiﬁcantly outperforms shallow transfer learning (i.e. feature extraction)
for a comparable number of parameters. In the next section,
we review some related work. Sec. 3 details the proposed
method. In Sec. 4 we present various experiments, including
comparison to related methods, as well as exploring various
strategies on how to make our method more effective,
followed by some discussion & concluding remarks.
RELATED WORK
Multi-task Learning
In multi-task learning, the goal is to train one network to
perform several tasks simultaneously on the same input.
This is usually done by jointly training on all tasks. Such
training is advantageous in that a single representation is
used for all tasks. In addition, multiple losses are said to act
as an additional regularizer. Some examples include facial
landmark localization , semantic segmentation , 3Dreasoning , object and part detection and others.
While all of these learn to perform different tasks on the
same dataset, the recent work of explores the ability
of a single network to perform tasks on various image
classiﬁcation datasets. We also aim to classify images from
multiple datasets but we propose doing so in a manner
which learns them one-by-one rather than jointly. Concurrent with our method is that of which introduces
dataset-speciﬁc additional residual units. We compare to
this work in Sec 4. Our work bears some resemblance to
 , where two networks are trained jointly, with additional
“cross-stitch” units, allowing each layer from one network
to have as additional input linear combinations of outputs
from a lower layer in another. However, our method does
not require joint training and requires signiﬁcantly fewer
parameters.
Incremental Learning
Adding a new ability to a neural net often results in
so-called “catastrophic forgetting” , hindering the network’s ability to perform well on old tasks. The simplest
way to overcome this is by ﬁxing all parameters of the
network and using its penultimate layer as a feature extractor, upon which a classiﬁer may be trained , .
While guaranteed to leave the old performance unaltered, it
is observed to yield results which are substantially inferior
to ﬁne-tuning the entire architecture . The work of Li and
Hoiem provides a succinct taxonomy of several variants
of such methods. In addition, they propose a mechanism of
ﬁne-tuning the entire network while making sure to preserve old-task performance by incorporating a loss function
which encourages the output of the old features to remain
constant on newly introduced data. While their method
adds a very small number of parameters for each new task,
it does not guarantee that the model retains its full ability
on the old task. Rusu et al. shows new representations
can be added alongside old ones while leaving the old
task performance unaffected. However, this comes at a cost
of duplicating the number of parameters of the original
network for each added task. In Kirkpatrick et al. the
learning rate of neurons is lowered if they are found to
be important to the old task. Our method fully preserves
the old representation while causing a modest increase in
the number of parameters for each added task. Recently,
Sarwar et al. have proposed to train a network incrementally by sharing a subset of early layers and splitting
later ones. By construction, their method also preserves the
old representation. However, as their experiments show,
re-learning the parameters of the new network branches,
whether initialized randomly with a similar distribution or
simply copying the old ones results in worse performance
than learning without any network sharing. Our method,
while also re-utilizing existing weights, attains results which
are on average better than simple transfer learning or learning from scratch.
Network Compression
Multiple works have been published on reducing the number weights of a neural network as a means to represent it
compactly , , gain speedups or avoid over-ﬁtting
 , using combinations of coding, quantization, pruning
and tensor decomposition. Such methods can be used in
conjunction with ours to further improve results, as we
show in Sec. 4.
We begin with some notation. Let T be some task to be
learned. Speciﬁcally, we use a deep convolutional neural
net (DCNN) in order to learn a classiﬁer to solve T,
which is an image classiﬁcation task. Most contemporary
DCNN’s follow a common structure: for each input x, the
DCNN computes a representation of the input by passing
it through a set of l layers φi, i ∈1 . . . l interleaved with
non-linearities. The initial (lower) layers of the network
are computational blocks, e.g. convolutions with optional
residual units in more recent architectures . Our method
applies equally to networks with or without residual connections. At least one fully connected layer fi, i ∈1 . . . c is
attached to the output of the last convolutional layer. Let
ΦFN = σ(φl) ◦. . . σ(φ2) ◦σ(φ1) be the composition of all
of the convolutional layers of the network N, interleaved
by non-linearities. We use an architecture where all nonlinearities σ are the same function, with no tunable parameters. Denote by ΦFN (x) the feature part of N. Similarly,
denote by ΦCN = fc ◦. . . σ(f2) ◦σ(f1) the classiﬁer part
of N, i.e. the composition of all of the fully-connected layers
of N. The output of N is then simply deﬁned as:
N(x) = ΦCN ◦ΦFN (x)
We do not specify batch-normalization layers in the
above notation for brevity. It is possible to drop the ΦCN
term, if the network is fully convolutional, as in .
Adapting Representations
Assume that we are given two tasks, T1 and T2, to be
learned, and that we have learned a base network N to solve
T1. We assume that a good solution to T2 can be obtained
by a network with the same architecture as N but with
different parameters. We augment N so that it will be able
to solve T2 as well by attaching a controller module to each
of its convolutional layers. Each controller module uses the
existing weights of the corresponding layer of N to create
new convolutional ﬁlters adapted to the new task T2: for
each convolutional layer φl in N, let Fl ∈RCo×Ci×k×k be
the set of ﬁlters for that layer, where Co is the number of
output features, Cl the number of inputs, and k × k the
kernel size (assuming a square kernel). Denote by bl ∈RC
the bias. Denote by ˜Fl ∈RCo×D the matrix whose rows are
ﬂattened versions of the ﬁlters of Fl, where D = Ci · k · k;
let f ∈RCi×k×k be a ﬁlter from Fl whose values are
, · · · , f i =
The ﬂattened version of f is a row vector:
11, · · · , f 1
kk, · · · , · · · f i
11, · · · , f i
“Unﬂattening” a row vector ˜f reverts it to its tensor form
f ∈RCi×k×k. This way, we can write
l = Wl · ˜Fl
where Wl ∈RCo×Co is a weight matrix deﬁning linear
combinations of the ﬂattened ﬁlters of Fl, resulting in Co
new ﬁlters. Unﬂattening ˜
l to its original shape results in
∈RCo×Ci×k×k, which we call the adapted ﬁlters of
layer φl. Using the symbol X ⊗Y as shorthand for ﬂatten
Y →matrix multiply by X→unﬂatten, we can write:
l = Wl ⊗Fl
If the convolution contains a bias, we instantiate a new
weight vector ba
l instead of the original bl. The output of
layer φl is computed as follows: let xl be the input of φl
in the adapted network. For a given switching parameter
α ∈{0, 1}, we set the output of the modiﬁed layer to be
the application of the switched convolution parameters and
xl+1 = [α(Wl ⊗Fl) + (1 −α)Fl] ∗xl + αba
l + (1 −α)bl (6)
The above formulation is for switching between two
different behaviours - that of a base network and that of
the controller modules learned for a new task.
caltech256
caltech256
caltech256
caltech256
ft­full­bn­off
Fig. 3: Transferability of various datasets to each other (ft-last) ﬁne tuning only the last layer (full) ﬁne-tuning all layers
(ft-full-bn-off) ﬁne tuning all layers while disallowing batch-normalization layers’ weights to be updated. Overall, networks
tend to be more easily transferable to problems from related domains (e.g., natural / drawing). Zoom in to see numbers. It
is recommended to view this ﬁgure in color on-line.
To allow the network to perform multiple tasks, we turn
α to a vector α ∈{0, 1}n where n is the overall number of
tasks, so that αj = 1if we want to perform the j’th task and 0
otherwise. The output of the l layer will be then determined
similarly to Equation 6:
i=1αi(F ai
Where F ai
l are the set of adapted ﬁlters / bias for the
i’th task and we deﬁne for F a1
l = bl to include the
original task.
A set of fully connected layers f a
are learned from
scratch, attaching a new “head” to the network for each
new task. Throughout training & testing, the weights of F
(the ﬁlters of N) are kept ﬁxed and serve as basis functions
for F a. The weights of the controller modules are learned
via back-propagation given the loss function. Weights of
any batch normalization (BN) layers are either kept ﬁxed
or learned anew. The batch-normalized output is switched
between the values of the old and new BN layers, similarly
to Eq. 6. A visualization of the resulting DAN can be seen in
Fig. 1 and a single control module in Fig. 2.
We denote a network learned for a dataset/task S as
NS. A controller learned using NS as a base network will be
denoted as DANS, where DAN stands for Deep Adaptation
Network and DANS→T means using DANS for a speciﬁc
task T. While in this work we apply the method to classiﬁcation tasks it is applicable to other tasks as well.
Additional Design Choices
In the following, we mention some additional design
choices of possible ways to augment a network with controllers, as well as provide some analysis on the incurred
cost of parameters.
Weaker parametrization
A weaker variant of our method is one that forces the
matrices Wl to be diagonal, e.g, only scaling the output
of each ﬁlter of the original network. We call this variant
“diagonal” (referring only to scaling coefﬁcients, such as by a
diagonal matrix) and the full variant of our method “linear”
(referring to a linear combination of ﬁlters). The diagonal
variant can be seen as a form of explicit regularization which
limits the expressive power of the learned representation.
While requiring signiﬁcantly fewer parameters, it results
in poorer classiﬁcation accuracy, but as will be shown
later, also outperforms regular feature-extraction for transfer
learning, especially in network compression regimes.
Multiple Controllers
The above description mentions one base network and one
controller network. However, any number of controller networks can be attached to a single base network, regardless
of already attached ones. In this case α is extended to onehot vector of values determined by another sub-network,
allowing each controller network to be switched on or off as
Parameter Cost
The number of new parameters added for each task depends
on the number of ﬁlters in each layer and the number
of parameters in the fully-connected layers. As the latter
are not reused, their parameters are fully duplicated. Let
M = Co × D be the ﬁlter dimensions for some conv. layer
φl where D = Ci × k × k. A controller module for φl
requires C2
o coefﬁcients for F a
l and an additional Co for ba
Hence the ratio of new parameters w.r.t to the old for φl
is Co×(Co+1)
Co×(D+1) = Co+1
D . Example: for Co = Ci = 256
input and output units and a kernel size k = 5 this equals
256·52+1 ≈0.04. In the ﬁnal architecture we use the total
DANcaltech−256
DANimagenet
DANimagenet+sketch
TABLE 1: Perf: top-1 accuracy (%, higher is better) on various datasets and parameter cost (#par., lower is better) for a few
baselines and several variants of our method. Rows 1,2: independent baseline performance. VGG-B: VGG architecture
B. (S) - trained from scratch. (P) - pre-trained on ImageNet. Rows 3-7: (ours) controller network performance; DANsketch as
a base network outperforms DANcaltech−256 on most datasets. A controller network based on random weights (DANnoise)
works quite well given that its number of learned parameters is a ﬁfth of the other methods. DANimagenet: controller
networks initialized from VGG-B model pretrained on ImageNet. DANimagenet+sketch: selective control network based on
both VGG-B(P) & Sketch. We color code the ﬁrst, second and third highest values in each column (lowest for #par). #par:
amortized number of weights learned to achieve said performance for all tasks divided by number of tasks addressed
(lower is better).
number of weights required to adapt the convolutional layers Φl combined with a new fully-connected layer amounts
to about 13% of the original parameters. For VGG-B, this
is roughly 21%. For instance, constructing 10 classiﬁers
using one base network and 9 controller networks requires
(1 + 0.13 ∗9) · P=2.17 · P parameters where P is the
number for the base network alone, compared to 10 · P
required to train each network independently. The cost is
dependent on network architecture, for example it is higher
when applied on the VGG-B architecture. While our method
can be applied to any network with convolutional layers, if
Co ≥D, i.e., the number of output ﬁlters is greater than
the dimension of each input ﬁlter, it would only increase the
number of parameters.
Limitations
An immediate question that arises from our formulation
concerns the expressive power of the resulting adapted
networks. We provide some analysis here on this issue.
In regular ﬁne-tuning or learning schemes, the weights of
each layer may take on arbitrary values, depending on
the optimization scheme and training data. In contrast, the
proposed method constrains each ﬁlter to be a linear combination of the original ﬁlters in the corresponding layer.
This may, in general, become a strong limiting factor on the
performance of the network. The ﬁlters F = f1, . . . , fk of
a layer l can form vectors by ﬂattening their tensor form.
F deﬁnes a basis of a subspace of ﬁlters from which the
proposed method creates new ones. As our method deﬁnes
new ﬁlters by applying a linear transformation on F, the
initial values thereof can have several effects on the resulting
networks. Two important scenarios are when (a) network
expressibility and (b) network efﬁciency are affected. We
provide examples of when these scenarios can occur and
some experiments to demonstrate them.
Expressibility
One can easily construct cases where the expressive power
of a network is critically hindered by choosing to adapt it
from an inappropriate base network. For example, assume
that the inputs of a network are RGB images. Also assume
that - for some reason - all ﬁrst-layer ﬁlters have coefﬁcients
of exactly 0 in the green and blue channels. If information
in these channels is critical for the task to be learned by
the proposed method, it will clearly fail, as the information
resides in an orthogonal feature space to that represented by
the base network’s ﬁlters.
Residual Connections: The above claim does
not hold for networks with residual connections . In
Residual connections the output yi of the i′th layer is of
the following general form:
yi = F(xi; Wi) + xi
where xi is the input to this layer and Wi is the layer’s
parameters. This allows preserving information from the
previous layer, as a possible function permitted by this
is the identity function. If enough information from the
signal x is preserved then the next layer can still recover
from not representing it adequately in this layer. Hence, the
expressive power of the network is probably less sensitive
to the choice of basis function in a single layer. Nevertheless,
its efﬁciency may be compromised, as we describe next.
In some cases, the expressive power of the network is not
immediately affected by the choice of basis function but we
claim that the efﬁciency of the network is. We deﬁne the
efﬁciency of a network w.r.t to a given image patch as the
minimal number of layers (depth) required to discriminate
it from others by a single ﬁlter. In other words, the depth
of the shallowest layer where there exists some ﬁlter whose
response correlates strongly with the presence of the patch.
An example is a 3x3 gray-scale patch with 1 in all corners
and 0 otherwise. In the ﬁrst layer, a ﬁlter v of values:
would perfectly match this patch. On the other hand, one
could set 9 ﬁlters of the ﬁrst layer to delta functions with 1
in location i, j for each i and j in 1, 2, 3. These ﬁlters span all
possible patches of 3x3 so they do not effect expressibility.
However, keeping the ﬁrst layer ﬁxed, the second layer
mean performance
channel switch
channel switch + learn
channel switch + clean start
channel switch + noise
original +learn
transposed
maximal performance
channel switch
channel switch + learn
channel switch + clean start
channel switch + noise
original +learn
transposed
Fig. 4: Demonstration of different possible limits of possible method on a toy example. Transferring from a network where
the ﬁrst layer contains features from an orthogonal sub-space to that required for a task can result in chance accuracy.
Please see text for details.
would be required to create a ﬁlter to match p, whereas v
expressed this directly.
Toy Dataset
To further demonstrate the above claims, we constructed
a toy classiﬁcation task. We ﬁrst describe the dataset and
experiments and then discuss the results.
The task is: given an input image of size 28x28x3, predict
the length of a bar in the an image. In this dataset, the
images are restricted to be 1-pixel width bars of length 3 · k
where k ∈{3, 4, 5, 6, 7} over a blank background and the
class is the corresponding length of the bar. The bar lengths
were set to be in increments of 3 as lesser ones caused
difﬁculties for the network to learn, probably due to maxpooling operations.
The dataset is composed of 1000 examples, with a random 75%/25% train/test split. The network we tested is
deliberately quite degenerate: it contains one 5x5 ﬁlter on
the ﬁrst layer and 20 ﬁlters on the second, followed by two
fully connected layers of 320x50 and 50x5.
We created 3 variants of this dataset, to test different
transfer-learning scenarios on it. The dataset variants are (1)
only red horizontal bars, (2) only red vertical bars (referred
to as “transposed”) and (3) only green horizontal bars
(referred to as “new channel”). Note that we limited our
method to operate only on the ﬁrst layer, which in this case
boils down to multiplying the output of the ﬁlter by a scalar
and learning a new bias.
The network was trained for 50 epochs using the Adam
 optimizer (SGD did not converge as well in this case)
in all scenarios. We tested seven scenarios. These involve
different combinations of the images in the dataset, initialization of the ﬁrst layer and which layers may or may not
be modiﬁed by the optimizer.
original: horizontal dataset, ﬁrst ﬁlter ﬁxed to a
horizontal green bar, learn other layers
original+learn : horizontal dataset, learn all layers
transposed : transposed dataset, ﬁrst ﬁlter ﬁxed to a
horizontal bar, learn other layers
channel switch: new channel dataset, ﬁrst ﬁlter ﬁxed
to horizontal green bar, learn other layers
channel switch + noise: new channel dataset, ﬁrst
ﬁlter ﬁxed to horizontal green bar+ Gaussian noise,
learn other layers
channel switch + clean start: new channel dataset,
learn all layers
channel switch + learn: new channel dataset, ﬁrst
ﬁlter initialized to a horizontal green bar, learn all
Each experiment was repeated 20 times to account for
variability. We plot the mean accuracy on the validation set
over the 50 training epochs.
Fig. 4 summarizes the result of these experiments. The
left sub-ﬁgure shows the mean performance over the 50
epochs and the right one shows the maximal (of the 20
trials) attained for each scenario. These show that in most
cases, we can reach good accuracy, however on the average case there are drastic differences between the obtained
performance, although this is a very simple dataset. The
original scenario, where the ﬁrst layer is ﬁxed, attains
100% accuracy. Switching the channel from green to red
(“channel switch”) but keeping the ﬁrst ﬁlter ﬁxed results
in chance accuracy, as excepted: no linear combination of
a purely “green” ﬁlter can represent information from the
red channel. In this case, if we allow all layers to be learned
(“channel switch + learn”), indeed the network can ﬁnally
converge to a good accuracy (Fig. 4 (b)), but the average
performance over 20 trials is far from optimal (Fig. 4 (a))
- the network converges to around 50% accuracy (where
chance performance is 20%). This conﬁrms that even for
very simple cases, bad initialization can be detrimental to
the training process.
As a “sanity test”, we make sure that randomly initializing the network and allowing all layers to be learned in
the “channel switch” case achieves 100% performance (in
both the average and best case). This is indeed the case
(“channel switch + clean start”), although we see that the
random initialization converges at ﬁrst much slower than
the informed initialization of “original”. Next, we initialize
the network as in “original” but with the “channel switch”
dataset. However, we add random noise to the ﬁrst layer’s
ﬁlter, then keep it ﬁxed (“channel switch + noise”). The
optimization is able to recover from this initialization in
both the average and best case, as the initial ﬁlter is not
orthogonal to the green channel.
The “original + learn” is initialized as “original” but
allows the ﬁrst layer to be modiﬁed. While on average it
initially converges slower than the “original” scenario, on
average it performs better.
Finally, the “transposed” case, where the original ﬁlter
is ﬁxed to be orthogonal to the required shape (horizontal
bars), attains average lower performance, and near 100%
in the best case. Note that although we cannot generate an
horizontal bar by linearly transforming the vertical one, the
information is not lost and can be recovered by ﬁlters of the
second layer, due to their convolutional nature. This is done
by a weighted summation of the shifted responses of the
horizontal ﬁlter with proper coefﬁcients. This is an example
of the network “efﬁciency” being reduced, i.e, delegating to
the second layer computations which could be done in the
To summarize, we conclude from the experiments on this
toy dataset that:
Setting some network weights using strong domainspeciﬁc knowledge and ﬁxing their values, learning
only other layers can lead to fast convergence to a
strong solution.
An even stronger solution can emerge if all layers
are initialized randomly, then allowed to be updated
- though convergence will be slower.
Initializing some weights to “bad” values can cause
on-average dramatically inferior performance to the
previous two cases, with occasional but rare instances of good performance.
Initializing some weights to “bad” values, ﬁxing
them and learning the rest of the weights will totally
and irreparably prevent the network from learning.
In the next section, we test our method on richer and more
diverse datasets, less degenerate networks (e.g., more than
one ﬁlter in the ﬁrst layer), and in some cases residual
networks, which all are likely to diminish effects shown on
artiﬁcial examples.
EXPERIMENTS ON REAL DATASETS
We conduct several experiments to test our method and
explore different aspects of its behavior. The experimental
section is split into two parts, the ﬁrst being of a more exploratory nature and the second geared toward results in a
recent multi-task learning benchmark. We use two different
basic network architectures on two (somewhat overlapping)
sets of classiﬁcation benchmarks for the respective parts.
The ﬁrst is VGG-B . We begin by listing the datasets we
used (4.0.1), followed by establishing baselines by training a
separate model for each using a few initial networks. We
proceed to test several variants of our proposed method
(4.1) as well as testing different training schemes. Next, we
discuss methods of predicting how well a network would
fare as a base-network (4.3). We show how to discern the
domain of an input image and output a proper classiﬁcation
(4.3.1) without manual choice of the control parameters α.
In the second part of our experiments we show how our
applying our method results in leading scores in the Visual
Decathlon Challenge , using a different, more recent
architecture. Before concluding we demonstrate some useful
properties of our method.
All Experiments were performed using the PyTorch1
framework using a single Titan-X Pascal GPU.
Datasets and Evaluation
The ﬁrst part of our evaluation protocol resembles that of
 : we test our method on the following datasets: Caltech-
256 , CIFAR-10 , Daimler (DPed), GTSR ,
Omniglot , Plankton imagery data (Plnk), Human
Sketch dataset and SVHN . All images are resized
to 64 × 64 pixels, duplicating gray-scale images so that
they have 3 channels as do the RGB ones. We whiten all
images by subtracting the mean pixel value and dividing
by the variance per channel. This is done for each dataset
separately. We select 80% for training and 20% for validation
in datasets where no ﬁxed split is provided. We use the
B architecture described in , henceforth referred to as
VGG-B. It performs quite well on the various datasets when
trained from scratch (See Tab. 1. Kindly refer to for a
brief description of each dataset.
As a baseline, we train networks independently on each
of the 8 datasets. All experiments in this part are done with
the Adam optimizer , with an initial learning rate of 1e-3
or 1e-4, dependent on a few epochs of trial on each dataset.
The learning rate is halved after each 10 epochs. Most networks converge within the ﬁrst 10-20 epochs, with mostly
negligible improvements afterwards. We chose Adam for
this part due to its fast initial convergence with respect to
non-adaptive optimization methods (e.g, SGD), at the cost
of possibly lower ﬁnal accuracy . The top-1 accuracy (%)
is summarized in Table 1.
Controller Networks
To test our method, we trained a network on each of the
8 datasets in turn to be used as a base network for all
1. pytorch.org
minibatch number
Effect of Controller Initialization
linear_approx
Transferability
Mean Accuracy
caltech256
Transferability vs. Mean Accuracy
(a) Controller initialization schemes. Mean loss averaged over 5 experiments for different ways of initializing
controller modules, overlaid with minimal and maximal values. Random initialization performs the worst (random).
Approximating the behavior of a ﬁne-tuned network is slightly better (linear approx) and initializing by mimicking the
base network (diagonal) performs the best (b) Predictability of a control network’s overall accuracy average over all
datasets, given its transferability measure.
ft-full-bn-off
TABLE 2: Mean transfer learning performance. We show the mean top-1 accuracy (%) attained by ﬁne-tuning a network
from each domain to all domains. Out of the datasets above, starting with Caltech-256 proves most generic as a feature
extractor (ft-last). However, ﬁne tuning is best when initially training on the Sketch dataset (ft-full).
others. We compare this to the baseline of training on each
dataset from scratch (VGG-B(S)) or pretrained (VGG-B(P))
networks. Tab. 1 summarizes performance on all datasets
for two representative base nets: DANcaltech−256 (79.9%)
and DANsketch (83.7%). Mean performance for other base
nets are shown in Fig. 5 (a). The parameter cost (3.2.3) of
each setting is reported in the last column of the table. This
(similarly to is the total number of parameters required
for a set of tasks normalized by that of a single fullyparametrized network. We also check how well a network
can perform as a base-network after it has seen ample training examples: DANimagenet is based on VGG-B pretrained
on ImageNet . This improves the average performance
by a signiﬁcant amount (83.7% to 86.5%). On Caltech-256
we see an improvement from 88.2% (training from scratch).
However, for both Sketch and Omniglot the performance is
in favor of DANsketch. Note these are the only two domains
of strictly unnatural images. Additionally, DANimagenet is
still slightly inferior to the non-pretrained VGG-B(S) (86.5%
vs 87.7%), though the latter is more parameter costly.
Multiple Base Networks
Arguably, a good base network should have features generic
enough so that a controller network can use them for a
broad range of target tasks. In practice this may not necessarily be the case. We conjecture that using a diverse set
of features, such as that learned by training on more than
one task, will provide a better basis for transfer learning. To
use two base-networks simultaneously, we implemented a
dual-controlled network by using both DANcaltech−256 and
DANsketch and attaching to them controller networks. The
outputs of the feature parts of the resulting sub-networks
were concatenated before the fully-connected layer. This
resulted in the exact same performance as DANsketch alone.
However, by using selected controller-modules per group
of tasks, we can improve the results: for each dataset the
maximally performing network (based on validation) is the
basis for the control module, i.e., we used DANimagenet
for all datasets except Omniglot and Sketch. For the latter
two we use DANsketch as a base net. We call this network
DANimagenet+sketch. At the cost of more parameters, it
boosts the mean performance to 87.76% - better than using
any single base net for controllers or training from scratch.
Since it is utilized for 9 tasks (counting ImageNet), its
parameter cost (2.76) is still quite good.
Starting from a Randomly Initialized Base Network
We tested how well our method can perform without any
prior knowledge, e.g., building a controller network on a
randomly initialized base network. The total number of parameters for this architecture is ˜12M. However, as 10M
have been randomly initialized and only the controller
modules and fully-connected layers have been learned, the
effective number is actually 2M. Hence its parameter cost is
determined to be 0.22. We summarize the results in Tab. 1.
Notably, the results of this initialization worked surprisingly
well; the mean top-1 precision attained by this network was
76.3%, slightly worse than of DANcaltech−256 (79.9%). This
is better than initializing with DANdaimler, which resulted
in a mean accuracy of 75%. This is possible because the
random values in the base network can still be linearly
combined by our method to create ones that are useful for
classiﬁcation.
Initialization
One question which arises is how to initialize the weights W
of a control-module. We tested several options: (1) Setting
W to an identity matrix (diagonal). This is equivalent to
the controller module starting with a state which effectively
mimics the behavior of the base network (2) Setting W
to random noise (random) (3) Training an independent
network for the new task from scratch, then set W to best
linearly approximate the new weights with the base weights
(linear approx). To ﬁnd the best initialization scheme, we
trained DANsketch→caltech256 for one epoch with each and
observed the loss. Each experiment was repeated 5 times
and the results averaged. From Fig. 5(a), it is evident that
the diagonal initialization is superior, perhaps counterintuitively, there is no need to train a fully parametrized
target network. Simply starting with the behavior of the
base network and tuning it via the control modules results in
faster convergence. Hence we train controller modules with
the diagonal method. Interestingly, the residual adaptation
unit in is initially similar to the diagonal conﬁguration.
If all of the ﬁlters in their adapter unit are set to 1 (up to
normalization), the output of the adapter will be initially
the same as that of the controller unit initialized with the
identity matrix.
Transferability
How is one to choose a good network to serve as a basenetwork for others? As an indicator of the representative
power of the features of each independently trained network N, we test the performance on other datasets, using N
for ﬁne tuning. We deﬁne the transferability of a source task
S w.r.t a target task T as the top-1 accuracy attained by ﬁnetuning N trained on S to perform on T. We test 3 different
scenarios, as follows: (1) Fine-tuning only the last layer (a.k.a
feature extraction) (ft-last); (2) Fine-tuning all layers of N(ftfull); (3) same as ft-full, but freezing the parameters of the
batch-normalization layers - this has proven beneﬁcial in
some cases - we call this option ft-full-bn-off. The results in
Fig 3 show some interesting phenomena. First, as expected,
feature extraction (ft last) is inferior to ﬁne-tuning the entire
network. Second, usually training from scratch is the most
beneﬁcial option. Third, we see a distinction between natural images (Caltech-256, CIFAR-10, SVHN, GTSR, Daimler)
and unnatural ones (Sketch, Omniglot, Plankton); Plankton
images are essentially natural but seem to exhibit different
behavior than the rest.
Shifting Representations
caltech256
Fig. 6: Shifting Representations. Using a single base network
Nsketch, we check the method’s sensitivity to varying values
of α by varying it in the range . Increasing α shifts the
network away from the base representation and towards
learned tasks - gradually lowering performance on the base
task (diamonds) and improving on the learned ones (full
circles). The relatively slow decrease of the performance on
sketch (blue diamonds) and increase in that of Plankton
(blue circles) indicates a similarity between the learned
representations.
It is evident that features from the natural images are
less beneﬁcial for the unnatural images. Interestingly, the
converse is not true: training a network starting from Sketch
or Omniglot works quite well for most datasets, both natural and unnatural. This is further shown in Tab. 2: we
calculate the mean transferability of each dataset by the
mean value of each rows of the transferability matrix from
Fig. 3. DANCaltech−256 works best for feature extraction.
However, for full ﬁne-tuning using DANP lankton works as
the best starting point, closely followed by DANCaltech−256.
For controller networks, the best mean accuracy attained
for a single base net trained from scratch is attained using
DANsketch (83.7%). This is close to the performance attained
by full transfer learning from the same network (84.2%, see
Tab. 2) at a fraction of the number of parameters. This is
consistent with our transferability measure. To further test
the correlation between the transferability and the performance given a speciﬁc base network, we used each dataset
as a base for control networks for all others and measured
the mean overall accuracy. The results can be seen in Fig. 5
A Uniﬁed Network
Finally, we test the possibility of a single network which
can both determine the domain of an image and classify it.
We train a classiﬁer to predict from which dataset an image
originates, using the training images from the 8 datasets.
This is learned easily by the network (also VGG-B) which
rapidly converges to 99% accuracy. With this “datasetdecider”, named Ndc we augment DANsketch to set for each
input image I from any of the datasets Di the controller
value αi of DANsketch→Di to 1 if and only if Ndc deemed
Accuracy %
Accuracy vs. Method
FEATURE EXTRACTOR
Accuracy %
Accuracy vs. Quantization
nQuantizationBits
Fig. 7: (a) Accuracy vs. learning method. Using only the last layer (feature extractor) performs worst. ﬁnetune: vanilla ﬁnetuning. Diagonal : our controller modules with a diagonal combination matrix. Linear: our full method. On average, our full
method outperforms vanilla ﬁne tuning. (b) Accuracy vs. quantization: with as low as 8 bits, we see no signiﬁcant effect of
network quantization on our method, showing they can be applied together.
I to originate from Di and to 0 otherwise. This produces
a network which applies to each input image the correct
controllers, classifying it within its own domain. While the
predicted values of αi are real-valued, we set the highest
one to 1 and the rest to zero.
Visual Decathlon Challenge
We now show results on the recent Visual Decathlon Challenge of . The challenge introduces involves 10 different
image classiﬁcation datasets: ImageNet ; Aircraft ;
Cifar-100 ; Daimler Pedestrians ; Dynamic Textures
 ; GTSR ; Flowers ; Omniglot ; SVHN 
and UCF-101 . The goal is to achieve accurate classiﬁcation on each dataset while retaining a small model
size, using the train/val/test splits ﬁxed by the authors.
All images are resized so the smaller side of each image is
72 pixels. The classiﬁer is expected to operate on images
of 64x64 pixels. Each entry in the challenge is assigned
a decathlon score, which is a function designed to highlight methods which do better than the baseline on all 10
datasets. Please refer to the challenge website for details
about the scoring and datasets: 
uk/∼vgg/decathlon/. Similarly to , we chose to use a
wide residual network with an overall depth of 28 and
a widening factor of 4, with a stride of 2 in the convolution
at the beginning of each basic block. In what follows we
describe the challenge results, followed by some additional
experiments showing the added value of our method in
various settings. In this section we used the recent YellowFin
optimizer as it required less tuning than SGD. We use
an initial learning rate factor of 0.1 and reduce it to 0.01
after 25 epochs. This is for all datasets with the exception of
ImageNet which we train for 150 epochs with SGD with an
initial learning rate of 0.1 which is reduced every 35 epochs
by a factor of 10. This is the conﬁguration we determined
using the available validation data which was then used to
train on the validation set as well (as did the authors of
the challenge) and obtain results from the evaluation server.
Here we trained on the reduced resolution ImageNet from
scratch and used the resulting net as a base for all other
tasks. Tab. 3 summarizes our results as well as baseline
methods and the those of , all using a base architecture
of similar capacity. By using a signiﬁcantly stronger base
architecture they obtained higher results (mean of 79.43%)
but with a parameter cost of 12, i.e., requiring 12 times the
amount of original parameters. All of the rows are copied
from , including their re-implementation of LWF, except
the last which shows our results. The ﬁnal column of the
table shows the decathlon score. A score of 2500 reﬂects the
baseline resulting from ﬁnetuning the network learned on
ImageNet to each dataset independently. For the same architecture, the best results obtained by the Residual Adapters
method is slightly below ours in terms of decathlon score
and slightly above them in terms of mean performance.
However, unlike them, we avoid joint training over all of
the datasets and using dataset-dependent weight decay.
Compression and Convergence
In this section, we highlight some additional useful
properties of our method. All experiments in the following
were done using the same architecture as in the last section
but training was performed only on the training sets of the
Visual Decathlon Challenge and tested on the validation
sets. First, we check whether the effects of network compression are complementary to ours or can hinder them. Despite
the recent trend of sophisticated network compression techniques (for example ) we use only a simple method of
compression as a proof-of-concept, noting that using recent
compression methods will likely produce better results. We
TABLE 3: Results on Visual Decathlon Challenge. Scratch: training on each task independently. Feature: using a pre-trained
network as a feature extractor. Finetune : vanilla ﬁne tuning. Performs well but requires many parameters. Learningwithout-forgetting (LWF, slightly outperforms it but with a large parameter cost. Residual adapt. signiﬁcantly reduce
the number of parameters. Results improve when training jointly on all task (Res.Adapt(Joint)). The proposed method
(DAN) outperforms residual adapters despite adding each task independently of the others. S is the decathlon challenge
Mean Normalized Accuracy %
Convergence Speed
feature extractor
Mean Normalized Accuracy
Convergence Speed
feature extractor
Fig. 8: (a) Our method (linear) converges to a high accuracy faster than ﬁne-tuning. The weaker variant of our method
converges as fast as feature-extraction but reaches an overall higher accuracy (7 (a)). (b) zoom in on top-right of (a).
apply a simple linear quantization on the network weights,
using either 4, 6, 8, 16 or 32 bits to represent each weight,
where 32 means no quantization. We do not quantize batchnormalization coefﬁcients. Fig. 7 (b) shows how accuracy
is affected by quantizing the coefﬁcients of each network.
Using 8 bits results in only a marginal loss of accuracy. This
effectively means our method can be used to learn new tasks
while requiring an addition of 3.25% of the original amount
of parameters. Many maintain performance even at 6 bits
(DPed, Flowers, GTSR, Omniglot, SVHN. Next, we compare
the effect of quantization on different transfer methods:
feature extraction, ﬁne-tuning and our method (both diagonal and linear variants). For each dataset we record
the normalized (divided by the max.) accuracy for each
method/quantization level (which is transformed into the
percentage of required parameters). This is plotted in Fig.
9. Our method requires signiﬁcantly fewer parameters to
reach the same accuracy as ﬁne-tuning. If parameter usage
is limited, the diagonal variant of our method signiﬁcantly
outperforms feature extraction. Finally, we show that the
number of epochs until nearing the maximal performance is
markedly lower for our method. This can be seen in Fig. 8.
Discussion
We have observed that the proposed method converges to
a reasonably good solution faster than vanilla ﬁne-tuning
and eventually attains slightly better performance. The increase in classiﬁcation accuracy is despite the network’s
expressive power, which is limited by our construction. We
conjecture that constraining each layer to be expressed as a
linear combination of the corresponding layer in the original
network serves to regularize the space of solutions and is
beneﬁcial when the tasks are sufﬁciently related to each
other. One could come up with simple examples where the
proposed method would likely fail. This might occur if tasks
are not of the same kind. For example, one task requires
counting of horizontal lines and the other requires counting
of vertical ones, and such examples are all that appear in the
training sets, then the proposed method will likely work far
worse than vanilla ﬁne-tuning or training from scratch. We
0 1 2 3 4 6 9 11 12 13 17 18 19 23 24 25 47 48 51 95 96102
no. Params %
Mean Norm. Acc %
feature extractor
Fig. 9: Mean classiﬁcation accuracy (normalized, averaged
over datasets) w.r.t no. parameters. Our method achieve
better performance over baselines for a large range of parameter budgets. For very few parameters diagonal (ours)
outperforms features extraction. To obtain maximal accuracy our full method requires far fewer parameters (see
linear vs ﬁnetune).
experimented with such scenarios in Sec. 3.3. Empirically,
we did not see this happen in any of our experiments on
“real” datasets. This may simply stem from some inherent
similarity in image classiﬁcation tasks which makes some
basic set of features useful for most of them. We leave the
investigation of this issue, as well as ﬁnding ways between
striking a balance between reusing features and learning
new ones as future work.
CONCLUSIONS
We have presented a method for transfer learning thats
adapts an existing network to new tasks while fully preserving the existing representation. Our method matches or
outperforms vanilla ﬁne-tuning, though requiring a fraction
of the parameters, which when combined with net compression reaches 3% of the original parameters with no loss of
accuracy. The method converges quickly to high accuracy
while being on par or outperforming other methods with
the same goal. Built into our method is the ability to easily switch the representation between the various learned
tasks, enabling a single network to perform seamlessly on
various domains. The control parameter α can be cast as
a real-valued vector, allowing a smooth transition between
representations of different tasks. An example of the effect
of such a smooth transition can be seen in Fig. 6 where α
is used to linearly interpolate between the representation
of differently learned pairs tasks, allowing one to smoothly
control transitions between different behaviors. Allowing
each added task to use a convex combination of already
existing controllers will potentially utilize controllers more
efﬁciently and decouple the number of controllers from the
number of tasks.
ACKNOWLEDGMENTS
This research was funded by the Canada Research Chairs
program and by the Air Force Ofﬁce for Scientiﬁc Research
(USA) for which the authors are grateful.