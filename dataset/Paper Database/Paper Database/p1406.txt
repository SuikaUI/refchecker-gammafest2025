Wider or Deeper: Revisiting the ResNet Model for Visual Recognition∗
Zifeng Wu, Chunhua Shen, and Anton van den Hengel
School of Computer Science, The University of Adelaide, Adelaide, SA 5005, Australia
e-mail: 
The trend towards increasingly deep neural networks has
been driven by a general observation that increasing depth
increases the performance of a network. Recently, however,
evidence has been amassing that simply increasing depth
may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact
be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine
these issues, and in doing so arrive at a new interpretation
of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which signiﬁcantly
outperforms much deeper models such as ResNet-200 on
the ImageNet classiﬁcation dataset. We also show that this
performance is transferable to other problem domains by
developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on
datasets including PASCAL VOC, PASCAL Context, and
Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and
yet is more efﬁcient in memory use and sometimes also
in training time.
The code and models are available at
 
1. Introduction
2. Related work
3. Residual networks revisited
3.1. Residual networks unravelled online . . . . .
3.2. Residual networks behaviours revisited . . .
3.3. Wider or deeper? . . . . . . . . . . . . . . .
4. Approach to image classiﬁcation
∗Correspondence should be addressed to C. Shen.
5. Approach to semantic image segmentation
6. Experimental results
6.1. Image classiﬁcation results . . . . . . . . . .
6.2. Semantic image segmentation results
7. Conclusion
A. Appendix
A.1. Network structures . . . . . . . . . . . . . .
A.2. Gradients in residual networks . . . . . . . .
A.3. Qualitative results
. . . . . . . . . . . . . .
 
1. Introduction
The convolutional networks used by the computer vision community have been growing deeper and deeper each
year since Krizhevsky et al. proposed AlexNet in 2012.
The deepest network in the literature is a residual
network (ResNet) with 1,202 trainable layers, which was
trained using the tiny images in the CIFAR-10 dataset .
The image size here is important, because it means that
the size of corresponding feature maps is relatively small,
which is critical in training extremely deep models. Most
networks operating on more practically interesting image
sizes tend to have the order of one, to two, hundred layers, e.g. the 200-layer ResNet and 96-layer Inception-
ResNet . The progression to deeper networks continues,
however, with Zhao et al. having trained a 269-layer
network for semantic image segmentation. These networks
were trained using the ImageNet classiﬁcation dataset ,
where the images are of much higher resolution.
additional layer requires not only additional memory, but
also additional training. The marginal gains achieved by
each additional layer diminish with depth, however, to the
point where Zhao et al. achieved only an improvement of 1.1% (from 42.2% to 43.3% by mean intersectionover-union scores) after almost doubling the number of layers (from 152 to 269). On the other hand, Zagoruyko and
Komodakis showed that it is possible to train much shallower but wider networks on CIFAR-10, which outperform
a ResNet with its more than one thousand layers. The
question thus naturally arises as to whether deep, or wide,
is the right strategy.
In order to examine the issue we ﬁrst need to understand the mechanism behind ResNets. Veit et al. have
claimed that they actually behave as exponential ensembles
of relatively shallow networks. However, there is a gap between their proposed unravelled view of a ResNet, and a
real exponential ensemble of sub-networks, as illustrated
in the top row of Fig. 1. Since the residual units are nonlinear, we cannot further split the bottom path into two subnetworks, i.e., Mc and Md. It turns out that ResNets are only
assembling linearly growing numbers of sub-networks. Besides, the key characteristic of our introduced view is that
it depends on the effective depth l of a network. This l
amounts to the number of residual units which backward
gradients during training can go through. When l ≥2, the
two-unit ResNet in Fig. 1 can be seen as an ensemble of
three sub-networks, i.e., Ma, Mb, and M 2
e , as shown in the
bottom left. When l = 1, nothing changes except that we
replace the third sub-network with a shallower one M 1
shown in the bottom right example. The superscripts in M 1
e denote their actual depths. About the unravelled
view, the effective depth of a ResNet, and the actual depth
of a sub-network, more details will be provided in the sequence. It is also worth noting that Veit et al. empirically found that most gradients in a 110-layer ResNet can
only go through up to seventeen residual units, which supports our above hypothesis that the effective depth l exists
for a speciﬁc network.
In this paper, our contributions include:
• We introduce a further developed intuitive view of
ResNets, which helps us understand their behaviours,
and ﬁnd possible directions to further improvements.
• We propose a group of relatively shallow convolutional
networks based on our new understanding. Some of
them achieve the state-of-the-art results on the ImageNet classiﬁcation dataset .
• We evaluate the impact of using different networks
on the performance of semantic image segmentation,
and show these networks, as pre-trained features, can
boost existing algorithms a lot. We achieve the best
results on PASCAL VOC , PASCAL Context ,
and Cityscapes .
2. Related work
Our work here is closely related to two topics, residual
network (ResNet) based image classiﬁcation and semantic
image segmentation using fully convolutional networks .
As we have noted above, He et al. recently
proposed the ResNets to combat the vanishing gradient
problem during training very deep convolutional networks.
ResNets have outperformed previous models at a variety of
tasks, such as object detection and semantic image segmentation . They are gradually replacing VGGNets 
in the computer vision community, as the standard feature
extractors. Nevertheless, the real mechanism underpinning
the effectiveness of ResNets is not yet clear. Veit et al. 
claimed that they behave like exponential ensembles of relatively shallow networks, yet the ‘exponential’ nature of
the ensembles has yet to be theoretically veriﬁed. Residual
units are usually non-linear, which prevents a ResNet from
exponentially expanding into separated sub-networks, as illustrated in Fig. 1. It is also unclear as to whether a residual
structure is required to train very deep networks. For example, Szegedy et al. showed that it is ‘not very difﬁcult’
to train competitively deep networks, even without residual
shortcuts. Currently, the most clear advantage of ResNets
is in their fast convergence . Szegedy et al. observed similar empirically results to support that. On the
other hand, Zagoruyko and Komodakis found that a
wide sixteen-layer ResNet outperformed the original thin
thousand-layer ResNet on datasets composed of tiny
images such as CIFAR-10 . The analysis we present
here is motivated by their empirical testing, but aims at a
more theoretical approach, and the observation that a grid
Residual unit
The unravelled view of a simple ResNet. The fact that f2(·) is non-linear gives rise to the inequality in the top row, as
f2(a + b) ̸= f2(a) + f2(b). This shows that f2(·) never operates independently of the result of f1(·), and thus that the number of
independent classiﬁers increases linearly with the number of residual units. Analysing the interactions between residual units at a given
effective depth, here labelled l, illuminates the paths taken by gradients during training.
search of conﬁguration space is impractical on large scale
datasets such as the ImageNet classiﬁcation dataset .
Semantic image segmentation amounts to predicting the
categories for each pixel in an image. Long et al. proposed the fully convolutional networks (FCN) to this end.
FCNs soon became the mainstream approach to dense prediction based tasks, especially due to its efﬁciency. Besides,
empirical results in the literature showed that stronger
pre-trained features can yet further improve their performance. We thus here base our semantic image segmentation
approach on fully convolutional networks, and will show
the impact of different pre-trained features on ﬁnal segmentation results.
3. Residual networks revisited
We are concerned here with the full pre-activation version of residual networks (ResNet) . For shortcut connections, we consider identity mappings only. We omit
the raw input and the top-most linear classiﬁer for clarity.
Usually, there may be a stem block or several traditional convolution layers directly after the raw input. We omit these also, for the purpose of simplicity.
For the residual Unit i, let yi−1 be the input, and let fi(·)
be its trainable non-linear mappings, also named Block i.
The output of Unit i is recursively deﬁned as:
yi ≡fi(yi−1, wi) + yi−1,
where wi denotes the trainable parameters, and fi(·) is often two or three stacked convolution stages. In the full preactivation version, the components of a stage are in turn a
batch normalization , a rectiﬁed linear unit (ReLU)
non-linearity, and a convolution layer.
3.1. Residual networks unravelled online
Applying Eqn.(1) in one substitution step, we expand the
forward pass into:
y2 = y1+f2(y1,w2)
= y0+f1(y0,w1)+f2(y0+f1(y0,w1),w2)
̸= y0+f1(y0,w1)+f2(y0,w2)+f2(f1(y0,w1),w2),(4)
which describes the unravelled view by Veit et al. , as
shown in the top row of Fig. 1. Since f2(·) is non-linear, we
cannot derive Eqn.(4) from Eqn.(3). So the whole network
is not equivalent to an exponentially growing ensemble of
sub-networks. It is rather, more accurately, described as a
linearly growing ensemble of sub-networks. For the twounit ResNet as illustrated in Fig. 1, there are three, e.g.,
Ma, Mb, and M 2
e , sub-networks respectively corresponding to the three terms in Eqn.(3), i.e., y0, f1(y0, w1), and
f2(y0 + f1(y0, w1), w2).
Veit et al. in showed that the paths which gradients
take through a ResNet are typically far shorter than the total
depth of that network. They thus introduced the idea of effective depth as a measure for the true length of these paths.
By characterising the units of a ResNet given its effective
depth, we illuminate the impact of varying paths that gradients actually take, as in Fig. 1. We here illustrate this impact in terms of small effective depths, because to do so for
larger ones would require diagrams of enormous networks.
The impact is the same, however.
Take the ResNet in Fig. 1 for example again. In an SGD
iteration, the backward gradients are:
· ∆y2 + df1
2 denotes the derivative of f2(·) to its input y1.
When effective depth l ≥2, both terms in Eqn.(7) are nonzeros, which corresponds to the bottom-left case in Fig. 1.
Namely, Block 1 receives gradients from both Mb and M 2
However, when effective depth l = 1, the gradient ∆y2 vanishes after passing through Block 2. Namely, f ′
2 ·∆y2 →0.
So, the second term in Eqn.(7) also goes to zeros, which is
illustrated by the bottom-right case in Fig. 1. The weights
in Block 1 indeed vary across different iterations, but they
are updated only by Mb. To M 1
e , Block 1 is no more than
an additional input providing preprocessed representations,
because Block 1 is not end-to-end trained, from the point of
view of M 1
e . In this case, we name M 1
e to have an actual
depth of one. We say that the ResNet is over-deepened, and
that it cannot be trained in a fully end-to-end manner, even
with those shortcut connections.
Let d be the total number of residual units. We can see
a ResNet as an ensemble of different sub-networks, i.e.,
Mi, i = {0, 1, · · · , d}. The actual depth of Mi is min(i, l).
We show an unravelled three-unit ResNet with different effective depths in Fig. 2. By way of example, note that M1
in Fig. 2 contains only Block 1, whereas M2 contains both
Block 1 and Block 2. Among the cases illustrated, the bottom left example is more complicated, where d = 3 and
From the point of view of M 2
3 , the gradient of
Block 1 is f ′
3 · ∆y3 + f ′
3 · ∆y3, where the ﬁrst term
is non-zero. M 2
3 will thus update Block 1 at each iteration.
Considering the non-linearity in Block 3, it is non-trivial to
tell if this is as good as the fully end-to-end training case, as
illustrated by M3 in the top right example. An investigation
of this issue remains future work.
3.2. Residual networks behaviours revisited
Very deep ResNets. Conventionally, it is not easy to
train very deep networks due to the vanishing gradient problem . To understand how a very deep ResNet is trained,
the observation by Veit et al. is important, i.e., gradients vanish exponentially as the length of paths increases.
Now refer to the top-right example in Fig. 2. This is somewhat similar to the case of a shallow or reasonably deep
ResNet, when d ≤l. At the beginning, the shallowest subnetwork, i.e., M1, converges fast, because it gives Block 1
the largest gradients. From the point of view of M2, Block 2
may also receive large gradients due to the path with a
length of one. However, the input of Block 2 partly depends
Figure 2. The impact of inserting an extra residual unit into a twounit ResNet, which depends on the effective depth l.
on Block 1. It would not be easy for Block 2 to converge before the output of Block 1 stabilises. Similarly, Block 3 will
need to wait for Blocks 1 and 2, and so forth. In this way, a
ResNet seems like an ensemble with a growing number of
sub-networks. Besides, each newly added sub-network will
have a larger actual depth than all the previous ones. Note
that Littwin and Wolf , in a concurrent work, have theoretically showed that ResNets are virtual ensembles whose
depth grows as training progresses. Their result to some
extent coincides with the above described process.
The story will however be different when the actual
depth becomes as large as the effective depth. Refer to the
bottom-right example in Fig. 2. This is somewhat similar
to the case of an over-deepened ResNet, when d is much
larger than l. Again, Block 1 in M1 gets trained and stabilises ﬁrst. However, this time M 1
2 is not fully end-to-end
trained any more. Since M 1
2 gives no gradients to Block 1,
it becomes a one-block sub-network trained on top of some
preprocessed representations, which are obtained by adding
the output of Block 1 up to the original input. In this way,
the newly added sub-network M 1
2 still has an actual depth of
one, which is no deeper than the previous one, i.e., M1, and
so forth for M 1
3 . ResNets thus avoid the vanishing gradient
problem by reshaping themselves into multiple shallower
sub-networks. This is just another view of delivering gradients to bottom layers through shortcut connections. Researchers have claimed that the residual shortcut
connections are not necessary even in very deep networks.
However, there are usually short paths in their proposed networks as well. For example, the 76-layer Inception-v4 network has a much shorter twenty-layer route from its
input to the output. There might be differences in the details between fusion by concatenation (Inception-v4)
and fusion by summation (ResNets). However, the manner of avoiding the vanishing gradient problem is probably
similar, i.e., using shortcuts, either with trainable weights or
not. We are thus not yet in a position to be able to claim that
the vanishing gradient problem has been solved.
Wide ResNets. Conventionally, wide layers are more
prone to over-ﬁtting, and sometimes require extra regularization such as dropout .
However, Zagoruyko and
Komodakis showed the possibility to effectively train
times wider ResNets, even without any extra regularization.
To understand how a wide ResNet is trained, refer to the top
right example in Fig. 2. This simulates the case of a rather
shallow network, when d is smaller than l. We reuse the
weights of Block 1 for four times. Among these, Block 1
is located in three different kinds of circumstances. In the
bottom-most path of the sub-network M3, it is supposed to
learn some low-level features; in M2, it should learn both
low-level and mid-level features; and in M1, it has to learn
everything. This format of weight sharing may suppress
over-ﬁtting, especially for those units far from the top-most
linear classiﬁer. Hence ResNets inherently introduce regularization by weight sharing among multiple very different
sub-networks.
Residual unit choices. For better performance, we hope
that a ResNet should expand into a sufﬁciently large number
of sub-networks, some of which should have large model
capacity. So, given our previous observations, the requirements for an ideal mapping function in a residual unit are, 1)
being strong enough to converge even if it is reused in many
sub-networks, and 2) being shallow enough to enable an
large effective depth. Since it is very hard to build a model
with large capacity using a single trainable layer , the
most natural choice would be a residual unit with two wide
convolution stages. This coincides with empirical results
reported by Zagoruyko and Komodakis . They found
that, among the most trivial structure choices, the best one
is to stack two 3 × 3 convolution stages.
3.3. Wider or deeper?
To summarize the previous subsections, shortcut connections enable us to train wider and deeper networks. As they
growing to some point, we will face the dilemma between
width and depth. From that point, going deep, we will actually get a wider network, with extra features which are not
completely end-to-end trained; going wider, we will literally get a wider network, without changing its end-to-end
characteristic. We have learned the strength of depth from
the previous plain deep networks without any shortcuts,
e.g., the AlexNet and VGGNets . However, it is not
clear whether those extra features in very deep residual networks can perform as well as conventional fully end-to-end
trained features. So in this paper, we only favour a deeper
model, when it can be completely end-to-end trained.
In practice, algorithms are often limited by their spatial costs.
One way is to use more devices, which will
however increase communication costs among them. With
similar memory costs, a shallower but wider network can
have times more number of trainable parameters. Therefore, given the following observations in the literature,
• Zagoruyko and Komodakis found that the performance of a ResNet was related to the number of trainable parameters. Szegedy et al. came to a similar
conclusion, according to the comparison between their
proposed Inception networks.
• Veit et al. found that there is a relatively small
effective depth for a very deep ResNet, e.g., seventeen
residual units for a 110-layer ResNet.
most of the current state-of-the-art models on the ImageNet
classiﬁcation dataset seem over-deepened, e.g., the
200-layer ResNet and 96-layer Inception-ResNet .
The reason is that, to effectively utilize GPU memories, we
should make a model shallow. According to our previous
analysis, paths longer than the effective depth in ResNets
are not trained in a fully end-to-end manner. Thus, we can
remove most of these paths by directly reducing the number of residual units. For example, in our best performing
network, there are exactly seventeen residual units.
With empirical results, we will show that our fully endto-end networks can perform much better than the previous much deeper ResNets, especially as feature extractors.
However, even if a rather shallow network (eight-unit, or
twenty-layer) can outperform ResNet-152 on the ImageNet
classiﬁcation dataset, we will not go that shallow, because
an appropriate depth is vital to train good features.
4. Approach to image classiﬁcation
We show the proposed networks in Fig. 3. There are
three architectures, with different input sizes. Dashed blue
rectangles to denote convolution stages, which are respectively composed of a batch normalization, an ReLU nonlinearity and a convolution layer, following the second version of ResNets . The closely stacked two or three convolution stages denote different kinds of residual units (B1–
B7), with inner shortcut connections . Each kind corresponds to a level, where all units share the same kernel sizes
and numbers of channels, as given in the dashed black rectangles in the left-most column of Fig. 3. As mentioned before, there are two 3×3 convolution layers in most residual
units (B1–B5). However, in B6 and B7, we use bottleneck
structures as in ResNets , except that we adjust the numbers of channels to avoid drastic changes in width. Each of
our networks usually consists of one B6, one B7, and different numbers of B1–B5. For those with a 224×224 input,
we do not use B1 due to limited GPU memories. Each of
the green triangles denotes a down-sampling operation with
a rate of two, which is clear given the feature map sizes of
different convolution stages (in dashed blue rectangles). To
this end, we can let the ﬁrst convolution layer at according
levels have a stride of two. Or, we can use an extra spatial
pooling layer, whose kernel size is three and stride is two.
Global pool.
Classifier
Kernel size
Global pool.
Classifier
Global pool.
Classifier
Figure 3. Overview of our proposed networks with different input
sizes. Note that B1–B7 are respectively a residual unit.
In a network whose classiﬁcation results are reported in this
paper, we always use pooling layers for down-sampling. We
average the top-most feature maps into 4,096-dimensional
ﬁnal features, which matches the cases of AlexNet and
VGGNets . We will show more details about network
structures in Subsection 6.1.
Implementation details. We run all experiments using
the MXNet framework , with four devices (two K80 or
four Maxwell Titan X cards) on a single node. We follow settings in the re-implementation of ResNets by Gross
and Wilber as possible. But, we use a linear learning rate schedule, which was reported as a better choice by
Mishkin et al. . Take Model A in Table 1 for example.
We start from 0.1, and linearly reduce the learning rate to
10−6 within 450k iterations.
5. Approach to semantic image segmentation
Our approach is similar to the fully convolutional networks (FCN) implemented in the ﬁrst version of
DeepLab . However, without getting too many factors
entangled, we in this paper do not introduce any multiscale structures , deep supervision signals ,
or global context features .
Besides, we do not apply any multi-scale testing, model averaging or CRF based
post-processing, except for the test set of ADE20K .
Given a pre-trained network, there are three steps to reshape it into a network suitable for semantic image segmentation, as stated below.
1) Resolution. To generate score maps at 1/8 resolution,
we remove down-sampling operations and increase dilation
rates accordingly in some convolution layers. For clarity,
ﬁrst suppose that we always down-sample features maps using a convolution layer with a stride of two. Take networks
with 224×224 inputs for example. We set stride of the ﬁrst
convolution layer in B5 to one, and increase the dilation rate
from one to two for the following layers; We do the same
thing to the ﬁrst convolution layer in B6 too, and increase
the dilation rate from two to four for the following layers.
In the case of down-sampling using a pooling layer, everything is the same except that we set stride of that pooling
layer to one. Sometimes, we will have to apply a pooling
layer with dilation . On the other hand, we do not make
any change for networks with 56×56 inputs, since there are
only three down-sampling operations in each of them.
It is notable that all down-sampling operations are implemented using spatial pooling layers in our originally pretrained networks. We ﬁnd it harmful for FCNs in our preliminary experiments, probably due to too strong spatial invariance. To this end, we replace several top-most downsampling operations in a network, and then tune it for some
additional iterations. Take Model A in Table 1 for example
again. We remove the top-most three pooling layers (before
B4, B5 and B6), increase the strides of according convolution layers up to two, and tune it for 45k iterations using the
ImageNet dataset , starting from a learning rate of 0.01.
2) Classiﬁer. We remove the top-most linear classiﬁer
and the global pooling layer, and then consider two cases.
For one thing, we follow a basic large ﬁeld-of-view setting in DeepLab-v2 , called ‘1 convolution’. Namely,
we just add back a single linear layer as the new classiﬁer.
For anther, we insert an additional non-linear convolution
stage (without batch normalization) below the linear classi-
ﬁer. This case is called ‘2 convolutions’. Both of the added
layers have 3×3 kernels, with a dilation rate of twelve. The
top-most two-layer classiﬁer thus has a receptive ﬁeld of
392×392 on the ﬁnal feature maps. By default, we let the
number of channels in the hidden layer be 512.
3) Dropout. To alleviate over-ﬁtting, we also apply the
traditional dropout to very wide residual units. The
dropout rate is 0.3 for those with 2,048 channels, e.g., the
last three units in ResNets and the second last units (B6) in
our networks; while 0.5 for those with 4,096 channels, e.g.,
the top-most units (B7) in our networks.
Implementation details.
We ﬁx the moving means
and variations in batch normalization layers during ﬁnetuning . We use four devices on a single node. The
batch size is sixteen, so there are four examples per device.
We ﬁrst tune each network for a number of iterations, keeping the learning rate unchanged at 0.0016. And then, we
reduce the learning rate gradually during another number
of iterations, following a linear schedule . For datasets
with available testing sets, we evaluate these numbers of
iterations on validation sets. During training, we ﬁrst resize an image by a ratio randomly sampled from [0.7, 1.3],
and then generate a sample by cropping one 500×500 subwindow at a randomly selected location.
6. Experimental results
6.1. Image classiﬁcation results
We evaluate our proposed networks1 on the ILSVRC
2012 classiﬁcation dataset , with 1.28 million images
for training, respectively belonging to 1,000 categories. We
report top-1 and top-5 error rates on the validation set. We
compare various networks in Table 1, where we obtain all
the results by testing on a single crop. However, we list
the ten-crop result for VGG16 since it is not inherently a fully convolutional network. For networks trained
with 224×224 inputs, the testing crop size is 320×320, following the setting used by He et al. . For those with
112×112 and 56×56 inputs, we use 160×160 and 80×80
crops respectively. For Inception networks , the testing
crop size is 299×299 . The names of our proposed networks are composed of training crop sizes and the numbers
of residual units on different levels. Take 56-1-1-1-1-9-1-1
for example. Its input size is 56, and there are only one unit
on all levels except for Level 5 (B5 in Fig. 3).
Notable points about the results are as follows.
1) Relatively shallow networks can outperform very deep
ones, which is probably due to large model capacity, coinciding with the results reported by Zagoruyko and Komodakis . For example, the much shallower Model B
achieves similar error rates as ResNet-152, and even runs
slightly faster. And particularly, Model A performs the best
among all the networks.
2) We can trade performance for efﬁciency by using a
small input size. For example, Model D performs slightly
worse than ResNet-152, but is almost two times faster.
This may be useful when efﬁciency is strictly required.
Mishkin et al. also reduced the input size for efﬁciency.
However, they did not remove down-sampling operations
accordingly to preserve the size of ﬁnal feature maps, which
resulted in much degraded performance.
3) Models C, D and E perform comparably, even though
Model C has larger depth and more parameters. This comparison shows the importance of designing a network properly. In these models, we put too many layers on low resolution levels (7×7, B5 in Fig. 3).
6.2. Semantic image segmentation results
We evaluate our proposed networks on four widely used
datasets. When available, we report, 1) the pixel accuracy,
which is the percentage of correctly labelled pixels on a
whole test set, 2) the mean pixel accuracy, which is the
mean of class-wise pixel accuracies, and 3) the mean IoU
1We will release these networks soon.
depth tr. input top-1 top-5 speed
VGG16 , 10 crops
ResNet-50 , our tested
ResNet-101 , our tested
ResNet-152 , our tested
ResNet-152 
ResNet-152 , pre-act.
ResNet-200 , pre-act.
Inception-v4 
Inception-ResNet-v2 
56-1-1-1-1-9-1-1, Model F
112-1-1-1-1-5-1-1, Model E
112-1-1-1-1-9-1-1, Model D
112-1-1-1-1-13-1-1, Model C
224-0-1-1-1-1-1-1
224-0-1-1-1-3-1-1, Model B
224-0-3-3-6-3-1-1, Model A
Table 1. Comparison of networks by top-1 (%) and top-5 (%) errors on the ILSVRC 2012 validation set with 50k images,
obtained using a single crop. Testing speeds (images/second) are
evaluated with ten images/mini-batch using cuDNN 4 on a GTX
980 card. Input sizes during training are also listed. Note that a
smaller size often leads to faster training speed.
score, which is the mean of class-wise intersection-overunion scores.
PASCAL VOC 2012 . This dataset consists of daily
life photos. There are 1,464 labelled images for training
and another 1,449 for validation. Pixels either belong to
the background or twenty object categories, including bus,
car, cat, sofa, monitor, etc. Following the common criteria
in the literature , we augment the dataset with extra
labelled images from the semantic boundaries dataset .
So in total, there are 10,582 images for training.
We ﬁrst compare different networks in Table 2. Notable
points about the results are as follows.
1) We cannot make statistically signiﬁcant improvement
by using ResNet-152 instead of ResNet-101.
Model A performs better than ResNet-152 by 3.4%. Using
one hidden layer leads to a further improvement by 2.1%.
2) The very deep ResNet-152 uses too many memories
due to intentionally enlarged depth. With our settings, it
even cannot be tuned using many mainstream GPUs with
only 12GB memories.
3) Model B performs worse than ResNet-101, even if it
performs better on the classiﬁcation task as shown in Table 1. This shows that it is not reliable to tell a good feature
extractor only depending on its classiﬁcation performance.
And it again shows why we should favour deeper models.
4) Model A2 performs worse than Model A on this
dataset. We initialize it using weights from Model A, and
tune it with the Places 365 data for 45k iterations. This
is reasonable since there are only object categories in this
dataset, while Places 365 is for scene classiﬁcation tasks.
We then compare our method with previous ones on the
pixel acc. mean acc. mean IoU mem.
ResNet-101, 1 conv.
ResNet-152, 1 conv.
Model F, 1 conv.
Model E, 1 conv.
Model D, 1 conv.
Model C, 1 conv.
Model B, 1 conv.
Model A, 1 conv.
Model A2, 1 conv.
Model A, 2 conv.
Table 2. Comparison by semantic image segmentation scores (%)
and GPU memory usages (GB/device) during tuning on the PAS-
CAL VOC val set with 1,449 images. Memory usages are obtained with four images/device using MXNet .
test set in Table 3. Only using the augmented PASCAL
VOC data for training, we achieve a mean IoU score of
82.5%2, which is better than the previous best one by 3.4%.
This is a signiﬁcant margin, considering that the gap between ResNet-based and VGGNet-based methods is 3.8%.
Our method wins for seventeen out of the twenty object categories, which was the ofﬁcial criteria used in the PASCAL
VOC challenges . In some works , models were
further pre-trained using the Microsoft COCO data,
which consists of 120k labelled images. In this case, the
current best mean IoU is 79.7% reported by Chen et al. .
They also used multi-scale structure and CRF-based postprocessing in their submission, which we do not consider
here. Nevertheless, our method outperforms theirs by 2.8%,
which further shows the effectiveness of our features pretrained only using the ImageNet classiﬁcation data .
Cityscapes . This dataset consists of street scene photos taken by car-carried cameras. There are 2975 labelled
images for training and another 500 for validation. Besides,
there is also an extended set with 19,998 coarsely labelled
images. Pixels belong to nineteen semantic classes, including road, car, pedestrian, bicycle, etc. These classes further
belong to seven categories, i.e., ﬂat, nature, object, sky, construction, human, and vehicle.
We ﬁrst compare different networks in Table 4.
this dataset, ResNet-152 again shows no advantage against
ResNet-101. However, Model A1 outperforms ResNet-101
by 4.2% in terms of mean IoU scores, which again is a
signiﬁcant margin. Because there are many scene classes,
models pre-trained using Places 365 are supposed to
perform better, which coincides with our results.
We then compare our method with previous ones on the
test set in Table 5. The ofﬁcial criteria on this dataset includes two levels, i.e., class and category. Besides, there
is also an instance-weighted IoU score for each of the two,
which assigns high scores to those pixels of small instances.
2 
pixel acc.
results on the Cityscapes val set
ResNet-101, 1 conv.
ResNet-152, 1 conv.
Model A, 1 conv.
Model A2, 1 conv.
Model A2, 2 conv.
results on the ADE20K val set
ResNet-101, 2 conv.
ResNet-152, 2 conv.
Model E, 2 conv.
Model D, 2 conv.
Model C, 2 conv.
Model A, 2 conv.
Model A2, 2 conv.
Table 4. Comparison by semantic image segmentation scores (%)
on the Cityscapes val set with 500 images, and the ADE20K
val set with 2k images.
Dilation10 
DeepLab-v2∗ 
Context 
Model A2, 2 conv.
Table 5. Comparison by semantic image segmentation scores (%)
on the Cityscapes test set with 1,525 images. DeepLab-v2 
uses ResNet-101 , while others use VGG16 . LRR also
uses the coarse set for training.
Namely, this score penalizes methods ignoring small instances, which may cause fatal problems in vehicle-centric
scenarios. Our method achieves a class-level IoU score of
78.4%3, and outperforms the previous best one by 6.6%.
Furthermore, in the case of instance-weighted IoU score,
our method also performs better than the previous best one
by 6.4%. It is notable that these signiﬁcant improvements
show the strength of our pre-trained features, considering
that DeepLab-v2 uses ResNet-101, and LRR uses
much more data for training.
ADE20K . This dataset consists of both indoor and
outdoor images with large variations. There are 20,210 labelled images for training and another 2k for validation.
Pixels belong to 150 semantic categories, including sky,
house, bottle, food, toy, etc.
We ﬁrst compare different networks in Table 4. On this
dataset, ResNet-152 performs slightly better than ResNet-
101. However, Model A2 outperforms ResNet-152 by 4.0%
in terms of mean IoU scores. Being similar with Cityscapes,
this dataset has many scene categories. So, Model A2 performs slightly better than Model A. Another notable point
is that, Model C takes the second place on this dataset, even
if it performs worse than Model A in the image classiﬁca-
3 
using augmented PASCAL VOC data only
FCN-8s 
76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2
CRFasRNN 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0
DeconvNet 
89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5
87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1
Context 
90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3
VeryDeep∗ 
91.9 48.1 93.4 69.3 75.5 94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2 86.5 87.2 64.6 90.1 59.7 85.5 72.7 79.1
Model A, 2 conv. 94.4 72.9 94.9 68.8 78.4 90.6 90.0 92.1 40.1 90.4 71.7 89.9 93.7 91.0 89.1 71.3 90.7 61.3 87.7 78.1 82.5
using augmented PASCAL VOC & COCO data
Context 
94.1 40.4 83.6 67.3 75.6 93.4 84.4 88.7 41.6 86.4 63.3 85.5 89.3 85.6 86.0 67.4 90.1 62.6 80.9 72.5 77.8
DeepLab-v2∗ 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5 79.7
Table 3. Comparison with previous results by mean intersection-over-union scores (%) on the PASCAL VOC test set with 1,456 images.
Asterisked methods use ResNet-101 , while others use VGG16 .
ave. of pixel acc. & mean IoU
360+MCG-ICT-CAS SP
SenseCUSceneParsing
models ave. of pixel acc. & mean IoU
360+MCG-ICT-CAS SP
SenseCUSceneParsing
Table 6. Comparison by semantic image segmentation scores (%)
on the ADE20K test set with 3,352 images.
tion task on the ImageNet dataset. This shows that large
model capacity may become more critical in complicated
tasks, since there are more parameters in Model C.
We then compare our method with others on the test set
in Table 6. The ofﬁcial criteria on this dataset is the average of pixel accuracies and mean IoU scores. For better performance, we apply multi-scale testing, model averaging and post-processing with CRFs.
Our Model A2
performs the best among all methods using only a single
pre-trained model. However, in this submission, we only
managed to include two kinds of pre-trained features, i.e.,
Models A and C. Nevertheless, our method only performs
slightly worse than the winner by a margin of 0.47%.
PASCAL Context . This dataset consists of images
from PASCAL VOC 2010 with extra object and stuff labels. There are 4,998 images for training and another 5,105
for validation. Pixels either belong to the background category or 59 semantic categories, including bag, food, sign,
ceiling, ground and snow. All images in this dataset are no
larger than 500×500. Since the test set is not available, here
we directly apply the hyper-parameters which are used on
pixel acc. mean acc. mean IoU
FCN-8s 
BoxSup 
Context 
VeryDeep 
ResNet-101
DeepLab-v2 ResNet-101
Model A2, 2 conv.
Table 7. Comparison by semantic image segmentation scores (%)
on the PASCAL Context val set with 5,105 images.
the PASCAL VOC dataset. Our method again performs the
best with a clear margin by all the three kinds of scores, as
shown in Table 7. In particular, we improve the IoU score
by 2.4% compared to the previous best method .
7. Conclusion
We have analysed the ResNet architecture, in terms of
the ensemble classiﬁers therein and the effective depths of
the residual units. On the basis of that analysis we calculated a new, more spatially efﬁcient, and better performing architecture which actually achieves fully end-to-end
training for large networks.
Using this new architecture
we designed a group of correspondingly shallow networks,
and showed that they outperform the previous very deep
residual networks not only on the ImageNet classiﬁcation
dataset, but also when applied to semantic image segmentation. These results show that the proposed architecture delivers better feature extraction performance than the current
state-of-the-art.
A. Appendix
A.1. Network structures
The graph structures of Model A for the ImageNet classiﬁcation can be accessed at:
 
Model A2 for the PASCAL VOC 2012 segmentation can be accessed at:
 
A.2. Gradients in residual networks
We show results of the experiment on gradients proposed by Veit et al. , with various residual networks. Namely,
for a trained network with n units, we sample individual paths of a certain length k, and measure the norm of gradients that
arrive at the input. Each time, we ﬁrst feed a batch forward through the whole network; then during the backward pass,
we randomly sample k units. For them, we only propagate gradients through their trainable mapping functions, but without
their shortcut connections. For the remaining n −k units, we do the opposite, namely, only propagating gradients through
their shortcut connections. We record the norm of those gradients that reach the input for varying path length k, and show
the results in Fig. 4. Note the varying magnitude and maximum path length in individual ﬁgures. These are compared to
the middle part of Fig. 6 in . However, differently we further divide the computed norm of a batch by its number of
examples. According to the results in Fig. 4, ResNet-110 trained on CIFAR-10, as well as ResNet-101 and ResNet-152
trained on ILSVRC 2012, generate much smaller gradients from their long paths than from their short paths. In contrast, our
Model A trained on ILSVRC 2012, generates more comparable gradients from its paths with different lengths.
path length
gradient magnitude at input
ResNet-110 on CIFAR-10
path length
gradient magnitude at input
ResNet-101 on ILSVRC 2012
path length
gradient magnitude at input
ResNet-152 on ILSVRC 2012
path length
gradient magnitude at input
Model A on ILSVRC 2012
Figure 4. Gradient magnitude at input given a path length k in various residual networks. See the text for details.
A.3. Qualitative results
We show qualitative results of semantic image segmentation on PASCAL VOC , Cityscapes , ADE20K , and
PASCAL Context , respectively in Figs. 5, 6, 9, 10 and 11, and show some failure cases in Figs 7 and 8. In a difference
map, grey and black respectively denotes correctly and wrongly labelled pixels, while white denotes the ofﬁcially ignored
pixels during evaluation. Note that we do not apply post-processing with CRFs, which can smooth the output but is too slow
in practice, especially for large images.
Figure 5. Qualitative results on the PASCAL VOC 2012 val set. The model was trained using the train set augmented using SBD .
In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map
between the ground-truth and the predicted label.
Figure 6. Qualitative results on the Cityscapes val set. The model was trained using the train set. In each example, from top to bottom,
there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted
Failure cases on the PASCAL VOC 2012 val set. The model was trained using the train set augmented using SBD .
In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map
between the ground-truth and the predicted label.
Figure 8. Failure cases on the Cityscapes val set. The model was trained using the train set. In each example, from top to bottom, there
are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted
Figure 9. Qualitative results on the ADE20K val set. The model was trained using the train set. In each example, from top to bottom,
there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted
Figure 10. More qualitative results on the ADE20K val set. The model was trained using the train set. In each example, from top to
bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the
predicted label.
Figure 11. Qualitative results on the PASCAL Context val set. The model was trained using the train set. In each example, from top
to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and
the predicted label.