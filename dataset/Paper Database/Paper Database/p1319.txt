Relation Networks for Object Detection
Han Hu1∗Jiayuan Gu2∗†
Zheng Zhang1∗
Jifeng Dai1
Yichen Wei1
1Microsoft Research Asia
2Department of Machine Intelligence, School of EECS, Peking University
{hanhu,v-jiaygu,zhez,jifdai,yichenw}@microsoft.com
Although it is well believed for years that modeling relations between objects would help object recognition, there
has not been evidence that the idea is working in the deep
learning era. All state-of-the-art object detection systems
still rely on recognizing object instances individually, without exploiting their relations during learning.
This work proposes an object relation module. It processes a set of objects simultaneously through interaction
between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and
in-place. It does not require additional supervision and is
easy to embed in existing networks. It is shown effective on
improving object recognition and duplicate removal steps in
the modern object detection pipeline. It veriﬁes the efﬁcacy
of modeling object relations in CNN based detection. It
gives rise to the ﬁrst fully end-to-end object detector. Code
is available at 
Relation-Networks-for-Object-Detection.
1. Introduction
Recent years have witnessed signiﬁcant progress in object detection using deep convolutional neutral networks
(CNNs) . The state-of-the-art object detection methods mostly follow the region
based paradigm since it is established in the seminal work
R-CNN . Given a sparse set of region proposals, object
classiﬁcation and bounding box regression are performed
on each proposal individually. A heuristic and hand crafted
post-processing step, non-maximum suppression (NMS), is
then applied to remove duplicate detections.
It has been well recognized in the vision community for
years that contextual information, or relation between objects, helps object recognition . Most such works are before the prevalence of deep
learning. During the deep learning era, there is no signiﬁ-
∗Equal contribution. †This work is done when Jiayuan Gu is an intern
at Microsoft Research Asia.
cant progress about exploiting object relation for detection
learning. Most methods still focus on recognizing objects
separately.
One reason is that object-object relation is hard to model.
The objects are at arbitrary image locations, of different
scales, within different categories, and their number may
vary across different images. The modern CNN based methods mostly have a simple regular network structure .
It is unclear how to accommodate above irregularities in existing methods.
Our approach is motivated by the success of attention
modules in natural language processing ﬁeld . An
attention module can effect an individual element (e.g., a
word in the target sentence in machine translation) by aggregating information (or features) from a set of elements
(e.g., all words in the source sentence). The aggregation
weights are automatically learnt, driven by the task goal.
An attention module can model dependency between the
elements, without making excessive assumptions on their
locations and feature distributions. Recently, attention modules have been successfully applied in vision problems such
as image captioning .
In this work, for the ﬁrst time we propose an adapted attention module for object detection. It is built upon a basic
attention module. An apparent distinction is that the primitive elements are objects instead of words. The objects have
2D spatial arrangement and variations in scale/aspect ratio.
Their locations, or geometric features in a general sense,
play a more complex and important role than the word location in an 1D sentence. Accordingly, the proposed module
extends the original attention weight into two components:
the original weight and a new geometric weight. The latter
models the spatial relationships between objects and only
considers the relative geometry between them, making the
module translation invariant, a desirable property for object
recognition. The new geometric weight proves important in
our experiments.
The module is called object relation module. It shares
the same advantages of an attention module. It takes vari-
 
Image Feature
Generation
Recognition
Region Feature
Extraction
Figure 1. Current state-of-the-art object detectors are based on a
four-step pipeline. Our object relation module (illustrated as red
dashed boxes) can be conveniently adopted to improve both instance recognition and duplicate removal steps, resulting in an
end-to-end object detector.
able number of inputs, runs in parallel (as opposed to sequential relation modeling ), is fully differentiable and is in-place (no dimension change between input
and output). As a result, it serves as a basic building block
that is usable in any architecture ﬂexibly.
Speciﬁcally, it is applied to several state-of-the-art object detection architectures and show consistent improvement. As illustrated in Figure 1, it is applied
to improve the instance recognition step and learn the duplicate removal step (see Section 4.1 for details). For instance recognition, the relation module enables joint reasoning of all objects and improves recognition accuracy
(Section 4.2). For duplicate removal, the traditional NMS
method is replaced and improved by a lightweight relation
network (Section 4.3), resulting in the ﬁrst end-to-end object detector (Section 4.4), to our best knowledge.
In principle, our approach is fundamentally different
from and would complement most (if not all) CNN based
object detection methods. It exploits a new dimension: a
set of objects are processed, reasoned and affect each other
simultaneously, instead of recognized individually.
The object relation module is general and not limited to
object detection. We do not see any reason preventing it
from ﬁnding broader applications in vision tasks, such as instance segmentation , action recognition , object relationship detection , caption , VQA , etc. Code
is available at 
Relation-Networks-for-Object-Detection.
2. Related Works
Object relation in post-processing Most early works
use object relations as a post-processing step .
The detected objects are re-scored by considering object relationships. For example, co-occurrence,
which indicates how likely two object classes can exist in
a same image, is used by DPM to reﬁne object scores.
The subsequent approaches try more complex relation models, by taking additional position and size into
account. We refer readers to for a more detailed survey. These methods achieve moderate success in pre-deep
learning era but do not prove effective in deep ConvNets. A
possible reason is that deep ConvNets have implicitly incorporated contextual information by the large receptive ﬁeld.
Sequential relation modeling Several recent works perform sequential reasoning (LSTM and spatial memory network (SMN) ) to model object relations. During
detection, objects detected earlier are used to help ﬁnding
objects next. Training in such methods is usually sophisticated. More importantly, they do not show evidence of
improving the state-of-the-art object detection approaches,
which are simple feed-forward networks.
In contrast, our approach is parallel for multiple objects.
It naturally ﬁts into and improves modern object detectors.
Human centered scenarios Quite a few works focus on
human-object relation . They usually require
additional annotations of relation, such as human action. In
contrast, our approach is general for object-object relation
and does not need additional supervision.
Duplicate removal In spite of the signiﬁcant progress
of object detection using deep learning, the most effective
method for this task is still the greedy and hand-crafted nonmaximum suppression (NMS) and its soft version . This
task naturally needs relation modeling. For example, NMS
uses simple relations between bounding boxes and scores.
Recently, GossipNet attempts to learn duplicate removal by processing a set of objects as a whole, therefore sharing the similar spirit of ours. However, its network is speciﬁcally designed for the task and very complex
(depth>80). Its accuracy is comparable to NMS but computation cost is demanding. Although it allows end-to-end
learning in principle, no experimental evidence is shown.
In contrast, our relation module is simple, general and
applied to duplicate removal as an application. Our network
for duplicate removal is much simpler, has small computation overhead and surpasses SoftNMS . More importantly, we show that an end-to-end object detection learning
is feasible and effective, for the ﬁrst time.
Attention modules in NLP and physical system modeling Attention modules have recently been successfully applied in the NLP ﬁeld and in physical system
modeling . The attention module can
well capture the long-term dependencies in these problems.
In NLP, there is a recent trend of replacing recurrent neural
networks by attention models, enabling parallelized implementations and more efﬁcient learning .
Our method is motivated by these works. We extend attention modeling to the important problem of object detection. For modeling visual object relations, their locations,
or geometric features in a general sense, play a complex
and important role. Accordingly, the proposed module introduces a novel geometric weight to capture the spatial relationship between objects. The novel geometric weight is
translational invariant, which is an important property for
Algorithm 1 Object relation module. Input is N objects
n=1. Dimension of appearance feature fA is df.
After each algorithm line is the computation complexity.
1: hyper param: number of relations Nr
2: hyper param dk: key feature dimension
3: hyper param dg: geometric feature embedding dimension
4: learnt weights: {W r
5: for every (n, r) do
compute {ωmn,r
m=1 using Eq. (5)
compute {ωmn,r
m=1 using Eq. (4)
▷O(dk(2df + N))
compute {ωmn,r}N
m=1 using Eq. (3)
compute f r
R(n) using Eq. (2)
f /Nr + Ndf /Nr)
10: end for
11: output new feature {f n
n=1 using Eq. (6)
visual modeling.
3. Object Relation Module
We ﬁrst review a basic attention module, called “Scaled
Dot-Product Attention” . The input consists of queries
and keys of dimension dk, and values of dimension dv. Dot
product is performed between the query and all keys to obtain their similarity. A softmax function is applied to obtain the weights on the values. Given a query q, all keys
(packed into matrices K) and values (packed into V ), the
output value is weighted average over input values,
vout = softmax(qKt
We now describe object relation computation. Let an
object consists of its geometric feature fG and appearance
feature fA. In this work, fG is simply a 4-dimensional object
bounding box and fA is up to the task (Section 4.2 and 4.3).
Given input set of N objects {(f n
n=1, the relation
feature fR(n) of the whole object set with respect to the nth
object, is computed as
ωmn · (WV · f m
The output is a weighted sum of appearance features
from other objects, linearly transformed by WV (corresponding to values V in Eq. (1)). The relation weight ωmn
indicates the impact from other objects. It is computed as
G · exp(ωkn
Appearance weight ωmn
is computed as dot product,
similarly as in Eq. (1),
= dot(WKf m
Both WK and WQ are matrices and play a similar role
as K and Q in Eq. (1). They project the original features
scaled dot
weight eq. (3)
mat multiply
Figure 2. Left: object relation module as Eq. (6); Right: relation
feature computation as Eq. (2).
A into subspaces to measure how well they match.
The feature dimension after projection is dk.
Geometry weight is computed as
= max{0, WG · EG(f m
There are two steps.
First, the geometry features of
the two objects are embedded to a high-dimensional
representation,
denoted as EG.
To make it invariant
translation
transformations,
4dimensional
log( |xm−xn|
), log( |ym−yn|
), log( wn
wm ), log( hn
This 4-d feature is embedded to a high-dimensional
representation by method in , which computes cosine
and sine functions of different wavelengths. The feature
dimension after embedding is dg.
Second, the embedded feature is transformed by WG into
a scalar weight and trimmed at 0, acting as a ReLU nonlinearity. The zero trimming operation restricts relations
only between objects of certain geometric relationships.
The usage of geometric weight Eq. (5) in the attention
weight Eq. (3) makes our approach distinct from the basic
attention Eq. (1). To validate the effectiveness of Eq. (5),
we also experimented with two other simpler variants. The
ﬁrst is called none. It does not use geometric weight Eq. (5).
is a constant 1.0 in Eq. (3). The second is called unary.
It follows the recent approaches . Speciﬁcally, fG is
embedded into a high-dimension (same as fA) space in the
same way and added onto fA to form the new appearance feature. The attention weight is then computed as none
method. The effectiveness of our geometry weight is validated in Table 1(a) and Section 5.2.
An object relation module aggregates in total Nr relation
features and augments the input object’s appearance feature
via addition,
1It is a modiﬁed version of the widely used bounding box regression
target . The ﬁrst two elements are transformed using log(·) to count
more on close-by objects. The intuition behind this modiﬁcation is that we
need to model distant objects while original bounding box regression only
considers close-by objects.
A + Concat[f 1
R(n), ..., f Nr
R (n)], for all n.
Concat(·) is used to aggregate multiple relation features2.
To match the channel dimension, the output channel of each
V is set as
Nr of the dimension of input feature f m
The object relation module Eq. (6) is summarized in Algorithm 1. It is easy to implement using basic operators, as
illustrated in Figure 2.
Each relation function in Eq. (2) is parameterized by four
matrices (WK, WQ, WG, WV ), in total 4Nr. Let df be the
dimension of input feature fA. The number of parameters is
O(Space) = Nr(2dfdk + dg) + d2
Following Algorithm 1, the computation complexity is
O(Comp.) = Ndf(2Nrdk+df)+N 2Nr(dg+dk+df/Nr+1).
Typical parameter value is Nr = 16, dk = 64, dg = 64.
In general, N and df are usually at the scale of hundreds.
The overall computation overhead is low when applied to
modern object detectors.
The relation module has the same input and output dimension, and hence can be regarded as a basic building
block to be used in-place within any network architecture.
It is fully differentiable, and thus can be easily optimized
with back-propagation. Below it is applied in modern object detection systems.
4. Relation Networks For Object Detection
4.1. Review of Object Detection Pipeline
This work conforms to the region based object detection
paradigm. The paradigm is established in the seminal work
R-CNN and includes majority of modern object detectors 3. A four step pipeline is
used in all previous works, as summarized here.
First step generates full image features. From the input
image, a deep convolutional backbone network extracts full
resolution convolutional features (usually 16× smaller than
input image resolution). The backbone network is pre-trained on ImageNet classiﬁcation task 
and ﬁne-tuned during detection training.
Second step generates regional features.
convolutional features and a sparse set of region proposals , a RoI pooling layer extracts
2An alternative is Addition(·). However, its computation cost would
be much higher because we have to match the channel dimensions of two
terms in Eq. (6). Only Concat(·) is experimented in this work.
3Another object detection paradigm is based on dense sliding windows . In this paradigm, the object number N is much larger.
Directly applying relation module as in this work is computationally costly.
How to effectively model relations between dense objects is yet unclear.
ﬁxed resolution regional features (e.g., 7 × 7) for each proposal.
Third step performs instance recognition.
proposal’s regional features, a head network predicts the
probabilities of the proposal belonging to certain object categories, and reﬁne the proposal bounding box via regression. This network is usually shallow, randomly initialized,
and jointly trained together with backbone network during
detection training.
Last step performs duplicate removal. As each object
should be detected only once, duplicated detections on
the same object should be removed. This is usually implemented as a heuristic post-processing step called nonmaximum suppression (NMS). Although NMS works well
in practice, it is manually designed and sub-optimal. It prohibits the end-to-end learning for object detection.
In this work, the proposed object relation module is used
in the last two steps. We show that it enhances the instance
recognition (Section 4.2) and learns duplicate removal (Section 4.3). Both steps can be easily trained, either independently or jointly (Section 4.4). The joint training further
boosts the accuracy and gives rise to the ﬁrst end-to-end
general object detection system.
Our implementation of different architectures To validate the effectiveness and generality of our approach, we
experimented with different combination of state-of-theart backbone networks (ResNet ), and best-performing
detection architectures including faster RCNN , feature pyramid networks (FPN) , and deformable convolutional network (DCN) . Region proposal network
(RPN) is used to generate proposals.
• Faster RCNN . It is directly built on backbone networks such as ResNet . Following , RPN is
applied on the conv4 feature maps. Following ,
the instance recognition head network is applied on a
new 256-d 1 × 1 convolution layer added after conv5,
for dimension reduction. Note that the stride in conv5
is changed from 2 to 1, as common practice .
• FPN . Compared to Faster RCNN, it modiﬁes the
backbone network by adding top-down and lateral connections to build a feature pyramid that facilitates endto-end learning across different scales. RPN and head
networks are applied on features of all scales in the
pyramid. We follow the training details in .
• DCN . Compared to Faster RCNN, it modiﬁes the
backbone network by replacing the last few convolution layers in conv5 by deformable convolution layers.
It also replace the standard RoI pooling by deformable
RoI pooling. We follow the training details in .
Despite the differences, a commonality in above architectures is that they all adopt the same head network structure, that is, the RoI pooled regional features undergo two
fully connected layers (2fc) to generate the ﬁnal features for
proposal classiﬁcation and bounding box regression.
Below, we show that relation modules can enhance the
instance recognition step using the 2fc head.
4.2. Relation for Instance Recognition
Given the RoI pooled features for nth proposal, two fc
layers with dimension 1024 are applied. The instance classiﬁcation and bounding box regression are then performed
via linear layers. This process is summarized as
−−−−−−→(scoren, bboxn)
The object relation module (Section 3, Algorithm 1)
can transform the 1024-d features of all proposals without
changing the feature dimension. Therefore, it can be used
after either fc layer in Eq. (9) for arbitrary number of times4.
Such enhanced 2fc+RM (RM for relation module) head is
illustrated in Figure 3 (a) and summarized as
{RoI Featn}N
−−−−−→1024 · N
−−−−−→1024 · N
−−−−−−→{(scoren, bboxn)}N
In Eq. (10), r1 and r2 indicate how many times a relation module is repeated. Note that a relation module also
needs all proposals’ bounding boxes as input. This notation
is neglected here for clarify.
Adding relation modules can effectively enhance the instance recognition accuracy. This is veriﬁed via comprehensive ablation studies in experiments (Section 5.1).
4.3. Relation for Duplicate Removal
The task of duplicate removal naturally requires exploiting the relation between objects. The heuristic NMS
method is a simple example: the object with the highest
score will erase its nearby objects (geometric relation) with
inferior scores (score relation).
In spite of its simplicity, the greedy nature and manually chosen parameters in NMS makes it a clear sub-optimal
choice. Below we show that the proposed relation module
can learn to remove duplicate in a manner that is simple as
well but more effective.
Duplicate removal is a two class classiﬁcation problem. For each ground truth object, only one detected object
4The relation module can also be used directly on the regional features.
The high dimension (256×72 = 12544 in our implementation), however,
introduces large computational overhead. We did not do this experiment.
rank embed
back propagation
back propagation
(a) enhanced 2fc head
(b) duplicate removal network
Figure 3. Illustration of enhanced 2fc head (a) and duplicate classiﬁcation network (b) by object relation modules.
matched to it is classiﬁed as correct. Others matched to it
are classiﬁed as duplicate.
This classiﬁcation is performed via a network, as illustrated in Figure 3 (b). The input is a set of detected objects
(output from instance recognition, Eq. (9) or (10)). Each
object has its ﬁnal 1024-d feature, the classiﬁcation score
s0, and bounding box. The network outputs a binary classi-
ﬁcation probability s1 ∈ (1 for correct and 0 for duplicate) for each object. The multiplication of two scores s0s1
is the ﬁnal classiﬁcation score. Therefore, a good detection
should have both scores large.
The network has three steps. First, the 1024-d feature
and classiﬁcation score is fused to generate the appearance
feature. Second, a relation module transforms such appearance features of all objects. Last, the transformed features
of each object pass a linear classiﬁer (Ws in Figure 3 (b))
and sigmoid to output the probability ∈ .
The relation module is at the core of the network. It enables effective end-to-end learning using information from
multiple sources (the bounding boxes, original appearance
features and classiﬁcation scores). In addition, the usage of
the classiﬁcation scores also turns out important.
Rank feature We found that it is most effective to transform the score into a rank, instead of using its value. Specifically, the input N objects are sorted in descending order of
their scores. Each object is given a rank ∈[1, N] accordingly. The scalar rank is then embedded into a higher dimensional 128-d feature, using the same method as for
geometry feature embedding in Section 3.
Both the rank feature and original 1024-d appearance
feature are transformed to 128-d (via WfR and Wf in Figure 3 (b), respectively), and added as the input to the relation
Which object is correct? Given a number of detected
objects, it is not immediately clear which one should be
matched to a ground truth object as correct. The most obvious choice would be following the evaluation criterion of
Pascal VOC or COCO datasets . That is, given a
predeﬁned threshold η for the IoU between detection box
and ground truth box, all detection boxes with IoU ≥η are
ﬁrstly matched to the same ground truth. The detection box
with highest score is correct and others are duplicate.
Consequently, such selection criteria work best when
learning and evaluation use the same threshold η . For example, using η = 0.5 in learning produces best 
metric but not mAP @0.75. This is veriﬁed in Table 4.
This observation suggests a unique beneﬁt of our approach that is missing in NMS: the duplicate removal step
can be adaptively learnt according to needs, instead of using preset parameters. For example, a large η should be
used when a high localization accuracy is desired.
Motivated by the COCO evaluation criteria ( −
0.95), our best practice is to use multiple thresholds simultaneously, i.e., η ∈{0.5, 0.6, 0.7, 0.8, 0.9}. Speciﬁcally,
the classiﬁer Ws in Figure. 3 (b) is changed to output multiple probabilities corresponding to different IoU thresholds
and correct detections, resulting in multiple binary classi-
ﬁcation loss terms. The training is well balanced between
different cases. During inference, the multiple probabilities
are simply averaged as a single output.
Training The binary cross entropy loss is used on the
ﬁnal score (multiplication of two scores, see Figure 3 (b)).
The loss is averaged over all detection boxes on all object
categories. A single network is trained for all object categories.
Note that the duplicate classiﬁcation problem is extremely imbalanced. Most detections are duplicate. The
ratio of correct detections is usually < 0.01. Nevertheless,
we found the simple cross entropy loss works well. This
is attributed to the multiplicative behavior in the ﬁnal score
s0s1. Because most detections have very small s0 (mostly
< 0.01) and thus small s0s1. The magnitude of their loss
values L = −log(1 −s0s1) (for non-correct object) and
back-propagated gradients ∂L/∂s1 = s0/(1−s0s1) is also
very small and does not affect the optimization much. Intuitively, training is focused on a few real duplicate detections
with large s0. This shares the similar spirit to the recent focal loss work , where majority insigniﬁcant loss terms
are down weighted and play minor roles during optimization.
Inference The same duplicate removal network is applied for all object categories independently.
glance, the runtime complexity could be high, when the
number of object classes (80 for COCO dataset ) and
detections (N = 300) is high. Nevertheless, in practice
most detections’ original score s0 is nearly 0 in most object
classes. For example, in the experiments in Table 4, only
12.0% classes have detection scores > 0.01 and in these
classes only 6.8% detections have scores > 0.01.
After removing these insigniﬁcant classes and detections, the ﬁnal recognition accuracy is not affected. Running the duplicate removal network on remaining detections
is practical, taking about 2 ms on a Titan X GPU. Note
that NMS and SoftNMS methods are sequential and take
about 5 ms on a CPU . Also note that the recent learning NMS work uses a very deep and complex network
(depth up to 80), which is much less efﬁcient than ours.
4.4. End-to-End Object Detection
The duplicate removal network is trained alone in Section 4.3. Nevertheless, there is nothing preventing the training to be end-to-end. As indicated by the red arrows in
Figure 3 (b), the back propagated gradients can pass into
the original 1024-d features and classiﬁcation scores, which
can further propagate back into the head and backbone networks.
Our end-to-end training simply combines the region proposal loss, the instance recognition loss in Section 4.2
and duplicate classiﬁcation loss in Section 4.3, with equal
weights. For instance recognition, either the original head
Eq. (9) or enhanced head Eq. (10) can be used.
The end-to-end training is clearly feasible, but does it
work? At a ﬁrst glance, there are two issues.
First, the goals of instance recognition step and duplicate
removal step seem contradictory. The former expects all objects matched to the same ground truth object to have high
scores. The latter expects only one of them does. In our experiment, we found the end-to-end training works well and
converges equally fast for both networks, compared to when
they are trained individually as in Section 4.2 and 4.3. We
believe this seemingly conﬂict is reconciled, again, via the
multiplicative behavior in the ﬁnal score s0s1, which makes
the two goals complementary other than conﬂicting. The instance recognition step only needs to produce high score s0
for good detections (no matter duplicate or not). The duplicate removal step only needs to produce low score s1 for
duplicates. The majority non-object or duplicate detection
is correct as long as one of the two scores is correct.
Second, the binary classiﬁcation ground truth label in the
duplicate removal step depends on the output from the instance recognition step, and changes during the course of
end-to-end training. However, in experiments we did not
observe adverse effects caused by this instability. While
there is no theoretical evidence yet, our guess is that the duplicate removal network is relatively easy to train and the
instable label may serve as a means of regularization.
As veriﬁed in experiments (Section 5.3), the end-to-end
training improves the recognition accuracy.
5. Experiments
All experiments are performed on COCO detection
datasets with 80 object categories . A union of 80k
train images and a 35k subset of val images are used for
training . Most ablation experiments report detection
accuracies on a subset of 5k unused val images (denoted as
minival) as common practice . Table 5 also reports
accuracies on test-dev for system-level comparison.
For backbone networks, we use ResNet-50 and ResNet-
101 . Unless otherwise noted, ResNet-50 is used.
For Faster RCNN and DCN , our training
mostly follow . For FPN , our training mostly follow . See Appendix for details.
5.1. Relation for Instance Recognition
In this section, NMS with IoU threshold of 0.6 is used
for duplicate removal for all experiments.
Relation module improves instance recognition Table 1 compares the baseline 2fc head in Eq. (9) with the
proposed 2fc + RM head in Eq. (10), under various parameters.
We ﬁrstly note that our baseline implementation achieves
reasonable accuracy (29.6 mAP) when compared with the
literature (e.g., reports 28.0 using ResNet-50 and 
reports 29.4 using ResNet-101).
Ablation studies are performed on three key parameters.
Usage of geometric feature. As analyzed in Section 3,
our usage of geometric feature in Eq. (5) is compared to
two plain implementations. Results show that our approach
is the best, although all the three surpass the baseline.
Number of relations Nr. Using more relations steadily
improves the accuracy. The improvement saturates at Nr =
16, where +2.3 mAP gain is achieved.
Number of modules.
Using more relation modules
steadily improves accuracy, up to +3.2 mAP gain. As this
also increases the parameter and computation complexity,
by default r1 = 1, r2 = 1 is used.
Does the improvement come from more parameters
or depths? Table 2 answers this question by enhancing the
baseline 2fc head (a) in width or depth such that its complexity is comparable to that of adding relation modules.
A wider 2fc head (1432-d, b) only introduces small improvement (+0.1 mAP). A deeper 3fc head (c) deteriorates
the accuracy (-0.6 mAP), probably due to the difﬁculty of
training. To make the training easier, residual blocks 
are used5 (d), but only moderate improvement is observed
(+0.3 mAP). When global context is used , no improvement is observed. By contrast, our approach (f) signiﬁcantly
improves the accuracy (+2.3 mAP).
We also consider another baseline which concatenates
the original pooled features with the ones from a 2× larger
RoI (g), the performance is improved from 29.6 to 30.4
5Each residual branch in a block has three 1024-d fc layers to have
similar complexity as an object relation module. The residual blocks are
inserted at the same positions as our object relation modules.
mAP, indicating a better way of utilizing context cues. In
addition, we combine this new head with relation modules,
that is, replacing the 2fc with {r1, r2}={1, 1} (h). We get
32.5 mAP, which is 0.6 better than setting (f) (31.9 mAP).
This indicates that using a larger window context and relation modules are mostly complementary.
When more residual blocks are used and the head network becomes deeper (i), accuracy no longer increases.
While, accuracy is continually improved when more relation modules are used (j).
The comparison indicates that the relation module is effective and the effect is beyond increasing network capacity.
Complexity In each relation module, df = 1024, dk =
64, dg = 64. When Nr = 16, a module has about 3 million
parameters and 1.2 billion FLOPs, as from Eq. (7) and (8).
The computation overhead is relatively small, compared to
the complexity of whole detection networks as shown in Table. 5 (less than 2% for faster RCNN / DCN and
about 8% for FPN ).
5.2. Relation for Duplicate Removal
All the experiments in this section use the detected objects of the Faster RCNN baseline 2fc head in Table 1 (top
row, 29.6 mAP after NMS) for training and inference of our
approach in Section 4.3.
In our approach, the relation module parameters are set
as df = 128, dk = 64, dg = 64, Nr = 16, N = 100. Using
larger values no longer increases accuracy. The duplicate
removal network has 0.33 million parameters and about 0.3
billion FLOPs. This overhead is small, about 1% in both
model size and computation compared to a faster RCNN
baseline network with ResNet-50.
Table 3 investigates the effects of different input features
to the relation module (Figure 3 (b)). Using η = 0.5, our
approach improves the mAP to 30.3. When the rank feature
is not used, mAP drops to 26.6. When the class score s0
replaces the rank in a similar way (the score is embedded to
128-d), mAP drops to 28.3. When 1024-d appearance feature is not used, mAP slightly drops to 29.9. These results
suggest that rank feature is most crucial for ﬁnal accuracy.
When geometric feature is not used, mAP drops to 28.1.
When it is used by unary method as mentioned in Section 3
and in Table 1 (a), mAP drops to 28.2. These results verify
the effectiveness of our usage of geometric weight Eq. (5).
Comparison to NMS Table 4 compares our method with
NMS method and its better variant SoftNMS , which is
also the state-of-the-art method for duplicate removal.
Note that all three methods have a single parameter of
similar role of controlling the localization accuracy: the IoU
threshold Nt in NMS, the normalizing parameter σ in Soft-
NMS , and the ground truth label criteria parameter η in
ours. Varying these parameters changes accuracy under different localization metrics. However, it is unclear how to set
2fc baseline (a): usage of geometric feature
(b): number of relations Nr
(c): number of relation modules {r1, r2}
none unary
{1, 0} {0, 1} {1, 1}* {2, 2} {4, 4}
30.5 30.6 31.3 31.7 31.9 31.7
Table 1. Ablation study of relation module structure and parameters (* for default). mAP@all is reported.
(a) 2fc (1024)
(b) 2fc (1432)
(c) 3fc (1024)
(d) 2fc+res {r1, r2}={1, 1}
(e) 2fc (1024) + global
(f) 2fc+RM {r1, r2}={1, 1}
(g) 2fc (1024) + 2×
(h) 2fc+2×+RM {r1, r2}={1, 1} 32.5
(i) 2fc+res {r1, r2}={2, 2}
(j) 2fc+RM {r1, r2}={2, 2}
Table 2. Comparison of various heads with similar complexity.
appearance f
geometric bbox
{fR, f, bbox} none
Table 3. Ablation study of input features for duplicate removal network (none indicates without such feature).
parameters
mAP mAP50 mAP75
η ∈[0.5, 0.9]
ours (e2e) η ∈[0.5, 0.9]
Table 4. Comparison of NMS methods and our approach (Section 4.3). Last row uses end-to-end training (Section 4.4).
the optimal parameters for NMS methods, other than trialand-error. Our approach is easy to interpret because the parameter η directly specify the requirement on localization
accuracy. It performs best for mAP50 when η = 0.5, for
mAP75 when η = 0.75, and for mAP when η ∈[0.5, 0.9].
Our ﬁnal mAP accuracy is better than NMS and Soft-
NMS, establishing the new state-of-the-art. In the following
end-to-end experiments, η ∈[0.5, 0.9] is used.
Figure 4. Representative examples with high relation weights in
Eq. (3). The reference object n is blue. The other objects contributing a high weight (shown on the top-left) are yellow.
5.3. End-to-End Object Detection
The last row in Table 4 compares the end-to-end learning
with separate training of instance recognition and duplicate
removal. The end-to-end learning improves the accuracy by
Finally, we investigate our approach on some stronger
backbone networks, i.e., ResNet-101 and better detection architectures, i.e., FPN and DCN in Table 5.
Using faster RCNN with ResNet-101, by replacing the 2fc
head with 2fc+RM head in Table 1 (default parameters), our
approach improves by 2.5 mAP on COCO minival. Further
using duplicate removal network with end2end training, the
accuracy improves further by 0.5 mAP. The improvement
on COCO test-dev is similar. On stronger baselines, e.g.,
DCN and FPN , we also have moderate improvements on accuracy by both feature enhanced network and
duplicate removal with end2end training. Also note that our
implementation of baseline networks has higher accuracy
than that in original works (38.1 versus 33.1 , 37.2 versus 36.2 ).
6. Conclusions
The comprehensive ablation experiments suggest that
the relation modules have learnt information between objects that is missing when learning is performed on individual objects. Nevertheless, it is not clear what is learnt in the
relation module, especially when multiple ones are stacked.
Towards understanding, we investigate the (only) relation module in the {r1, r2} = {1, 0} head in Table 1(c).
Figure 4 show some representative examples with high relation weights. The left example suggests that several objects
overlapping on the same ground truth (bicycle) contribute
to the centering object. The right example suggests that the
person contributes to the glove. While these examples are
intuitive, our understanding of how relation module works
is preliminary and left as future work.
faster RCNN minival 32.2→34.7→35.2 52.9→55.3→55.8 34.2→37.2→38.2 58.3M→64.3M→64.6M 122.2B→124.6B→124.9B
test-dev 32.7→35.2→35.4 53.6→56.2→56.1 34.7→37.8→38.5
minival 36.8→38.1→38.8 57.8→59.5→60.3 40.7→41.8→42.9 56.4M→62.4M→62.8M 145.8B→157.8B→158.2B
test-dev 37.2→38.3→38.9 58.2→59.9→60.5 41.4→42.3→43.3
minival 37.5→38.1→38.5 57.3→57.8→57.8 41.0→41.3→42.0 60.5M→66.5M→66.8M 125.0B→127.4B→127.7B
test-dev 38.1→38.8→39.0 58.1→58.7→58.6 41.6→42.4→42.9
Table 5. Improvement (2fc head+SoftNMS , 2fc+RM head+SoftNMS and 2fc+RM head+e2e from left to right connected by →) in
state-of-the-art systems on COCO minival and test-dev. Online hard example mining (OHEM) is adopted. Also note that the strong
SoftNMS method (σ = 0.6) is used for duplicate removal in non-e2e approaches.
A1. Training Details
For Faster RCNN and DCN , the hyperparameters in training mostly follow . Images are resized such that their shorter side is 600 pixels. The number
of region proposals N is 300. 4 scales and 3 aspect ratios are
adopted for anchors. Region proposal and instance recognition networks are jointly trained. Both instance recognition (Section 5.1) and end-to-end (Section 5.3) training
have ∼450k iterations (8 epochs). Duplicate removal (Section 5.2) training has ∼170k iterations (3 epochs). The
learning rates are set as 2 × 10−3 for the ﬁrst 2
3 iterations
and 2 × 10−4 for the last 1
3 iterations.
For FPN , hyper-parameters in training mostly follow . Images are resized such that their shorter side is
800 pixels. The number of region proposals N is 10006.
5 scales and 3 aspect ratios are adopted for anchors. Region proposal network is trained for about 170k iterations
(3 epochs).
Both instance recognition (Section 5.1) and
end-to-end training (Section 5.3) have ∼340k iterations (6
epochs). The learning rates are set as 5 × 10−3 for the ﬁrst
3 iterations and 5 × 10−4 for the last 1
3 iterations.
For all training, SGD is performed on 4 GPUs with 1
image on each. Weight decay is 1×10−4 and momentum is
0.9. Class agnostic bounding box regression is adopted
as it has comparable accuracy with the class aware version
but higher efﬁciency.
For instance recognition subnetwork, all N proposals are
used to compute loss. We ﬁnd it has similar accuracy with
the usual practice that a subset of sampled proposals are
used .
We also consider online hard example mining (OHEM) 
approach in Table 5 for better overall baseline performance.
For Faster RCNN and DCN, 128 hard examples are sampled
from 300 proposals. For FPN, 512 are sampled from 1000
proposals.
6In , 2000 are used for training while 1000 are used for test. Here
we use 1000 in both training and test for consistency.