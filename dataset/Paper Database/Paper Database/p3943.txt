Vision Transformers are Robust Learners
Sayak Paul,1* Pin-Yu Chen 2*
1 Carted 2 IBM Research
 , 
Transformers, composed of multiple self-attention layers, hold
strong promises toward a generic learning primitive applicable
to different data modalities, including the recent breakthroughs
in computer vision achieving state-of-the-art (SOTA) standard
accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the
robustness of the Vision Transformer (ViT) against common
corruptions and perturbations, distribution shifts, and natural
adversarial examples. We use six different diverse ImageNet
datasets concerning robust classiﬁcation to conduct a comprehensive performance comparison of ViT models and SOTA
convolutional neural networks (CNNs), Big-Transfer. Through
a series of six systematically designed experiments, we then
present analyses that provide both quantitative and qualitative
indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset
and pre-training combinations, ViT gives a top-1 accuracy of
28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier
spectrum sensitivity, and spread on discrete cosine energy
spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is
available at 
Introduction
Transformers are becoming a preferred
architecture for various data modalities. This is primarily because they help reduce inductive biases that go into designing
network architectures. Moreover, Transformers have been
shown to achieve tremendous parameter efﬁciency without
sacriﬁcing predictive performance over architectures that are
often dedicated to speciﬁc types of data modalities. Attention,
in particular, self-attention is one of the foundational blocks
of Transformers. It is a computational primitive that allows
us to quantify pairwise entity interactions thereby helping a
network learn the hierarchies and alignments present inside
the input data . These are desirable properties to eliminate the
need for carefully designed inductive biases to a great extent.
Although Transformers have been used in prior works
 it was only un-
*These authors contributed equally.
Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
til 2020, the performance of Transformers were on par with
the SOTA CNNs on standard image recognition tasks . Attention has been shown to be an important element for vision
networks to achieve better empirical robustness . Since attention is a core component of ViTs (and
Transformers in general), a question that naturally gets raised
here is – could ViTs be inherently more robust? If so, why
are ViTs more robust learners? In this work, we provide an
afﬁrmative answer to the ﬁrst question and provide empirical
evidence to reason about the improved robustness of ViTs.
Various recent works have opened up the investigation on
evaluating the robustness of ViTs 
but with a relatively limited scope. We build on top of these
and provide further and more comprehensive analyses to
understand why ViTs provide better robustness for semantic shifts, common corruptions and perturbations, and natural adversarial examples to input images in comparison to
SOTA CNNs like Big Transfer (BiT) .
Through a set of carefully designed experiments, we ﬁrst
verify the enhanced robustness of ViTs to common robustness benchmark datasets . We then provide quantitative and qualitative analyses to help understand
the reasons behind this enhancement. In summary, we make
the following contributions:
• We use 6 diverse ImageNet datasets concerning different types of robustness evaluation and conclude that ViTs
achieve signiﬁcantly better performance than BiTs.
• We design 6 experiments, including robustness to masking, energy/loss landscape analysis, and sensitivity to highfrequency artifacts to study ViT’s improved robustness.
• Our analysis provides novel insights for robustness attribution of ViT. Moreover, our robustness evaluation and
analysis tools are generic and can be used to benchmark
and study future image classiﬁcation models.
Related Work
To the best of our knowledge, ﬁrst explored the use of Transformers for the
task of image super-resolution which essentially belongs to
the category of image generation. Image-GPT used Transformers for unsupervised pre-training from
The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)
pixels of images. However, the transfer performance of the
pre-training method is not on par with supervised pre-training
methods. ViT takes the original
Transformers and makes very minimal changes to make it
work with images. In fact, this was one of the primary objectives of ViT i.e. to keep the original Transformer architecture as original as possible and then examining how that
pans out for image classiﬁcation in terms of large-scale pretraining. As noted in , because of the lesser number of inductive biases, ViT
needs to be pre-trained on a relatively larger dataset ) with strong regularization
for achieving reasonable downstream performance. Strong
regularization is particularly needed in the absence of a larger
dataset during pre-training .
Multiple variants of Transformers have been proposed to
show that it is possible to achieve comparable performance on
ImageNet-1k without using additional data. DeIT introduces a novel distillation strategy to learn a student Transformersbased network from a well-performing teacher network based
on RegNets . With this approach,
DeIT achieves 85.2% top-1 accuracy on ImageNet-1k without any external data. T2T-ViT proposes a
novel tokenization method enabling the network to have more
access to local structures of the images. For the Transformerbased backbone, it follows a deep-narrow network topology
inspired by . With proposed changes, T2T-ViT achieves 83.3% top-1 accuracy on
ImageNet-1k. LV-ViT introduces a new
training objective namely token labeling and also tunes the
structure of the Transformers. It achieves 85.4% top-1 accuracy on ImageNet-1k. CLIP and Swin
Transformers are also two recent models that
make use of Transformers for image recognition problems.
In this work, we only focus on ViT .
Concurrent to our work, there are a few recent works that
study the robustness of ViTs from different perspectives. In
what follows, we summarize their key insights and highlight
the differences from our work. showed
that ViTs has better robustness than CNNs against adversarial input perturbations. The major performance gain can be
attributed to the capability of learning high-frequency features that are more generalizable and the ﬁnding that convolutional layers hinder adversarial robustness. studied improved robustness of ViTs over ResNets against adversarial and natural adversarial examples as well as common corruptions. Moreover, it is shown
that ViTs are robust to the removal of almost any single layer.
 studied adversarial robustness of ViTs through various white-box, black-box
and transfer attacks and found that model ensembling can
achieve unprecedented robustness without sacriﬁcing clean
accuracy against a black-box adversary. This paper shows
novel insights that are fundamentally different from these
works: (i) we benchmark the robustness of ViTs on a wide
spectrum of ImageNet datasets (see Table 2), which are the
most comprehensive robustness performance benchmarks to
date; (ii) we design six new experiments to verify the superior
robustness of ViTs over BiT and ResNet models.
Robustness Performance Comparison on
ImageNet Datasets
Multi-head Self Attention (MHSA)
Here we provide a brief summary of ViTs. Central to ViT’s
model design is self-attention . Here, we ﬁrst compute three quantities from linear
projections (X ∈RN×D): (i) Query = XWQ, (ii) Key =
XWK , and (iii) Value = XWV, where WQ, WK, and WV are
linear transformations. The linear projections (X) are computed from batches of the original input data. Self-attention
takes these three input quantities and returns an output matrix
(N × d) weighted by attention scores using (1):
Attention(Q, K, V ) = Softmax
To enable feature-rich hierarchical learning, h self-attention
layers (or so-called ”heads”) are stacked together producing
an output of N × dh. This output is then fed through a linear
transformation layer that produces the ﬁnal output of N × d
from MHSA. MHSA then forms the core Transformer block.
Additional details about ViT’s foundational elements are
provided in Appendix.
Performance Comparison on Diverse
ImageNet Datasets for Robustness Evaluation
In this work, our baseline is a ResNet50V2
model pre-trained on the ImageNet-1k
dataset except for a few results
where we consider ResNet-50 1. To study
how ViTs hold up with the SOTA CNNs we consider BiT
 . At its core, BiT networks are scaledup versions of ResNets with Group Normalization and Weight Standardization layers added in place of Batch Normalization . Since ViT and BiT share similar pre-training strategies and JFT-300 , longer pre-training
schedules, and so on) they are excellent candidates for our
comparison purposes. So, a question, central to our work is:
Where does ViT stand with respect to BiT in terms of
robustness under similar parameter and FLOP
regime, pre-training setup, and data regimes, and
how to attribute their performance difference?
Even though BiT and ViT share similar pre-training schedules and dataset regimes there are differences that are worth
mentioning. For example, ViT makes use of Dropout while BiT does not. ViT is trained using
Adam while BiT is trained using
SGD with momentum. In this work, we focus our efforts
on the publicly available BiT and ViT models only. Later
variants of ViTs have used Sharpness-Aware Minimization
1In these cases, we directly referred to the previously reported
results with ResNet-50.
Figure 1: Mean top-1 accuracy scores (%) on the ImageNet-
C dataset as yielded by different variants of ViT and BiT.
Figure 2: Top-1 accuracy (%) of ViT and BiT for contrast
corruption (with the highest severity level) on ImageNet-C.
# Parameters
ImageNet-1k
ResNet50V2
4144.854528
BiT m-r50x1
BiT m-r50x3
BiT m-r101x1
BiT m-r101x3
387.934888
BiT m-r152x4
186897.679
304.715752
Table 1: Parameter counts, FLOPS (Floating-Point Operations), and top-1 accuracy (%) of different variants of ViT and
BiT. All the reported variants were pre-trained on ImageNet-
21k and then ﬁne-tuned on ImageNet-1k.
 and stronger regularization techniques to
compensate the absence of favored inductive priors . However, we do
not investigate how those aspects relate to robustness in this
Table 1 reports the parameter counts and FLOPS of different ViT and BiT models along with their top-1 accuracy2 on
the ImageNet-1k dataset . It is clear
that different variants of ViT are able to achieve comparable
performance to BiT but with lesser parameters.
In what follows, we compare the performance of ViT and
BiT on six robustness benchmark datasets , as summa-
2Figure 4 of and Table 5 of were used to collect the top-1 accuracy scores.
ImageNet-C
 
corruptions
ImageNet-P
 
perturbations
ImageNet-R
 
Semantic shifts
ImageNet-O
 
Out-of-domain
distribution
ImageNet-A
 
Natural adversarial
ImageNet-9
 
Background
dependence
Table 2: Summary of the studied datasets and their purpose.
rized in Table 2. These datasets compare the robustness of
ViT, BiT and the baseline ResNet50V2 in different perspectives, including (i) common corruptions, (ii) semantic shifts,
(iii) natural adversarial examples, and (iv) out-of-distribution
detection. A summary of the datasets and their purpose is
presented in Table 2 for easier reference.
Notably, in these datasets ViT exhibits signiﬁcantly better
robustness than BiT of comparable parameter counts. Section
4 gives the attribution analysis of improved robustness in ViT.
ImageNet-C 
of 15 types of algorithmically generated corruptions, and
each type of corruption has ﬁve levels of severity. Along with
these, the authors provide additional four types of general
corruptions making a total of 19 corruptions. We consider
all the 19 corruptions at their highest severity level (5) and
Model / Method
BiT m-r101x3
DeepAugment+AugMix
Noisy Student Training
Table 3: mCEs (%) of different models and methods on
ImageNet-C (lower is better). Note that Noisy Student Training incorporates additional training with data augmentation
for noise injection.
Model / Method
BiT-m r101x3
AugMix 
Table 4: mFRs (%) and mT5Ds (%) on ImageNet-P dataset
(lower is better).
report the mean top-1 accuracy in Figure 1 as yielded by the
variants of ViT and BiT. We consistently observe a better
performance across all the variants of ViT under different parameter regimes. Note that BiT m-r50x1 and m-r101x1
have lesser parameters than the lowest variant of ViT (B-16)
but for other possible groupings, variants of ViT have lesser
parameters than that of BiT. Overall, we notice that ViT performs consistently better across different corruptions except
for contrast. In Figure 2, we report the top-1 accuracy of ViT
and BiT on the highest severity level of the contrast corruption. This observation leaves grounds for future research to
investigate why this is the case since varying contrast factors
are quite common in real-world use-cases. Based on our ﬁndings, contrast can be an effective but unexplored approach to
studying ViT’s robustness, similar to the study of human’s
vision performance .
In , mean corruption error
(mCE) is used to quantify the robustness factors of a model
on ImageNet-C. Speciﬁcally, the top-1 error rate is computed
for each of the different corruption (c) types (1 ≤c ≤15)
and for each of the severity (s) levels (1 ≤s ≤5). When
error rates for all the severity levels are calculated for a particular corruption type, their average is stored. This process
is repeated for all the corruption types and the ﬁnal value is
an average over all the average error rates from the different
corruption types. The ﬁnal score is normalized by the mCE
of AlexNet .
We report the mCEs for BiT-m r101x3, ViT L-16, and
a few other models in Table 3. The mCEs are reported for
15 corruptions as done in .
We include two additional models/methods in Table 3 because of the following: (a) Noisy Student Training uses external data and training choices , Stochastic Depth
 , etc.) that are helpful in enhancing the
robustness of a vision model; (b) DeepAugment and AugMix
 are designed
explicitly to improve the robustness of models against corruptions seen in ImageNet-C. This is why, to provide a fair
ground to understand where BiT and ViT stand in comparison to state-of-the-art, we add these two models. It is indeed
interesting to notice that ViT is able to outperform the combination of DeepAugment and AugMix which are speciﬁcally
designed to provide robustness against the corruptions found
in ImageNet-C. As we will discuss in Section 4, this phenomenon can be attributed to two primary factors: (a) better
Figure 3: Top-1 accuracy scores (%) on ImageNet-R dataset.
pre-training and (b) self-attention. It should also be noted
that Noisy Student Training incorporates
various factors during training such as an iterative training
procedure, strong data augmentation transformations from
RandAugment for noise injection, test-time augmentation,
and so on. These factors largely contribute to the improved
robustness gains achieved by Noisy Student Training.
ImageNet-P 
types of common perturbations. Unlike the common corruptions, the perturbations are subtly nuanced spanning across
fewer number of pixels inside images. As per mean ﬂip rate (mFR) and mean top-5 distance (mT5D) are the standard metrics to evaluate a model’s
robustness under these perturbations. They are reported in
Table 4. Since the formulation of mFR and mT5D are more
involved than mCE and for brevity, we refer the reader to
 for more details on these
two metrics. We ﬁnd ViT’s robustness to common perturbations is signiﬁcantly better than BiT as well as AugMix.
ImageNet-R 
contains images labelled with ImageNet labels by collecting renditions of ImageNet classes. It helps verify the robustness of vision networks under semantic shifts under different domains. Figure
3 shows that ViT’s treatment to domain adaptation is better
than that of BiT.
Figure 4: Top-1 accuracy scores (%) on ImageNet-A dataset.
ImageNet-A 
is comprised of natural images that cause misclassiﬁcations due to reasons
such as multiple objects associated with single discrete categories. In Figure 4, we report the top-1 accuracy of ViT and
BiT on the ImageNet-A dataset . In
 , self-attention is noted as an important element to tackle these problems. This may help explain
why ViT performs signiﬁcantly better than BiT in this case.
For example, the top-1 accuracy of ViT L-16 is 4.3x higher
than BiT-m r101x3.
ImageNet-O 
consists of images
that belong to different classes not seen by a model during
its training and are considered as anomalies. For these images, a robust model is expected to output low conﬁdence
scores. We follow the same evaluation approach of using
area under the precision-recall curve (AUPR) as for this dataset. In Figure 5, we report the AUPR
of the different ViT and BiT models on the ImageNet-O
dataset . ViT demonstrates superior
performance in anomaly detection than BiT.
ImageNet-9 
helps to verify the
background-robustness of vision models. In most cases, the
foregrounds of images inform our decisions on what might
be present inside images. Even if the backgrounds change, as
long as the foregrounds stay intact, these decisions should not
be inﬂuenced. However, do vision models exhibit a similar
kind of treatment to image foregrounds and backgrounds?
It turns out that the vision models may break down when
the background of an image is changed .
It may suggest that the vision models may be picking up
unnecessary signals from the image backgrounds. In it is also shown that background-robustness can
be important for determining models’ out of distribution performance. So, naturally, this motivates us to investigate if
ViT would have better background-robustness than BiT. We
ﬁnd that is indeed the case (refer to Table 5). Additionally,
Figure 5: AUPR (higher is better) on ImageNet-O dataset.
in Table 6, we report how well BiT and ViT can detect if the
foreground of an image is vulnerable3. It appears that for this
task also, ViT signiﬁcantly outperforms BiT. Even though we
notice ViT’s better performance than BiT but it is surprising
to see ViT’s performance being worse than ResNet-50. We
suspect this may be due to the simple tokenization process of
ViT to create small image patches that limits the capability
to process important local structures .
Why ViT has Improved Robustness?
In this section, we systematically design and conduct six
experiments to identify the sources of improved robustness
in ViTs from both qualitative and quantitative standpoints.
Attention is Crucial for Improved Robustness
In , the authors study the idea of “Attention Distance” to investigate how ViT uses self-attention
to integrate information across a given image. Speciﬁcally,
they analyze the average distance covered by the learned
attention weights from different layers. One key ﬁnding is
that in the lower layers some attention heads attend to almost
the entirety of the image and some heads attend to small
regions. This introduces high variability in the attention distance attained by different attention heads, particularly in
the lower layers. This variability gets roughly uniform as the
depth of the network increases. This capability of building
rich relationships between different parts of images is crucial
for contextual awareness and is different from how CNNs
interpret images as investigated in .
Since the attention mechanism helps a model learn better
contextual dependencies we hypothesize that this is one of
the attributes for the superior performance ViTs show on
three robustness benchmark datasets. To this end, we study
the performance of different ImageNet-1k models that make
3For details, we refer the reader to the ofﬁcial repository of the
background robustness challenge: 
Mixed-Same
Mixed-Rand
BiT-m r101x3
Table 5: Top-1 accuracy (%) of ImageNet-9 dataset and its different variants. ”BG-Gap” is the gap between ”Mixed-Same”
and ”Mixed-Rand”. It measures how impactful background correlations are in presence of correct-labeled foregrounds.
Accuracy (%)
BiT-m r101x3
Table 6: Performance on detecting vulnerable image foregrounds from ImageNet-9 dataset.
Table 7: Mean top-1 accuracy (%) of BiT (m-r101x3)
and ViT (L-16) with different masking factors.
use of attention in some form (spatial, channel, or both)4.
These models include EfﬁcientNetV2 with
Global Context (GC) blocks , several ResNet
variants with Gather-Excite (GE) blocks and
Selective Kernels (SK) . We also include a ViT
S/16 model pre-trained on ImageNet-1k for a concrete comparison. We summarize our ﬁndings in Table 8. The results
suggest that adding some form of attention is usually a good
design choice especially when robustness aspects are concerned as there is almost always a consistent improvement in
performance compared to that of a vanilla ResNet-50. This is
also suggested by Hendrycks et al. 
but only in the context of ImageNet-A. We acknowledge that
the models reported in Table 8 differ from the corresponding ViT model with respect to their training conﬁgurations,
regularization in particular. But exploring how regularization
affects the robustness aspects of a model is not the question
we investigate in this work.
Self-attention constitutes a fundamental block for ViTs. So,
in a realistic hope, they should be able to perform even better
when they are trained in the right manner to compensate for
the absence of strong inductive priors as CNNs. We conﬁrm
this in Table 8 (last row). Note that the work on AugReg
 showed that it is important to incorporate stronger regularization to train better performing ViTs
in the absence of inductive priors and larger data regimes.
More experiments and attention visualizations showing the
connection between attention and robustness are presented in
Role of Pre-training
ViTs yield excellent transfer performance when they are pretrained on larger datasets . This is why, to better isolate the effects of pretraining with larger data regimes we consider a ViT B/16
4We used implementations from the timm library for this.
model but trained with different conﬁgurations and assess
their performance on the same benchmark datasets as used
in Section 4.1. These conﬁgurations primarily differ in terms
of the pre-training dataset. We report our ﬁndings in the
Table 9. We notice that the model pre-trained on ImageNet-
1k performs worse than the one pre-trained on ImageNet-21k
and then ﬁne-tuned on ImageNet-1k.
Observations from Table 9 lead us to explore another questions i.e., under similar pre-training conﬁgurations how do
the ViT models stand out with respect to BiT models. This further helps to validate which architectures should be preferred
for longer pre-training with larger datasets as far as robustness aspects are concerned. This may become an important
factor to consider when allocating budgets and resources for
large-scale experiments on robustness. Throughout Section 3
and the rest of Section 4, we show that ViT models signiﬁcantly outperform similar BiT models across six robustness
benchmark datasets that we use in this work. We also present
additional experiments in Appendix by comparing ViTs to
BiTs of similar parameters.
ViT Has Better Robustness to Image Masking
In order to further establish that attention indeed plays an important role for the improved robustness of ViTs, we conduct
the following experiment:
• Randomly sample a common set of 1000 images from the
ImageNet-1k validation set.
• Apply Cutout at four different
levels: {5,10,20,50}% and calculate the mean top-1 accuracy scores for each of the levels with BiT (m-r101x3)
and ViT (L-16)5. In Cutout, square regions from input images are randomly masked out. It was originally proposed
as a regularization technique.
5We use these two variants because they are comparable with
respect to the number model parameters.
# Parameters
ImageNet-A
(Top-1 Acc)
ImageNet-R
(Top-1 Acc)
ImageNet-O
4144.854528
EfﬁcientV2 (GC)
ResNet-L (GE)
ResNet-M (GE)
ResNet-S (GE)
ResNet18 (SK)
ResNet34 (SK)
ResNet-50 (SK)
4608.338304
Table 8: Complexity and performance of different attention-fused models on three benchmark robustness datasets. All models
reported here operate on images of size 224 × 224.
Pre-training
ImageNet-A
(Top-1 Acc)
ImageNet-R
(Top-1 Acc)
ImageNet-O
ImageNet-1k
ImageNet-21k
Table 9: Performance of the ViT B/16 model on three benchmark datasets.
BiT-m r101x3
Table 10: Different percentiles (P) of the error matrix computed from Fourier analysis (Figure 6).
Table 7 reports that ViT is able to consistently beat BiT
when square portions of the input images have been randomly
masked out. Randomness is desirable here because ViT can
utilize global information. If we ﬁxate the region of masking
it may be too restrictive for a ViT to take advantage of its
ability to utilize global information. Note that the ViT variant
(L-16) we use in this experiment is shallower than the BiT
variant (m-r101x3). This may suggest that attention indeed
is the strong force behind this signiﬁcant gain.
Fourier Spectrum Analysis Reveals Low
Sensitivity for ViT
A common hypothesis about vision models is that they can
easily pick up the spurious correlations present inside input
data that may be imperceptible and unintuitive to humans
 . To
measure how ViT holds up with this end of the bargain, we
conduct a Fourier analysis of ViT, BiT, and
our baseline ResNet-50. The experiment goes as follows:
• Generate a Fourier basis vector with varying frequencies.
• Add the basis vector to 1000 randomly sampled images
from the ImageNet-1k validation set.
• Record error-rate for every perturbed image and generate a
heatmap of the ﬁnal error matrix.
For additional details on this experiment, we refer the
reader to . In Figure 6, it is noticed that
both ViT and BiT stay robust (have low sensitivity) to most
of the regions present inside the perturbed images while
the baseline ResNet50V2 loses its consistency in the highfrequency regions. The value at location (i, j) shows the
error rate on data perturbed by the corresponding Fourier
basis noise.
The low sensitivity of ViT and BiT may be attributed to the
following factors: (a) Both ViT and BiT are pre-trained on a
larger dataset and then ﬁne-tuned on ImageNet-1k. Using a
larger dataset during pre-training may be acting as a regularizer here . (b) Evidence also suggests
that increased network width has a positive effect on model
robustness . To get a deeper insight into the heatmaps shown in
Figure 6, in Table 10, we report error-rate percentiles for the
three models under consideration. For a more robust model,
we should expect to see lower numbers across all the ﬁve
different percentiles reported in Table 10 and we conﬁrm
that is indeed the case. This may also help explain the better
behavior of BiT and ViT in this experiment.
Adversarial Perturbations of ViT Has Wider
Spread in Energy Spectrum
In , it is shown that small adversarial perturbations can change the decision boundary of
neural networks (especially CNNs) and that adversarial training exploits this sensitivity to induce
robustness. Furthermore, CNNs primarily exploit discriminative features from the low-frequency regions of the input
data. Following , we conduct the
following experiment on 1000 randomly sampled images
from the ImageNet-1k validation set with ResNet-50, BiT-m
r50x3, and ViT B-166:
• Generate small adversarial perturbations (δ) with DeepFool
 with a step
6For computational constraints we used smaller BiT and ViT
variants for this experiment.
Figure 6: Sensitivity heatmap of 2D discrete Fourier transform spectrum . The
low-frequency/high-frequency components are shifted to the center/corner of the spectrum.
Figure 7: Spectral decomposition of adversarial perturbations generated using DeepFool
 . The top-left/bottom-right quadrants denote lowfrequency/high-frequency regions.
size of 507.
• Change the basis of the perturbations with discrete cosine
transform (DCT) to compute the energy spectrum of the
perturbations.
This experiment aims to conﬁrm that ViT’s perturbations
will spread out the whole spectrum, while perturbations of
ResNet-50 and BiT will be centered only around the lowfrequency regions. This is primarily because ViT has the
ability to better exploit information that is only available in a
global context. Figure 7 shows the energy spectrum analysis.
It suggests that to attack ViT, (almost) the entire frequency
spectrum needs to be affected, while it is less so for BiT and
ResNet-50.
ViT Has Smoother Loss Landscape to Input
Perturbations
One way to attribute the improved robustness of ViT over
BiT is to hypothesize ViT has a smoother loss landscape
with respect to input perturbations. Here we explore their
loss landscapes based on a common set of 100 ImageNet-1k
validation images that are correctly classiﬁed by both models.
We apply the multi-step projected gradient descent (PGD)
attack with an ℓ∞perturbation budget
of ϵ = 0.002 when normalizing the pixel value range to be
7Rest of the hyperparameters are same as what is speciﬁed https:
//git.io/JEhpG.
Figure 8: Loss progression (mean and standard deviation)
ViT (L-16) and BiT-m (r101x3) during PGD attacks .
between [−1, 1]8 (refer to Appendix for details on hyperparameters). Figure 8 shows that the classiﬁcation loss (cross
entropy) of ViT increases at a much slower rate than that of
BiT as one varies the attack steps, validating our hypothesis
of smoother loss landscape to input perturbations.
In summary, in this section, we broadly verify that ViT
can yield improved robustness (even with fewer parame-
8We follow the PGD implementation from 
ters/FLOPS in some cases). This indicates that the use of
Transformers can be orthogonal to the known techniques to
improve the robustness of vision models .
Conclusion
Robustness is an important aspect to consider when deploying deep learning models into the wild. This work provides a
comprehensive robustness performance assessment of ViTs
using 6 different ImageNet datasets and concludes that ViT
signiﬁcantly outperforms its CNN counterpart (BiT) and the
baseline ResNet50V2 model. We further conducted 6 new
experiments to verify our hypotheses of improved robustness
in ViT, including the use of large-scale pre-training and attention module, the ability to recognize randomly masked
images, the low sensibility to Fourier spectrum domain perturbation, and the property of wider energy distribution and
smoother loss landscape under adversarial input perturbations. Our analyses and ﬁndings show novel insights toward
understanding the source of robustness and can shed new
light on robust neural network architecture design.
Acknowledgements
We are thankful to the Google Developers Experts program9
(speciﬁcally Soonson Kwon and Karl Weinmeister) for providing Google Cloud Platform credits to support the experiments. We also thank Justin Gilmer (of Google), Guillermo
Ortiz-Jimenez (of EPFL, Switzerland), and Dan Hendrycks
(of UC Berkeley) for fruitful discussions.