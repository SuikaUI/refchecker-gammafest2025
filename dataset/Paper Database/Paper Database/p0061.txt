Semi-intrinsic Mean Shift on Riemannian
Rui Caseiro, Jo˜ao F. Henriques, Pedro Martins, and Jorge Batista
Institute of Systems and Robotics - University of Coimbra, Portugal
{ruicaseiro,henriques,pedromartins,batista}@isr.uc.pt
Abstract. The original mean shift algorithm on Euclidean spaces
(MS) was extended in to operate on general Riemannian manifolds.
This extension is extrinsic (Ext-MS) since the mode seeking is performed
on the tangent spaces , where the underlying curvature is not fully considered (tangent spaces are only valid in a small neighborhood). In 
was proposed an intrinsic mean shift designed to operate on two particular Riemannian manifolds (IntGS-MS), i.e. Grassmann and Stiefel
manifolds (using manifold-dedicated density kernels). It is then natural
to ask whether mean shift could be intrinsically extended to work on a
large class of manifolds. We propose a novel paradigm to intrinsically
reformulate the mean shift on general Riemannian manifolds. This is accomplished by embedding the Riemannian manifold into a Reproducing
Kernel Hilbert Space (RKHS) by using a general and mathematically
well-founded Riemannian kernel function, i.e. heat kernel . The key
issue is that when the data is implicitly mapped to the Hilbert space,
the curvature of the manifold is taken into account (i.e. exploits the
underlying information of the data). The inherent optimization is then
performed on the embedded space. Theoretic analysis and experimental
results demonstrate the promise and eﬀectiveness of this novel paradigm.
Introduction
Mean shift (MS) is a popular nonparametric and unsupervised clustering algorithm which does not assume a ﬁxed number of clusters and their shape . It
allows to analyze complex multimodal feature spaces from data points, which are
assumed to be sampled from an unknown distribution on a Euclidean space. This
conjecture is not always true which poses a challenge for some computer vision applications where data often lies in complex manifolds, namely in Riemannian manifolds, i.e. a nonlinear, curved yet smooth, metric space (e.g. motion/pose/epipolar
segmentation, multi-body factorization , object recognition/classiﬁcation , foreground segmentation , diﬀusion tensor processing , activity recognition, text categorization, shape analysis . It is mandatory to take into account
the Riemannian structure of the space in order to extract the underlying data information. From this point of view, it is natural to see several attempts in the
recently literature for devise mean shift algorithms to operate on the Riemannian
manifolds .
A. Fitzgibbon et al. (Eds.): ECCV 2012, Part I, LNCS 7572, pp. 342–355, 2012.
⃝Springer-Verlag Berlin Heidelberg 2012
Semi-intrinsic Mean Shift on Riemannian Manifolds
Prior Work.
Tuzel et al. proposed a mean shift speciﬁcally to operate
on Lie groups (it was applied to multiple rigid motion estimation). Subbarao et
al. extended the MS to operate on general Riemannian manifolds. This
extension is extrinsic (Ext-MS) since the mode seeking is performed on the tangent spaces . The mean shift is computed as a weighted sum of tangent vectors
(logarithmic map) and the resulting vector is mapped back to the manifold using the exponential map at each iteration. It was employed to camera pose
based segmentation, aﬃne image motion, camera pose estimation (Lie Groups),
epipolar segmentation (Essential manifold), translation viewed by a calibrated
camera, multi-body factorization, chromatic noise ﬁltering (Grassmann manifold) and diﬀusion tensor ﬁltering (Tensor manifold). The underlying curvature
is not fully considered since the tangent spaces only provide an approximation
of the manifold in a small neighborhood. This important constraint induces a
loss of accuracy in the clustering results. Following the above extensions, the
medoid shift and the quick shift methods were proposed to cluster data
on non-Euclidean spaces and applied for image categorization and segmentation . Ertan et al. derived an intrinsic mean shift designed to operate on
two particular Riemannian manifolds (IntGS-MS), i.e. Stiefel/Grassmann (using
manifold-dedicated density kernels). The Ertan’s paradigm cannot be generalized to other Riemannian manifolds due to the speciﬁcity of the inherent
kernel density function. It is then natural to ask whether mean shift could be
intrinsically extended to work on general Riemannian manifolds.
Learning problems on Riemannian manifolds are generally solved by fattening
the manifold via local diﬀeomorphisms (tangent spaces), i.e. the manifold is locally embedded into a Euclidean space. However, embedding the manifold using
those local diﬀeomorphisms leads to some problems. The exponential map is onto
but only one-to-one in a neighborhood of a point. Therefore, the inverse mapping (logarithmic map) is uniquely deﬁned only around a small neighborhood
of that point. Those constraints are restrictive in the sense that the intrinsic
structure and curvature of the space are not fully considered.
Mercer kernels have been widely used to devise several well-known statistical learning techniques (e.g. support vector machines), particularly in order to
convert linear classiﬁcation and regression algorithms into nonlinear counterparts
 . A key assumption of the most commonly used Mercer kernels (e.g. the radial
basis function kernel) is that the data points needs to be represented as vectors in
an Euclidean space . This issue is often solved in an ad hoc manner since there
is little theoretical knowledge on how the representation of data as real-valued
(Euclidean) feature vectors should be carried out . Recently a new paradigm
emerged . This new paradigm suggests to use speciﬁc Grassmann kernel
functions in order to embed a particular Riemannian manifold, i.e. Grassmann
manifold, into a Reproducing Kernel Hilbert Space .
Contributions. We proposed a novel paradigm to intrinsically reformulate the
mean shift on general Riemannian manifolds. This is accomplished by embedding the Riemannian manifold into a Reproducing Kernel Hilbert Space by using
a general and mathematically well-founded Riemannian kernel function, i.e. heat
R. Caseiro et al.
kernel . The novelty is that when the data is implicitly mapped from the manifold to the Hilbert space, the structure and curvature of the manifold is taken
into account (i.e. exploits the underlying information of the data). This is the
reason for the expression semi-intrinsic - SInt-MS). The inherent optimization
is then performed on the embedded space.
As proved in the mathematics literature a Riemannian manifold can be
embedded into a Hilbert space using the heat kernel (HK) paradigm. The heat
kernel on Riemannian manifolds is based on expansions involving eigenfunctions
of a Laplace operator on the manifold. Considering that diﬀerent Laplace operators give diﬀerent heat kernels, the use of the HK in this context involves
two important challenges : ﬁrstly, one must prove that the heat kernel used is
a Mercer kernel ; secondly, for most geometries there is not a closed form
and tractable solution to compute the HK. The heat kernel is a concept that
proved to be a powerful tool in physics, geometric analysis and has been a important subject of research in mathematical and physical literature. In physics,
this study is motivated, in particular, by the fact that it gives a framework for
investigating the quantum ﬁeld theories . In fact we tackle the above challenges by seeking inspiration from physics. We adopted a framework applicable
for generic Riemannian manifolds proposed by Avramidi , which gives a
tractable solution to compute the heat kernel and deﬁnes a Mercer kernel (to
our knowledge this is the ﬁrst time that the Avramidi’s work is used outside the
ﬁeld of theoretical Physics/Mathematics and related areas).
Considering the speciﬁcity of the density-kernels used by Ertan et al. ,
to the best of our knowledge this is the ﬁrst work that proposes an intrinsic
reformulation of the mean shift for general Riemannian manifolds. Consequently,
we believe this mean shift can be used to solve many diﬀerent problems in the
same way that the original MS has so widely been used in Euclidean cases.
Diﬀerential Geometry
In this section we brieﬂy review and deﬁne some elements of diﬀerential geometry
crucial to understand the proposed work. For the sake of brevity, our treatment
will not be complete. For more details, please refer to . A manifold is a
topological space locally similar to an Euclidean space. A Riemannian manifold
is a diﬀerentiable manifold M endowed with a Riemannian metric g. Let (M, g)
be a smooth, compact and complete n-dimensional Riemannian manifold. Let
T M and T ∗M be the tangent and cotangent bundles of M. We denote a smooth
vector bundle over a Riemannian manifold as V, and the respective dual as V∗.
The End(V) ∼= V ⊗V∗is the bundle of all smooth endomorphisms of V .
The space of smooth real-valued functions on M is denoted as C∞(M). The
C∞(M, V) and C∞(M, End(V)) are the spaces of all smooth sections of the bundles V and End(V), respectively . The vector bundle V is assumed to be
equipped with a Hermitian metric, identiﬁng the dual bundle V∗with V, and de-
ﬁnes L2 inner product. The Hilbert space L2(M, V) of square integrable sections
is deﬁned to be the completion of C∞(M, V) in this norm . Let ∇LC be
Semi-intrinsic Mean Shift on Riemannian Manifolds
the canonical connection (Levi-Civita) on the tangent bundle T M. Let, further,
∇V : C∞(M, V) →C∞(M, T ∗M ⊗V) be a connection, on the vector bundle V.
Using ∇LC together with ∇V, result in connections on all bundles in the tensor
algebra over V, V∗, T M and T ∗M . Let ∇∗: C∞(T ∗M) →C∞(M) be
the formal adjoint of ∇: C∞(M) →C∞(T ∗M), and let Q ∈C∞(M, End(V))
be a smooth Hermitian section of the endomorphism bundle End(V) . Let
xμ, (μ = 1, 2, ..., n), be a system of local coordinates. Greek indices, μ, ν, ..., label
the components with respect to local coordinate frames x = (xμ) on M. Let ∂μ
be a local coordinate basis (frames) for the tangent space T M at some point
∈M and dxμ be dual basis for the cotangent space T ∗M. Let gμν = (∂μ, ∂ν)
and gμν = (dxμ, dxν) be the metric on the tangent/cotangent bundle .
Kernel Mean Shift
In this section we describe a kernel-based mean shift technique proposed by
Tuzel et al. which will be the basis for our semi-intrinsic mean shift
on general Riemannian manifolds (SInt-MS). In order to study the Riemannian
structure of the space, the data is implicitly mapped to an enlarged feature
space H (Hilbert space) by using an appropriate Riemannian kernel function
(a Riemannian manifold can be embedded into a Hilbert space using the heat
kernel ). Consider M as the input space and the N data points given by
Zi ∈M , i = 1, ..., N. Let K : M×M →ℜbe a positive deﬁnite kernel function
: K(Z, Z′) = φ(Z)T φ(Z′) for all Z, Z′ ∈M. The input space M is projected into
the d-dimensional feature space H by the mapping function φ (e.g. z = φ(Z)
with z ∈H). Let’s now present the derivation of the mean shift on the space H
in function of the mapping φ . In the feature space H, the density estimator
at some point sample z ∈H is deﬁned as 
where hi is the bandwidth and k(·) is a multivariate normal proﬁle. The stationary points of the function fH are obtained computing the gradient of Eq. 1 with
respect to the mapping φ , i.e. they satisfy the condition
(φ(Zi) −z)g
where g(·) = −k′(·). This problem is solved iteratively similarly to the
standard mean shift 
Let the matrix K = ΦT Φ be the N × N Kernel (Gram) matrix, where Φ =
[φ(Z1) φ(Z2) ... φ(ZN)] is the d × N matrix of the feature points. Given the
R. Caseiro et al.
Input: 1 - K computed using Heat Kernel on Riemannian manifolds (Section 4)
2 - Bandwidth selection parameter k
Calculate the bandwidths hi as the kth smallest distance from the point using
Eq. 4 with K and d = rank(K)
for All data points i = 1, ..., N do
a) - Let αzi = ei
b) - Repeat until convergence (with D′ = αT
ziKαzi + eT
j Kej −2αT
c) - Group the points ¯αzi and ¯αzj, i, j = 1, ..., N satisfying
i K¯αi + ¯αT
j K¯αj −2¯αT
i K¯αj = 0.
Algorithm 1.
Semi-Intrinsic Meanshift on Riemannian Manifolds. The bandwidth
hi is computed as the kth smallest distance from the point i to all the N data points
on the feature space, where k is computed as a fraction of N .
feature matrix Φ, the solution ¯z at each iteration of the mean shift algorithm
(Eq. 3) lies in the column space of Φ . Considering the subspace spanned
by the columns of the matrix Φ, a point z is deﬁned as z = Φαz (αz is an Ndimensional vector of weights ) . All the computation involving data points
can be expressed in terms of inner products, e.g. the distances can be formulated
in the form of the inner product of the points and αz can be iteratively updated.
In this case the distance D = ∥z −z′∥2 between z and z′ is given by 
D = ∥Φαz −Φαz′∥2 = αT
z Kαz + αT
z′Kαz′ −2αT
We can therefore use the well-known kernel trick , i.e. it is possible to compute
the distances between z = φ(Z) and z′ = φ(Z′) in the mapped space without the
explicit knowledge of the mapping of Z and Z′ to φ(Z) and φ(Z′) respectively. The
mappings are replaced by the dot product K(Z, Z′) = φ(Z)T φ(Z′). The input of
the algorithm is the kernel matrix K of inner products. It can be demonstrated
that a positive semi-deﬁnite matrix K can be seen as a kernel matrix . Consider φ(Zi) = Φei, where ei represent the i-th canonical basis for ℜN. Substituting
Eq. 4 into Eq. 3 the solution ¯αz is obtained as follows .
where D′ = αT
z Kαz + eT
i Kei −2αT
z Kei. Since any positive semi-deﬁnite matrix
K is a kernel for some feature space , the original proof of convergence in
the Euclidean case is suﬃcient to prove the convergence of this kernel-based
mean shift algorithm (as argued in ). The vectors of weights at the initial of
Semi-intrinsic Mean Shift on Riemannian Manifolds
the iterative procedure are deﬁned as αzi = ei, which results in zi = Φαzi =
φ(Zi), i = 1, ..., N. The ﬁnal mode is represented as Φ¯αzi. For more details about
this kernel-based mean shift, please refer to . The matrix K is computed using
the heat kernel on Riemannian manifolds, which will be presented in Section 4.
Mercer Kernel on Riemannian Manifolds
Our goal is to deﬁne a class of positive deﬁnite kernels (Mercer kernels )
on general Riemannian manifolds (M), and thus both prove that the problem
presented in Section 3 is well-posed and deﬁne tools for solving it. The functions
must be devised in order to take into account the intrinsic geometry of the
manifold (M). These objectives will be accomplished by considering expansions
involving eigenfunctions of the Laplace operator on M (i.e. heat kernel ).
A continuous, complex-valued kernel K(· , · ) ∈C(M×M) is a positive deﬁnite
function on M if ¯K(Z′, Z) = K(Z, Z′) with Z, Z′ ∈M and if for all ﬁnite set
of points C = {Z1, ..., ZN} ∈M, the self-adjoint, N × N matrix with entries
K(Zj, Zk) is positive semi-deﬁnite . We will be particulary interested in
C∞positive deﬁnite kernels. This positivity can be seen in terms of distributions.
The concept of a distribution can be represented on M ; typically it is a linear
functional deﬁned on C∞(M). Let D′(M) be the set of all distributions on M
If δZ is the Dirac delta function located at Z and if u = (N
j=1 cjδZj) ∈
D′(M), then the matrix K = [K(Zj, Zk)] being positive semi-deﬁnite is equivalent to the quadratic form (¯u ⊗u, K) ≥0 for arbitrary cj, where ⊗corresponds
to the tensor product between distributions (Theorem 2.1 - ).
Theorem 1. Let K(Z, Z′) ∈C∞(M × M) have the eigenfunction expansion.
K(Z, Z′) = 
l∈A = alϕl(Z) ¯ϕl(Z′) where A is some countable index set, and
the ϕl are the eigen-functions of the Laplace operator on M. Then K is positive
deﬁnite on M if and only if al ≥0 for all l ∈A. Moreover, K is strictly positive
deﬁnite if and only if al > 0 for all l ∈A .
Corollary 1. For some countable index set A, let {ϕl}l∈A be an orthogonal
basis for L2(M, g) comprising eigenfunctions of the Laplace operator on M. In
addition, let K ∈C∞(M × M) be such that ( ¯ϕl ⊗ϕr, K) = alδlr. Then, K is
positive deﬁnite on M if and only if al ≥0 for all l ∈A. Moreover, K is strictly
positive deﬁnite if and only if al > 0 for all l ∈A .
On a Riemannian manifold M, a Laplace type operator L: C∞(M, V) →
C∞(M, V) can be deﬁned as a second-order partial diﬀerential operator of the
form 
L = ∇∗∇+ Q = −gμν∇μ∇ν + Q
where ∇∗∇is the generalized Laplacian and Q is an endomorphism (Section
2). The Laplace operator L is elliptic and has a positive leading symbol. L is
symmetric with respect to the natural L2 inner product and is self-adjoint, i.e.
its closure is self-adjoint, meaning that there is a unique self-adjoint extension
¯L of the operator L .
R. Caseiro et al.
Gilkey deﬁned an important theorem about the spectrum of a self-adjoint,
elliptic diﬀerential operator L with a positive deﬁnite principal symbol, operating over a compact manifold M (on smooth sections of a vector bundle V) ,
L: C∞(M, V) →C∞(M, V), : 1 - the spectrum of the operator L is
constituted by a sequence of discrete real nondecreasing eigenvalues {λl}∞
2 - the eigenvectors {ϕl}∞
l=1 form a complete orthonormal basis in L2(M, V)
(are smooth sections of the vector bundle V) ; and the eigenspaces are ﬁnitedimensional ; 3 - as l →∞the eigenvalues increase as λl ∼βl2 as l →∞, with
some positive constant β. Hence, the operator UL(t) = exp(tL) for t > 0 is well
deﬁned as a bounded operator (form a semi-group of bounded operators) on the
Hilbert space L2(M, V) of square integrable sections of the bundle V .
The kernel UL(t|Z, Z′) of that operator satisﬁes the heat equation, (∂t +
L)UL(t|Z, Z′) = 0 and is called the heat kernel, being deﬁned as follows 
Kt(t|Z, Z′) = UL(t|Z, Z′) =
e−tλlϕl(Z) ⊗¯ϕl(Z′)
The heat kernel can be seen as a smooth section of the external tensor
product of the vector bundles (V ⊠V∗) over the tensor product manifold
M × M: Kt(t|Z, Z′) ∈C∞(ℜ+ × M × M, V ⊠V∗), i.e. is an endomorphism
from the ﬁber of V over Z′ to the ﬁber of V over Z .
Since all of the coeﬃcients of Kt(t|Z, Z′) given by Eq. 8 (derived from the
operator L) are positive, Theorem 1 and Corollary 1 yield the following result :
Corollary 2. The heat kernel Kt(Z, Z′) ∈C∞(M × M) is a strictly positive
deﬁnite kernel on M.
From the Corollary 2 we conclude that the heat kernel can deﬁne a Mercer Kernel and therefore it is a suitable kernel for represent the similarity between data
points ∈M, while respecting the intrinsic geometry of the Riemannian space.
In the current literature, there is not a closed form and tractable solution to
estimate the heat kernel on general Riemann manifolds. However, the asymptotic solution of the heat kernel in the short time is a well-studied problem in
theoretical physics and mathematics. In mathematics - corresponding to the determination of the spectrum of the Laplacian - can give topological information
and in physics, it gives the solution, on a ﬁxed spacetime background, of the
Euclidean Schr¨odinger equation. Avramidi proposed an asymptotic expansion of the heat kernel for second-order elliptic partial diﬀerential operators
acting on sections of vector bundles over a Riemannian manifold, deﬁned as
Kt(t|Z, Z′) = (4πt)−n
2 Δ(Z, Z′)
P(Z, Z′)Ω(t|Z, Z′)
We consider t = σ2 by analogy with the typical variance σ2 of the standard
gaussian kernel in Euclidean spaces. Let W = W(Z, Z′) be the geodetic interval,
also called world function, deﬁned as one half the square of the length of the
geodesic (geodesic distance Dg) connecting the points Z and Z′, i.e. W(Z, Z′) =
Semi-intrinsic Mean Shift on Riemannian Manifolds
1/2Dg((Z, Z′). The ﬁrst derivatives of this function with respect to Z and Z′
deﬁne tangent vector ﬁelds to the geodesic at the points Z and Z′ 
uμ = uμ(Z, Z′) = gμν∇νW
uμ′ = uμ′(Z, Z′) = gμ′ν′∇′
and the determinant of the mixed second derivatives deﬁnes the so-called Van
Vleck-Morette determinant 
Δ = Δ(Z, Z′) = |g(Z)|−1
2 |g(Z′)|−1
2 det(−∇μ∇′
Let, P = P(Z, Z′) denote the parallel transport operator of sections of the vector
bundle V along the geodesic from the point Z′ to the point Z . It is an
endomorphism from the ﬁber of V over Z′ to the ﬁber of V over Z (or a section of
the external tensor product V⊠V∗over M×M). The function Ω(t) = Ω(t|Z, Z′),
called the transport function, is a section of the endomorphism bundle End(V)
over the point Z′ and satisﬁes the transport equation 
Ω(t|Z, Z′) = 0
Ω(t|Z, Z′) ∼
ak(Z, Z′) (12)
with the intial condition Ω(0|Z, Z′) = I, where I is the identity endomorphism
of the bundle V over Z′, D and L are operators deﬁned as 
L = P−1Δ−1/2FΔ1/2P
where D is the radial vector ﬁeld, i.e. operator of diﬀerentiation along the
geodesic and L is a second-order diﬀerential operator .
The transport function Ω(t) can be deﬁned using an asymptotic expansion
in terms of the coeﬃcients ak = ak(Z, Z′), called Hadamard-Minakshisundaram-
DeWitt-Seeley coeﬃcients (HMDS) - Eq. 12 . Taking into account that
the calculation of the HMDS coeﬃcients in the general case oﬀers a complex
theorical and technical problem, we will use a trade-oﬀsolution by considering
only the two lowest order terms, i.e. k = 0, 1 (which is in pratical a very good
solution). The so-called oﬀ-diagonal HMDS coeﬃcients (i.e. ak = ak(Z, Z′), with
Z ̸= Z′) are determined by a diﬀerential recursive system given by 
ak = Lak−1
k−1L...D−1
with ao = I and Dk = 1 + 1
kD. A close solution (explicit solution) to compute
the so-called diagonal HMDS coeﬃcients (i.e. adiag
= ak(Z, Z′), with Z = Z′) has
been a important subject of research in recent years in mathematical and physical literature . Recently adiag
has been computed up to order k = 8. These
formulas become exponentially more complicated as k increases. For example, the
formula for adiag
has 46 terms. However, the explicit formula for the ﬁrst two coeﬃcients (k = 0, 1) are fairly easy to compute and are given by adiag
R. Caseiro et al.
= Q−1/6R where R is the scalar curvature. . This paradigm is applicable to general Riemannian manifolds M and it is very algorithmic. Considering
that there are a number of usual algebraic operations on symmetric tensors that are
easily pre-programmed, this technique is appropriate to automatic computation,
i.e. symbolic computations easily lead to the components of the heat kernel.
Experimental Results
The clustering accuracy of our method is evaluated on synthetic data (Section 5.1)
as well as on real data (Section 5.2). Speciﬁcally, we compare the proposed semiintrinsic meanshift(SInt-MS)withtheextrinsic counterpart(Ext-MS) .The
experimental evaluation, serve mainly as a proof of concept, which is reasonable
given the novelty of the method. We present results for the following manifolds : Lie
Group (Special Orthogonal Group - SO3), Tensor manifold (Symmetric Positive-
Deﬁnite Matrices - S+), Grassmann (G) and Stiefel (V) manifolds. However, our
paradigm is general and applicable to a large number of diﬀerent Riemannian manifoldsandapplications.Weremarkthatthegoalisnotto compareour semi-intrinsic
meanshift (SInt-MS) with the intrinsic version (Int-MS) proposed by Ertan et al.
 . The Ertan’s method is not generalizable due to the speciﬁcity of the inherent
kernel density function. The proof of concept (extrinsic general vs semi-extrinsic
general) is not aﬀected since unlike Ertan we have looked to the Grassmman and
Stiefel manifolds as general Riemannian manifolds and not as Riemannian manifolds with speciﬁc kernel density functions. Let C be the number of classes and P
be the number of points per class.
Simulations on Synthetic Data
We conduct our synthetic experiments using the following data conﬁgurations for
clustering : (A = 4 classes |100 points/class) ; (B = 4 classes |200 points/class);
(C = 8 classes |100 points/class); (D = 8 classes |200 points/class). Table 1
shows the clustering rates of the Ext-MS and the SInt-MS on synthetic data.
Grassmann (Gk,m−k) and Stiefel (Vk,m) Manifolds
Regarding the synthetic tests, we generate pseudo-random matrices on the
Grassmann (Gk,m−k) and Stiefel (Vk,m) manifolds using the procedures presented in . It is possible to represent a orthogonal matrix S ∈O(m)
as a product of (0.5m2 −0.5m) orthogonal matrices of the form Rν
1 ≤ν ≤m −1 (please refer to ). The orthogonal matrix S ∈O(m) can
be deﬁned as S = m−1
m, with Sν
m(θν,j). For each one of
the C classes, we generated (0.5m2 −0.5m) angles {θν,j}. Each angle {θν,j} is
randomly drawn from one of (0.5m2 −0.5m) bins in [0, π] . Then the angles
are corrupted with random noise. Given the set of matrices {S} for each class,
we form the matrices X ∈Vk,m by taking the ﬁrst k orthonormal columns
of each S. Regarding the Grassmann manifold, the matrices are computed as
P = XXT ∈Gk,m−k .
Semi-intrinsic Mean Shift on Riemannian Manifolds
Clustering rates (%) of extrinsic (Ext-MS) and semi-intrinsic (SInt-MS)
meanshift on Riemannian manifolds in the case of simulations on synthetic data
Performance
Performance
Performance
Performance
m k Ext-Ms SInt-Ms m k Ext-Ms SInt-Ms r
Ext-Ms SInt-Ms d d Ext-Ms SInt-Ms
A →Classes = 4
Points per Class = 100
B →Classes = 4
Points per Class = 200
C →Classes = 8
Points per Class = 100
D →Classes = 8
Points per Class = 200
Average Improvement : Δ = (SInt-MS) - (Ext-MS)
ΔG = + 20 %
ΔV = + 22 %
ΔSO3 = + 28 %
ΔS+ = + 32 %
Lie Group (SO3)
The Lie Group used in the synthetic tests was the special orthogonal group SO3,
which is the group of rotations in 3D. The set of 3×3 skew-symmetric matrices Ω
forms the Lie algebra so3 (tangent space to the identity element of the group).
The skew-symmetric matrices can be deﬁned in vector form ω = (ωx, ωy, ωz)
 . From a geometrical perspective Ω can be seen as a rotation of an angle
||ω|| around the axis ω/||ω||. Let x ∈so(3) be an element on the Lie algebra and
X = exp(x) ∈SO(3) be its exponential mapping to the manifold and consider
that the tangent space is a vectorial space. Using these facts we generated the
synthetic points of the several classes directly in the Lie algebra. We choose
C points in the Lie algebra deﬁned in the vectorial form as ωi = (ωi
for i = 1, ..., C. Each of those C points corresponds to the ground-truth center
of a class/cluster. Let r be the radius of a 2-sphere centered in each of the C
clusters. We randomly drawn P points inside of each one of the C spheres. We
then corrupted all the CP points with random noise. Finally, we map back to
the manifold all the CP points using the exponential map and we obtain CP
rotation matrices distributed in C known clusters/classes.
Tensor Manifold (S+
d ) manifold correspondsto the Riemannian manifold of the d×d symmetric
positive-deﬁnite matrices. We generated the synthetic points directly in a tangent
space of the manifold S+
d . There exist two well-founded Riemannian metrics for
d ) e.g. Aﬃne-Invariant and Log-Euclidean. When the (S+
d ) manifold is endowed
R. Caseiro et al.
Clustering rates (%) of extrinsic (Ext-MS) and semi-intrinsic (SInt-MS)
meanshift on Riemannian manifolds on object categorization for selected objects on
the data set ETH-80 
Performance
Performance
Performance
Scales Bins m k Ext-Ms SInt-Ms Scales Bins m k Ext-Ms SInt-Ms d d Ext-Ms SInt-Ms
E →Classes = 3
| Points per Class = 250
F →Classes = 3
| Points per Class = 300
G →Classes = 4
| Points per Class = 250
H →Classes = 4
| Points per Class = 300
Average Improvement : Δ = (SInt-MS) - (Ext-MS)
ΔG = + 25 %
ΔV = + 23 %
ΔS+ = + 30 %
with the Log-Euclidean metric turns into a space with a null curvature, i.e. we can
map all the points ∈S+
d for the tangent space centered at Id (identity matrix) using
the ordinary matrix logarithm (log) and map back to the manifold with the matrix
exponential (exp). The tangent space corresponds to the space of d × d symmetric
matrices Sd with m = d(d+1)/2 independent elements. We choose C points in ℜm.
Each of those C points corresponds to the ground-truth center of a class/cluster.
Let r be the radius of a sphere Sm−1 centered in each of the C clusters. We randomly
drawn P points inside of each one of the C spheres. We then corrupted all the CP
points with random noise. Finally, we construct the corresponding C symmetric
matrices d × d and map back to the manifold all the CP points using the matrix
exponential (exp). We obtain CP symmetric positive-deﬁnite matrices (tensors)
distributed in C known clusters/classes. We used Log-Euclidean metric to generate
the synthetic tensors, but we endowed the (S+
d ) manifold with the Aﬃne-Invariant
metric for the clustering task, because with the Log-Euclidean metric the tensor
space turns into a null curvature space.
In our real experiments we select two well-known data sets, ETH-80 and
CIFAR10 , typically used in visual object categorization tasks (grouping
similar objects of the same class). We conduct our real experiments using the
following data conﬁgurations for clustering : (E = 3 classes |250 points/class);
(F = 3 classes |300 points/class) ; (G = 4 classes |250 points/class) ; (H =
4 classes |300 points/class). Tables 2 and 3 show the clustering rates of the
Ext-MS and the SInt-MS on real data. The process to extract features on the
diﬀerent Riemannian manifolds will be described next.
Semi-intrinsic Mean Shift on Riemannian Manifolds
Clustering rates (%) of extrinsic (Ext-MS) and semi-intrinsic (SInt-MS)
meanshift on Riemannian manifolds on object categorization for selected objects on
the data set CIFAR10 
Performance
Performance
Performance
Scales Bins m k
Ext-Ms SInt-Ms Scales Bins m k Ext-Ms SInt-Ms d d Ext-Ms SInt-Ms
E →Classes = 3
| Points per Class = 250
F →Classes = 3
| Points per Class = 300
G →Classes = 4
| Points per Class = 250
H →Classes = 4
| Points per Class = 300
Average Improvement : Δ = (SInt-MS) - (Ext-MS)
ΔG = + 28 %
ΔV = + 30 %
ΔS+ = + 34 %
Stiefel (Vk,m) Manifold
Regarding the real tests with the Stiefel manifold (Vk,m) we follow the procedures
presented in . As refered in , directional data in the form of unit-norm feature vectors can be extracted from an object image. The authors in proposed
to obtain this type of feature vector from the magnitude of the image gradient
and the Laplacian at three diﬀerent scales. In our experiments a maximum of ﬁve
diﬀerent scales and two types of histograms were used, that is, υ1 = {1, 2, 4, 6, 8}
where υ1 is the variance of the Gaussian ﬁlter and υ2 = {32, 64} where υ2 is the
number of bins of the histogram. Let s be the number of scales used, for each
of the 2s images a υ2-bin histogram is computed and concatenated as a feature
vector of length m = 2sυ2 (and then normalized). Therefore, the problem is
posed as a clustering problem of points on the Stiefel manifold V1,m ≡Sm−1 .
Grassmann (Gk,m−k) Manifold
Regarding the real tests with the Grassmann manifold (Gk,m−k) we follow the procedures presented in . As refered in the normalization process of the feature
vector (used on the Stiefel manﬁold case) may corrupt the underlying information
of the individual histograms and then decrease the class separability. To tackle the
above problem we assume that the l1-norm of the each histogram is 1 and then
we take the square root of each entry to make their l2-norms equal to 1 . Next,
we form a feature matrix by stacking the 2s aforementioned υ2-bin histograms as
columns and then we take the SVD of the resulting υ2 × 2s matrix . Its singular
vectors span a subspace of dimension k = 2s in ℜm=υ2 . Therefore, it is obtained
a new representation of the feature as a point on G2s,υ2−2s .
Tensor Manifold (S+
We used the well-known concept of region covariance matrix (RCM) in
order to test the proposed method in the tensor manifold using real data. We
R. Caseiro et al.
constructed three diﬀerent types of d×d RCM’s with d = {5, 7, 9}. The respective
features extracted for each one of the conﬁgurations are (d = 5 →[x y I |Ix| |Iy|])
; (d = 7 →[x y R G B |Ix| |Iy|]) ; (d = 9 →[x y R G B |Ix| |Iy| |Ixx| |Iyy|]) where
(x, y) is the pixel image position, Ix, Iy, Ixx, Iyy are the gradients and I, R, G, B
are the gray/color components.
Conclusions
To thebestofour knowledgethisistheﬁrstworkthatproposesanintrinsicreformulation of the mean shift algorithm for general Riemannian manifolds. Experimental
results on synthetic data as well as on real data clearly demonstrate that signiﬁcant
improvements in clustering accuracycan be achieved by employing this novel semiintrinsic mean shift (SInt-MS) over the extrinsic counterpart (Ext-MS). We conclude that : the consequent usage of the intrinsic Riemannian structure of the space,
in conjunction with an embedding of the Riemannian manifold into a Reproducing
Kernel Hilbert Space (RKHS) by using a general and mathematically well-founded
Riemannian kernel function (i.e. heat kernel), yields the most accurate and reliable
approach presented so far to extend the well-known mean shift algorithm to general
Riemannian manifolds. This allows us to extend mean shift based clustering and
ﬁltering techniques to a large class of frequently occurring Riemannian manifolds
in vision and related areas, paving the way for other researchers.
Acknowledgments. This work was supported by the Portuguese Science Foundation (FCT) under the project Diﬀerential Geometry for Computer Vision
and Pattern Recognition . Rui Caseiro, Jo˜ao
F. Henriques and Pedro Martins acknowledge the FCT through the grants
SFRH/ BD74152/2010, SFRH/BD/75459/2010 and SFRH/BD/45178/2008, respectively.