The iNaturalist Species Classiﬁcation and Detection Dataset
Grant Van Horn1
Oisin Mac Aodha1
Yang Song2
Alex Shepard4
Hartwig Adam2
Pietro Perona1
Serge Belongie3
3Cornell Tech
4iNaturalist
Existing image classiﬁcation datasets used in computer
vision tend to have a uniform distribution of images across
object categories. In contrast, the natural world is heavily
imbalanced, as some species are more abundant and easier
to photograph than others. To encourage further progress in
challenging real world conditions we present the iNaturalist species classiﬁcation and detection dataset, consisting of
859,000 images from over 5,000 different species of plants
and animals. It features visually similar species, captured
in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have
been veriﬁed by multiple citizen scientists. We discuss the
collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classiﬁcation and detection models. Results show that current nonensemble based methods achieve only 67% top one classi-
ﬁcation accuracy, illustrating the difﬁculty of the dataset.
Speciﬁcally, we observe poor results for classes with small
numbers of training examples suggesting more attention is
needed in low-shot learning.
1. Introduction
Performance on existing image classiﬁcation benchmarks such as is close to being saturated by the current generation of classiﬁcation algorithms .
However, the number of training images is crucial. If one
reduces the number of training images per category, typically performance suffers. It may be tempting to try and acquire more training data for the classes with few images but
this is often impractical, or even impossible, in many application domains. We argue that class imbalance is a property
of the real world and computer vision models should be able
to deal with it. Motivated by this problem, we introduce the
iNaturalist Classiﬁcation and Detection Dataset (iNat2017).
Just like the real world, it exhibits a large class imbalance,
as some species are much more likely to be observed.
Figure 1. Two visually similar species from the iNat2017 dataset.
Through close inspection, we can see that the ladybug on the left
has two spots while the one on the right has seven.
It is estimated that the natural world contains several million species with around 1.2 million of these having already
been formally described . For some species, it may only
be possible to determine the species via genetics or by dissection. For the rest, visual identiﬁcation in the wild, while
possible, can be extremely challenging. This can be due to
the sheer number of visually similar categories that an individual would be required to remember along with the challenging inter-class similarity; see Fig. 1. As a result, there
is a critical need for robust and accurate automated tools to
scale up biodiversity monitoring on a global scale .
The iNat2017 dataset is comprised of images and labels
from the citizen science website iNaturalist1. The site allows naturalists to map and share photographic observations of biodiversity across the globe.
Each observation
consists of a date, location, images, and labels containing
the name of the species present in the image. As of November 2017, iNaturalist has collected over 6.6 million observations from 127,000 species. From this, there are close to
12,000 species that have been observed by at least twenty
1www.inaturalist.org
people and have had their species ID conﬁrmed by multiple
annotators.
The goal of iNat2017 is to push the state-of-the-art in
image classiﬁcation and detection for ‘in the wild’ data
featuring large numbers of imbalanced, ﬁne-grained, categories.
iNat2017 contains over 5,000 species, with a
combined training and validation set of 675,000 images,
183,000 test images, and over 560,000 manually created
bounding boxes. It is free from one of the main selection
biases that are encountered in many existing computer vision datasets - as opposed to being scraped from the web
all images have been collected and then veriﬁed by multiple
citizen scientists. It features many visually similar species,
captured in a wide variety of situations, from all over the
world. We outline how the dataset was collected and report
extensive baseline performance for state-of-the-art classiﬁcation and detection algorithms. Our results indicate that
iNat2017 is challenging for current models due to its imbalanced nature and will serve as a good experimental platform
for future advances in our ﬁeld.
2. Related Datasets
In this section we review existing image classiﬁcation
datasets commonly used in computer vision. Our focus is
on large scale, ﬁne-grained, object categories as opposed to
datasets that feature common everyday objects, e.g. . Fine-grained classiﬁcation problems typically exhibit
two distinguishing differences from their coarse grained
counter parts. First, there tends to be only a small number of domain experts that are capable of making the classiﬁcations.
Second, as we move down the spectrum of
granularity, the number of instances in each class becomes
smaller. This motivates the need for automated systems that
are capable of discriminating between large numbers of potentially visually similar categories with small numbers of
training examples for some categories. In the extreme, face
identiﬁcation can be viewed as an instance of ﬁne-grained
classiﬁcation and many existing benchmark datasets with
long tail distributions exist e.g. . However, due
to the underlying geometric similarity between faces, current state-of-the-art approaches for face identiﬁcation tend
to perform a large amount of face speciﬁc pre-processing
 .
The vision community has released many ﬁne-grained
datasets covering several domains such as birds , dogs , airplanes , ﬂowers ,
leaves , food , trees , and cars .
ImageNet is not typically advertised as a ﬁne-grained
dataset, yet contains several groups of ﬁne-grained classes,
including about 60 bird species and about 120 dog breeds.
In Table 1 we summarize the statistics of some of the most
common datasets. With the exception of a small number e.g.
 , many of these datasets were typically constructed
Dataset Name
Flowers 102 
Aircraft 
Oxford Pets 
DogSnap 
CUB 200-2011 
Stanford Cars 
Stanford Dogs 
Urban Trees 
NABirds 
LeafSnap∗ 
CompCars∗ 
VegFru∗ 
Census Cars 
ILSVRC2012 
Table 1. Summary of popular general and ﬁne-grained computer
vision classiﬁcation datasets. ‘Imbalance’ represents the number
of images in the largest class divided by the number of images in
the smallest. While susceptible to outliers, it gives an indication
of the imbalance found in many common datasets. ∗Total number
of train, validation, and test images.
to have an approximately uniform distribution of images
across the different categories. In addition, many of these
datasets were created by searching the internet with automated web crawlers and as a result can contain a large proportion of incorrect images e.g. . Even manually vetted
datasets such as ImageNet have been reported to contain up to 4% error for some ﬁne-grained categories .
While current deep models are robust to label noise at training time, it is still very important to have clean validation
and test sets to be able to quantify performance .
Unlike web scraped datasets , the annotations in iNat2017 represent the consensus of informed enthusiasts. Images of natural species tend to be challenging
as individuals from the same species can differ in appearance due to sex and age, and may also appear in different
environments. Depending on the particular species, they
can also be very challenging to photograph in the wild. In
contrast, mass-produced, man-made object categories are
typically identical up to nuisance factors, i.e. they only differ in terms of pose, lighting, color, but not necessarily in
their underlying object shape or appearance .
3. Dataset Overview
In this section we describe the details of the dataset, including how we collected the image data (Section
how we constructed the train, validation and test splits (Section 3.2), how we vetted the test split (Section 3.2.1) and
how we collected bounding boxes (Section 3.3). Future researchers may ﬁnd our experience useful when constructing
their own datasets.
Super-Class
Actinopterygii
Table 2. Number of images, classes, and bounding boxes in
iNat2017 broken down by super-class. ‘Animalia’ is a catch-all
category that contains species that do not ﬁt in the other superclasses. Bounding boxes were collected for nine of the superclasses. In addition, the public and private test sets contain 90,427
and 92,280 images, respectively.
3.1. Dataset Collection
iNat2017 was collected in collaboration with iNaturalist,
a citizen science effort that allows naturalists to map and
share observations of biodiversity across the globe through
a custom made web portal and mobile apps.
Observations, submitted by observers, consist of images, descriptions, location and time data, and community identiﬁcations. If the community reaches a consensus on the taxa in
the observation, then a “research-grade” label is applied to
the observation. iNaturalist makes an archive of researchgrade observation data available to the environmental science community via the Global Biodiversity Information
Facility (GBIF) . Only research-grade labels at genus,
species or lower are included in this archive. These archives
contain the necessary information to reconstruct which photographs belong to each observation, which observations
belong to each observer, as well as the taxonomic hierarchy
relating the taxa. These archives are refreshed on a rolling
basis and the iNat2017 dataset was created by processing
the archive from October 3rd, 2016.
3.2. Dataset Construction
The complete GBIF archive had 54k classes (genus level
taxa and below), with 1.1M observations and a total of 1.6M
images. However, over 19k of those classes contained only
one observation. In order to construct train, validation and
test splits that contained samples from all classes we chose
to employ a taxa selection criteria: we required that a taxa
have at least 20 observations, submitted from at least 20
unique observers (i.e. one observation from each of the 20
Sorted Species
Number of Training Images
Figure 2. Distribution of training images per species. iNat2017
contains a large imbalance between classes, where the top 1%
most populated classes contain over 16% of training images.
unique observers). This criteria limited the candidate set to
5,089 taxa coming from 13 super-classes, see Table 2.
The next step was to partition the images from these taxa
into the train, validation, and test splits. For each of the
selected taxa, we sorted the observers by their number of
observations (fewest ﬁrst) and selected the ﬁrst 40% of observers to be in the test split, and the remaining 60% to be
in the “train-val” split. By partitioning the observers in this
way, and subsequently placing all of their photographs into
one split or the other, we ensure that the behavior of a particular user (e.g. camera equipment, location, background,
etc.) is contained within a single split, and not available as a
useful source of information for classiﬁcation on the other
split for a speciﬁc taxa. Note that a particular observer may
be put in the test split for one taxa, but the “train-val” split
for another taxa. By ﬁrst sorting the observers by their number of observations we ensure that the test split contains
a high number of unique observers and therefore a high
degree of variability. To be concrete, at this point, for a
taxa that has exactly 20 unique observers (the minimum allowed), 8 observers would be placed in the the test split and
the remaining 12 observers would be placed in the “trainval” split. Rather than release all test images, we randomly
sampled ∼183,000 to be included in the ﬁnal dataset. The
remaining test images were held in reserve in case we encountered unforeseen problems with the dataset.
To construct the separate train and validation splits for
each taxa from the “train-val” split we again partition on the
observers. For each taxa, we sort the observers by increasing observation counts and repeatedly add observers to the
validation split until either of the following conditions occurs: (1) The total number of photographs in the validation
set exceeds 30, or (2) 33% of the available photographs in
the “train-val” set for the taxa have been added to the validation set. The remaining observers and all of their photographs are added to the train split. To be concrete, and
continuing the example from above, exactly 4 images would
be placed in the validation split, and the remaining 8 images
would be placed in the train split for a taxa with 20 unique
Figure 3. Sample bounding box annotations.
Annotators were
asked to annotate up to 10 instances of a super-class, as opposed
to the ﬁne-grained class, in each image.
observers. This results in a validation split that has at least
4 and at most ∼30 images for each class (the last observer
added to the validation split for a taxa may push the number
of photographs above 30), and a train split that has at least
8 images for each class. See Fig. 2 for the distribution of
train images per class.
At this point we have the ﬁnal image splits, with a total
of 579,184 training images, 95,986 validation images and
182,707 test images. All images were resized to have a max
dimension of 800px. Sample images from the dataset can
be viewed in Fig. 8. The iNat2017 dataset is available from
our project website2.
Test Set Veriﬁcation
Each observation on iNaturalist is made up of one or more
images that provide evidence that the taxa was present.
Therefore, a small percentage of images may not contain
the taxa of interest but instead can include footprints, feces, and habitat shots. Unfortunately, iNaturalist does not
distinguish between these types of images in the GBIF export, so we crowdsourced the veriﬁcation of three superclasses (Mammalia, Aves, and Reptilia) that might exhibit
these “non-instance” images. We found that less than 1.1%
of the test set images for Aves and Reptilia had non-instance
images. The fraction was higher for Mammalia due to the
prevalence of footprint and feces images, and we ﬁltered
these images out of the test set. The training and validation
images were not ﬁltered.
3.3. Bounding Box Annotation
Bounding boxes were collected on 9 out of the 13 superclasses (see Table 2), totaling 2,854 classes. Due to the inherit difﬁcultly of asking non-expert crowd annotators to
both recognize and box speciﬁc ﬁne-grained classes, we instructed annotators to instead box all instances of the associated super-class for a taxa in the training set, per
super-class. Most objects are relatively small or medium sized.
“Box all Red-winged Black Birds”). We collected superclass boxes only on taxa that are part of that super-class. For
some super-classes (e.g. Mollusca), there are images containing taxa which are unfamiliar to many of the annotators
(e.g. Fig. 3(a)). For those cases, we instructed the annotators to box the prominent objects in the images.
The task instructions speciﬁed to draw boxes tightly
around all parts of the animal (including legs, horns, antennas, etc.). If the animal is occluded, the annotators were
instructed to draw the box around the visible parts (e.g.
Fig. 3(b)). In cases where the animal is blurry or small (e.g.
Fig. 3(c) and (d)), the following rule-of-thumb was used:
“if you are conﬁdent that it is an animal from the requested
super-class, regardless of size, blurriness or occlusion, put
a box around it.” For images with multiple instances of
the super-class, all of them are boxed, up to a limit of 10
(Fig. 3(f)), and bounding boxes may overlap (Fig. 3(e)). We
observe that 12% of images have more than 1 instance and
1.3% have more than 5. If the instances are physically connected (e.g. the mussels in Fig. 3(g)), then only one box is
placed around them.
Bounding boxes were not collected on the Plantae,
Fungi, Protozoa or Chromista super-classes because these
super-classes exhibit properties that make it difﬁcult to box
the individual instances (e.g. close up of trees, bushes, kelp,
etc.). An alternate form of pixel annotations, potentially
from a more specialized group of crowd workers, may be
more appropriate for these classes.
Under the above guidelines, 561,767 bounding boxes
were obtained from 449,313 images in the training and validation sets. Following the size conventions of COCO ,
the iNat2017 dataset is composed of 5.7% small instances
(area < 322), 23.6% medium instances (322 ≤area ≤962)
and 70.7% large instances (area > 962), with area computed as 50% of the annotated bounding box area (since
segmentation masks were not collected). Fig. 4 shows the
distribution of relative bounding box sizes, indicating that a
majority of instances are relatively small and medium sized.
4. Experiments
In this section we compare the performance of state-ofthe-art classiﬁcation and detection models on iNat2017.
4.1. Classiﬁcation Results
To characterize the classiﬁcation difﬁculty of iNat2017,
we ran experiments with several state-of-the-art deep
network architectures, including ResNets , Inception
V3 , Inception ResNet V2 and MobileNet .
During training, random cropping with aspect ratio augmentation was used. Training batches of size 32 were
created by uniformly sampling from all available training images as opposed to sampling uniformly from the
classes. We ﬁne-tuned all networks from ImageNet pretrained weights with a learning rate of 0.0045, decayed exponentially by 0.94 every 4 epochs, and RMSProp optimization with momentum and decay both set to 0.9. Training and testing were performed with an image size of
299 × 299, with a single centered crop at test time.
Table 3 summarizes the top-1 and top-5 accuracy of the
models. From the Inception family, we see that the higher
capacity Inception ResNet V2 outperforms the Inception
V3 network. The addition of the Squeeze-and-Excitation
(SE) blocks further improves performance for both
models by a small amount. ResNets performed worse on
iNat2017 compared to the Inception architectures, likely
due to over-ﬁtting on categories with small number of training images. We found that adding a 0.5 probability dropout
layer (drp) could improve the performance of ResNets. MobileNet, designed to efﬁciently run on embedded devices,
had the lowest performance.
Overall, the Inception ResNetV2 SE was the best performing model. As a comparison, this model achieves a single crop top-1 and top-5 accuracy of 80.2% and 95.21% respectively on the ILSVRC 2012 validation set , as
opposed to 67.74% and 87.89% on iNat2017, highlighting
the comparative difﬁculty of the iNat2017 dataset. A more
detailed super-class level breakdown is available in Table 4
for the Inception ResNetV2 SE model. We can see that the
Reptilia super-class (with 289 classes) was the most difﬁcult
with an average top-1 accuracy of 45.87%, while the Protozoa super-class (with 4 classes) had the highest accuracy
at 89.19%. Viewed as a collection of ﬁne-grained datasets
(one for each super-class) we can see that the iNat2017
dataset exhibits highly variable classiﬁcation difﬁculty.
In Fig. 5 we plot the top one public test set accuracy
against the number of training images for each class from
the Inception ResNet V2 SE model. We see that as the number of training images per class increases, so does the test
accuracy. However, we still observe a large variance in accuracy for classes with a similar amount of training data, revealing opportunities for algorithmic improvements in both
the low data and high data regimes.
Validation
Public Test
Private Test
IncResNetV2 SE
IncResNetV2
ResNet152 drp
ResNet101 drp
MobileNet V1
Table 3. Classiﬁcation results for various CNNs trained on only the
training set, using a single center crop at test time. Unlike some
current datasets where performance is near saturation, iNat2017
still poses a challenge for state-of-the-art classiﬁers.
Super-Class
Public Test
Actinopterygii
Table 4. Super-class level accuracy (computed by averaging across
all species within each super-class) for the best performing model
Inception ResNetV2 SE . “Avg Train” indicates the average
number of training images per class for each super-class. We observe a large difference in performance across the different superclasses.
Binned Number of Training Images
Test Accuracy
Figure 5. Top one public test set accuracy per class for IncRes-
Net V2 SE . Each box plot represents classes grouped by the
number of training images. The number of classes for each bin is
written on top of each box plot. Performance improves with the
number of training images, but the challenge is how to maintain
high accuracy with fewer images?
4.2. Detection Results
To characterize the detection difﬁculty of iNat2017, we
adopt Faster-RCNN for its state-of-the-art performance
as an object detection setup (which jointly predicts object
bounding boxes along with class labels). We use a TensorFlow implementation of Faster-RCNN with default
hyper-parameters . Each model is trained with 0.9 momentum, and asynchronously optimized on 9 GPUs to expedite experiments. We use an Inception V3 network, initialized from ImageNet, as the backbone for our Faster-RCNN
models. Finally, each input image is resized to have 600
pixels as the short edge while maintaining the aspect ratio.
As discussed in Section 3.3, we collected bounding
boxes on 9 of the 13 super-classes, translating to a total of
2,854 classes with bounding boxes. In the following experiments we only consider performance on this subset of
classes. Additionally, we report performance on the the validation set in place of the test set and we only evaluate on
images that contained a single instance. Images that contained only evidence of the species’ presence and images
that contained multiple instances were excluded. We evaluate the models using the detection metrics from COCO .
We ﬁrst study the performance of ﬁne-grained localization and classiﬁcation by training the Faster-RCNN model
on the 2,854 class subset. Fig. 7 shows some sample detection results. Table 5 provides the break down in performance for each super-class, where super-class performance
is computed by taking an average across all classes within
the super-class. The precision-recall curves (again at the
super-class level) for 0.5 IoU are displayed in Fig. 6. Across
all super-classes we achieve a comprehensive average precision (AP) of 43.5. Again the Reptilia super-class proved
to be the most difﬁcult, with an AP of 21.3 and an AUC
of 0.315. At the other end of the spectrum we achieved
an AP of 49.4 for Insecta and an AUC of 0.677. Similar
to the classiﬁcation results, when viewed as a a collection
of datasets (one for each super-class) we see that iNat2017
exhibits highly variable detection difﬁculty, posing a challenge to researchers to build improved detectors that work
across a broad group of ﬁne-grained classes.
Next we explored the effect of label granularity on detection performance. We trained two more Faster-RCNN models, one trained to detect super classes rather ﬁne-grained
classes (so 9 classes in total) and another model trained
with all labels pooled together, resulting in a generic object / not object detector. Table 6 shows the resulting AP
scores for the three models when evaluated at different granularities. When evaluated on the coarser granularity, detectors trained on ﬁner-grained categories have lower detection performance when compared with detectors trained at
coarser labels. The performance of the 2,854-class detector
is particularly poor on super-class recognition and object
localization. This suggests that the Faster-RCNN algorithm
Insecta (0.677)
Aves (0.670)
Arachnida (0.664)
Animalia (0.557)
Actinopterygii (0.521)
Mollusca (0.500)
Mammalia (0.486)
Amphibia (0.402)
Reptilia (0.315)
Figure 6. Precision-Recall curve with 0.5 IoU for each super-class,
where the Area-Under-Curve (AUC) corresponds to AP50 in Table 5. Super-class performance is calculated by averaging across
all ﬁne-grained classes. We can see that building a detector that
works well for all super-classes in iNat2017 will be a challenge.
Actinopterygii
Table 5. Super-class-level Average Precision (AP) and Average
Recall (AR) for object detection, where AP, AP50 and AP75 denotes AP@[IoU=.50:.05:.95], AP@[IoU=.50] and AP@[IoU=.75]
respectively; AR1 and AR10 denotes AR given 1 detection and 10
detections per image.
Evaluation
2854-class
9-super-class
2854-class
9-super-class
Table 6. Detection performance (AP@[IoU=.50:.05:.95]) with different training and evaluation class granularity.
Using ﬁnergrained class labels during training has a negative impact on
coarser-grained super-class detection. This presents an opportunity for new detection algorithms that maintain precision at the
ﬁne-grained level.
has plenty of room for improvements on end-to-end ﬁnegrained detection tasks.
Chaetodon lunula(1.00)
Chaetodon lunula(0.98)
Anaxyrus fowleri(0.95)
Pseudacris regilla(0.58)
Setophaga petechia(0.91)
Orcinus orca(0.99)
Rabdotus dealbatus(0.92)
Sylvilagus audubonii(0.97)
Equus quagga(1.00)
Equus quagga(0.98)
Zalophus californianus(0.88)
Megaptera novaeangliae(0.74)
Hippodamia convergens(0.83)
Phalacrocorax auritus(0.54)
Figure 7. Sample detection results for the 2,854-class model that was evaluated across all validation images. Green boxes represent correct
species level detections, while reds are mistakes. The bottom row depicts some failure cases. We see that small objects pose a challenge
for classiﬁcation, even when localized well.
5. Conclusions and Future Work
We present the iNat2017 dataset, in contrast to many existing computer vision datasets it is: 1) unbiased, in that
it was collected by non-computer vision researchers for a
well deﬁned purpose, 2) more representative of real-world
challenges than previous datasets, 3) represents a long-tail
classiﬁcation problem, and 4) is useful in conservation and
ﬁeld biology. The introduction of iNat2017 enables us to
study two important questions in a real world setting: 1)
do long-tailed datasets present intrinsic challenges?
2) do our computer vision systems exhibit transfer learning
from the well-represented categories to the least represented
ones? While our baseline classiﬁcation and detection results
are encouraging, from our experiments we see that state-ofthe-art computer vision models have room to improve when
applied to large imbalanced datasets. Small efﬁcient models designed for mobile applications and embedded devices
have even more room for improvement .
Unlike traditional, researcher-collected datasets, the
iNat2017 dataset has the opportunity to grow with the
iNaturalist community. Currently, every 1.7 hours another
species passes the 20 unique observer threshold, making it
available for inclusion in the dataset . Thus, the current challenges of the dataset (long
tail with sparse data) will only become more relevant.
In the future we plan to investigate additional annotations
such as sex and life stage attributes, habitat tags, and pixel
level labels for the four super-classes that were challenging
to annotate. We also plan to explore the “open-world problem” where the test set contains classes that were never seen
during training. This direction would encourage new error
measures that incorporate taxonomic rank . Finally,
we expect this dataset to be useful in studying how to teach
ﬁne-grained visual categories to humans , and plan
to experiment with models of human learning.
Acknowledgments This work was supported by a Google
Focused Research Award. We would like to thank: Scott
Loarie and Ken-ichi Ueda from iNaturalist; Steve Branson,
David Rolnick, Weijun Wang, and Nathan Frey for their
help with the dataset; Wendy Kan and Maggie Demkin from
Kaggle; the iNat2017 competitors, and the FGVC2017
workshop organizers. We also thank NVIDIA and Amazon
Web Services for their donations.
Figure 8. Example images from the training set. Each row displays randomly selected images from each of the 13 different super-classes.
For ease of visualization we show the center crop of each image.