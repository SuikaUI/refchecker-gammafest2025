SEGAN: Speech Enhancement Generative Adversarial Network
Santiago Pascual1, Antonio Bonafonte1, Joan Serr`a2
1Universitat Polit`ecnica de Catalunya, Barcelona, Spain
2Telef´onica Research, Barcelona, Spain
 , 
Current speech enhancement techniques operate on the spectral
domain and/or exploit some higher-level feature. The majority
of them tackle a limited number of noise conditions and rely on
ﬁrst-order statistics. To circumvent these issues, deep networks
are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the
waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same
model, such that model parameters are shared across them. We
evaluate the proposed model using an independent, unseen test
set with two speakers and 20 alternative noise conditions. The
enhanced samples conﬁrm the viability of the proposed model,
and both objective and subjective evaluations conﬁrm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to
improve their performance.
Index Terms: speech enhancement, deep learning, generative
adversarial networks, convolutional neural networks.
1. Introduction
Speech enhancement tries to improve the intelligibility and
quality of speech contaminated by additive noise . Its main
applications are related to improving the quality of mobile communications in noisy environments. However, we also ﬁnd important applications related to hearing aids and cochlear implants, where enhancing the signal before ampliﬁcation can
signiﬁcantly reduce discomfort and increase intelligibility .
Speech enhancement has also been successfully applied as a
preprocessing stage in speech recognition and speaker identiﬁcation systems .
Classic speech enhancement methods are spectral subtraction , Wiener ﬁltering , statistical model-based methods , and subspace algorithms . Neural networks have
been also applied to speech enhancement since the 80s .
Recently, the denoising auto-encoder architecture has been
widely adopted. However, recurrent neural networks (RNNs)
are also used.
For instance, the recurrent denoising autoencoder has shown signiﬁcant performance exploiting the temporal context information in embedded signals. Most recent approaches apply long short-term memory networks to the denoising task . In and , noise features are estimated
and included in the input features of deep neural networks. The
use of dropout, post-ﬁltering, and perceptually motivated metrics are shown to be effective.
Most of the current systems are based on the short-time
Fourier analysis/synthesis framework . They only modify the
spectrum magnitude, as it is often claimed that short-time phase
is not important for speech enhancement . However, further studies show that signiﬁcant improvements of speech
quality are possible, especially when a clean phase spectrum is
known. In 1988, Tamura et al. proposed a deep network
that worked directly on the raw audio waveform, but they used
feed-forward layers that worked frame-by-frame (60 samples)
on a speaker-dependent and isolated-word database.
A recent breakthrough in the deep learning generative modeling ﬁeld are generative adversarial networks (GANs) .
GANs have achieved a good level of success in the computer vision ﬁeld to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions . As far as we are concerned, GANs have
not yet been applied to any speech generation nor enhancement
task, so this is the ﬁrst approach to use the adversarial framework to generate speech signals.
The main advantages of the proposed speech enhancement
GAN (SEGAN) are:
• It provides a quick enhancement process. No causality is
required and, hence, there is no recursive operation like in
• It works end-to-end, with the raw audio.
Therefore, no
hand-crafted features are extracted and, with that, no explicit assumptions about the raw data are done.
• It learns from different speakers and noise types, and incorporates them together into the same shared parametrization.
This makes the system simple and generalizable in those dimensions.
In the following, we give an overview of GANs (Sec. 2).
Next, we describe the proposed model (Sec. 3) and its experimental setup (Sec. 4). We ﬁnally report the results (Sec. 5) and
discuss some conclusions (Sec. 6).
2. Generative Adversarial Networks
GANs are generative models that learn to map samples z
from some prior distribution Z to samples x from another distribution X, which is the one of the training examples (e.g., images, audio, etc.). The component within the GAN structure
that performs the mapping is called the generator (G), and its
main task is to learn an effective mapping that can imitate the
real data distribution to generate novel samples related to those
of the training set. Importantly, G does so not by memorizing
input-output pairs, but by mapping the data distribution characteristics to the manifold deﬁned in our prior Z.
The way in which G learns to do the mapping is by means
of an adversarial training, where we have another component,
called the discriminator (D). D is typically a binary classiﬁer,
and its inputs are either real samples, coming from the dataset
that G is imitating, or fake samples, made up by G. The adversarial characteristic comes from the fact that D has to classify the samples coming from X as real, whereas the samples
 
Figure 1: GAN training process. First, D back-props a batch
of real examples. Then, D back-props a batch of fake examples that come from G, and classiﬁes them as fake. Finally, D’s
parameters are frozen and G back-props to make D misclassify.
coming from G, ˆ
X, have to be classiﬁed as fake. This leads
to G trying to fool D, and the way to do so is that G adapts
its parameters such that D classiﬁes G’s output as real. During
back-propagation, D gets better at ﬁnding realistic features in its
input and, in turn, G corrects its parameters to move towards the
real data manifold described by the training data (Fig. 1). This
adversarial learning process is formulated as a minimax game
between G and D, with the objective
V (D, G) = Ex∼pdata(x) [log D(x)] +
+ Ez∼pz(z) [log (1 −D (G (z)))] .
We can also work with a conditioned version of GANs,
where we have some extra information in G and D to perform
mapping and classiﬁcation (see and references therein).
In that case, we may add some extra input xc, with which we
change the objective function to
V (D, G) = Ex,xc∼pdata(x,xc) [log D(x, xc)] +
+ Ez∼pz(z),xc∼pdata(xc) [log (1 −D (G (z, xc) , xc))] .
There have been recent improvements in the GAN methodology to stabilize training and increase the quality of the generated samples in G. For instance, the classic approach suffered from vanishing gradients due to the sigmoid cross-entropy
loss used for training. To solve this, the least-squares GAN
(LSGAN) approach substitutes the cross-entropy loss by
the least-squares function with binary coding (1 for real, 0 for
fake). With this, the formulation in Eq. 2 changes to
D VLSGAN(D) = 1
2 Ex,xc∼pdata(x,xc)[(D(x, xc) −1)2]+
2 Ez∼pz(z),xc∼pdata(xc)[D(G(z, xc), xc)2]
G VLSGAN(G) = 1
2 Ez∼pz(z),xc∼pdata(xc)[(D(G(z, xc), xc)−1)2].
3. Speech Enhancement GAN
The enhancement problem is deﬁned so that we have an input
noisy signal ˜x and we want to clean it to obtain the enhanced
signal ˆx. We propose to do so with a speech enhancement GAN
Figure 2: Encoder-decoder architecture for speech enhancement (G network). The arrows between encoder and decoder
blocks denote skip connections.
(SEGAN). In our case, the G network performs the enhancement. Its inputs are the noisy speech signal ˜x together with the
latent representation z, and its output is the enhanced version
ˆx = G(˜x). We design G to be fully convolutional, so that there
are no dense layers at all. This enforces the network to focus on
temporally-close correlations in the input signal and throughout
the whole layering process. Furthermore, it reduces the number
of training parameters and hence training time.
The G network is structured similarly to an auto-encoder
(Fig. 2). In the encoding stage, the input signal is projected
and compressed through a number of strided convolutional layers followed by parametric rectiﬁed linear units (PReLUs) ,
getting a convolution result out of every N steps of the ﬁlter. We
choose strided convolutions as they were shown to be more stable for GAN training than other pooling approaches . Decimation is done until we get a condensed representation, called
the thought vector c, which gets concatenated with the latent
vector z. The encoding process is reversed in the decoding stage
by means of fractional-strided transposed convolutions (sometimes called deconvolutions), followed again by PReLUs.
The G network also features skip connections, connecting
each encoding layer to its homologous decoding layer, and bypassing the compression performed in the middle of the model
(Fig. 2). This is done because the input and output of the model
share the same underlying structure, which is that of natural
speech. Therefore, many low level details could be lost to reconstruct the speech waveform properly if we force all information to ﬂow through the compression bottleneck. Skip connections directly pass the ﬁne-grained information of the waveform
to the decoding stage (e.g., phase, alignment). In addition, they
offer a better training behavior, as the gradients can ﬂow deeper
through the whole structure .
An important feature of G is its end-to-end structure, so that
it processes raw speech sampled at 16 kHz, getting rid of any
intermediate transformations to extract acoustic features (contrasting to many common pipelines). In this type of model, we
have to be careful with typical regression losses like mean absolute error or mean squared error, as noted in the raw speech
generative model WaveNet .
These losses work under
strong assumptions on how our output distribution is shaped
and, therefore, impose important modeling limitations (like not
allowing multi-modal distributions and biasing the predictions
towards an average of all the possible predictions). Our solution
to overcome these limitations is to use the generative adversarial setting. This way, D is in charge of transmitting information
to G of what is real and what is fake, such that G can slightly
correct its output waveform towards the realistic distribution,
getting rid of the noisy signals as those are signaled to be fake.
In this sense, D can be understood as learning some sort of loss
for G’s output to look real.
In preliminary experiments, we found it convenient to add
a secondary component to the loss of G in order to minimize
the distance between its generations and the clean examples. To
measure such distance, we chose the L1 norm, as it has been
proven to be effective in the image manipulation domain . This way, we let the adversarial component to add more
ﬁne-grained and realistic results. The magnitude of the L1 norm
is controlled by a new hyper-parameter λ. Therefore, the G loss,
which we choose to be the one of LSGAN (Eq. 4), becomes
G VLSGAN(G) = 1
2 Ez∼pz(z),˜x∼pdata(˜x)[(D(G(z, ˜x), ˜x) −1)2]+
+ λ ∥G(z, ˜x) −x∥1.
4. Experimental Setup
4.1. Data Set
To evaluate the effectiveness of the SEGAN, we resort to the
data set by Valentini et al. . We choose it because it is open
and available1, and because the amount and type of data ﬁts our
purposes for this work: generalizing on many types of noise for
many different speakers. The data set is a selection of 30 speakers from the Voice Bank corpus : 28 are included in the train
set and 2 in the test set.
To make the noisy training set, a total of 40 different conditions are considered : 10 types of noise (2 artiﬁcial and
8 from the Demand database ) with 4 signal-to-noise ratio
(SNR) each (15, 10, 5, and 0 dB). There are around 10 different
sentences in each condition per training speaker. To make the
test set, a total of 20 different conditions are considered :
5 types of noise (all from the Demand database) with 4 SNR
each (17.5, 12.5, 7.5, and 2.5 dB). There are around 20 different
sentences in each condition per test speaker. Importantly, the
test set is totally unseen by (and different from) the training set,
using different speakers and conditions.
4.2. SEGAN Setup
The model is trained for 86 epochs with RMSprop and a
learning rate of 0.0002, using an effective batch size of 400. We
structure the training examples in two pairs (Fig. 3): the real
pair, composed of a noisy signal and a clean signal (˜x and x),
and the fake pair, composed of a noisy signal and an enhanced
signal (˜x and ˆx). To adequate the data set ﬁles to our waveform
generation purposes, we down-sample the original utterances
1 
Figure 3: Adversarial training for speech enhancement. Dashed
lines represent gradient backprop.
from 48 kHz to 16 kHz.
During train, we extract chunks of
waveforms with a sliding window of approximately one second
of speech (16384 samples) every 500 ms (50% overlap). During test, we basically slide the window with no overlap through
the whole duration of our test utterance and concatenate the results at the end of the stream. In both train and test, we apply a
high-frequecy preemphasis ﬁlter of coefﬁcient 0.95 to all input
samples (during test, output is correspondingly deemphasized).
Regarding the λ weight of our L1 regularization, after some
experimentation, we set it to 100 for the whole training. We initially set it to 1, but we observed that the G loss was two orders
of magnitude under the adversarial one, so the L1 had no practical effect on the learning. Once we set it to 100, we saw a
minimization behavior in the L1 and an equilibrium behavior in
the adversarial one. As the L1 got lower, the quality of the output samples increased, which we hypothesize helped G being
more effective in terms of realistic generation.
Regarding the architecture, G is composed of 22 onedimensional strided convolutional layers of ﬁlter width 31 and
strides of N = 2. The amount of ﬁlters per layer increases
so that the depth gets larger as the width (duration of signal in
time) gets narrower. The resulting dimensions per layer, being
it samples × feature maps, is 16384×1, 8192×16, 4096×32,
2048×32, 1024×64, 512×64, 256×128, 128×128, 64×256,
32×256, 16×512, and 8×1024. There, we sample the noise
samples z from our prior 8×1024-dimensional normal distribution N (0, I). As mentioned, the decoder stage of G is a mirroring of the encoder with the same ﬁlter widths and the same
amount of ﬁlters per layer. However, skip connections and the
addition of the latent vector make the number of feature maps
in every layer to be doubled.
The network D follows the same one-dimensional convolutional structure as G’s encoder stage, and it ﬁts to the conventional topology of a convolutional classiﬁcation network. The
differences are that (1) it gets two input channels of 16384 samples, (2) it uses virtual batch-norm before LeakyReLU nonlinearities with α = 0.3, and (3) in the last activation layer there
is a one-dimensional convolution layer with one ﬁlter of width
one that does not downsample the hidden activations (1×1 convolution). The latter (3) reduces the amount of parameters required for the ﬁnal classiﬁcation neuron, which is fully connected to all hidden activations with a linear behavior. This
means that we reduce the amount of required parameters in that
fully-connected component from 8 × 1024 = 8192 to 8, and
Table 1: Objective evaluation results comparing the noisy signal and the Wiener- and SEGAN-enhanced signals.
the way in which the 1024 channels are merged is learnable in
the parameters of the convolution.
All the project is developed with TensorFlow , and the
code is available at 
segan. We refer to this resource for further details of our implementation. A sample of the enhanced speech audios is provided at 
5. Results
5.1. Objective Evaluation
To evaluate the quality of the enhanced speech, we compute
the following objective measures (the higher the better). All
metrics compare the enhanced signal with the clean reference
of the 824 test set ﬁles. They have been computed using the
implementation included in , and available at the publisher
• PESQ: Perceptual evaluation of speech quality, using the
wide-band version recommended in ITU-T P.862.2 
(from –0.5 to 4.5).
• CSIG: Mean opinion score (MOS) prediction of the signal
distortion attending only to the speech signal (from 1
• CBAK: MOS prediction of the intrusiveness of background
noise (from 1 to 5).
• COVL: MOS prediction of the overall effect (from 1
• SSNR: Segmental SNR [35, p. 41] (from 0 to ∞).
Table 1 shows the results of these metrics. To have a comparative reference, it also shows the results of these metrics
when applied directly to the noisy signals and to signals ﬁltered
using the Wiener method based on a priori SNR estimation ,
as provided in . It can be observed how SEGAN gets slightly
worse PESQ. However, in all the other metrics, which better
correlate with speech/noise distortion, SEGAN outperforms the
Wiener method. It produces less speech distortion (CSIG) and
removes noise more effectively (CBAK and SSNR). Therefore,
it achieves a better tradeoff between the two factors (COVL).
5.2. Subjective Evaluation
A perceptual test has also been carried out to compare SEGAN
with the noisy signal and the Wiener baseline. For that, 20 sentences were selected from the test set. As the database does not
indicate the amount and type of noise for each ﬁle, the selection
was done by listening to some of the provided noisy ﬁles, trying to balance different noise types. Most of the ﬁles have low
SNR, but a few with high SNR were also included.
A total of 16 listeners were presented with the 20 sentences
2 
K14513_CD_Files.zip
Table 2: Subjective evaluation results comparing the noisy signal and the Wiener- and SEGAN-enhanced signals.
Figure 4: CMOS box plot (the median line in the SEGAN–
Wiener comparison is located at 1). Positive values mean that
SEGAN is preferred.
SEGAN–Noisy
SEGAN–Wiener
in a randomized order. For each sentence, the following three
versions were presented, also in random order: noisy signal,
Wiener-enhanced signal, and SEGAN-enhanced signal.
each signal, the listener rated the overall quality, using a scale
from 1 to 5. In the description of the 5 categories, they were
instructed to pay attention to both the signal distortion and the
noise intrusiveness (e.g., 5=excellent: very natural speech with
no degradation and not noticeable noise). Listeners could listen
to each signal as many times as they wanted, and were asked to
pay attention to the comparative rate of the three signals.
In Table 2, it can be observed how SEGAN is preferred
over both the noisy signal and the Wiener baseline. However,
as there is a large variation in the SNR of the noisy signal, the
MOS range is very large, and the difference between Wiener
and SEGAN is not signiﬁcant. However, as the listeners compared all the systems at same time, it is possible to compute
the comparative MOS (CMOS) by subtracting the MOS of the
two systems being compared. Fig. 4 depicts this relative comparison. We can see how the signals generated by SEGAN are
preferred. More speciﬁcally, SEGAN is preferred over the original (noisy) signal in 67% of the cases, while the noisy signal
is preferred in 8% of the cases (no preference in 25% of the
cases). With respect to the Wiener system, SEGAN is preferred
in 53% of cases and Wiener is preferred in 23% of the cases (no
preference in 24% of the cases).
6. Conclusions
In this work, an end-to-end speech enhancement method has
been implemented within the generative adversarial framework.
The model works as an encoder-decoder fully-convolutional
structure, which makes it fast to operate for denoising waveform chunks. The results show that, not only the method is viable, but it can also represent an effective alternative to current
approaches. Possible future work involves the exploration of
better convolutional structures and the inclusion of perceptual
weightings in the adversarial training, so that we reduce possible high frequency artifacts that might be introduced by the
current model. Further experiments need to be done to compare
SEGAN with other competitive approaches.
7. Acknowledgements
This work was supported by the project TEC2015-69266-P
(MINECO/FEDER, UE).
8. References
 P. C. Loizou, Speech Enhancement: Theory and Practice, 2nd ed.
Boca Raton, FL, USA: CRC Press, Inc., 2013.
 L.-P. Yang and Q.-J. Fu, “Spectral subtraction-based speech enhancement for cochlear implant patients in background noise,”
The Journal of the Acoustical Society of America, vol. 117, no. 3,
pp. 1001–1004, 2005.
 D. Yu, L. Deng, J. Droppo, J. Wu, Y. Gong, and A. Acero, “A
minimum-mean-square-error noise reduction algorithm on melfrequency cepstra for robust speech recognition,” in Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal Processing
IEEE, 2008, pp. 4041–4044.
 A. L. Maas, Q. V. Le, T. M. O’Neil, O. Vinyals, P. Nguyen, and
A. Y. Ng, “Recurrent neural networks for noise reduction in robust
asr.” in Proc. of INTERSPEECH, 2012, pp. 22–25.
 J. Ortega-Garcia and J. Gonzalez-Rodriguez, “Overview of
speech enhancement techniques for automatic speaker recognition,” in Spoken Language, 1996. ICSLP 96. Proceedings., Fourth
International Conference on, vol. 2, Oct 1996, pp. 929–932 vol.2.
 M. Berouti, R. Schwartz, and J. Makhoul, “Enhancement of
speech corrupted by acoustic noise,” in Proc. of the Int. Conf. on
Acoustics, Speech, and Signal Processing (ICASS), vol. 4, Apr
1979, pp. 208–211.
 J. Lim and A. Oppenheim, “All-pole modeling of degraded
speech,” IEEE Trans. on Acoustics, Speech, and Signal Processing, vol. 26, no. 3, pp. 197–210, Jun 1978.
 Y. Ephraim, “Statistical-model-based speech enhancement systems,” Proceedings of the IEEE, vol. 80, no. 10, pp. 1526–1555,
 M. Dendrinos, S. Bakamidis, and G. Carayannis, “Speech enhancement from noise: A regenerative approach,” Speech Communication, vol. 10, no. 1, pp. 45–57, 1991.
 Y. Ephraim and H. L. Van Trees, “A signal subspace approach for
speech enhancement,” IEEE Trans. on speech and audio processing, vol. 3, no. 4, pp. 251–266, 1995.
 S. Tamura and A. Waibel, “Noise reduction using connectionist
models,” in Proc. of the IEEE Int. Conf. on Acoustics, Speech and
Signal Processing (ICASSP), 1988, pp. 553–556.
 S. Parveen and P. Green, “Speech enhancement with missing data
techniques using recurrent neural networks,” in Proc. of the IEEE
Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),
2004, pp. 733–736.
 X. Lu, Y. Tsao, S. Matsuda, and C. Hori, “Speech enhancement based on deep denoising autoencoder.” in Proc. of INTER-
SPEECH, 2013, pp. 436–440.
 F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le Roux,
J. R. Hershey, and B. Schuller, “Speech enhancement with LSTM
recurrent neural networks and its application to noise-robust
ASR,” in Proc. of the Int. Conf. on Latent Variable Analysis and
Signal Separation, 2015, pp. 91–99.
 Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “A regression approach to
speech enhancement based on deep neural networks,” IEEE/ACM
Trans. on Audio, Speech and Language Processing, vol. 23, no. 1,
pp. 7–19, 2015.
 A. Kumar and D. Florencio, “Speech enhancement in multiplenoise conditions using deep neural networks,” in Proc. of the
Int. Speech Communication Association Conf. (INTERSPEECH),
2016, pp. 3738–3742.
 D. Wang and J. Lim, “The unimportance of phase in speech enhancement,” IEEE Trans. on Acoustics, Speech, and Signal Processing, vol. 30, no. 4, pp. 679–681, Aug 1982.
 K. Paliwal, K. W´ojcicki, and B. Shannon, “The importance of
phase in speech enhancement,” Speech Communication, vol. 53,
no. 4, pp. 465 – 494, 2011. [Online]. Available: 
sciencedirect.com/science/article/pii/S0167639310002086
 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems (NIPS), 2014, pp. 2672–2680.
 P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-toimage translation with conditional adversarial networks,” ArXiv:
1611.07004, 2016.
 X. Mao, Q. Li, H. Xie, R. Y. K. Lau, and Z. Wang, “Least squares
generative adversarial networks,” ArXiv: 1611.04076, 2016.
 A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434, 2015.
 K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,”
in Proc. of the IEEE Int. Conf. on Computer Vision (ICCV), 2015,
pp. 1026–1034.
 ——, “Deep residual learning for image recognition,” in Proc.
of the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2016, pp. 770–778.
 A. van den Oord,
S. Dieleman,
K. Simonyan,
O. Vinyals,
A. Graves,
N. Kalchbrenner,
A. Senior,
K. Kavukcuoglu, “Wavenet: A generative model for raw audio,”
CoRR abs/1609.03499, 2016.
 D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros,
“Context encoders: Feature learning by inpainting,” in Proc. of the
IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),
2016, pp. 2536–2544.
 C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi,
“Investigating rnn-based speech enhancement methods for noiserobust text-to-speech,” in 9th ISCA Speech Synthesis Workshop,
pp. 146–152.
 C. Veaux, J. Yamagishi, and S. King, “The voice bank corpus:
Design, collection and data analysis of a large regional accent
speech database,” in Int. Conf. Oriental COCOSDA, held jointly
with 2013 Conference on Asian Spoken Language Research and
Evaluation (O-COCOSDA/CASLRE).
IEEE, 2013, pp. 1–4.
 J. Thiemann, N. Ito, and E. Vincent, “The diverse environments
multi-channel acoustic noise database: A database of multichannel environmental noise recordings,” The Journal of the Acoustical Society of America, vol. 133, no. 5, pp. 3591–3591, 2013.
 T. Tieleman and G. Hinton, “Lecture 6.5-RMSprop: divide the
gradient by a running average of its recent magnitude,” COURS-
ERA: Neural Networks for Machine Learning 4, 2, 2012.
 T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
and X. Chen, “Improved techniques for training gans,” in Advances in Neural Information Processing Systems (NIPS), 2016,
pp. 2226–2234.
 M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,
G. S. Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow:
Large-scale machine learning on heterogeneous distributed systems,” arXiv preprint arXiv:1603.04467, 2016.
 P.862.2: Wideband extension to Recommendation P.862 for the
assessment of wideband telephone networks and speech codecs,
ITU-T Std. P.862.2, 2007.
 Y. Hu and P. C. Loizou, “Evaluation of objective quality measures for speech enhancement,” IEEE Trans. on Audio, Speech,
and Language Processing, vol. 16, no. 1, pp. 229–238, Jan 2008.
 S. R. Quackenbush, T. P. Barnwell, and M. A. Clements, Objective
Measures of Speech Quality.
Englewood Cliffs, NJ: Prentice-
Hall, 1988.
 P. Scalart and J. V. Filho, “Speech enhancement based on a priori signal to noise estimation,” in Proc. of the IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing (ICASSP), vol. 2, May
1996, pp. 629–632 vol. 2.