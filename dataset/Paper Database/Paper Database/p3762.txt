IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
Multiple Feature Learning for Hyperspectral
Image Classiﬁcation
Jun Li, Xin Huang, Senior Member, IEEE, Paolo Gamba, Fellow, IEEE,
José M. Bioucas-Dias, Member, IEEE, Liangpei Zhang, Senior Member, IEEE,
Jón Atli Benediktsson, Fellow, IEEE, and Antonio Plaza, Senior Member, IEEE
Abstract—Hyperspectral image classiﬁcation has been an active
topic of research in recent years. In the past, many different types
of features have been extracted (using both linear and nonlinear
strategies) for classiﬁcation problems. On the one hand, some
approaches have exploited the original spectral information or
other features linearly derived from such information in order to
have classes which are linearly separable. On the other hand, other
techniques have exploited features obtained through nonlinear
transformations intended to reduce data dimensionality, to better
model the inherent nonlinearity of the original data (e.g., kernels)
or to adequately exploit the spatial information contained in the
scene (e.g., using morphological analysis). Special attention has
been given to techniques able to exploit a single kind of features,
such as composite kernel learning or multiple kernel learning,
developed in order to deal with multiple kernels. However, few
approaches have been designed to integrate multiple types of features extracted from both linear and nonlinear transformations.
In this paper, we develop a new framework for the classiﬁcation
of hyperspectral scenes that pursues the combination of multiple
features. The ultimate goal of the proposed framework is to be
able to cope with linear and nonlinear class boundaries present in
the data, thus following the two main mixing models considered
for hyperspectral data interpretation. An important characteristic
of the presented approach is that it does not require any regularization parameters to control the weights of considered features
so that different types of features can be efﬁciently exploited
and integrated in a collaborative and ﬂexible way. Our experimental results, conducted using a variety of input features and
hyperspectral scenes, indicate that the proposed framework for
multiple feature learning provides state-of-the-art classiﬁcation
results without signiﬁcantly increasing computational complexity.
Index Terms—Hyperspectral imaging, linear and nonlinear features, multiple feature learning.
Manuscript received March 19, 2014; revised July 18, 2014; accepted
July 22, 2014.
J. Li is with the School of Geography and Planning and Guangdong Key
Laboratory for Urbanization and Geo-Simulation, Sun Yat-sen University,
Guangzhou 510275, China.
X. Huang and L. Zhang are with the State Key Laboratory of Information
Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University,
Wuhan 430072, China.
P. Gamba is with the Telecommunications and Remote Sensing Laboratory,
University of Pavia, 27100 Pavia, Italy.
J. M. Bioucas-Dias is with the Instituto de Telecomunicações, Instituto
Superior Técnico, Universidade de Lisboa, 1049-001 Lisboa, Portugal.
J. A. Benediktsson is with the Faculty of Electrical and Computer Engineering, University of Iceland, 101 Reykjavik, Iceland.
A. Plaza is with the Hyperspectral Computing Laboratory, Department of
Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, 10003 Cáceres, Spain.
Color versions of one or more of the ﬁgures in this paper are available online
at 
Digital Object Identiﬁer 10.1109/TGRS.2014.2345739
I. INTRODUCTION
HE recent availability of remotely sensed hyperspectral
images has fostered the development of techniques able
to interpret such high-dimensional data in many different application contexts . It is now commonly accepted that using
the spatial and the spectral information simultaneously provides
signiﬁcant advantages in terms of improving the performance
of classiﬁcation techniques. A detailed overview of recent
advances in the spatial–spectral classiﬁcation of hyperspectral
data is available in . Resulting from the need to model
both the spectral and the spatial information contained in the
original data, different types of features have been exploited
for spectral–spatial classiﬁcation. These features can be mainly
classiﬁed into two categories.
1) On the one hand, several methods exploit the original
spectral information or other features linearly derived
from such information. These kind of features have been
widely used to exploit the linear separability of certain
classes . Techniques commonly used for this purpose
include the maximum noise fraction , independent
component analysis , linear spectral unmixing , or
projection pursuit , among many others .
2) On the other hand, in real analysis scenarios, it is likely to
ﬁnd cases in which nonlinear features are more effective
for class discrimination due to the existence of nonlinear
class boundaries. As a result, several techniques have
focused on exploiting features obtained through nonlinear
transformations to better model the inherent nonlinearity
of the original data. Examples include kernel methods ,
 and manifold regularization , . Other nonlinear approaches are focused on adequately exploiting
the spatial information contained in the scene, e.g., using
morphological analysis , .
Once relevant features have been extracted from the original
data, the classiﬁcation process itself can also be either linear or
nonlinear. For instance, in linear discriminant analysis , a
linear function is used in order to maximize the discriminatory
power and separate the available classes effectively. However,
such a linear function may not be the best choice, and nonlinear
strategies such as quadratic discriminant analysis or logarithmic
discriminant analysis have also been used. The main problem of
these supervised classiﬁers, however, is their sensitivity to the
Hughes effect .
In turn, kernel methods have been widely used in order
to deal effectively with the Hughes phenomenon , .
0196-2892 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See for more information.
LI et al.: MULTIPLE FEATURE LEARNING FOR HYPERSPECTRAL IMAGE CLASSIFICATION
The idea is to use a kernel trick that allows the separation
of the classes in a higher dimensional space by means of a
nonlinear transformation, particularly in those cases in which
the problem is not linearly separable in the original feature
space. The combination of kernel methods and nonlinearly
derived features (such as morphological features) has also
been widely explored in the context of hyperspectral image
classiﬁcation .
Recently, a new trend has been oriented towards the composition of different kernels for improved learning, inspired by
multiple kernel learning (MKL) approaches – . Some
of these aspects were particularly discussed in , in which
a detailed overview of machine learning in remote sensing
data processing is given. For instance, a simple strategy to
incorporate the spatial context into kernel-based classiﬁers is
to deﬁne a pixel entity both in the spectral domain (using
its spectral content) and in the spatial domain, e.g., by applying some feature extraction to its surrounding area which
yields spatial (contextual) features, such as those derived using
morphological analysis. These separated entities lead to two
different kernel matrices, which can be easily computed. At
this point, one can sum spectral and textural dedicated kernel
matrices and introduce the cross-information between textural
and spectral features in the formulation. This methodology
yields a full family of composite kernel (CK)-based methods
for hyperspectral data classiﬁcation .
More recently, CKs have been generalized in using
the multinomial logistic regression (MLR) classiﬁer and
extended multiattribute proﬁles (EMAPs) . The MLR has
been recently explored in hyperspectral imaging as a technique
able to model the posterior class distributions in a Bayesian
framework, thus supplying (in addition to the boundaries between the classes) a degree of plausibility for such classes
 . The resulting generalized CK (GCK)-based MLR can
combine multiple kernels without any restriction of convexity.
This introduces a different approach with regards to traditional
CK and MKL methods, in which CKs need to be convex
combinations of kernels.
At this point, it is important to emphasize that both CK
learning and MKL focus on kernels, which are obtained either
from the original (linear) spectral features or from (nonlinear) features such as morphological proﬁles. These approaches
exploit the information contained in the kernels using linear
combinations, due to the fact that the optimization problem
is much easier to solve under a linear framework. With these
assumptions in mind, very good performance has been reported
for MKL or other CK learning approaches in different remote
sensing problems , , . However, these approaches
focus on kernels, while kernel transformations of nonlinear
features might bring redundancy or lose the physical meaning
of the features themselves. Instead, in certain situations, it may
be desirable to exploit the information carried out by each
feature under its speciﬁcal physical or acquisition conditions.
Inspired by these ideas and based on the fact that it is common
to have both linear and nonlinear class boundaries in the same
scene, this paper develops a new framework for classiﬁcation
of hyperspectral images which integrates multiple features extracted from linear and nonlinear transformations.
A main characteristic of the presented approach is that it
can adaptively exploit information from both linear and nonlinearly derived features, thus being able to address practical
scenarios in which different classes may need different (linear
or nonlinear) strategies. It should be noted that, as it is the
case of MKL, the proposed approach also follows a linear
optimization framework due to model complexity. However,
the proposed approach has been designed in a way that it
exhibits great ﬂexibility to combine different types of features
without any regularization parameters to control the weight
of each feature, thus taking advantage of the complementarity
that the features can provide without any a priori restrictions.
In turn, MKL (which can be seen as a special instance of
our proposed framework) generally needs to learn the weight
parameters which is difﬁcult from the viewpoint of both optimization and computational cost. Our presented approach is
thus aimed at exploiting the different properties that both linear
and nonlinear features can provide, with the ultimate goal of
being able to characterize both linear and nonlinear boundaries
independent of which type of features dominate the scene. In
order to achieve the desired spectral–spatial integration that
is normally expected in advanced classiﬁcation problems, we
consider morphological features as an important part of our
framework, which also exploits kernel-based features and the
original spectral information contained in the hyperspectral
The remainder of this paper is organized as follows.
Section II presents the proposed classiﬁcation framework,
which uses the sparse MLR (SMLR) as the baseline
classiﬁer. It will be shown that this classiﬁer provides a natural framework to achieve the desired integration of multiple features. Section III reports the classiﬁcation results
obtained by the proposed multiple feature learning approach
using different real hyperspectral data sets, which comprise
a scene collected by the Airborne Visible/Infrared Imaging
Spectrometer (AVIRIS) over the Indian Pines region in Indiana,
two scenes collected by the Reﬂective Optics Spectrographic
Imaging System (ROSIS) over the city of Pavia, Italy, and
a scene collected by the Hyperspectral Digital Imagery Collection Experiment (HYDICE) over the city of Washington
DC. These data sets have been widely used for evaluating the
performance of hyperspectral image classiﬁcation algorithms,
and the results reported in this work rank among the most accurate ones ever reported for these scenes. Section IV concludes
this paper with some remarks and hints at plausible future
research lines.
II. PROPOSED FRAMEWORK FOR MULTIPLE
FEATURE LEARNING
First of all, we deﬁne the notations that will be adopted
throughout this paper. Let K ≡{1, . . . , K} denote a set of K
class labels, S ≡{1, . . . , n} denote a set of integers indexing
the n pixels of a hyperspectral image, x ≡(x1, . . . , xn) ∈
Rd denote such hyperspectral image, which is made up of
d-dimensional feature vectors, y ≡(y1, . . . , yn) denote an image of labels, and DL ≡{(x1, y1), . . . , (xL, yL)} be the labeled training set with L being the number of samples in DL.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
In this work, we model the posterior class probabilities using
the MLR as follows:
p(yi = k|xi, ω) ≡
ω(k)T h(xi)
ω(k)T h(xi)
where h(xi) is the input feature, ω denotes the regressors,
and ω ≡[ω(1)T , . . . , ω(K−1)T ]
T . Since the density (1) does not
depend on translations on the regressors ω(k), in this work, we
take ω(K) = 0. It should be noted that the input feature h can
be linear or nonlinear. In the former case, we have
h(xi) = [1, xi,1, . . . , xi,d]T
where xi,j denotes the jth component of xi. On the other hand,
the input feature h can also be nonlinear, in which case we have
h(xi) = [1, ψ1(xi), . . . , ψl1(xi)]T
which is a feature vector with l1 elements and which is built
based on part of or the complete observation x, with ψ(·) being
a nonlinear function. Depending on the nonlinear function used,
there are many possible ways to build nonlinear features. For
instance, a kernel is some symmetric function with the form
h(xi) = [1, K(xi, x1), . . . , K(xi, xl)]T
K(xi, xj) = ⟨φ(xi)φ(xj)⟩
and φ(·) is a nonlinear mapping function. Kernels have been
largely used in this context since they tend to improve data separability in the transformed space. However, other types of nonlinear functions for feature extraction may also be considered
h(xi) = [1, f1(xi), . . . , fl2(xi)]T
where f(·) is a nonlinear feature extraction transformation
on the original data (for instance, the EMAP in ) and l2
is the number of elements in h(xi). It should be noted that
both the linear function h(xi) = xi and the kernel function
h(xi) = K(xi, x) can be simply regarded as two instances of
the nonlinear case.
As mentioned before, there have been some efforts in the
literature to combine different types of features, such as MKL.
Linear features have been generally less effective for hyperspectral image classiﬁcation than nonlinear features. In turn,
kernel-based features (obtained from linear or nonlinear transformations) have been more widely used. This trend has been
exploited by MKL by focusing on kernel features, which are
extracted from the original spectral data or the nonlinear transformed data. However, few efforts have attempted to exploit
both linear and nonlinear features in simultaneous fashion,
despite the fact that they can exhibit some complementary
properties (e.g., some classes may be properly separated using
linear boundaries, while other classes may require nonlinear
boundaries for separability). In real analysis scenarios, it is
likely to have both linear and nonlinear class boundaries in
the same hyperspectral image. At the same time, kernel transformations of nonlinear features may lead to data redundancy
and loss of physical meaning for the features. It is therefore
important for a methodology to be able to cope with such
linear and nonlinear boundaries simultaneously and adaptively.
In this regard, the proposed framework provides the possibility
to interpret multiple boundaries together. Again, different features have different characteristics, and the joint exploitation
of different kinds of features could lead to improved data
separability. Inspired by this idea, we develop a framework
for the integration of multiple features, with the ultimate goal
of exploiting the characteristics of each type of feature in the
classiﬁcation process. For this purpose, we ﬁrst deﬁne
1, h1(xi)T , h2(xi)T , . . . , hl(xi)T T
a vector of l ﬁxed functions of the input data xi, where
hj(xi) ≡[hj,1(xi), . . . , hj,lj(xi)] ∈Rlj (for j = 1, . . . , l) is a
feature obtained by a linear/nonlinear transformation and lj is
the number of elements in hj(xi). Notice that, if hj(xi) is a
kernel function, then (6) is a combination of multiple kernels
(this is the particular case addressed by MKL). Instead, our
proposed framework opens the structure to the exploitation
of multiple features, not necessarily kernels. In this scenario,
learning the class densities amounts to estimating the logistic
regressors ω given by the input features h(x). Following previous work , – , we compute ω by calculating the
maximum a posteriori estimate
ω = arg max
ℓ(ω) + log p(ω)
where ℓ(ω) is the log-likelihood function given by
p(yi|xi, ω)
hT (xi)ω(yi)−log
hT (xi)ω(k)
and log p(ω) is a prior over ω which is independent from the
observation x. In order to control the machine complexity
and, thus, its generalization capacity, we model ω as a random
vector with Laplacian density p(ω) ∝exp(−λ∥ω∥1), where
λ is the regularization parameter controlling the degree of
sparsity , .
Let νj = [ωj,1, . . . , ωj,lj]T denote the regressors associated
with feature hj(·). By introducing the input features in (6),
problem (7) can be solved as follows:
ω= arg max
hT (xi)ω(yi)−log
hT (xi)ω(k)
+ log p(ω)
+ log p(ω)
hj,p(xi)ω(yi)
1 +hj,p(xi)ω(k)
LI et al.: MULTIPLE FEATURE LEARNING FOR HYPERSPECTRAL IMAGE CLASSIFICATION
where the term in (10) is independent from the observation data.
It is also independent from the nonlinear functions used. At this
point, several important observations can be made.
1) First and foremost, if h(xi) is a combination of multiple
kernels, then (9) stands for a typical MKL problem.
However, as compared with the simple MKL implemented on the support vector machine model, problem
(9) require no convexity constraint for the combination
of multiple kernels. From this observation, we can also
see MKL as a speciﬁc instance of the proposed multiple
learning framework.
2) As shown in (11) we have a linear combination of
multiple nonlinear features which is not restricted to
kernels, and the logistic weights νj are speciﬁc for each
associated nonlinear feature hj(·) and independent from
any other νp, for p = 1, . . . , l and p ̸= j. This is quite
important as, on the one hand, the linear combination
provides great ﬂexibility for the classiﬁer to search for
the most representative features, which could be linear
or nonlinear, thus balancing the information provided
by different features while reducing the computational
complexity due to the possibility to use a conventional
optimization approach.
3) Furthermore, the linear combination in (11) provides
sufﬁcient ﬂexibility to ﬁnd the most representative feature
hj and also provides the potential to ﬁnd the most representative elements in each feature. As a result, the ﬁnal
logistic weights could be derived from a combination
of different features, which is a collaborative solution
involving multiple (linear or nonlinear) features.
4) It is ﬁnally important to point out that, by introducing the
Laplacian prior p(ω) which can lead to sparse solutions,
the proposed approach can deal with high-dimensional
input features using limited training samples, thus addressing ill-posed problems.
To conclude this section, we emphasize that the optimization
problem (9) can be solved by the SMLR in and by the
fast SMLR in . However, most hyperspectral data sets are
beyond the reach of these algorithms, as their processing becomes unbearable when the dimensionality of the input features
increases. This is even more critical in our framework, in which
we use multiple features. In order to address this issue, we take
advantage of the logistic regression via variable splitting and
augmented Lagrangian (LORSAL) algorithm in and ,
with overall complexity O(L × (l1 + · · · + ll) × K). At this
point, we recall that L is the number of training samples, K
is the number of classes, and lj is the number of elements in
the jth linear/nonlinear feature. LORSAL is able to deal with
high-dimensional features and plays a central role in this work,
as in previous contributions , . A full demo with our
algorithm implementation is given.
III. EXPERIMENTAL RESULTS
In this section, we provide an experimental evaluation for
the presented framework using four real hyperspectral data sets.
In our experiments, we consider four different linear/nonlinear
features as reported in Table I. Speciﬁcally, we use a linear
TYPES OF FEATURES CONSIDERED IN THIS WORK
feature hlinear (the original spectral information), a nonlinear
feature hEMAP (which uses the concept of EMAP in and
 ), and two kernel features constructed over the two previously mentioned sources of information (spectral and spatial,
respectively) using the Gaussian radial-basis-function kernel:
K(xi, xj) = exp(−∥xi −xj∥2/2σ2) which is widely used in
hyperspectral image classiﬁcation . In this work, the spectral kernel Klinear is built by using the original spectral data,
and the spatial kernel KEMAP is built by using the EMAP. At
this point, we emphasize that the linear and nonlinear features
that have been selected for experiments in this work can be
considered highly representative of the spectral and spatial
information contained in the scene. While hlinear is a linear
feature related to the original spectral information, hEMAP exploits the interpretation of the data in spatial terms, and Klinear
and KEMAP are nonlinear representations of the original data
and EMAPs, respectively. For the considered problems, we
only use four different features as these features are able to
provide very good performance. However, we would like to
emphasize again that any other kind of features can be included
in our framework, according to the considered application. At
this point, we reiterate that the proposed framework has been
designed to cope with both linear and nonlinear boundaries
in a general way so that other additional features (linear and
nonlinear) could be included in accordance with the speciﬁc
application domain. We believe, however, that the selected
features are sufﬁciently representative in order to demonstrate
the advantages of our proposed framework.
We emphasize that, in all our experiments, the parameter
values involved have been carefully optimized so that the best
performance is reported for each considered method. For the
EMAP-based feature extraction, we have used a grid search
approach to optimize parameter values, and for the LORSAL
classiﬁcation, we have also carefully optimized the parameter
λ. Nevertheless, as shown in , we may have a large amount
of suboptimal options, and the solution is insensitive to different
suboptimal values. The reported ﬁgures of overall accuracy
(OA) [%], average accuracy (AA) [%], κ statistic [%], and individual classiﬁcation accuracies [%] are obtained by averaging
the results obtained after conducting ten independent Monte
Carlo runs with respect to the training set DL. At the same time,
we include the standard deviation in order to assess the statistical signiﬁcance of the results. Finally, in order to show the ef-
ﬁciency of the proposed framework, the computational time in
seconds for learning the features is also reported in all cases (the
time for deriving the features is not included for simplicity).
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
(a) False color composition of the AVIRIS Indian Pines scene. (b) Reference map containing 16 mutually exclusive land-cover classes (right).
(a) False color composition of the ROSIS University of Pavia scene. (b) Reference map containing nine mutually exclusive land-cover classes. (c) Training
set used in experiments.
The remainder of the section is organized as follows. In
Section III-A, we introduce the data sets used for evaluation.
Section III-B describes the experiments with the AVIRIS Indian
Pines data set. Section III-C conducts experiments using the
ROSIS Pavia University data set. Finally, Section III-D presents
theresultsobtainedforthetworemaininghyperspectraldatasets.
A. Hyperspectral Data Sets
Four hyperspectral data sets collected by two different instruments are used in our experiments.
1) The ﬁrst hyperspectral image used in experiments was
collected by the AVIRIS sensor over the Indian Pines
region in Northwestern Indiana in 1992. This scene, with
a size of 145 lines by 145 samples, was acquired over
a mixed agricultural/forest area, early in the growing
season. The scene comprises 202 spectral channels in the
wavelength range from 0.4 to 2.5 μm, nominal spectral
resolution of 10 nm, moderate spatial resolution of 20 m
by pixel, and 16-b radiometric resolution. After an initial
screening, several spectral bands were removed from the
data set due to noise and water absorption phenomena,
leaving a total of 164 radiance channels to be used in
the experiments. For illustrative purposes, Fig. 1(a) shows
a false color composition of the AVIRIS Indian Pines
scene, while Fig. 1(b) shows the reference map available
for the scene, displayed in the form of a class assignment for each labeled pixel, with 16 mutually exclusive
reference classes, for a total of 10 366 samples. These
data, including reference information, are available online at ftp://ftp.ecn.purdue.edu/biehl/MultiSpec/92AV3C.
tif.zip, a fact which has made this scene a widely used
benchmark for testing the accuracy of hyperspectral data
classiﬁcation algorithms. This scene constitutes a challenging classiﬁcation problem due to the presence of
mixed pixels in all available classes and because of the
unbalanced number of available labeled pixels per class.
2) The second hyperspectral data set was collected by
the ROSIS optical sensor over the urban area of the
University of Pavia, Italy. The ﬂight was operated by
the Deutschen Zentrum for Luftund Raumfahrt (DLR,
the German Aerospace Agency) in the framework of the
HySens project, managed and sponsored by the European
Union. The image size in pixels is 610 × 340, with very
high spatial resolution of 1.3 me per pixel. The number of
data channels in the acquired image is 103 (with spectral
range from 0.43 to 0.86 μm). Fig. 2(a) shows a false
color composite of the image, while Fig. 2(b) shows
nine reference classes of interest, which comprise urban
features, as well as soil and vegetation features. Out of
the available reference pixels, 3921 were used for training
[see Fig. 2(c)], and 42 776 samples were used for testing.
LI et al.: MULTIPLE FEATURE LEARNING FOR HYPERSPECTRAL IMAGE CLASSIFICATION
OVERALL, AVERAGE, AND INDIVIDUAL CLASSIFICATION ACCURACIES [%] OBTAINED BY THE PROPOSED FRAMEWORK
(WITH DIFFERENT TYPES OF FEATURES) WHEN APPLIED TO THE AVIRIS INDIAN PINES HYPERSPECTRAL DATA SET
WITH A BALANCED TRAINING SET IN WHICH 5% OF THE LABELED SAMPLES PER CLASS ARE USED FOR TRAINING
(A TOTAL OF 515 SAMPLES) AND THE REMAINING LABELED SAMPLES ARE USED FOR TESTING
3) The third data set was also collected by the ROSIS optical
sensor over a different location in the city center of Pavia,
Italy. The ﬂight was also operated by DLR in the HySens
framework. The number of data channels in the acquired
image is 102 (with spectral range from 0.43 to 0.86 μm),
and the spatial resolution is again 1.3 m per pixel. These
data were used in the 2008 IEEE Geoscience and Remote
Sensing Data Fusion Technical Committee contest. Additional details about the data and the training/test samples
are available in .
4) The fourth data set was collected by HYDICE over the
Mall area in Washington DC. The data set comprises 210
spectral bands from 0.4 to 2.4 μm. Bands in the 0.9- and
1.4-μm region where the atmosphere is opaque have been
omitted from the data set, leaving 191 bands. The data
set contains 1208 × 307 pixels, with a spatial resolution
of about 2.8 m. Seven thematic land-cover classes are
present in the scene: roofs, street, path (graveled paths
down the mall center), grass, trees, water, and shadow,
with 19 629 labeled samples in the ground-truth image.
The scene is available online at 
edu/~biehl/Hyperspectral_Project.zip.
B. Experiments With the AVIRIS Indian Pines Data Set
For this data set, the EMAPs were built using threshold
values in the range from 2.5% to 10% with respect to the mean
of the individual features, with a step of 2.5% for the standard
deviation attribute and thresholds of 200, 500, and 1000 for the
area attribute.
1) Experiment 1: In our ﬁrst set of experiments, we evaluated the classiﬁcation accuracy of the proposed approach using
a balanced training set per class in which around 5% of the
labeled samples per class were used for training (a total of
515 samples), and the remaining labeled samples were used for
testing. For very small classes, we took a minimum of three
training samples per class. Table II shows the overall, average,
and individual classiﬁcation accuracies (in percentage) and the
κ statistic, along with the standard deviations, obtained after
using the proposed framework with different types of features
when applied to the AVIRIS Indian Pines scene.
From Table II, we can conclude that the proposed framework
achieved the best results in terms of classiﬁcation accuracies
when all the considered features were used. This is expected
since, in this case, the proposed scheme seeks for the best
solution among all the available (linear and nonlinear) features.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
COMPARISON BETWEEN THE PROPOSED FRAMEWORK AND CK AND GCK USING THE AVIRIS
INDIAN PINES SCENE. THE PROCESSING TIME (IN SECONDS) IS ALSO REPORTED IN EACH CASE
NUMBER OF PIXELS DOMINATED BY EACH CONSIDERED TYPE OF FEATURE AND CLASSIFICATION ACCURACIES
OBTAINED WHEN APPLYING THE PROPOSED FRAMEWORK TO THE AVIRIS INDIAN PINES DATA SET.
IN THIS EXPERIMENT, WE USED APPROXIMATELY 30 TRAINING SAMPLES PER CLASS
On the other hand, the results obtained using the nonlinear
feature hEMAP are better than those obtained using the original
spectral information. This is consistent with previous studies,
indicating that the EMAP provides a powerful tool for feature
extraction, where the features extracted in the spatial domain
can improve class separability , . Another interesting
observation is that the results obtained using only the nonlinear
feature hEMAP are better than those obtained from its kernel
transformation KEMAP. This suggests that the kernel transformation of this particular nonlinear feature may not be able to
improve the class separability.
2) Experiment 2: In our second experiment, we compare
the proposed framework with CK learning and GCK
learning . Notice that all the experiments share exactly the
same training and test sets. Table III shows that the proposed
framework with hall (i.e., using all the considered features)
leads to the best classiﬁcation results. However, the proposed
framework exhibits the highest computational cost. Another
important observation is that the results obtained by the EMAPs
hEMAP were better than those obtained by the kernel transformation KEMAP. As discussed, this is an indication that a kernel
transformation of high-dimensional nonlinear features may not
be able to improve the class separability.
3) Experiment 3: In our third experiment, we analyze the
relevance of linear and nonlinear features in the ﬁnal classiﬁcation results, with the ultimate goal of analyzing their capacity to
characterize different complex classes in the scene. That is, in
the set of all nonlinear features hall, we would like to analyze
which feature has the most signiﬁcant contribution. Here, we
will use approximately 30 training samples per class, which is
an unbalanced scenario in comparison with the one considered
in the former experiment. Let (νj)T hj be the numerator of
the MLR in (1). For p = 1, . . . , K and p ̸= j, if (νj)T hj ≥
(νp)T hp, then we conclude that the classiﬁcation is dominated
by hj. Table IV reports the total number of pixels in the scene
which are dominated by each kind of feature.
LI et al.: MULTIPLE FEATURE LEARNING FOR HYPERSPECTRAL IMAGE CLASSIFICATION
Logistic regressors of the MLR classiﬁer obtained from the AVIRIS Indian Pines data set corresponding to the experiment reported in Table IV.
(a) Class Corn-no till is dominated by the spectral kernel Klinear. (b) Class Soybeans-min till is dominated by the original spectral information hlinear.
(c) Class Woods is dominated by the EMAP features hEMAP. (d) Class Soybean-clean till has contributions from all the considered features.
Several conclusions can be observed from Table IV. First and
foremost, it is remarkable that, for most classes, the dominating
feature according to Table IV is hEMAP. This is consistent with
previous works, revealing the power of EMAP for separating
most classes which are nonlinearly separable in the spatial domain , . Furthermore, it is remarkable that the original
spectral information is highly relevant. This is due to the fact
that some of the classes, e.g., Soybeans-min till, are likely to be
linearly separable. It is also observable that the kernel version
of the spectral information provides important contributions,
particularly for the Corn-no till. This is because no-till is an
agricultural technique which increases the amount of water that
inﬁltrates into the soil and increases organic matter retention
and cycling of nutrients in the soil. Along with the complexity
of corn itself, this may lead to nonlinearities that appear to
be better explained by kernel-based features, as indicated in
Table IV. However, the kernel version of EMAP rarely dominates the classiﬁcation. This conﬁrms our introspection that a
kernel transformation of the nonlinear EMAP feature may not
signiﬁcantly improve the class separability, which is already
fully exploited by the original EMAP itself.
In order to further illustrate the relative weights of the logistic
regressors in the MLR classiﬁcation, Fig. 3 shows the speciﬁc
regressors calculated for classes Corn-no till, Soybeans-min
till, and Woods, which are respectively dominated by Klinear,
hlinear, and hEMAP. Fig. 3(d) shows the regressors calculated
for class Soybeans-clean till, which has combined contributions
from all features. From Fig. 3, it is clear that the original
spectral information and the EMAP features are more relevant
than the other tested features. A ﬁnal observation resulting
from this experiment is that, given the high computational
complexity associated to using all the features hall, we can
obtain a suitable subset of features, including using only hlinear
and hEMAP, i.e., hsubset = [hlinear, hEMAP], which leads to a
comparable solution with very competitive computational cost.
In this case, the kernel transformations are not relevant for
improving classiﬁcation accuracies, and the combination of the
original (spectral and EMAP-based) features can lead to very
similar performance.
For illustrative purposes, Fig. 4 shows some of the obtained
classiﬁcation maps after applying the proposed framework to
the AVIRIS Indian Pines scene using approximately 30 training
samples per class. These maps correspond to one of the ten
Monte Carlo runs conducted for each considered type of feature. As we can observe in Fig. 4, the best classiﬁcation accuracies are obtained using hall, but the accuracies obtained using
hEMAP and KEMAP are also signiﬁcant. Finally, the accuracies
obtained using the original spectral information only (hlinear)
are low in comparison with that of the other approaches, while
the introduction of the kernel version Klinear improves the
obtained results but not to the levels achieved when EMAP
features are also used for the proposed framework. In turn,
EMAP-based features alone can lead to signiﬁcant accuracies
without the need for a kernel-based transformation.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
Classiﬁcation maps (along with the overall accuracies) obtained by the proposed framework for the AVIRIS Indian Pines data set, using approximately
30 training samples per class.
OVERALL, AVERAGE, AND INDIVIDUAL CLASS ACCURACIES [%] OBTAINED BY THE PROPOSED FRAMEWORK
(WITH DIFFERENT TYPES OF FEATURES) WHEN APPLIED TO THE ROSIS PAVIA UNIVERSITY HYPERSPECTRAL DATA SET
USING THE FIXED TRAINING SET IN FIG. 2(c). THE PROCESSING TIME (IN SECONDS) IS ALSO REPORTED IN EACH CASE
C. Experiments With ROSIS University of Pavia Data Set
1) Experiment 1: In our ﬁrst experiment with the ROSIS
Pavia University scene, we evaluate the classiﬁcation accuracies achieved by the proposed framework. In this experiment,
we consider, in addition to the features used in the previous
experiment, a subset given by hsubset = [hlinear, hEMAP] for
comparison. Table V shows the overall, average, and individual
classiﬁcation accuracies (in percentage) and the κ statistic
obtained by the proposed framework using different types of
input features. In all cases, we used the ﬁxed training set in
LI et al.: MULTIPLE FEATURE LEARNING FOR HYPERSPECTRAL IMAGE CLASSIFICATION
COMPARISON BETWEEN THE PROPOSED FRAMEWORK AND CK AND GCK USING THE ROSIS
PAVIA UNIVERSITY SCENE. THE PROCESSING TIME (IN SECONDS) IS ALSO REPORTED IN EACH CASE
STATISTICAL SIGNIFICANCE OF THE DIFFERENCES IN CLASSIFICATION ACCURACIES (MEASURED USING MCNEMAR’S TEST IN )
FOR THE PROPOSED FRAMEWORK, USING DIFFERENT TYPES OF FEATURES EXTRACTED FROM THE ROSIS PAVIA UNIVERSITY SCENE
Fig. 2(c) to train the classiﬁer. The EMAPs in this particular
experiment were built using threshold values in the range from
2.5% to 10% with respect to the mean of the individual features
and with a step of 2.5% for the deﬁnition of the criteria based
on the standard deviation attribute. Values of 100, 200, 500,
and 1000 were selected as references for the area attribute. The
threshold values considered for the area attribute were chosen
according to the resolution of the data and, thus, the size of the
objects present in the scene.
As shown by Table V, the classiﬁcation accuracies obtained
by the proposed framework are very high. Furthermore, as it
was already the case in the previous experiment, the results
using hsubset are comparable to those obtained using the full
set of features, hall. However, in the case of hsubset, the results
can be obtained with much less computational complexity when
compared to hall. This conﬁrms our introspection that, even
though our multiple learning framework can adequately exploit
all available features, a selection of the most relevant features
for classiﬁcation (in this case, the original spectral information
and the spatial characterization provided by EMAPs) can lead
to similar results but with less computational complexity. In this
experiment, as it was already the case in our experiment with
the AVIRIS Indian Pines scene, kernel transformations cannot
bring relevant additional information for classiﬁcation.
2) Experiment 2: In our second experiment, we provide a
comparison between the proposed framework with CK and
GCK . Table VI shows the obtained results, in which all the
experiments share exactly the same training and test sets. Similar observations can be reported for the ROSIS Pavia University
scene as the case already shown in the previous section with
the AVIRIS Indian Pines data, i.e., the proposed framework
with hall (which learns all the available linear and nonlinear features) obtained very competitive results with minimum
computational cost.
3) Experiment 3: Since the accuracy values obtained by
hEMAP, KEMAP, hall, and hsubset are apparently similar, in
our third experiment with the ROSIS Pavia University scene,
we analyze the statistical differences among all the considered
features using McNemar’s test . In this test, a value of
|Z| > 1.96 indicates that there is a signiﬁcant difference in
accuracy between two classiﬁcation methods. The sign of Z
is also a criterion to indicate whether a ﬁrst classiﬁer is more
accurate than a second one (Z > 0) or vice versa (Z < 0).
Table VII provides the results obtained for all the considered
types of features with the ROSIS Pavia University data set.
As it can be seen from Table VII, the performances of EMAP
features (hEMAP) and their kernel transformation (KEMAP)
are very similar in the statistical sense. Therefore, instead of
using KEMAP, we can simply resort to hEMAP, which provides
similar accuracies with lower computational cost. Furthermore,
it is noticeable that the performance of the original spectral information (hlinear) is signiﬁcantly different from that achieved
by the nonlinear transformations. As a result, this experiment
reveals that it is very important to combine both linear and nonlinear features for classiﬁcation. This is successfully achieved
by the presented method using all the features (hall) and a
carefully selected subset (hsubset), providing very competitive
results in the considered analysis scenario.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
Classiﬁcation maps (along with the overall accuracies) obtained by the proposed framework for the ROSIS Pavia University data set, using the ﬁxed
training set in Fig. 2(c).
For illustrative purposes, Fig. 5 shows some of the classiﬁcation maps obtained after applying the proposed framework to
the ROSIS Pavia University scene using the ﬁxed training set
depicted in Fig. 2(c). As we can observe in Fig. 5, a very good
delineation of complex urban structures can be observed in the
results obtained using any of the features including EMAPs,
such as hEMAP. Quite opposite, the accuracies obtained using
the original spectral information only (hlinear) are low in
comparison with that of the other approaches. In this particular
case, as it was already observed in the experiments with the
AVIRIS Indian Pines scene, the introduction of the kernel
version Klinear improves the obtained results, but not to the
levels observed when EMAP features are used in the proposed
framework.
D. Other Experiments
In this section, we conduct an evaluation of the proposed
approach using the ROSIS Pavia Centre and HYDICE Washington DC data sets. In the previously conducted experiments,
we observed that the proposed framework with hsubset (which
integrates both linear and nonlinear features) could obtain very
good performance with minimum computational cost. Therefore, in this section, we only evaluate the proposed framework
by using hsubset. Table VIII shows the obtained classiﬁcation
accuracies (as a function of the number of training samples)
for these two data sets in this particular case. From Table VIII,
it can be concluded that the proposed approach achieved
very good performance, even with very limited training sets.
Also, since the proposed approach does not require kernel
LI et al.: MULTIPLE FEATURE LEARNING FOR HYPERSPECTRAL IMAGE CLASSIFICATION
TABLE VIII
OA, AA, AND κ STATISTICS—PLUS/MINUS THE STANDARD DEVIATION—AS A FUNCTION OF THE NUMBER OF LABELED
SAMPLES PER CLASS (WITH THE TOTAL NUMBER OF LABELED SAMPLES IN PARENTHESES) OBTAINED BY THE
PROPOSED METHOD FOR THE ROSIS PAVIA CENTRE AND HYDICE WASHINGTON DC DATA SETS
transformations, it exhibits low computational cost. The low
standard deviation values reported in Table VIII also indicate
that the proposed framework is quite robust.
IV. CONCLUSION AND FUTURE RESEARCH LINES
In this paper, we have developed a new framework for
multiple feature learning which is based on the integration
of different types of (linear and nonlinear) features. A main
contribution of the presented approach is the joint consideration
of both linear and nonlinear features without any regularization
parameters to control the weight of each feature so that different
types of available features can be jointly exploited (in a collaborative and ﬂexible way) for hyperspectral image classiﬁcation.
Our main goal is to address a common situation in practice,
in which some classes may be separated using linearly derived
features while others may require nonlinearly derived features.
Until now, a main trend when using multiple feature learning
relies on the use of kernels, i.e., MKL. However, very few
techniques have been explored in order to adaptively select
the most useful type of feature for different classes in the
scene. In this work, we give a ﬁrst step in this direction and
contribute a framework which is ﬂexible and able to deal with
both linear and nonlinear class boundaries. A main innovation
of our proposed approach is that it is more ﬂexible than MKL, in
the sense that it can consider linear and nonlinear features and
not only kernel features. As a result, MKL can be considered
as a special case of the proposed framework. Although the
presented framework is general and suitable to incorporate
any kind of input features, in this work, we have considered
a set of highly representative features such as the original
(spectral) information contained in the scene, a set of (spatial)
morphological features extracted using different attributes, as
well as kernel-based transformations of the aforementioned
features. The framework therefore permits great ﬂexibility in
the exploitation of the advantages of each type of feature,
as well as the incorporation of additional features in future
developments.
Our experimental results, conducted with four widely used
hyperspectral scenes, indicate that spatial-based features are
very important for classiﬁcation, while there is no signiﬁcant
difference between the original (spectral- and spatial-based)
features and their kernel-based transformations. However, the
joint consideration of a pool of linear and nonlinear features
allowed us to approach the classiﬁcation problem in a way
that is more general and ﬂexible. In addition, our proposed
strategy allowed us to reduce the computational complexity of
the framework by selecting the most relevant features a priori,
although the proposed framework can naturally select the most
useful out of a large pool of input features for classiﬁcation,
without any requirement in terms of setting of the regularization
parameters or a priori information to control the weight of
each feature. It should also be noted that the classiﬁcation accuracies reported for the four considered hyperspectral scenes
rank among the most accurate ones ever reported for these
scenes. An important observation from our experiments is that,
under the proposed multiple feature learning framework, kernel
transformations may not be able to improve class separability
(in particular, for nonlinear features). Since, in this context,
kernel transformations increase computational complexity, our
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 53, NO. 3, MARCH 2015
proposed framework allows excluding such kernel features and
using the original features instead for speciﬁc applications.
As future work, we will conduct a more detailed investigation
of other possible (linear and nonlinear) features that can be
integrated in the proposed framework. Based on the observation that kernel-based features may not be as important as
other features in our presented framework, the computational
complexity can be further reduced by adaptively selecting the
most relevant features for classiﬁcation. We are also developing
parallel versions of the proposed framework in a variety of
architectures, such as commodity graphics processing units
(GPUs) or multi-GPU platforms.
ACKNOWLEDGMENT
The authors would like to thank Prof. D. Landgrebe for providing the Indian Pines and Washington DC Mall hyperspectral
data sets used in the experiments. Last but not least, the authors
would also like to thank the Associate Editor who handled the
manuscript and the anonymous reviewers for their outstanding
comments and suggestions, which greatly helped to improve the
technical quality and presentation of the manuscript.