ELI5: Long Form Question Answering
Angela Fan1,2 Yacine Jernite∗1 Ethan Perez∗3
David Grangier4 Jason Weston1 Michael Auli1
1Facebook AI Research
4Google AI ‡
[angelafan,yjernite,jase,michaelauli]@fb.com,
 , 
We introduce the ﬁrst large-scale corpus for
long-form question answering, a task requiring elaborate and in-depth answers to openended questions. The dataset comprises 270K
threads from the Reddit forum “Explain Like
I’m Five” (ELI5) where an online community
provides answers to questions which are comprehensible by ﬁve year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers.
provide a large set of web documents to help
answer the question. Automatic and human
evaluations show that an abstractive model
trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.
However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample
opportunity for future improvement.1
Introduction
Existing question answering datasets have enabled
signiﬁcant progress in models that provide extractive or unambigious short answers. However,
less attention has been paid to open-ended questions that require explanations. In this work, we
present ELI5: a Long Form Question Answering dataset that emphasizes the dual challenges of
isolating relevant information within long source
documents and generating paragraph-length explanations in response to complex, diverse questions (see illustrations in Figures 1 and 2).
The ﬁrst challenge of ELI5 is the length and diversity of answers that span multiple sentences:
∗Equal contribution
‡ Work done while at Facebook AI Research
1Dataset, Pretrained Models, and Additional Information
available:
 
github.io/ELI5,
 
facebookresearch/ELI5
Question: How do Jellyﬁsh function without brains or nervous systems? [...] (60 words)
Answer: Jellyﬁsh may not have a brain, but they have a rough
nervous system and innate behaviours. However, they are
very simple creatures. They’re invertebrate: creatures without a backbone. Most jellyﬁsh have really short life spans.
Sometimes just a couple of hours. [...] As their name implies, they are largely composed of basically jelly inside a
thin membrane. They’re over 95% water. (327 words)
Documents: [...]
Jellyﬁsh do not have brains, and most
barely have nervous systems. They have primitive nerve cells
that help them orient themselves in the water and sense light
and touch. [...] While they dont possess brains, the animals
still have neurons that send all sorts of signals throughout
their body. [...] They may accomplish this through the assistance of their nerve rings. Jellyﬁsh don’t have brains, and
that’s just where things begin. They don’t have many of the
body parts that are typical in other animals. [...] (1070 words)
Figure 1: ELI5 example. Models must write multi-sentence
answers given questions and supporting web documents.
questions are complex and cannot be easily addressed by a short response 
or by extracting a word or phrase from an evidence
document . Answers also
represent one of several valid ways of addressing
the query. Many state-of-the-art question answering models perform well compared to human performance for extractive answer selection . However, their
success does not directly carry over to our setting.
The second challenge is the length and diversity
of the content from knowledge sources required
to answer our questions. We leverage evidence
queried from the web for each question. In contrast to previous datasets where the human written
answer could be found with lexical overlap methods , ELI5 poses a signiﬁcant challenge in siphoning out important information, as no single sentence or phrase contains
the full answer.
While there are some datasets
that do require multi-sentence supporting knowlarXiv:1907.09190v1 [cs.CL] 22 Jul 2019
Figure 2: ELI5 questions by starting word, where box size represents frequency. Questions are open ended and diverse.
edge such as TriviaQA , their
answers are still short.
We benchmark the performance of several extractive, retrieval, and generative models. Evaluation of our task, and of multi-sentence text generation in general, is challenging. We draw upon several evaluation metrics that quantify performance
on intermediary ﬁll-in tasks that lead up to the full
answer generation. The overall answer generation
quality is measured with ROUGE and
various human evaluation studies.
We develop a strong abstractive baseline by
training a Seq2Seq model on multiple tasks over
the same data: language modeling, masked word
prediction and answer generation. We show this approach outperforms conventional Seq2Seq and language modeling, as well as
a strong extractive baseline based on BidAF but generalized to multi-sentence output. However, our best-performing model is still
far from the quality of human written answers,
with raters preferring the gold answers 86% of the
time. Further, we show that model performance
is strongly limited by the ability to comprehend
long multi-document input and generate long outputs to form a comprehensive answer, leaving this
challenge for future research.
Related Work
Various QA datasets have been proposed in
roughly two categories: extractive answers and
short abstractive answers (see Table 1).
Extractive
Extractive
 , SQuAD ,
NewsQA , SearchQA , and QuAC constrain the answer to a word or short phrase from
the input and evaluate using exact match or F1
with the ground truth span.
HotpotQA extends this approach by building
questions which challenge models to conduct
multi-hop reasoning across multiple paragraphs,
but the answer is still a short span.
the answer must be straightforward, as it needs
to be copied from the supporting evidence —
precluding most “how” or “why” type questions.
Abstractive QA
Abstractive datasets include
NarrativeQA , a dataset of
movie and book summaries and CoQA , a multi-domain dialogue dataset.
Both collect responses with crowdworkers and
ﬁnd that written answers are mostly extractive
and short. MS MARCO ,
a dataset of crowdsourced responses to Bing
queries, has written answers around 1 sentence
long with short input passages. TriviaQA contains longer multi-document web
input, collected using Bing and Wikipedia. As the
dataset is built from trivia, most questions can be
answered with a short extractive span.
Multi-document
summarization
task of writing a paragraph length response
from multiple supporting documents can be
seen as a form of query-based multi-document
summarization .
Summarization tasks such as DUC 20042 involve
long input and multi-sentence generation, but
contain much less training data compared to
WikiSum proposes
writing Wikipedia articles as a multi-document
summarization task. ELI5 requires more directed
2 
Average # of Words
1st Question Word Frequency (%)
Document(s)
# Q-A Pairs
857.6 (212K)
MS MARCO v2 
TriviaQA 
NarrativeQA 
CoQA 
SQuAD (2.0) 
HotpotQA 
Table 1: Comparing large-scale QA datasets. ELI5 has answers an order of magnitude longer and more open-ended questions.
text generation to answer a question, rather than
to write about a general topic. In addition, ELI5
contains a diverse set of questions which can
involve more than one Wikipedia concept.
Making a Long Form QA Dataset
Creating the Dataset from ELI5
There are several websites which provide forums
to ask open-ended questions such as Yahoo Answers, Quora, as well as numerous Reddit forums,
or subreddits. We focus on the subreddit Explain
Like I’m Five (ELI5) where users are encouraged
to provide answers which are comprehensible by a
ﬁve year old.3 ELI5 is appealing because answers
are supposed to be entirely self contained, and thus
rely less on pre-existing knowledge of the world
and use simpler language that is easier to model.
Questions and answers.
We select a set of questions and answers from the ELI5 forum up to July
2018 and then ﬁlter it based on how users rated
these pairs. First, we only retain questions which
have a score of at least two, that is two more ‘upvotes’ than ‘down-votes’. Second, there must be at
least one answer with a score of at least two. This
yields a ﬁnal number of 272K questions, and ensures that at least one person other than the author
has read the thread and deemed it appropriate. For
each thread, we select the answer with the highest voting score as the reference. Note that 63%
have one or more other valid answers by our upvote criteria, potentially doubling the size of the
available training data.
Preparing supporting information.
collect web sources for every question to provide relevant information that a system can draw
upon when generating an answer. Wikipedia has
been found effective for factoid-oriented questions
 . However,
3 
explainlikeimfive
early experiments in our setting showed it to be insufﬁcient to cover the wide range of topics present
in ELI5 and to address the open-ended nature of
the questions.
Instead, we use web data provided by Common Crawl.4 Speciﬁcally, we consider each of the individual pages in the July 2018
archive (roughly one per URL) as a single document. The data is tokenized with Spacy5 and we
select English documents with FastText language
identiﬁcation . Finally,
we index the data with Apache Lucene.6
Creating support documents.
We query the index for the 272K questions and gather the 100
most relevant web sources for each question, excluding Reddit. Each web source is the extracted
text of one page in Common Crawl. This leads to
supporting text for each question of a few hundred
thousand words. There is a good chance that the
supporting text contains the necessary information
to answer the question, but the sheer amount of
data is far beyond the scope of what many modern models can handle. We therefore ﬁlter the 100
web sources by selecting speciﬁc passages using
a simple heuristic: we split each web source into
sentences, ﬁnd sentences with the highest TFIDF
similarity with respect to the question, add some
local context for each of these, and concatenate
the result into a single support document, with
special tokens indicating non-contiguous passages
and document shifts. Each support document is
the result of this processing to concatenate relevant information from the web sources.
We ﬁnd that extracting 15 passages with a context of one sentence before and after the initial selection provides the best trade-off between support
document length and likelihood of containing relevant information, where relevance is measured as
the likelihood of containing a sentence which has
4 
5 
6 
% Correct Human Answers
% Correct Human Answers with Explanation
% Support Document contains Full Answer
% Support Document contains Relevant Info
Table 2: Annotated subset of ELI5 to assess answerability.
high ROUGE with the answer. We release all 100
Common Crawl IDs for each question and a script
to create the support document so future research
can use the support document or choose to further
investigate the information retrieval problem.
Finalizing the data set.
If the training data contains questions that are too similar to the validation and test data, a model may perform well on
these examples by memorizing related examples.
We prevent this by building the validation and test
set to contain questions that are sufﬁciently different from the training data. We compute the TFIDF
similarity between each pair of questions in the
entire dataset and sample the validation and test
set from the subset which has no close neighbor
by TFIDF score. The ﬁnal dataset contains 237K
train examples, 10K for valid, and 25K for test.
Dataset Analysis
Table 1 compares ELI5 to related datasets in terms
of the length of the question, support document,
answer, as well as statistics on the question types.
First, ELI5 questions are much longer than in
other datasets. This is because the initial question
is often followed by a clarifying paragraph detailing what aspect of the general theme should be
addressed or the question’s starting assumptions,
which need to be considered to answer well. To
get a rough idea of the different questions, we categorize them based on interrogative words. ELI5
focuses on open-ended queries which are less represented in other extractive or abstractive datasets.
Figure 2 shows examples of ELI5 questions split
by type and Appendix Figure 11 displays random
examples from the ELI5 training set. Interestingly,
even What questions tend to require paragraphlength explanations (What is the difference...).
Support documents contain 22-60 sentences or
on average 858 words, which puts ELI5 on the
higher end of published datasets for document
length. ELI5 contains long-form answers with an
average length of 6.6 sentences, or 130 words.
Next, we analyze a random subset of ELI5 to
assess the feasability of answering the questions
in the dataset. We judge if the question is answerable by reading each question, the gold answer,
and the support document we have created with
TF-IDF extraction. Note that questions can have
multiple parts and all parts of the question must
be answered. We sample 500 randomly questionanswer pairs from the training set and ﬁnd that
94.5% of gold answers fully address the question
(Table 2) based on the information in the support
document. Figure 12 in Appendix F displays examples of human answers that do not correctly answer the question. A small proportion of answers
are correct but do not explain the answer. On the
support document side, 65% of the support documents we construct provide the complete answer
to the question, and 92% of support documents
provide information relevant to the question.
Evaluation Methods
Evaluating long-form answers.
There are several aspects to quality: answers should be topical and accurate, ﬂuent, and coherent from start to
end. We judge the accuracy aspect by comparing
to the gold answer. ROUGE measures
similarity between a model output and one or several references, and is often used in summarization. While our task presents different challenges,
such as the diversity of possible answers to a question, we still ﬁnd the corpus-level metric to be useful to rank different related models (§6). We report
F1 for ROUGE-1, ROUGE-2, and ROUGE-L.
Abstractive
generative
models, perplexity (PPL) measures the ability to
predict the next word in a sequence given its context.
For a variant which focuses on semantically important words, we report FILL-1, the accuracy at which models generate different Nouns,
Verbs, and Adjectives given the correct preceding
tokens in the ﬁrst 2K examples of the test set. Finally, ROUGE-20% measures the model’s ability to complete an answer given the ﬁrst 80% of
the reference answer, the question, and the support document. Speciﬁcally, we generate a number of tokens corresponding to 20% of the average answer length in the validation set, and measure ROUGE between these and the last 20% of
the reference. We mentioned that there are several
valid ways to answer most questions. This measure abstracts away this variability and evaluates a
system’s ability to complete an answer.
Extractive: But none of these hypotheses make much sense for a jellyﬁsh because they do nt have brains at all. They just have
a nerve net — a loose ring of neurons that runs around the rim of their pulsating bells. They have an unusual nervous system,
because jellyﬁsh are not bilaterally symmetrical — that is, they dont have a left side and a right side. Jellyﬁsh don’t have brains,
but their nervous systems detect smells, light and other stimuli, and they coordinate their physical responses.
Language Model: They don’t have brains. They have a nervous system. They do not have a brain. They are not conscious.
They can not move. They move. Their nervous system is not a muscle. They use their muscles to move. The nervous system
does not have any nerves. They just have a nerve. They also have a muscle that is very similar to a muscle in that it is very
much like a muscle, but instead of a muscle it is a muscle and it is like a nerve, but it is not muscle. The nerve is like the muscle,
it is just a muscle but it has a nerve and it has nerves. The nerves are like the muscles, but they have nerves.
Multi-task Seq2Seq: Jellyﬁsh don’t have brains. Jellyﬁsh have a nervous system. They know how to swim, they don’t even
know what to do with their bodies. They have a “nervous system”, which is basically a bunch of nerve cells that connect to the
body. The nervous system is made up of neurons, neurons, nerves, etc. Each neuron doesn’t have a speciﬁc set of “neurons”,
it just has a different set of nerves. It has a set of sensory neurons, which are all connected to each other. It just knows that it
does react to the stimulus, and it knows that the stimulus is there. They have a bunch of nervous systems that act like a ﬁlter to
get information back.
Figure 3: Example answers from the extractive BidAF model, Question + Document + Answer language model, and Multi-task
Seq2Seq model for the question “How do Jellyﬁsh function without brains or nervous systems?” (cf. Figure 1).
Human evaluation.
We use crowdworkers to
conduct three assessments. First, evaluators rate
the ﬂuency of human and model generated answers
on a 5-point Likert Scale, from “very poorly written” to “easily readable” (500 evaluations). Second, evaluators are given question-answer pairs
and are asked if the answer is correct (500 evaluations) 7.
We also evaluated a smaller subset
ourselves while additionally looking at the support
documents (100 evaluations) to assess answer accuracy. Lastly, crowdworkers are given the question and answers from two models and asked to
decide which answer they prefer while considering readability and accuracy (1000 evaluations).
Each crowdworker assessment is made by 3 different evaluators. The same questions are used for
all models and must be at least 5 words long.
Extractive and Retrieval Models
ROUGE for a retrieval system that returns the
answer of the closest question in the training
Speciﬁcally, we perform a nearest neighbor search over the average word embeddings of the question using FAST-
TEXT . We also compute
an approximate oracle score for extractive systems
by using the reference answer to select similar sentences from the support document to maximize
ROUGE. Computing ROUGE between the reference and all sets of sentences from the source
7We experimented with a variant where crowdworkers
were allowed to select a third I don’t know option, but found
it was used only around 8% of the time.
is intractable. Instead, we perform a beam search
that adds sentences maximizing TFIDF with respect to the answer. The ﬁnal beam is re-ranked
using ROUGE with respect to the reference answer. We run this algorithm on our support document and on the full set of web sources for each
validation and test question, selecting up to 10 sentences with a beam of size 10.
Extractive models.
The ﬁrst baseline we explore simply returns the 7 sentences from the support document which have the highest TFIDF similarity with the question. We also evaluate models which score sentences from the support document based on the question and return the highest scoring sentences in their original order (the
number is tuned on the validation set to maximize
ROUGE). We train a model based on BidAF . We create an extractive training set
by ﬁnding the span of up to 5 contiguous sentences
in the support document which have the highest
ROUGE with respect to the reference answer, and
sub-sample other support document sentences so
that the ﬁnal training document is shorter than 400
words. We then train a BidAF model to predict the
extracted span in the sub-sampled support document based on the question. For test, we compute
the span score for each individual sentence, and
return the 5 with the highest score as it performed
best compared to returning 3 or 7 sentences.
Abstractive Models
Language and Seq2Seq models.
We train several models based on the Transformer architecture , both in its language
model and sequence-to-sequence (Seq2Seq) con-
Support Document
Nearest Neighbor
Extractive (TFIDF)
Extractive (BidAF)
Oracle support doc
Oracle web sources
LM Q + D + A
Seq2Seq Q to A
Seq2Seq Q + D to A
Seq2Seq Multi-task
Table 3: Comparison of oracles, baselines, retrieval, extractive, and abstractive models on the full proposed answers.
FILL-1 acc.
31.0 29.6 20.6
LM Q + D + A
30.9 28.9 19.9
S2S Q to A
21.7 23.0 15.5
33.6 11.5 29.5
S2S Q + D to A
27.6 26.3 19.4
32.7 10.7 28.6
S2S Multi-task
27.9 26.7 19.9
37.2 14.6 33.0
Table 4: Intermediary ﬁll-in tasks for sequential generation.
ﬁgurations. To investigate how much information
from the document the model uses, we train a language model on the concatenation of Question,
Support Document, and Answer (Q + D + A) as
well as on the Question and Answer (Q + A). Similarly, one Seq2Seq conﬁguration goes from Q to
A, and the other from Q + D to A. In all cases, Q,
D, and A are separated by special tokens.
Multi-task
trained to predict all tokens in the question,
web source, and answer. However, the standard
Seq2Seq model only receives training signal from
predicting the answer which is much less than
the language model gets. This can contribute to
learning poor quality representations compared
to language models.
To address this, we train
a multi-task Seq2Seq model:
during training,
we multi-task between several generation tasks,
including language modeling of Q + D + A by the
decoder and variations of source/target pairs (see
Appendix A). We add a masked word prediction
task where 15% of tokens in
the input are masked and must be recovered by the
model in the correct order, and append a marker
at the start of each sequence to indicate the task.
Data processing.
To reduce the vocabulary, we
apply byte-pair encoding 
to generate 40K codes which are applied to all
datasets. We model a vocabulary of 52,863 tokens for answer generation.
We use the Transformer implementation of fairseq-py and train with the big architecture following the details in . Given
our data length, we train with a large batch size by
delaying gradient updates until a sufﬁcient number
of examples have been seen .
Generation.
We generate from abstractive models using beam search with beam 5.
We disallow repeated trigrams to prevent repetition, a technique commonly used in multi-sentence summarization . For
the full answer generation task, we tune a minimum and maximum length for generation on the
valid set and apply these settings to the test set.
Overview of Model Performance
Full answer ROUGE.
Table 3 shows that the
nearest neighbor baseline performs similarly to
simply returning the support document which indicates that memorizing answers from the training set is insufﬁcient.
For extractive models,
the oracle provides an approximate upper bound
of 27.4 ROUGE-1.
The BidAF model is the
strongest (23.5), better than TFIDF between the
question and the support document to select sentences. However, these approaches are limited by
the support document, as an oracle computed on
the full web sources achieves 54.8.
Abstractive methods achieve higher ROUGE,
likely because they can adapt to the domain shift
between the web sources and the ELI5 subreddit.
In general, Seq2Seq models perform better than
language models and the various Seq2Seq settings
do not show large ROUGE differences. Figure 3
shows an example of generation for the language
model and the best Seq2Seq and extractive settings
(see Appendix F for additional random examples).
Perplexity and ﬁll-in tasks.
Tables 3 and 4
present metrics speciﬁc to sequential generation
models: perplexity of the answer, accuracy of
the model’s FILL-1 word prediction for Nouns,
Verbs, and Adjectives, and ROUGE of the conditional generation of the last 20% answer words.
The language model perplexity is much lower than
that of the standard Seq2Seq setting – this is likely
linked to the number of output tokens the system
Figure 4: Human evaluation of answer ﬂuency and accuracy — with and without access to supporting evidence documents
Figure 5: Human preferences for pairwise comparisons. The
better model’s % preference is bolded. * indicates statistical
signiﬁcance.
is required to predict at training time. The multitask Seq2Seq experiment, in which the Seq2Seq
decoder is trained to predict the question and the
document, in addition to the answer, can reach the
same perplexity as the language model. ROUGE-
20% shows a much starker contrast between language modeling and Seq2Seq, as well as between
standard Seq2Seq and multi-task training. The latter achieves strong performance of 37.2 ROUGE-
1. However, both versions of the language model
are still better at FILL-1. These results suggest
that the Seq2Seq model is better than the language
model in maintaining coherence and that Seq2Seq
relies on information over many time steps.
Human evaluation.
Human answers are rated
highest in terms of ﬂuency (Figure 4, left). The extractive model outputs human-written text which
is likely ﬂuent but with the failure mode of concatenating unrelated sentences.
The multi-task
model performs similarly to the extractive model
which indicates that abstractive methods can generate coherent answers. The language model and
standard Seq2Seq trail behind.
To get a sense of the stability of our results, we
analyzed the standard deviation of three independent ﬂuency trials conducted on separate days and
we ﬁnd low variation (Appendix E, Figure 10).
We also measure agreement between crowdworkers in selecting positive (scores 4 and 5), negative
(1 and 2), or neutral (3) choices on the 5-point
Likert scale, and ﬁnd that 2 crowdworkers agree
almost 100% of the time (Appendix E, Figure 10).
In answer accuracy (Figure 4, middle), there is
a large gap between human performance and all
models. The language model is almost never accurate, while the extractive model is slightly more so
than the multi-task model. Crowdworkers assessing accuracy do not have the support document.
We evaluate accuracy ourselves with the support
document in Figure 4, right. Similar to crowdworkers, we ﬁnd 40% of extractive answers to be
accurate. We ﬁnd only 19% of multi-task model
answers are fully accurate; even if the model output answers the question, it can generate a sentence with an incorrect statement. In contrast, the
extractive model copies sentences from humanwritten text. However, the multi-task model is better at generating relevant answers (84% relevancy
compared to 68% for extractive), as the extractive
model is constrained by the support document.
Figure 5 presents pairwise preference judgments of human annotators shown answers from
two of the ﬁve systems. The reference answer is
preferred over the output of all of our trained models in at least 85.5% of cases, indicating there is
substantial room for improvement. The multi-task
abstractive setting comes next, closely followed by
the extractive (multi-task is only preferred in 57%
of comparisons), then the standard Seq2Seq and
ﬁnally the language model, considered worse than
any other setting in at least 91% of cases.
We use a two-tailed binomial test to test statistical signiﬁcance of the pairwise judgments and it
shows that all judgments are statistically signiﬁcant at p < 0.05.
Quantitative and Qualitative Analysis
Discussion
present a number of metrics which provide insight
into various model behaviors.
We recommend
Figure 6: Attention over the question and supporting evidence for the Multi-task Seq2Seq model and Question + Document +
Answer language model. Attention is shown for the ﬁrst word of answer generation.
future work to report full ROUGE and ROUGE-
Perplexity and FILL-1 focus on local
prediction and are poor indicators of overall
appropriateness for the full task.
Full answer
ROUGE discriminates reasonably well between
models with the same general architecture, but
cannot rate an abstractive system against an
extractive one.
The ROUGE-20% measure
abstracts away some variability and focuses on
coherence between the beginning and end of
an answer.
This metric correlates with human
judgments of quality but can only be reported for
sequential generation.
extractive,
Language models perform better than
Seq2Seq in terms of perplexity and FILL-1, while
being signiﬁcantly worse at ROUGE-20% and
human evaluations. To investigate this, we visualize the attention mechanism at the start of answer generation in Figure 6.
The attention of
the language model is strongly focused on nearby
context when generating the ﬁrst word of the answer, whereas the multi-task Seq2Seq model attends more evenly to relevant information in the
question and the document. This validates our assumption that the language model’s focus on local
context is insufﬁcient for high quality answers.
In Figure 7 (left), we further investigate how the
relevance and quality of the support document extraction step affects the answers provided by the
extractive and abstractive setting. The ROUGE
score is displayed for data subsets, partitioned by
percentile of word overlap of the answer with the
support document (e.g. how many answer words
While both models perform better for
documents with higher ROUGE overlap between
support document and human answer, the abstractive setting is much better at compensating for
when the support document has lower relevance.
Data size and initial selection.
There is a large
difference between the extractive oracle ROUGE
using our support document and the oracle on full
Figure 7: (left) Model score by document-answer similarity.
(right) Seq2Seq multi-task score by amount of training data.
Figure 8: (left) TFIDF rank of source passage for oracle sentences. (right) Highest rank used per question.
web sources. This suggests that the initial selection of our support document severely limits access to relevant information. To assess the impact
of support document size, we re-run the selection
step for 1000 examples to extract 500 passages instead of 20, and run the oracle on these new inputs.
Figure 8 shows the TFIDF rank of the passages
from which sentences are selected. While slightly
more sentences are extracted from the higher ranking passages, less than 9% come from the ﬁrst 20,
and most oracles have at least one sentence from
the last 100. For a model to perform best, it would
have to handle inputs tens of thousands of words
long. In Table 3, we show an oracle computed
on the full web sources has much higher ROUGE
than an oracle computed on the support document.
We analyze the impact of data size on performance in Figure 7. We train the multi-task model
on 25%, 50%, and 75%, and the all of the data
to compare performance. ROUGE increases as a
function of the data used and even though ELI5 is
one of the larger QA datasets (§3), this shows that
collecting more still helps. While we only used
one reference answer per question here, recall that
over half of them have multiple answers, which
could be leveraged to train better models.
Combining challenges.
Our task blends the
inter-dependent challenges of retrieving information, reasoning, and writing long outputs. Studying each of these aspects in context is particularly
important. For example, we show that the abstractive model’s ability to compensate for a (realistically) imperfect support document is essential to
its relative success over extractive methods. The
ﬂuency gap between the reference and the extractive system in human evaluation also suggests that
the latter may require sequential decision capabilities. This kind of decision making is necessary to
address the dual challenges of reasoning over several supporting facts and generating long coherent
outputs. We see our task’s need to combine complementary systems as critical to gaining insights
into their individual behaviors.
Conclusion
We introduce the ﬁrst large-scale long form question answering dataset of open-ended queries with
explanatory multi-sentence answers.
that abstractive models generate coherent answers
and are competitive with extractive models in human evaluation.
Proposed models are far from
human performance, in part due to the inability
to exploit the long full web text. We hope ELI5
will inspire future work in all aspects of long-form
QA, from the information extraction problem of
obtaining information from long, multi-document
input to generating more coherent and accurate
paragraph-length answers.