Large-Scale Live Active Learning:
Training Object Detectors with Crawled Data and Crowds
Sudheendra Vijayanarasimhan and Kristen Grauman
University of Texas at Austin
{svnaras,grauman}@cs.utexas.edu
Active learning and crowdsourcing are promising ways
to efﬁciently build up training sets for object recognition,
but thus far techniques are tested in artiﬁcially controlled
settings. Typically the vision researcher has already determined the dataset’s scope, the labels “actively” obtained
are in fact already known, and/or the crowd-sourced collection process is iteratively ﬁne-tuned. We present an approach for live learning of object detectors, in which the
system autonomously reﬁnes its models by actively requesting crowd-sourced annotations on images crawled from the
To address the technical issues such a large-scale
system entails, we introduce a novel part-based detector
amenable to linear classiﬁers, and show how to identify its
most uncertain instances in sub-linear time with a hashingbased solution. We demonstrate the approach with experiments of unprecedented scale and autonomy, and show it
successfully improves the state-of-the-art for the most challenging objects in the PASCAL benchmark. In addition, we
show our detector competes well with popular nonlinear
classiﬁers that are much more expensive to train.
1. Introduction
Object detection is a fundamental vision problem: given
an image, which object categories are present, and where?
Ongoing research is devoted to developing novel representations and classiﬁcation algorithms in support of this task,
and challenge datasets encourage further progress . Today’s best-performing detection methods employ
discriminative learning together with window-based search,
and assume that a large number of cleanly labeled training
examples are available. For example, thousands of bounding box annotations per category is standard.
Given the substantial human effort required to gather
good training sets—as well as the expectation that more
data is almost always advantageous—researchers have begun to explore novel ways to collect labeled data. Both
active learning and crowd-sourced labeling are promising
ways to efﬁciently build up training sets for object recognition. Active learning work shows how to minimize human effort by focusing label requests on those that appear most informative to the classiﬁer ,
whereas crowd-sourcing work explores how to package annotation tasks such that they can be dispersed effectively
online . The interesting questions raised
in these areas—such as dealing with noisy labels, measuring reliability, mixing strong and weak annotations—make
it clear that data collection is no longer a mundane necessity, but a thriving research area in itself.
However, while ostensibly intended to distance algorithm developers from the data collection process, in practice existing techniques are tested in artiﬁcially controlled
settings. Speciﬁcally, we see four limiting factors. First,
previous work uses “sandbox” datasets, where the vision
researcher has already determined the dataset’s source and
scope, meaning there is a ﬁxed (and possibly biased) set of
images that will even be considered for labeling. In fact,
active learning methods have only been tested on sandbox
data where the true labels are really known, and merely
temporarily withheld from the selection algorithm. These
common simulations likely inﬂate the performance of both
active and passive learners, since anything chosen for labeling is relevant. Second, nearly all work targets the active
image classiﬁcation problem—not detection—and so images in the unlabeled pool are artiﬁcially assumed to contain
only one prominent object. Third, most crowd-sourced collection processes require iterative ﬁne-tuning by the algorithm designer (e.g., revising task requirements, pruning responses, barring unreliable Mechanical Turkers) before the
data is in usable form. Fourth, the computational complexity of the active selection process is generally ignored, yet it
is critical when running a live system to avoid keeping the
human annotators idle. Thus, it is unknown to what extent
current approaches could translate to real settings.
Our goal is to take crowd-sourced active annotation out
of the “sandbox”. We present an approach for live learning of object detectors, in which the system directly inter-
acts with human annotators online and iteratively poses annotation requests to reﬁne its models. Rather than ﬁll the
data pool with some canned dataset, the system itself gathers possibly relevant images via keyword search (we use
Flickr). It repeatedly surveys the data to identify unlabeled
sub-windows that are most uncertain according to the current model, and generates tasks on MTurk to get the corresponding bounding box annotations. After an annotation
budget is spent, we evaluate the resulting detectors both on
benchmark data, as well as a novel test set from Flickr. Notably, throughout the procedure we do not intervene with
what goes into the system’s data pool, nor the annotation
quality from the hundreds of online annotators.
To make the above a reality requires handling some important technical issues. Active selection for window-based
detection is particularly challenging since the object extents
(bounding boxes) are unknown in the unlabeled examples;
naively one would need to evaluate all possible windows
within the image in order to choose the most uncertain. This
very quickly leads to a prohibitively large unlabeled pool
to evaluate exhaustively. Thus, we introduce a novel partbased detector amenable to linear classiﬁers, and show how
to identify its most uncertain instances in sub-linear time
with a hashing-based solution we recently developed.
We show that our detector strikes a good balance between speed and accuracy, with results competitive with and
even exceeding the state-of-the-art on the PASCAL VOC.
Most importantly, we show successful live learning in an
uncontrolled setting. The system learns accurate detectors
with much less human effort than strong baselines that rely
on human-veriﬁed keyword search results.
2. Related Work
Object detection has received various treatments in the
literature; see and references therein for an overview.
Currently window-based approaches based on gradient features and subwindow parts provide state-of-the-art results
using discriminative classiﬁers. A known limitation, however, is their fairly signiﬁcant computational expense, due
both to the need to search exhaustively through all windows
in the image, as well as the classiﬁers’ complexity (e.g.,
SVMs with nonlinear kernels or latent variables ).
Various ways to reduce detection time have been
branch-and-bound
search , or jumping windows . To reduce classiﬁer
training and testing costs, simpler linear models are
appealing.
While linear models tend to underperform
with common representations (e.g., see tests in ),
recent work in image classiﬁcation shows very good
results when also incorporating sparse coding and feature
pooling . We propose a part-based object model
that exploits a related representation, and show it to be
competitive with state-of-the-art detection results. To our
knowledge, no previous work considers sparse coding and
linear models for object detection.
Active learning has been shown to better focus annotation effort for image recognition tasks and region
labeling . However, no previous work uses active
learning to train a window-based detector. To do so introduces major scalability issues, which we address with a new
linear detector combined with a hashing algorithm for
sub-linear time search of the unlabeled pool. Further, all
previous work tests active selection only in a sandbox.
Researchers have investigated issues in annotation
tools and large-scale database collection for recognition.
Keyword-based search is often used for dataset creation,
and several recent efforts integrate crowd-sourced labeling or online and incremental learning .
Even with a human in the loop, annotation precision varies
when using Web interfaces and crowds, and so some research explores ways to automatically provide quality assurance . Other work attempts to directly learn object
models from noisy keyword search (e.g., ); however,
such methods assume a single prominent object of interest
per image, whereas for detection we will have cluttered candidate images that require a bounding box to identify the
object of interest.
Overall, previous active learning methods focus on image classiﬁcation, and/or demonstrate results under controlled settings on prepared datasets of modest scale. Ours
is the ﬁrst complete end-to-end approach for scalable, automatic online learning of object detectors.
3. Approach
Our goal is to enable online active crowd-sourced object
detector training. Given the name of a class of interest, our
system produces a detector to localize novel instances using
automatically obtained images and annotations. To make
this feasible, we ﬁrst propose a part-based linear SVM detector, and then show how to identify its uncertain examples
efﬁciently using a hashing scheme.
3.1. Object Representation and Linear Classiﬁer
We ﬁrst introduce our part-based object model. Our goal
is to design the representation such that a simple linear classiﬁer will be adequate for robust detection. A linear model
has many complexity advantages important to our setting:
i) SVM training requires time linear in the number of training examples, rather than cubic , ii) classiﬁcation of
novel instances requires constant time rather than growing
linearly with the number of training examples, iii) exact incremental classiﬁer updates are possible, which makes an iterative active learning loop practical, and iv) hash functions
enable sub-linear time search to map a query hyperplane to
its nearest points according to a linear kernel .
Sparse Max Pooling
[ (r), (p1) … (pP) , (c1) … (cC)]
Figure 1. Our part-based object representation.
Inspired by recent ﬁndings in sparse coding for image
classiﬁcation , we explore a detection model
based on sparse coding of local features combined with a
max pooling scheme. Previous representations pool coded
SIFT features in a single global window or in a ﬁxed classindependent hierarchy of grid cells (i.e., a spatial pyramid
structure). While sufﬁcient for whole-image classiﬁcation,
we instead aim to represent an object separately from its
context, and to exploit its part-based structure with classdependent subwindow pooling.
To this end,
we propose an object model consisting of a root window r,
multiple part windows
{p1, . . . , pP } that overlap the root, and context windows
{c1, . . . , cC} surrounding it.
See Figure 1.
[r, p1, . . . , pP , c1, . . . , cC] denote a candidate object conﬁguration within an image, and let φ(W) denote the sparse
feature encoding for local image descriptors extracted from
a given subwindow W (to be deﬁned below). The detector
scores a candidate conﬁguration as a simple linear sum:
wpiφ(pi) +
where w denotes the learned classiﬁer weights, which we
obtain with SVM training. We next ﬂesh out the window descriptions; Sec. 3.2 explains how we obtain candidate root
placements.
Window descriptions
Given a novel test image, we ﬁrst
extract local image descriptors; we use a dense multi-scale
sampling of SIFT in our implementation. Each window
type (r, pi, or cj) uses these features to create its encoding
φ(·). The root window provides a global summary of the
object appearance, and is invariant to spatial translations of
features within it.
Similarly, each part window summarizes the local features within it, discarding their positions; however, the location of each part is deﬁned relative to the current root,
and depends on the object class under consideration (i.e.,
bicycles and cars each have a different conﬁguration of the
pi windows). Thus, they capture the locations and spatial
conﬁgurations of the most important parts of the object.
We train with the part locations and bounds learned by the
detector in on an initial labeled set; alternatively, they
could be requested once directly from annotators.
The context windows incorporate contextual cues surrounding the object, such as the presence of “sky”,
“ground”, “road”, etc., and also help discard poorer candidate windows that cover only parts of objects (in which case
object features spill into the context window). We create the
context windows using a 3 × 1 partition of r’s complement,
as shown in the top right of Figure 1.
Feature encoding
Each window is represented using a
nonlinear feature encoding based on sparse coding and
max-pooling, which we refer to as Sparse Max Pooling
(SMP). The SMP representation is related to the wellknown bag-of-features (BoF); however, unlike BoF, each
component local descriptor is ﬁrst encoded as a combination of multiple visual words, and the weights are then
pooled within a region of interest using the max function.
Ofﬂine, we cluster a large corpus of randomly selected
features to obtain a dictionary of |V | visual words: V =
[v1, . . . , v|V |], where each column vi ∈ℜ128 is a cluster center in SIFT space.
For any window W (whether
root/part/context), let F = {fi}|F |
i=1 be the set of local features falling within it, where each fi ∈ℜ128 is a SIFT
descriptor. We represent this window with a sparse |V |dimensional vector, as follows.
First, each feature fi is quantized into a |V |-dimensional
sparse vector si that approximates fi using some existing
sparse coding algorithm and the dictionary V , that is, fi ≈
siV . Taking this encoding for every fi as input, the SMP
representation of W is given by:
[ φ1, . . . , φ|V | ], where
max (si(j)) , i = 1, . . . , |F|,
for j = 1, . . . , |V |, and si(j) is the j-th dimension of the
sparse vector encoding the i-th original feature, fi. Finally,
we normalize φ(W) by its L2 norm.1
The rationale behind the SMP window encoding is
twofold: the sparse coding gives a fuller representation of
the original features by reﬂecting nearness to multiple dictionary elements (as opposed to BoF’s usual hard vector
quantization), while the max pooling gives better discriminability amidst high-variance clutter .
See 
for useful comparisons between various sparse coding approaches, which shows their clear advantage when combined with a linear kernel as compared to the popular BoF.
Relationship to existing detection models
Our model intentionally strikes a balance between two recent state-of-
1We use Locality-constrained Linear Coding (LLC) to obtain the
sparse coding, though other algorithms could also be used for this step.
+avg pooling
local features,
discard locs per window
(a) Spatial pyramid model (SP)
deformations
dense gradients at fixed locs within window
(b) Latent deformable part model (LSVM)
Sparse code
+max pooling
(p1) … (pP)
local features,
discard locs per window
(c) Proposed model
Figure 2. Sketch to illustrate contrasts with related existing models. See text for details.
the-art detection models: i) a nonlinear SVM with a spatial
pyramid (SP) in which each grid cell is a histogram of unordered visual words , and ii) a latent SVM (LSVM) with
a root+deformable part model in which each part is a rigid
histogram of ordered oriented gradients . See Figure 2.
On the one hand, the SP model is robust to spatial translations of local features within each grid cell.
other hand, its nonlinear kernels (required for good performance ) makes the classiﬁer quite expensive to train and
test, and rigid class-independent bins may fail to capture
the structure of the best parts on an object (see Fig. 2(a)). In
contrast, the LSVM model can robustly capture the parts,
since it learns multiple part ﬁlters that deform relative to the
root. However, its dynamic programming step to compute
parts’ alignment makes it expensive to train. Furthermore,
its use of the spatially dense gradient histograms for both
the root and parts make them less tolerant to internal shifts
and rotations (see Fig. 2(b)).
Our model attempts to incorporate positive aspects of the
above two models, while maintaining a much lower computational cost. In particular, we have class-speciﬁc part con-
ﬁgurations, like , but they are ﬁxed relative to the root,
like . Our SMP-based encoding is robust to shifts within
the part and object windows, thereby tolerating some deformation to the exact part placement without needing the
additional DP alignment step during detection. In short,
by utilizing a part-based representation and a linear classiﬁer, our approach provides a very good trade-off in terms
of model complexity and accuracy.
3.2. Generating Candidate Root Windows
So far we have deﬁned a representation and scoring function for any candidate window. Now we discuss how to
generate the candidates, whether in novel test images or unlabeled images the system is considering for annotation. A
thorough but prohibitively expensive method would be the
standard sliding window approach; instead, we use a gridbased variant of the jumping window method of .
The jumping window approach generates candidate windows with a Hough-like projection using visual word
matches, and prioritizes these candidates according to a
measure of how discriminative a given word and coarse location is for the object class (see Figure 3). First, each root
window in the training images is divided into an N × M
Let Wloc(r) denote a root window’s position and
Figure 3. Illustration of jumping window root candidates. Grid cells serve
to reﬁne the priority given to each box (but do not affect its placement).
Here, location g = 1 has higher priority than g = 4 for visual word v = ⋆
since it appears more consistently in the training images (left two images).
scale. Given a training window r and a visual word v occurring at grid position g ∈{1, . . . , NM}, we record the
triplet (v, g, Wloc(r)). We build a lookup table indexed by
the v entries for all training examples. Then, given a test
image, for each occurrence of a visual word v, we use the
lookup table to retrieve all possible Wloc(r)’s, and project a
bounding box in the test image relative to that v’s position.
Note, candidates can vary in aspect ratio and scale.
The grid cell g in each triple is used to assign a priority
score to each candidate, since we may not want to examine all possible candidates mapped from the lookup table.
Speciﬁcally, we score a given pair (v, g) based on how predictive it is for the true object bounding box across the training set: P(v, g) is the fraction of the occurrences of word v
that appear at grid location g. This function gives a higher
score to bounding boxes where the visual word occurs consistently across positive training examples at a particular position (see Figure 3).
Given a test image, we take the top K candidate jumping
windows based on their priority scores. The detector is run
only on these boxes. In experiments, we obtain 95% recall
on most categories when taking just K = 3, 000 candidates
per test image. The same level of recall would require up to
105 bounding boxes if using sliding windows (see ).
3.3. Active Selection of Object Windows
We initialize our online active learning system with a
linear SVM trained with a small number of labeled examples for the object. Then, it crawls for a pool of potentially
relevant unlabeled data using keyword search with the object name (i.e., it downloads a set of images tagged ‘dog’
when learning to detect dogs). We want to efﬁciently determine which images among those retrieved should be labeled next by the human annotators. As an active learning
criterion, we use the “simple margin” selection method for
SVMs , a widely used criterion that seeks points that
most reduce the version space. Given an SVM with hyperplane normal w and an unlabeled pool of data UO =
{φ(O1), . . . , φ(On}), the point that minimizes the distance
to the current decision boundary is selected for labeling:
O∗= arg minOi∈UO |wT φ(Oi)|.
A naive application of this criterion entails computing
the classiﬁer response on all unlabeled data, ranking them
by |wT φ(Oi)|. However, even with a linear classiﬁer, exhaustively evaluating all unlabeled examples at each iteration is prohibitively expensive. Whereas previous active
learning work is generally unconcerned about the amount of
time it actually takes to compute the next labeling request,
it becomes a real issue when working out of the sandbox,
since we have live annotators awaiting the next labeling jobs
and massive unlabeled data pools. In particular, since we
need to apply the active selection function at the level of
the object, not the entire image, we have an enormous number of instances—all bounding boxes within the unlabeled
image data. Even using jumping windows, thousands of images yield millions of candidates. Thus, a simple linear scan
of all unlabeled data is infeasible.
Therefore, we adopt our hyperplane-hashing algorithm to identify the most promising candidate windows in sub-linear time. The algorithm maps inputs to binary keys using a randomized hash function that is localitysensitive for the angle between the hyperplane normal and
a database point. Given a “query hyperplane”, one can hash
directly to those points that are nearest to the hyperplane,
with high probability.
Formally, let UI denote the set of unlabeled images,
and UO denote the pool of candidate object windows obtained using the jumping window predictor on UI. Note
that |UO| = K × |UI|. The locality-sensitive hash family H
generates randomized functions with two-bit outputs:
hu,v(φ(Oi), φ(Oi)),
if z is a database vector,
hu,v(w, −w),
if z is a query hyperplane,
where the component function is deﬁned as
hu,v(a, b) = [sign(uT a), sign(vT b)],
sign(uT a) returns 1 if uT a ≥0, and 0 otherwise, and u
and v are sampled from a standard multivariate Gaussian,
u, v ∼N(0, I). These functions guarantee high probability of collision for a query hyperplane and the points nearest
to its boundary. The two-bit hash limits the retrieved points’
deviation from the perpendicular by constraining the angle
with respect to both w and −w. See for details.
We use these functions to hash the crawled data into the
table.2 Then, at each iteration of the active learning loop, we
2Hyperplane hashes can be used with existing approximate nearneighbor search algorithms; we use Charikar’s formulation, which guarantees the probability with which the nearest neighbor will be returned.
Figure 4. MTurk interface to obtain bboxes on actively selected examples.
hash the current classiﬁer as a query, and directly retrieve
examples closest to its decision boundary. We search only
those examples, i.e., we compute |wT φ(Oi)| = |f(Oi)| for
each one, and rank them in order of increasing value. Finally, the system issues a label request for the top T images
under this ranking. Since we only need to evaluate the classiﬁer for examples that fall into a particular hash bucket—
typically less than 0.1% of the total number of unlabeled
examples—this strategy combined with our new detector
makes online selection from large datasets feasible.
3.4. Online Annotation Requests
To automatically obtain annotations on the actively selected examples, our system posts jobs on Mechanical Turk,
where it can pay workers to provide labels. The system
gathers the images containing the most uncertain bounding
boxes, and the annotators are instructed to use a rectangledrawing tool to outline the object of interest with a bounding box (or else to report that none is present). We ask annotators to further subdivide instances into “normal”, “truncated”, or “unusual”, consistent with PASCAL annotations,
and to ﬂag images containing more than 3 instances. Figure 4 shows the annotation interface.
While MTurk provides easy access to a large number of
annotators, the quality of their labels varies. Thus, we design a simple but effective approach to account for the variability. We issue each request to 10 unique annotators, and
then cluster their bounding boxes using mean shift to obtain
a consensus. We keep only those clusters with boxes from
more than half of the annotators. Finally, we obtain a single
representative box from each cluster by selecting the one
with the largest mean overlap with the rest.
Note how each image consists of thousands of unlabeled window instances, each of which serves as a candidate
query; once a single image annotation is obtained, however,
it tells us the labels for all windows within it.
3.5. Training the Detector
Training our detector entails learning the linear SVM
weights in Eqn. 2 to distinguish windows that contain the
object of interest from all others. To limit the number of
negative windows used to train, we mine for “hard” negatives: at each iteration, we apply the updated classiﬁer to the
newly labeled images, and add the 10 top-scoring windows
as negatives if they overlap the target class by < 20%.
cands aero. bicyc. bird boat bottl bus
cat chair cow dinin. dog horse motor. person potte. sheep sofa train tvmon. Mean
48.3 14.1 13.6 15.3 43.9 49.0 30.7 11.6 30.3 13.3 21.8 43.6
28.8 33.0 47.7
3.5 10.8 35.8 45.0 17.7 11.5 24.6
19.1 14.7 35.7
6.3 16.5 45.6 49.8 26.7 12.5 27.3
23.2 22.6 41.3
LSVM+HOG nonlinear yes
slide 32.8
2.5 16.8 28.5 39.7 51.6 21.3 17.9 18.5 25.9
16.2 24.4 39.2
SP+MKL 
multiple jump 37.6
47.8 15.3 15.3 21.9 50.7 50.6 30.0 17.3 33.0 22.5 21.5 51.2
23.9 28.5 45.3
Table 1. Average precision compared to a spatial pyramid BoF baseline (BoF SP), a sparse coding max pooling spatial pyramid baseline modeled after 
(LLC SP), and two state-of-the-art approaches on the PASCAL VOC, where all methods are trained and tested on the standard benchmark splits.
(Mean shift)
Annotated data
hyperplane
Hash table
prediction
Figure 5. Summary of our system for live learning of object detectors.
We can now actively train an object detector automatically using minimal crowd-sourced human effort. To recap,
the main loop consists of using the current classiﬁer to generate candidate jumping windows, storing all candidates in
a hash table, querying the hash table using the hyperplane
classiﬁer, giving the actively selected examples to online
annotators, taking their responses as new ground truth labeled data, and updating the classiﬁer. See Figure 5.
4. Results
The goal of our experiments is three-fold. First, we compare the proposed detector to the most closely related stateof-the-art techniques. Second, we validate our large-scale
active selection approach with benchmark data. Third, we
deploy our complete live learning system with crawled images, and compare to strong baselines that request labels for
the keyword search images in a random sequence. We use
two datasets: the PASCAL VOC 2007, and a new Flickr
dataset (details below).
Implementation details
We use dense SIFT at three
scales (16, 24, 32 pixels) with grid spacing of 4 pixels, for
30K features per image. We obtain |V | = 56, 894 visual
words with two levels of hierarchical k-means on a sample of training images. We use the fast linear SVM code
svm perf , C = 100. We use the LLC code ,
and set k, the number of non-zero values in the sparse vector si to 5, following . We use P = 6 parts per object from each of a 2-mixture detector from trained on
PASCAL data, take T = 100 instances per active cycle,
set K = 3000, and N, M = 4. We ﬁx N ρ = 500 and
ǫ′ = 0.01 for the hash table . During detection we run
non-max suppression on top ranked boxes and select 10 per
image. We score all results with standard PASCAL metrics
and train/test splits.
4.1. Comparison to State-of-the-Art Detectors
First we compare our detector to the algorithms with the
current best performance on VOC 2007 benchmark of 20
objects, as well as our own implementation of two other
relevant baselines.
Table 1 shows the results. The ﬁrst three rows all use the
same original SIFT features, a linear SVM classiﬁer, and
the same jumping windows in the test images. They differ,
however, in the feature coding and pooling. The BoF SP
baseline maps the local features to a standard 3-level spatial
pyramid bag-of-words descriptor with L2-normalization.
The LLC SP baseline applies sparse coding and max pooling within the spatial pyramid cells. LLC SP is the method
of ; note, however, we are applying it for detection,
whereas the authors propose their approach for image classiﬁcation.
The linear classiﬁer with standard BoF coding is the
weakest. The LLC SP baseline performs quite well in comparison, but its restriction to a global spatial pyramid structure appears to hinder accuracy. In contrast, our detector
improves over LLC SP noticeably for most objects (compare rows 1 and 3), likely due to its part windows.
Our detector is competitive with both of the state-ofthe-art approaches discussed in Section 3.1: SP+MKL ,
which uses a cascade of classiﬁers that culminates with a
learned combination of nonlinear SVM kernels over multiple feature types, and LSVM+HOG , which uses the
latent SVM and deformation models for parts. In fact, our
detector outperforms all existing results for 6 of the 20 objects, improving the state-of-the-art. At the same time, it is
signiﬁcantly faster to train (about 50 to 600 times faster; see
The classes where we see most improvements seem
to make sense, too: our approach outperforms the rigid
spatial pyramid representation used in for cases with
more class-speciﬁc part structure (aeroplane, bicycle, train),
while it outperforms the dense gradient parts used in for
the more deformable objects (dog, cat, cow).
Annotations added, out of 4.5 million examples
Average Precision
Active (Ours)
SP−MKL 
Figure 6. Active detector training on PASCAL. Our large-scale active selection yields steeper learning curves than passive selection, and reaches
peak state-of-the-art performance using only ∼30% of the data.
aeroplane bird
dog sheep sofa train
15.8∗18.9∗30.7 25.3∗28.8 33.0 47.7
Previous best
16.8 30.0 21.5
23.9 28.5 45.3
Table 2. Categories for which our method yields the best AP on PASCAL
VOC 2007, compared to any result we found in the literature. (∗means
extra Flickr data automatically obtained by our system was used to train.)
4.2. Active Detector Training on PASCAL
We next compare our active selection scheme to a passive learning baseline that randomly selects images for
bounding box annotation. We select six representative categories from PASCAL: we take two each from those that are
“easier” (>40 AP), “medium” (25-40 AP) and “hard” (0-25
AP) according to the state-of-the-art result (max of rows 4
and 5 in Table 1). We initialize each object’s classiﬁer with
20 examples, and then let the remainder of the training data
serve as the unlabeled pool, a total of 4.5 million examples.
At each iteration, both methods select 100 examples, add
their true bounding boxes (if any) to the labeled data, and
retrain. This qualiﬁes as learning in the “sandbox”, but is
useful to test our jumping window and hashing-based approach. Furthermore, the natural cluttered images are signiﬁcantly more challenging than data considered by prior
active object learning approaches, and our unlabeled pool is
orders of magnitude larger.
Figure 6 shows the results. We see our method’s clear
advantage; the steeper learning curves indicate it improves
accuracy on the test set using fewer labels.
In fact, in
most cases our approach reaches state-of-the-art performance (see markers above 5000 labels) using only one-third
of the available training data.
4.3. Online Live Learning on Flickr
Finally, we deploy our complete live learning system,
where new training data is crawled on Flickr. We consider
all object classes for which state-of-the-art AP is less than
25.0 (boat, dog, bird, pottedplant, sheep, chair) in order to
provide the most challenging case study, and to seek improvement through live learning where other methods have
struggled most. To form the Flickr test set, we download
Annotations added, out of 3 million examples
Average Precision
pottedplant
Live active (ours)
Keyword+image
Keyword+window
SP-MKL 
Figure 7. Live learning results on PASCAL test set.
bird boat chair dog pottedplant sheep
Flickr-crawled 2936 3138 2764 1831
Flickr-test
Table 3. Number of images in the crawled data and the new Flickr test set.
images tagged with the class names dated in 2010; when
running live training, our system is restricted to images
dated in 2009. See Table 3 for the data stats.
We compare to (1) a Keyword+image baseline that uses
the same crawled image pool, but randomly selects images
to get annotated on MTurk, and (2) a Keyword+window
baseline that randomly picks jumping windows to get labeled. These are strong baselines since most of the images
will contain the relevant object. In fact, they exactly represent the status quo approach, where one creates a dataset by
manually pruning keyword search results. We initialize all
methods with the PASCAL-trained models (5000 training
images), and run for 10 iterations.
Live learning applied to PASCAL test set
shows the results. For four of the six categories, our system improves test accuracy, and outperforms the keyword
approaches. The ﬁnal AP also exceeds the current state-ofthe-art for three categories (see Table 2, comparing to best
of ). This is an exciting result, given the size of the unlabeled pools (∼3 million examples), and the fact that the
system reﬁned its models completely automatically.
However, for two classes (chair, sheep), live learning decreases accuracy. Of course, more data cannot guarantee
improved performance on a ﬁxed test set. We suspect the
decline is due to stark differences in the distribution of PAS-
CAL and Flickr images, since the PASCAL dataset creators
do some manual preparation and pruning of all PASCAL
data. Our next result seems to conﬁrm this.
Live learning applied to Flickr test set
Figure 8 shows
the results on the new Flickr test set, where we apply the
same live-learned models from above. While this test set
appears more challenging than PASCAL, the improvements
made by our approach are dramatic—both in terms of its
absolute climb, as well as its margin over the baselines.
In all, the results indicate that our large-scale live learning approach can autonomously build models appropriate
Annotations added, out of 3 million examples
Average Precision
pottedplant
Live active (ours)
Keyword+image
Keyword+window
Figure 8. Live learning results on Flickr test set.
selected box
selected box
selected box
selected box
selected box
selected box
Figure 9. Selections by our live approach (top), Keyword+image (bottom).
for detection tasks with realistic and unbiased data. Figure 9 shows selections made by either method when learning “boat”, illustrating how ours focuses human attention
among the crawled tagged images.
4.4. Computation Time
Table 4 shows the time complexity of each stage, and illustrates our major advantages for selection and retraining
compared to existing strong detectors. Our times are based
on a dual-core 2.8 GHz CPU, comparable to . Our
jumping window+hashing scheme requires on average 2-3
seconds to retrieve 2,000 examples nearest the current hyperplane, and an additional 250 seconds to rank and select
100 images to query. In contrast, a linear scan over the entire unlabeled pool would require about 60 hours.
The entire online learning process requires 45-75 minutes per iteration: 5-10 min. to retrain, 5 min. for selection,
and ∼1 hour to wait for the MTurk annotations to come
back (typically 50 unique workers gave labels per task).
Thus, waiting on MTurk responses takes the majority of the
time, and could likely be reduced with better payment. In
comparison, the same selection with would require
about 8 hours to 1 week, respectively.
5. Conclusions
Our contributions are i) a novel efﬁcient part-based linear detector that provides excellent performance, ii) a jumping window and hashing scheme suitable for the proposed
detector that retrieves relevant instances among millions of
candidates, and iii) the ﬁrst active learning results for which
both data and annotations are automatically obtained, with
minimal involvement from vision experts. Tying it all together, we demonstrated an effective end-to-end system on
two challenging datasets.
Active selection Training Detection per image
Ours + active
Ours + passive
Table 4. Run-time comparisons. Our detection time is mostly spent pooling
the sparse codes. Active times are estimated for models based on
linear scan. Our approach’s efﬁciency makes live learning practical.
Acknowledgements
This research is supported in part by
ARL W911NF-10-2-0059 and NSF CAREER 0747356.