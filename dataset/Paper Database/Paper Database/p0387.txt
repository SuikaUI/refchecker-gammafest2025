Multi-view Convolutional Neural Networks for 3D Shape Recognition
Subhransu Maji
Evangelos Kalogerakis
Erik Learned-Miller
University of Massachusetts, Amherst
{hsu,smaji,kalo,elm}@cs.umass.edu
A longstanding question in computer vision concerns the
representation of 3D shapes for recognition: should 3D
shapes be represented with descriptors operating on their
native 3D formats, such as voxel grid or polygon mesh, or
can they be effectively represented with view-based descriptors? We address this question in the context of learning
to recognize 3D shapes from a collection of their rendered
views on 2D images. We ﬁrst present a standard CNN architecture trained to recognize the shapes’ rendered views
independently of each other, and show that a 3D shape
can be recognized even from a single view at an accuracy
far higher than using state-of-the-art 3D shape descriptors.
Recognition rates further increase when multiple views of
the shapes are provided. In addition, we present a novel
CNN architecture that combines information from multiple
views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The
same architecture can be applied to accurately recognize
human hand-drawn sketches of shapes. We conclude that
a collection of 2D views can be highly informative for 3D
shape recognition and is amenable to emerging CNN architectures and their derivatives.
1. Introduction
One of the fundamental challenges of computer vision is
to draw inferences about the three-dimensional (3D) world
from two-dimensional (2D) images. Since one seldom has
access to 3D object models, one must usually learn to recognize and reason about 3D objects based upon their 2D appearances from various viewpoints. Thus, computer vision
researchers have typically developed object recognition algorithms from 2D features of 2D images, and used them to
classify new 2D pictures of those objects.
But what if one does have access to 3D models of each
object of interest?
In this case, one can directly train
recognition algorithms on 3D features such as voxel occupancy or surface curvature. The possibility of building such
classiﬁers of 3D shapes directly from 3D representations
has recently emerged due to the introduction of large 3D
shape repositories, such as 3D Warehouse, TurboSquid, and
Shapeways. For example, when Wu et al. introduced
the ModelNet 3D shape database, they presented a classi-
ﬁer for 3D shapes using a deep belief network architecture
trained on voxel representations.
While intuitively, it seems logical to build 3D shape classiﬁers directly from 3D models, in this paper we present
a seemingly counterintuitive result – that by building classiﬁers of 3D shapes from 2D image renderings of those
shapes, we can actually dramatically outperform the classi-
ﬁers built directly on the 3D representations. In particular,
a convolutional neural network (CNN) trained on a ﬁxed set
of rendered views of a 3D shape and only provided with a
single view at test time increases category recognition accuracy by a remarkable 7% (77% →84%) over the best
models trained on 3D representations.
One reason for this result is the relative efﬁciency of the
2D versus the 3D representations. In particular, while a full
resolution 3D representation contains all of the information
about an object, in order to use a voxel-based representation in a deep network architecture that can be trained with
available samples and in a reasonable amount of time, it
would appear that the resolution needs to be signiﬁcantly
reduced. For example, ModelNet uses a coarse representation of shape, a 30×30×30 grid of binary voxels. In contrast a single projection of the 3D model of the same input
size corresponds to an image of 164×164 pixels, or slightly
smaller if multiple projections are used. Indeed, there is an
inherent trade-off between increasing the amount of explicit
depth information (3D models) and increasing spatial resolution (projected 2D models).
Another advantage of using 2D representations is that
we can leverage (i) advances in image descriptors in the
computer vision community and (ii) massive image databases (such as ImageNet ) to pre-train our CNN
architectures. Because images are ubiquitous and large labeled datasets are abundant, we can learn a good deal about
generic features for 2D image categorization and then ﬁnetune to speciﬁcs about 3D model projections. While it is
possible that some day as much 3D training data will be
 
3D shape model
rendered with
diﬀerent virtual cameras
2D rendered
our multi-view CNN architecture
output class
predictions
Figure 1. Multi-view CNN for 3D shape recognition. At test time a 3D shape is rendered from 12 different views and are passed thorough
CNN1 to extract view based features. These are then pooled across views and passed through CNN2 to obtain a compact shape descriptor.
available, for the time being this is a signiﬁcant advantage
of our representation.
Although the simple strategy of classifying views independently works remarkably well (Sect. 3.2), we present
new ideas for how to “compile” the information in multiple
2D views of an object into a compact descriptor of the object using a new architecture called multi-view CNN (Fig. 1
and Sect. 3.3). This descriptor is at least as informative for
classiﬁcation (and for retrieval is slightly more informative)
than the full collection of view-based descriptors of the object. Moreover it facilitates efﬁcient retrieval using either a
similar 3D object or a simple hand-drawn sketch of a similar
object, without resorting to slower methods that are based
on pairwise comparisons of image descriptors. We present
state-of-the-art results on 3D object classiﬁcation, 3D object retrieval using 3D objects, and 3D object retrieval using
sketches (Sect. 4).
Our multi-view CNN is related to “jittering” where transformed copies of the data are added during training to learn
invariances to transformations such as rotation or translation. In the context of 3D recognition the views can be
seen as jittered copies. The multi-view CNN learns to combine the views instead of averaging, and thus can use the
more informative views of the object for prediction while
ignoring others. Our experiments show that this improves
performance (Sect. 4.1) and also lets us visualize informative views of the object by back-propagating the gradients
of the network to the views (Fig. 3). Even on traditional
image classiﬁcation tasks multi-view CNN can be a better
alternative to jittering. For example, on the sketch recognition benchmark a multi-view CNN trained on jittered
copies performs better than a standard CNN trained with
the same jittered copies (Sect. 4.2). This also advances the
state-of-the-art from 79.0% to 87.2% approaching human performance on this task.
2. Related Work
Our method is related to prior work on shape descriptors
for 3D objects and image-based CNNs. Next we discuss
some most representative work in these areas.
Shape descriptors.
A large corpus of shape descriptors
has been developed for drawing inferences about 3D objects
in both the computer vision and graphics literature. Shape
descriptors can be classiﬁed into two broad categories: 3D
shape descriptors that directly work on the native 3D representations of objects, such as polygon meshes, voxel-based
discretizations, point clouds, or implicit surfaces, and viewbased descriptors that describe the shape of an 3D object by
“how it looks” in a collection of 2D projections.
With the exception of the recent work of Wu et al. 
which learns shape descriptors from the voxel-based representation of an object through 3D convolutional nets, previous 3D shape descriptors were largely “hand-designed”
according to a particular geometric property of the shape
surface or volume. For example, shapes can be represented
with histograms or bag-of-features models constructed out
of surface normals and curvatures , distances, angles,
triangle areas or tetrahedra volumes gathered at randomly
sampled surface points , properties of spherical functions deﬁned in volumetric grids , local shape diameters
measured at densely sampled surface points , heat kernel
signatures on polygon meshes , or extensions of the
SIFT and SURF feature descriptors to 3D voxel grids .
Developing classiﬁers and other supervised machine learning algorithms on top of such 3D shape descriptors poses a
number of challenges. First, the size of organized databases
with annotated 3D models is rather limited compared to image datasets, e.g. ModelNet contains about 150K shapes (its
40 category benchmark contains about 4K shapes). In contrast, the ImageNet database already includes tens of
millions of annotated images.
On the other hand view-based descriptors have a number
of desirable properties: they are relatively low-dimensional,
efﬁcient to evaluate, and robust to 3D shape representation
artifacts, such as holes, imperfect polygon mesh tesselations, noisy surfaces. The rendered shape views can also
be directly compared with other 2D images, silhouettes or
even hand-drawn sketches. An early example of a viewbased approach is the work by Murase and Nayar that
recognizes objects by matching their appearance in parametric eigenspaces formed by large sets of 2D renderings of
3D models under varying poses and illuminations. Another
example, which is particular popular in computer graphics
setups, is the LightField descriptor that extracts a set of
geometric and Fourier descriptors from object silhouettes
rendered from several different viewpoints. Alternatively,
the silhouette of an object can be decomposed into parts
and then be represented by a directed acyclic graph (shock
graph) . Cyr and Kimia deﬁned similarity metrics
based on curve matching and grouped similar views, called
aspect graphs of 3D models . Eitz et al. compared human sketches with line drawings of 3D models produced from several different views based on local Gabor
ﬁlters, while Schneider et al. proposed to use Fisher
Vectors on SIFT features for representing human
sketches. These descriptors are largely “hand-engineered”
and some do not generalize well across different domains,
e.g. the LightField descriptor requires closed silhouettes and
thus cannot be applied to sketches.
Convolutional neural networks.
Our work is also related to recent advances in image recognition using deep
CNNs . In particular CNNs trained on the large datasets
such as ImageNet have been shown to learn general purpose image descriptors for a number of recognition tasks
such as object detection, scene recognition, texture recognition and ﬁne-grained classiﬁcation . We
show that these deep architectures can be adapted to speciﬁc domains including shaded illustrations of 3D objects,
line drawings, and human sketches to produce descriptors
that have dramatically superior performance compared to
other view-based descriptors and 3D shape descriptors (including 3D ShapeNets ) in a variety of setups. Furthermore, they are compact and efﬁcient to compute.
Although there is signiﬁcant work on 3D and 2D shape
descriptors, and estimating informative views of the objects
(or, aspect graphs), there is relatively little work on learning
to combine the view-based descriptors for 3D shape recognition. Most methods resort to simple strategies such as
performing exhaustive pairwise comparisons of descriptors
extracted from different views of each shape. In contrast our
multi-view CNN architecture learns to recognize 3D shapes
from views of the shapes using image-based CNNs but in
the context of other views via a view-pooling layer. As a
result, information from multiple views is effectively accumulated into a single, compact shape descriptor.
As discussed above, our focus in this paper is on developing view-based descriptors for 3D shapes that are trainable, produce informative representations for recognition
and retrieval tasks, and are efﬁcient to compute.
Our view-based representations start from multiple
views of a 3D shape, generated by a rendering engine. A
simple way to use multiple views is to generate a 2D image
descriptor per each view, and then use the individual descriptors directly for recognition tasks based on some voting scheme. For example, a na¨ıve approach would be to
average the individual descriptors, treating all the views as
equally important. Alternatively, if the views are rendered
in a reproducible order, one could also concatenate the 2D
descriptors of all the views. Unfortunately, aligning a 3D
shape to a canonical orientation is hard and sometimes illdeﬁned. In contrast to the above simple approaches, an aggregated representation combining features from multiple
views is more desirable since it yields a single, compact descriptor representing the 3D shape.
Our approach is to learn to combine information from
multiple views using a uniﬁed CNN architecture that includes a view-pooling layer (Fig. 1). All the parameters of
our CNN architecture are learned discriminatively to produce a single compact descriptor for the 3D shape. Compared to exhaustive pairwise comparisons between singleview representations of 3D shapes, our resulting descriptors
can be directly used to compare 3D shapes leading to signiﬁcantly higher computational efﬁciency.
3.1. Input: A multi-view representation
3D models in online databases are typically stored as
polygon meshes, which are collections of points connected
with edges forming faces. We assume that the 3D models
are consistently upright oriented. Most models in modern
online repositories, such as the 3D Warehouse, satisfy this
requirement.
For each mesh, we render 12 projected views as follows.
We place 12 virtual cameras (viewpoints) around the mesh
every 30 degrees (see Fig. 1). The cameras are elevated 30
degrees from the ground plane, pointing towards the centroid of the mesh. The centroid is calculated as the weighted
average of the mesh face centers, where the weights are the
face areas. The shapes are illuminated using the Phong re-
ﬂection model . The mesh polygons are rendered under
a perspective projection and the pixel color is determined by
interpolating the reﬂected intensity of the polygon vertices.
Shapes are uniformly scaled to ﬁt into the viewing volume.
We note that using different shading coefﬁcients or illumination models did not affect our output descriptors due to
the invariance of the learned ﬁlters to illumination changes,
as also observed in image-based CNNs . Adding
more or different viewpoints is trivial, however, we found
that the above camera setup was already enough to achieve
high performance. Finally, rendering each mesh from all the
viewpoints takes no more than ten milliseconds on modern
graphics hardware.
3.2. Recognition with the multi-view representation
We claim that our multi-view representation contains
rich information about 3D shapes and can be applied to various types of tasks. In the ﬁrst setting, we make use of existing 2D image features directly and produce a descriptor for
each view. This is the most straightforward approach to utilize the multi-view representation, and can beneﬁt from the
fact that building image features has been a very active and
fruitful research area and many powerful image features exist. This however results in multiple 2D image descriptors
per 3D shape, one per view, which need to be integrated
somehow for recognition tasks.
Image descriptors.
We consider two types of image descriptors for each 2D view: a state-of-the-art “hand-crafted”
image descriptor based on Fisher Vectors with multiscale SIFT, as well as CNN activation features .
The Fisher Vector image descriptor is implemented using
VLFeat . For each image multi-scale SIFT descriptors
are extracted densely. These are then PCA projected to 80
dimensions followed by Fisher Vector pooling with a Gaussian mixture model with 64 components, square-root and ℓ2
normalization.
For our CNN features we use the VGG-M network
from which consists of mainly ﬁve convolutional layers
conv1,...,5 followed by three fully connected layers fc6,...,8
and a softmax classiﬁcation layer. The penultimate layer
fc7 (after ReLU non-linearity, 4096-dimensional) is used
as image descriptor. The network is pre-trained on ImageNet images from 1k categories, and then ﬁne-tuned on all
2D views of the 3D shapes in training set. As we show in
our experiments, ﬁne-tuning improves performance significantly. Both Fisher Vectors and CNN features yield very
good performance in classiﬁcation and retrieval compared
with popular 3D shape descriptors (e.g. SPH , LFD )
as well as 3D ShapeNets .
Classiﬁcation.
We train one-vs-all linear SVMs (each
view as separate training samples) to classify shapes with
their image features. At test time, we simply sum up SVM
decision values over all 12 views and return the class with
the highest sum. Alternative approaches, e.g. averaging image descriptors, lead to worse accuracy.
Retrieval.
A distance or similarity measure is required for
retrieval tasks. For shape x with nx image descriptors and
shape y with ny image descriptors, distance between them
is deﬁned in Eq. 1. Note that distance between two 2D images is deﬁned as the L2 distance between their feature vectors, i.e. ∥xi −yj∥2.
Dist(x, y) =1
j mini ∥xi −yj∥2
i minj ∥xi −yj∥2
To interpret this deﬁnition, we can ﬁrst deﬁne the distance between a 2D image xi and a 3D shape y as
d(xi, y) = minj ∥xi −yj∥2. Then given all nx distances
between x’s 2D projections and y, the distance between
these two shapes can be get by simple averaging. In Eq. 1,
this idea is applied in both directions to ensure symmetry.
We investigated alternative distance measures, such as
minimun distance among all nx · ny image pairs, distance
between average image descriptors, but they all led to inferior performance.
3.3. Multi-view CNN: Learning to aggregate views
Although very successful for classiﬁcation and retrieval
compared with existing 3D descriptors, having multiple
separate descriptors for each 3D shape can be inconvenient
and inefﬁcient in many cases. For example, in Eq. 1, we
need to compute all nx · ny pairwise distances between images in order to compute distance between two 3D shapes.
Simply averaging or concatenating the image descriptors
leads to inferior performance. In this section, we focus on
the problem of learning to aggregate multiple views in order
to synthesize the information from all views into a single,
compact 3D shape descriptor.
We design the multi-view CNN (MVCNN) on top of
image-based CNNs (Fig. 1). Each image in a 3D shape’s
multi-view representation is passed through the ﬁrst part
of the network (CNN1) separately, aggregated at a viewpooling layer, and then sent through the remaining part
of the network (CNN2). All branches in the ﬁrst part of
the network share the same parameters in CNN1. We use
element-wise maximum operation across the views in the
view-pooling layer. An alternative is element-wise mean
operation, but it is not as effective in our experiments. The
view-pooling layer can be placed anywhere in the network.
We show in our experiments that it should be placed close
to the last convolutional layer (conv5) for optimal classiﬁcation and retrieval performance. View-pooling layers are
closely related to max-pooling layers and maxout layers
 , with the only difference in terms of implementation
being the dimension their pooling operations are carried
on. MVCNN are directed acyclic graphs and can be trained
Training Conﬁg.
Test Conﬁg.
Classiﬁcation
(Accuracy)
(1) SPH 
(2) LFD 
(3) 3D ShapeNets 
ModelNet40
ModelNet40
ModelNet40
(5) FV, 12×
ModelNet40
ImageNet1K
(7) CNN, f.t.
ImageNet1K ModelNet40
(8) CNN, 12×
ImageNet1K
(9) CNN, f.t.,12×
ImageNet1K ModelNet40
(10) MVCNN, 12×
ImageNet1K
(11) MVCNN, f.t., 12×
ImageNet1K ModelNet40
(12) MVCNN, f.t.+metric, 12× ImageNet1K ModelNet40
* f.t.=ﬁne-tuning, metric=low-rank Mahalanobis metric learning
Table 1. Classiﬁcation and retrieval results on the ModelNet40 dataset. On the top are results using state-of-the-art 3D shape descriptors.
Our view-based descriptors including Fisher Vectors (FV) signiﬁcantly outperform these even when a single view is available at test time
(#Views = 1). When multiple views (#Views=12) are available at test time, the performance of view-based methods improve signiﬁcantly.
The multi-view CNN (MVCNN) architecture outperforms the view-based methods, especially for retrieval.
or ﬁne-tuned using stochastic gradient descent with backpropagation.
Using fc7 (after ReLU non-linearity) in MVCNN as
an aggregated shape descriptor, we achieve higher performance than using separate image descriptors from imagebased CNN directly (this approach will be referred to as
single-view CNN for clarity), especially in retrieval (62.1%
→69.4%). And perhaps more importantly, the aggregated
descriptor is ready for use out of the box for a variety of
tasks, e.g. shape classiﬁcation and retrieval, and offers signiﬁcant speed-ups against multiple image descriptors.
MVCNN can also be used as a general framework to integrate perturbed image samples (also known as data jittering). We illustrate this capability of MVCNN in the context
of sketch recognition in Sect. 4.2.
Low-rank Mahalanobis metric.
Our networks are ﬁnetuned for classiﬁcation, thus retrieval performance is not
directly optimized. Although we could train our networks
with a different objective function suitable for retrieval, we
found that a simpler approach can readily yield a signiﬁcant
retrieval performance boost (see row 12 in Tab. 1). We learn
a Mahalanobis metric W that directly projects MVCNN descriptors φ ∈Rd to Wφ ∈Rp, such that the L2 distances
in the projected space are small between shapes of the same
category, and large otherwise. We use the large-margin metric learning algorithm and implementation from , and
use p < d to make the ﬁnal descriptor compact (we set
p = 128 in our experiments). The fact that we can readily
use metric learning for the single shape descriptor demonstrates another advantage of MVCNN. It is less clear how
to apply metric learning when a different shape descriptor
is produced from each view.
4. Experiments
4.1. 3D shape classiﬁcation and retrieval
We evaluate our shape descriptors on the Princeton ModelNet dataset .
ModelNet currently contains 127,915
3D CAD models from 662 categories1. A 40-class wellannotated subset containing 12,311 shapes from 40 common categories, ModelNet40, is provided on the ModelNet
website. For our experiments, we use the same training and
test split of ModelNet40 as in 2.
Our shape descriptors are compared against the 3D
ShapeNet descriptor by Wu et al. , the Spherical Harmonics descriptor (SPH) by Kazhdan et al. , the Light-
Field descriptor (LFD) by Chen et al. , and Fisher Vectors extracted on the same rendered views of the shapes used
as input to our network.
Results on shape classiﬁcation and retrieval are summarized in Tab. 1. Precision-recall curves are provided in
Fig. 2. Remarkably the Fisher Vector baseline with just
a single view achieves a classiﬁcation accuracy of 79.3%
outperforming the state-of-the-art learned 3D descriptors
(77.3% ). When all 12 views of the shape are available
1As of 04/21/2015.
2The training/test split they used is different from the one on their website, and covers about 100 shapes per category, within which 20 shapes are
used for testing and the rest for training.
Spherical Harmonic
Light Field
3D ShapeNets
Fisher Vectors
Ours (MVCNN)
Ours (MVCNN+metric)
Figure 2. Precision-recall curves for various methods for 3D shape
retrieval on the ModelNet40 dataset. Our method signiﬁcantly outperforms the state-of-the-art on this task achieving 78.9% mAP.
at test time, we can also average the predictions over these
views. Averaging increases the performance of Fisher Vectors to 85.1%. The performance of Fisher Vectors further
supports our claim that 3D objects can be effectively represented using view-based 2D representations. The trends in
performance for shape retrieval are similar.
Using our CNN baseline trained on ImageNet in turn
outperforms Fisher Vectors by a signiﬁcant margin. Finetuning the CNN on the rendered views of the training shapes
of ModelNet40 further improves the performance. By using
all 12 views of the shape, its classiﬁcation accuracy reaches
87.3%. The retrieval accuracy is also improved to 62.1%.
Our MVCNN outperforms all the state-of-the-art descriptors as well as the Fisher Vector and CNN baselines.
With ﬁne-tuning on the ModelNet40 training set, our model
achieves 88.8% classiﬁcation accuracy, and 69.4% mean
average precision (mAP) on retrieval.
MVCNN constitutes an absolute gain of 11.5% in classiﬁcation accuracy
compared to the state-of-the-art learned 3D shape descriptor (77.3% →88.8%).
Similarly, retrieval mAP is
improved by 20.2% (49.2% →69.4%).
Finally, learning a low-rank Mahalanobis metric improves retrieval mAP
further while classiﬁcation accuracy remains almost unchanged, and the resulting shape descriptors become much
more compact (d = 4096, p = 128).
We considered different locations to place the viewpooling layer in our MVCNN architecture (see Tab. 2). Although performance is not very sensitive to the location, we
ﬁnd conv5 to be a good choice for balanced performance,
and thus use it for all other experiments.
Classiﬁcation
(Accuracy)
Table 2. Comparison of various view-pooling locations in the
MVCNN architecture.
Saliency map among views.
For each 3D shape S, our
multi-view representation consists of a set of K 2D views
{I1, I2 . . . IK}. We would like to rank pixels in the 2D
views w.r.t. their inﬂuence on the output score Fc of the network (e.g. taken from fc8 layer) for its ground truth class c.
Following , saliency maps can be deﬁned as the derivatives of Fc w.r.t. the 2D views of the shape:
[w1, w2 . . . wK] =
For MVCNN, w in Eq. 2 can be computed using backpropagation with all the network parameters ﬁxed, and can
then be rearranged to form saliency maps for individual
views. Examples of saliency maps are shown in Fig. 3.
4.2. Sketch recognition: jittering revisited
Given the success of our aggregated descriptors on multiple views of a 3D object, it is logical to ask whether aggregating multiple views of a 2D image could also improve
performance. Here we show that this is indeed the case by
exploring its connection with data jittering in the context of
sketch recognition.
Data jittering, or data augmentation, is a method to generate extra samples from a given image. It is the process
of perturbing the image by transformations that change its
appearance while leaving the high-level information (class
label, attributes, etc.) intact. Jittering can be applied at training time to augment training samples and to reduce overﬁtting, or at test time to provide more robust predictions. In
particualr, several authors have used data jittering
to improve the performance of deep representations on 2D
image classiﬁcation tasks. In these applications, jittering at
training time usually includes random images translations
(implemented as random crops), horizontal reﬂections, and
color perturbations. At test time jittering usually only includes a few crops (for example, four at the corners, one at
the center and their horizontal reﬂections). We now examine whether we can get more beneﬁt out of jittered views of
an image by using the same feature aggregation scheme we
developed for recognizing 3D shapes.
Figure 3. Top three views with the highest saliency are highlighted in blue and the relative magnitudes of gradient energy for each view is
shown on top. The saliency maps are computed by back-propagating the gradients of the class score onto the image via the view-pooling
layer. Notice that the handles of the dresser and of the desk are the most discriminative features. (Figures are enhanced for visibility).
The human sketch dataset contains 20,000 handdrawn sketches of 250 object categories such as airplanes,
apples, bridges, etc. The accuracy of humans in recognizing
these hand-drawings is only 73% because of the low quality of some sketches. In a subsequent paper , Schneider
and Tuytelaars cleaned up the dataset by removing instances
and categories that humans ﬁnd hard to recognize. This
cleaned dataset (SketchClean) contains 160 categories, on
which human can achieve 93% recognition accuracy. The
current state-of-the-art sketch recognition performance is
67.6% accuracy on the original dataset and 79.0% accuracy
on the SketchClean dataset, achieved by using SIFT Fisher
Vectors with spatial pyramid pooling and linear SVMs .
We split the SketchClean dataset randomly into training,
validation and test set3, and report classiﬁcation accuracy
on the test set in Tab. 3.
With an off-the-shelf CNN (VGG-M from ), we are
able to get 77.3% classiﬁcation accuracy without any network ﬁne-tuning. With ﬁne-tuning on the training set, the
accuracy can be further improved to 84.0%, signiﬁcantly
surpassing the Fisher Vector approach. These numbers are
achieved by using the penultimate layer (fc7) in the network
as image descriptors and linear SVMs.
Although it is impracticable to get multiple views from
2D images, we can use jittering to mimic the effect of views.
For each hand-drawn sketch, we do in-plane rotation with
3The dataset does not come with a standard training/val/test split.
three angles: −45°, 0°, 45°, and also horizontal reﬂections (hence 6 samples per image). We apply the two CNN
variants (regular CNN and MVCNN) discussed earlier for
aggregating multiple views of 3D shapes, and get 85.5%
(CNN w/o view-pooling) and 86.3% (MVCNN w/ viewpooling on fc7) classiﬁcation accuracy respectively. The
latter also has the advantage of a single, more compact descriptor for each hand-drawn sketch.
With a deeper network architecture (VGG-VD, a network with 16 weight layers from ), we are able to
achieve 87.2% accuracy, advancing the state of the art by a
large margin, and closely approaching human performance.
4.3. Sketch-based 3D shape retrieval
Due to the growing number of online 3D repositories,
many approaches have been investigated to perform efﬁcient 3D shape retrieval.
Most online repositories (e.g.
3D Warehouse, TurboSquid, Shapeways) provide only textbased search engines or hierarchical catalogs for 3D shape
retrieval. However, it is hard to convey stylistic and geometric variations using only textual descriptions, so sketchbased shape retrieval has been proposed as
an alternative for users to retrieve shapes with an approximate sketch of the desired 3D shape in mind.
Sketchbased retrieval is challenging since it involves two heterogeneous data domains (hand-drawn sketches and 3D shapes),
and sketches can be highly abstract and visually different
(1) FV 
(3) CNN M, ﬁne-tuned
(4) CNN M, ﬁne-tuned
(5) MVCNN M, ﬁne-tuned
(6) CNN VD
(7) CNN VD, ﬁne-tuned
(8) CNN VD, ﬁne-tuned
(9) MVCNN VD, ﬁne-tuned
(10) Human performance
Table 3. Sketch classiﬁcation results. Fine-tuned CNN models signiﬁcantly outperform the state-of-the-art by a signiﬁcant margin.
MVCNNs are better than CNN trained with data jittering. The results are shown with two different CNN architectures – VGG-M
(row 2-5) and VGG-VD (row 6-9).
from target 3D shapes. Here we demonstrate the potential
strength of MVCNN in sketch-based shape retrieval.
For this experiment, we construct a dataset containing
193 sketches and 790 CAD models from 10 categories existing in both SketchClean and ModelNet40.
 , we produce renderings of 3D shapes with a style similar to hand-drawn sketches (see Fig. 4). This is achieved by
detecting Canny edges on the depth buffer (also known as
z-buffer) from 12 viewpoints. These edge maps are then
passed through CNNs to obtain image descriptors.
Descriptors are also extracted from 6 perturbed samples of
each query sketch in the manner described in Sect. 4.2. Finally we rank 3D shapes w.r.t. “average minimum distance”
(Eq. 1) to the sketch descriptors. Representative retrieval
results are shown in Fig. 5.
We are able to retrieve 3D objects from the same class
with the query sketch, as well as being visually similar,
especially in the top few matches.
Our performance is
36.1% mAP on this dataset. Here we use the VGG-M network trained on ImageNet without any ﬁne-tuning on either
sketches or 3D shapes. With a ﬁne-tuning procedure that
optimizes a distance measure between hand-drawn sketches
and 3D shapes, e.g. by using a Siamese Network , retrieval performance can be further improved.
5. Conclusion
While the world is full of 3D shapes, as humans at least,
we understand that world mostly through 2D images. We
have shown that using images of shapes as inputs to modern
learning architectures, we can achieve performance better
than any previously published results, including those that
operate on direct 3D representations of shapes.
While even a n¨aive usage of these multiple 2D pro-
depth bu er
Canny edge
human sketch
Figure 4. Line-drawing style rendering from 3D shapes.
top 10 retrieved 3D shapes
Figure 5. Sketch-based 3D shape retrieval examples. Top matches
are shown for each query, with mistakes highlighted in red.
jections yields impressive discrimination performance, we
have shown that by building descriptors that are aggregations of information from multiple views, we can achieve
compactness, efﬁciency, and high accuracy. In addition, by
relating the content of 3D shapes to 2D representations like
sketches, we can retrieve these 3D shapes at high accuracy
with the sketches, and leverage the implicit knowledge of
3D shapes contained in their 2D views.
There are a number of directions to explore in future
work. One is to experiment with different combinations of
2D views. Which views are most informative? How many
views are necessary for a given level of accuracy? Can informative views be selected on the ﬂy?
Another obvious question is whether our view aggregating techniques can be used for building compact and discriminative descriptors for real-world 3D objects from multiple views, or automatically from video, rather than merely
for 3D polygon mesh models. Such investigations could be
immediately applicable to widely studied problems such as
object recognition and face recognition.
Here we provide additional evaluations and visulizations
of our multi-view CNN (MVCNN), including a) confusion
matrix of 3D shape classiﬁcation; b) additional view-based
saliency maps; and c) examples of correctly and wrongly
classiﬁed hand-drawn sketches.
A. 3D shape classiﬁcation
Confusion matrix of 3D shape classiﬁcation on Model-
Net40 is given in Figure 6. Here MVCNN with ﬁne-tuning
on 12 views (row 11 in Table 1 of main submission) is used.
Top confusions occur at 1) ﬂower pot →plant (45%),
2) table →desk (32%), 3) ﬂower pot →vase (20%),
4) plant →ﬂower (19%), and 5) stool →chair (15%). Distinctions between some of these pairs are ambiguous even
for humans.
.01 .04 .01
flower pot
night stand
range hood
cupcurtain
flower pot
night stand
range hood
tenttoilet
Figure 6. Confusion matrix of ModelNet40 classiﬁcation.
B. Image-speciﬁc class saliency visualization
across views
Additional examples of saliency maps are shown in Figure 7. Note that the saliency maps tend to highlight a) the
most canonical views accross views, e.g. the front view
of the bench; and b) the most discriminative parts within
views, e.g. the faucet and the sink hole of the bathtub.
C. Sketch classiﬁcation
Examples of correctly and wrongly classiﬁed handdrawn sketches are shown in Figure 8. Most misclassiﬁed
sketches contain visually similar components with the target class, e.g. spider and crab have a similar layout of legs,
and some are difﬁcult to recognize even for humans.
D. Document changelog
v1 Initial version.
v2 An updated ModelNet40 training/test split is used for experiments in order to be consistent with . Performance
of most methods drops a bit because of the smaller training set (the full ModelNet40 was used in v1). Results with
low-rank Mahalanobis metric learning are added.