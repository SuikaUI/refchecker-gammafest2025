Scalable Diffusion Models with Transformers
William Peebles*
UC Berkeley
Saining Xie
New York University
Figure 1. Diffusion models with transformer backbones achieve state-of-the-art image quality. We show selected samples from two
of our class-conditional DiT-XL/2 models trained on ImageNet at 512√ó512 and 256√ó256 resolution, respectively.
We explore a new class of diffusion models based on the
transformer architecture. We train latent diffusion models
of images, replacing the commonly-used U-Net backbone
with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs)
through the lens of forward pass complexity as measured by
GÔ¨Çops. We Ô¨Ånd that DiTs with higher GÔ¨Çops‚Äîthrough increased transformer depth/width or increased number of input tokens‚Äîconsistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2
models outperform all prior diffusion models on the classconditional ImageNet 512√ó512 and 256√ó256 benchmarks,
achieving a state-of-the-art FID of 2.27 on the latter.
1. Introduction
Machine learning is experiencing a renaissance powered
by transformers. Over the past Ô¨Åve years, neural architectures for natural language processing , vision 
and several other domains have largely been subsumed by
transformers .
Many classes of image-level generative models remain holdouts to the trend, though‚Äîwhile
transformers see widespread use in autoregressive models , they have seen less adoption in other generative modeling frameworks. For example, diffusion models
have been at the forefront of recent advances in image-level
generative models ; yet, they all adopt a convolutional
U-Net architecture as the de-facto choice of backbone.
* Work done during an internship at Meta AI, FAIR Team.
Code and project page available here.
 
Figure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the Ô¨Çops of the diffusion model. Left:
FID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model Ô¨Çops increase.
Right: Our best model, DiT-XL/2, is compute-efÔ¨Åcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.
The seminal work of Ho et al. Ô¨Årst introduced the
U-Net backbone for diffusion models. Having initially seen
success within pixel-level autoregressive models and conditional GANs , the U-Net was inherited from Pixel-
CNN++ with a few changes. The model is convolutional, comprised primarily of ResNet blocks. In
contrast to the standard U-Net , additional spatial selfattention blocks, which are essential components in transformers, are interspersed at lower resolutions. Dhariwal and
Nichol ablated several architecture choices for the U-
Net, such as the use of adaptive normalization layers to
inject conditional information and channel counts for convolutional layers. However, the high-level design of the U-
Net from Ho et al. has largely remained intact.
With this work, we aim to demystify the signiÔ¨Åcance of
architectural choices in diffusion models and offer empirical baselines for future generative modeling research. We
show that the U-Net inductive bias is not crucial to the performance of diffusion models, and they can be readily replaced with standard designs such as transformers. As a
result, diffusion models are well-poised to beneÔ¨Åt from the
recent trend of architecture uniÔ¨Åcation‚Äîe.g., by inheriting
best practices and training recipes from other domains, as
well as retaining favorable properties like scalability, robustness and efÔ¨Åciency. A standardized architecture would
also open up new possibilities for cross-domain research.
In this paper, we focus on a new class of diffusion models
based on transformers. We call them Diffusion Transformers, or DiTs for short. DiTs adhere to the best practices of
Vision Transformers (ViTs) , which have been shown to
scale more effectively for visual recognition than traditional
convolutional networks (e.g., ResNet ).
More speciÔ¨Åcally, we study the scaling behavior of transformers with respect to network complexity vs. sample
We show that by constructing and benchmarking the DiT design space under the Latent Diffusion Models (LDMs) framework, where diffusion models are
trained within a VAE‚Äôs latent space, we can successfully
replace the U-Net backbone with a transformer. We further
show that DiTs are scalable architectures for diffusion models: there is a strong correlation between the network complexity (measured by GÔ¨Çops) vs. sample quality (measured
by FID). By simply scaling-up DiT and training an LDM
with a high-capacity backbone (118.6 GÔ¨Çops), we are able
to achieve a state-of-the-art result of 2.27 FID on the classconditional 256 √ó 256 ImageNet generation benchmark.
2. Related Work
Transformers.
Transformers have replaced domainspeciÔ¨Åc architectures across language, vision , reinforcement learning and meta-learning . They
have shown remarkable scaling properties under increasing model size, training compute and data in the language
domain , as generic autoregressive models and
as ViTs . Beyond language, transformers have been
trained to autoregressively predict pixels . They
have also been trained on discrete codebooks as both
autoregressive models and masked generative models ; the former has shown excellent scaling behavior
up to 20B parameters . Finally, transformers have been
explored in DDPMs to synthesize non-spatial data; e.g., to
generate CLIP image embeddings in DALL¬∑E 2 . In
this paper, we study the scaling properties of transformers
when used as the backbone of diffusion models of images.
Multi-Head
Self-Attention
Layer Norm
Scale, Shift
Feedforward
Layer Norm
Scale, Shift
Input Tokens
Conditioning
DiT Block with adaLN-Zero
Latent Diffusion Transformer
Timestep ùë°
Layer Norm
Linear and Reshape
32 x 32 x 4
32 x 32 x 4
32 x 32 x 4
Multi-Head
Self-Attention
Layer Norm
Feedforward
Layer Norm
Input Tokens
Conditioning
DiT Block with Cross-Attention
Multi-Head
Cross-Attention
Layer Norm
Multi-Head
Self-Attention
Layer Norm
Feedforward
Input Tokens
Conditioning
DiT Block with In-Context Conditioning
Layer Norm
Concatenate
on Sequence
Figure 3. The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed
into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer
blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.
Denoising diffusion probabilistic models (DDPMs).
Diffusion and score-based generative models have been particularly successful as generative models
of images , in many cases outperforming generative adversarial networks (GANs) which had previously been state-of-the-art. Improvements in DDPMs over
the past two years have largely been driven by improved
sampling techniques , most notably classiÔ¨Åerfree guidance , reformulating diffusion models to predict noise instead of pixels and using cascaded DDPM
pipelines where low-resolution base diffusion models are
trained in parallel with upsamplers . For all the diffusion models listed above, convolutional U-Nets are
the de-facto choice of backbone architecture. Concurrent
work introduced a novel, efÔ¨Åcient architecture based
on attention for DDPMs; we explore pure transformers.
Architecture complexity.
When evaluating architecture
complexity in the image generation literature, it is fairly
common practice to use parameter counts. In general, parameter counts can be poor proxies for the complexity of
image models since they do not account for, e.g., image resolution which signiÔ¨Åcantly impacts performance .
Instead, much of the model complexity analysis in this paper is through the lens of theoretical GÔ¨Çops. This brings us
in-line with the architecture design literature where GÔ¨Çops
are widely-used to gauge complexity.
In practice, the
golden complexity metric is still up for debate as it frequently depends on particular application scenarios. Nichol
and Dhariwal‚Äôs seminal work improving diffusion models is most related to us‚Äîthere, they analyzed the
scalability and GÔ¨Çop properties of the U-Net architecture
class. In this paper, we focus on the transformer class.
3. Diffusion Transformers
3.1. Preliminaries
Diffusion formulation.
Before introducing our architecture, we brieÔ¨Çy review some basic concepts needed to
understand diffusion models (DDPMs) .
Gaussian diffusion models assume a forward noising process
which gradually applies noise to real data x0: q(xt|x0) =
N(xt; ‚àö¬ØŒ±tx0, (1 ‚àí¬ØŒ±t)I), where constants ¬ØŒ±t are hyperparameters. By applying the reparameterization trick, we can
sample xt = ‚àö¬ØŒ±tx0 + ‚àö1 ‚àí¬ØŒ±tœµt, where œµt ‚àºN(0, I).
Diffusion models are trained to learn the reverse process
that inverts forward process corruptions: pŒ∏(xt‚àí1|xt) =
N(¬µŒ∏(xt), Œ£Œ∏(xt)), where neural networks are used to predict the statistics of pŒ∏.
The reverse process model is
trained with the variational lower bound of the loglikelihood of x0, which reduces to L(Œ∏) = ‚àíp(x0|x1) +
t DKL(q‚àó(xt‚àí1|xt, x0)||pŒ∏(xt‚àí1|xt)), excluding an additional term irrelevant for training. Since both q‚àóand pŒ∏
are Gaussian, DKL can be evaluated with the mean and covariance of the two distributions. By reparameterizing ¬µŒ∏ as
a noise prediction network œµŒ∏, the model can be trained using simple mean-squared error between the predicted noise
œµŒ∏(xt) and the ground truth sampled Gaussian noise œµt:
Lsimple(Œ∏) = ||œµŒ∏(xt) ‚àíœµt||2
2. But, in order to train diffusion models with a learned reverse process covariance Œ£Œ∏,
the full DKL term needs to be optimized. We follow Nichol
and Dhariwal‚Äôs approach : train œµŒ∏ with Lsimple, and
train Œ£Œ∏ with the full L. Once pŒ∏ is trained, new images can
be sampled by initializing xtmax ‚àºN(0, I) and sampling
xt‚àí1 ‚àºpŒ∏(xt‚àí1|xt) via the reparameterization trick.
ClassiÔ¨Åer-free guidance.
Conditional diffusion models
take extra information as input, such as a class label c.
In this case, the reverse process becomes pŒ∏(xt‚àí1|xt, c),
where œµŒ∏ and Œ£Œ∏ are conditioned on c.
In this setting,
classiÔ¨Åer-free guidance can be used to encourage the sampling procedure to Ô¨Ånd x such that log p(c|x) is high .
By Bayes Rule, log p(c|x) ‚àùlog p(x|c) ‚àílog p(x), and
hence ‚àáx log p(c|x) ‚àù‚àáx log p(x|c)‚àí‚àáx log p(x). By interpreting the output of diffusion models as the score function, the DDPM sampling procedure can be guided to sample x with high p(x|c) by: ÀÜœµŒ∏(xt, c) = œµŒ∏(xt, ‚àÖ) + s ¬∑
‚àáx log p(x|c) ‚àùœµŒ∏(xt, ‚àÖ)+s¬∑(œµŒ∏(xt, c)‚àíœµŒ∏(xt, ‚àÖ)), where
s > 1 indicates the scale of the guidance (note that s = 1 recovers standard sampling). Evaluating the diffusion model
with c = ‚àÖis done by randomly dropping out c during
training and replacing it with a learned ‚Äúnull‚Äù embedding
‚àÖ. ClassiÔ¨Åer-free guidance is widely-known to yield signiÔ¨Åcantly improved samples over generic sampling techniques , and the trend holds for our DiT models.
Latent diffusion models.
Training diffusion models directly in high-resolution pixel space can be computationally
prohibitive. Latent diffusion models (LDMs) tackle this
issue with a two-stage approach: (1) learn an autoencoder
that compresses images into smaller spatial representations
with a learned encoder E; (2) train a diffusion model of
representations z = E(x) instead of a diffusion model of
images x (E is frozen). New images can then be generated
by sampling a representation z from the diffusion model
and subsequently decoding it to an image with the learned
decoder x = D(z).
As shown in Figure 2, LDMs achieve good performance
while using a fraction of the GÔ¨Çops of pixel space diffusion
models like ADM. Since we are concerned with compute
efÔ¨Åciency, this makes them an appealing starting point for
architecture exploration. In this paper, we apply DiTs to
latent space, although they could be applied to pixel space
without modiÔ¨Åcation as well. This makes our image generation pipeline a hybrid-based approach; we use off-the-shelf
convolutional VAEs and transformer-based DDPMs.
3.2. Diffusion Transformer Design Space
We introduce Diffusion Transformers (DiTs), a new architecture for diffusion models. We aim to be as faithful to
the standard transformer architecture as possible to retain
its scaling properties. Since our focus is training DDPMs of
images (speciÔ¨Åcally, spatial representations of images), DiT
is based on the Vision Transformer (ViT) architecture which
operates on sequences of patches . DiT retains many of
the best practices of ViTs. Figure 3 shows an overview of
the complete DiT architecture. In this section, we describe
the forward pass of DiT, as well as the components of the
design space of the DiT class.
Noised Latent
Input Tokens T √ó d
Figure 4. Input speciÔ¨Åcations for DiT. Given patch size p √ó p,
a spatial representation (the noised latent from the VAE) of shape
I √ó I √ó C is ‚ÄúpatchiÔ¨Åed‚Äù into a sequence of length T = (I/p)2
with hidden dimension d. A smaller patch size p results in a longer
sequence length and thus more GÔ¨Çops.
The input to DiT is a spatial representation z
(for 256 √ó 256 √ó 3 images, z has shape 32 √ó 32 √ó 4). The
Ô¨Årst layer of DiT is ‚Äúpatchify,‚Äù which converts the spatial
input into a sequence of T tokens, each of dimension d,
by linearly embedding each patch in the input. Following
patchify, we apply standard ViT frequency-based positional
embeddings (the sine-cosine version) to all input tokens.
The number of tokens T created by patchify is determined
by the patch size hyperparameter p. As shown in Figure 4,
halving p will quadruple T, and thus at least quadruple total
transformer GÔ¨Çops. Although it has a signiÔ¨Åcant impact on
GÔ¨Çops, note that changing p has no meaningful impact on
downstream parameter counts.
We add p = 2, 4, 8 to the DiT design space.
DiT block design.
Following patchify, the input tokens
are processed by a sequence of transformer blocks. In addition to noised image inputs, diffusion models sometimes
process additional conditional information such as noise
timesteps t, class labels c, natural language, etc. We explore
four variants of transformer blocks that process conditional
inputs differently. The designs introduce small, but important, modiÔ¨Åcations to the standard ViT block design. The
designs of all blocks are shown in Figure 3.
‚Äì In-context conditioning. We simply append the vector embeddings of t and c as two additional tokens in
the input sequence, treating them no differently from
the image tokens. This is similar to cls tokens in
ViTs, and it allows us to use standard ViT blocks without modiÔ¨Åcation. After the Ô¨Ånal block, we remove the
conditioning tokens from the sequence. This approach
introduces negligible new GÔ¨Çops to the model.
Training Steps
XL/2 In-Context
XL/2 Cross-Attention
XL/2 adaLN
XL/2 adaLN-Zero
Figure 5. Comparing different conditioning strategies. adaLN-
Zero outperforms cross-attention and in-context conditioning at all
stages of training.
‚Äì Cross-attention block. We concatenate the embeddings
of t and c into a length-two sequence, separate from
the image token sequence. The transformer block is
modiÔ¨Åed to include an additional multi-head crossattention layer following the multi-head self-attention
block, similar to the original design from Vaswani et
al. , and also similar to the one used by LDM for
conditioning on class labels. Cross-attention adds the
most GÔ¨Çops to the model, roughly a 15% overhead.
‚Äì Adaptive layer norm (adaLN) block.
the widespread usage of adaptive normalization layers in GANs and diffusion models with U-
Net backbones , we explore replacing standard layer
norm layers in transformer blocks with adaptive layer
norm (adaLN). Rather than directly learn dimensionwise scale and shift parameters Œ≥ and Œ≤, we regress
them from the sum of the embedding vectors of t and
c. Of the three block designs we explore, adaLN adds
the least GÔ¨Çops and is thus the most compute-efÔ¨Åcient.
It is also the only conditioning mechanism that is restricted to apply the same function to all tokens.
‚Äì adaLN-Zero block. Prior work on ResNets has found
that initializing each residual block as the identity
function is beneÔ¨Åcial. For example, Goyal et al. found
that zero-initializing the Ô¨Ånal batch norm scale factor Œ≥
in each block accelerates large-scale training in the supervised learning setting . Diffusion U-Net models use a similar initialization strategy, zero-initializing
the Ô¨Ånal convolutional layer in each block prior to any
residual connections.
We explore a modiÔ¨Åcation of
the adaLN DiT block which does the same. In addition to regressing Œ≥ and Œ≤, we also regress dimensionwise scaling parameters Œ± that are applied immediately
prior to any residual connections within the DiT block.
Hidden size d
GÔ¨Çops (I=32, p=4)
Table 1. Details of DiT models. We follow ViT model con-
Ô¨Ågurations for the Small (S), Base (B) and Large (L) variants; we
also introduce an XLarge (XL) conÔ¨Åg as our largest model.
We initialize the MLP to output the zero-vector for all
Œ±; this initializes the full DiT block as the identity
function. As with the vanilla adaLN block, adaLN-
Zero adds negligible GÔ¨Çops to the model.
We include the in-context, cross-attention, adaptive layer
norm and adaLN-Zero blocks in the DiT design space.
Model size.
We apply a sequence of N DiT blocks, each
operating at the hidden dimension size d. Following ViT,
we use standard transformer conÔ¨Ågs that jointly scale N,
d and attention heads . SpeciÔ¨Åcally, we use four
conÔ¨Ågs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a
wide range of model sizes and Ô¨Çop allocations, from 0.3
to 118.6 GÔ¨Çops, allowing us to gauge scaling performance.
Table 1 gives details of the conÔ¨Ågs.
We add B, S, L and XL conÔ¨Ågs to the DiT design space.
Transformer decoder.
After the Ô¨Ånal DiT block, we need
to decode our sequence of image tokens into an output noise
prediction and an output diagonal covariance prediction.
Both of these outputs have shape equal to the original spatial input. We use a standard linear decoder to do this; we
apply the Ô¨Ånal layer norm (adaptive if using adaLN) and linearly decode each token into a p√óp√ó2C tensor, where C is
the number of channels in the spatial input to DiT. Finally,
we rearrange the decoded tokens into their original spatial
layout to get the predicted noise and covariance.
The complete DiT design space we explore is patch size,
transformer block architecture and model size.
4. Experimental Setup
We explore the DiT design space and study the scaling
properties of our model class. Our models are named according to their conÔ¨Ågs and latent patch sizes p; for example, DiT-XL/2 refers to the XLarge conÔ¨Åg and p = 2.
We train class-conditional latent DiT models at
256 √ó 256 and 512 √ó 512 image resolution on the ImageNet dataset , a highly-competitive generative modeling benchmark. We initialize the Ô¨Ånal linear layer with
zeros and otherwise use standard weight initialization techniques from ViT. We train all models with AdamW .
Figure 6. Scaling the DiT model improves FID at all stages of training. We show FID-50K over training iterations for 12 of our DiT
models. Top row: We compare FID holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the
transformer backbone yields better generative models across all model sizes and patch sizes.
We use a constant learning rate of 1 √ó 10‚àí4, no weight decay and a batch size of 256. The only data augmentation
we use is horizontal Ô¨Çips. Unlike much prior work with
ViTs , we did not Ô¨Ånd learning rate warmup nor
regularization necessary to train DiTs to high performance.
Even without these techniques, training was highly stable
across all model conÔ¨Ågs and we did not observe any loss
spikes commonly seen when training transformers. Following common practice in the generative modeling literature,
we maintain an exponential moving average (EMA) of DiT
weights over training with a decay of 0.9999. All results
reported use the EMA model. We use identical training hyperparameters across all DiT model sizes and patch sizes.
Our training hyperparameters are almost entirely retained
from ADM. We did not tune learning rates, decay/warm-up
schedules, Adam Œ≤1/Œ≤2 or weight decays.
Diffusion.
We use an off-the-shelf pre-trained variational
autoencoder (VAE) model from Stable Diffusion .
The VAE encoder has a downsample factor of 8‚Äîgiven an
RGB image x with shape 256 √ó 256 √ó 3, z = E(x) has
shape 32 √ó 32 √ó 4. Across all experiments in this section,
our diffusion models operate in this Z-space. After sampling a new latent from our diffusion model, we decode it
to pixels using the VAE decoder x = D(z). We retain diffusion hyperparameters from ADM ; speciÔ¨Åcally, we use a
tmax = 1000 linear variance schedule ranging from 1√ó10‚àí4
to 2 √ó 10‚àí2, ADM‚Äôs parameterization of the covariance Œ£Œ∏
and their method for embedding input timesteps and labels.
Evaluation metrics.
We measure scaling performance
with Fr¬¥echet Inception Distance (FID) , the standard
metric for evaluating generative models of images.
We follow convention when comparing against prior works
and report FID-50K using 250 DDPM sampling steps.
FID is known to be sensitive to small implementation details ; to ensure accurate comparisons, all values reported in this paper are obtained by exporting samples and
using ADM‚Äôs TensorFlow evaluation suite . FID numbers reported in this section do not use classiÔ¨Åer-free guidance except where otherwise stated. We additionally report
Inception Score , sFID and Precision/Recall 
as secondary metrics.
We implement all models in JAX and train
them using TPU-v3 pods. DiT-XL/2, our most computeintensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global batch size of 256.
5. Experiments
DiT block design.
We train four of our highest GÔ¨Çop
DiT-XL/2 models, each using a different block design‚Äî
in-context (119.4 GÔ¨Çops), cross-attention (137.6 GÔ¨Çops),
adaptive layer norm (adaLN, 118.6 GÔ¨Çops) or adaLN-zero
(118.6 GÔ¨Çops). We measure FID over the course of training.
Figure 5 shows the results. The adaLN-Zero block yields
lower FID than both cross-attention and in-context conditioning while being the most compute-efÔ¨Åcient. At 400K
training iterations, the FID achieved with the adaLN-Zero
model is nearly half that of the in-context model, demonstrating that the conditioning mechanism critically affects
model quality.
Initialization is also important‚ÄîadaLN-
Zero, which initializes each DiT block as the identity function, signiÔ¨Åcantly outperforms vanilla adaLN. For the rest
of the paper, all models will use adaLN-Zero DiT blocks.
Increasing transformer size
Decreasing patch size
Figure 7. Increasing transformer forward pass GÔ¨Çops increases sample quality. Best viewed zoomed-in. We sample from all 12 of
our DiT models after 400K training steps using the same input latent noise and class label. Increasing the GÔ¨Çops in the model‚Äîeither by
increasing transformer depth/width or increasing the number of input tokens‚Äîyields signiÔ¨Åcant improvements in visual Ô¨Ådelity.
Transformer Gflops
Correlation: -0.93
Figure 8. Transformer GÔ¨Çops are strongly correlated with FID.
We plot the GÔ¨Çops of each of our DiT models and each model‚Äôs
FID-50K after 400K training steps.
Scaling model size and patch size.
We train 12 DiT models, sweeping over model conÔ¨Ågs (S, B, L, XL) and patch
sizes (8, 4, 2). Note that DiT-L and DiT-XL are signiÔ¨Åcantly
closer to each other in terms of relative GÔ¨Çops than other
conÔ¨Ågs. Figure 2 (left) gives an overview of the GÔ¨Çops of
each model and their FID at 400K training iterations. In
all cases, we Ô¨Ånd that increasing model size and decreasing
patch size yields considerably improved diffusion models.
Figure 6 (top) demonstrates how FID changes as model
size is increased and patch size is held constant. Across all
four conÔ¨Ågs, signiÔ¨Åcant improvements in FID are obtained
over all stages of training by making the transformer deeper
and wider. Similarly, Figure 6 (bottom) shows FID as patch
size is decreased and model size is held constant. We again
observe considerable FID improvements throughout training by simply scaling the number of tokens processed by
DiT, holding parameters approximately Ô¨Åxed.
DiT GÔ¨Çops are critical to improving performance.
results of Figure 6 suggest that parameter counts do not
uniquely determine the quality of a DiT model. As model
size is held constant and patch size is decreased, the transformer‚Äôs total parameters are effectively unchanged (actually, total parameters slightly decrease), and only GÔ¨Çops are
increased. These results indicate that scaling model GÔ¨Çops
is actually the key to improved performance. To investigate this further, we plot the FID-50K at 400K training steps
against model GÔ¨Çops in Figure 8. The results demonstrate
that different DiT conÔ¨Ågs obtain similar FID values when
their total GÔ¨Çops are similar (e.g., DiT-S/2 and DiT-B/4).
We Ô¨Ånd a strong negative correlation between model GÔ¨Çops
and FID-50K, suggesting that additional model compute is
the critical ingredient for improved DiT models. In Figure 12 (appendix), we Ô¨Ånd that this trend holds for other
metrics such as Inception Score.
Training Compute (Gflops)
Larger DiT models use large compute more efÔ¨Åciently. We plot FID as a function of total training compute.
Larger DiT models are more compute-efÔ¨Åcient. In
Figure 9, we plot FID as a function of total training compute
for all DiT models. We estimate training compute as model
GÔ¨Çops ¬∑ batch size ¬∑ training steps ¬∑ 3, where the factor of
3 roughly approximates the backwards pass as being twice
as compute-heavy as the forward pass. We Ô¨Ånd that small
DiT models, even when trained longer, eventually become
compute-inefÔ¨Åcient relative to larger DiT models trained for
fewer steps. Similarly, we Ô¨Ånd that models that are identical except for patch size have different performance proÔ¨Åles
even when controlling for training GÔ¨Çops. For example,
XL/4 is outperformed by XL/2 after roughly 1010 GÔ¨Çops.
Visualizing scaling.
We visualize the effect of scaling on
sample quality in Figure 7. At 400K training steps, we sample an image from each of our 12 DiT models using identical starting noise xtmax, sampling noise and class labels.
This lets us visually interpret how scaling affects DiT sample quality. Indeed, scaling both model size and the number
of tokens yields notable improvements in visual quality.
5.1. State-of-the-Art Diffusion Models
256√ó256 ImageNet.
Following our scaling analysis, we
continue training our highest GÔ¨Çop model, DiT-XL/2, for
7M steps. We show samples from the model in Figures 1,
and we compare against state-of-the-art class-conditional
generative models. We report results in Table 2. When using classiÔ¨Åer-free guidance, DiT-XL/2 outperforms all prior
diffusion models, decreasing the previous best FID-50K of
3.60 achieved by LDM to 2.27. Figure 2 (right) shows that
DiT-XL/2 (118.6 GÔ¨Çops) is compute-efÔ¨Åcient relative to latent space U-Net models like LDM-4 (103.6 GÔ¨Çops) and
substantially more efÔ¨Åcient than pixel space U-Net models such as ADM (1120 GÔ¨Çops) or ADM-U (742 GÔ¨Çops).
Class-Conditional ImageNet 256√ó256
Precision‚Üë
BigGAN-deep 
StyleGAN-XL 
ADM-G, ADM-U
LDM-8 
LDM-4-G (cfg=1.25)
LDM-4-G (cfg=1.50)
DiT-XL/2-G (cfg=1.25)
DiT-XL/2-G (cfg=1.50)
Table 2. Benchmarking class-conditional image generation on
ImageNet 256√ó256. DiT-XL/2 achieves state-of-the-art FID.
Class-Conditional ImageNet 512√ó512
Precision‚Üë
BigGAN-deep 
StyleGAN-XL 
ADM-G, ADM-U
DiT-XL/2-G (cfg=1.25)
DiT-XL/2-G (cfg=1.50)
Table 3. Benchmarking class-conditional image generation on
ImageNet 512√ó512. Note that prior work measures Precision
and Recall using 1000 real samples for 512 √ó 512 resolution; for
consistency, we do the same.
Our method achieves the lowest FID of all prior generative
models, including the previous state-of-the-art StyleGAN-
XL . Finally, we also observe that DiT-XL/2 achieves
higher recall values at all tested classiÔ¨Åer-free guidance
scales compared to LDM-4 and LDM-8. When trained for
only 2.35M steps (similar to ADM), XL/2 still outperforms
all prior diffusion models with an FID of 2.55.
512√ó512 ImageNet.
We train a new DiT-XL/2 model on
ImageNet at 512 √ó 512 resolution for 3M iterations with
identical hyperparameters as the 256 √ó 256 model. With a
patch size of 2, this XL/2 model processes a total of 1024
tokens after patchifying the 64 √ó 64 √ó 4 input latent (524.6
GÔ¨Çops). Table 3 shows comparisons against state-of-the-art
methods. XL/2 again outperforms all prior diffusion models
at this resolution, improving the previous best FID of 3.85
achieved by ADM to 3.04. Even with the increased number of tokens, XL/2 remains compute-efÔ¨Åcient. For example, ADM uses 1983 GÔ¨Çops and ADM-U uses 2813 GÔ¨Çops;
XL/2 uses 524.6 GÔ¨Çops. We show samples from the highresolution XL/2 model in Figure 1 and the appendix.
Sampling Compute (Gflops)
Figure 10. Scaling-up sampling compute does not compensate
for a lack of model compute. For each of our DiT models trained
for 400K iterations, we compute FID-10K using sampling steps. For each number of steps, we plot the
FID as well as the GÔ¨Çops used to sample each image. Small models cannot close the performance gap with our large models, even
if they sample with more test-time GÔ¨Çops than the large models.
5.2. Scaling Model vs. Sampling Compute
Diffusion models are unique in that they can use additional compute after training by increasing the number of
sampling steps when generating an image. Given the impact of model GÔ¨Çops on sample quality, in this section we
study if smaller-model compute DiTs can outperform larger
ones by using more sampling compute. We compute FID
for all 12 of our DiT models after 400K training steps, using sampling steps per-image.
The main results are in Figure 10. Consider DiT-L/2 using 1000 sampling steps versus DiT-XL/2 using 128 steps.
In this case, L/2 uses 80.7 TÔ¨Çops to sample each image;
XL/2 uses 5√ó less compute‚Äî15.2 TÔ¨Çops‚Äîto sample each
image. Nonetheless, XL/2 has the better FID-10K (23.7
vs 25.9). In general, scaling-up sampling compute cannot
compensate for a lack of model compute.
6. Conclusion
We introduce Diffusion Transformers (DiTs), a simple
transformer-based backbone for diffusion models that outperforms prior U-Net models and inherits the excellent scaling properties of the transformer model class. Given the
promising scaling results in this paper, future work should
continue to scale DiTs to larger models and token counts.
DiT could also be explored as a drop-in backbone for textto-image models like DALL¬∑E 2 and Stable Diffusion.
Acknowledgements.
We thank Kaiming He, Ronghang
Hu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija
Radosavovic and Tete Xiao for helpful discussions. William
Peebles is supported by the NSF GRFP.