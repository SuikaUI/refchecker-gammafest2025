Proceedings of SemEval-2016, pages 52–57,
San Diego, California, June 16-17, 2016. c⃝2016 Association for Computational Linguistics
CUFE at SemEval-2016 Task 4: A Gated Recurrent Model for Sentiment
Classiﬁcation
Mahmoud Nabil1, Mohamed Aly2 and Amir F. Atiya3
1,2,3Computer Engineering, Cairo University, Egypt
2Visual Computing Center, KAUST, KSA
 
 
 
In this paper we describe a deep learning system that has been built for SemEval
2016 Task4 (Subtask A and B). In this work
we trained a Gated Recurrent Unit (GRU)
neural network model on top of two sets of
word embeddings: (a) general word embeddings generated from unsupervised neural language model; and (b) task speciﬁc word embeddings generated from supervised neural
language model that was trained to classify
tweets into positive and negative categories.
We also added a method for analyzing and
splitting multi-words hashtags and appending
them to the tweet body before feeding it to our
model. Our models achieved 0.58 F1-measure
for Subtask A (ranked 12/34) and 0.679 Recall
for Subtask B (ranked 12/19).
Introduction
Twitter is a huge microbloging service with more
than 500 million tweets per day1 from different locations in the world and in different languages. This
large, continuous, and dynamically updated content
is considered a valuable resource for researchers.
However many issues should be taken into account
while dealing with tweets, namely: (1) informal language used by the users; (2) spelling errors; (3) text
in the tweet may be referring to images, videos, or
external URLs; (4) emoticons; (5) hashtags used
(combining more than one word as a single word);
(6) usernames used to call or notify other users; (7)
1 
twitter-statistics/
spam or irrelevant tweets; and (8) character limit for
a tweet to 140 characters. This poses many challenges when analyzing tweets for natural language
processing tasks. In this paper we describe our system used for SemEval 2016 
Subtasks A and B. Subtask A (Message Polarity
Classiﬁcation) requires classifying a tweet’s sentiment as positive; negative; or neutral,. Subtask B
(Tweet classiﬁcation according to a two-point scale)
requires classifying a tweet’s sentiment given a topic
as positive or negative. Our system uses a GRU neural network model with one
hidden layer on top of two sets of word embeddings
that are slightly ﬁne-tuned on each training set (see
Fig. 1). The ﬁrst set of word embeddings is considered as general purpose embeddings and was obtained by training word2vec 
on 20.5 million tweets that we crawled for this purpose. The second set of word embeddings is considered as task speciﬁc set, and was obtained by training on a supervised sentiment analysis dataset using
another GRU model. We also added a method for
analyzing multi-words hashtags by splitting them
and appending them to the body of the tweet before
feeding it to the GRU model. In our experiments
we tried both keeping the word embeddings static
during the training or ﬁne-tuning them and reported
the result for each experiment. We achieved 0.58
F1-measure for Subtask A (ranked 12/34) and 0.679
Recall for Subtask B (ranked 12/19).
Related Work
A considerable amount of research has been done
to address the problem of sentiment analysis for
Figure 1: The architecture of the GRU deep Learning model
social content.
Nevertheless, most of the stateof-the-art systems still extensively depends on feature engineering, hand coded features, and linguistic resources. Recently, deep learning model gained
much attention in sentence text classiﬁcation inspired from computer vision and speech recognition
tasks. Indeed, two of the top four performing systems from SemEval 2015 used deep learning models. used a Convolution Neural Network (CNN) on top of skip-gram
model word embeddings trained on 50 million unsupervised tweets.
In the
author built a model that uses skip-gram word embeddings trained on 52 million unsupervised tweets
then they project these embeddings into a small
subspace, ﬁnally they used a non-linear model that
maps the embedding subspace to the classiﬁcation
space. In the author presented a series of CNN experiments for sentence classiﬁcation
where static and ﬁne-tuned word embeddings were
used. Also the author proposed an architecture modiﬁcation that allow the use of both task-speciﬁc and
static vectors. In the author proposed a recurrent convolutional neural network for
text classiﬁcation. Finally regarding feature engineering methods, the top
performing team in SemEval 2015, used an ensemble learning approach that averages the conﬁdence
scores of four classiﬁers. The model uses a large set
of linguistic resources and hand coded features.
System Description
Fig 1 shows the architecture of our deep learning
model. The core of our network is a GRU layer,
which we chose because (1) it is more computational
efﬁcient than Convolutional Neural Network (CNN)
models that we experimented with
but were much slower; (2) it can capture long semantic patterns without tuning the model parameter, unlike CNN models where the model depends
on the length of the convolutional feature maps for
capturing long patterns; (3) it achieved superior performance to CNNs in our experiments.
Our network architecture is composed of a word
embeddings layer, a merge layer, dropout layers, a
GRU layer, a hyperbolic tangent tanh layer, and a
soft-max classiﬁcation layer. In the following we
give a brief description of the main components of
the architecture.
Embedding Layer
This is the ﬁrst layer in the network where each
tweet is treated as a sequence of words w1, w2...wS
of length S, where S is the maximum tweet length.
We set S to 40 as the length of any tweet is limited
to 140 character. We used zero padding while dealing with short tweets. Each word wi is represented
by two embedding vectors wi1, wi2∈Rd where d is
the embedding dimension, and according to setting d to 200 is a good choice
with respect to the performance and the computation efﬁciency. wi1 is considered a general-purpose
embedding vector while wi2 is considered a task-
speciﬁc embedding vector. We performed the following steps to initialize both types of word embeddings:
1. For the general word embeddings we collected
about 40M tweets using twitter streaming API
over a period of two month . We used three criteria while collecting
the tweets: (a) they contain at least one emoticon in a set of happy and sad emoticons like
’:)’ ,’: ; (b) hash
tags collected from SemEval 2016 data set; (c)
hash tags collected from SemEval 2013 data
set. After preparing the tweets as described in
Section 4 and removing retweets we ended up
with about 19 million tweet. We also appended
1.5 million tweets from Sentiment140 corpus after preparation so we end up
with about 20.5 million tweet. To train the general embeddings we used word2vec neural language model skipgram
model with window size 5, negative sampling
and ﬁltered out words with frequency less than
2. For the task speciﬁc word embeddings we used
semi-supervised 1.5 million tweets from sentiment140 corpus, where each tweet is tagged
either positive or negative according to the
tweet’s sentiment . Then we applied another
GRU model similar to Fig 1 with a modiﬁcation to the soft-max layer for the purpose of the
two classes classiﬁcation and with random initialized embeddings that are ﬁne-tuned during
the training. We used the resulting ﬁne-tuned
embeddings as task-speciﬁc since they contain
contextual semantic meaning from the training
Merge Layer
The purpose of this layer is to concatenate the two
types of word embeddings used in the previous layer
in order to form a sequence of length 2S that can be
used in the following GRU layer.
Dropout Layers
The purpose of this layer is to prevent the previous
layer from overﬁtting where
some units are randomly dropped during training so
the regularization of these units is improved.
This is the core layer in our model which takes an input sequence of length 2S words each having dimension d (i.e. input dimension is 2Sd) . The gated recurrent network proposed in 
is a recurrent neural network )
where the activation hj
t of the neural unit j at time t
is a linear interpolation between the previous activation hj
t−1 at time t −1 and the candidate activation
t :
t is the update gate that determines how
much the unit updates its content, and ˜hj
newly computed candidate state.
Tanh Layer
The purpose of this layer is to allow the neural network to make complex decisions by learning nonlinear classiﬁcation boundaries. Although the tanh
function takes more training time than the Rectiﬁed
Linear Units (ReLU), tanh gives more accurate results in our experiments.
Soft-Max Layer
This is last layer in our network where the output of
the tanh layer is fed to a fully connected soft-max
layer. This layer calculates the classes probability
distribution.
P(y = c | x, b) =
where c is the target class, x is the output from the
previous layer, wk and bk are the weight and the bias
of class k, and K is the total number of classes. The
difference between the architecture used for Subtask
A and Subtask B is in this layer, where for Subtask
A three neurons were used (i.e. K = 3) while for
Subtask B only two neurons were used (i.e. K = 2).
Data Preparation
All the data used either for training the word embeddings or for training the sentiment classiﬁcation
model undergoes the following preprocessing steps:
Normalization
@user1,@user2
Happy emotions
:), :-), :=)
Sad emotions
:( , :-(, :=(
Laugh emotions
:D, :-D, :=D
Kiss emotions
:-*, :*, :-)*
Surprise emotions
Tongue emotions
www.google.com
Topic (Subtask B only)
Table 1: Normalization Patterns
1. Using NLTK twitter tokenizer2 to tokenize
each tweet.
2. Using hand-coded tokenization regex to split
the following sufﬁxes: ’s, ’ve, ’t , ’re, ’d, ’ll.
3. Using the patterns described in Table 1 to normalize each tweet.
4. Adding StartToken and EndToken at
the beginning and the ending of each tweet.
5. Splitting multi-word hashtags as explained below.
Consider the following tweet “Thinking of reverting back to 8.1 or 7. #Windows10Fail”. The sentiment of the tweet is clearly negative and the simplest way to give the correct tag is by looking at the
word “Fail“ in the hashtag “#Windows10Fail”. For
this reason we added a depth ﬁrst search dictionary
method in order to infer the location of spaces inside
each hashtag in the tweet and append the result tokens to the tweet’s end. We used 125k words dictionary3 collected from Wikipedia. In the given example, we ﬁrst lower the hashtag case, remove numbers
and underscores from the hashtag then we apply our
method to split the hashtag this results in two tokens
“windows” and “fail”. Hence, we append these two
tokens to the end of the tweet and the normal preparation steps continue. After the preparation the tweet
will look like “ StartToken Thinking of reverting
back to NUM or NUM . #Windows10Fail. windows fail EndToken ”.
2 
3 
Table 2: Tweets distribution for Subtask A and B
GRU-static
GRU-ﬁne-tuned
GRU-ﬁne-tuned + Split Hashtag
Table 3: Development results for Subtask A and B. Note: average F1-mesure for positive and negative classes is used for
Subtask A, while the average recall is used for Subtask B.
Experiments
In order to train and test our model for Subtask A, we
used the dataset provided for SemEval-2016 Task 4
and SemEval-2013 Task 2. We obtained 8,978 from
the ﬁrst dataset and 7,130 from the second, the remaining tweets were not available. So, we ended up
with a dataset of 16,108 tweets. Regarding Subtask
B we obtained 6,324 from SemEval-2016 provided
dataset. We partitioned both datasets into train and
development portions of ratio 8:2. Table 2 shows the
distribution of tweets for both Subtasks.
For optimizing our network weights we used
Adam , a new and computationally efﬁcient stochastic optimization method.
All the experiments have been developed using
Keras4 deep learning library with Theano5 backend
and with CUDA enabled. The model was trained
using the default parameters for Adam optimizer,
and we tried either to keep the weights of embedding layer static or slightly ﬁne-tune them by using
a dropout probability equal to 0.9. Table 3 shows
our results on the development part of the data set
for Subtask A and B where we report the ofﬁcial
performance measure for both subtasks .
From 3 the results it is shown that
ﬁne-tuning word embeddings with hashtags splitting
gives the best results on the development set. All our
experiments were performed on a machine with Intel Core i7-4770 CPU @ 3.40GHz (8 cores), 16GB
4 
5 
F-measure (Old)
F-measure (New)
Tweet-2013
Tweet-2014
Tweet-sarcasm
Live-Journal
Tweet-2015
Tweet-2016
Table 4: Results for Subtask A on different SemEval datasets.
Recall (Old)
Recall (New)
Tweet-2016
Table 5: Result for Subtask B on SemEval 2016 dataset.
of RAM and GeForce GT 640 GPU. Table 4 shows
our individual results on different SemEval datasets.
Table 5 shows our results for Subtask B. From the results and our rank in both Subtasks, we noticed that
our system was not satisfactory compared to other
teams this was due to the following reasons:
1. We used the development set to validate our
model in order to ﬁnd the best learning parameters, However we mistakenly used the learning accuracy to ﬁnd the optimal learning parameters especially the number of the training
This signiﬁcantly affected our rank
based on the ofﬁcial performance measure. Table 4 and Table 5 show the old and the new results after ﬁxing this bug.
2. Most of the participating teams in this year
competition used deep learning models and
they used huge datasets (more than 50M
tweets) to train and reﬁne word embeddings according to the emotions of the tweet. However,
we only used 1.5M from sentiment140 corpus
to generate task-speciﬁc embeddings.
3. The model used for generating the task-speciﬁc
embeddings for Subtask A should be trained on
three classes not only two (positive, negative,
and neutral) where if the tweet contains positive emotions like “:)” should be positive, if
it contains negative emotions like “:(“ should
be negative, and if it contains both or none it
should be neutral.
Conclusion
In this paper, we presented our deep learning model
used for SemEval2016 Task4 (Subtasks A and B).
The model uses a gated recurrent layer as a core
layer on top of two types of word embeddings
(general-purpose and task-speciﬁc).
Also we described our steps in generating both types word embeddings and how we prepared the dataset used especially when dealing with multi-words hashtags.
The system ranked 12th on Subtask A and 12th for
Subtask B.
Acknowledgments
This work has been funded by ITIDA’s ITAC project
number CFP65.