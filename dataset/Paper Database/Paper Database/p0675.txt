IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Diffusion Models in Vision: A Survey
Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Member, IEEE, and Mubarak Shah, Fellow, IEEE
Abstract—Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the
area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a
reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian
noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion
process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their
known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a
comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising
diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations
between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks,
energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion
models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting
directions for future research.
Index Terms—diffusion models, denoising diffusion models, noise conditioned score networks, score-based models, image
generation, deep generative modeling.
INTRODUCTION
IFFUSION models – form a category of deep generative models which has recently become one of the
hottest topics in computer vision (see Figure 1), showcasing
impressive generative capabilities, ranging from the high
level of details to the diversity of the generated examples.
We can even go as far as stating that these generative
models raised the bar to a new level in the area of generative modeling, particularly referring to models such as
Imagen and Latent Diffusion Models (LDMs) . This
statement is confirmed by the image samples illustrated
in Figure 2, which are generated by Stable Diffusion, a
version of LDMs that generates images based on text
prompts. The generated images exhibit very few artifacts
and are very well aligned with the text prompts. Notably,
the prompts are purposely chosen to represent unrealistic
scenarios (never seen at training time), thus demonstrating
the high generalization capacity of diffusion models.
To date, diffusion models have been applied to a wide
variety of generative modeling tasks, such as image generation – , , , – , image super-resolution ,
 , – , image inpainting , , , , , ,
 – , image editing – , image-to-image translation , – , among others. Moreover, the latent representation learned by diffusion models was also found to
be useful in discriminative tasks, e.g. image segmentation
F.A. Croitoru, V. Hondru and R.T. Ionescu are with the Department of
Computer Science, University of Bucharest, Bucharest, Romania. F.A.
Croitoru and V. Hondru have contributed equally. R.T. Ionescu is the
corresponding author.
E-mail: 
M. Shah is with the Center for Research in Computer Vision (CRCV),
Department of Computer Science, University of Central Florida, Orlando,
FL, 32816.
Manuscript received April 19, 2022; revised August 26, 2022.
2015 2016 2017 2018 2019 2020 2021 2022
Number of papers
Fig. 1. The rough number of papers on diffusion models per year.
 – , classification and anomaly detection – .
This confirms the broad applicability of denoising diffusion
models, indicating that further applications are yet to be
discovered. Additionally, the ability to learn strong latent
representations creates a connection to representation learning , , a comprehensive domain that studies ways
to learn powerful data representations, covering multiple
approaches ranging from the design of novel neural architectures – to the development of learning strategies
 – .
According to the graph shown in Figure 1, the number
of papers on diffusion models is growing at a very fast pace.
To outline the past and current achievements of this rapidly
developing topic, we present a comprehensive review of
articles on denoising diffusion models in computer vision.
More precisely, we survey articles that fall in the category of
generative models defined below. Diffusion models represent
a category of deep generative models that are based on (i) a
forward diffusion stage, in which the input data is gradually
perturbed over several steps by adding Gaussian noise,
and (ii) a reverse (backward) diffusion stage, in which a
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
A bunny reading his
e-mail on a computer.
A crocodile fishing on a boat
while reading a paper.
A bear astronaut
playing tennis.
A stone rabbit statue
sitting on the moon.
A diffusion model
generating an image.
A gummy bear riding
a bike at the beach.
A green cow eating red
grass during winter.
A Bichon Maltese and a black
bunny playing backgammon.
Two people playing
chess on Mars.
A pig with wings flying
over a rainbow.
A blue car covered in fur in
front of a rainbow house.
A panda bear
eating pasta.
A tree with all kinds
of fruits.
A wombat with sunglasses
at the swimming pool.
A red boat flying upside
down in the rain.
A Bichon Maltese reading
a book on a flight.
An astronaut walking a
crocodile in a park.
A monkey with a white
hat playing the piano.
Fig. 2. Images generated by Stable Diffusion based on various text prompts, via the platform.
generative model is tasked at recovering the original input
data from the diffused (noisy) data by learning to gradually
reverse the diffusion process, step by step.
We underline that there are at least three sub-categories
of diffusion models that comply with the above definition.
The first sub-category comprises denoising diffusion probabilistic models (DDPMs) , , which are inspired by the
non-equilibrium thermodynamics theory. DDPMs are latent
variable models that employ latent variables to estimate the
probability distribution. From this point of view, DDPMs
can be viewed as a special kind of variational auto-encoders
(VAEs) , where the forward diffusion stage corresponds
to the encoding process inside VAE, while the reverse diffusion stage corresponds to the decoding process. The second
sub-category is represented by noise conditioned score networks (NCSNs) , which are based on training a shared
neural network via score matching to estimate the score
function (defined as the gradient of the log density) of the
perturbed data distribution at different noise levels. Stochastic differential equations (SDEs) represent an alternative
way to model diffusion, forming the third sub-category
of diffusion models. Modeling diffusion via forward and
reverse SDEs leads to efficient generation strategies as well
as strong theoretical results . This latter formulation
(based on SDEs) can be viewed as a generalization over
DDPMs and NCSNs.
We identify several defining design choices and synthesize them into three generic diffusion modeling frameworks
corresponding to the three sub-categories introduced above.
To put the generic diffusion modeling framework into context, we further discuss the relations between diffusion
models and other deep generative models. More specifically, we describe the relations to variational auto-encoders
(VAEs) , generative adversarial networks (GANs) ,
energy-based models (EBMs) , , autoregressive models and normalizing flows , . Then, we introduce
a multi-perspective categorization of diffusion models applied in computer vision, classifying the existing models
based on several criteria, such as the underlying framework, the target task, or the denoising condition. Finally,
we illustrate the current limitations of diffusion models and
envision some interesting directions for future research. For
example, perhaps one of the most problematic limitations
is the poor time efficiency during inference, which is caused
by a very high number of evaluation steps, e.g. thousands, to
generate a sample . Naturally, overcoming this limitation
without compromising the quality of the generated samples
represents an important direction for future research.
In summary, our contribution is twofold:
Since many contributions based on diffusion models
have recently emerged in vision, we provide a comprehensive and timely literature review of denoising
diffusion models applied in computer vision, aiming
to provide a fast understanding of the generic diffusion modeling framework to our readers.
We devise a multi-perspective categorization of diffusion models, aiming to help other researchers
working on diffusion models applied to a specific
domain in quickly finding relevant related works in
the respective domain.
GENERIC FRAMEWORK
Diffusion models are a class of probabilistic generative models that learn to reverse a process that gradually degrades
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
!" = $ ", & !& + ((&)!+
1 −.! / "!"# +
.! / 0!, 0! ~ 2(0, 4)
Forward SDE
"! = "!"# +
/ 0!, 0! ~ 2(0, 4)
Reverse SDE
Annealed Langevin dynamics
!" = $ ", & −( & $ / ∇%log 9!(") !& + ((&)! :+
"!"# = ;&("!, &) +
.! / 0!, 0! ~ 2(0, 4)
Fig. 3. A generic framework composing three alternative formulations of diffusion models based on: stochastic differential equations (SDEs),
denoising diffusion probabilistic models (DDPMs) and noise conditioned score networks (NCSNs). In general, a diffusion model consists of two
processes. The first one, called the forward process, transforms data into noise, while the second one is a generative process that reverses the
effect of the forward process. This latter process learns to transform the noise back into data. We illustrate these processes for all three formulations.
The forward SDE shows that a change over time in x is modeled by a function f plus a stochastic component ∂ω ∼N(0, ∂t) scaled by σ(t). We
underline that different choices of f and σ will lead to different diffusion processes. This is why the SDE formulation is a generalization of the other
two. The reverse (generative) SDE shows how to change x in order to recover the data from pure noise. We keep the random component and modify
the deterministic one using the gradients of the log probability ∇x log pt(x), so that x moves to regions where the data density p(x) is high. DDPMs
sample the data points during the forward process from a normal distribution N
 xt;√1−βt·xt−1, βt·I
, where βt ≪1. This iterative sampling
slowly destroys information in data, and replaces it with Gaussian noise. The sampling is illustrated via the reparametrization trick (see details in
Section 2.1). The reverse process of DDPM also performs iterative sampling from a normal distribution, but the mean µθ(xt, t) of the distribution
is derived by subtracting the noise, estimated by a neural network, from the image at the previous step xt. The variance is equal to the one used
in the forward process. The initial image going into the reverse process contains only Gaussian noise. The forward process of NCSN simply adds
normal noise to the image at the previous step. This can also be seen as sampling from a normal distribution N(xt; xt−1, (σ2
t−1)) · I), with the
mean being the image at the previous step. The reverse process of NCSN is based on an algorithm described in Section 2.2. Best viewed in color.
the training data structure. Thus, the training procedure
involves two phases: the forward diffusion process and the
backward denoising process.
The former phase consists of multiple steps in which
low-level noise is added to each input image, where the
scale of the noise varies at each step. The training data is
progressively destroyed until it results in pure Gaussian
The latter phase is represented by reversing the forward diffusion process. The same iterative procedure is employed, but backwards: the noise is sequentially removed,
and hence, the original image is recreated. Therefore, at
inference time, images are generated by gradually reconstructing them starting from random white noise. The noise
subtracted at each time step is estimated via a neural network, typically based on a U-Net architecture , allowing
the preservation of dimensions.
In the following three subsections, we present three formulations of diffusion models, namely denoising diffusion
probabilistic models, noise conditioned score networks, and
the approach based on stochastic differential equations that
generalizes over the first two methods. For each formulation, we describe the process of adding noise to the data,
the method which learns to reverse this process, and how
new samples are generated at inference time. In Figure 3, all
three formulations are illustrated as a generic framework.
We dedicate the last subsection to discussing connections to
other deep generative models.
Denoising Diffusion Probabilistic Models (DDPMs)
Forward process. DDPMs , slowly corrupt the training
data using Gaussian noise. Let p(x0) be the data density,
where the index 0 denotes the fact that the data is uncorrupted (original). Given an uncorrupted training sample
x0 ∼p(x0), the noised versions x1, x2 . . . , xT are obtained
according to the following Markovian process:
p(xt|xt−1)=N
1−βt·xt−1, βt·I
, ∀t∈{1, . . . , T}, (1)
where T is the number of diffusion steps, β1, . . . , βT ∈[0, 1)
are hyperparameters representing the variance schedule
across diffusion steps, I is the identity matrix having the
same dimensions as the input image x0, and N(x; µ, σ)
represents the normal distribution of mean µ and covariance σ that produces x. An important property of this
recursive formulation is that it also allows the direct sampling of xt, when t is drawn from a uniform distribution,
i.e. ∀t ∼U({1, . . . , T}):
p(xt|x0) = N
ˆβt · x0, (1 −ˆβt) · I
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
where ˆβt = Qt
i=1 αi and αt = 1 −βt. Essentially, Eq. (2)
shows that we can sample any noisy version xt via a single
step, if we have the original image x0 and fix a variance
schedule βt.
reparametrization trick. In general, to standardize a sample
x of a normal distribution x ∼N(µ, σ2 · I), we subtract the
mean µ and divide by the standard deviation σ, resulting
in a sample z =
of the standard normal distribution
z ∼N(0, I). The reparametrization trick does the inverse of
this operation, starting with z and yielding the sample x by
multiplying z with the standard deviation σ and adding the
mean µ. If we translate this process to our case, then xt is
sampled from p(xt|x0) as follows:
ˆβt · x0 +
(1 −ˆβt) · zt,
where zt ∼N(0, I).
Properties of βt. If the variance schedule (βt)T
t=1 is chosen
such that ˆβT →0, then, according to Eq. (2), the distribution of xT should be well approximated by the standard
Gaussian distribution π(xT ) = N(0, I). Moreover, if each
t=1 ≪1, then the reverse steps p(xt−1|xt) have the
same functional form as the forward process p(xt|xt−1) ,
 . Intuitively, the last statement is true when xt is created
with a very small step, as it becomes more likely that xt−1
comes from a region close to where xt is observed, which
allows us to model this region with a Gaussian distribution.
To conform to the aforementioned properties, Ho et al. 
choose (βt)T
t=1 to be linearly increasing constants between
β1 = 10−4 and βT = 2 · 10−2, where T = 1000.
Reverse process. By leveraging the above properties, we
can generate new samples from p(x0) if we start from
a sample xT
∼N(0, I) and follow the reverse steps
p(xt−1|xt) = N(xt−1; µ(xt, t), Σ(xt, t)). To approximate
these steps, we can train a neural network pθ(xt−1|xt) =
N(xt−1; µθ(xt, t), Σθ(xt, t)) that receives as input the noisy
image xt and the embedding at time step t, and learns to
predict the mean µθ(xt, t) and the covariance Σθ(xt, t).
In an ideal scenario, we would train the neural network
with a maximum likelihood objective such that the probability assigned by the model pθ(x0) to each training example
x0 is as large as possible. However, pθ(x0) is intractable
because we have to marginalize over all the possible reverse
trajectories to compute it. The solution to this problem ,
 is to minimize a variational lower-bound of the negative
log-likelihood instead, which has the following formulation:
Lvlb = −log pθ(x0|x1) + KL (p(xT |x0)∥π(xT ))
KL(p(xt−1|xt, x0)∥pθ(xt−1|xt)),
where KL denotes the Kullback-Leibler divergence between
two probability distributions. The full derivation of this
objective is presented in Appendix A. Upon analyzing each
component, we can see that the second term can be removed
because it does not depend on θ. The last term shows that
the neural network is trained such that, at each time step t,
pθ(xt−1|xt) is as close as possible to the true posterior of the
forward process when conditioned on the original image.
Moreover, it can be proven that the posterior p(xt−1|xt, x0)
is a Gaussian distribution, implying closed-form expressions for the KL divergences.
Algorithm 1 DDPM sampling method
T – the number of diffusion steps.
σ1, . . . , σT – the standard deviations for the reverse transitions.
x0 – the sampled image.
Computation:
1: xT ∼N(0, I)
2: for t = T, . . . , 1 do
if t > 1 then
z ∼N(0, I)
βt · zθ(xt, t)
xt−1 = µθ + σt · z
Ho et al. propose to fix the covariance Σθ(xt, t) to a
constant value and rewrite the mean µθ(xt, t) as a function
of noise, as follows:
xt −1 −αt
· zθ(xt, t)
These simplifications (more details in Appendix B) unlocked
a new formulation of the objective Lvlb, which measures, for
a random time step t of the forward process, the distance
between the real noise zt and the noise estimation zθ(xt, t)
of the model:
Lsimple = Et∼[1,T ]Ex0∼p(x0)Ezt∼N(0,I)∥zt −zθ(xt, t)∥2 , (6)
where E is the expected value, and zθ(xt, t) is the network
predicting the noise in xt. We underline that xt is sampled
via Eq. (3), where we use a random image x0 from the
training set.
The generative process is still defined by pθ(xt−1|xt),
but the neural network does not predict the mean and the
covariance directly. Instead, it is trained to predict the noise
from the image, and the mean is determined according to
Eq. (5), while the covariance is fixed to a constant. Algorithm
1 formalizes the whole generative procedure.
Noise Conditioned Score Networks (NCSNs)
The score function of some data density p(x) is defined as
the gradient of the log density with respect to the input,
∇x log p(x). The directions given by these gradients are
used by the Langevin dynamics algorithm to move from
a random sample (x0) towards samples (xN) in regions
with high density. Langevin dynamics is an iterative method
inspired from physics that can be used for data sampling.
In physics, this method is used to determine the trajectory
of a particle in a molecular system that allows interactions
between the particle and the other molecules. The trajectory
of the particle is influenced by a drag force of the system
and by a random force motivated by the fast interactions
between the molecules. In our case, we can think of the
gradient of the log density as a force that drags a random
sample through the data space into regions with high data
density p(x). There is another term ωi that accounts, in
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
physics, for the random force, but for us, it is useful to
escape local minima. Lastly, a value denoted by γ weighs
the impact of both forces, because it represents the friction
coefficient of the environment where the particle resides.
From the sampling point of view, γ controls the magnitude
of the updates. In summary, the iterative updates of the
Langevin dynamics are the following:
xi = xi−1 + γ
2 ∇x log p(x) + √γ · ωi,
where i ∈{1, . . . , N}, γ controls the magnitude of the
update in the direction of the score, x0 is sampled from a
prior distribution, the noise ωi ∼N(0, I) addresses the issue
of getting stuck in local minima, and the method is applied
recursively for N →∞steps. Therefore, a generative model
can employ the above method to sample from p(x) after estimating the score with a neural network sθ(x) ≈∇x log p(x).
This network can be trained via score matching, a method
that requires the optimization of the following objective:
Lsm = Ex∼p(x) ∥sθ(x) −∇x log p(x)∥2
In practice, it is impossible to minimize this objective directly, because ∇x log p(x) is unknown. However, there are
other methods such as denoising score matching and
sliced score matching that overcome this problem.
Although the described approach can be used for data
generation, Song et al. emphasize several issues when
applying this method on real data. Most of the problems are
linked with the manifold hypothesis. For example, the score
estimation sθ(x) is inconsistent when the data resides on a
low-dimensional manifold and, among other implications,
this could cause the Langevin dynamics to never converge
to the high-density regions. In the same work , the authors demonstrate that these problems can be addressed by
perturbing the data with Gaussian noise at different scales.
Furthermore, they propose to learn score estimates for the
resulting noisy distributions via a single noise conditioned
score network (NCSN). Regarding the sampling, they adapt
the strategy in Eq. (7) and use the score estimates associated
with each noise scale.
Formally, given a sequence of Gaussian noise scales
σ1 < σ2 < · · · < σT such that pσ1(x) ≈p(x0) and
pσT (x) ≈N(0, I), we can train an NCSN sθ(x, σt) with denoising score matching so that sθ(x, σt) ≈∇x log(pσt(x)),
∀t ∈{1, . . . , T}. We can derive ∇x log(pσt(x)) as follows:
∇xt log pσt(xt|x) = −xt −x
given that:
pσt(xt|x) = N(xt; x, σ2
where xt is a noised version of x, and exp is the exponential
function. Consequently, generalizing Eq. (8) for all (σt)T
and replacing the gradient with the form in Eq. (9) leads
to training sθ(xt, σt) by minimizing the following objective,
∀t ∈{1, . . . , T}:
λ(σt)Ep(x)Ext∼pσt(xt|x)
sθ(xt, σt)+ xt −x
Algorithm 2 Annealed Langevin dynamics
σ1, . . . , σT – a sequence of Gaussian noise scales.
N – the number of Langevin dynamics iterations.
γ1, . . . , γT – the update magnitudes for each noise scale.
0 – the sampled image.
Computation:
T ∼N(0, I)
2: for t = T, . . . , 1 do
for i = 1, . . . , N do
ω ∼N(0, I)
2 · sθ(xi−1
, σt) + √γt · ω
where λ(σt) is a weighting function. After training, the
neural network sθ(xt, σt) will return estimates of the scores
∇xt log(pσt(xt)), having as input the noisy image xt and the
corresponding time step t.
At inference time, Song et al. introduce the annealed
Langevin dynamics, formally described in Algorithm 2.
Their method starts with white noise and applies Eq. (7) for
a fixed number of iterations. The required gradient (score)
is given by the trained neural network conditioned on the
time step T. The process continues for the following time
steps, propagating the output of one step as input to the
next. The final sample is the output returned for t = 0.
Stochastic Differential Equations (SDEs)
Similar to the previous two methods, the approach presented in gradually transforms the data distribution
p(x0) into noise. However, it generalizes over the previous
two methods because, in its case, the diffusion process being
considered to be continuous, thus becoming the solution of
a stochastic differential equation (SDE). As shown in ,
the reverse process of this diffusion can be modeled with
a reverse-time SDE which requires the score function of the
density at each time step. Therefore, the generative model of
Song et al. employs a neural network to estimate the score
functions, and generates samples from p(x0) by employing
numerical SDE solvers. As in the case of NCSNs, the neural
network receives the perturbed data and the time step as
input, and produces an estimation of the score function.
The SDE of the forward diffusion process (xt)T
[0, T] has the following form:
∂t =f(x, t)+σ(t)·ωt ⇐⇒∂x=f(x, t)·∂t+σ(t)·∂ω, (12)
where ωt is Gaussian noise, f is a function of x and t that
computes the drift coefficient, and σ is a time-dependent
function that computes the diffusion coefficient. In order
to have a diffusion process as a solution for this SDE, the
drift coefficient should be designed such that it gradually
nullifies the data x0, while the diffusion coefficient controls
how much Gaussian noise is added. The associated reversetime SDE is defined as follows:
f(x, t) −σ(t)2 · ∇x log pt(x)
 · ∂t + σ(t) · ∂ˆω, (13)
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Algorithm 3 Euler-Maruyama sampling method
∆t < 0 – a negative step close to 0.
f – a function of x and t that computes the drift coefficient.
σ – a time-dependent function that computes the diffusion
coefficient.
∇x log pt(x) – the (approximated) score function.
T – the final time step of the forward SDE.
x – the sampled image.
Computation:
2: while t > 0 do
f(x, t) −σ(t)2·∇x log pt(x)
·∆t + σ(t)·∆ˆω
x = x + ∆x
t = t + ∆t
where ˆω represents the Brownian motion when the time is
reversed, from T to 0. The reverse-time SDE shows that,
if we start with pure noise, we can recover the data by
removing the drift responsible for data destruction. The
removal is performed by subtracting σ(t)2 · ∇x log pt(x).
We can train the neural network sθ(x, t) ≈∇x log pt(x)
by optimizing the same objective as in Eq. (11), but adapted
for the continuous case, as follows:
λ(t)Ep(x0)Ept(xt|x0)∥sθ(xt, t)−∇xt log pt(xt|x0)∥2
where λ is a weighting function, and t ∼U([0, T]). We underline that, when the drift coefficient f is affine, pt(xt|x0)
is a Gaussian distribution. When f does not conform to this
property, we cannot use denoising score matching, but we
can fallback to sliced score matching .
The sampling for this approach can be performed with
any numerical method applied on the SDE defined in
Eq. (13). In practice, the solvers do not work with the
continuous formulation. For example, the Euler-Maruyama
method fixes a tiny negative step ∆t and executes Algorithm 3 until the initial time step t = T becomes t = 0. At
step 3, the Brownian motion is given by ∆ˆω =
where z ∼N(0, I).
Song et al. present several contributions in terms
of sampling techniques. They introduce the Predictor-
Corrector sampler which generates better examples. This
algorithm first employs a numerical method to sample from
the reverse-time SDE, and then uses a score-based method
as a corrector, for example the annealed Langevin dynamics
described in the previous subsection. Furthermore, they
show that ordinary differential equations (ODEs) can also be
used to model the reverse process. Hence, another sampling
strategy unlocked by the SDE interpretation is based on
numerical methods applied to ODEs. The main advantage
of this latter strategy is its efficiency.
Relation to Other Generative Models
We discuss below the connections between diffusion models and other types of generative models. We start with
likelihood-based methods and finish with generative adversarial networks.
Diffusion models have more aspects in common with
VAEs . For instance, in both cases, the data is mapped to
a latent space and the generative process learns to transform
the latent representations into data. Moreover, in both situations, the objective function can be derived as a lower-bound
of the data likelihood. Nevertheless, there are essential differences between the two approaches and, further, we will
mention some of them. The latent representation of a VAE
contains compressed information about the original image,
while diffusion models destroy the data entirely after the
last step of the forward process. The latent representations of
diffusion models have the same dimensions as the original
data, while VAEs work better when the dimensions are
reduced. Ultimately, the mapping to the latent space of a
VAE is trainable, which is not true for the forward process
of diffusion models because, as stated before, the latent is
obtained by gradually adding Gaussian noise to the original
image. The aforementioned similarities and differences can
be the key for future developments of the two methods. For
example, there already exists some work that builds more
efficient diffusion models by applying them on the latent
space of a VAE , .
Autoregressive models , represent images as
sequences of pixels. Their generative process produces new
samples by generating an image pixel by pixel, conditioned
on the previously generated pixels. This approach implies
a unidirectional bias that clearly represents a limitation of
this class of generative models. Esser et al. see diffusion
and autoregressive models as complementary and solve the
above issue. Their method learns to reverse a multinomial
diffusion process via a Markov chain where each transition
is implemented as an autoregressive model. The global
information provided to the autoregressive model is given
by the previous step of the Markov chain.
Normalizing flows , are a class of generative
models that transform a simple Gaussian distribution into
a complex data distribution. The transformation is done via
a set of invertible functions which have an easy-to-compute
Jacobian determinant. These conditions translate in practice
into architectural restrictions. An important feature of this
type of model is that the likelihood is tractable. Hence, the
objective for training is the negative log-likelihood. When
comparing with diffusion models, the two types of models
have in common the mapping of the data distribution to
Gaussian noise. However, the similarities between the two
methods end here, because normalizing flows perform the
mapping in a deterministic fashion by learning an invertible and differentiable function. These properties imply, in
contrast to diffusion models, additional constraints on the
network architecture, and a learnable forward process. A
method which connects these two generative algorithms is
DiffFlow. Introduced in , DiffFlow extends both diffusion models and normalizing flows such that the reverse
and forward processes are both trainable and stochastic.
Energy-based models (EBMs) , , , focus
on providing estimates of unnormalized versions of density
functions, called energy functions. Thanks to this property
and in contrast to the previous likelihood-based methods,
this type of model can be represented with any regression
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
neural network. However, due to this flexibility, the training
of EBMs is difficult. One popular training strategy used in
practice is score matching , . Regarding the sampling, among other strategies, there is the Markov Chain
Monte Carlo (MCMC) method, which is based on the score
function. Therefore, the formulation from Subsection 2.2 of
diffusion models can be considered to be a particular case
of the energy-based framework, precisely the case when the
training and sampling only require the score function.
GANs were considered by many as state-of-the-art
generative models in terms of the quality of the generated
samples, before the recent rise of diffusion models . GANs
are also known as being difficult to train due to their adversarial objective , and often suffer from mode collapse.
In contrast, diffusion models have a stable training process
and provide more diversity because they are likelihoodbased. Despite these advantages, diffusion models are still
inefficient when compared to GANs, requiring multiple
network evaluations during inference. A key aspect for comparison between GANs and diffusion models is their latent
space. While GANs have a low-dimensional latent space,
diffusion models preserve the original size of the images.
Furthermore, the latent space of diffusion models is usually
modeled as a random Gaussian distribution, being similar
to VAEs. In terms of semantic properties, it was discovered
that the latent space of GANs contains subspaces associated
with visual attributes . Thanks to this property, the
attributes can be manipulated with changes in the latent
space , . In contrast, when such transformations are
desired for diffusion models, the preferred procedure is the
guidance technique , , which does not exploit any
semantic property of the latent space. However, Song et
al. demonstrate that the latent space of diffusion models
has a well-defined structure, illustrating that interpolations
in this space lead to interpolations in the image space. In
summary, from the semantic perspective, the latent space of
diffusion models has been explored much less than in the
case of GANs, but this may be one of the future research
directions to be followed by the community.
A CATEGORIZATION OF DIFFUSION MODELS
We categorize diffusion models into a multi-perspective
taxonomy considering different criteria of separation. Perhaps the most important criteria to separate the models are
defined by (i) the task they are applied to, and (ii) the
input signals they require. Furthermore, as there are multiple approaches in formulating a diffusion model, (iii) the
underlying framework is another key factor for classifying
diffusion models. Finally, the (iv) data sets used during
training and evaluation are also of high importance, because
they provide the means to compare different models on the
same task. Our categorization of diffusion models according
to the criteria enumerated above is presented in Table 1.
In the remainder of this section, we present several contributions on diffusion models, choosing the target task as
the primary criterion to separate the methods. We opted for
this classification criterion as it is fairly well-balanced and
representative for research on diffusion models, facilitating
a quick grasping of related works by readers working on
specific tasks. Although the main task is usually related to
image generation, a considerable amount of work has been
conducted to match and even surpass the performance of
GANs on other topics, such as super-resolution, inpainting,
image editing, image-to-image translation or segmentation.
Unconditional Image Generation
The diffusion models presented below are used to generate
samples in an unconditional setting. Such models do not
require supervision signals, being completely unsupervised.
We consider this as the most basic and generic setting for
image generation.
Denoising Diffusion Probabilistic Models
The work of Sohl-Dickstein et al. formalizes diffusion
models as described in Section 2.1. The proposed neural
network is based on a convolutional architecture containing
multi-scale convolution.
Austin et al. extend the approach of Sohl-Dickstein
et al. to discrete diffusion models, studying different
choices for the transition matrices used in the forward process. Their results are competitive with previous continuous
diffusion models for the image generation task.
Ho et al. extend the work presented in , proposing
to learn the reverse process by estimating the noise in the
image at each step. This change leads to an objective that
resembles the denoising score matching applied in . To
predict the noise in an image, the authors use the Pixel-
CNN++ architecture, which was introduced in .
On top of the work proposed by Ho et al. , Nichol
et al. introduce several improvements, observing that
the linear noise schedule is suboptimal for low resolution.
They propose a new option that avoids a fast information
destruction towards the end of the forward process. Further,
they show that it is required to learn the variance in order
to improve the performance of diffusion models in terms
of log-likelihood. This last change allows faster sampling,
somewhere around 50 steps being required.
Song et al. replace the Markov forward process used
in with a non-Markovian one. The generative process
changes such that the model first predicts the normal sample, and then, it is used to estimate the next step in the
chain. The change leads to a faster sampling procedure with
a small impact on the quality of the generated samples. The
resulting framework is known as the denoising diffusion
implicit model (DDIM).
The work of Sinha et al. presents the diffusiondecoding model with contrastive representations (D2C), a
generative method which trains a diffusion model on latent
representations produced by an encoder. The framework,
which is based on the DDPM architecture presented in ,
produces images by mapping the latent representations to
In , the authors present a method to estimate the
noise parameters given the current input at inference time.
Their change improves the Fr´echet Inception Distance (FID),
while requiring less steps. The authors employ VGG-11 to
estimate the noise parameters, and DDPM to generate
The work of Nachmani et al. suggests replacing the
Gaussian noise distributions of the diffusion process with
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Our multi-perspective categorization of diffusion models applied in computer vision. To classify existing models, we consider three criteria: the
task, the denoising condition, and the underlying approach (architecture). Additionally, we list the data sets on which the surveyed models are
applied. We use the following abbreviations in the architecture column: D3PM (Discrete Denoising Diffusion Probabilistic Models), DSB (Diffusion
Schr¨odinger Bridge), BDDM (Bilateral Denoising Diffusion Models), PNDM (Pseudo Numerical Methods for Diffusion Models), ADM (Ablated
Diffusion Model), D2C (Diffusion-Decoding Models with Contrastive Representations), CCDF (Come-Closer-Diffuse-Faster), VQ-DDM (Vector
Quantised Discrete Diffusion Model), BF-CNN (Bias-Free CNN), FDM (Flexible Diffusion Model), RVD (Residual Video Diffusion), RaMViD
(Random Mask Video Diffusion).
Denoising Condition
Architecture
Austin et al. 
image generation
unconditional
Bao et al. 
image generation
unconditional
DDIM, Improved
CelebA, ImageNet, LSUN Bedroom, CIFAR-10
Benny et al. 
image generation
unconditional
DDPM, DDIM
CIFAR-10, ImageNet, CelebA
Bond-Taylor et al. 
image generation
unconditional
LSUN Bedroom, LSUN Church,
Choi et al. 
image generation
unconditional
FFHQ, AFHQ-Dog, CUB, Met-
De et al. 
image generation
unconditional
MNIST, CelebA
Deasy et al. 
image generation
unconditional
Fashion-MNIST,
CIFAR-10, CelebA
Deja et al. 
image generation
unconditional
Improved DDPM Fashion-MNIST,
Dockhorn et al. 
image generation
unconditional
Ho et al. 
image generation
unconditional
CIFAR-10, CelebA-HQ, LSUN
Huang et al. 
image generation
unconditional
CIFAR-10, MNIST
Jing et al. 
image generation
unconditional
CelebA-256-HQ,
LSUN Church
Jolicoeur et al. 
image generation
unconditional
Stacked-MNIST
Jolicoeur et al. 
image generation
unconditional
Kim et al. 
image generation
unconditional
CIFAR-10, CelebA, MNIST
Kingma et al. 
image generation
unconditional
CIFAR-10, ImageNet
Kong et al. 
image generation
unconditional
DDIM, DDPM
Lam et al. 
image generation
unconditional
CIFAR-10, CelebA
Liu et al. 
image generation
unconditional
CIFAR-10, CelebA
Ma et al. 
image generation
unconditional
NCSN, NCSN++
CIFAR-10, CelebA, LSUN Bedroom, LSUN Church, FFHQ
Nachmani et al. 
image generation
unconditional
DDIM, DDPM
CelebA, LSUN Church
Nichol et al. 
image generation
unconditional
CIFAR-10, ImageNet
Pandey et al. 
image generation
unconditional
CelebA-HQ, CIFAR-10
San et al. 
image generation
unconditional
CelebA, LSUN Bedroom, LSUN
Sehwag et al. 
image generation
unconditional
CIFAR-10, ImageNet
Sohl-Dickstein et al. 
image generation
unconditional
MNIST, CIFAR-10, Dead Leaf
Song et al. 
image generation
unconditional
Church Outdoor
Song et al. 
image generation
unconditional
CIFAR-10, ImageNet 32×32
Song et al. 
image generation
unconditional
CIFAR-10, CelebA, LSUN
Vahdat et al. 
image generation
unconditional
CIFAR-10, CelebA-HQ, MNIST
Wang et al. 
image generation
unconditional
CIFAR-10, CelebA
Wang et al. 
image generation
unconditional
StyleGAN2, ProjectedGAN
CIFAR-10, STL-10, LSUN Bedroom, LSUN Church, AFHQ,
Watson et al. 
image generation
unconditional
CIFAR-10, ImageNet
Watson et al. 
image generation
unconditional
Improved DDPM CIFAR-10, ImageNet 64×64
Xiao et al. 
image generation
unconditional
Zhang et al. 
image generation
unconditional
CIFAR-10, MNIST
Zheng et al. 
image generation
unconditional
CIFAR-10, CelebA, CelebA-HQ,
LSUN Bedroom, LSUN Church
Bordes et al. 
conditional image generation
conditioned on latent representations
Improved DDPM ImageNet
Campbell et al. 
conditional image generation
unconditional, conditioned
CIFAR-10, Lakh Pianoroll
Chao et al. 
conditional image generation
conditioned on class
Improved DDPM
CIFAR-10, CIFAR-100
Dhariwal et al. 
conditional image generation
unconditional,
classifier
LSUN Bedroom, LSUN Horse,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Ho et al. 
conditional image generation
conditioned on label
LSUN, ImageNet
Ho et al. 
conditional image generation
unconditional,
classifierfree guidance
Karras et al. 
conditional image generation
unconditional, conditioned
DDPM, DDIM
CIFAR-10, ImageNet 64×64
Liu et al. 
conditional image generation
conditioned on text, image,
style guidance
FFHQ, LSUN Cat, LSUN Horse,
LSUN Bedroom
Liu et al. 
conditional image generation
conditioned on text, 2D positions, relational descriptions between items, human
facial attributes
Improved DDPM CLEVR,
Relational
Lu et al. 
conditional image generation
unconditional, conditioned
CIFAR-10, CelebA, ImageNet,
LSUN Bedroom
Salimans et al. 
conditional image generation
unconditional, conditioned
CIFAR-10, ImageNet, LSUN
Singh et al. 
conditional image generation
conditioned on noise
Sinha et al. 
conditional image generation
unconditional, conditioned
CIFAR-10, CIFAR-100, fMoW,
CelebA-64,
CelebA-HQ-256,
Ho et al. 
image-to-image translation
conditioned on image
Improved DDPM ctest10k, places10k
Li et al. 
image-to-image translation
conditioned on image
Face2Comic,
Edges2Shoes,
Edges2Handbags
Sasaki et al. 
image-to-image translation
conditioned on image
CMP Facades, KAIST Multispectral Pedestrian
Wang et al. 
image-to-image translation
conditioned on image
ADE20K, COCO-Stuff, DIODE
Wolleb et al. 
image-to-image translation
conditioned on image
Improved DDPM BRATS
Zhao et al. 
image-to-image translation
conditioned on image
CelebaA-HQ, AFHQ
Gu et al. 
text-to-image generation
conditioned on text
VQ-Diffusion
CUB-200, Oxford 102 Flowers,
Jiang et al. 
text-to-image generation
conditioned on text
Transformerbased
encoderdecoder
DeepFashion-MultiModal
Ramesh et al. 
text-to-image generation
conditioned on text
MS-COCO, AVA
Rombach et al. 
text-to-image generation
conditioned on text
OpenImages, WikiArt, LAION-
2B-en, ArtBench
Saharia et al. 
text-to-image generation
conditioned on text
MS-COCO, DrawBench
Shi et al. 
text-to-image generation
unconditional, conditioned
Improved DDPM Conceptual
Zhang et al. 
text-to-image generation
unconditional, conditioned
CIFAR-10, CelebA, ImageNet
Daniels et al. 
super-resolution
conditioned on image
CIFAR-10, CelebA
Saharia et al. 
super-resolution
conditioned on image
FFHQ, CelebA-HQ, ImageNet-
Avrahami et al. 
image editing
conditioned on image and
Bedroom, MS-COCO
Avrahami et al. 
region image editing
text guidance
PaintByWord
Meng et al. 
image editing
conditioned on image
Improved DDPM
LSUN, CelebA-HQ
Lugmayr et al. 
inpainting
unconditional
CelebA-HQ, ImageNet
Nichol et al. 
inpainting
conditioned on image, text
Amit et al. 
image segmentation
conditioned on image
Improved DDPM Cityscapes,
Vaihingen,
Baranchuk et al. 
image segmentation
conditioned on image
Improved DDPM LSUN,
Bedroom-30, CelebA-19
Batzolis et al. 
multi-task
(inpainting,
superresolution, edge-to-image)
conditioned on image
CelebA, Edges2Shoes
Batzolis et al. 
multi-task
generation,
super-resolution,
inpainting,
image-to-image translation)
unconditional
ImageNet, CelebA-HQ, CelebA,
Edges2Shoes
Blattmann et al. 
multi-task (image generation)
unconditional, conditioned
on text, class
Choi et al. 
multi-task
generation,
image-to-image translation, image
conditioned on image
FFHQ, MetFaces
Chung et al. 
multi-task
(inpainting,
superresolution, MRI reconstruction)
conditioned on image
FFHQ, AFHQ, fastMRI knee
Esser et al. 
multi-task (image generation, inpainting)
unconditional, conditioned
on class, image and text
Conceptual
Captions, FFHQ, LSUN
Gao et al. 
multi-task (image generation, inpainting)
unconditional, conditioned
CIFAR-10, LSUN, CelebA
Graikos et al. 
multi-task (image generation, image segmentation)
conditioned on class
FFHQ-256, CelebA
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Hu et al. 
multi-task (image generation, inpainting)
unconditional, conditioned
CelebA-HQ, LSUN Church
Khrulkov et al. 
multi-task
generation,
image-to-image translation)
conditioned on class
Improved DDPM AFHQ, FFHQ, MetFaces, ImageNet
Kim et al. 
multi-task
translation,
multi-attribute transfer)
conditioned on image, portrait, stroke
ImageNet, CelebA-HQ, AFHQ-
Dog, LSUN Bedroom, Church
Luo et al. 
multi-task (point cloud generation,
auto-encoding, unsupervised representation learning)
conditioned on shape latent DDPM
Lyu et al. 
multi-task (image generation, image editing)
unconditional, conditioned
CIFAR-10, CelebA, ImageNet,
LSUN Bedroom, LSUN Cat
Preechakul et al. 
multi-task (latent interpolation, attribute manipulation)
conditioned on latent representation
Rombach et al. 
multi-task (super-resolution, image
generation, inpainting)
unconditional, conditioned
ImageNet, CelebA-HQ, FFHQ,
Shi et al. 
multi-task
(super-resolution,
inpainting)
conditioned on image
Improved DDPM MNIST, CelebA
Song et al. 
multi-task (image generation, inpainting)
unconditional, conditioned
MNIST, CIFAR-10, CelebA
Kadkhodaie et al. 
multi-task
superresolution,
Deblurring,
Compressive sensing, Inpainting,
Random missing pixels)
conditioned on linear measurements
MNIST, Set5, Set68, Set14
Song et al. 
multi-task (image generation, inpainting, colorization)
unconditional, conditioned
on image, class
CelebA-HQ, CIFAR-10, LSUN
Hu et al. 
image-to-image
translation
conditioned on image
Chung et al. 
medical image generation
conditioned
measurements
fastMRI knee
¨Ozbey et al. 
medical image generation
conditioned on image
Improved DDPM IXI, Gold Atlas - Male Pelvis
Song et al. 
medical image generation
conditioned
measurements
LIDC, LDCT Image and Projection, BRATS
Wolleb et al. 
medical image segmentation
conditioned on image
Improved DDPM BRATS
Sanchez et al. 
medical image segmentation and
anomaly detection
conditioned on image and
binary variable
Pinaya et al. 
medical image segmentation and
anomaly detection
conditioned on image
MedNIST, UK Biobank Images,
WMH, BRATS, MSLUB
Wolleb et al. 
medical image anomaly detection
conditioned on image
CheXpert, BRATS
Wyatt et al. 
medical image anomaly detection
conditioned on image
NFBS, 22 MRI scans
Harvey et al. 
video generation
conditioned on frames
GQN-Mazes, MineRL Navigate,
CARLA Town01
Ho et al. 
video generation
unconditional, conditioned
101 Human Actions
Yang et al. 
video generation
conditioned on video representation
BAIR, KTH Actions, Simulation, Cityscapes
H¨oppe et al. 
video generation and infilling
conditioned on frames
BAIR, Kinetics-600, UCF-101
Giannone et al. 
few-shot image generation
conditioned on image
Improved DDPM CIFAR-FS,
mini-ImageNet,
Jeanneret et al. 
counterfactual explanations
unconditional
Sanchez et al. 
counterfactual estimates
conditional
MNIST, ImageNet
Kawar et al. 
image restoration
conditioned on image
FFHQ, ImageNet
¨Ozdenizci et al. 
image restoration
conditioned on image
Outdoor-Rain,
Kim et al. 
image registration
conditioned on image
Radboud Faces, OASIS-3
Nie et al. 
adversarial purification
conditioned on image
DDPM, DDIM
CIFAR-10, ImageNet, CelebA-
Wang et al. 
semantic image generation
conditioned
Cityscapes,
CelebAMask-HQ
Zhou et al. 
shape generation and completion
unconditional,
conditional
shape completion
ShapeNet, PartNet
Zimmermann et al. 
classification
conditioned on label
two other distributions, a mixture of two Gaussians and the
Gamma distribution. The results show better FID values and
faster convergence thanks to the Gamma distribution that
has higher modeling capacity.
Lam et al. learn the noise scheduling for sampling.
The noise schedule for training remains linear as before.
After training the score network, they assume it to be close
to the optimal value in order to use it for noise schedule
training. The inference is composed of two steps. First, the
schedule is determined by fixing two initial hyperparameters. The second step is the usual reverse process with the
determined schedule.
Bond-Taylor et al. present a two-stage process, where
they apply vector quantization to images to obtain discrete
representations, and use a transformer to reverse a
discrete diffusion process, where the elements are randomly
masked at each step. The sampling process is faster because
the diffusion is applied to a highly compressed representa-
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
tion, which allows fewer denoising steps (50-256).
Watson et al. propose a dynamic programming algorithm that finds the optimal inference schedule, having
a time complexity of O(T), where T is the number of
steps. They conduct their image generation experiments on
CIFAR-10 and ImageNet, using the DDPM architecture.
In a different work, Watson et al. begin by presenting
how a reparametrization trick can be integrated within the
backward process of diffusion models in order to optimize a
family of fast samplers. Using the Kernel Inception Distance
as loss function, they show how optimization can be done
using stochastic gradient descent. Next, they propose a
special parametrized family of samplers, which, using the
same process as before, can achieve competitive results with
fewer sampling steps. Using FID and Inception Score (IS) as
metrics, the method seems to outperform some diffusion
model baselines.
Similar to Bond-Taylor et al. and Watson et al. ,
 , Xiao et al. try to improve the sampling speed,
while also maintaining the quality, coverage and diversity
of the samples. Their approach is to integrate a GAN in
the denoising process to discriminate between real samples
(forward process) and fake ones (denoised samples from the
generator), with the objective of minimizing the softened reverse KL divergence . However, the model is modified
by directly generating a clean (fully denoised) sample and
conditioning the fake example on it. Using the NCSN++
architecture with adaptive group normalization layers for
the GAN generator, they achieve similar FID values in
both image synthesis and stroke-based image generation,
at sampling rates of about 20 to 2000 times faster than other
diffusion models.
Kingma et al. introduce a class of diffusion models
that obtains state-of-the-art likelihoods on image density
estimation. They add Fourier features to the input of the
network to predict the noise, and investigate if the observed
improvement is specific to this class of models. Their results
confirm the hypothesis, i.e. previous state-of-the-art models
did not benefit from this change. As a theoretical contribution, they show that the diffusion loss is impacted by the
signal-to-noise ratio function only through its extremes.
Following the work presented in , Bao et al. propose an inference framework that does not require training
using non-Markovian diffusion processes. By first deriving
an analytical estimate of the optimal mean and variance
with respect to a score function, and using a pretrained
scored-based model to obtain score values, they show better
results, while being 20 to 40 times more time-efficient. The
score is approximated by Monte Carlo sampling. However,
the score is clipped within some precomputed bounds in
order to diminish any bias of the pretrained DDPM model.
Zheng et al. suggest truncating the process at an arbitrary step, and propose a method to inverse the diffusion
from this distribution by relaxing the constraint of having
Gaussian random noise as the final output of the forward
diffusion. To solve the issue of starting the reverse process
from a non-tractable distribution, an implicit generative
distribution is used to match the distribution of the diffused
data. The proxy distribution is fit either through a GAN or a
conditional transport. We note that the generator utilizes the
same U-Net model as the sampler of the diffusion model,
thus not adding extra parameters to be trained.
Deja et al. begin by analyzing the backward process
of a diffusion model and postulate that it is formed of two
models, a generator and a denoiser. Thus, they propose to
explicitly split the process into two components: the denoiser via an auto-encoder, and the generator via a diffusion
model. Both models use the same U-Net architecture.
Wang et al. start from the idea presented by Arjovsky
et al. and Sønderby et al. to augment the input
data of the discriminator by adding noise. This is achieved
in by injecting noise from a Gaussian mixture distribution composed of weighted diffused samples from the clean
image at various time steps. The noise injection mechanism
is applied to both real and fake images. The experiments are
conducted on a wide range of data sets covering multiple
resolutions and high diversity.
Score-Based Generative Models
Starting from a previous work , Song et al. present
several improvements which are based on theoretical and
empirical analyses. They address both training and sampling phases. For training, the authors show new strategies
to choose the noise scales and how to incorporate the noise
conditioning into NCSNs . For sampling, they propose
to apply an exponential moving average to the parameters
and select the hyperparameters for the Langevin dynamics
such that the step size verifies a certain equation. The
proposed changes unlock the application of NCSNs on highresolution images.
Jolicoeur-Martineau et al. introduce an adversarial
objective along with denoising score matching to train scorebased models. Furthermore, they propose a new sampling
procedure called Consistent Annealed Sampling and prove
that it is more stable than the annealed Langevin method.
Their image generation experiments show that the new
objective returns higher quality examples without an impact on diversity. The suggested changes are tested on the
architectures proposed in , , .
Song et al. improve the likelihood of score-based
diffusion models. They achieve this through a new weighting function for the combination of the score matching
losses. For their image generation experiments, they use the
DDPM++ architecture introduced in .
In , the authors present a score-based generative
model as an implementation of Iterative Proportional Fitting
(IPF), a technique used to solve the Schr¨odinger bridge
problem. This novel approach is tested on image generation,
as well as data set interpolation, which is possible because
the prior can be any distribution.
Vahdat et al. train diffusion models on latent representations. They use a VAE to encode to and decode
from the latent space. This work achieves up to 56 times
faster sampling. For the image generation experiments, the
authors employ the NCSN++ architecture introduced in .
Stochastic Differential Equations
DiffFlow is introduced in as a new generative modeling
approach that combines normalizing flows and diffusion
probabilistic models. From the perspective of diffusion models, the method has a sampling procedure that is up to 20
times more efficient, thanks to a learnable forward process
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
which skips unneeded noise regions. The authors perform
experiments using the same architecture as in .
Jolicoeur-Martineau et al. introduce a new SDE
solver that is between 2× and 5× faster than Euler-
Maruyama and does not affect the quality of the generated
images. The solver is evaluated in a set of image generation
experiments with pretrained models from .
Wang et al. present a new deep generative model
based on Schr¨odinger bridge. This is a two-stage method,
where the first stage learns a smoothed version of the target
distribution, and the second stage derives the actual target.
Focusing on scored-based models, Dockhorn et al. 
utilize a critically-damped Langevin diffusion process by
adding another variable (velocity) to the data, which is the
only source of noise in the process. Given the new diffusion
space, the resulting score function is demonstrated to be
easier to learn. The authors extend their work by developing
a more suitable score objective called hybrid score matching,
as well as a sampling method, by solving the SDE through
integration. The authors adapt the NCSN++ and DDPM++
architectures to accept both data and velocity, being evaluated on unconditional image generation and outperforming
similar score-based diffusion models.
Motivated by the limitations of high-dimensional scorebased diffusion models due to the Gaussian noise distribution, Deasy et al. extend denoising score matching to
generalize to the normal noising distribution. By adding a
heavier tailed distribution, their experiments on several data
sets show promising results, as the generative performance
improves in certain cases (depending on the shape of the
distribution). An important scenario in which the method
excels is on data sets with unbalanced classes.
Jing et al. try to shorten the duration of the sampling
process of diffusion models by reducing the space onto
which diffusion is realized, i.e. the larger the time step in the
diffusion process, the smaller the subspace. The data is projected onto a finite set of subspaces, at specific times, each
being associated with a score model. This results in reduced
computational costs, while the performance is increased.
The work is limited to natural image synthesis. Evaluating
the method in unconditional image generation, the authors
achieve similar or better performance compared with stateof-the-art models, while having a lower inference time. The
method is demonstrated to work for the inpainting task as
Kim et al. propose to change the diffusion process
into a non-linear one. This is achieved by using a trainable
normalizing flow model which encodes the image in the
latent space, where it can now be linearly diffused to the
noise distribution. A similar logic is then applied to the denoising process. This method is applied on the NCSN++ and
DDPM++ frameworks, while the normalizing flow model is
based on ResNet.
Ma et al. aim to make the backward diffusion process
more time-efficient, while maintaining the synthesis performance. Within the family of score-based diffusion models,
they begin to analyze the reverse diffusion in the frequency
domain, subsequently applying a space-frequency filter to
the sampling process, which aims to integrate information
about the target distribution into the initial noise sampling. The authors conduct experiments with NCSN and
NCSN++ , where the proposed method clearly shows
speed improvements in image synthesis (by up to 20 times
less sampling steps), while keeping the same satisfactory
generation quality for both low and high-resolution images.
Conditional Image Generation
We next showcase diffusion models that are applied to conditional image synthesis. The condition is commonly based
on various source signals, in most cases some class labels
being used. Some methods perform both unconditional and
conditional generation, which are also discussed here.
Denoising Diffusion Probabilistic Models
Dhariwal et al. introduce few architectural changes to
improve the FID of diffusion models. They also propose
classifier guidance, a strategy which uses the gradients of a
classifier to guide the diffusion during sampling. They conduct both unconditional and conditional image generation
experiments.
Bordes et al. examine representations resulting from
self-supervised tasks by visualizing and comparing them
to the original image. They also compare representations
generated from different sources. Thus, a diffusion model
is used to generate samples that are conditioned on these
representations. The authors implement several modifications to the U-Net architecture presented by Dhariwal et
al. , such as adding conditional batch normalization layers, and mapping the vector representation through a fully
connected layer.
The method presented in allows diffusion models
to produce images from low-density regions of the data
manifold. They use two new losses to guide the reverse
process. The first loss guides the diffusion towards lowdensity regions, while the second enforces the diffusion to
stay on the manifold. Moreover, they demonstrate that their
diffusion model does not memorize the examples from the
low-density neighborhoods, generating novel images. The
authors employ an architecture similar to that of Dhariwal
et al. .
Kong et al. define a bijection between the continuous
diffusion steps and the noise levels. With the defined bijection, they are able to construct an approximate diffusion
process which requires less steps. The method is tested
using the previous DDIM and DDPM architectures
on image generation.
Pandey et al. build a generator-refiner framework,
where the generator is a VAE and the refiner is a DDPM
conditioned by the output of the VAE. The latent space
of the VAE can be used to control the content of the
generated image because the DDPM only adds the details.
After training the framework, the resulting DDPM is able to
generalize to different noise types. More specifically, if the
reverse process is not conditioned on the VAE’s output, but
on different noise types, the DDPM is able to reconstruct the
initial image.
Ho et al. introduce Cascaded Diffusion Models
(CDM), an approach for generating high-resolution images
conditioned on ImageNet classes. Their framework contains
multiple diffusion models, where the first model from the
pipeline generates low-resolution images conditioned on
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
the image class. The subsequent models are responsible for
generating images of increasingly higher resolutions. These
models are conditioned on both the class and the lowresolution image.
Benny et al. study the advantages and disadvantages
of predicting the image instead of the noise during the
reverse process. They conclude that some of the discovered
problems could be addressed by interpolating the two types
of output. They modify previous architectures to return both
the noise and the image, as well as a value that controls the
importance of the noise when performing the interpolation.
The strategy is evaluated on top of the DDPM and DDIM
architectures.
Choi et al. investigate the impact of the noise levels
on the visual concepts learned by diffusion models. They
modify the conventional weighting scheme of the objective
function to a new one that enforces diffusion models to
learn rich visual concepts. The method groups the noise
levels into three categories (coarse, content and clean-up)
according to the signal-to-noise ratio, i.e. small SNR is
coarse, medium SNR is content, large SNR is clean-up. The
weighting function assigns lower weights to the last group.
Singh et al. propose a novel method for conditional image generation. Instead of conditioning the signal
throughout the sampling process, they present a method to
condition the noise signal (from where the sampling starts).
Using Inverting Gradients , the noise is injected with
information about localization and orientation of the conditioned class, while maintaining the same random Gaussian
distribution.
Describing the resembling functionality of diffusion
models and energy-based models, and leveraging the compositional structure of the latter models, Liu et al. propose to combine multiple diffusion models for conditional
image synthesis. In the reverse process, the composition of
multiple diffusion models, each associated with a different
condition, can be achieved either through conjunction or
Score-Based Generative Models
The works of Song et al. and Dhariwal et al. on scoredbased conditional diffusion models based on classifier guidance inspired Chao et al. to develop a new training
objective which reduces the potential discrepancy between
the score model and the true score. The loss of the classifier
is modified into a scaled cross-entropy added to a modified
score matching loss.
Stochastic Differential Equations
Ho et al. introduce a guidance method that does not
require a classifier. It just needs one conditional diffusion
model and one unconditional version, but they use the
same model to learn both cases. The unconditional model
is trained with the class identifier being equal to 0. The idea
is based on the implicit classifier derived from the Bayes
Liu et al. investigate the usage of conventional
numerical methods to solve the ODE formulation of the
reverse process. They find that these methods return lower
quality samples compared with the previous approaches.
Therefore, they introduce pseudo-numerical methods for
diffusion models. Their idea splits the numerical methods
into two parts, the gradient part and the transfer part. The
transfer part (standard methods have a linear transfer part)
is replaced such that the result is as close as possible to the
target manifold. As a last step, they show how this change
solves the problems discovered when using conventional
approaches.
Tachibana et al. address the slow sampling problem of DDPMs. They propose to decrease the number of
sampling steps by increasing the order (from one to two) of
the stochastic differential equation solver (denoising part).
While preserving the network architecture and score matching function, they adopt the Itˆo-Taylor expansion scheme for
the sampler, as well as substitute some derivative terms in
order to simplify the calculation. They reduce the number
of backward steps while retaining the performance. On top
of these, another contribution is the new noise schedule.
Karras et al. try to separate diffusion scored-based
models into individual components that are independent
of each other. This separation allows modifying a single
part without affecting the other units, thus facilitating the
improvement of diffusion models. Using this framework,
the authors first present a sampling process that uses Heun’s
method as the ODE solver, which reduces the neural function evaluations while maintaining the FID score. They
further show that a stochastic sampling process brings great
performance benefits. The second contribution is related
to training the score-based model by preconditioning the
neural network on its input and the corresponding targets,
as well as using image augmentation.
Within the context of both unconditional and classconditional image generation, Salimans et al. propose
a technique for reducing the number of sampling steps.
They distill the knowledge of a trained teacher model,
represented by a deterministic DDIM, into a student model
that has the same architecture, but halving the number of
sampling steps. In other words, the target of the student is
to take two consecutive steps of the teacher. Furthermore,
this process can be repeated until the desired number of
sampling steps is reached, while maintaining the same
image synthesis quality. Finally, three versions of the model
and two loss functions are explored in order to facilitate
the distillation process and reduce the number of sampling
steps (from 8192 to 4).
Campbell et al. demonstrate a continuous-time
formulation of denoising diffusion models that is capable
of working with discrete data. The work models the forward continuous-time Markov chain diffusion process via a
transition rate matrix, and the backward denoising process
via a parametric approximation of the inverse transition rate
matrix. Further contributions are related to the training objective, the matrix construction, and an optimized sampler.
The interpretation of diffusion models as ODEs proposed by Song et al. is reformulated by Lu et al. 
in a form that can be solved using an exponential integrator.
Other contributions of Lu et al. are an ODE solver
that approximates the integral term of the new formulation
using Taylor expansion (first order to third order), and an
algorithm that adapts the time step schedule, being 4 to 16
times faster.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Image-to-Image Translation
Saharia et al. propose a framework for image-to-image
translation using diffusion models, focusing on four tasks:
colorization, inpainting, uncropping and JPEG restoration.
The proposed framework is the same across all four tasks,
meaning that it does not suffer custom changes for each
task. The authors begin by comparing L1 and L2 losses,
suggesting that L2 is preferred, as it leads to a higher sample
diversity. Finally, they reconfirm the importance of selfattention layers in conditional image synthesis.
To translate an unpaired set of images, Sasaki et al. 
propose a method involving two jointly trained diffusion
models. During the reverse denoising process, at every step,
each model is also conditioned on the other’s intermediate
sample. Furthermore, the loss function of the diffusion
models is regularized using the cycle-consistency loss .
The aim of Zhao et al. is to improve current image-toimage translation score-based diffusion models by utilizing
data from a source domain with an equal significance. An
energy-based function trained on both source and target
domains is employed in order to guide the SDE solver.
This leads to generating images that preserve the domainagnostic features, while translating characteristics specific to
the source domain to the target domain. The energy function
is based on two feature extractors, each specific to a domain.
Leveraging the power of pretraining, Wang et al. 
employ the GLIDE model and train it to obtain a rich
semantic latent space. Starting from the pretrained version
and replacing the head to adapt to any conditional input,
the model is fine-tuned on some specific image generation
downstream tasks. This is done in two steps, where the first
step is to freeze the decoder and train only the new encoder,
and the second step is to train them simultaneously. Finally,
the authors employ adversarial training and normalize the
classifier-free guidance to enhance generation quality.
Li et al. introduce a diffusion model for image-toimage translation that is based on Brownian bridges, as well
as GANs. The proposed process begins by encoding the
image with a VQ-GAN . Within the resulting quantized
latent space, the diffusion process, formulated as a Brownian bridge, maps between the latent representations of
the source domain and target domain. Finally, another VQ-
GAN decodes the quantized vectors in order to synthesize
the image in the new domain. The two GAN models are
independently trained on their specific domains.
Continuing their previous work proposed in , Wolleb
et al. extend the diffusion model by replacing the classifier with another model specific to the task. Thus, at every
step of the sampling process, the gradient of the task-specific
network is infused. The method is demonstrated with a
regressor (based on an encoder) or with a segmentation
model (using the U-Net architecture), whereas the diffusion
model is based on existing frameworks , . This setting
has the advantage of eliminating the need to retrain the
whole diffusion model, except for the task-specific model.
Text-to-Image Synthesis
Perhaps the most impressive results of diffusion models are
attained on text-to-image synthesis, where the capability of
combining unrelated concepts, such as objects, shapes and
textures, to generate unusual examples comes to light. To
confirm this statement, we used Stable Diffusion to
generate images based on various text prompts, and the
results are shown in Figure 2.
Imagen is introduced in as an approach for textto-image synthesis. It consists of one encoder for the text
sequence and a cascade of diffusion models for generating
high-resolution images. These models are also conditioned
on the text embeddings returned by the encoder. Moreover,
the authors introduce a new set of captions (DrawBench)
for text-to-image evaluations. Regarding the architecture,
the authors develop Efficient U-Net to improve efficiency,
and apply this architecture in their text-to-image generation
experiments.
Gu et al. introduce the VQ-Diffusion model, a
method for text-to-image synthesis that does not have the
unidirectional bias of previous approaches. With its masking
mechanism, the proposed method avoids the accumulation
of errors during inference. The model has two stages, where
the first stage is based on a VQ-VAE that learns to represent
an image via discrete tokens, and the second stage is a
discrete diffusion model that operates on the discrete latent
space of the VQ-VAE. The training of the diffusion model is
conditioned on caption embeddings. Inspired from masked
language modeling, some tokens are replaced with a [mask]
Avrahami et al. present a text-conditional diffusion
model conditioned on CLIP image and text embeddings. This is a two-stage approach, where the first stage
generates the image embedding, and the second stage (decoder) produces the final image conditioned on the image
embedding and the text caption. To generate image embeddings, the authors use a diffusion model in the latent space.
They perform a subjective human assessment to evaluate
their generative results.
Addressing the slow sampling inconvenience of diffusion models, Zhang et al. focus their work on a new
discretization scheme that reduces the error and allows a
greater step size, i.e. a lower number of sampling steps.
By using high-order polynomial extrapolations in the score
function and an Exponential Integrator for solving the reverse SDE, the number of network evaluations is drastically
reduced, while maintaining the generation capabilities.
Shi et al. combine a VQ-VAE and a diffusion
model to generate images. Starting from the VQ-VAE, the
encoding functionality is preserved, while the decoder is
replaced by a diffusion model. The authors use the U-Net
architecture from , injecting the image tokens into the
middle block.
Building on top of the work presented in , Rombach
et al. introduce a modification to create artistic images
using the same procedure: extract the k-nearest neighbors
in the CLIP latent space of an image from a database,
then generate a new image by guiding the reverse denoising
process with these embeddings. As the CLIP latent space
is shared by text and images, the diffusion can be guided
by text prompts as well. However, at inference time, the
database is replaced with another one that contains artistic
images. Thus, the model generates images within the style
of the new database.
Jiang et al. present a framework to generate images
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
of full-body humans with rich clothing representation given
three inputs: a human pose, a text description of the clothes’
shape, and another text of the clothing texture. The first
stage of the method encodes the former text prompt into an
embedding vector and infuses it into the module (encoderdecoder based) that generates a map of forms. In the second
stage, a diffusion-based transformer samples an embedded
representation of the latter text prompt from multiple multilevel codebooks (each specific to a texture), a mechanism
suggested in VQ-VAE . Initially, the codebook indices at
coarser levels are sampled, and then, using a feed-forward
network, the finer-level indices are predicted. The text is
encoded using Sentence-BERT .
Image Super-Resolution
Saharia et al. apply diffusion models to super-resolution.
Their reverse process learns to generate high quality images
conditioned on low-resolution versions. This work employs
the architectures presented in , and the following data
sets: CelebA-HQ, FFHQ and ImageNet.
Daniels et al. use score-based models to sample from
the Sinkhorn coupling of two distributions. Their method
models the dual variables with neural networks, then solves
the problem of optimal transport. After training the neural
networks, the sampling can be performed via Langevin
dynamics and a score-based model. They run experiments
on image super-resolution using a U-Net architecture.
Image Editing
Meng et al. utilize diffusion models in various guided
image generation tasks, i.e. stroke painting or stroke-based
editing and image composition. Starting from an image that
contains some form of guidance, its properties (such as
shapes and colors) are preserved, while the deformations
are smoothed out by progressively adding noise (forward
process of the diffusion model). Then, the result is denoised
(reverse process) to create a realistic image according to the
guidance. Images are synthesized with a generic diffusion
model by solving the reverse SDE, without requiring any
custom data set or modifications for training.
One of the first approaches for editing specific regions
of images based on natural language descriptions is introduced in . The regions to be modified are specified by
the user via a mask. The method relies on CLIP guidance
to generate an image according to the text input, but the
authors observe that combining the output with the original
image at the end does not produce globally coherent images.
Hence, they modify the denoising process to fix the issue.
More precisely, after each step, the authors apply the mask
on the latent image, while also adding the noisy version of
the original image.
Extending the work presented in , Avrahami et
al. apply latent diffusion models for editing images locally, using text. A VAE encodes the image and the adaptiveto-time mask (region to edit) into the latent space where
the diffusion process occurs. Each sample is iteratively denoised, while being guided by the text within the region of
interest. However, inspired by Blended Diffusion , the
image is combined with the masked region in the latent
space that is noised at the current time step. Finally, the
sample is decoded with the VAE to generate the new image. The method demonstrates superior performance while
being comparably faster.
Image Inpainting
Nichol et al. train a diffusion model conditioned on text
descriptions and also study the effectiveness of classifierfree and CLIP-based guidance. They obtain better results
with the first option. Moreover, they fine-tune the model for
image inpainting, unlocking image modifications based on
text input.
Lugmay et al. present an inpainting method agnostic
to the mask form. They use an unconditional diffusion
model for this, but modify its reverse process. They produce
the image at step t −1 by sampling the known region from
the masked image, and the unknown region by applying
denoising to the image obtained at step t. With this procedure, the authors observe that the unknown region has
the right structure, while also being semantically incorrect.
Further, they solve the issue by repeating the proposed step
for a number of times and, at each iteration, they replace
the previous image from step t with a new sample obtained
from the denoised version generated at step t −1.
Image Segmentation
Baranchuk et al. demonstrate how diffusion models can
be used in semantic segmentation. Taking the feature maps
(middle blocks) at different scales from the decoder of the
U-Net (used in the denoising process) and concatenating
them (upsampling the feature maps in order to have the
same dimensions), they can be used to classify each pixel by
further attaching an ensemble of multi-layer perceptrons.
The authors show that these feature maps, extracted at later
steps in the denoising process, contain rich representations.
The experiments show that segmentation based on diffusion
models outperforms most baselines.
Amit et al. propose the use of diffusion probabilistic
models for image segmentation through extending the architecture of the U-Net encoder. The input image and the
current estimated image are passed through two different
encoders and combined together through summation. The
result is then supplied to the encoder-decoder of the U-
Net. Due to the stochastic noise infused at every time step,
multiple samples for a single input image are generated and
used to compute the mean segmentation map. The U-Net
architecture is based on a previous work , while the input
image generator is built with Residual Dense Blocks .
The denoised sample generator is a simple 2D convolutional
Multi-Task Approaches
A series of diffusion models have been applied to multiple
tasks, demonstrating a good generalization capacity across
tasks. We discuss such contributions below.
Song et al. present the noise conditional score network
(NCSN), an approach which estimates the score function
at different noise scales. For sampling, they introduce an
annealed version of Langevin dynamics and use it to report
results in image generation and inpainting. The NCSN
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
architecture is mainly based on the work presented in ,
with small changes such as replacing batch normalization
with instance normalization.
Kadkhodaie et al. train a neural network to restore
images corrupted with Gaussian noise, generated using random standard deviations that are restricted to a particular
range. After training, the difference between the output of
the neural network and the noisy image received as input
is proportional with the gradient of the log-density of the
noisy data. This property is based on previous work done in
 . For image generation, the authors use the mentioned
difference as gradient (score) estimation and sample from
the implicit data prior of the network by employing an
iterative method similar to the annealed Langevin dynamics
from . However, the two sampling methods have some
dissimilarities, for example the noise injected in the iterative
updates follow distinct strategies. In , the injected noise
is adapted according to the network’s estimate, while in
 , it is fixed. Moreover, the gradient estimates in are
learned by score matching, while Kadkhodaie et al. 
rely on the previously mentioned property to compute
the gradients. The contribution of Kadkhodaie et al. 
develops even further by adapting the algorithm to linear
inverse problems, such as deblurring and super-resolution.
The SDE formulation of diffusion models introduced
in generalizes over several previous methods – .
Song et al. present the forward and reverse diffusion
processes as solutions of SDEs. This technique unlocks new
sampling methods, such as the Predictor-Corrector sampler,
or the deterministic sampler based on ODEs. The authors
carry out experiments on image generation, inpainting and
colorization.
Batzolis et al. introduce a new forward process
in diffusion models, called non-uniform diffusion. This is
determined by each pixel being diffused with a different
SDE. Multiple networks are employed in this process, each
corresponding to a different diffusion scale. The paper further demonstrates a novel conditional sampler that interpolates between two denoising score-based sampling methods.
The model, whose architecture is based on and ,
is evaluated on unconditional synthesis, super-resolution,
inpainting and edge-to-image translation.
Esser et al. propose ImageBART, a generative model
which learns to revert a multinomial diffusion process on
compact image representations. A transformer is used to
model the reverse steps autoregressively, where the encoder’s representation is obtained using the output at the
previous step. ImageBART is evaluated on unconditional,
class-conditional and text-conditional image generation, as
well as local editing.
Gao et al. introduce diffusion recovery likelihood,
a new training procedure for energy-based models. They
learn a sequence of energy-based models for the marginal
distributions of the diffusion process. Thus, instead of approximating the reverse process with normal distributions,
they derive the conditional distributions from the marginal
energy-based models. The authors run experiments on both
image generation and inpainting.
Batzolis et al. analyze the previous score-based diffusion models on conditional image generation. Moreover,
they present a new method for conditional image generation
called conditional multi-speed diffusive estimator (CMDE).
This method is based on the observation that diffusing the
target image and the condition image at the same rate, might
be suboptimal. Therefore, they propose to diffuse the two
images, which have the same drift and different diffusion
rates, with an SDE. The approach is evaluated on inpainting,
super-resolution and edge-to-image synthesis.
Liu et al. introduce a framework which allows text,
content and style guidance from a reference image. The core
idea is to use the direction that maximizes the similarity
between the representations learned for image and text.
The image and text embeddings are produced by the CLIP
model . To address the need of training CLIP on noisy
images, the authors present a self-supervised procedure that
does not require text captions. The procedure uses pairs
of normal and noised images to maximize the similarity
between positive pairs and minimize it for negative ones
(contrastive objective).
Choi et al. propose a novel method, which does
not require further training, for conditional image synthesis
using unconditional diffusion models. Given a reference
image, i.e. the condition, each sample is drawn closer to
it by eliminating the low frequency content and replacing
it with content from the reference image. The low pass
filter is represented by a downsampling operation, which
is followed by an upsampling filter of the same factor. The
authors show how this method can be applied on various
image-to-image translation tasks, e.g. paint-to-image, and
editing with scribbles.
Hu et al. propose to apply diffusion models on discrete representations given by a discrete VAE. They evaluate
the idea in image generation and inpainting experiments,
considering the CelebA-HQ and LSUN Church data sets.
Rombach et al. introduce latent diffusion models,
where the forward and reverse processes happen on the
latent space learned by an auto-encoder. They also include
cross-attention in the architecture, which brings further improvements on conditional image synthesis. The method is
tested on super-resolution, image generation and inpainting.
The method introduced by Preechakul et al. contains a semantic encoder that learns a descriptive latent
space. The output of this encoder is used to condition an
instance of DDIM. The proposed method allows DDPMs
to perform well on tasks such as interpolation or attribute
manipulation.
Chung et al. introduce an algorithm for sampling,
which reduces the number of steps required for the conditional case. Compared to the standard case, where the
reverse process starts from Gaussian noise, their approach
first executes one forward step to obtain an intermediary
noised image and resumes the sampling from this point on.
The approach is tested on inpainting, super-resolution, and
magnetic resonance imaging (MRI) reconstruction.
In , the authors fine-tune a pretrained DDIM to
generate images according to a text description. They propose a local directional CLIP loss that basically enforces
the direction between the generated image and the original
image to be as close as possible to the direction between the
reference (original domain) and target text (target domain).
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
The tasks considered in the evaluation are image translation
between unseen domains, and multi-attribute transfer.
Starting from the formulation of diffusion models as
SDEs of Meng et al. , Khrulkov et al. investigate the
latent space and the resulting encoder maps. As per Monge
formulation, it is shown that these encoder maps are the
optimal transport maps, but this is demonstrated only for
multivariate normal distributions. The authors further support this with numerical experiments, as well as practical
experiments, using the model implementation of Dhariwal
et al. .
Shi et al. start by observing how an unconditional score-based diffusion model can be formulated as
a Schr¨odinger bridge, which can be solved using a modified version of Iterative Proportional Fitting. The previous
method is reformulated to accept a condition, thus making conditional synthesis possible. Further adjustments are
made to the iterative algorithm in order to optimize the time
required to converge. The method is first validated with
synthetic data from Kovachki et al. , showing improved
capabilities in estimating the ground truth. The authors also
conduct experiments on super-resolution, inpainting, and
biochemical oxygen demand, the latter task being inspired
by Marzouk et al. .
Inspired by the Retrieval Transformer , Blattmann
et al. propose a new method for training diffusion
models. First, a set of similar images is fetched from a
database using a nearest neighbor algorithm. The images
are further encoded by an encoder with fixed parameters
and projected into the CLIP feature space. Finally, the
reverse process of the diffusion model is conditioned on this
latent space. The method can be further extended to use
other conditional signals, e.g. text, by simply enhancing the
latent space with the encoded representation of the signal.
Lyu et al. introduce a new technique to reduce the
number of sampling steps of diffusion models, boosting
the performance at the same time. The idea is to stop the
diffusion process at an earlier step. As the sampling cannot
start from a random Gaussian noise, a GAN or VAE model is
used to encode the last diffused image into a Gaussian latent
space. The result is then decoded into an image which can
be diffused into the starting point of the backward process.
The aim of Graikos et al. is to separate diffusion
models into two independent parts, a prior (the base part)
and a constraint (the condition). This enables models to be
applied on various tasks without further training. Changing
the equation of DDPMs from leads to independently
training the model and using it in a conditional setting,
given that the constraint becomes differentiable. The authors
conduct experiments on conditional image synthesis and
image segmentation.
Medical Image Generation and Translation
Wolleb et al. introduce a method based on diffusion
models for image segmentation within the context of brain
tumor segmentation. The training consists of diffusing the
segmentation map, then denoising it to obtain the original
image. During the backward process, the brain MR image is
concatenated into the intermediate denoising steps in order
to be passed through the U-Net model, thus conditioning
the denoising process on it. Furthermore, for each input,
the authors propose to generate multiple samples, which
will be different due to stochasticity. Thus, the ensemble
can generate a mean segmentation map and its variance
(associated with the uncertainty of the map).
Song et al. introduce a method for score-based
models that is able to solve inverse problems in medical imaging, i.e. reconstructing images from measurements.
First, an unconditional score model is trained. Then, a
stochastic process of the measurement is derived, which can
be used to infuse conditional information into the model
via a proximal optimization step. Finally, the matrix that
maps the signal to the measurement is decomposed to allow
sampling in closed-form. The authors carry out multiple
experiments on different medical image types, including
computed tomography (CT), low-dose CT and MRI.
Within the area of medical imaging, but focusing on reconstructing the images from accelerated MRI scans, Chung
et al. propose to solve the inverse problem using a
score-based diffusion model. A score model is pretrained
only on magnitude images in an unconditional setting.
Then, a variance exploding SDE solver is employed in
the sampling process. By adopting a Predictor-Corrector
algorithm interleaved with a data consistency mapping,
the split image (real and imaginary parts) is fed through,
enabling conditioning the model on the measurement. Furthermore, the authors present an extension of the method
which enables conditioning on multiple coil-varying measurements.
¨Ozbey et al. propose a diffusion model with adversarial inference. In order to increase each diffusion step,
and thus make fewer steps, inspired by , the authors
employ a GAN model in the reverse process to estimate the
denoised image at every step. Using a similar method as
 , they introduce a cycle-consistent architecture to allow
training on unpaired data sets.
The aim of Hu et al. is to remove the speckle noise in
optical coherence tomography (OCT) b-scans. The first stage
is represented by a method called self-fusion, as described
in , where additional b-scans close to the given 2D slice
of the input OCT volume are selected. The second stage
consists of a diffusion model whose starting point is the
weighted average of the original b-scan and its neighbors.
Thus, the noise can be removed by sampling a clean scan.
Anomaly Detection in Medical Images
Auto-encoders are widely used for anomaly detection .
Since diffusion models can be seen as a particular type of
VAEs, it seems natural to employ diffusion models for the
same tasks as VAEs. So far, diffusion models have shown
promising results in detecting anomalies in medical images,
as further discussed below.
Wyatt et al. train a DDPM on healthy medical
images. The anomalies are detected at inference time by
subtracting the generated image from the original image.
The work also proves that using simplex noise instead of
Gaussian noise yields better results for this type of task.
Wolleb et al. propose a weakly-supervised method
based on diffusion models for anomaly detection in medical
images. Given two unpaired images, one healthy and one
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
with lesions, the former is diffused by the model. Then,
the denoising process is guided by the gradient of a binary
classifier in order to generate the healthy image. Finally, the
sampled healthy image and the one containing lesions are
subtracted to obtain the anomaly map.
Pinaya et al. propose a diffusion-based method for
detecting anomalies in brain scans, as well as segmenting
those regions. The images are encoded by a VQ-VAE ,
and the quantized latent representation is obtained from a
codebook. The diffusion model operates in this latent space.
Averaging the intermediate samples from median steps of
the backward process and then applying a precomputed
threshold map, a binary mask implying the anomaly location is created. Starting the backward process from the
middle, the binary mask is used to denoise the anomalous
regions, while maintaining the rest. Finally, the sample at
the final step is decoded, resulting in a healthy image. The
segmentation map of the anomaly is obtained by subtracting
the input image and the synthesized image.
Sanchez et al. follow the same principle for detecting and segmenting anomalies in medical images: a
diffusion model generates healthy samples which are then
subtracted from the original images. The input image is
diffused using the model, reversing the denoising equation
and nullifying the condition, and then the backward conditioned process is applied. Utilizing a classifier-free model,
the guidance is achieved through an attention mechanism
integrated in the U-Net. The training utilizes both healthy
and unhealthy examples.
Video Generation
The recent progress towards making diffusion models more
efficient has enabled their application in the video domain.
We next present works applying diffusion models to video
generation.
Ho et al. introduce diffusion models to the task
of video generation. When compared to the 2D case, the
changes are applied only to the architecture. The authors
adopt the 3D U-Net from , presenting results in unconditional and text conditional video generation. Longer
videos are generated in an autoregressive manner, where the
latter video chunks are conditioned on the previous ones.
Yang et al. generate videos frame by frame, using
diffusion models. The reverse process is entirely conditioned
on a context vector provided by a convolutional recurrent
neural network. The authors perform an ablation study to
decide if predicting the residual of the next frame returns
better results than the case of predicting the actual frame.
The conclusion is that the former option works better.
H¨oppe et al. present random mask video diffusion
(RaMViD), a method which can be used for video generation
and infilling. The main contribution of their work is a novel
strategy for training, which randomly splits the frames into
masked and unmasked frames. The unmasked frames are
used to condition the diffusion, while the masked ones are
diffused by the forward process.
The work of Harvey et al. introduces flexible diffusion models, a type of diffusion model that can be used
with multiple sampling schemes for long video generation.
As in , the authors train a diffusion model by randomly
choosing the frames used in the diffusion and those used
for conditioning the process. After training the model, they
investigate the effectiveness of multiple sampling schemes,
concluding that the sampling choice depends on the data
Other Tasks
There are some pioneering works applying diffusion models
to new tasks, which have been scarcely explored via diffusion modeling. We gather and discuss such contributions
Luo et al. apply diffusion models on 3D point cloud
generation, auto-encoding, and unsupervised representation learning. They derive an objective function from the
variational lower bound of the likelihood of point clouds
conditioned on a shape latent. The experiments are conducted using PointNet as the underlying architecture.
Zhou et al. introduce Point-Voxel Diffusion (PVD), a
novel method for shape generation which applies diffusion
on point-voxel representations. The approach addresses the
tasks of shape generation and completion on the ShapeNet
and PartNet data sets.
Zimmermann et al. show a strategy to apply scorebased models for classification. They add the image label
as a conditioning variable to the score function and, thanks
to the ODE formulation, the conditional likelihood can be
computed at inference time. Thus, the prediction is the
label with the maximum likelihood. Further, they study the
impact of this type of classifier on out-of-distribution scenarios considering common image corruptions and adversarial
perturbations.
Kim et al. propose to solve the image registration
task using diffusion models. This is achieved via two networks, a diffusion network, as per , and a deformation
network that is based on U-Net, as described in .
Given two images (one static, one moving), the role of the
former network is to assess the deformation between the
two images, and feed the result to the latter network, which
predicts the deformation fields, enabling sample generation.
This method also has the ability to synthesize the deformations through the whole transition. The authors carried out
experiments for different tasks, one on 2D facial expressions
and one on 3D brain images. The results confirm that the
model is capable of producing qualitative and accurate
registration fields.
Jeanneret et al. apply diffusion models for counterfactual explanations. The method starts from a noised
query image, and generates a sample with an unconditional
DDPM. With the generated sample, the gradients required
for guidance are computed. Then, one step of the reversed
guided process is applied. The output is further used in the
next reverse steps.
Sanchez et al. adapt the work of Dhariwal et al. 
for counterfactual image generation. As in , the denoising
process is guided by classifier gradients to generate samples
from the desired counterfactual class. The key contribution
is the algorithm used to retrieve the latent representation
of the original image, from where the denoising process
starts. Their algorithm inverts the deterministic sampling
procedure of and maps each original image to a unique
latent representation.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
Nie et al. demonstrate how a diffusion model can
be used as a defensive mechanism for adversarial attacks.
Given an adversarial image, it gets diffused up until an
optimally computed time step. The result is then reversed
by the model, producing a purified sample at the end. To
optimize the computations of solving the reverse-time SDE,
the adjoint sensitivity method of Li et al. is used for the
gradient score calculations.
In the context of few-shot learning, an image generator
based on diffusion models is proposed by Giannone et
al. . Given a small set of images that condition the synthesis, a visual transformer encodes these, and the resulting
context representation is integrated (via two different techniques) into the U-Net model employed in the denoising
Wang et al. present a framework based on diffusion
models for semantic image synthesis. Leveraging the U-Net
architecture of diffusion models, the input noise is supplied
to the encoder, while the semantic label map is passed to the
decoder using multi-layer spatially-adaptive normalization
operators . To further improve the sampling quality
and the condition on the semantic label map, an empty
map is also supplied to the sampling method to generate
the unconditional noise. Then, the final noise uses both
estimates.
Concerning the task of restoring images negatively affected by various weather conditions (e.g. snow, rain),
¨Ozdenizci et al. demonstrate how diffusion models
can be used. They condition the denoising process on the
degraded image by concatenating it channel-wise to the
denoised sample, at every time step. In order to deal with
varying image sizes, at every step, the sample is divided
into overlapping patches, passed in parallel through the
model, and combined back by averaging the overlapping
pixels. The employed diffusion model is based on the U-Net
architecture, as presented in , , but modified to accept
two concatenated images as input.
Formulating the task of image restoration as a linear
inverse problem, Kawar et al. propose the use of diffusion models. Inspired by Kawar et al. , the linear
degradation matrix is decomposed using singular value
decomposition, such that both the input and the output
can be mapped onto the spectral space of the matrix where
the diffusion process is carried out. Leveraging the pretrained diffusion models from and , the evaluation
is conducted on various tasks: super-resolution, deblurring,
colorization and inpainting.
Theoretical Contributions
Huang et al. demonstrate how the method proposed by
Song et al. is linked with maximizing a lower bound on
the marginal likelihood of the reverse SDE. Moreover, they
verify their theoretical contribution with image generation
experiments on CIFAR-10 and MNIST.
CLOSING REMARKS AND FUTURE DIRECTIONS
In this paper, we reviewed the advancements made by the
research community in developing and applying diffusion
models to various computer vision tasks. We identified
three primary formulations of diffusion modeling based
on: DDPMs, NCSNs, and SDEs. Each formulation obtains
remarkable results in image generation, surpassing GANs
while increasing the diversity of the generated samples. The
outstanding results of diffusion models are achieved while
the research is still in its early phase. Although we observed
that the main focus is on conditional and unconditional
image generation, there are still many tasks to be explored
and further improvements to be realized.
Limitations. The most significant disadvantage of diffusion
models remains the need to perform multiple steps at
inference time to generate only one sample. Despite the
important amount of research conducted in this direction,
GANs are still faster at producing images. Other issues
of diffusion models can be linked to the commonly used
strategy to employ CLIP embeddings for text-to-image generation. For example, Ramesh et al. highlight that their
model struggles to generate readable text in an image and
motivate the behavior by stating that CLIP embeddings do
not contain information about spelling. Therefore, when
such embeddings are used for conditioning the denoising
process, the model can inherit this kind of issues.
Future directions. To reduce the uncertainty level, diffusion
models generally avoid taking large steps during sampling.
Indeed, taking small steps ensures the data sample generated at each step is explained by the learned Gaussian
distribution. A similar behavior is observed when applying
gradient descent to optimize neural networks. Indeed, taking a large step in the negative direction of the gradient,
i.e. using a very large learning rate, can lead to updating the
model to a region with high uncertainty, having no control
over the loss value. In future work, transferring update
rules borrowed from efficient optimizers to diffusion models
could perhaps lead to a more efficient sampling (generation)
Aside from the current tendency of researching more
efficient diffusion models, future work can study diffusion
models applied in other computer vision tasks, such as image dehazing, video anomaly detection, or visual question
answering. Even if we found some works studying anomaly
detection in medical images – , this task could also
be explored in other domains, such as video surveillance or
industrial inspection.
An interesting research direction is to assess the quality
and utility of the representation space learned by diffusion
models in discriminative tasks. This could be carried out in
at least two distinct ways. In a direct way, by learning some
discriminative model on top of the latent representations
provided by a denoising model, to address some classification or regression task. In an indirect way, by augmenting
training sets with realistic samples generated by diffusion
models. The latter direction might be more suitable for tasks
such as object detection, where inpainting diffusion models
could do a good job at blending in new objects in images.
Another future work direction is to employ conditional
diffusion models to simulate possible futures in video. The
generated videos could further be given as input to reinforcement learning models.
Recent diffusion models have shown impressive
text-to-video synthesis capabilities compared to the previous state of the art, significantly reducing the number of
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 14, NO. 8, AUGUST 2022
artifacts and reaching an unprecedented generative performance. However, we believe this direction requires more
attention in future work, as the generated videos are rather
short. Hence, modeling long-term temporal relations and
interactions between objects remains an open challenge.
In future, the research on diffusion models can also
be expanded towards learning multi-purpose models that
solve multiple tasks at once. Creating a diffusion model to
generate multiple types of outputs, while being conditioned
on various types of data, e.g. text, class labels or images,
might take us closer to understanding the necessary steps
towards developing artificial general intelligence (AGI).
ACKNOWLEDGMENTS
This work was supported by a grant of the Romanian Ministry of Education and Research, CNCS - UEFISCDI, project
no. PN-III-P2-2.1-PED-2021-0195, contract no. 690/2022,
within PNCDI III.