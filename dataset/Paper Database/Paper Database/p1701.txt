Image Segmentation Using Deep Learning:
Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos
Abstract—Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding,
medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various
algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide
range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using
deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad
spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks,
encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative
models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most
widely used datasets, report performances, and discuss promising future research directions in this area.
Index Terms—Image segmentation, deep learning, convolutional neural networks, encoder-decoder models, recurrent models,
generative models, semantic segmentation, instance segmentation, medical image segmentation.
INTRODUCTION
MAGE segmentation is an essential component in many
visual understanding systems. It involves partitioning
images (or video frames) into multiple segments or objects
 . Segmentation plays a central role in a broad range of applications , including medical image analysis (e.g., tumor
boundary extraction and measurement of tissue volumes),
autonomous vehicles (e.g., navigable surface and pedestrian
detection), video surveillance, and augmented reality to
count a few. Numerous image segmentation algorithms have
been developed in the literature, from the earliest methods,
such as thresholding , histogram-based bundling, regiongrowing , k-means clustering , watersheds , to more
advanced algorithms such as active contours , graph cuts
 , conditional and Markov random ﬁelds , and sparsitybased - methods. Over the past few years, however,
deep learning (DL) models have yielded a new generation of
image segmentation models with remarkable performance
improvements —often achieving the highest accuracy rates
on popular benchmarks— resulting in a paradigm shift in
the ﬁeld. For example, Figure 1 presents image segmentation
outputs of a popular deep learning model, DeepLabv3 .
Image segmentation can be formulated as a classiﬁcation problem of pixels with semantic labels (semantic
segmentation) or partitioning of individual objects (instance
segmentation). Semantic segmentation performs pixel-level
labeling with a set of object categories (e.g., human, car,
tree, sky) for all image pixels, thus it is generally a harder
undertaking than image classiﬁcation, which predicts a
single label for the entire image. Instance segmentation
extends semantic segmentation scope further by detecting
and delineating each object of interest in the image (e.g.,
partitioning of individual persons).
Our survey covers the most recent literature in image
segmentation and discusses more than a hundred deep
learning-based segmentation methods proposed until 2019.
We provide a comprehensive review and insights on different
Fig. 1. Segmentation results of DeepLabV3 on sample images.
aspects of these methods, including the training data, the
choice of network architectures, loss functions, training strategies, and their key contributions. We present a comparative
summary of the performance of the reviewed methods and
discuss several challenges and potential future directions for
deep learning-based image segmentation models.
We group deep learning-based works into the following
categories based on their main technical contributions:
Fully convolutional networks
Convolutional models with graphical models
Encoder-decoder based models
Multi-scale and pyramid network based models
R-CNN based models (for instance segmentation)
Dilated convolutional models and DeepLab family
Recurrent neural network based models
Attention-based models
Generative models and adversarial training
Convolutional models with active contour models
Other models
 
Some the key contributions of this survey paper can be
summarized as follows:
This survey covers the contemporary literature with
respect to segmentation problem, and overviews more
than 100 segmentation algorithms proposed till 2019,
grouped into 10 categories.
We provide a comprehensive review and an insightful
analysis of different aspects of segmentation algorithms using deep learning, including the training
data, the choice of network architectures, loss functions, training strategies, and their key contributions.
We provide an overview of around 20 popular image
segmentation datasets, grouped into 2D, 2.5D (RGB-
D), and 3D images.
We provide a comparative summary of the properties and performance of the reviewed methods for
segmentation purposes, on popular benchmarks.
We provide several challenges and potential future directions for deep learning-based image segmentation.
The remainder of this survey is organized as follows:
Section 2 provides an overview of popular deep neural
network architectures that serve as the backbone of many
modern segmentation algorithms. Section 3 provides a
comprehensive overview of the most signiﬁcant state-of-theart deep learning based segmentation models, more than 100
till 2020. We also discuss their strengths and contributions
over previous works here. Section 4 reviews some of the
most popular image segmentation datasets and their characteristics. Section 5.1 reviews popular metrics for evaluating
deep-learning-based segmentation models. In Section 5.2, we
report the quantitative results and experimental performance
of these models. In Section 6, we discuss the main challenges
and future directions for deep learning-based segmentation
methods. Finally, we present our conclusions in Section 7.
OVERVIEW OF DEEP NEURAL NETWORKS
This section provides an overview of some of the most
prominent deep learning architectures used by the computer
vision community, including convolutional neural networks
(CNNs) , recurrent neural networks (RNNs) and long
short term memory (LSTM) , encoder-decoders ,
and generative adversarial networks (GANs) . With
the popularity of deep learning in recent years, several
other deep neural architectures have been proposed, such as
transformers, capsule networks, gated recurrent units, spatial
transformer networks, etc., which will not be covered here.
It is worth mentioning that in some cases the DL-models
can be trained from scratch on new applications/datasets
(assuming a sufﬁcient quantity of labeled training data), but
in many cases there are not enough labeled data available
to train a model from scratch and one can use transfer
learning to tackle this problem. In transfer learning, a model
trained on one task is re-purposed on another (related) task,
usually by some adaptation process toward the new task. For
example, one can imagine adapting an image classiﬁcation
model trained on ImageNet to a different task, such as texture
classiﬁcation, or face recognition. In image segmentation
case, many people use a model trained on ImageNet (a
larger dataset than most of image segmentation datasets), as
the encoder part of the network, and re-train their model
from those initial weights. The assumption here is that those
pre-trained models should be able to capture the semantic
information of the image required for segmentation, and
therefore enabling them to train the model with less labeled
Convolutional Neural Networks (CNNs)
CNNs are among the most successful and widely used
architectures in the deep learning community, especially
for computer vision tasks. CNNs were initially proposed by
Fukushima in his seminal paper on the “Neocognitron” ,
based on the hierarchical receptive ﬁeld model of the visual
cortex proposed by Hubel and Wiesel. Subsequently, Waibel
et al. introduced CNNs with weights shared among
temporal receptive ﬁelds and backpropagation training for
phoneme recognition, and LeCun et al. developed a CNN
architecture for document recognition (Figure 2).
Convolution
Convolution
Fully-connected
Fig. 2. Architecture of convolutional neural networks. From .
CNNs mainly consist of three type of layers: i) convolutional layers, where a kernel (or ﬁlter) of weights is convolved
in order to extract features; ii) nonlinear layers, which apply
an activation function on feature maps (usually elementwise) in order to enable the modeling of non-linear functions
by the network; and iii) pooling layers, which replace a
small neighborhood of a feature map with some statistical
information (mean, max, etc.) about the neighborhood and
reduce spatial resolution. The units in layers are locally
connected; that is, each unit receives weighted inputs from
a small neighborhood, known as the receptive ﬁeld, of
units in the previous layer. By stacking layers to form
multi-resolution pyramids, the higher-level layers learn
features from increasingly wider receptive ﬁelds. The main
computational advantage of CNNs is that all the receptive
ﬁelds in a layer share weights, resulting in a signiﬁcantly
smaller number of parameters than fully-connected neural
networks. Some of the most well-known CNN architectures
include: AlexNet , VGGNet , ResNet , GoogLeNet
 , MobileNet , and DenseNet .
Recurrent Neural Networks (RNNs) and the LSTM
RNNs are widely used to process sequential data, such
as speech, text, videos, and time-series, where data at any
given time/position depends on previously encountered
data. At each time-stamp the model collects the input from
the current time Xi and the hidden state from the previous
step hi−1, and outputs a target value and a new hidden state
(Figure 3).
RNNs are typically problematic with long sequences
as they cannot capture long-term dependencies in many
real-world applications (although they exhibit no theoretical
Fig. 3. Architecture of a simple recurrent neural network.
limitations in this regard) and often suffer from gradient
vanishing or exploding problems. However, a type of RNNs
called Long Short Term Memory (LSTM) is designed to
avoid these issues. The LSTM architecture (Figure 4) includes
three gates (input gate, output gate, forget gate), which
regulate the ﬂow of information into and out from a memory
cell, which stores values over arbitrary time intervals.
Fig. 4. Architecture of a standard LSTM module. Courtesy of Karpathy.
Encoder-Decoder and Auto-Encoder Models
Encoder-Decoder models are a family of models which learn
to map data-points from an input domain to an output
domain via a two-stage network: The encoder, represented
by an encoding function z = f(x), compresses the input into
a latent-space representation; the decoder, y = g(z), aims
to predict the output from the latent space representation
 , . The latent representation here essentially refers to
a feature (vector) representation, which is able to capture
the underlying semantic information of the input that is
useful for predicting the output. These models are extremely
popular in image-to-image translation problems, as well as
for sequence-to-sequence models in NLP. Figure 5 illustrates
the block-diagram of a simple encoder-decoder model. These
models are usually trained by minimizing the reconstruction
loss L(y, ˆy), which measures the differences between the
ground-truth output y and the subsequent reconstruction
ˆy. The output here could be an enhanced version of the
image (such as in image de-blurring, or super-resolution),
or a segmentation map. Auto-encoders are special case of
encoder-decoder models in which the input and output are
Fig. 5. The architecture of a simple encoder-decoder model.
Generative Adversarial Networks (GANs)
GANs are a newer family of deep learning models . They
consist of two networks—a generator and a discriminator
(Figure 6). The generator network G = z →y in the
conventional GAN learns a mapping from noise z (with
a prior distribution) to a target distribution y, which is
similar to the “real” samples. The discriminator network D
attempts to distinguish the generated samples (“fakes”) from
the “real” ones. The GAN loss function may be written as
LGAN = Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 −D(G(z)))].
We can regard the GAN as a minimax game between G
and D, where D is trying to minimize its classiﬁcation
error in distinguishing fake samples from real ones, hence
maximizing the loss function, and G is trying to maximize
the discriminator network’s error, hence minimizing the loss
function. After training the model, the trained generator
model would be G∗= arg minG maxD LGAN In practice,
this function may not provide enough gradient for effectively
training G, specially initially (when D can easily discriminate fake samples from real ones). Instead of minimizing
Ez∼pz(z)[log(1 −D(G(z)))], a possible solution is to train it
to maximize Ez∼pz(z)[log(D(G(z)))].
D-dimensional Noise Vector
Discriminator Network
Fig. 6. Architecture of a generative adversarial network.
Since the invention of GANs, researchers have endeavored to improve/modify GANs several ways. For example,
Radford et al. proposed a convolutional GAN model,
which works better than fully-connected networks when
used for image generation. Mirza proposed a conditional
GAN model that can generate images conditioned on class
labels, which enables one to generate samples with speciﬁed
labels. Arjovsky et al. proposed a new loss function based
on the Wasserstein (a.k.a. earth mover’s distance) to better
estimate the distance for cases in which the distribution of
real and generated samples are non-overlapping (hence the
Kullback–Leiber divergence is not a good measure of the
distance). For additional works, we refer the reader to .
DL-BASED IMAGE SEGMENTATION MODELS
This section provides a detailed review of more than a
hundred deep learning-based segmentation methods proposed until 2019, grouped into 10 categories (based on their
model architecture). It is worth mentioning that there are
some pieces that are common among many of these works,
such as having encoder and decoder parts, skip-connections,
multi-scale analysis, and more recently the use of dilated
convolution. Because of this, it is difﬁcult to mention the
unique contributions of each work, but easier to group
them based on their underlying architectural contribution
over previous works. Besides the architectural categorization
of these models, one can also group them based on the
segmentation goal into: semantic, instance, panoptic, and
depth segmentation categories. But due to the big difference
in terms of volume of work in those tasks, we decided to
follow the architectural grouping.
Fully Convolutional Networks
Long et al. proposed one of the ﬁrst deep learning works
for semantic image segmentation, using a fully convolutional network (FCN). An FCN (Figure 7) includes only
convolutional layers, which enables it to take an image of
arbitrary size and produce a segmentation map of the same
size. The authors modiﬁed existing CNN architectures, such
as VGG16 and GoogLeNet, to manage non-ﬁxed sized input
and output, by replacing all fully-connected layers with the
fully-convolutional layers. As a result, the model outputs a
spatial segmentation map instead of classiﬁcation scores.
Fig. 7. A fully convolutional image segmentation network. The FCN learns
to make dense, pixel-wise predictions. From .
Through the use of skip connections in which feature
maps from the ﬁnal layers of the model are up-sampled and
fused with feature maps of earlier layers (Figure 8), the model
combines semantic information (from deep, coarse layers)
and appearance information (from shallow, ﬁne layers) in
order to produce accurate and detailed segmentations. The
model was tested on PASCAL VOC, NYUDv2, and SIFT Flow,
and achieved state-of-the-art segmentation performance.
Fig. 8. Skip connections combine coarse, high-level information and ﬁne,
low-level information. From .
This work is considered a milestone in image segmentation, demonstrating that deep networks can be trained for
semantic segmentation in an end-to-end manner on variablesized images. However, despite its popularity and effectiveness, the conventional FCN model has some limitations—it
is not fast enough for real-time inference, it does not take
into account the global context information in an efﬁcient
way, and it is not easily transferable to 3D images. Several
efforts have attempted to overcome some of the limitations
of the FCN.
For instance, Liu et al. proposed a model called
ParseNet, to address an issue with FCN—ignoring global
context information. ParseNet adds global context to FCNs by
using the average feature for a layer to augment the features
at each location. The feature map for a layer is pooled over
the whole image resulting in a context vector. This context
vector is normalized and unpooled to produce new feature
maps of the same size as the initial ones. These feature
maps are then concatenated. In a nutshell, ParseNet is an
FCN with the described module replacing the convolutional
layers (Figure 9).
Fig. 9. ParseNet, showing the use of extra global context to produce
smoother segmentation (d) than an FCN (c). From .
FCNs have been applied to a variety of segmentation
problems, such as brain tumor segmentation , instanceaware semantic segmentation , skin lesion segmentation
 , and iris segmentation .
Convolutional Models With Graphical Models
As discussed, FCN ignores potentially useful scene-level
semantic context. To integrate more context, several approaches incorporate probabilistic graphical models, such
as Conditional Random Fields (CRFs) and Markov Random
Field (MRFs), into DL architectures.
Chen et al. proposed a semantic segmentation algorithm based on the combination of CNNs and fully connected
CRFs (Figure 10). They showed that responses from the ﬁnal
layer of deep CNNs are not sufﬁciently localized for accurate
object segmentation (due to the invariance properties that
make CNNs good for high level tasks such as classiﬁcation).
To overcome the poor localization property of deep CNNs,
they combined the responses at the ﬁnal CNN layer with a
fully-connected CRF. They showed that their model is able to
localize segment boundaries at a higher accuracy rate than it
was possible with previous methods.
Fig. 10. A CNN+CRF model. The coarse score map of a CNN is upsampled via interpolated interpolation, and fed to a fully-connected CRF
to reﬁne the segmentation result. From .
Schwing and Urtasun proposed a fully-connected
deep structured network for image segmentation. They
presented a method that jointly trains CNNs and fullyconnected CRFs for semantic image segmentation, and
achieved encouraging results on the challenging PASCAL
VOC 2012 dataset. In , Zheng et al. proposed a similar
semantic segmentation approach integrating CRF with CNN.
In another relevant work, Lin et al. proposed an
efﬁcient algorithm for semantic segmentation based on
contextual deep CRFs. They explored “patch-patch” context
(between image regions) and “patch-background” context to
improve semantic segmentation through the use of contextual information.
Liu et al. proposed a semantic segmentation algorithm
that incorporates rich information into MRFs, including highorder relations and mixture of label contexts. Unlike previous
works that optimized MRFs using iterative algorithms, they
proposed a CNN model, namely a Parsing Network, which
enables deterministic end-to-end computation in a single
forward pass.
Encoder-Decoder Based Models
Another popular family of deep models for image segmentation is based on the convolutional encoder-decoder
architecture. Most of the DL-based segmentation works use
some kind of encoder-decoder models. We group these works
into two categories, encoder-decoder models for general
segmentation, and for medical image segmentation (to better
distinguish between applications).
Encoder-Decoder Models for General Segmentation
Noh et al. published an early paper on semantic
segmentation based on deconvolution (a.k.a. transposed
convolution). Their model (Figure 11) consists of two parts,
an encoder using convolutional layers adopted from the
VGG 16-layer network and a deconvolutional network that
takes the feature vector as input and generates a map of
pixel-wise class probabilities. The deconvolution network
is composed of deconvolution and unpooling layers, which
identify pixel-wise class labels and predict segmentation
Fig. 11. Deconvolutional semantic segmentation. Following a convolution
network based on the VGG 16-layer net, is a multi-layer deconvolution
network to generate the accurate segmentation map. From .
This network achieved promising performance on the
PASCAL VOC 2012 dataset, and obtained the best accuracy
(72.5%) among the methods trained with no external data at
In another promising work known as SegNet, Badrinarayanan et al. proposed a convolutional encoderdecoder architecture for image segmentation (Figure 12).
Similar to the deconvolution network, the core trainable
segmentation engine of SegNet consists of an encoder network, which is topologically identical to the 13 convolutional
layers in the VGG16 network, and a corresponding decoder
network followed by a pixel-wise classiﬁcation layer. The
main novelty of SegNet is in the way the decoder upsamples
its lower resolution input feature map(s); speciﬁcally, it
uses pooling indices computed in the max-pooling step
of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to up-sample.
The (sparse) up-sampled maps are then convolved with
trainable ﬁlters to produce dense feature maps. SegNet is also
signiﬁcantly smaller in the number of trainable parameters
than other competing architectures. A Bayesian version of
SegNet was also proposed by the same authors to model the
uncertainty inherent to the convolutional encoder-decoder
network for scene segmentation .
Fig. 12. SegNet has no fully-connected layers; hence, the model is fully
convolutional. A decoder up-samples its input using the transferred pool
indices from its encoder to produce a sparse feature map(s). From .
Another popular model in this category is the recentlydeveloped segmentation network, high-resolution network
(HRNet) Figure 13. Other than recovering highresolution representations as done in DeConvNet, SegNet,
U-Net and V-Net, HRNet maintains high-resolution representations through the encoding process by connecting the
high-to-low resolution convolution streams in parallel, and
repeatedly exchanging the information across resolutions.
Many of the more recent works on semantic segmentation
use HRNet as the backbone by exploiting contextual models,
such as self-attention and its extensions.
Several other works adopt transposed convolutions, or
encoder-decoders for image segmentation, such as Stacked
Deconvolutional Network (SDN) , Linknet , W-Net
 , and locality-sensitive deconvolution networks for RGB-
D segmentation . One limitation of Encoder-Decoder
based models is the loss of ﬁne-grained information of the
image, due to the loss of high-resolution representations
through the encoding process. This issue is however addressed in some of the recent architectures such as HR-Net.
Encoder-Decoder Models for Medical and Biomedical
Image Segmentation
There are several models initially developed for medical/biomedical image segmentation, which are inspired by
FCNs and encoder-decoder models. U-Net , and V-Net
 , are two well-known such architectures, which are now
also being used outside the medical domain.
Ronneberger et al. proposed the U-Net for segmenting
biological microscopy images. Their network and training
strategy relies on the use of data augmentation to learn
from the very few annotated images effectively. The U-Net
architecture (Figure 14) comprises two parts, a contracting
path to capture context, and a symmetric expanding path that
enables precise localization. The down-sampling or contracting part has a FCN-like architecture that extracts features
with 3 × 3 convolutions. The up-sampling or expanding
part uses up-convolution (or deconvolution), reducing the
number of feature maps while increasing their dimensions.
Feature maps from the down-sampling part of the network
are copied to the up-sampling part to avoid losing pattern
information. Finally, a 1×1 convolution processes the feature
maps to generate a segmentation map that categorizes each
Fig. 13. Illustrating the HRNet architecture. It consists of parallel high-to-low resolution convolution streams with repeated information exchange
across multi-resolution steams. There are four stages. The 1st stage consists of high-resolution convolutions. The 2nd (3rd, 4th) stage repeats
two-resolution (three-resolution, four-resolution) blocks. From .
pixel of the input image. U-Net was trained on 30 transmitted
light microscopy images, and it won the ISBI cell tracking
challenge 2015 by a large margin.
Fig. 14. The U-net model. The blue boxes denote feature map blocks
with their indicated shapes. From .
Various extensions of U-Net have been developed for
different kinds of images. For example, Cicek proposed
a U-Net architecture for 3D images. Zhou et al. developed
a nested U-Net architecture. U-Net has also been applied
to various other problems. For example, Zhang et al. 
developed a road segmentation/extraction algorithm based
V-Net is another well-known, FCN-based model, which
was proposed by Milletari et al. for 3D medical image
segmentation. For model training, they introduced a new
objective function based on Dice coefﬁcient, enabling the
model to deal with situations in which there is a strong imbalance between the number of voxels in the foreground and
background. The network was trained end-to-end on MRI
volumes of prostate, and learns to predict segmentation for
the whole volume at once. Some of the other relevant works
on medical image segmentation includes Progressive Dense
V-net (PDV-Net) et al. for fast and automatic segmentation
of pulmonary lobes from chest CT images, and the 3D-CNN
encoder for lesion segmentation .
Multi-Scale and Pyramid Network Based Models
Multi-scale analysis, a rather old idea in image processing,
has been deployed in various neural network architectures.
One of the most prominent models of this sort is the Feature
Pyramid Network (FPN) proposed by Lin et al. , which
was developed mainly for object detection but was then also
applied to segmentation. The inherent multi-scale, pyramidal
hierarchy of deep CNNs was used to construct feature
Fig. 15. The PSPN architecture. A CNN produces the feature map
and a pyramid pooling module aggregates the different sub-region
representations. Up-sampling and concatenation are used to form the
ﬁnal feature representation from which, the ﬁnal pixel-wise prediction is
obtained through convolution. From .
pyramids with marginal extra cost. To merge low and high
resolution features, the FPN is composed of a bottom-up
pathway, a top-down pathway and lateral connections. The
concatenated feature maps are then processed by a 3 × 3
convolution to produce the output of each stage. Finally,
each stage of the top-down pathway generates a prediction
to detect an object. For image segmentation, the authors use
two multi-layer perceptrons (MLPs) to generate the masks.
Zhao et al. developed the Pyramid Scene Parsing
Network (PSPN), a multi-scale network to better learn
the global context representation of a scene (Figure 15).
Different patterns are extracted from the input image using
a residual network (ResNet) as a feature extractor, with a
dilated network. These feature maps are then fed into a
pyramid pooling module to distinguish patterns of different
scales. They are pooled at four different scales, each one
corresponding to a pyramid level and processed by a 1 × 1
convolutional layer to reduce their dimensions. The outputs
of the pyramid levels are up-sampled and concatenated with
the initial feature maps to capture both local and global
context information. Finally, a convolutional layer is used to
generate the pixel-wise predictions.
Ghiasi and Fowlkes developed a multi-resolution
reconstruction architecture based on a Laplacian pyramid
that uses skip connections from higher resolution feature
maps and multiplicative gating to successively reﬁne segment boundaries reconstructed from lower-resolution maps.
They showed that, while the apparent spatial resolution of
convolutional feature maps is low, the high-dimensional feature representation contains signiﬁcant sub-pixel localization
information.
There are other models using multi-scale analysis for
segmentation, such as DM-Net (Dynamic Multi-scale Filters
Network) , Context contrasted network and gated multiscale aggregation (CCN) , Adaptive Pyramid Context
Network (APC-Net) , Multi-scale context intertwining
(MSCI) , and salient object segmentation .
R-CNN Based Models (for Instance Segmentation)
The regional convolutional network (R-CNN) and its extensions (Fast R-CNN, Faster R-CNN, Maksed-RCNN) have
proven successful in object detection applications. In particular, the Faster R-CNN architecture (Figure 16) developed
for object detection uses a region proposal network (RPN) to
propose bounding box candidates. The RPN extracts a Region
of Interest (RoI), and a RoIPool layer computes features
from these proposals in order to infer the bounding box
coordinates and the class of the object. Some of the extensions
of R-CNN have been heavily used to address the instance
segmentation problem; i.e., the task of simultaneously performing object detection and semantic segmentation.
Fig. 16. Faster R-CNN architecture. Courtesy of .
In one extension of this model, He et al. proposed
a Mask R-CNN for object instance segmentation, which
beat all previous benchmarks on many COCO challenges.
This model efﬁciently detects objects in an image while
simultaneously generating a high-quality segmentation mask
for each instance. Mask R-CNN is essentially a Faster R-
CNN with 3 output branches (Figure 17)—the ﬁrst computes
the bounding box coordinates, the second computes the
associated classes, and the third computes the binary mask to
segment the object. The Mask R-CNN loss function combines
the losses of the bounding box coordinates, the predicted
class, and the segmentation mask, and trains all of them
jointly. Figure 18 shows the Mask-RCNN result on some
sample images.
Fig. 17. Mask R-CNN architecture for instance segmentation. From .
Fig. 18. Mask R-CNN results on sample images from the COCO test set.
From .
The Path Aggregation Network (PANet) proposed by Liu
et al. is based on the Mask R-CNN and FPN models
(Figure 19). The feature extractor of the network uses an
FPN architecture with a new augmented bottom-up pathway
improving the propagation of low-layer features. Each stage
of this third pathway takes as input the feature maps of the
previous stage and processes them with a 3×3 convolutional
layer. The output is added to the same stage feature maps
of the top-down pathway using a lateral connection and
these feature maps feed the next stage. As in the Mask R-
CNN, the output of the adaptive feature pooling layer feeds
three branches. The ﬁrst two use a fully connected layer to
generate the predictions of the bounding box coordinates
and the associated object class. The third processes the RoI
with an FCN to predict the object mask.
Fig. 19. The Path Aggregation Network. (a) FPN backbone. (b) Bottomup path augmentation. (c) Adaptive feature pooling. (d) Box branch. (e)
Fully-connected fusion. Courtesy of .
Dai et al. developed a multi-task network for instanceaware semantic segmentation, that consists of three networks,
respectively differentiating instances, estimating masks, and
categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features.
Hu et al. proposed a new partially-supervised training
paradigm, together with a novel weight transfer function,
that enables training instance segmentation models on a large
set of categories, all of which have box annotations, but only
a small fraction of which have mask annotations.
Chen et al. developed an instance segmentation
model, MaskLab (Figure 20), by reﬁning object detection with
semantic and direction features based on Faster R-CNN. This
model produces three outputs, box detection, semantic segmentation, and direction prediction. Building on the Faster-
RCNN object detector, the predicted boxes provide accurate
localization of object instances. Within each region of interest,
MaskLab performs foreground/background segmentation
by combining semantic and direction prediction.
Fig. 20. The MaskLab model. MaskLab generates three outputs—reﬁned
box predictions (from Faster R-CNN), semantic segmentation logits for
pixel-wise classiﬁcation, and direction prediction logits for predicting each
pixel’s direction toward its instance center. From .
Another interesting model is Tensormask, proposed by
Chen et al. , which is based on dense sliding window
instance segmentation. They treat dense instance segmentation as a prediction task over 4D tensors and present
a general framework that enables novel operators on 4D
tensors. They demonstrate that the tensor view leads to large
gains over baselines and yields results comparable to Mask
R-CNN. TensorMask achieves promising results on dense
object segmentation.
Many other instance segmentation models have been
developed based on R-CNN, such as those developed for
mask proposals, including R-FCN , DeepMask ,
PolarMask , boundary-aware instance segmentation ,
and CenterMask . It is worth noting that there is another
promising research direction that attempts to solve the
instance segmentation problem by learning grouping cues for
bottom-up segmentation, such as Deep Watershed Transform
 , real-time instance segmentation , and Semantic
Instance Segmentation via Deep Metric Learning .
Dilated Convolutional Models and DeepLab Family
Dilated convolution (a.k.a. “atrous” convolution) introduces
another parameter to convolutional layers, the dilation rate.
The dilated convolution (Figure 21) of a signal x(i) is deﬁned
as yi = PK
k=1 x[i + rk]w[k], where r is the dilation rate that
deﬁnes a spacing between the weights of the kernel w. For
example, a 3 × 3 kernel with a dilation rate of 2 will have
the same size receptive ﬁeld as a 5 × 5 kernel while using
only 9 parameters, thus enlarging the receptive ﬁeld with no
increase in computational cost. Dilated convolutions have
been popular in the ﬁeld of real-time segmentation, and many
recent publications report the use of this technique. Some
of most important include the DeepLab family , multiscale context aggregation , dense upsampling convolution
and hybrid dilatedconvolution (DUC-HDC) , densely
connected Atrous Spatial Pyramid Pooling (DenseASPP) ,
and the efﬁcient neural network (ENet) .
Fig. 21. Dilated convolution. A 3 × 3 kernel at different dilation rates.
DeepLabv1 and DeepLabv2 are among some of
the most popular image segmentation approaches, developed
by Chen et al.. The latter has three key features. First is the use
of dilated convolution to address the decreasing resolution
in the network (caused by max-pooling and striding). Second
is Atrous Spatial Pyramid Pooling (ASPP), which probes an
incoming convolutional feature layer with ﬁlters at multiple
sampling rates, thus capturing objects as well as image
context at multiple scales to robustly segment objects at
multiple scales. Third is improved localization of object
boundaries by combining methods from deep CNNs and
probabilistic graphical models. The best DeepLab (using a
ResNet-101 as backbone) has reached a 79.7% mIoU score
on the 2012 PASCAL VOC challenge, a 45.7% mIoU score on
the PASCAL-Context challenge and a 70.4% mIoU score on
the Cityscapes challenge. Figure 22 illustrates the Deeplab
model, which is similar to , the main difference being the
use of dilated convolution and ASPP.
Fig. 22. The DeepLab model. A CNN model such as VGG-16 or ResNet-
101 is employed in fully convolutional fashion, using dilated convolution.
A bilinear interpolation stage enlarges the feature maps to the original
image resolution. Finally, a fully connected CRF reﬁnes the segmentation
result to better capture the object boundaries. From 
Subsequently, Chen et al. proposed DeepLabv3,
which combines cascaded and parallel modules of dilated
convolutions. The parallel convolution modules are grouped
in the ASPP. A 1 × 1 convolution and batch normalisation
are added in the ASPP. All the outputs are concatenated and
processed by another 1 × 1 convolution to create the ﬁnal
output with logits for each pixel.
In 2018, Chen et al. released Deeplabv3+, which uses
an encoder-decoder architecture (Figure 23), including atrous
separable convolution, composed of a depthwise convolution
(spatial convolution for each channel of the input) and
pointwise convolution (1×1 convolution with the depthwise
convolution as input). They used the DeepLabv3 framework
as encoder. The most relevant model has a modiﬁed Xception
backbone with more layers, dilated depthwise separable
convolutions instead of max pooling and batch normalization.
The best DeepLabv3+ pretrained on the COCO and the
JFT datasets has obtained a 89.0% mIoU score on the 2012
PASCAL VOC challenge.
Fig. 23. The DeepLabv3+ model. From .
Recurrent Neural Network Based Models
While CNNs are a natural ﬁt for computer vision problems, they are not the only possibility. RNNs are useful in
modeling the short/long term dependencies among pixels to
(potentially) improve the estimation of the segmentation map.
Using RNNs, pixels may be linked together and processed
sequentially to model global contexts and improve semantic
segmentation. One challenge, though, is the natural 2D
structure of images.
Visin et al. proposed an RNN-based model for
semantic segmentation called ReSeg. This model is mainly
based on another work, ReNet , which was developed for
image classiﬁcation. Each ReNet layer is composed of four
RNNs that sweep the image horizontally and vertically in
both directions, encoding patches/activations, and providing
relevant global information. To perform image segmentation
with the ReSeg model (Figure 24), ReNet layers are stacked on
top of pre-trained VGG-16 convolutional layers that extract
generic local features. ReNet layers are then followed by
up-sampling layers to recover the original image resolution
in the ﬁnal predictions. Gated Recurrent Units (GRUs) are
used because they provide a good balance between memory
usage and computational power.
Fig. 24. The ReSeg model. The pre-trained VGG-16 feature extractor
network is not shown. From .
In another work, Byeon et al. developed a pixellevel segmentation and classiﬁcation of scene images using
long-short-term-memory (LSTM) network. They investigated
two-dimensional (2D) LSTM networks for images of natural
scenes, taking into account the complex spatial dependencies
of labels. In this work, classiﬁcation, segmentation, and
context integration are all carried out by 2D LSTM networks,
allowing texture and spatial model parameters to be learned
within a single model.
Liang et al. proposed a semantic segmentation model
based on the Graph Long Short-Term Memory (Graph LSTM)
network, a generalization of LSTM from sequential data
or multidimensional data to general graph-structured data.
Instead of evenly dividing an image to pixels or patches
in existing multi-dimensional LSTM structures (e.g., row,
grid and diagonal LSTMs), they take each arbitrary-shaped
superpixel as a semantically consistent node, and adaptively
construct an undirected graph for the image, where the spatial relations of the superpixels are naturally used as edges.
Figure 25 presents a visual comparison of the traditional
pixel-wise RNN model and graph-LSTM model. To adapt
the Graph LSTM model to semantic segmentation (Figure 26)
, LSTM layers built on a super-pixel map are appended
on the convolutional layers to enhance visual features with
global structure context. The convolutional features pass
through 1 × 1 convolutional ﬁlters to generate the initial
conﬁdence maps for all labels. The node updating sequence
for the subsequent Graph LSTM layers is determined by
the conﬁdence-drive scheme based on the initial conﬁdence
maps, and then the Graph LSTM layers can sequentially
update the hidden states of all superpixel nodes.
Xiang and Fox proposed Data Associated Recurrent
Neural Networks (DA-RNNs), for joint 3D scene mapping
and semantic labeling. DA-RNNs use a new recurrent neural
network architecture for semantic labeling on RGB-D videos.
The output of the network is integrated with mapping
techniques such as Kinect-Fusion in order to inject semantic
information into the reconstructed 3D scene.
Hu et al. developed a semantic segmentation algorithm based on natural language expression, using a
combination of CNN to encode the image and LSTM to
Fig. 25. Comparison between the graph-LSTM model and traditional
pixel-wise RNN models. From .
Fig. 26. The graph-LSTM model for semantic segmentation. From .
encode its natural language description. This is different
from traditional semantic segmentation over a predeﬁned
set of semantic classes, as, e.g., the phrase “two men sitting
on the right bench” requires segmenting only the two people
on the right bench and no one standing or sitting on another
bench. To produce pixel-wise segmentation for language
expression, they propose an end-to-end trainable recurrent
and convolutional model that jointly learns to process visual
and linguistic information (Figure 27). In the considered
model, a recurrent LSTM network is used to encode the
referential expression into a vector representation, and an
FCN is used to extract a spatial feature map from the image
and output a spatial response map for the target object. An
example segmentation result of this model (for the query
“people in blue coat”) is shown in Figure 28.
Fig. 27. The CNN+LSTM architecture for segmentation from natural
language expressions. From .
Fig. 28. Segmentation masks generated for the query “people in blue
coat”. From .
One limitation of RNN based models is that, due to the
sequential nature these models, they will be slower than their
CNN counterpart, since this sequential calculation cannot be
parallelized easily.
Attention-Based Models
Attention mechanisms have been persistently explored in
computer vision over the years, and it is therefore not
surprising to ﬁnd publications that apply such mechanisms
to semantic segmentation.
Chen et al. proposed an attention mechanism that
learns to softly weight multi-scale features at each pixel
location. They adapt a powerful semantic segmentation
model and jointly train it with multi-scale images and
the attention model (Figure 29). The attention mechanism
outperforms average and max pooling, and it enables the
model to assess the importance of features at different
positions and scales.
Fig. 29. Attention-based semantic segmentation model. The attention
model learns to assign different weights to objects of different scales;
e.g., the model assigns large weights on the small person (green dashed
circle) for features from scale 1.0, and large weights on the large child
(magenta dashed circle) for features from scale 0.5. From .
In contrast to other works in which convolutional classi-
ﬁers are trained to learn the representative semantic features
of labeled objects, Huang et al. proposed a semantic
segmentation approach using reverse attention mechanisms.
Their Reverse Attention Network (RAN) architecture (Figure 30) trains the model to capture the opposite concept (i.e.,
features that are not associated with a target class) as well.
The RAN is a three-branch network that performs the direct,
and reverse-attention learning processes simultaneously.
Fig. 30. The reverse attention network for segmentation. From .
Li et al. developed a Pyramid Attention Network
for semantic segmentation. This model exploits the impact
of global contextual information in semantic segmentation.
They combined attention mechanisms and spatial pyramids
to extract precise dense features for pixel labeling, instead of
complicated dilated convolutions and artiﬁcially designed
decoder networks.
More recently, Fu et al. proposed a dual attention
network for scene segmentation, which can capture rich contextual dependencies based on the self-attention mechanism.
Fig. 31. The GAN for semantic segmentation. From .
Speciﬁcally, they append two types of attention modules
on top of a dilated FCN which models the semantic interdependencies in spatial and channel dimensions, respectively.
The position attention module selectively aggregates the
feature at each position by a weighted sum of the features at
all positions.
Various other works explore attention mechanisms for
semantic segmentation, such as OCNet which proposed
an object context pooling inspired by self-attention mechanism, Expectation-Maximization Attention (EMANet) ,
Criss-Cross Attention Network (CCNet) , end-to-end
instance segmentation with recurrent attention , a pointwise spatial attention network for scene parsing , and a
discriminative feature network (DFN) , which comprises
two sub-networks: a Smooth Network (that contains a
Channel Attention Block and global average pooling to select
the more discriminative features) and a Border Network (to
make the bilateral features of the boundary distinguishable).
Generative Models and Adversarial Training
Since their introduction, GANs have been applied to a wide
range tasks in computer vision, and have been adopted for
image segmentation too.
Luc et al. proposed an adversarial training approach
for semantic segmentation. They trained a convolutional
semantic segmentation network (Figure 31), along with an
adversarial network that discriminates ground-truth segmentation maps from those generated by the segmentation
network. They showed that the adversarial training approach
leads to improved accuracy on the Stanford Background and
PASCAL VOC 2012 datasets.
Souly et al. proposed semi-weakly supervised
semantic segmentation using GANs. It consists of a generator
network providing extra training examples to a multi-class
classiﬁer, acting as discriminator in the GAN framework,
that assigns sample a label y from the K possible classes or
marks it as a fake sample (extra class).
In another work, Hung et al. developed a framework for semi-supervised semantic segmentation using an
adversarial network. They designed an FCN discriminator
to differentiate the predicted probability maps from the
ground truth segmentation distribution, considering the
spatial resolution. The considered loss function of this model
contains three terms: cross-entropy loss on the segmentation
ground truth, adversarial loss of the discriminator network,
and semi-supervised loss based on the conﬁdence map; i.e.,
the output of the discriminator.
Xue et al. proposed an adversarial network with
multi-scale L1 Loss for medical image segmentation. They
used an FCN as the segmentor to generate segmentation label
maps, and proposed a novel adversarial critic network with a
multi-scale L1 loss function to force the critic and segmentor
to learn both global and local features that capture long and
short range spatial relationships between pixels.
Various other publications report on segmentation models
based on adversarial training, such as Cell Image Segmentation Using GANs , and segmentation and generation of
the invisible parts of objects .
CNN Models With Active Contour Models
The exploration of synergies between FCNs and Active
Contour Models (ACMs) has recently attracted research
interest. One approach is to formulate new loss functions
that are inspired by ACM principles. For example, inspired
by the global energy formulation of , Chen et al. 
proposed a supervised loss layer that incorporated area and
size information of the predicted masks during training of
an FCN and tackled the problem of ventricle segmentation
in cardiac MRI.
A different approach initially sought to utilize the ACM
merely as a post-processor of the output of an FCN and
several efforts attempted modest co-learning by pre-training
the FCN. One example of an ACM post-processor for the
task of semantic segmentation of natural images is the
work by Le et al. in which level-set ACMs are implemented as RNNs. Deep Active Contours by Rupprecht et al.
 , is another example. For medical image segmentation,
Hatamizadeh et al. proposed an integrated Deep Active
Lesion Segmentation (DALS) model that trains the FCN backbone to predict the parameter functions of a novel, locallyparameterized level-set energy functional. In another relevant
effort, Marcos et al. proposed Deep Structured Active
Contours (DSAC), which combines ACMs and pre-trained
FCNs in a structured prediction framework for building
instance segmentation (albeit with manual initialization)
in aerial images. For the same application, Cheng et al.
 proposed the Deep Active Ray Network (DarNet),
which is similar to DSAC, but with a different explicit
ACM formulation based on polar coordinates to prevent
contour self-intersection. A truly end-to-end backpropagation trainable, fully-integrated FCN-ACM combination was
recently introduced by Hatamizadeh et al. , dubbed
Deep Convolutional Active Contours (DCAC).
Other Models
In addition to the above models, there are several other
popular DL architectures for segmentation, such as the
following: Context Encoding Network (EncNet) that uses
a basic feature extractor and feeds the feature maps into
a Context Encoding Module . ReﬁneNet , which
is a multi-path reﬁnement network that explicitly exploits
all the information available along the down-sampling
process to enable high-resolution prediction using long-range
residual connections. Seednet , which introduced an automatic seed generation technique with deep reinforcement
learning that learns to solve the interactive segmentation
problem. ”Object-Contextual Representations” (OCR) ,
which learns object regions under the supervision of the
ground-truth, and computes the object region representation,
and the relation between each pixel and each object region,
and augment the representation pixels with the objectcontextual representation. Yet additional models include
BoxSup , Graph convolutional networks , Wide
ResNet , Exfuse (enhancing low-level and high-level
features fusion) , Feedforward-Net , saliency-aware
models for geodesic video segmentation , dual image
segmentation (DIS) , FoveaNet (Perspective-aware scene
parsing) , Ladder DenseNet , Bilateral segmentation
network (BiSeNet) , Semantic Prediction Guidance
for Scene Parsing (SPGNet) , Gated shape CNNs
 , Adaptive context network (AC-Net) , Dynamicstructured semantic propagation network (DSSPN) ,
symbolic graph reasoning (SGR) , CascadeNet ,
Scale-adaptive convolutions (SAC) , Uniﬁed perceptual
parsing (UperNet) , segmentation by re-training and selftraining , densely connected neural architecture search
 , hierarchical multi-scale attention .
Panoptic segmentation is also another interesting
segmentation problem with rising popularity, and there are
already several interesting works on this direction, including
Panoptic Feature Pyramid Network , attention-guided
network for Panoptic segmentation , Seamless Scene
Segmentation , panoptic deeplab , uniﬁed panoptic
segmentation network , efﬁcient panoptic segmentation
Figure 32 illustrates the timeline of popular DL-based
works for semantic segmentation, as well as instance segmentation since 2014. Given the large number of works
developed in the last few years, we only show some of the
most representative ones.
IMAGE SEGMENTATION DATASETS
In this section we provide a summary of some of the
most widely used image segmentation datasets. We group
these datasets into 3 categories—2D images, 2.5D RGB-D
(color+depth) images, and 3D images—and provide details
about the characteristics of each dataset. The listed datasets
have pixel-wise labels, which can be used for evaluating
model performance.
It is worth mentioning that some of these works, use data
augmentation to increase the number of labeled samples,
specially the ones which deal with small datasets (such
as in medical domain). Data augmentation serves to increase the number of training samples by applying a set
of transformation (either in the data space, or feature space,
or sometimes both) to the images (i.e., both the input image
and the segmentation map). Some typical transformations
include translation, reﬂection, rotation, warping, scaling,
color space shifting, cropping, and projections onto principal
components. Data augmentation has proven to improve the
performance of the models, especially when learning from
limited datasets, such as those in medical image analysis. It
can also be beneﬁcial in yielding faster convergence, decreasing the chance of over-ﬁtting, and enhancing generalization.
For some small datasets, data augmentation has been shown
to boost model performance more than 20%.
Fig. 32. The timeline of DL-based segmentation algorithms for 2D images, from 2014 to 2020. Orange, green, andn yellow blocks refer to semantic,
instance, and panoptic segmentation algorithms respectively.
2D Datasets
The majority of image segmentation research has focused on
2D images; therefore, many 2D image segmentation datasets
are available. The following are some of the most popular:
PASCAL Visual Object Classes (VOC) is one of
most popular datasets in computer vision, with annotated
images available for 5 tasks—classiﬁcation, segmentation,
detection, action recognition, and person layout. Nearly all
popular segmentation algorithms reported in the literature
have been evaluated on this dataset. For the segmentation
task, there are 21 classes of object labels—vehicles, household,
animals, aeroplane, bicycle, boat, bus, car, motorbike, train,
bottle, chair, dining table, potted plant, sofa, TV/monitor,
bird, cat, cow, dog, horse, sheep, and person (pixel are labeled
as background if they do not belong to any of these classes).
This dataset is divided into two sets, training and validation,
with 1,464 and 1,449 images, respectively. There is a private
test set for the actual challenge. Figure 33 shows an example
image and its pixel-wise label.
Fig. 33. An example image from the PASCAL VOC dataset. From 
PASCAL Context is an extension of the PASCAL
VOC 2010 detection challenge, and it contains pixel-wise
labels for all training images. It contains more than 400 classes
(including the original 20 classes plus backgrounds from
PASCAL VOC segmentation), divided into three categories
(objects, stuff, and hybrids). Many of the object categories
of this dataset are too sparse and; therefore, a subset of 59
frequent classes are usually selected for use.
Microsoft Common Objects in Context (MS COCO)
 is another large-scale object detection, segmentation,
and captioning dataset. COCO includes images of complex
everyday scenes, containing common objects in their natural
contexts. This dataset contains photos of 91 objects types,
with a total of 2.5 million labeled instances in 328k images.
Figure 34 shows the difference between MS-COCO labels and
the previous datasets for a given sample image. The detection
challenge includes more than 80 classes, providing more than
82k images for training, 40.5k images for validation, and
more than 80k images for its test set.
Fig. 34. A sample image and its segmentation map in COCO, and its
comparison with previous datasets. From .
Cityscapes is a large-scale database with a focus on
semantic understanding of urban street scenes. It contains a
diverse set of stereo video sequences recorded in street scenes
from 50 cities, with high quality pixel-level annotation of 5k
frames, in addition to a set of 20k weakly annotated frames.
It includes semantic and dense pixel annotations of 30 classes,
grouped into 8 categories—ﬂat surfaces, humans, vehicles,
constructions, objects, nature, sky, and void. Figure 35 shows
four sample segmentation maps from this dataset.
Fig. 35. Three sample images with their corresponding segmentation
maps from the Cityscapes dataset. From .
ADE20K / MIT Scene Parsing (SceneParse150) offers a
standard training and evaluation platform for scene parsing
algorithms. The data for this benchmark comes from the
ADE20K dataset , which contains more than 20K scenecentric images exhaustively annotated with objects and object
parts. The benchmark is divided into 20K images for training,
2K images for validation, and another batch of images for
testing. There are 150 semantic categories in this dataset.
SiftFlow includes 2,688 annotated images from a
subset of the LabelMe database. The 256 × 256 pixel images
are based on 8 different outdoor scenes, among them streets,
mountains, ﬁelds, beaches, and buildings. All images belong
to one of 33 semantic classes.
Stanford background contains outdoor images of
scenes from existing datasets, such as LabelMe, MSRC, and
PASCAL VOC. It contains 715 images with at least one
foreground object. The dataset is pixel-wise annotated, and
can be used for semantic scene understanding. Semantic
and geometric labels for this dataset were obtained using
Amazon’s Mechanical Turk (AMT).
Berkeley Segmentation Dataset (BSD) contains
12,000 hand-labeled segmentations of 1,000 Corel dataset
images from 30 human subjects. It aims to provide an
empirical basis for research on image segmentation and
boundary detection. Half of the segmentations were obtained
from presenting the subject a color image and the other half
from presenting a grayscale image.
Youtube-Objects contains videos collected from
YouTube, which include objects from ten PASCAL VOC
classes (aeroplane, bird, boat, car, cat, cow, dog, horse,
motorbike, and train). The original dataset did not contain
pixel-wise annotations (as it was originally developed for
object detection, with weak annotations). However, Jain et
al. manually annotated a subset of 126 sequences, and
then extracted a subset of frames to further generate semantic
labels. In total, there are about 10,167 annotated 480x360 pixel
frames available in this dataset.
KITTI is one of the most popular datasets for
mobile robotics and autonomous driving. It contains hours of
videos of trafﬁc scenarios, recorded with a variety of sensor
modalities (including high-resolution RGB, grayscale stereo
cameras, and a 3D laser scanners). The original dataset does
not contain ground truth for semantic segmentation, but
researchers have manually annotated parts of the dataset
for research purposes. For example, Alvarez et al. 
generated ground truth for 323 images from the road
detection challenge with 3 classes, road, vertical, and sky.
Other Datasets are available for image segmentation
purposes too, such as Semantic Boundaries Dataset (SBD)
 , PASCAL Part , SYNTHIA , and Adobe’s
Portrait Segmentation .
2.5D Datasets
With the availability of affordable range scanners, RGB-D
images have became popular in both research and industrial
applications. The following RGB-D datasets are some of the
most popular:
NYU-D V2 consists of video sequences from a
variety of indoor scenes, recorded by the RGB and depth
cameras of the Microsoft Kinect. It includes 1,449 densely
labeled pairs of aligned RGB and depth images from more
than 450 scenes taken from 3 cities. Each object is labeled
with a class and an instance number (e.g., cup1, cup2, cup3,
etc.). It also contains 407,024 unlabeled frames. This dataset is
relatively small compared to other existing datasets. Figure 36
shows a sample image and its segmentation map.
Fig. 36. A sample from the NYU V2 dataset. From left: the RGB image,
pre-processed depth, and set of labels. From .
SUN-3D is a large-scale RGB-D video dataset that
contains 415 sequences captured for 254 different spaces in
41 different buildings; 8 sequences are annotated and more
will be annotated in the future. Each annotated frame comes
with the semantic segmentation of the objects in the scene,
as well as information about the camera pose.
SUN RGB-D provides an RGB-D benchmark for
the goal of advancing the state-of-the-art in all major scene
understanding tasks. It is captured by four different sensors
and contains 10,000 RGB-D images at a scale similar to
PASCAL VOC. The whole dataset is densely annotated and
includes 146,617 2D polygons and 58,657 3D bounding boxes
with accurate object orientations, as well as the 3D room
category and layout for scenes.
UW RGB-D Object Dataset contains 300 common
household objects recorded using a Kinect style 3D camera.
The objects are organized into 51 categories, arranged
using WordNet hypernym-hyponym relationships (similar to
ImageNet). This dataset was recorded using a Kinect style 3D
camera that records synchronized and aligned 640×480 pixel
RGB and depth images at 30 Hz. This dataset also includes
8 annotated video sequences of natural scenes, containing
objects from the dataset (the UW RGB-D Scenes Dataset).
ScanNet is an RGB-D video dataset containing 2.5
million views in more than 1,500 scans, annotated with
3D camera poses, surface reconstructions, and instancelevel semantic segmentations. To collect these data, an easyto-use and scalable RGB-D capture system was designed
that includes automated surface reconstruction, and the
semantic annotation was crowd-sourced. Using this data
helped achieve state-of-the-art performance on several 3D
scene understanding tasks, including 3D object classiﬁcation,
semantic voxel labeling, and CAD model retrieval.
3D Datasets
3D image datasets are popular in robotic, medical image
analysis, 3D scene analysis, and construction applications.
Three dimensional images are usually provided via meshes
or other volumetric representations, such as point clouds.
Here, we mention some of the popular 3D datasets.
Stanford 2D-3D: This dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains,
with instance-level semantic and geometric annotations ,
and is collected in 6 indoor areas. It contains over 70,000
RGB images, along with the corresponding depths, surface
normals, semantic annotations, global XYZ images as well as
camera information.
ShapeNet Core: ShapeNetCore is a subset of the full
ShapeNet dataset with single clean 3D models and
manually veriﬁed category and alignment annotations .
It covers 55 common object categories with about 51,300
unique 3D models.
Sydney Urban Objects Dataset: This dataset contains
a variety of common urban road objects, collected in the
central business district of Sydney, Australia. There are
631 individual scans of objects across classes of vehicles,
pedestrians, signs and trees .
PERFORMANCE REVIEW
In this section, we ﬁrst provide a summary of some of
the popular metrics used in evaluating the performance
of segmentation models, and then we provide the quantitative performance of the promising DL-based segmentation
models on popular datasets.
Metrics For Segmentation Models
Ideally, a model should be evaluated in multiple respects,
such as quantitative accuracy, speed (inference time), and
storage requirements (memory footprint). However, most of
the research works so far, focus on the metrics for evaluating
the model accuracy. Below we summarize the most popular
metrics for assessing the accuracy of segmentation algorithms. Although quantitative metrics are used to compare
different models on benchmarks, the visual quality of model
outputs is also important in deciding which model is best
(as human is the ﬁnal consumer of many of the models
developed for computer vision applications).
Pixel accuracy simply ﬁnds the ratio of pixels properly
classiﬁed, divided by the total number of pixels. For K + 1
classes (K foreground classes and the background) pixel
accuracy is deﬁned as Eq 1:
where pij is the number of pixels of class i predicted as
belonging to class j.
Mean Pixel Accuracy (MPA) is the extended version of
PA, in which the ratio of correct pixels is computed in a
per-class manner and then averaged over the total number
of classes, as in Eq 2:
Intersection over Union (IoU) or the Jaccard Index is
one of the most commonly used metrics in semantic segmentation. It is deﬁned as the area of intersection between the
predicted segmentation map and the ground truth, divided
by the area of union between the predicted segmentation
map and the ground truth:
IoU = J(A, B) = |A ∩B|
where A and B denote the ground truth and the predicted
segmentation maps, respectively. It ranges between 0 and 1.
Mean-IoU is another popular metric, which is deﬁned as
the average IoU over all classes. It is widely used in reporting
the performance of modern segmentation algorithms.
Precision / Recall / F1 score are popular metrics for reporting the accuracy of many of the classical image segmentation
models. Precision and recall can be deﬁned for each class, as
well as at the aggregate level, as follows:
Precision =
TP + FP, Recall
where TP refers to the true positive fraction, FP refers to the
false positive fraction, and FN refers to the false negative
fraction. Usually we are interested into a combined version
of precision and recall rates. A popular such a metric is
called the F1 score, which is deﬁned as the harmonic mean
of precision and recall:
F1-score = 2 Prec Rec
Prec + Rec.
Dice coefﬁcient is another popular metric for image
segmentation (and is more commonly used in medical image
analysis), which can be deﬁned as twice the overlap area
of predicted and ground-truth maps, divided by the total
number of pixels in both images. The Dice coefﬁcient is very
similar to the IoU:
Dice = 2|A ∩B|
|A| + |B|.
When applied to boolean data (e.g., binary segmentation
maps), and referring to the foreground as a positive class,
the Dice coefﬁcient is essentially identical to the F1 score,
deﬁned as Eq 7:
2TP + FP + FN = F1.
Quantitative Performance of DL-Based Models
In this section we tabulate the performance of several of the
previously discussed algorithms on popular segmentation
benchmarks. It is worth mentioning that although most
models report their performance on standard datasets and
use standard metrics, some of them fail to do so, making
across-the-board comparisons difﬁcult. Furthermore, only
a small percentage of publications provide additional information, such as execution time and memory footprint,
in a reproducible way, which is important to industrial
applications of segmentation models (such as drones, selfdriving cars, robotics, etc.) that may run on embedded
consumer devices with limited computational power and
storage, making fast, light-weight models crucial.
The following tables summarize the performances of
several of the prominent DL-based segmentation models
on different datasets. Table 1 focuses on the PASCAL VOC
test set. Clearly, there has been much improvement in the
accuracy of the models since the introduction of the FCN, the
ﬁrst DL-based image segmentation model. Table 2 focuses
on the Cityscape test dataset. The latest models feature about
23% relative gain over the initial FCN model on this dataset.
Table 3 focuses on the MS COCO stuff test set. This dataset is
more challenging than PASCAL VOC, and Cityescapes, as the
highest mIoU is approximately 40%. Table 4 focuses on the
ADE20k validation set. This dataset is also more challenging
than the PASCAL VOC and Cityescapes datasets.
Table 5 provides the performance of prominent instance
segmentation algorithms on COCO test-dev 2017 dataset, in
terms of average precision, and their speed. Table 6 provides
Accuracies of segmentation models on the PASCAL VOC test set.
(* Refers to the model pre-trained on another dataset (such as
MS-COCO, ImageNet, or JFT-300M).)
CRF-RNN 
CRF-RNN∗ 
BoxSup* 
Piecewise∗ 
DeepLab-CRF 
ResNet-101
ResNet-152
ReﬁneNet 
ResNet-152
Wide ResNet 
WideResNet-38
PSPNet 
ResNet-101
DeeplabV3 
ResNet-101
PSANet 
ResNet-101
EncNet 
ResNet-101
ResNet-101
Exfuse 
ResNet-101
DenseNet-161
ResNet-101
DM-Net∗ 
ResNet-101
APC-Net∗ 
ResNet-101
EMANet 
ResNet-101
DeeplabV3+ 
Xception-71
Exfuse 
ResNeXt-131
ResNet-152
EMANet 
ResNet-152
DeeplabV3+∗ 
Xception-71
EfﬁcientNet+NAS-FPN 
Accuracies of segmentation models on the Cityescapes dataset.
FCN-8s 
Dilation10 
DeeplabV2 
ResNet-101
ReﬁneNet 
ResNet-101
FoveaNet 
ResNet-101
Ladder DenseNet 
Ladder DenseNet-169
ResNet-101
DUC-HDC 
ResNet-101
Wide ResNet 
WideResNet-38
PSPNet 
ResNet-101
BiSeNet 
ResNet-101
ResNet-101
PSANet 
ResNet-101
DenseASPP 
DenseNet-161
SPGNet 
2xResNet-50
DANet 
ResNet-101
CCNet 
ResNet-101
DeeplabV3 
ResNet-101
AC-Net 
ResNet-101
ResNet-101
GS-CNN 
WideResNet
HRNetV2+OCR (w/ASPP) 
HRNetV2-W48
Hierarchical MSA 
the performance of prominent panoptic segmentation algorithms on MS-COCO val dataset, in terms of panoptic quality
 . Finally, Table 7 summarizes the performance of several
prominent models for RGB-D segmentation on the NYUD-v2
and SUN-RGBD datasets.
To summarize the tabulated data, there has been signiﬁcant progress in the performance of deep segmentation
Accuracies of segmentation models on the MS COCO stuff dataset.
ReﬁneNet 
ResNet-101
Ladder DenseNet-101
DANet 
DSSPN 
ResNet-101
EMA-Net 
ResNet-101
ResNet-101
DANet 
ResNet-101
EMA-Net 
AC-Net 
ResNet-101
HRNetV2-W48
Accuracies of segmentation models on the ADE20k validation dataset.
DilatedNet 
CascadeNet 
ReﬁneNet 
ResNet-152
PSPNet 
ResNet-101
PSPNet 
ResNet-269
EncNet 
ResNet-101
ResNet-101
PSANet 
ResNet-101
UperNet 
ResNet-101
DSSPN 
ResNet-101
DM-Net 
ResNet-101
AC-Net 
ResNet-101
Instance Segmentation Models Performance on COCO test-dev 2017
YOLACT-550 
YOLACT-700 
RetinaMask 
TensorMask 
SharpMask 
Mask-RCNN 
CenterMask 
Panoptic Segmentation Models Performance on the MS-COCO val
dataset. ∗denotes use of deformable convolution.
Panoptic FPN 
Panoptic FPN 
ResNet-101
AU-Net 
Panoptic-DeepLab 
Xception-71
OANet 
OANet 
ResNet-101
AdaptIS 
AdaptIS 
ResNet-101
UPSNet∗ 
OCFusion∗ 
OCFusion∗ 
ResNet-101
OCFusion∗ 
ResNeXt-101
Performance of segmentation models on the NYUD-v2, and SUN-RGBD
datasets, in terms of mIoU, and mean Accuracy (mAcc).
Mutex 
MS-CNN 
Joint-Seg 
SegNet 
Structured Net 
B-SegNet 
3D-GNN 
LSD-Net 
ReﬁneNet 
D-aware CNN 
RDFNet 
G-Aware Net 
MTI-Net 
models over the past 5–6 years, with a relative improvement of 25%-42% in mIoU on different datasets. However,
some publications suffer from lack of reproducibility for
multiple reasons—they report performance on non-standard
benchmarks/databases, or they report performance only on
arbitrary subsets of the test set from a popular benchmark, or
they do not adequately describe the experimental setup and
sometimes evaluate the model performance only on a subset
of object classes. Most importantly, many publications do not
provide the source-code for their model implementations.
However, with the increasing popularity of deep learning
models, the trend has been positive and many research
groups are moving toward reproducible frameworks and
open-sourcing their implementations.
CHALLENGES AND OPPORTUNITIES
There is not doubt that image segmentation has beneﬁted
greatly from deep learning, but several challenges lie ahead.
We will next introduce some of the promising research
directions that we believe will help in further advancing
image segmentation algorithms.
More Challenging Datasets
Several large-scale image datasets have been created for
semantic segmentation and instance segmentation. However,
there remains a need for more challenging datasets, as well
as datasets for different kinds of images. For still images,
datasets with a large number of objects and overlapping
objects would be very valuable. This can enable training
models that are better at handling dense object scenarios,
as well as large overlaps among objects as is common in
real-world scenarios.
With the rising popularity of 3D image segmentation,
especially in medical image analysis, there is also a strong
need for large-scale 3D images datasets. These datasets
are more difﬁcult to create than their lower dimensional
counterparts. Existing datasets for 3D image segmentation
available are typically not large enough, and some are
synthetic, and therefore larger and more challenging 3D
image datasets can be very valuable.
Interpretable Deep Models
While DL-based models have achieved promising performance on challenging benchmarks, there remain open questions about these models. For example, what exactly are
deep models learning? How should we interpret the features
learned by these models? What is a minimal neural architecture that can achieve a certain segmentation accuracy on a
given dataset? Although some techniques are available to
visualize the learned convolutional kernels of these models, a
concrete study of the underlying behavior/dynamics of these
models is lacking. A better understanding of the theoretical
aspects of these models can enable the development of better
models curated toward various segmentation scenarios.
Weakly-Supervised and Unsupervised Learning
Weakly-supervised (a.k.a. few shot learning) and unsupervised learning are becoming very active research
areas. These techniques promise to be specially valuable
for image segmentation, as collecting labeled samples for
segmentation problem is problematic in many application
domains, particularly so in medical image analysis. The
transfer learning approach is to train a generic image segmentation model on a large set of labeled samples (perhaps
from a public benchmark), and then ﬁne-tune that model
on a few samples from some speciﬁc target application.
Self-supervised learning is another promising direction that
is attracting much attraction in various ﬁelds. There are
many details in images that that can be captured to train
a segmentation models with far fewer training samples,
with the help of self-supervised learning. Models based
on reinforcement learning could also be another potential
future direction, as they have scarcely received attention for
image segmentation. For example, MOREL introduced
a deep reinforcement learning approach for moving object
segmentation in videos.
Real-time Models for Various Applications
In many applications, accuracy is the most important factor;
however, there are applications in which it is also critical to
have segmentation models that can run in near real-time, or
at least near common camera frame rates (at least 25 frames
per second). This is useful for computer vision systems that
are, for example, deployed in autonomous vehicles. Most of
the current models are far from this frame-rate; e.g., FCN-
8 takes roughly 100 ms to process a low-resolution image.
Models based on dilated convolution help to increase the
speed of segmentation models to some extent, but there is
still plenty of room for improvement.
Memory Efﬁcient Models
Many modern segmentation models require a signiﬁcant
amount of memory even during the inference stage. So
far, much effort has been directed towards improving the
accuracy of such models, but in order to ﬁt them into
speciﬁc devices, such as mobile phones, the networks must be
simpliﬁed. This can be done either by using simpler models,
or by using model compression techniques, or even training
a complex model and then using knowledge distillation
techniques to compress it into a smaller, memory efﬁcient
network that mimics the complex model.
3D Point-Cloud Segmentation
Numerous works have focused on 2D image segmentation,
but much fewer have addressed 3D point-cloud segmentation. However, there has been an increasing interest in pointcloud segmentation, which has a wide range of applications,
in 3D modeling, self-driving cars, robotics, building modeling, etc. Dealing with 3D unordered and unstructured data
such as point clouds poses several challenges. For example,
the best way to apply CNNs and other classical deep learning
architectures on point clouds is unclear. Graph-based deep
models can be a potential area of exploration for point-cloud
segmentation, enabling additional industrial applications of
these data.
Application Scenarios
In this section, we brieﬂy investigate some application
scenarios of recent DL-based segmentation methods, and
some challenges ahead. Most notably, these methods have
been successfully applied to segment satellite images in the
ﬁeld of remote sensing , including techniques for urban
planning or precision agriculture . Remote sensing
images collected by airborne platforms and drones
 have also been segmented using DL-based techniques,
offering the opportunity to address important environmental
problems such as those involving climate change. The main
challenges of segmenting this kind of images are related to
the very large dimensionality of the data (often collected by
imaging spectrometers with hundreds or even thousands of
spectral bands) and the limited ground-truth information to
evaluate the accuracy of the results obtained by segmentation
algorithms. Another very important application ﬁeld for DLbased segmentation has been medical imaging . Here,
an opportunity is to design standardized image databases
that can be used to evaluate fast spreading new diseases
and pandemics. Last but not least, we should also mention
DL-based segmentation techniques in biology and
evaluation of construction materials , which offer the
opportunity to address highly relevant application domains
but are subject to challenges related to the massive volume of
the related image data and the limited reference information
for validation purposes.
CONCLUSIONS
We have surveyed more than 100 recent image segmentation
algorithms based on deep learning models, which have
achieved impressive performance in various image segmentation tasks and benchmarks, grouped into ten categories such
as: CNN and FCN, RNN, R-CNN, dilated CNN, attentionbased models, generative and adversarial models, among
others. We summarized quantitative performance analyses
of these models on some popular benchmarks, such as
the PASCAL VOC, MS COCO, Cityscapes, and ADE20k
datasets. Finally, we discussed some of the open challenges
and potential research directions for image segmentation that
could be pursued in the coming years.
ACKNOWLEDGMENTS
The authors would like to thank Tsung-Yi Lin from Google
Brain, and Jingdong Wang and Yuhui Yuan from Microsoft
Research Asia, for reviewing this work, and providing very
helpful comments and suggestions.