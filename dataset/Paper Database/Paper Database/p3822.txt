Noisy intermediate-scale quantum (NISQ) algorithms
Kishor Bharti,1, ∗Alba Cervera-Lierta,2, 3, ∗Thi Ha Kyaw,2, 3, ∗Tobias Haug,4 Sumner Alperin-Lea,3 Abhinav
Anand,3 Matthias Degroote,2, 3, 5 Hermanni Heimonen,1 Jakob S. Kottmann,2, 3 Tim Menke,6, 7, 8 Wai-Keong
Mok,1 Sukin Sim,9 Leong-Chuan Kwek,1, 10, 11, † and Alán Aspuru-Guzik2, 3, 12, 13, ‡
1Centre for Quantum Technologies, National University of Singapore 117543, Singapore
2Department of Computer Science, University of Toronto, Toronto, Ontario M5S 2E4, Canada
3Chemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, Ontario M5G 1Z8, Canada
4QOLS, Blackett Laboratory, Imperial College London SW7 2AZ, UK
5current address: Boehringer Ingelheim, Amsterdam, Netherlands
6Department of Physics, Harvard University, Cambridge, MA 02138, USA
7Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
8Department of Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
9Department of Chemistry and Chemical Biology, Harvard University, Cambridge, MA 02138, USA
10MajuLab, CNRS-UNS-NUS-NTU International Joint Research Unit UMI 3654, Singapore
11National Institute of Education and Institute of Advanced Studies, Nanyang Technological University 637616, Singapore
12Vector Institute for Artiﬁcial Intelligence, Toronto, Ontario M5S 1M1, Canada
13Canadian Institute for Advanced Research, Toronto, Ontario M5G 1Z8, Canada
 
A universal fault-tolerant quantum computer that can solve eﬃciently problems such as
integer factorization and unstructured database search requires millions of qubits with
low error rates and long coherence times. While the experimental advancement towards
realizing such devices will potentially take decades of research, noisy intermediate-scale
quantum (NISQ) computers already exist. These computers are composed of hundreds
of noisy qubits, i.e. qubits that are not error-corrected, and therefore perform imperfect
operations in a limited coherence time. In the search for quantum advantage with these
devices, algorithms have been proposed for applications in various disciplines spanning
physics, machine learning, quantum chemistry and combinatorial optimization. The goal
of such algorithms is to leverage the limited available resources to perform classically
challenging tasks.
In this review, we provide a thorough summary of NISQ computational paradigms and algorithms. We discuss the key structure of these algorithms,
their limitations, and advantages. We additionally provide a comprehensive overview
of various benchmarking and software tools useful for programming and testing NISQ
I. Introduction
A. Computational complexity theory in a nutshell
B. Experimental progress
C. NISQ and near-term
D. Scope of the review
II. Building blocks of variational quantum algorithms
A. Objective function
1. Pauli strings
2. Fidelity
3. Other objective functions
B. Parameterized quantum circuits
1. Problem-inspired ansätze
2. Hardware-eﬃcient ansätze
C. Measurement
D. Parameter optimization
1. Gradient-based approaches
2. Gradient-free approaches
∗These authors contributed equally to this work.
 
 
 
† 
‡ 
3. Resource-aware optimizers
III. Other NISQ approaches
A. Quantum annealing
B. Gaussian boson sampling
1. The protocol
2. Applications
C. Analog quantum simulation
1. Implementations
2. Programmable quantum simulators
D. Digital-analog quantum simulation and
computation
E. Iterative quantum assisted eigensolver
IV. Theoretical challenges
A. Barren plateaus
B. Expressibility of variational ansätze
C. Reachability
D. Theoretical guarantees of the QAOA algorithm
V. Programming and Maximizing NISQ utility
A. Quantum error mitigation (QEM)
1. Zero-noise extrapolation
2. Probabilistic error cancellation
3. Other QEM strategies
B. Circuit compilation
1. Native and universal gate sets
2. Circuit decompositions
3. The qubit mapping problem
 
4. Resource-aware circuit design
C. Quantum software tools
VI. Applications
A. Many-body physics and chemistry
1. Qubit encodings
2. Constructing electronic Hamiltonians
3. Variational quantum eigensolver
4. Variational quantum eigensolver for excited
5. Hamiltonian simulation
6. Quantum information scrambling and
thermalization
7. Simulating open quantum systems
8. Nonequilibrium steady state
9. Gibbs state preparation
10. Simulation of topological phases and phase
transitions
11. Many-body ground state preparation
12. Quantum autoencoder
13. Quantum computer-aided design
B. Machine learning
1. Supervised learning
2. Unsupervised learning
3. Reinforcement learning
C. Combinatorial optimization
1. Max-Cut
2. Other combinatorial optimization problems
D. Numerical solvers
1. Variational quantum factoring
2. Singular value decomposition
3. Linear system problem
4. Non-linear diﬀerential equations
E. Other applications
1. Quantum foundations
2. Quantum optimal control
3. Quantum metrology
4. Fidelity estimation
5. Quantum error correction
6. Nuclear physics
7. Entanglement properties
VII. Benchmarking
A. Randomized benchmarking
B. Quantum volume
C. Cross-entropy benchmarking
D. Application benchmarks
VIII. Outlook
A. NISQ goals
B. Long-term goal: fault-tolerant quantum computing
Acknowledgements
References
Appendices
A. NISQ algorithms and tools tables
A. Tables of applications
B. Table of software packages
C. Table of external libraries
B. Classical optimization strategies
A. Gradient-based approaches
B. Gradient-free approaches
C. Resource-aware optimizers
C. NISQ Applications for Finance
A. Portfolio optimization
B. Fraud detection
D. Unitary t-design
I. INTRODUCTION
Quantum computing originated in the eighties when
physicists started to speculate about computational models that integrate the laws of quantum mechanics . Starting with the pioneering works of Benioﬀand
Deutsch, which involved the study of quantum Turing
machines and the notion of universal quantum computation , the ﬁeld continued to
develop towards its natural application: the simulation of
quantum systems . Arguably, the drive for quantum computing took
oﬀin 1994 when Peter Shor provided an eﬃcient quantum
algorithm for ﬁnding prime factors of composite integers,
rendering most classical cryptographic protocols unsafe
 .
Since then, the study of quantum algorithms has matured as a sub-ﬁeld of quantum computing
with applications in search and optimization, machine
learning, simulation of quantum systems and cryptography .
In the last forty years, many scientiﬁc disciplines
have converged towards the study and development of
quantum algorithms and their experimental realization.
Quantum computers are, from the computational complexity perspective, fundamentally diﬀerent tools available to computationally intensive ﬁelds. The implementation of quantum algorithms requires that the minimal
quantum information units, qubits, are as reliable as classical bits. Qubits need to be protected from environmental noise that induces decoherence but, at the same time,
their states have to be controlled by external agents.
This control includes the interaction that generates entanglement between qubits and the measurement operation that extracts the output of the quantum computation. It is technically possible to tame the eﬀect of noise
without compromising the quantum information process
by developing quantum error correction (QEC) protocols
 . Unfortunately, the overhead of QEC in terms of the number
of qubits is, at the present day, still far from current
experimental capabilities. To achieve the goal of faulttolerant quantum computation, the challenge is to scale
up the number of qubits with suﬃciently high qubit quality and ﬁdelity in operations such as quantum gate implementation and measurement . As the system size
grows, it becomes highly challenging to contain the errors associated with cross-talk and measurements below
the required error-correction threshold.
Most of the originally proposed quantum algorithms
require millions of physical qubits to incorporate these
QEC techniques successfully, realizing the daunting goal
of building a fault-tolerant quantum computer may take
Existing quantum devices contain on the order of 100 phyisical qubits.
They are sometimes denoted as “Noisy Intermediate-Scale Quantum (NISQ)”
devices , meaning their qubits and quantum operations are not QEC and, therefore, imperfect.
One of the goals in the NISQ era is to extract the maximum quantum computational power from current devices
while developing techniques that may also be suited for
the long-term goal of the fault-tolerant quantum computation .
A. Computational complexity theory in a nutshell
The deﬁnition of a new computational paradigm opens
the window to tackle those problems that are ineﬃcient
with the existing ones. New computational complexity
classes have been recognized through the study of quantum computing, and proposed algorithms and goals have
to be developed within well-known mathematical boundaries.
In this review, we will often use some computational
complexity-theoretic ideas to establish the domain and
eﬃciency of the quantum algorithms covered. For this
reason, we provide in this subsection a brief synopsis for
a general audience and refer to 
for a more comprehensive treatment.
Complexity classes are groupings of problems by hardness, namely the scaling of the cost of solving the problem
with respect to some resource, as a function of the “size”
of an instance of the problem. The most well-known ones
being described informally here. i) P: problems that can
be solved in time polynomial with respect to input size
by a deterministic classical computer. ii) NP: a problem is said to be in NP, if the problem of verifying the
correctness of a proposed solution lies in P, irrespective
of the diﬃculty of obtaining a correct solution. iii) PH:
stands for Polynomial Hierarchy. This class is a generalization of NP in the sense that it contains all the problems
which one gets if one starts with a problem in the class
NP and adds additional layers of complexity using quantiﬁers, i.e. there exists (∃) and for all (∀). As we add
more quantiﬁers to a problem, it becomes more complex
and is placed higher up in the polynomial hierarchy. Let
us denote the classes in PH by Σi such that PH = ∪iΣi.
We have Σ1 = NP. The class Σi in PH can be interpreted
in the context of two-player games where problems correspond to asking whether there exists a winning strategy
2 rounds for the player 1 in a game. Here, one can
interpret the quantiﬁers by asking whether there exists
a move k1, such that no matter what move k2 is played,
there exists a move k3, and so on for i
2 rounds such that
player 1 wins the two-player game. With increasing i,
one would expect the problem to become more complex
and hence Σi ⊆Σi+1.
iv) BPP: stands for Boundederror Probabilistic Polynomial-time. A problem is said
to be in BPP, if it can be solved in time polynomial in
the input size by a probabilistic classical computer. v)
BQP: stands for Bounded-error Quantum Polynomialtime. Such problems can be solved in time polynomial
in the input size by a quantum computer. vi) PSPACE:
stands for Polynomial Space. The problems in PSPACE
can be solved in space polynomial in the input size by a
deterministic classical computer. Each class in PH is contained in PSPACE. However, it is not known whether PH
is equal to PSPACE. vii) EXPTIME: stands for Exponential Time. The problems in EXPTIME can be solved
in time exponential in the input size by a deterministic classical computer. viii) QMA: stands for Quantum
Merlin Arthur and is the quantum analog of the complexity class NP. A problem is said to be in QMA, if given a
“yes" as an answer, the solution can be veriﬁed in time
polynomial (in the input size) by a quantum computer.
Widely believed containment relations for some of the
complexity classes are shown in a schematic way in Fig. 1.
To understand the internal structure of complexity
classes, the idea of “reductions” can be quite useful. One
says that problem A is reducible to problem B if a
method for solving B implies a method for solving A;
one denotes the same by A ≤B. It is a common practice
to assume the reductions as polynomial-time reductions.
Intuitively, it could be thought as solving B is at least as
diﬃcult as solving A. Given a class C, a problem X is
said to be C-hard if every problem in class C reduces to
X. We say a problem X to be C-complete if X is C-hard
and also a member of C. The C-complete problems could
be understood as capturing the diﬃculty of class C, since
any algorithm which solves one C-complete problem can
be used to solve any problem in C.
A canonical example of a problem in the class BQP is
integer factorization, which can be solved in polynomial
time by a quantum computer using Shor’s factoring algorithm . However, no classical polynomialtime algorithm is known for the aforementioned problem.
Thus, the integer factorization problem is in BQP, but
not believed to be in P . While
analyzing the performance of algorithms, it is prudent to
perform complexity-theoretic sanity checks. For example, though quantum computers are believed to be powerful, they are not widely expected to be able to solve
NP-Complete problems, such as the travelling-salesman
problem, in polynomial time. The quantum algorithms,
however, could provide a speedup with respect to classical algorithms for NP-Complete problems.
B. Experimental progress
Here we present a somewhat biased and not exhausted
summary of very recent quantum experiments.
Interested readers should consult and references therein for more information about various quan-
EXPTIME: classically solvable in exponential time
Unrestricted chess on an nxn board
PSPACE: classically solvable in polynomial space
Restricted chess on an nxn board
QMA: quantumly verifiable in polynomial time
QMA-Complete: hardest problems in QMA
Quantum Hamiltonian ground state problem
NP-Complete: hardest problems in NP
Traveling salesman problem
NP: classically verifiable in polynomial time
P: classically solvable in polynomial time
Testing whether a number is prime
BQP: quantumly solvable in polynomial time
Integer factorization
Figure 1 An illustrative picture of some relevant complexity classes together with a problem examples. For the chess
example, the word “restricted” refers to a polynomial upper
bound on the number of moves. The containment relations
are suggestive. Some of them have not been mathematically
proven, being a well-known open problem whether P is equal
tum computing architectures.
Experimental progress in quantum computation can
be measured by diﬀerent ﬁgures of merit. The number of
physical qubits available must exceed a certain threshold
to solve problems beyond the capabilities of a classical
However, there exist several classical techniques capable of eﬃciently simulating certain quantum
many-body systems. The success of some of these techniques, such as Tensor Networks , rely on the eﬃcient representation of states
that are not highly entangled . With
the advent of universal quantum computers, one would
expect to be able to generate and manipulate these highly
entangled quantum states.
Hence, one imminent and practical direction towards
demonstrating quantum advantage over classical machines consist of focusing on a region of the Hilbert space
whose states can not be represented eﬃciently with classical methods.
Alternatively, one might tackle certain
computational tasks which are believed to be intractable
with any classical computer, as the ones belonging to
only quantum complexity classes.
Two recent experimental ventures exhibit this focus. In
2019, the Google AI Quantum team implemented an experiment with the 53-qubit Sycamore chip in which single-qubit gate ﬁdelities of 99.85% and
two-qubit gate ﬁdelities of 99.64% were attained on average. Quantum advantage was demonstrated against the
best current classical computers in the task of sampling
the output of a pseudo-random quantum circuit.
An additional quantum advantage experiment was
carried out by Jian-Wei Pan’s group using a Jiuzhang
photonic quantum device performing Gaussian boson
sampling (GBS) with 50 indistinguishable single-mode
squeezed states . Here, quantum advantage was seen in sampling time complexity of a Torontonian of a matrix , which scales
exponentially with the photon clicks output. The Torontonian is a matrix function that determines the probability distribution of measurement outcomes, much like the
permanent and Hafnian in other boson sampling models.
Intuitively speaking, while the total number of perfect
matchings in a bipartite graph is given by the permanent,
the Hafnian corresponds to the total number of perfect
matchings in an arbitrarily given graph. Moreover, while
the Hafnian is used in experiments counting the number
of photons in each mode, the Torontonian corresponds
to the case where one detects whether there are photons
in each mode (see Sec. III.B for more details about GBS
and the related terms).
There are several quantum computing platforms that
researchers are actively developing at present in order to
achieve scalable and practical universal quantum computers.
By “universal”, it is meant that such a quantum computer can perform native gate operations that
allow it to easily and accurately approximate any unitary gate (see Sec. V.B for more details).
Two of the
most promising platforms, superconducting circuits and
quantum optics, have already been mentioned; In addition to these, trapped-ion devices are also leading
candidates.
For instance, major achievements are recent high-ﬁdelity entangling gates reported by the Oxford group , all-to-all connectivity
achieved by IonQ , and transport and
reordering capabilities in 2D trap array by the Boulder
group . In the last example, besides
facilitating eﬃcient transport of ions and quantum information exchange, the 2D architecture can be viewed towards attaining much more sophisticated quantum error
correction code or surface code ,
the smallest of it has been realized in superconducting
qubit setup .
Scientists and engineers are also developing hybrid
quantum computing platforms trying to achieve similar
feats described above. These devices might not necessarily possess universal quantum gate sets, as many are built
to solve speciﬁc problems. Notably, coherent Ising machines based on mutually coupled optical parametric oscillators are promising and have shown success in solving
instances of hard combinatorial optimization problems.
Recently, it has been shown that the eﬃciency of these
machines can be improved with error detection and correction feedback mechanisms . The
reader is advised to refer to the recent review article for an in-depth discussion about
coherent Ising machines.
Quantum annealing has been another prominent approach towards quantum advantage in
the NISQ era . Refer to Sec. III.A for more
details about quantum annealing.
Lastly, unlike the past decades of academic research in
lab-based quantum technologies, we are witnessing the
emergence of cloud quantum computers with which anyone with internet access can now control and manipulate
delicate qubits and perform quantum computations on
Presently, IBM Quantum is leading the eﬀort
followed by Rigetti Computing and Xanadu Quantum
C. NISQ and near-term
The experimental state-of-the-art and the demand for
QEC have encouraged the development of innovative algorithms capable of reaching the long-expected quantum
advantage. This goal can be deﬁned as a purpose-speciﬁc
computation that involves a quantum device and that can
not be performed classically with a reasonable amount of
time and energy resources. The term near-term quantum
computation has been coined to cluster all these quantum algorithms specially developed to be run on current
quantum computing hardware or those which could be
developed in the next few years,. It is important to note
that NISQ is a hardware-focused deﬁnition, and does not
necessarily imply a temporal connotation. NISQ devices
can implement the model of quantum circuits, in which
all gates adhere to the topology of a speciﬁed graph G,
the nodes of which correspond to qubits. The gates typically operate on one or two qubits. Because each gate
operation involves a certain amount of noise, NISQ algorithms are naturally limited to shallow depths .
Near-term algorithms, however,
refers to those algorithms designed for quantum devices
available in the next few years and carries no explicit reference to the absence of QEC. The phrase “near-term”
is subjective since diﬀerent researchers may have other
thoughts on how many years can be considered “nearterm”. Predicting experimental progress is always challenging, and such predictions are inﬂuenced by human
bias. Algorithms developed for near-term hardware may
be unfeasible if hardware advancement does not match
the algorithm’s experimental requirements.
D. Scope of the review
This review aims to accomplish three main objectives.
The ﬁrst is to provide a proper compilation of the available algorithms suited for the NISQ era and which can
deliver results in the near-term. We present a summary
of the crucial tools and techniques that have been proposed and harnessed to design such algorithms. The second objective is to discuss the implications of these algorithms in various applications such as quantum machine
learning (QML), quantum chemistry, and combinatorial
optimization. Finally, the third objective is to give some
perspectives on potential future developments given the
recent quantum hardware progress.
Most of the current NISQ algorithms rely on harnessing the power of quantum computers in a hybrid
quantum-classical arrangement.
Such algorithms delegate the classically diﬃcult part of some computation to
the quantum computer and perform the other on some
suﬃciently powerful classical device. These algorithms
update variationally the variables of a parametrized
quantum circuit and hence are referred to as Variational
Quantum Algorithms (VQA) .
The ﬁrst proposals of VQA were the Variational Quantum Eigensolver (VQE) , originally proposed to
solve quantum chemistry problems, and the Quantum
Approximate Optimization Algorithm (QAOA) , proposed to solve combinatorial optimization problems. These two algorithms may be thought of
as the parents of the whole VQA family. While NISQ devices can arguably achieve quantum advantage for sampling problems, the corresponding question for the optimization problems remains unanswered . It is important to mention that, as of now, there is no provable quantum advantage for VQA with NISQ devices . We cover the main VQA blocks in Sec. II.
Other quantum computing paradigms propose diﬀerent kinds of algorithms. They are inspired and hybridized
with analog approaches.These include quantum annealing, digital-analog quantum computation, Gaussian Boson Sampling and analog quantum computation.
present their fundamental properties in Sec. III.
In Sec. V, we examine the theoretical and experimental challenges faced by NISQ algorithms and the methods
developed to best exploit them. We include the theoretical guarantees that some of these algorithms lay on as
well as techniques to mitigate the errors coming from the
use of noisy quantum devices. We also cover the possible trainability challenges that VQA have and how to
map theoretical NISQ circuits to real hardware. Section
VI presents the large variety of applications that NISQ
algorithms introduce.
Techniques to benchmark, compare and quantify current quantum devices performance
are presented in Sec. VII. Like any other computational
paradigm, quantum computing requires a language to establish human-machine communication. We explain different levels of quantum programming and provide a list
of open-source quantum software tools in Sec. V.C. Finally, we conclude this review in Sec. VIII by highlighting
the increasing community involvement in this ﬁeld and
by presenting the NISQ, near-term and long-term goals
of quantum computational research.
II. BUILDING BLOCKS OF VARIATIONAL QUANTUM
ALGORITHMS
A VQA comprises several modular components that
can be readily combined, extended and improved with developments in quantum hardware and algorithms. Chief
among these are the objective function, the cost function
to be variationally minimized; the parameterized quantum circuit (PQC), those unitaries whose parameters are
manipulated in the minimization of objective; the measurement scheme, which extracts the expectation values
needed to evaluate the objective; and the classical optimizer, the method used to obtain the optimal circuit
parameters that minimize the objective. In the following
subsections, we will deﬁne each of these pieces, presented
diagrammatically in Fig. 2.
A. Objective function
The Hamiltonian is a quantum operator that encodes
information about a given physical system, such as a
molecule or a spin chain.
Its expectation value yields
the energy of a quantum state, which is often used as the
minimization target of a VQA, i.e. obtaining the Hamiltonian ground state. Other problems not related to real
physical systems can also be encoded into a Hamiltonian
form, thereby opening a path to solve them on a quantum
computer. Hamiltonian operators are not all that can be
measured on quantum devices; in general, any expectation value of a function written in an operational form
(i.e. decomposed or encoded into a quantum operator)
can be also be evaluated on a quantum computer. After
the Hamiltonian or operator of a problem has been determined, it must be decomposed into a set of particular
operators that can be measured with a quantum processor.
Such a decomposition, which is further discussed
in Sec. II.A.1, is an important step of many quantum
algorithms in general and of VQA in particular.
Within a VQA, one has access to measurements on
qubits whose outcome probabilities are determined by
the prepared quantum state. Let us consider only measurements on individual qubits in the standard computational basis and denote the probability to measure qubit
q in state ∣0⟩by pq
0, where the qubit label q will be omitted whenever possible. The central element of a VQA
is a parametrized cost or objective function O subject to
a classical optimization algorithm, minθ O (θ,{p0 (θ)}).
The objective function O and the measurement outcomes
p0 of one or many quantum circuit evaluations depend
on the set of parameters θ.
In practice it is often inconvenient to work with the
probabilities of the measurement outcomes directly when
evaluating the objective function. Higher level formulations employ expectation value of the Hamiltonian H of
⟨H⟩U(θ) ≡⟨0∣U † (θ)HU (θ)∣0⟩,
describing measurements on the quantum state generated
by the unitary U (θ), instead of using the probabilities
for the individual qubit measurements directly. Arbitrary
observables can be decomposed into basic measurements
of the so-called Paulis strings, which can be evaluated
in the computational basis, as explained below and in
Sec. II.C. Restricting ourselves to expectation values instead of pure measurement probabilities, the objective
function becomes
θ O (θ,{⟨H⟩U(θ)}).
This formulation often allows for more compact deﬁnitions of the objective function. For the original VQE and QAOA it can,
for example, be described as a single expectation value
minθ⟨H⟩U(θ),
where the diﬀerences solely appear in the speciﬁc form
and construction of the qubit Hamiltonian.
The choice of the objective function is crucial in a VQA
to achieve the desired convergence. Vanishing gradient
issues during the optimization, known as barren plateaus,
are dependent on the cost function used (see Sec. IV.A for details).
1. Pauli strings
To extract the expectation value of the problem Hamiltonian, it is suﬃcient to express it as a linear combination of primitive tensor products of Pauli matrices
ˆσx, ˆσy, ˆσz.
We refer to these tensor products as Pauli
strings ˆP = ⊗n
j=1 ˆσ, where n is the number of qubits,
ˆσ ∈{ˆI, ˆσx, ˆσy, ˆσz} and ˆI the identity operator. Then, the
Hamiltonian can be decomposed as
where ck is a complex coeﬃcient of the k-th Pauli string
and the number of Pauli strings M in the expansion depends on the operator at hand.
An expectation value
d Classical optimization
Quantum-classical loop
b Parametrized quantum circuit
a Objective function
Figure 2 Diagrammatic representation of a Variational Quantum Algorithm (VQA). A VQA workﬂow can be divided into four
main components: a) the objective function O that encodes the problem to be solved; b) the parameterized quantum circuit
(PQC) U, which variables θ are tuned to minimize the objective; c) the measurement scheme, which performs the basis changes
and measurements needed to compute expectation values that are used to evaluate the objective; and d) the classical optimizer
that minimizes the objective. The PQC can be deﬁned heuristically, following hardware-inspired ansätze, or designed from the
knowledge about the problem Hamiltonian H. Inputs of a VQA are the circuit ansatz U(θ) and the initial parameter values
θ0. Outputs include optimized parameter values θ∗and the minimum of the objective.
in the sense of Eq. (1) then naturally decomposes into a
set of expectation values, each deﬁned by a single Pauli
ck⟨ˆPk⟩U .
Examples of Hamiltonian objectives include molecules
(by means of some fermionic transformation to Pauli
strings, as detailed in Sec. VI.A), condensed matter models written in terms of spin chains, or optimization problems encoded into a Hamiltonian form (see Sec. VI.C).
2. Fidelity
Instead of optimizing in respect to the expectation
value of an operator, several VQAs require a subroutine to optimize the state obtained from the PQC U (θ),
∣Ψ⟩U(θ) in respect to a speciﬁc target state ∣Ψ⟩. A commonly used cost function is the ﬁdelity between the PQC
and the target state
F (Ψ,ΨU(θ)) ≡∣⟨Ψ∣ΨU(θ)⟩∣2,
which is equivalent to the expectation value over the projector ˆΠΨ = ∣Ψ⟩⟨Ψ∣. The state preparation objective is
then the minimization of the inﬁdelity 1 −F (Ψ,ΨU(θ))
or just the negative ﬁdelity
F (Ψ,ΨU(θ)) = min
θ (−⟨ˆΠΨ⟩U(θ)).
If we know the eﬃcient circuit UΨ that prepares the target state ∣Ψ⟩, we can compute the ﬁdelity with the inversion test by preparing the quantum state U †
and measuring the projector into the zero state ˆΠ0 =
∣0⟩⊗n⟨0∣⊗n with the ﬁdelity given by F (Ψ,ΨU(θ)) =
ΨU(θ) .
If one wants to
avoid optimizing in respect to a projector onto a single state, one can instead use a local observable that
also becomes maximal for the target state, namely ˆO =
k=1 ∣0k⟩⟨0k∣⊗I¯k, where I¯k is the identity matrix for
all qubits except k and ∣0k⟩is the zero state for qubit
 . Alternatively, one can use randomized measurements to measure
the ﬁdelity Tr(ρ1ρ2) of two density matrices ρ1, ρ2 .
First, one selects m unitaries {Vk}k, which are chosen as
tensor product of Haar random unitaries over the local
d-dimensional subspace. These unitaries are applied on
each quantum state ρi = VkρV †
k and ρi is sampled in the
computational basis. Then, one estimates the probabil-
Vk (s) of measuring the computational basis state s
for each quantum state ρi and unitary Vk. The ﬁdelity is
Tr[ρ1ρ2] = dN
s,s′(−d)−D[s,s′] P (1)
Vk (s)P (2)
where D[s,s′] is the Hamming distance between sampled computational basis states s and s′. The number
of measurements scales exponentially with the number
of qubits, however the scaling is far better compared to
state tomography. Importance sampling has been proposed to substantially reduce the number of samples necessary .
Objective formulations over ﬁdelities are prominent
within state preparation algorithms in quantum optics , excited state algorithms and QML (see
also Sec. VI.B for more references and details). In these
cases, the ﬁdelities are often deﬁned in respect to computational basis states ei, such that Fei = ∣⟨Ψ(θ)∣ei⟩∣2.
3. Other objective functions
Hamiltonian expectation values are not the only objective functions that are used in VQAs. Any cost function
that is written in an operational form can constitute a
good choice. One such example is the conditional valueat-risk (CVaR). Given the set of energy basis measurements {E1,...EM} arranged in a non-decreasing order,
instead of using the expectation value from Eq. (1) as
the objective function, it was proposed to use 
which measures the expectation value of the α-tail of the
energy distribution.
Here, α ∈(0,1] is the conﬁdence
level. The CVaR(α) can be thought of as a generalization
of the sample mean (α = 1) and the sample minimum
Another proposal is to use the Gibbs
objective function
G = −ln⟨e−ηH⟩,
which is the cumulant generating function of the energy.
The variable η > 0 is a hyperparameter to be tuned. For
small η, the Gibbs objective function reduces to the mean
energy in Eq. (1). Since both the CVaR and the Gibbs
objective function can be reduced to the mean energy for
suitable limits of the hyperparameters (α →1 and η →0
respectively), their performances are guaranteed to be at
least as good as using the mean energy ⟨H⟩. Empirically,
by tuning the hyperparameters, both measures have been
shown to outperform ⟨H⟩for certain combinatorial optimization problems ⟩= U (θ)∣Ψ0⟩,
where θ are the variational parameters and ∣Ψ0⟩is some
initial state. Typically, ∣Ψ0⟩is a product state with all
qubits in the ∣0⟩state, i.e. ∣00⋯0⟩= ∣0⟩⊗n, where n is
the number of qubits. In some VQAs, it is convenient to
prepare that state in a particular form before applying
the PQC. The state preparation operation would then
depend on some other unitary operation P that may depend on variational parameters φ, ∣Ψ0⟩= P (φ)∣0⟩⊗n.
One example are the quantum feature maps deﬁned in
Sec. VI.B.1 that encode the data into the PQC.
Any known property about the ﬁnal state can also be
used to obtain the initial guess. For instance, if we expect that the ﬁnal state solution will contain all elements
of the computational basis, or if we want to exploit a
superposition state to seed the optimization, an initial
state choice may be P ∣0⟩⊗n = H⊗n
∣0⟩⊗n, where Hd is the
Hadamard gate. Applied to all qubits, Hd generates the
even superposition of all basis states, i. e.
where ∣ei⟩are the computational basis states.
In quantum chemistry algorithms, the initial state usually corresponds to the Hartree-Fock approximation (see
Sec. VI.A for details). The choice of a good initial state
will allow the VQA to start the search in a region of the
parameter space that is closer to the optimum, helping
the algorithm converge towards the solution.
The choice of the ansatz U greatly aﬀects the performance of a VQA. From the perspective of the problem,
the ansatz inﬂuences both the convergence speed and the
closeness of the ﬁnal state to a state that optimally solves
the problem. On the other hand, the quantum hardware
on which the VQA is executed has to be taken into account: Deeper circuits are more susceptible to errors,
and some ansatz gates are costly to construct from native gates. Accordingly, most of the ansätze developed
to date are classiﬁed either as more problem-inspired or
more hardware eﬃcient, depending on their structure and
application.
1. Problem-inspired ansätze
An arbitrary unitary operation can be generated by an
Hermitian operator ˆg which, physically speaking, deﬁnes
an evolution in terms of the t parameter,
G(t) = e−iˆgt.
As an example, the generator ˆg can be a Pauli matrix
ˆσi and thus, G(t) becomes a single-qubit rotation of the
Rk (θ) = e−i θ
2 ˆσk = cos(θ/2)I −isin(θ/2)ˆσk,
with t = θ and ˆg = 1
2 ˆσk, corresponding to the spin operator.
From a more abstract viewpoint, those evaluations can
always be described as time evolution of the corresponding quantum state, so that the generator ˆg is often just
referred to as a Hamiltonian. Note, however, that this
Hamiltonian does not necessarily need to be the operator that describes the energy of the system of interest.
In general, such generators can be decomposed into Pauli
strings in the form of Eq. (3).
Within so-called problem-inspired approaches, evolutions in the form of
Eq. (12), with generators derived
from properties of the system of interest are used to construct the parametrized quantum circuits. The unitary
coupled-cluster approach (see below), mostly applied for
quantum chemistry problems, is one prominent example. The generators then are elementary fermionic excitations, as shown in Eq. (17).
The Suzuki-Trotter (ST) expansion or decomposition
 is a general method to approximate a general, hard to implement unitary in the form of Eq. (12)
as a function of the t parameter. This can be done by
decomposing ˆg into a sum of non-commuting operators
{ˆok}k, with ˆg = ∑k ckˆok and some coeﬃcients ck. The
operators ˆok are chosen such that the evolution unitary
e−iˆokt can be easily implemented, for example as Pauli
strings ˆPk. The full evolution over t can now be decomposed into integer m equal-sized steps as
e−iˆgt = lim
For practical purposes, the time evolution can be approximated by a ﬁnite number m. When Pauli strings
are used, this provides a systematic method to decompose an arbitrary unitary, generated by ˆg, into a product of multi-qubit rotations e−i
, that can themselves
be decomposed into primitive one and two qubit gates.
Above, we have used the second order ST decomposition
to approximate the true unitary at each time step t. The
error incurred from the approximation can be bounded
by ∣∣Uˆg(∆t) −U ST
k=1 ∣∣[[Hk,H>k],Hk]] +
[[H>k,Hk],H>k]]∣∣∆3
t, where H>k = ∑β>k Hβ and Hk =
ckˆok .
Knowledge about the physics of the particular Hamiltonian to be trotterized can reduce substantially the number of gates needed to implement this method. For instance, in , it is shown that by
using fermionic swap gates, it is possible to implement
a Trotter step for electronic structure Hamiltonians using ﬁrst-neighbour connectivity circuits with N 2/2 twoqubit gates width and N depth, where N is the number
of spin orbitals. They also show that implementing arbitrary Slater determinants can be done eﬃciently with
N/2 gates of circuit depth.
Unitary Coupled Cluster.
Historically, problem-inspired
ansätze were proposed and implemented ﬁrst. They arose
from the quantum chemistry-speciﬁc observation that
the unitary coupled cluster (UCC) ansatz , which adds quantum correlations to the
Hartree-Fock approximation, is ineﬃcient to represent
on a classical computer . Leveraging
quantum resources, the UCC ansatz was instead realized
as a PQC on a photonic processor .
It is constructed from the parametrized cluster operator
T(θ) and acts on the Hartree-Fock ground state ∣ΨHF⟩
∣Ψ(θ)⟩= eT (θ)−T (θ)† ∣ΨHF⟩.
The cluster operator is given by T(θ) = T1(θ)+T2(θ)+⋯
j1,j2∈virt
and higher-order terms following accordingly . The operator ˆak is the annihilation operator
of the k-th Hartree-Fock orbital, and the sets occ and
virt refer to the occupied and unoccupied Hartree-Fock
Due to their decreasing importance, the series is usually truncated after the second or third term. The ansatz
is termed UCCSD or UCCSDT, respectively, referring
to the inclusion of single, double, and triple excitations
from the Hartree-Fock ground state. The k-UpCCGSD
approach restricts the double excitations to pairwise excitations but allows k layers of the approach (Lee et al.,
After mapping to Pauli strings as described in
Sec. II.A.1, the ansatz is converted to a PQC usually via
the Trotter expansion in Eq. (14).
In its original form, the UCC ansatz faces several drawbacks in its application to larger chemistry problems as
well as to other applications. For strongly correlated systems, the widely proposed UCCSD ansatz is expected to
have insuﬃcient overlap with the true ground state and
results typically in large circuit depths .
Consequently, improvements
and alternative ansätze are proposed to mitigate these
challenges.
We restrict our discussion here to provide a short
overview of alternative ansatz developments. For more
details about the UCC ansatz, see Sec. VI.A.
Factorized
Coupled-Cluster
Approaches.
The non commuting nature of the fermionic
excitation
generators,
operators Eq. (17) leads to diﬃculties in decomposing the
canonical UCC ansatz Eq. (15) into primitive one- and
two-qubit untiaries. First approaches employed the Trotter decomposition Eq. (14) using a single step . The accuracy of the so
obtained factorized ansatz depends however on the order
of the primitive fermionic excitations .
Alternative approaches propose to use factorized unitaries, constructed from primitive fermionic excitations,
directly .
Adaptive approaches, are a special case of a factorized
ansatz, where the unitary is iteratively grown by subsequently screening and adding primitive unitary operators
from a predeﬁned operator pool. The types of operator
pools can be divided into two classes: Adapt-VQE , that constructs the operator pool
from primitive fermionic excitations, and Qubit-Coupled-
Cluster that uses Pauli Strings.
In both original works, the screening process is based on
energy gradients with respect to the prospective operator
candidate. Since this operator is the trailing part of the
circuit, the gradient can be evaluated through the commutator of the Hamiltonian with the generator of that
operator. In contrast to commutator based gradient evaluation, direct diﬀerentiation, as proposed in allows gradient evaluations with similar cost
as the original objective and generalizes the approach by
allowing screening and insertion of operators at arbitrary
positions in the circuit. This is, for example, necessary
for excited state objectives as discussed in Sec. VI.A.4.
approaches
 , Pauli string pools from
decomposed fermionic pools , mutual
information based operator pool reduction , measurement reduction schemes based on the
density matrix reconstruction , and
external perturbative corrections (Ryabinkin et al.,
Variational Hamiltonian Ansatz.
Motivated by adiabatic
state preparation, the Variational Hamiltonian Ansatz
(VHA) was developed to reduce the number of parameters and accelerate the convergence . Instead of the Hartree-Fock
operators, the terms of the fermionic Hamiltonian itself
are used to construct the PQC. For this purpose, the
fermionic Hamiltonian H is written as a sum of M terms
H = ∑i ˆhi. Which parts of the Hamiltonian are grouped
into each term ˆhi depends on the problem and there is
a degree of freedom in the design of the algorithm. The
PQC is then chosen as
e(iθiˆhi),
with the operators in the product ordered by decreasing
i. The unitary corresponds to n short time evolutions under diﬀerent parts of the Hamiltonian, where the terms ˆhi
of the Hamiltonian can be repeated multiple times. The
initial state is chosen so that it is easy to prepare yet it
is related to the Hamiltonian, for example the eigenstate
of the diagonal part of H. The Fermi-Hubbard model
with its few and simple interaction terms is proposed as
the most promising near-term application of the method.
However, it is also shown that the VHA can outperform
speciﬁc forms of the UCCSD ansatz for strongly correlated model systems in quantum chemistry. In Sec. VI.A
we discuss some VQE-inspired algorithms that also use
adiabatic evolution to improve the performance of the
algorithm.
Quantum Approximate Optimization Algorithm.
the canonical NISQ era algorithms, designed to provide approximate solutions to combinatorial optimization
problems, is the Quantum Approximate Optimization
Algorithm (QAOA) . While QAOA
can be thought of as a special case of VQA, it has been
studied in depth over the years both empirically and theoretically, and it deserves special attention.
The cost function C of a QAOA is designed to encode a
combinatorial problem by means of bit strings that form
the computational basis. With the computational basis
vectors ∣ei⟩, one can deﬁne the problem Hamiltonian HP
as (see Sec. VI.C.1 for an example)
C(ei)∣ei⟩⟨ei∣,
and the mixing Hamiltonian HM as
The initial state in the QAOA algorithm is conventionally
chosen to be the uniform superposition state ∣D⟩from
Eq. (11). The ﬁnal quantum state is given by alternately
applying HP and HM on the initial state p-times,
∣Ψ(γ,β)⟩≡e−iβpHM e−iγpHP ⋯e−iβ1HM e−iγ1HP ∣D⟩, (20)
with γ ≡(γ1,γ2,⋯,γp) and β ≡(β1,β2,⋯,βp). A quantum computer is used to evaluate the objective function
C(γ,β) ≡⟨Ψ(γ,β)∣HP (γ,β)∣Ψ(γ,β)⟩,
and a classical optimizer is used to update the 2p angles γ and β until C is maximized, i.e.
C(γ∗,β∗) ≡
maxγ,β C(γ,β). Here, p is often referred to as the QAOA
level or depth.
Since the maximization at level p −1
is a constrained version of the maximization at level p,
the performance of the algorithm improves monotonically
with p in the absence of experimental noise and inﬁdelities.
In adiabatic quantum computing (see Sec. III.A), we
start from the ground state of HM and slowly move towards the ground state of HP by slowly changing the
Hamiltonian. In QAOA, instead, we alternate between
HM and HP . One can think of QAOA as a Trotterized
version of quantum annealing. Indeed, the adiabatic evolution as used in quantum annealing can be recovered in
the limit of p →∞.
For a combinatorial optimization problem with hard
constraints to be satisﬁed, penalties in the cost function
can be added. This might not be an eﬃcient strategy in
practice as it is still possible to obtain solutions which
violate some of the hard constraints. A variation of the
QAOA to deal with these constraints was also discussed
in the Sec. VII from the original proposal ,
it was proposed to encode the hard constraints directly in
the mixing Hamiltonian . This approach yields the main advantage of restricting the state
evolution to the feasible subspace where no hard constraints are violated, which consequently speeds up the
classical optimization routine to ﬁnd the optimal angles.
This framework was later generalized as the Quantum Alternating Operator Ansatz to consider phase-separation
and mixing unitary operators (UP (γ) and UM(β) respectively) which need not originate from the time-evolution
of a Hamiltonian . The operators
e−iβHM and e−iγHP from Eq. (20) are replaced by UM(β)
and UP (γ) respectively.
It is worth noting that both
the Quantum Approximate Optimization Algorithm and
the Quantum Alternating Operator Ansatz are abbreviated “QAOA” in the literature. In this case, we suggest
“QuAltOA” as an acronym for the Quantum Alternating Operator Ansatz to distinguish the same from the
Quantum Approximate Optimization Algorithm.
a Problem-inspired ansatz
b Hardware-efficient ansatz
Figure 3 Example problem-inspired and hardware-eﬃcient
ansätze. (a) Circuit of the Unitary Coupled Cluster ansatz
with a detailed view of a fermionic excitation as discussed
in . (b) Hardware-eﬃcient ansatz tailored to a processor that is optimized for single-qubit x- and
z-rotations and nearest-neighbor two-qubit CNOT gates.
The use of QAOA for combinatorial optimization is
presented in Sec. VI.C. Some theoretical guarantees of
this ansatz are introduced in Sec. IV.D.
2. Hardware-eﬃcient ansätze
Thus far, we have described circuit ansätze constructed
from the underlying physics of the problem to be solved.
Although it has been shown computationally that such
ansätze can ensure fast convergence to a satisfying solution state, they can be challenging to realize experimentally. Quantum computing devices possess a series
of experimental limitations that include, among others,
a particular qubit connectivity, a restricted gate set, and
limited gate ﬁdelities and coherence times. Therefore, existing quantum hardware is not suited to implement the
deep and highly connected circuits required for the UCC
and similar ansätze for applications beyond basic demonstrations such as the H2 molecule .
A class of hardware-eﬃcient ansätze has been proposed to accommodate device constraints . The common trait of these circuits is the use of
a limited set of quantum gates as well as a particular
qubit connection topology. The gate set usually consists
of a two-qubit entangling gate and up to three singlequbit gates. The circuit is then constructed from blocks
of single-qubit gates and entangling gates, which are applied to multiple or all qubits in parallel. Each of these
blocks is usually called layer, and the ansatz circuit generally has multiple such layers.
The quantum circuit of a hardware-eﬃcient ansatz
with L layers is usually given by
Uk (θk)Wk,
where θ = (θ1,,⋯,θL) are the variational parameters,
Uk (θk) = exp(−iθkVk) is a unitary derived from a Hermitian operator Vk, and Wk represents non-parametrized
quantum gates. Typically, the Vk operators are singlequbit rotation gates, i.e. Vk are Pauli strings acting locally on each qubit. In those cases, Uk becomes a product
of combinations of single-qubit rotational gates, each one
deﬁned as in Eq. (13). Wk is an entangling unitary constructed from gates that are native to the architecture at
hand, for example CNOT or CZ gates for superconducting qubits or XX gates for trapped ions . Following this approach, the
so-called Alternating Layered Ansatz is a particular case
of these Hardware-eﬃcient ansätze which consists of layers of single qubit rotations, and blocks of entangling
gates that entangle only a local set of qubits and are
shifted every alternating layer.
The choice of these gates, their connectivity, and their
ordering inﬂuences the portion of the Hilbert space that
the ansatz covers and how fast it converges for a speciﬁc problem. Some of the most relevant properties of
hardware-eﬃcient ansätze, namely expressibility, entangling capability and number of parameters and layers
needed are studied in Refs. and further discussed in Sec. IV.B.
Instead of making a choice between the probleminspired and hardware-eﬃcient modalities, some PQC designers have chosen an intermediate path. One example
is the use of an exchange-type gate, which can be implemented natively in transmons, to construct a PQC
that respects the symmetry of the variational problem
 . Such
an ansatz leads to particularly small parameter counts
for quantum chemistry problems such as the H2 and LiH
molecules . Another intermediate approach, termed QOCA for its inspiration from quantum
optimal control, is to add symmetry-breaking unitaries,
akin to a hardware-eﬃcient ansatz, into the conventional
VHA circuit . This modiﬁcation
enables excursions of the variational state into previously
restricted sections of the Hilbert space, which is shown
to yield shortcuts in solving fermionic problems.
C. Measurement
To gain information about the quantum state that has
been prepared on the quantum hardware, one needs to
estimate the expectation value of the objective function
The most direct approach to estimate expectation values is to apply a unitary transformation on the quantum state to the diagonal basis of the observable ˆO and
obtaining the probability of measuring speciﬁc computational states corresponding to an eigenvalue of ˆO. In
other words, to determine whether a measured qubit is
in the ∣0⟩or ∣1⟩state. For experimental details on this
task we refer to existing reviews, such as for superconducting qubits or ion traps . However, on NISQ devices, the tranformation to the diagonal basis mentioned before can be an
overly costly one. As a NISQ friendly alternative, most
observables of interest can be eﬃciently parameterized in
terms of Pauli strings, as shown above, and transformed
into their diagonal basis by simple single-qubit rotations,
as shown below.
Measurement of Pauli strings.
The expectation value of
the ˆσz operator on a particular qubit can be measured by
reading out the probabilities of the computational basis
state {∣0⟩,∣1⟩} as
⟨ψ∣ˆσz∣ψ⟩≡⟨ˆσz⟩= ∣α∣2 −∣β∣2,
where ∣α∣2 is the probability to measure the qubit in state
∣0⟩, ∣β∣2 is the probability to measure the qubit in state
∣1⟩and ∣ψ⟩= α ∣0⟩+ β ∣1⟩. Measurements deﬁned by ˆσx
and ˆσy can be deﬁned similarly by transforming them
into the ˆσz basis ﬁrst. The transformation is given by
primitive single-qubit gates
y (π/2) ˆσzRy (π/2) = HdˆσzHd,
x (π/2) ˆσzRx (π/2) = SHdˆσzHdS†,
where S = √ˆσz and Hd = (ˆσx + ˆσz)/
2 is the Hadamard
gate. Then, to measure ˆσx on a quantum state ∣ψ⟩, we
rotate ˆσx into the z-axis by applying Hd and measure in
logical ˆσz basis, i.e.
⟨ˆσx⟩≡⟨ψ∣ˆσx ∣ψ⟩= ⟨ψ∣HdˆσzHd ∣ψ⟩= αβ∗+ α∗β.
The same applies for ⟨ˆσy⟩.
Arbitrary Pauli strings ˆP,
with primitive Pauli operations ˆσf(k) ∈{σx,σy,σz} on
qubits k ∈K, can then be measured by the same procedure on each individual qubit as
⟨ˆP⟩U = ⟨∏
where ˜U is a product of single qubit rotations according
to Eq. (24) and Eq. (25) depending on the Pauli operations ˆσf(k) at qubit k.
So far we discussed expectation values of a physical
observable ⟨ˆO⟩, which is the mean value averaged over
an inﬁnite number of measurements.
In practice, one
can sample only a ﬁnite number of single-shot measurements Ns of the quantum state and thus can estimate the
expectation values within some ﬁnite error. For a Pauli
string ˆP, the number of measurement samples Ns needed
to estimate the expectation value ⟨ˆP⟩U with an additive
error of at most ϵ with a failure probability of at most δ
is bounded by Hoeﬀding’s inequality 
In particular, the error ϵ decreases with the inverse
square-root of the number of measurements ϵ ∝1/√Ns.
For many problems, such as quantum chemistryrelated tasks, the number of terms in the cost Hamiltonian to be estimated can become very large. A naive
way of measuring each Pauli string separately may incur
a prohibitively large number of measurements. Recently,
several more eﬃcient approaches have been proposed for an overview). The common idea is to group diﬀerent Pauli strings that can be
measured simultaneously such that a minimal number of
measurements needs to be performed.
Pauli strings that commute qubit-wise, i.e. the Pauli
operators on each qubit commute, can be measured at the
same time .
The problem of ﬁnding the minimal number of groups can
be mapped to the minimum clique cover problem, which
is NP-hard in general, but good heuristics exist .
One can collect mutually commuting operators and transform them into a shared eigenbasis, which adds an additional unitary transformation
to the measurement scheme . Combinations of
single qubit and Bell measurements have been proposed
as well .
Alternatively, one can use a method called unitary partitioning to linearly combine diﬀerent operators into a
unitary, and use the so-called Hadamard test (see below) to evaluate it . In , the observables can
be decomposed into the so-called mean-ﬁeld Hamiltonians, which can be measured more eﬃciently if one measures one qubit after the other, and uses information from
previous measurement outcomes.
For speciﬁc problems such as chemistry and condensed
matter systems, it is possible to use the structure of the
problem to reduce the number of measurements . In particular, in , where a
Fermi-Hubbard model is studied using VQE, the number of measurements is reduced by considering multiple orderings of the qubit operators when applying the
Jordan-Wigner transformation. In the context of quantum chemistry, the up-to-date largest reduction could be
achieved by the Cartan subalgebra approach of .
Other approaches use classical shadows , a classical approximation of
the quantum state of interest, or neural network estimators to decrease the number of measurements.
All those kind of optimizations require an
understanding of the underlying problem and are usually
not applicable for every use of the VQE.
Measurement of overlaps.
Several VQA require the measurement of an overlap of a quantum state ∣ψ⟩with unitary U in the form of ⟨ψ∣U ∣ψ⟩. This overlap is in general
not an observable and has both real and imaginary parts.
The Hadamard test can evaluate such a quantity on the
quantum computer using a single extra qubit . The idea is to apply a controlled U operation, with control on that qubit, and target U on the
quantum state.
Then, one can measure from the this
single qubit state both real and imaginary part of the
overlap. A downside of this method is the requirement
to be able to implement a controlled unitary, which may
require too many resources on current quantum processors. Alternative methods to measure the overlap without the use of control unitaries have been proposed . One idea is to decompose U into
a sum of Pauli strings, and then to measure the expectation value of each Pauli string individually. Another
approach is possible if U can be rewritten into a product of unitaries Uq that act locally on only a few qubits.
Then, one can ﬁnd via classical means the diagonalization of Uq = V †
q DVq, with diagonal matrix D and Vq being
a unitary. The overlap can be found by applying the Vq
unitaries on the state ∣ψ⟩, measure the outcomes in the
computational basis and do post-processing of the results
with the classically calculated eigenvalues of D.
Classical shadows.
This is a powerful technique to accurately predict M expectation values Tr( ˆOiρ), 1 ≤i ≤M
of an unknown quantum state ρ .
The method is based on and inspired from shadow tomography .
First, a random unitary
U is applied on the state ρ →UρU † and then all the
qubits are measured in the computational basis.
step is repeated with several random unitaries U. Common choices for U are unitaries which can be eﬃciently
computed on a classical computer such as random nqubit Cliﬀord circuits or tensor products of single qubit
rotations. By post-processing the measurement results,
one can gather a classical shadow, which is a classical
representation of the quantum state ρ.
There exists performance guarantees that classical
shadows with size of order log M suﬃce to predict M
expectation values simultaneously. For investigations involving classical shadow tomography protocols in the
presence of noise, refer to . Experimental realizations have been performed recently as well .
D. Parameter optimization
In principle, the PQC parameter optimization to minimize the objective is not diﬀerent from any multivariate
optimization procedure and standard classical methods
can be applied . However, in the
NISQ era, the coherence time is short, which means that
high-depth analytical gradient circuits cannot be implemented.
In addition, one of the biggest challenges in
parameter optimization is the large number of measurements required for estimating the mean value of an observable to a high precision. Due to this high sampling
rate, the measurement process can become a signiﬁcant
bottleneck in the overall algorithm runtime. Thus, an
eﬀective optimizer for PQCs should try to minimize the
number of measurements or function evaluations. As a
last criterion, the optimizer should be resilient to noisy
data coming from current devices and precision on expectation values that is limited by the number of shots in
the measurement. These three requirements imply that
certain existing algorithms are better suited for PQC optimization and are more commonly used, and that new
algorithms are being developed speciﬁcally for PQC optimization. Some intuitive concepts of the mechanisms
behind optimisation of quantum problems have been investigated in . Recently, have shown that the classical optimization corresponding to VQAs is a NP-hard problem.
In this section, we ﬁrst review two classes of optimization, gradient-based and gradient-free. We also consider resource-aware optimization methods and strategies
that additionally minimize quantities associated with the
quantum cost of optimization. While we reserve more
detailed descriptions to the respective references and the
Supplementary Material, we highlight the main features
and advantages for each optimization strategy.
1. Gradient-based approaches
A common approach to optimise an objective function
f(θ) is via its gradient, i.e.
the change of the function with respect to a variation of its M parameters
θ = (θ1,⋯,θM). The gradient indicates the direction in
which the objective function shows the greatest change.
This is a local optimization strategy as one uses information starting from some initial parameter value θ(0) and
iteratively updates θ(t) over multiple discrete steps t. A
common update rule for each θi is
−η ∂if(θ),
or θ(t+1) = θ(t) −η ∇f(θ), where η is a small parameter
called learning rate and
, ∇= (∂1,⋯,∂M)
is the partial derivative with respect to the parameter
θi and the gradient vector, respectively, using Einstein
There are various ways of estimating the gradient on a
quantum computer . The most relevant of them are detailed in the Supplementary Material
and summarized in the following paragraphs.
Finite diﬀerence.
One can compute the gradients using
ﬁnite diﬀerences, i.e. ∂if(θ) ≈(f(θ+ϵei)−f(θ−ϵei))/2ϵ,
where ϵ is a small number and ei is the unit vector with
1 as its i-th element and 0 otherwise. As the objective
function f(θ) is obtained with limited accuracy, a good
estimation of the gradient requires smaller ϵ, i.e. more
samples taken from the quantum hardware.
in and developed in . This method computes the
gradients exact and ϵ can be large (commonly ϵ = π/2).
This method assumes that the unitary to be optimized
can be written as U(θ) = V G(θi)W, where G = e−iθig is
the unitary aﬀected by the parameter θi, g is the generator of G and V,W are unitaries independent of θi. If
g has a spectrum of two eigenvalues ±λ only, the gradient can be calculated by measuring the observable at two
shifted parameter values as follows:
∂i⟨f(θ)⟩= λ(⟨f(θ+)⟩−⟨f(θ−)⟩),
where θ± = θ ± (π/4λ)ei. This rule can be generalised
to the case where the generator g does not satisfy the
eigenspectrum condition (see Supplementary Material for
details). It can also be adapted to calculate analytical
gradients for fermionic generators of Unitary Coupled-
Cluster operators and higher
order derivatives .
It is a quasi-Newton method that eﬃciently
approximates the “inverse Hessian” using a limited history of positions and gradients .
While eﬀective in simulations, recent
studies observed BFGS methods do not perform well in
experimental demonstrations of VQA due to the level of
noise in the cost function and gradient estimates . Two heuristics were proposed to ﬁnd
quasioptimal parameters for QAOA using BFGS , INTERP and FOURIER explained in
the supplementary material.
Eﬃcient initialization of
parameters has also been reported using the Trotterized
quantum annealing (TQA) protocol . These heuristic strategies can be easily extended
to gradient-free optimization methods such as Nelder-
Quantum natural gradient.
The update rule of standard
gradient descent assumes that the parameter space is a
ﬂat Euclidean space. However, in general this is not the
case, which can severely hamper the eﬃciency of gradient descent methods.
In classical machine learning,
the natural gradient was proposed that adapts the update rule to the non-Euclidean metric of the parameter
space . Its extension, the quantum natural
gradient (QNG) deﬁnes the following update rule :
−η F−1(θ)∂if(θ),
where F(θ) is the Fubini-Study metric tensor or quantum Fisher information metric given by
Fij = Re(⟨∂iψ(θ)∣∂jψ(θ)⟩−⟨∂iψ(θ)∣ψ(θ)⟩⟨ψ(θ)∣∂jψ(θ)⟩).
Superior performance of the QNG compared to other
gradient methods has been reported and it has been shown that it can avoid
becoming stuck in local minima .
It can be generalized to noisy quantum circuits . The QNG can be combined with
adaptive learning rates η(θt
i) that change for every step
of gradient descent to speed up training. For hardware
eﬃcient PQCs, one can calculate adaptive learning rates
using the quantum Fisher information metric . While the full Fubini-Study metric tensor is
diﬃcult to estimate on quantum hardware, diagonal and
block-diagonal approximations can be eﬃciently evaluated and improved classical techniques to calculate the full tensor exist . A
special type of PQC, the natural PQC, has a euclidean
quantum geometry such that the gradient is equivalent
to the QNG close to a particular set of parameters .
Quantum imaginary time evolution.
Instead of using the
standard gradient descent for optimization, a variational imaginary time evolution method was proposed
in to govern the evolution of
parameters.
They focused on many-body systems described by a k-local Hamiltonian and considered a PQC
that encodes the state ∣ψ(τ)⟩as a parameterized trial
state ∣ψ(θ(τ))⟩. The evolution of θ(τ) with respect to
all the parameters can then be obtained by solving a
diﬀerential equation (see Supplementary for details). It
was later shown in that this method
is analogous to the gradient descent via the QNG when
considering inﬁnitesimal small step sizes.
Hessian-aided gradient descent.
A recent work proposed computing the Hessian and
its eigenvalues to help analyze the cost function landscapes of QML algorithms.
Tracking the numbers of
positive, negative, and zero eigenvalues provides insight
whether the optimizer is heading towards a stationary
point. The Hessian can be computed by doubly applying
the parameter shift rule as shown in and reproduced in the supplementary material.
While a deeper analysis is necessary to compare their
performance, both QNG and Hessian-based methods try
to accelerate optimization by leveraging local curvature
information.
Quantum Analytic Descent.
A method consisting of using a classical model of the local energy landscape to
estimate the gradients is proposed in . In this hybrid approach, a quantum device
is used to construct an approximate ansatz landscape
and the optimization towards the minima of the corresponding approximate surfaces can be carried out eﬃciently on a classical computer. Using this approximate
ansatz landscape, the full energy surface, gradient vector and metric tensor can be expressed in term of the
ansatz parameters. The analytic descent has been shown
to achieve faster convergence as compared to the QNG.
Stochastic
gradient-based methods is the high number of measurements. The stochastic gradient descent (SGD) algorithm
addresses this issue by replacing the normal parameter
update rule with a modiﬁed version
θ(t+1) = θ(t) −α g(θ(t)),
where α is the learning rate and g is an unbiased estimator of the gradient of the cost function.
As an estimator, one can take the measurement of the gradient
with a ﬁnite number of shots .
This technique can be combined with sampling of the
parameter-shift rule terms or by extending it to doubly stochastic gradient. For the latter,
the ﬁnite measurements are performed for only a subset of the expectation values of the Hamiltonian terms.
This sampling can be performed in the extreme situation
where only one Pauli-term is evaluated at a single point
in the quadrature. This is a very powerful method that
reduces the number of measurements drastically . This method can be extended beyond circuits that allow the parameter-shift rule by expressing
the gradient as an integral .
To accelerate the convergence of SGD for VQA, diﬀerent strategies are proposed and brieﬂy
explained in the Supplementary.
2. Gradient-free approaches
In this section, we discuss optimization methods for
VQA that do not rely on gradients measured on the quantum computer.
Evolutionary
algorithms.
Evolutionary
strategies
(Rechenberg,
optimization tools for high dimensional problems that
use a search distribution, from which they sample data,
to estimate the gradient of the expected ﬁtness to update
the parameters in the direction of steepest ascent. More
recently, natural evolutionary strategies (NES) have demonstrated considerable progress in
solving these high dimensional optimization problems.
They use natural gradient estimates for parameter
updates instead of the standard gradients.
been adapted for optimization of VQA and have been shown to
have similar performance as the state-of-the-art gradient
based method.
In it is shown
that NES, along with techniques like Fitness shaping,
local natural coordinates, adaptive sampling and batch
optimization, can be used for optimization of deep
quantum circuits.
Reinforcement learning.
Several authors have used reinforcement learning (RL) to optimize the QAOA parameters . This framework consists of a decision-making agent with policy
πθ(a∣s) parameterized by θ, which is a mapping from
the state space s ∈{S} to an action space a ∈{A}. In response to the action, the environment provides the agent
with a reward r from the set of rewards {R}. The goal of
RL is to ﬁnd a policy which maximizes the expected total
discounted reward. For more details, refer to Sec. VI.B.3.
In the context of QAOA, for example, {S} can be the set
of QAOA parameters (γ,β) used, a can be the value of
γ and β for the next iteration, and the reward can be the
ﬁnite diﬀerence in the QAOA objective function between
two consecutive iterations. The policy can be parameterized by a deep neural network with the weights θ. The
policy parameters θ can be optimized using a variety of
algorithms such as Monte-Carlo methods , Q-Learning and policy gradient methods .
Sequential minimal optimization.
In machine learning, the
sequential minimal optimization (SMO) method has proven successful in optimizing the highdimensional parameter landscape of support vector machines. The method breaks the optimization into smaller
components for which the solution can be found analytically. This method has been applied to variational circuit
optimization , circuit optimization with classical acceleration and
circuit optimization and learning with Rotosolve and Rotosolect .
Surrogate model-based optimization.
When function evaluations are costly, it pays oﬀto not only use the current
function value to inform a next parameter value, but to
use all previous evaluations to extract information about
the search space.
The function values in memory are
used to build a surrogate model, an auxiliary function
that represents the full expensive cost function based on
the current information.
All optimization happens on
the surrogate cost landscape, so no explicit derivatives of
the cost function are needed. Through the use of a ﬁtted cost function, these methods are also expected to be
more resilient to noise. Several classical surrogate models
have been included in the scikit-quant package . In the Bound optimization by quadratic
approximation (BOBYQA) algorithm , a
local quadratic model is formulated from the previous
function values. It is then minimized in the trust region
to obtain a new parameter value. When the evaluation at
this new parameter value does not result in a lower function value, the trust region is altered and the quadratic
model is optimized in this new parameter space. It was
shown that this method works well when the PQC is initialized close to the optimal parameters but has more
problems with shallow optimization landscapes and gets
stuck in local minima . The stable
noisy optimization by branch and ﬁt (SnobFit) algorithm uses a branching algorithm to explore new areas in parameter space.
3. Resource-aware optimizers
Optimization methods and strategies adopted for early
demonstrations of VQA are largely general-purpose and
black-box with minimal emphasis on reducing the quantum resources used in the optimization. Therefore, they
are more costly and prone to errors than their classical counterparts. Optimizers developed in more recent
years are tailored to additionally minimize quantities associated with the quantum cost of the optimization, e.g.
number of measurements or real hardware properties.
Additionally, one can use circuit compilation methods
as the ones described in Sec. V.B.
While VQA leverage low-depth circuits to
execute on near-term quantum processors, a signiﬁcant
challenge in implementing these algorithms is the prohibitive number of measurements, or shots, required to
estimate each expectation value that is used to compute the objective.
To address the challenge, developed a shot-frugal optimizer
called ROSALIN (Random Operator Sampling for Adaptive Learning with Individual Number of shots) that effectively distributes fractions of a predeﬁned number of
shots to estimate each term of the Hamiltonian as well
as each partial derivative. Given the expectation value
of the Hamiltonian decomposed into the hi terms as in
the authors note several strategies for allocating shots
for estimating each term ⟨hi⟩. While a naive strategy
would allocate equal numbers of shots per term, the
authors observed lower variance in the energies using
weighted approaches in which the number of shots allocated to the i-th term bi is proportional to the corresponding Hamiltonian coeﬃcient ci.
In experimental realizations of VQA, the optimizer is often hindered by statistical noise.
In this issue is circumvented by applying the simultaneous perturbation stochastic approximation (SPSA) algorithm , in which the algorithm hyperparameters are determined by experimental
data on the level of statistical noise. Compared to the
ﬁnite-diﬀerence gradient approximation, which requires
O(p) function evaluations for p parameters, SPSA requires only two evaluations, as explained in the supplementary. The convergence of SPSA with various types of
PQCs has been studied .
III. OTHER NISQ APPROACHES
We proceed to review some of the notable NISQ algorithms, besides VQA. These algorithms do not require
tuning the parameters of a PQC in an adaptive feedback
manner and often exploit analog or hybrid paradigms
that constitute alternatives to the digital quantum computation.
A. Quantum annealing
Quantum annealing (QA) derives its inspiration from
simulated annealing (SA), a classical global optimization technique, usually employed to solve combinatorial
optimization problems.
SA can be valuable in discovering global optima in situations involving many local
optima. The word “annealing” comes from metallurgy,
which represents heating and slow cooling. In SA, one
identiﬁes the objective function with the energy of a
statistical-mechanical system.
The system is assigned
an artiﬁcially-induced control parameter, called temperature. Like annealing, SA starts with some high temperature T, and then the value of T is brought down following some temperature variation function called “annealing schedule” such that the ﬁnal temperature is T = 0.
The algorithm chooses a candidate state close to the current state randomly.
If it improves the solution, it is
always accepted with probability 1. If it does not, then
the acceptance is determined based on a temperaturedependent probability function. The idea of tolerating
worse solutions can be considered as a virtue of the algorithm. In SA, the probability that a bad solution is
accepted is slowly decreased as the solution space is explored.
This relates to the notion of “slow cooling” in
annealing.
In QA, one utilises quantum-mechanical ﬂuctuations,
like quantum tunnelling, to explore the solution space.
This is analogous to the idea of using thermal ﬂuctuations
in SA to explore the solution space. In QA, artiﬁcial degrees of freedom of quantum nature via non-commutative
operators are introduced, which induces quantum ﬂuctuations.
The strength of these quantum ﬂuctuations is
controlled using an annealing schedule (similar to SA,
where we decrease the temperature). The physical idea
behind annealing schedule in QA is to move the system from some initial Hamiltonian ground state to the
ground state of the problem Hamiltonian. The concept
of QA is related to the notion of quantum adiabatic evolution, which is being used for adiabatic quantum computation .
We proceed to a formal discussion now.
quantum computation is model of computation based on
quantum mechanical processes operating under adiabatic
conditions .
Before understanding adiabatic quantum computation,
one needs to grasp the concept of k-local Hamiltonian.
Deﬁnition 1 k-local Hamiltonian: A k-local Hamiltonian is a Hermitian matrix of the form H = ∑r
where each term is a Hermitian operator acting nontrivially on at-most k qudits, i.e., ˆhi = h ⊗I where h is
a Hamiltonian acting on at-most k neighbouring qudits
and I is the identity operator.
Let us consider a time-dependent hamiltonian H(s),
T ∈ and a quantum system initialized in
the ground state of H(0). We assume that H(s) varies
smoothly as a function of s and H(s) has a unique
ground state for s ∈ . A quantum state initialized in
∣ψ(t = 0)⟩evolves according to the following Schrödinger
equation (setting ̵h = 1),
dt ∣ψ(t)⟩= H(t)∣ψ(t)⟩.
The above equation can be further, equivalently, written
ds ∣ψ(s)⟩= TH(s)∣ψ(s)⟩.
Assuming ∣ψ(0)⟩is a ground state of H(0), then in the
limit T →∞, ∣ψ(t)⟩is a ground state of H(1) obtained
via evolution Eq. (35). Such an evolution will be, henceforth, referred as adiabatic evolution according to H for
Now we proceed to deﬁne adiabatic quantum computation.
Deﬁnition 2 Adiabatic
computation
 ):
adiabatic quantum computation is speciﬁed by two k-local
Hamiltonians H0 and H1 acting on n qudits and a map
s(t) ∶[0,T] Ð→ . The input of the computation
is the ground state of H0, which is unique and is a
product state. The desired output is given by a quantum
state which is ϵ−close in l2-norm to the ground state of
H1. Furthermore, T is the smallest time such that the
adiabatic evolution generated via H(s) = (1 −s)H0 + sH1
for time T yields the desired output. The running time
of the algorithm is given by T.maxs ∥H(s)∥, where ∥.∥
denotes the spectral norm.
QA relaxes the strict requirement of adiabatic evolution, thus allowing diabatic transitions due to the ﬁnite
temperature of the system, from fast changes of Hamiltonian parameters, and the interaction with the noisy
environment . Because of diabatic
transitions, QA is prone to getting trapped in excited
QA has been investigated for problems in diverse areas including machine learning , protein folding , fault
diagnosis , compressive sensing , ﬁnance , fermionic simulation and
high energy physics .
The protein folding problem entails calculating a protein’s lowest free-energy structure given its amino-acid
sequence. The goal is to solve the protein folding problem by mapping it to a Hamiltonian and then using QA to
identify low-energy conformations of the protein model.
In , authors use ﬁve and eight
qubits for the four-amino-acid sequence to encode and
solve the protein folding problem for a short tetrapeptide and hexapeptide chain.
QA has been one of the
prominent approaches in the NISQ era in the search for
quantum advantage .
A major experimental implementation of QA is the D-
Wave machine. It attempts to solve problems in a particular form called Quadratic Unconstrained Binary Optimization (QUBO) . Optimization problems can be cast as a polynomial unconstrained binary
optimization (PUBO) expressed in the form of a k-local
interaction with k ≥3 over binary variables xi ∈{0,1}
 . QUBO
is a special case of PUBO with k = 2. For a vector of n
binary variables x ∈{0,1}n and problem speciﬁed values
of Q ∈Rn×n and c ∈Rn, QUBO is deﬁned as
arg minxT Qx + cT x.
Using the map xi →1−σi
2 , one can convert the problem
in expression 37 to ground state ﬁnding problem of the
following diagonal n-qubit Ising Hamiltonian (up to a
constant),
HQUBO = −∑
where ˆJi,j = −Qi,j
−ci+∑j Qi,j
Starting with the ground state of the base Hamiltonian H0 = −∑i ˆσi
x, solving the QUBO problem on a quantum annealer corresponds to implementing the annealing
schedule A(t) and B(t) for the Hamiltonian
H(t) = A(t)H0 + B(t)HQUBO.
Here, A(0) = B(T) = 1 and A(T) = B(0) = 0, where T is
computation time. Because annealing does not necessarily satisfy the constraints of adiabatic evolution, one can
end up in excited states as mentioned earlier. However,
one can run the annealing schedule multiple times and
take the best answer i.e, the one corresponding to lowest energy. The qubits in an annealer are not necessarily
all-to-all connected, necessitating additional engineering
restrictions, such as the minor embedding problem .
The potential of QA has been studied extensively
 . The performance of D-Wave annealers have also been explored
comprehensively . In particular, an extensive study comparing the performance of quantum annealing with other
quantum-inspired and classical optimization state-of-theart strategies, and in the context of a real- world application, can be found in .
For the details of QA, refer to and
the references therein. A review on Adiabatic Quantum
Computation is presented in .
Refer to Sec. VI.B and Appendix Sec. C for the discussions regarding applications of QA in machine learning
and ﬁnance respectively.
B. Gaussian boson sampling
Figure 4 Gaussian boson sampling circuit for a photonic
The qumodes are prepared in gaussian states from
the vacuum by squeezing operations S(zi), followed by an interferometer consisting of phaseshifters R(θ) = eiθj and beamsplitters BS. At the end, photon number resolving measurements are made in each mode.
Boson sampling was ﬁrst proposed as a candidate for
quantum computational supremacy by .
The scenario consists of having n
photons that enter an optical circuit consisting of m
This state is then acted upon by a series of
phase-shifters and beam-splitters. A phase-shifter adds
a phase R(θ) = eiθj with some angle θj to the amplitude
in mode j, and acts as the identity in the other m −1
modes. A beam-splitter acts on two modes with a rotation (cosφ −sinφ
cosφ ) for some angle φ and as the identity
in the other m−2 modes. Finally, a measurement is made
where the number of photons in each mode is found. An
optical circuit with these elements is shown in Figure 4.
Each of these measurement outcomes represent a sample from the the symmetric wavefunction that bosonic
systems have. Aaronson and Arkhipov found that the
existence of an eﬃcient classical algorithm for sampling
from the distribution implies the existence of a classically
eﬃcient algorithm for the calculation of the permanent
of a related matrix. Such an algorithm would imply the
collapse of the polynomial hierarchy (see I.A) to the third
order, which is believed to be unlikely (Arora and Barak,
Consequently, such an algorithm is unlikely to
Gaussian boson sampling (GBS) is a variant of boson sampling, where instead of photon states as inputs
into the optical circuit, Gaussian states are used as inputs . Gaussian states are those
whose Wigner quasi-probability distributions W(q,p)
have Gaussian shape. A good introduction to the theory
can be found in . They have the advantage that they can be created deterministically . They also provide additional degrees of freedom in comparison to boson sampling. Where boson sampling is equivalent to sampling
from the permanent of a matrix, GBS is computationally equivalent to sampling from the Hafnian function of
a matrix. Given a graph G with adjacency matrix E, the
Hafnian of E is the number of perfect matchings of the
graph G. A matching of a graph G is a subset of edges
M such that no two edges in M have a vertex in common. A matching M is perfect if every vertex is incident
to exactly one edge in M. While the Permanent gives
the number of perfect matchings for a bipartite graph;
the Hafnian gives perfect matching for any graph. Thus,
the Hafnian can be thought of as a generalization of the
Permanent. Using the adjacency matrix E, the relation
between the Hafnian and the Permanent is given by
0) = Per(E).
The hardness of simulating a noisy version of GBS has
been studied and GBS has recently become the second platform to show quantum computational supremacy , and the latest experimental venture towards dynamically programmable
GBS nanophotonic chip was carried out by .
1. The protocol
In GBS we consider m quantum modes (qumodes), represented by harmonic oscillators with canonically conjugate variables q and p. Gaussian states of the qumodes
are those represented by a Wigner-function W(q,p) that
has a Gaussian form. These states can be eﬃciently represented by the complex amplitude α =
2̵h(q + ip) and
a covariance matrix Σ ∈C2m×2m. A general pure Gaussian state can be generated from a vacuum with three
steps: i) Single mode squeezing; ii) multi-mode linear
interferometry; and iii) single-mode displacements.
the GBS protocol the state is then measured in the Fockbasis, performed in practice using photon number resolving detectors. The optical circuit in Figure 4 shows how
the system is initialized in the vacuum state, followed
by single- and multi-mode squeezing operators S(zi) and
S(zi,zj), respectively, and an interferometer with phaseshifters R(θj) and beamsplitters BS. At the end of the
protocol, the photon number in each mode is measured.
For a Gaussian state with zero mean (of the Wignerfunction), the probability of detecting si photons in the
i-th qumode is given by: 
P(s1,s2...sm) =
√s1!s2!⋯sm!
where all the matrices are deﬁned in terms of the covariance matrix Σ:
Q = Σ + 1/2
A = X(1 −Q−1)
The As matrix is a matrix created from A such that if
si = 0, we delete the rows and colums i and i + m of the
matrix, and if si ≠0, we repeat the rows and columns si
This means that by manipulating the covariance matrix Σ, we control the matrix from which we sample the
Hafnian. For a pure Gaussian state, it can be shown that
the A matrix is symmetric .
A simpler form of the experiment where instead of
counting the number of photons in each mode, we only
detect if there are photons or not in each mode, can be
used to sample from the so-called Torontonian function
of a matrix . If the probability of
observing more than one photon per output mode stays
low enough, this model has been shown to stay classically
intractable to simulate. A more general experiment instead, where the mean of the Gaussian states is non-zero,
can be used to sample from the loop Hafnian .
2. Applications
A number of algorithms for applications of GBS have
been investigated, and are reviewed by . Here we only brieﬂy summarise that work. Typically GBS algorithms are based on heuristics, and GBS
devices are often used to provide a seed for starting points
of classical algorithms. GBS can also be viewed as directly giving access to a statistical distribution, such as
in the case of point processes .
Problems in chemistry have been tackled using GBS.
Vibrational spectra of molecules have been computed using GBS by mapping the phononic modes of the molecule
to the qumodes of the GBS device ,
and by extension electron-transfer reactions have been
studied . The technique of sampling high-weight cliques has also been applied to predict
molecular docking conﬁgurations .
The largest number of GBS algorithms are for graph
problems, since the adjacency matrix of a graph is a natural ﬁt as the symmetric A matrix. The Hafnian function
computes the number of perfect matchings of a graph, so
the samples from the GBS device are with high likelihood
from sub-graphs with high density. This is how GBS is
used to identify dense subgraphs , and to get good initial guesses for classical search
algorithms to compute the max-clique of a graph .
GBS can also be used to build succinct feature vectors, or “ﬁngerprints”, of larger graphs via coarse-graining
techniques. These feature vectors can then be used as inputs to statistical methods or machine learning to classify
graphs. One such problem is to measure the similarity
between graphs , which has applications in tasks such as checking ﬁngerprint comparison
or detecting mutations of molecules.
GBS can also be used as a type of importance sampling device to speed up algorithms requiring randomness. This is how stochastic search algorithms have been
sped up by sampling from a GBS device encoding the
graph to be searched, instead of sampling uniformly .
Recently variational methods have been used within
the GBS framework and applied to
stochastic optimization and unsupervised learning. The
method is based on varying the squeezing and interferometer parameters in the device and updating based on
the measurement outcomes.
C. Analog quantum simulation
Simulating a quantum system is a hard problem for
classical computers as the Hilbert space increases exponentially with the size of the system. As a solution
to this long-standing problem, Feynman suggested the
ground breaking idea to harness that physical systems
given us by nature are quantum-mechanical.
He proposed to use quantum systems that are well-controlled
in the lab to simulate other quantum systems of interest . This concept has spurred the ﬁeld
of analog quantum simulation .
The core idea diﬀers from digital quantum simulation . Digital quantum simulators decompose the quantum dynamics to be simulated into a circuit of discrete gate operations that are implemented on
a quantum processor. The quantum processor is a well
controlled quantum system, that is engineered to be able
to eﬃciently apply a set of speciﬁc quantum gates that
are universal, i.e. a sequential application of those gates
can realize arbitrary unitaries (see Sec. V.B.1).
With this universal approach, a wide range of quantum
problems can be simulated to a desired accuracy with a
polynomially increase in quantum resources only . However, current quantum processors have only
limited coherence time and lack the capability to correct
errors that inevitably appear during the computation,
severely limiting the range of dynamics that can be simulated. In contrast, the idea of analog quantum simulators is to map the problem Hamiltonian to be simulated
ˆHsys to the Hamiltonian of the quantum simulator ˆHsim,
which can be controlled to some degree, ˆHsys ↔ˆHsim.
One then runs the quantum simulator, and maps the results back to the problem.
The range of problems that can eﬃciently mapped to
the simulator is limited, however as one uses the native
quantum dynamics of the simulator, the accessible system size, coherence length and errors is often more favorable compared to current digital quantum simulators.
1. Implementations
A wide-range of implementations in various controlled
quantum systems has been achieved, ranging from solid
state superconducting circuits , quantum dot arrays , nitrogen-vacancy
centers , atomic and molecular physics
based platforms such as trapped ions , interacting photons , Rydberg atoms , and
cold atoms .
Concepts of analog quantum simulation have been used
within VQA as well, such as problem inspired ansätze
(see Sec. II.B.1) or protocols inspired by quantum control . Experimental results for a quantum many-body problem beyond
current classical computational capabilities have been reported for 2-D systems .
2. Programmable quantum simulators
An analogue quantum system, such as a superconducting circuit, can be adapted to simulate arbitrary dynamics . The idea is to drive the parameters of the Hamiltonian H(t) that describes the analog
quantum simulator in time t. This can be done by adjusting the physical parameters of the quantum simulator
in time. The driving protocol is engineered via machine
learning methods such that the effective dynamics of the driven system over a time T corresponds to the evolution of a problem Hamiltonian one
wants to simulate. The eﬀective dynamics that is generated can realize long-range interactions as well as complicated many-body terms, which are natively not supported by the quantum simulator and are often hard to
simulate on digital quantum simulators. By periodically
driving the analog quantum simulator with the aforementioned driving protocol, various problem Hamiltonians
can be simulated . One can
realize complicated many-body dynamics or chemistry
problems , as well as solve combinatorial tasks such as
SAT-3. Trapped ion based analog quantum simulators
have been recently used for the implementation of quantum approximate optimization algorithm 
as well as quantum spin model with tunable interactions
for system sizes ranging from 64 to 256 qubits . For ion traps, a programmable quantum
simulator can be designed by light ﬁelds, that are applied to manipulate the internal degrees of freedom as
well as the interaction between diﬀerent ions. This allows one to simulate various types of spin Hamiltonians
with a high degree of control over the parameters .
D. Digital-analog quantum simulation and computation
As opposed to analog simulators that are limited by
the Hamiltonians they can simulate , digital quantum simulators can simulate any system’s Hamiltonian,
but with sometimes costly quantum resources. To beneﬁt
from a combination of the two approaches, the digitalanalog method to quantum computation and simulation has been proposed.
These schemes combine the application of digital singlequbit gates with the underlying analog Hamiltonian of
the quantum processor.
This approach allows for universal simulation of quantum dynamics while replacing
two qubit gates for an analog Hamiltonian and has been
argued to be more resilient against certain types of noise
than digital quantum computing .
Digital-analog quantum simulation has been proposed
to simulate the Rabi model ,
Dicke model ,
and fermionic systems .
Digital-analog quantum simulation has
been reviewed in , whereas digitalanalog quantum computing is more recent. The implementation of digital-analog quantum computing has been
proposed for superconducting platforms . So far the computing framework has been used to simulate Ising models , where the analog blocks can be
used to enhance the eﬀective connectivity of the qubits to
simulate graphs that have a diﬀerent connectivity from
the native connectivity of the quantum device . The analog blocks have also been applied
to reduce the operation count required to perform the
quantum Fourier transform .
The digital-analog approach has also been combined
with VQA (see Sec. II) resulting in a digital-analog
QAOA algorithm, where the two-qubit gates have been
replaced by analog blocks . This
also has two versions: i) where a layer of entangling gates
is replaced by an analog block; and ii) where a continuous analog block is applied continuously with single-qubit
operations overlayed.
E. Iterative quantum assisted eigensolver
Almost all of the VQAs update a PQC’s parameters in
a feedback loop. However, there exist alternative algorithms that can circumvent this approach with the ansatz
given by 
∣φ(α(t),θ)⟩=
αi(t)∣ψi (θi)⟩,
where αi ∈C and θi ∈Rki for non-negative integers ki.
This ansatz is a linear combination of quantum states,
where the αi parameters are stored on a classical device.
In the special case m = 1 it corresponds to the usual PQC,
whereas for m > 1 this ansatz subsumes it. This ansatz
has been used for ﬁnding the ground state of Hamiltonians , excited
state the simulation of
quantum dynamics , error mitigation ,
nonlinear dynamics , linear systems and
semideﬁnite programming . If one
keeps the parameters of the PQC θi ﬁxed and only varies
the αi, the algorithm can be considered a borderline
non-VQA algorithm. Update of θi parameters has been
shown to cause trainability issues in VQA (see Sec. IV.A)
and thus by ﬁxing θi one can by construction circumvent
these issues. We present here the iterative quantum assisted eigensolver algorithm (IQAE) as an illustration,
and in the applications subsection the quantum assisted
simulator for closed systems (see Sec. VI.A.5), open systems (see Sec. VI.A.7) and Gibbs state preparation (see
Sec. VI.A.9).
The IQAE algorithm provides an approximation to the
ground state of a Hamiltonian H. Without loss of generality, the N-qubit Hamiltonian H is assumed to be a
linear combination of unitaries
Here, βi ∈C and Ui ∈SU (2N) for i ∈{1,2,⋯,m}. The
unitaries Ui act on at most O (poly (log N)) qubits. This
condition can be relaxed if the unitaries Pauli strings (see
Sec. II.A.1). The ansatz state is taken as linear combination of “cumulative K-moment states” CSK, which is generated using some eﬃciently preparable quantum states
and the unitaries deﬁning the Hamiltonian in Eq. (43).
For pedagogical reasons, we present the deﬁnition of Kmoment states and cumulative K-moment states.
Deﬁnition 3 )
For a given positive integer K, a set of unitaries U ≡
j=1 and a quantum state ∣ψ⟩, K-moment states is
the set of quantum states of the form {UjK⋯Uj2Uj1∣ψ⟩}j
for Ujl ∈U. Let us denote the aforementioned set by SK.
We deﬁne the singleton set {∣ψ⟩} as the 0-moment state
(denoted by S0). Finally, we deﬁne the cumulative Kmoment states CSK as CSK ≡∪K
As instructive example, note that the set of 1-moment
states is {Uj∣ψ⟩}m
j=1, where the unitaries {Uj}m
up the Hamiltonian H. The set of cumulative 1-moment
states is CS1 = {∣ψ⟩} ∪{Uj∣ψ⟩}m
j=1 , and the set of cumulative K-moment states is CSK = {∣ψ⟩} ∪{Uj1∣ψ⟩}m
⋅⋅⋅∪{UjK ...Uj1∣ψ⟩}r
j1=1,...,jK=1.
∣ξ (α)⟩(K)
∑∣χj⟩∈CSK αj∣χj⟩. The ground state problem reduces to
the following optimization program
subject to α†E(K)α = 1.
Here, the overlap matrices D(K) and E(K) are given by
= ∑i βi⟨χn∣Ui∣χm⟩and E(K)
= ⟨χn∣χm⟩.
overlap matrices can be computed on a quantum computer without the requirement of any complicated measurement involving multi-qubit controlled unitaries. For
example, for a Hamiltonian composed of Pauli strings,
the product of Pauli strings is a Pauli string up to a
phase factor ±1 or ±ι. Thus, the overlap matrices are
simply expectation values ⟨ψ∣ˆP∣ψ⟩of some Pauli string
ˆP, which can be easily measured (see Sec. II.C).
The optimization program 44 is a quadratically constrained quadratic program (QCQP) with single equality
constraint. The Algorithms proceeds in three serial and
disjoint steps.
1. Select ansatz, which can be done on paper
2. Estimate overlap matrices on quantum computer,
which can be done eﬃciently in a parallel fashion.
3. Post-processing on a classical computer to solve the
QCQP based on the overlap matrices from step 2.
As a major speedup compared to standard VQA, there
is no feedback loop between classical and quantum computer such that the calculations can be easily parallelized.
The ansatz can be improved by changing K to K + 1.
The ansatz construction is systematic and there is no
trainability issue such as the barren plateau problem (see
Sec. IV.A). For the QCQP, there exist conditions which
tell whether a local minima is a global minima as a stopping criteria for the classical solver. Moreover, the Lagrangian relaxation of the program 44 is a semideﬁnite
program and eﬃciently solvable.
IV. THEORETICAL CHALLENGES
A. Barren plateaus
It was recently shown that the expectation value of
the gradient of the objective function corresponding to
randomly initialized PQCs (RPQC) decays exponentially
to zero as a function of the number of qubits . The mathematical basis of this result hinges
on the fact that the PQC from Eq. (22) becomes a unitary
2-design as the circuit depth increases polynomially with
the circuit width i.e, the number of qubits. The notion of
unitary 2-design has been used extensively in the recent
proofs of barren plateau in RPQCs, which necessitates a
small discussion about their mathematical structure.
Using the notion of 2-design, the appearance of barren plateaus in the training landscape has been established for various kind of ansätze. Barren Plateaus can
be thought of as a consequence of the exponentially large
dimension of the Hilbert space when the number of qubits
increases and the fact that the variational circuit unitary,
when the parameters are initialized at random, is a 2design. Consequently, the strategies proposed to tackle
this problem focus on reducing the space dimension of
this unitary or breaking the randomness properties related to the 2-designs. Another way to think of the origin
of the barren plateau issue could be the problem-agnostic
nature of the ansatz, faced with exponentially large parameter space. Thus, one could attempt to devise ansätze
as well as the optimization methodology in a problemaware manner by using physically-inspired or problemspeciﬁc ansätze as the ones presented in Sec. II.B or those
proposed in Sec. III.E.
Besides the exponential parameter space that induced
barren plateaus, other physical phenomena can also generate them.
In particular, the noise and decoherence
present in the quantum computing experiments also
generates this problem in VQAs .
Entanglement-induced barren plateaus have also been reported recently .
While certain ansätze can be assumed or proven to
form (approximate) 2-designs, such proofs are challenging for the general ones. To numerically verify the presence of barren plateaus, past studies often considered
computing the gradients and variances of a local observable using a particular ansatz over increasing system
sizes .
Another attempt to avoid a barren plateau is to initialize the variational circuit with a particular state choice.
Intuitively, the algorithm will start in a particular region of the Hilbert space allowing the optimization subroutine to potentially ﬁnd the minima in a closer region. This strategy include all physically inspired methods mentioned in Sec. II.B. The use of clever encodings for the algorithm parameters can also be understood
as a initialization strategy 
(see Sec. VI.B). Classical algorithms such as neural networks can also be used to learn the proper circuit encodings .
A good choice for the initial state is often not enough to
reduce the size of the Hilbert space. Although expressive
circuit ansätze are usually a requirement for the success
of a VQA (see Sec. IV.B for more details), it can expand
the parameter space that the optimizer has to explore.
Several works propose circuit structures that reduce that
space by introducing correlations between the variational
parameters of the circuit , blockwise initialization of those parameters 
or exploring particular ansatz structures that barren plateaus also
emerge in shallow depth circuits, and that the use of
local cost functions reduces the exponential decay tendency to a polynomial one. The optimization strategy
may also reduce the eﬀect of the vanishing gradients,
for instance by training the circuit layer by layer or by measuring low depth
gradients . Certain variational
quantum algorithms for quantum simulation can be free
of barren plateaus when at every training step the state
to be learned is close to the state of the circuit .
Barren plateaus are a roadblock in the trainability
and hence any PQC ansatz which suﬀers from this phenomena will fail to properly train the parameters in its
search for the near-optimal (or optimal) performance. As
shown in Ref. , even the family
of gradient-free approaches which perform local search,
therefore mimicking gradient-based optimization, seem
to face similar challenges. However, one can circumvent
this issue by using hybrid quantum states of the form
of equation Eq. (42) or hybrid density matrices as introduced in Ref. . The idea is
to write the overall ansatz as a classical combination of
quantum states i.e, ∣φ(α(t),θ)⟩= ∑m−1
i=0 αi(t)∣ψi (θi)⟩.
Tuning the θi can often lead to barren plateaus. One can
avoid such issues by ﬁxing θi by harnessing the structure
of the problem to ﬁnd the basis states of the ansatz, i.e,
{∣ψi (θi)⟩} (see Sec. III.E for more details). Interestingly,
quantum convolutional neural networks also do not exhibit barren plateaus .
B. Expressibility of variational ansätze
A cornerstone in the success of VQA is choosing the
proper ansatz for the problem. In addition to trainability, i.e. how well the ansatz can be optimized, another
major quality is expressibility. This concerns whether a
given PQC is able to generate a rich class of quantum
states. The number of PQC layers, parameters or entangling gates required to achieve a given accuracy is also
linked to the expressibility of the circuit.
Expressibility.
Sampling states from a PQC ∣ψθ⟩for randomly chosen θ generates a distribution of states. Expressibility is deﬁned as the deviation of this distribution
from the Haar measure, which samples uniformly from
the full Hilbert space
A(t) = ∣∣∫Haar(∣ψ⟩⟨ψ∣)⊗tdψ −∫θ(∣ψθ⟩(⟨ψθ∣)⊗tdψθ∣∣2
where ∫Haar dψ denotes the integration over a state ∣ψ⟩
distributed according to the Haar measure and ∣∣A∣∣2
Tr(A†A) the Hilbert-Schmidt norm. An ansatz circuit
U with small A(t)
is more expressive, with A(t)
U = 0 corresponding to being maximally expressive, as it generates quantum states with a distribution closer to the
Haar measure.
The PQC samples uniformly from the
full Hilbert space and thus is able to approximate any
possible state. This is especially important in the case
where one wants to train the PQC to represent a particular quantum state while having little prior information
about the state. A highly expressive PQC is more likely
to be able to represent the target state.
Entangling capability.
This measure denotes the power
of a PQC to create entangled states and can be used as
another quantiﬁer of the expressiveness of an ansatz. In
 the Meyer-Wallach Q measure has been proposed to estimate the
number and types of entangled states a particular PQC
can generate.
One deﬁnes a linear mapping ιj(e)
that acts on the computational basis ιj(b)∣b1⋯bn⟩=
δbbj ∣b1⋯˜bj⋯bn⟩, where bj ∈{0,1} and ˜bj denotes absence
of the j-th qubit. The entanglement measure Q is then
D(ιj(0)∣ψ⟩,ιj(1)∣ψ⟩),
where D is the generalized distance deﬁned by the coef-
ﬁcients of two states ∣u⟩= ∑ui ∣ei⟩and ∣v⟩= ∑vi ∣ei⟩,
D(∣u⟩,∣v⟩) = 1
∣uivj −ujvi∣2.
It can be rewritten as the average of the purity of each
qubit 
Q(∣ψ⟩) = 2(1 −1
where ρk is the density matrix of the k-th qubit. Thus,
Q(∣ψ⟩) is an entangling monotone and can
be interpreted as the average of the entanglement of each
qubit with the rest of the system.
Only if the state is a product state we ﬁnd Q = 0,
whereas Q = 1 is reached for certain entangled states
such as the GHZ state. The entangling capability of a
PQC is then deﬁned as the average Q of states randomly
sampled from the circuit.
where S = {θi}i is the set of sampled circuit parameters.
Parameter dimension
The parameter dimension DC is
the number of independent parameters of the quantum
state that is generated by the PQC .
From this measure, one can calculate the redundancy
of a PQC, i.e. the fraction of parameters that can be
removed without loss of expressive power.
local measure of expressibility is the eﬀective quantum
dimension GC(θ), which can be used to calculate the
expressive power of initialization strategies for the PQC.
Under a small variation of the PQC parameter θ, it measures how many independent directions in the parameter
space exist for the quantum state.
Both measures
can be calculated as the number of non-zero eigenvalues of the Fubini-Study metric tensor deﬁned in Eq. (33).
In , a wide class of circuits have been investigated with the aforementioned expressibility measures. It has been found that certain types of ansätze are
more expressive, e.g. layered PQCs consisting of CNOT
iSWAP gates are more expressive than CZ. There
is a trade-oﬀbetween an ansatz being expressive and
trainable. Making an ansatz more expressive most likely
will result in reducing the gradient of the objective function. In , the authors suggest several
strategies for reducing expressibility and improving trainability, including correlating parameters or restricting rotation angles of parameterized gates. Interpolating the
PQC parameters between ﬁxed and random angles has
been proposed as another method .
Expressibility of PQCs have been further explored using classical Fisher information and
memory capacity .
It has been shown that alternating layered ansatz (see
Sec. II.B.2) is both relatively expressive as well as does
not exhibit barren plateaus in certain regimes . In VQE algorithms, there is a
trade-oﬀbetween the number of layers in this ansatz and
the correlation length of critical Hamiltonians. However,
in the critical phase, the number of layers must exceed
a certain threshold dictated by the system size to show
an exponential improvement. The circuit depth unravels
an eﬀective correlation length that can be used as an
estimation of the number of free parameters in the ansatz
 .
C. Reachability
Reachability discusses the question whether a given
PQC ∣Ψ(θ)⟩with parameters θ is capable of representing
a quantum state that minimizes some objective function.
This can be quantiﬁed by the reachability deﬁcit over
ﬁnding the minimum of an objective function ˆO as
fR = minψ∈H ⟨ψ∣O ∣ψ⟩−minθ ⟨Ψ(θ)∣O ∣Ψ(θ)⟩,
where the ﬁrst term on the right side is the minimum
over all states ∣ψ⟩of the Hilbert space, whereas the second term is the minimum over all states that can be represented by the PQC. The reachability deﬁcit is equal or
greater than zero fR ≥0, with fR = 0 when the PQC can
generate a state ∣Ψ(θ∗)⟩, where θ∗are the parameters
that minimizes the objective function.
Reachability has been studied in-depth for QAOA.
Although QAOA has been shown to exhibit quantum
computational universality , which implies that any unitary operator is reachable under the QAOA ansatz, this statement does not
hold true for ﬁnite ﬁxed depths p. In fact, it was shown
that QAOA exhibits reachability deﬁcits for the MAX-2-
SAT and MAX-3-SAT problems, where the optimal value
of the objective function cannot be found using a ﬁxed
circuit depth p beyond a critical clause density (deﬁned
as the ratio between the number of clauses and the number of variables in the problem) . In
other words, for problems with a certain clause density,
there is a critical depth p∗for which the optimal solution
can only be found (up to a threshold) if p ≥p∗. As p∗
grows with the clause density, this limits the performance
of QAOA for problem instances with high clause density.
Similar reachability deﬁcits have also been found in the
variational Grover search problem .
Moreover, by re-analyzing the experimental data from
Google’s Sycamore quantum processor on the application
of QAOA to various graph optimization problems , authors from 
also discovered reachability deﬁcits in this case, where the
graph density (deﬁned as the ratio between the number
of graph edges to the number of graph nodes) replaces
the clause density as the order parameter.
Note that the reachability deﬁcits are distinct from the
barren plateau problem, where the gradients of the objective function concentrate to zero for many choices of
initial variational parameters, thus slowing down the optimization process. On the other hand, the reachability
deﬁcit for p < p∗is independent of the initial parameters.
D. Theoretical guarantees of the QAOA algorithm
The QAOA has several key analytical results which
have contributed to its considerable interest in recent
years. The quantum advantage of QAOA algorithm has
been studied in , where they
showed that the eﬃcient sampling of the output distribution of QAOA, even for the lowest depth case of
p = 1, implies the collapse of the polynomial hierarchy
(see Sec. I.A ). Following the conjecture from complexity
theory that the polynomial hierarchy does not collapse,
this result propels QAOA as a possible candidate for establishing some quantum advantage.
In particular, it
has been shown that for p = 1, 420 qubits would suﬃce
to demonstrate quantum advantage .
The power of QAOA compared to classical algorithms
is an ongoing topic of research.
For speciﬁc instances
of the Max-Cut problem, QAOA for p = 1 was shown
to perform equally well or worse than classical algorithms . For more
discussion on QAOA for Max-Cut, refer to Sec. VI.C.1.
For QAOA of depth p, the measurement outcomes of
a qubit depend on the p-neighbourhood of that qubit.
Thus if p is too small, it does not ‘see’ the whole graph
 . For large p, the QAOA algorithms
can ‘see’ the whole graph with no known indications regarding the performance limitations.
For the case where the problem Hamiltonian HP takes
z + ωBˆσ2i+1
+ γBAˆσ2i+1
where ωA(B) are the coeﬃcients for the even (odd) sites
and γAB(BA) are the interaction strength between ﬁrst
(second) neighbour spins.
Taking HM as deﬁned in
Eq. (19) for a 1D lattice, Ref.
 showed
that QAOA can be used to implement universal quantum computation. This result was proven and generalized in a later work to include a
larger class of problem and mixing Hamiltonians that can
provide computational universality for QAOA.
By connecting VQA with optimal control theory, Pontryagin’s minimum principle of optimal control is used to
show that the bang-bang protocol (in which the evolution
switches abruptly between two Hamiltonians) is optimal
for a ﬁxed total time T . Since QAOA
can be regarded as a bang-bang ansatz by switching between unitary evolution under HP and HM respectively,
this suggests the optimality of QAOA as a VQA. However, recent works have challenged this claim. By generalizing the argument in Ref. , it has been
shown that the optimal protocol actually possesses the
‘bang-anneal-bang’ structure . Such
protocols begin and end with a bang, with regions of
smoothly varying control function (akin to quantum annealing) in between. It was also shown that when the
total time T is large, bang-bang QAOA suﬀers from the
proliferation of local minima in the control parameters,
rendering it diﬃcult to ﬁnd optimal (or near-optimal)
QAOA parameters.
V. PROGRAMMING AND MAXIMIZING NISQ UTILITY
Current NISQ devices have a limited number of qubits
(∼50 −100) available.
In addition, due to their noisy
nature and short coherence time, one can only perform
a restricted number of gate operations.
In order to
make maximal use of the currently available quantum
resources, there are two approaches from the operational
point-of-view: the bottom-up and the top-down.
bottom-up approach refers to the scenario where one has
full control over the design of any quantum computing
platform to keep pushing the performance quality such
as gate ﬁdelity and coherence time for a given hardware
design constraints. The top-down approach means implies that one does not get involved in hardware design
and simply makes use of what has already been made
or fabricated in the experimental labs. In this section,
we focus on the latter approach, i.e. extending or maximizing the utility of current and near-term quantum devices from an algorithmical perspective. Finally, we also
present a summary of software tools to control, program
and maximize the utility of NISQ algorithms.
A. Quantum error mitigation (QEM)
Sensitivity to errors and noise are the two most prominent roadblocks towards scalable universal quantum computers.
Fault-tolerant quantum computing can be attained by encoding non-Abelian anyons in topological
materials or applying quantum error correction codes . While
the former is still in its infancy, the latter mandates
physical resources exceeding our current experimental
capabilities. In the NISQ era of running hybrid quantum/classical algorithms, it is desirable to use all the
restricted and available qubits as logical qubits without
applying QEC techniques. As we discuss throughout this
review article, the hybrid quantum/classical algorithms
rely on computing the expectation value of some physical
observables using quantum processors. Quantum error
mitigation (QEM) techniques discussed in this subsection
need no extra qubit and can suppress errors in ﬁnding expectation values with simple classical post-processing and
diﬀerent runs of quantum circuits. To be precise, with
QEM, we are not interested to recover the ideal quantum
output state ˆρ(0), but to estimate the ideal observables
ˆA expectation value: E[µ(0)] = ⟨ˆA(0)⟩= Tr(ˆρ(0) ˆA) , sometimes surpassing the break-even point, where
the eﬀective gates are superior to their physical building
blocks, at an aﬀordable cost with respect to near-term
quantum hardware . Here, µ is the
outcome of a measurement and we use superscript (0) to
denote an ideal noise-free realization of a state, operation
or observable quantity. Recently, it was also shown how
to achieve stochastic error mitigation for a continuous
time evolution . For a comprehensive
treatment on quantum error mitigation, refer to .
1. Zero-noise extrapolation
 , and 
independently and concurrently proposed the Richardson extrapolation QEM, namely zero-noise extrapolation
(ZNE), where a quantum program is to operate at various eﬀective noise levels of a quantum processor. It is
then extrapolated to an estimated value at a noiseless
Formally, a quantum circuit/system in the presence
of noise can be modelled as an open quantum system using the Gorini-Kossakowski-
Sudarshan-Lindblad equation or in short the Lindblad
master equation (setting ̵h = 1):
dt ˆρ(t) = −i[ ˆK (t), ˆρ(t)] + ˆˆL[ˆρ(t)],
where ˆK(t) acts as time-dependent driving Hamiltonian,
and ˆˆL[.] = ∑k Γk( ˆOk[.] ˆO†
2{ ˆOk ˆO†
k,[.]}) is a superoperator. The above equation in general describes Markovian dynamics for Γk ≥0. Whenever loss rate Γk become
negative , the
above equation would also describe non-Markovian dynamics . To ensure the complete positivity, we require ∫
0 Γ(t′)dt′ > 0, ∀t. In general, Γk are ﬁxed by the
nature of the noise experienced by a quantum system.
Mathematically, one can parametrize Γk with a dimensionless scalar λ, i.e., Γk →λΓk. When λ = 0, there is no
noise and the second term (loss term) in Eq. (52) is zero,
resulting in pure unitary dynamics. When λ = 1, the actual quantum device loss rate is matched. In summary,
ZNE involves two steps.
1. Noise-scaling: we make a number of measurements
E[µ(λj)] for λ ≥1.
2. Extrapolation: we estimate E[µ(0)] from the previous step.
Noise-scaling
can be accomplished in three ways. Firstly,
 it was proposed to use
a time-scaling approach by taking λ > 1, which means
that the time-dependent driving Hamiltonian
now rescaled by
λ ˆK(t/λ). This approach is only possible if the user has full control over back-end quantum
processor.
Control pulses for each quantum gate have
to be recalibrated and be applied for longer duration.
Secondly, one can apply a technique called circuit folding . Suppose that a quantum circuit is composed of d unitary layers such that
U = Ld⋯L2L1 where d refers to the circuit depth and
each Lj either represents a single layer of gate operations or just a single quantum gate. The circuit folding
is then achieved by
U →U(U †U)n,
where n is some positive integer. Since U †U is an identity, this action has no eﬀect on an ideal circuit. However,
in noisy circuit, U is imperfect and the above 1 + 2n circuit operations would increase the noise level. Thirdly,
instead of entire circuit folding, one can also use gate
folding technique :
The second and third techniques do not require users to
have full control of quantum computer back-end and thus
we expect to be of greater use in software level control of
quantum circuits.
Extrapolation step
of the ZNE method can be considered as a regression problem if we choose to consider
a generic model for calculating the expectation value
Emodel[µ(λ;Υ)], where the meaning of model would become clear shortly and Υ corresponds to the model parameters.
We note that the expectation value E is
a real number that can only be obtained in the inﬁnite measurement limit. With limited number of measurement samples N, statistical estimation is ˆE[µ(λ)] =
E[µ(λ)] + ˆδ 1, where ˆδ is a random variable with zero
1 The hat notation used is in accordance with statistics notation
and should not be confused with a quantum operator.
mean and variance σ2 = E(ˆδ2) = σ2
the single-shot variance.
Given a set of m scaling parameters λ = {λ1,λ2,⋯,λm} with λj ≥1, and the corresponding measurement outcomes µ = {µ1,µ2,⋯,µm}, the
ZNE is nothing but to build a good estimator ˆE[µ(0)] for
E[µ(0)] such that its bias E( ˆE[µ(0)] −E[µ(0)]), and its
variance E( ˆE[µ(0)]2) −E( ˆE[µ(0)])2 are both reasonably
Onwards, let us adopt a simpliﬁed notation of
E[µ(λ)] = E(λ). Now let us mention brieﬂy the statistical models. The expectation value E(λ) cannot be of
any arbitrary function, which would make ZNE impossible to extrapolate back to E(0). Depending on some
underlying noise model assumption, one can apply various statistical models.
The polynomial extrapolation
is based on the polynomial
model of degree d such that
poly(λ) = c0 + c1λ + ⋯+ cdλd,
where cj are d + 1 unknown real parameters. This extrapolation is justiﬁed in weak noise limit and we need
the number of data points m to be equal or larger than
d+1. Consequently, we can obtain two other variants: the
linear extrapolation (d = 1) and the Richardson extrapolation (d = m−1) . By construction,
the error with respect to the true expectation value is
O(m) when we have large sample size N →∞. By using
the interpolating Lagrange polynomial, the estimator is
explicitly given by:
ˆERich(0) = ˆc0 =
with the assumption that all λj are diﬀerent. One important observation is that the Richardson model based ZNE
is dictated by a statistical uncertainty which is exponentially scaling with the number of data points. There are
also other statistical models such as poly-exponential extrapolation and exponential
extrapolation .
Various exponential
extrapolation methods have been proposed and investigated in and applied to depolarizing noise
in .
In fact, shows
that the ZNE, quasi-probability and stabiliser-based approach can be combined by exploiting novel aspects of
the individual technique.
The ZNE scheme suﬀers from a few limitations. The
scheme works by extrapolation, and hence it is challenging to obtain result guarantees in general. The number
of measurement shots required to obtain the mitigated
expectation value can be relatively high compared to the
unmitigated case as seen above. More importantly, the
fundamental drawback of both ZNE and probabilistic error cancellation (PEC) , or quasiprobability method (which is discussed next) is that one
needs to know the precise physical noise model in advance, which in itself is a diﬃcult problem. Experimentalists in the lab will have imperfect knowledge about
the real noise, which will typically diﬀer from the canonical ones. We will also discuss a more practical approach
based on gate set tomography proposed in Ref. , which does not require explicit knowledge of
the noise model and mitigates any localized Markovian
errors, so that the error in the ﬁnal output is only due to
unbiased statistical ﬂuctuation.
2. Probabilistic error cancellation
Let us familiarize ourselves with the notations used in
quantum tomography , which we adopt here. A quantum state is represented by a density matrix ˆρ, and a physical observable
is denoted by a Hermitian ˆA operator. An operation is
a map on the states space such that one can use the
Kraus representation to denote it as: ˆˆL[ˆρ] = ∑j ˆKj ˆρ ˆK†
We note that this equivalence with Eq. (52) is only valid
when we have Markovian dynamics. Here, ˆKj are Kraus
operators.
In terms of the Pauli transfer matrix representation, ˆρ in Eq. (52) can be written as a column
vector , denoted as ∣ρ⟫. Similarly, the Lindblad superoperator ˆˆL can be recast as
a square matrix, i.e., using the Pauli transfer matrix
representation, since it is a linear map. For simplicity
and without loss of generality, we may absorb the unitary dynamics (the ﬁrst term in Eq. (52)) into ˆˆL onwards.
A physical observable ˆA is now written as a
row vector ⟪A∣. Consequently, the expectation value is
⟨ˆA⟩= Tr[ ˆAˆρ] = ⟪A∣ρ⟫. Likewise, the expectation of ˆA
after the state ˆρ passing through a series of linear maps
is read as: Tr[ ˆA ˆˆLN ○⋯○ˆˆL1(ˆρ)] = ⟪A∣LN⋯L1∣ρ⟫.
The central theme of probabilistic error cancellation
(PEC) or quasiprobability decomposition introduced by
the IBM team in Ref. is that one
can estimate the expectation value of an observable by
sampling from a set of erroneous circuits, labelled by L(l)
for l = 1,2,⋯, such that
⟨ˆA(0)⟩= ∑
ql⟪A(l)∣L(l)
tot∣ρ(l)⟫.
The expectation of an observable is going to be far-oﬀ
(without QEM) from the ideal value due to the presence of noise2.
Given speciﬁc error models (assuming
experimentalist has full and correct knowledge about
2 The superscript (0) to denote the ideal noise-free realisation of
a state, operation or observable quantity.
them), the real numbers ql, which represent quasiprobabilities, can be eﬃciently derived. Here, each L(l)
tot represents the total sequence of noisy gates in the lth circuit.
Monte Carlo sampling could be used to compute ⟨ˆA(0)⟩
by randomly choosing the lth circuit with the probability
pl = ∣ql∣/C, where C = ∑l ∣ql∣. Lastly, the computed result
is given by the expected value of eﬀective measurement
outcomes ⟨A(0)⟩= CE[µeﬀ], where the eﬀective outcome
is µeﬀ= sgn(ql)µ(l) if the lth circuit is chosen and µ(l) is
the outcome from the lth circuit.
As a consequence, the mean value of the PEC outcome
centers around the ideal one with larger variance due to
C (see Fig. 5 right corner).
The above PEC method relies on the correct knowledge of error model L(l)
tot as is apparent from Eq. (57).
To enable practical implementations, 
proposes to combine linearly independent basis set operations and gate set tomography to fully remove impact of
localized Markovian errors by systematically measuring
the eﬀect of errors to design eﬃcient QEM circuits. The
set of operations including measurement and single-qubit
Cliﬀord gates is universal in computing expected values
of observables. For the single-qubit case, any operation
L which is a 4×4 real matrix in the Pauli transfer matrix
representation, can be expressed as a linear combination
of 16 basic operations, i.e., L = ∑16
i=1 qiB(0)
, which are
composed of {π,H,S,Rx,Ry} gates .
Similarly, the same decomposition can be applied to the
two-qubit case. See Fig. 5 for example decompositions.
A way to systematically measure errors is through gate
set tomography (GST), with which one can even mitigate
state preparation and measurement errors. In short, the
purpose of GST is to measure noisy individual quantum
circuit performance a priori. For a single-qubit gate, one
prepares initial states ∣0⟩,∣1⟩,∣+x⟩, and ∣+y⟩, where ∣+x⟩
and ∣+y⟩are the eigenstates of Pauli operators ˆσx and
ˆσy with +1 eigenvalue, respectively. For noisy devices,
these four states are denoted as ¯ρ1, ¯ρ2, ¯ρ3 and ¯ρ4, accordingly.
We also use ¯L (superoperator) to denote a
noisy/imperfect gate to be measured. Since what we care
about are expectation value of physical observables, for
single-qubit case, we have observables ˆI, ˆσx, ˆσy, ˆσz, denoted as ¯A1, ¯A2, ¯A3, ¯A4. The mean value of observables,
the 4×4 matrix ˜A, is nothing but ˜Aj,k = Tr[ ¯Aj ¯L¯ρk]. Similarly, we can also construct the 4 × 4 matrix g without
applying any gate to the initial states as gj,k = Tr[ ¯Aj ¯ρk].
This is repeated for each qubit and each single-qubit gate.
Statistical estimation of the initial states ¯ρk and the observables ¯Aj are then given by
∣ˆρk⟫= T●,k,
⟪ˆAj∣= (gT −1)j,●,
where we note that the hat symbol represents the statistical estimate and T●,k(Tj,●) denotes the kth column (jth
row) of the matrix T, where T is an invertible 4×4 matrix
Probability
Expectation
ideal value
with error mitigation
no error mitigation
Quasi-probability decomposition
Figure 5 Quantum computing of the expected value of an observable using gate set tomography-based PEC. Quasiprobability
decomposition of initial state preparation, examplary single- and two-qubit processes are computed. Implementing the resulting
decomposition is done using the Monte Carlo approach. With QEM, the probability distribution of expected value of the physical
observable is now centered around an ideal value with larger variance as compared to the one without QEM. Inspired by .
with the following relationship ˆL = Tg−1 ˜LT −1. The same
procedure applies for the two-qubit case with the only
diﬀerence being that there are total of 16 initial states:
¯ρk1 ⊗¯ρk2 and 16 observables: ¯Aj1 ⊗¯Aj2 to be measured.
Similarly, we have g = g1 ⊗g2 and T = T1 ⊗T2. We have
to implement two-qubit gate GST for each qubit pair involved in a quantum program run.
Quasiprobability decomposition is then computed
based on GST results above. From GST, we have estimation of initial states ∣ˆρk⟫, observables to be measured
⟪ˆAj∣, and gates ˆL. Let’s denote L(0) as the Pauli transfer
matrix of the ideal gate with no error. The main idea of
decomposition comes from a very simple idea that a noisy
gate operation comes from an ideal operation followed by
a noise operation, i.e., L = NL(0). Hence, the inverse of
the noise is given by N −1 = L(0) ˆL−1 = ∑i qL,i ˆBi. And,
by applying the inverse of the noise after the operation,
we can obtain the operation without error: L(0) = N −1L.
Notice that the matrices in the above equation are obtained from the ﬁrst GST step. The remaining task is
to determine quasiprobabilities qL,i for each qubit and
gate involved by solving the above equation. We note
that instead of quasiprobabilistic decomposition of quantum gates, one could in principle use randomized compiling technique proposed in Ref. and superconducting circuits . Lastly, a similar
strategy was recently applied to mitigating errors in measurement readout .
3. Other QEM strategies
We have seen that the quantum error mitigation techniques discussed so far do not require any ancilla or extra
qubits with the caveat that one needs to perform more
measurements. At the same time, one is only interested
in information about the expectation value. Along this
line of thought, there exist several proposals, which we
will outline below. However, some of the methods might
require ancilla qubits.
Subspace expansion method
 are designed to mitigate
errors in the VQE routine, where we often tend to ﬁnd
an approximate ground state ∣ψa⟩of a system Hamiltonian H. However, such state may diﬀer from the true
ground state ∣ψg⟩due to noisy processes.
In general,
we do not know which error occurred to the state. The
subspace expansion method works by resolving the action of H on the linear combination of quantum states
ansatz Eq. (42).
The subspace is spanned by a set of
operators ˆOi, i.e., {∣ˆOiψa⟩}. Now, one proceeds to evaluate Hij = ⟨ψa∣ˆOiH ˆOj ∣ψa⟩, and Sij = ⟨ψa∣ˆOi ˆOj ∣ψa⟩. The
latter is needed since the subspace states are in general
not orthogonal to each other.
By solving the generalized eigenvalue problem HC = SCE, with eigenvectors
C and diagonal matrix of eigenvalues E, we can obtain
the Hamiltonian spectra including the excited states (see
Sec. VI.A.4).
This method requires an appropriate choice of subspace operators to mitigate errors due to external noise.
In general, without knowing the noise models of quantum device, it would require an exponential number of
expansion operators to obtain the optimal groundstate.
Stabilizer based approach
 relies on the information associated with conserved quantities such as spin and particle number conserving ansatz. If any change in such quantities is detected, one can pinpoint an error in the circuit, which is
akin to stabilizer measurement in quantum error correction schemes. We can implement the stabilizer measurements by adding ancilla qubits to the qubit registers or
taking additional measurements and post-processing.
Individual error reduction
method was proposed in Ref.
 . As we have seen earlier, Markovian noise can be modelled using the Lindblad master
equation, Eq. (52), where we have dˆρ
dt = ˆˆL(ˆρ) = ∑i Li(ˆρ),
where each Li denotes a noise channel present.
we have absorbed the unitary component into ˆˆL. It was
shown that
˜ρ(T) = ˆρ(T) −
(ˆρ(T) −ˆρj(T)),
= ˆρ(0)(T) + O(τ 2).
Notational explanations are as follows. ˆρ(T) is the density matrix after applying quantum gates with the presence of all associated noise channels at the ﬁnal evolution
time T. In contrast, ˆρj(T) is the state under the inﬂuence
of all the noise channels but one less Lj according to the
ratio gj. Notice that if gj = 1, we have fully removal of the
entire channel Lj. ˆρ(0)(T) is the ideal output state without any error, while τ is the evolution time for each noise
process after the gate application. We note that the ﬁrstorder error O(τ) is removed. As usual, what we want to
obtain is ⟨ˆA⟩= Tr[ˆρ(0)(T) ˆA] for a physical observable ˆA.
We can arrive at it by using Eq. (60). Though its result
is neat and beautiful, this method assumes a perfect removal of individual noise channel. Hence, it is relatively
unrealistic on current quantum hardware as compared to
other strategies.
Dynamic error suppression/robust control techniques
concern suppression of experimental gate errors at the pulse
control level, which can be passive as well as active one.
Pulse shaping technique is a strategy for passive cancellation of system-bath interaction. Traditionally, this
method stands on the shoulder of a mean to obtain high-
ﬁdelity quantum gate in nonlinear qubits such as transmons, commonly known as derivative removal of adiabatic gate (DRAG) scheme . On the other hand, dynamical decoupling (DD) is a very well-known and
widely used quantum control technique in the literature, which is designed to suppress decoherence via fancy
pulses to the system so that it cancels the system-bath interaction to a given order in time-dependent perturbation
theory in an active manner. Recently, DD
experiments were performed on the 16-qubit IBMQX5,
5-qubit IBMQX4, and the 19-qubit Rigetti Acorn chips
 , where the gain in substantial gate
ﬁdelity relative to unprotected, free evolution of individual transmon qubits was demonstrated. One may combine DD and pulse shaping technique to obtain dynamically corrected gates composing of shaped pulses which actively drive state evolution within a Hilbert space in order
to cancel certain system-bath couplings. With the availability of Qiskit Pulse that allows
users to control backend pulse shapes and sequences of a
quantum processor on the ﬂy, a recent study , based on the Qiskit Pulse and robust control techniques, demonstrates enhancement up to: ∼10×
single-qubit gate coherent-error reduction; ∼5× average
coherent-error reduction; ∼10× increase in calibration
window to one week of valid pulse calibration; ∼12× reduction gate-error variability across qubits and over time;
and up to ∼9× reduction in single-qubit gate error (including crosstalk). The improvements rendered by have implications on the performance
of multiqubit gates in trapped ions .
In light of these recent developments, together with
IBM Qiskit Pulse , we envisage
a possibility to realize/encode holonomic quantum gates
 which
are robust against parameter ﬂuctuations and attain even
better gate ﬁdelity and performance.
Lanczos-inspired
(Suchsland
expectation
observable
Tr[ˆρ(0) ˆA]
constructing
{∣Ψ⟩,H ∣Ψ⟩,H2 ∣Ψ⟩,...,Hm ∣Ψ⟩}.
A way to look at
this is to systematically construct the objective function
to be minimised. For the order-m, the objective function
EL,k,n,m = min
À⟨Ψ∣Hk(∑m−1
i=0 aiHi)n ∣Ψ⟩
i=0 aiHi)n ∣Ψ⟩
Due to the Krylov expansion, this technique can reduce
the impact of diﬀerent sources of noise with cost of an increase in the number of measurements to be performed,
without additional experimental overhead. Calculating
dynamic quantities such as Hamiltonian moments and quantum power method based on
higher-order Suzuki-Trotter expansion on near-term quantum computers are two recent
examples that fall under the same approach here.
Learning-based and AI-inspired methods
employ machine
learning techniques such as regression for error mitigation.
The process consists of training diﬀerent candidate circuit variants with non-Cliﬀord gates substituted
with gates with eﬃcient classical simulability .
A recent approach
suggests merging zero noise extrapolation with learningbased methods for near-Cliﬀord circuits . There are also genetic algorithms to mitigate errors in quantum simulations .
B. Circuit compilation
As it will be discussed in Sec. V.C, a quantum computer is composed of its hardware (quantum) and software (classical). The software translates a quantum algorithm into a set of instructions that implement the desired quantum operations and read out the qubit states.
This process can be understood as quantum compilation
 , but the term is not limited to this
particular application. When mapping a quantum circuit
to a speciﬁc device architecture, one needs to consider the
available quantum gates, the qubit connectivity that allows two-qubit gates implementation, and experimental
limitations such as decoherence time, which imposes a
certain maximum circuit depth in terms of the number
For these reasons, it has become indispensable to develop tools that allow for circuit simpliﬁcations
and eﬃcient mappings of the general algorithm to speciﬁc hardware. These tools are also known as quantum
compilers since they translate the theoretical circuit to
the realistic simulator or device. In the following lines,
we describe some of these tools. Many of them are suited
both for NISQ and fault-tolerant quantum computation.
1. Native and universal gate sets
The available gates that can be implemented experimentally on a particular hardware platform are sometimes referred to as the native gate set.
With a universal gate set G ∈SU(d) (also called instruction gate
set), any unitary operation can be constructed eﬃciently.
More formally, the Solovay-Kitaev theorem states that given this universal set G, any
unitary operation U ∈SU(d) can be approximated with
ϵ accuracy with a ﬁnite sequence S of gates from G. This
sequence scales logarithmically as O(logc(1/ϵ)), where c
is a constant that depends on the theorem proof. For
d = 2n this theorem guarantees that qubit quantum circuits can be decomposed using a ﬁnite gate sequence.
Although this is one of the most important theorems in
quantum computation, it is an existence theorem, i.e. it
does not provide the decomposition that it predicts. It
also requires that the gate set contains the inverse of all
gates. Further developments presented in tried to remove this assumption.
The Cliﬀord group is an important object in quantum information science because of its applications in
quantum error correction, randomized benchmarking and
investigations for quantum advantage. The generalized
Pauli operators in prime dimension p are given by
(a,b) ∈Zp × Zp,p ≠2
(a,b) ∈Z2 × Z2,p = 2
where ω = exp( 2πi
p ) and Zp denotes an integer modulo
p. The Z and X operators are deﬁned via their action
on computational basis states {∣k⟩}k, with X∣k⟩= ∣k + 1
mod p⟩, and Z∣k⟩= ωk∣k⟩. The unitaries which map the
set of generalized Pauli operators to themselves up to a
phase are called Cliﬀord unitaries. Let us denote the set
of p dimensional Cliﬀord unitaries by Cp. Mathematically
U ∈Cp ⇐⇒∃φ ∶UT(a1,b1)U † = exp(iφ)T(a2,b2)
where T(a1,b1) and T(a2,b2) are generalized Pauli operators. The set of Cliﬀord unitaries Cp forms a group, called
the Cliﬀord group. In this review, we focus on p = 2, i.e.
the qubit Cliﬀord group.
There are inﬁnitely many universal gate sets, but the
Cliﬀord group’s gates H, S and CNOT, together with
the T gate, compose the most commonly used set. The
Cliﬀord group alone can be simulated eﬃciently as stated
by the Gottesman–Knill theorem . The theorem states that no quantum advantage can be found without the use of the T gate. For this
reason, many algorithms try to simplify and reduce quantum circuits to the minimal number of T gates, giving
an estimation of the classical eﬃciency of that particular
circuit .
Besides these minimal reduction algorithms, other basic decompositions are useful. Even if only a native gate
set is available experimentally, other basic gates can be
constructed and used in algorithms. As an example, S
and T gates are particular cases of the single-qubit rotational gate Rz, and the H gate can be obtained from
Ry and Rx gates as H = Ry(−π/2)Rx(π). Any singlequbit gate can be decomposed into the gate sequence
U(θ,φ,λ) = Rz(φ)Ry(θ)Rz(λ).
This motivates using
single-qubit rotational gates and at least one entangling
gate (e.g. CNOT or CZ gate) as native gate sets. Any
two-qubit gate can be obtained from this minimal set
by using circuit decompositions . The particular choice of the entangling gate can be motivated from
the experimental platform. Depending on the technology
used to construct the quantum device, a natural 2-qubit
gate implementation can be more suited. Some examples are the use of CZ gates in tunable superconducting circuits , cross-resonance gates
in ﬁxed frequency superconducting qubits , or the XX gates in
trapped ions . More expressive gate
sets with continuous gate parameters or long-range interactions can be achieved by further control over the hardware parameters in time .
The complexity of the circuit decomposition into CNOT
and Rz gates is analyzed in .
2. Circuit decompositions
Once the native gate set is ﬁxed, the next step consists of decomposing the theoretical unitary circuit into
this basic set. A raw translation of all single and twoqubit gates into the native set might imply a large circuit depth, reducing the eﬀectiveness of that decomposition. Moreover, ﬁnding the decomposition of gates acting
on more than one qubit might prove challenging in general. Besides common circuit decompositions mentioned
before, one may need mathematical tools to understand
and derive general circuit reductions to particular smaller
One of these mathematical tools is the so-called ZX-
Calculus. It is a graphical language that maps quantum
circuits to particular graph representations and derives a
set of rules to manipulate these graphs. Its application
range goes from measurement-based quantum computation to quantum error correction. For a complete review
about ZX-calculus and its variety of applications, see Ref.
 . For the purpose of this review,
we are interested in the quantum circuit simpliﬁcation
applications .
Other approaches use well-known artiﬁcial intelligence
algorithms to ﬁnd optimal circuit decompositions, for instance, the use of reinforcement learning . Evolutionary
algorithms such as genetic algorithms have been widely
studied .
approaches, multiple random circuits composed by the
native gate set are generated and evolved later on. The
evolution strategy includes the deﬁnition of possible mutations such as introducing a new gate on a particular
qubit, the swap between circuit gates or the deletion of
a particular gate. Then, a multi-objective loss function
is used to estimate the success of each circuit family until a given convergence criterion, after which the circuit
with the best performance is selected. These works have
to be added to those focusing ﬁnding the optimal PQC
for a given VQA, as discussed in Sec. II.B. A VQA for
circuit compilation using a genetic algorithm as optimization subroutine is presented in Ref. ,
called Quantum Assisted Quantum Compiler.
3. The qubit mapping problem
After decomposing and simplifying the quantum circuit into the native gates, a hardware-speciﬁc task remains: mapping the resultant circuit to the particular
qubit connectivity or topology, a task also known as the
qubit routing problem. In general, due to experimental
limitations, not all qubits are connected, which means
that two-qubit operations are not always possible.
naive approach to circumvent this limitation consist of
swapping each qubit state with its neighbour (by using
SWAP gates) until we ﬁnd a connected pair, perform the
desired two-qubit operation and swap back the states of
the qubits involved, returning to the original state with
the intended two-qubit gate applied to it. This translates
into a signiﬁcant growth of the circuit depth for circuit
topologies with a sparse qubit connection graph.
Some NISQ algorithms presented in this review may
include the qubits’ connectivity by means of the loss
function or the available rules used to decompose the
unitaries. However, quantum compilation is a hardwarespeciﬁc transformation; it might be more useful to apply this step independently of the quantum circuit and
depending on the chip architecture. Unfortunately, the
qubit mapping problem is NP-complete as well as using reinforcement learning . Exact methodologies based on reasoning engines
such as Boolean satisﬁability solvers have also been proposed .
so-called LHZ architecture is an approach that solves the
connectivity issue at the cost of increasing the number
of qubits . The same framework can be
applied to a quantum annealing system as well . Encoding this problem into a QUBO (see
Sec. III.A) to solve it using classical simulated annealing has also been proposed in Ref. . Approaches for circuit compilation based on
commutation algebra of quantum gates have been been
suggested in .
There can be many possible qubit mappings of a given
algorithm into a particular device if not all qubits are
required. In those cases, one can put some extra eﬀort
to ﬁnd the best performing qubits in terms of error rates
and coherence times .
In that direction, ﬁnding the mapping with the lowest
circuit depth may prove valuable to reduce the errors
due to decoherence .
Finally, the use of circuit synthesis with connectivity
constraints have also been proposed. Some of these works
are based on Gaussian elimination processes where you
take the matrix representation of the circuit transformation and manipulate it to extract the basic transformations (in particular, the CNOT gates that respect the
connectivity) , the strategy consist on slicing the circuit into smaller parts that can be
adapted and transformed to ﬁt into the particular topology. One can also adapt this problem to the syndrome
decoding problem . Reference
 solves the qubit routing
for phase polynomial circuits, i.e. circuits that only contain CNOTs and Rz gates.
4. Resource-aware circuit design
As described in Sec. II.B, there are diﬀerent strategies
to design a circuit ansatz. Unfortunately, many of them
require circuit depths, qubit connectivity and a number
of parameters beyond the capabilities of current quantum
hardware. In the following paragraphs, we discuss strategies to design and adapt PQC and VQA to the devices
characteristics.
ADAPT-VQE.
Early VQA employed a ﬁxed ansatz
parameters
classical optimizer.
The Adaptive Derivative-Assembled
Pseudo-Trotter ansatz Variational Quantum Eigensolver
(ADAPT-VQE) was introduced as a more scalable and
eﬃcient way to simultaneously design and optimize a parameterized ansatz . At each iteration, ADAPT-VQE constructs an ansatz by adding
an operator corresponding to the largest gradient from a
carefully designed operator pool. That is, given an operator ˆτi from the operator pool, the gradient of the energy
with respect to the corresponding parameter θi is deﬁned
∂iE = ⟨ψ∣[H, ˆτi]∣ψ⟩,
where ∣ψ⟩is the ansatz at the current iteration to be
updated. After computing the gradient components and
choosing the operator corresponding to the largest gradient, the gate operation implementing ˆτi is added to
the ansatz with its parameter value initialized at 0. The
ansatz is then optimized before adding the next operator.
The ADAPT-VQE algorithm terminates when the norm
of the gradient vector falls below a predeﬁned threshold.
In the case of fermionic ADAPT-VQE, the operator
pool consists of fermionic operators that are transformed
into quantum gate operations through, e.g., the Jordan-
Wigner mapping. A more hardware-eﬃcient variant of
the ADAPT-VQE algorithm is the qubit ADAPT-VQE,
in which the pool consists of gate operators acting directly on qubits .
Both versions of
ADAPT-VQEs were able to to generate optimized circuits with reduced depths and CNOT counts compared
to previous ansatz construction and optimization methods.
MI-ADAPT-VQE.
information-assisted
(MI-ADAPT-VQE),
introduced
 , leverages the density matrix
renormalization group (DMRG) method to accelerate the circuit constructions for
the ADAPT-VQE routine. Instead of gradients, it uses
the mutual information to guide circuit constructions.
At the beginning of the algorithm, the pair-wise quantum mutual information is approximated using DMRG,
which is then applied to construct a reduced pool of
entangling gates.
In each iteration of the method,
new circuits are generated in which quantum gates are
mainly distributed among pairs of qubits corresponding
to large mutual information.
This avoids allocating
quantum resources on pairs of qubits that are less
important to entangle. Numerical experiments suggest
that the number of new circuits needed in each step of
the adaptive construction can be signiﬁcantly reduced
using MI-ADAPT-VQE, saving both time and quantum
resources. The number of trial circuits in certain cases
can be reduced to about 5% for H2 and 10% for H2O
as compared to ADAPT-VQE, using an operation pool
based on the qubit coupled-cluster method .
To reduce two-qubit gate counts for nearterm experiments, the multiobjective genetic variational
quantum eigensolver (MoG-VQE) optimizes for both the
energy and the number of CNOTs in the quantum circuit . The MoG-VQE algorithm
combines two evolutionary strategies: i) NSGA-II , a multiobjective genetic algorithm, to propose a circuit structure to minimize both the energy and
CNOT count, and ii) CMA-ES to
tune parameters and evaluate optimized energies for the
qubit topologies suggested by the NSGA-II algorithm.
MoG-VQE initializes a diverse population by sampling a
checkerboard pattern of two-qubit circuit blocks. To vary
the populations over diﬀerent generations, the three possible mutation operators are: i) inserting a two-qubit circuit block in a random position; ii) removing a two-qubit
circuit block in a random position; and iii) adding or removing 10 circuit blocks to help escape from local minima. The authors note that iii) is selected with a lower
probability than mutation operators i) and ii). Parents
are selected using the tournament selection method. For
each circuit topology, the corresponding energy is evaluated using the CMA-ES optimizer.
These steps repeat until some termination criteria are satisﬁed.
Using MoG-VQE, the authors reported signiﬁcant reductions in the CNOT counts compared to those of other
hardware-eﬃcient circuits when estimating ground state
energies of several molecules. For example, for a 12-qubit
LiH Hamiltonian, MoG-VQE generated a circuit corresponding to estimating the ground state energy to within
chemical precision using only 12 (non-nearest-neighbor)
An alternative approach for adaptively constructing and optimizing an ansatz was introduced by the
Parameter-eﬃcient circuit training (PECT) scheme . PECT enables optimizations of predeﬁned
ansätze, such as unitary coupled-cluster or the low-depth
circuit ansatz (LDCA) ,
by dynamically pruning and adding back parameterized
gates during an optimization. After selecting an ansatz
U, a subset of gate operations from A is chosen while
other parameterized gate operations are tuned to identity operations. This results in an ansatz substructure
A′ with reduced circuit depth and gate count. Parameters of A′ are then optimized, following what the authors
call a “local optimization” step. After local optimization,
to reﬁne or reparameterize the ansatz substructure, parameters with small magnitudes are pruned or removed.
A heuristic growth rule is used to grow back the same
number of parameters that was pruned. Steps of local
optimization and ansatz reparameterization are repeated
until termination criteria are met. Because PECT optimizes parameter subsets at any iteration, circuits that are
executed on the quantum computer have reduced depths
and CNOT counts compared to the original ansatz. Using PECT, the authors were able to optimize 12-qubit
LDCA circuits, naively equipped with hundreds to low
thousands of parameters, to estimate ground state energies of LiH and H2O. Previous optimizations of LDCA
were limited to 8 qubits.
C. Quantum software tools
A quantum computer is a hybrid device composed of
quantum hardware and classical software that controls
it by sending a list of instructions and processing the
results of the computation.
This hybrid nature is accentuated in the NISQ era, as explained in the current
review. Thus, the classical subroutines are part of the
core in state-of-the-art NISQ algorithms and a language
to communicate with the quantum device is a bare necessity. On top of that, almost all progress in quantum
algorithms is tested in quantum simulators making them
essential to perform proof-of-concept simulations, before
or until the algorithm is applicable on real devices.
Figure 6 represents diagrammatically the typical work-
ﬂow of a NISQ algorithm. The individual parts of the
problem, such as the objective function to optimize, the
quantum circuit design or the initialization parameters,
are translated into quantum circuits by a classical precomputation step. The syntax of this language includes
the quantum gates, qubit initialization, objective function deﬁnition, etc. The theoretical circuit is then compiled to fulﬁll experimental limitations such as qubit connectivity, native quantum gate set or circuit depth. To
accomplish this task, compilers that allow for circuit simpliﬁcation (see Sec. V.B), or noise models (for simulation
purposes) might be useful. After this pre-processing step,
the algorithm is ready to enter into the quantum-classical
loop. The quantum circuit can be run in a quantum simulator or real hardware. In the latter case, an assembly
language will translate the
quantum circuit into a set of instructions for the device.
After the qubits are measured, the result can be postprocessed and techniques such as error mitigation might
be used. Either the algorithm ﬁnishes or the result is
sent to the classical optimizer that computes the next
loop variables (e.g. for VQA).
We deﬁne a quantum software library as a library or
a set of libraries written in a classical programming language (e.g. python or C++) that allows writing a quantum program. In some cases, these libraries are opensource and can be used directly on real hardware or on
a quantum simulator. The proliferation of all these libraries, simulators and devices has also created a necessity for some multi-platform languages. These are those
that can use multiple quantum software libraries as a
backend, reducing the programming eﬀorts substantially
Libraries for
chemistry, ML, etc.
Problem definition:
cost function, circuit
ansatz, initialization
Settings for compiler,
error mitigation, noise model
instructions
postprocessing
Optimization
Figure 6 Schematic representation of a standard NISQ programming workﬂow (color online). Green circular boxes represent
the libraries and languages used for designing, optimizing and running a quantum algorithm in a real quantum device or in a
simulator. External libraries can be used to deﬁne the problem or to improve the performance of the algorithm by simplifying
the circuit or using error-mitigation techniques. An assembly language will be needed to translate the theoretical algorithm to
a set of physical operations on the quantum hardware. Classical post-processing is necessary to manipulate the result of the
computation and to either obtain the ﬁnal result or send the provisional one to a classical optimizer (VQA).
by unifying the language syntax. Some of these packages
include built-in sub-libraries suited for particular applications, from chemistry to QML, or particular well-known
algorithms such as VQE or QAOA.
We provide a list of some open-source libraries suited
for NISQ computation in Tab. VII from App. A. This list
represents just a snapshot of the state-of-the-art of the
quantum software ecosystem as new tools are being developed and some projects are being abandoned. An updated list of quantum software resources can be found in
Ref. 
and a detailed comparison analysis between some of these
languages and libraries in Ref.
 . Due
to the broad applications of NISQ algorithms, speciﬁc
libraries used in other ﬁelds beyond quantum computation can also be required, e.g. quantum chemistry and
machine learning libraries or external compilers and simulators. These libraries are used for other applications
besides purely quantum computation, so we consider and
list them as external libraries in Tab. VIII from App. A,
although most of them integrated in the quantum software libraries.
VI. APPLICATIONS
A. Many-body physics and chemistry
Understanding the static and dynamic properties of
quantum mechanical systems is a core challenge at the
heart of many ﬁelds such as chemistry and physics. Classical numerical methods often struggle in solving these
problems, due to the exponential increase of resources
needed with growing number of particles to simulate.
Owing to their quantum mechanical nature, quantum
computers oﬀer a way to simulate even large-scale manybody systems .
The initial application for chemistry was to obtain molecular energies via quantum phase estimation on a quantum computer . Besides the
molecular energy, properties that can be extracted from a
successfully prepared ground state, such as energy derivatives with respect to the nuclear framework, are of similar interest
 .
Fault tolerant quantum algorithms have
the potential to become killer applications in the computational discovery of chemical reaction mechanisms and NISQ algorithms could play a major
role in their realization. Here we review various NISQ
algorithms that have been proposed to tackle quantum
chemistry and many-body physics related problems. We
start by introducing concepts on mapping physical problems onto the quantum computer. Then, we introduce
algorithms for common challenges, such as ﬁnding the
static as well as dynamic properties of quantum systems
in various settings. All NISQ algorithms discussed in this
section are listed in Table I.
1. Qubit encodings
In general, any physical system can be written in terms
of a Hamiltonian which is the sum of its kinetic and potential energy. In quantum theory, each physical system
is associated with a language of operators and an algebra establishing such language. Depending on the system
constituents, there are three types of particles (operators) in Nature: fermions, bosons and anyons. The ﬁrst
two are elementary particles obeying Fermi-Dirac (FD)
and Bose-Einstein (BE) statistics, respectively. The latter being quasiparticles obeying continuous or anyonic
statistics, and existing only in two-dimensional conﬁnement. Quantum computers (QC) operate in a language
of qubits (a distinguishable set of spin-1/2 particles).
Hence, the quantum simulation of a physical system
refers to performing a one-to-one mapping from the system operator to the QC language, preserving the underlying statistics. For a recent review on hardware-dependent
mappings of spin Hamiltonians into their corresponding
quantum circuit, refer to .
In the standard model of quantum computation, a twolevel system or spin-1/2 particle is denoted by its spin
orientation ∣↑⟩= ∣0⟩= (1,0)T and ∣↓⟩= ∣1⟩= (0,1)T .
An N-qubit system is then constructed from the standard Pauli matrices ˆσi
z, where the superscript i
refers to the ith local qubit site.
These operators satisfy the commutation relations of an ⊕N
i=1 su(2)i algebra
ν ] = 2iδlmϵµνλˆσl
λ, where ϵµνλ is the totally antisymmetric Levi-Civita symbol with µ,ν,λ ∈{x,y,z}.
In the second quantized notation,
fermions are denoted by fermionic operators ˆf †
i ( ˆfi), the
creation (annihilation) operators of a fermion in the
ith mode/site (i = 1,⋯,N). The fermionic operators
obey Pauli’s exclusion principle and the anti-symmetric
nature of the fermion wave function.
fermionic algebra is deﬁned by the anti-commutators
{ ˆfi, ˆfj} = 0,{ ˆf †
i , ˆfj} = δij. There are a number of
well-known mappings that allow the description of a
fermionic system by the standard model of QC. They are
the Jordan-Wigner transformation , Bravyi-Kitaev transformation and Ball-Verstraete-Cirac transformation
 . In , the two-dimensional topology of
most proposed qubit architectures is taken explicitly into
account and compared to some of the aforementioned
one-dimensional mappings.
More advanced mappings,
using the interaction graph of the Hamiltonian or customized
quasi-local and local encodings have been introduced as well.
Other approaches try to lower the qubit requirements of
the mapped fermionic operators by taking inspiration
from classical error correction codes and the internal
symmetries of the system , other examples are mappings based
on point-group symmetries of molecular Hamiltonians . Recently, mappings of SU(N)
fermions to qubits have been proposed (Consiglio et al.,
In the following, we will brieﬂy outline the oldest and
most intuitive mapping:
the Jordan-Wigner transformation.
In this mapping, the qubit states are equivalent to the second-quantized occupation number vectors, and fermionic creation and annihilation operators
are transformed to qubit raising and lowering operators
y)/2 combined with strings of ˆσz operators
that ensure the correct anti-commutation properties:
In this new transformation, one can verify that ˆf †
j , ˆfj satisfy the above anticommutation relations, while ˆσj
the commutation relations showed above. The reader is
referred to the literature and the respective original references for details
and comparisons regarding the other transformations.
Bosonic operators satisfy the commutation relations [ˆ˜bi,ˆ˜bj] = 0,[ˆ˜bi,ˆ˜b†
j] = δij in an inﬁnite-dimensional
Hilbert space. At ﬁrst, it seems it is impossible to simulate bosonic systems due to the nature of inﬁnite dimensions. However, sometimes we are interested in studying
some ﬁnite modes of excitations above the ground state.
Hence, the use of the entire inﬁnite dimensional Hilbert
space is unnecessary. In a ﬁnite dimensional basis, the
bosons ˆb†
i,ˆbi obey the following commutation relations
 
[ˆbi,ˆbj] = 0, [ˆbi,ˆb†
j] = δij [1 −Nb + 1
i)Nb(ˆbi)Nb], (67)
iˆbi ∣ni⟩= ni ∣ni⟩with ni = 0,⋯,Nb, where Nb is
the maximum truncated excitation number, corresponding to the ith bosonic site/mode. A direct consequence
is one can then write down the creation and annihilation
operators as
n + 1∣n + 1⟩⟨n∣,
and ˆbi is complex conjugate of ˆb†
There are inﬁnite
means to translate such truncated operators into the
QC language, the so-called Pauli words. A Commonly
used one is known as standard binary or compact encoding ,
where {α,β ∈W} in ∣α⟩⟨β∣are now written in terms of
binary strings.
Using the following identities: ∣0⟩⟨1∣≡
ˆσ−;∣1⟩⟨0∣≡ˆσ+;∣0⟩⟨0∣≡(I + ˆσz)/2;∣1⟩⟨1∣≡(I −ˆσz)/2, Pauli
words translation can be accomplished.
Recently, detailed studies on various encodings (binary, Gray, Unary,
block Unary), have been studied and Gray code in particular is found to be resource eﬃcient (in terms of number of qubits and two-qubit entangling gates) in simulating some speciﬁc bosonic and spin Hamiltonians .
As seen above, we can now proceed to simulate more general particle statistics, in particular, hardcore anyons. With “hard-core”, we refer to the Pauli’s
exclusion principle where only zero or one particle can
occupy a single mode.
The anyonic operators ˆai, ˆa†
obey the commutation relations [ˆai, ˆaj]θ = [ˆa†
j]−θ = δij(1 −(e−iθ+1)ˆnj) and [ˆni, ˆa†
j] = δijˆa†
jˆaj,[ ˆA, ˆB]θ =
ˆA ˆB −eiθ ˆB ˆA, with (i ≤j) and
0 ≤θ < 2π.
Speciﬁcally, θ = π mod(2π) gives rise to
canonical fermions, and θ = 0 mod(2π) would recover
hard-core bosons. By simply applying the following isomorphic mapping between algebras :
ˆnj = 1 + ˆσj
we would obtain Pauli words for the QC. The above mapping would also ensure the anyonic algebra shown above.
2. Constructing electronic Hamiltonians
The electronic structure problem is one of the most
prominent task within VQA ) and was the
pioneering task for the variational quantum eigensolver
 . In this section, we will illustrate how the original continuous manyelectron problem can be discretized to a second-quantized
formulation that can itself be encoded into qubits by the
techniques introduced in the beginning of Sec. VI.A. This
encoded qubit systems deﬁne then the central problem of
the VQAs further described in Sec. VI.A.3.
The electronic structure problem aims to approximate
eigenfunctions of an electronic Hamiltonians
He = Te + Vee + Vext,
describing a system of Ne electrons through their accumulated kinetic energies Te = −1
k=1 ∆rk, the electronic
Coulomb repulsion Vee = ∑k≠l Vee (rk −rl) = ∑k≠l
and an external potential Vext = ∑Ne
k=1 Vext (rk) that is
usually given by the accumulated Coulomb potential of
nuclear point charges. If the external potential is not explicitly spin dependent, the electronic Hamiltonian only
acts on the spatial coordinates rk ∈R3 of the electrons and, to ensure proper electronic wave functions, the
fermionic anti-symmetry is achieved via restrictions in
the Hilbert-space. We refer to and the textbook for the direct construction and discretization of
this continous Hilbert spaces.
A more compact, but formally equivalent, deﬁnition is
oﬀered through second quantization by introducing the
abstract anti-commuting ﬁeld operators ˆψ† (x) and ˆψ (x)
that create and annihilate electrons at spin-coordinate
xk ∈R3 ×{± 1
2} . The electronic Hamiltonian can then be
written as
He =∫dx ˆψ† (x)(T (x) + Vext (x)) ˆψ (x)
+ ∫dxdy ˆψ† (x) ˆψ† (y)Vee (x −y) ˆψ (y) ˆψ (x)
where the potential operators still only act on the spatial part of the spin components. Although direct approaches on real-space grids are possible the majority of VQA employs
a ﬁxed set of three dimensional functions (so-called orbitals) to capture the spatial part of the electronic Hilbert
space. The orbitals are usually determined by solving a
mean-ﬁeld problem (Hartree–Fock) within a set of globally deﬁned atomic orbitals.
Alternatives to the standard representation are, for example, direct determination of system adapted orbitals ,
compactiﬁcation of basis sets through intrinsic atomic orbitals and optimized virtual orbitals
represented by plane-waves .
For the formal description of the discretized secondquantized electronic Hamiltonian, the origin of the orbitals is not important as long as they form an orthonormal set of H1 (R3) functions. Using such a set of spatial
orbitals we can formally expand the ﬁeld operators in the
corresponding spin-orbitals
ˆψ† (x) = ∑
ˆψ (x) = ∑
k and fk are fermionic creation and annihilation operators obeying the anticommutation relations
shown in the previous subsection. Using the expansion
from Eq. (71) leads to the common discretized secondquantized Hamiltonian,
with the molecular integrals 
k (x)(T (x) + Vext (x))φl (x)dx,
l (y)Vee (x −y)φ∗
n (y)dxdy.
Note that the indices of the two body integrals are denoted in the standard Dirac notation gklmn ≡⟨kl∣Vee ∣mn⟩
but other notations,
such as Mulliken (km∣lm)
⟨kl∣Vee ∣mn⟩are sometimes used.
Generally speaking,
an arbitrary set of spatial orbitals, that can in principle
be any set of orthonormal H1 (R3) functions, deﬁnes a
discretized second-quantized Hamiltonian as in Eq. (73)
over the corresponding molecular integrals Eq. (74). This
discretized Hamiltonian can then be encoded into a qubit
Hamiltonian by corresponding fermion to qubit mappings
discussed in Sec. VI.A.
3. Variational quantum eigensolver
Estimating the ground state and its energy of Hamiltonians is an important problem in physics, which has
numerous applications ranging from solid-state physics
to combinatorial optimization (see Sec. VI.C). While this
problem is in general QMA-hard and even quantum computers are not expected to be able to eﬃciently solve it in
general , there is hope that approximate solutions of the ground state could be found faster
and for larger system sizes compared to what is possible
with classical computers.
To this end, VQE has been proposed, to ﬁnd the ground state of a
Hamiltonian H in a manner that is suited for NISQ devices . Following the concept introduced in Sec. II.A and Sec. II.B, a parameterized circuit
U(θ) is minimized with respect to the objective function,
which in general is the expectation value of the energy of
the Hamiltonian from Eq. (1).
The approximated ground state is given by the quantum state ∣ψmin⟩= U(θmin)∣0⟩which minimizes the energy minθ⟨Hθ⟩≥Eg upper bounded by the true ground
state energy Eg as guaranteed by the Rayleigh-Ritz variational principle .
VQE has been intensively studied in both theory and experiments, and various adaptions and extensions have been proposed, which
we discuss in the following paragraphs.
Self-veriﬁcation.
Whether the variational quantum simulator has converged to an actual eigenstate of the Hamiltonian, can be checked directly on the quantum processor by verifying that the variance of the energy var =
⟨(H −⟨H⟩)2⟩is zero. This has been demonstrated for
solving a many-body Hamiltonian on 8 qubits on a iontrap (see also Sec. VI.E.6).
Accelerated VQE.
A key computational eﬀort in VQE
lies in estimating the cost function, which is achieved by
repeatedly running the circuit and taking measurements
of the Pauli strings (see Sec. II.C). For a given desired
additive error bounded by ϵ, it takes O(1/ϵ2) number of
samples. This can be improved by using the Quantum
Phase estimation algorithm to estimate the expectation
value, which takes only O(log(1/ϵ)) samples, however at
the cost of additional computation which may be hard in
the NISQ era. To leverage a trade-oﬀbetween the advantages and disadvantages of both methods, an accelerated versions of VQE that interpolates between regular
measurements and quantum phase estimation has been
proposed .
Measurement-based VQE.
In , the
authors present two strategies to implement the VQE algorithm on a measurement-based quantum computer, an
alternative quantum computing paradigm that uses entanglement as a resource and achieves the the desired
computation by performing particular sets of local measurements for a review). They
propose a way to generate the needed variational state
families using measurements on a highly entangled state
and provide an equivalence between the measurementand gate-based schemes.
Reusing qubits in VQE.
A recent proposal suggested a
VQE method that relies on fewer qubits by re-using some
of them . The core idea is to represent
a virtual N qubit state by R + V < N physical qubits,
where R qubits have to be reusable qubits, e.g. they can
be measured and re-initialized during the circuit runtime.
These intermittent measurements are possible on current
ion trap hardware . The R + V qubits
are entangled by a PQC, then R qubits are measured and
the outcome is recorded. The R qubits are re-initialized
to the ∣0⟩⊗R state, and again entangled with the V other
qubits by another PQC. This procedure is repeated until in total N qubits have been measured. The concept
and expressiveness of this type of ansatz is the same as
Tensor networks methods such as MPS, which have been
highly useful for the classical calculation of many-body
problems, and open up a way to perform quantum computing of many qubits on devices with limited number of
Adiabatically assisted VQE.
The ground state of more
challenging Hamiltonians can be diﬃcult to ﬁnd for standard VQE due to convergence to local minima instead of
the global minima of the energy. To alleviate this, quantum annealing (see Sec. III.A) can be used to adiabatically assist the optimisation procedure, as proposed in
the adiabatically assisted VQE . This approach uses an objective function O(s) =
⟨0∣U †(θ)H(s)U(θ)∣0⟩, where H(s) = (1 −s)H0 + sH1.
Here, H0 is a Hamiltonian with easily preparable groundstate and the goal is to ﬁnd the ground state of a Hamiltonian H1. In this algorithm, VQE is run for multiple
discrete steps sn. One starts with s0 = 0 and ﬁnds the
minimal parameters θ∗
0 of the objective function O(s0).
0 is used as initial guess for VQE for the next increasing step s1 = s0 + ∆s with objective function O(s1).
This procedure is repeated until s = 1 is reached. This approach eases the optimization task, as the initial Hamiltonian H0 is a simple Hamiltonian with a ground state that
can be easily found via optimization. For small steps ∆s,
the ground state of the Hamiltonian H(s) and H(s+∆s)
will not diﬀer too much, making the optimization task at
every step less challenging compared to directly solving
for H(1). Previous works also suggest to use adiabatically prepared states as initial states
of a VQE algorithm (see Sec. II.B).
4. Variational quantum eigensolver for excited states
The methods of VQE have been extended to obtain the
excited states of a given Hamiltonian. Finding excited
states or the spectrum of a Hamiltonian is an important
problem in quantum chemistry and many-body physics.
Various proposals have been put forward.
Folded spectrum method.
A straightforward way of calculating excited states is the folded spectrum method
proposed by . To ﬁnd an excited
state of a Hamiltonian H with approximate energy λ,
the above deﬁned VQE method is here applied to the
objective function C(θ) = ⟨(H −λ)2⟩U(θ). VQE will target the eigenstate with an energy that is closest to λ.
This method requires an approximate knowledge of the
energy of the excited state that one wants to ﬁnd, as well
as estimating ⟨H2⟩, which may require a excessively large
number of measurements to be performed.
An extension of this method can also be used to ﬁnd
states that are constrained to a speciﬁc value of the
conserved quantity of the problem, such as total particle number or magnetization .
Here, one deﬁnes the objective function C(θ) = ⟨H⟩U(θ)+
∑i µi(⟨Si⟩U(θ)−si)2, where Si is the operator corresponding to the conserved quantity, and si is the target value
of that quantity. Note, that this does not restrict the
target space to be an eigenstate of Si.
Orthogonally constrained VQE.
Excited states can be
found by constraining the VQE objective function such
that it penalizes the ground state .
First, one ﬁnds an approximation to the ground state
of Hamiltonian H via VQE with θ0 = arg minθ⟨H⟩U(θ)
and approximated ground state ∣ψ(θ0)⟩= U(θ0)∣0⟩.
Then, one uses this information to formally project out
the approximate ground state to ﬁnd the next highest excited state.
One deﬁnes the Hamiltonian H1 =
H + a∣ψ(θ0)⟩⟨ψ(θ0)∣, with some suﬃciently large positive parameter a. The ground state of H1 then corresponds to the ﬁrst excited state of H and can be found
with a VQE. This procedure can be repeated to ﬁnd
higher excited states up to any order by sequentially accumulating the projector terms of all states found. The
Hamiltonian for the k-th excited state is then given by
Hk = H+∑k−1
ai ∣ψ(θi)⟩⟨ψ(θi)∣. Combined with the unitary coupled cluster ansatz, the orthogonally constrained
VQE can ﬁnd excited states of small molecules . It was further extended for
adaptive circuit construction 
and imaginary time evolution .
The projector term requires calculating the overlap
∣⟨ψ(θ)∣ψ(θ0)⟩∣, which can be achieved for example by
the SWAP test, by applying the inverse of the circuit
that generated the ground state ∣⟨0∣U †(θ)U(θ0)∣0⟩∣2, or
randomized measurements . An alternative approach that relies on a discriminator circuit
that is trained in parallel to distinguish between the excited state to be learned and previously found lower-lying
states has been proposed and demonstrated on a small model system. Scalable proposals still
remain an open research question. Since the projector
term does not require the overlap itself, but the absolute
square of it, it can be computed with the help of Eq. (5)
by computing the ﬁdelity of the current trial state with
the previously found states .
Subspace expansion.
The subspace expansion method
was introduced in Sec. V.A.3 for error mitigation. This
method can be also used to ﬁnd excited states and it was demonstrated for a small molecule
in . After ﬁnding the ground state of
a Hamiltonian H with VQE, one follows the steps that
were detailed in Sec. V.A.3. One expands the prepared
quantum state with diﬀerent appropriate operators that
match the low-energy excitations of H and generates a
set of states that span the low-energy subspace. Then,
overlaps between the states are measured, which are then
used to solve a generalized eigenvalue problem on a classical computer. The eigenvalues and eigenstates give the
excited states of the Hamiltonian. For quantum chemistry problems, the subspace expansion method was also
proposed for including dynamical correlations to ground
states over external corrections ,
in the spirit of classical quantum chemistry methods, like
for example CAS-CI .
As alternative approach, the expansion in the subspace
can also be accomplished by real-time evolving a reference state, and picking states at diﬀerent evolution times
as basis for expansion . This is motivated by the fact that the time evolution can be seen as
an approximate Krylov expansion of the quantum state.
Then, one proceeds to solve the generalized eigenvalue
problem to ﬁnd eigenstates and eigenvalues of the Hamiltonian.
Subspace-search VQE/State-averaged VQE.
The core idea
of a subspace-search VQE (SSVQE) or state-averaged VQE (SAVQE) is to minimize the energy of a
PQC U(θ) over a set of orthogonal quantum states. The
goal is to ﬁnd the k-th eigenstates with lowest eigenenergy of a Hamiltonian H. In the weighted SSVQE the
cost function is
wj ⟨ϕj∣U †(θ)HU(θ)∣ϕj⟩,
where {∣ϕj⟩}k
j=0 is a set of k easily preparable mutually orthogonal quantum states (with ⟨ϕi∣ϕj⟩= δi,j)
and {wj}j are positive real numbers with wi > wj for
Minimizing θ∗= arg minθL(θ) to its global
minimum gives us the ground state and excited states
∣ψj⟩= U(θ)∣ϕj⟩, where j = 1 is the ground state and
j > 1 the excited states sorted in ascending order. This
algorithm gives all k eigenstates in a single optimization
Note however that the more states to be optimized, the more complex the optimization landscape
and the eﬀort to minimize becomes. An alternative formulation of the algorithm to ﬁnd speciﬁcally the k-th
lowest eigenstate is the unweighted SSVQE. Here, one
minimizes L1(θ) = ∑k
j=1 ⟨ϕj∣U †(θ)HU(θ)∣ϕj⟩.
However, due to the absence of weights, the found states
j⟩= U(θ∗)∣ϕj⟩for minimal θ∗are not proper eigenstates of H, but are superposition states that span the
subspace of the k lowest energies. As ﬁnal step to ﬁnd the
k-th eigenstate, one ﬁxes θ = θ∗to its minimized value,
and then maximizes φ∗= maxφL2(φ), with L2(φ) =
j=1 ⟨ϕj∣V †(φ)U †(θ∗)HU(θ∗)V (φ)∣ϕj⟩and V (φ) being a unitary that acts only on the Hilbert space of the
k lowest eigenstates. Then, for the maximized φ∗, the kth lowest eigenstate is given by ∣ψk⟩= U(θ∗)V (φ∗)∣ϕk⟩.
Besides general applications that involve excited states,
state-averaged orbital-optimized VQEs (SA-OO-VQE)
were proposed to treat chemical systems that require a
“democratic description of multiple states” as for example
necessary in the vicinity of conical intersections . Here, “democratic description” corresponds
to treating degenerate or quasi-degenerate states at the
same footing.
Multistate contracted VQE.
This algorithm combines the
non-weighted SSVQE with the subspace expansion to
ﬁnd the ground state and excited states . First, one runs the non-weighted SSVQE routine
to ﬁnd the unitary U(θ∗) to ﬁnd k states that span the
subspace of the k smallest eigenvalues ∣ψ′
j⟩= U(θ∗)∣ϕj⟩.
Then, to ﬁnd the correct eigenstates, one runs the subspace expansion and measures the overlap matrix Hij =
j⟩, and diagonalizes it to ﬁnd estimates of the k
lowest eigenergies and eigenstates.
Fourier transform of evolution.
Recent experiments have
determined the spectra of molecular and many-body
Hamiltonians using superconducting processors .
A particular method to determine the eigenergies of
Hamiltonians via Fourier transforming the dynamics of
observables has been applied in . The idea is to prepare a Fock state
that has overlap with the eigenstates whose eigenvalues
one wants to calculate. The Fock state is then evolved in
time with the Hamiltonian and speciﬁc observables are
measured over a range of time. The Fourier transform
of the time evolution of the observables can be used to
deduce the eigenenergies of the Hamiltonian.
Witness-assisted variational eigenspectra solver (WAVES).
WAVES core idea is to use a single reference qubit as an
eigenstate witness to variationally ﬁnd the ground state
and excited states . A variational
ansatz applied to a reference state is chosen. Then, the
time evolution operator U(t) = exp(−iHt) is evolved on
the ansatz state as a control unitary CU(t), with the
control being the single qubit in a superposition state.
Then, full tomography is performed on the single qubit
to read out its von-Neumann entropy. If the variational
state is an eigenstate of the Hamiltonian H, then the entropy is zero. Further, the energy of the state can be
estimated from the state of the qubit as well. The ansatz
is variationally updated using the information from the
qubit in a iterative fashion until the ground state is found.
Excited states can be found by applying an appropriate
excitation operator on the found ground state, and then
variationally minimizing the von-Neumann entropy of the
qubit. As last step, the authors suggest to use the iterative phase estimation algorithm to further improve the
accuracy of the excited state as well determine its eigenvalue. This method requires to implement a controlled
time evolution operator, similar to non-variational proposals , which are considered to be
challenging for larger systems on NISQ devices.
5. Hamiltonian simulation
A major application for quantum computers is the simulation of the dynamics of Hamiltonians for problems
such as many-body physics and chemistry.
One standard approach for quantum simulation of Hamiltonians
is based on the Trotter-Suzuki expansion from Eq. (14),
where the evolving unitary is split up into small discrete
timesteps of eﬃciently implementable unitaries, which
can be run on the quantum computer.
Naturally, the
depth of the quantum circuit increases polynomially with
the desired time to be evolved and target accuracy, which
may not be feasible on NISQ devices without access to
error correction. The relevant algorithms are reviewed
in the following. We remark that some necessary tools
to simulate many-body interaction Hamiltonian have also been proposed.
Variational quantum simulator.
VQA have been proposed to solve dynamical problems in the NISQ era .
The core idea is to iteratively
update an eﬃciently implementable variational quantum
state ∣ψ(θ)⟩with a new set of parameters θ →θ′ such
that it minimizes the error between the actual time evolution exp(−iHδt)∣ψ(θ)⟩for a timestep δt and the updated
variational state ∣ψ(θ′)⟩. The rules to update the parameters θ to solve the Schrödinger equation id/dt∣ψ(t)⟩=
H ∣ψ(t)⟩can be found by the variational McLachlan’s
principle δ∣∣(d/dt + iH)∣ψ(θ)⟩∣∣= 0 with ∣∣∣ψ⟩∣∣=
and demanding that θ remains real-valued. One ﬁnds a
set of linear equations of motion A ˙θ = C with
Ai,j = Re(∂θi ⟨ψ(θ)∣∂θj ∣ψ(θ)⟩),
Ci = Im(∂θi ⟨ψ(θ)∣H ∣ψ(θ)⟩).
At a given step of the iteration, one needs to measure
the elements of A and C using the Hadamard test or
methods from (see Sec. II.C),
and then update θ with the solution of the linear equation of motion by a small timestep δt. The solver can
be combined with adaptive strategies to reduce the complexity of the Ansatz circuit .
VQS has been applied on the IBM quantum processor
to simulate energy transfer in molecules 
as well as to simulate a time-dependent Hamiltonian . A straightforward extension of the variational quantum simulator can be applied to solve the
Schrödinger equation in imaginary time , for time-dependent problems 
or for general linear diﬀerential equations . Its implementation to open
quantum systems is
discussed in Sec. VI.A.7.
Using the hardware-eﬃcient
structure of the PQC, it is possible to reduce the cost of
measuring the A and C matrices .
Alternatively, the projected - Variational Quantum Dynamics method (p-VQD) has been proposed to bypass
the measurement of aforementioned matrices .
Here, one variationally maximizes the ﬁdelity between the PQC ∣ψ(θ)⟩and
the Trotter evolved state exp(−iHδt)∣ψ(θ′)⟩. The optimized PQC yields the state evolved by a time δt. This
algorithm is then repeated to gain evolution for longer
times. By appropriately choosing the evolution time δt,
barren plateaus can be avoided .
Subspace variational quantum simulator.
The subspace
variational quantum simulator (SVQS) builds upon the idea of the SSVQE introduced earlier in Sec. VI.A.4. The core
idea is to rotate the initial state to be evolved onto the
low-energy subspace found by the weighted SSVQE, then
evolve it in time within the subspace, and then apply
the reverse mapping. First, run the weighted SSVQE by
preparing k initial states {∣ϕj⟩= σx
j=0 which are orthogonal with each other (⟨ϕi∣ϕj⟩= δi,j) and lie in the
computational subspace, as well as a PQC U(θ). Now
as in the weighted SSVQE minimize Eq. (75).
Then, prepare an initial state ∣ψin⟩to be evolved,
which is encoded into the computational subspace by applying the Hermitian conjugate of the obtained circuit
U †(θ). Here, the evolution of the state in time is performed by applying single-qubit rotations on each qubit
T (t) = ⊗j RZ(−Ejt), where {Ej}j are the eigenenergies
of the eigenstates {∣Ej⟩}j obtained by SSVQE earlier.
Finally, the state T (t)U †(θ)∣ψin⟩in the computational
subspace is reverse mapped by applying U(θ), giving the
evolved state
∣ψ(t)⟩= U(θ)T (t)U †(θ)∣ψin⟩.
This method has the key advantage that since the evolution is directly implemented as simple rotations in the
computational subspace, the circuit depth is independent
of the evolution time to be simulated. However, the initial SSVQE optimization can be diﬃcult, especially when
one considers many eigenstates k.
Variational fast forwarding.
Similar to the idea of the
SVQS, variational fast forwarding (VFF) relies on the
idea of evolving a quantum state in time exp(−iHt)
within a diagonal subspace, such that an enhanced evolution time can be achieved . First,
a circuit that implements a small timestep of the desired
evolution is implemented as V (δt) = exp(−iHδt). Then,
an approximate diagonal factorization of V (δt) is trained
for a particularly structured variational circuit
U(θ,γ,δt) = W(θ)D(γ,δt)W †(θ).
Here, D(γ,δt) is composed of commuting unitaries and
chosen to parameterize the eigenvalues of unitary V (δt),
whereas W(θ) represents its eigenvectors. Then, the evolution to an arbitrary time T = Nδt, where N is some
integer, is found by fast forwarding with U(θ,γ,Nδt) =
W(θ)DN(γ,δt)W †(θ).
For the training of the variational Ansatz, the ﬁdelity between V (δt) and U(θ,γ,δt)
is maximized by a quantum-classical feedback loop with
a cost function that uses the local Hilbert-Schmidt
test .
As alternative approach, it
was proposed to diagonalize the Hamiltonian H instead
of the unitary V (δt), and fast forward via U(θ,γ,T) =
W(θ)exp(−iD(γ)T)W †(θ)
 .
Fast-forwarding can also be performed without requiring
to train via a feedback loop using the linear combination
of states approach (Eq. (42)) .
Quantum Assisted Simulator.
The VQS algorithm employs a classical-quantum feedback loop to update the
parameters of the PQC. Until the classical processor
has calculated its output, the classical-quantum feedback
loop delays any use of the quantum device, slowing the
algorithm on current cloud computing framework. The
VQS algorithm, as well as its VQE based variant, i.e.
SVQS share similarities and most of the concerns faced
by VQE, such as the barren plateau issue (see Sec. IV.A).
Further, the VQS algorithm requires controlled-unitaries,
which make it diﬃcult to realize for current-term devices. To tackle the issues faced by VQS, the quantum
assisted simulator (QAS) was suggested recently . The QAS algorithm does not need
any classical-quantum feedback loop, can be parallelized,
evades the barren plateau problem by construction, supplies a systematic approach to constructing the ansatz
and does not require any complicated unitaries.
The QAS algorithm shares its approach with IQAE
(see Sec. III.E). The ansatz is given as a linear combination of states ∣φ(α(t))⟩= ∑∣ψi⟩∈CSK αi(t)∣ψi⟩(see
Eq. (42)), with classical coeﬃcients α(t) for ansatz state
∣ψi⟩, which can be systematicly constructed (see Deﬁnition 3). The Hamiltonian H is given as a linear combination of unitaries (see Eq. (43)). The QAS algorithm
employs Dirac-Frenkel principle to obtain the following
classical evolution equation for α(t)
= −ιDα(t).
Here, Ei,j = ⟨ψi∣ψj⟩and Di,j = ∑k βk⟨ψi∣Uk∣ψj⟩are overlap matrices that can be eﬃciently measured on a quantum computer, i.e. for H given as combination of Pauli
strings, the overlaps are measurement of Pauli strings.
Recently, QAS was run on the IBM quantum computer
and showed superior performance compared to Trotter
and VQS for a time-dependent Hamiltonian . A novel Hamiltonian simulation algorithm based
on truncated Taylor series was proposed recently . The classical post-processing in the aforementioned algorithm corresponds to a QCQP.
6. Quantum information scrambling and thermalization
Quantum information scrambling is a quantum phenomena occurring when initially local states become increasingly non-local with the time-evolution of the system. It can be analyzed by computing the so-called outof-time-ordered correlation function (OTOC) and has
strong implications in thermalization in closed quantum
systems dynamics. Recent experiments have been carried
out to study this phenomena in a few qubits trappedion devices and simulators , and in a 53 superconducting qubit processor . The algorithms proposed are based
on the well-known teleportation algorithm and use single and two-qubit gates to reproduce the the scrambling
In the context of VQAs, a variation of the VQE algorithm has been proposed to obtain the thermal evolution of quantum systems . The
authors present the Quantum Hamiltonian-Based Models (QHBM), an extension of the VQA’s PQC to mixed
states instead of pure states. Within this approach, the
QHBM are classically trained to learn a mixed state distribution as a function of the optimization parameters.
A direct application of such a model is the Variational
Quantum Thermalizer (VQT), an algorithm which goal
is to prepare a ﬁxed-temperature thermal state of a given
Hamiltonian.
The limitations of using variational QML algorithms
to learn a scrambling unitary have also been studied in
 , where it is found trainability issues
related with barren plateaus (see Sec. IV.A).
7. Simulating open quantum systems
In the following, we deal with the physics of open quantum systems which are well-described
by the Lindblad master equation from Eq. (52). By sampling from a mixture of pure state trajectories evolved
by a non-Hermitian Hamiltonian and random quantum
jumps, one recovers the Lindblad dynamics.
Trotter simulation of open systems.
NISQ quantum hardware can be used to directly simulate the dynamics
of small-scale open systems by using ancillas combined
with measurements in the spirit of the quantum jump
method . Here,
the unitary part of the dynamics is implemented via a
Suzuki-Trotter decomposition (see Sec. II.B). The nonunitary part of the dynamics that encodes the interaction
with the external degrees of freedom is simulated by entangling the circuit with ancillas and subsequently measuring them. For every time step of the dynamics a new
set of ancilla qubits has to be provided. Current quantum computers based on superconducting circuits do not
allow one to measure and re-use qubits, thus requiring a
linear increase in the number qubits with every timestep.
Further, in general the circuit depth scales polynomially
with simulation time.
Generalized variational quantum simulator.
In Ref. the VQS algorithm is extended to simulate the method of quantum jumps in a variational
setting. They implemented the algorithm for 2D Ising
Hamiltonians for 6 qubits and observed a dissipation induced phase transition. In another work , VQS is extended to mixed states and simulate the
Lindblad dynamics fully without the need of stochastic
The idea is to write the density matrix as
ρ = ρ(θ (t)) and simulate the evolution of ρ via evolution
of the parameters θ(t). One can re-express Eq. (52) as
dtρ = ∑i giSiρT †
i , where Si and Ti are unitaries and gi
are coeﬃcients. Using Dirac and Frenkel equation, the
evolution of parameters is given by
Mi,j ˙θj = Vi,
Mi,j = Tr[(∂iρ(θ(t)))† ∂jρ(θ(t))]
(∂iρ(θ(t)))† ∑
This method can also be extended to deep quantum neural network type Ansatzes .
These algorithms, however, suﬀer from the canonical
drawbacks of the VQS algorithm, such as the requirement of feedback loop, trainability issues and necessity
of controlled unitaries.
Generalized quantum assisted simulators.
Recently, the
generalized quantum assisted simulator (GQAS) was proposed as extension of the
quantum assisted simulator to tackle above issues (see
Sec. VI.A.5).
Instead of using a density matrix, the
GQAS algorithm introduced the concept of “hybrid density matrix”
βk,l∣ψk⟩⟨ψl∣
for βk,l ∈C and ∣ψl⟩are chosen from the set of cumulative K moment states (see Deﬁnition 3). A classical
device stores the coeﬃcients β and the quantum states
correspond to some quantum register.
A hybrid density matrix is a valid density matrix, if Tr(ˆρ) = 1 and
ˆρ ≽0. Note that the normalization condition is fulﬁlled
when Tr(ˆρ) = Tr(βE) = 1, where Ek,l = ⟨ψk∣ψl⟩. Using
Dirac-Frenkel principle, the simulation of open system
dynamics for the hybrid density matrix is given by
dtβ(t)E = −ι(Dβ(t)E −Eβ(t)D)+
γn(Rnβ(t)R†
2Fnβ(t)E −1
2Eβ(t)Fn),
where Dk,l = ⟨ψk∣H∣ψl⟩, Rn
k,l = ⟨ψk∣Ln∣ψl⟩and Fn
nLn∣ψl⟩. For a given choice of ansatz, the quantum
computers only have to compute the overlap matrices
as measurements of Pauli strings.
Then, the classical
computer uses this information to simulate the dynamics.
There is no quantum-classical feedback loop, which on
the currently available quantum computers can speed up
computations substantially.
8. Nonequilibrium steady state
Unlike the previous Sec. VI.A.7, we concern the physics
of open quantum system that is out-of-equilibrium in nature, which is common in designing devices for molecularscale electronics , excitonic transport as well as quantum thermodynamics . By “out-ofequilibrium”, we mean that a quantum system and bath/s
are constantly driven by external forces such as voltage
diﬀerences, during which the composite particles of the
system and bath are also interacting each other.
Sec. VI.A.7 would also lead to extremely high dimensional matrices in the Lindblad like master equation approach dˆρ/dt = ˆˆLˆρ (see Sec. V.A.1, Eq. (52)), and it
deems impossible to capture all the degrees-of-freedom
involved. However, one may relax some of the constraints
involved in the problem setup, say time-independent dissipation and non-interaction among particles with small
system size. The steady state density matrix of a quantum system ˆρSS at the limit t →∞is then given by
ˆˆL∣ˆρSS⟩= 0,
or equivalently ˆˆL† ˆˆL∣ˆρSS⟩= 0. A recent study has shown that with ancilla qubits, the above
non-Hermitian superoperator ˆˆL can be simulated. The
main idea is to map the density matrix of N qubits onto
a vector of twice the number of qubits 2N
ρij ∣i⟩⟨j∣→∣ˆρ⟩= ∑
C ∣i⟩P ∣j⟩A ,
∑ij ∣ρij∣2.
variational
iteratively
expectation
a parameterized density matrix ∣ρθ⟩
minθ ⟨0∣⊗2N U †(θ) ˆˆL† ˆˆLU(θ)∣0⟩⊗2N. A drawback of this
approach is that measuring expectation values from the
parameterized density matrix directly is diﬃcult and thus
requires an additional transformation.
Beyond the Lindblad master equation, to capture
and describe truly “out-of-equilibrium” processes, the
nonequilibrium Green’s function (NEGF) formalism
 is commonly used. These existing Green’s function techniques are very complicated to
be solved. Many assumptions need to be made in order to
have some closed form and do some calculations. In particular, it requires that the interaction among particles
are weak such that one does not need to ﬁnd higher-order
Feynman diagrams in ﬁnding the self-energy functional.
Since some of the existing quantum algorithms provide
promising speedup over classical ones, one may wonder
to use quantum algorithms to solve the NEGF, with a
strategy of leaving classically hard computational tasks
to the quantum processor and feeding its output back to
classical computer, which could be done in a variational
fashion. There exists a number of proposals in the
literature that undertake such hybrid quantum-classical
approach. However, these methods assume no interaction
among composite particles. In generic open quantum system in which many-body eﬀects cannot be neglected, one
would like to go beyond those assumptions. It is yet to see
any quantum advantage of those near-term quantum algorithms over existing methods for solving nonequilibrium steady state solution of an extremely complex physical setup such as vibrationally-coupled electron transport
with multiple electronic levels .
9. Gibbs state preparation
Finding the ground state of quantum Hamiltonians is
known to be QMA-hard. Under reasonable assumptions,
preparing Gibbs state corresponding to arbitrarily small
temperatures is as challenging as the Hamiltonian ground
state problem. Gibbs state preparation has applications
in many areas including quantum annealing, quantum
SDP solvers, Boltzmann training and simulation of equilibrium physics. For a Hamiltonian H, the Gibbs state
at temperature T (with kB = 1) is given by
Some of the approaches to prepare Gibbs state are mentioned in the following
1. Starting with d-dimensional maximally mixed state
d , under imaginary time evolution for time τ, one
gets Gibbs state corresponding to temperature T =
2τ .
2. One can start with maximally entangled state ∣ξ⟩d =
d ∑j ∣j,j⟩AB of a system combined of two equally
sized subsystems A and B, and evolve it under
imaginary time evolution using Hamiltonian H ⊗I.
After tracing out system B, the state of system A
at time τ is given by Gibbs state corresponding to
temperature T =
3. The Gibbs state of a system is the density matrix
which corresponds to minimum of its free energy.
Thus, one can variationally tune the parameters of
a parametrized density matrix such that it leads to
minimization of free energy.
Recently, a few NISQ algorithms for Gibbs state preparation have been proposed, which apply the aforementioned
ideas. In , authors used VQS based
imaginary time evolution to prepare Gibbs state following the second approach. The ﬁrst approach does not
work in VQS based imaginary time evolution. In another
work , the third approach was
used to prepare Gibbs states. The aforementioned works
require complicated controlled unitaries and classicalquantum feedback loop.
In ,
QAS based imaginary time evolution (see Sec. VI.A.5)
was suggested to prepare the Gibbs state with either
ﬁrst and second approach. The QAS approach does not
require any classical quantum feedback loop or complicated controlled unitaries. Using random circuits as initial state, suggested an approach
based on imaginary time evolution prepare Gibbs state.
10. Simulation of topological phases and phase transitions
NISQ devices can be used to study the ground states
of quantum Hamiltonians for understanding topological phases and phase transitions. An important example is the one-dimensional cluster Ising Hamiltonian, describing a symmetry-protected topological phase of matter.
The ground state of this Hamiltonian is the onedimensional cluster state, which can be created by applying Hadamard gates to all qubits, followed by control-
Z gates on each pair of neighboring qubits.
State tomography and symmetry arguments were used to study
the entanglement measures of this state and to highlight its topological nature .
A modiﬁed algorithm was implemented
to simulate an enlarged family of Hamiltonians and
study the quantum phase transition between a topological and a topologically-trivial phases of matter . NISQ devices were also used to simulate
the dynamics of fundamental models of quantum magnetism and
topological phases in one and two dimensions (Mei et al.,
11. Many-body ground state preparation
The preparation of non-trivial many-body quantum
states is crucial for many applications in quantum metrology and quantum information processing (Kyaw et al.,
QAOA has been used as a resource-eﬃcient
scheme for many-body quantum state preparation.
this context, the state ∣ψ⟩for a system with linear dimension L (e.g. L can refer to the number of spins in a
1D spin chain) is non-trivial if there is no local unitary
circuit U with depth O(1) which can generate ∣ψ⟩from
a product state ∣φ⟩: ∣ψ⟩= U ∣φ⟩ .
The Greenberger-Horne-Zeilinger (GHZ) state, which is
an essential resource in several quantum metrology proposals , is
an example of a non-trivial quantum state due to its
highly-entangled nature, and is the ground state of the
1D Ising Hamiltonian with periodic boundary conditions,
i.e. HP = −∑L
Using QAOA, it has been shown that the GHZ state
can be prepared eﬃciently with perfect ﬁdelity using p =
L/2, where p is the QAOA depth .
The authors conjectured that the ground state of the 1D
transverse-ﬁeld Ising model
with L even and periodic boundary conditions, can
be prepared perfectly at any point in the phase diagram
using QAOA with p = L/2.
The ground state of the
antiferromagnetic Heisenberg model with open boundary
conditions HP = ∑L−1
i=1 ˆσi ⋅ˆσi+1,
where ˆσi ≡(ˆσi
z), has also been prepared with
near perfect ﬁdelity using QAOA. Using a long-range 1D
Ising Hamiltonian HP = −∑i<j Jij ˆσz
where Jij = J0/∣i−j∣α, QAOA can achieve the ultrafast
preparation of a GHZ state with a circuit depth of O(1)
(for α = 0) . This result was generalized
by , which showed that QAOA
can prepare the ground states of the fully-connected ferromagnetic q-spin model (note that q is used here instead
of the conventional p in order to avoid confusion with the
QAOA depth p)
with resources scaling polynomially with the number of
Since the system can encounter a ﬁrst-order
phase transition where the spectral gap becomes very
small, QAOA greatly outperforms quantum annealing in
this instance since an exponentially long annealing time
is needed.
12. Quantum autoencoder
The quantum autoencoder 
(QAE) is a VQA for the compression of data on a quantum computer. It ﬁnds a new data state representation
which requires fewer qubits than the data was originally
deﬁned upon. This new encoding is said to be a representation in the latent space. The process of transforming
the data into the latent space is referred to as encoding,
and the converse, transformation of states in the latent
space back onto the original, is known as decoding.
Training a QAE requires the minimization of an objective deﬁned over several related quantum states. For a
set of n-qubit states {∣ψ⟩i}, the goal of the QAE is to ﬁnd
a unitary circuit E(θ) which accomplishes the following
transformation
E ∶Hn →Hk ⊗Hn−k∣E ∣ψ⟩i = ∣φ⟩i ⊗∣0⟩⊗(n−k) ,
where k is the dimension of the latent space.
the application of a perfectly trained autoencoder to any
state of the relevant set yields a product state that consists of the transformed state on k qubits with a (n −k)qubit “trash” state. In principle, the trash state could be
any state, but the all-zero state is chosen for simplicity.
The loss function of the QAE may be deﬁned in several
ways. It is a ﬁdelity loss function (see Sec. II.A), in which
minimization is performed by increasing the overlap between a (partial) measurement of the state resulting from
the application of the encoder and a known state. The
most practical deﬁnition for training the autoencoder,
called “trash training”, uses as its objective the overlap
between the “trash” qubits and the ∣0⟩⊗(n−k) state. Formulated in the density matrix picture, the objective of
minimization is
O = −Tr(I⊗k ⊗∣0⟩⟨0∣⊗(n−k)ρi),
where ρi = ∑i pi∣ψi⟩⟨ψi∣with, in general, all states in the
set equally weighted.
The QAE can be trained by training only the encoding
circuit, due to the unitarity of the encoder; the decoding
operation is achieved by the complex conjugate of the
encoder circuit.
Improvements in the encoding results
translates to improvements in the decoder, a boon not
possessed by classical autoencoders.
After the successful training of a QAE, the encoder
and decoder circuits may be used for data transformation
in further algorithms, action upon the data in the latent
space, in which the data is represented more densely, may
prove powerful in further applications.
A data re-uploading strategy to construct a QAE encoder is presented in , where the socalled enhanced feature quantum autoencoder (EF-QAE)
is trained to compress the ground state of the 1-D Ising
model as a function of the external ﬁeld and samples of
handwritten digits. The QAE has also been deployed experimentally in the compression of qutrits on a photonic
device . Small states have been experimentally compressed losslessly on photonic devices
by . designed a QAE capable of denoising entangled
quantum states, such as GHZ or W states, subject to
spin ﬂip errors and random unitary noise.
13. Quantum computer-aided design
Two recent proposals focus the computing power of
NISQ devices back on the processors themselves: Techniques were developed to simulate quantum hardware
on a quantum computer .
They establish the paradigm of “quantum computer-aided design”, indicating that classically
intractable simulations of quantum hardware properties
can be performed on a quantum computer, thereby improving the prediction of device performance and reducing experimental testing cycles.
In the ﬁrst approach, optical path modes are mapped
to sets of qubits, and quantum optical elements are
mapped to digital quantum circuits that act on the qubits
 . Photonic setups can then be
ﬂexibly simulated. The framework is used to simulate
both a Boson sampling experiment and the optimization
of a setup to prepare a high-dimensional multipartite entangled state.
The second proposal introduces quantum simulation
techniques for superconducting circuit hardware .
A circuit module consisting of coupled
transmon qubits is designed. The corresponding superconducting circuit Hamiltonian, which is written in a
basis of multi-level operators, is eﬃciently mapped to
a set of data qubits . Simulations
of a multi-level extension to the VQE algorithm are used to determine the spectrum
of the superconducting circuit. The resulting states and
eigenenergies are directly related to experimentally relevant device characteristics and can be used to seed simulations of time dynamics.
Device and setup design is a key challenge in improving
and scaling quantum systems. Therefore, digital quantum simulation of quantum processors will be a relevant
application for NISQ quantum computers as classical resources become too small to capture the relevant Hilbert
space of the hardware.
B. Machine learning
The goal of machine learning is to facilitate a computer
to act without being explicitly programmed to. As per
Tom Mitchell , given some class of tasks
T and performance metric P, a computer program is said
to learn from experience E if
i.e. its performance measured by P for task T increases
Depending on the kind of experience E permitted to
have during the learning process, the machine learning
algorithms are classiﬁed into three categories:
1. Supervised learning. Given a function y = f(x), the
goal is to learn f so that it returns the label y for
the unlabelled data x. A canonical example would
be pictures of cats and monkeys, with the task to
recognize the correct animal. Given training examples from the joint distribution P(Y,X), the task
of supervised learning is to infer the probability of a
label y given example data, x, i.e., P (Y = y∣X = x).
2. Unsupervised learning. The data is provided without any label. The task is to recognize an underlying pattern in this data. Given access to several
examples x ∈X the algorithm goal is to learn the
probability distribution P(X) or some important
properties of the aforementioned distribution.
3. Reinforcement learning. In this case, neither data
nor label is provided. The machine has to generate
data and improve the aforementioned data generation process via optimizing a given reward function.
This is similar to how a human child learns to walk.
If it fails, the output acts as a negative reward.
Machine learning has uncovered applications in physics
such as Monte Carlo Simulation , many-body physics , phase transition , quantum foundations , and state tomography .
For a meticulous review on machine learning for
physics, refer to .
Most of the success in machine learning come from the
use of artiﬁcial neural networks, structures capable of
learning sophisticated distributions and that encompass
multiple features that can be ﬁne-tuned depending on
the problem to tackle. In that direction, there are several proposals to deﬁne a model for quantum neural networks with diﬀerent kind of activation functions .
For implementations of artiﬁcial neuron
and artiﬁcial neural network on the NISQ hardware, refer to .
The merger of quantum theory and machine learning
has recently led birth to a new discipline, known as quantum machine learning (QML). Both algorithms that deal
classically with data from a quantum origin and quantum
algorithms that process quantum and classical data are
usually known as QML applications. However, in this review, we will focus only on those algorithms that process
data quantum-mechanically, in particular, those that use
quantum algorithms that can be run in NISQ computers. For QML review that mainly focus on fault-tolerant
quantum algorithms check . For
a survey of quantum computational learning theory, refer to . An analysis of
QML from a classical ML perspective can be found at
 , and
for near-term devices in .
It might be surprising that a linear theory as quantum physics can generate the non-linearities that a machine learning model needs.
However, the linearity
of quantum mechanics comes from the dynamical part
(quantum states evolution) and one can encounter multiple sources of non-linearities arising from measurement,
post-selection or coupling the system with environment.
Quantum operations in the Hilbert space can also encode
non-linear behaviour, as it will be shown with Kernel
In the following subsections, we will present the quantum mechanical analogs of the three machine learning
categories deﬁned above. The algorithms discussed will
be listed in Table II.
1. Supervised learning
The two prominent methods to perform a supervised
learning classiﬁcation task using a NISQ computer are
quantum Kernel estimation and Variational Quantum Classiﬁer (VQC) .
Classical Kernel methods include well-known machine
learning algorithms such as Support Vector Machines
(SVM) , Principal Component
Analysis (PCA) or Gaussian Processes, among others.
The rich theoretical structure of Kernel methods can be
expanded to the quantum world by deﬁning and working
in the Hilbert space with the quantum equivalent of feature vectors . To that aim,
one needs to modify and adapt the well-known theorems
to work in a quantum feature space. For more details
about classical Kernel methods we refer to . A review on Kernel methods in the context
of QML can be found in .
In the following lines, we will directly describe the quantum versions of them. The basics of supervise learning
with quantum computers are presented in .
Given an input set X and quantum Hilbert space H,
data x ∈X is encoded into a quantum state (quantum
feature vector) ∣Φ(x)⟩by means of the quantum feature
map, i.e. Φ ∶X →H. The inner product of two quantum
feature vectors deﬁnes a kernel
κ(xi,xj) ≡⟨Φ(xi)∣Φ(xj)⟩H ,
for xi,xj ∈X. In comparison with classical kernels, the
inner product is deﬁned in a Hilbert space by replacing
the standard deﬁnition ⟨⋅,⋅⟩by the Dirac brackets ⟨⋅∣⋅⟩.
For a map Φ, the reproducing kernel Hilbert space takes
Rφ = {f ∶X →C∣f(x) = ⟨w∣Φ(x)⟩H ,∀x ∈X,∣w⟩∈H}.
The orthogonality of ∣w⟩w.r.t. ∣Φ(x)⟩deﬁnes a decision
boundary, i.e. depending on the sign of the inner product, x lies in one side of the hyperplane. The function
f is thus a linear function in H. The representer theorem states that this function can
be approximated by the linear function f ⋆by using the
kernel deﬁned above, i.e.
for an input dataset D. Using Eq. (94), one can solve a
convex optimization problem to get the coeﬃcients αi.
The analysis so far entails the connection between linear
models in reproducing kernel Hilbert space with kernelized models in the input space.
One can use a quantum computer to calculate the inner product of feature mapped quantum states to obtain
the kernel κ. This kernel can be fed to a classical device, which can use Eq. (94) to obtain the coeﬃcients αi,
for instance, by maximizing a cost function of the form
 
yiyjαiαjκ(xi,xj),
where yi are the labels of the training points and constrained to ∑D
i=1 αiyi = 0.
Ideas based on connections
between kernel methods and quantum circuit based machine learning has been used to justify that the models
for QML can be framed as kernel methods .
For some of the other relevant works on quantum kernel
methods, refer to .
A high-dimensional data classiﬁcation experiment with
quantum kernel methods was carried out recently . The encoding of data into quantum
circuits is characterized by the quantum Fisher information metric . For hardware eﬃcient
PQCs, the kernel can be related to radial basis function kernels .
Measuring the quantum kernel using the SWAP or inversion test scales as D2.
Using randomized measurements , the kernel can be computed
in a time that scales linearly with the dataset size D,
which allows for processing large datasets with quantum
computers .
Sec. III.B) can be used for computing kernel functions
 .
Another approach is to use a variational circuit U(θ)
and directly perform the classiﬁcation task in the reproducing kernel Hilbert space, without using Eq. (94). This
approach is sometimes referred as a variational quantum classiﬁcation. Data is also embedded into the state
∣Φ(x)⟩and then processed with a PQC U(θ). The resultant state becomes
∣Ψ(x,θ)⟩= U(θ)∣Φ(x)⟩,
which parameters are estimated by training it to match
the target states ∣yi⟩that represent the yi labels of the
training points, i.e. by minimizing the inﬁdelity
(1 −∣⟨yi∣Ψ(xi,θ)⟩∣2).
Both methods require a way to encode the data into a
quantum state. There are several strategies to deﬁne the
quantum feature map. It is a key step in the success of
the classiﬁcation task, as the needed non-linearities must
come from it.
Furthermore, to eventually obtain any
quantum advantage, one should search from the set of
classically intractable feature maps. One of the ﬁrst proposed approaches was the amplitude encoding
et al., 2016) also required in other quantum algorithms
 . This approach encodes the classical data points into the amplitudes of a quantum state,
∣Φ(x)⟩= ∑i xi∣ei⟩, where ∣ei⟩are the basis states.
However, this raw encoding requires i) knowing which
gates can be used to perform this operation for general
data points and ii) having an eﬃcient way to extract and
process these amplitudes. Although the ﬁrst point can
eventually be overcome by using similar approaches as
the ones used to deﬁne a PQC, the second one requires
tools as QRAM , experimentally challenging for the NISQ era. The studies towards
QRAM in proposed an approach to
update classical data consisting of M entries of n bits
each using O(n) qubits and O(Mn) steps. A forkingbased sampling scheme was suggested in to reduce the resource requirements for state
preparation for tasks involving repeated state preparation and sampling. At the moment of writing, building
a QRAM remains challenging and further investigations
are required.
In general, the encoding strategies used in state-of-theart algorithms consist on introducing the classical data
points into the parameters of the quantum circuit gates.
As brieﬂy mentioned in Sec. II.B, one designs a state
preparation circuit E that encodes the data points,
∣Φ(x)⟩= E(x,φ)∣0⟩.
The use of φ parameters is optional and they can be
subject to the optimization subroutine too.
Typically, the encoding gate is designed using the same
structure of a layer-wise PQC from Eq. (22). Data points
are introduced in layers of single-qubit rotational gates
R, as deﬁned in Eq. (13), followed by an entangling gate
unitary W, e.g.
Rk(xi))Wk,
with LE being the total number of encoding layers. Then,
the whole Variational Quantum Classiﬁer (VQC) is composed by this encoding circuit and the processing one to
be optimized, i.e. UV QC(θ,x) = E(x)U(θ).
Alternatively, some works propose to remove the distinction between the encoding E and processing U circuits and introduce the data values along the circuit
 . This strategy, sometimes called input-redundancy
or data-reuploading, introduce the data in all circuit layers, e.g.
UV QC(θ,x) =
Rk(xi,θ))Wk,
where L is now the total number of circuit layers. This
strategy has proved the universality when applied to one
qubit and can reconstruct
the coeﬃcients of the Fourier series .
The inclusion of encoding strategies and, in particular,
the data re-uploading, can help well-known VQA such as
the VQE. In general, one of the ﬁnal goals of a VQE can
be the identiﬁcation of interesting points on a potential
energy surface generated by a parametrized Hamiltonian.
Often, one is interested in the ground state energy as a
function of some Hamiltonian parameter λ, e.g. the interatomic distance, but other properties, like the energy
gap between ground state and ﬁrst excited state can be
interesting as well. To do so, one
often needs to scan discretely over λ for some particular interval and run a VQE to obtain the ground state
energy on each of these points. This becomes an extra
computational cost, especially if we are interested only
in a particular region of this ground state proﬁle, e.g.
to extract the λmin whose ground state has minimal energy. In that direction, some proposals suggest to encode
the parameters of the Hamiltonian into the PQC and
learn the energy proﬁles . In particular, the Meta-VQE algorithm proposes to encode the λ into the PQC gates together with the optimization parameters. Then optimize
an objective function that corresponds with the sum of
expectation values for some M training λ parameters,
i.e. ⟨ˆO⟩= ∑M
i=1⟨H⟩U(θ,λi). Once the circuit has been optimized, one can run it again with the new λi to directly
extract an estimation of the ground state, without having
to optimize the full circuit again. An extension of this
approach is the optimized Meta-VQE (opt-meta-VQE),
which consist of using the optimized parameters from
the Meta-VQE as starting points of a standard VQE.
This tries to avoid vanishing gradients issues (discussed
in Sec. IV.A) by starting in a particular region of the
parameter space instead of random initialization.
Some VQC also need an extra piece, the deﬁnition of
the target state ∣yi⟩to construct the objective function
to be optimized using the ﬁdelity with respect to these
states. The goal of the quantum circuit is to divide and
push the quantum states that encode the data points into
two or more regions of the Hilbert space. To that aim, the
parameters of the circuit are trained to match every encoded state into a particular representative of one of these
regions. Therefore, the more separated these regions are,
the lesser misclassiﬁed points are expected. As discussed
in Sec. II.C, qubits measurement implies a certain computational cost. For that reason, many proposals suggest
to use the state of only one qubit to train the whole circuit . The
cost function estimation reduces to measuring the probability distribution of one qubit. Other works use a more
sophisticated deﬁnition of these target states by selecting the most orthogonal states of the qubits space . This strategy
is inspired from optimal state discrimination .
Quantum reservoir computing has been
proposed for many experimental platforms such as Gaussian states in the optical set-up ,
two-dimensional fermionic lattices 
and nuclear spins . Quantum gate
based implementation of quantum reservoir computing
for NISQ devices has also been discussed . A Gaussian Boson Sampler (see Sec. III.B) can
also be used for quantum reservoir computing as suggested in to perform machine learning tasks such as classiﬁcation. NISQ devices
have also been used for regression .
Distance-based classiﬁer using quantum interference circuits has been proposed in .
Quantum annealing has been also applied to supervised learning to predict biological data .
Here, the quantum annealer is used to train the parameters of the classiﬁcation model, which is done by mapping
the problem of ﬁnding the optimal parameters to a minimization of a QUBO.
2. Unsupervised learning
The use of quantum devices to speed up diﬀerent unsupervised learning tasks has been investigated thoroughly,
leading to diﬀerent algorithms for generative modelling
 , clustering , among others . An
analysis of quantum speedup in unsupervised learning for
Fault-Tolerance algorithms is presented in . The task of learning probabilistic generative models in particular has been of interest to the QML community, because of the potential advantage quantum computers may exhibit over their classical counterparts in
the near future . For the advantages rendered by quantum correlations such as contextuality and Bell non-locality for generative modelling,
refer to .
Generative Modelling involves learning the underlying
probability distribution from a ﬁnite set of samples from
a data set, and generating new samples from the distribution. There have been several proposals for using parameterized quantum circuits as models for generative learning , including quantum Boltzmann machines, quantum circuit
Born machines, quantum assisted Helmholtz machines,
quantum generative adversarial networks, amongst others .
We discuss some of these proposals in detail hereafter.
Quantum Boltzmann Machines.
The quantum Boltzmann machine (QBM) extends the
classical Boltzmann machine , a neural architecture capable of several tasks including generative modeling of data.
Such models take their name
from their physical inspiration, namely, the Boltzmann
distribution over the Ising model in the classical case,
and the Boltzmann distribution over the transverse-ﬁeld
Ising model, for the quantum case. Such a network consists of a mixture of visible and hidden vertices, connected
by weighted edges. The visible vertices function as both
input and outputs to the network, whilst the hidden vertices add extra degrees of freedom to the network.
The QBM can be modeled with the Hamiltonian
where ba, Γa, and ωab are the parameters to be ﬁnetuned to generate the training data. Deﬁning the density matrix ρ = e−H
with Z the usual partition function,
Z = Tr(e−H), the marginal probability that the visible
variables are in some state v is given by Pv = Tr(Λvρ),
with Λv = (⊗ν
) ⊗Ih, a projector onto the subspace spanned by the visible variables tensor the identity
acting on the hidden variables. The objective of training
the QBM, then, is to get the family of probability distributions Pv to match the family inherent to the data,
, for arbitrary v. This is achieved by minimizing
the negative log-likelihood measure shown below
logTrΛve−H
The gradients of L with respect to the Hamiltonian parameters are diﬃcult to calculate by sampling the Boltzmann machine, both classically and in the quantum variant. Methodologies of approximating these gradients are
necessary to advance the deployment of QBM’s.
The QBM may be trained both to be a generator,
or a discriminator, with respect to the distribution it
is trained to mimic. Consider the joint distribution of
input and output variables x and y respectively. In the
discriminative case, the objective is to minimize negative log-likelihood with respect to Py∣x, For generative
learning, the goal is to learn the joint distribution Px,y
The implementation of the QBM designed by found that a ten qubit QBM with only visible vertices is able to learn a mixture of randomly generated Bernoulli distributions more eﬀectively than a classical Boltzmann machine, and performed better in generative applications. found
that a QBM outperformed classical Boltzmann machines
in generative training to reproduce small Haar-random
states. Extensions to the QBM, such as the Variational
Quantum Boltzmann Machine (VQBM) , have improved upon trainability. Using ideas similar to , VQBM were also proposed
in . In addition to its generative capacities, QBMs have shown potential in reinforcement
learning , in which they have
been shown to achieve better ﬁdelity to data distributions
than do restricted Boltzmann machines or deep Boltzmann machines (classical boltzman machines with layers
of hidden vertices) of similar sizes.
To suit NISQ devices, suggested that QBM can be
approximated using QAOA as a subroutine in , an eﬃcient method for training QBMs
in NISQ devices based on the eigenstate thermalization
hypothesis has been proposed.
Quantum Circuit Born Machines.
Parametrized quantum circuits can function as generative models to sample from probability distribution. The Quantum Circuit
Born Machine (QCBM) outputs
bitstrings x sampled from measurements in the computational basis of a quantum circuit U(θ), with the
probability of each bit string given by the Born rule
pθ(x) ∼∣⟨x∣U(θ)∣0⟩∣2.
The goal is that the distribution of the QCBM matches the one from a given target
distribution.
QCBMs can prepare classical probability distributions
as well as entangled quantum states by training them to
match the probability distribution corresponding to the
desired quantum state . In , training of QCBMs using the gradients
of the parameterized quantum circuit was proposed using
the maximum mean discrepancy loss, which calculates
the diﬀerence of the sampled output from the quantum
circuit and the desired distribution in a kernel feature
QCBMs are well suited to be run on current NISQ
hardware and can serve as benchmarks 
and have been applied to tasks such as generating images or ﬁnancial data . It has been shown that
QCBMs can potentially outperform classical computers
as they are able to sample from probability distributions
that are diﬃcult for classical computers .
Quantum Generative Adversarial Networks.
Generative
adversarial learning has been
one of the most recent breakthrough in machine learning, and have become very powerful tool in the machine
learning community, for image and video generation, and
materials discovery. The GAN consists of two networks,
a generator, FG(z;θg) and a discriminator, FD(x;θd) with parameters θg and θd respectively, playing an adversarial game, which can be summarized as follows:
θd (Ex∼pdata(x)[log(FD(x)]
+Ez∼pz(z)[log(1 −FD(FG(z)))]
where pz(z) is a ﬁxed prior distribution, pdata(x) is the
target distribution, x is the data sampled from pdata(x),
and z is the noise sampled from pz(z). The training of
GAN is carried iteratively, until the generator produces
a distribution indistinguishable from the target distribution.
A quantum version of generative adversarial networks
(GANs) was proposed theoretically in Refs. 
and further developed for near term quantum devices in
Refs. , where parameterized quantum circuits
are used for adversarial learning instead of classical neural networks.
The diﬀerent adaptions of quantum GANs can be divided into diﬀerent categories, based on the data and
networks used being classical and quantum .
There have been diﬀerent studies with hybrid models of GANs using both classical and
quantum data, and it has been shown that the training of these networks are robust to moderate level of
noise .
The training of quantum GANs has been demonstrated experimentally on various quantum processing
units, for a variety of tasks including, quantum state estimation , image generation , generating continuous distributions , learning distribution ,
among others .
3. Reinforcement learning
The general framework of reinforcement learning (RL)
involves an agent interacting with an environment attempting to maximize an underlying reward function.
The mathematics of RL can be captured using Markov
decision process (MDP) . An
MDP is a 4-tuple (S,A,R,P), where S is the set of all
possible valid states; A is the set of all possible actions; R
is the reward function, i.e. a map R ∶S×A×S →R; and P
is the transition probability, i.e. a map P ∶S×A → .
Speciﬁcally, the transition probability P(˜s∣s,a) represents the probability of transition to state ˜s given the
present state is s and the action a has been taken.
The term “Markov” in MDP means that transitions are
memory-less and depend only on the current state and action. The agents in reinforcement learning learn via trial
and error. For a successful training, a proper balance between exploration of unknown strategies and exploitation
of prior experience is required.
The training happens via agent-environment interaction. At the beginning of time step t, the environment
state is st. From the set A, the agent selects an action at.
The transition probability dictates the next state of the
environment st+1 and the agent gets reward rt+1 based
on the reward function R. The agent-environment interaction yields a series of states and actions of the form
τ = (s1,a1,s2,a2,⋯,sH,aH). The aforementioned series
is called a trajectory and the number of interactions (H)
in an episode is called horizon. Suppose the probability of a trajectory is P (τ) and the corresponding cumulative reward is Rtot (τ). Then, the expected reward is
∑τ P (τ)Rtot (τ).
By harnessing quantum mechanical phenomena such
as superposition and entanglement, one can expect to
achieve speedups in the reinforcement learning tasks
 .
The aforementioned intuition has led to
recent works towards quantum reinforcement learning
 
We discuss the essence of quantum reinforcement lerning by providing a brief synopsis of quantum agent environment (AE) paradigm. For details, refer to . In the AE paradigm, agent and environment
are modelled via sequences of unitary maps {Ej
E}j respectively. The agent and environment have access to memory registers belonging to Hilbert spaces HA
and HE. The communication register between the agent
and the environment belongs to Hilbert space HC. The
agent maps {Ej
A}j act on HA ⊗HC and the environment
E}j act on HE ⊗HC. The agent and environment interact with each other by applying their maps
sequentially. The set of actions and states correspond to
orthonormal set of vectors {∣a⟩∣a ∈A} and {∣s⟩∣s ∈S} respectively. The Hilbert space corresponding to the communication register is given by HC = span(∣y⟩∣y ∈S ∪A).
The classical AE paradigm corresponds to the case where
the agent and environment maps are classical.
Quantum reinforcement learning has been studied for
algorithm such as SARSA, and Q Learning , which are some of the elementary reinforcement
learning algorithms .
In the set-up of variational quantum circuits, reinforcement learning has been explored for small input
sizes .
This work revealed a possibility of quadratic advantage in parameter space complexity. Using better encoding schemes, showed the case of reinforcement learning with
variational quantum circuits for larger input sizes. In a
follow-up work, demonstrated
the possibility of dealing with the relatively complicated
example of playing Atari games.
Reinforcement learning with quantum annealers has
also been investigated by
 .
their framework, they explore reinforcement learning
with quantum Boltzmann machines.
A detailed study
of basic reinforcement learning protocols with superconducting circuits is provided in . Some exciting proposals of reinforcement learning with trapped
ions and superconducting circuits have also been proposed recently . For quantum eigensolvers, reinforcement learning study has been
carried out recently .
Reinforcement learning with optical set-up has been discussed in .
C. Combinatorial optimization
Given a ﬁnite set of objects, say S, combinatorial optimization deals with ﬁnding an optimal object from the
set S. It is a sub-discipline of mathematical optimization theory, with applications in diverse ﬁelds such as
artiﬁcial intelligence, logistics, supply chain and theoretical computer science.
Some typical examples of combinatorial optimization problems are the traveling salesman problem , job-shop scheduling , max-cut and
Boolean satisﬁability .
To understand combinatorial optimization, let us consider the canonical problem of Boolean satisﬁability.
Boolean variables admit two truth values, TRUE and
FALSE. These can be combined together using operators
AND or conjunction (denoted by ∧), NOT or negation
(denoted by ¬), and OR or disjunction (denoted by ∨).
These combinations are called Boolean expressions.
A Boolean expression is said to be satisﬁable if it can be
TRUE for appropriate assignment of logical values to its
constituent Boolean variables. Given a Boolean expression E, the Boolean satisﬁability problem (SAT) consist
of checking if E is satisﬁable.
The well-known Cook-
Levin theorem showed that SAT is NP-complete .
Every combinatorial optimization problem can be expressed as m clauses over n Boolean variables. A Boolean
variable is known as positive literal, while its negation is
known as a negative literal. A disjunction of literals is
known as clause or constraint. For every constraint Cα
for α ∈{1,2,⋯,m} and every string z ∈{0,1}n , let deﬁne
if z satisﬁes Cα(z)
if z does not satisfy .
The goal of a combinatorial optimization problem,
framed as such, is to ﬁnd a string which maximizes the
following objective function,
which counts the number of satisﬁed constraints.
Approximate optimization algorithms such as QAOA
seeks to ﬁnd a solution z (usually a bit-string) with a desired approximation ratio r∗≤C(z)/Cmax, where Cmax
is the maximum value of C(z). Using C(z) and computational basis vectors ∣ei⟩∈C2n for i = 1,...,2n, one can
construct the problem Hamiltonian as the one in Eq. (18),
and thus mapping the combinatorial optimization problem to a Hamiltonian ground state problem.
The list of the NISQ algorithms for combinatorial optimization discussed in the following lines are listed in
Table III.
1. Max-Cut
Max-Cut is an important combinatorial optimization
problem with applications in diverse ﬁelds such as theoretical physics and circuit design. In theoretical physics,
the Max-Cut problem is equivalent to ﬁnding the ground
state and its energy of a spin glass Hamiltonian. Given
a graph G = (V,E) with a vertex set V and edge set E,
a cut is a partition of the elements of V into two disjoint
subsets. Given a weight function w ∶E →R+ such that
the edge (i,j) ∈E has weight Eij, the Max Cut problem
consist of ﬁnding a cut K ∪¯K = V that maximizes
For every vertex vi ∈V , let us associate a variable xi
which takes values ±1. Given an arbitrary cut K ∪¯K = V ,
let us deﬁne xi = 1 if vi ∈K and −1 otherwise. Then, the
Max-Cut problem is equivalent to the following quadratic
subject to xi ∈{−1,+1}∀vi ∈V .
Considering n vertices as n qubits in the computational
basis, we can classify qubits by assigning quantum states
∣0⟩or ∣1⟩. For the classical objective function in the optimization program from Eq. (106), we can use the following Hamiltonian as the problem Hamiltonian,
It has been shown that it is NP-hard to achieve an
approximation ratio of r∗≥16/17 ≈0.9412 for Max-Cut
on all graphs . For the QAOA with p = 1,
it has been shown that for a general graph,
4(sin4β sinγ)(cosdi γ + cosdj γ)
4(sin2 β cosdi+dj−2λij γ)(1 −cosλij 2γ),
where di+1 and dj+1 denote the degrees of vertices i and j
respectively, and λij is the number of triangles containing
the edge (i,j) in the graph . Here,
γ and β refer to the QAOA parameters from Eq. (20).
Analytical results for general Ising optimization problems
with p = 1 have also been found .
In the case of unweighted 3-regular (u3R) graphs, the
above result gives the approximation ratio of 0.692, which
is consistent with the pioneering result by Farhi, Goldstone and Gutman . In comparison,
the best classical algorithms to date give the approximation ratio of r∗≈0.8786 for general graphs , and r∗≈0.9326 for u3R graphs
 using semideﬁnite programming.
While QAOA for p = 1 does not outperform its classical counterparts for the Max-Cut problem, QAOA has
been found to surpass the Goemans-Williamson bound
for larger values of p .
QAOA has also been applied to the clustering problem
(from unsupervised learning) by mapping it to Max-Cut
problem .
Remarkably, it was
shown that by ﬁxing the QAOA parameters and selecting the typical problem instances from a reasonable distribution, the objective function value concentrates, i.e.
the objective function value is almost independent on the
instance . This implies that the parameters optimized for one instance can be used for other
typical instances, which would drastically reduce the optimization cost. Similar concentration behavior was also
reported for the Sherrington-Kirkpatrick model in the in-
ﬁnite size limit (n →∞) .
Recently, a non-local version of QAOA called recursive
QAOA (RQAOA) was proposed . It
consist of running a QAOA as a subroutine on a speciﬁc
problem with N qubits and measuring the expectation
values of the correlations between the all qubit pairs (i,j)
with Mij = ⟨σi
z⟩. Then, one picks out the pair of qubits
(n,m) that have maximal absolute value of correlation
n,m = arg max(i,j)∣Mij∣. For Mnm > 0, the selected qubit
pair (n,m) are positively correlated and very likely to be
in the same state, whereas for Mnm < 0 they are anticorrelated and likely to be in opposite state. Now, this
correlation is ﬁxed as a constraint on the problem by
ﬁxing the state of the qubit σm
z = sign(Mnm)σn
this constraint, one of the two qubits can be removed as
its state is completely determined by the other, reducing
the total qubit number by one. Now, the above procedure is repeated for the now reduced problem of size N −1
qubits, i.e. one runs the QAOA subroutine, measures the
correlations and ﬁxes the qubit pairs with maximal correlation. The RQAOA algorithm is run recursively until
the size of the problem is reduced to a small number of
qubits such that it can be solved easily classically. When
RQAOA is run with the QAOA subroutine of depth p = 1,
it can eﬃciently simulated on a classical computer, which
can serve as an important benchmark with classical algorithms . Numerical experiments
with higher p suggest similar or better performance on
combinatorial problems compared to other classical algorithms .
Finally, QAOA with depth p = 1 has been investigated
in comparison with quantum annealing (Streif and Leib,
QAOA is connected to quantum annealing in
the sense that in the limit of inﬁnite depth p, QAOA
is equivalent to quantum annealing (refer to Sec. II.B.1
for QAOA, as well as Sec. III.A for quantum annealing).
However, QAOA can outperform quantum annealing on
speciﬁc problems even at depth p = 1. In fact, QAOA
can solve speciﬁc problems perfectly for p = 1, arriving at
the correct solution with unit probability, whereas quantum annealing struggles here to ﬁnd the solution . This shows that QAOA is strictly more
powerful than quantum annealing.
2. Other combinatorial optimization problems
While the usage of QAOA on Max-Cut has been studied extensively, QAOA has also applications in other
important combinatorial optimization problems such as
Max-k Vertex Cover, which seeks to ﬁnd the set of k
vertices on a graph that maximizes the number of edges
incident on the vertices ; Exact-cover
problem (given a set X and several subsets Si, ﬁnd the
combination of subsets which contains all elements just
once) with applications to the tail-assignment problem
 ; lattice
protein folding ; knapsack problem as applied to battery revenue
optimization ; multicoloring graph problems ; maximum
independent set problems with applications to scheduling; and the vehicle
routing problem .
An adiabatically assisted approach was suggested in
 to tackle combinatorial optimization
Investigations involving variational Grover
search could be helpful to solve combinatorial optimization problems .
Gaussian Boson Sampling (see Sec. III.B) has been used
to assist in a wide variety of combinatorial optimization
problems ,
most prominently to solve Max-Clique .
This has applications in predicting molecular docking conﬁgurations
 , computing vibrational spectra of
molecules , and electron-transfer reactions . Using NISQ devices, an
approach was suggested in for the
triangle ﬁnding problem and its k-clique generalization.
Quantum Annealing, which has been the inspiration of
QAOA, is a prominent platform that has been applied to
various combinatorial optimization problems and its applications, such as protein folding , reviewed in . As gate-based
devices mature, it will open the possibility for experimental benchmarking of QAOA against state-of-the- art
solvers for suitable real-world applications, as performed
in in the context of quantum annealing machines and including proposals beyond
the capabilities of current D-wave devices.
D. Numerical solvers
We proceed to discuss NISQ algorithms used to solve
numerical problems such as factoring, singular value decomposition, linear equations and non-linear diﬀerential
equations, all of them listed in Table IV.
1. Variational quantum factoring
The factoring problem accepts a composite positive integer N as input and returns its prime factors as output.
There is no known eﬃcient classical algorithm for prime
factorization and the hardness of factoring is used to provide the security in the RSA public-key cryptosystems.
The famous Shor’s factoring algorithm is a polynomial
time quantum algorithm for the factoring problem (implying prime factorization is in BQP) and hence
has been extensively investigated by quantum computing
researchers 
and references therein). The resource estimates for implementing the Shor’s algorithm is, however, way beyond
the capabilities of the NISQ era. A detailed analysis has
shown that factoring a 2048-bit RSA number would necessitate a quantum processor with 105 logical qubits and
circuit depth on the order of 109 to run for roughly 10
days .
a photonic architecture, using 1.9 billion photonic modules, factoring a 1024-bit RSA number is expected to tale
around 2.3 years . To tackle the factoring problem in the near-term quantum devices, it is
imperative to develop NISQ-era compatible alternatives
to Shor’s factoring algorithm.
The factoring problem can be mapped to the ground
state problem of an Ising Hamiltonian .
To understand the aforesaid mapping, let us consider the factoring of m = p × q.
Suppose the binary representations of m,p and q are
2imk, p = ∑
k=0 2ipk and q = ∑
Here, mk ∈{0,1}is the kth bit of m and the total number
of bits for m has been denoted by nm. Similar notation
has been employed for p and q. Since m = p×q, it induces
nc = np + nq −1 constraints on the individual bits of m,p
zj,i −mi −
2jzi,i+j = 0,
for i ∈[0,nc) and the carry bit from position i to position j has been represented by zi,j. The constraint i in
Eq. (109) induces clause Ci ≡∑i
j=0 qipi−j +∑i
j=0 zj,i −mi −
j=1 2jzi,i+j over Z such that factoring can be modelled
as assignment of binary variables {mi}, {pi} and {qi}
which solves ∑nc−1
One can map the binary variables to quantum observables to quantize the clause Ci to
ˆCi using the
mapping bk →1
b,k) and obtain the Hamiltonian
HP = ∑nc−1
2, which we refer as factoring Hamiltonian. Note that the factoring Hamiltonian is a 4-local
Ising Hamiltonian.
By using the aforementioned ideas, one can use NISQ
algorithms for the ground state problem to tackle the
factoring problem (see Sec. VI.A.3 and Sec. VI.C). In
Ref. , authors employ QAOA to
ﬁnd the ground state of the factoring Hamiltonian and
refer to their Algorithm as variational quantum factoring (VQF) algorithm. Numerical simulations were provided for numbers as high as 291311. For a recent experimental realization and detailed analysis of VQF, refer to
 .
2. Singular value decomposition
Given a matrix M ∈Cm×n, the Singular Value Decomposition (SVD) provides a factorization of the form
M = UΣV †, where U ∈Cm×m is a unitary matrix,
is a rectangular diagonal matrix with nonnegative real diagonal entries and V ∈Cn×n is a unitary
matrix. The diagonal entries of Σ are called the singular
values of matrix M. The columns of the unitary matrices U and V are called left-singular and right-singular
vectors of M. Using Dirac notation, one can write
dj∣uj⟩⟨vj∣.
where dj,∣uj⟩,∣vj⟩are singular values, left-singular vectors and right-singular vectors. The rank of matrix M is
r and is equal to the number of non-zero singular values.
SVD ﬁnds applications in calculating pseudoinverse
 , solving homogeneous linear equations
 , signal processing and recommendation systems . Moreover, the notion of Schmidt decomposition which is used to study entanglement of bipartite
quantum states, is related to SVD.
In the quantum information context, the SVD can be
used to compute the Schmidt decomposition of bipartite
quantum states. For a quantum state ∣ψ⟩∈HA ⊗HB, the
Schmidt decomposition is given by
di∣ui⟩∣vi⟩,
where di are non-negative real numbers such that ∑i d2
1. Moreover, {∣ui⟩}i and {∣vi⟩}i correspond to orthonormal basis sets for HA and HB respectively. The number
of non-zero di, say χ, is called the Schmidt rank of the
quantum state ∣ψ⟩and is used to quantify the bipartite
entanglement. To calculate the Schmidt decomposition,
one can write the bipartite quantum state as a matrix
∣ψ⟩= ∑i,j Aij∣i⟩∣j⟩,where ∣i⟩and ∣j⟩are the computational basis states of each qubit, and perform SVD of
the matrix A.
Ref. provide a NISQ algorithm to perform SVD of pure bipartite states. Starting with two unitary circuits, which act on diﬀerent bipartitions of the system, the authors variationally determine the singular values and singular vectors by training
the circuits on exact coincidence of outputs. The central ideas of their method is to variationally ﬁnd circuits
that provides the following transformation of the initial
quantum state ∣ψ⟩AB with Schmidt rank χ,
UA ⊗VB∣ψ⟩AB =
λieiγi∣ei⟩A∣ei⟩B,
where UA∣vi⟩A = eiαi∣ei⟩A, VB∣vi⟩B = eiβi∣ei⟩B such that
αi = βi +γi ∈[0,2π) and {∣ek⟩A,B}k are the compuational
basis states in HA,B. Using their algorithm, authors also
suggest the possibility to implement SWAP gate between
parties A and B without the requirement of any gate
connecting the two subsystems.
Using variational principles for singular values and Ky
Fan theorem , provide
an alternative NISQ algorithm for SVD. The authors provide proof of principle application of their algorithm in
image compression of handwritten digits. They also discuss the applications of their algorithm in recommendation systems and polar decomposition.
3. Linear system problem
Systems of linear equations play a crucial role in various areas of science, engineering and ﬁnance. Given a
matrix A ∈CN×M and b ∈CN, the task of the linear
system problem (LSP) consists of ﬁnding x ∈CM such
Depending on the dimensions M and N, the LSP takes
various forms. If M = N and A is invertible, x = A−1b is
unique. If M ≠N, the LSP can be under-determined or
over-determined. For the sake of simplicity, it is natural
to assume the matrix A to be square i.e. M = N. If the
matrix A has at most s non-zero elements per row or
column, the LSP is called s-sparse.
The quantum version of the LSP, known as the quantum linear system problem (QLSP), assumes A to be
N × N Hermitian matrix and b to be a unit vector, i.e.
it can be represented as a quantum state ∣b⟩= ∑N
i=1 bi∣ei⟩.
The QLSP problem thus is formulated as
A∣x⟩= ∣b⟩→∣x⟩= A†∣b⟩.
The ﬁrst quantum algorithm proposed for solving the
QLSP was the famous Harrow-Hassidim-Lloyd (HHL) algorithm . Apart from the size of the
matrix A, i.e. N, and its sparsity s, two other dominant
factors determining the running time of a LSP or QLSP
algorithm are the condition number (κ) of the matrix A
and the additive error (ϵ) corresponding to the solution.
The condition number of a matrix A is given by ratio of
maximal and minimal singular values of A. The best classical algorithm for LSP is the conjugate gradient method
with runtime complexity O (Nsκlog ( 1
ϵ )). On the other
hand, the HHL algorithm for QLSP, as originally proposed, has runtime complexity O (log (N)s2 κ2
ϵ ). Further works on the HHL algorithm has improved κ scaling to linear and error dependence to
poly (log ( 1
ϵ )) . Implementation of
HHL algorithm, however, requires the fault-tolerant architecture and hence its guarantees can not be leveraged
on the NISQ architecture. The largest QLSP solved on
a gate based quantum computer corresponds to its implementation on an nuclear magnetic resonance (NMR)
processor for N = 8 .
Recently, a few VQA based implementations of the
QLSP were proposed . Given a QLSP with input A and ∣b⟩, the idea is to ﬁnd the ground state of the
following Hamiltonian,
H(u) = A(u)P ⊥
where A(u) and P ⊥
+,b are deﬁned as
A(u) ≡(1 −u)σz ⊗I + u σx ⊗A,
+,b = I −∣+,b⟩⟨b,+∣.
Both A and ∣b⟩are assumed to be constructed eﬃciently
with a quantum circuit, i.e.
k=1 βkUk and ∣b⟩=
Ub∣0⟩, with KA = O (poly (log N)). The phase in βk can
be absorbed in Uk and hence one can assume βk > 0.
The Hamiltonian in Eq. (115) for u = 1, has a unique
ground state, ∣+⟩∣x⋆⟩= ∣+⟩
∥A−1∣b⟩∥2 , with zero ground
state energy. After removing the ancilla, the ground state
can be seen to be proportional to A−1∣b⟩. Thus, one can
deﬁne the following loss function,
LH (∣x⟩) = ⟨+,x∣H(1)∣+,x⟩.
Without the ancilla, the above loss function can be written as LH (∣x⟩) = ⟨x∣A2∣x⟩−⟨x∣A∣b⟩⟨b∣A∣x⟩.
In , authors analyze the optimization landscape for VQA based optimization for the loss
function of Eq. (118) and show the presence of barren plateaus which persist independent of the architecture of the quantum circuit for generating ∣x(θ)⟩.
Even techniques based on adiabatic morphing fail to evade the eﬀect of the
barren plateaus.
To circumvent the barren plateau
 proposed a classicalquantum hybrid state (see also Sec. III.E and Eq. (42))
i=1 αi∣ψi (θi)⟩, where αi ∈C and θi ∈Rki for
i ∈{1,2,⋯,r}. Note that θi are the usual variational parameters and αi are the combination parameters. These
parameters are stored on a classical device and the state
x is not explicitly created on a quantum processor. Moreover, x may not be normalized. To solve the QLSP, one
minimizes the following loss function,
LR (x) = ∥Ax −∣b⟩∥2
2 = x†A†Ax −2Re{⟨b∣Ax} + 1. (119)
Since optimization with respect to θi suﬀers from the
barren plateau problem, one can ﬁx and subsequently
drops the variational parameter θi.
optimization
(α1,α2,⋯,αr). Starting from ∣ψ1⟩= ∣b⟩, other quantum
states can be generated using the Ansatz tree approach
in .
It was proved that ﬁnding
the combination parameters of ∣ψ1⟩,∣ψ2⟩,⋯,∣ψr⟩to minimize LR (∑r
i=1 αi∣ψi⟩) is BQP complete. Moreover, using
ϵ ) measurements, one can ﬁnd ϵ-suboptimal solution.
With this approach, linear systems as high as
2300 × 2300 can be solved by considering cases which are
also classically tractable.
4. Non-linear diﬀerential equations
Nonlinear diﬀerential equations (NLDE) are a system
of diﬀerential equations (DE) that cannot be expressed
as a linear system. The numerical approaches to tackle
DE can be local or global. Local methods employ numerical diﬀerentiation techniques such
as the Runge-Kutta or discretization of the space of variables. Global methods, on the other hand, represent the
solution via a suitable basis set, and the goal remains
to ﬁnd optimal coeﬃcients .
In many cases, as the number of variables or nonlinearity in the diﬀerential equations increase, ﬁnding solutions
becomes challenging. To achieve higher accuracy, local
methods require a ﬁne grid, which renders high computational cost. In the case of global methods, high accuracy necessitates a large number of elements in the basis
set, leading to more extensive resource requirements. To
tackle resource challenges, quantum algorithms are proposed.
Linear DE can be re-expressed as a system of linear
equations using the ﬁnite diﬀerence method, and one can
employ NISQ linear system algorithms to tackle the problem (see Sec. VI.D.3). For a recent theoretical proposal
with experimental work on linear diﬀerential equations,
 .
However NLDE defy this
approach for large nonlinearities.
A canonical example of a NLDE appearing in quantum theory is the 1-D nonlinear Schrödinger equation
dx2 + V (x) + g ∣f(x)∣2]f(x) = Ef(x).
Here, E denotes energy, g quantiﬁes nonlinearity, and V is the external potential. Recently, NISQ algorithms for NLDE
have been proposed. Ref. use ancillary quantum registers and controlled-multiqubit operations to implement nonlinearities to simulate the nonlinear Schrodinger equation.
Ref. propose the nonlinear quantum assisted simulator
(NLQAS) to tackle NLDE without any controlled unitaries. Using NLQAS, they simulate this equation for 8
qubit system. NLDE have also been studied in for ﬂuid dynamics problems. Using diﬀerentiable
quantum circuits, have also proposed an interesting approach to solving NLDE via global
E. Other applications
In this subsection, we cover other applications for
which NISQ algorithms can provide promising improvements. They are listed in Table VI.
1. Quantum foundations
One of the ﬁrst experiments in digital quantum computers were the Bell nonlocality tests known as Bell inequalities .
Those experiments
computed a type of Bell inequalities known as Mermin inequalities in up to ﬁve superconducting quantum qubits.
The experiment consisted in preparing the GHZ state
 , measure it in a particular basis state and obtain the expectation value of the Mermin
operator . These nonlocality
tests can be extended to higher dimensions by controlling quantum levels beyond the ∣0⟩and ∣1⟩. As example,
 experimentally generate a
qutrit GHZ state using a programmable device controlled
with Qiskit Pulse software , the
ﬁrst step towards performing a GHZ test.
In the context of VQA, the non-classicality in VQEs is
examined using contextuality, which is a nonclassical feature of quantum theory . Using the notion of “strong contextuality”, categorized VQE experiments into two categories:
contextual and non-contextual. Such foundational works
could be utilized to comprehend the possible sources of
quantum advantage in NISQ algorithms.
Using novel
concepts from this ﬁeld, contextual subspace VQE (CS-
VQE) was recently proposed .
In another work, the variational consistent history
(VCH) algorithm was suggested to investigate foundational questions .
The consistent history approach has been used to examine topics
from quantum cosmology and quantum-classical transition. In the VCH algorithm, the quantum computer is
used to compute the “decoherence functional”, which is
challenging to calculate classically.
The classical com-
puter is employed to tune the history parameter so that
the consistency is improved.
2. Quantum optimal control
Quantum optimal control is a topic of paramount importance in the pursuit to harness the potential of Near-
Term quantum devices.
For a given quantum control
system and a cost function that measures the quality of
control, it aims to ﬁnd a control that can achieve optimal
performance.
Some recent works have investigated quantum optimal
control in the NISQ framework. Recent detailed perspective in this direction can be found in Ref. . Ref. provides a hybrid quantumclassical approach to quantum optimal control. To remedy some of the diﬃculties of classical approaches to optimal control related to scaling of resources, proposed another NISQ framework. Experimental
demonstration of quantum control for a 12-qubit system
has also been carried on . The aforementioned approaches, however, restrict their target states to
be sparse matrices. For dense target states, recently proposed a NISQ
algorithm.
Along with their algorithm, they also suggested a few algorithmic primitives to calculate overlap of
quantum states and transition matrix elements. Hybrid
quantum-classical algorithm have also been implemneted
for computing quantum optimal control pulses, in particular for controlling molecular systems .
3. Quantum metrology
Quantum metrology harnesses non-classical features
of quantum theory for parameter estimation tasks.
canonical example could be estimating the parameter φ
of a unitary map under the action of Hamiltonian ˆH,
given by ˆρ(φ) = e−i ˆ
Hφˆρ0e+i ˆ
Hφwhere the density matrix
ˆρ0 refers to the initial state of the system. The goal is
to estimate φ via measurements on ˆρ(φ). The quantum
Cramér-Rao bound provides a lower bound to the achievable precision,
nFQ (ˆρ(φ)).
Here, n represents number of samples, FQ (ˆρ(φ)) is quantum Fisher information and (∆φ)2 is the variance in the
estimation of φ. In most of the experiments, the parameter of interest is either temperature or magnetic ﬁeld.
Notice that the precision of the estimation procedure
increases as the quantum Fisher information increases.
Using it as a cost function, a few works have recently
explored quantum metrology to prepare a better probe
state in a VQA set-up . In addition, provided a toolbox for multiparameter estimation and the
natural PQC with the lowest possible quantum Cramér-
Rao bound for a general class of circuits.
4. Fidelity estimation
In Sec. II.A, we discussed how to use the ﬁdelity as
an objective function, a quantity which is useful to train
some VQA algorithms.
In addition, estimating the ﬁdelity of a quantum state with respect to another state
has a general interest in the context of quantum computing. For this reason, algorithms that can estimate this
property may become useful in the NISQ era.
Given the density matrices of two quantum states ρ1
and ρ2, their ﬁdelity is given by
F (ρ1,ρ2) = (Tr
Due to the large dimensionality of the Hilbert spaces,
computing ﬁdelity can be challenging.
variational
estimation
(VQFE) algorithm was proposed to tackle slightly modi-
ﬁed version of the ﬁdelity estimation task which works ef-
ﬁciently when ρ1 has low-rank. Ref. 
provide lower and upper bounds on F (ρ1,ρ2) via VQFE.
The algorithm calculate ﬁdelity between ρn
1, which is a
truncated version of ρ1 obtained by projecting ρ1 to subspace spanned by n largest eigenvalue eigenvectors of ρ1.
The bounds improve monotonically with n and is exact
for n = rank (ρ1). The VQFE algorithm proceeds in three
steps: i) a variational diagonalization of ρ1; ii) the matrix elements of ρ2 are computed in the eigenbasis of ρ1;
and iii) using the matrix elements from ii), the ﬁdelity
is estimated.
5. Quantum error correction
The leading error correction schemes carry high resource overheads, which renders them impractical for
near-term devices . Moreover, many of the schemes mandate knowledge of the underlying noise model . For an encoding process E, decoding process D and noise model
N, the quality of a quantum error correction scheme can
be characterized by how close D◦N◦E is close to identity.
The range of E is called code space C.
In , a variational error-correcting
scheme i.e, quantum variational error corrector (QVEC-
TOR) was proposed by deﬁning an objective function
over the code space C. The authors employ two trainable
parametric quantum circuits V (p) and W (q) for encoding and decoding respectively, with tunable parameter
vectors p and q.
For a given encoding-decoding pair,
characterized by (p,q), the authors calculate a quantity
called “average code ﬁdelity” with respect to Haar distribution of states over the code space C. The algorithm
is model-free, i.e. no assumption of the noise model is
The goal of the QVECTOR algorithm is to
maximize average code ﬁdelity in a variational set-up.
In the context of VQA, error correction has also been
explored in where the target logical states are encoded as ground state of appropriate
Hamiltonian. employ imaginary time
evolution to ﬁnd the ground state. The authors implement there scheme for ﬁve and seven qubit codes. For a
brief discussion on error correction and quantum faulttolerance, refer to Sec. VIII.B.
6. Nuclear physics
The Standard Model of particle physics is the theory
that describes the nature of the electromagnetic and nuclear interactions. Its current formulation consist of describing the forces as quantum ﬁelds, i.e. by using quantum ﬁeld theory (QFT) formalism. Perturbative calculations of QFT provide with the dynamics of the physical processes at a given energy scale. However, in some
cases as in quantum chromodynamics (QCD), perturbation theory can not be applied because the impossibility
of observe a free quark or gluon (the fundamental particles aﬀected by QCD interaction) due to conﬁnement.
For this reason, QCD calculations are obtained by means
of numerical methods such as Monte Carlo simulations
in a discretized version of QFT on a lattice structure
(LQFT). The high computational cost of LQFT has motivated the study of using quantum computation or simulation to obtain the desired QCD predictions (Joó et al.,
The Schwinger model describes the dynamics of the
quantum electromagnetic (QED) interaction in one spatial and temporal dimensions. It is used as a toy model to
study QCD since it shows fermion conﬁnement but it is
simple enough to be solved analytically. The ﬁrst experimental quantum simulations of this model were carried
out in trapped ions and later on
a superconducting circuit quantum computer . A ﬁrst proposal to use a quantum-classical
algorithm to simulate this model was presented in , where the quantum computer simulates the
dynamics of the symmetry sectors suggested by a classical computation. In Ref. a VQS
is used in an analog setup to reduce the number of variational parameters and thus, reduce the computational
cost of the algorithm. Their proposal is experimentally
implemented in a trapped-ion analog simulator. A signiﬁcant reduction of the computational cost of LQFT is
proposed in by using a VQA approach to compute the optimized interpolating operators
(approximators of the quantum state wavefunction).
Adaptations of the UCC quantum chemistry ansatz,
introduced in Sec. II.B.1, to study quantum-variationally
QCD are presented in , and for the study neutrino-nucleus scattering
in . A 10-qubit VQC is used in to study Higgs boson decays and production
processes and in a QCNN model is
proposed to study basic high-energy processes. Recently,
a PQC is used to learn the parton distribution function
of protons .
7. Entanglement properties
Entanglement is a resource for numerous quantum
information tasks.
A bipartite quantum state ρAB ∈
HA ⊗HB is called separable if it admits the form ρAB =
i , where pi are non-negative and ∑i pi = 1.
If a state is not separable, then it is called entangled.
The problem of detecting whether a state is separable or
entangled is known as the separability problem and has
been shown to be NP-hard .
As mentioned in Sec. VI.D.2, computing the Schmidt
rank of ρAB gives a measure of the bipartite entanglement. Thus, those algorithms that tackle the SVD problem can also be used to extract entanglement properties . In ,
authors propose a NISQ algorithm for the separability
problem by providing a variational approach to employ
the positive map criterion. This criterion establishes that
the quantum state ρAB is separable if and only if for arbitrary quantum system R and arbitrary positive map
NB→R from B to R, we have NB→R (ρAB) ≥0. The authors start with a positive map and decompose it into
a linear combination of NISQ implementable operations.
These operations are executed on the target state, and
the minimal eigenvalue of the ﬁnal state is variationally
estimated. The target state is deemed entangled if the
optimized minimal eigenvalue is negative.
Exploring a similar strategy as the one presented in
 , 
propose a VQA to compute the tangle, a measure of
tripartite-entanglement.
VQA have also been employed for extracting the entanglement spectrum of quantum systems in .
VII. BENCHMARKING
One of the central questions at the intersection of software and hardware for NISQ devices is evaluating de-
vices’ performance and capabilities. This is where benchmarking concepts come in, to provide various metrics
that attempt to measure diﬀerent machines’ capabilities
and compare them across time and other devices.
benchmarking protocol can be characterized by its inherent assumptions, resource costs and the information
gain. The goal is to build benchmarking protocols that
make minimal and practical assumptions, have low resource costs, and have high information-gain.
Benchmarking protocols have been developed for NISQ
as well as fault-tolerant devices.
For a pedagogical
summary, refer to .
In this review, we focus on quantum benchmarking protocols for
NISQ devices.
Some of the leading NISQ benchmarking schemes are randomized benchmarking, quantum volume, cross-entropy benchmarking and application-based
benchmarks.
A. Randomized benchmarking
The most straightforward way of comparing devices is
by simply counting qubits. To really compare diﬀerent
qubits, we must also have a sense of how many operations we can do with them before the noise arising from
errors drowns out the signal. Randomized benchmarking
(RB) is a convenient method for ﬁnding average error
rates for quantum operations, in particular for single and
two-qubit gates . RB is robust against state preparation and measurement (SPAM)
errors and, unlike tomography, admits an eﬃcient and
practical implementation.
RB involves the following assumptions: i) for every
gate, the incurred noise is independent of other Cliﬀord
gates; ii) the involved unitaries should constitute a 2design (see Sec. IV.A) and should not be universal. In
other words, no T gate is allowed; iii) during the experiment, there is no drifting in the noise processes; and iv)
one can describe noise processes using completely positive trace-preserving (CPTP) maps.
A RB protocol starts by sampling a sequence of m
Cliﬀord gates (see Sec. V.B.1). The sequence is applied
to the initial state, followed by its inverse.
two-outcome POVM measurement is done to calculate
the ﬁdelity between initial state and the output state,
followed by classical post-processing. The RB protocol
discretizes time so that it is measured in the number of
gates and it then averages over many sequences of each
length m. More formally, a 4-step RB protocol consist of
1. Generate Km sequences of m quantum operations
Cij with i ∈[1,m] and j ∈[1,Km], where i indexes over the sequence of operations, and j over
the statistical samples. These operations are randomly chosen from the Cliﬀord group, and a m+1th operation is chosen that cancels the ﬁrst m operations such that the net operation is the identity.
The operations can be chosen from the 2-,4- or 2ndimensional Cliﬀord groups, depending on whether
we are benchmarking single-, two- or n-qubit operations .
These operations
come with some error, which is modeled with linear operators Λij,j, so that the full sequence of m
operations is given by
SKm = ◯m+1
j=1 (Λij,j ○Cij)
Here, ○denotes composition and ◯represents composition of the terms deﬁned with index j.
2. For each sequence we ﬁnd the ﬁdelity with the initial state by measuring Tr[EψSKm(ρ(ψ))], where
ρ(ψ) is the initial state (with preparation errors)
and Eψ is a POVM measurement operator corresponding to the measurement including noise.
Without noise, this would be the projector Eψ =
3. Average over the Km statistical samples to ﬁnd
the sequence ﬁdelity F(m,ψ) = Tr[EψSm(ρ(ψ))]
where Sm is the mean over the operations SKm.
4. Fit the data with the function
Ffit(m,ψ) = A0pm + B0,
where we have assumed the errors are independent
of gate and time. This is not a fundamental assumption, but can be relaxed /2n, and the constants A0 and
B0 absorb the SPAM errors.
The operations Cij are chosen from the Cliﬀord group,
because these are relatively easy to perform on quantum
hardware, and because the ﬁnal m + 1-th operation that
undoes the sequence can easily be pre-computed on a
classical computer. Averaging over the Cliﬀord group (or
any other ﬁnite group) also has the property that even
though the real noise-channel would be more complicated
than the purely depolarizing one, the average over the
group will still give rise to an exponential decay.
These gate errors extracted from randomized benchmarking can be used to compare the quality of quantum
gates, and to estimate that an algorithm of depth ∼1/ϵRB
gates can be run on the device before only statistical noise
is output. The intuition behind the RB protocol is that a
(purely) depolarizing channel will cause exponential decay of an excited state over time.
Simultaneous randomized benchmarking (SRB) has
been proposed to acquire information about crosstalk
neighbouring
qubits .
RB has also been
extended for gatesets that do not form a Cliﬀord
group . In such cases, the expression for
Ffit(m,ψ) does not follow equation Eq. (123) .
Employing concepts from representation
theory, an extension of RB has been proposed to extract
the ﬁdelity for a broad category of gatesets, including Tgate . A practically scalable protocol
called cycle benchmarking was developed lately to characterize local and global errors for multi-qubit quantum
computers .
B. Quantum volume
To further reﬁne the concept of the computational
power of a quantum computer from just qubit count
and gate-errors, the IBM Quantum team introduced the
“quantum volume” .
It is one of the widely accepted metrics for benchmarking
NISQ-era quantum computers. As mentioned earlier, one
can not rank quantum computers based on the number of
qubits alone. Quantum volume gives a rough estimate of
the number of eﬀective qubits a quantum computer has
based on their performance on the “heavy output generation problem”. The heavy output generation problem
is related to the random circuit sampling task used in
Google’s quantum supremacy experiment. Quantum volume treats the depth and width of a quantum circuit at
the equal footing. Hence, its estimation depends on the
largest square-sized circuit, which can successfully implement the heavy output generation problem. The quantum computer’s performance also depends on its software
stack, for example, compiler, and thus quantum volume
can increase with the improvements in the software stack.
The quantum volume benchmark can be thought analogous to the classical LINPACK benchmark . Like the LINPACK benchmark, it is architectureagnostic and provides a single real number metric based
on the quantum computer’s performance for a model
problem, i.e., heavy output generation problem.
More formally, quantum volume can be deﬁned in the
following terms. Given an n qubit quantum computer
with the largest achievable model circuit depth d(m) for
model circuit width m ∈{1,2,⋯,n} such that the probability of observing a heavy output for a random selection
of model circuit is strictly greater than 2/3, the quantum
volume VQ is deﬁned as 
log2 VQ = argmax
min(m,d(m)).
Intuitively speaking, quantum volume estimates the
largest square random quantum circuit which the quantum computer can successfully implement the so-called
heavy output generation problem. To conclude the discussion, it remains to describe the “model circuit” and
the “heavy output generation problem”.
The model circuit with depth d and width m for estimating quantum volume is given by d-layered sequence
U = U (d)U (d−1)⋯U (1) where layer t consists of random
permutations πt ∈Sm applied to qubit labels, followed by
the tensor product of Haar-random two-qubit unitaries
from SU(4). If the model circuit width m is odd, one of
the qubits is left idle in every layer. See Figure 7 for a
pictorial description.
Figure 7 Model circuit for the quantum volume benchmark.
Each layer consists of random permutations of qubit labels,
followed by application of two-qubit haar-random unitaries.
Inspired by .
Given a model circuit U with width m, the ideal output distribution over bit strings x ∈{0,1}m is given
byPU(x) = ∣⟨x∣U∣0⟩∣2.
One can arrange the probabilities for various bitstrings
in ascending order in a set P = {p0 ≤p ≤⋯≤p2m−1}.
p2m−1+p2m−1−1
. The Heavy outputs are deﬁned as HU =
{x ∈{0,1}m ∣pU(x) > pmed}. The goal of the heavy output problem is to sample a set of strings such that at least
2/3 are heavy output. For an ideal quantum circuit, the
expected heavy output probability asymoptotically tends
to ∼0.85. For a completely depolarized device, it is ∼0.5.
On the target system, one implements ˜U by using
a circuit compiler with native gate set such that 1 −
Favg (U, ˜U) ≤ϵ ≤1 for some approximation error ϵ, where
Favg is average gate ﬁdelity, as deﬁned in Ref. . The role of circuit compiler is crucial in the
aforementioned step. Suppose the observed distribution
for the implemented circuit ˜U of the model circuit U is
gu(x). The probability of sampling heavy output is given
For a randomly selected circuit of depth d, the probability
of sampling a heavy output is given by
hd = ∫U hUdU.
The term d(m) in Eq. (124) is equal to the largest depth
d for model circuit of width m ∈{1,2,⋯,n} such that
The quantum volume benchmark requires simulation
of the model circuit’s heavy output generation problem
on a classical computer.
It, hence, is not a scalable
method as the quantum volume increases.
the special treatment for square circuits is not entirely
justiﬁed. Investigations are needed to devise other interesting benchmarks. A benchmark for rectangular circuits
has also been proposed in the literature .
At the time of writing, Honeywell’s system model
H1 has achieved log2 VQ = 9 , and
IBM quantum device named “IBM Montreal” has demonstrated log2 VQ = 6 .
C. Cross-entropy benchmarking
The linear cross-entropy benchmarking is a statistical
test used by Google in their quantum supremacy experiment . It measures
how often high-probability bitstrings are sampled in an
experimental scenario. Suppose we perform a sampling
task and obtain bitstrings {xj}j via measurement on a
given m-qubit circuit CE. The linear cross-entropy benchmarking ﬁdelity is given by
FXEB = 2m ⟨P (xj)⟩j −1.
Here, the average ⟨.⟩j is over the experimentally observed
bitstrings {xj}j and P (xj) denotes the probability of observing bitstring xj for the ideal circuit version of CE. In
other words, P (xj) denotes the ideal probability of the
generated sample xj. Since one can not have an ideal circuit in practice, P (xj) are calculated using a classical
computer simulation of the ideal circuit.
FXEB compares how often a bitstring xj is observed experimentally
with its classically simulated ideal probability. For the
ideal case, FXEB approaches unity for a large number of
qubits. On the other hand, it is equal to zero for uniform
distribution. As the circuit’s nose grows, FXEB decreases
and approaches zero. Since the probabilities P (xj) are
calculated via classical simulation; it renders the computation of FXEB intractable in the supremacy regime. The
classical hardness of spooﬁng linear cross-entropy benchmarking was studied by Aaranson and Gunn , where they suggested the absence of
any eﬃcient classical algorithm for the aforementioned
D. Application benchmarks
While hardware benchmarks,
such as randomized
benchmarking or quantum volume, provide valuable insight into the performance of quantum devices, they may
not well represent or predict the performance of VQAs
which employ structured circuits.
Application benchmarks were developed to complement hardware benchmarks and provide a more complete picture of both
the performance and (near-term) utility of quantum devices. These benchmarks consist executing experimental demonstrations of VQA instances that can be compared to classically computed exact results. Examples
of application benchmarks can be found in Refs. .
In particular,
 demonstrated VQE experiments for
hydrogen chain binding energy and diazene isomerization mechanism with PQC sizes as big as 12 qubits and
72 two-qubit gates.
As a speciﬁc example of an application benchmark,
quantum circuits that diagonalize spin Hamiltonians
have been proposed in recent years .
comparing the results obtained from the quantum device
with the analytical solution, one can discern the performance of the computation for a speciﬁc purpose experiment. Small experiments have shown that gate ﬁdelities
and decoherence times alone do not provide a complete
picture of the noise model .
In that direction, authors of proposed a ﬁgure-of-merit called the eﬀective
fermionic length to quantify the performance of a NISQ
device in which the application-at-hand is estimating the
energy density of the one-dimensional Fermi-Hubbard
model over increasing chain lengths.
Theoretically, as
the chain length increases, the energy density should approach the inﬁnite chain limit.
In practice, the NISQ
device will accrue some level of noise and decoherence,
which will cause the computed energy density to diverge
past some chain length. The maximum chain length after which noise and decoherence start degrading the algorithm performance reveals the “limit” of the quantum
device in carrying out related algorithms. Ref. abstracts this idea to redeﬁne an
application benchmark as a way to systematically test
the limits of a quantum processor using exactly solvable
VQA instances that can also be scaled to larger system
sizes or number of preprocessing steps in Ref. ).
Generative models such as the QCBM (see Sec. VI.B.2)
can serve as benchmarks for NISQ devices .
Here, the measurement output of hardware eﬃcient variational ansäte are used to represent diﬀerent types of
distributions and study the eﬀect of noise and hardware
limitations on the result.
In addition to VQAs, one can analyze more fundamental benchmarks, such as the ability of NISQ devices
to violate local-realism by means of Mermin inequalities
 or the entanglement power
of the devices by trying to construct maximal entangled
states .
VIII. OUTLOOK
In the last decade, quantum computing has experienced notable progress in applications, experimental
demonstrations, and theoretical results. The number of
papers in quantum computation, particularly in NISQ
applications, is growing almost exponentially.
reasons explain this community drive, one of those being
tremendous improvements in quantum hardware. Quantum computing is a relatively young ﬁeld in science and,
as such, there is plenty of room for pioneering research
and discoveries.
This fact, together with the theoretical, practical, and experimental challenges (several of
them covered in this review), has strengthened the motivation for an open-source strategy in the ﬁeld. Nowadays, many universities and research centers subscribe
to an open-access policy that pushes towards the free
and open-source publication of all computational tools,
data, and programs used in their research. These policies
have proved valuable for rapid scientiﬁc development as
well as for democratizing community knowledge.
way of thinking has percolated through academia walls
and it has been introduced into several private companies, not just for its advantage, but also because it facilitates the continuous healthy ﬂow of quantum computing researchers to themselves (and, in some cases, resulting in foundation of startups). Consequently, there
is a rich open quantum computing ecosystem composed
of universities, institutes, big corporations, startups, and
uncountable individual enthusiasts. Another product of
the symbiosis between academia and the private sector
is cloud quantum computing. Companies are oﬀering access to their hardware remotely, in some cases at zerocost for their small prototypes and simulators. On the
one hand, scientists and quantum computing enthusiasts
around the world have the opportunity to experience real
quantum devices from their homes. On the other hand,
this increases the chances of ﬁnding real-world applications in quantum computation and solving the current
challenges of this ﬁeld. The proliferation of open-source
quantum computing languages, simulators, and tools (detailed in Sec. V.C) have burgeoned many user communities. Various international initiatives have been set up
to attract quantum computing talent, and the private
sector’s involvement is ramping up. Several non-proﬁt
initiatives are also encouraging the use and development
of these tools .
Experimental realizations of quantum computation, although in the early stages, have interested many communities in this quantum information subﬁeld. Healthy
competition has also arisen between the classical and
quantum computing branches. Classical computational
scientists have put their eﬀorts into moving the quantum
advantage frontier further, raising the bar to claim that a
quantum algorithm shows a signiﬁcant speed-up. Along
that direction, an oﬀ-shoot is an eﬀort in dequantization,
ﬁrst exhibited in the case of recommendation-systems,
to devise quantum-inspired classical algorithms that are
nearly as fast as their quantum counterparts . Such attempts have eliminated examples of speedup for some problems in linear algebra.
So far, dequantized machine learning algorithms have been developed for recommendation systems , principal component analysis and supervised clustering , stochastic regression and lowrank linear systems will be required to ﬁnd out how much noise
a NISQ algorithm can endure until its classical simulation becomes eﬃcient. This is crucial in order to understand the boundary where quantum computers provide
an advantage.
Investigating the potential of NISQ algorithms using ideas from quantum foundations such as
contextuality and entanglement are helpful in that respect . More theoretical results as the ones presented in may also
prove valuable. It is also imperative to develop strategies
that help us bypass complicated measurements involving controlled multi-qubit unitaries . For machine learning tasks, ideas similar to would be valuable.
Another fascinating frontier that needs to be investigated in the next few years, we believe, is quantum
and classical certiﬁcation schemes for quantum devices
and quantum computation . The intractability of quantum computation by classical devices
poses the challenge to verify the correct functioning of
the quantum devices as well as the correctness of the ﬁnal output . The existence of multiple
quantum computing platforms requires new methodologies and ﬁgures of merit to benchmark and compare these
devices. Other works are being proposed in that direction , as well
as the development of benchmarking measures discussed
in Sec. VII. Ideas from complexity theory and quantum foundations could be valuable in this
direction.
At the moment of documenting this review, there is no
known demonstration of industrially relevant quantum
advantage. Quantum computing is still in its early days,
and so far a useful quantum computer is missing. The
potential of NISQ devices is not fully understood, and a
lot of rigorous research is required to release the power of
the early quantum computers. However, a number of experiments overcoming classical computational resources
have been performed and many theoretical and practical tools are being used and developed, as explained in
A. NISQ goals
We expect experimental pursuit in the NISQ era would
focus on the design of quantum hardware with a larger
number of qubits, and gates with lower error rates capable of executing deeper circuits. Along the way, one of
the goals is to demonstrate quantum advantage for practical use cases. If the NISQ paradigm is not powerful
enough to exhibit any quantum advantage, theoretical
pursuits would be required to understand its limitations.
The prime direction of the NISQ and near-term era is
to engineer the best possible solution with the limited
quantum resources available. The tools and techniques
invented during this period could be valuable in the faulttolerant era as well.
To conduct a successful demonstration of quantum advantage, the right blend of the following three crucial
components is required:
1. Hardware development:
The design of quantum
computers with more qubits, lesser error rates,
longer coherence times, and more connectivity between the qubits will be one of the top priorities
in the NISQ era. Intensive research in new qubits
developments, quantum optimal control and material discovery will be indispensable for both universal programmable quantum computers or specialpurpose ones.
A way to scale up the number of
qubits present in a quantum platform is to design
a novel qubit which has built-in autonomous quantum error correction down to the hardware level
 
or protected novel qubit which is robust
against speciﬁc noises in the hardware. As a quantum processor size grows, there is a tremendous
need to store quantum information during quantum information processing can be seen as a mean
to scale up the quantum platform although it has
nothing to do with novel qubit design.
2. Algorithm design: To harness the potential of noisy
but powerful quantum devices, we expect breakthroughs on the algorithm frontier.
Algorithms
with realistic assumptions, as the ones mentioned
in Sec. V.B, regarding device capabilities will be
favored. To lessen the eﬀect of noise, progress towards the design of error mitigation algorithms is
expected. Eﬀorts have to be made to develop algorithms that harness the problem’s structure in the
best possible manner and map it to the given hardware in eﬃcient ways, such as in Sec. III.D. VQA
with better expressibility and trainability will also
be helpful.
3. Application problem: We have discussed the existing applications of NISQ devices in many areas in
Sec. VI. Collaborations between experts with domain knowledge from these ﬁelds and quantum algorithm researchers will be required more and more
to develop the ﬁeld and integrate quantum computation into industrial workﬂows.
New collaborations might reveal diﬃcult problems for classical
computers that are well suited for NISQ devices. It
is not clear yet which applications will be the ﬁrst
ones to witness quantum advantage, though there
is plenty of speculation and opinions.
B. Long-term goal: fault-tolerant quantum computing
Noise is regarded as one of the most prominent threats
to a quantum computer’s practical realization. In 1995,
Peter Shor established that by encoding quantum information redundantly using extra qubits, one could circumvent the eﬀect of noise . The quantum information is spread over multiple physical qubits to generate
a logical qubit . Most of the
transformative algorithms such as Shor’s factoring algorithm, Grover search algorithm, and HHL require errorcorrected qubits for their execution. Soon after Shor’s
error-correcting code, many others were developed. Some
of the famous error-correcting codes are stabilizer and
topological error-correcting codes .
While the stabilizer code utilizes extra
qubits to protect the logical qubit, topological codes employ a set of qubits positioned on a surface, such as a
torus, in a lattice structure.
Over the years, quantum error correction has evolved
as a subﬁeld of quantum computation and has transformed from a theoretical pursuit to a practical possibility. The process of detecting and correcting errors can
be, itself, prone to noise.
Thus error correction alone
does not guarantee the prospect of storing or processing
quantum information for an arbitrarily long period. The
aforesaid issue can be tackled by utilizing the Quantum
Fault-Tolerant threshold theorem. Informally speaking, it
is possible to execute arbitrarily large quantum computation by arbitrarily suppressing the quantum error rate,
given the noise in the individual quantum gates are below
a certain threshold . If one
wants to simulate an ideal circuit of size N, the size of the
noisy quantum circuit for fault-tolerant quantum computation scales O (N (log N)c), for some constant c, given
the noisy circuit is subjected to stochastic noise strength
p < pc for some noise threshold pc . This
theorem rises some practically relevant questions such as
i) How high is pc; ii) what is the value of the constant c;
and iii) what is the value of the multiplicative constant
in O (.). These questions determine the practicality of
any fault-tolerant quantum computation scheme . Quantum error-correcting
codes amenable to architectures with limited qubit connectivity have also been proposed . As we transition towards fault-tolerant quantum
computing, partial quantum error correction demonstrations such as exponential suppression of bit or phase errors and approximate quantum error correction schemes 
become highly relevant. Recently, Monroe and Brown’s
groups have conﬁrmed the ﬁrst-ever fault-tolerant operation on a logical qubit .
We are at an exciting juncture in the history of computing. Completely new kinds of computers that were
once only ﬁgments of imagination are rapidly becoming
a reality. The NISQ era oﬀers fantastic opportunities to
current and future researchers to explore the theoretical
limits of these devices and discover practical and exciting
applications in the near-term. Theoretical investigations
and experimental challenges will help us to comprehend
quantum devices power and build better algorithms. The
success of the ﬁeld lies in the hands of the researchers and
practitioners of the area, so we encourage everyone with
interest to join the eﬀort.
ACKNOWLEDGEMENTS
A.A.-G. acknowledges the generous support from
Google, Inc.
in the form of a Google Focused Award.
This work was supported by the U.S. Department of
Energy under Award No.
DESC0019374 and the U.S.
Oﬃce of Naval Research (ONS506661).
A.A.-G. also
acknowledges support from the Canada Industrial Research Chairs Program and the Canada 150 Research
Chairs Program. T.H. is supported by a Samsung GRC
project and the UK Hub in Quantum Computing and
Simulation, part of the UK National Quantum Technologies Programme with funding from UKRI EPSRC
grant EP/T001062/1. L.-C.K and K.B acknowledge the
ﬁnancial support from the National Research Foundation
and the Ministry of Education, Singapore.
Michael Biercuk, Naresh Boddu, Zhenyu Cai, Sam Gutmann, Edward Farhi, Rahul Jain, Dax Koh, Alejandro
Perdomo-Ortiz and Mark Steudtner for interesting discussions, feedback and comments.