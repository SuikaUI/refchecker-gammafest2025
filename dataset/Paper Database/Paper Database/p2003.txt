HAL Id: hal-00642891
 
Submitted on 19 Nov 2011
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Semi-Supervised Learning with Max-Margin Graph Cuts
Branislav Kveton, Michal Valko, Ali Rahimi, Ling Huang
To cite this version:
Branislav Kveton, Michal Valko, Ali Rahimi, Ling Huang. Semi-Supervised Learning with Max-Margin
Graph Cuts. International Conference on Artificial Intelligence and Statistics, May 2010, Chia Laguna,
Sardinia, Italy. ￿hal-00642891￿
Semi-Supervised Learning with Max-Margin Graph Cuts
Branislav Kveton
Michal Valko
Ali Rahimi and Ling Huang
Intel Labs Santa Clara
University of Pittsburgh
Intel Labs Berkeley
This paper proposes a novel algorithm for semisupervised learning. This algorithm learns graph
cuts that maximize the margin with respect to the
labels induced by the harmonic function solution.
We motivate the approach, compare it to existing
work, and prove a bound on its generalization error. The quality of our solutions is evaluated on a
synthetic problem and three UCI ML repository
datasets. In most cases, we outperform manifold
regularization of support vector machines, which
is a state-of-the-art approach to semi-supervised
max-margin learning.
INTRODUCTION
Semi-supervised learning is a ﬁeld of machine learning that
studies learning from both labeled and unlabeled examples.
This learning paradigm is suitable for real-world problems,
where data is often abundant but the resources to label them
are limited. As a result, many semi-supervised learning algorithms have been proposed in the past years .
The closest to this work are semi-supervised support vector
machines (S3VMs) , manifold
regularization of support vector machines (SVMs) , and harmonic function solutions on data adjacency graphs . Manifold regularization of
SVMs essentially combines the ideas of harmonic function
solutions and semi-supervised SVMs in a single convex objective.
This paper proposes a different way of combining these two
ideas. First, we compute the harmonic function solution on
the data adjacency graph and then, we learn a discriminator,
which is conditioned on the labels induced by this solution.
We refer to our method as max-margin graph cuts because
the discriminator maximizes the margin with respect to the
inferred labels. The method has many favorable properties.
For instance, it incorporates the kernel trick ,
Appearing in Proceedings of the 13th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: W&CP 9. Copyright 2010 by the authors.
it takes advantage of sparse data adjacency matrices, and its
generalization error can be bounded. Moreover, it typically
yields better results than manifold regularization of SVMs,
especially for linear and cubic decision boundaries.
In addition to proposing a new algorithm, this paper makes
two contributions. First, we show how manifold regularization of linear and cubic SVMs fails on almost a trivial problem. Second, we show how to make the harmonic function
solution with soft labeling constraints 
The paper is organized as follows. In Section 2, we review
the harmonic function solution and discuss how to regularize it to interpolate between supervised learning on labeled
examples and semi-supervised learning on all data. In Section 3, we introduce our learning algorithm. The algorithm
is compared to existing work in Section 4 and we bound its
generalization error in Section 5. In Section 6, we evaluate
the quality of our solutions on UCI ML repository datasets,
and show that they usually outperform manifold regularization of SVMs.
The following notation is used in the paper. The symbols xi
and yi refer to the i-th data point and its label, respectively.
The data are divided into labeled and unlabeled examples, l
and u, and labels yi ∈{−1, 1} are observed for the labeled
data only. The cardinality of the labeled and unlabeled sets
is nl = |l| and nu = |u|, respectively, and the total number
of training examples is n = nl + nu.
REGULARIZED HARMONIC
FUNCTION SOLUTION
In this section, we review the harmonic function solution of
Zhu et al. . Moreover, we show how to regularize it
to interpolate between semi-supervised learning on all data
and supervised learning on labeled examples.
A standard approach to semi-supervised learning on graphs
is to minimize the quadratic objective function:
ℓi = yi for all i ∈l;
where ℓdenotes the vector of predictions, L = D −W is
the Laplacian of the data adjacency graph, which is represented by a matrix W of pairwise similarities wij, and D is
Semi-Supervised Learning with Max-Margin Graph Cuts
a diagonal matrix whose entries are given by di = P
This problem has a closed-form solution:
ℓu = (Duu −Wuu)−1Wulℓl,
which satisﬁes the harmonic property ℓi =
j∼i wijℓj,
and therefore is commonly known as the harmonic function
solution. Since the solution can be also computed as:
ℓu = (I −Puu)−1Pulℓl,
it can be viewed as a product of a random walk on the graph
W with the transition matrix P = D−1W. The probability
of moving between two arbitrary vertices i and j is wij/di,
and the walk terminates when the reached vertex is labeled.
Each element of the solution is given by:
ℓi = (I −Puu)−1
(I −Puu)−1
(I −Puu)−1
are probabilities by which the walk starting from the vertex i ends at vertices with labels 1 and −1,
respectively. Therefore, when ℓi is rewritten as |ℓi| sgn(ℓi),
|ℓi| can be interpreted as a conﬁdence of assigning the label
sgn(ℓi) to the vertex i. The maximum value of |ℓi| is 1, and
it is achieved when either p1
i = 1 or p−1
= 1. The closer
the conﬁdence |ℓi| to 0, the closer the probabilities p1
to 0.5, and the more uncertain the label sgn(ℓi).
To control the conﬁdence of labeling unlabeled examples,
we suggest regularizing the Laplacian L as L + γgI, where
γg is a scalar and I is the identity matrix. Similarly to our
original problem (1), the corresponding harmonic function
T(L + γgI)ℓ
ℓi = yi for all i ∈l
can be computed in a closed form:
ℓu = (Luu + γgI)−1Wulℓl.
It can be also interpreted as a random walk on the graph W
with an extra sink. At each step, this walk may terminate at
the sink with probability γg/(di+γg). Therefore, the scalar
γg essentially controls how the conﬁdence |ℓi| of labeling
unlabeled vertices decreases with the number of hops from
labeled vertices.
Several examples of how γg affects the regularized solution
are shown in Figure 1. When γg = 0, the solution turns into
the ordinary harmonic function solution. When γg =∞, the
conﬁdence of labeling unlabeled vertices decreases to zero.
Finally, note that our regularization corresponds to increasing all eigenvalues of the Laplacian L by γg . In Section 5, we use this property to bound the
generalization error of our solutions.
MAX-MARGIN GRAPH CUTS
Our semi-supervised learning algorithm involves two steps.
First, we obtain the regularized harmonic function solution
ℓ∗(Equation 6). The solution is computed from the system
of linear equations (Luu + γgI)ℓu = Wulℓl. This system
of linear equations is sparse when the data adjacency graph
W is sparse. Second, we learn a max-margin discriminator,
which is conditioned on the labels induced by the harmonic
solution. The optimization problem is given by:
V (f, xi, sgn(ℓ∗
i )) + γ ∥f∥2
ℓ∗= arg min
T(L + γgI)ℓ
s.t. ℓi = yi for all i ∈l;
where V (f, x, y) = max{1 −yf(x), 0} denotes the hinge
loss, f is a function from some reproducing kernel Hilbert
space (RKHS) HK, and ∥·∥K is the norm that measures the
complexity of f.
Training examples xi in our problem are selected based on
our conﬁdence into their labels. When the labels are highly
uncertain, which means that |ℓ∗
i | < ε for some small ε ≥0,
the examples are excluded from learning. Note that as the
regularizer γg increases, the values |ℓ∗
i | decrease towards 0
(Figure 1), and the ε thresholding allows for smooth interpolations between supervised learning on labeled examples
and semi-supervised learning on all data. The tradeoff between the regularization of f and the minimization of hinge
losses V (f, xi, sgn(ℓ∗
i )) is controlled by the parameter γ.
Due to the representer theorem , the optimal
solution f ∗to our problem has a special form:
i k(xi, x),
where k(·, ·) is a Mercer kernel associated with the RKHS
HK. Therefore, we can apply the kernel trick and optimize
rich classes of discriminators in a ﬁnite-dimensional space
of α = (α1, . . . , αn). Finally, note that when γg = ∞, our
solution f ∗corresponds to supervised learning with SVMs.
EXISTING WORK
Most of the existing work on semi-supervised max-margin
learning can be viewed as manifold regularization of SVMs
 or semi-supervised SVMs with the hat
loss on unlabeled data . The two
approaches are reviewed in the rest of the section.
SEMI-SUPERVISED SVMS
Semi-supervised support vector machines with the hat loss
bV (f, x) = max{1−|f(x)| , 0} on unlabeled data :
V (f, xi, yi) + γ ∥f∥2
bV (f, xi)
compute max-margin decision boundaries that avoid dense
regions of data. The hat loss makes the optimization problem non-convex. As a result, it is hard to solve the problem
optimally and most of the work in this ﬁeld has focused on
approximations. A comprehensive review of these methods
was done by Zhu .
In comparison to semi-supervised SVMs, learning of maxmargin graph cuts (7) is a convex problem. The convexity
is achieved by having a two-stage learning algorithm. First,
we infer labels of unlabeled examples using the regularized
harmonic function solution, and then, we minimize the corresponding convex losses.
MANIFOLD REGULARIZATION OF SVMS
Manifold regularization of SVMs :
V (f, xi, yi) + γ ∥f∥2
where f = (f(x1), . . . , f(xn)), computes max-margin decision boundaries that are smooth in the feature space. The
smoothness is achieved by the minimization of the regularization term f TLf. Intuitively, when two examples are close
on a manifold, the minimization of f TLf leads to assigning
the same label to both examples.
In some aspects, manifold regularization is similar to maxmargin graph cuts. In particular, note that its objective (10)
is similar to the regularized harmonic function solution (5).
Both objectives involve regularization by a manifold, f TLf
and ℓTLℓ, regularization in the space of learned parameters,
K and ℓTIℓ, and some labeling constraints V (f, xi, yi)
and ℓi = yi. Since max-margin graph cuts are learned conditionally on the harmonic function solution, the problems
(7) and (10) may sometimes have similar solutions. A necessary condition is that the regularization terms in both objectives are weighted in the same proportions, for instance,
by setting γg = γ/γu. We adopt this setting when manifold
regularization of SVMs is compared to max-margin graph
cuts in Section 6.
MANIFOLD REGULARIZATION FAILS
The major difference between manifold regularization (10)
and the regularized harmonic function solution (5) is in the
space of optimized parameters. In particular, manifold regularization is performed on a class of functions HK. When
this class is severely restricted, such as linear functions, the
minimization of f TLf may lead to results, which are significantly worse than the harmonic function solution.
This issue can be illustrated on the problem from Figure 1,
where we learn a linear decision boundary f(x) = α1x1 +
α2x2 through manifold regularization of linear SVMs:
V (f, xi, yi) + γ[α2
The structure of our problem simpliﬁes the computation of
the regularization term f TLf. In particular, since all edges
in the data adjacency graph are either horizontal or vertical,
the term f TLf can be expressed as a function of α2
wij(f(xi) −f(xj))2
wij(α1(xi1 −xj1) + α2(xi2 −xj2))2
wij(xi1 −xj1)2
wij(xi2 −xj2)2
and incorporated in our objective function as an additional
Semi-Supervised Learning with Max-Margin Graph Cuts
weight at the regularizer [α2
V (f, xi, yi) +
Thus, manifold regularization of linear SVMs on our problem can be viewed as supervised learning with linear SVMs
with a varying weight at the regularizer. Since the problem
involves only two labeled examples, changes in the weight
do not affect the direction of the discriminator
f ∗(x) = 0 and only change the slope of f ∗(Figure 2).
The above analysis shows that the discriminator f ∗(x) = 0
does not change with γu. As a result, all discriminators are
equal to the discriminator for γu = 0, which can be learned
by linear SVMs, and none of them solves our problem optimally. Max-margin graph cuts solve the problem optimally
for small values of γg (Figure 2).
A similar line of reasoning can be used to extend our results
to polynomial kernels. Figure 2 indicates that max-margin
learning with the cubic kernel exhibits similar trends to the
linear case.
THEORETICAL ANALYSIS
The notion of algorithmic stability can be used to bound the
generalization error of many learning algorithms . In this section, we discuss how to make
the harmonic function solution stable and prove a bound on
the generalization error of max-margin cuts (7). Our bound
combines existing transductive and inductive bounds.
GENERALIZATION ERROR
Our objective is to show that the risk of our solutions f:
RP (f) = EP (x)[L(f(x), y(x))]
is bounded by the empirical risk on graph-induced labels:
L(f(xi), sgn(ℓ∗
and error terms, which can be computed from training data.
The function L(y′, y) = 1{sgn(y′) = y} returns the zeroone loss of the prediction sgn(y′) given the ground truth y,
and P(x) is the distribution of our data. For simplicity, we
assume that the label y is a deterministic function of x. Our
proof starts by relating RP (f) and graph-induced labels ℓ∗
Lemma 1. Let f be from a function class with the VC dimension h and xi be n examples, which are sampled i.i.d.
with respect to the distribution P(x). Then the inequality:
L(f(xi), sgn(ℓ∗
h(ln(2n/h) + 1) −ln(η/4)
inductive error ∆I(h,n,η)
holds with probability 1 −η, where yi and ℓ∗
i represent the
true and graph-induced soft labels, respectively.
Proof: Based on Equations 3.15 and 3.24 ,
the inequality:
L(f(xi), yi) + ∆I(h, n, η)
holds with probability 1 −η. Our ﬁnal claim follows from
bounding all terms L(f(xi), yi) as:
L(f(xi), yi) ≤L(f(xi), sgn(ℓ∗
i )) + (ℓ∗
The above bound holds for any yi ∈{−1, 1} and ℓ∗
It is hard to bound the error term 1
i −yi)2 when the
constraints ℓi = yi (5) are enforced in a hard manner. Thus,
in the rest of our analysis, we consider a relaxed version of
the harmonic function solution :
ℓ∈Rn (ℓ−y)
TC(ℓ−y) + ℓ
where L is the Laplacian of the data adjacency graph, C is a
diagonal matrix such that Cii =cl for all labeled examples,
and Cii = cu otherwise, and y is a vector of pseudo-targets
such that yi is the label of the i-th example when the example is labeled, and yi = 0 otherwise.
The generalization error of the solution to the problem (16)
is bounded in Lemma 2. To simplify the proof, we assume
that cl = 1 and cl > cu.
Lemma 2. Let ℓ∗be a solution to the problem:
ℓ∈Rn (ℓ−y)
TC(ℓ−y) + ℓ
where Q = L+γgI and all labeled examples l are selected
i.i.d. Then the inequality:
P (ℓ∗) ≤bR
P (ℓ∗) + β +
transductive error ∆T (β,nl,δ)
λM(L) + γg
Branislav Kveton, Michal Valko, Ali Rahimi, Ling Huang
Figure 2: Linear, cubic, and RBF decision boundaries obtained by manifold regularization of SVMs (MR) and max-margin
graph cuts (GC) on the problem from Figure 1. The regularization parameter γg = γ/γu is set as suggested in Section 4.2,
γ =0.1, and ε=0.01. The pink and blue colors denote parts of the feature space x where the discriminators f are positive
and negative, respectively. The yellow color marks regions where |f(x)| < 0.05.
Semi-Supervised Learning with Max-Margin Graph Cuts
holds with probability 1 −δ, where:
P (ℓ∗) = 1
are risk terms for all and labeled vertices, respectively, and
β is the stability coefﬁcient of the solution ℓ∗.
Proof: Our risk bound follows from combining Theorem 1
of Belkin et al. with the assumptions |yi| ≤1 and
i | ≤1. The coefﬁcient β is derived based on Section 5 of
Cortes et al. . In particular, based on the properties
of the matrix C and Proposition 1 , we
λm(Q) + 1 +
(λm(Q) + 1)2
where λm(Q) and λM(Q) refer to the smallest and largest
eigenvalues of Q, respectively, and can be further rewritten
as λm(Q) = λm(L)+γg and λM(Q) = λM(L)+γg. Our
ﬁnal claim directly follows from applying the lower bounds
λm(L) ≥0 and (λm(L) + γg + 1)2 ≥γ2
Lemma 2 is practical when the error ∆T (β, nl, δ) decreases
at the rate of O(n
). This is achieved when β =O(1/nl),
which corresponds to γg =Ω(n
l ). Thus, when the problem
(16) is sufﬁciently regularized, its solution is stable, and the
generalization error of the solution is bounded.
Lemmas 1 and 2 can be combined using the union bound.
Proposition 1. Let f be from a function class with the VC
dimension h. Then the inequality:
L(f(xi), sgn(ℓ∗
P (ℓ∗) + ∆T (β, nl, δ) + ∆I(h, n, η)
holds with probability 1 −(η + δ).
The above result can be viewed as follows. If both n and nl
are large, the sum of 1
i L(f(xi), sgn(ℓ∗
i )) and bRW
provides a good estimate of the risk RP (f). Unfortunately,
our bound is not practical for setting γg because it is hard to
ﬁnd γg that minimizes both bRW
P (ℓ∗) and ∆T (β, nl, δ). The
same phenomenon was observed by Belkin et al. in
a similar context. To solve our problem, we suggest setting
γg based on the validation set. This methodology is used in
the experimental section.
THRESHOLD ε
Finally, note that when |ℓ∗
i | < ε, where ε is a small number,
i −yi| is close to 1 irrespective of yi, and a trivial upper
bound L(f(xi), yi)≤1 is almost as good as L(f(xi), yi)≤
L(f(xi), sgn(ℓ∗
i −yi)2 for any f. This allows us to
justify the ε threshold in the problem (7). In particular, note
that L(f(xi), yi) is bounded by 1−(ℓ∗
i −yi)2 +(ℓ∗
i | < ε, 1 −(ℓ∗
i −yi)2 < 2ε −ε2, and we conclude
the following.
Proposition 2. Let f be from a function class with the VC
dimension h and nε be the number of examples such that
i | < ε. Then the inequality:
L(f(xi), sgn(ℓ∗
i )) + 2εnε
P (ℓ∗) + ∆T (β, nl, δ) + ∆I(h, n, η)
holds with probability 1 −(η + δ).
Proof: The generalization bound is proved as:
RP (f) ≤bRP (f) + ∆I(h, n, η)
L(f(xi), yi) + 1
L(f(xi), yi) +
∆I(h, n, η)
L(f(xi), sgn(ℓ∗
i )) + (ℓ∗
i −yi)2 + (ℓ∗
∆I(h, n, η)
L(f(xi), sgn(ℓ∗
∆I(h, n, η)
L(f(xi), sgn(ℓ∗
i )) + 2εnε
P (ℓ∗) + ∆T (β, nl, δ) + ∆I(h, n, η).
The last step follows from the inequality 1−(ℓ∗
i −yi)2 < 2ε
and Lemma 2.
, the new upper bound is asymptotically as
good as the bound in Proposition 1. As a result, we get the
same convergence guarantees although highly-uncertain labels |ℓ∗
i | < ε are excluded from our optimization.
In practice, optimization of the thresholded objective often
yields a lower risk 1
i |≥ε L(f ∗(xi), sgn(ℓ∗
i ))+ 2εnε
and also lower training and test errors. This is a result of excluding the most uncertain examples |ℓ∗
i |<ε from learning.
Figure 3 illustrates these trends on three learning problems.
Branislav Kveton, Michal Valko, Ali Rahimi, Ling Huang
Thresholded risks
Training errors [%]
Test errors [%]
Used training data [%]
Figure 3: The thresholded empirical risk 1
i |≥ε L(f ∗(xi), sgn(ℓ∗
i )) + 2εnε
of the optimal max-margin graph cut f ∗
(7), its training and test errors, and the percentage of training examples such that |ℓ∗
i | ≥ε, on 3 letter recognition problems
from the UCI ML repository. The plots are shown as functions of the parameter γg, and correspond to the thresholds ε = 0
(light gray lines), ε = 10−6 (dark gray lines), and ε = 10−3 (black lines). All results are averaged over 50 random choices
of 1 percent of labeled examples.
Note that the parameters γg and ε are redundant in the sense
that the same result is often achieved by different combinations of parameter values. This problem is addressed in the
experimental section by ﬁxing ε and optimizing γg only.
EXPERIMENTS
The experimental section is divided into two parts. The ﬁrst
part compares max-margin graph cuts to manifold regularization of SVMs on the problem from Figure 1. The second
part compares max-margin graph cuts, manifold regularization of SVMs, and supervised learning with SVMs on three
UCI ML repository datasets .
Manifold regularization of SVMs is evaluated based on the
implementation of Belkin et al. . Max-margin graph
cuts and SVMs are implemented using LIBSVM .
SYNTHETIC PROBLEM
The ﬁrst experiment (Figure 2) illustrates linear, cubic, and
RBF graph cuts (7) on the synthetic problem from Figure 1.
The cuts are shown for various settings of the regularization
parameter γg. As γg decreases, note that the cuts gradually
interpolate between supervised learning on just two labeled
examples and semi-supervised learning on all data. The resulting discriminators are max-margin decision boundaries
that separate the corresponding colored regions in Figure 1.
Figure 2 also shows that manifold regularization of SVMs
(10) with linear and cubic kernels cannot perfectly separate
the two clusters in Figure 1 for any setting of the parameter
γu. The reason for this problem is discussed in Section 4.3.
Finally, note the similarity between max-margin graph cuts
and manifold regularization of SVMs with the RBF kernel.
This similarity was suggested in Section 4.2.
UCI ML REPOSITORY DATASETS
The second experiment (Figure 4) shows that max-margin
graph cuts (7) typically outperform manifold regularization
of SVMs (10) and supervised learning with SVMs. The experiment is done on three UCI ML repository datasets: letter recognition, digit recognition, and image segmentation.
The datasets are multi-class and thus, we transform each of
them into a set of binary classiﬁcation problems. The digit
recognition and image segmentation datasets are converted
into 45 and 15 problems, respectively, where all classes are
discriminated against every other class. The letter recognition dataset is turned into 25 problems that involve pairs of
consecutive letters. Each dataset is divided into three folds.
The ﬁrst fold is used for training, the second one for selecting the parameters γ ∈[0.01, 0.1]nl, γu ∈[10−3, 103]γ, and
γg = γ/γu, and the last fold is used for testing.1 The fraction of labeled examples in the training set is varied from 1
to 10 percent. All examples in the validation set are labeled
1Alternatively, the regularization parameters γ, γu, and γg can
be set using leave-one-out cross-validation on labeled examples.
Semi-Supervised Learning with Max-Margin Graph Cuts
Misclassiﬁcation errors [%]
Linear kernel
Cubic kernel
RBF kernel
recognition
recognition
segmentation
Figure 4: Comparison of SVMs, max-margin graph cuts (GC), and manifold regularization of SVMs (MR) on three datasets
from the UCI ML repository. The fraction of labeled examples L varies from 1 to 10 percent.
and its size is limited to the number of labeled examples in
the training set.
In all experiments, we use 5-nearest neighbor graphs whose
edges are weighted as wij = exp[−∥xi −xj∥2
2 /(2Kσ2)],
where K is the number of features and σ denotes the mean
of their standard deviations. The width of radial basis functions (RBFs) is set accordingly to
Kσ, and the threshold
ε for choosing training examples (7) is 10−6.
Test errors of all compared algorithms are averaged over all
binary problems within each dataset and shown in Figure 4.
Max-margin graph cuts outperform manifold regularization
of SVMs in 29 out of 36 experiments. Note that the lowest
errors are usually obtained for linear and cubic kernels, and
our method improves the most over manifold regularization
of SVMs in these settings.
CONCLUSIONS
This paper proposes a novel algorithm for semi-supervised
learning. The algorithm learns max-margin graph cuts that
are conditioned on the labels induced by the harmonic function solution. We motivate the approach, prove its generalization bound, and compare it to state-of-the-art algorithms
for semi-supervised max-margin learning. The approach is
evaluated on a synthetic problem and three UCI ML repository datasets, and we show that it usually outperforms manifold regularization of SVMs.
In our future work, we plan to investigate some of the shortcomings of this paper. For instance, note that the theoretical
analysis of max-margin graph cuts (Section 5) assumes soft
labels but our solutions (7) are computed using the hard labels ℓi = yi. Whether the theoretically sound setting yields
better results in practice is an open question.