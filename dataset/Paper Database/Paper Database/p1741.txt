Feature Engineering for Predictive Modeling
Using Reinforcement Learning
Udayan Khurana
 
IBM Research AI
Horst Samulowitz
 
IBM Research AI
Deepak Turaga
 
IBM Research AI
Feature engineering is a crucial step in the process of predictive modeling. It involves the transformation of given feature space, typically using mathematical functions, with the
objective of reducing the modeling error for a given target.
However, there is no well-deﬁned basis for performing effective feature engineering. It involves domain knowledge, intuition, and most of all, a lengthy process of trial and error.
The human attention involved in overseeing this process signiﬁcantly inﬂuences the cost of model generation. We present
a new framework to automate feature engineering. It is based
on performance driven exploration of a transformation graph,
which systematically and compactly captures the space of
given options. A highly efﬁcient exploration strategy is derived through reinforcement learning on past examples.
Introduction
Predictive analytics are widely used in support for decision
making across a variety of domains including fraud detection, marketing, drug discovery, advertising, risk management, amongst others. Predictive models are constructed using supervised learning algorithms where classiﬁcation or
regression models are trained on historical data to predict
future outcomes. The underlying representation of the data
is crucial for the learning algorithm to work effectively. In
most cases, appropriate transformation of data is an essential prerequisite step before model construction.
For instance, Figure 1 depicts two different representations for points belonging to a classiﬁcation problem dataset.
On the left, one can see that instances corresponding to the
two classes are present in alternating small clusters. For most
machine learning (ML) algorithms, it is hard to draw a reasonable classiﬁer on this representation that separates the
two classes. On the other hand if the feature x is replaced
by its sine, as seen in the image on the right, it makes the
two classes reasonably separable by most classiﬁers. The
task or process of altering the feature representation of a
predictive modeling problem to better ﬁt a training algorithm is called feature engineering (FE). The sine function
is an instance of a transformation used to perform FE. Consider the schema of a dataset for forecasting hourly bike
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
(a) Original data
(b) Engineered data.
Figure 1: Illustration of different representation choices.
rental demand (kaggle.com/c/bike-sharing-demand) in Figure 2(a). Deriving several features (Figure 2(b)) dramatically
reduces the modeling error. For instance, extracting the hour
of the day from the given timestamp feature helps to capture
certain trends such as peak versus non-peak demand. Note
that certain valuable features are derived through a composition of multiple simpler functions. FE is perhaps the central task in improving predictive modeling performance, as
documented in a detailed account of the top performers at
various Kaggle competitions .
In practice, FE is orchestrated by a data scientist, using
hunch, intuition and domain knowledge based on continuously observing and reacting to the model performance
through trial and error. As a result, FE is often timeconsuming, and is prone to bias and error. Due to this inherent dependence on human decision making, FE is colloquially referred to as an art, making it difﬁcult to automate.
The existing approaches to automate FE are either computationally expensive and/or lack the capability to discover
complex features.
We present a novel approach to automate FE based on
reinforcement learning (RL). It involves training an agent
on FE examples to learn an effective strategy of exploring
available FE choices under a given budget. The learning
and application of the exploration strategy is performed on
a transformation graph, a directed acyclic graph representing relationships between different transformed versions of
the data. To the best of our knowledge, this is the ﬁrst work
that learns a performance-guided strategy for effective feature transformation from historical instances. Also, this is
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
(a) Original features and target count.
(b) Additionally engineered features using our technique.
Figure 2: In Kaggle’s biking rental count prediction dataset,
FE through our technique reduced Relative Absolute Error
from 0.61 to 0.20 while retaining interpretability of features.
the only work in FE space that provides an adaptive, budget
constrained solution. Finally, the output features are compositions of well-deﬁned mathematical functions which make
them human readable and usable as insights into a predictive
analytics problem, such as the one illustrated in Figure 2(b).
Related Work
Given a supervised learning dataset, FICUS performs a beam search over the space
of possible features, constructing new features by applying
constructor functions. FICUS’s search for better features is
guided by heuristic measures based on information gain in a
decision tree, and other surrogate measures of performance.
In contrast, our approach optimizes for the prediction performance criterion directly, rather than surrogate criteria. Note
that FICUS is more general than a number of less recent approaches .
FCTree uses a decision tree to partition
the data using both original and constructed features as splitting points at nodes in the tree. As in FICUS , FCTree uses surrogate tree-based
information-theoretic criteria to guide the search, as opposed to the true prediction performance. FCTree is capable of generating only simple features, and is not capable of
composing transformations, and hence searches in a smaller
space than our approach. They propose a weight update
mechanism that helps identify good transformations for a
dataset, such that they are used more frequently.
The Deep Feature Synthesis component of Data Science
Machine (DSM) relies
on applying all transformations on all features at once (but
no combinations of transformations), and then performing
feature selection and model hyper-parameter optimization
over the combined augmented dataset. A similar approach
is adopted by One Button Machine . We
will call this category as the expansion-reduction approach.
This approach suffers performance performance and scalability bottleneck due to performing feature selection on a
large number of features that are explicitly generated by simultaneous application of all transformations. In spite of the
expansion of the explicit expansion of the feature space, it
does not consider the composition of transformations.
FEADIS relies on a combination
of random feature generation and feature selection. It adds
constructed features greedily, and as such requires many
expensive performance evaluations. A related work, ExploreKit expands the feature space explicitly. It employs learning to rank the newly constructed
features and evaluating the most promising ones. While this
approach is more scalable than the expand-select type, it still
is limited due to the explicit expansion of the feature space,
and hence time-consuming. For instance, their reported results were obtained after running FE for days on moderately
sized datasets. Due to the complex nature of this method, it
does not consider compositions of transformations. We refer
to this FE approach as evolution-centric.
Cognito introduces the notion of
a tree-like exploration of transform space; they present a
few simple handcrafted heuristics traversal strategies such as
breadth-ﬁrst and depth-ﬁrst search that do not capture several factors such as adapting to budget constraints. This paper generalizes the concepts introduced there. LFE proposes a learning based method to predict the most likely useful transformation for each feature.
It considers features independent of each other; it is demonstrated to work only for classiﬁcation so far, and does not
allow for composition of transformations. A combination of
the learning-based and heuristic tree-based exploration approaches has also been suggested .
Other plausible approaches to FE are hyper-parameter optimization where each transformation
choice could be a parameter, black-box optimization strategies , or bayesian optimization such as the
ones for model- and feature-selection . To
the best of our knowledge, these approaches have not been
employed for solving FE. employ a
genetic algorithm to determine a suitable transformation for
a given data set, but is limited to single transformations.
Certain ML methods perform some level of FE indirectly.
 present a recent survey on the topic. Dimensionality reduction methods such as Principal Component Analysis (PCA) and its
non-linear variants (Kernel PCA) aim at mapping the input dataset into a lower-dimensional space with
fewer features.Such methods are also known as embedding
methods . Kernel methods such as
Support Vector Machines (SVM) are a class of learning algorithms that use kernel functions to implicitly map the input feature space into a higher-dimensional space.
Multi-layer neural networks allow for useful features to be
learned automatically, such that they minimize the training
loss function. Deep learning methods have achieved remarkable success on data such as video, image and speech, where
manual FE is very tedious . However, deep learning methods require massive amounts of data to avoid overﬁtting and are not suitable
for problems instances of small or medium sizes, which are
quite common. Additionally, deep learning has mostly been
successful with video, image, speech and natural language
data, whereas the general numerical types of data encompass a wide variety of domains and need FE. Our technique
is domain and model independent and works generally irrespective of the scale of data. The features learned by a deep
network may not always be easily explained, limiting application domains such as healthcare . On the
contrary, features generated by our algorithm are compositions of well-understood mathematical functions that can be
analyzed by a domain expert.
The automation of FE is challenging computationally, as
well as in terms of decision-making. First, the number of
possible features that can be constructed is unbounded since
the transformations can be composed, i.e., applied repeatedly to features generated by previous transformations. In
order to conﬁrm whether a new feature provides value, it
requires training and validation of a new model upon including the feature. It is an expensive step and infeasible to
perform with respect to each newly constructed feature. The
evolution-centric approaches described in the Related Work
section operate in such manner and take days to complete
even on moderately-sized datasets. Unfortunately, there is
no reusability of results from one evaluation trial to another.
On the other hand, the expansion-reduction approach performs fewer or only one training-validation attempts by ﬁrst
explicitly applying all transformations, followed by feature
selection on the large pool of features. It presents a scalability and speed bottleneck itself; in practice, it restricts
the number of new features than can be considered. In both
cases, there is a lack of performance oriented search. With
these insights, our proposed framework performs a systematic enumeration of the space of FE choices for any given
dataset through a transformation graph. Its nodes represent
different versions of a given dataset, obtained by the application of transformation functions (represented on edges). A
transformation when applied to a dataset, applies the function on all possible features (or sets of features in case
non-unary functions), and produces multiple additional features, followed by optional feature selection, and trainingevaluation. Therefore, it batches the creation of new features
by each transformation function. This lies somewhat in the
middle of the evolution-centric and the expansion-reduction
approaches. It not only provides a computational advantage,
but also a logical unit of measuring performance of various
transformations, which is used in composing different functions in a performance-oriented manner. This translates the
FE problem to ﬁnding the node (dataset) on the transformation graph with the highest cross-validation performance,
while only exploring the graph as little as possible.
Secondly, the decision making in manual FE exploration
involves intuition and complex associations, that are based
on a variety of factors. Two such examples are: prioritizing transformations based on the performance with the given
dataset or even based on past experience; whether to explore
different transformations or exploit the combinations of the
ones that have shown promise thus far on this dataset, and so
on. It is hard to articulate the notions or set of rules that are
the basis of such decisions; hence, we recognize the factors
involved and learn a strategy as a function of those factors in
order to perform the exploration automatically. We use reinforcement learning on FE examples on a variety of datasets,
to ﬁnd an optimal strategy. This is based on the transformation graph. The resultant strategy is a policy that maps each
instance of the transformation graph to an action of applying
a transformation on a particular node in the graph.
Notation and Problem Description
Consider a predictive modeling task consisting of: (1) a set
of features, F = {f1, f2 . . . fm}; (2) a target vector, y. A
pair of the two is speciﬁed as a dataset, D = ⟨F, y⟩. The
nature of y, whether categorical or continuous, describes if
it pertains to a classiﬁcation or regression problem, respectively. Consider an applicable learning algorithm L (such
as Random Forest Classiﬁer or Linear Regression) and a
measure of performance, m (such as F1-score or -RMSE).
L (F, y) (or simply, A(F) or A(D)) to signify
the cross-validation performance using measure m for the
model constructed on given data with algorithm L.
Additionally, consider a set of k transformation functions
at our disposal, T = {t1, t2 . . . tk}. The application of a
transformation on a set of features, t(F), suggests the application of the corresponding function on all valid input feature subsets in F, applicable to t. For instance, a square
transformation applied to a set of features, F with eight numerical and two categorical features will produce eight new
output features, ˆfo = square(fi), ∀fi ∈F, fi ∈Rn. This
extends to k-ary functions, which work on k input features.
In this paper (in the context of transformation graph), for
feature sets, Fo and Fi, and a transformation, t, such that
Fo = t(Fi), it is implied that Fo includes all the newly
generated features besides the original features from Fi, i.e.,
Fi ⊆Fo. The entire (open) set of derived features that may
be derived from F through T is denoted by ˆFT .
A ‘+’ operator on two feature sets (associated with the
same target y) is a union of the two feature sets, Fo =
F1 + F2 = F1 ∪F2, preserving row order. Generally, transformations add features; on the other hand, a feature selection operator, which is to a transformation in the algebraic
notation, removes features. Note that all operations speci-
ﬁed on a feature set , t(F), can exchangeably be written for
a corresponding dataset, D = ⟨F, y⟩, as, t(D).
The goal of feature engineering is stated as follows. Given
a set of features, F, and target, y and a set of transformations, T , ﬁnd a set of features, F ∗= F1∪F2, where F1 ⊆F
(original) and F2 ⊂ˆFT (derived), to maximize the modeling
accuracy for a given algorithm, L and measure, m.
F ∗= arg max
L (F1 ∪F2, y)
Transformation Graph
Transformation Graph, G, for a given dataset, D0, and a ﬁnite set of transformations, T, is a directed acyclic graph
in which each node corresponds to a either D0 or a dataset
derived from it through a transformation sequence. Every
D4,9 =D4+D9
D5,3 =D5+D3
Figure 3: Example of a Transformation Graph (a DAG). The
start node D0 corresponds to the given dataset; that and the
hierarchical nodes are circular. The sum nodes are rectangular. In this example, we can see three transformations, log,
sum, and square, as well as a feature selection operator FS1.
node’s dataset contains the same target and number of rows.
The nodes are divided into three categories: (a) the start or
the root node, D0 corresponding to the given dataset; (b) hierarchical nodes, Di, where i > 0, which have one and only
one incoming node from a parent node Dj, i > j ≥0, and
the connecting edge from Dj to Di corresponds to a transform t ∈T (including feature selection), i.e., Dj = t(Di);
(c) sum nodes, D+
i,j = Di + Dj, a result of a dataset sum
such that i ̸= j.
Edges correspond to either transformations or ‘+’ operations, with children as type (b) or type (c) nodes, respectively. The direction of an edge represents the application of
transform from source to a target dataset (node). Height (h)
of the transformation graph refers to the maximum distance
between the root and any other node.
A transformation graph is illustrated in Figure 3. Each
node of a transformation graph is a candidate solution for
the FE problem in Equation 1. Also, a complete transformation graph must contain a node that is the solution to
the problem, through a certain combination of transformations including feature selection. The operator θ(G) signi-
ﬁes all nodes of graph G. λ(Di, Dj) signiﬁes the transformation T, such that its application on Di created the child
Dj; alternatively if Dj is a sum node and Di is one of
its parents, then λ(Di, Dj) = +. A complete transformation graph is unbounded for a non-empty transformation set.
A constrained (bounded height, h) complete transformation
graph for t transformations will have th+1 −2 hierarchical nodes (and an equal number of corresponding edges),
and (th+1−1)×(th+1−2)
sum nodes (and 2 times corresponding edges). It can be seen that for even a height bounded
tree with a modest number of transformations, computing
the cross-validation performance across the entire graph is
computationally expensive.
Graph Exploration under a Budget Constraint
It should be emphasized that due to the enormity transformation graphs, their exhaustive exploration is usually not feasible. For instance, with 20 transformations and a height =
5, the complete graph contains about 3.2 million nodes; an
exhaustive search would imply as many model training and
testing iterations. On the other hand, there is no known property that allows us to deterministically verify the optimal solution in a subset of the trials. Hence, the focus of this work
is to ﬁnd a performance driven exploration policy, which
maximizes the chances of improvement in accuracy within
in a limited time budget. The exploration of the transformation graph begins with a single node, D0, and grows one
node at a time. The general philosophy is that it is reasonable to perform exploration of the environment, i.e., stumble
upon the transformations that signal an improvement. Over
time (elapsed budget) it is desirable to reduce the amount of
exploration and focus more on exploitation.
Algorithm 1: Transformation Graph Exploration
Input: Dataset D0, Budget Bmax;
Initialize G0 with root D0;
while i < Bmax do
N ←θ(Gi) bratio =
n∈N ,t∈T |∄n′, t=λ(n,n′)
R(Gi, n, t, bratio)
Gi+i ←Apply t∗to n∗in Gi
Output: argmax
Algorithm 1 outlines the general methodology for exploration. At each step, an estimated reward from each possible move, R(Gi, n, t,
Bmax ) is used to rank the options
of actions available at each given state of the transformation graph Gi, ∀i ∈[0, Bmax), where Bmax is the overall allocated budget in number of steps. The budget could
be considered in terms of any quantity that is monotonically increasing in i, such as time elapsed. For simplicity, we work with “number of steps”. Note that the algorithm allows for different exploration strategies, which is
left up to the deﬁnition of the function R(. . . ), that de-
ﬁnes the relative importance of various actions available at
each step. The parameters of the function suggest that it depends on the various aspects of the graph at that point, Gi,
the remaining budget, and speciﬁcally, attributes of the action (node+transformation) being characterized. Below, we
brieﬂy discuss such factors that inﬂuence the exploration
choice at each step. These factors are compared across all
choices of node+transformation pairs < n, t > at Gi:
1. Node n’s Accuracy: Higher accuracy of a node incentives
further exploration from that node, compared to others.
2. Transformation, t’s average immediate reward till Gi.
3. Number of times transform t has already been used in the
path from root node to n.
4. Accuracy gain for node n (from its parent) and gain for
n’s parent, i.e., testing if n’s gains are recent.
5. Node Depth: A higher value is used to penalize the relative complexity of the transformation sequence.
6. The fraction of budget exhausted till Gi.
7. Ratio of feature counts in n to the original dataset: This
indicates the bloated factor of the dataset.
8. Is the transformation a feature selector?
9. Whether the dataset contains numerical features, datetime
features, or string features, or others?
Simple graph traversal strategies can be handcrafted. A
strategy essentially translates to the design of the reward estimation function, R(. . . ). In line with Cognito , a breadth-ﬁrst or a depth-ﬁrst strategy, or perhaps
a mix of them can be described. While such simplistic strategies work suitably in speciﬁc circumstances, it seems hard
to handcraft a uniﬁed strategy that works well under various
circumstances. We instead turn to machine learning to learn
the complex strategy from several historical runs.
Traversal Policy Learning
So far, we have discussed a hierarchical organization of FE
choices through a transformation graph and a general algorithm to explore the graph in a given budget. At the heart
of the algorithm is a function to estimate the reward of each
possible action at any given state. The design of the reward
estimation function determines the strategy of exploration.
Strategies could be handcrafted; however, in this section we
try to learn an optimal strategy from examples of FE on several datasets through transformation graph exploration. Because of the behavioral nature of this problem - which can
be perceived as continuous decision making (which transformations to apply to which node) while interacting with an
environment (the data, model, etc.) in discrete steps and observing reward (accuracy improvement), with the notion of
a ﬁnal optimization target (ﬁnal improvement in accuracy),
we model it as a RL problem. We are interested in learning an action-utility function to satisfy the expected reward
function, R(. . . ) in Algorithm 1. In the absence of an explicit
model of the environment, we employ Q-learning with function approximation due to the large number of states (recall,
millions of nodes in a graph with small depth) for which it
is infeasible to learn state-action transitions explicitly.
Consider the graph exploration process as a Markov Decision Process (MDP) where the state at step i is a combination of two components: (a) transformation graph after i
node additions, Gi (G0 consists of the root node corresponding to the given dataset. Gi contains i nodes); (b) the remaining budget at step i, i.e., bratio =
Bmax , discretized to two
decimal points in precision. Let the entire set of states be S.
On the other hand, an action at step i is a pair of existing tree
node and transformation, i.e., < n, t > where n ∈θ(Gt),
t ∈T and ∄n′ ∈Gi such that λ(n, n′) = t; it signiﬁes the
application of the one transform (which hasn’t already been
applied) to one of the exiting nodes in the graph. Let the entire set of actions be C. A policy, Π : S →C, determines
which action is taken given a state. Note that the objective of
RL here is to learn the optimal policy (exploration strategy)
by learning the action-value function, which we elaborate
later in the section.
Such formulation uniquely identiﬁes each state of the
MDP, including the context of “remaining budget”, which
helps the policy implicitly play an adaptive explore-exploit
tradeoff. It decides whether to focus on exploiting gains
(depth) or exploring (breadth) or a compromise, in different
regions of the graph, at different steps. Overall, the policy selects the action with the highest expected long-term reward
contribution; however, upon evaluating a new node’s actual
immediate contribution, the expectations are often revised
and explore/exploit gears are (implicitly) calibrated through
the policy. For example, upon ﬁnding an exceptionally high
improvement at a node during early stages, the (bredth) exploration can be temporarily localized under that node instead of the same level as it. Overall, value estimation a complex function (which is to be learned through RL) of multiple attributes of the MDP state such as current remaining
budget, graph structure and relative performance at various
nodes, etc.
Note that the runtime explore/exploit trade-off mentioned
above is different from the explore/exploit tradeoff seen in
RL training in context of selecting actions to balance reward
and not getting stuck in a local optimum. For the latter, we
employ an ϵ−Greedy methodology, where an action is chosen at random with probability ϵ (random exploration), and
from the current policy with probability 1 −ϵ (policy exploitation). The trade-off in this case is exercised randomly
and is independent of the state of MDP. The value of ϵ is
constant and is chosen based on experimentation.
Q-learning with Function Approximation
At step i, the occurrence of an action results in a new node,
ni, and hence a new dataset on which a model is trained and
tested, and its accuracy A(ni) is obtained. To each step, we
attribute an immediate scalar reward:
n′∈θ(Gi+1) A(n′) −max
n∈θ(Gi) A(n)
with r0 = 0, by deﬁnition. The cumulative reward over time
from state si onwards is deﬁned as:
where γ ∈[0, 1) is a discount factor, which prioritizes earlier rewards over the later ones. The objective is to ﬁnd the
optimal policy Π∗that maximizes the cumulative reward.
We use Q-learning with function approximation to learn the action-value Q-function. For
each state, s ∈S and action, c ∈C, Q-function with respect
to policy Π is deﬁned as:
Q(s, c) = r(s, c) + γRΠ(δ(s, c))
where δ : S × C →S is a hypothetical transition function,
and RΠ(s) is the cumulative reward following state s. The
optimal policy is:
Π∗(s) = arg max
However, given the size of S, it is infeasible to learn
Q-function directly. Instead, a linear approximation the Qfunction is used as follows:
Q(s, c) = wc.f(s)
where wc is a weight vector for action c and f(s) =
f(g, n, t, b) is a vector of the state characteristics described
in the previous section and the remaining budget ratio.
Therefore, we approximate the Q-functions with linear combinations of characteristics of a state of the MDP. Note that,
in each heuristic rule strategy, we used a subset of these state
characteristics, in a self-conceived manner. However, in the
ML based approach here, we select the entire set of characteristics and let the RL training process ﬁnd the appropriate weights of those characteristics (for different actions).
Hence, this approach generalizes the other handcrafted approaches. The update rule for wc is as follows:
wcj ←wcj + α.(rj + γ. max
n′,t′ Q(g′, c′) −Q(g, c)).f(g, b)
where g′ is the state of the graph at step j + 1, and α is
the learning rate parameter. The proof follows from .
A variation of the linear approximation where the coefﬁcient vector w is independent of the action c, is as follows:
Q(s, c) = w.f(s)
This method reduces the space of coefﬁcients to be learnt
by a factor of c, and makes it faster to learn the weights. It is
important to note that the Q-function is still not independent
of the action c, as one of the factors in f(s) or f(g, n, t, b) is
actually the average immediate reward for the transform for
the present dataset. Hence, Equation 5 based approximation
still distinguishes between various actions (t) based on their
performance in the transformation graph exploration so far;
however, it does not learn a bias for different transformations
in general and based on the feature types (factor #9). We
refer to this type of strategy as RL2. In our experiments RL2
efﬁciency is somewhat inferior to the strategy to the strategy
learned with Equation 3, which we refer to as RL1.
Experiments
overlapping
{25, 50, 75, 100, 150, 200, 300, 500}
each dataset, in a random order. We used the discount
factor, γ = 0.99, and learning rate parameter, α = 0.05.
The weight vectors, wc or w, each of size 12, were initialized with 1’s. The training example steps are drawn
randomly with the probability ϵ = 0.15 and the current
policy with probability 1 −ϵ. We have used the following
transformation functions in general (except when speciﬁed
a different set): Log, Square, Square Root, Product, ZScore,
Min-Max-Normalization, TimeBinning, Aggregation (using
Min,Max,Mean,Count,Std), Temporal window aggregate,
Spatial Aggregation, Spatio Temporal Aggregation, k-term
frequency, Sum, Difference, Division, Sigmoid, BinningU,
BinningD, NominalExpansion, Sin, Cos, TanH.
Handcrafted
Figure 4: Comparing efﬁciencies of exploration policies.
Comparison: We tested the impact of our FE on a 48 publicly available datasets (different from the datasets used for
training) from a variety of domains, and of various sizes. We
report the accuracy of (a) base dataset; (b) Our FE routine
with RL1, Bmax = 100; (c) Expansion-reduction implementation where all transformations are ﬁrst applied separately and add to original columns, followed by a feature selection routine; (d) Random: randomly applying a transformation function to a random feature(s) and adding the result
to the original dataset and measuring the CV performance;
this is repeated 100 times and ﬁnally, we consider all the
new features whose cases showed an improvement in performance, along with the original features to train a model
(e) Tree-Heur: our implementation of Cognito’s global search heuristic for 100 nodes. We used
Random Forest with default parameters as our learning algorithm for all the comparisons as it gave us the strongest baseline (no FE) average. A 5-fold cross validation using random
stratiﬁed sampling was used. The results for a representative
set of 24 of those datasets (due to lack of space) are captured
in Table 1. It can be seen that our FE outperforms others in
most of the cases but one (where expand-reduce is better)
and tied for two with Cognito global search. Our technique
reduces the error (relative abs. error, or 1- mean unweighted
FScore) by 24.6% (by median) for all the 48 test datasets and
23.8% (by median) for the 24 datasets presented in Table 1.
For reference to runtime, it took the Bikeshare DC dataset
4 minutes, 40 seconds to run for 100 nodes for our FE, on a
single thread on a 2.8GHz processor. Generally, the runtimes
for the Random and Cognito were similar to our FE for all
datasets, while expand-reduce took 0.1 to 0.9 times the time
of our FE, for different datasets.
Traversal Policy Comparison: In Figure 4, we see that
on an average for 10 datasets, the RL-based strategies are 4-8
times more efﬁcient than any handcrafted strategy ), in ﬁnding the optimal dataset in a given graph with
6 transformations and bounded height, hmax = 4. Also, Figure 5 tells us that while RL1 (Eqn. 3) takes more data to
train, it is more efﬁcient than RL2 (Eqn. 5), demonstrating
that learning a general bias for transformations and one conditioned on data types makes the exploration more efﬁcient.
Higgs Boson
Amazon Employee
PimaIndian
German Credit
Bikeshare DC
Housing Boston
AP-omentum-ovary
Lymphography
Ionosphere
Openml 618
Openml 589
Openml 616
Openml 607
Openml 620
Openml 637
Openml 586
Credit Default
Messidor features
Wine Quality Red
Wine Quality White
Table 1: Comparing performance for base dataset (no FE), Our FE, Expansion-Reduction style FE, Random FE, and Tree
heuristic FE, using 24 datasets. Performance here is FScore for classiﬁcation (C) and (1−rel. absolute error) for regression (R).
Figure 5: Policy effectiveness with training dataset sources
Internal System Comparisons: We additionally performed experimentation to test and tune the internals of our
system. Figure 6 shows the maximum accuracy node (for
5 representative datasets) found when the height was constrained to a different numbers, using Bmax = 100 nodes;
hmax = 1 signiﬁes base dataset. Majority of datasets ﬁnd
the maxima with hmax = 4 with most ﬁnd it with hmax = 5.
For hmax = 6, a tiny fraction shows deterioration, which
can be interpreted as unsuccessful exploration cost due to a
Figure 6: Performance vs. hmax
higher permissible depth. Due to lack of space, we omit detailed experiments about system training and performance,
but summarize a couple of observations. Using feature selection (compared to none) as a transform improves the ﬁnal gain in performance by about 51%, measured on the 48
datasets. Finally, the use of different learning algorithms led
to different optimal features being engineered for the same
dataset, even for similar improvements in performance.
Conclusion and Future Work
In this paper, we presented a novel technique to efﬁciently
perform feature engineering for supervised learning problems. The cornerstone of our framework are – a transformation graph that enumerates the space of feature options, and
a RL-based, performance-driven exploration of the available
choices to ﬁnd valuable features. The models produced using our proposed technique considerably reduce the error
rate (25% by median) across a variety of datasets, for a relatively small computational budget. This methodology can
potentially save a data analyst hours to weeks worth of time.
One direction to further improve the efﬁciency of the system
is through a complex non-linear modeling of state variables.
Additionally, extending the described framework to other aspects of predictive modeling, such as missing value imputation or model selection, is of potential interest as well. Since
optimal features depend on model type (learning algorithm),
a joint optimization of the two is particularly interesting.