IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 8, NO. 11, NOVEMBER 2013
Audio Recording Location Identiﬁcation Using
Acoustic Environment Signature
Hong Zhao and Haﬁz Malik, Member, IEEE
Abstract—An audio recording is subject to a number of possible
distortions and artifacts. Consider, for example, artifacts due to
acoustic reverberation and background noise. The acoustic reverberation depends on the shape and the composition of the room,
and it causes temporal and spectral smearing of the recorded
sound. The background noise, on the other hand, depends on
the secondary audio source activities present in the evidentiary
recording. Extraction of acoustic cues from an audio recording
is an important but challenging task. Temporal changes in the
estimated reverberation and background noise can be used for
dynamic acoustic environment identiﬁcation (AEI), audio forensics, and ballistic settings. We describe a statistical technique
based on spectral subtraction to estimate the amount of reverberation and nonlinear ﬁltering based on particle ﬁltering to
estimate the background noise. The effectiveness of the proposed
method is tested using a data set consisting of speech recordings
of two human speakers (one male and one female) made in eight
acoustic environments using four commercial grade microphones.
Performance of the proposed method is evaluated for various
experimental settings such as microphone independent, semi- and
full-blind AEI, and robustness to MP3 compression. Performance
of the proposed framework is also evaluated using Temporal
Derivative-based Spectrum and Mel-Cepstrum (TDSM)-based
features. Experimental results show that the proposed method
improves AEI performance compared with the direct method (i.e.,
feature vector is extracted from the audio recording directly). In
addition, experimental results also show that the proposed scheme
is robust to MP3 compression attack.
Index Terms—Acoustic environment identiﬁcation, acoustic reverberation, audio forensics, background noise, particle ﬁltering.
I. INTRODUCTION
HE use of digital media (audio, video, and images) as
evidence in litigation and criminal justice is increasingly
common. For digital media to be admitted as evidence in a
Manuscript received June 09, 2013; revised August 03, 2013; accepted August 03, 2013. Date of publication August 16, 2013; date of current version
September 26, 2013. This work was supported in part by the National Natural
Science Foundation of China under Grant 61170226, in part by the Fundamental
Research Funds for the Central Universities under Grants SWJTU11CX047 and
SWJTU12ZT02, in part by the Young Innovative Research Team of Sichuan
Province under Grant 2011JTD0007, and in part by Chengdu Science and Technology program under Grant 12DXYB214JH-002. The associate editor coordinating the review of this manuscript and approving it for publication was
Prof. Jiwu Huang.
H. Zhao is with the School of Science and Technology, Southwest Jiaotong
University, Jiaotong, 610031, China, and also with the Department of Electronic
and Electrical Engineering, South University of Science and Technology, Shenzhen, 518055, China (e-mail: ).
H. Malik is with the Department of Electrical and Computer Engineering, The
University of Michigan, Dearborn, MI 48128 USA (e-mail: ).
Color versions of one or more of the ﬁgures in this paper are available online
at 
Digital Object Identiﬁer 10.1109/TIFS.2013.2278843
court of law, its authenticity and integrity must be veriﬁed.
This requirement is a complex and challenging task, especially
without helping data, such as digital watermarks or ﬁngerprints, and if the media is only available in a compressed format.
The availability of powerful, sophisticated, and easy-to-use
digital media manipulation tools has made authenticating the
integrity of digital media even more difﬁcult. In this context,
digital media forensics aims to determine the underlying facts
about an evidentiary recording and to provide authoritative
answers (in the absence of helping data) to various questions,
• Is an evidentiary recording “original” or was it created by
splicing multiple recordings together?
• What are the types and locations of forgeries, if there are
any, in an evidentiary recording?
• Was the evidentiary recording captured using acquisition
at location
, as claimed?
• Is the auditory scene in the evidentiary recording original
or was it digitally altered to deceive the listener?
It is therefore critical to authenticate the integrity of digital
evidence. Digital audio forensic techniques have been developed to detect traces of forgeries and tempering by exploiting:
• inconsistencies in the electric network frequency (ENF)
• acquisition device nonlinearities – ,
• artifacts
reverberation
 – ,
 – ,
• inconsistencies due to spectral distance and phase shift 
gunshot characterization , , and
• inconsistencies due to lossy compression – and
‘butt-splicing’ .
The acoustic environment identiﬁcation (AEI) has a wide range
of applications ranging from audio recording integrity authentication to real-time acoustic space localization/identiﬁcation.
For instance, consider a scenario where an audio recording
presented in the court as evidence claiming that the recording
was made in the claimed environment, e.g., ofﬁce, hallway,
outdoors, etc. For the admissibility of the evidence in a court of
law, integrity authentication of the evidentiary recording is required. Temporal consistency of estimated acoustic signatures
(e.g., reverberation and background noise) from the evidentiary
recording can be used for integrity authentication. Similarly,
consider a scenario where a police call center receives an
emergency call from a victim being harassed or chased by
an offender. Under such crime situations it is very common
that the harassed persons are unable to provide any relevant
information about their actual location. The acoustic signals
and reverberations in the test audio recording can be used to
1556-6013 © 2013 IEEE
ZHAO AND MALIK: AUDIO RECORDING LOCATION IDENTIFICATION USING ACOUSTIC ENVIRONMENT SIGNATURE
determine the acoustic space (i.e., car, street, neighborhood,
living room, bath room, bed room, kitchen, etc.) of the crime
This paper presents a new approach using a detailed analysis
of audio data to provide evidence in terms of features characterizing the location where the recording was made. Motivation behind considering acoustic artifacts for audio forensics and AEI
is that existing audio forensic analysis methods, e.g., ENF-based
methods , , , and recording device identiﬁcation
based methods – cannot withstand lossy compress attack,
e.g., MP3 compression. In our recent work we have shown
that acoustic reverberations can survive the lossy compression
attack. Location speciﬁc acoustic features therefore can be used
for AEI and digital audio forensic applications.
The main goal of this paper is to develop a statistical framework for automatic AEI and its applications to digital audio
forensics. Here we exploited speciﬁc artifacts introduced at the
time of recording for the AEI and for audio recording integrity
authentication. Both the acoustic reverberation and the background (ambient) noise are considered to achieve this objective.
Audio reverberation is caused by the persistence of sound after
the source has terminated which is due to the multiple reﬂections
from various surfaces in a room. As such, differences in a room’s
geometry and composition will lead to different amounts of
reverberation time. There is a signiﬁcant amount of literature on
modeling and estimating audio reverberation (see, for example,
 ). Inverse ﬁltering using spectral subtraction based methods
is considered to estimate acoustic reverberation. The blind reverberation estimation approach method used here is a variant
of that described in . The background noise is modeled
using a dynamical system and estimated using nonlinear ﬁltering
based on particle ﬁltering. An 128-dimensional feature vector is
extracted from the estimated background noise and acoustic reverberation components. A multiclass Support Vector Machine
(SVM) classiﬁer is trained using the data set recorded in eight
acoustic environments and tested for the AEI. The performance
of the proposed scheme is tested using a data set consisting of
2240 audio recordings made in eight acoustic environments
using four microphones. Experimental results show that the proposed system can successfully identify a recording environment
for both the compressed and uncompressed audio recordings.
The rest of the paper is organized as follows: a brief
overview of the existing state of art audio forensics is provided in Section II; details of background noise modeling and
estimation are given in Section III-A. Reverberation acoustic
environment artifacts modeling is outlined in Section III.
Block-based inverse ﬁltering for blind dereverberation is provided in Section III-B. Details of feature extraction for the
estimated reverberation and background noise components
are provided in Section III-C. Experimental setup, results, and
performance analysis are provided in Section IV. Finally, the
concluding remarks along with future research directions are
discussed in Section V.
II. AUDIO FORENSICS: STATE OF THE ART
Forensic experts have been analyzing audio recordings since
the 1960s. For example, the U.S. Federal Bureau of Investigation has conducted an examination of audio recordings for
speech intelligibility enhancement and authentication . In
the United States, US vs. McKeever established a set of requirements for the admissibility of audio recordings in a court of law.
Audio forensics has traditionally focused on analog magnetic
tape recordings by relying on analog recorder ﬁngerprints, such
as head switching transients, mechanical splices, and overdubbing signatures, to determine the integrity of the recording
 – . The question of authenticity becomes more complicated and challenging for digital recordings because digital
recorders do not leave such traces in the recording. Therefore,
linking a recorder to the recording, detecting copies, and
determining the chronology of recorded events is difﬁcult to
determine for digital recordings.
Over the last few decades, several efforts have been initiated
to ﬁll the rapidly growing gap between digital media manipulation technologies and digital media authentication tools.
For example, recent efforts have focused on residual signals
(i.e., electric network frequency (ENF), which has power-line
frequency 60/50 Hz) may be present in frequency to the digital
recording system – to authenticate the digital recording.
The ENF-based methods use the random ﬂuctuations in the
power-line frequency caused by mismatches between the electrical system load and generation for authentication purposes.
The ENF-based approaches may not always be applicable
if well-designed audio equipment (e.g., precessional microphones) or battery-operated devices (e.g., smartphones) are
used to capture the recordings.
Statistical
recognition-based
techniques
been proposed for identifying recording locations – ,
 – and acquisition devices – , – . However,
these methods are limited by their low accuracy and the inability
to link a recording to an acquisition device in a unique manner.
Additionally, these techniques work only in the raw digital
domain. We have also developed model-driven approaches to estimate acoustic reverberation signatures for automatic acoustic
environment identiﬁcation and forgery detection – .
Techniques based on time-domain analysis – have
been proposed to determine the authenticity of MP3 audio ﬁles
against editing and double compression attacks. Similarly, a
framework based on frequency-domain statistical analysis has
also been proposed by Grigoras to detect traces of audio
(re)compression and to discriminate among different audio
compression algorithms. Similarly, Liu et al. have also
proposed a statistical learning based method to detect traces of
double compression. The performance of Liu et al.’s method
deteriorates for low to high bit rate transcoding. Qiao et al.
 addresses this issue by considering nonzero dequantized
Modiﬁed Discrete Cosine Transform (MDCT) coefﬁcients
for their statistical machine learning method. Brixen in 
has proposed a time-domain method based on acoustic reverberation estimated from digital audio recordings of mobile
phone calls for crime scene identiﬁcation. A method based on
higher-order time-differences and correlation analysis has been
proposed by Cooper to detect traces of “butt-splicing” in
digital recordings. Recently, Pan et al. have also proposed
a time-domain method based on higher-order statistics to detect
traces of splicing. The proposed method uses differences in
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 8, NO. 11, NOVEMBER 2013
Fig. 1. Simpliﬁed digital audio recording diagram.
the local noise levels in an audio signal for splice detection.
Similarly, Zhou et al. in proposed to use the noise patterns
of different codecs to identify the speech codec used and tested
the effectiveness of their method on commonly-used audio
codecs such as GSM-HR, GSM-EFR, GSM-AMR, and SILK.
III. METHODOLOGY
Consider a digital audio recording signal
, which is a
combination of several components such as, direct speech signal
, acoustic environment distortion (consisting of reverberant
and background noise
), microphone distortion
, encoding distortion
, and transcoding distortion
. A simpliﬁed model for digital audio recording is shown
in Fig. 1.
The combined effect of the direct and reﬂected signals and the
background (or ambient) noise at the input of the microphone
can be expressed as,
denotes the room impulse response (RIR). If
denotes the microphone impulse response, then the digital audio
recording,
, can be expressed as,
can be expressed as,
Extraction of the dry signal,
, from the observation,
without the knowledge of
is a challenging task. To simplify the complicated model, we
make the following necessary assumptions:
• The impulse response of microphone
is time-invariant or at least short-time-invariant. Under this assumption, the estimation of
is possible, without any
prior knowledge of
• The recording system introduces negligible microphone
distortion, i.e.,
, and transcoding distortion
. These are reasonable assumptions especially
when a noise-canceling microphone is used for audio
recording and when the recorded audio is saved in raw
Fig. 2. Conceptual information ﬂow of the proposed acoustic environment
identiﬁcation system.
• The background noise is not very strong. This is a reasonable assumption for non-Gaussian background noise. Removing strong non-Gaussian noise from the audio signal
is a challenging task.
• Audio recording is captured in a stationary acoustic environment. It is important to note that an acoustic space does
not have a stationary impulse response when either the
sound source or the microphone is moving (even by a small
distance). Theoretically speaking, for any given room there
are effectively an inﬁnite number of possible impulse responses, as there are effectively an inﬁnite number of possible combinations of locations of sound source and microphone pairs. Estimating the reverberant component for
nonstationary acoustic environments is a challenging task.
It is therefore reasonable to assume that the recordings are
made with a ﬁxed set of microphone and sound source locations in each acoustic environment.
With the above assumptions, the observation can be expressed
In the following section, we will describe how to estimate
the reverberation signal
and the background noise
without exploiting any prior knowledge of
the acoustic environment.
The proposed system can be divided into three subsystems:
1) The background noise estimation subsystem,
2) The blind dereverberation subsystem, and
3) The feature extraction, fusion, and classiﬁcation subsystem.
Shown in Fig. 2 is the conceptual information ﬂow of the
proposed acoustic environment identiﬁcation system. Details of
each processing block are provided in the following sections.
A. Background Noise Estimation
The background (or ambient) noise provides very useful
acoustic cues that can be used for acoustic environment identi-
ﬁcation. Various noise estimation methods have been proposed
 , in the past. These methods work well for stationary
ZHAO AND MALIK: AUDIO RECORDING LOCATION IDENTIFICATION USING ACOUSTIC ENVIRONMENT SIGNATURE
and synthetic noise. Most of the real-world background noise,
however, is dynamic (nonstationary) in nature. Therefore,
we modeled background noise using a dynamical system.
To estimate the noise as accurately as possible, a particle
ﬁlter-based sequential estimation method has attracted much
attention. During past two decades, various particle ﬁlter
(PF)-based approaches have been proposed to track nonstationary additive distortions on speech features in the log-power
frequency domain – . It has been demonstrated that the
PF based speech feature enhancement technique will improve
the recognition accuracy. The PF based noise estimation can be
formulated as a tracking problem where the noise feature have
to be estimated for each frame, given the current observation
and its history of the noisy features. In this section, particle
ﬁlter based noise estimation will be introduced.
1) Dynamical System for Noise Estimation: The short-time
discrete-time representation of the observation signal model described in (4) can be expressed as,
where positive integer
denotes the index of the time-frame.
The corresponding observation signal in the Mel-frequency
domain can be expressed as follow. Let vectors
denote the logarithmic output energy of the Mel-ﬁlter bank
 of the
time-frame of observation
, clean speech
, respectively. The dynamical system
for noise sequence can be represented as , ,
represents the process noise in the Mel-frequency domain,
denotes the diagonal covariance matrix of the Gaussian distribution, and
represents the identity matrix with the same dimension as
The Gaussian Mixture Model (GMM) is used to model
the clean speech feature vector
in the Mel-frequency domain,
that is, the clean speech features
at time index
is generated by a Gaussian distribution trained using clean speech. In
addition to the observation equation, the state transition equation of noise is the most important factor for the state-space
model based noise signal estimation. Several state transition
models, e.g., random walk, random walk by Polyak averaging,
and feedback, predicted walk by static AR processes, etc., have
been proposed to model nonstationary background noise .
Random walk, the simplest next state predictor, is commonly
used to predict the next state of a dynamical system. It simply
takes the previous state as the estimate of the current state and
adds a random variable
, which is considered to be independent and an identically distributed (iid) Gaussian random variable with zero mean and covariance
The random walk-based prediction of the next state is give as,
The random walk process is the simplest model used for
tracking the state transition of a dynamical system. Therefore,
it is unable to accurately model the state transition of nonstationary real-world noise processes. To this end, random walk
by Polyak averaging and feedback based state transition model
is used to model the nonstationarity in the background noise.
The motivation behind Polyak averaging is to limit the range
of the predicted noise hypothesis within a ﬁxed interval of the
preceding frames. The weighted average of noise sample for
the next prediction is calculated over the particles is,
The polyak averaging based state transition can be expressed
represents forgetting factor,
represents
scaling factor of feedback, and
represents the Polyak average
of preceding
frames given as,
It is important to mention that the ﬁrst and the second terms
in (11) shift noise sample
close to the weighted average
using a forgetting factor which helps to reduce the scattering of
samples and removes outliers in the predicted state. The third
term, Polyak averaging and feedback, controls the compensation range for the predicted state parameters.
The Polyak averaging and feedback based state-transition
equation can estimate the noise samples more accurately than
the random walk based state-transition equation because it
predicts the next frame parameters depending on the previous
2) Sequential Importance Sampling (SIS) for PF: Given the
dynamical system characterized by (7) and (11), the joint a posteriori probability density function (pdf) for the noise sequence,
, can be represented by a ﬁrst-order Markov chain, i.e.,
in (13) can be obtained recursively from
Since the derivation from
is analytically intractable. To get around this problem, Monte
Carlo sampling is used to approximate the joint a posteriori. The
approximated using Monte Carlo sampling is given
as , ,
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 8, NO. 11, NOVEMBER 2013
is the normalized importance weight and
a Dirac delta function.
It is important to mention that drawing samples directly from
the posterior density is often infeasible, therefore, a suboptimal
importance density is generally used . When samples are
drawn from the importance density
then the importance weights
can be deﬁned as,
Now, let us approximate
According to (15)–(18), the weights can be expressed as,
The weights are represented by the corresponding likelihood
for each sample
samples. Those samples are known
as particles and the ﬁlter process is called particle ﬁltering. It
is also called a sequential important sampling (SIS) particle
ﬁlter .
In practice, sampling step many results in degeneracy condition, that is, it might select a reasonable number of samples
with insigniﬁcant weights. To address this issue, residual resampling is used which discard samples with insigniﬁcant
weights and maintain a constant number of samples. Accordingly, weights after the resampling step are also proportionally
redistributed.
After resampling, the samples are distributed approximately
according to (15). However, the discrete nature of the approximation can skew the importance weight distribution. To get
around this problem, Metropolis-Hastings sampling is used. The
Metropolis-Hastings sampling method samples a candidate
according to a proposed importance distribution given the current state.
3) Weights Calculation: The likelihood-based method 
is used to calculate the weight of each sample
given in (19),
denotes the prior speech density represented
by a GMM, which has been trained on clean speech. The model
is represented in a
dimensional space, where each dimension
represents a frequency bin.
The weight of each sample is then normalized as,
It can be observed from (21) that the normalized weight can
be evaluated if the estimated noise is less than the observation
in all spectral bins, that is,
If this condition is not satisﬁed then the
be evaluated. One simple solution to such situations is to set
weights of those samples to zero. However, it may result in a
complete annihilation especially when all the weights are set to
zeros. To handle this problem, a fast acceptance test (FAT) 
is used to prevent aggressive dropouts in the number of particles
from the candidate list. The FAT accepts a drawn sample only
if the likelihood can be evaluated. Otherwise, a new sample is
drawn by propagating a randomly chosen particle.
4) Particles Initialization: The ﬁrst step for particle ﬁltering
is to initialize the samples from a joint prior noise density,
. To achieve this goal, the
can be initialized
Here is the variance terms
are initialized by
setting each of them to a small real value. It is important to note
that particle ﬁltering performance does not depend on the initial
but it does depend on the initial value
of the mean term, e.g.,
. Therefore, the
is initialized by
setting it equal to the estimated mean of the nonvoiced portions
of the audio recordings, which can be obtained through voice
activity detection.
B. Blind Dereverberation
Dereverberation is the next stage in the process of acoustic
environment identiﬁcation (AEI). Blind dereverberation is
the separation of the reverberant and dry signal from a single
channel audio recording without exploiting knowledge of
the room impulse response,
. Dereverberation is a
widely used method with application ranging from speech enhancement to distant speech recognition, acoustic environment
identiﬁcation, etc. It is important to note that speech recognition
applications rely on the direct signal component,
, whereas,
the AEI applications rely on the reverberant component,
the reverberant component
embodies the characterization
of the acoustic environment. The goal of dereverberation here
is to estimate
from the enhanced (denoised) recording,
and use it for the AEI. The output of the denoising stage
can be expressed as
represents
direct sound component, also referred as the dry signal, and
which represents the reverberant component. Furthermore,
the reverberant signal
can be expressed as,
represents the convolution operation and
represents acoustic environment (or room) impulse response.
Under the assumption of a stationary acoustic environment,
can be modeled by a ﬁnite impulse response (FIR)
ﬁlter, provided that ﬁlter is of sufﬁcient length . Shown in
Fig. 3 is an example of a typical room impulse response. An
ZHAO AND MALIK: AUDIO RECORDING LOCATION IDENTIFICATION USING ACOUSTIC ENVIRONMENT SIGNATURE
Fig. 3. Block-based representation of room impulse response (RIR).
impulse response can also be expressed in the frequency domain
, as shown in Fig. 3. The Fourier representation of the
impulse response provides us with both the magnitude response
and the phase response. Changing the location of a sound source
or the microphone does not have much effect on the magnitude
response, whereas, it does have a pronounced effect on the phase
response. Fortunately, the human auditory system is relatively
insensitive to phase over a short period of time. So, instead of
estimating the phase information, the phase of the reverberant
input signal will be used to approximate the phase of the original
dry signal. To this end, the magnitude of the reverberant signal,
, is estimated using block-based processing of
following section provides the details of estimating the
It can be observed from Fig. 3 that the room impulse response,
, can be divided into
blocks consisting of
(with corresponding frequency domain representation
). Assuming
that each ﬁlter block is of same length, say
units of time. The
can be expressed using block-based FIR ﬁltering as,
Likewise, the reverberant signal component,
, can be expressed as,
Similarly, the observation signal
and reverberant component can be expressed in frequency domain as,
The effect of an FIR ﬁlter can be reversed using an appropriate inﬁnite impulse response (IIR) ﬁlter. The dry signal
therefore can be recovered from
using such a ﬁlter. For
example, if the FIR ﬁlter response (
known then the reverberant component
can be estimated
using (28). Under blind dereverberation however estimation of
FIR ﬁlter response from a monophonic audio recording,
an ill-posed problem. To overcome this problem, a perceptually
relevant estimation of FIR ﬁlter block is used. The perceptually
relevant estimates,
, are estimated
from magnitude response of the FIR ﬁlter blocks, i.e.,
Details of the reverberant component estimation are provided
1) Estimating the Room Impulse Response (RIR): The room
impulse response is estimated from the audio recording in the
frequency domain,
. More speciﬁcally, a block-based
framework based on spectral subtraction is used for blind RIR
estimation from
. Consider the audio frame containing
the direct signal
only. This is generally the ﬁrst frame of
the voiced part of the audio recording, that is,
The impulse response for the ﬁrst block
is estimated by calculating ratio of the magnitude of the second
, to that of a previous block
. Similarly,
impulse response for the
is estimated
by calculating the ratio of the magnitude of the current block,
, to that of the previous block
, that is,
is a small positive
The minimum of the above ratio corresponds to the fastest
rate of decay which is used to estimate
. It is important to mention that the unbounded rate of decay may lead to
nonreal acoustic spaces. The following constraints are used to
ensure a bounded impulse response estimate
where the frequency-dependent
reﬂects the
type of decay expected in real reverberant systems.
Finally, frequency-dependent temporal smoothing is applied
to obtain a stable estimate of the room impulse response as,
indicates the current time frame, and
frequency-dependent parameter that controls the amount of
The current estimates of RIR
used for reverberant component estimation. For the following
frames, the
is updated recursively to obtain a stable
RIR estimate.
2) Reverberant Component Estimation: With the perceptually relevant estimates,
, (27) can be expressed as,
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 8, NO. 11, NOVEMBER 2013
To better understand this operation, consider the process for
a given block of the input signal
, which consists of the
current block of the dry signal convolved with
the previous block of the dry signal convolved with
and so on. The operation of the block-based IIR structure can be
described mathematically as,
is an estimate of the true value of
block size
is chosen to be small enough, setting
1 is reasonable, so (34) now can be expressed as,
Here estimate of the reverberant signal component (the second
term in the above (35)) can be expressed as,
The block-based impulse response is approximated by the
magnitude of the frequency domain representations of the
blocks. Lack of phase information leads us to make the following perceptually motivated approximations,
With these approximations, the necessary gains required to
, is deﬁned as,
Substituting (38) into (42), the gain can be expressed as,
To avoid the gain being negative, it should be limited to an
appropriate range. In order to guarantee the reverberant component extraction process is stable, the
should not exceed
Fig. 4. Dereverberation experimental result for a weakly reverberant environment in the presence of weak background noise: shown in the top panel is the
plot of the test recording
, in the middle panel is the estimated dry speech
, and in the bottom panel is the estimated reverberant speech
The gain for the reverberant component estimation
can be expressed as,
Smoothing is used to mitigate any abrupt changes in the estimated gain,
indicates the current time frame of the process and
ranges between 0 and 1. It determines the amount of smoothing
that is applied to gain vectors at each frequency over time.
The estimated reverberant component in the frequency-domain can be expressed as,
The above block-based blind reverberant component estimation process is repeated for each frame of the input signal.
The effectiveness of the block-based blind dereverberation
method discussed in this Section is tested for a speech recording
captured in a small ofﬁce. Shown in Fig. 4 are the temporal
plots of the test recording captured in a reverberant environment
(top), estimated dry signal (middle), and estimated reverberant
signal (bottom). It can be observed from Fig. 4 that the blind
dereverberation scheme is effective in separating the dry and
reverberant signal from the test speech recording.
Effectiveness of this method has also been evaluated through
a number of experiments on speech recordings of two speakers
reading different texts and made in four different environments
with different levels of reverberation and ambient noise. Due
to space limitation, dereverberation results for a highly reverberant acoustic environment with strong background noise are
included here. Shown in Fig. 5 are the temporal plots of the
test recording made in a highly reverberant environment with
strong background noise (top), estimated dry signal (middle),
and estimated reverberant signal (bottom). It can be observed
from Fig. 5 that even for highly reverberant environment with
the presence of strong background noise, this method still works
at an acceptable level.
ZHAO AND MALIK: AUDIO RECORDING LOCATION IDENTIFICATION USING ACOUSTIC ENVIRONMENT SIGNATURE
Fig. 5. Dereverberation experimental result for a highly reverberant environment in the presence of strong background noise: shown in the top panel is the
plot of the test recording
, in the middle panel is the estimated dry speech
, and in the bottom panel is the estimated reverberant speech
C. Acoustic Feature Fusion
The background noise and the reverberant components
estimated from the test-recording are fused to obtain a feature
vector characterizing the acoustic environment. A feature
vector consisting of feature vectors estimated from background
and the reverberant component
is obtained.
Estimated background noise and reverberant component can
be modeled as the output of RIR ﬁlter
. Cepestral
analysis is therefore performed to capture traces of acoustic
environment. We have shown in , that Mel-frequency
Cepstral Coefﬁcients (MFCC) and Logarithmic Mel-spectral
Coefﬁcients (LMSC) perform signiﬁcantly better than other
features such as DFT (discrete Fourier transform), DCT (discrete cosine transform), etc.
The motivation behind selecting MFCC for acoustic environment identiﬁcation can be justiﬁed base on the fact that the
MFCC has been successfully used for speaker recognition applications. In speaker recognition systems, the speech signal
is generally modeled as
is the vocal track impulse response,
represents the excitation source and
represents the convolution operator. The
vocal track impulse response
is used to fully characterize each speaker. In MFCC-based speaker recognition systems, the MFCC relies on the
to discriminate between
the speakers. There exists a strong analogy between the acoustic
environment identiﬁcation process and the speaker recognition
process. As, an acoustic environment is also characterized by
the acoustic impulse response,
. The recording made in
an acoustic environment with impulse response
modeled as,
is the recording
is the direct signal. It is therefore expected that the
MFCC/LMSE will perform well for the acoustic environment
identiﬁcation task.
To extract feature vector, estimated components
are transformed into the cepstral domain. More speciﬁcally, MFCC and LMSC are computed from both
To this end, a 60-D feature vector (consisting of 30-D MFCC
and 30-D LMSC vectors) is obtained from
. Similarly, a
60-D feature vector is also obtained from
. It is important
Fig. 6. Flowchart of feature extraction stage.
to mention that the PF-based noise estimation method works
in the logarithmic spectral domain, e.g.,
is the LMSC.
The 30-D feature vector consisting of MFCC is therefore
estimated by calculating forward DCT of
. In addition, for
each estimated component, four higher-order statistics, i.e.,
mean, variance, skewness, and kurtosis are also used to capture
the characteristic of environments which resulted in a 128-D
feature vector.
The proposed framework for joint background noise and reverberation estimation is summarized in the Fig. 7 and details of
each processing block is provided in the following Algorithm.
Algorithm: Outline of each step of our proposed framework
The following processing steps are used to estimate noise
and reverberation components from the input audio.
1) Blind Dereverberation
The reverberant component
is estimated
according to (25)–(32).
2) Spectral Estimation
The spectral estimation is achieved by segmenting
the input audio into 25 ms overlap blocks with 50%
overlapping followed by temporal smoothing using
2048-point Hamming window, a frequency domain
transformation using the magnitude square of a
2048-point DFT, ﬁltering using Mel-ﬁltering bank
and rescaling using the logarithmic operator. The
ﬂowchart of the spectral estimation process is shown
in Fig. 6.
3) Prior Noise Density Estimation
The prior noise density
, required to initialize
the particle ﬁlter, is estimated via (23). The parameters
are trained from the noise-only signal, which is
obtained by the voice activity detection process.
4) Particle Evolution
All particles,
are propagated by the
state transition function described in (11).
5) Prediction Model Updating
The prediction model given in (11) is updated for each
iteration and model parameters are updated using (10)
6) Noise Evaluation
The noise samples
are evaluated according to (20)
7) Importance Resampling
The weights are resampled to avoid the degeneracy
8) Steps: 2 to 7 are repeated until all frames are
processed.
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 8, NO. 11, NOVEMBER 2013
Fig. 7. Proposed framework for generating feature vector used for AEI. Solid
arrow represents the ﬂow of the signal. The part highlighted by dotted rectangles
represents the particle ﬁltering-based noise estimation.
DESCRIPTION OF ACOUSTIC ENVIRONMENTS CONSIDERED
MICROPHONES USED
IV. PERFORMANCE EVALUATION
The effectiveness of the proposed framework is evaluated
using a data set of human speech. Details for the data set used,
experimental setup and experimental results are provided next.
A. Data Set
The performance of the proposed scheme is evaluated using
a data set consisting of more than 2240 speech recordings.
Speech recordings of two speakers, a male and a female, were
recorded with four different types of microphones in eight
different acoustic environments. A description of each acoustic
environment is listed in Table I. Audio was recorded using four
commercial-grade microphones with 44.1 kHz sampling rate
and 16 bits/sample resolution. Details of makes and models of
the microphones used are provided in Table II.
CLASSIFICATION PERFORMANCE OF MICROPHONE
INDEPENDENT TEST
B. Experimental Setup
Given the input audio,
, the background noise,
, is estimated according to the Algorithm described above. To extract
the reverberation component from the input audio recording, the
reverberation estimation parameters
are set to 30, 1.1, 0.0001 and 0.5 respectively.
extraction
reverberation
component,
pre-emphasized
. The pre-emphasized signal is decomposed into overlapping frames with
25 ms duration and shift of 10 ms. For each frame, a 60-D
feature vector is extracted. Similar procedure is applied to the
observation
, which is used for noise estimation. Noise is
estimated in the Mel-Cepstral domain so that input audio is
pre-emphasized according to
applying the particle ﬁltering.
For classiﬁcation, a multiclass Support Vector Machine
(SVM) with a radial basis kernel function (RBF) is used.
The kernel selection of SVM plays a role on the classiﬁcation
accuracy. Motivation behind selecting SVM with RBF kernel
is that it outperforms the linear and the polynomial kernels
 , . In addition, the SVM with RBF also performs
signiﬁcantly better than other classiﬁers such as Fisher Linear
Discernment , Logistic regression . For each experiment, the optimal parameters for the classiﬁer are determined
using a grid-search technique with ﬁve-fold cross-validation on
the training data. The kernel parameter
(for all the kernels)
(for RBF and polynomial kernels) were carried out on
the multiplicative grid
. Each experiment is repeated 10
times and classiﬁcation accuracy averaged over all runs is used
for performance evaluation.
C. Experimental Results
1) Microphone Independent Test: In our ﬁrst experiment, we
investigated the microphone independent acoustic environment
identiﬁcation performance. The motivation behind using a microphone independent analysis is that for AEI we do not have
any knowledge of the microphone used for the recording. So,
acoustic environment identiﬁcation algorithm should be robust
(not sensitive) to the type of microphones used for recording. To
this end, we considered a two-class identiﬁcation scenario, that
is, the identiﬁcation of two acoustic environments, say
. The SVM classiﬁer was trained using feature vectors extracted from the recordings made in acoustic environments
with microphone
. The trained classiﬁer was tested
using recordings made in acoustic environments
microphones
Shown in Table III is the classiﬁcation accuracy for the
microphone independent test. Shown in the second column
ZHAO AND MALIK: AUDIO RECORDING LOCATION IDENTIFICATION USING ACOUSTIC ENVIRONMENT SIGNATURE
CLASSIFICATION PERFORMANCE OF SEMIBLIND ENVIRONMENT IDENTIFICATION
CLASSIFICATION PERFORMANCE OF BLIND ENVIRONMENT IDENTIFICATION
USING REVERBERATION AND NOISE FEATURES
of Table III are the classiﬁcation performance for the original
audio recordings without dereverberation or denoising, that is,
the feature vector is extracted from the original audio directly.
Classiﬁcation performance for the proposed framework is
shown in the third column. It can be observed from Table III
that the AEI performance for the proposed method is independent of the microphone type used. It is important to note that
the original audio case classiﬁcation performance is not stable,
since, it varies from 87.5% to 97.22%. However, the performance of the proposed method is stable and on average higher
than the original audio case. One of the reasons for improved
AEI performance for the proposed method is that the training
and testing recordings come from the same environments so
the source mismatch problem does not exist. It is reasonable to
claim that the proposed reverberation-based method not only
improves the AEI performance but is also independent of the
microphone used.
2) Semi-blind Environment Identiﬁcation: In our second experiment, we considered a four-class blind classiﬁcation scenario, that is, the SVM classiﬁer is trained using recordings
made in environments
and tested on the
recordings made in environments
. For this
experiment recording made with
Shown in Table IV is the AEI performance of the original audio
(second column), proposed method using reverberation component only (third column), and proposed method using reverberation + background noise components (fourth column). It can be
observed from Table IV that dereverberation provided on average 6.7% performance improvement averaged over all microphones. Merging dereverberation and noise estimation provided
addition 1% performance improvement. It can also be observed
from Table IV (fourth column) that AEI using joint reverberation and noise components on average does improve the classi-
ﬁcation performance.
3) Blind Environment Identiﬁcation: In the third experiment, performance of the proposed framework is tested for the
blind AEI case, that is, knowledge of the microphone (used
for making the recording) and the recording environment are
not exploited during classiﬁcation process. For this test, the
SVM classiﬁer was trained using feature vectors based on joint
reverberation and noise components extracted from recordings
made in environments
using microphone
, and tested on the recordings captured in environments
CONFUSION MATRIX OF BLIND ENVIRONMENT IDENTIFICATION
USING REVERBERATION AND NOISE FEATURES FOR
CONFUSION MATRIX OF BLIND ENVIRONMENT IDENTIFICATION
USING REVERBERATION AND NOISE FEATURES FOR
TABLE VIII
CONFUSION MATRIX OF BLIND ENVIRONMENT IDENTIFICATION
USING REVERBERATION AND NOISE FEATURES FOR
using microphones
. The classiﬁcation performance for the four-class blind environments
identiﬁcation are shown in Table V.
It can be observed from Table V that classiﬁcation performance for the original recordings deteriorates signiﬁcantly
(roughly 18% on average) compared with the semi-blind case
(see Table IV). However, performance for the proposed system
deteriorates marginally, that is, it decreases only about 1% on
average compared with the semi-blind case (see Table IV).
Similar observations were made when we trained the classiﬁer
on recordings collected using microphone
tested on the recordings made using the rest of the microphones.
Blind identiﬁcation performance of the proposed system is
also evaluated using confusion matrix based measure. To this
end, confusion matrices are computed for all three microphones.
Shown in Tables VI–VIII are the confusion matrices for
, respectively. It can be observed from Tables VI–VII
that the error distribution is slightly dependent on a given microphone types. Moreover, it can also be observed that for all microphones classiﬁcation errors for
are slightly higher
, and the false rates between
also higher. The higher error for
can be attributed to
the similarity in the acoustic environment structure, that is, both
acoustic environments, e.g.,
have concrete ﬂoors.
It is worth mentioning that classiﬁcation accuracy of SVM
classiﬁer depends on the underlying kernel used. In AEI results
shown in Table V are obtained for RBF kernel. Superior classi-
ﬁcation accuracy is the major motivation behind selection RBF
kernel , . To validate this claim, we compared classiﬁcation performance of SVM with RBF kernel with SVM with
IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 8, NO. 11, NOVEMBER 2013
BLIND ENVIRONMENT IDENTIFICATION PERFORMANCE COMPARISON
OF SVM WITH RBF, LINEAR, AND POLYNOMIAL KERNELS
Fig. 8. Classiﬁcation performance of blind environment identiﬁcation under
MP3 compression with various compression biterates.
linear and SVM with polynomial kernels. The this end, the blind
environment identiﬁcation experiment was repeated for both the
linear and the polynomial kernels with same conﬁgurations (that
is, training on
and testing on the rest of the microphones)
and similar parameter selection as used for RBF kernel. The average classiﬁcation accuracies for RBF, linear, and polynomial
kernels are shown in Table IX.
It can be observed from Table IX that on average RBF kernel
performs signiﬁcantly better that both the linear and the polynomial kernels. It can also be observed that the polynomial kernel
performs relatively better than the linear kernel, which is not a
surprising observation.
4) Robustness Against MP3 Compression: In our fourth experiment, we tested the robustness of our scheme against MP3
compression attacks. To this end, audio recordings were compressed into MP3 format using ffmpeg with bitrates
. For feature extraction, the compressed recordings were converted back to wave
format. The SVM classiﬁer was trained using feature vectors
based on joint reverberation and noise components extracted
from decompressed recordings made in environments
using microphone
, and tested on the recordings captured in environments
using microphones
The average classiﬁcation performance, averaged over
100 runs, of the proposed scheme for blind identiﬁcation of the
data set compressed using biterates ranging from 32 to 320 kbps
is shown in Fig. 8. Here, the dashed-lines with circles, squares
and cross represent the classiﬁcation results for compressed
recordings (without dereverberation) made with microphones
, respectively; and the solid lines triangle, stars
and diamonds represent the blind identiﬁcation performance of
the proposed scheme.
Fig. 9. Cascaded feature extraction framework.
CLASSIFICATION PERFORMANCE OF CASCADED FEATURE EXTRACTION
It can be observed from Fig. 8 that the performance of the
proposed scheme is robust to MP3 compression attack. It can
also be observed from both the Table V and Fig. 8 that microphone
are more sensitive to MP3 compression,
since, compression attack resulted in an average performance
drop of 4.05% and 6.91%, respectively. Whereas, microphones
is experienced a negligible performance degradations of
1.12%. Whatever the compression bitrate, our proposed scheme
has better performance than that of using original audio.
Another interesting observation can be made from Fig. 8 that
classiﬁcation performance is relatively more unstable performance for bitrate
. Performance ﬂuctuation in low bitrate can be attributed to relatively unstable particle ﬁltering performance in the presence of strong distortion due to aggressive
MP3 compression. In addition, distortion due to MP3 compression might have also resulted in dereverberation performance
degradation.
5) Performance Comparison-Parallel Vs Cascaded Framework: In our ﬁfth experiment, performance of the proposed
framework for parallel reverberation and noise estimation is
compared with the cascaded architecture, that is, noise estimation followed by reverberation estimation. The feature extraction from the cascaded architecture is shown in Fig. 9. Here, the
particle ﬁltering (PF) stage estimates background noise from the
input audio and the output of PF stage is applied at the input of
the blind dereverberation stage for reverberant component estimation. For each architecture, the SVM classiﬁer was trained
on the recordings made in environments
microphone
, and tested on the recordings made in environments
using microphones
Classiﬁcation performance for the cascaded framework is
shown in Table X. It can be observed from Table X that the
cascaded framework does provide a marginal performance
improvement (1% to be exact) for
; whereas, there is a signiﬁcant performance degradation for
. The performance
degradation for cascade framework can be attributed to the fact
ZHAO AND MALIK: AUDIO RECORDING LOCATION IDENTIFICATION USING ACOUSTIC ENVIRONMENT SIGNATURE
CLASSIFICATION ACCURACY USING TDSM FEATURES
that denoising removes some of the late reverberations. The
distortion introduced by denoising stage propagates to the dereverberation stage which also results in performance degradation.
This experiment also indicates that the performance of the
cascaded framework is strongly dependent on the microphone
sensitivity. The performance degradation for a low quality microphone
might be attributed to the following: (1) Denoising
removes the late reverberation which is the primary contributor
of the AEI, and (2) Distortion introduced by denoising decreases
the dereverberation accuracy. The parallel architecture of the
proposed framework, on the other hand, decouples both signal
estimation stages, that is, the noise and reverberation estimation stages. In case of parallel architecture, the artifacts in one
estimation stage does not deteriorate estimation performance of
the other stage. The parallel architecture is therefore expected
perform better than the cascade architecture. The classiﬁcation
performance shown in Tables V and X conﬁrms this claim. It
is worth mentioning that parallel architecture is not an optimal
solution to this interdependent component estimation problem.
We have observed through extensive experimentation that both
the background noise and the reverberation components are
tightly coupled. The optimal solution therefore would require
joint estimation of both the noise and reverberation. In our
future work we will investigate this issue.
6) Performance Comparison-Using TDSM-Based Features:
In our ﬁnal experiment, we compare the performance of proposed framework using existing state-of-the-art features used in
audio steganalysis – . Liu et al. – proposed Temporal Derivative-based Spectrum and Mel-Cepstrum (TDSM)
features for audio steganalysis. Liu et al. in – , have experimentally conﬁrmed that the TDSM-based features are effective for audio steganalysis, and exhibit superior performance
than existing state-of-the-art .
To investigate impact of the TDSM-based features on the performance of the proposed AEI framework, we perform blind
AEI using temporal derivative-based spectrum and Mel-Cepstrum features instead of MFCC and LMCC. Shown in Table XI
are the detection performance for TDSM-based feature vector.
The results shown in Table XI are obtained using the same experimental setting as used in Section IV-C3.
It can be noticed from Table XI that the classiﬁcation accuracies decrease signiﬁcantly for TDSM when compared with
the used MFCC-based features. In addition, the performance
for TDSM-based features is highly dependent on the microphone sensitivity. For example, for built-in microphone
TDSM-based AEI exhibits the worst accuracy (e.g., 38.19%)
which is relatively better than the random guessing (e.g., 25%
for four-class classiﬁcation). The performance degradation due
to TDSM-based features can be attributed to the following reasons: (1) As, TDSM-based feature extraction uses second-order
derivative (a kind of high-pass ﬁlter) which removes the speech
content as well as the reverberations (the main contributor of environment identiﬁcation), and (2) The speech band ﬁlter used in
Liu’s feature extraction methods – removes the speech
component along with the reverberation component.
V. CONCLUSION
In this paper, a novel method for acoustic environment identiﬁcation (AEI) is proposed. Acoustic reverberations and background noise are used to characterize the acoustic environment.
Background noise is modeled using a dynamical system and estimated using particle ﬁltering. A blind dereverberation method
based on spectral subtraction and inverse ﬁltering is used to estimate the reverberation component. Both the background noise
and the reverberation components are used for feature extraction. An 128-D feature vector consisting of MFCC, LMSC, and
higher order statistics is used to characterize the acoustic environment. The SVM based classiﬁer is used for the AEI. Performance of the proposed scheme is evaluated using a data set consisting 2240 speech recordings made with four different type of
microphones in eight different acoustic environments. The proposed method is tested for various experimental settings such as
microphone independent, semi- and full-blind AEI and robustness against MP3 compression attacks. In addition, performance
of the proposed framework is also evaluated using Temporal
Derivative-based Spectrum and Mel-Cepstrum (TDSM)-based
features. Experimental results show that the proposed method
improves AEI performance compared with the direct method
(i.e., the feature vector is extracted from the audio recording
directly). In addition, the proposed scheme is robust to MP3
compression.
Currently we are investigating joint reverberation and background noise estimation and extensions of the proposed method
to audio forensic application.
ACKNOWLEDGMENT
The authors would like to thank Prof. Hongxia Wang for the
useful discussion and funding support.