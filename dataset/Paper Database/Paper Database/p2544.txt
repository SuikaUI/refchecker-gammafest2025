Pivotal Tuning for Latent-based Editing of Real Images
Daniel Roich1, Ron Mokady1, Amit H. Bermano1, and Daniel Cohen-Or1
1The Blavatnik School of Computer Science, Tel Aviv University
Recently, a surge of advanced facial editing techniques
have been proposed that leverage the generative power of
a pre-trained StyleGAN. To successfully edit an image this
way, one must ﬁrst project (or invert) the image into the
pre-trained generator’s domain. As it turns out, however,
StyleGAN’s latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining
the original appearance and convincingly altering some of
its attributes. Practically, this means it is still challenging
to apply ID-preserving facial latent-space editing to faces
which are out of the generator’s domain.
In this paper,
we present an approach to bridge this gap. Our technique
slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The
key idea is pivotal tuning — a brief training process that
preserves the editing quality of an in-domain latent region,
while changing its portrayed identity and appearance. In
Pivotal Tuning Inversion (PTI), an initial inverted latent
code serves as a pivot, around which the generator is ﬁnedtuned. At the same time, a regularization term keeps nearby
identities intact, to locally contain the effect. This surgical training process ends up altering appearance features
that represent mostly identity, without affecting editing capabilities. To supplement this, we further show that pivotal tuning can also adjust the generator to accommodate
a multitude of faces, while introducing negligible distortion on the rest of the domain. We validate our technique
through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced
edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities.
we demonstrate resilience to harder cases, including heavy
make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited
by state-of-the-art methods. Source code can be found at:
 
Figure 1. Pivotal Tuning Inversion (PTI) enables employing offthe-shelf latent-based semantic editing techniques on real images using StyleGAN. PTI excels in identity preserving edits,
portrayed through recognizable ﬁgures — Serena Williams and
Robert Downey Jr. (top), and in handling faces which are clearly
out-of-domain, e.g., due to heavy makeup (bottom).
1. Introduction
In recent years, unconditional image synthesis has made
huge progress with the emergence of Generative Adversarial Networks (GANs) . In essence, GANs learn the
domain (or manifold) of the desired image set and produce new samples from the same distribution. In particular, StyleGAN is one of the most popular choices
for this task. Not only does it achieve state-of-the-art visual
 
Before PTI
Figure 2. An illustration of the PTI method. StyleGAN’s latent space is portrayed in two dimensions (see Tov et al. ), where the
warmer colors indicate higher densities of W, i.e. regions of higher editability. On the left, we illustrate the generated samples before
pivotal tuning. We can see the Editability-Distortion trade-off. A choice must be made between Identity ”A” and Identity ”B”. ”A”
resides in a more editable region but does not resemble the ”Real” image. ”B” resides in a less editable region, which causes artifacts, but
induces less distortion. On the right - After the pivotal tuning procedure. ”C” maintains the same high editing capabilities of ”A”, while
achieving even better similarity to ”Real” compared to ”B”.
ﬁdelity and diversity, but it also demonstrates fantastic editing capabilities due to an organically formed disentangled
latent space. Using this property, many methods demonstrate realistic editing abilities over StylGAN’s latent space
 , such as changing facial orientations, expressions, or age, by traversing the learned manifold.
While impressive, these edits are performed strictly in
the generator’s latent space, and cannot be applied to real
images that are out of its domain. Hence, editing a real
image starts with ﬁnding its latent representation. This process, called GAN inversion, has recently drawn considerable attention . Early attempts inverted
the image to W — StyleGAN’s native latent space. However, Abdal et al. have shown that inverting real images to this space results in distortion, i.e. a dissimilarity
between the given and generated images, causing artifacts
such as identity loss, or an unnatural appearance. Therefore, current inversion methods employ an extended latent
space, often denoted as W+, which is more expressive and
induces signiﬁcantly less distortion .
However, even though employing codes from W+ potentially produces great visual quality even for out-ofdomain images, these codes suffer from weaker editability,
since they are not from the generator’s trained domain. Tov
et al. deﬁne this conﬂict as the distortion-editability
tradeoff, and show that the closer the codes are to W, the
better their editability is. Indeed, recent works 
suggest a compromise between edibility and distortion, by
picking latent codes in W+ which are more editable.
In this paper, we introduce a novel approach to mitigate
the distortion-editability trade-off, allowing convincing edits on real images that are out-of-distribution. Instead of
projecting the input image into the learned manifold, we
augment the manifold to include the image by slightly altering the generator, in a process we call pivotal tuning. This
adjustment is analogous to shooting a dart and then shifting
the board itself to compensate for a near hit.
Since StyleGAN training is expensive and the generator
achieves unprecedented visual quality, the popular approach
is to keep the generator frozen. In contrast, we propose producing a personalized version of the generator, that accommodates the desired input image or images. Our approach
consists of two main steps. First, we invert the input image to an editable latent code, using off-the-shelf inversion
techniques. This, of course, yields an image that is similar
to the original, but not necessarily identical. In the second
step, we perform Pivotal Tuning — we lightly tune the pretrained StyleGAN, such that the input image is generated
when using the pivot latent code found in the previous step
(see Figure 2 for an illustration.). The key idea is that even
though the generator is slightly modiﬁed, the latent code
keeps its editing qualities. As can be seen in our experiments, the modiﬁed generator retains the editing capabili-
ties of the pivot code, while achieving unprecedented reconstruction quality. As we demonstrate, the pivotal tuning is
a local operation in the latent space, shifting the identity of
the pivotal region to the desired one with minimal repercussions. To minimize side-effects even further, we introduce
a regularization term, enforcing only a surgical adaptation
of the latent space. This yields a version of the generator
StyleGAN that can edit multiple target identities without
interference.
In essence, our method extends the high quality editing
capabilities of the pretrained StyleGAN to images that are
out of its distribution, as demonstrated in Figure 1.
validate our approach through quantitative and qualitative
results, and demonstrate that our method achieves stateof-the-art results for the task of StyleGAN inversion and
real image editing. In Section 4, we show that not only do
we achieve better reconstruction, but also superior editability. We show this through the utilization of several existing editing techniques, and achieve realistic editing even on
challenging images. Furthermore, we conﬁrm that using
our regularization restricts the pivotal tuning side effect to
be local, with negligible effect on distant latent codes, and
that pivotal tuning can be applied for multiple images simultaneously to incorporate several identities into the same
model (see Figure 3). Finally, we show through numerous
challenging examples that our pivotal tuning-based inversion approach achieves completely automatic, fast, faithful,
and powerful editing capabilities.
2. Related Work
2.1. Latent Space Manipulation
Most real-life applications require control over the generated image.
Such control can be obtained in the unconditional setting, by ﬁrst learning the manifold, and
then realizing image editing through latent space traversal.
Many works have examined semantic directions in
the latent spaces of pre-trained GANs. Some using fullsupervision in the form of semantic labels ,
 ﬁnd meaningful directions in a selfsupervised fashion, and ﬁnally recent works present unsupervised methods to achieve the same goal , requiring no manual annotations.
More speciﬁcally for StyleGAN, Shen et al. use supervision in the form of facial attribute labels to ﬁnd meaningful linear directions in the latent space. Similar labels
are used by Abdal et al. to train a mapping network
conditioned on these labels. Harkonen et al. identify
latent directions based on Principal Component Analysis
(PCA). Shen et al. perform eigenvector decomposition
on the generator’s weights to ﬁnd edit directions without additional supervision. Collins et al. borrow parts of the
latent code of other samples to produce local and semantically aware edits. Wu et al. discover disentangled editing controls in the space of channel wise style parameters.
Other works focus on facial editing, as they utilize
a prior in the form of a 3D morphable face model. Most recently, Patashnik et al. utilize a contrastive languageimage pre-training (CLIP) models to explore new editing capabilities. In this paper, we demonstrate our inversion
approach by utilizing these editing methods as downstream
tasks. As seen in Section 4, our PTI process induces higher
visual quality for several of these popular approaches.
2.2. GAN inversion
As previously mentioned, in order to edit a real image
using latent manipulation, one must perform GAN inversion
 , meaning one must ﬁnd a latent vector from which the
generator would generate the input image. Inversion methods can typically be divided into optimization-based ones
— which directly optimize the latent code using a single
sample , or encoder-based ones — which train
an encoder over a large number of samples .
Many works consider speciﬁcally the task of StyleGAN inversion, aiming at leveraging the high visual quality and editability of this generator. Abdal et al. demonstrate that it
is not feasible to invert images to StyleGAN’s native latent
space W without signiﬁcant artifacts. Instead, it has been
shown that the extended W+ is much more expressive, and
enables better image preservation. Menon et al. use
direct optimization for the task of super-resolution by inverting a low-resolution image to W+ space. Zhu et al. 
use a hybrid approach: ﬁrst, an encoder is trained, then a direct optimization is performed. Richardson et al. were
the ﬁrst to train an encoder for W+ inversion which was
demonstrated to solve a variety of image-to-image translation tasks.
2.3. Distortion-editability tradeoff
Even though W+ inversion achieves minimal distortion, it has been shown that the results of latent manipulations over W+ inversions are inferior compared to the
same manipulations over latent codes from StyleGAN’s native space W. Tov et al. deﬁne this as the distortioneditability tradeoff, and design an encoder that attempts to
ﬁnd a ”sweet-spot” in this trade-off.
Similarly, the tradeoff was also demonstrated by Zhu et
al. , who suggests an improved embedding algorithm
using a novel regularization method. StyleFlow also
concludes that real image editing produces signiﬁcant artifacts compared to images generated by StyleGAN. Both
Zhu et al. and Tov et al. achieve better editability compared
to previous methods but also suffer from more distortion.
In contrast, our method combines the editing quality of W
inversions with highly accurate reconstructions, thus mitigating the distortion-editability tradeoff.
2.4. Generator Tuning
Typically, editing methods avoid altering StyleGAN, in
order to preserve its excellent performance. Some works,
however, do take the approach we adopt as well, and tune
the generator. Pidhorskyi et al. train both the encoder
and the StyleGAN generator, but their reconstruction results
suffer from signiﬁcant distortion, as the StyleGAN tuning
step is too extensive. Bau et al. propose a method for
interactive editing which tunes the generator proposed by
Karras et al. to reconstruct the input image.
claim, however, that directly updating the weights results
in sensitivity to small changes in the input, which induces
unrealistic artifacts. In contrast, we show that after directly
updating the weights, our generator keeps its editing capabilities, and demonstrate this over a variety of editing techniques. Pan et al. invert images to BigGAN’s latent
space by optimizing a random noise vector and tuning the
generator simultaneously. Nonetheless, as we demonstrate
in Section 4, optimizing a random vector decreases reconstruction and editability quality signiﬁcantly for StyleGAN.
Our method seeks to provide high quality editing for a
real image using StyleGAN. The key idea of our approach is
that due to StyleGAN’s disentangled nature, slight and local
changes to its produced appearance can be applied without
damaging its powerful editing capabilities. Hence, given
an image, possibly is out-of-distribution in terms of appearance (e.g., real identities, extreme lighting conditions,
heavy makeup, and/or extravagant hair and headwear), we
propose ﬁnding its closest editable point within the generator’s domain. This pivotal point can then be pulled toward
the target, with only minimal effect in its neighborhood,
and negligible effect elsewhere. In this section, we present
a two-step method for inverting real images to highly editable latent codes. First, we invert the given input to wp
in the native latent space of StyleGAN, W. Then, we apply a Pivotal Tuning on this pivot code wp to tune the pretrained StyleGAN to produce the desired image for input
The driving intuition here is that since wp is close
enough, training the generator to produce the input image from the pivot can be achieved through augmenting
appearance-related weights only, without affecting the wellbehaved structure of StyleGAN’s latent space.
3.1. Inversion
The purpose of the inversion step is to provide a convenient starting point for the Pivotal Tuning one (Section 3.2).
As previously stated, StyleGAN’s native latent space W
provides the best editability. Due to this and since the distortion is diminished during Pivotal Tuning, we opted to invert
the given input image x to this space, instead of the more
popular W+ extension. We use an off-the-shelf inversion
method, as proposed by Karras et al. . In essence, a direct optimization is applied to optimize both latent code w
and noise vector n to reconstruct the input image x, measured by the LPIPS perceptual loss function . As described in , optimizing the noise vector n using a noise
regularization term improves the inversion signiﬁcantly, as
the noise regularization prevents the noise vector from containing vital information. This means that once wp has been
determined, the n values play a minor role in the ﬁnal visual appearance. Overall, the optimization deﬁned as the
following objective:
wp, n = arg min
LLPIPS(x, G(w, n; θ)) + λnLn(n),
where G(w, n, θ) is the generated image using a generator
G with weights θ. Note that we do not use StyleGAN’s
mapping network (converting from Z to W). LLPIPS denotes the perceptual loss, Ln is a noise regularization term
and λn is a hyperparameter. At this step, the generator remains frozen.
3.2. Pivotal Tuning
Applying the latent code w obtained in the inversion,
produces an image that is similar to the original one x, but
may yet exhibit signiﬁcant distortion. Therefore, in the second step, we unfreeze the generator and tune it to reconstruct the input image x given the latent code w obtained in
the ﬁrst step, which we refer to as the pivot code wp. As we
demonstrate in Section 4, it is crucial to use the pivot code,
since using random or mean latent codes lead to unsuccessful convergence. Let xp = G(wp; θ∗) be the generated image using wp and the tuned weights θ∗. We ﬁne tune the
generator using the following loss term:
Lpt = LLPIPS(x, xp) + λL2LL2(x, xp),
where the generator is initialized with the pretrained
weights θ. At this step, wp is constant. The pivotal tuning can trivially be extended to N images {xi}N
i=0, given
the N inversion latent codes {wi}N
(LLPIPS(xi, xp
i ) + λL2LL2(xi, xp
i = G(wi; θ∗).
Once the generator is tuned, we can edit the input image
using any choice of latent-space editing techniques, such as
those proposed by Shen et al. or Harkonen et al. .
Numerous results are demonstrated in Section 4.
Real Image
Figure 3. Real Images editing example using a Multi-ID Personalized StyleGAN. All depicted images are generated by the same model,
ﬁne-tuned on political and industrial world leaders. As can be seen, applying various edit operations on these newly introduced, highly
recognizable identities preserves them well.
3.3. Locality Regularization
As we demonstrate in Section 4, applying pivotal tuning
on a latent code indeed brings the generator to reconstruct
the input image in high accuracy, and even enables successful edits around it. At the same time, as we demonstrate in
Section 4.3, Pivotal tuning induces a ripple effect — the visual quality of images generated by non-local latent codes
is compromised. This is especially true when tuning for a
multitude of identities (see Figure. 14). To alleviate this side
effect, we introduce a regularization term, that is designed
to restrict the PTI changes to a local region in the latent
space. In each iteration, we sample a normally distributed
random vector z and use StyleGAN’s mapping network f to
produce a corresponding latent code wz = f(z). Then, we
interpolate between wz and the pivotal latent code wp using the interpolation parameter α, to obtain the interpolated
wr = wp + α
Finally, we minimize the distance between the image
generated by feeding wr as input using the original weights
xr = G(wr; θ) and the image generated using the currently
tuned ones x∗
r = G(wr; θ∗):
LR = LLPIPS(xr, x∗
L2LL2(xr, x∗
This can be trivially extended to Nr random latent codes:
(LLPIPS(xr,i, x∗
L2LL2(xr,i, x∗
The new optimization is deﬁned as:
θ∗= arg min
Lpt + λRLR,
L2, λR, Nr are constant positive hyperparameters.
Additional discussion regarding the effects of different α
values can be found in the Supplementary Materials.
Figure 4. Reconstruction of out-of-domain samples. Our method
(right) reconstructs out-of-domain visual details (left), such as face
paintings or hands, signiﬁcantly better than state-of-the-art methods (middle).
Figure 5. Reconstruction quality comparison using examples from
the CelebA-HQ Dataset. As can be seen, even for less challenging inputs, our method offers higher level reconstruction for unseen identities compared to the state-of-the-art. Zoom-in recommended.
4. Experiments
In this section, we justify the design choices made and
evaluate our method.
For all experiments we use the
StyleGAN2 generator . For facial images, we use a
generator pre-trained over the FFHQ dataset , and we
use the CelebA-HQ dataset for evaluation. In addition, we have also collected a handful of images of out-ofdomain and famous ﬁgures, to highlight our identity preservation capabilities, and the unprecedented extent of images
we can handle that could not be edited until now.
We start by qualitatively and quantitatively comparing
our approach to current inversion methods, both in terms of
reconstruction quality and the quality of downstream editing. We use the direct optimization scheme proposed by
Karras et al. to invert real images to W space, which
we denote by SG2. A similar optimization is used to invert to the extended W+ space , denoted by SG2 W+.
We also compare to e4e, the encoder designed by Tov et
ID Similarity ↑
Table 1. Quantitative reconstruction quality. Using a StyleGAN2
generator trained over the FFHQ dataset, we invert images from
the CelebA-HQ test set and measure their reconstruction using
four different metrics. All metrics indicate superior reconstruction
for our method.
al. , which uses the W+ space but seeks to remain relatively close to W. Each baseline inverts to a different part of
the latent space, demonstrating the different aspects of the
distortion-editability trade-off. Note that we do not include
Richardson et al. in our comparisons, since Tov et al.
have convincingly shown editing superiority, rendering this
comparison redundant.
4.1. Reconstruction Quality
Qualitative evaluation. Figures 4 and 5 present a qualitative comparison of visual quality of inverted images. As
can be seen, even before considering editability, our method
achieves superior reconstruction results for all examples, especially for out-of-domain ones, as our method is the only
one to successfully reconstruct challenging details such as
face painting or hands (Figure 4). Our method is also capable of reconstructing ﬁne-details which most people are
sensitive to, such as the make-up, lighting, wrinkles, and
more (Figure 5). For more visual results, see the Supplementary Materials.
Quantitative evaluation. For quantitative evaluation, we
employ the following metrics: pixel-wise distance using
MSE, perceptual similarity using LPIPS , structural
similarity using MS −SSIM , and identity similarity by employing a pretrained face recognition network .
The results are shown in Table 1. As can be seen, the results
align with our qualitative evaluation as we achieve the best
score for each metric by a substantial margin.
4.2. Editing Quality
Editing a facial image should preserve the original identity while performing a meaningful and visually plausible
modiﬁcation. However, it has been shown that using less editable embedding spaces, such as W+, results
in better reconstruction, but also in less meaningful editing compared to the native W space. For example, using
the same latent edit, rotating a face in W space results in a
higher rotation angle compared to W+. Hence, in cases of
minimal effective editing, the identity may seem to be preserved rather well. Therefore, we evaluate editing quality
Figure 6. Editing comparison of images from the CelebA-HQ
dataset. We demonstrate the pose (top) and smile removal (bottom) edits. The edits over SG2 W+ do not create the desired
effect, e.g., mouth is not closed in the bottom row. SG2 and e4e
achieve better editing, but lose the original identity. PTI achieves
high quality editing while preserving the identity. For more uncurated examples, see the Supplementary Materials. Zoom-in recommended.
PTI+StyleClip
Figure 7. StyleClip editing demonstration. Using StyleClip to
perform the ”bowl cut” and ”mohawk” edits (middle column), a
clear improvement in identity preservation can be seen when ﬁrst
employing PTI (right).
on two axes: identity preservation and editing magnitude.
Qualitative
evaluation.
GANSpace and InterfaceGAN methods for
latent-based editing. These approaches are orthogonal to
ours, as they require the use of an inversion algorithm
to edit real images. As can be expected, the W+-based
method preserves the identity rather well, but fails to
perform signiﬁcant edits, the W-based one is able to
PTI+StyleClip
Figure 8. Sequential editing of StyleClip and InterfaceGAN edits
with and without pivotal tuning inversion (PTI). Top row: ”Bob
cut hair”, smile, and rotation. Middle row: ”bowl cut hair” and
older. Bottom row: ”curly hair”, younger and rotation.
perform the edit, but loses the identity, and e4e provides
a compromise between the two. In all cases, our method
preserves identity the best and displays the same editing
quality as for W-based inversions. Figure 6 presents an
editing comparison over the CelebA-HQ dataset.
also investigate our performance using images of other
iconic characters (Figures 1 and 9) and more challenging
out-of-domain facial images (Figure 10).
The ability to
perform sequential editing is presented in Figures 12,
and 13. In addition, we demonstrate our ability to invert
multiple identities using the same generator in Figures 3
For more visual and uncurated results, see the
Supplementary Materials.
As can be seen, our method
successfully performs meaningful edits, while preserving
the original identity successfully.
The recent work of StyleClip demonstrates unique
edits, driven by natural language. In Figures 7, and 8 we
demonstrate editing results using this model, and demonstrate substantial identity preservation improvement, thus
extending StyleClip’s scope to more challenging images.
We use the mapper-based variant proposed by the paper,
where the edits are achieved by training a mapper network
to edit input latent codes. Note that the StyleClip model is
trained to handle codes returned by the e4e method. Hence,
to employ this model, our PTI process uses e4e-based pivots instead of W ones. As can be expected, we observe
Figure 9. Editing comparison of famous ﬁgures collected from the web. We demonstrate the following edits (top to bottom): pose, mouth
closing, and smile. Similar to Figure 6, we again witness how SG2 W+ do not induce signiﬁcant edits, and the others do not preserve
identity, in contrast to our approach, which achieves both.
that the editing capabilities of the e4e codes are preserved,
while the inherent distortion caused by e4e is diminished
using PTI. More results for this experiment can be found in
the supplementary materials.
Quantitative evaluation results are summarized in Table 2.
To measure the two aforementioned axes, we compare the
effects of the same latent editing operation between the various aforementioned baselines, and the effects of editing operations that yield the same editing magnitude. To evaluate
editing magnitude, we apply a single pose editing operation
and measure the rotation angle using Microsoft Face API
 , as proposed by Zhu et al.
 . As the editability
increases, the magnitude of the editing effect increases as
well. As expected, W-based inversion induces a more signiﬁcant edit compared to W+ inversion for the same editing
operation. As can be seen, our approach yields a magnitude
that is almost identical to W’s, surpassing e4e and W+ inversions, which indicates we achieve high editability (ﬁrst
In addition, we report the identity preservation for several edits.
We evaluate the identity change using a pretrained facial recognition network , and the edits we report for are smile, pose, and age. We report both the mean
identity preservation induced by each of these edits (second
row), and the one induced by performing them sequentially
one after the other (third row). Results indeed validate that
Edit Magnitude
ID similarity, same edit
single edits
sequential edits
ID similarity, same magnitude
±5 rotation
±10 rotation
Table 2. Editing evaluation. Top: we compare the edit magnitude
for the same latent edit over the different baselines, as proposed by
Zhu et al. . The conjecture is that more editable codes yield
more signiﬁcant change from the same latent edit. Middle rows:
ID preservation is measured after editing. We have used rotation,
smile, and age. We report the mean ID correlation for the different
edits (single edits), as well as the ID preservation after applying all
three edits sequentially (sequential edits). Bottom: Id preservation
when applying an edit that yields the same effect for all baselines.
The yaw angle change is measured by Microsoft Face API . As
can be seen, our editability is similar to W-based inversion, while
our identity preservation is better even than W+-based inversions.
our method obtains better identity similarity compared to
the baselines.
Since the lack of editability might increase identity similarity, as previously mentioned, we also measure the identity similarity while performing rotation of the same magnitude. Expectedly, the identity similarity for W+ inversion
decreases signiﬁcantly when using a ﬁxed rotation angle
editing, demonstrating it is less editable compared to other
inversion methods. Overall, the quantitative results demonstrate the distortion-editability tradeoff, as W+ inversion
achieves better ID similarity but lower edit magnitude, and
W inversion achieves inferior ID similarity but higher edit
magnitude. In contrast, our method preserves the identity
well and provides highly editable embeddings, or in other
words, we alleviate the distortion-editability trade-off.
4.3. Regularization
Our locality regularization restricts the pivotal tuning
side effects, causing diminishing disturbance to distant latent codes. We evaluate this effect by sampling random latent codes and comparing their generated images between
the original and tuned generators. Visual results, presented
in Figure 14, demonstrate that the regularization signiﬁcantly minimizes the change. The images generated without
regularization suffer from artifacts and ID shifting, while
the images generated while employing the regularization
are almost identical to the original ones. We perform the
regularization evaluation using a model tuned to invert 12
identities, as the side effects are more substantial in the multiple identities case. In addition, Figure 15 presents quantitative results. We measure the reconstruction of random
latent codes with and without the regularization compared
to using the original pretrained generator. To demonstrate
that our regularization does not decrease the pivotal tuning
results, we also measure the reconstruction of the target image. As can be seen, our regularization reduces the side
effects signiﬁcantly while obtaining similar reconstruction
for the target image.
4.4. Ablation study
An ablation analysis is presented in Figure 16. First, we
show that using a pivot latent code from W+ space rather
of W ((B)), results in less editability, as the editing is less
meaningful for both smile and pose. Skipping the initial
inversion step and using the mean latent code ((C)) or a
random latent code ((E)), results in substantially more distortion compared to ours. Similar results were obtained by
optimizing the pivot latent code in addition to the generator, initialized to mean latent code ((D)) or random latent code ((F)) similar to Pan et al. . In addition, we
demonstrate that optimizing the pivot latent code is not necessary even when starting from an inverted pivot code. To
do this, we start from an inverted code wp and perform PTI
while allowing wp to change. We then feed the resulting
wp back to the original StyleGAN. Inspecting the two
images, produced by wp and ˜
wp over the same generator,
we see negligible change: 0.015 ± 5e−6 for LPIPS and
0.0012 ± 1e−6 for MSE. We conclude that our choice for
pivot code is almost optimal and hence can lighten the optimization process by keeping the code ﬁxed.
4.5. Implementation details
For the initial inversion step, we use the same hyperparameters as described by Karras et al. , except for the
learning rate which is changed to 5e−3. We run the inversion for 450 iterations. Then, for pivotal tuning, we further
optimize for 350 iterations with a learning rate of 3e−4 using the Adam optimizer. For reconstruction, we use
λL2 = 1 and λLP IP S = 1 and for the regularization we use
α = 30, λR
L2 = 1, λR = 0.1, and Nr = 1.
All quantitative experiments were performed on the ﬁrst
1000 samples from the CelebA-HQ test set.
Our two-step inversion takes less than 3 minutes on a single Nvidia GeForce RTX 2080. The initial W-space inversion step takes approximately one minute, just like the SG2
inversion does. The pivotal tuning takes less than a minute
without regularization, and less than two with it. This training time grows linearly with the number of inverted identities. The SG2 W+ inversion takes 4.5 minutes for 2100
iterations. The inversion time of e4e is less than a second,
as it is encoder-based and does not require optimization at
inference.
Figure 10. Editing of smile, age, and beard removal (top to bottom) comparison over out-of-domain images collected from the web. The
collected images portray unique hairstyles, hair colors, and apparel, along with unique facial features, such as heavy make-up and scars.
Even in these challenging cases, our results retain the original identity while enabling meaningful edits.
5. Conclusions
We have presented Pivotal Tuning Inversion — an inversion method that allows using latent-based editing techniques on practical, real-life facial images.
In a sense,
we break the notorious trade-off between reconstruction
and editability through personalization, or in other words
through surgical adjustments to the generator that address
the desired image speciﬁcally well. This is achieved by
leveraging the disentanglement between appearance and geometry that naturally emerges from StyleGAN’s behavior.
In other words, we have demonstrated increased quality
at the cost of additional computation. As it turns out, this
deal is quite lucrative: Our PTI optimization boosts performance considerably, while entailing a computation cost of
around three minutes to incorporate a new identity — similar to what some of the current optimization-based inversion
methods require. Furthermore, we have shown that PTI can
be successfully applied to several individuals. We envision
this mode of editing sessions to apply, for example, to a
casting team of a movie.
Nevertheless, it is still desirable to develop a trainable
mapper that approximates the PTI in a short forward pass.
This would diminish the current low computational cost that
entails real image editing, situating StyleGAN as a practical and accessible facial editing tool for the masses. In addition to a single-pass PTI process, in the future we plan
also to consider using a set of photographs of the individual for PTI. This would extend and stabilize the notion of
personalization of the target individual, compared to seeing
just a single example. Another research direction is to take
PTI beyond the architecture of StyleGAN, for example to
BigGAN or other novel generative models.
In general, we believe the presented approach of ad-hoc
ﬁne tuning a pretrained generator potentially bears merits
for many other applications in editing and manipulations of
speciﬁc images or other generation-based tasks in Machine
Acknowledgements
We thank Or Patashnik, Rinon Gal and Dani Lischinski
for their help and useful suggestions.
Figure 11. ”Friends” StyleGAN. We simultaneously invert multiple identities into StyleGAN latent space, while retaining high editability
and identity similarity.
± Smile, Pose
Figure 12. Sequential editing. We perform pivotal tuning inversion followed by two edits sequentially: rotation and smile.