Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identiﬁer 10.1109/ACCESS.2017.DOI
Benchmark Analysis of Representative
Deep Neural Network Architectures
SIMONE BIANCO1, REMI CADENE2, LUIGI CELONA1, AND PAOLO NAPOLETANO1.
1University of Milano-Bicocca, Department of Informatics, Systems and Communication, viale Sarca, 336, 20126 Milano, Italy
2Sorbonne Université, CNRS, LIP6, F-75005 Paris, France
Corresponding author: Luigi Celona (e-mail: ).
 
ABSTRACT This work presents an in-depth analysis of the majority of the deep neural networks (DNNs)
proposed in the state of the art for image recognition. For each DNN multiple performance indices are
observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and
inference time. The behavior of such performance indices and some combinations of them are analyzed and
discussed. To measure the indices we experiment the use of DNNs on two different computer architectures,
a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson
TX1 board. This experimentation allows a direct comparison between DNNs running on machines with
very different computational capacity. This study is useful for researchers to have a complete view of what
solutions have been explored so far and in which research directions are worth exploring in the future;
and for practitioners to select the DNN architecture(s) that better ﬁt the resource constraints of practical
deployments and applications. To complete this work, all the DNNs, as well as the software used for the
analysis, are available online.
INDEX TERMS Deep neural networks, Convolutional neural networks, Image recognition.
I. INTRODUCTION
Deep neural networks (DNNs) have achieved remarkable
results in many computer vision tasks . AlexNet , that is
the ﬁrst DNN presented in the literature in 2012, drastically
increased the recognition accuracy (about 10% higher) with
respect to traditional methods on the 1000-class ImageNet
Large-Scale Visual Recognition Competition (ImageNet-1k)
 . Since then, literature has worked both in designing more
accurate networks as well as in designing more efﬁcient
networks from a computational-cost point of view.
Although there is a lot of literature discussing new architectures from the point of view of the layers composition and
recognition performance, there are few papers that analyze
the aspects related to the computational cost (memory usage,
inference time, etc.), and more importantly how computational cost impacts on the recognition accuracy.
Canziani et al. in the ﬁrst half of 2016 proposed
a comprehensive analysis of some DNN architectures by
performing experiments on an embedded system based on a
NVIDIA Jetson TX1 board. They measured accuracy, power
consumption, memory footprint, number of parameters and
operations count, and more importantly they analyzed the relationship between these performance indices. It is a valuable
work, but it has been focused on a limited number (i.e. 14) of
DNNs and more importantly the experimentation has been
carried out only on the NVIDIA Jetson TX1 board. In ,
speed/accuracy trade-off of modern DNN-based detection
systems has been explored by re-implementing a set of metaarchitectures inspired by well-known detection networks in
the state of the art. Results include performance comparisons
between an Intel Xeon CPU and a NVIDIA Titan X GPU.
The aim of this work is to provide a more comprehensive and complete analysis of existing DNNs for image
recognition and most importantly to provide an analysis on
two hardware platforms with a very different computational
capacity: a workstation equipped with a NVIDIA Titan X
Pascal (often referred to as Titan Xp) and an embedded
system based on a NVIDIA Jetson TX1. To this aim we
analyze and compare more than 40 state-of-the-art DNN
architectures in terms of computational cost and accuracy. In
particular we experiment the selected DNN architectures on
the ImageNet-1k challenge and we measure: accuracy rate,
model complexity, memory usage, computational complexity, and inference time. Further, we analyze relationships be-
VOLUME 4, 2018
©2018 IEEE
 
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
tween these performance indices that provide insights for: 1)
understanding what solutions have been explored so far and
in what direction it would be appropriate to go in the future;
2) selecting the DNN architecture that better ﬁts the resource
constraints of practical deployments and applications.
The most important ﬁndings are that: i) the recognition
accuracy does not increase as the number of operations
increases; ii) there is not a linear relationship between model
complexity and accuracy; iii) the desired throughput places
an upper bound to the achievable accuracy; iv) not all the
DNN models use their parameters with the same level of
efﬁciency; v) almost all models are capable of super real-time
performance on a high-end GPU, while just some of them can
guarantee it on an embedded system; vi) even DNNs with
a very low level model complexity have a minimum GPU
memory footprint of about 0.6GB.
The rest of paper is organized as follows: in Section II
hardware and software used for experiments are detailed;
in Section III the considered DNN architectures are brieﬂy
introduced; in Section IV the measured performance indices
are described; ﬁnally, in Section V obtained results are
reported and analyzed, and Section VI presents our ﬁnal
considerations.
II. BENCHMARKING
We implement the benchmark framework for DNNs comparison in Python. The PyTorch package is used for neural
networks processing with cuDNN-v5.1 and CUDA-v9.0 as
back-end. All the code for the estimation of the adopted
performance indices, as well as all the considered DNN
models are made publicly available .
We run all the experiments on a workstation and on an
embedded system:
1) The workstation is equipped with an Intel Core I7-7700
CPU @ 3.60GHZ, 16GB DDR4 RAM 2400 MHz,
NVIDIA Titan X Pascal GPU with 3840 CUDA cores
(top-of-the-line consumer GPU). The operating system
is Ubuntu 16.04.
2) The embedded system is a NVIDIA Jetson TX1 board
with 64-bit ARM®A57 CPU @ 2GHz, 4GB LPDDR4
1600MHz, NVIDIA Maxwell GPU with 256 CUDA
cores. The board includes the JetPack-2.3 SDK.
The use of these two different systems allows to highlight
how critical the computational resources can be depending
on the DNN model adopted especially in terms of memory
usage and inference time.
III. ARCHITECTURES
In this section we brieﬂy describe the analyzed architectures.
We select different architectures, some of which have been
designed to be more performing in terms of effectiveness,
while others have been designed to be more efﬁcient and
therefore more suitable for embedded vision applications.
In some cases there is a number following the name of the
architecture. Such a number depicts the number of layers that
contains parameters to be learned (i.e. convolutional or fully
connected layers).
We consider the following architectures: AlexNet ; the
family of VGG architectures (VGG-11, -13, -16, and -
19) without and with the use of Batch Normalization (BN)
layers ; BN-Inception ; GoogLeNet ; SqueezeNetv1.0 and -v1.1 ; ResNet-18, -34, -50, -101, and -152
 ; Inception-v3 ; Inception-v4 and Inception-ResNetv2 ; DenseNet-121, -169, and -201 with growth rate
corresponding to 32, and DenseNet-161 with growth rate
equal to 48 ; ResNeXt-101 (32x4d), and ResNeXt-101
(64x4d), where the numbers inside the brackets denote respectively the number of groups per convolutional layer
and the bottleneck width ; Xception ; DualPathNet-
68, -98, and -131, ; SE-ResNet-50, SENet-154, SE-
ResNet-101, SE-ResNet-152, SE-ResNeXt-50 (32x4d), SE-
ResNeXt-101 (32x4d) ; NASNet-A-Large, and NASNet-
A-Mobile, whose architecture is directly learned .
Furthermore, we also consider the following efﬁcientcyoriented models: MobileNet-v1 , MobileNet-v2 , and
ShufﬂeNet .
IV. PERFORMANCE INDICES
In order to perform a direct and fair comparison, we exactly
reproduce the same sampling policies: we directly collect
models trained using the PyTorch framework , or we
collect models trained with other deep learning frameworks
and then we convert them in PyTorch.
All the pre-trained models expect input images normalized
in the same way, i.e. mini-batches of RGB images with shape
3 × H × W, where H and W are expected to be:
- 331 pixels for the NASNet-A-Large model;
InceptionResNet-v2,
Inception-v3,
Inception-v4, and Xception models;
- 224 pixels for all the other models considered.
We consider multiple performance indices useful for a
comprehensive benchmark of DNN models. Speciﬁcally, we
measure: accuracy rate, model complexity, memory usage,
computational complexity, and inference time.
A. ACCURACY RATE
We estimate Top-1 and Top-5 accuracy on the ImageNet-1k
validation set for image classiﬁcation task. The predictions
are computed by evaluating the central crop only. Slightly
better performances can be achieved by considering the average prediction coming from multiple crops (four corners plus
central crop and their horizontal ﬂips).
B. MODEL COMPLEXITY
We analyze model complexity by counting the total amount
of learnable parameters. Speciﬁcally, we collect the size
of the parameter ﬁle in terms of MB for the considered
models. This information is very useful for understanding the
minimum amount of GPU memory required for each model.
VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
FIGURE 1: Ball chart reporting the Top-1 and Top-5 accuracy vs. computational complexity. Top-1 and Top-5 accuracy using
only the center crop versus ﬂoating-point operations (FLOPs) required for a single forward pass are reported. The size of each
ball corresponds to the model complexity. (a) Top-1; (b) Top-5.
C. MEMORY USAGE
We evaluate the total memory consumption, which includes
all the memory that is allocated, i.e. the memory allocated for
the network model and the memory required while processing the batch. We measure memory usage for different batch
sizes: 1, 2, 4, 8, 16, 32, and 64.
D. COMPUTATIONAL COMPLEXITY
We measure the computational cost of each DNN model
considered using the ﬂoating-point operations (FLOPs) in
the number of multiply-adds as in . More in detail, the
multiply-adds are counted as two FLOPs because, in many
recent models, convolutions are bias-free and it makes sense
to count multiply and add as separate FLOPs.
E. INFERENCE TIME
We report inference time per image for each DNN model
for both the NVIDIA Titan X Pascal GPU and the NVIDIA
Jetson TX1. We measure inference time in terms of milliseconds and by considering the same batch sizes described
in Section IV-C. For statistical validation the reported time
corresponds to the average over 10 runs.
V. RESULTS
A. ACCURACY-RATE VS COMPUTATIONAL
COMPLEXITY VS MODEL COMPLEXITY
The ball charts reported in Figures 1 (a) and (b) show Top-
1 and Top-5 accuracy on the ImageNet-1k validation set
with respect to the computational complexity of the considered architectures for a single forward pass measured
for both the workstation and the embedded board, The ball
size corresponds to the model complexity. From the plots
it can be seen that the DNN model reaching the highest
Top-1 and Top-5 accuracy is the NASNet-A-Large that is
also the one having the highest computational complexity.
Among the models having the lowest computational complexity instead (i.e. lower than 5 G-FLOPs), SE-ResNeXt-
50 (32x4d) is the one reaching the highest Top-1 and Top-
5 accuracy showing at the same time a low level of model
complexity, with approximately 2.76 M-params. Overall, it
seems that there is no relationship between computational
complexity and recognition accuracy, for instance SENet-154
needs about 3× the number of operations that are needed
by SE-ResNeXt-101(32x4d) while having almost the same
accuracy. Moreover, it seems that there is no relationship
also between model complexity and recognition accuracy:
for instance VGG-13 has a much higher level of model
complexity (size of the ball) than ResNet-18 while having
almost the same accuracy.
B. ACCURACY-RATE VS LEARNING POWER
It is known that DNNs are inefﬁcient in the use of their
full learning power (measured as the number of parameters
with respect to the degrees of freedom). Although many
papers exist that exploit this feature to produce compressed
DNN models with the same accuracy of the original models
 we want here to measure how efﬁciently each model
uses its parameters. We follow and measure it as Top-1
VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
FIGURE 2: Top-1 accuracy density (a) and Top-1 accuracy vs. Top-1 accuracy density (b). The accuracy density measures how
efﬁciently each model uses its parameters.
accuracy density, i.e. Top-1 accuracy divided by the number
of parameters. The higher is this value and the higher is
the efﬁciency. The plot is reported in Figure 2(a), where it
can be seen that the models that use their parameters most
efﬁciently are the SqueezeNets, ShufﬂeNet, the MobileNets
and NASNet-A-Mobile. To focus to the density information,
we plot the Top-1 accuracy with respect to the Top-1 accuracy
density (see Figure 2(b)), that permits to ﬁnd more easily
the desired trade-off. In this way it is possible to easily see
that among the most efﬁcient models, NASNet-A-Mobile
and MobileNet-v2 are the two providing a much higher Top-
1 accuracy. Among the models having the highest Top-1
accuracy (i.e. higher than 80%) we can observe how the
models using their parameters more efﬁciently are Inceptionv4 and SE-ResNeXt-101 (32x4d).
C. INFERENCE TIME
Average per image inference time over 10 runs for all the
DNN models considered are reported in Tables 1(a) and (b)
for batch size equal to 1, 2, 4, 8, 16, 32, and 64 on both
the Titan Xp and the Jetson. Inference time is measured
in milliseconds and the entries in Tables 1(a) and (b) are
color coded to easily convert them in frames per second
(FPS). From the table it is possible to see that all the DNN
models considered are able to achieve super real-time performances on the Titan Xp with the only exception of SENet-
154, when a batch size of 1 is considered. On the Jetson
instead, only a few models are able to achieve super real-time
performances when a batch size of 1 is considered, namely:
the SqueezeNets, the MobileNets, ResNet-18, GoogLeNet,
and AlexNet. Missing measurements are due to the lack
of enough system memory required to process the larger
D. ACCURACY-RATE VS INFERENCE TIME
In Figure 3(a) and (b) we report the plots of the top-1
accuracy with respect to the number of images processed
per second (i.e. the number of inferences per second) with
a batch size of 1 on both the Titan Xp and the Jetson TX1.
On each plot the linear upper bound is also reported; the two
have almost the same intercept (≈83.3 for Titan Xp and
≈83.0 for the Jetson TX1), but the ﬁrst has a slope that
is almost 8.3× smaller than the second one ; these bounds show that the Titan Xp guarantees
a lower decay of the maximum accuracy achievable when a
larger throughput is needed. Note that this bound appears a
curve instead of line in the plots because of the logarithmic
scale of the images per second axis. From the Titan Xp
plot it is possible to see that if one targets a throughput of
more than 250 FPS, the model giving the highest accuracy
is ResNet-34, with 73.27% Top-1 accuracy; with a target of
more than 125 FPS the model giving the highest accuracy
is Xception, with 78,79% Top-1 accuracy; with a target of
more than 62.5 FPS the model giving the highest accuracy is
SE-ResNeXt-50 (32x4d), with 79,11% Top-1 accuracy; with
a target of more than 30 FPS the model giving the highest
accuracy is NASNet-A-Large, with 82,50% Top-1 accuracy.
This analysis shows how even the most accurate model in
the state of the art, i.e. NASNet-A-Large, is able to provide
super real-time performance (30.96 FPS) on the Titan Xp.
Considering the Jetson TX1 plot it is possible to see that if
one targets super real-time performance, the model giving the
VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
highest accuracy is MobileNet-v2, with a Top-1 accuracy of
71.81%; if one targets a Top-1 accuracy larger than 75%, the
maximum throughput is achieved by ResNet-50 (18,83 FPS);
targeting a Top-1 accuracy larger than 80%, the maximum
throughput is achieved by SE-ResNeXt-101 (32x4d) (7,16
FPS); targeting the highest Top-1 accuracy in the state of the
art the throughput achievable is 2,29 FPS.
E. MEMORY USAGE
In Table 2 we analyze the memory consumption for all the
DNN models considered for different batch sizes on the Titan
Xp. From the memory footprints reported it can be seen that
when a batch size of 1 is considered, most models require
less than 1GB of memory, with the exception of NASNet-A-
Large, the SE-ResNets, the SE-ResNeXTs, SENet-154, the
VGGs and Xception. However none of them requires more
than 1.5GB for a batch size of 1.
F. MEMORY USAGE VS MODEL COMPLEXITY
In Figure 4 we analyze the relationship between the initial
static allocation of the model parameters (i.e. the model
complexity) and the total memory utilization for a batch size
of 1 on the Titan Xp. We can see that the relationship is linear,
and follows two different lines with approximately the same
slope (i.e. 1.10 and 1.15) and different intercepts (i.e. 910
and 639 respectively). This means that the model complexity
can be used to reliably estimate the total memory utilization.
From the plots we can observe that families of models belong
to the same line, as for example the VGGs, the SE-ResNets
ans SqueezeNets lie on the line with higher intercept, while
the ResNets, DualPathNets, DenseNets, Inception nets and
MobileNets line on the line with the lower intercept. In particular we can observe how models having the smallest complexity (i.e. SqueezeNet-v1.0 and SqueezeNet-v1.1 both with
5MB) have a 943MB and 921MB memory footprint, while
models having slightly higher complexity (i.e. MobileNetv1 and MobileNet-v2 with respectively 17MB and 14MB)
have a much smaller memory footprint, equal to 650MB and
648MB respectively.
G. BEST DNN AT GIVEN CONSTRAINTS
Table 3 shows the best DNN architectures in terms of
recognition accuracy when speciﬁc hardware resources are
given as computational constraints. This analysis is done for
both the Titan Xp and Jetson TX1. We deﬁne the following
constraints:
- Memory usage: high (≤1.4GB), medium (≤1.0GB) and
low (≤0.7GB);
- Computational time: half real-time (@15FPS), real-time
(@30FPS), super real-time (@60FPS);
A Titan Xp, with a low memory usage as constraint, achieves
a recognition accuracy of at most 75.95% by using the
DPN-68 network independently of the computational time.
Having more resources, for instance medium and high memory usage, Titan Xp achieves a recognition accuracy of at
most 79.11% by using the SE-ResNeXt-50 (32x4d) with a
super real-time throughput. Having no requirements in terms
of memory usage, the Jetson TX1 achieves a recognition
accuracy of at most 69.52% by using the MobileNet-v1,
which guarantees a super real-time throughput. To have a
DNN running on a Jetson that is comparable in terms of
recognition accuracy to the best DNNs running on the Titan
Xp, a memory size of at least 1GB is needed. In this case the
most performing is the ResNet-50, able to guarantee an half
real-time throughput, with a recognition accuracy of 76.01%.
VI. CONCLUSION
The design of Deep neural networks (DNNs) with increasing complexity able to improve the performance of the
ImageNet-1k competition plays a central rule in advancing
the state-of-the-art also on other vision tasks. In this article
we present a comparison between different DNNs in order to
provide an immediate and comprehensive tool for guiding in
the selection of the appropriate architecture responding to resource constraints in practical deployments and applications.
Speciﬁcally, we analyze more than 40 state-of-the-art DNN
architectures trained on ImageNet-1k in terms of accuracy,
number of parameters, memory usage, computational complexity, and inference time.
The key ﬁndings of this paper are the following:
- the recognition accuracy does not increase as the number of operations increases: in fact, there are some architectures that with a relatively low number of operations,
such as the SE-ResNeXt-50 (32x4d), achieve very high
accuracy (see Figures 1a and b). This ﬁnding is independent on the computer architecture experimented;
- there is not a linear relationship between model complexity and accuracy (see Figures 1a and b);
- not all the DNN models use their parameters with the
same level of efﬁciency (see Figures 2a and b);
- the desired throughput (expressed for example as the
number of inferences per second) places an upper bound
to the achievable accuracy (see Figures 3a and b);
- model complexity can be used to reliably estimate the
total memory utilization (see Figure 4);
- almost all models are capable of real-time or super realtime performance on a high-end GPU, while just a few
of them can guarantee them on an embedded system
(see Tables 1a and b);
- even DNNs with a very low level model complexity
have a minimum GPU memory footprint of about 0.6GB
(see Table 2).
All the DNNs considered, as well as the software used
for the analysis, are available online . We plan to add to
the repository interactive plots that allow other researchers to
better explore the results of this study, and more importantly
to effortlessly add new entries.
ACKNOWLEDGMENTS
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for
VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
TABLE 1: Inference time vs. batch size. Inference time per image is estimated across different batch sizes for the Titan Xp
(left), and Jetson TX1 (right). Missing data are due to the lack of enough system memory required to process the larger batches.
BN-Inception
CaffeResNet-101
DenseNet-121 (k=32)
DenseNet-169 (k=32)
DenseNet-201 (k=32)
DenseNet-161 (k=48)
FBResNet-152
Inception-ResNet-v2
Inception-v3
Inception-v4
MobileNet-v1
MobileNet-v2
NASNet-A-Large
NASNet-A-Mobile
ResNet-101
ResNet-152
ResNeXt-101 (32x4d)
ResNeXt-101 (64x4d)
SE-ResNet-101
SE-ResNet-152
SE-ResNet-50
SE-ResNeXt-101 (32x4d)
SE-ResNeXt-50 (32x4d)
SqueezeNet-v1.0
SqueezeNet-v1.1
BN-Inception
CaffeResNet-101
DenseNet-121 (k=32)
DenseNet-169 (k=32)
DenseNet-201 (k=32)
DenseNet-161 (k=48)
FBResNet-152
Inception-ResNet-v2
Inception-v3
Inception-v4
MobileNet-v1
MobileNet-v2
NASNet-A-Large
NASNet-A-Mobile
ResNet-101
ResNet-152
ResNeXt-101 (32x4d)
ResNeXt-101 (64x4d)
SE-ResNet-101
SE-ResNet-152
SE-ResNet-50
SE-ResNeXt-101 (32x4d)
SE-ResNeXt-50 (32x4d)
SqueezeNet-v1.0
SqueezeNet-v1.1
this research.