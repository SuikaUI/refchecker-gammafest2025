A Dataset and Taxonomy for Urban Sound Research
Justin Salamon1,2, Christopher Jacoby1, Juan Pablo Bello1
1Music and Audio Research Laboratory, New York University
2Center for Urban Science and Progress, New York University
{justin.salamon, cbj238, jpbello}@nyu.edu
Automatic urban sound classiﬁcation is a growing area of
research with applications in multimedia retrieval and urban informatics. In this paper we identify two main barriers
to research in this area – the lack of a common taxonomy
and the scarceness of large, real-world, annotated data. To
address these issues we present a taxonomy of urban sounds
and a new dataset, UrbanSound, containing 27 hours of audio with 18.5 hours of annotated sound event occurrences
across 10 sound classes.
The challenges presented by the
new dataset are studied through a series of experiments using a baseline classiﬁcation system.
Categories and Subject Descriptors
H.3.1 [Information Systems]: Content Analysis and Indexing; H.5.5 [Information Systems]: Sound and Music
Urban sound; dataset; taxonomy; classiﬁcation
INTRODUCTION
The automatic classiﬁcation of environmental sound is a
growing research ﬁeld with multiple applications to largescale, content-based multimedia indexing and retrieval (e.g.
 ). In particular, the sonic analysis of urban environments is the subject of increased interest, partly enabled
by multimedia sensor networks , as well as by large quantities of online multimedia content depicting urban scenes.
However, while there is a large body of research in related
areas such as speech, music and bioacoustics, work on the
analysis of urban acoustics environments is relatively scarce.
Furthermore, when existent, it mostly focuses on the classi-
ﬁcation of auditory scene type, e.g. street, park, as opposed
to the identiﬁcation of sound sources in those scenes, e.g.
car horn, engine idling, bird tweet. See for an example.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
MM’14, November 3–7, 2014, Orlando, Florida, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3063-3/14/11 ...$15.00.
 
One of the main challenges and hindrances to urban sound
research is the lack of labeled audio data. Previous work has
focused on audio from carefully produced movies or television tracks ; from speciﬁc environments such as elevators or oﬃce spaces ; and on commercial or proprietary datasets . The large eﬀort involved in manually
annotating real-world data means datasets based on ﬁeld
recordings tend to be relatively small (e.g. the event detection dataset of the IEEE AASP Challenge consists of
24 recordings per each of 17 classes). A second challenge
faced by the research community is the lack of a common
vocabulary when working with urban sounds. This means
the classiﬁcation of sounds into semantic groups may vary
from study to study, making it hard to compare results.
The goal of this paper is to address the two aforementioned challenges.
In Section 2 we propose a taxonomy
for urban sound sources to facilitate a common framework
for research.
Then, in Section 3 we present UrbanSound,
a dataset of 27 hours of ﬁeld recordings containing thousands of labeled sound source occurrences. To the best of
the authors’ knowledge this is the largest free dataset of
labelled urban sound events available for research. To understand the complexity and challenges presented by this
new dataset, we run a series of baseline sound classiﬁcation
experiments, described in Section 4. The paper concludes
with a summary in Section 5.
URBAN SOUND TAXONOMY
The taxonomical categorization of environmental sounds,
a common ﬁrst step in sound classiﬁcation, has been extensively studied in the context of perceptual soundscape
research . Speciﬁc eﬀorts to describe urban sounds have
often been limited to subsets of broader taxonomies of acoustic environments , and thus only partially address the
needs of systematic urban sound analysis. For an exhaustive
review of previous work the reader is referred to .
In our view, an urban sound taxonomy should satisfy the
following three requirements: (1) it should factor in previous
research and proposed taxonomies, (2) it should aim to be as
detailed as possible, going down to low-level sound sources
such as “car horn” (versus “transportation”) and “jackhammer” (versus “construction”), (3) it should, in its ﬁrst iteration, focus on sounds that are of speciﬁc relevance to urban sound research, such as sounds that contribute to urban
noise pollution. To address (1), we decided to base our taxonomy on the subset of dedicated to the urban acoustic
environment. We deﬁne 4 top level groups: human, nature,
mechanical and music, which are common to most previ-
Urban Acoustic
Environment
Mechanical
Vegetation
Nonampliﬁed
Construction
Ventilation
Non-motorized
Social/Signals
- Laughter
- Shouting
- Coughing
- Sneezing
- Children
- Footsteps
- Dog {bark}
- Dog {howl}
- Bird {tweet}
- Leaves {rustling}
- Airplane
- Helicopter
(overground)
(underground)
Fire engine
Motorcycle
- Engine {idling}
- Engine {passing}
- Engine {accelerating}
- Brakes {screeching}
- Wheels {passing}
- Pneumatics
- Backing up {beeping}
- Rattling parts
- Jackhammer
- Hammering
- Drilling
- Explosion
- Engine {running}
- Air conditioner
Skateboard
- House party
- Car radio
- Ice cream truck
- Boombox / speakers
- Wheels on tracks
- Breaks {screeching}
- Recorded announcements
- Clock chimes
- Alarm / siren
- Fireworks
- Gun shot
- Hydraulic rams
Figure 1: Urban Sound Taxonomy.
ously proposed taxonomies . To address (2), we specify
that the leaves of the taxonomy should be suﬃciently lowlevel to be unambiguous – e.g.
car “brakes”, “engine” or
“horn”, instead of simply “car”. Finally to address (3), we
examined all the noise complaints ﬁled through New York
City’s 311 service from 2010 to date1 (over 370,000), and
built the taxonomy around the most frequently complained
about sound categories and sources – construction (e.g. jackhammer), traﬃc noise (car and truck horns, idling engines),
loud music, air conditioners and dog barks to name a few.
The resulting taxonomy is provided in Figure 1.
Further information about the principles and process behind
the construction of the taxonomy, as well as a scalable digital version, are available online2. Rounded rectangles represent high-level semantic classes (e.g. mechanical sounds).
The leaves of the taxonomy (rectangles with sharp edges)
correspond to classes of concrete sound sources (e.g. siren,
footsteps).
For conciseness, leaves can be shared by several high-level classes (indicated by an earmark). Since the
number of possible sound sources in an urban setting is very
large (potentially inﬁnite), we consider the taxonomy to be
a constant work in progress rather than ﬁxed. We plan to
continue expanding and reformulating the taxonomy as we
increase the scope of sounds covered by our research, by engaging the international research community and promoting
a collaborative eﬀort via (for instance) dedicated workshops.
THE URBANSOUND DATASET
In addition, we have collected a dataset of annotated urban sounds including 10 low-level classes from the taxonomy:
air conditioner, car horn, children playing, dog bark, drilling,
engine idling, gun shot, jackhammer, siren and street music. With the exception of “children playing” and “gun shot”
which were added for variety, all other classes were selected
due to the high frequency in which they appear in urban
noise complaints, as noted in the previous section.
dataset is called UrbanSound. Due to the manual annotation
eﬀort required (see below) we limited the number of classes
to 10, which we consider a good starting point. We intend to
extend it to more classes in future iterations. For a review
of existing datasets and related literature cf. footnote 2.
Before compiling the dataset, we set three main goals: (1)
it should contain sounds that occur in an urban environment, (2) all recordings must be real ﬁeld-recordings, (3)
1 
2 
the dataset should be suﬃciently large and varied in terms
of sounds and recording conditions such that it will be useful for training scalable algorithms capable of analyzing real
data from sensor networks or multimedia repositories. To
accomplish these goals, we turned to Freesound3, an online sound repository containing over 160,000 user-uploaded
recordings under a creative commons license.
contains a large amount of ﬁeld recordings, many of which
are in urban settings.
Using the Freesound API we were
able to search and download a subset of the repository, and
exploit the user-provided metadata (title, description and
tags) to signiﬁcantly speed up the annotation process.
For each class, we started by downloading all sounds returned by the Freesound search engine when using the class
name as a query (e.g. “jackhammer”), resulting in over 3000
recordings summing to just over 60 hours of audio.
then manually checked every recording by listening to it and
inspecting the user-provided metadata, only keeping those
that were actual ﬁeld recordings where the sound class of
interest was present somewhere in the recording. After this
ﬁrst ﬁltering stage we were left with 1302 recordings summing to just over 27 hours of audio.
Next, given all the
recordings for a speciﬁc sound class, we used Audacity4 to label the start and end times of every occurrence of the sound
in each recording, with an additional salience description indicating whether the occurrence was subjectively perceived
to be in the foreground or background of the recording. This
resulted in a total of 3075 labeled occurrences amounting to
18.5 hours of labeled audio. The distribution of total occurrence duration per class and per salience is in Fig. 2(a).
The resulting collection of 1302 full length recordings with
corresponding sound occurrence and salience annotations,
UrbanSound, is freely available online (cf. footnote 2) for
research purposes. The audio is provided in the same format
in which it was originally uploaded to Freesound. Note that
the duration of source occurrences in the set can vary from
1-2 s (e.g. gun shot sounds) to over 30 s (continuous sounds
such as jackhammers or idling engines).
For research on sound source identiﬁcation, we created
an additional subset of short audio snippets which we call
the UrbanSound8K subset, also available online. In the
authors conducted a listening test and found that 4 seconds
were suﬃcient for subjects to identify environmental sounds
with 82% accuracy, and consequently use a 4 s clip duration
in their experiments with automatic classiﬁcation. Following
3 
4 
Total duration (minutes)
Figure 2: (a) Total occurrence duration per class in
UrbanSound. (b) Slices per class in UrbanSound8K.
Breakdown by foreground (FG) / background (BG).
their ﬁndings, we set a maximum occurrence duration limit
of 4 seconds, and segment longer occurrences into 4 s slices
using a sliding window with a hop size of 2 s. To avoid large
diﬀerences in the class distribution, we set a limit of 1000
slices per class, resulting in a total of 8732 labeled slices (8.75
hours). The distribution of slices per class in UrbanSound8K
with a breakdown into salience is provided in Figure 2(b).
SOUND CLASSIFICATION
In order to learn about the characteristics and challenges
presented by this new dataset, we run a set of classiﬁcation
experiments using a baseline approach. Note that we are not
searching for an optimal combination of feature/classiﬁer
parameters to maximize accuracy, but rather are interested
in learning about the characteristics of the dataset itself.
Feature extraction
In all of the following experiments, we extract Mel-Frequency
Cepstral Coeﬃcients (MFCC) from the audio slices using the
Essentia audio analysis library . MFCCs are commonly
used in environmental sound analysis (including recent work
 ) and frequently used as a competitive baseline to benchmark novel techniques . In all experiments we extract the
features on a per-frame basis using a window size of 23.2 ms
and 50% frame overlap. We compute 40 Mel bands between
0 and 22050 Hz and keep the ﬁrst 25 MFCC coeﬃcients (we
do not apply any pre-emphasis nor liftering). The per-frame
values for each coeﬃcient are summarized across time using
the following summary statistics: minimum, maximum, median, mean, variance, skewness, kurtosis and the mean and
variance of the ﬁrst and second derivatives, resulting in a
feature vector of dimension 225 per slice.
Experimental setup
To experiment with diﬀerent classiﬁcation algorithms we
use the Weka data mining software . Every experiment
is run using 10-fold cross validation. Within each fold we
perform correlation-based attribute selection to avoid over-
ﬁtting the training data. As it is not our goal to ﬁnd an optimal parametrization, all classiﬁcation algorithms are used
with their default parameter settings. For each experiment
we report the average accuracy across all 10 folds.
Important care must be taken when creating the folds –
since there are multiple slices coming from the same original
recording, if we generate the folds completely randomly, we
may end up with slices from the same recording used both
Classification accuracy
RandomForest500
Maximum slice duration (s)
Classification accuracy
air conditioner
children playing
engine idling
jackhammer
police siren
street music
Figure 3: Classiﬁcation accuracy vs maximum slice
duration: (a) by classiﬁer, (b) by class for SVM.
for training and testing, which can lead to artiﬁcially high
classiﬁcation accuracies. To avoid this, we designed a random allocation process of slices into folds such that all slices
originating from the same Freesound recording go into the
same fold, whilst trying to balance the number of slices-per
fold for each sound class. The UrbanSound8K subset available online provides the audio slices grouped into 10 folds
generated using this methodology. In this way, researchers
interested in comparing their results to our baseline are guaranteed unbiased and comparable results.
In Section 3 we motivated our choice of 4 s as the maximum slice duration for UrbanSound8K. In this ﬁrst experiment we examine how the choice of this threshold aﬀects the
performance of the baseline approach. To this end, we generated 10 copies of UrbanSound8K, each time varying the
maximum slice duration from 10 s down to 1 s. To ensure
the observed changes in accuracy are not an artifact of a
speciﬁc classiﬁcation algorithm, we compare 5 diﬀerent algorithms: decision tree (J48), k-NN (k = 5), random forest
(500 trees), support vector machine (radial basis function
kernel), and a baseline majority vote classiﬁer (ZeroR).
The results are presented in Figure 3(a). The diﬀerence
in performance between all classiﬁers is statistically significant (paired t-test with p < 0.001) except for the top two
(SVM and random forest). More importantly, we observe
consistent behavior across all classiﬁers – performance remains stable from 10 to 6 s, after which it starts decreasing
gradually. However, if we consider the top performing classiﬁer (SVM), there is no statistically signiﬁcant diﬀerence
between performance using 6s slices and 4s slices (whereas
below 4s the diﬀerence becomes signiﬁcant), in accordance
with and supporting the choice of 4 s slices for Urban-
Sound8K. Further insight can be gained by observing the
per-class accuracies for the SVM, provided in Figure 3(b).
We see that diﬀerent sound classes are aﬀected diﬀerently
by the slice duration: classes such as gun shot and siren
have fast events that are clearly identiﬁable at short temporal scales and are thus mostly unaﬀected by duration; while
classes such as street music and children playing drop almost monotonically, showing the importance of analyzing
them at longer temporal scales and suggesting multi-scale
analysis could be a relevant path for urban sound research.
To understand the relative diﬀerence in performance between classes, we examined the confusion matrix for the
SVM classiﬁer on UrbanSound8K. We found that the clasair c
Classification accuracy
Figure 4: Accuracy as a function of salience.
siﬁer mostly confuses three pairs of classes: air conditioners and idling engines, jackhammers and drills, children and
street music. This makes sense, as the timbre of each pair is
quite similar (for the last pair the presence of complex harmonic tones is a possible cause). We see that the baseline
bag-of-frames approach based on MFCCs fails especially on
wide-band noise-like continuous sounds, and we intend to
investigate approaches that better model the temporal dynamics of energy and timbre as part of future work.
However, the relative similarity between sound classes is
only part of the story.
As explained in Section 3, every
sound occurrence in the dataset is also labeled with a (subjective) salience label – foreground (FG) and background
(BG, also used to label occurrences where there are other
equally-salient sources). Intuitively, one would expect the
baseline algorithm to do better on slices where there is less
background interference, especially since MFCCs are known
to be sensitive to noise . In this ﬁnal experiment, we compare the performance of the SVM classiﬁer for each class as a
function of salience, displayed in Figure 4. As expected, we
see a considerable diﬀerence in performance between sounds
labeled FG and BG, the exception being sirens, possibly because their frequency content does not overlap with most
other sources (by design). Whilst we cannot quantify the
eﬀect of interference from this experiment due to the subjectivity of the labeling, the results point to an important challenge presented by the dataset – identifying sound sources
in the presence of (real) background noise. This problem is
an active area of research (e.g. ), and we believe this realworld dataset will further empower the research community
in coming up with novel solutions to this problem.
Automatic urban sound classiﬁcation can beneﬁt a variety
of multimedia applications. In this paper we identiﬁed two
main barriers to research in this area – the lack of a common
taxonomy and the scarceness of large, real-world, annotated
To address the ﬁrst issue we presented the Urban
Sound Taxonomy, based on previous soundscape research
with a focus on sound classes from real noise-complaint data.
To address the second issue we presented UrbanSound, a
dataset containing 27 hours of audio with 18.5 hours of
manually labelled sound occurrences. We also presented UrbanSound8K, a subset of the dataset designed for training
sound classiﬁcation algorithms. Through a series of classi-
ﬁcation experiments we studied the challenges presented by
the dataset, and identiﬁed avenues for future research: sensitivity to temporal scale in the analysis, confusion due to timbre similarity (especially for noise-like continuous sounds),
and sensitivity to background interference. We believe the
dataset will open the path to new an exciting research in
sound and multimedia applications with a focus on urban
environments and urban informatics.
ACKNOWLEDGMENTS
This work was supported by a seed grant from New York
University’s Center for Urban Science and Progress (CUSP).