Joint Topic-Semantic-aware Social Recommendation
for Online Voting
Hongwei Wang1,2, Jia Wang2, Miao Zhao2, Jiannong Cao2, Minyi Guo1∗
1Shanghai Jiao Tong University, 2Te Hong Kong Polytechnic University
 ,{csjiawang,csmiaozhao,csjcao}@comp.polyu.edu.hk, 
Online voting is an emerging feature in social networks, in which
users can express their atitudes toward various issues and show
their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends
on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors
in a comprehensive manner when doing voting recommendation.
First, due to the fact that existing text mining methods such as
topic model and semantic model cannot well process the content of
votings that is typically short and ambiguous, we propose a novel
Topic-Enhanced Word Embedding (TEWE) method to learn word
and document representation by jointly considering their topics
and semantics. Ten we propose our Joint Topic-Semantic-aware
social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings
by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social
similarity during matrix factorization. To evaluate the performance
of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. Te results prove the
eﬃcacy of our approach against several state-of-the-art baselines.
Online voting; recommender systems; topic-enhanced word embedding; matrix factorization
INTRODUCTION
Online voting has recently become a popular function on social platforms, through which a user can share his opinion towards
various interested subjects, ranging from livelihood issues to entertainment news. More advanced than simple like-dislike type of
votings, some social networks, such as Weibo1, have empowered
users to run their own voting campaigns. Users can freely initiate
votings on any topics of their own interests and customize voting
options. Tese votings are visible to the friends of initiator, who
can then choose to participate to make the votings further seen by
their friends or simply retweet the votings to their friends. In such
a way, in addition to the system recommendation, a voting can
widely propagate over the network along social paths. Te voting
propagation scheme is shown in Fig. 1.
∗M. Guo is the corresponding author.
1htp://www.weibo.com.
CIKM’17, Singapore
2017. 978-1-4503-4918-5/17/11...$15.00
DOI: 10.1145/3132847.3132889
Movie Group
social relation
voting propagation
Who is your favorite
movie star?
James Steward
Marlon Brando
Audrey Hepburn
recommendation
Fig. 1: Propagation scheme of online voting.
Facing a large volume of diversiﬁed votings, a critical challenge
is to present “right” votings to the “right” person. An eﬀective
recommender system is desired to be able to deal with information
overload by precisely locating what votings favor each user most,
thus improves user experience and maximizes user engagement in
votings. Such a recommender system can also beneﬁt a variety of
other online services such as personalized advertising , market
research , public opinion analysis , etc.
Despite the great importance, there is litle prior work considering recommending votings to users in social networks. Te challenges are two-fold. First, the propagation of online votings relies
heavily on the structure of social networks. A user can see the
votings initiated, participated or retweeted by his followees, which
implies that the user is more likely to be exposed to the votings
that his friends are involved in. Moreover, in most social networks,
a user can join diﬀerent interest groups, which is another type
of social structure that potentially aﬀects users’ voting behavior.
Tough several prior works have been
proposed to leverage social network information in recommendation, it is still an open question how to comprehensively incorporate
structural social information into the task of voting recommendation considering its propagation patern. Second, users’ interest
in votings is strongly connected with voting content presented in
question text (e.g., “Who is your favorite movie star?”). Topic model
 is regarded as a possible approach to mine the voting interests
through discovering the latent topic distribution of relevant voting
text. However, the voting questions are typically short and lack
suﬃcient topic information, leading to severe performance degradation of topic models. Alternatively, semantic analytics can
also possibly be used to mine voting interests through learning text
representations. However, such semantic models typically represent each word using a single vector, making them indiscriminative
for homonymy and polysemy, which are especially common in
voting questions (e.g., “Do you use apple products?” and “Do you
peel apple before eating?”). In brief, these inherent defects of the
 
CIKM’17, November 6–10, Singapore
H. Wang et al.
above models limit their power in the scenario of social voting
recommendation.
To address aforementioned challenges, in this paper, we propose
a novel Joint Topic-Semantic-aware Matrix Factorization (JTS-MF)
model for online voting recommendation. JTS-MF model considers
social network structure and representation of voting content in
a comprehensive manner. For social network structure, JTS-MF
model fully encodes the information of social relationship and group
aﬃliation into the objective function. We will further justify the
usage of social network structure in Section 3. For representation
of voting content, we propose a Topic-Enhanced Word Embedding
(TEWE) method to build a multi-prototype word and document2
representation, which jointly considers their topics and semantics.
Te key idea of TEWE is to enable each word to have diﬀerent representations under diﬀerent word topics and diﬀerent documents.
We will detail TEWE in Section 5. Once obtaining TEWE representation for each document, JTS-MF model combines them with
the structural information of social networks to calculate the topicsemantic-social similarity among users and votings. Te reason
of calculating such similarity is that, inspired by Locally Linear
Embedding , we try to preserve the similarity among users
and votings during matrix factorization, as it contains abundant
proximity information and can greatly beneﬁt feature learning for
users and votings. JTS-MF model is detailed in Section 6.
We conduct extensive investigation on JTS-MF with real online
voting dataset. Te experimental results in Section 7 demonstrate
that JTS-MF model achieves substantial gains compared with baselines. Te results also prove that TEWE is able to well combine
topic and semantic information of texts and generates a beter kind
of document representation.
In summary, the contributions of this paper are as follows:
• We formally formulate the online voting recommendation
problem, which has not been fully investigated yet.
• We indicate that user’s voting behavior is highly correlated
with social network structure by conducting thorough statistical measurements.
• We propose a novel Topic-Enhanced Word Embedding
model to jointly consider topics and semantics of words
and documents to learn their representation. TEWE takes
advantages of both topic model and semantic model, and
consequently learns more informative embeddings.
• We develop a novel matrix factorization based models,
named JTS-MF, for online voting recommendation. JTS-
MF is able to preserve the topic-semantic-social similarity
among users and votings from original embedding space
during learning process.
• We carry out extensive experiments on real online voting
dataset, the results of which reveal that JTS-MF significantly outperforms baseline (variant) methods, say for
example, surpassing basic matrix factorization model with
57%, 38% and 25% enhancement in terms of recall for top-1,
top-5 and top-20 recommendation, respectively.
2In this paper, a “document” can be related to a voting, a user or a group. A voting
document is the content of its question, a user document is formed by aggregating
all the documents of votings he participates, and a group document is formed by
aggregating all the documents of users who join the group.
RELATED WORK
Recommender Systems
Roughly speaking, existing recommender systems can be categorized into three classes : content-based, collaborative ﬁltering,
and hybrid methods. Content-based methods make use of
user proﬁles or item descriptions as features for recommendation.
Collaborative ﬁltering methods use either explicit
feedback (e.g., users’ ratings on items) or implicit feedback (e.g.,
users’ browsing records about items) data of user-item interactions
to ﬁnd user preference and make the recommendation. In addition,
various models are incorporated into collaborative ﬁltering, such
as Support Vector Machine , Restricted Boltzmann Machine
 , and Stacked Denoising Auto Encoder . Hybrid methods
 combine content-based and collaborative ﬁltering models
in many hybridization approaches, such as weighted, switching,
cascade and feature combination or augmentation.
Social Recommendation
Traditional recommender systems are vulnerable to data sparsity
problem and cold-start problem. To mitigate this issue, many approaches have been proposed to utilize social network information
in recommender systems . For example, represents a social network as a star-structured hybrid
graph centered on a social domain which connects with other item
domains to help improve the prediction accuracy. investigates the seed selection problem for viral marketing that considers
both eﬀects of social inﬂuence and item inference for product recommendation. studies the eﬀects of strong and weak ties in
social recommendation, and extends Bayesian Personalized Ranking model to incorporate the distinction of strong and weak ties.
However, the above works only utilize users’ social links without
considering the topic and semantic information for mining the
similarities among users and items, which we found quite helpful for voting recommendation tasks. Another diﬀerence between
these works and ours is that we also take social group aﬃliation
into consideration, which can further improve the performance of
recommendation.
Topic and Semantic Language Models
Latent Dirichlet Allocation (LDA) is a well-known generative
topic model that learns the latent topic distributions for documents.
LDA is widely used in sentiment analysis , aspects and opinions mining , and recommendation . Word2vec is generally recognized as a neural network model, which learns word
representations that capture precise syntactic and semantic word
relationships. Word2vec as well as associated Skip-Gram model are
extensively used in document classiﬁcation , dependency parser
 , and network embedding . However, LDA and Word2vec are
not directly applicable in the scenario of voting recommendation
because the content of voting is usually short and ambiguous. As
a combination, tries to learn topical word embeddings based
on both words and their topics. Te diﬀerence between and
ours is that we also take topics of documents into consideration,
which enables our model to learn a even more discriminative and
informative representations for words and documents.
Joint Topic-Semantic-aware Social Recommendation for Online Voting
CIKM’17, November 6–10, Singapore
Table 1: Basic Statistics of Weibo Dataset.
# users with votings
# user-voting
# users with groups
# user-user
83,636,677
# user-group
BACKGROUND AND DATA ANALYSIS
In this section, we brieﬂy introduce the background of Weibo voting
and present detailed analysis of Weibo voting dataset.
Background
Weibo is one of the most popular Chinese microblogging website
launched by Sina corporation, which is akin to a hybrid of Twiter
and Facebook platforms. Users on Weibo can follow each other,
write tweets and share with his followers. Users can also join
diﬀerent groups based on their atributes (e.g., region) or interests
of topics (e.g., career).
Voting3 is one of the embedded features of Weibo. As of January
2013, more than 92 million users have participated in at least one
voting and more than 2.2 million ongoing votings are available on
Weibo every day. Any user can freely initiate, retweet and participate a voting campaign in Weibo. As shown in Fig. 1, votings can
propagate in two ways. Te ﬁrst way is through social propagation:
a user can see the voting initiated, retweeted or participated by his
followees and potentially participates the voting. Te second way
is through Weibo voting recommendation list, which consists of
popular votings and personalized recommendation for each user.
Data Measurements
Our Weibo voting dataset comes from the technical team of Sina
Weibo, which contains detailed information about votings from November 2010 to January 2012, as well as other auxiliary information.
Speciﬁcally, the dataset includes users’ participation status on each
voting4, content of each voting, social connection between users,
name and category of each group, and user-group aﬃliation.
Basic statistics. Te basic statistics are summarized in
Table 1. From Table 1 we can learn that, each user has 165.4 followers/followees, participates 3.9 votings, and joins 5.6 groups on
average. If we only count users who participate at least one voting and users who join at least on group, the average number of
votings and average number of joined groups of each user is 7.4
and 7.8, respectively. Fig. 2 depicts the distribution curves of the
above statistics, where the meaning of each subﬁgure is given in
the caption.
To get an intuitive understanding of whether user’s voting behavior is correlated with his social relation and group aﬃliation,
we conduct the following two sets of statistical experiments:
Correlation between the number of common votings of user
pairs and the types of user pairs. We randomly select ten million
user pairs from the set of all users, and count the average number
of votings that the two users both participate under the following
four circumstances: 1) one of the users follows the other in the
pair, i.e., they are social-level friends; 2) the two users are in at
3htp://www.weibo.com/vote?is all=1.
4We only know whether a user participated a voting or not, rather than user voting
results, i.e., we do not know which voting option a user chose.
Voting rank
# participants
# followers/followees
Group rank
Group rank
Fig. 2: (a) Distribution of the number of votings participated
by a user; (b) Distribution of the number of participants of a
voting; (c) Distribution of the number of followers/followees
of a user; (d) Distribution of the number of users in a group;
(e) Distribution of the number of votings (may contain duplicated votings) participated by all users in a group; (f) Distribution of the number of groups joined by a user.
least one common group, i.e., they are group-leven friends; 3) the
two users are neither social-level friends nor group-level friends;
4) all cases. Te results are ploted in Fig. 3a, which clearly shows
the diﬀerence among these cases. In fact, the average number of
common votings of social-level friends (3.54 × 10−4) and group
level friends (1.79 × 10−4) are 17.4 and 8.8 times higher than that of
“strangers” (2.04 × 10−5). Te results demonstrate that if two users
are social-level or group-level friends, they are likely to participate
more votings in common.
Correlation between the probability of two users being
friends and whether they participate common voting. We ﬁrst randomly select ten thousand votings from the set of all votings. For
each sampled voting vj, we calculate the probability that two of
its participants are social-level or group-level friends, i.e., pj =
# of social/group-level friends among participants of vj
nj×(nj−1)/2
, wherenj is the number of vj’ participants. We calculate pj over all sampled votings
and plot the average result (blue bar) in Fig. 3b. For comparison, we
also plot the result for randomly sampled set of users (green bar)
in Fig. 3b. It is clear that if two users ever participated common
voting, they are more likely to be social-level or group-level friends.
In fact, probabilities of two users being social-level or group-level
friends are raised by 5.3 and 3.6 times given the observation that
they are with common voting.
CIKM’17, November 6–10, Singapore
H. Wang et al.
Types of user pairs (i, k)
# common votings
social−level
group−level
Types of friendship
Prob. of being friends
with common voting(s)
randomly sampled
Fig. 3: (a) Average number of common votings participated
by user ui and uk in four cases: 1. ui follows/is followed by
uk; 2. ui and uk are in at least one common group; 3. ui and
uk have no social-level and group-level relationship; 4. all
cases; (b) Probability of two users being social-level or grouplevel friends in two cases: 1. they ever participated at least
one common voting; 2. they are randomly sampled.
Te above two ﬁndings eﬀectively prove the strong correlation
between voting behavior and social network structure, which motivates us to take users’ social relation and group aﬃliation into
consideration when making voting recommendation.
PROBLEM FORMULATION
In this paper, we consider the problem of recommending Weibo
votings to users. We denote the set of all users, the set of all votings,
and the set of all groups by U = {u1, ...,uN }, V = {v1, ...,vM },
and G = {G1, ...,GL}, respectively. Moreover, we model three
types of relationship in Weibo platform: user-voting, user-user, and
user-group relationship as follows:
(1) Te user-voting relationship for ui and vj is deﬁned as
if ui participates vj;
otherwise.
(2) Te user-user relationship for ui and uk is deﬁned as
if ui f ollows uk;
otherwise.
We further use F +
to denote the set of ui’s followees, and
to denote the set of ui’s followers (“+” means “out”
and “−” means “in”).
(3) Te user-group relationship for ui and Gc is deﬁned as
if ui joins Gc;
otherwise.
Given the above sets of users and votings as well as three types
of relationship, we aim to recommend a list of votings for each user,
in which the votings are not participated by the user but may be
interesting to him.
JOINT-TOPIC-SEMANTIC EMBEDDING
In this section, we explain how to learn the embeddings of users,
votings, and groups in a joint topic and semantic way, and apply the
embeddings to calculate similarities. We ﬁrst introduce the methods
of learning topic information and semantic information by LDA and
Skip-Gram models, respectively, and propose our method which
combines these two models to learn more powerful embeddings.
Topic Distillation
In this subsection, we introduce how to proﬁle users, votings, and
groups in terms of topic interest distribution by performing topic
distillation on the associated textual content information.
In general, LDA is a popular generative model to discover latent
topic information from a collection of documents . In LDA, each
document d is represented as a multinomial distribution Θd over a
set of topics, and each topic z is also represented as a multinomial
distribution Φz over a number of words. Subsequently, each word
position l in document d is assigned a topic zd,l according to Θd,
and the word wd,l is ﬁnally generated according to Φzd,l . By LDA
approach, the topic distribution for each document and the topic
assignment for each word can be obtained, which would be utilized
later in our proposed model.
Here, we discuss how to apply LDA in the scenario of Weibo
voting. According to the Weibo voting dataset, each voting vj
associates a sentence of question, which can be regarded as document dvj 5. Te document dui for user ui can thus be formed by
aggregating the content of all votings he participates, i.e., dui =
∪{dvj |Iui,vj = 1}, and the document dGc for group Gc is formed by
aggregating documents of all its members, i.e.,dGc = ∪{dui |Iui,Gc =
1}. Note that though our target is to learn the topic distributions of
all users, votings, and groups, it is inadvisable to train LDA model
on dui ’s and dvj ’s because: (1) the entitled sentence associated with
a single voting is typically short-presented and topic-ambiguous;
(2) even with user-level voting content aggregation, some documents of inactive users are not long enough to accurately extract
the authentic topic distribution, yet showing relatively ﬂat distribution over all the topics. Terefore, we choose to feed group-level
aggregated documents dGc ’s to LDA model as training samples.
Te process of group-level voting content aggregation will cover
all the content the aﬃliated users are interested in and help beter
identify their interests in terms of voting topic.
Denote Dir(α) as the Dirichlet prior of Θd, and Dir(β) as the
Dirichlet prior of Φz. Given α and β, the joint distribution of
document-topic distributions Θ, topic-word distributions Φ, topics
of words z, and a set of words w is
p(Θ, Φ,z,w|α, β)
z p(Φz |β) · Î
p(Θd |α) Î
p(zd,l |Θd)p(wd,l |Φzd,l )
where d traverses all group-level aggregated documents. In general,
it is computationally intractable to directly maximize the joint
likelihood in Eq. (4), thus Gibbs Sampling is usually applied
to estimate the posterior probability p(z|w,α, β) and solve Θ, Φ
iteratively. Denote θ(z)
the z-th component of Θd, and ϕ(w)
w-th component of Φz. With the sampling results, Θd and Φz can
be estimated as:
+ α(z)/  Í
z = 1, ...,Z,
+ β(w)/  Í
w = 1, ...,V,
where α(z) is the z-th component of α, β(w) is the w-th component
of β, n(z)
is the observation counts of topic z for document d, n(w)
5dvj is segmented by Jieba (htps://github.com/fxsjy/jieba) and all stop words are
Joint Topic-Semantic-aware Social Recommendation for Online Voting
CIKM’17, November 6–10, Singapore
is the frequency of word w assigned as topic z, Z is the number of
topics and V is vocabulary size.
So far, we have obtained the topic assignment for each word and
topic distribution for each group. Topic distributions for users and
votings can thus be inferred by using the learned model and Gibbs
Sampling, which is similar to the calculation of θ(z)
in Eq. (5).
Semantic Distillation
In this subsection, we introduce how to proﬁle users, votings, and
groups in terms of semantic information. Word embedding, which
represents each word using a vector, is widely used to capture
semantic information of words. Skip-Gram model is a well-known
framework for word embedding, which ﬁnds word representation
that are useful for predicting surrounding words in a document
given a target word in a sliding window . More formally, given
a word sequence D = {w1,w2, . . . ,wT }, the objective function of
Skip-Gram is to maximize the average log probability
logp(wt+c |wt ),
where k is the training context size of the target word, which
can be a function of the centered work wt . Te basic Skip-Gram
formulation deﬁnes p(wi |wt ) using the sofmax function as follows:
p(wi |wt ) =
w ∈V exp(w⊤wt ),
where wi and wt are the vector representation of context word wi
and target word wt , respectively, and V is the vocabulary. To avoid
traversing the entire vocabulary, hierarchical sofmax or negative
sampling are used in general during learning process .
Topic-Enhanced Word Embedding
In this subsection, we propose a joint topic and semantic learning
model, named Topic-Enhanced Word Embedding (TEWE), to analyze documents of users, votings, and groups. Te motivation of
proposed TEWE is based on the following two observations: (1)
Te voting content typically involves short texts. Even we infer
the topic distribution for each voting based on the learned topicword distribution from group-level aggregated documents, it is
still topic-ambiguous to some extent. (2) Te Skip-Gram model
for word embedding assumes that each word always preserves a
single vector, which sometimes is indiscriminate under diﬀerent
circumstances due to the homonymy and polysemy. Terefore, the
basic idea of TEWE is to preserve topic information of documents
and words when measuring the interaction between target word wt
and context word wi. In this way, a word with diﬀerent associated
topics has diﬀerent embeddings, and a word in documents with
diﬀerent topics has diﬀerent embeddings, too.
Speciﬁcally, rather than solely using the target word w to predict
context words in Skip-Gram, TEWE also jointly utilizes zw, the
topic of the word in a document, as well as zdw, the most likely
topic of the document that the word belongs to. Recall that in Section 5.1, we have obtained the topic of each word zw and topic
distributions of each document Θd, thus zdw can be calculated
(a) Skip-Gram
𝐰𝑡, 𝐳𝑡, 𝐳𝑡
𝐰𝑡−1, 𝐳𝑡−1, 𝐳𝑡−1
𝐰𝑡+1, 𝐳𝑡+1, 𝐳𝑡+1
Fig. 4: Comparison between Skip-Gram and TEWE. Te
gray circles in (a) indicate the embeddings of original words,
while the blue circles in (b) indicate the TEWE representation of pseudo words, which preserves semantic and topic
information of words and documents.
as zdw = arg maxz θ(z)
d , where θ(z)
is the probability that document d belongs to topic z, as introduced in Eq. (5). TEWE regards
each word-topics triplet ⟨w,zw,zdw⟩as a pseudo word and learns a
unique vector wz,zd for it. Te objective function of TEWE is as
logp(⟨wt+c,zt+c,zd
t+c⟩|⟨wt,zt,zd
where p(⟨wi,zi,zd
i ⟩|⟨wt,zt,zd
t ⟩) is a sofmax function as
p(⟨wi,zi,zd
i ⟩|⟨wt,zt,zd
⟨w,z,zd ⟩∈⟨V,Z,Z ⟩exp
wz,zd ⊤wz,zd
Te comparison between Skip-Gram and TEWE is shown in Fig.
4. Instead of solely utilizing the target and context words as in Skip-
Gram, TEWE further preserves word topic and document topic
along with these words, and incorporates both topic and semantic
information in embedding learning.
Once obtaining TEWE representation for each pseudo word, the
representation of each document can be correspondingly derived
by aggregating the embeddings of its containing words weighted
by term frequency-inverse document frequency (TF-IDF) coeﬃcient.
Speciﬁcally, for each document d, its TEWE can be calculated as
w ∈d TF-IDF(w,d) · wz,zd ,
where TF-IDF(w,d) is the product of the raw count of w in d and
the logarithmically scaled inverse fraction of the documents that
contains w, i.e., TF-IDF(w,d) = fw,d · log
|d ∈D:w ∈d | (D is the set
of all documents). TEWE document representations can be used in
measuring inter-document similarities. For example, the similarity
of two user documents dui and duk can be calculated as the cosine
similarity between their TEWE representations, i.e.,
∥eui ∥2 ∥euk ∥2 .
Tis similarity encodes both topic and semantic proximity information of user documents, which implicitly reveals the similarity of
voting interests between two users.
RECOMMENDATION MODEL
In this section, we present our Joint Topic-Semantic-aware Matrix
Factorization (JTS-MF) model for online social votings, in which
social relationship, group aﬃliation, and topic-semantic similarities
are combined and taken into account for voting recommendation in
CIKM’17, November 6–10, Singapore
H. Wang et al.
𝑖= 1, 2, … , 𝑁
𝑗= 1, 2, … , 𝑀
Fig. 5: Graphic Model of JTS-MF.
a comprehensive manner. Motivated by Locally Linear Embedding
 which tries to preserve the local linear dependency among
inputs in the low-dimensional embedding space, we expect to keep
inter-user and inter-voting topic-semantic similarities in latent
feature space as well. To this end, in JTS-MF model, while the
rating Ri,j is factorized as user latent feature Qi and voting latent
feature Pj, we deliberately enforce Qi and Pj to be dependent on
their social-topic-semantic similar counterparts, respectively. Te
graphic model of JTS-MF model is as shown in Figure 5.
Similarity Coeﬃcients
In order to characterize the inﬂuence of inter-user common interests
and inter-voting content relevance, we ﬁrst introduce the following
three similarity coeﬃcients:
• Normalized social-level similarity coeﬃcient of users: bSi,k,
where uk is the social-level friend of ui;
• Normalized group-level similarity coeﬃcient of users: b
where uk is the group-level friend of ui;
• Normalized similarity coeﬃcient of voting: bTj,t , where vj
and vt are two distinct votings.
Generally speaking, in JTS-MF, the latent feature Qi for user ui
is tied up with the latent feature of his social-level and group-level
friends who are weighted through bSi,k’s and b
Gi,k’s. Likewise, the
latent feature Pj for voting vj is tied up with the latent feature of
its similar votings, which are weighted through bTj,t ’s.
Normalized social-level similarity coeﬀicient of users. Sociallevel similarity coeﬃcient of users is represented by matrix SN ×N ,
which incorporates both social relationship and user-user topicsemantic similarity. Speciﬁcally, for each ui, the social-level similarity coeﬃcient with respect to uk is deﬁned as
Si,k = Iui,uk ·
∥eui ∥2∥euk ∥2
where Iui,uk indicates whether ui follows uk as described in Eq. (2),
i is the out-degree of ui in the social network (i.e., d+
k is the in-degree of uk in the social network (i.e., d−
is the smoothing constant (d = 1 in this paper), and
∥eui ∥2 ∥euk ∥2
is the topic-semantic similarity between user ui and user uk as
mentioned in Section 5.3.
k +d incorporates the information
of local authority and local hub value to diﬀerentiate the importance of diﬀerent users . Essentially, Si,k counts the closeness
between two users from both topic-semantic interests and their
social inﬂuence perspectives.
To avoid the impact of diﬀerent numbers of followees, we use
the normalized social-level similarity coeﬃcient of users in JTS-MF,
which is deﬁned as
denotes the set of ui’s followees in social network.
Normalized group-level similarity coeﬀicient of users. Grouplevel similarity coeﬃcient of users is represented by matrix GN ×N ,
which actually measures the topic-semantic similarity among users
from viewpoint of groups. For each ui, the group-level similarity
coeﬃcient with respect to uk is deﬁned as
G ∈G Iui,G · Iuk,G ·
∥eui ∥2∥eG ∥2
where G represents the set of all groups, Iui,G and Iuk,G indicate
whether ui and uk join group G respectively as described in Eq.
(3), and the last term is the topic-semantic similarity between user
ui and group G. Essentially speaking, Gi,k reﬂects the interest
closeness between user ui and its group-level friend uk by using
ui’s topic-semantic engagement extent to the corresponding group.
We also normalize the group-level similarity coeﬃcient of users as
k ∈Gi Gi,k
where Gi is the set of ui’s group-level friends in social network.
Normalized similarity coeﬀicient of votings. Similarity coef-
ﬁcient of votings is represented by matrix T M×M, which is directly
deﬁned as the topic-semantic similarity among votings, i.e.,
∥evj ∥2∥evt ∥2
Since the number of votings is typically huge, we only consider
the similarity between two votings with suﬃciently high coeﬃcient
value. Speciﬁcally, for each voting vj, we deﬁne a set of votings Vj
containing those votings whose similarity coeﬃcients with vj exceed a threshold, i.e., Vj = {vt |Tj,t ≥threshold}. Correspondingly,
the similarity coeﬃcient of votings are normalized as
t ∈Vj Tj,t
Objective Function
Using the notations listed above, the objective function of JTS-MF
can be writen as
Ri,j −Qi P ⊤
Te basic idea of the objective function in Eq. (17) lies in that,
besides considering explicit feedback between users and votings,
we also impose penalties on the discrepancy among features of
Joint Topic-Semantic-aware Social Recommendation for Online Voting
CIKM’17, November 6–10, Singapore
similar users and similar votings. We give detailed explanation as
follows. Te ﬁrst term of Eq. (17) measures the mean squared error
between prediction and ground truth, where I ′
i,j is the training
weights deﬁned as
if ui participates vj
Te reason we do not directly use Iui,vj deﬁned in Eq. (1) as the
training weights is because we found a small and positive Im makes
the training process more robust and can greatly improve the results.
Ri,j is the actual rating of user ui on voting vj, and QiP⊤
predicted value of Ri,j. Without loss of generality, in JTS-MF model,
we set Ri,j = 1 if ui participates vj and Ri,j = 0 otherwise.
Te second, third, and fourth terms of Eq. (17) measure the
penalty of discrepancy among similar users and similar votings. In
particular, the second term enforces user ui’s latent feature Qi to
be similar to the weighted average of his like-minded followees’
proﬁles Qk ’s. Weight bSi,k’s address both the followee uk’s social
inﬂuence on ui as well as the degree of common voting interests
shared between uk and ui. Te third term enables user ui’s latent
feature Qi to be similar to the weighted average of all his group
peers’ proﬁles Qk ’s. Weight b
Gi,k’s emphasize both the same group
aﬃliation of usersui anduk and also the tie strength betweenui and
the associated group with respect to voting interests. Tis implies
that, among all group-level friends, ui would have more similar
latent feature with the users who frequently join those groups ui is
interested in. Te fourth term ensures voting vj’s latent feature Pj
to be similar to the weighted average of votings that share similar
topic-semantic information with vj.
Finally, the last term of Eq. (17) is the regularizer to prevent
over-ﬁting, and λ is the regularization weight.
Te trade-oﬀamong user social-level similarities, user grouplevel similarities, and voting similarities is controlled by the parameters α, β, and γ, respectively. Obviously, users’ social-level
similarity, users’ group-level similarity, or votings’ similarity is/are
ignored if α, β, or γ is/are set to 0, while increasing these values
shifs the trade-oﬀmore towards their respective directions.
Learning Algorithm
To solve the optimization in Eq. (17), we apply batch gradient
descent approach to minimize the objective function6. Te gradients
of loss function in Eq. (17) with respect to each variable Qi and Pj
are as follows:
Ri,j −Qi P ⊤
6Note that it is impractical to apply Alternating Least Squares (ALS) method here
because it requires calculating the inverse of two matrices with extremely large size.
Ri,j −Qi P ⊤
To clearly understand the gradients in Eq. (19) and (20), it is
worth pointing out that Qi appears not only in the i-th sub-term
in the second and third lines of Eq. (17) explicitly, but also exists
in other t-th sub-terms followed by bSt,i or b
Gt,i, where ui plays as
one of the followees or group members of other users. Te case is
similar for Pj. Given the gradients in Eq. (19) and (20), we list the
pseudo code of the learning algorithm for JTS-MF as follows:
(1) Randomly initialize Q and P;
(2) In each iteration of the algorithm, do:
a) update each Qi: Qi ←Qi −δ ∂L
b) update each Pj: Pj ←Pj −δ ∂L
until convergence, where δ is an conﬁgurable learning rate.
EXPERIMENTS
In this section, we evaluate our proposed JTS-MF model on the
aforementioned Weibo voting dataset7. We ﬁrst introduce baselines and parameter setings used in the experiments, and then
present the experimental results of JTS-MF and the comparison
with baselines.
We use the following seven methods as the baselines against JTS-
MF model. Note that the ﬁrst three baselines are reduced versions
of JTS-MF, which only consider one particular type of similarity
among users or votings.
• JTS-MF(S) only considers social-level similarity of users,
i.e., sets β,γ = 0 in JTS-MF model.
• JTS-MF(G) only considers group-level similarity of users,
i.e., sets α,γ = 0 in JTS-MF model.
• JTS-MF(V) only considers similarity of votings, i.e., sets
α, β = 0 in JTS-MF model.
• MostPop recommends the most popular items to users,
i.e., the votings that have been participated by the most
numbers of users.
• Basic-MF simply uses matrix factorization method
to predict the user-voting matrix while ignores additional
social relation, group aﬃliation and voting content information.
• Topic-MF is similar to JTS-MF except that we substitute Θd for ed when calculating similarities in Eq. (11),
(13), and (15). Note that Θd can also be viewed as the embedding of document with respect to topics. Terefore,
Topic-MF only considers the topic similarity among users
and votings.
• Semantic-MF is similar to JTS-MF except that we use
the Skip-Gram model in directly to learn the word
embeddings. Terefore, Semantic-MF only considers the
semantic similarity among users and votings.
7Experiment code is provided at htps://github.com/hwwang55/JTS-MF.
CIKM’17, November 6–10, Singapore
H. Wang et al.
Fig. 6: Convergence of JTS-MF models with respect to Recall@10.
Parameter Settings
We use GibbsLDA++8, an open-source implementation of LDA
using Gibbs sampling, to calculate topic information of words and
documents in JTS-MF and Topic-MF models. We set the number of
topics to 50 and leave all other parameters in LDA as default values.
For word embeddings in JTS-MF and Semantic-MF models, we use
the same setings as follows: length of embedding dimension as 50,
window size as 5, and number of negative samples as 3.
For all MF-based methods, we set the learning rate δ = 0.001 and
regularization weight λ = 0.5 by 10-fold cross validation. Typically,
we set Im = 0.01 in Eq. (18). Taking into consideration the balance
of experimental results and time complexity, we run 200 iterations
for each of the experiment cases. To conduct the recommendation
task, we randomly select 20% of users’ voting records in the dataset
as test set and use the remaining data as the trainning examples for
our JTS-MF model as well as all baselines. Te choice of remaining
hyper-parameters (trade-oﬀparameters α, β, γ, and dimension of
latent features dim) is discussed in Section 7.4.
To quantitatively analyze the performance of voting recommendation, in our experiment, we use top-k recall (Recall@k), top-k
precision (Precision@k), and top-k micro-F1 (Micro-F1@k) as the
evaluation metrics.
Experiment Results
Study of convergence. To study the convergence of JTS-
MF model, we run the learning algorithm up to 200 iterations for
JTS-MF(S) with α = 10, JTS-MF(G) with β = 140, JTS-MF(V) with
γ = 30, JTS-MF with α = 10, β = 140, γ = 30 (dim = 10 for Qi and
Pj in all models), then calculate Recall@10 for every 10 iterations.
Te result of convergence of JTS-MF models is ploted in Fig. 6.
From Fig. 6 we can see that, the recall of JTS-MF models rises
rapidly before 100 iterations, and starts to oscillate slightly afer
around 150 iterations. Te same changing patern is observed for
all four JTS-MF variants. Terefore, we set the number of learning
iterations as 200 to achieve a balance between running time and
performance of models.
Study of JTS-MF. To study the performance of JTS-MF
model and the eﬀectiveness of three types of similarities, we run
JTS-MF model as well as its three reduced versions on Weibo voting
dataset, and report the results of Recall, Precision, and Micro-F1 in
8GibbsLDA++: htp://gibbslda.sourceforge.net
Fig. 7. Te parameter setings of α, β, γ, and dim are the same as in
Section 7.3.1. Fig. 7a, 7b, and 7c consistently demonstrate that JTS-
MF(S) performs best and JTS-MF(G) performs worst among three
types of reduced versions of JTS-MF. Note that JTS-MF(S) only considers users’ social-level similarity and JTS-MF(G) only considers
users’ group-level similarity. Terefore, it could be concluded that
social-level friends are more helpful than group-level friends when
determining users’ voting interest. Tis is in accordance with our
intuition, since a user typically has much more group-level friends
than social-level friends, which inevitably dilutes its eﬀectiveness
and brings noises into group-level relationship. In addition, the
result in Fig. 7 also demonstrates the eﬀectiveness of the usage of
votings’ similarity. Furthermore, it can be evidently observed that
JTS-MF model outperforms its three reduced versions in all cases,
which proves that the three types of similarities are well combined
in JTS-MF model to achieve much beter results.
Comparison of Models. To further compare JTS-MF model
with other baselines, we gradually increase k from 1 to 500 and
report the results in Table 2 with the best performance highlighted
in bold. Te value of α, β, and γ for JTS-MF and its reduced models
are the same as in Section 7.3.1. Te parameter setings are α = 2,
β = 60, γ = 15 for Topic-MF, α = 8, β = 120, γ = 20 for Semantic-
MF, and dim = 10 for Qi and Pj in all MF-based methods. Te
above parameter setings are the optimal results of ﬁne tuning for
given dim. In Table 2, several observations stand out:
• MostPop performs worst among all methods, because Most-
Pop simply recommends the most popular votings to all
users without considering users’ speciﬁc interests.
• Topic-MF and Semantic-MF outperforms Basic-MF, which
proves the usage of similarities with respect to topic and semantic helpful for recommending votings. Besides, Semantic-
MF outperforms Topic-MF. Tis suggests that semantic
information is more accurate than topic information when
measuring similarities through mining short-length texts.
• JTS-MF outperforms Topic-MF and Semantic-MF. Tis is
the most important observation from Table 2, since it justiﬁes our aforementioned claim that joint-topic-semantic
model can beneﬁt from both topic and semantic aspects
and achieve beter performance.
• Te signiﬁcance of JTS-MF over other models is evident for
smallk. However, this margin becomes smaller whenk gets
larger, and JTS-MF is even slightly inferior to JTS-MF(S)
when k ≥50. Tis means that users’ group-level similarities and votings’ similarities “drag the feet” of JTS-MF
model when k is large. However, JTS-MF is still preferred
in practice, since a real recommender system would only
recommend a small set of votings to users in general.
Parameter Sensitivity
We investigate parameter sensitivity in this subsection. Speciﬁcally,
we evaluate how diﬀerent value of trade-oﬀparameters α, β, γ, and
diﬀerent numbers of latent feature dimensions dim can aﬀect the
performance.
Trade-oﬀparameters. We ﬁx dim = 10, keep two of the
trade-oﬀparameters as 0, and vary the value of the lef trade-oﬀ
Joint Topic-Semantic-aware Social Recommendation for Online Voting
CIKM’17, November 6–10, Singapore
Precision@k
Micro−F1@k
Fig. 7: (a) Recall@k, (b) Precision@k, and (c) Micro-F1@k of JTS-MF models.
fall to 0 when
fall to 0 when
fall to 0 when
Fig. 8: Parameter sensitivity with respect to (a) α, (b) β, (c) γ, and (d) dim.
Table 2: Result of Recall@k, Precision@k, and Micro-F1@k for JTS-MF model and baselines.
Semantic-MF
parameter. Ten we report Recall@10 in Fig. 8a, 8b, and 8c, respectively.
As shown in Fig. 8a, the Recall@10 increases constantly as α gets
larger and reaches a maximum of 0.0558 when α = 10. Tis suggests
that the usage of users’ social-level similarity do help to improve the
recommendation performance. However, when α is too large (α =
12), the learning algorithm of JTS-MF is misled to wrong direction
when updating latent features of users and votings, resulting in
performance deterioration. Te similar phenomenon are observed
in Fig. 8b and Fig. 8c, too. According to the results, when the
other two trade-oﬀparameters are set to 0, Recall@10 reaches
the maximum when α = 10, β = 140, and γ = 30, respectively.
CIKM’17, November 6–10, Singapore
H. Wang et al.
Terefore, in previous experiments we adopt these optimal setings
for JTS-MF(S), JTS-MF(G), and JTS-MF(V), respectively, and use
their combination as the parameter setings in JTS-MF.
Dimension of latent features. We ﬁx α = 10, β = 0, γ = 0
and tune the dimension of latent features of users and votings from
5 to 90. Te result is shown in Fig. 8d. From the ﬁgure, we can
see clearly that the recall is increasing when dim gets larger, this
is because latent features with larger number of dimensions have
more capacity to characterize users and votings. But a larger dim
leads to more running time in experiments. Moreover, we notice
that the improvement of performance stagnates afer dim reaches
80. On balance, we set dim = 10 in our experiment scenarios to
ensure the experiments can complete within rational time duration.
CONCLUSIONS
In this paper, we study the problem of recommending online votings to users in social networks. We ﬁrst formalize the voting
recommendation problem and justify the motivation of leveraging
social structure and voting content information. To overcome the
limitations of topic models and semantic models when learning representation of voting content, we propose Topic-Enhanced Word
Embedding method to jointly consider topics and semantics of
words and documents. We then propose our Joint-Topic-Semanticaware social Matrix Factorization model to learn latent features of
users and votings based on the social network structure and TEWE
representation. We conduct extensive experiments to evaluate JTS-
MF with Weibo voting dataset. Te experimental results prove the
competitiveness of JTS-MF against other state-of-the-art baselines
and demonstrate the eﬃcacy of TEWE representation.
ACKNOWLEDGMENTS
Tis work was partially sponsored by the National Basic Research
973 Program of China under Grant 2015CB352403, the NSFC Key
Grant (No. 61332004), PolyU Project of Strategic Importance 1-ZE26,
and HK-PolyU Grant 1-ZVHZ.