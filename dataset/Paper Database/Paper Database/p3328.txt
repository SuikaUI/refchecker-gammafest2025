Toward Native Explainable and Robust AI in 6G
Networks: Current State, Challenges and Road Ahead
Claudio Fiandrinoa, Giulia Attanasioa,b, Marco Fiorea, Joerg Widmera
aIMDEA Networks Institute, Madrid, Spain
bUniversidad Carlos III de Madrid, Spain
6G networks are expected to face the daunting task of providing support to a
set of extremely diverse services, each more demanding than those of previous
generation networks (e.g., holographic communications, unmanned mobility,
etc.), while at the same time integrating non-terrestrial networks, incorporating new technologies, and supporting joint communication and sensing. The
resulting network architecture, component interactions, and system dynamics
are unprecedentedly complex, making human-only operation impossible, and
thus calling for AI-based automation and configuration support. For this to
happen, AI solutions need to be robust and interpretable, i.e., network engineers
should trust the way AI operates and understand the logic behind its decisions.
In this paper, we revise the current state of tools and methods that can make
AI robust and explainable, shed light on challenges and open problems, and
indicate potential future research directions.
6G networks, AI, explainable AI, robust AI.
1. Introduction
Fifth-generation (5G) networks are now entering a stable phase in terms
of system architecture and commercial release, and the identification of the
Email addresses: (Claudio Fiandrino),
 (Giulia Attanasio), (Marco Fiore),
 (Joerg Widmer)
 
May 5, 2022
advanced features that will shape the evolution of 5G into the sixth generation
(6G) of mobile network systems has already started . Despite being in the
early stages of conceptualization, some key aspects of the future infrastructure
have been identified by the community: 6G will bring a paradigm shift from
“connected things” to “connected intelligence,” supporting even more stringent
KPI requirements than 5G, and global coverage . Therefore, there are strong
expectations that Artificial Intelligence (AI) will permeate the 6G network in-
frastructure, allowing for much swifter and more effective decision-making in
scheduling, control, and orchestration operations of the end-to-end communication systems . Ultimately, this will allow 6G to support ambitious performance
targets such as near-zero latency, apparent infinite capacity, and near 100%
reliability and availability, to support new and diverse classes of innovative
mobile services.
When applied to specific network functionalities, AI systems will either
employ pre-trained models or adapt those at operation time. These models
include machine and deep learning models, and are specialized in analyzing large
data and identifying complex relations and patterns that extend beyond human
knowledge. In a nutshell, this process happens by relating input data to outputs
with stochastic processes. The deep learning models that are typically used
to address wireless network problems are stochastic while traditional software
is by nature deterministic. This implies that existing formal verification tools
that are usually employed for testing software robustness are no longer valid .
To add another dimension to the challenge, deep learning models are regarded
as black-box . In other words, it is very hard to understand the underlying
operation and the reasons why the models have taken certain actions .
In light of the considerations above, it is of paramount importance that
AI becomes trustworthy, in the sense that AI models should be robust and
explainable for humans to trust such non-deterministic systems. Given the
growing interest in the matter, the landscape of regulations by national and
international bodies is continuously evolving . Among others, Article 13 of
the EU Regulatory Framework for AI1 states: “AI systems should be sufficiently
transparent, explainable, and well-documented.” In human-in-the-loop scenarios,
understanding how complex models operate is critical for system experts to
perform root-cause analysis . This applies to the vast majority of zero-touch
network configuration and automation scenarios under discussion within the
ETSI ZSM (Zero-touch network and Service Management) group. Furthermore,
to be trusted, AI models should be robust. Previous research has revealed that
adding a small change to the inputs is sufficient to fool a classifier, e.g., the
infamous tape strip over a speed limit sign that leads a classifier to accelerate
and not to brake or, in the context of mobile networks, to misclassify wireless
signals sent for authentication that are generated by non-legitimate users .
In this paper, we provide an up-to-date primer on robust and explainable AI
for mobile networks. We outline and review existing tools, their applicability,
and shortcomings to address 6G network challenges (§ 2). Next, we discuss how
to enable robust and explainable AI in 6G networks and integrate it into the
current network architecture models (§ 3). We then present a case study to
expose the complexity of applying explainability concepts to a deep learning
mobile traffic predictor based on real-world traffic data (§ 4). Finally, we draw
conclusions and analyze future research directions (§ 5).
2. Tools and Methods for Explainable, Robust and Verifiable AI
This section presents background on explainable (§ 2.1) and robust AI (§ 2.2)
and formal verification techniques (§ 2.3) for AI models. Next, it provides a
discussion (§ 2.4) that highlights shortcomings of existing tools when applied to
mobile networks.
2.1. Explainable AI
The growing interest in promoting trust in ICT systems has been addressed
by regulatory bodies at different levels . In the context of AI/ML, DARPA has
1Available online at: - Last accessed: 04/05/2022
introduced the Explainable AI (XAI) initiative to promote research around model
interpretability to ultimately open up the AI/ML models’ black-box behavior and
make it more intelligible to humans . Such initiative sparked the interest of
the AI community and model interpretability is becoming an important feature
as a basis for new designs. For example, Auric , a framework that is used by
AT&T (a US mobile operator) to automatically configure base stations (known
as eNB and gNB in LTE and 5G jargon respectively) parameters, is based on
decision trees that offer good results in trading off accuracy and interpretability.
Despite some first results, XAI remains a wide-open research area. While
some models like decision trees are easy to interpret and have already been
utilized in practice , in the mobile network domain, the vast majority of
AI/ML applications (e.g., routing, load balancing, and resource allocation) use
much more complex AI models like deep learning models . The computer
vision and Natural Language Processing (NLP) domains received comparatively
more attention than the domain of time-series analysis because of the rich
semantics of the inputs that are intuitive to humans. By setting to zero a
given set of pixels of an image (perturbation) , it is possible to visually
understand their contribution to a model (e.g., which lung regions from X-
RAY images are important to detect COVID-19 ). Using such an approach
for time series is technically feasible at the cost of disrupting the temporal
dependencies. Explainability allows to better comprehend how models operate,
thereby allowing to strengthen robustness and resiliency . At the same
time, assessing robustness and resilience with specific perturbations allows to
understand better which input patterns are prone to weaken the model accuracy.
Layer-wise Relevance Propagation (LRP), DeepLIFT, Local Interpretable Model-
Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) are
existing methods for interpretability . Unlike LRP, all the other methods
resort to perturbing the inputs to measure the accuracy drop with respect to
the original model. By contrast, LRP uses the neural network weights and the
activations that have been created with the forward-pass to propagate back
the output until the input layer. For this reason, LRP can not be applied to
any model out of the box like SHAP: there are existing implementations for
popular models like LSTM, and bi-directional LSTM. TSVis , Long-Short
Term Memories (LSTM)-Vis and Sequence to Sequence (Seq2Seq)-Vis 
are visualization tools that apply respectively to CNN, LSTM and Seq2Seq
learning models and aim at tracking the hidden state changes. The latter two
are conceived as a tool for NLP applications.
2.2. Adversarial Machine Learning
Perturbation is key to test robustness and resilience against adversarial
attacks. Adversarial Machine Learning (AML) comprises several techniques that
build on this concept and ultimately define the trustworthiness of an AI model.
Seminal works like revealed that adding a small change to the inputs is
sufficient to fool a classifier. Attacks performed against an AI model can be
white-box, gray-box, or black-box, depending on the amount of information the
attacker has about the model itself. The first category assumes that the adversary
has full knowledge of the training data, model architecture, and parameters, the
latter none, and gray-box attacks assume partial knowledge. Known attacks
on time-series are modifications of attacks originally designed for images like
the Fast Gradient Sign Method (FGSM) and its iterative version Basic Iterative
Method (BIM) . Both generate a crafted input that resembles the original
one, but the values of its elements are equal to the sign of the elements of the
gradient of the cost function. This is enough to increase the classification/forecast
2.3. Formal Verification for AI
Besides being able to counteract adversaries, to be fully trustworthy AI would
require formal verification. In early 2000, formal verification boosted software
development with systematic bug detection in code, vulnerability analysis, threat
analysis, and run-time monitoring. However, applying the same concepts to AI
is challenging because i) many AI models like deep learning ones are stochastic
by nature as opposed to the deterministic nature of computing systems, and ii)
the role of data becomes crucial . A learning model is trained on a particular
dataset and it is well known that adding additional input data usually leads to
an increase of accuracy. Applying formal methods to AI is not new and a recent
article surveys the tools and methods proposed so far . Those applicable to
neural networks are categorized into complete and incomplete formal methods.
The former methods suffer from scalability but are sound, i.e., they can report
if a given specification holds or not. In contrast, the latter scales better at the
cost of reporting false positives. Complete methods can be further categorized
into satisfiability modulo theory (SMT) and mixed-integer linear programming
(MILP) based methods. SMT boils down the verification problem to a constraint
satisfiability problem: if the modeled constraints can be satisfied, then the
property is not verified. MILP-based methods transform the verification problem
into a MILP-one. If the objective function can be maximized or minimized, then
the property is not verified because it exists at least one counter-example.
2.4. Discussion
The existing systems for explainability and trustworthiness outlined above
have several shortcomings when applied to the mobile network domain, as follows.
First, while the interpretability and visualization tools are relevant, they fail
to explain at a deeper level the model operation. Just highlighting that a given
load pattern at a given time triggers the activation of many neurons does not
explain how important this is in relation to the nature of input data that produced
such behavior. Visualization tools should be extended and coupled with data
mining techniques like Gramian Angular Field or Markov Transition Field 
to fully comprehend and exploit the nature of the input data. In addition to
understanding the reason for producing a given output, a comprehensive tool
should also unveil which patterns are responsible for the errors.
Second, the existing time-series attacks do not consider the specific requirements of mobile network inputs like traffic load or channel propagation. For
instance, the traffic load cannot be a negative value for a given base station. Or,
jamming multiple transmissions at a base station to decrease the observed load is
an extremely hard task in practice, which requires knowledge of the exact timing
of data transmission and of the time-varying characteristics of the channels from
the base stations to the users and from the jammers to the users. XAI and
AML tools can be used jointly, for example to understand if a model over-learns
outliers and if these outliers are legitimate or rather forged ad-hoc.
Third, the existing AI formal verification tools are conceived for simple
neural networks and model parameters, and have all been tested only for popular
image datasets (e.g., MNIST, CIFAR-10). Therefore, they require a significant
extension to accommodate complex architectures that are commonly applied to
address mobile networking problems .
3. Explainability and Robustness: Integration in Next Generation
Mobile Networks
We now discuss how techniques for AI robustness, explainability, and verification, based on and possibly extending the approaches presented in the previous
Section, can be integrated in the 6G network architecture.
While the 6G Radio Access Network (RAN) will significantly evolve from 5G,
at least in its earlier deployment, the 6G core will retain part of the concepts
and functionalities of the 5G core network. The reason is twofold. First, the 5G
core network is radically different from previous generations’ core networks and
Mobile Network Operators (MNO) are likely not willing to make a significant
capital expenditure for a new change. Second, the 5G core network was designed
to adhere to cloud-native and service-based architecture principles, which make it
easy to extend it to support new functionalities like location-based analytics .
Fig. 1 shows the components of the mobile network architecture with 3GPP
standardized functions in the user-plane, control-plane, and radio part .2 For
5G, initial support for AI is provided by the Network Data Analytics Function
2A comparison with previous generation architectures, and a thorough presentation of the
purpose of each of the 5G standardized functions is out the scope of this work. We refer the
interested reader to the complete description in .
Data Networks
User Plane Function
Control Plane Functions
NSSF (Network Slice Selection Function)
NEF (Network Exposure Function)
NRF (Network Repository Function)
PCF (Policy Control Function)
UDM (Unified Data Manager)
AF (Application Function)
AUSF (Authentication Server Function)
AMF (Access and Mobility Management Function)
SMF (Session Management Function)
UPF (User Plane Function)
UE (User Equipment)
Figure 1: Integration of AI/ML with tools to support robustness and explainability in the 6G
network architecture
(NWDAF) in the core and by the Radio Network Information Base (RNIB) in
the RAN . Beyond 5G and 6G network architectures will likely comprise
functions to exploit AI/ML as a service to optimize specific mechanisms and
functionalities. The recently proposed AI/ML Platform (AIMLP) is an example
on how to implement AI/ML as a service . Specifically, the AI/ML-Function
(AIML-F) in Fig. 1 (that can be mapped to AIMLP) will contain pre-trained
learning models ready to be used by other functions (once trained, learning
models can be exported with information on all the weights in hd5 format).
Similarly to the AIML-F, a new function will host ready-to-apply tools for
assessing the robustness of the AI models and provide explanations of their
execution (the XAI/AML-F in Fig. 1). Standard interfaces will provide AI
models and human-in-the-loop capabilities to access and execute XAI/AML
tools on the data or model of interest. For this to happen, the computing platform
of an MNO needs to adapt to accommodate computing- and memory-intensive AI
tasks. While the community has well highlighted this need for executing all the
operations involved with an ML pipeline (e.g., collection of measurement data,
distributed/centralized model training and inference execution, actuation with a
change of network mechanism policy), also XAI/AML tools are computationally
expensive. The case study we present in the next Section provides a practical
example of the substantial computational requirements of XAI methods for
networking.
4. Case Study
We consider a specific case study, which we employ to set forward the path to
address one of the shortcomings highlighted in Section 2.4, i.e., that of ensuring
deeper model explainability with the help of time series mining techniques. For
this, we focus on mobile traffic forecasting, one of the most popular applications
of AI for mobile networking. Next, we present the dataset used (§ 4.1) and
discuss how to ensure deeper model explainability and the associated execution
time, CPU and memory footprint (§ 4.2).
4.1. The Dataset
For our experiments, we rely on a measurement dataset of real-world traffic
collected in a production 4G network serving a major metropolitan region in
Europe. The data consists of information on the traffic volume generated by a
set of target mobile applications, including popular services like Apple iCloud,
Facebook, Netflix, and Whatsapp, among others, at each eNB. The traffic maps
to the demand of the whole user base of the operator in the region, which has a
market share of more than 30% there.
The data was collected via commercial passive probes that tap into interfaces
of the Gateway GPRS Support Nodes (GGSNs) and the Packet Data Network
Gateways (PGWs), monitor individual flows, and perform traffic classification
using Deep Packet Inspection (DPI) and proprietary fingerprinting solutions.
The processing of flow-level captures into per-minute traffic volumes at each eNB
occurred in the secure premises of the network operator, under the supervision
of the local Data Protection Officer (DPO). We only had access to the de-
personalized aggregates for our study, in compliance with applicable international
regulations.
Overall, the dataset comprises the per-eNB time series of 23 mobile services
at the granularity of three minutes. All the time series cover the same period of
11 weeks in the fall of 2019.
60.28 59.65 61.74 60.01 63.92 63.91 65.61 65.54 68.35 69.52 68.45 69.66 69.66 69.24 72.30 70.10 69.19 70.66 72.44 72.03
History: 20 samples
Figure 2: Model explanation with LRP
4.2. Explaining DL Models for Traffic and Capacity Forecasting
The univariate time series of the service-level demand aggregated over all
eNBs is fed to a deep neural network.
The forecasting task then maps to
anticipating the future load in the region of interest. To this end, we employ an
LSTM layer with 200 memory cells followed by a fully connected output layer
with a single hidden unit for the actual prediction3. The deep neural network
receives a history of past observations Tn ∈T = tn−k+1, tn−k+2, . . . , tn of the
input feature, i.e., load expressed in MB/min, and aims at forecasting the load
at the time instant tn+1. For our analysis, we set k = 20 (i.e., 1 hour of the
time series as each sample characterizes the load over 3 minutes) and k = 120
(i.e., 6 hours). We train our network over 9 weeks and we test over the last two
weeks of the dataset. The model is trained using MAE as the loss function and
the Adam optimizer with a learning rate of 0.0001 during 470 epochs. We verify
that the model outperforms by 15% in terms of Mean Absolute Error a naive
predictor whose forecast at tn+1 corresponds to the load at tn. We perform our
explainability analysis on the test set by using both LRP and SHAP methods.
For LRP we use the implementation by Warnecke et al. with ϵ = 10−3.
Instead, for SHAP we use the open-source implementation by Lundberg et al.
 including the DeepExplainer method.
We now compare the model operation explained by LRP and SHAP. Our
methodology is as follows: we explain how the model predicts the value tn+1 of
the load time-series T = t1, t2, . . . , tn, using as history the last k = 20 values
Tn ⊂T = tn−k+1, tn−k+2, . . . , tn. Both methods identify a general trend in the
way in which the model works: recent and old samples contribute positively to
the forecast, while samples in the center of Tn are less relevant. Fig. 2 and Fig. 3
3The neural network architecture was selected on the basis of extensive tests.
History samples
Shap value
Contribution:
Not relevant
Figure 3: Model explanation with SHAP
History (Tn) with 20 samples
History (Tn) with 20 samples
History (Tn) with 20 samples
History (Tn) with 20 samples
History (Tn) with 20 samples
Gramian Angular Field (Sum) Gramian Angular Field (Difference)
Markov Transition Field
Figure 4: Gramian Angular Fields and Markov Transition Field applied to our data
show respectively an example of LRP and SHAP explanations.
After having identified that the both XAI methods provide a similar explanation regarding how the LSTM model operates, we mine the input data with two
techniques that encode time-series as images: the Gramian Angular Field (GAF)
in its sum and difference forms and the Markov Transition Fields (MTS) .
Fig 4 shows an example of application of GAF and MTS over a generic Tn ⊂T .
While the MTS does not seem to provide any deeper explanation, the GAF
does shed some light on the model operation. We know that samples at the
extremities of Tn are highly relevant to predict tn+1. By focusing on the GAF
(sum, the most left plot), we can appreciate that at the extremities of Tn (bottom
left and top right part of the first plot in Fig 4), many values are positive and
many peak at 1. This means for the model, the most relevant samples of Tn are
those values of load that are either very low (these are also old values) or very
high (the most recent values of Tn).
We now characterize how computing intensive it is to execute the XAI
tools. Table 1 and Table 2 show execution time and resource utilization for
LRP and SHAP, respectively, for machines with different hardware capabilities.
Specifically, we test an Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz (12 cores),
equipped with 64 GB of RAM (Server 1), an AMD Ryzen 9 5950X Processor
(16 cores) with 64 GB of RAM (Server 2), an 11th Gen Intel(R) Core(TM)
i9-11900K @ 3.50GHz (16 cores) with 64 GB of RAM (Server 3) and an Intel(R)
Xeon(R) Gold 6240R CPU @ 2.40GHz (97 cores) equipped with 264 GB of
RAM (Server 4). Values of mean and standard deviation for CPU and memory
consumption are obtained with the glances tool, with 1 sample every 3 seconds
throughout the execution (shown in the second and third columns of Table 2).
This analysis shows that for an increasing number of past observations fed
to the explainability method, both the CPU usage and the allocated memory
increase too. We registered an increment in the CPU usage of 20-100% while
the allocated memory almost doubles. In particular, the measured memory
allocation is more stable compared to CPU usage. The reason may be related to
the fact that memory is preallocated by the explainability method and released
to the Operating System (OS) at the end of the process execution, while the
CPU is allocated and reclaimed by the OS when is needed.
Table 1: Profiling resource utilization of LRP on different machines
5. Concluding Remarks
In this paper, we looked at how to promote trustworthiness in the way
future AI systems will be applied to next-generation mobile networks. For this
Table 2: Profiling resource utilization of SHAP on different machines
to happen, AI should become explainable, robust and verifiable. After having
outlined the existing landscape of tools that enable the aforementioned properties,
we discussed the shortcomings if applied directly to mobile networking problems.
For the case study of mobile traffic prediction, we showed the benefit of
superior interpretability if the explanations that the XAI tools provide on the
model operation are coupled with techniques that mine input data. Further
work should be done in this area: understanding which mining techniques offer
suitable explanations in more complex cases than the one presented in this
work is nontrivial. Correlation or more complex mathematical tools like causal
analysis could help in this regard to connect the dots. Besides understanding the
reasons for producing a given output, a comprehensive tool should also unveil
which patterns are responsible for the errors.
Acknowledgment
This work is partially supported by a Juan de la Cierva grant from the
Spanish Ministry of Science and Innovation (IJC2019-039885-I), by the European
Union’s Horizon 2020 research and innovation programme under grant agreement
no.101017109 “DAEMON”, by the Atracción de Talento Investigador grant
number 2019-T1/TIC-16037 NetSense funded by the Comunidad de Madrid,
and by the Madrid Regional Government through the TAPIR-CM program
(S2018/TCS-4496).