HAL Id: hal-03065927
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Toward Trustworthy AI Development: Mechanisms for
Supporting Verifiable Claims
Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen
Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth
Fong, et al.
To cite this version:
Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, et al..
Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. 2020. ￿hal-03065927￿
 
Toward Trustworthy AI Development:
Mechanisms for Supporting Veriﬁable Claims∗
Miles Brundage1†, Shahar Avin3,2†, Jasmine Wang4,29†‡, Haydn Belﬁeld3,2†, Gretchen Krueger1†,
Gillian Hadﬁeld1,5,30, Heidy Khlaaf6, Jingying Yang7, Helen Toner8, Ruth Fong9,
Tegan Maharaj4,28, Pang Wei Koh10, Sara Hooker11, Jade Leung12, Andrew Trask9,
Emma Bluemke9, Jonathan Lebensold4,29, Cullen O’Keefe1, Mark Koren13, Théo Ryffel14,
JB Rubinovitz15, Tamay Besiroglu16, Federica Carugati17, Jack Clark1, Peter Eckersley7,
Sarah de Haas18, Maritza Johnson18, Ben Laurie18, Alex Ingerman18, Igor Krawczuk19,
Amanda Askell1, Rosario Cammarota20, Andrew Lohn21, David Krueger4,27, Charlotte Stix22,
Peter Henderson10, Logan Graham9, Carina Prunkl12, Bianca Martin1, Elizabeth Seger16,
Noa Zilberman9, Seán Ó hÉigeartaigh2,3, Frens Kroeger23, Girish Sastry1, Rebecca Kagan8,
Adrian Weller16,24, Brian Tse12,7, Elizabeth Barnes1, Allan Dafoe12,9, Paul Scharre25,
Ariel Herbert-Voss1, Martijn Rasser25, Shagun Sodhani4,27, Carrick Flynn8,
Thomas Krendl Gilbert26, Lisa Dyer7, Saif Khan8, Yoshua Bengio4,27, Markus Anderljung12
1OpenAI, 2Leverhulme Centre for the Future of Intelligence, 3Centre for the Study of Existential Risk,
4Mila, 5University of Toronto, 6Adelard, 7Partnership on AI, 8Center for Security and Emerging Technology,
9University of Oxford, 10Stanford University, 11Google Brain, 12Future of Humanity Institute,
13Stanford Centre for AI Safety, 14École Normale Supérieure (Paris), 15Remedy.AI,
16University of Cambridge, 17Center for Advanced Study in the Behavioral Sciences,18Google Research,
19École Polytechnique Fédérale de Lausanne, 20Intel, 21RAND Corporation,
22Eindhoven University of Technology, 23Coventry University, 24Alan Turing Institute,
25Center for a New American Security, 26University of California, Berkeley,
27University of Montreal, 28Montreal Polytechnic, 29McGill University,
30Schwartz Reisman Institute for Technology and Society
April 2020
∗Listed authors are those who contributed substantive ideas and/or work to this report. Contributions include writing,
research, and/or review for one or more sections; some authors also contributed content via participation in an April 2019
workshop and/or via ongoing discussions. As such, with the exception of the primary/corresponding authors, inclusion as
author does not imply endorsement of all aspects of the report.
†Miles Brundage ( ), Shahar Avin ( ), Jasmine Wang ( ),
Haydn Belﬁeld ( ), and Gretchen Krueger ( ) contributed equally and are corresponding authors. Other authors are listed roughly in order of contribution.
‡Work conducted in part while at OpenAI.
Executive Summary
List of Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Introduction
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Institutional, Software, and Hardware Mechanisms . . . . . . . . . . . . . . . . . . . . . . . .
Scope and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Outline of the Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Institutional Mechanisms and Recommendations
Third Party Auditing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Red Team Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Bias and Safety Bounties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Sharing of AI Incidents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Software Mechanisms and Recommendations
Audit Trails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Privacy-Preserving Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Hardware Mechanisms and Recommendations
Secure Hardware for Machine Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
High-Precision Compute Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Compute Support for Academia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusion
Acknowledgements
References
Appendices
Workshop and Report Writing Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Key Terms and Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Nature and Importance of Veriﬁable Claims . . . . . . . . . . . . . . . . . . . . . . . . . .
AI, Veriﬁcation, and Arms Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Cooperation and Antitrust Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Supplemental Mechanism Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Formal Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Veriﬁable Data Policies in Distributed Computing Systems
. . . . . . . . . . . . . . .
Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Executive Summary
Recent progress in artiﬁcial intelligence (AI) has enabled a diverse array of applications across commercial, scientiﬁc, and creative domains. With this wave of applications has come a growing awareness of
the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry
and academia are insufﬁcient to ensure responsible AI development .
Steps have been taken by the AI community to acknowledge and address this insufﬁciency, including
widespread adoption of ethics principles by researchers and technology companies. However, ethics
principles are non-binding, and their translation to actions is often not obvious. Furthermore, those
outside a given organization are often ill-equipped to assess whether an AI developer’s actions are consistent with their stated principles. Nor are they able to hold developers to account when principles and
behavior diverge, fueling accusations of "ethics washing" . In order for AI developers to earn trust
from system users, customers, civil society, governments, and other stakeholders that they are building
AI responsibly, there is a need to move beyond principles to a focus on mechanisms for demonstrating responsible behavior . Making and assessing veriﬁable claims, to which developers can be held
accountable, is one crucial step in this direction.
With the ability to make precise claims for which evidence can be brought to bear, AI developers can more
readily demonstrate responsible behavior to regulators, the public, and one another. Greater veriﬁability
of claims about AI development would help enable more effective oversight and reduce pressure to
cut corners for the sake of gaining a competitive edge . Conversely, without the capacity to verify
claims made by AI developers, those using or affected by AI systems are more likely to be put at risk by
potentially ambiguous, misleading, or false claims.
This report suggests various steps that different stakeholders in AI development can take to make it easier
to verify claims about AI development, with a focus on providing evidence about the safety, security,
fairness, and privacy protection of AI systems. Implementation of such mechanisms can help make
progress on the multifaceted problem of ensuring that AI development is conducted in a trustworthy
fashion.1 The mechanisms outlined in this report deal with questions that various parties involved in AI
development might face, such as:
• Can I (as a user) verify the claims made about the level of privacy protection guaranteed by a new
AI system I’d like to use for machine translation of sensitive documents?
• Can I (as a regulator) trace the steps that led to an accident caused by an autonomous vehicle?
Against what standards should an autonomous vehicle company’s safety claims be compared?
• Can I (as an academic) conduct impartial research on the impacts associated with large-scale AI
systems when I lack the computing resources of industry?
• Can I (as an AI developer) verify that my competitors in a given area of AI development will follow
best practices rather than cut corners to gain an advantage?
Even AI developers who have the desire and/or incentives to make concrete, veriﬁable claims may not
be equipped with the appropriate mechanisms to do so. The AI development community needs a robust "toolbox" of mechanisms to support the veriﬁcation of claims about AI systems and development
processes.
1The capacity to verify claims made by developers, on its own, would be insufﬁcient to ensure responsible AI development.
Not all important claims admit veriﬁcation, and there is also a need for oversight agencies such as governments and standards
organizations to align developers’ incentives with the public interest.
This problem framing led some of the authors of this report to hold a workshop in April 2019, aimed at
expanding the toolbox of mechanisms for making and assessing veriﬁable claims about AI development.2
This report builds on the ideas proposed at that workshop. The mechanisms outlined do two things:
• They increase the options available to AI developers for substantiating claims they make about AI
systems’ properties.
• They increase the speciﬁcity and diversity of demands that can be made of AI developers by other
stakeholders such as users, policymakers, and members of civil society.
Each mechanism and associated recommendation discussed in this report addresses a speciﬁc gap preventing effective assessment of developers’ claims today. Some of these mechanisms exist and need to
be extended or scaled up in some way, and others are novel. The report is intended as an incremental
step toward improving the veriﬁability of claims about AI development.
The report organizes mechanisms under the headings of Institutions, Software, and Hardware, which are
three intertwined components of AI systems and development processes.
• Institutional Mechanisms: These mechanisms shape or clarify the incentives of people involved
in AI development and provide greater visibility into their behavior, including their efforts to ensure that AI systems are safe, secure, fair, and privacy-preserving. Institutional mechanisms play a
foundational role in veriﬁable claims about AI development, since it is people who are ultimately
responsible for AI development. We focus on third party auditing, to create a robust alternative
to self-assessment of claims; red teaming exercises, to demonstrate AI developers’ attention to
the ways in which their systems could be misused; bias and safety bounties, to strengthen incentives to discover and report ﬂaws in AI systems; and sharing of AI incidents, to improve societal
understanding of how AI systems can behave in unexpected or undesired ways.
• Software Mechanisms: These mechanisms enable greater understanding and oversight of speciﬁc
AI systems’ properties. We focus on audit trails, to enable accountability for high-stakes AI systems
by capturing critical information about the development and deployment process; interpretability, to foster understanding and scrutiny of AI systems’ characteristics; and privacy-preserving
machine learning, to make developers’ commitments to privacy protection more robust.
• Hardware Mechanisms: Mechanisms related to computing hardware can play a key role in substantiating strong claims about privacy and security, enabling transparency about how an organization’s resources are put to use, and inﬂuencing who has the resources necessary to verify
different claims. We focus on secure hardware for machine learning, to increase the veriﬁability of privacy and security claims; high-precision compute measurement, to improve the value
and comparability of claims about computing power usage; and compute support for academia,
to improve the ability of those outside of industry to evaluate claims about large-scale AI systems.
Each mechanism provides additional paths to verifying AI developers’ commitments to responsible AI
development, and has the potential to contribute to a more trustworthy AI ecosystem. The full list of
recommendations associated with each mechanism is found on the following page and again at the end
of the report.
2See Appendix I, "Workshop and Report Writing Process."
List of Recommendations
Institutional Mechanisms and Recommendations
1. A coalition of stakeholders should create a task force to research options for conducting and funding third party auditing of AI systems.
2. Organizations developing AI should run red teaming exercises to explore risks associated with
systems they develop, and should share best practices and tools for doing so.
3. AI developers should pilot bias and safety bounties for AI systems to strengthen incentives and
processes for broad-based scrutiny of AI systems.
4. AI developers should share more information about AI incidents, including through collaborative
Software Mechanisms and Recommendations
5. Standards setting bodies should work with academia and industry to develop audit trail requirements for safety-critical applications of AI systems.
6. Organizations developing AI and funding bodies should support research into the interpretability
of AI systems, with a focus on supporting risk assessment and auditing.
7. AI developers should develop, share, and use suites of tools for privacy-preserving machine
learning that include measures of performance against common standards.
Hardware Mechanisms and Recommendations
8. Industry and academia should work together to develop hardware security features for AI accelerators or otherwise establish best practices for the use of secure hardware (including secure
enclaves on commodity hardware) in machine learning contexts.
9. One or more AI labs should estimate the computing power involved in a single project in great
detail (high-precision compute measurement), and report on the potential for wider adoption
of such methods.
10. Government funding bodies should substantially increase funding of computing power resources
for researchers in academia, in order to improve the ability of those researchers to verify claims
made by industry.
Introduction
Motivation
With rapid technical progress in artiﬁcial intelligence (AI)3 and the spread of AI-based applications
over the past several years, there is growing concern about how to ensure that the development and
deployment of AI is beneﬁcial – and not detrimental – to humanity. In recent years, AI systems have
been developed in ways that are inconsistent with the stated values of those developing them. This
has led to a rise in concern, research, and activism relating to the impacts of AI systems . AI
development has raised concerns about ampliﬁcation of bias , loss of privacy , digital addictions
 , social harms associated with facial recognition and criminal risk assessment , disinformation
 , and harmful changes to the quality and availability of gainful employment .
In response to these concerns, a range of stakeholders, including those developing AI systems, have
articulated ethics principles to guide responsible AI development. The amount of work undertaken to
articulate and debate such principles is encouraging, as is the convergence of many such principles on a
set of widely-shared concerns such as safety, security, fairness, and privacy.4
However, principles are only a ﬁrst step in the effort to ensure beneﬁcial societal outcomes from AI
 . Indeed, studies , surveys , and trends in worker and community organizing make
clear that large swaths of the public are concerned about the risks of AI development, and do not trust
the organizations currently dominating such development to self-regulate effectively. Those potentially
affected by AI systems need mechanisms for ensuring responsible development that are more robust
than high-level principles. People who get on airplanes don’t trust an airline manufacturer because of its
PR campaigns about the importance of safety - they trust it because of the accompanying infrastructure
of technologies, norms, laws, and institutions for ensuring airline safety.5
Similarly, along with the
growing explicit adoption of ethics principles to guide AI development, there is mounting skepticism
about whether these claims and commitments can be monitored and enforced .
Policymakers are beginning to enact regulations that more directly constrain AI developers’ behavior
 . We believe that analyzing AI development through the lens of veriﬁable claims can help to inform
such efforts. AI developers, regulators, and other actors all need to understand which properties of AI
systems and development processes can be credibly demonstrated, through what means, and with what
tradeoffs.
We deﬁne veriﬁable claims6 as falsiﬁable statements for which evidence and arguments can be brought
3We deﬁne AI as digital systems that are capable of performing tasks commonly thought to require intelligence, with these
tasks typically learned via data and/or experience.
4Note, however, that many such principles have been articulated by Western academics and technology company employees,
and as such are not necessarily representative of humanity’s interests or values as a whole. Further, they are amenable to various
interpretations and agreement on them can mask deeper disagreements . See also Beijing AI Principles and
Zeng et. al. for examples of non-Western AI principles.
5Recent commercial airline crashes also serve as a reminder that even seemingly robust versions of such infrastructure are
imperfect and in need of constant vigilance.
6While this report does discuss the technical area of formal veriﬁcation at several points, and several of our recommendations
are based on best practices from the ﬁeld of information security, the sense in which we use "veriﬁable" is distinct from how
the term is used in those contexts. Unless otherwise speciﬁed by the use of the adjective "formal" or other context, this report
uses the word veriﬁcation in a looser sense. Formal veriﬁcation seeks mathematical proof that a certain technical claim is
to bear on the likelihood of those claims being true. While the degree of attainable certainty will vary
across different claims and contexts, we hope to show that greater degrees of evidence can be provided
for claims about AI development than is typical today. The nature and importance of veriﬁable claims
is discussed in greater depth in Appendix III, and we turn next to considering the types of mechanisms
that can make claims veriﬁable.
Institutional, Software, and Hardware Mechanisms
AI developers today have many possible approaches for increasing the veriﬁability of their claims. Despite the availability of many mechanisms that could help AI developers demonstrate their claims and
help other stakeholders scrutinize their claims, this toolbox has not been well articulated to date.
We view AI development processes as sociotechnical systems,7 with institutions, software, and hardware
all potentially supporting (or detracting from) the veriﬁability of claims about AI development. AI developers can make claims about, or take actions related to, each of these three interrelated pillars of AI
development.
In some cases, adopting one of these mechanisms can increase the veriﬁability of one’s own claims,
whereas in other cases the impact on trust is more indirect (i.e., a mechanism implemented by one actor
enabling greater scrutiny of other actors). As such, collaboration across sectors and organizations will be
critical in order to build an ecosystem in which claims about responsible AI development can be veriﬁed.
• Institutional mechanisms largely pertain to values, incentives, and accountability.
Institutional
mechanisms shape or clarify the incentives of people involved in AI development and provide
greater visibility into their behavior, including their efforts to ensure that AI systems are safe, secure, fair, and privacy-preserving. These mechanisms can also create or strengthen channels for
holding AI developers accountable for harms associated with AI development. In this report, we
provide an overview of some such mechanisms, and then discuss third party auditing, red team
exercises, safety and bias bounties, and sharing of AI incidents in more detail.
• Software mechanisms largely pertain to speciﬁc AI systems and their properties. Software mechanisms can be used to provide evidence for both formal and informal claims regarding the properties
of speciﬁc AI systems, enabling greater understanding and oversight. The software mechanisms
we highlight below are audit trails, interpretability, and privacy-preserving machine learning.
• Hardware mechanisms largely pertain to physical computational resources and their properties.
Hardware mechanisms can support veriﬁable claims by providing greater assurance regarding the
privacy and security of AI systems, and can be used to substantiate claims about how an organization is using their general-purpose computing capabilities. Further, the distribution of resources
across different actors can inﬂuence the types of AI systems that are developed and which actors are capable of assessing other actors’ claims (including by reproducing them). The hardware
mechanisms we focus on in this report are hardware security features for machine learning,
high-precision compute measurement, and computing power support for academia.
true with certainty (subject to certain assumptions). In contrast, this report largely focuses on claims that are unlikely to be
demonstrated with absolute certainty, but which can be shown to be likely or unlikely to be true through relevant arguments
and evidence.
7Broadly, a sociotechnical system is one whose "core interface consists of the relations between a nonhuman system and a
human system", rather than the components of those systems in isolation. See Trist .
Scope and Limitations
This report focuses on a particular aspect of trustworthy AI development: the extent to which organizations developing AI systems can and do make veriﬁable claims about the AI systems they build, and the
ability of other parties to assess those claims. Given the backgrounds of the authors, the report focuses
in particular on mechanisms for demonstrating claims about AI systems being safe, secure, fair, and/or
privacy-preserving, without implying that those are the only sorts of claims that need to be veriﬁed.
We devote particular attention to mechanisms8 that the authors have expertise in and for which concrete
and beneﬁcial next steps were identiﬁed at an April 2019 workshop. These are not the only mechanisms
relevant to veriﬁable claims; we survey some others at the beginning of each section, and expect that
further useful mechanisms have yet to be identiﬁed.
Making veriﬁable claims is part of, but not equivalent to, trustworthy AI development, broadly deﬁned.
An AI developer might also be more or less trustworthy based on the particular values they espouse, the
extent to which they engage affected communities in their decision-making, or the extent of recourse
that they provide to external parties who are affected by their actions. Additionally, the actions of AI
developers, which we focus on, are not all that matters for trustworthy AI development–the existence
and enforcement of relevant laws matters greatly, for example.
Appendix I discusses the reasons for the report’s scope in more detail, and Appendix II discusses the
relationship between different deﬁnitions of trust and veriﬁable claims. When we use the term "trust"
as a verb in the report, we mean that one party (party A) gains conﬁdence in the reliability of another
party’s claims (party B) based on evidence provided about the accuracy of those claims or related ones.
We also make reference to this claim-oriented sense of trust when we discuss actors "earning" trust,
(providing evidence for claims made), or being "trustworthy" (routinely providing sufﬁcient evidence
for claims made). This use of language is intended to concisely reference an important dimension of
trustworthy AI development, and is not meant to imply that veriﬁable claims are sufﬁcient for attaining
trustworthy AI development.
Outline of the Report
The next three sections of the report, Institutional Mechanisms and Recommendations, Software
Mechanisms and Recommendations, and Hardware Mechanisms and Recommendations, each begin with a survey of mechanisms relevant to that category. Each section then highlights several mechanisms that we consider especially promising. We are uncertain which claims are most important to
verify in the context of AI development, but strongly suspect that some combination of the mechanisms
we outline in this report are needed to craft an AI ecosystem in which responsible AI development can
The way we articulate the case for each mechanism is problem-centric: each mechanism helps address
a potential barrier to claim veriﬁcation identiﬁed by the authors. Depending on the case, the recommendations associated with each mechanism are aimed at implementing a mechanism for the ﬁrst time,
researching it, scaling it up, or extending it in some way.
8We use the term mechanism generically to refer to processes, systems, or approaches for providing or generating evidence
about behavior.
The Conclusion puts the report in context, discusses some important caveats, and reﬂects on next steps.
The Appendices provide important context, supporting material, and supplemental analysis. Appendix
I provides background on the workshop and the process that went into writing the report; Appendix II
serves as a glossary and discussion of key terms used in the report; Appendix III discusses the nature
and importance of veriﬁable claims; Appendix IV discusses the importance of veriﬁable claims in the
context of arms control; Appendix V provides context on antitrust law as it relates to cooperation among
AI developers on responsible AI development; and Appendix VI offers supplemental analysis of several
mechanisms.
Institutional Mechanisms and Recommendations
"Institutional mechanisms" are processes that shape or clarify the incentives of the people involved in
AI development, make their behavior more transparent, or enable accountability for their behavior.
Institutional mechanisms help to ensure that individuals or organizations making claims regarding AI
development are incentivized to be diligent in developing AI responsibly and that other stakeholders can
verify that behavior. Institutions9 can shape incentives or constrain behavior in various ways.
Several clusters of existing institutional mechanisms are relevant to responsible AI development, and
we characterize some of their roles and limitations below. These provide a foundation for the subsequent, more detailed discussion of several mechanisms and associated recommendations. Speciﬁcally,
we provide an overview of some existing institutional mechanisms that have the following functions:
• Clarifying organizational goals and values;
• Increasing transparency regarding AI development processes;
• Creating incentives for developers to act in ways that are responsible; and
• Fostering exchange of information among developers.
Institutional mechanisms can help clarify an organization’s goals and values, which in turn can provide a basis for evaluating their claims. These statements of goals and values–which can also be viewed
as (high level) claims in the framework discussed here–can help to contextualize the actions an organization takes and lay the foundation for others (shareholders, employees, civil society organizations,
governments, etc.) to monitor and evaluate behavior. Over 80 AI organizations , including technology companies such as Google , OpenAI , and Microsoft have publicly stated the principles
they will follow in developing AI. Codes of ethics or conduct are far from sufﬁcient, since they are typically abstracted away from particular cases and are not reliably enforced, but they can be valuable by
establishing criteria that a developer concedes are appropriate for evaluating its behavior.
The creation and public announcement of a code of ethics proclaims an organization’s commitment to
ethical conduct both externally to the wider public, as well as internally to its employees, boards, and
shareholders. Codes of conduct differ from codes of ethics in that they contain a set of concrete behavioral
standards.10
Institutional mechanisms can increase transparency regarding an organization’s AI development
processes in order to permit others to more easily verify compliance with appropriate norms, regulations, or agreements. Improved transparency may reveal the extent to which actions taken by an AI
developer are consistent with their declared intentions and goals. The more reliable, timely, and complete the institutional measures to enhance transparency are, the more assurance may be provided.
9Institutions may be formal and public institutions, such as: laws, courts, and regulatory agencies; private formal arrangements between parties, such as contracts; interorganizational structures such as industry associations, strategic alliances,
partnerships, coalitions, joint ventures, and research consortia. Institutions may also be informal norms and practices that
prescribe behaviors in particular contexts; or third party organizations, such as professional bodies and academic institutions.
10Many organizations use the terms synonymously. The speciﬁcity of codes of ethics can vary, and more speciﬁc (i.e., actionguiding) codes of ethics (i.e. those equivalent to codes of conduct) can be better for earning trust because they are more
falsiﬁable. Additionally, the form and content of these mechanisms can evolve over time–consider, e.g., Google’s AI Principles,
which have been incrementally supplemented with more concrete guidance in particular areas.
Transparency measures could be undertaken on a voluntary basis or as part of an agreed framework
involving relevant parties (such as a consortium of AI developers, interested non-proﬁts, or policymakers). For example, algorithmic impact assessments are intended to support affected communities and
stakeholders in assessing AI and other automated decision systems . The Canadian government, for
example, has centered AIAs in its Directive on Automated Decision-Making . Another path
toward greater transparency around AI development involves increasing the extent and quality of documentation for AI systems. Such documentation can help foster informed and safe use of AI systems by
providing information about AI systems’ biases and other attributes .
Institutional mechanisms can create incentives for organizations to act in ways that are responsible.
Incentives can be created within an organization or externally, and they can operate at an organizational
or an individual level. The incentives facing an actor can provide evidence regarding how that actor will
behave in the future, potentially bolstering the credibility of related claims. To modify incentives at an
organizational level, organizations can choose to adopt different organizational structures (such as beneﬁt
corporations) or take on legally binding intra-organizational commitments. For example, organizations
could credibly commit to distributing the beneﬁts of AI broadly through a legal commitment that shifts
ﬁduciary duties.11
Institutional commitments to such steps could make a particular organization’s ﬁnancial incentives more
clearly aligned with the public interest. To the extent that commitments to responsible AI development
and distribution of beneﬁts are widely implemented, AI developers would stand to beneﬁt from each
others’ success, potentially12 reducing incentives to race against one another . And critically, government regulations such as the General Data Protection Regulation (GDPR) enacted by the European
Union shift developer incentives by imposing penalties on developers that do not adequately protect
privacy or provide recourse for algorithmic decision-making.
Finally, institutional mechanisms can foster exchange of information between developers. To avoid
"races to the bottom" in AI development, AI developers can exchange lessons learned and demonstrate
their compliance with relevant norms to one another. Multilateral fora (in addition to bilateral conversations between organizations) provide opportunities for discussion and repeated interaction, increasing
transparency and interpersonal understanding. Voluntary membership organizations with stricter rules
and norms have been implemented in other industries and might also be a useful model for AI developers
Steps in the direction of robust information exchange between AI developers include the creation of
consensus around important priorities such as safety, security, privacy, and fairness;14 participation in
multi-stakeholder fora such as the Partnership on Artiﬁcial Intelligence to Beneﬁt People and Society
(PAI), the Association for Computing Machinery (ACM), the Institute of Electrical and Electronics Engineers (IEEE), the International Telecommunications Union (ITU), and the International Standards
Organization (ISO); and clear identiﬁcation of roles or ofﬁces within organizations who are responsible
11The Windfall Clause is one proposal along these lines, and involves an ex ante commitment by AI ﬁrms to donate a
signiﬁcant amount of any eventual extremely large proﬁts.
12The global nature of AI development, and the national nature of much relevant regulation, is a key complicating factor.
13See for example the norms set and enforced by the European Telecommunications Standards Institute (ETSI). These norms
have real "teeth," such as the obligation for designated holders of Standard Essential Patents to license on Fair, Reasonable and
Non-discriminatory (FRAND) terms. Breach of FRAND could give rise to a breach of contract claim as well as constitute a
breach of antitrust law . Voluntary standards for consumer products, such as those associated with Fairtrade and Organic
labels, are also potentially relevant precedents .
14An example of such an effort is the Asilomar AI Principles .
for maintaining and deepening interorganizational communication .15
It is also important to examine the incentives (and disincentives) for free ﬂow of information within
an organization. Employees within organizations developing AI systems can play an important role in
identifying unethical or unsafe practices. For this to succeed, employees must be well-informed about
the scope of AI development efforts within their organization and be comfortable raising their concerns,
and such concerns need to be taken seriously by management.16 Policies (whether governmental or
organizational) that help ensure safe channels for expressing concerns are thus key foundations for
verifying claims about AI development being conducted responsibly.
The subsections below each introduce and explore a mechanism with the potential for improving the
veriﬁability of claims in AI development: third party auditing, red team exercises, bias and safety
bounties, and sharing of AI incidents. In each case, the subsections below begin by discussing a
problem which motivates exploration of that mechanism, followed by a recommendation for improving
or applying that mechanism.
15Though note competitors sharing commercially sensitive, non-public information (such as strategic plans or R&D plans)
could raise antitrust concerns. It is therefore important to have the right antitrust governance structures and procedures in
place (i.e., setting out exactly what can and cannot be shared). See Appendix V.
16Recent revelations regarding the culture of engineering and management at Boeing highlight the urgency of this issue
Third Party Auditing
The process of AI development is often opaque to those outside a given organization, and
various barriers make it challenging for third parties to verify the claims being made by a
developer. As a result, claims about system attributes may not be easily veriﬁed.
AI developers have justiﬁable concerns about being transparent with information concerning commercial
secrets, personal information, or AI systems that could be misused; however, problems arise when these
concerns incentivize them to evade scrutiny. Third party auditors can be given privileged and secured
access to this private information, and they can be tasked with assessing whether safety, security, privacy,
and fairness-related claims made by the AI developer are accurate.
Auditing is a structured process by which an organization’s present or past behavior is assessed for
consistency with relevant principles, regulations, or norms. Auditing has promoted consistency and
accountability in industries outside of AI such as ﬁnance and air travel. In each case, auditing is tailored
to the evolving nature of the industry in question.17 Recently, auditing has gained traction as a potential
paradigm for assessing whether AI development was conducted in a manner consistent with the stated
principles of an organization, with valuable work focused on designing internal auditing processes (i.e.
those in which the auditors are also employed by the organization being audited) .
Third party auditing is a form of auditing conducted by an external and independent auditor, rather
than the organization being audited, and can help address concerns about the incentives for accuracy
in self-reporting. Provided that they have sufﬁcient information about the activities of an AI system, independent auditors with strong reputational and professional incentives for truthfulness can help verify
claims about AI development.
Auditing could take at least four quite different forms, and likely further variations are possible: auditing
by an independent body with government-backed policing and sanctioning power; auditing that occurs
entirely within the context of a government, though with multiple agencies involved ; auditing by
a private expert organization or some ensemble of such organizations; and internal auditing followed
by public disclosure of (some subset of) the results.18 As commonly occurs in other contexts, the results produced by independent auditors might be made publicly available, to increase conﬁdence in the
propriety of the auditing process.19
Techniques and best practices have not yet been established for auditing AI systems. Outside of AI,
however, there are well-developed frameworks on which to build. Outcomes- or claim-based "assurance
frameworks" such as the Claims-Arguments-Evidence framework (CAE) and Goal Structuring Notation
(GSN) are already in wide use in safety-critical auditing contexts.20 By allowing different types of arguments and evidence to be used appropriately by auditors, these frameworks provide considerable
ﬂexibility in how high-level claims are substantiated, a needed feature given the wide ranging and fast-
17See Raji and Smart et al. for a discussion of some lessons for AI from auditing in other industries.
18Model cards for model reporting and data sheets for datasets reveal information about AI systems publicly, and
future work in third party auditing could build on such tools, as advocated by Raji and Smart et al. .
19Consumer Reports, originally founded as the Consumers Union in 1936, is one model for an independent, third party
organization that performs similar functions for products that can affect the health, well-being, and safety of the people using
those products. ( 
20See Appendix III for further discussion of claim-based frameworks for auditing.
evolving societal challenges posed by AI.
Possible aspects of AI systems that could be independently audited include the level of privacy protection
guaranteed, the extent to (and methods by) which the AI systems were tested for safety, security or
ethical concerns, and the sources of data, labor, and other resources used. Third party auditing could
be applicable to a wide range of AI applications, as well. Safety-critical AI systems such as autonomous
vehicles and medical AI systems, for example, could be audited for safety and security. Such audits
could conﬁrm or refute the accuracy of previous claims made by developers, or compare their efforts
against an independent set of standards for safety and security. As another example, search engines and
recommendation systems could be independently audited for harmful biases.
Third party auditors should be held accountable by government, civil society, and other stakeholders
to ensure that strong incentives exist to act accurately and fairly. Reputational considerations help to
ensure auditing integrity in the case of ﬁnancial accounting, where ﬁrms prefer to engage with credible
auditors . Alternatively, a licensing system could be implemented in which auditors undergo a
standard training process in order to become a licensed AI system auditor. However, given the variety
of methods and applications in the ﬁeld of AI, it is not obvious whether auditor licensing is a feasible
option for the industry: perhaps a narrower form of licensing would be helpful (e.g., a subset of AI such
as adversarial machine learning).
Auditing imposes costs (ﬁnancial and otherwise) that must be weighed against its value. Even if auditing
is broadly societally beneﬁcial and non-ﬁnancial costs (e.g., to intellectual property) are managed, the
ﬁnancial costs will need to be borne by someone (auditees, large actors in the industry, taxpayers, etc.),
raising the question of how to initiate a self-sustaining process by which third party auditing could mature
and scale. However, if done well, third party auditing could strengthen the ability of stakeholders in the
AI ecosystem to make and assess veriﬁable claims. And notably, the insights gained from third party
auditing could be shared widely, potentially beneﬁting stakeholders even in countries with different
regulatory approaches for AI.
Recommendation: A coalition of stakeholders should create a task force to research options for
conducting and funding third party auditing of AI systems.
AI developers and other stakeholders (such as civil society organizations and policymakers) should collaboratively explore the challenges associated with third party auditing. A task force focused on this
issue could explore appropriate initial domains/applications to audit, devise approaches for handling
sensitive intellectual property, and balance the need for standardization with the need for ﬂexibility as
AI technology evolves.21 Collaborative research into this domain seems especially promising given that
the same auditing process could be used across labs and countries. As research in these areas evolves, so
too will auditing processes–one might thus think of auditing as a "meta-mechanism" which could involve
assessing the quality of other efforts discussed in this report such as red teaming.
One way that third party auditing could connect to government policies, and be funded, is via a "regulatory market" . In a regulatory market for AI, a government would establish high-level outcomes
to be achieved from regulation of AI (e.g., achievement of a certain level of safety in an industry) and
then create or support private sector entities or other organizations that compete in order to design and
implement the precise technical oversight required to achieve those outcomes.22 Regardless of whether
such an approach is pursued, third party auditing by private actors should be viewed as a complement
21This list is not exhaustive - see, e.g., , , and for related discussions.
22Examples of such entities include EXIDA, the UK Ofﬁce of Nuclear Regulation, and the private company Adelard.
to, rather than a substitute, for governmental regulation. And regardless of the entity conducting oversight of AI developers, in any case there will be a need to grapple with difﬁcult challenges such as the
treatment of proprietary data.
Red Team Exercises
It is difﬁcult for AI developers to address the "unknown unknowns" associated with AI systems,
including limitations and risks that might be exploited by malicious actors. Further, existing
red teaming approaches are insufﬁcient for addressing these concerns in the AI context.
In order for AI developers to make veriﬁable claims about their AI systems being safe or secure, they need
processes for surfacing and addressing potential safety and security risks. Practices such as red teaming
exercises help organizations to discover their own limitations and vulnerabilities as well as those of the
AI systems they develop, and to approach them holistically, in a way that takes into account the larger
environment in which they are operating.23
A red team exercise is a structured effort to ﬁnd ﬂaws and vulnerabilities in a plan, organization, or
technical system, often performed by dedicated "red teams" that seek to adopt an attacker’s mindset
and methods. In domains such as computer security, red teams are routinely tasked with emulating
attackers in order to ﬁnd ﬂaws and vulnerabilities in organizations and their systems. Discoveries made
by red teams allow organizations to improve security and system integrity before and during deployment.
Knowledge that a lab has a red team can potentially improve the trustworthiness of an organization with
respect to their safety and security claims, at least to the extent that effective red teaming practices exist
and are demonstrably employed.
As indicated by the number of cases in which AI systems cause or threaten to cause harm, developers of an
AI system often fail to anticipate the potential risks associated with technical systems they develop. These
risks include both inadvertent failures and deliberate misuse. Those not involved in the development
of a particular system may be able to more easily adopt and practice an attacker’s skillset. A growing
number of industry labs have dedicated red teams, although best practices for such efforts are generally
in their early stages.24 There is a need for experimentation both within and across organizations in order
to move red teaming in AI forward, especially since few AI developers have expertise in relevant areas
such as threat modeling and adversarial machine learning .
AI systems and infrastructure vary substantially in terms of their properties and risks, making in-house
red-teaming expertise valuable for organizations with sufﬁcient resources. However, it would also be
beneﬁcial to experiment with the formation of a community of AI red teaming professionals that draws
together individuals from different organizations and backgrounds, speciﬁcally focused on some subset
of AI (versus AI in general) that is relatively well-deﬁned and relevant across multiple organizations.25
A community of red teaming professionals could take actions such as publish best practices, collectively
analyze particular case studies, organize workshops on emerging issues, or advocate for policies that
would enable red teaming to be more effective.
Doing red teaming in a more collaborative fashion, as a community of focused professionals across
23Red teaming could be aimed at assessing various properties of AI systems, though we focus on safety and security in this
subsection given the expertise of the authors who contributed to it.
24For an example of early efforts related to this, see Marshall et al., "Threat Modeling AI/ML Systems and Dependencies"
25In the context of language models, for example, 2019 saw a degree of communication and coordination across AI developers
to assess the relative risks of different language understanding and generation systems . Adversarial machine learning,
too, is an area with substantial sharing of lessons across organizations, though it is not obvious whether a shared red team
focused on this would be too broad.
organizations, has several potential beneﬁts:
• Participants in such a community would gain useful, broad knowledge about the AI ecosystem,
allowing them to identify common attack vectors and make periodic ecosystem-wide recommendations to organizations that are not directly participating in the core community;
• Collaborative red teaming distributes the costs for such a team across AI developers, allowing those
who otherwise may not have utilized a red team of similarly high quality or one at all to access its
beneﬁts (e.g., smaller organizations with less resources);
• Greater collaboration could facilitate sharing of information about security-related AI incidents.26
Recommendation: Organizations developing AI should run red teaming exercises to explore risks
associated with systems they develop, and should share best practices and tools for doing so.
Two critical questions that would need to be answered in the context of forming a more cohesive AI
red teaming community are: what is the appropriate scope of such a group, and how will proprietary
information be handled?27 The two questions are related. Particularly competitive contexts (e.g., autonomous vehicles) might be simultaneously very appealing and challenging: multiple parties stand to
gain from pooling of insights, but collaborative red teaming in such contexts is also challenging because
of intellectual property and security concerns.
As an alternative to or supplement to explicitly collaborative red teaming, organizations building AI
technologies should establish shared resources and outlets for sharing relevant non-proprietary information. The subsection on sharing of AI incidents also discusses some potential innovations that could
alleviate concerns around sharing proprietary information.
26This has a precedent from cybersecurity; MITRE’s ATT&CK is a globally accessible knowledge base of adversary tactics and
techniques based on real-world observations, which serves as a foundation for development of more speciﬁc threat models and
methodologies to improve cybersecurity ( 
27These practical questions are not exhaustive, and even addressing them effectively might not sufﬁce to ensure that collaborative red teaming is beneﬁcial. For example, one potential failure mode is if collaborative red teaming fostered excessive
homogeneity in the red teaming approaches used, contributing to a false sense of security in cases where that approach is
insufﬁcient.
Bias and Safety Bounties
There is too little incentive, and no formal process, for individuals unafﬁliated with a particular
AI developer to seek out and report problems of AI bias and safety. As a result, broad-based
scrutiny of AI systems for these properties is relatively rare.
"Bug bounty" programs have been popularized in the information security industry as a way to compensate individuals for recognizing and reporting bugs, especially those related to exploits and vulnerabilities . Bug bounties provide a legal and compelling way to report bugs directly to the institutions
affected, rather than exposing the bugs publicly or selling the bugs to others. Typically, bug bounties
involve an articulation of the scale and severity of the bugs in order to determine appropriate compensation.
While efforts such as red teaming are focused on bringing internal resources to bear on identifying risks
associated with AI systems, bounty programs give outside individuals a method for raising concerns
about speciﬁc AI systems in a formalized way. Bounties provide one way to increase the amount of
scrutiny applied to AI systems, increasing the likelihood of claims about those systems being veriﬁed or
Bias28 and safety bounties would extend the bug bounty concept to AI, and could complement existing
efforts to better document datasets and models for their performance limitations and other properties.29
We focus here on bounties for discovering bias and safety issues in AI systems as a starting point for
analysis and experimentation, but note that bounties for other properties (such as security, privacy protection, or interpretability) could also be explored.30
While some instances of bias are easier to identify, others can only be uncovered with signiﬁcant analysis
and resources. For example, Ziad Obermeyer et al. uncovered racial bias in a widely used algorithm
affecting millions of patients . There have also been several instances of consumers with no direct
access to AI institutions using social media and the press to draw attention to problems with AI . To
date, investigative journalists and civil society organizations have played key roles in surfacing different
biases in deployed AI systems. If companies were more open earlier in the development process about
possible faults, and if users were able to raise (and be compensated for raising) concerns about AI to
institutions, users might report them directly instead of seeking recourse in the court of public opinion.31
In addition to bias, bounties could also add value in the context of claims about AI safety. Algorithms or
models that are purported to have favorable safety properties, such as enabling safe exploration or robustness to distributional shifts , could be scrutinized via bounty programs. To date, more attention
has been paid to documentation of models for bias properties than safety properties,32 though in both
28For an earlier exploration of bias bounties by one of the report authors, see Rubinovitz .
29For example, model cards for model reporting and datasheets for datasets are recently developed means of
documenting AI releases, and such documentation could be extended with publicly listed incentives for ﬁnding new forms of
problematic behavior not captured in that documentation.
30Bounties for ﬁnding issues with datasets used for training AI systems could also be considered, though we focus on trained
AI systems and code as starting points.
31We note that many millions of dollars have been paid to date via bug bounty programs in the computer security domain,
providing some evidence for this hypothesis. However, bug bounties are not a panacea and recourse to the public is also
appropriate in some cases.
32We also note that the challenge of avoiding harmful biases is sometimes framed as a subset of safety, though for the
cases, benchmarks remain in an early state. Improved safety metrics could increase the comparability
of bounty programs and the overall robustness of the bounty ecosystem; however, there should also be
means of reporting issues that are not well captured by existing metrics.
Note that bounties are not sufﬁcient for ensuring that a system is safe, secure, or fair, and it is important
to avoid creating perverse incentives (e.g., encouraging work on poorly-speciﬁed bounties and thereby
negatively affecting talent pipelines) . Some system properties can be difﬁcult to discover even with
bounties, and the bounty hunting community might be too small to create strong assurances. However,
relative to the status quo, bounties might increase the amount of scrutiny applied to AI systems.
Recommendation: AI developers should pilot bias and safety bounties for AI systems to strengthen
incentives and processes for broad-based scrutiny of AI systems.
Issues to be addressed in setting up such a bounty program include :
• Setting compensation rates for different scales/severities of issues discovered;
• Determining processes for soliciting and evaluating bounty submissions;
• Developing processes for disclosing issues discovered via such bounties in a timely fashion;33
• Designing appropriate interfaces for reporting of bias and safety problems in the context of deployed AI systems;
• Deﬁning processes for handling reported bugs and deploying ﬁxes;
• Avoiding creation of perverse incentives.
There is not a perfect analogy between discovering and addressing traditional computer security vulnerabilities, on the one hand, and identifying and addressing limitations in AI systems, on the other.
Work is thus needed to explore the factors listed above in order to adapt the bug bounty concept to
the context of AI development. The computer security community has developed norms (though not a
consensus) regarding how to address "zero day" vulnerabilities,34 but no comparable norms yet exist in
the AI community.
There may be a need for distinct approaches to different types of vulnerabilities and associated bounties,
depending on factors such as the potential for remediation of the issue and the stakes associated with the
AI system. Bias might be treated differently from safety issues such as unsafe exploration, as these have
distinct causes, risks, and remediation steps. In some contexts, a bounty might be paid for information
even if there is no ready ﬁx to the identiﬁed issue, because providing accurate documentation to system
users is valuable in and of itself and there is often no pretense of AI systems being fully robust. In other
purposes of this discussion, little hinges on this terminological issue. We distinguish the two in the title of this section in order
to call attention to the unique properties of different types of bounties.
33Note that we speciﬁcally consider public bounty programs here, though instances of private bounty programs also exist
in the computer security community. Even in the event of a publicly advertised bounty, however, submissions may be private,
and as such there is a need for explicit policies for handling submissions in a timely and legitimate fashion–otherwise such
programs will provide little assurance.
34A zero-day vulnerability is a security vulnerability that is unknown to the developers of the system and other affected
parties, giving them "zero days" to mitigate the issue if the vulnerability were to immediately become widely known. The
computer security community features a range of views on appropriate responses to zero-days, with a common approach being
to provide a ﬁnite period for the vendor to respond to notiﬁcation of the vulnerability before the discoverer goes public.
cases, more care will be needed in responding to the identiﬁed issue, such as when a model is widely
used in deployed products and services.
Sharing of AI Incidents
Claims about AI systems can be scrutinized more effectively if there is common knowledge of
the potential risks of such systems. However, cases of desired or unexpected behavior by AI
systems are infrequently shared since it is costly to do unilaterally.
Organizations can share AI "incidents," or cases of undesired or unexpected behavior by an AI system
that causes or could cause harm, by publishing case studies about these incidents from which others can
learn. This can be accompanied by information about how they have worked to prevent future incidents
based on their own and others’ experiences.
By default, organizations developing AI have an incentive to primarily or exclusively report positive
outcomes associated with their work rather than incidents. As a result, a skewed image is given to the
public, regulators, and users about the potential risks associated with AI development.
The sharing of AI incidents can improve the veriﬁability of claims in AI development by highlighting
risks that might not have otherwise been considered by certain actors. Knowledge of these risks, in turn,
can then be used to inform questions posed to AI developers, increasing the effectiveness of external
scrutiny. Incident sharing can also (over time, if used regularly) provide evidence that incidents are
found and acknowledged by particular organizations, though additional mechanisms would be needed
to demonstrate the completeness of such sharing.
AI incidents can include those that are publicly known and transparent, publicly known and anonymized,
privately known and anonymized, or privately known and transparent. The Partnership on AI has begun
building an AI incident-sharing database, called the AI Incident Database.35 The pilot was built using
publicly available information through a set of volunteers and contractors manually collecting known AI
incidents where AI caused harm in the real world.
Improving the ability and incentive of AI developers to report incidents requires building additional
infrastructure, analogous to the infrastructure that exists for reporting incidents in other domains such
as cybersecurity. Infrastructure to support incident sharing that involves non-public information would
require the following resources:
• Transparent and robust processes to protect organizations from undue reputational harm brought
about by the publication of previously unshared incidents. This could be achieved by anonymizing
incident information to protect the identity of the organization sharing it. Other informationsharing methods should be explored that would mitigate reputational risk to organizations, while
preserving the usefulness of information shared;
• A trusted neutral third party that works with each organization under a non-disclosure agreement
to collect and anonymize private information;
35See Partnership on AI’s AI Incident Registry as an example ( A related resource is a list
called Awful AI, which is intended to raise awareness of misuses of AI and to spur discussion around contestational research
and tech projects . A separate list summarizes various cases in which AI systems "gamed" their speciﬁcations in unexpected
ways . Additionally, AI developers have in some cases provided retrospective analyses of particular AI incidents, such as
with Microsoft’s "Tay" chatbot .
• An organization that maintains and administers an online platform where users can easily access
the incident database, including strong encryption and password protection for private incidents
as well as a way to submit new information. This organization would not have to be the same as
the third party that collects and anonymizes private incident data;
• Resources and channels to publicize the existence of this database as a centralized resource, to accelerate both contributions to the database and positive uses of the knowledge from the database;
• Dedicated researchers who monitor incidents in the database in order to identify patterns and
shareable lessons.
The costs of incident sharing (e.g., public relations risks) are concentrated on the sharing organization, although the beneﬁts are shared broadly by those who gain valuable information about AI incidents. Thus, a cooperative approach needs to be taken for incident sharing that addresses the potential
downsides. A more robust infrastructure for incident sharing (as outlined above), including options
for anonymized reporting, would help ensure that fear of negative repercussions from sharing does not
prevent the beneﬁts of such sharing from being realized.36
Recommendation: AI developers should share more information about AI incidents, including
through collaborative channels.
Developers should seek to share AI incidents with a broad audience so as to maximize their usefulness,
and take advantage of collaborative channels such as centralized incident databases as that infrastructure
matures. In addition, they should move towards publicizing their commitment to (and procedures for)
doing such sharing in a routine way rather than in an ad-hoc fashion, in order to strengthen these
practices as norms within the AI development community.
Incident sharing is closely related to but distinct from responsible publication practices in AI and coordinated disclosure of cybersecurity vulnerabilities . Beyond implementation of progressively more
robust platforms for incident sharing and contributions to such platforms, future work could also explore
connections between AI and other domains in more detail, and identify key lessons from other domains
in which incident sharing is more mature (such as the nuclear and cybersecurity industries).
Over the longer term, lessons learned from experimentation and research could crystallize into a mature
body of knowledge on different types of AI incidents, reporting processes, and the costs associated with
incident sharing. This, in turn, can inform any eventual government efforts to require or incentivize
certain forms of incident reporting.
36We do not mean to claim that building and using such infrastructure would be sufﬁcient to ensure that AI incidents are
addressed effectively. Sharing is only one part of the puzzle for effectively managing incidents. For example, attention should
also be paid to ways in which organizations developing AI, and particularly safety-critical AI, can become "high reliability
organizations" (see, e.g., ).
Software Mechanisms and Recommendations
Software mechanisms involve shaping and revealing the functionality of existing AI systems. They can
support veriﬁcation of new types of claims or verify existing claims with higher conﬁdence. This section
begins with an overview of the landscape of software mechanisms relevant to verifying claims, and then
highlights several key problems, mechanisms, and associated recommendations.
Software mechanisms, like software itself, must be understood in context (with an appreciation for the
role of the people involved). Expertise about many software mechanisms is not widespread, which
can create challenges for building trust through such mechanisms. For example, an AI developer that
wants to provide evidence for the claim that "user data is kept private" can help build trust in the lab’s
compliance with a a formal framework such as differential privacy, but non-experts may have in mind
a different deﬁnition of privacy.37 It is thus critical to consider not only which claims can and can’t be
substantiated with existing mechanisms in theory, but also who is well-positioned to scrutinize these
mechanisms in practice.38
Keeping their limitations in mind, software mechanisms can substantiate claims associated with AI development in various ways that are complementary to institutional and hardware mechanisms. They
can allow researchers, auditors, and others to understand the internal workings of any given system.
They can also help characterize the behavioral proﬁle of a system over a domain of expected usage.
Software mechanisms could support claims such as:
• This system is robust to ’natural’ distributional shifts ;
• This system is robust even to adversarial examples ;
• This system has a well-characterized error surface and users have been informed of contexts in
which the system would be unsafe to use;
• This system’s decisions exhibit statistical parity with respect to sensitive demographic attributes39;
• This system provides repeatable or reproducible results.
Below, we summarize several clusters of mechanisms which help to substantiate some of the claims
Reproducibility of technical results in AI is a key way of enabling veriﬁcation of claims about system
37For example, consider a desideratum for privacy: access to a dataset should not enable an adversary to learn anything
about an individual that could not be learned without access to the database. Differential privacy as originally conceived does
not guarantee this–rather, it guarantees (to an extent determined by a privacy budget) that one cannot learn whether that
individual was in the database in question.
38In Section 3.3, we discuss the role that computing power–in addition to expertise–can play in inﬂuencing who can verify
which claims.
39Conceptions of, and measures for, fairness in machine learning, philosophy, law, and beyond vary widely. See, e.g., Xiang
and Raji and Binns .
properties, and a number of ongoing initiatives are aimed at improving reproducibility in AI.4041 Publication of results, models, and code increase the ability of outside parties (especially technical experts)
to verify claims made about AI systems. Careful experimental design and the use of (and contribution
to) standard software libraries can also improve reproducibility of particular results.42
Formal veriﬁcation establishes whether a system satisﬁes some requirements using the formal methods
of mathematics. Formal veriﬁcation is often a compulsory technique deployed in various safety-critical
domains to provide guarantees regarding the functional behaviors of a system. These are typically guarantees that testing cannot provide. Until recently, AI systems utilizing machine learning (ML)43 have
not generally been subjected to such rigor, but the increasing use of ML in safety-critical domains, such
as automated transport and robotics, necessitates the creation of novel formal analysis techniques addressing ML models and their accompanying non-ML components. Techniques for formally verifying ML
models are still in their infancy and face numerous challenges,44 which we discuss in Appendix VI(A).
The empirical veriﬁcation and validation of machine learning by machine learning has been proposed as an alternative paradigm to formal veriﬁcation. Notably, it can be more practical than formal
veriﬁcation, but since it operates empirically, the method cannot as fully guarantee its claims. Machine
learning could be used to search for common error patterns in another system’s code, or be used to
create simulation environments to adversarially ﬁnd faults in an AI system’s behavior.
For example, adaptive stress testing (AST) of an AI system allows users to ﬁnd the most likely failure of a
system for a given scenario using reinforcement learning , and is being used by to validate the next
generation of aircraft collision avoidance software . Techniques requiring further research include
using machine learning to evaluate another machine learning system (either by directly inspecting its
policy or by creating environments to test the model) and using ML to evaluate the input of another
machine learning model. In the future, data from model failures, especially pooled across multiple labs
and stakeholders, could potentially be used to create classiﬁers that detect suspicious or anomalous AI
Practical veriﬁcation is the use of scientiﬁc protocols to characterize a model’s data, assumptions, and
performance. Training data can be rigorously evaluated for representativeness ; assumptions
can be characterized by evaluating modular components of an AI model and by clearly communicating
output uncertainties; and performance can be characterized by measuring generalization, fairness, and
performance heterogeneity across population subsets. Causes of differences in performance between
40We note the distinction between narrow senses of reproducibility that focus on discrete technical results being reproducible
given the same initial conditions, sometimes referred to as repeatability, and broader senses of reproducibility that involve
reported performance gains carrying over to different contexts and implementations.
41One way to promote robustness is through incentivizing reproducibility of reported results.
There are increasing effort to award systems the recognition that they are robust, e.g., through ACM’s artifact evaluation badges
 
Conferences are also introducing artifact evaluation,
e.g., in the intersection between computer systems research and ML. See, e.g., and The Reproducibility Challenge is another notable effort in this area:
 
42In the following section on hardware mechanisms, we also discuss how reproducibility can be advanced in part by leveling
the playing ﬁeld between industry and other sectors with respect to computing power.
43Machine learning is a subﬁeld of AI focused on the design of software that improves in response to data, with that data
taking the form of unlabeled data, labeled data, or experience. While other forms of AI that do not involve machine learning
can still raise privacy concerns, we focus on machine learning here given the recent growth in associated privacy techniques
as well as the widespread deployment of machine learning.
44Research into perception-based properties such as pointwise robustness, for example, are not sufﬁciently comprehensive
to be applied to real-time critical AI systems such as autonomous vehicles.
models could be robustly attributed via randomized controlled trials.
A developer may wish to make claims about a system’s adversarial robustness.45 Currently, the security
balance is tilted in favor of attacks rather than defenses, with only adversarial training having
stood the test of multiple years of attack research. Certiﬁcates of robustness, based on formal proofs, are
typically approximate and give meaningful bounds of the increase in error for only a limited range of
inputs, and often only around the data available for certiﬁcation (i.e. not generalizing well to unseen
data ). Without approximation, certiﬁcates are computationally prohibitive for all but the
smallest real world tasks . Further, research is needed on scaling formal certiﬁcation methods to
larger model sizes.
The subsections below discuss software mechanisms that we consider especially important to advance
further. In particular, we discuss audit trails, interpretability, and privacy-preserving machine learning.
45Adversarial robustness refers to an AI system’s ability to perform well in the context of (i.e. to be robust against) "adversarial" inputs, or inputs designed speciﬁcally to degrade the system’s performance.
Audit Trails
AI systems lack traceable logs of steps taken in problem-deﬁnition, design, development, and
operation, leading to a lack of accountability for subsequent claims about those systems’ properties and impacts.
Audit trails can improve the veriﬁability of claims about engineered systems, although they are not yet a
mature mechanism in the context of AI. An audit trail is a traceable log of steps in system operation, and
potentially also in design and testing. We expect that audit trails will grow in importance as AI is applied
to more safety-critical contexts. They will be crucial in supporting many institutional trust-building
mechanisms, such as third-party auditors, government regulatory bodies,46 and voluntary disclosure of
safety-relevant information by companies.
Audit trails could cover all steps of the AI development process, from the institutional work of problem
and purpose deﬁnition leading up to the initial creation of a system, to the training and development of
that system, all the way to retrospective accident analysis.
There is already strong precedence for audit trails in numerous industries, in particular for safety-critical
systems. Commercial aircraft, for example, are equipped with ﬂight data recorders that record and capture multiple types of data each second . In safety-critical domains, the compliance of such evidence
is usually assessed within a larger "assurance case" utilising the CAE or Goal-Structuring-Notation (GSN)
frameworks.47 Tools such as the Assurance and Safety Case Environment (ACSE) exist to help both the
auditor and the auditee manage compliance claims and corresponding evidence. Version control tools
such as GitHub or GitLab can be utilized to demonstrate individual document traceability. Proposed
projects like Veriﬁable Data Audit could establish conﬁdence in logs of data interactions and usage.
Recommendation: Standards setting bodies should work with academia and industry to develop
audit trail requirements for safety-critical applications of AI systems.
Organizations involved in setting technical standards–including governments and private actors–should
establish clear guidance regarding how to make safety-critical AI systems fully auditable.48 Although
application dependent, software audit trails often require a base set of traceability49 trails to be demonstrated for qualiﬁcation;50 the decision to choose a certain set of trails requires considering trade-offs
about efﬁciency, completeness, tamperprooﬁng, and other design considerations. There is ﬂexibility in
the type of documents or evidence the auditee presents to satisfy these general traceability requirements
46Such as the National Transportation Safety Board with regards to autonomous vehicle trafﬁc accidents.
47See Appendix III for discussion of assurance cases and related frameworks.
48Others have argued for the importance of audit trails for AI elsewhere, sometimes under the banner of "logging." See, e.g.,
49Traceability in this context refers to "the ability to verify the history, location, or application of an item by means of documented recorded identiﬁcation," where the item in question is digital in nature,
and might relate to various aspects of an AI system’s development and deployment process.
50This includes traceability: between the system safety requirements and the software safety requirements, between the
software safety requirements speciﬁcation and software architecture, between the software safety requirements speciﬁcation
and software design, between the software design speciﬁcation and the module and integration test speciﬁcations, between
the system and software design requirements for hardware/software integration and the hardware/software integration test
speciﬁcations, between the software safety requirements speciﬁcation and the software safety validation plan, and between
the software design speciﬁcation and the software veriﬁcation (including data veriﬁcation) plan.
(e.g., between test logs and requirement documents, veriﬁcation and validation activities, etc.).51
Existing standards often deﬁne in detail the required audit trails for speciﬁc applications. For example,
IEC 61508 is a basic functional safety standard required by many industries, including nuclear power.
Such standards are not yet established for AI systems. A wide array of audit trails related to an AI
development process can already be produced, such as code changes, logs of training runs, all outputs
of a model, etc. Inspiration might be taken from recent work on internal algorithmic auditing and
ongoing work on the documentation of AI systems more generally, such as the ABOUT ML project .
Importantly, we recommend that in order to have maximal impact, any standards for AI audit trails
should be published freely, rather than requiring payment as is often the case.
51See Appendix III.
Interpretability
It’s difﬁcult to verify claims about "black-box" AI systems that make predictions without explanations or visibility into their inner workings. This problem is compounded by a lack of
consensus on what interpretability means.
Despite remarkable performance on a variety of problems, AI systems are frequently termed "black
boxes" due to the perceived difﬁculty of understanding and anticipating their behavior. This lack of
interpretability in AI systems has raised concerns about using AI models in high stakes decision-making
contexts where human welfare may be compromised . Having a better understanding of how the internal processes within these systems work can help proactively anticipate points of failure, audit model
behavior, and inspire approaches for new systems.
Research in model interpretability is aimed at helping to understand how and why a particular model
works. A precise, technical deﬁnition for interpretability is elusive; by nature, the deﬁnition is subject
to the inquirer. Characterizing desiderata for interpretable models is a helpful way to formalize interpretability . Useful interpretability tools for building trust are also highly dependent on the
target user and the downstream task. For example, a model developer or regulator may be more interested in understanding model behavior over the entire input distribution whereas a novice layperson
may wish to understand why the model made a particular prediction for their individual case.52
Crucially, an "interpretable" model may not be necessary for all situations. The weight we place upon a
model being interpretable may depend upon a few different factors, for example:
• More emphasis in sensitive domains (e.g., autonomous driving or healthcare,53 where an incorrect prediction adversely impacts human welfare) or when it is important for end-users to have
actionable recourse (e.g., bank loans) ;
• Less emphasis given historical performance data (e.g., a model with sufﬁcient historical performance may be used even if it’s not interpretable); and
• Less emphasis if improving interpretability incurs other costs (e.g., compromising privacy).
In the longer term, for sensitive domains where human rights and/or welfare can be harmed, we anticipate that interpretability will be a key component of AI system audits, and that certain applications of
AI will be gated on the success of providing adequate intuition to auditors about the model behavior.
This is already the case in regulated domains such as ﬁnance .54
An ascendent topic of research is how to compare the relative merits of different interpretability methods
in a sensible way. Two criteria appear to be crucial: a. The method should provide sufﬁcient insight for
52While deﬁnitions in this area are contested, some would distinguish between "interpretability" and "explainability" as
categories for these two directions, respectively.
53See, e.g., Sendak et. al. which focuses on building trust in a hospital context, and contextualizes the role of interpretability in this process.
54In New York, an investigation is ongoing into apparent gender discrimination associated with the Apple Card’s credit line
allowances. This case illustrates the interplay of (a lack of) interpretability and the potential harms associated with automated
decision-making systems .
the end-user to understand how the model is making its predictions (e.g., to assess if it aligns with
human judgment), and b. the interpretable explanation should be faithful to the model, i.e., accurately
reﬂect its underlying behavior.
Work on evaluating a., while limited in treatment, has primarily centered on comparing methods using
human surveys . More work at the intersection of human-computer interaction, cognitive science,
and interpretability research–e.g., studying the efﬁcacy of interpretability tools or exploring possible
interfaces–would be welcome, as would further exploration of how practitioners currently use such
tools .
Evaluating b., the reliability of existing methods is an active area of research 
 . This effort is complicated by the lack of ground truth on system behavior (if we
could reliably anticipate model behavior under all circumstances, we would not need an interpretability
method). The wide use of interpretable tools in sensitive domains underscores the continued need to
develop benchmarks that assess the reliability of produced model explanations.
It is important that techniques developed under the umbrella of interpretability not be used to provide
clear explanations when such clarity is not feasible. Without sufﬁcient rigor, interpretability could be
used in service of unjustiﬁed trust by providing misleading explanations for system behavior. In identifying, carrying out, and/or funding research on interpretability, particular attention should be paid to
whether and how such research might eventually aid in verifying claims about AI systems with high
degrees of conﬁdence to support risk assessment and auditing.
Recommendation: Organizations developing AI and funding bodies should support research into
the interpretability of AI systems, with a focus on supporting risk assessment and auditing.
Some areas of interpretability research are more developed than others. For example, attribution methods for explaining individual predictions of computer vision models are arguably one of the most welldeveloped research areas. As such, we suggest that the following under-explored directions would be
useful for the development of interpretability tools that could support veriﬁable claims about system
properties:
• Developing and establishing consensus on the criteria, objectives, and frameworks for interpretability research;
• Studying the provenance of a learned model (e.g., as a function of the distribution of training data,
choice of particular model families, or optimization) instead of treating models as ﬁxed; and
• Constraining models to be interpretable by default, in contrast to the standard setting of trying to
interpret a model post-hoc.
This list is not intended to be exhaustive, and we recognize that there is uncertainty about which research
directions will ultimately bear fruit. We discuss the landscape of interpretability research further in
Appendix VI(C).
Privacy-Preserving Machine Learning
A range of methods can potentially be used to veriﬁably safeguard the data and models involved
in AI development. However, standards are lacking for evaluating new privacy-preserving machine learning techniques, and the ability to implement them currently lies outside a typical AI
developer’s skill set.
Training datasets for AI often include sensitive information about people, raising risks of privacy violation. These risks include unacceptable access to raw data (e.g., in the case of an untrusted employee or a
data breach), unacceptable inference from a trained model (e.g., when sensitive private information can
be extracted from a model), or unacceptable access to a model itself (e.g., when the model represents
personalized preferences of an individual or is protected by intellectual property).
For individuals to trust claims about an ML system sufﬁciently so as to participate in its training, they
need evidence about data access (who will have access to what kinds of data under what circumstances),
data usage, and data protection. The AI development community, and other relevant communities, have
developed a range of methods and mechanisms to address these concerns, under the general heading of
"privacy-preserving machine learning" (PPML) .
Privacy-preserving machine learning aims to protect the privacy of data or models used in machine
learning, at training or evaluation time and during deployment. PPML has beneﬁts for model users, and
for those who produce the data that models are trained on.
PPML is heavily inspired by research from the cryptography and privacy communities and is performed
in practice using a combination of techniques, each with its own limitations and costs. These techniques
are a powerful tool for supporting trust between data owners and model users, by ensuring privacy of
key information. However, they must be used judiciously, with informed trade-offs among (1) privacy
beneﬁts, (2) model quality, (3) AI developer experience and productivity, and (4) overhead costs such as
computation, communication, or energy consumption. They are also not useful in all contexts; therefore,
a combination of techniques may be required in some contexts to protect data and models from the
actions of well-resourced malicious actors.
Before turning to our recommendation, we provide brief summaries of several PPML techniques that
could support veriﬁable claims.
Federated learning is a machine learning technique where many clients (e.g., mobile devices or whole
organizations) collaboratively train a model under the orchestration of a central server (e.g., service
provider), while keeping the training data decentralized . Each client’s raw data is stored locally and
not exchanged or transferred . Federated learning addresses privacy concerns around the centralized
collection of raw data, by keeping the data where it is generated (e.g., on the user’s device or in a local
silo) and only allowing model updates to leave the client.
Federated learning does not, however, fully guarantee the privacy of sensitive data on its own, as some
aspects of raw data could be memorized in the training process and extracted from the trained model if
measures are not taken to address this threat. These measures include quantifying the degree to which
models memorize training data , and incorporating differential privacy techniques to limit the contribution of individual clients in the federated setting . Even when used by itself, federated learning
addresses the threats that are endemic to centralized data collection and access, such as unauthorized
access, data hacks, and leaks, and the inability of data owners to control their data lifecycle.
Differential privacy is a system for publicly sharing information derived from a dataset by describing the patterns of groups within the dataset, while withholding information about individuals in
the dataset; it allows for precise measurements of privacy risks for current and potential data owners,
and can address the raw-data-extraction threat described above. Differential privacy works through the
addition of a controlled amount of statistical noise to obscure the data contributions from records or
individuals in the dataset.55 Differential privacy is already used in various private and public AI settings,
and researchers are exploring its role in compliance with new privacy regulations .
Differential privacy and federated learning complement each other in protecting the privacy of raw data:
federated learning keeps the raw data on the personal device, so it is never seen by the model trainer,
while differential privacy ensures the model sufﬁciently prevents the memorization of raw data, so that
it cannot be extracted from the model by its users.56 These techniques do not, however, protect the
model itself from theft .
Encrypted computation addresses this risk by allowing the model to train and run on encrypted data
while in an encrypted state, at the cost of overhead in terms of computation and communication. As a result, those training the model will not be able to see, leak, or otherwise abuse the data in its unencrypted
form. The most well known methods for encrypted computation are homomorphic encryption, secure
multi-party computation, and functional encryption . For example, one of OpenMined’s upcoming
projects is Encrypted Machine Learning as a Service, which allows a model owner and data owner to
use their model and data to make a prediction, without the model owner disclosing their model, and
without the data owner disclosing their data.57
These software mechanisms can guarantee tighter bounds on AI model usage than the legal agreements
that developers currently employ, and tighter bounds on user data usage than institutional mechanisms
such as user privacy agreements. Encrypted computation could also potentially improve the veriﬁability
of claims by allowing sensitive models to be shared for auditing in a more secure fashion. A hardwarebased method to protect models from theft (and help protect raw data from adversaries) is the use of
secure enclaves, as discussed in Section 4.1 below.
In the future, it may be possible to rely on a platform that enables veriﬁable data policies which address
some of the security and privacy vulnerabilities in existing IT systems. One proposal for such a platform
is Google’s Project Oak,58 which leverages open source secure enclaves (see Section 4.1) and formal
veriﬁcation to technically enforce and assure policies around data storage, manipulation, and exchange.
As suggested by this brief overview of PPML techniques, there are many opportunities for improving
the privacy and security protections associated with ML systems. However, greater standardization of
55To illustrate how statistical noise can be helpful in protecting privacy, consider the difference between a survey that solicits
and retains "raw" answers from individuals, on the one hand, and another survey in which the respondents are asked to ﬂip
a coin in order to determine whether they will either provide the honest answer right away or ﬂip the coin again in order
to determine which answer to provide. The latter approach would enable individual survey respondents to have plausible
deniability regarding their true answers, but those conducting the survey could still learn useful information from the responses,
since the noise would largely cancel out at scale. For an accessible discussion of the ideas behind differential privacy and its
applications, from which this short summary was adapted, see .
56For an example of the combination of federated learning and differential privacy, see McMahan et. al. .
57See 
58See the Appendix VI(B) for further discussion of this project.
of PPML techniques–and in particular, the use of open source PPML frameworks that are benchmarked
against common performance measures–may be needed in order for this to translate into a major impact
on the veriﬁability of claims about AI development. First, robust open source frameworks are needed in
order to reduce the skill requirement for implementing PPML techniques, which to date have primarily
been adopted by large technology companies with in-house expertise in both ML and cryptography.
Second, common standards for evaluating new PPML techniques could increase the comparability of new
results, potentially accelerating research progress. Finally, standardization could improve the ability of
external parties (including users, auditors, and policymakers) to verify claims about PPML performance.
Recommendation: AI developers should develop, share, and use suites of tools for privacypreserving machine learning that include measures of performance against common standards.
Where possible, AI developers should contribute to, use, and otherwise support the work of open-source
communities working on PPML, such as OpenMined, Microsoft SEAL, tf-encrypted, tf-federated, and
nGraph-HE. These communities have opened up the ability to use security and privacy tools in the ML
setting, and further maturation of the software libraries built by these communities could yield still
further beneﬁts.
Open-source communities projects or projects backed by a particular company can sometimes suffer from
a lack of stable funding support59 or independence as organizational priorities shift, suggesting a need for
an AI community-wide approach to supporting PPML’s growth. Notwithstanding some challenges associated with open source projects, they are uniquely amenable to broad-based scrutiny and iteration, and
have yielded beneﬁts already. Notably, integrated libraries for multiple techniques in privacy-preserving
ML have started being developed for major deep learning frameworks such as TensorFlow and PyTorch.
Benchmarks for PPML could help unify goals and measure progress across different groups.60 A centralized repository of real-world implementation cases, a compilation of implementation guides, and work
on standardization/interoperability would all also aid in supporting adoption and scrutiny of privacypreserving methods.61
59Novel approaches to funding open source work should also be considered in this context, such as GitHub’s "sponsors"
initiative. 
th-github-sponsors/about-github-sponsors
60The use of standard tools, guides, and benchmarks can also potentially advance research in other areas, but we focus on
privacy-preserving ML in particular here given the backgrounds of the authors who contributed to this subsection. Additionally,
we note that some benchmarks have been proposed in the PPML literature for speciﬁc subsets of techniques, such as DPComp
for differential privacy, but we expect that further exploration of benchmarks across the full spectra of PPML techniques would
be valuable.
61On the other hand, we note that benchmarks also have potential disadvantages, as they incentivize developers to perform
well on the speciﬁc benchmark, rather than focusing on the speciﬁcs of the intended use case of their product or service, which
may signiﬁcantly diverge from the benchmark setting; the design of benchmarks, and more diverse and adaptive evaluation
and comparison methods, is its own technical challenge, as well as an institutional challenge to incentivize appropriate curation
and use of benchmarks to establish a common understanding of what is achievable.
Hardware Mechanisms and Recommendations
Computing hardware enables the training, testing, and use of AI systems. Hardware relevant to AI
development ranges from sensors, networking, and memory, to, perhaps most crucially, processing power
 .62 Concerns about the security and other properties of computing hardware, as well as methods
to address those concerns in a veriﬁable manner, long precede the current growth in adoption of AI.
However, because of the increasing capabilities and impacts of AI systems and the particular hardware
demands of the ﬁeld, there is a need for novel approaches to assuring the veriﬁability of claims about
the hardware used in AI development.
Hardware mechanisms involve physical computing resources (e.g., CPUs and GPUs), including their
distribution across actors, the ways they are accessed and monitored, and their properties (e.g., how they
are designed, manufactured, or tested). Hardware can support veriﬁable claims in various ways. Secure
hardware can play a key role in private and secure machine learning by translating privacy constraints
and security guarantees into scrutable hardware designs or by leveraging hardware components in a
software mechanism. Hardware mechanisms can also be used to demonstrate the ways in which an
organization is using its general-purpose computing capabilities.
At a higher level, the distribution of computing power across actors can potentially inﬂuence who is
in a position to verify certain claims about AI development. This is true on the assumption that, all
things being equal, more computing power will enable more powerful AI systems to be built, and that
a technical capability to verify claims may itself require non-negligible computing resources.63
use of standardized, publicly available hardware (sometimes called "commodity hardware") across AI
systems also aids in the independent reproducibility of technical results, which in turn could play a role
in technical auditing and other forms of accountability. Finally, hardware mechanisms can be deployed
to enforce and verify policies relating to the security of the hardware itself (which, like software, might
be compromised through error or malice).
Existing mechanisms performing one or more of these functions are discussed below.
Formal veriﬁcation, discussed above in the software mechanisms section, is the process of establishing
whether a software or hardware system satisﬁes some requirements or properties, using formal methods
to generate mathematical proofs. Practical tools, such as GPUVerify for GPU kernels,65 exist to formally
verify components of the AI hardware base, but veriﬁcation of the complete hardware base is currently
an ambitious goal. Because only parts of the AI hardware ecosystem are veriﬁed, it is important to map
which properties are being veriﬁed for different AI accelerators and under what assumptions, who has
access to evidence of such veriﬁcation processes (which may be part of a third party audit), and what
properties we should invest more research effort into verifying (or which assumption would be a priority
62Experts disagree on the extent to which large amounts of computing power are key to progress in AI development. See,
e.g., Sutton and Brooks for different opinions about the importance of computing power relative to other factors.
63Since training AI systems is more compute-intensive64 than running them, it is not clear that equivalent computational
resources will always be required on the part of those verifying claims about an AI system. However, AI systems are also
beginning to require non-trivial computing resources to run, sometimes requiring the model to be split over multiple machines.
Additionally, one might need to run an AI system many times in order to verify claims about its characteristics, even if each
run is inexpensive. We thus make the conservative assumption that more computing resources would be (all things being
equal) helpful to the scrutiny of claims about large-scale AI systems, as discussed below in the context of academic access to
computing resources, while recognizing that this may not always be true in particular cases.
65 
Remote attestation leverages a "root of trust" (provided in hardware or in software, e.g., a secret key
stored in isolated memory) to cryptographically sign a measurement or property of the system, thus
providing a remote party proof of the authenticity of the measurement or property. Remote attestation
is often used to attest that a certain version of software is currently running, or that a computation took
a certain amount of time (which can then be compared to a reference by the remote party to detect
tampering) .
Cloud computing: Hardware is also at the heart of the relationship between cloud providers and cloud
users (as hardware resources are being rented). Associated veriﬁcation mechanisms can help ensure
that computations are being performed as promised, without the client having direct physical access
to the hardware. For example, one could have assurances that a cloud-based AI service is not skimping on computations by running a less powerful model than advertised, operating on private data in a
disallowed fashion, or compromised by malware .
Cloud providers are a promising intervention point for trust-building mechanisms; a single cloud provider
services, and therefore has inﬂuence over, many customers. Even large AI labs rely predominantly on
cloud computing for some or all of their AI development. Cloud providers already employ a variety of
mechanisms to minimize risks of misuse on their platforms, including "Know Your Customer" services
and Acceptable Use Policies. These mechanisms could be extended to cover AI misuse . Additional
mechanisms could be developed such as a forum where cloud providers can share best-practices about
detecting and responding to misuse and abuse of AI through their services.66
We now turn to more detailed discussions of three hardware mechanisms that could improve the veriﬁability of claims: we highlight the importance of secure hardware for machine learning, high-precision
compute measurement, and computing power support for academia.
conversations
( or through the establishment of new fora dedicated to AI cloud providers.
Secure Hardware for Machine Learning
Hardware security features can provide strong assurances against theft of data and models,
but secure enclaves (also known as Trusted Execution Environments) are only available on
commodity (non-specialized) hardware. Machine learning tasks are increasingly executed on
specialized hardware accelerators, for which the development of secure enclaves faces signiﬁcant up-front costs and may not be the most appropriate hardware-based solution.
Since AI systems always involve physical infrastructure, the security of that infrastructure can play a
key role in claims about a system or its components being secure and private. Secure enclaves have
emerged in recent years as a way to demonstrate strong claims about privacy and security that cannot
be achieved through software alone. iPhones equipped with facial recognition for screen unlocking, for
example, store face-related data on a physically distinct part of the computer known as a secure enclave
in order to provide more robust privacy protection. Increasing the range of scenarios in which secure
enclaves can be applied in AI, as discussed in this subsection, would enable higher degrees of security
and privacy protection to be demonstrated and demanded.
A secure enclave is a set of software and hardware features that together provide an isolated execution
environment that enables a set of strong guarantees regarding security for applications running inside the
enclave . Secure enclaves reduce the ability of malicious actors to access sensitive data or interfere
with a program, even if they have managed to gain access to the system outside the enclave. Secure
enclaves provide these guarantees by linking high-level desired properties (e.g., isolation of a process
from the rest of the system) to low-level design of the chip layout and low-level software interacting
with the chip.
The connection between physical design and low-level software and high-level security claims relies
on a set of underlying assumptions. Despite the fact that researchers have been able to ﬁnd ways to
invalidate these underlying assumptions in some cases, and thus invalidate the high-level security claims
 , these mechanisms help to focus defensive efforts and assure users that relatively extreme
measures would be required to invalidate the claims guaranteed by the design of the enclave.
While use of secure enclaves has become relatively commonplace in the commodity computing industries, their use in machine learning is less mature. Execution of machine learning on secure enclaves has
been demonstrated, but comes with a performance overhead .67 Demonstrations to date have been
carried out on commodity hardware (CPUs and GPUs ) or have secure and veriﬁable
outsourcing of parts of the computation to less secure hardware , rather than on hardware
directly optimized for machine learning (such as TPUs).
For most machine learning applications, the cost of using commodity hardware not specialized for machine learning is fairly low because the hardware already exists, and their computational demands can
be met on such commodity hardware. However, cutting edge machine learning models often use signiﬁcantly more computational resources , driving the use of more specialized hardware for both
training and inference. If used with specialized AI hardware, the use of secure enclaves would require
renewed investment for every new design, which can end up being very costly if generations are short
67For example, training ResNet-32 using Myelin (which utilises a CPU secure enclave) requires 12.9 mins/epoch and results
in a ﬁnal accuracy of 90.8%, whereas the same training on a non-private CPU requires 12.3 mins/epoch and results in a ﬁnal
accuracy of 92.4% .
and of limited batch sizes (as the cost is amortized across all chips that use the design). Some specialized
AI hardware layouts may require entirely novel hardware security features – as the secure enclave model
may not be applicable – involving additional costs.
One particularly promising guarantee that might be provided by ML-speciﬁc hardware security features,
coupled with some form of remote attestation, is a guarantee that a model will never leave a particular
chip, which could be a key building block of more complex privacy and security policies.
Recommendation: Industry and academia should work together to develop hardware security
features for AI accelerators68 or otherwise establish best practices for the use of secure hardware
(including secure enclaves on commodity hardware) in machine learning contexts.
A focused and ongoing effort to integrate hardware security features into ML-specialized hardware could
add value, though it will require collaboration across the sector.
Recent efforts to open source secure enclave designs could help accelerate the process of comprehensively analyzing the security claims made about certain systems . As more workloads move to
specialized hardware, it will be important to either develop secure enclaves for such hardware (or alternative hardware security solutions), or otherwise deﬁne best practices for outsourcing computation
to "untrusted" accelerators while maintaining privacy and security. Similarly, as many machine learning applications are run on GPUs, it will be important to improve the practicality of secure enclaves or
equivalent privacy protections on these processors.
The addition of dedicated security features to ML accelerators at the hardware level may need to take
a different form than a secure enclave. This is in part due to different architectures and different use
of space on the chip; in part due to different weighting of security concerns (e.g., it may be especially
important to prevent unauthorized access to user data); and in part due to a difference in economies
of scale relative to commodity chips, with many developers of ML accelerators being smaller, less-wellresourced actors relative to established chip design companies like Intel or NVIDIA.
68An AI accelerator is a form of computing hardware that is specialized to perform an AI-related computation efﬁciently,
rather than to perform general purpose computation.
High-Precision Compute Measurement
The absence of standards for measuring the use of computational resources reduces the value
of voluntary reporting and makes it harder to verify claims about the resources used in the AI
development process.
Although we cannot know for certain due to limited transparency, it is reasonable to assume that a
signiﬁcant majority of contemporary computing hardware used for AI training and inference is installed
in data centers (which could be corporate, governmental, or academic), with smaller fractions in server
rooms or attached to individual PCs.69
Many tools and systems already exist to monitor installed hardware and compute usage internally (e.g.,
across a cloud provider’s data center or across an academic cluster’s user base). A current example of
AI developers reporting on their compute usage is the inclusion of training-related details in published
research papers and pre-prints, which often share the amount of compute used to train or run a model.70
These are done for the purposes of comparison and replication, though often extra work is required to
make direct comparisons as there is no standard method for measuring and reporting compute usage.71
This ambiguity poses a challenge to trustworthy AI development, since even AI developers who want to
make veriﬁable claims about their hardware use are not able to provide such information in a standard
form that is comparable across organizations and contexts.
Even in the context of a particular research project, issues such as mixed precision training,72 use of
heterogeneous computing resources, and use of pretrained models all complicate accurate reporting
that is comparable across organizations.73 The lack of a common standard or accepted practice on how
to report the compute resources used in the context of a particular project has led to several efforts to
extract or infer the computational requirements of various advances and compare them using a common
framework .
The challenge of providing accurate and useful information about the computational requirements of a
system or research project is not unique to AI – computer systems research has struggled with this problem for some time.74 Both ﬁelds have seen an increasing challenge in comparing and reproducing results
now that organizations with exceptionally large compute resources (also referred to as "hyperscalers")
69For reference, the Cisco Global Cloud Index forecasts that the ratio of data center trafﬁc to non-data center trafﬁc by 2021
will be 103:1. When looking just at data centers, they forecast that by 2021, 94% of workloads and compute instances will be
processed by cloud data centers, with the remaining 6% processed by traditional data centers. Note, however, that these are
for general workloads, not AI speciﬁc .
70Schwartz and Dodge et al. have recommended that researchers always publish ﬁnancial and computational costs alongside
performance increases .
71There are, however, emerging efforts at standardization in particular contexts. The Transaction Processing Performance
Council has a related working group, and efforts like MLPerf are contributing to standardization of some inference-related
calculations, though accounting for training remains especially problematic.
72Mixed precision refers to the growing use of different binary representations of ﬂoating point numbers with varying levels
of precision (e.g., 8 bit, 16 bit, 32 bit or 64 bit) at different points of a computation, often trading-off lower precision for higher
throughput or performance in ML contexts relative to non-ML contexts. Since an 8 bit ﬂoating point operation, say, differs in
hardware requirements from a 64 bit ﬂoating point operation, traditional measures in terms of Floating Point Operations Per
Second (FLOPS) fail to capture this heterogeneity.
73For an illustrative discussion of the challenges associated with reporting compute usage for a large-scale AI project, see,
e.g., OpenAI’s Dota 2 project .
74See, e.g., Vitek & Kalibera and Hoeﬂer & Belli .
play an ever-increasing role in research in those ﬁelds. We believe there is value in further engaging
with the computer systems research community to explore challenges of reproducibility, benchmarking,
and reporting, though we also see value in developing AI-speciﬁc standards for compute reporting.
Increasing the precision and standardization of compute reporting could enable easier comparison of
research results across organizations. Improved methods could also serve as building blocks of credible
third party oversight of AI projects: an auditor might note, for example, that an organization has more
computing power available to it than was reportedly used on an audited project, and thereby surface
unreported activities relevant to that project. And employees of an organization are better able to ensure
that their organization is acting responsibly to the extent that they are aware of how computing power,
data, and personnel are being allocated internally for different purposes.
Recommendation: One or more AI labs should estimate the computing power involved in a single
project in great detail, and report on the potential for wider adoption of such methods.
We see value in one or more AI labs conducting a "comprehensive" compute accounting effort, as a means
of assessing the feasibility of standardizing such accounting. "Comprehensive" here refers to accounting
for as much compute usage pertinent to the project as is feasible, and increasing the precision of reported
results relative to existing work.
It is not clear how viable standardization is, given the aforementioned challenges, though there is likely
room for at least incremental progress: just in the past few years, a number of approaches to calculating
and reporting compute usage have been tried, and in some cases have propagated across organizations.
AI researchers interested in conducting such a pilot should work with computer systems researchers who
have worked on related challenges in other contexts, including the automating of logging and reporting.
Notably, accounting of this sort has costs associated with it, and the metrics of success are unclear. Some
accounting efforts could be useful for experts but inaccessible to non-experts, for example, or could only
be workable in a particular context (e.g., with a relatively simple training and inference pipeline and
limited use of pretrained models). As such, we do not advocate for requiring uniformly comprehensive
compute reporting.
Depending on the results of early pilots, new tools might help automate or simplify such reporting,
though this is uncertain. One reason for optimism about the development of a standardized approach
is that a growing fraction of computing power usage occurs in the cloud at "hyperscale" data centers,
so a relatively small number of actors could potentially implement best practices that apply to a large
fraction of AI development .
It is also at present unclear who should have access to reports about compute accounting. While we
applaud the current norm in AI research to voluntarily share compute requirements publicly, we expect
for-proﬁt entities would have to balance openness with commercial secrecy, and government labs may
need to balance openness with security considerations. This may be another instance in which auditors or independent veriﬁers could play a role. Standardization of compute accounting is one path to
formalizing the auditing practice in this space, potentially as a building block to more holistic auditing
regimes. However, as with other mechanisms discussed here, it is insufﬁcient on its own.
Compute Support for Academia
The gap in compute resources between industry and academia limits the ability of those outside
of industry to scrutinize technical claims made by AI developers, particularly those related to
compute-intensive systems.
In recent years, a large number of academic AI researchers have transitioned into industry AI labs. One
reason for this shift is the greater availability of computing resources in industry compared to academia.
This talent shift has resulted in a range of widely useful software frameworks and algorithmic insights,
but has also raised concerns about the growing disparity between the computational resources available
to academia and industry .
The disparity between industry and academia is clear overall, even though some academic labs are
generously supported by government75 or industry76 sponsors, and some government agencies are on
the cutting edge of building and providing access to supercomputers.77
Here we focus on a speciﬁc beneﬁt of governments78 taking action to level the playing ﬁeld of computing
power: namely, improving the ability of ﬁnancially disinterested parties such as academics to verify
the claims made by AI developers in industry, especially in the context of compute-intensive systems.
Example use cases include:
• Providing open-source alternatives to commercial AI systems: given the current norm in AI
development of largely-open publication of research, a limiting factor in providing open source alternatives to commercially trained AI models is often the computing resources required. As models
become more compute-intensive, government support may be required to maintain a thriving open
source AI ecosystem and the various beneﬁts that accrue from it.
• Increasing scrutiny of commercial models: as outlined in the institutional mechanisms section
(see the subsections on red team exercises and bias and safety bounties), there is considerable
value in independent third parties stress-testing the models developed by others. While "black box"
testing can take place without access to signiﬁcant compute resources (e.g., by remote access to an
instance of the system), local replication for the purpose of testing could make testing easier, and
could uncover further issues than those surfaced via remote testing alone. Additional computing
resources may be especially needed for local testing of AI systems that are too large to run on a
single computer (such as some recent language models).
• Leveraging AI to test AI: as AI systems become more complex, it may be useful or even necessary
to deploy adaptive, automated tests to explore potential failure modes or hidden biases, and such
testing may become increasingly compute-intensive.
75 
 
 and for examples of industry support for academic computing.
77For example, the US Department of Energy’s supercomputing division currently hosts the fastest supercomputer worldwide.
78While industry actors can and do provide computing power support to non-industry actors in beneﬁcial ways, the scale and
other properties of such programs are likely to be affected by the rises and falls of particular companies’ commercial fortunes,
and thus are not a reliable long-term solution to the issues discussed here.
• Verifying claims about compute requirements: as described above, accounting for the compute
inputs of model training is currently an open challenge in AI development. In tandem with standardization of compute accounting, compute support to non-industry actors would enable replication efforts, which would verify or undermine claims made by AI developers about the resource
requirements of the systems they develop.
Recommendation: Government funding bodies should substantially increase funding of computing power resources for researchers in academia, in order to improve the ability of those researchers to verify claims made by industry.
While computing power is not a panacea for addressing the gap in resources available for research in
academia and industry, funding bodies such as those in governments could level the playing ﬁeld between
sectors by more generously providing computing credits to researchers in academia.79 Such compute
provision could be made more affordable by governments leveraging their purchasing power in negotiations over bulk compute purchases. Governments could also build their own compute infrastructures
for this purpose. The particular amounts of compute in question, securing the beneﬁts of scale while
avoiding excessive dependence on a particular compute provider, and ways of establishing appropriate
terms for the use of such compute are all exciting areas for future research.80
79As advocated by various authors, e.g., Sastry et al. , Rasser & Lambert et. al. , and Etchemendy and Li .
80This may require signiﬁcant levels of funding, and so the beneﬁts should be balanced against the opportunity cost of public
Conclusion
Artiﬁcial intelligence has the potential to transform society in ways both beneﬁcial and harmful. Beneﬁcial applications are more likely to be realized, and risks more likely to be avoided, if AI developers
earn rather than assume the trust of society and of one another. This report has ﬂeshed out one way
of earning such trust, namely the making and assessment of veriﬁable claims about AI development
through a variety of mechanisms. A richer toolbox of mechanisms for this purpose can inform developers’ efforts to earn trust, the demands made of AI developers by activists and civil society organizations,
and regulators’ efforts to ensure that AI is developed responsibly.
If the widespread articulation of ethical principles can be seen as a ﬁrst step toward ensuring responsible
AI development, insofar as it helped to establish a standard against which behavior can be judged, then
the adoption of mechanisms to make veriﬁable claims represents a second. The authors of this report
are eager to see further steps forward and hope that the framing of these mechanisms inspires the AI
community to begin a meaningful dialogue around approaching veriﬁability in a collaborative fashion
across organizations. We are keen to discover, study, and foreground additional institutional, software,
and hardware mechanisms that could help enable trustworthy AI development. We encourage readers
interested in collaborating in these or other areas to contact the corresponding authors of the report.81
As suggested by the title of the report (which references supporting veriﬁable claims rather than ensuring them), we see the mechanisms discussed here as enabling incremental improvements rather than
providing a decisive solution to the challenge of verifying claims in the AI ecosystem. And despite the
beneﬁts associated with veriﬁable claims, they are also insufﬁcient to ensure that AI developers will
behave responsibly. There are at least three reasons for this.
First, there is a tension between veriﬁability of claims and the generality of such claims. This tension
arises because the narrow properties of a system are easier to verify than the general ones, which tend
to be of greater social interest. Safety writ large, for example, is inherently harder to verify than performance on a particular metric for safety. Additionally, broad claims about the beneﬁcial societal impacts
of a system or organization are harder to verify than more circumscribed claims about impacts in speciﬁc
Second, the veriﬁability of claims does not ensure that they will be veriﬁed in practice. The mere existence of mechanisms for supporting veriﬁable claims does not ensure that they will be demanded by
consumers, citizens, and policymakers (and even if they are, the burden ought not to be on them to do
so). For example, consumers often use technologies in ways that are inconsistent with their stated values
(e.g., a concern for personal privacy) because other factors such as convenience and brand loyalty also
play a role in inﬂuencing their behavior .
Third, even if a claim about AI development is shown to be false, asymmetries of power may prevent
corrective steps from being taken. Members of marginalized communities, who often bear the brunt of
harms associated with AI , often lack the political power to resist technologies that they deem detrimental to their interests. Regulation will be required to ensure that AI developers provide evidence that
bears on important claims they make, to limit applications of AI where there is insufﬁcient technical and
social infrastructure for ensuring responsible development, or to increase the variety of viable options
81The landing page for this report, www.towardtrustworthyai.com, will also be used to share relevant updates after the
report’s publication.
available to consumers that are consistent with their stated values.
These limitations notwithstanding, veriﬁable claims represent a step toward a more trustworthy AI development ecosystem. Without a collaborative effort between AI developers and other stakeholders to
improve the veriﬁability of claims, society’s concerns about AI development are likely to grow: AI is
being applied to an increasing range of high-stakes tasks, and with this wide deployment comes a growing range of risks. With a concerted effort to enable veriﬁable claims about AI development, there is
a greater opportunity to positively shape AI’s impact and increase the likelihood of widespread societal
Acknowledgements
We are extremely grateful to participants in the April 2019 workshop that catalyzed this report, as well
as the following individuals who provided valuable input on earlier versions of this document: David
Lansky, Tonii Leach, Shin Shin Hua, Chris Olah, Alexa Hagerty, Madeleine Clare Elish, Larissa Schiavo,
Heather Roff, Rumman Chowdhury, Ludwig Schubert, Joshua Achiam, Chip Huyen, Xiaowei Huang,
Rohin Shah, Genevieve Fried, Paul Christiano, Sean McGregor, Tom Arnold, Jess Whittlestone, Irene
Solaiman, Ashley Pilipiszyn, Catherine Olsson, Bharath Ramsundar, Brandon Perry, Justin Wang, Max
Daniel, Ryan Lowe, Rebecca Crootof, Umang Bhatt, Ben Garﬁnkel, Claire Leibowicz, Ryan Khurana,
Connor Leahy, Chris Berner, Daniela Amodei, Erol Can Akbaba, William Isaac, Iason Gabriel, Laura
Weidinger, Thomas Dietterich, Olexa Bilaniuk, and attendees of a seminar talk given by author Miles
Brundage on this topic at the Center for Human-Compatible AI (CHAI). None of these people necessarily
endorses the content of the report.