Attention Guided Anomaly Localization in
Shashanka Venkataramanan⋆[0000−0003−1096−1342], Kuan-Chuan
Peng†[0000−0002−2682−9912], Rajat Vikram Singh‡[0000−0002−1416−8344], and
Abhijit Mahalanobis⋆[0000−0002−2782−8655]
⋆Center for Research in Computer Vision, University of Central Florida, Orlando, FL
†Mitsubishi Electric Research Laboratories, Cambridge, MA
‡Siemens Corporate Technology, Princeton, NJ
 , , ,
 
Abstract. Anomaly localization is an important problem in computer
vision which involves localizing anomalous regions within images with
applications in industrial inspection, surveillance, and medical imaging.
This task is challenging due to the small sample size and pixel coverage of
the anomaly in real-world scenarios. Most prior works need to use anomalous training images to compute a class-speciﬁc threshold to localize
anomalies. Without the need of anomalous training images, we propose
Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent
variable to preserve the spatial information. In the unsupervised setting,
we propose an attention expansion loss where we encourage CAVGA to
focus on all normal regions in the image. Furthermore, in the weaklysupervised setting we propose a complementary guided attention loss,
where we encourage the attention map to focus on all normal regions
while minimizing the attention map corresponding to anomalous regions
in the image. CAVGA outperforms the state-of-the-art (SOTA) anomaly
localization methods on MVTec Anomaly Detection (MVTAD), modiﬁed
ShanghaiTech Campus (mSTC) and Large-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting and when using only
2% anomalous images in the weakly-supervised setting. CAVGA also outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10,
Fashion-MNIST, MVTAD, mSTC and LAG datasets.
Keywords: guided attention, anomaly localization, convolutional adversarial variational autoencoder
Introduction
Recognizing whether an image is homogeneous with its previously observed distribution or whether it belongs to a novel or anomalous distribution has been
identiﬁed as an important problem . In this work, we focus on a related task,
anomaly localization in images, which involves segmenting the anomalous regions
 
S. Venkataramanan et al.
(i) CAVGA main idea
improvement
(9/15; 4∼85%)
(9/15; 2∼30%)
(7/12; 2∼42%)
(8/12; 1∼38%)
(1/1; 16%)
(1/1; 1.1%)
MNIST 
d (8/10; 0.1∼2.5%)
CIFAR-10 d
(7/10; 3∼31%)
F-MNIST 
(8/10; 2∼24%)
l: localization; d: detection
F-MNIST: Fashion-MNIST 
• metric for l : IoU
• metric for d in MVTAD, mSTC,
and LAG: classiﬁcation accuracy
• metric for d in MNIST,
CIFAR-10 and Fashion-MNIST:
area under ROC curve
(ii) improvement summary
Fig. 1: (i) CAVGA uses the proposed complementary guided attention loss to
encourage the attention map to cover the entire normal regions while suppressing
the attention map corresponding to anomalous class in the training image. This
enables the trained network to generate the anomalous attention map to localize
the anomaly better at testing (ii) CAVGA’s improvement over SOTA in the form
of (number of outperforming/total categories; improvement (%) in its metric)
within them. Anomaly localization has been applied in industrial inspection settings to segment defective product parts , in surveillance to locate intruders
 , in medical imaging to segment tumor in brain MRI or glaucoma in retina
images , etc. There has been an increase in analysis towards segmenting
potential anomalous regions in images as acknowledged in .
Existing state-of-the-art (SOTA) anomaly localization methods are
based on deep learning. However, developing deep learning based algorithms for
this task can be challenging due to the small pixel coverage of the anomaly and
lack of suitable data since images with anomalies are rarely available in real-world
scenarios . Existing SOTA methods tackle this challenge using autoencoders
 and GAN based approaches , which use a thresholded pixelwise diﬀerence between the input and reconstructed image to localize anomalies.
But, their methods need to determine class-speciﬁc thresholds using anomalous
training images which can be unavailable in real-world scenarios.
To tackle these drawbacks of using anomalous training images, we propose
Convolutional Adversarial Variational autoencoder with Guided Attention
(CAVGA), an unsupervised anomaly localization method which requires no
anomalous training images. CAVGA comprises of a convolutional latent variable to preserve the spatial relation between the input and latent variable. Since
real-world applications may have access to only limited training data , we
propose to localize the anomalies by using supervision on attention maps. This
Attention Guided Anomaly Localization in Images
is motivated by the ﬁnding in that attention based supervision can alleviate
the need of using large amount of training data. Intuitively, without any prior
knowledge of the anomaly, humans need to look at the entire image to identify
the anomalous regions. Based on this idea, we propose an attention expansion
loss where we encourage the network to generate an attention map that focuses
on all normal regions of the image.
Since annotating segmentation training data can be laborious , in the
case when the annotator provides few anomalous training images without ground
truth segmented anomalous regions, we extend CAVGA to a weakly supervised
setting. Here, we introduce a classiﬁer in CAVGA and propose a complementary
guided attention loss computed only for the normal images correctly predicted
by the classiﬁer. Using this complementary guided attention loss, we expand the
normal attention but suppress the anomalous attention on the normal image,
where normal/anomalous attention represents the areas aﬀecting the classiﬁer’s
normal/anomalous prediction identiﬁed by existing network visualization methods (e.g. Grad-CAM ). Fig. 1 (i) (a) illustrates our attention mechanism
during training, and Fig. 1 (i) (b) demonstrates that the resulting normal attention and anomalous attention on the anomalous testing images are visually
complementary, which is consistent with our intuition. Furthermore, Fig. 1 (ii)
summarizes CAVGA’s ability to outperform SOTA methods in anomaly localization on industrial inspection (MVTAD) , surveillance (mSTC) and
medical imaging (LAG) datasets. We also show CAVGA’s ability to outperform SOTA methods in anomaly detection on common benchmarks.
To the best of our knowledge, we are the ﬁrst in anomaly localization to
propose an end-to-end trainable framework with attention guidance which explicitly enforces the network to learn representations from the entire normal
image. As compared to the prior works, our proposed approach CAVGA needs
no anomalous training images to determine a class-speciﬁc threshold to localize
the anomaly. Our contributions are:
– An attention expansion loss (Lae), where we encourage the network to
focus on the entire normal images in the unsupervised setting.
– A complementary guided attention loss (Lcga), which we use to minimize the anomalous attention and simultaneously expand the normal attention for the normal images correctly predicted by the classiﬁer.
– New SOTA: In anomaly localization, CAVGA outperforms SOTA methods
on the MVTAD and mSTC datasets in IoU and mean Area under ROC
curve (AuROC) and also outperforms SOTA anomaly localization methods
on LAG dataset in IoU. We also show CAVGA’s ability to outperform SOTA
methods for anomaly detection on the MVTAD, mSTC, LAG, MNIST ,
CIFAR-10 and Fashion-MNIST datasets in classiﬁcation accuracy.
Related Works
Often used interchangeably, the terms anomaly localization and anomaly segmentation involve pixel-accurate segmentation of anomalous regions within an
S. Venkataramanan et al.
Table 1: Comparison between CAVGA and other anomaly localization methods
in the unsupervised setting in terms of the working properties. Among all the
listed methods, only CAVGA satisﬁes all the listed properties
Does the method satisfy each property?
not using anomalous training images
localize multiple modes of anomalies
pixel (not patch) based localization
use convolutional latent variable
image . They have been applied to industrial inspection settings to segment
defective product parts , medical imaging to segment glaucoma in retina images , etc. Image based anomaly localization has not been fully studied as
compared to anomaly detection, where methods such as employ
a thresholded pixel wise diﬀerence between the input and reconstructed image
to segment the anomalous regions. proposes an inpainter-detector network
for patch-based localization in images. proposes gradient descent on a regularized autoencoder while Liu et al. (denoted as ADVAE) generate gradient
based attention maps from the latent space of the trained model. We compare
CAVGA with the existing methods relevant to anomaly localization in the unsupervised setting in Table 1 and show that among the listed methods, only
CAVGA shows all the listed properties.
Anomaly detection involves determining an image as normal or anomalous
 . One-class classiﬁcation and anomaly detection are related to novelty detection which has been widely studied in computer vision 
and applied to video analysis , remote sensing , etc. With the advance in
GANs , SOTA methods perform anomaly detection by generating realistic
normal images during training . proposes to search the latent space of the generator for detecting anomalies. introduces latent-spacesampling-based network with information-negative mining while proposes
normality score function based on capsule network’s activation and reconstruction error. proposes a deep autoencoder that learns the distribution of latent
representation through autoregressive procedure. Unlike where
anomalous training images are used for anomaly detection, CAVGA does not
need anomalous training images.
Proposed Approach: CAVGA
Unsupervised Approach: CAVGAu
Fig. 2 (a) illustrates CAVGA in the unsupervised setting (denoted as CAVGAu).
CAVGAu comprises of a convolutional latent variable to preserve the spatial
information between the input and latent variable. Since attention maps obtained
from feature maps illustrate the regions of the image responsible for speciﬁc
Attention Guided Anomaly Localization in Images
Fig. 2: (a) The framework of CAVGAu where the attention expansion loss Lae
guides the attention map A computed from the latent variable z to cover the entire normal image. (b) Illustration of CAVGAw with the complementary guided
attention loss Lcga to minimize the anomalous attention Aca
and expand the
normal attention Acn
x for the normal images correctly predicted by the classiﬁer
activation of neurons in the feature maps , we propose an attention expansion
loss such that the feature representation of the latent variable encodes all the
normal regions. This loss encourages the attention map generated from the latent
variable to cover the entire normal training image as illustrated in Fig. 1 (i) (a).
During testing, we localize the anomaly from the areas of the image that the
attention map does not focus on.
Convolutional latent variable Variational Autoencoder (VAE) is a
generative model widely used for anomaly detection . The loss function
of training a vanilla VAE can be formulated as:
L = LR(x, ˆx) + KL(qφ(z|x)||pθ(z|x)),
where LR(x, ˆx) = −1
i=1 xilog(ˆxi)+(1−xi)log(1−ˆxi) is the reconstruction loss
between the input (x) and reconstructed images (ˆx), and N is the total number of
images. The posterior pθ(z|x) is modeled using a standard Gaussian distribution
prior p(z) with the help of Kullback-Liebler (KL) divergence through qφ(z|x).
Since the vanilla VAE results in blurry reconstruction , we use a discriminator
(D(.)) to improve the stability of the training and generate sharper reconstructed
images ˆx using adversarial learning formulated as follows:
log(D(xi)) + log(1 −D( ˆxi))
S. Venkataramanan et al.
Unlike traditional autoencoders where the latent variable is ﬂattened,
inspired from , we use a convolutional latent variable to preserve the spatial
relation between the input and the latent variable.
Attention expansion loss Lae The main contribution of our work involves
using supervision on attention maps to spatially localize the anomaly in the image. Most methods employ a thresholded pixel-wise diﬀerence between
the reconstructed image and the input image to localize the anomaly where the
threshold is determined by using anomalous training images. However, CAVGAu
learns to localize the anomaly using an attention map reﬂected through an endto-end training process without the need of any anomalous training images. We
use the feature representation of the latent variable z to compute the attention
map (A). A is computed using Grad-CAM such that Ai,j ∈ , where Ai,j
is the (i, j) element of A.
Intuitively, A obtained from feature maps focuses on the regions of the image based on the activation of neurons in the feature maps and its respective
importance . Due to the lack of prior knowledge about the anomaly, in
general, humans need to look at the entire image to identify anomalous regions.
We use this notion to learn the feature representation of the entire normal image by proposing an attention expansion loss, where we encourage the network
to generate an attention map covering all the normal regions. This attention
expansion loss for each image Lae,1 is deﬁned as:
where |A| is the total number of elements in A. The ﬁnal attention expansion
loss Lae is the average of Lae,1 over the N images. Since the idea of attention
mechanisms involves locating the most salient regions in the image which
typically does not cover the entire image, we use Lae as an additional supervision
on the network, such that the trained network generates an attention map that
covers all the normal regions. Fig. 1 (i) (a) shows that before using Lae i.e. training CAVGAu only with adversarial learning (Ladv + L) does not encode all the
normal regions into the latent variable, and that the attention map fails to cover
the entire image, which is overcome after using Lae. Furthermore, supervising
on attention maps prevents the trained model to make inference based on incorrect areas and also alleviates the need of using large amount of training data as
shown in , which is not explicitly enforced in existing methods .
We form the ﬁnal objective function Lfinal below:
Lfinal = wrL + wadvLadv + waeLae,
where wr, wadv, and wae are empirically set as 1, 1, and 0.01 respectively.
During testing, we feed an image xtest into the encoder followed by the decoder, which reconstructs an image ˆxtest. As deﬁned in , we compute the
pixel-wise diﬀerence between ˆxtest and xtest as the anomalous score sa. Intuitively, if xtest is drawn from the learnt distribution of z, then sa is small. Without
Attention Guided Anomaly Localization in Images
using any anomalous training images in the unsupervised setting, we normalize
sa between and empirically set 0.5 as the threshold to detect an image as
anomalous. The attention map Atest is computed from z using Grad-CAM and
is inverted (1 - Atest) to obtain an anomalous attention map which localizes the
anomaly. Here, 1 refers to a matrix of all ones with the same dimensions as Atest.
We empirically choose 0.5 as the threshold on the anomalous attention map to
evaluate the localization performance.
Weakly Supervised Approach: CAVGAw
CAVGAu can be further extended to a weakly supervised setting (denoted as
CAVGAw) where we explore the possibility of using few anomalous training
images to improve the performance of anomaly localization. Given the labels
of the anomalous and normal images without the pixel-wise annotation of the
anomaly during training, we modify CAVGAu by introducing a binary classiﬁer
C at the output of z as shown in Fig. 2 (b) and train C using the binary cross
entropy loss Lbce. Given an image x and its ground truth label y, we deﬁne
p ∈{ca, cn} as the prediction of C, where ca and cn are anomalous and normal
classes respectively. From Fig. 2 (b) we clone z into a new tensor, ﬂatten it to
form a fully connected layer zfc, and add a 2-node output layer to form C. z
and zfc share parameters. Flattening zfc enables a higher magnitude of gradient
backpropagation from p .
Complementary guided attention loss Lcga Although, attention maps
generated from a trained classiﬁer have been used in weakly supervised semantic
segmentation tasks , to the best of our knowledge, we are the ﬁrst to
propose supervision on attention maps for anomaly localization in the weakly
supervised setting. Since the attention map depends on the performance of C
 , we propose the complementary guided attention loss Lcga based on C’s
prediction to improve anomaly localization. We use Grad-CAM to compute the
attention map for the anomalous class Aca
x and the attention map for the normal
on the normal image x (y = cn). Using Aca
x , we propose
Lcga where we minimize the areas covered by Aca
but simultaneously enforce
to cover the entire normal image. Since the attention map is computed
by backpropagating the gradients from p, any incorrect p would generate an
undesired attention map. This would lead to the network learning to focus on
erroneous areas of the image during training, which we avoid using Lcga. We
compute Lcga only for the normal images correctly classiﬁed by the classiﬁer i.e.
if p = y = cn. We deﬁne Lcga,1, the complementary guided attention loss for
each image, in the weakly supervised setting as:
Lcga,1 = 1 (p = y = cn)
x )i,j + (Aca
where 1 (·) is an indicator function. Lcga is the average of Lcga,1 over the N
images. Our ﬁnal objective function Lfinal is deﬁned as:
Lfinal = wrL + wadvLadv + wcLbce + wcgaLcga,
S. Venkataramanan et al.
Table 2: Our experimental settings. Notations: u: unsupervised; w: weakly supervised; DM: MNIST ; DF : Fashion-MNIST ; DC: CIFAR-10 
property \ dataset
# total classes
# normal training images
# anomalous training images
# normal testing images
# anomalous testing images
where wr, wadv, wc, and wcga are empirically set as 1, 1, 0.001, and 0.01 respectively. During testing, we use C to predict the input image xtest as anomalous or
normal. The anomalous attention map Atest of xtest is computed when y = ca.
We use the same evaluation method as that in Sec. 3.1 for anomaly localization.
Experimental Setup
Benchmark datasets: We evaluate CAVGA on the MVTAD , mSTC 
and LAG datasets for anomaly localization, and the MVTAD, mSTC, LAG,
MNIST , CIFAR-10 and Fashion-MNIST datasets for anomaly detection. Since STC dataset is designed for video instead of image anomaly
detection, we extract every 5th frame of the video from each scene for training
and testing without using any temporal information. We term the modiﬁed STC
dataset as mSTC and summarize the experimental settings in Table 2.
Baseline methods: For anomaly localization, we compare CAVGA with
AVID , AEL2 , AESSIM , AnoGAN , CNN feature dictionary (CN-
NFD) , texture inspection (TI) , γ-VAE grad (denoted as γ-VAEg),
LSA , ADVAE and variation model (VM) based approaches on the
MVTAD and mSTC datasets. Since does not provide the code for their
method, we adapt the code from and report its best result using our experimental settings. We also compare CAVGAu with CAM , GBP , Smooth-
Grad and Patho-GAN on the LAG dataset. In addition, we compare
CAVGAu with LSA , OCGAN , ULSLM , CapsNet PP-based and CapsNet RE-based (denoted as CapsNetPP and CapsNetRE), AnoGAN ,
ADGAN , and β-VAE on the MNIST, CIFAR-10 and Fashion-MNIST
datasets for anomaly detection.
Architecture details: Based on the framework in Fig. 2 (a), we use the convolution layers of ResNet-18 as our encoder pretrained from ImageNet 
and ﬁnetuned on each category / scene individually. Inspired from , we propose
to use the residual generator as our residual decoder by modifying it with a convolution layer interleaved between two upsampling layers. The skip connection
added from the output of the upsampling layer to the output of the convolution
layer, increases mutual information between observations and latent variable and
also avoids latent variable collapse . We use the discriminator of DC-GAN
Attention Guided Anomaly Localization in Images
Table 3: Performance comparison of anomaly localization in category-speciﬁc
IoU, mean IoU (IoU), and mean AuROC (AuROC) on the MVTAD dataset.
The darker cell color indicates better performance ranking in each row
AVID AESSIM AEL2 AnoGAN γ-VAEg LSA ADVAE CAVGA CAVGA CAVGA CAVGA
Transistor
Toothbrush 0.43
 pretrained on the Celeb-A dataset and ﬁnetuned on our data as our
discriminator and term this network as CAVGA-R. For fair comparisons with
the baseline approaches in terms of network architecture, we use the discriminator and generator of DC-GAN pretrained on the Celeb-A dataset as our encoder
and decoder respectively. We keep the same discriminator as discussed previously
and term this network as CAVGA-D. CAVGA-Du and CAVGA-Ru are termed
as CAVGAu in the unsupervised setting, and CAVGA-Dw and CAVGA-Rw as
CAVGAw in weakly supervised setting respectively.
Training and evaluation: For anomaly localization and detection on the
MVTAD, mSTC and LAG datasets, the network is trained only on normal images in the unsupervised setting. In the weakly supervised setting, since none
of the baseline methods provide the number of anomalous training images they
use to compute the threshold, we randomly choose 2% of the anomalous images
along with all the normal training images for training. On the MNIST, CIFAR-
10 and Fashion-MNIST datasets, we follow the same procedure as deﬁned in
 (training/testing uses single class as normal and the rest of the classes as
anomalous. We train CAVGA-Du using this normal class). For anomaly localization, we show the AuROC and the Intersection-over-Union (IoU) between the
generated attention map and the ground truth. Following , we use the mean
of accuracy of correctly classiﬁed anomalous images and normal images to evaluate the performance of anomaly detection on both the normal and anomalous
images on the MVTAD, mSTC and LAG datasets. On the MNIST, CIFAR-10,
and Fashion-MNIST datasets, same as , we use AuROC for evaluation.
S. Venkataramanan et al.
Experimental Results
We use the cell color in the quantitative result tables to denote the performance
ranking in that row, where darker cell color means better performance.
Performance on anomaly localization: Fig. 3 (a) shows the qualitative
results and Table 3 shows that CAVGAu localizes the anomaly better compared to the baselines on the MVTAD dataset. CAVGA-Du outperforms the
best performing baseline method (γ-VAEg) in mean IoU by 5%. Most baselines
use anomalous training images to compute class-speciﬁc threshold to localize
anomalies. Needing no anomalous training images, CAVGA-Du still outperforms
all the mentioned baselines in mean IoU. In terms of mean AuROC, CAVGA-Du
outperforms CNNFD, TI and VM by 9%, 12% and 10% respectively and achieves
comparable results with best baseline method. Table 3 also shows that CAVGA-
Dw outperforms CAVGA-Du by 22% and 8% on mean IoU and mean AuROC
respectively. CAVGA-Dw also outperforms the baselines in mean AuROC. Fig. 4
illustrates that one challenge in anomaly localization is the low contrast between
the anomalous regions and their background. In such scenarios, although still
outperforming the baselines, CAVGA does not localize the anomaly well.
Fig. 3: Qualitative results on (a) MVTAD & (b) mSTC datasets respectively.
The anomalous attention map (in red) depicts the localization of the anomaly
Fig. 3 (b) illustrates the qualitative results and Table 4 shows that CAVGA
also outperforms the baseline methods in mean IoU and mean AuROC on the
mSTC dataset. Table 5 shows that CAVGA outperforms the most competitive
baseline Patho-GAN by 16% in IoU on the LAG dataset. CAVGA is practically reasonable to train on a single GTX 1080Ti GPU, having comparable
training and testing time with baseline methods.
Attention Guided Anomaly Localization in Images
Table 4: Performance comparison of anomaly localization in IoU and its mean
(IoU) along with anomaly detection in terms of mean of accuracy of correctly
classiﬁed anomalous images and normal images on the mSTC dataset for each
scene ID si. For anomaly localization, we also list the mean AuROC (AuROC)
Task \ Method
γ-VAEg AVID LSA AESSIM AEL2 CAVGA CAVGA CAVGA CAVGA
0.182 0.244
0.206 0.183
0.162 0.265
0.263 0.271
0.234 0.287
Localization
0.314 0.238
0.214 0.137
0.168 0.233
0.193 0.187
0.137 0.146
0.264 0.286
0.180 0.108
0.210 0.215
Table 5: Performance comparison of anomaly localization in IoU along with
anomaly detection in terms of classiﬁcation accuracy on the LAG dataset 
Task \ Method CAM GBP SmoothGrad Patho-GAN CAVGA-Du
Localization
Fig. 4: Examples of incorrect localization of the anomaly on the MVTAD dataset
by CAVGA-Ru and CAVGA-Rw
S. Venkataramanan et al.
Table 6: The mean of accuracy of correctly classiﬁed anomalous images and
normal images in anomaly detection on the MVTAD dataset
AVID AESSIM AEL2 AnoGAN γ-VAEg LSA CAVGA CAVGA CAVGA CAVGA
Transistor
Toothbrush
Performance on anomaly detection: Table 6 shows that CAVGAu outperforms the baselines in the mean of accuracy of correctly classiﬁed anomalous
images and normal images on the MVTAD dataset. CAVGA-Du outperforms the
best performing baseline (γ-VAEg) in mean of classiﬁcation accuracy by 1.3%.
Table 4 and Table 5 show that CAVGA outperforms the baseline methods in
classiﬁcation accuracy on both the mSTC and LAG datasets by 2.6% and 1.1%
respectively. Furthermore, Table 7 shows that CAVGA-Du outperforms all the
baselines in mean AuROC in the unsupervised setting on the MNIST, CIFAR-10
and Fashion-MNIST datasets. CAVGA-Du also outperforms MemAE and
β-VAE by 1.1% and 8% on MNIST and by 21% and 38% on CIFAR-10
datasets respectively. CAVGA-Du also outperforms all the listed baselines in
mean AuROC on the Fashion-MNIST dataset.
Ablation Study
All the ablation studies are performed on 15 categories on the MVTAD dataset,
of which 5 are reported here. The mean of all 15 categories is shown in Table
8. We illustrate the eﬀectiveness of the convolutional z in CAVGA, Lae in the
unsupervised setting, and Lcga in the weakly supervised setting. The qualitative
results are shown in Fig. 5. The column IDs to refer to the columns in Table 8.
Eﬀect of convolutional latent variable z: To show the eﬀectiveness of the
convolutional z, we ﬂatten the output of the encoder of CAVGA-Ru and CAVGA-
Rw, and connect it to a fully connected layer as latent variable. Following ,
the dimension of latent variable is chosen as 100. We call these network as
u and CAVGA-R∗
w in the unsupervised and weakly supervised settings
respectively. In the unsupervised setting, we train CAVGA-Ru and CAVGA-R∗
using L + Ladv as our objective function and compute the anomalous attention
Attention Guided Anomaly Localization in Images
Table 7: Performance comparison of anomaly detection in terms of AuROC and
mean AuROC with the SOTA methods on MNIST (DM) and CIFAR-10 (DC)
datasets . We also report the mean AuROC on Fashion-MNIST (DF ) dataset
Dataset Class γ-VAEg LSA OCGAN ULSLM CapsNetPP CapsNetRE AnoGAN ADGAN CAVGA
DF mean
Table 8: The ablation study on 5 randomly chosen categories showing anomaly
localization in IoU on the MVTAD dataset. The mean of all 15 categories is
reported. CAVGA-R∗
u and CAVGA-R∗
w are our base architecture with a ﬂattened
z in the unsupervised and weakly supervised settings respectively. “conv z”
means using convolutional z
CAVGA CAVGA CAVGA
CAVGA CAVGA CAVGA CAVGA
+ conv z + conv z
+ conv z + conv z
map from the feature map of the latent variable during inference. Similarly,
in the weakly supervised setting, we train CAVGA-Rw and CAVGA-R∗
L + Ladv + Lbce as our objective function and compute the anomalous attention
map from the classiﬁer’s prediction during inference. Comparing column c1 with
S. Venkataramanan et al.
Fig. 5: Qualitative results of the ablation study to illustrate the performance of
the anomaly localization on the MVTAD dataset
c3 and c5 with c7 in Table 8, we observe that preserving the spatial relation of
the input and latent variable through the convolutional z improves the IoU in
anomaly localization without the use of Lae in the unsupervised setting and Lcga
in the weakly supervised setting. Furthermore, comparing column c2 with c4 and
c6 with c8 in Table 8, we observe that using convolutional z in CAVGA-Ru and
CAVGA-Rw outperforms using a ﬂattened latent variable even with the help of
Lae in the unsupervised setting and Lcga in the weakly supervised setting.
Eﬀect of attention expansion loss Lae: To test the eﬀectiveness of using
Lae in the unsupervised setting, we train CAVGA-R∗
u and CAVGA-Ru with eq.
4. During inference, the anomalous attention map is computed to localize the
anomaly. Comparing column c1 with c2 and c3 with c4 in Table 8, we observe that
Lae enhances the IoU regardless of a ﬂattened or convolutional latent variable.
Eﬀect of complementary guided attention loss Lcga: We show the
eﬀectiveness of Lcga by training CAVGA-R∗
w and CAVGA-Rw using eq. 6. Comparing column c5 with c6 and c7 with c8 in Table 8, we ﬁnd that using Lcga
enhances the IoU regardless of a ﬂattened or convolutional latent variable.
Conclusion
We propose an end-to-end convolutional adversarial variational autoencoder using guided attention which is a novel use of this technique for anomaly localization. Applicable to diﬀerent network architectures, our attention expansion loss
and complementary guided attention loss improve the performance of anomaly
localization in the unsupervised and weakly supervised (with only 2% extra
anomalous images for training) settings respectively. We quantitatively and qualitatively show that CAVGA outperforms the state-of-the-art (SOTA) anomaly
localization methods on the MVTAD, mSTC and LAG datasets. We also show
CAVGA’s ability to outperform SOTA anomaly detection methods on the MV-
TAD, mSTC, LAG, MNIST, Fashion-MNIST and CIFAR-10 datasets.
Acknowledgments : This work was done when Shashanka was an intern
and Kuan-Chuan was a StaﬀScientist at Siemens. Shashanka’s eﬀort was partially supported by DARPA under Grant D19AP00032.