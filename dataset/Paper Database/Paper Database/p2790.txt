This document is downloaded from DR‑NTU ( 
Nanyang Technological University, Singapore.
EEG‑based emotion recognition using regularized
graph neural networks
Zhong, Peixiang; Wang, Di; Miao, Chunyan
Zhong, P., Wang, D. & Miao, C.  . EEG‑based emotion recognition using regularized
graph neural networks. IEEE Transactions On Affective Computing.
 
 
 
© 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new
collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted
component of this work in other works. The published version is available at:
 
Downloaded on 27 Mar 2025 00:24:39 SGT
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
EEG-Based Emotion Recognition Using
Regularized Graph Neural Networks
Peixiang Zhong, Di Wang, Senior Member, IEEE, and Chunyan Miao, Senior Member, IEEE
Abstract—Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing
studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this paper, we propose a regularized
graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain
regions to capture both local and global relations among different EEG channels. Speciﬁcally, we model the inter-channel relations in
EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are
inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain
adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations
and noisy labels, respectively. Extensive experiments on two public datasets, SEED and SEED-IV, demonstrate the superior
performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the
proposed adjacency matrix and two regularizers contribute consistent and signiﬁcant gain to the performance of our RGNN model.
Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion
recognition.
Index Terms—Affective Computing, EEG, Graph Neural Network, SEED
INTRODUCTION
MOTION recognition focuses on the recognition of human emotions based on a variety of modalities, such
as audio-visual expressions, body language, physiological
signals, etc. Compared to other modalities, physiological
signals, such as electroencephalography (EEG), electrocardiogram (ECG), electromyography (EMG), etc., have the
advantage of being difﬁcult to hide or disguise. In recent
years, due to the rapid development of noninvasive, easyto-use and inexpensive EEG recording devices, EEG-based
emotion recognition has received an increasing amount of
attention in both research and applications .
Emotion models can be broadly categorized into discrete
models and dimensional models. The former categorizes
emotions into discrete entities, e.g., anger, disgust, fear,
happiness, sadness, and surprise in Ekman’s theory .
The latter describes emotions using their underlying dimensions, e.g., valence, arousal and dominance , which
measures emotions from unpleasant to pleasant, passive to
active, and submissive to dominant, respectively.
EEG signals measure voltage ﬂuctuations from the cortex
in the brain and have been shown to reveal important
information about human emotional states . For example,
greater relative left frontal EEG activity has been observed
when experiencing positive emotions . The voltage ﬂuctuations in different brain regions are measured by electrodes
attached to the scalp. Each electrode collects EEG signals in
one channel. The collected EEG signals are often analyzed
P. Zhong, D. Wang and C. Miao are with the Joint NTU-UBC Research
Centre of Excellence in Active Living for the Elderly (LILY), Nanyang
Technological University, Singapore.
P. Zhong and C. Miao are also with the Alibaba-NTU Singapore Joint
Research Institute and the School of Computer Science and Engineering,
Nanyang Technological University, Singapore.
E-mail: , {wangdi, ascymiao}@ntu.edu.sg
in speciﬁc frequency bands, namely delta (1-4 Hz), theta (4-7
Hz), alpha (8-13 Hz), beta (13-30 Hz), and gamma (>30 Hz).
Many existing EEG-based emotion recognition methods
are primarily based on the supervised machine learning
approach, wherein features are often extracted from preprocessed EEG signals in each channel over a time window.
Then, a classiﬁer is trained on the extracted features to
recognize emotions. Wang et al. compared power spectral density features (PSD), wavelet features and nonlinear
dynamical features with a Support Vector Machine (SVM)
classiﬁer. Zheng and Lu investigated critical frequency
bands and channels using PSD, differential entropy (DE) 
and PSD asymmetry features, and obtained robust accuracy
using deep belief networks (DBN). However, most existing
EEG-based emotion recognition approaches do not address
the following three challenges: 1) the topological structure
of EEG channels are not effectively exploited to learn more
discriminative EEG representations; 2) EEG signals vary signiﬁcantly across different subjects, which hinders the generalizability of the trained classiﬁers in subject-independent
classiﬁcation settings; and 3) participants may not always
generate the intended emotions when watching emotioneliciting stimuli. Consequently, the emotion labels in the
collected EEG data may be noisy and inconsistent with the
actual elicited emotions.
There have been several attempts to address the ﬁrst
challenge. Zhang et al. and Zhang et al. incorporated spatial relations in EEG signals using convolutional
neural networks (CNN) and recurrent neural networks
(RNN), respectively. However, their approaches require a
2D representation of EEG channels on the scalp, which may
cause information loss during ﬂattening because channels
are actually arranged in the 3D space. In addition, their
approach of using CNNs and RNNs to capture inter-channel
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
relations has difﬁculty in learning long-range dependencies
 . Graph neural networks (GNN) has been applied in 
to capture inter-channel relations using an adjacency matrix.
However, similar to CNNs and RNNs, the GNN approach
 only considers relations between the nearest channels,
which thus may lose valuable information between distant
channels, such as the PSD asymmetry between channels on
the left and right hemispheres in the frontal region, which
has been shown to be informative in valence prediction .
In recent years, several studies , attempted to
tackle the second challenge by investigating the transferability of EEG-based emotion recognition models across
subjects. Lan et al. compared several domain adaptation techniques such as maximum independence domain
adaptation (MIDA), transfer component analysis (TCA),
subspace alignment (SA), etc. They found that the subjectindependent classiﬁcation accuracy can be improved by
around 10%. Li et al. applied domain adversarial training to lower the inﬂuence of individual subject on EEG data
and obtained improved performance as well. However, their
adversarial training does not exploit any graph structure
of the EEG signals and only leads to small performance
improvement in our experiment (see Section 7.1).
To the best of our knowledge, no attempt has been made
to address the third challenge, i.e., noisy emotion labels, in
EEG-based emotion recognition.
In this paper, we propose a regularized graph neural
network (RGNN) aiming to address all the three aforementioned challenges. Graph analysis for human brain has been
studied extensively in the neuroscience literature , .
However, making an accurate connectome is still an open
question and subject to different scales . Inspired by ,
 , we consider each EEG channel as a node in our graph.
Our RGNN model extends the simple graph convolution
network (SGC) and leverages the topological structure
of EEG channels. Speciﬁcally, we propose a sparse adjacency
matrix to capture both local and global inter-channel relations based on the biological topology of human brain .
Local inter-channel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale
 , . Global inter-channel relations connect distant
groups of neurons between the left and right hemispheres
and may reveal emotion-related functional connectivity ,
In addition, we propose a node-wise domain adversarial training (NodeDAT) method to regularize RGNN for
better generalization in subject-independent classiﬁcation
scenarios. Different from the domain adversarial training in
 , , our NodeDAT method provides a ﬁner-grained
regularization by minimizing the domain discrepancies between features in the source and target domains for each
channel/node. Moreover, we propose an emotion-aware
distribution learning (EmotionDL) method to address the
problem of noisy labels in the datasets. Prior studies have
shown that noisy labels can adversely impact classiﬁcation
accuracy . Instead of learning the traditional single-label
classiﬁcation, our EmotionDL method learns a distribution
of labels of the training data and thus acts as a regularizer
to improve the robustness of our model against noisy labels.
Finally, we conduct extensive experiments to validate the
effectiveness of our RGNN model and investigate emotionrelated informative neuronal activities.
In summary, the main contributions of this paper are as
We propose a regularized graph neural network
(RGNN) model to recognize emotions based on EEG
signals. Our biologically inspired model captures
both local and global inter-channel relations.
We propose two regularizers: node-wise domain
adversarial training (NodeDAT) and emotion-aware
distribution learning (EmotionDL), to improve the
robustness of our model against cross-subject variations and noisy labels, respectively.
We conduct extensive experiments in both subjectdependent and subject-independent classiﬁcation
settings on two public EEG datasets, namely SEED
 and SEED-IV . Experimental results demonstrate the effectiveness of our proposed model and
regularizers. In addition, our RGNN model achieves
superior performance over the state-of-the-art models in most experimental settings.
We investigate the emotional neuronal activities
and the results reveal that pre-frontal, parietal and
occipital regions may be the most informative regions for emotion recognition. In addition, global
inter-channel relations between the left and right
hemispheres are important, and local inter-channel
relations between (FP1, AF3), (F6, F8) and (FP2,
AF4) may also provide useful information.
RELATED WORK
In this section, we review related work in the ﬁelds of EEGbased emotion recognition, graph neural network, unsupervised domain adaptation, and learning with noisy labels.
EEG-Based Emotion Recognition
EEG feature extractors and classiﬁers are the two fundamental components in the machine learning approach of EEGbased emotion recognition. EEG features can be broadly
divided into single-channel features and multi-channel ones
 . The majority of existing features are computed on
a single channel, e.g., statistical features , PSD ,
differential entropy (DE) , and wavelet features . A
few number of features are computed on multiple channels
to capture the inter-channel relations, e.g., the asymmetry of
PSD between two hemispheres and functional connectivity , , where common indices such as correlation,
coherence and phase synchronization were used estimate
brain functional connectivity between channels. Another
line of research in multi-channel features is to use common
spatial ﬁlters and spatial-temporal ﬁlters , to
extract class-discriminative EEG features. In contrast, our
model is deigned to operate on single-channel features and
learn to effectively combine them using a graph neural
EEG classiﬁers can be broadly divided into topologyinvariant classiﬁers and topology-aware ones. The majority
of existing classiﬁers are topology-invariant classiﬁers such
as SVM, k-Nearest Neighbors (KNN), DBN and RNN
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
 , which do not take the topological structure of EEG features into account when learning the EEG representations.
In contrast, topology-aware classiﬁers such as CNN , ,
 and GNN consider the inter-channel topological
relations and learn EEG representations for each channel
by aggregating features from nearby channels using convolutional operations either in the Euclidean space or in the
non-Euclidean space. However, as discussed in Section 1,
existing CNNs and GNNs have difﬁculty in learning the
dependencies between distant channels, which may reveal
important emotion-related information. Recently, Zhang et
al. and Li et al. proposed to use RNNs to learn
spatial topological relations between channels by scanning
electrodes in both vertical and horizontal directions. However, their approaches do not fully exploit the topological
structure of EEG channels. For example, two topologically
close channels may be far away from each other in their
scanning sequence. In contrast, our model is able to learn
relations between distant channels using global connections.
Graph Neural Network
Graph neural network (GNN) is a type of neural network
dealing with data in the graph domain, e.g., molecular structures, social networks and knowledge graphs . One early
work on GNN aimed to learn a converged static state
embedding for each node in the graph using a transition
function applied to its neighborhood. Later, inspired by the
convolutional operation of CNN in the Euclidean domain,
Bruna et al. combined spectral graph theory with
neural network and deﬁned convolutional operations in the
graph domain using the spectral ﬁlters computed from the
normalized graph Laplacian. Following this line of research,
Defferrard et al. proposed fast localized convolutions
by using a recursive formulation of the K-order Chebyshev polynomials to approximate the ﬁlters. The resulting
representation for each node is an aggregation of its Kthorder neighborhood. Kipf and Welling further limited
K = 1 and proposed the standard graph convolutional
network (GCN) with a faster localized graph convolutional
operation. The convolutional layers in GCN can be stacked
K times to effectively convolve the Kth-order neighborhood
of a node. Recently, Wu et al. simpliﬁed GCN by
removing the nonlinearities between convolutional layers
in GCN and proposed the simple graph convolution network (SGC), which effectively behaves like a linear feature
transformation followed by a logistic regression. Apart from
the convolution operation used in GCNs, there are other
types of operations used in GNNs, such as attention .
However, they are often trained signiﬁcantly slower than
SGC . In this paper, we extend SGC to model EEG
signals because it performs orders of magnitude faster than
other networks with comparable classiﬁcation accuracy.
Unsupervised Domain Adaptation
Unsupervised domain adaptation aims to mitigate the domain shift in knowledge transfer from a supervised source
domain to an unsupervised target domain. The most common approaches are instance re-weighting and domaininvariant feature learning. Instance re-weighting methods
 aim to infer the resampling weight directly by feature
distribution matching across source and target domains in
a non-parametric manner. Domain-invariant feature leaning
methods align features from both source and target domains
to a common feature space. The alignment can be achieved
by minimizing divergence , maximizing reconstruction
 , or adversarial training . Our proposed NodeDAT
regularizer extends the domain adversarial training to
graph neural networks and achieves ﬁner-grained regularization by minimizing the discrepancies between features in
source and target domains for each node individually.
Learning with Noisy Labels
Commonly adopted approaches to learning with noisy labels are based on the noise transition matrix and robust
loss functions. The noise transition matrix speciﬁes the
probabilities of transition from each ground truth label to
each noisy label and is often applied to modify the crossentropy loss. The matrix can be pre-computed as a prior 
or estimated from noisy data . A few studies tackle noisy
labels by using noise-tolerant robust loss functions, such
as unhinged loss and ramp loss . Our proposed
EmotionDL regularizer is inspired by , which applies
distribution learning to classify ambiguous images.
PRELIMINARIES
In this section, we introduce the preliminaries of the simple
graph convolution network (SGC) and spectral graph
convolution, which are the basis of our RGNN model.
Simple Graph Convolution Network (SGC)
Given a graph G = (V, E), where V denotes a set of nodes
and E denotes a set of edges between nodes in V. Data on
V can be represented by a feature matrix X ∈Rn×d, where
n = |V| and d denotes the input feature dimension. The
edge set E can be represented by a weighted adjacency matrix A ∈Rn×n with self-loops, i.e., Aii = 1, i = 1, 2, ..., n.
In general, GNNs learn a feature transformation function
for X and produces output Z ∈Rn×d
the output feature dimension.
Between adjacent layers in GNNs, the feature transformation can be written as
Hl+1 = f(Hl, A),
where l = 0, 1, ..., L −1, L denotes the number of layers,
H0 = X, HL = Z, and f denotes the function we want to
learn. A simple deﬁnition of f would be
Hl+1 = σ(AHlWl),
where σ denotes a non-linear function and Wl denotes a
weight matrix at layer l. For each node x, function f simply
computes the weighted sum of all the node features in its
neighborhood including x itself, followed by a non-linear
transformation. However, one major limitation of the f in
(2) is that repeatedly applying f along multiple layers may
lead to Hl with overly large values due to summation. Kipf
and Welling alleviated this limitation by proposing the
graph convolution network (GCN) as follows
Hl+1 = σ 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
where D denotes the diagonal degree matrix of A,
j Aij. The normalized adjacency matrix
2 prevents H from growing overly large. If we
ignore σ and Wl temporarily and expand (3), the hidden
state Hl+1
for node xi, i = 1, 2, ..., n, can be computed via
(Dii + 1)(Djj + 1)Hj
Note that each neighboring Hl
j is now normalized by the
degrees of both xi and xj. Successively applying L layers
aggregates node features within a neighborhood of size L.
To further accelerate training while keeping comparable
performance, Wu et al. proposed SGC by removing
the non-linear function σ in (3) and reparameterizing all
linear transformations Wl across all layers into one linear
transformation W as follows
Z = HL = SHL−1WL−1 = ... = SLXW,
where S = D−1
2 and W = W0W1...WL−1. Essentially, SGC computes a topology-aware linear transformation ˆX = SLX, followed by a ﬁnal linear transformation
Spectral Graph Convolution
We analyze GCN from the perspective of spectral graph
theory . Graph Fourier analysis relies on the graph
Laplacian L = D −A or the normalized graph Laplacian
ˆL = I −D−1
2 . Since ˆL is a symmetric positive
semideﬁnite matrix, it can be decomposed as ˆL = UΛUT ,
where U is the orthonormal eigenvector matrix of ˆL and
Λ = diag(λ1, ..., λN) is the diagonal matrix of corresponding eigenvalues. Given graph data X, the graph Fourier
transform of X is ˆX = UT X, and the inverse Fourier
transform of ˆX is X = UˆX. Hence, the graph convolution
between X and a ﬁlter G is computed as follows
X ∗G = U((UT G) ⊙(UT X)) = U ˆGUT X,
where ⊙denotes element-wise multiplication and ˆG =
diag( ˆg1, ..., ˆgn) denotes a diagonal matrix with n spectral
ﬁlter coefﬁcients.
To reduce the current learning complexity of O(n) to that
of conventional CNN, i.e., O(K), (6) can be approximated
using the Kth order polynomials as follows
U ˆGUT X ≈U(
θiΛi)UT X =
where θi denotes learnable parameters. To further reduce
computational cost, Defferrard et al. proposed to use
Chebyshev polynomials to approximate the ﬁltering operation as follows
U ˆGUT X =
λmax ˆL −I denotes the scaled normalized
Laplacian with its eigenvalues lying within [−1, 1], λmax
denotes the maximum eigenvalue of ˆL, and Ti(x) denotes
the Chebyshev polynomials recursively deﬁned as Ti(x) =
2xTi−1(x) −Ti−2(x) with T0(x) = 1 and T1(x) = x.
The GCN proposed in made a few approximations
to simplify the ﬁltering operation in (8): 1) use K = 1; 2) set
λmax = 2; and 3) set θ1 = −θ0. The resulted GCN arrives at
(3). Essentially, the graph convolutional operations deﬁned
in (3) and (5) behave like a low-pass ﬁlter by smoothing the
features of each node on the graph using node features in
its neighborhood.
REGULARIZED GRAPH NEURAL NETWORK
In this section, we present our regularized graph neural
network (RGNN), speciﬁcally, the biologically inspired adjacency matrix, the dynamics of RGNN, and two regularizers,
i.e., node-wise domain adversarial training (NodeDAT) and
emotion-aware distribution learning (EmotionDL).
Adjacency Matrix in RGNN
The adjacency matrix A ∈Rn×n in RGNN represents the
topological structure of EEG channels and is essential to
graph representation learning, where n denotes the number
of channels in EEG signals. Each entry Aij is learnable and
indicates the weight of connection between channels i and
j. Note that A contains self-loops. To reduce overﬁtting,
we model A as a symmetric matrix by using only n(n+1)
number of parameters instead of n2. Salvador et al. observed that the strength of connection between brain regions
decays as an inverse square function of physical distance.
Hence, we initialize the local inter-channel relations in our
adjacency matrix as follows
Aij = min(1, δ
where dij, i, j = 1, 2, ..., n, denotes the physical distance
between channels i and j, which is computed from their 3D
coordinates obtained from the data sheet of the recording
device, and δ > 0 denotes a calibration constant. Achard
and Bullmore observed that sparse fMRI networks,
comprising around 20% of all possible connections, typically
maximize the efﬁciency of the network topology. Therefore,
we choose δ = 5 such that around 20% of the entries in
A are non-negligible. We empirically regard entries having
values larger than 0.1 as non-negligible connections.
Bullmore and Sporns suggested that the brain organization is shaped by an economic trade-off between minimizing wiring costs and network running costs. Minimizing
wiring costs encourages local inter-channel connections as
modelled in (9). However, minimizing network running
costs encourages certain global inter-channel connections
for high efﬁciency of information transfer across the network as a whole. To this end, we add several global connections to our adjacency matrix to improve the network
efﬁciency. The global connections depend on speciﬁc electrode placement adopted in experiments. Fig. 2 depicts the
global connections in both SEED and SEED-IV . The
selection of global channels is supported by prior studies
showing that the asymmetry in neuronal activities between
the left and right hemispheres is informative in valence and
arousal predictions . To leverage the differential asymmetry information, we initialize the global inter-channel
relations in A to [−1, 0] as follows
Aij = Aij −1,
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
Training Samples
L-step Feature Propagation
Testing Samples
L-step Feature Propagation
Domain Classifier
Probability
Distribution
Domain Loss
FC & Softmax
Probability
Fig. 1: The overall architecture of our RGNN model. FC denotes fully-connected layer. CE denotes cross-entropy loss. KL
denotes Kullback-Leibler divergence . GRL denotes gradient reversal layer .
Fig. 2: The 62-channel EEG placement used to collect data in
SEED and SEED-IV. Gray symmetric channels are connected
globally via red dashed lines.
where (i, j) denotes the indices of global channel pairs:
(FP1, FP2), (AF3, AF4), (F5, F6), (FC5, FC6), (C5, C6), (CP5,
CP6), (P5, P6), (PO5, PO6) and (O1, O2). Note that we
select these indices because 1) they are connected to a
large number of nodes in their immediate neighborhood,
which maximizes the effects of EEG asymmetry; and 2) they
empirically perform slightly better than alternative sets of
indices (see Section 7.1). Our adjacency matrix A obtained
in (9) and (10) aims to represent the brain network which
combines both local anatomical connectivity and emotionrelated global functional connectivity.
Dynamics of RGNN
Our RGNN model extends the SGC model . The architecture of RGNN is illustrated in Fig. 1. Given EEG features
X ∈RN×n×d and labels Y ∈ZN, where N denotes the
number of training samples, Yi ∈{0, 1, ..., C −1} denotes
the class index, and C denotes the number of classes. Our
model aims to minimize the following cross-entropy loss:
log(p(Yi|Xi, θ)) + α||A||1,
where θ denotes the model parameters we want to optimize,
and α denotes the strength of L1 sparse regularization for
our adjacency matrix A.
By passing each feature matrix Xi into our RGNN, the
output probability of class Yi can be computed as follows
Zi = SLXiW,
p(Yi|Xi, θ) = softmaxYi(pool(σ(Zi))WO),
where S ∈Rn×n, W ∈Rd×d
and L follow the deﬁnitions
in (5), σ(x) = max(0, x) denotes a non-linear transformation, WO ∈Rd
′×C denotes the output weight matrix, and
pool(·) denotes the sum pooling across all nodes on the
graph. We choose sum pooling because it demonstrated
more expressive power than mean pooling and max pooling
 . Note that we use the absolute values of A to compute
the degree matrix D (see (3)) because A has negative entries,
e.g., global connections.
Node-wise Domain Adversarial Training
EEG signals vary signiﬁcantly across different subjects,
which hinders the generalizability of trained classiﬁers in
subject-independent classiﬁcation settings. To improve the
robustness of our model across subjects, we extend the
domain adversarial training by proposing a node-wise
domain adversarial training (NodeDAT) method to reduce
the discrepancies between source and target domains, i.e.,
training and testing sets, respectively. Speciﬁcally, a domain
classiﬁer is proposed to classify each node representation
into either source domain or target domain. During optimization, our model aims to confuse the domain classiﬁer
by learning domain-invariant representations. Compared
to , which only regularizes the pooled representation
in the last layer, our NodeDAT method has ﬁner-grained
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
regularization because it explicitly regularizes each node
representation before pooling.
Speciﬁcally, given labelled source/training data XS ∈
RN×n×d (in this subsection, we denote X by XS for better
clarity) and unlabelled target/testing data XT ∈RN×n×d,
where in practice XT can be either oversampled or donwsampled to have the same number of samples as XS
 , the domain classiﬁer aims to minimize the sum of the
following two binary cross-entropy losses:
(log(pj(0|XS
i , θD)) + log(pj(1|XT
i , θD))),
where θD denotes the parameters of the domain classiﬁer,
0 and 1 denote source and target domains, respectively.
Intuitively, the domain classiﬁer is learned to classify source
data as 0 and target data as 1. The domain probabilities pj(·)
for the jth node on the ith example are computed as
i , θD) = softmax0(σ(ZS
i , θD) = softmax1(σ(ZT
where Z{S,T }
denotes the jth node representation in Z{S,T }
and WD ∈Rd
′×2 denotes the matrix parameter in the
domain classiﬁer, i.e., θD.
In order to confuse the domain classiﬁer and learn domain invariant node presentation Z{S,T }
, we implement a
gradient reversal layer (GRL) that acts like an identity
layer in the forward propagation and reverses the gradients
of the domain classiﬁer during backpropagation. Consequently, the parameters in the feature extractor essentially
perform gradient ascent with respect to the gradients from
the domain classiﬁer. The reversed gradients are further
scaled by a GRL scaling factor β which gradually increases
from 0 to 1 as the training progresses. The gradually increasing β allows our domain classiﬁer to be less sensitive
to noisy inputs at the early stages of the training process.
Speciﬁcally, as suggested in , we let β =
1+e−10p −1,
where p ∈ denotes the progression of training.
Emotion-aware Distribution Learning
Participants may not always generate the intended emotions
when watching emotion-eliciting stimuli, which may have
negative impact on model performance . To this end,
we propose an emotion-aware distribution learning (EmotionDL) method to learn a distribution of classes instead
of one single class for each training sample. Speciﬁcally,
we convert each training label Yi ∈{0, 1, ..., C −1} into
a prior probability distribution of all classes ˆYi ∈RC,
where ˆYic denotes the probability of class c in ˆYi. The
conversion is dataset-dependent. SEED has three classes:
negative, neutral, and positive with corresponding class
indices 0, 1, and 2, respectively. We convert Y as follows
where ϵ ∈ denotes a hyper-parameter controlling the
noise level in the training labels. This conversion mechanism
is based on our assumption that participants are unlikely
to generate opposite emotions when watching emotioneliciting stimuli. Therefore, for each class, the converted
class distribution centers on the original class and has zero
probabilities at its opposite classes.
SEED-IV has four classes: neutral, sad, fear, and happy
with corresponding class indices 0, 1, 2, and 3, respectively.
We convert Y as follows
This conversion is based on the distances between the four
emotion classes on the valence-arousal plane. Speciﬁcally, in
the self-reported ratings for SEED-IV, neutral, sad, fear,
and happy movie ratings cluster in the zero valence zero
arousal, low valence low arousal, low valence high arousal,
and high valence high arousal regions, respectively. We
assume that participants are likely to generate emotions that
have similar ratings in either valence or arousal dimensions,
e.g., both angry and happy have high arousal, but unlikely
to generate emotions that are far away in both dimensions,
e.g., sad and happy are different in both valence and arousal
dimensions.
After obtaining the converted class distributions ˆY,
our model can be optimized by minimizing the following
Kullback-Leibler (KL) divergence instead of (11):
KL(p(Y|Xi, θ), ˆYi) + α||A||1,
where p(Y|Xi, θ) denotes the output probability distribution computed via (12). Note that EmotionDL incorporates
more prior knowledge than label smoothing, which simply
adds uniform noise to other classes.
Optimization of RGNN
Combining both NodeDAT and EmotionDL, the overall loss
function Φ
′′ of RGNN is computed as follows
The detailed algorithm for training RGNN is presented in
Algorithm 1.
EXPERIMENTAL SETTINGS
In this section, we present the datasets, classiﬁcation settings
and model settings in our experiments.
We conduct experiments on two public datasets, namely
SEED and SEED-IV. The SEED dataset comprises EEG
data of 15 subjects (7 males) recorded in 62 channels using
the ESI NeuroScan System1. The data were collected when
participants watch emotion-eliciting movies in three types
of emotions, namely negative, neutral and positive. Each
movie lasts around 4 minutes. Three sessions of data are
collected and each session comprises 15 trials/movies for
1. 
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
Algorithm 1 The Training Algorithm of RGNN
Input: Training samples X and ˆY, unlabelled testing samples XT , learning rate η, number of epochs T, batch size
B, other regularization hyper-parameters;
Output: The learned model parameters in RGNN;
1: Randomly initialize model parameters in RGNN using
Xavier initialization ;
2: Initialize adjacency matrix A based on (9) and (10);
3: for i = 1: T do
Draw one batch of training samples XB and ˆYB
from X and ˆY, respectively;
Draw one batch of testing samples XT
B from XT ;
Compute degree matrix D based on (3);
Compute normalized adjacency matrix S based
Compute output representation Z based on (12);
Use XB and ˆYB to compute KL loss Φ
′ based on
Use XB and XT
B to compute domain loss ΦD
based on (13);
Compute GRL scaling factor β;
Update WD ←WD −η ∂ΦD
Update WO ←WO −η ∂Φ
Update W ←W −η( ∂Φ
Update A ←A −η( ∂Φ
until all samples in X have been drawn;
each subject. To make a fair comparison with existing studies, we directly use the pre-computed differential entropy
(DE) features smoothed by linear dynamic systems (LDS)
 in SEED. DE extends the idea of Shannon entropy and
measures the complexity of a continuous random variable.
In SEED, DE features are pre-computed over ﬁve frequency
bands (delta, theta, alpha, beta and gamma) for each second
of EEG signals (without overlapping) in each channel.
The SEED-IV dataset comprises EEG data of 15
subjects (7 males) recorded in 62 channels2. The recording
device is the same as the one used in SEED. The data
were collected when participants watch emotion-eliciting
movies in four types of emotions, namely neutral, sad,
fear, and happy. Each movie lasts around 2 minutes. Three
sessions of data are collected and each session comprises 24
trials/movies for each subject. Similar to SEED, we adopt
the pre-computed DE features from SEED-IV.
Classiﬁcation Settings
We closely follow prior studies to conduct both subjectdependent and subject-independent classiﬁcations on both
SEED and SEED-IV to evaluate our model.
Subject-Dependent Classiﬁcation
For SEED, we follow the experimental settings in , ,
 to evaluate our RGNN model using subject-dependent
classiﬁcation. Speciﬁcally, for each subject, we train our
2. SEED-IV also contains eye movement data, which we do not use
in our experiments.
model using the ﬁrst 9 trials as the training set and the
remaining 6 trials as the testing set. We evaluate the model
performance by using the accuracy averaged across all
subjects over two sessions of EEG data . Similarly, for
subject-dependent classiﬁcation on SEED-IV, we follow the
experimental settings in , to use the ﬁrst 16 trials for
training and the remaining 8 trials containing all emotions
(two trials per emotion class) for testing. We evaluate our
model using data from all three sessions .
Subject-Independent Classiﬁcation
For SEED, we follow the experimental settings in ,
 , to evaluate our RGNN model using subjectindependent classiﬁcation. Speciﬁcally, we adopt leave-onesubject-out cross-validation, i.e, during each fold, we train
our model on 14 subjects and test on the remaining subject.
We evaluate the model performance using the accuracy
averaged cross all test subjects over one session of EEG
data . Similarly, for SEED-IV, we follow the experimental
settings in to evaluate our RGNN model using subjectindependent classiﬁcation. We evaluate our model using
data from all three sessions .
Model Settings in RGNN
For hyper-parameters of RGNN in all experiments, we
empirically set the number of convolutional layers L = 2,
dropout rate of 0.7 at the output fully-connected layer
 , and batch size of 16. We use Adam to optimize
model parameters using gradient descent. We only tune the
output feature dimension d
′, label noise level ϵ, learning
rate η, L1 regularization factor α, and L2 regularization for
each experiment. Note that we only adopt NodeDAT in
subject-independent classiﬁcation experiments. Our model
is publicly available3. We compare our model with several
baselines, which are all cited from published results ,
 , , .
PERFORMANCE EVALUATIONS
In this section, we present model evaluation results and
investigate the critical frequency bands and confusion matrices of our RGNN model.
Subject-Dependent Classiﬁcation
Table 1 presents the subject-dependent classiﬁcation accuracy of our RGNN model and all baselines on both SEED
and SEED-IV. The performance on SEED in the individual
delta, theta, alpha, beta, and gamma bands is reported as
well. It is encouraging to see that our model achieves better
performance than all baselines including the state-of-the-art
BiHDM on both datasets when features from all frequency
bands are used. In particular, our model performs better
than DGCNN, another GNN-based model that leverages the
topological structure of EEG channels. Besides the proposed
two regularizers (see Table 3), the main performance improvement can be attributed to two factors: 1) our adjacency
matrix incorporates the emotion-discriminative global interchannel asymmetry relation between the left and right hemispheres; and 2) our model has less concern of overﬁtting by
3. 
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
TABLE 1: Subject-dependent classiﬁcation accuracy (mean/std) on SEED and SEED-IV
delta band
theta band
alpha band
gamma band
60.50/14.14
60.95/10.20
66.64/14.41
80.76/11.56
79.56/11.38
83.99/09.92
56.61/20.05
GSCCA 
63.92/11.16
64.64/10.33
70.10/14.76
76.93/11.00
77.98/10.72
82.96/09.95
69.08/16.66
64.32/12.45
60.77/10.42
64.01/15.97
78.92/12.48
79.19/14.58
86.08/08.34
66.77/07.38
STRNN 
80.90/12.27
83.35/09.15
82.69/12.99
83.41/10.16
69.61/15.65
89.50/07.63
DGCNN 
74.25/11.42
71.52/05.99
74.43/12.16
83.65/10.17
85.73/10.64
90.40/08.49
69.88/16.29
BiDANN 
76.97/10.95
75.56/07.88
81.03/11.74
89.65/09.59
88.64/09.46
92.38/07.04
70.29/12.63
EmotionMeter 
70.58/17.01
BiHDM (SOTA)
93.12/06.06
74.35/14.09
RGNN (Our model)
76.17/07.91
72.26/07.25
75.33/08.85
84.25/12.54
89.23/08.90
94.24/05.95
79.37/10.54
TABLE 2: Subject-independent classiﬁcation accuracy (mean/std) on SEED and SEED-IV
delta band
theta band
alpha band
gamma band
43.06/08.27
40.07/06.50
43.97/10.89
48.63/10.29
51.59/11.83
56.73/16.29
37.99/12.52
44.10/08.22
41.26/09.21
42.93/14.33
43.93/10.06
48.43/09.73
63.64/14.88
56.56/13.77
53.23/07.47
50.60/08.31
55.06/10.60
56.72/10.78
64.47/14.96
69.00/10.89
64.44/09.46
T-SVM 
72.53/14.00
DGCNN 
49.79/10.94
46.36/12.06
48.29/12.28
56.15/14.01
54.87/17.53
79.95/09.02
52.82/09.23
83.81/08.56
58.87/08.13
BiDANN-S 
63.01/07.49
63.22/07.52
63.50/09.50
73.59/09.12
73.72/08.67
84.14/06.87
65.59/10.39
BiHDM (SOTA)
85.40/07.53
69.03/08.66
RGNN (Our model)
64.88/06.87
60.69/05.79
60.84/07.57
74.96/08.94
77.50/08.10
85.30/06.72
73.84/08.02
extending SGC, which is much simpler than ChebNet 
used in DGCNN.
Subject-Independent Classiﬁcation
Similar to Table 1, Table 2 presents the subject-independent
classiﬁcation results. When using features from all frequency bands, our model performs marginally worse than
BiHDM on SEED but much better than BiHDM on SEED-IV
(nearly 5% improvement). In addition, our model achieves
the lowest standard deviation in accuracy compared to all
baselines on both datasets, showing the robustness of our
model against cross-subject variations.
Comparing the results shown in Tables 1 and 2, we
ﬁnd that the accuracy obtained in subject-independent settings is consistently worse than the accuracy obtained in
subject-dependent settings by around 5% to 30% for every
model. This ﬁnding is unsurprising because the variability
of EEG signals across subjects makes subject-independent
classiﬁcation more challenging. However, an interesting
observation is that the performance gap between these
two settings is gradually decreasing from around 27% on
SEED and 19% on SEED-IV using SVM to around 9% on
SEED and 6% on SEED-IV using our model. One possible
reason for the diminishing performance gap is that recent
deep learning models in subject-independent classiﬁcation
settings are becoming better at leveraging a large amount
of data and learning subject-invariant EEG representations.
This observation seems to indicate that transfer learning
may be a necessary tool for emotion recognition in crosssubject settings.
Performance Comparison of Frequency Bands
We further compare the performance of our model and all
baselines on SEED using features from different frequency
bands, as reported in Tables 1 and 2. In subject-dependent
experiments, STRNN achieves the highest accuracy in delta,
theta and alpha bands, BiDANN performs best in beta band,
and our model performs best in gamma band. In subjectindependent experiments, BiDANN-S achieves the highest
accuracy in theta and alpha bands, and our model performs
best in delta, beta and gamma bands.
We investigate the critical frequency bands for emotion recognition. For both subject-dependent and subjectindependent settings on SEED, we compare the performance of each model across different frequency bands. In
general, most models including ours achieve better performance on beta and gamma bands than delta, theta and
alpha bands, with one exception of STRNN, which performs
the worst on gamma band. This observation is consistent
with the literature , . One subtle difference between
our model and other models is that our model performs
consistently better in gamma band than beta band, whereas
other models perform comparably in both bands, indicating
that gamma band may be the most discriminative band for
our model.
Confusion Matrix
We present the confusion matrices of our model in Fig. 3.
For SEED, our model can recognize positive and neutral
emotions better than negative emotion in both classiﬁcation
settings. Comparing subject-independent classiﬁcation (see
Fig. 3(b)) to subject-dependent classiﬁcation (see Fig. 3(a)),
the performance of our model gets relatively much worse at
detecting negative emotion, indicating that participants are
likely to generate distinct EEG patterns when experiencing
negative emotion.
For SEED-IV, our model performs signiﬁcantly better on
sad emotion than all other emotions in both classiﬁcation
settings. Comparing subject-independent classiﬁcation (see
Fig. 3(d)) to subject-dependent classiﬁcation (see Fig. 3(c)),
the performance of our model gets relatively much worse
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
Fig. 3: Confusion matrices of RGNN. (a) Subject-dependent
classiﬁcation on SEED. (b) Subject-independent classiﬁcation on SEED. (c) Subject-dependent classiﬁcation on SEED-
IV. (d) Subject-independent classiﬁcation on SEED-IV.
TABLE 3: Ablation study for subject-independent classiﬁcation accuracy (mean/std) on SEED and SEED-IV. Symbol
“−” indicates the following component is removed.
85.30/06.72
73.84/08.02
correlation-based adjacency matrix
84.41/06.94
72.73/08.36
coherence-based adjacency matrix
84.02/07.05
72.26/08.48
random adjacency matrix
83.57/07.34
71.78/08.64
−symmetric adjacency matrix
83.69/07.92
72.02/08.66
−global connection
82.42/08.24
71.13/08.78
global connection alternative 1
84.52/06.87
73.29/08.18
global connection alternative 2
84.23/07.04
73.08/08.35
81.92/09.35
71.65/09.43
83.51/08.11
72.40/08.54
−EmotionDL
82.27/08.81
70.76/09.22
at detecting sad emotion, which is similar to SEED. We note
that fear is the only emotion that performs better in subjectindependent classiﬁcation than in subject-dependent classiﬁcation. This ﬁnding indicates that participants watching
horror movies may generate similar EEG patterns.
DISCUSSION
In this section, we conduct ablation study and sensitivity
analysis for our RGNN model. We also analyze important
brain regions and inter-channel relations for emotion recognition.
Ablation Study
We conduct ablation study to investigate the contribution
of each key component in our model. Table 3 reports the
subject-independent classiﬁcation results on both datasets.
We compared different initialization methods of the adjacency matrix and found that our distance-based method
(see (9)) obtains slightly better performance than functional
connectivity-based methods, i.e., correlation and coherence
computed from the training dataset. The uniformly randomly initialized adjacency matrix in performs worst,
indicating that properly initializing the adjacency matrix is
beneﬁcial to model performance. Our symmetric adjacency
L1 sparsity coefficient
SEED subject-dependent
SEED subject-independent
SEED-IV subject-dependent
SEED-IV subject-independent
Noise coefficient
SEED subject-dependent
SEED subject-independent
SEED-IV subject-dependent
SEED-IV subject-independent
Fig. 4: Classiﬁcation accuracy of RGNN with varying hyperparameters. (a) L1 sparsity coefﬁcient α in (11). (b) Noise
coefﬁcient ϵ in (15) and (16).
matrix design also proves to be useful in reducing overﬁtting and improving accuracy.
Removing the global connection causes noticeable performance drop on both datasets, demonstrating the importance of global connections in modelling the EEG differential
asymmetry. Moreover, we compared the performance of
alternative sets of global connections. Alternative 1 has
global indices that are nearer to the central region, i.e., (FP1,
FP2), (AF3, AF4), (F3, F4), (FC3, FC4), (C3, C4), (CP3, CP4),
(P3, P4), (PO5, PO6) and (O1, O2). Alternative 2 has global
indices that are further from the central region, i.e., (FP1,
FP2), (AF3, AF4), (F7, F8), (FT7, FT8), (T7, T8), (TP7, TP8),
(P7, P8), (PO7, PO8) and (O1, O2). Both alternatives perform
slightly worse than our model but much better than no
global connection, indicating that they are able to model
EEG asymmetry to a certain extent.
Our NodeDAT regularizer has a noticeable positive impact on the performance of our model, suggesting that
domain adaptation is helpful in cross-subject classiﬁcation.
To further investigate the impact of our node-level domain
classiﬁer, we experimented with replacing NodeDAT with
a generic domain classiﬁer DAT . The clear performance gap between DAT and our RGNN model indicates
that NodeDAT can better regularize the model by learning
subject-invariant representation at node level than graph
level. In addition, if NodeDAT is removed, the performance
of our model has a greater variance, validating the importance of our NodeDAT regularizer in improving the
robustness of RGNN against cross-subject variations.
Our EmotionDL regularizer improves the performance
of our model by around 3% in accuracy on both datasets.
This performance gain validates our assumption that participants are not always generating the intended emotions
when watching emotion-eliciting stimuli. In addition, our
EmotionDL regularizer can be easily adopted by other deep
learning based emotion recognition models.
Sensitivity Analysis
We analyze the performance of our model across varying
L1 sparsity coefﬁcient α (see (11)) and noise coefﬁcient ϵ
in EmotionDL (see (15) and (16)), as illustrated in Fig. 4.
For subject-dependent classiﬁcation, increasing α from 0 to
0.1 generally increases the model performance. However,
for subject-independent classiﬁcation, increasing α beyond
a certain threshold, i.e, 0.01 in Fig. 4(a), decreases the model
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
Fig. 5: Activation maps learned from subject-dependent classiﬁcation on SEED-IV. (a) Delta band. (b) Theta band. (c) Alpha
band. (d) Beta band. (e) Gamma band.
Fig. 6: Top 10 connections between channels in the adjacency
matrix A, excluding global connections in (10) for better
clarity. (a) Initialized A according to (9). (b) Learned and averaged A across ﬁve frequency bands in subject-dependent
classiﬁcation on both SEED and SEED-IV.
performance. One possible explanation for the difference
in model behaviors is that there is much less training
data in subject-dependent classiﬁcation, which thus requires
a stronger regularization to reduce overﬁtting, whereas
for subject-independent classiﬁcation where the amount of
training data is less of a concern, adding stronger regularization may introduce bias and hinder the learning efﬁcacy.
As illustrated in Fig. 4(b), our model behaves consistently across different experimental settings with varying
noise coefﬁcient ϵ. Speciﬁcally, by increasing ϵ, the performance of our model ﬁrst increases and then decreases. In
particular, our model usually performs best when ϵ is set
to 0.2, demonstrating the existence of label noises and the
necessity of addressing them on both datasets. Introducing
excessive noise in EmotionDL causes performance drop,
which is expected because excessive noise weakens the true
learning signals.
Analysis of Important Brain Regions and Interchannel Relations
We identify important brain regions for emotion recognition. Fig. 5 shows the heatmaps of the diagonal elements
in our learned adjacency matrix A in subject-dependent
classiﬁcation on SEED-IV for each frequency band. The
values are scaled to the interval for better visualization.
Conceptually, as shown in (4), the diagonal values in A
represents the contribution of each channel in computing
the ﬁnal EEG representation. It is clear from 5 that there
is strong activation on the pre-frontal, parietal and occipital
regions for all frequency bands, indicating that these regions
may be strongly related to the emotion processing in the
brain. Our ﬁnding is consistent with existing studies, which
observed that asymmetrical frontal and parietal EEG activity may reﬂect changes on both valence and arousal , .
The synchronization between frontal and occipital regions
has also been reported to be related to positive and fear
emotions , . In addition, there is strong activation on
the temporal regions for beta and gamma bands, which is
consistent with . The symmetry pattern on the activation
maps of channels also indicates that the asymmetry in EEG
activity between the left and right hemispheres is critical for
emotion recognition.
We identify important inter-channel relations for emotion recognition. Fig. 6 shows the top 10 connections between channels having the largest edge weights in our
adjacency matrix A. Note that all global connections remain among the strongest connections after A is learned,
demonstrating again that global inter-channel relations are
essential for emotion recognition. It is clear from Fig. 6(b)
that the connection between the channel pair (FP1, AF3) is
the strongest, followed by (F6, F8), (FP2, AF4) and (PO8,
CB2), indicating that local inter-channel relations in the
frontal region may be important for emotion recognition.
CONCLUSION
In this paper, we propose a regularized graph neural network for EEG-based emotion recognition. Our model is
inspired by neuroscience theories on human brain organization and captures both local and global inter-channel
relations in EEG signals. In addition, we propose two regularizers, namely NodeDAT and EmotionDL, to improve the
robustness of our model against cross-subject EEG variations and noisy labels, respectively. Extensive experiments
on two public datasets demonstrate the superior performance of our model than several competitive baselines and
the state-of-the-art BiHDM in most experimental settings.
Our model analysis shows that our proposed biologically
inspired adjacency matrix and two regularizers contribute
consistent and signiﬁcant gain to the performance of our
model. Investigations on the brain regions reveal that prefrontal, parietal and occipital regions may be the most
informative regions for emotion recognition. In addition,
global inter-channel relations between the left and right
hemispheres are important, and local inter-channel relations
between (FP1, AF3), (F6, F8) and (FP2, AF4) may also
provide useful information.
In the future, we plan to explore: 1) training a more discriminative domain classiﬁer, e.g., by using more advanced
classiﬁers or applying more sophisticated techniques to
Authorized licensed use limited to: Nanyang Technological University. Downloaded on September 17,2021 at 03:36:40 UTC from IEEE Xplore. Restrictions apply.
1949-3045 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2020.2994159, IEEE
Transactions on Affective Computing
handle imbalanced samples between training and test sets,
to help our model learn more domain-invariant EEG representations; 2) applying our model to EEG signals that have a
smaller number of channels. A simpler version of our model
and more advanced regularizations may be necessary to
avoid over-smoothing on these small graphs. In addition,
data processing techniques that can improve the spatial
resolution of EEG signals, e.g., spatial ﬁltering, may be
worth exploring.
ACKNOWLEDGMENTS
The authors would like to thank anonymous reviewers for
their valuable comments. This research is supported, in
part, by the Alibaba-NTU Singapore Joint Research Institute
(Alibaba-NTU-AIR2019B1), Nanyang Technological University, Singapore.