Tensor Representation in High-Frequency Financial
Data for Price Change Prediction
Dat Thanh Tran∗, Martin Magris†, Juho Kanniainen†, Moncef Gabbouj∗& Alexandros Iosiﬁdis‡
∗Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
†Laboratory of Industrial and Information Management, Tampere University of Technology, Tampere, Finland
‡Department of Engineering, Electrical & Computer Engineering, Aarhus University, Aarhus, Denmark
Email:{dat.tranthanh,martin.magris,juho.kanniainen,moncef.gabbouj}@tut.ﬁ, 
Abstract—Nowadays, with the availability of massive amount
of trade data collected, the dynamics of the ﬁnancial markets pose
both a challenge and an opportunity for high-frequency traders.
In order to take advantage of the rapid, subtle movement of
assets in High-Frequency Trading (HFT), an automatic algorithm
to analyze and detect patterns of price change based on transaction records must be available. The multichannel, time-series
representation of ﬁnancial data naturally suggests tensor-based
learning algorithms. In this work, we investigate the effectiveness
of two multilinear methods for the mid-price prediction problem
against other existing methods. The experiments in a large-scale
dataset which contains more than 4 million limit orders show that
by utilizing tensor representation, multilinear models outperform
vector-based approaches and other competing ones.
I. INTRODUCTION
High-Frequency Trading (HFT) is a form of automated
trading that relies on the rapid, subtle changes of the markets to buy or sell assets. The main characteristic of HFT
is high speed and short-term investment horizon. Different
from long-term investors, high-frequency traders proﬁt from
a low margin of the price changes with large volume within
a relatively short time. This requires the ability to observe
the dynamics of the market to predict prospective changes
and act accordingly. In quantitative analysis, mathematical
models have been employed to simulate certain aspects of the
ﬁnancial market in order to predict asset price, stock trends,
etc. The performance of traditional mathematical models relies
heavily on hand-crafted features. With recent advances in
computational power, more and more machine learning models
have been introduced to predict ﬁnancial market behaviors.
Popular machine learning methods in HFT include regression
analysis , , , , , multilayer feed forward network
 , , , convolutional neural network , recurrent neural
network , , .
With a large volume of data and the erratic behaviors of
the market, neural network-based solutions have been widely
adopted to learn both the suitable representation of the data
and the corresponding classiﬁers. This resolves the limitation in hand-crafted models. All kinds of deep architectures
have been proposed, ranging from traditional multilayer feedforward models , , to Convolutional Neural Network
(CNN) , Recurrent Neural Network (RNN) , , ,
Deep Belief Networks , , . For example, in 
a CNN with both 2D and 1D convolution masks was trained
to predict stock price movements. On a similar benchmark
HFT dataset, an RNN with Long Short-Term Memory Units
(LSTM) and or a Neural Bag-of-Features (N-BoF) 
network generalizing the (discriminant) Bag-of-Feature model
(BoF) were proposed to perform the same prediction task.
Tensor representation offers a natural representation of the
time-series data, where time corresponds to one of the tensor
orders. Therefore, it is intuitive to investigate machine learning
models that utilize tensor representations. In traditional vectorbased models, the features are extracted from the time-series
representation and form an input vector to the model. The
preprocessing step to convert a tensor representation to a
vector representation might lead to the loss of temporal information. That is, the learned classiﬁers might fail to capture the
interactions between spatio-temporal information due to vectorization. Because many neural network-based solutions, such
as CNN or RNN, learn the data directly in the tensor form,
this could explain why many neural network implementations
outperform traditional vector-based models with hand-crafted
features. With advances in mathematical tools and algorithms
dealing with tensor input, many multilinear discriminant techniques, as well as tensor regression models have been proposed
for image and video classiﬁcation problems such as , ,
 , , , , , , , . However, there
are few works investigating the performance of the tensorbased multilinear methods in ﬁnancial problems . Different
from neural network methodology which requires heavy tuning
of network topology and parameters, the beauty of tensorbased multilinear techniques is that the objective function
is straightforward to interpret and very few parameters are
required to tune the model. In this work, we propose to use
two multilinear techniques based on the tensor representation
of time-series ﬁnancial data to predict the mid-price movement
based on information obtained from Limit Order Book (LOB)
data. Speciﬁcally, the contribution of this paper is as follows
• We investigate the effectiveness of tensor-based discriminant techniques, particularly Multilinear Discriminant
Analysis (MDA) in a large-scale prediction problem
of mid-price movement with high-frequency limit order
book data.
• We propose a simple regression classiﬁer that operates on
the tensor representation, utilizing both the current and
past information of the stock limit order book to boost
the performance of the vector-based regression technique.
Based on the observation of the learning dynamics of the
proposed algorithm, efﬁcient scheme to select the best
model’s state is also discussed.
The rest of the paper is organized as follows. Section 2
reviews the mid-price movement prediction problem given the
information collected from LOB as well as related methods
that were proposed to tackle this problem. In Section 3, MDA
and our proposed tensor regression scheme are presented.
Section 4 shows the experimental analysis of the proposed
methods compared with existing results on a large-scale
dataset. Finally, conclusions are drawn in Section 5.
II. HIGH-FREQUENCY LIMIT ORDER DATA
In ﬁnance, a limit order placed with a bank or a brokerage
is a type of trade order to buy or sell a set amount of assets
with a speciﬁed price. There are two types of limit order: a
buy limit order and a sell limit order. In a sell limit order
(ask), a minimum sell price and the corresponding volume of
assets are speciﬁed. For example, a sell limit order of 1000
shares with a minimum prize of $20 per share indicates that
the investors wish to sell the share with maximum prize of
$20 only. Similarly, in a buy limit order (bid), a maximum
buy price and its respective volume must be speciﬁed. The
two types of limit orders consequently form two sides of the
LOB, the bid and the ask side. LOB aggregates and sorts the
order from both sides based on the given price. The best bid
price p(1)
b (t) at the time instance t is deﬁned as the highest
available price that the buyer is willing to pay per share. The
best ask price p(1)
a (t) is, in turn, the lowest available price
at a time instance t that a seller is willing to sell per share.
The LOB is sorted so that best bid and ask price is on top of
the book. The trading happens through a matching mechanism
based on several conditions. When the best bid price exceeds
the best ask price, i.e. p(1)
b (t) > p(1)
a (t), the trading happens
between the two investors. In addition to executions, the order
can disappear from the order book by cancellations.
Given the availability of LOB data, several problems can
be formulated, such as price trend prediction, order ﬂow
distribution estimation or detection of anomalous events that
cause turbulence in the price change. One of the popular tasks
given the availability of LOB data is to predict the mid-price
movements, i.e. to classify whether the mid-price increases,
decreases or remains stable based on a set of measurements.
The mid-price is a quantity deﬁned as the mean between the
best bid price and the best ask price at a given time, i.e.
a (t) + p(1)
which gives a good estimate of the price trend.
The LOB dataset used in this paper, referred as FI-
2010, was collected from 5 different Finnish stocks (Kesko,
Outokumpu, Sampo, Rautaruukki and Wartsila) in 5 different
industrial sectors. The collection period is from 1st of June
to 14th of June 2010, producing order data of 10 working
days. The provided data was extracted based on event inﬂow
 which aggregates to approximately 4.5 million events.
Each event contains information from the top 10 orders
from each side of the LOB. Since each order consists of a
price (bid or ask) and a corresponding volume, each order
event is represented by a 40-dimensional vector. In , a
144-dimensional feature vector was extracted for every 10
events, leading to 453, 975 feature vector samples. For each
feature vector, FI-2010 includes an associated label which
indicates the movement of mid-price (increasing, decreasing,
stationary) in the next 10 order events. In order to avoid the
effect of different scales from each dimension, the data was
standardized using z-score normalization
xnorm = x −¯x
Given the large-scale of FI-2010, many neural network solutions have been proposed to predict the prospective movement
of the mid-price. In , a CNN that operates on the raw data
was proposed. The network consists of 8 layers with an input
layer of size 100 × 40, which contains 40-dimensional vector
representation of 100 consecutive events. The hidden layers
contain both 2D and 1D convolution layers as well as max
pooling layer. In , an RNN architecture with LSTM units
that also operates on a similar raw data representation was
proposed with separate normalization schemes for order prices
and volumes. Beside conventional deep architecture, an N-
BoF classiﬁer was proposed for the problem of the midprice prediction. The N-BoF network in was trained on
15 consecutive 144-dimensional feature vectors which contain
order information from 150 most recent order events and
predicted the movements in the next k = {10, 50, 100} order
It should be noted that all of the above mentioned neural
network solutions utilized not only information from the
current order events but also information from the recent past.
We believe that the information of the recent order events plays
a signiﬁcant role in modeling the dynamics of the mid-price.
The next section presents MDA classiﬁer and our proposed
regression model that take into account the contribution of
past order information.
III. TENSOR-BASED MULTILINEAR METHODS FOR
FINANCIAL DATA
Before introducing the classiﬁers to tackle mid-price prediction problem, we will start with notations and concepts used
in multilinear algebra.
A. Multilinear Algebra Concepts
In this paper, we denote scalar values by either low-case or
upper-case characters (x, y, X, Y . . . ), vectors by lower-case
bold-face characters (x, y, . . . ), matrices by upper-case boldface characters (A, B, . . . ) and tensor as calligraphic capitals
(X, Y, . . . ). A tensor with K modes and dimension Ik in the
mode-k is represented as X ∈RI1×I2×···×IK. The entry in
the ikth index in mode-k for k = 1, . . . , K is denoted as
Xi1,i2,...,iK.
Deﬁnition 1 (Mode-k Fiber and Mode-k Unfolding): The
mode-k ﬁber of a tensor X ∈RI1×I2×···×IK is a vector
of Ik-dimensional, given by ﬁxing every index but ik. The
mode-k unfolding of X, also known as mode-k matricization,
transforms the tensor X to matrix X(k), which is formed by
arranging the mode-k ﬁbers as columns. The shape of X(k) is
RIk×I¯k with I¯k = QK
i=1,i̸=k Ii.
Deﬁnition 2 (Mode-k Product): The mode-k product between a tensor X
= [xi1, . . . , xiK] ∈RI1×...IK and a
matrix W ∈RJk×Ik is another tensor of size I1 × · · · ×
Jk × · · · × IK and denoted by X ×k W. The element of
X ×k W is deﬁned as [X ×k W]i1,...,ik−1,jk,ik+1,...,iK =
ik=1[X]i1,...,ik−1,ik,...,iK[W]jk,ik.
With the deﬁnition of mode-k product and mode-k unfolding, the following equation holds
(X ×k WT )(k) = WT X(k)
For convenience, we denote X ×1 W1 × · · · ×K WK by
B. Multilinear Discriminant Analysis
MDA is the extended version of the Linear Discriminant
Analysis (LDA) which utilizes the Fisher criterion as the
optimal criterion of the learned subspace. Instead of seeking
an optimal vector subspace, MDA learns a tensor subspace
in which data from different classes are separated by maximizing the interclass distances and minimizing the intraclass
distances. The objective function is thus maximizing the ratio
between interclass distances and intraclass distances in the
projected space. Formally, let us denote the set of N tensor
samples as X1, . . . , XN ∈RI1×···×IK, i = 1, . . . , N, each
with an associated class label ci, i = 1, . . . , C. In addition,
Xi,j denotes the jth sample from class ci and ni denotes the
number of samples in class ci. The mean tensor of class ci is
calculated as Mi =
j=1 Xi,j and the total mean tensor
j=1 Xi,j = 1
projection
k < Ik, k = 1, . . . , K that map Xi,j to Yi,j ∈
K, with the subspace projection deﬁned as
Yi,j = Xi,j
The set of optimal projection matrices are obtained by maximizing the ratio between interclass and intraclass distances,
measured in the tensor subspace RI
K. Particularly,
MDA maximizes the following criterion
J(W1, . . . , WK) = Db
are respectively interclass distance and intraclass distance.
The subscript F in (6) and (7) denotes the Frobenius norm.
Db measures the total square distances between each class
mean Mi and the global mean M after the projection while
Dw measures the total square distances between each sample
and its respective mean tensor. By maximizing (5), we are
seeking a tensor subspace in which the dispersion of data in
the same class is minimum while the dispersion between each
class is maximum. Subsequently, the classiﬁcation can then
be performed by simply selecting the class with a minimum
distance between a test sample to each class mean in the
discriminant subspace. Since the projection in (4) exposes
a dependancy between each mode-k, each Wk cannot be
optimized independently. An iterative approach is usually
employed to solve the optimization in (5) [ , , .
In this work, we propose to use the CMDA algorithm 
that assumes orthogonal constraints on each projection matrix
k Wk = I, k = 1, . . . , K and solves (5) by iteratively
solving a trace ratio problem for each mode-k. Speciﬁcally,
Db and Dw can be calculated by unfolding the tensors in
mode-k as follows
h Xi,j −Mi
h Xi,j −Mi
where tr() in (8) and (9) denotes the trace operator. By
utilizing the identity in (3), Db and Dw are further expressed
(Xi,j −Mi)
(Xi,j −Mi)
w in (10) and (11) denote the interclass and
intraclass scatter matrices in mode-k. The criterion in (5) can
then be converted to a trace ratio problem with respect to Wk
while keeping other projection matrices ﬁxed as
With the orthogonality constraint of Wk, the solution of
(12) is given by I
k eigenvectors corresponding to I
eigenvalues of (Sk
w)−1Sb. Usually, a positive λ is added to
the diagonal of Sk
w as a regularization, which also enables
stable computation in case Sk
w is not a full rank matrix. In
the training phase, after randomly initializes Wk, CMDA
algorithm iteratively goes through each mode k, optimizes the
Fisher ratio with respect to Wk while keeping other projection
matrices ﬁxed. The algorithm terminates when the changes in
Wk below a threshold or the speciﬁed maximum iteration
reached. In the test phase, the class with a minimum distance
between the class mean and the test sample in the tensor
subspace is assigned to the test sample.
C. Weighted Multichannel Time-series Regression
For the FI-2010 dataset, in order to take into account the past
information one could concatenate T 144-dimensional feature
vectors corresponding to the 10T most recent order events to
form a 2-mode tensor sample, i.e. a matrix Xi ∈R144×T , i =
1, . . . , N. For example, a training tensor sample of size
144×10 contains information of 100 most recent order events
in the FI-2010 dataset. 10 columns represent information at
10 time-instances with the 10th column contains the latest
order information. Each of the 144 rows encode the temporal
evolution of the 144 features (or channels) through time. Generally, given N 2-mode tensor Xi ∈RD×T , i = 1, . . . , N that
belong to C classes indicated by the class label ci = 1, . . . , C,
the proposed Weighted Multichannel Time-series Regression
(WMTR) learns the following mapping function
where W1 ∈RD×C and w2 ∈RT are learnable parameters.
The function f in (13) maps each input tensor to a Cdimensional (target) vector. One way to interpret f is that W1
maps D-dimensional representation of each time-instance to a
C-dimensional (sub)space while w2 combines the contribution
of each time-instance into a single vector, by using a weighted
average approach. In order to deal with unbalanced datasets,
such as FI-2010, the parameters W1, w2 of the WMTR model
are determined by minimizing the following weighted least
square criterion
1 Xiw2 −yi∥2
F + λ2∥w2∥2
where yi ∈RC is the corresponding target of the ith sample
with all elements equal to −1 except the cith element, which
is set equal to 1. λ1 and λ2 are predeﬁned regularization
parameters associated with W1 and w2. We set the value of
the predeﬁned weight si equal to 1/ rp
Nci, r > 0, i.e. inversely
proportional to the number of training samples belonging
to the class of sample i, so that errors in smaller classes
contribute more to the loss. The weight of each class is
controlled by parameter r: the smaller r, the more contribution
of the minor classes in the loss. The unweighted least square
criterion is a special case of (14) when r →+∞, i.e.
si = 1, ∀i.
We solve (14) by applying an iterative optimization process
that alternatively keeps one parameter ﬁxed while optimizing
the other. Speciﬁcally, by ﬁxing w2 we have the following
minimization problem
F + λ1∥W1∥2
X1w2, . . . , XNw2
[y1, . . . , yN] ∈RC×N and S2 ∈RN×N is a diagonal matrix
with the S2i,i = √si, i = 1, . . . , N. By solving
∂W1 = 0, we
obtain the solution of (15) as
where I is the identity matrix of the appropriate size.
Similarly, by ﬁxing W1, we have the following regression
problem with respect to w2
J1(w2) = ∥S1
F + λ2∥w2∥2
where X1 =
1 W1, . . . , X T
T ∈RCN×T , Y(1) =
1 , . . . , yT
N]T ∈RCN and S1 ∈RCN×CN is a diagonal
matrix with S1C(i−1)+k,C(i−1)+k = √si; k = 1, . . . , C; i =
1, . . . , N. Similar to W1, optimal w2 is obtained by solving
for the stationary point of (17), which is given as
1 S1X1 + λ2I
The above process is formed by two convex problems,
for which each processing step obtains the global optimum
solution. Thus, the overall process is guaranteed to reach
a local optimum for the combined regression criterion. The
algorithm terminates when the changes in W1 and w2 are
below a threshold, or the maximum number of iterations is
reached. In the test phase, f in (13) maps a test sample to the
feature space, and the class label is inferred by the index of
the maximum element of the projected test sample.
Usually, multilinear methods (including multilinear regression ones) are randomly initialized. This means that, in our
case, one would randomly initialize the parameters in w2
Fig. 1. Performance measure of WMTR on training data
in order to deﬁne the optimal regression values stored in
W1 on the ﬁrst iteration. However, since for WMTR when
applied to LOB data, the values of w2 encode the contribution
of each time-instance in the overall regression, we chose to
initialize it as w2 = [0 0 . . . 1]T. That is, the ﬁrst iteration
of WMTR corresponds to the vector-based regression using
only the representation for the current time-instance. After
obtaining this mapping, the optimal weighted average of all
time-instances is determined by solving for w2.
IV. EXPERIMENTS
A. Experiment Setting
We conducted extensive experiments on the FI-2010 dataset
to compare the performance of the multilinear methods, i.e.
MDA and the proposed WMTR, with that of the other existing
methods including LDA, Ridge Regression (RR), Singlehidden Layer Feed Forward Network (SLFN), BoF and N-BoF.
In addition, we also compared WMTR with its unweighted
version, denoted by MTR, to illustrate the effect of weighting
in the learned function. Regarding the train/test evaluation
protocol, we followed the anchored forward cross-validation
splits provided by the database . Speciﬁcally, there are 9
folds for cross-validation based on the day basis; the training
set increases by one day for each consecutive fold and the day
following the last day used for training is used for testing, i.e.
for the ﬁrst fold, data from the ﬁrst day is used for training and
data from the second day is used for testing; for the second
fold, data from the ﬁrst and second day is used as for training
and data from the third day used for testing; for the last fold,
data from the ﬁrst 9 days is used for training and the 10th day
is used for testing.
Regarding the input representation of the proposed multilinear techniques, MDA and WMTR both accept input tensor
of size R144×10, which contains information from 100 consecutive order events with the last column contains information
from the last 10 order events. For LDA, RR and SLFN, each
Fig. 2. Performance measure of MTR on training data
input vector is of size R144, which is the last column of the
input from MDA and WMTR, representing the most current
information of the stock. The label of both tensor input and
vector input is the movement of the mid-price in the next 10
order events, representing the future movement that we would
like to predict. Since we followed the same experimental
protocol as in and , we directly report the result of
RR, SLFN, BoF, N-BoF in this paper.
The parameter settings of each model are as follows. For
WMTR, we set maximum number of iterations to 50, the
terminating threshold to 1e−6; λ1, λ2 ∈{0.01, 0.1, 1, 10, 100}
and si = n−1/r
with r ∈{2, 3, 4}. For MTR, all paramter
settings were similar to WMTR except si = 1, ∀i. For MDA,
the number of maximum iterations and terminating threshold
were set similar to WMTR, the projected dimensions of the
ﬁrst mode is from 5 to 60 with a step of 5 while for the second
mode from 1 to 8 with a step of 1. In addition, a regularization
amount λ ∈{0.01, 0.1, 1, 10, 100} was added to the diagonal
B. Performance Evaluation
It should be noted that FI-2010 is a highly unbalanced
dataset with most samples having a stationary mid-price.
PERFORMANCE ON FI-2010
46.00 ± 2.85
43.30 ± 9.9
43.54 ± 5.2
42.52 ± 1.22
53.22 ± 7.04
49.60 ± 3.81
41.28 ± 4.04
38.24 ± 5.66
63.82 ± 4.98
37.93 ± 6.00
45.80 ± 4.07
36.28 ± 1.02
71.92 ± 5.46
44.21 ± 1.35
60.07 ± 2.10
46.06 ± 2.22
86.08 ± 4.99
51.68 ± 7.54
40.81 ± 6.18
40.14 ± 5.26
81.89 ± 3.65
46.25 ± 1.90
51.29 ± 1.88
47.87 ± 1.91
57.59 ± 7.34
39.26 ± 0.94
51.44 ± 2.53
36.28 ± 2.85
62.70 ± 6.73
42.28 ± 0.87
61.41 ± 3.68
41.63 ± 1.90
Train Accuracy
Test Accuracy
Fig. 3. Performance measure of WMTR on both train and test set
Therefore we use average f1 score per class as a performance measure to select model parameters since f1 expresses
a trade-off between precision and recall. More speciﬁcally, for
each cross-validation fold, the competing methods are trained
with all combinations of the above-mentioned parameter settings on the training data. We selected the learned model
that achieved the highest f1 score on the training set and
reported the performance on the test set. In addition to f1,
the corresponding average precision per class, average recall
per class and accuracy are also reported. Accuracy measures
the percentage the predicted labels that match the ground
truth. Precision is the ratio between true positives over the
number of samples predicted as positive, and recall is the
ratio between true positive over the total number of true
positives and false negatives. f1 is the harmonic mean between
precision and recall. For all measures, higher values indicate
better performance.
Table 1 shows the average performance with standard deviation over all 9 folds of the competing methods. Comparing two
discriminant methods, i.e. LDA and MDA, it is clear that MDA
signiﬁcantly outperforms LDA on all performance measures.
This is due to the fact that MDA operates on the tensor input,
which could hold both current and past information as well as
the temporal structure of the data. The improvement of tensorbased approaches over vector-based approach is consistent also
in case of regression (WMTR vs RR). Comparing multilinear
techniques with N-BoF, MDA and WMTR perform much
better than N-BoF in terms of f1, accuracy and precision
while recall scores nearly match. WMTR outperforming MTR
in large margin suggests that weighting is important for the
highly unbalanced dataset such as FI-2010. Overall, MDA
and WMTR are the leading methods among the competing
methods in this mid-price prediction problem.
C. WMTR analysis
Figure 1 shows the dynamic of the learning process of
WMTR on the training data of the ﬁrst fold. There is one
Train Accuracy
Test Accuracy
Fig. 4. Performance measure of MTR on both train and test set
interesting phenomenon that can be observed during the training process. In the ﬁrst 10 iterations, all performance measures
improve consistently. After the 10th iteration, f1 score drops a
little then remains stable while accuracy continues to improve.
This phenomenon can be observed in every parameter setting.
Since WMTR minimizes the squared error between the target
label and the predicted label, constant improvement before
converging observed from the training accuracy is expected.
The drop in f1 score after some k iterations can be explained
as follows: in the ﬁrst k iterations, WMTR truly learns the
generating process behind the training samples; however, at a
certain point, WMTR starts to overﬁt the data and becomes
bias towards the dominant class. The same phenomenon was
observed from MTR with a more signiﬁcant drop in f1 since
without weight MTR overﬁts the dominant class severely.
Figure 2 shows the training dynamic of MTR with similar
parameter setting except for the class weight in the loss
function. Due to this behavior, in order to select the best
learned state of WMTR and MTR, we measured f1 score
on the training data at each iteration and selected the model’s
state which produced the best f1. The question is whether the
selected model performs well on the test data? Figure 3 and
Figure 4 plots accuracy and f1 of WMTR and MTR measured
on the training set and the test set at each iteration. It is clear
that the learned model that produced best f1 during training
also performed best on the test data. The margin between
training and testing performance is relatively small for both
WMTR and MTR which shows that our proposed algorithm
did not suffer from overﬁtting. Although the behaviors of
WMTR and MTR are similar, the best model learned from
MTR is biased towards the dominant class, resulting in inferior
performance as shown in the experimental result.
V. CONCLUSIONS
In this work, we have investigated the effectiveness of
multilinear discriminant analysis in dealing with ﬁnancial data
prediction based on Limit Order Book data. In addition, we
proposed a simple bilinear regression algorithm that utilizes
both current and past information of a stock to boost the
performance of traditional vector-based regression. Experimental results showed that the proposed methods outperform
their counterpart exploiting vectorial representations, and outperform existing solutions utilizing (possibly deep) neural
network architectures.
VI. ACKNOWLEDGEMENT
This project has received funding from the European
Union's Horizon 2020 research and innovation programme
under the Marie Skodowska-Curie grant agreement No 675044
BigDataFinance.