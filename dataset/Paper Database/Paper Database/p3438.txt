Proceedings of NAACL-HLT 2018, pages 2227–2237
New Orleans, Louisiana, June 1 - 6, 2018. c⃝2018 Association for Computational Linguistics
Deep contextualized word representations
Matthew E. Peters†, Mark Neumann†, Mohit Iyyer†, Matt Gardner†,
{matthewp,markn,mohiti,mattg}@allenai.org
Christopher Clark⇤, Kenton Lee⇤, Luke Zettlemoyer†⇤
{csquared,kentonl,lsz}@cs.washington.edu
†Allen Institute for Artiﬁcial Intelligence
⇤Paul G. Allen School of Computer Science & Engineering, University of Washington
We introduce a new type of deep contextualized word representation that models both (1)
complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses
vary across linguistic contexts (i.e., to model
polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that
these representations can be easily added to
existing models and signiﬁcantly improve the
state of the art across six challenging NLP
problems, including question answering, textual entailment and sentiment analysis.
also present an analysis showing that exposing
the deep internals of the pre-trained network is
crucial, allowing downstream models to mix
different types of semi-supervision signals.
Introduction
Pre-trained word representations are a key component in many neural language understanding models.
However, learning high quality representations can be challenging.
They should ideally
model both (1) complex characteristics of word
use (e.g., syntax and semantics), and (2) how these
uses vary across linguistic contexts (i.e., to model
polysemy). In this paper, we introduce a new type
of deep contextualized word representation that
directly addresses both challenges, can be easily
integrated into existing models, and signiﬁcantly
improves the state of the art in every considered
case across a range of challenging language understanding problems.
Our representations differ from traditional word
type embeddings in that each token is assigned a
representation that is a function of the entire input
sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations.
Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep, in
the sense that they are a function of all of the internal layers of the biLM. More speciﬁcally, we
learn a linear combination of the vectors stacked
above each input word for each end task, which
markedly improves performance over just using
the top LSTM layer.
Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level
LSTM states capture context-dependent aspects
of word meaning (e.g., they can be used without modiﬁcation to perform well on supervised
word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can
be used to do part-of-speech tagging). Simultaneously exposing all of these signals is highly bene-
ﬁcial, allowing the learned models select the types
of semi-supervision that are most useful for each
Extensive experiments demonstrate that ELMo
representations work extremely well in practice.
We ﬁrst show that they can be easily added to
existing models for six diverse and challenging
language understanding problems, including textual entailment, question answering and sentiment
analysis. The addition of ELMo representations
alone signiﬁcantly improves the state of the art
in every case, including up to 20% relative error
reductions. For tasks where direct comparisons
are possible, ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder.
Finally, an analysis of both ELMo and
CoVe reveals that deep representations outperform
those derived from just the top layer of an LSTM.
Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1
Related work
Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors are a standard component of most state-ofthe-art NLP architectures, including for question
answering , textual entailment
 and semantic role labeling
 . However, these approaches for
learning word vectors only allow a single contextindependent representation for each word.
Previously proposed methods overcome some
of the shortcomings of traditional word vectors
by either enriching them with subword information or learning separate vectors for each word
sense . Our approach also beneﬁts from subword units through
the use of character convolutions, and we seamlessly incorporate multi-sense information into
downstream tasks without explicitly training to
predict predeﬁned sense classes.
context-dependent
representations.
context2vec uses a
bidirectional Long Short Term Memory to encode the
context around a pivot word. Other approaches
for learning contextual embeddings include the
pivot word itself in the representation and are
computed with the encoder of either a supervised
neural machine translation (MT) system or an unsupervised language model . Both of these
approaches beneﬁt from large datasets, although
the MT approach is limited by the size of parallel
corpora. In this paper, we take full advantage of
access to plentiful monolingual data, and train
our biLM on a corpus with approximately 30
million sentences . We also
generalize these approaches to deep contextual
representations, which we show work well across
a broad range of diverse NLP tasks.
1 
Previous work has also shown that different layers of deep biRNNs encode different types of information.
For example, introducing multi-task
syntactic supervision (e.g., part-of-speech tags) at
the lower levels of a deep LSTM can improve
overall performance of higher level tasks such as
dependency parsing or
CCG super tagging .
In an RNN-based encoder-decoder machine translation system, Belinkov et al. showed that
the representations learned at the ﬁrst layer in a 2layer LSTM encoder are better at predicting POS
tags then second layer. Finally, the top layer of an
LSTM for encoding word context has been shown to learn representations of
word sense. We show that similar signals are also
induced by the modiﬁed language model objective
of our ELMo representations, and it can be very
beneﬁcial to learn models for downstream tasks
that mix these different types of semi-supervision.
Dai and Le and Ramachandran et al.
 pretrain encoder-decoder pairs using language models and sequence autoencoders and then
ﬁne tune with task speciﬁc supervision. In contrast, after pretraining the biLM with unlabeled
data, we ﬁx the weights and add additional taskspeciﬁc model capacity, allowing us to leverage
large, rich and universal biLM representations for
cases where downstream training data size dictates
a smaller supervised model.
ELMo: Embeddings from Language
Unlike most widely used word embeddings , ELMo word representations
are functions of the entire input sentence, as described in this section. They are computed on top
of two-layer biLMs with character convolutions
(Sec. 3.1), as a linear function of the internal network states (Sec. 3.2). This setup allows us to do
semi-supervised learning, where the biLM is pretrained at a large scale (Sec. 3.4) and easily incorporated into a wide range of existing neural NLP
architectures (Sec. 3.3).
Bidirectional language models
Given a sequence of N tokens, (t1, t2, ..., tN), a
forward language model computes the probability
of the sequence by modeling the probability of to-
ken tk given the history (t1, ..., tk−1):
p(t1, t2, . . . , tN) =
p(tk | t1, t2, . . . , tk−1).
Recent state-of-the-art neural language models
 compute a context-independent token representation xLM
(via token embeddings or
a CNN over characters) then pass it through L layers of forward LSTMs. At each position k, each
LSTM layer outputs a context-dependent representation −!
k,j where j = 1, . . . , L. The top layer
LSTM output, −!
k,L , is used to predict the next
token tk+1 with a Softmax layer.
A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context:
p(t1, t2, . . . , tN) =
p(tk | tk+1, tk+2, . . . , tN).
It can be implemented in an analogous way to a
forward LM, with each backward LSTM layer j
in a L layer deep model producing representations
k,j of tk given (tk+1, . . . , tN).
A biLM combines both a forward and backward
LM. Our formulation jointly maximizes the log
likelihood of the forward and backward directions:
( log p(tk | t1, . . . , tk−1; ⇥x, −!
⇥LSTM, ⇥s)
+ log p(tk | tk+1, . . . , tN; ⇥x, −
⇥LSTM, ⇥s) ) .
We tie the parameters for both the token representation (⇥x) and Softmax layer (⇥s) in the forward
and backward direction while maintaining separate parameters for the LSTMs in each direction.
Overall, this formulation is similar to the approach
of Peters et al. , with the exception that we
share some weights between directions instead of
using completely independent parameters. In the
next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the
biLM layers.
ELMo is a task speciﬁc combination of the intermediate layer representations in the biLM. For
each token tk, a L-layer biLM computes a set of
2L + 1 representations
k,j | j = 1, . . . , L}
k,j | j = 0, . . . , L},
is the token layer and hLM
k,j ], for each biLSTM layer.
For inclusion in a downstream model, ELMo
collapses all layers in R into a single vector,
ELMok = E(Rk; ⇥e).
In the simplest case,
ELMo just selects the top layer, E(Rk) = hLM
as in TagLM and CoVe . More generally, we compute a
task speciﬁc weighting of all biLM layers:
= E(Rk; ⇥task) = γtask
In (1), stask are softmax-normalized weights and
the scalar parameter γtask allows the task model to
scale the entire ELMo vector. γ is of practical importance to aid the optimization process (see supplemental material for details). Considering that
the activations of each biLM layer have a different
distribution, in some cases it also helped to apply
layer normalization to each biLM
layer before weighting.
Using biLMs for supervised NLP tasks
Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process
to use the biLM to improve the task model. We
simply run the biLM and record all of the layer
representations for each word. Then, we let the
end task model learn a linear combination of these
representations, as described below.
First consider the lowest layers of the supervised model without the biLM. Most supervised
NLP models share a common architecture at the
lowest layers, allowing us to add ELMo in a
consistent, uniﬁed manner.
Given a sequence
of tokens (t1, . . . , tN), it is standard to form a
context-independent token representation xk for
each token position using pre-trained word embeddings and optionally character-based representations. Then, the model forms a context-sensitive
representation hk, typically using either bidirectional RNNs, CNNs, or feed forward networks.
To add ELMo to the supervised model, we
ﬁrst freeze the weights of the biLM and then
concatenate the ELMo vector ELMotask
xk and pass the ELMo enhanced representation
[xk; ELMotask
] into the task RNN. For some
tasks (e.g., SNLI, SQuAD), we observe further
improvements by also including ELMo at the output of the task RNN by introducing another set
of output speciﬁc linear weights and replacing hk
with [hk; ELMotask
]. As the remainder of the
supervised model remains unchanged, these additions can happen within the context of more complex neural models. For example, see the SNLI
experiments in Sec. 4 where a bi-attention layer
follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs.
Finally, we found it beneﬁcial to add a moderate amount of dropout to ELMo and in some cases to regularize the ELMo
weights by adding λkwk2
2 to the loss. This imposes an inductive bias on the ELMo weights to
stay close to an average of all biLM layers.
Pre-trained bidirectional language model
architecture
The pre-trained biLMs in this paper are similar to
the architectures in J´ozefowicz et al. and
Kim et al. , but modiﬁed to support joint
training of both directions and add a residual connection between LSTM layers. We focus on large
scale biLMs in this work, as Peters et al. 
highlighted the importance of using biLMs over
forward-only LMs and large scale training.
To balance overall language model perplexity
with model size and computational requirements
for downstream tasks while maintaining a purely
character-based input representation, we halved all
embedding and hidden dimensions from the single
best model CNN-BIG-LSTM in J´ozefowicz et al.
 . The ﬁnal model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections
and a residual connection from the ﬁrst to second
layer. The context insensitive type representation
uses 2048 character n-gram convolutional ﬁlters
followed by two highway layers and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely
character input. In contrast, traditional word embedding methods only provide one layer of representation for tokens in a ﬁxed vocabulary.
After training for 10 epochs on the 1B Word
Benchmark , the average forward and backward perplexities is 39.7, compared
to 30.0 for the forward CNN-BIG-LSTM. Generally, we found the forward and backward perplexities to be approximately equal, with the backward
value slightly lower.
Once pretrained, the biLM can compute representations for any task. In some cases, ﬁne tuning
the biLM on domain speciﬁc data leads to signiﬁcant drops in perplexity and an increase in downstream task performance. This can be seen as a
type of domain transfer for the biLM. As a result,
in most cases we used a ﬁne-tuned biLM in the
downstream task. See supplemental material for
Evaluation
Table 1 shows the performance of ELMo across a
diverse set of six benchmark NLP tasks. In every
task considered, simply adding ELMo establishes
a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base
models. This is a very general result across a diverse set model architectures and language understanding tasks. In the remainder of this section we
provide high-level sketches of the individual task
results; see the supplemental material for full experimental details.
Question answering The Stanford Question
Answering Dataset (SQuAD) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given
Wikipedia paragraph. Our baseline model is an improved version of the
Bidirectional Attention Flow model in Seo et al.
 . It adds a self-attention layer after the bidirectional attention component, simpli-
ﬁes some of the pooling operations and substitutes
the LSTMs for gated recurrent units . After adding ELMo to the baseline
model, test set F1 improved by 4.7% from 81.1%
to 85.8%, a 24.9% relative error reduction over the
baseline, and improving the overall single model
state-of-the-art by 1.4%. A 11 member ensemble pushes F1 to 87.4, the overall state-of-the-art
at time of submission to the leaderboard.2
increase of 4.7% with ELMo is also signiﬁcantly
larger then the 1.8% improvement from adding
CoVe to a baseline model .
2As of November 17, 2017.
PREVIOUS SOTA
 
4.7 / 24.9%
Chen et al. 
88.7 ± 0.17
0.7 / 5.8%
He et al. 
3.2 / 17.2%
Lee et al. 
3.2 / 9.8%
Peters et al. 
91.93 ± 0.19
92.22 ± 0.10
2.06 / 21%
McCann et al. 
54.7 ± 0.5
3.3 / 6.8%
Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across
six benchmark NLP tasks. The performance metric varies across tasks – accuracy for SNLI and SST-5; F1 for
SQuAD, SRL and NER; average F1 for Coref. Due to the small test sizes for NER and SST-5, we report the mean
and standard deviation across ﬁve runs with different random seeds. The “increase” column lists both the absolute
and relative improvements over our baseline.
Textual entailment Textual entailment is the
task of determining whether a “hypothesis” is
true, given a “premise”.
The Stanford Natural Language Inference (SNLI) corpus provides approximately 550K hypothesis/premise pairs.
Our baseline, the ESIM sequence model from Chen et al. , uses a biL-
STM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and ﬁnally a pooling operation before
the output layer.
Overall, adding ELMo to the
ESIM model improves accuracy by an average of
0.7% across ﬁve random seeds. A ﬁve member
ensemble pushes the overall accuracy to 89.3%,
exceeding the previous ensemble best of 88.9%
 .
Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument
structure of a sentence, and is often described as
answering “Who did what to whom”. He et al.
 modeled SRL as a BIO tagging problem
and used an 8-layer deep biLSTM with forward
and backward directions interleaved, following
Zhou and Xu . As shown in Table 1, when
adding ELMo to a re-implementation of He et al.
 the single model test set F1 jumped 3.2%
from 81.4% to 84.6% – a new state-of-the-art on
the OntoNotes benchmark ,
even improving over the previous best ensemble
result by 1.2%.
Coreference resolution Coreference resolution
is the task of clustering mentions in text that refer to the same underlying real world entities. Our
baseline model is the end-to-end span-based neural model of Lee et al. . It uses a biLSTM
and attention mechanism to ﬁrst compute span
representations and then applies a softmax mention ranking model to ﬁnd coreference chains. In
our experiments with the OntoNotes coreference
annotations from the CoNLL 2012 shared task
 , adding ELMo improved the
average F1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the
previous best ensemble result by 1.6% F1.
Named entity extraction The CoNLL 2003
NER task consists of
newswire from the Reuters RCV1 corpus tagged
with four different entity types (PER, LOC, ORG,
MISC). Following recent state-of-the-art systems
 , the baseline model uses pre-trained word embeddings, a
character-based CNN representation, two biLSTM
layers and a conditional random ﬁeld (CRF) loss
 , similar to Collobert et al.
 . As shown in Table 1, our ELMo enhanced
biLSTM-CRF achieves 92.22% F1 averaged over
ﬁve runs. The key difference between our system
and the previous state of the art from Peters et al.
 is that we allowed the task model to learn a
weighted average of all biLM layers, whereas Peters et al. only use the top biLM layer. As
shown in Sec. 5.1, using all layers instead of just
the last layer improves performance across multiple tasks.
Sentiment analysis The ﬁne-grained sentiment
classiﬁcation task in the Stanford Sentiment Treebank involves selecting one of ﬁve labels (from very negative to very
positive) to describe a sentence from a movie review.
The sentences contain diverse linguistic
phenomena such as idioms and complex syntac-
All layers
Table 2: Development set performance for SQuAD,
SNLI and SRL comparing using all layers of the biLM
(with different choices of regularization strength λ) to
just the top layer.
Table 3: Development set performance for SQuAD,
SNLI and SRL when including ELMo at different locations in the supervised model.
tic constructions such as negations that are difﬁcult for models to learn. Our baseline model is
the biattentive classiﬁcation network (BCN) from
McCann et al. , which also held the prior
state-of-the-art result when augmented with CoVe
embeddings. Replacing CoVe with ELMo in the
BCN model results in a 1.0% absolute accuracy
improvement over the state of the art.
This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations. Sec. 5.1
shows that using deep contextual representations
in downstream tasks improves performance over
previous work that uses just the top layer, regardless of whether they are produced from a biLM or
MT encoder, and that ELMo representations provide the best overall performance. Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better
represented at lower layers while semantic information is captured a higher layers, consistent with
MT encoders. It also shows that our biLM consistently provides richer representations then CoVe.
Additionally, we analyze the sensitivity to where
ELMo is included in the task model (Sec. 5.2),
training set size (Sec. 5.4), and visualize the ELMo
learned weights across the tasks (Sec. 5.5).
Alternate layer weighting schemes
There are many alternatives to Equation 1 for combining the biLM layers. Previous work on contextual representations used only the last layer,
whether it be from a biLM or
an MT encoder . The
choice of the regularization parameter λ is also
important, as large values such as λ = 1 effectively reduce the weighting function to a simple
average over the layers, while smaller values (e.g.,
λ = 0.001) allow the layer weights to vary.
Table 2 compares these alternatives for SQuAD,
SNLI and SRL. Including representations from all
layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline.
For example, in the
case of SQuAD, using just the last biLM layer improves development F1 by 3.9% over the baseline.
Averaging all biLM layers instead of using just the
last layer improves F1 another 0.3% (comparing
“Last Only” to λ=1 columns), and allowing the
task model to learn individual layer weights improves F1 another 0.2% (λ=1 vs. λ=0.001). A
small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set,
the results are insensitive to λ (not shown).
The overall trend is similar with CoVe but with
smaller increases over the baseline. For SNLI, averaging all layers with λ=1 improves development
accuracy from 88.2 to 88.7% over using just the
last layer. SRL F1 increased a marginal 0.1% to
82.2 for the λ=1 case compared to using the last
layer only.
Where to include ELMo?
All of the task architectures in this paper include
word embeddings only as input to the lowest layer
biRNN. However, we ﬁnd that including ELMo at
the output of the biRNN in task-speciﬁc architectures improves overall results for some tasks. As
shown in Table 3, including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer, but for SRL (and
coreference resolution, not shown) performance is
highest when it is included at just the input layer.
One possible explanation for this result is that both
the SNLI and SQuAD architectures use attention
layers after the biRNN, so introducing ELMo at
this layer allows the model to attend directly to the
biLM’s internal representations. In the SRL case,
Nearest Neighbors
playing, game, games, played, players, plays, player,
Play, football, multiplayer
Chico Ruiz made a spectacular play on Alusik ’s
grounder {.. . }
Kieffer , the only junior in the group , was commended
for his ability to hit in the clutch , as well as his all-round
excellent play .
signed to do a Broadway
play for Garson {. .. }
{. ..} they were actors who had been handed fat roles in
a successful play , and had talent enough to ﬁll the roles
competently , with nice understatement .
Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM.
WordNet 1st Sense Baseline
Raganato et al. 
Iacobacci et al. 
CoVe, First Layer
CoVe, Second Layer
biLM, First layer
biLM, Second layer
Table 5: All-words ﬁne grained WSD F1. For CoVe
and the biLM, we report scores for both the ﬁrst and
second layer biLSTMs.
the task-speciﬁc context representations are likely
more important than those from the biLM.
What information is captured by the
biLM’s representations?
Since adding ELMo improves task performance
over word vectors alone, the biLM’s contextual
representations must encode information generally useful for NLP tasks that is not captured
in word vectors.
Intuitively, the biLM must
be disambiguating the meaning of words using
their context.
Consider “play”, a highly polysemous word.
The top of Table 4 lists nearest neighbors to “play” using GloVe vectors.
They are spread across several parts of speech
(e.g., “played”, “playing” as verbs, and “player”,
“game” as nouns) but concentrated in the sportsrelated senses of “play”. In contrast, the bottom
two rows show nearest neighbor sentences from
the SemCor dataset (see below) using the biLM’s
context representation of “play” in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in
the source sentence.
These observations can be quantiﬁed using an
Collobert et al. 
Ma and Hovy 
Ling et al. 
CoVe, First Layer
CoVe, Second Layer
biLM, First Layer
biLM, Second Layer
Table 6: Test set POS tagging accuracies for PTB. For
CoVe and the biLM, we report scores for both the ﬁrst
and second layer biLSTMs.
intrinsic evaluation of the contextual representations similar to Belinkov et al. . To isolate
the information encoded by the biLM, the representations are used to directly make predictions for
a ﬁne grained word sense disambiguation (WSD)
task and a POS tagging task. Using this approach,
it is also possible to compare to CoVe, and across
each of the individual layers.
Word sense disambiguation Given a sentence,
we can use the biLM representations to predict
the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud
et al. .
To do so, we ﬁrst use the biLM
to compute representations for all words in Sem-
Cor 3.0, our training corpus ,
and then take the average representation for each
sense. At test time, we again use the biLM to compute representations for a given target word and
take the nearest neighbor sense from the training
set, falling back to the ﬁrst sense from WordNet
for lemmas not observed during training.
Table 5 compares WSD results using the evaluation framework from Raganato et al. 
across the same suite of four test sets in Raganato
et al. . Overall, the biLM top layer rep-
resentations have F1 of 69.0 and are better at
WSD then the ﬁrst layer. This is competitive with
a state-of-the-art WSD-speciﬁc supervised model
using hand crafted features 
and a task speciﬁc biLSTM that is also trained
with auxiliary coarse-grained semantic labels and
POS tags .
biLSTM layers follow a similar pattern to those
from the biLM (higher overall performance at the
second layer compared to the ﬁrst); however, our
biLM outperforms the CoVe biLSTM, which trails
the WordNet ﬁrst sense baseline.
POS tagging To examine whether the biLM
captures basic syntax, we used the context representations as input to a linear classiﬁer that predicts POS tags with the Wall Street Journal portion
of the Penn Treebank (PTB) .
As the linear classiﬁer adds only a small amount
of model capacity, this is direct test of the biLM’s
representations. Similar to WSD, the biLM representations are competitive with carefully tuned,
task speciﬁc biLSTMs . However, unlike WSD, accuracies
using the ﬁrst biLM layer are higher than the
top layer, consistent with results from deep biL-
STMs in multi-task training and MT . CoVe POS tagging accuracies
follow the same pattern as those from the biLM,
and just like for WSD, the biLM achieves higher
accuracies than the CoVe encoder.
Implications for supervised tasks Taken together, these experiments conﬁrm different layers
in the biLM represent different types of information and explain why including all biLM layers is
important for the highest performance in downstream tasks. In addition, the biLM’s representations are more transferable to WSD and POS tagging than those in CoVe, helping to illustrate why
ELMo outperforms CoVe in downstream tasks.
Sample efﬁciency
Adding ELMo to a model increases the sample ef-
ﬁciency considerably, both in terms of number of
parameter updates to reach state-of-the-art performance and the overall training set size. For example, the SRL model reaches a maximum development F1 after 486 epochs of training without
ELMo. After adding ELMo, the model exceeds
the baseline maximum at epoch 10, a 98% relative
decrease in the number of updates needed to reach
Figure 1: Comparison of baseline vs. ELMo performance for SNLI and SRL as the training set size is varied from 0.1% to 100%.
Figure 2: Visualization of softmax normalized biLM
layer weights across tasks and ELMo locations. Normalized weights less then 1/3 are hatched with horizontal lines and those greater then 2/3 are speckled.
the same level of performance.
ELMo-enhanced
smaller training sets more efﬁciently than models without ELMo. Figure 1 compares the performance of baselines models with and without
ELMo as the percentage of the full training set is
varied from 0.1% to 100%. Improvements with
ELMo are largest for smaller training sets and
signiﬁcantly reduce the amount of training data
needed to reach a given level of performance. In
the SRL case, the ELMo model with 1% of the
training set has about the same F1 as the baseline
model with 10% of the training set.
Visualization of learned weights
visualizes
softmax-normalized
learned layer weights.
At the input layer, the
task model favors the ﬁrst biLSTM layer.
coreference and SQuAD, the this is strongly
favored, but the distribution is less peaked for
the other tasks.
The output layer weights are
relatively balanced, with a slight preference for
the lower layers.
Development set ablation analysis for
SQuAD, SNLI and SRL comparing different choices
for the context-independent type representation and
contextual representation. From left to right, the table
compares systems with only GloVe vectors; only the
ELMo context-independent type representation without the ELMo biLSTM layers; full ELMo representations without GloVe; both GloVe and ELMo.
Contextual vs. sub-word information
In addition to the contextual information captured in the biLM’s biLSTM layers, ELMo representations also contain sub-word information in
the fully character based context insensitive type
layer, xLM
To analyze the relative contribution of the contextual information compared to the
sub-word information, we ran an additional ablation that replaced the GloVe vectors with just
the biLM character based xLM
layer without the
biLM biLSTM layers. Table 7 summarizes the results for SQuAD, SNLI and SNLI. Replacing the
GloVe vectors with the biLM character layer gives
a slight improvement for all tasks (e.g. from 80.8
to 81.4 F1 for SQuAD), but overall the improvements are small compared to the full ELMo model.
From this, we conclude that most of the gains in
the downstream tasks are due to the contextual information and not the sub-word information.
Are pre-trained vectors necessary with
All of the results presented in Sec.4 include pretrained word vectors in addition to ELMo representations. However, it is natural to ask whether
pre-trained vectors are still necessary with high
quality contextualized representations. As shown
in the two right hand columns of Table 7, adding
GloVe to models with ELMo generally provides
a marginal improvement over ELMo only models
(e.g. 0.2% F1 improvement for SRL from 84.5 to
Conclusion
We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of
NLP tasks. Through ablations and other controlled
experiments, we have also conﬁrmed that the
biLM layers efﬁciently encode different types of
syntactic and semantic information about wordsin-context, and that using all layers improves overall task performance.