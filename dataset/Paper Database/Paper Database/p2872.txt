DeepMutation: Mutation Testing of
Deep Learning Systems
Lei Ma1,2∗, Fuyuan Zhang2, Jiyuan Sun3, Minhui Xue2, Bo Li4, Felix Juefei-Xu5,
Chao Xie3, Li Li6, Yang Liu2, Jianjun Zhao3, and Yadong Wang1
1Harbin Institute of Technology, China 2Nanyang Technological University, Singapore 3Kyushu University, Japan
4University of Illinois at Urbana–Champaign, USA 5Carnegie Mellon University, USA 6Monash University, Australia
Abstract—Deep learning (DL) deﬁnes a new data-driven programming paradigm where the internal system logic is largely
shaped by the training data. The standard way of evaluating
DL models is to examine their performance on a test dataset.
The quality of the test dataset is of great importance to gain
conﬁdence of the trained models. Using an inadequate test
dataset, DL models that have achieved high test accuracy may
still lack generality and robustness. In traditional software
testing, mutation testing is a well-established technique for
quality evaluation of test suites, which analyzes to what extent
a test suite detects the injected faults. However, due to the
fundamental difference between traditional software and deep
learning-based software, traditional mutation testing techniques
cannot be directly applied to DL systems. In this paper, we
propose a mutation testing framework specialized for DL systems
to measure the quality of test data. To do this, by sharing the same
spirit of mutation testing in traditional software, we ﬁrst deﬁne
a set of source-level mutation operators to inject faults to the
source of DL (i.e., training data and training programs). Then we
design a set of model-level mutation operators that directly inject
faults into DL models without a training process. Eventually, the
quality of test data could be evaluated from the analysis on to
what extent the injected faults could be detected. The usefulness
of the proposed mutation testing techniques is demonstrated on
two public datasets, namely MNIST and CIFAR-10, with three
DL models.
Index Terms—Deep learning, Software testing, Deep neural
networks, Mutation testing
I. INTRODUCTION
Over the past decades, deep learning (DL) has achieved
tremendous success in many areas, including safety-critical
applications, such as autonomous driving , robotics ,
games , video surveillance . However, with the witness of
recent catastrophic accidents (e.g., Tesla/Uber) relevant to DL,
the robustness and safety of DL systems become a big concern.
Currently, the performance of DL systems is mainly measured
by the accuracy on the prepared test dataset. Without a systematic way to evaluate and understand the quality of the test data,
it is difﬁcult to conclude that good performance on the test
data indicates the robustness and generality of a DL system.
This problem is further exacerbated by many recently proposed
adversarial test generation techniques, which performs minor
perturbation (e.g., invisible to human eyes ) on the input
data to trigger the incorrect behaviors of DL systems. Due
to the unique characteristics of DL systems, new evaluation
∗Lei Ma is the corresponding author. Email: .
criteria on the quality of DL systems are highly desirable, and
the quality evaluation of test data is of special importance.
For traditional software, mutation testing (MT) has
been established as one of the most important techniques to
systematically evaluate the quality and locate the weakness
of test data. A key procedure of MT is to design and select
mutation operators that introduce potential faults into the
software under test (SUT) to create modiﬁed versions (i.e.,
mutants) of SUT , . MT measures the quality of tests by
examining to what extent a test set could detect the behavior
differences of mutants and the corresponding original SUT.
Unlike traditional software systems, of which the decision
logic is often implemented by software developers in the form
of code, the behavior of a DL system is mostly determined by
the structure of Deep Neural Networks (DNNs) as well as the
connection weights in the network. Speciﬁcally, the weights
are obtained through the execution of training program on
training data set, where the DNN structures are often deﬁned
by code fragments of training program in high-level languages
(e.g., Python , and Java ).1 Therefore, the training
data set and the training program are two major sources of
defects of DL systems. For mutation testing of DL systems, a
reasonable approach is to design mutation operators to inject
potential faults into the training data or the DNN training
program. After the faults are injected, the training process
is re-executed, using the mutated training data or training
program, to generate the corresponding mutated DL models. In
this way, a number of mutated DL models {M ′
2, . . . , M ′
are generated through injecting various faults. Then, each of
the mutant models M ′
i is executed and analyzed against the
test set T, in correspondence to original DL model M. Given
a test input t ∈T, t detects the behavior difference of M
i if the outputs of M and M ′ are inconsistent on t.
Similar to mutation testing for traditional software , the
more behavior differences of the original DL model M and
the mutant models {M ′
2, . . . , M ′
n} could be detected by
T, the higher quality of T is indicated.
In this paper, we propose a mutation testing framework
specialized for DL systems, to enable the test data quality evaluation. We ﬁrst design eight source-level mutation
testing operators that directly manipulate the training data
1Although the training program of a DNN is often written in high-level
languages, the DNN itself is represented and stored as a hierarchical data
structure (e.g., .h5 format for Keras ).
 
Traditional
Software Developer
Executable
Compilation Transformation
Deep Learning
Software Developer
Deep Learning
Collection
Fig. 1: A comparison of traditional and DL software development.
and training programs. The design intention is to introduce
possible faults and problems into DL programming sources,
which could potentially occur in the process of collecting
training data and implementing the training program. For
source-level mutation testing, training DNN models can be
computationally intensive: the training process can take minutes, hours, even longer . Therefore, we further design
eight mutation operators to directly mutate DL models for
fault inclusion. These model-level mutation operators not only
enable more efﬁcient generation of large sets of mutants but
also could introduce more ﬁne-grained model-level problems
that might be missed by mutating training data or programs.
We have performed an in-depth evaluation of the proposed
mutation testing techniques on two widely used datasets,
namely MNIST and CIFAR-10, and three popular DL models
with diverse structures and complexity. The evaluation result
demonstrates the usefulness of the proposed techniques as a
promising measurement towards designing and constructing
high-quality test datasets, which would eventually facilitate
the robustness enhancement of DL systems. It is worth noting
that the intention of the proposed mutation operators is for
fault injection on DL models so that test data quality could be
evaluated, instead of directly simulating the human faults.
Currently, testing for DL software is still at an early stage,
with some initial research work focused on accuracy and
neuron coverage, such as DeepXplore , DeepGauge ,
and DeepCover . To the best of our knowledge, our
work is the ﬁrst attempt to design mutation testing techniques
specialized for DL systems. The main contributions of this
paper are summarized as follows:
• We propose a mutation testing framework and workﬂow
specialized for DL systems, which enables the quality
evaluation and weakness localization of the test dataset.
• We design eight source-level (i.e., on the training data and
training program) mutation operators to introduce faults into
the DL programming elements. We further design eight
mutation operators that directly inject faults into DL models.
• We propose two DL-speciﬁc mutation testing metrics to
allow quantitative measurement for test quality.
• We evaluate the proposed mutation testing framework on
widely studied DL data sets and models, to demonstrate the
usefulness of the technique, which could also potentially
facilitate the test set enhancement.
Input Test
Mutants P'
Execute T'
Analysis &
Enhancement
Fig. 2: Key process of general mutation testing.
II. BACKGROUND
A. Programming Paradigms
Building deep learning based systems is fundamentally
different from that of traditional software systems. Traditional
software is the implementation of logic ﬂows crafted by
developers in the form of source code (see Figure 1), which
can be decomposed into units (e.g., classes, methods, statements, branches). Each unit speciﬁes some logic and allows
to be tested as targets of software quality measurement (e.g.,
statement coverage, branch coverage). After the source code is
programmed, it is compiled into executable form, which will
be running in respective runtime environments to fulﬁll the
requirements of the system. For example, in object-oriented
programming, developers analyze the requirements and design
the corresponding software architecture. Each of the architectural units (e.g., classes) represents speciﬁc functionality, and
the overall goal is achieved through the collaborations and
interactions of the units.
Deep learning, on the other hand, follows a data-driven programming paradigm, which programs the core logic through
the model training process using a large amount of training data. The logic is encoded in a deep neural network,
represented by sets of weights fed into non-linear activation
functions . To obtain a DL software F for a speciﬁc task
M, a DL developer (see Figure 1) needs to collect training
data, which speciﬁes the desired behavior of F on M, and
prepare a training program, which describes the structure of
DNN and runtime training behaviors. The DNN is built by
running the training program on the training data. The major
effort for a DL developer is to prepare a set of training
data and design a DNN model structure, and DL logic is
determined automatically through the training procedure. In
contrast to traditional software, DL models are often difﬁcult
to be decomposed or interpreted, making them unamenable
to most existing software testing techniques. Moreover, it is
challenging to ﬁnd high-quality training and test data that
represent the problem space and have good coverage of the
models to evaluate their generality.
Original Training
Original Training
Deep Learning
Mutant Model M'
Create Mutant
Data Mutation
Prog. Mutation
Create Mutant
Program P'
Deep Learning
Passed Tests T'
Fig. 3: Source-level mutation testing workﬂow of DL systems.
B. Mutation Testing
The general process of mutation testing , for traditional software is illustrated in Figure 2. Given an original
program P, a set of faulty programs P ′ (mutants) are created
based on predeﬁned rules (mutation operators), each of which
slightly modiﬁes P. For example, a mutation operator can
syntactically change ‘+’ operator in the program to ‘−’
operator – . A step of preprocessing, usually before
the actual mutation testing procedure starts, is used to ﬁlter
out irrelevant tests. Speciﬁcally, the complete test set T is
executed against P and only the passed tests T ′ (a subset of
T) are used for mutation testing. In the next step, each mutant
of P ′ is executed on T ′. If the test result for a mutant p′ ∈P ′
is different from that of P, then p′ is killed; otherwise, p′ is
survived. When all the mutants in P ′ have been tested against
T ′, mutation score is calculated as the ratio of killed mutants to
all the generated mutants (i.e., #mutantskilled/#mutantsall),
which indicates the quality of test set. Conceptually, a test
suite with a higher mutation score is more likely to capture
real defects in the program . After obtaining the mutation
testing results, the developer could further enhance the quality
of test set (e.g., by adding/generating more tests) based on the
feedback from mutation testing , . The general goal
of mutation testing is to evaluate the quality of test set T, and
further provide feedback and guide the test enhancement.
III. SOURCE-LEVEL MUTATION TESTING OF DL SYSTEMS
In general, traditional software is mostly programmed by
developers in the form of source code (Figure 1), which could
be a major source of defect introduction. Mutation testing
slightly modiﬁes the program code to introduce faults, which
enables the measurement of test data quality through detecting
such deliberately changes.
With the same spirit of mutation testing for traditional
software, directly introducing potential defects into the programming sources of a DL system is a reasonable approach
to create mutants. In this section, we propose a sourcelevel mutation testing technique for DL systems. We design
a general mutation testing workﬂow for DL systems, and
propose a set of mutation operators as the key components.
TABLE I: Source-level mutation testing operators for DL systems.
Fault Type
Operation Description
Data Repetition (DR)
Duplicates training data
Duplicates speciﬁc type of data
Label Error (LE)
Falsify results (e.g., labels) of data
Falsify speciﬁc results of data
Data Missing (DM)
Remove selected data
Remove speciﬁc types of data
Data Shufﬂe (DF)
Shufﬂe selected training data
Shufﬂe speciﬁc types of data
Noise Perturb. (NP)
Add noise to training data
Add noise to speciﬁc type of data
Layer Removal (LR)
Remove a layer
Layer Addition (LAs)
Add a layer
Act. Fun. Remov. (AFRs)
Remove activation functions
Furthermore, we deﬁne the mutation testing metrics for quantitative measurement and evaluation of the test data quality.
A. Source-level Mutation Testing Workﬂow for DL Systems
Figure 3 shows the key workﬂow of our source-level
mutation testing technique. At the initialization phase, a DL
developer prepares a training program P and a set of training
data D. After the training process, which runs P with D, a
DL model M is obtained. When the mutation testing starts, the
original training data D and program P are slightly modiﬁed
by applying mutation operators (deﬁned in Table I), and the
corresponding mutants D′ and P ′ are generated. In the next
step, either a training data mutant or training program mutant
participates in the training process to generate a mutated DL
model M ′. When mutated DL models are obtained, they are
executed and analyzed against the ﬁltered test set T ′ for
evaluating the quality of test data.2 We emphasize that, the
proposed mutation operators in this paper are not intended to
directly simulate human faults; instead, they aim to provide
ways for quantitative measurement on the quality of test data
set. In particular, the more behavior differences between the
original DL model and the mutant models (generated by
mutation operators) T ′ could detect, the higher quality of T ′
is indicated. The detailed quality measurement metrics are
deﬁned in Section III-C.
B. Source-level Mutation Operators for DL Systems
We propose two groups of mutation operators, namely
data mutation operators and program mutation operators,
which perform the corresponding modiﬁcation on sources to
introduce potential faults (see Table I).
1) Data Mutation Operators: Training data plays a vital
role in building DL models. The training data is usually large
in size and labeled manually – . Preparing training
data is usually laborious and sometimes error-prone. Our data
mutation operators are designed based on the observation of
potential problems that could occur during the data collection
process. These operators can either be applied globally to all
types of data, or locally only to speciﬁc types of data within
the entire training data set.
2T ′ is consisted of the test data points in T that are correctly processed by
the original DL model M.
• Data Repetition (DR): The DR operator duplicates a small
portion of training data. The training data is often collected
from multiple sources, some of which are quite similar, and
the same data point can be collected more than once.
• Label Error (LE): Each data point (d, l) in the training
dataset D, where d represents the feature data and l is
the label for d. As D is often quite large (e.g., MNIST
dataset contains 60, 000 training data), it is not uncommon that some data points can be mislabeled. The LE
operator injects such kind of faults by changing the label
for a data.
• Data Missing (DM): The DM operator removes some of
the training data. It could potentially happen by inadvertent
or mistaken deletion of some data points.
• Data Shufﬂe (DF): The DF operator shufﬂes the training
data into different orders before the training process. Theoretically, the training program runs against the same set of
training data should obtain the same DL model. However,
the implementation of training procedure is often sensitive
to the order of training data. When preparing training data,
developers often pay little attention to the order of data, and
thus can easily overlook such problems during training.
• Noise Perturbation (NP): The NP operator randomly adds
noise to training data. A data point could carry noise from
various sources. For example, a camera-captured image
could include noise caused by different weather conditions (i.e., rain, snow, dust, etc.). The NP operator tries to
simulate potential issues relevant to noisy training data (e.g.,
NP adds random perturbations to some pixels of an image).
2) Program Mutation Operators: Similar to traditional programs, a training program is commonly coded using high-level
programming languages (e.g., Python and Java) under speciﬁc
DL framework. There are plenty of syntax-based mutation
testing tools available for traditional software – , and
it seems straightforward to directly apply these tools to the
training program. However, this approach often does not work,
due to the fact that DL training programs are sensitive to code
changes. Even a slight change can cause the training program
to fail at runtime or to produce noticeable training process
anomalies (e.g., obvious low prediction accuracy at the early iterations/epochs of the training). Considering the characteristics
of DL training programs, we design the following operators
to inject potential faults.
• Layer Removal (LR): The LR operator randomly deletes
a layer of the DNNs on the condition that input and output
structures of the deleted layer are the same. Although it
is possible to delete any layer that satisﬁes this condition,
arbitrarily deleting a layer can generate DL models that are
obviously different from the original DL model. Therefore,
the LR operator mainly focuses on layers (e.g., Dense,
BatchNormalization layer ), whose deletion does
not make too much difference on the mutated model. The
LR operator mimics the case that a line of code representing
a DNN layer is removed by the developer.
Mutant&decision&boundary&2
Mutant&decision&boundary&1
Original&decision&boundary
Fig. 4: Example of DL model and its two generated mutant models
for binary classiﬁcation with their decision boundaries. In the ﬁgure,
some data scatter closer to the decision boundary (in green color).
Our mutation testing metrics favor to identify the test data that locate
in the sensitive region near the decision boundary.
• Layer Addition (LAs): In contrast to the LR operator, the LAs operator adds a layer to the DNNs structure. LAs focuses on adding layers like Activation,
BatchNormalization, which introduces possible faults
caused by adding or duplicating a line of code representing
a DNN layer.
• Activation Function Removal (AFRs): Activation function plays an important role of the non-linearity of
DNNs for higher representativeness (i.e., quantiﬁed as VC
dimension ). The AFRs operator randomly removes
all the activation functions of a layer, to mimic the situation
that the developer forgets to add the activation layers.
C. Mutation Testing Metrics for DL Systems
After the training data and training program are mutated by
the mutation operators, a set of mutant DL models M ′ can be
obtained through training. Each test data point t′ ∈T ′ that is
correctly handled by the original DL model M, is evaluated
on the set of mutant models M ′. We say that test data T ′
kill mutant m′ if there exists a test input t′ ∈T ′ that is not
correctly handled by m′. The mutation score of traditional
mutation testing is calculated as the ratio of killed mutants
to all mutants. However, it is inappropriate to use the same
mutation score metrics of traditional software as the metrics
for mutation testing of DL systems. In the mutation testing
of DL systems, it is relatively easy for T ′ to kill a mutant
m′ when the size of T ′ is large, which is also convinced from
our experiment in Section V. Therefore, if we were to directly
use the mutation score for DL systems as the ratio of killed
mutants to all mutants, our metric would lose the precision to
evaluate the quality of test data for DL systems.
In this paper, we focus on DL systems for classiﬁcation
problems.3 Suppose we have a k-classiﬁcation problem and
let C = {c1, . . . , ck} be all the k classes of input data. For a
test data point t′ ∈T ′, we say that t′ kills ci ∈C of mutant
3Although, the mutation score metric deﬁned in this paper mainly focuses
on classiﬁcation problems, the similar idea can be easily extended to handle
numerical predication problem as well, with a user-deﬁned threshold as the
error allowance margin .
Deep Learning
Original Training
Original Training
Deep Learning
Mutant Model M'
Model Mutation
Fig. 5: The model level mutation testing workﬂow for DL systems.
m′ ∈M ′ if the following conditions are satisﬁed: (1) t′ is
correctly classiﬁed as ci by the original DL model M, and (2)
t′ is not classiﬁed as ci by m′. We deﬁne the mutation score
for DL systems as follows, where KilledClasses(T ′, m′) is
the set of classes of m′ killed by test data in T ′:
MutationScore(T ′, M′) =
m′∈M′ |KilledClasses(T ′, m′)|
|M′| × |C|
In general, it could be difﬁcult to precisely predict the
behavioural difference introduced by mutation operators. To
avoid introducing too many behavioural differences for a DL
mutant model from its original counterpart, we propose a
DL mutant model quality control procedure. In particular, we
measure the error rate of each mutant m′ on T ′. If the error
rate of m′ is too high for T ′, we don’t consider m′ a good
mutant candidate as it introduces a large behavioral difference.
We excluded such mutant models from M ′ for further analysis.
We deﬁne average error rate (AER) of T ′ on each mutant
model m′ ∈M ′ to measure the overall behavior differential
effects introduced by all mutation operators.
AveErrorRate(T ′, M′) =
m′∈M′ ErrorRate(T ′, m′)
Figure 4 shows an example of a DL model for binary
classiﬁcation, with the decision boundary of the original model
and the decision boundaries of two mutant models. We can
see that the mutant models are more easily to be killed by
data in green, which lies near the decision boundary of the
original DL model. The closer a data point is to the decision
boundary, the higher chance it has to kill more mutant models,
which is reﬂected as the increase of the mutation score and
AER deﬁned for DL systems. In general, mutation testing
facilitates to evaluate the effectiveness of test set, by analyzing
to what extent the test data is closed to the decision boundary
of DNNs, where the robustness issues more often occur.
IV. MODEL-LEVEL MUTATION TESTING OF DL SYSTEMS
In Section III, we deﬁne the source-level mutation testing
procedure and workﬂow, which simulate the traditional mutation testing techniques designed to work on source code
TABLE II: Model-level mutation testing operators for DL systems.
Mutation Operator
Description
Gaussian Fuzzing (GF)
Fuzz weight by Gaussian Distribution
Weight Shufﬂing (WS)
Shufﬂe selected weights
Neuron Effect Block. (NEB)
Block a neuron effect on following layers
Neuron Activation Inverse (NAI)
Invert the activation status of a neuron
Neuron Switch (NS)
Switch two neurons of the same layer
Layer Deactivation (LD)
Deactivate the effects of a layer
Layer Addition (LAm)
Add a layer in neuron network
Act. Fun. Remov. (AFRm)
Remove activation functions
(see Figure 1). In general, to improve mutation testing ef-
ﬁcient, many traditional mutation testing techniques are designed to work on a low-level software representation (e.g.,
Bytecode , , Binary Code , ) instead of
the source code, which avoid the program compilation and
transformation effort. In this section, we propose the modellevel mutation testing for DL system towards more efﬁcient
DL mutant model generation.
A. Model-Level Mutation Testing Workﬂow for DL Systems
Figure 5 shows the overall workﬂow of DL model level
mutation testing workﬂow. In contrast to the source-level
mutation testing that modiﬁes the original training data D
and training program P, model level mutation testing directly
changes the DL model M obtained through training from D
and P. For each generated DL mutant model m′ ∈M ′ by
our deﬁned model-level mutation operators in Table II, input
test dataset T is run on M to ﬁlter out all incorrect data
and the passed data are sent to run each m′. The obtained
execution results adopt the same mutation metrics deﬁned in
Section III-C for analysis and report.
Similar to source-level mutation testing, model-level mutation testing also tries to evaluate the effectiveness and locate
the weakness of a test dataset, which helps a developer to
further enhance the test data to exercise the fragile regions
of a DL model under test. Since the direct modiﬁcation of
DL model avoids the training procedure, model-level mutation
testing is expected to be more efﬁcient for DL mutant model
generation, which is similar to the low-level (e.g., intermediate
code representation such as Java Bytecode) mutation testing
techniques of traditional software.
B. Model-level Mutation Operators for DL Systems
Mutating training data and training program will eventually
mutate the DL model. However, the training process can be
complicated, being affected by various parameters (e.g., the
number of training epochs). To efﬁciently introduce possible
faults, we further propose model-level mutation operators,
which directly mutate the structure and parameters of DL
models. Table II summarizes the proposed model-level mutation operators, which range from weight level to layer level
in terms of application scopes of the operators.
• Gaussian Fuzzing (GF): Weights are basic elements of
DNNs, which describe the importance of connections between neurons. Weights greatly contribute to the decision
logic of DNNs. A natural way to mutate the weight is
to fuzz its value to change the connection importance it
represents. The GF operator follows the Gaussian distribution N(w, σ2) to mutate a given weight value w, where
σ is a user-conﬁgurable standard deviation parameter. The
GF operator mostly fuzzes a weight to its nearby value
range (i.e., the fuzzed value locates in [w −3σ, w + 3σ]
with 99.7 % probability), but also allows a weight to be
changed to a greater distance with a smaller chance.
• Weight Shufﬂing (WS): The output of a neuron is often
determined by neurons from the previous layer, each of
which has connections with weights. The WS operator
randomly selects a neuron and shufﬂes the weights of its
connections to the previous layer.
• Neuron Effect Blocking (NEB): When a test data point is
read into a DNN, it is processed and propagated through
connections with different weights and neuron layers until
the ﬁnal results are produced. Each neuron contributes
to the DNN’s ﬁnal decision to some extent according to
its connection strength. The NEB operator blocks neuron
effects to all of the connected neurons in the next layers,
which can be achieved by resetting its connection weights
of the next layers to zero. The NEB removes the inﬂuence
of a neuron to the ﬁnal DNN’s decision.
• Neuron Activation Inverse (NAI): The activation function
plays a key role in creating the non-linear behaviors of
the DNNs. Many widely used activation functions (e.g.,
ReLU , Leaky ReLU ) show quite different behaviors depending on their activation status. The NAI operator
tries to invert the activation status of a neuron, which
can be achieved by changing the sign of the output value
of a neuron before applying its activation function. This
facilitates to create more mutant neuron activation patterns,
each of which can show new mathematical properties (e.g.,
linear properties) of DNNs .
• Neuron Switch (NS): The neurons of a DNN’s layer often
have different impacts on the connected neurons in the next
layers. The NS operator switches two neurons within a layer
to exchange their roles and inﬂuences for the next layers.
• Layer Deactivation (LD): Each layer of a DNN transforms
the output of its previous layer and propagates its results to
its following layers. The LD operator is a layer level mutation operator that removes a whole layer’s transformation
effects as if it is deleted from the DNNs. However, simply
removing a layer from a trained DL model can break the
model structure. We restrict the LD operator to layers whose
the input and output shapes are consistent.
• Layer Addition (LAm): The LAm operator tries to make the
opposite effects of the LD operator, by adding a layer to the
DNNs. Similar to the LD operator, the LAm operator works
under the same conditions to avoid breaking original DNNs;
besides, the LAm operator also includes the duplication and
insertion of copied layer after its original layers, which also
requires the shape of layer input and output to be consistent.
• Activation Function Removal (AFRm): AFRm operator
removes the effects of activation function of a whole layer.
The AFRm operator differs from the NAI operator in two
TABLE III: Evaluation subject datasets and DL models. Our selected
subject datasets MNIST and CIFAR-10 are widely studied in previous
work. We train the DNNs model with its corresponding original
training data and training program. The obtained DL model refers
to the original DL (i.e., the DL model M in Figure 3 and 5), which
we use as the baseline in our evaluation. Each studied DL model
structure and the obtained accuracy are summarized below.
A (LeNet5) 
Conv(6,5,5)+ReLU
Conv(32,3,3)+ReLU
Conv(64,3,3)+ReLU
MaxPooling (2,2)
Conv(32,3,3)+ReLU
Conv(64,3,3)+ReLU
Conv(16,5,5)+ReLU
MaxPooling(2,2)
MaxPooling(2,2)
MaxPooling(2,2)
Conv(64,3,3)+ReLU
Conv(128,3,3)+ReLU
Conv(64,3,3)+ReLU
Conv(128,3,3)+ReLU
FC(120)+ReLU
MaxPooling(2,2)
MaxPooling(2,2)
FC(84)+ReLU
FC(10)+Softmax
FC(200)+ReLU
FC(256)+ReLU
FC(10)+Softmax
FC(256)+ReLU
#Train. Para. 107,786
Train. Acc. 97.4%
Test. Acc. 97.0%
perspectives: (1) AFRm works on the layer level, (2) AFRm
removes the effects of activation function, while NAI operator keeps the activation function and tries to invert the
activation status of a neuron.
V. EVALUATION
We have implemented DeepMutation, a DL mutation testing framework including both proposed source-level and
model-level mutation testing techniques based on Keras
(ver.2.1.3) with Tensorﬂow (ver.1.5.0) backend . The
source-level mutation testing technique is implemented by
Python and has two key components: automated training
data mutant generator and Python training program mutant
generator (see Figure 3 and Table I). The model-level mutation
testing automatically analyzes a DNN’s structure and uses
our deﬁned operators to mutate on a copy of the original
DNN. Then the generated mutant models are serialized and
stored as .h5 ﬁle format. The weight-level and neuronlevel mutation operators (see Table II) are implemented by
mutating the randomly selected portion of the DNN’s weight
matrix elements. The implementation of layer-level mutation
operators is more complex. We ﬁrst analyze the whole DNN’s
structure to identify the candidate layers of the DNN that
satisfy the layer-level mutation conditions (see Section IV-B).
Then, we construct a new DL mutant model based on the
original DL model through the functional interface of Keras
and Tensforﬂow .
In order to demonstrate the usefulness of our proposed
mutation testing technique, we evaluated the implemented
mutation testing framework on two practical datasets and three
DL model architectures, which will be explained in the rest
of this section.
A. Subject Dataset and DL Models
MNIST and CIFAR-10 as the evaluation subjects.
MNIST is for handwritten digit image recognition, containing
60, 000 training data and 10, 000 test data, with a total number
of 70, 000 data in 10 classes (digits from 0 to 9). CIFAR-10
dataset is a collection of images for general purpose image
classiﬁcation, including 50, 000 training data and 10, 000 test
data in 10 different classes (e.g., airplanes, cars, birds, and
For each dataset, we study popular DL models , ,
 that are widely used in previous work. Table III summarizes the structures and complexity of the studied DNNs,
as well as the prediction accuracy obtained on our trained
DNNs. The studied DL models A, B, and C contain 107, 786,
694, 402, and 1, 147, 978 trainable parameters, respectively.
The trainable parameters of DNNs are those parameters that
could be adjusted during the training process for higher learning performance. It is often the case that the more trainable
parameters a DL model has, the more complex a model would
be, which requires higher training and prediction effort. We
follow the training instructions of the papers , , 
to train the original DL models. Overall, on MNIST, model
A achieves 97.4% training accuracy and 97.0% test accuracy;
model B achieves 99.3% and 98.7%, comparable to the state
of the art. On CIFAR-10, model C achieves 97.1% training
accuracy and 78.3% test accuracy, similar to the accuracy
given in .
Based on the selected datasets and models, we design experiments to investigate whether our mutation testing technique is
helpful to evaluate the quality and provide feedback on the test
data. To support large scale evaluation, we run the experiments
on a high performance computer cluster. Each cluster node
runs a GNU/Linux system with Linux kernel 3.10.0 on a 18core 2.3GHz Xeon 64-bit CPU with 196 GB of RAM and also
an NVIDIA Tesla M40 GPU with 24G.
B. Controlled Dataset and DL Mutant Model Generation
1) Test Data: The ﬁrst step of the mutation testing is to
prepare the test data for evaluation. In general, a test dataset
is often independent of the training dataset, but follows a
similar probability distribution as the training dataset ,
 . A good test data set should be comprehensive and
covers diverse functional aspects of DL software use-case, so
as to assess performance (i.e., generalization) and reveal the
weakness of a fully trained DL model. For example, in the
autonomous driving scenario, the captured road images and
signals from camera, LIDAR, and infrared sensors are used
as inputs for DL software to predict the steering angle and
braking/acceleration control . A good test dataset should
contain a wide range of driving cases that could occur in
practice, such as strait road, curve road, different road surface
conditions and weather conditions. If a test dataset only covers
limited testing scenarios, good performance on the test dataset
does not conclude that the DL software has been well tested.
To demonstrate the usefulness of our mutation testing for the
measurement of test data quality, we performed a controlled
experiment on two data settings (see Table IV). Setting one
samples 5, 000 data from original training data while setting
TABLE IV: The controlled experiment data preparation settings.
Controlled
MNIST/CIFAR-10
Train. data
Train. data
Non-uniform
Non-uniform
two sampled 1, 000 from the accompanied test data, both
of which take up approximately 10% of the corresponding
dataset.4 Each setting has a pair of dataset (T1, T2), where T1
is uniformly sampled from all classes and T2 is non-uniformly
sampled.5 The ﬁrst group of each setting covers more diverse
use-case of the DL software of each class, while the second
group of dataset mainly focuses on a single class. It is expected
that T1 should obtain a higher mutation score, and we check
whether our mutation testing conﬁrms this. We repeat the data
sampling for each setting ﬁve times to counter randomness
effects during sampling. This allows to obtain ﬁve pairs of
data for each setting (i.e., (T1, T2)1, (T1, T2)2, .. ., (T1, T2)5).
Each pair of data is evaluated on the generated DL mutant
models, and we average the mutation testing analysis results.
After the candidate data are prepared for mutation testing,
they are executed on each of corresponding original DL
models to ﬁlter out those failed cases, and only the passed
data are used for further mutation analysis. This procedure
generates a total of 30 (=2 settings * 3 models * 5 repetition)
pairs of candidate datasets, where each of the three DL models
has 10 pairs (i.e., 5 for each setting) of dataset for analysis.
2) DL Mutant Model Generation: After preparing the controlled datasets, we start the mutation testing procedure. One
key step is to generate the DL mutant models. For each studied
DL model in Table III, we generate the DL mutant models
using both the source-level and model-level mutant generators.
To generate source-level DL mutant models, we conﬁgure
our data-level mutation operators to automatically mutate 1%
of original training data and apply each of the program-level
mutation operators to the training program (see Table I). After
the mutant dataset (program) are generated, they are trained
on the original training program (training data) to obtain the
mutant DL models. Considering the intensive training effort,
we conﬁgure to generate 20 DL mutants for each data-level
mutation operator (i.e., 10 for global level and 10 for local
level). For program-level mutators, we try to perform mutation
whenever the conditions are satisﬁed with a maximal 20
mutant models for each program-level operator.
To generate model-level mutants at the weight and neuron
level, we conﬁgure to sample 1%, weights and neurons from
the studied DNNs, and use the corresponding mutation operators to randomly mutate the selected targets (see Table II).
On the layer level, our tool automatically analyzes the layers
that satisfy the mutation conditions (see Section IV-B) and
4We use sampling in evaluation since the general ground-truth for test set
quality is unavailable
5To be speciﬁc, we prioritize to select one random class data with 80%
probability, while data from other classes share the remaining 20% chance.
TABLE V: The average error rate of controlled experiment data on
the DL mutant models. We control the sampling method and data
size to be the same, and let the data selection scope as the variable.
The ﬁrst group sample data from all classes of original passed test
data, while the second group sample data from a single class.
Source Level (%)
Model Level (%)
5000 train.
1000 test.
5000 train.
1000 test.
randomly applies the corresponding mutation operator. The
model-level mutant generation is rather efﬁcient without the
training effort. Therefore, for each weight- and neuron-level
mutation operator we generate 50 mutant models. Similarly,
our tool tries to generate layer-level mutant models when
DNN’s structure conditions are satisﬁed with maximal 50
mutant models for each layer-level mutation operator.
C. Mutation Testing Evaluation and Results
After the controlled datasets and mutant models are generated, the mutation testing starts the execution phase by
running candidate test dataset on mutant models, after which
we calculate the mutation score and average error rate (AER)
for each dataset. Note that the dataset used for evaluation are
those data that passed on original DL models. In addition,
we also introduce a quality control procedure for generated
mutant models. After we obtained the passed test data T ′ on
the original model (see Figure 3), we run it against each of
its corresponding generated mutant models, and remove those
models with high error rate,6 as such mutant model show big
behavioral differences from original models.
Table V summarizes the AER obtained for each controlled
dataset on all DL mutant models. We can see that the obtained
DL mutant models indeed enable to inject faults into DL
models with the AER ranging from 0.13% to 17.20%, where
most of the AERs are relatively small. In all the experimentally
controlled data settings, the uniformly sampled data group
achieves higher average error rate on the mutant models,
which indicates the uniformly sampled data has higher defect
detection ability (better quality from a testing perspective).
For model C, when considering both source-level and modellevel, a relatively low AER is obtained for the sampled training
data sets from 2.99% up to 9.11%, but with a higher AER of
sampled testing data from 9.00% to 17.20%. This indicates
that the sampled test data quality of model C is better in
terms of killing the mutants compared with the sampled
training data, although the sampled training data has larger
data size (i.e., 5, 000).
In line with the AER, the averaged mutation score for each
setting in Table IV is also calculated, as shown in Figure 6
and 7. Again, on all the controlled data pair settings, a higher
mutation score is obtained by uniform sampling method, which
6This study sets the error rate bar to be 20%. It could be conﬁgured to smaller
values to keep models with even more similar behaviors with the original
Uniform-Sampling
NonUniform-Sampling
(a) 5000 training data sampling
(b) 1000 test data sampling
Fig. 6: The averaged mutation score of source-level mutation testing.
Uniform-Sampling
NonUniform-Sampling
(a) 5000 training data sampling
(b) 1000 test data sampling
Fig. 7: The averaged mutation score of model-level mutation testing.
also conﬁrms our expectation on the test data quality. Besides
the AER that measures the ratio of data that detect the defects
of mutant models, mutation score measures how well the test
data cover mutation models from the testing use-case diversity
perspective. The mutation score does not necessarily positively
correlate with the AER, as demonstrated in the next section.
Intuitively, a test dataset with more data might uncover more
defects and testing aspects. However, this is not generally
correct as conﬁrmed in our experiment. In Table V, for sourcelevel mutation testing of model B, the obtained AER of 1, 000
uniformly sampled test data (i.e., 0.66%) is higher than the one
obtained from the uniformly sampled 5, 000 training data (i.e.,
0.49%). This is more obvious on model C. When the same
sampling method is used, the AER obtained from the sampled
1000 test data is all higher than the sampled 5, 000 training
data. The same conclusion could also be reached by observing
the mutation score (see Figure 6(a) and (b)). The mutation
scores on model A and B are the cases where a larger data
size obtains a higher mutation score, whereas the result on
model C shows the opposite case.
When performed on the same set of data, the source-level
mutation testing and model-level mutation testing show some
different behaviors. Note that, on source-level, we conﬁgure
to mutate 1% of the training data; on the model-level, we use
the same ratio (i.e., 1%) for weight and neuron level mutators.
Overall, the generated mutant models by source-level and
TABLE VI: The model-level MT score and average error rate of
test data by class. According to our mutation score deﬁnition, the
maximal possible mutation score for a single class is 10%.
Classiﬁcation Class (%)
model-level mutation testing behave differently. For example,
comparing the same data pair setting of Figures 6(a) and 7(a),
the source-level mutation testing obtains lower mutation score
on model A, but obtains higher mutation score on model
B. This means that the same 1% mutation ratio results in
different DL mutant model effects by source-level and modellevel mutation testing procedure. For ﬂexibility, in our tool,
we provide the conﬁgurable option for both source-level and
model-level mutant generation.
In both Figure 6 and 7, we observe that the mutation scores
are still low for many cases. It indicates that corresponding
evaluated tests are low-quality, which is understandable in
high-dimensional space. The fact that DL could be easily
attacked by many existing adversarial techniques despite high
performance on test data also conﬁrms our ﬁndings .
D. Mutation Testing of Original Test Data by Class
Given a DL classiﬁcation task, the developers often prepare
the test data with great care. On one hand, they try to collect
data from diverse classes that cover more use-case scenarios.
On the other hand, they also try to obtain more sensitive data
for each class to facilitate the detection of DNN robustness
issues. The same test dataset might show different testing
performance on different DL models; the data from different
classes of the same test data might contribute differently to
testing performance as well. In this section, we investigate
how each class of the original test dataset behaves from the
mutation testing perspective.
1) Test Data and Mutant Models: Similar to the experimental procedure in Section V-B ,we ﬁrst prepare the test data of
each class for mutation testing. For the accompanied original
test data in MNIST (CIFAR-10), we separate them into the
corresponding 10 test dataset by class (i.e., t1, t2, . . . , t10).
For each class of the test data ti, we follow the same mutation
testing procedure to perform data ﬁltering procedure on model
A, B and C, respectively. In the end, we obtain 30 test datasets,
including 10 datasets by class (i.e., t′
2, . . . , t′
10) for each
studied DL model. We reuse the generated model-level DL
mutant models of Section V-B and perform mutation testing
on the prepared dataset.
2) Mutation Testing Results of Test Data by Class: Table VI
summarizes the obtained mutation score and AER for each
model. We can see that, in general, the test data of different
classes obtain different mutation scores and AER. Consider
the results of model A as an example, the test data of class 3
obtains the lowest mutation score and AER (i.e., 6.25% and
1.48%). It indicates that, compared with the test data of other
classes, the test data of class 3 could still be further enhanced.
In addition, this experiment demonstrates that a higher AER
does not necessarily result in a higher mutation score. For
model A, the AER obtained by class 1 is larger than class 2
while the mutation score of class 1 is smaller.
Remark. Our mutation testing technique enables the
quantitative analysis on test data quality of each class.
It also helps to localize the weakness in test data. Based
on the mutation testing feedback, DL developers could
prioritize to augment and enhance the weak test data to
cover more defect-sensitive cases.
E. Threats To Validity
The selection of the subject datasets and DL models could
be a threat to validity. In this paper, we try to counter this issue
by using two widely studied datasets (i.e., MNIST and CIFAR-
10), and DL models with different network structures, complexities, and have competitive prediction accuracy. Another
threat to validity could be the randomness in the procedure
of training source-level DL mutant models. The TensorFlow
framework by default uses multiple threads for training procedure, which can cause the same training dataset to generate
different DL models. To counter such effects, we tried our best
to rule out non-deterministic factors in training process. We
ﬁrst ﬁx all the random seeds for training programs, and use
a single thread for training by setting Tensorﬂow parameters.
Such a setting enables the training progress deterministic when
running on CPU, which still has non-deterministic behavior
when running on GPU. Therefore, for the controlled evaluation
described in this paper, we performed the source-level DL
mutant model training by CPU to reduce the threat caused by
randomness factor in training procedure. Another threat is the
randomness during data sampling. To counter this, we repeat
the sampling procedure ﬁve times and average the results.
VI. RELATED WORK
A. Mutation Testing of Traditional Software
The history of mutation testing dated back to 1971 in
Richard Liption’s Paper , and the ﬁeld started to grow
with DeMillo et al. and Hamlet pioneering works in
late 1970s. Afterwards, mutation testing has been extensively
studied for traditional software, which has been proved to be a
useful methodology to evaluate the effectiveness of test data.
As a key component in mutation testing procedure, mutation operators are widely studied and designed for different
programming languages. Budd et al. was the ﬁrst to design
mutation operators for Fortran , . Arawal et al. later
proposed a set of 77 mutation operators for ANSI C .
Due to the fast development of programming languages that
incorporates many features (e.g., Object Oriented, Aspect-
Oriented), mutation operators are further extended to cover
more advanced features in popular programming languages,
like Java , , C# , , SQL , and AspectJ .
Different from traditional software, DL deﬁnes a novel datadriven programming paradigm with different software representations, causing the mutation operators deﬁned for traditional software unable to be directly applied to DL based
software. To the best of our knowledge, DeepMutation is the
ﬁrst to propose mutation testing frameworks for DL systems,
with the design of both source-level and model-level mutators.
Besides the design of mutation operators, great efforts have
also been devoted to other key issues of mutation testing,
such as theoretical aspects – of mutation testing,
performance enhancement , , – , platform and
tool support , – , as well as more general mutation testing applications for test generation , , ,
networks , . We refer interesting readers to a recent
comprehensive survey on mutation testing .
B. Testing and Veriﬁcation of DL Systems
Testing. Testing machine learning systems mainly relies on
probing the accuracy on test data which are randomly drawn
from manually labeled datasets and ad hoc simulations .
DeepXplore proposes a white-box differential testing
algorithm to systematically generate adversarial examples that
cover all neurons in the network. By introducing the deﬁnition
of neuron coverage, they measure how much of the internal
logic of a DNN has been tested. DeepCover proposes
the test criteria for DNNs, adapted from the MC/DC test
criteria of traditional software. Their test criteria have
only been evaluated on small scale neural networks (with only
Dense layers, and at most 5 hidden layers, and no more than
400 neurons). The effectiveness of their test criteria remain
unknown on real-world-sized DL systems with multiple types
of layers. DeepGauge proposes multi-granularity testing
coverage for DL systems, which is based on the observation
of DNNs’ internal state. Their testing criteria shows to be a
promising as a guidance for effective test generation, which is
also scalable to complex DNNs like ResNet-50 (with hundreds
of layers and approximately 100, 000 neurons). Considering
the high the dimension and large potential testing space of a
DNN, DeepCT proposes a set of combinatorial testing
criteria based on the neuron input interaction for each layer
of DNNs, towards balancing the defect detection ability and a
reasonable number of tests.
Veriﬁcation. Another interesting avenue is to provide reliable
guarantees on the security of deep learning systems by formal
veriﬁcation. The abstraction-reﬁnement approach in veri-
ﬁes safety properties of a neural network with 6 neurons. DLV
 enables to verify local robustness of deep neural networks.
Reluplex adopts an SMT-based approach that veriﬁes
safety and robustness of deep neural networks with ReLU
activation functions. Reluplex has demonstrated its usefulness
on a network with 300 ReLU nodes in . DeepSafe 
uses Reluplex as its underlying veriﬁcation component to
identify safe regions in the input space. AI2 proposes the
veriﬁcation of DL systems based on abstract interpretation,
and designs the speciﬁc abstract domains and transformation
operators. VERIVIS is able to verify safety properties of
deep neural networks when inputs are modiﬁed through given
transformation functions. But the transformation functions in
 are still simpler than potential real-world transformations.
The existing work of formal veriﬁcation shows that formal technique for DNNs is promising – . However,
most veriﬁcation techniques were demonstrated only on simple DNNs network architectures. Designing more scalable
and general veriﬁcation methods towards complex real-world
DNNs would be important research directions.
DeepMutation originally proposes to use mutation testing to
systematically evaluate the test data quality of DNNs, which
is mostly orthogonal to these existing testing and veriﬁcation
techniques.
VII. CONCLUSION AND FUTURE WORK
In this paper, we have studied the usefulness of mutation
testing techniques for DL systems. We ﬁrst proposed a sourcelevel mutation testing technique that works on training data
and training programs. We then designed a set of source-level
mutation operators to inject faults that could be potentially
introduced during the DL development process. In addition, we
also proposed a model-level mutation testing technique and designed a set of mutation operators that directly inject faults into
DL models. Furthermore, we proposed the mutation testing
metrics to measure the quality of test data. We implemented
the proposed mutation testing framework DeepMutation and
demonstrated its usefulness on two popular datasets, MNIST
and CIFAR-10, with three DL models.
Mutation testing is a well-established technique for the test
data quality evaluation in traditional software and has also
been widely applied to many application domains. We believe
that mutation testing is a promising technique that could
facilitate DL developers to generate higher quality test data.
The high-quality test data would provide more comprehensive
feedback and guidance for further in-depth understanding
and constructing DL systems. This paper performs an initial
exploratory attempt to demonstrate the usefulness of mutation
testing for deep learning systems. In future work, we will
perform a more comprehensive study to propose advanced
mutation operators to cover more diverse aspects of DL
systems and investigate the relations of the mutation operators,
as well as how well such mutation operators introduce faults
comparable to human faults. Furthermore, we will also investigate novel mutation testing guided automated testing, attack
and defense, as well as repair techniques for DL systems.
ACKNOWLEDGEMENTS
This work was partially supported by National Key R&D
Program of China 2017YFC1201200 and 2017YFC0907500,
Fundamental Research Funds for Central Universities of China
AUGA5710000816, JSPS KAKENHI Grant 18H04097. We
gratefully acknowledge the support of NVIDIA AI Tech
Center (NVAITC) to our research. We also appreciate Cheng
Zhang, Jie Zhang, and the anonymous reviewers for their
insightful and constructive comments.