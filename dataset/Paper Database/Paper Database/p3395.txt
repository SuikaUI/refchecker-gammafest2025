Discrete Signal Processing on Graphs
Aliaksei Sandryhaila, Member, IEEE and Jos´e M. F. Moura, Fellow, IEEE
Abstract—In social settings, individuals interact through webs
of relationships. Each individual is a node in a complex network
(or graph) of interdependencies and generates data, lots of data.
We label the data by its source, or formally stated, we index
the data by the nodes of the graph. The resulting signals (data
indexed by the nodes) are far removed from time or image
signals indexed by well ordered time samples or pixels. DSP,
discrete signal processing, provides a comprehensive, elegant,
and efﬁcient methodology to describe, represent, transform,
analyze, process, or synthesize these well ordered time or image
signals. This paper extends to signals on graphs DSP and its
basic tenets, including ﬁlters, convolution, z-transform, impulse
response, spectral representation, Fourier transform, frequency
response, and illustrates DSP on graphs by classifying blogs,
linear predicting and compressing data from irregularly located
weather stations, or predicting behavior of customers of a mobile
service provider.
Keywords: Network science, signal processing, graphical
models, Markov random ﬁelds, graph Fourier transform.
I. INTRODUCTION
There is an explosion of interest in processing and analyzing
large datasets collected in very different settings, including
social and economic networks, information networks, internet
and the world wide web, immunization and epidemiology
networks, molecular and gene regulatory networks, citation
and coauthorship studies, friendship networks, as well as
physical infrastructure networks like sensor networks, power
grids, transportation networks, and other networked critical
infrastructures. We brieﬂy overview some of the existing work.
Many authors focus on the underlying relational structure
of the data by: 1) inferring the structure from community
relations and friendships, or from perceived alliances between
agents as abstracted through game theoretic models , ;
2) quantifying the connectedness of the world; and 3) determining the relevance of particular agents, or studying the
strength of their interactions. Other authors are interested
in the network function by quantifying the impact of the
network structure on the diffusion of disease, spread of news
and information, voting trends, imitation and social inﬂuence,
crowd behavior, failure propagation, global behaviors developing from seemingly random local interactions , , .
Much of these works either develop or assume network models
that capture the interdependencies among the data and then
analyze the structural properties of these networks. Models
often considered may be deterministic like complete or regular
Copyright (c) 2012 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to .
This work was supported in part by AFOSR grant FA95501210087.
A. Sandryhaila and J. M. F. Moura are with the Department of Electrical
Engineering,
University,
Pittsburgh, PA 15213-3890. Ph: (412)268-6341; fax: (412)268-3890. Email:
 , .
graphs, or random like the Erd˝os-R´enyi and Poisson graphs,
the conﬁguration and expected degree models, small world or
scale free networks , , to mention a few. These models
are used to quantify network characteristics, such as connectedness, existence and size of the giant component, distribution
of component sizes, degree and clique distributions, and node
or edge speciﬁc parameters including clustering coefﬁcients,
path length, diameter, betweenness and closeness centralities.
Another body of literature is concerned with inference and
learning from such large datasets. Much work falls under
the generic label of graphical models , , , , ,
 . In graphical models, data is viewed as a family of
random variables indexed by the nodes of a graph, where
the graph captures probabilistic dependencies among data
elements. The random variables are described by a family of
joint probability distributions. For example, directed (acyclic)
graphs , represent Bayesian networks where each
random variable is independent of others given the variables
deﬁned on its parent nodes. Undirected graphical models, also
referred to as Markov random ﬁelds , , describe data
where the variables deﬁned on two sets of nodes separated by
a boundary set of nodes are statistically independent given the
variables on the boundary set. A key tool in graphical models
is the Hammersley-Clifford theorem , , , and the
Markov-Gibbs equivalence that, under appropriate positivity
conditions, factors the joint distribution of the graphical model
as a product of potentials deﬁned on the cliques of the graph.
Graphical models exploit this factorization and the structure
of the indexing graph to develop efﬁcient algorithms for
inference by controlling their computational cost. Inference
in graphical models is generally deﬁned as ﬁnding from the
joint distributions lower order marginal distributions, likelihoods, modes, and other moments of individual variables or
their subsets. Common inference algorithms include belief
propagation and its generalizations, as well as other message
passing algorithms. A recent block-graph algorithm for fast
approximate inference, in which the nodes are non-overlapping
clusters of nodes from the original graph, is in . Graphical
models are employed in many areas; for sample applications,
see and references therein.
Extensive work is dedicated to discovering efﬁcient data
representations for large high-dimensional data , ,
 , . Many of these works use spectral graph theory and
the graph Laplacian to derive low-dimensional representations by projecting the data on a low-dimensional subspace
generated by a small subset of the Laplacian eigenbasis. The
graph Laplacian approximates the Laplace-Beltrami operator
on a compact manifold , , in the sense that if the
dataset is large and samples uniformly randomly a lowdimensional manifold then the (empirical) graph Laplacian
acting on a smooth function on this manifold is a good discrete
approximation that converges pointwise and uniformly to the
elliptic Laplace-Beltrami operator applied to this function as
the number of points goes to inﬁnity , , . One
can go beyond the choice of the graph Laplacian by choosing discrete approximations to other continuous operators
and obtaining possibly more desirable spectral bases for the
characterization of the geometry of the manifold underlying
the data. For example, if the data represents a non-uniform
sampling of a continuous manifold, a conjugate to an elliptic
Schr¨odinger-type operator can be used , , .
More in line with our paper, several works have proposed
multiple transforms for data indexed by graphs. Examples include regression algorithms , wavelet decompositions ,
 , , , , ﬁlter banks on graphs , , denoising , and compression . Some of these transforms
focus on distributed processing of data from sensor ﬁelds
while addressing sampling irregularities due to random sensor
placement. Others consider localized processing of signals on
graphs in multiresolution fashion by representing data using
wavelet-like bases with varying “smoothness” or deﬁning
transforms based on node neighborhoods. In the latter case,
the graph Laplacian and its eigenbasis are sometimes used
to deﬁne a spectrum and a Fourier transform of a signal on a
graph. This deﬁnition of a Fourier transform was also proposed
for use in uncertainty analysis on graphs , . This graph
Fourier transform is derived from the graph Laplacian and
restricted to undirected graphs with real, non-negative edge
weights, not extending to data indexed by directed graphs or
graphs with negative or complex weights.
The algebraic signal processing (ASP) theory , ,
 , is a formal, algebraic approach to analyze data
indexed by special types of line graphs and lattices. The
theory uses an algebraic representation of signals and ﬁlters
as polynomials to derive fundamental signal processing concepts. This framework has been used for discovery of fast
computational algorithms for discrete signal transforms ,
 , . It was extended to multidimensional signals and
nearest neighbor graphs , and applied in signal
compression , . The framework proposed in this paper
generalizes and extends the ASP to signals on arbitrary graphs.
Contribution
Our goal is to develop a linear discrete signal processing
(DSP) framework and corresponding tools for datasets arising
from social, biological, and physical networks. DSP has been
very successful in processing time signals (such as speech,
communications, radar, or econometric time series), spacedependent signals (images and other multidimensional signals
like seismic and hyperspectral data), and time-space signals
(video). We refer to data indexed by nodes of a graph as
a graph signal or simply signal and to our approach as
DSP on graphs (DSPG)1. We introduce the basics of linear2
1The term “signal processing for graphs” has been used in , in
reference to graph structure analysis and subgraph detection. It should not be
confused with our proposed DSP framework, which aims at the analysis and
processing of data indexed by the nodes of a graph.
2We are concerned with linear operations; in the sequel, we refer only to
DSPG but have in mind that we are restricted to linear DSPG.
DSPG, including the notion of a shift on a graph, ﬁlter
structure, ﬁltering and convolution, signal and ﬁlter spaces
and their algebraic structure, the graph Fourier transform,
frequency, spectrum, spectral decomposition, and impulse and
frequency responses. With respect to other works, ours is a
deterministic framework to signal processing on graphs rather
than a statistical approach like graphical models. Our work
is an extension and generalization of the traditional DSP,
and generalizes the ASP theory , , , and its
extensions and applications , , . We emphasize
the contrast between the DSPG and the approach to the graph
Fourier transform that takes the graph Laplacian as a point of
departure , , , , , . In the latter case,
the Fourier transform on graphs is given by the eigenbasis of
the graph Laplacian. However, this deﬁnition is not applicable
to directed graphs, which often arise in real-world problems,
as demonstrated by examples in Section VI, and graphs with
negative weights. In general, the graph Laplacian is a secondorder operator for signals on a graph, whereas an adjacency
matrix is a ﬁrst-order operator. Deriving a graph Fourier transform from the graph Laplacian is analogous in traditional DSP
to restricting signals to be even (like correlation sequences)
and Fourier transforms to represent power spectral densities
of signals. Instead, we demonstrate that the graph Fourier
transform is properly deﬁned through the Jordan normal form
and generalized eigenbasis of the adjacency matrix3. Finally,
we illustrate the DSPG with applications like classiﬁcation,
compression, and linear prediction for datasets that include
blogs, customers of a mobile operator, or collected by a
network of irregularly placed weather stations.
II. SIGNALS ON GRAPHS
Consider a dataset with N elements, for which some relational information about its data elements is known. Examples
include preferences of individuals in a social network and
their friendship connections, the number of papers published
by authors and their coauthorship relations, or topics of
online documents in the World Wide Web and their hyperlink
references. This information can be represented by a graph
G = (V, A), where V = {v0, . . . , vN−1} is the set of nodes
and A is the weighted4 adjacency matrix of the graph. Each
dataset element corresponds to node vn, and each weight
An,m of a directed edge from vm to vn reﬂects the degree
of relation of the nth element to the mth one. Since data
elements can be related to each other differently, in general,
G is a directed, weighted graph. Its edge weights An,m are not
restricted to being nonnegative reals; they can take arbitrary
real or complex values (for example, if data elements are
negatively correlated). The set of indices of nodes connected
to vn is called the neighborhood of vn and denoted by
Nn = {m | An,m ̸= 0}.
3 Parts of this material also appeared in , . In this paper, we present
a complete theory with all derivations and proofs.
4Some literature deﬁnes the adjacency matrix A of a graph G = (V, A)
so that An,m only takes values of 0 or 1, depending on whether there is an
edge from vm to vn, and speciﬁes edge weights as a function on pairs of
nodes. In this paper, we incorporate edge weights into A.
(a) Time series
(b) Digital image
(c) Sensor ﬁeld
(d) Hyperlinked documents
Graph representations for different datasets (graph signals.)
Assuming, without a loss of generality, that dataset elements
are complex scalars, we deﬁne a graph signal as a map from
the set V of nodes into the set of complex numbers C:
Notice that each signal is isomorphic to a complex-valued
vector with N elements. Hence, for simplicity of discussion,
we write graph signals as vectors s =
but remember that each element sn is indexed by node vn of
a given representation graph G = (V, A), as deﬁned by (1).
The space S of graphs signals (1) then is identical to CN.
We illustrate representation graphs with examples shown
in Fig. 1. The directed cyclic graph in Fig. 1(a) represents a
ﬁnite, periodic discrete time series . All edges are directed
and have the same weight 1, reﬂecting the causality of a time
series; and the edge from vN−1 to v0 reﬂects its periodicity.
The two-dimensional rectangular lattice in Fig. 1(b) represents
a general digital image. Each node corresponds to a pixel, and
each pixel value (intensity) is related to the values of the four
adjacent pixels. This relation is symmetric, hence all edges are
undirected and have the same weight, with possible exceptions
of boundary nodes that may have directed edges and/or different edge weights, depending on boundary conditions .
Other lattice models can be used for images as well .
The graph in Fig. 1(c) represents temperature measurements
from 150 weather stations (sensors) across the United States.
We represent the relations of temperature measurements by
geodesic distances between sensors, so each node is connected
to its closest neighbors. The graph in Fig. 1(d) represents a
set of 50 political blogs in the World Wide Web connected by
hyperlink references. By their nature, the edges are directed
and have the same weights. We discuss the two latter examples
in Section VI, where we also consider a network of customers
of a mobile service provider. Clearly, representation graphs
depend on prior knowledge and assumptions about datasets.
For example, the graph in Fig. 1(d) is obtained by following
the hyperlinks networking the blogs, while the graph in
Fig. 1(c) is constructed from known locations of sensors under
assumption that temperature measurements at nearby sensors
have highly correlated temperatures.
III. FILTERS ON GRAPHS
In classical DSP, signals are processed by ﬁlters—systems
that take a signal as input and produce another signal as output.
We now develop the equivalent concept of graph ﬁlters for
graph signals in DSPG. We consider only linear, shift-invariant
ﬁlters, which are a generalization of linear time-invariant ﬁlters
used in DSP for time series. This section uses Jordan normal
form and characteristic and minimal polynomials of matrices;
these concepts are reviewed in Appendix A. The use of Jordan
decomposition is required since for many real-world datasets
the adjacency matrix A is not diagonalizable. One example is
the blog dataset, considered in Section VI.
Graph Shift
In classical DSP, the basic building block of ﬁlters is a
special ﬁlter x = z−1 called the time shift or delay . This
is the simplest non-trivial ﬁlter that delays the input signal s
by one sample, so that the nth sample of the output is esn =
sn−1 mod N. Using the graph representation of ﬁnite, periodic
time series in Fig. 1(a), for which the adjacency matrix is the
N × N circulant matrix A = CN, with weights , 
if n −m = 1
we can write the time shift operation as
˜s = CN s = As.
In DSPG, we extend the notion of the shift (3) to general
graph signals s where the relational dependencies among the
data are represented by an arbitrary graph G = (V, A). We
call the operation (3) the graph shift. It is realized by replacing
the sample sn at node vn with the weighted linear combination
of the signal samples at its neighbors:
Note that, in classical DSP, shifting a ﬁnite signal requires
one to consider boundary conditions. In DSPG, this problem
is implicitly resolved, since the graph G = (V, A) explicitly
captured the boundary conditions.
Graph Filters
Similarly to traditional DSP, we can represent ﬁltering
on a graph using matrix-vector multiplication. Any system
H ∈CN×N, or graph ﬁlter, that for input s ∈S produces
output Hs represents a linear system, since the ﬁlter’s output
for a linear combination of input signals equals the linear
combination of outputs to each signal:
H(αs1 + βs2) = αHs1 + βHs2.
Furthermore, we focus on shift-invariant graph ﬁlters, for
which applying the graph shift to the output is equivalent to
applying the graph shift to the input prior to ﬁltering:
The next theorem establishes that all linear, shift-invariant
graph ﬁlters are given by polynomials in the shift A.
Theorem 1: Let A be the graph adjacency matrix and
assume that its characteristic and minimal polynomials are
equal: pA(x) = mA(x). Then, a graph ﬁlter H is linear and
shift invariant if and only if (iff) H is a polynomial in the
graph shift A, i.e., iff there exists a polynomial
h(x) = h0 + h1x + . . . + hLxL
with possibly complex coefﬁcients hℓ∈C, such that:
H = h(A) = h0 I +h1A + . . . + hLAL.
Proof: Since the shift-invariance condition (5) holds for
all graph signals s ∈S = CN, the matrices A and H
commute: AH = HA. As pA(x) = mA(x), all eigenvalues
of A have exactly one eigenvector associated with them, ,
 . Then, the graph matrix H commutes with the shift A iff
it is a polynomial in A (see Proposition 12.4.1 in ).
Analogous to the classical DSP, we call the coefﬁcients hℓ
of the polynomial h(x) in (6) the graph ﬁlter taps.
Properties of Graph Filters
Theorem 1 requires the equality of the characteristic and
minimal polynomials pA(x) and mA(x). This condition does
not always hold, but can be successfully addressed through
the concept of equivalent graph ﬁlters, as deﬁned next.
Deﬁnition 1: Given any shift matrices A and eA, ﬁlters
h(A) and g( eA) are called equivalent if for all input signals
s ∈S they produce equal outputs: h(A)s = g( eA)s.
Note that, when no restrictions are placed on the signals,
so that S = CN, Deﬁnition 1 is equivalent to requiring
h(A) = g( eA) as matrices. However, if additional restrictions
exist, ﬁlters may not necessarily be equal as matrices and still
produce the same output for the considered set of signals.
It follows that, given an arbitrary G = (V, A) with pA(x) ̸=
mA(x), we can consider another graph eG = (V, eA) with the
same set of nodes V but potentially different edges and edge
weights, for which p e
A(x) = m e
A(x) holds true. Then graph
ﬁlters on G can be expressed as equivalent ﬁlters on eG, as
described by the following theorem (proven in Appendix B).
Theorem 2: For any matrix A there exists a matrix eA and
polynomial r(x), such that A = r( eA) and p e
A(x) = m e
As a consequence of Theorem 2, any ﬁlter on the graph
G = (V, A) is equivalent to a ﬁlter on the graph eG = (V, eA),
since h(A) = h(r( eA)) = (h ◦r)( eA), where h ◦r is the
composition of polynomials h and r and thus is a polynomial.
Thus, the condition pA(x) = mA(x) in Theorem 1 can be
assumed to hold for any graph G = (V, A). Otherwise, by
Theorem 2, we can replace the graph by another eG = (V, eA)
for which the condition holds and assign eA to A.
The next result demonstrates that we can limit the number
of taps in any graph ﬁlter.
Theorem 3: Any graph ﬁlter (7) has a unique equivalent
ﬁlter on the same graph with at most deg mA(x) = NA taps.
Proof: Consider the polynomials h(x) in (6). By polynomial division, there exist unique polynomials q(x) and r(x):
h(x) = q(x)mA(x) + r(x),
where deg r(x) < NA. Hence, we can express (7) as
h(A) = q(A)mA(A) + r(A) = q(A) 0N +r(A) = r(A).
Thus, h(A) = r(A) and deg r(x) < deg mA(x).
As follows from Theorem 3, all linear, shift-invariant ﬁlters (7) on a graph G = (V, A) form a vector space
Moreover, addition and multiplication of ﬁlters in F produce
new ﬁlters that are equivalent to ﬁlters in F. Thus, F is closed
under these operations, and has the structure of an algebra .
We discuss it in detail in Section IV.
Another consequence of Theorem 3 is that the inverse of a
ﬁlter on a graph, if it exists, is also a ﬁlter on the same graph,
i.e., it is a polynomial in (9).
Theorem 4: A graph ﬁlter H = h(A) ∈F is invertible
iff polynomial h(x) satisﬁes h(λm) ̸= 0 for all distinct
eigenvalues λ0, . . . , λM−1, of A. Then, there is a unique
polynomial g(x) of degree deg g(x) < NA that satisﬁes
h(A)−1 = g(A) ∈F.
Appendix C contains the proof and the procedure for the
construction of g(x). Theorem 4 implies that instead of
inverting the N × N matrix h(A) directly we only need to
construct a polynomial g(x) speciﬁed by at most NA taps.
Finally, it follows from Theorem 3 and (9) that any
graph ﬁlter h(A) ∈F is completely speciﬁed by its taps
h0, · · · , hNA−1. As we prove next, in DSPG, as in traditional
DSP, ﬁlter taps uniquely determine the impulse response of
the ﬁlter, i.e., its output u = (g0, . . . , gN−1)T for unit impulse
input δ = (1, 0, . . . , 0)T , and vice versa.
Theorem 5: The ﬁlter taps h0, . . . , hNA−1 of the ﬁlter h(A)
uniquely determine its impulse response u. Conversely, the impulse response u uniquely determines the ﬁlter taps, provided
rank bA = NA, where bA =
 A0δ, . . . , ANA−1δ
Proof: The ﬁrst part follows from the deﬁnition of ﬁltering: u = h(A)δ = bAh yields the ﬁrst column of h(A), which
is uniquely determined by the taps h = (h0, . . . , hNA−1)T .
Since we assume pA(x) = mA(x), then N = NA, and the
second part holds if bA is invertible, i.e., rank bA = NA.
Notice that a relabeling of the nodes v0, . . . , vN−1 does not
change the impulse response. If P is the corresponding permutation matrix, then the unit impulse is Pδ, the adjacency matrix
is PAPT , and the ﬁlter becomes h(PAPT ) = Ph(A)PT .
Hence, the impulse response is simply reordered according to
same permutation: Ph(A)PT Pδ = Pu.
IV. ALGEBRAIC MODEL
So far, we presented signals and ﬁlters on graphs as vectors
and matrices. An alternative representation exists for ﬁlters and
signals as polynomials. We call this representation the graph
z-transform, since, as we show, it generalizes the traditional
z-transform for discrete time signals that maps signals and
ﬁlters to polynomials or series in z−1. The graph z-transform
is deﬁned separately for graph ﬁlters and signals.
Consider a graph G = (V, A), for which the characteristic
and minimal polynomials of the adjacency matrix coincide:
pA(x) = mA(x). The mapping A 7→x of the adjacency
matrix A to the indeterminate x maps the graph ﬁlters H =
h(A) in F to polynomials h(x). By Theorem 3, the ﬁlter
space F in (9) becomes a polynomial algebra 
A = C[x]/mA(x).
This is a space of polynomials of degree less than deg mA(x)
with complex coefﬁcients that is closed under addition and
multiplication of polynomials modulo mA(x). The mapping
F →A, h(A) 7→h(x), is a isomorphism of C-algebras ,
which we denote as F ∼= A. We call it the graph z-transform
of ﬁlters on graph G = (V, A).
The signal space S is a vector space that is also closed
under ﬁltering, i.e., under multiplication by graph ﬁlters from
F: for any signal s ∈S and ﬁlter h(A), the output is a signal
in the same space: h(A)s ∈S. Thus, S is an F-module .
As we show next, the graph z-transform of signals is deﬁned
as an isomorphism (13) from S to an A-module.
Theorem 6: Under the above conditions, the signal space S
is isomorphic to an A-module
M = C[x]/pA(x) =
under the mapping
s = (s0, . . . , sN−1)T 7→s(x) =
The polynomials b0(x), . . . , bN−1(x) are linearly independent
polynomials of degree at most N −1. If we write
b(x) = (b0(x), . . . , bN−1(x))T ,
then the polynomials satisfy
b(r)(λm) =
= r!˜vm,0,r
for 0 ≤r < Rm,0 and 0 ≤m < M, where λm and ˜vm,0,r
are generalized eigenvectors of AT , and b(r)
n (λm) denotes the
rth derivative of bn(x) evaluated at x = λm. Filtering in M
is performed as multiplication modulo pA(x): if ˜s = h(A)s,
˜s 7→˜s(x) =
˜snbn(x) = h(x)s(x)
mod pA(x). (16)
Proof: Due to the linearity and shift-invariance of graph
ﬁlters, we only need to prove (16) for h(A) = A. Let us write
s(x) = b(x)T s and ˜s(x) = b(x)T˜s, where b(x) is given
by (14). Since (16) must hold for all s ∈S, for h(A) = A it
is equivalent to
b(x)T˜s = b(x)T (As) = b(x)T (xs)
b(x) = cpA(x),
where c ∈CN is a vector of constants, since deg pA(x) = N
and deg (xbn(x)) ≤N for 0 ≤n < N.
It follows from the factorization (43) of pA(x) that, for each
eigenvalue λm and 0 ≤k < Am, the characteristic polynomial
satisﬁes p(k)
A (λm) = 0. By taking derivatives of both sides
of (17) and evaluating at x = λm, 0 ≤m < M, we construct
A0 + . . . + AM−1 = N linear equations
rb(λm), 1 ≤r < Am
Comparing these equations with (35), we obtain (15). Since N
polynomials bn(x) = bn,0 + . . . + bn,N−1xN−1 are characterized by N 2 coefﬁcients bn,k, 0 ≤n, k < N, (15) is a system
of N linear equations with N 2 unknowns that can always be
solved using inverse polynomial interpolation .
Theorem 6 extends to the general case pA(x) ̸= mA(x).
By Theorem 2, there exists a graph
eG = (V, eA) with
A(x) = m e
A(x), such that A = r( eA). By mapping eA to x,
the ﬁlter space (9) has the structure of the polynomial algebra
A = C[x]/mA(r(x)) = C[x]/(mA ◦r)(x)) and the signal
space has the structure of the A-module M = C[x]/p e
Multiplication of ﬁlters and signals is performed modulo
A(x). The basis of M satisﬁes (15), where λm and vm,d,r
are eigenvalues and generalized eigenvectors of eA.
V. FOURIER TRANSFORM ON GRAPHS
After establishing the structure of ﬁlter and signal spaces in
DSPG, we deﬁne other fundamental DSP concepts, including
spectral decomposition, signal spectrum, Fourier transform,
and frequency response. They are related to the Jordan normal
form of the adjacency matrix A, reviewed in Appendix A.
Spectral Decomposition
In DSP, spectral decomposition refers to the identiﬁcation
of subspaces S0, . . . , SK−1 of the signal space S that are
invariant to ﬁltering, so that, for any signal sk ∈Sk and ﬁlter
h(A) ∈F, the output esk = h(A)sk lies in the same subspace
Sk. A signal s ∈S can then be represented as
s = s0 + s1 + . . . + sK−1,
with each component sk ∈Sk. Decomposition (18) is uniquely
determined for every signal s ∈S if and only if: 1) invariant
subspaces Sk have zero intersection, i.e., Sk ∩Sm = {0} for
k ̸= m; 2) dim S0 + . . . + dim SK−1 = dim S = N; and
3) each Sk is irreducible, i.e., it cannot be decomposed into
smaller invariant subspaces. In this case, S is written as a
direct sum of vector subspaces
S = S0 ⊕S1 ⊕. . . ⊕SK−1.
As mentioned, since the graph may have arbitrary structure, the adjacency matrix A may not be diagonalizable;
in fact, A for the blog dataset (see Section VI) is not
diagonalizable. Hence, we consider the Jordan decomposition (39) A = V J V−1, which is reviewed in Appendix
A. Here, J is the Jordan normal form (40), and V is
the matrix of generalized eigenvectors (38). Let Sm,d =
span{vm,d,0, . . . , vm,d,Rm,d−1} be a vector subspace of S
spanned by the dth Jordan chain of λm. Any signal sm,d ∈
Sm,d has a unique expansion
ˆsm,d,0vm,d,0 + . . . + ˆsm,d,Rm,d−1vm,d,Rm,d−1
ˆsm,d,Rm,d−1
where Vm,d is the block of generalized eigenvectors (37).
As follows from the Jordan decomposition (39), shifting the
signal sm,d produces the output bsm,d ∈Sm,d from the same
subspace, since
Asm,d = A Vm,d
ˆsm,d,Rm,d−1
Vm,d JRm,d(λm)
ˆsm,d,Rm,d−1
λmˆsm,d,0 + ˆsm,d,1
λmˆsm,d,Rm,d−2 + ˆsm,d,Rm,d−1
λmˆsm,d,Rm,d−1
Hence, each subspace Sm,d ≤S is invariant to shifting.
Using (39) and Theorem 1, we write the graph ﬁlter (7) as
hℓ(V J V−1)ℓ
V−1 = V h(J) V−1 .
Similarly to (20), we observe that ﬁltering a signal sm,d ∈
Sm,d produces an output bsm,d ∈Sm,d from the same subspace:
h(A)sm,d = h(A) Vm,d
ˆsm,d,Rm,d−1
h(JRm,d(λm))
ˆsm,d,Rm,d−1
Since all N generalized eigenvectors of A are linearly independent, all subspaces Sm,d have zero intersections, and their
dimensions add to N. Thus, the spectral decomposition (19)
of the signal space S is
Graph Fourier Transform
The spectral decomposition (23) expands each signal s ∈S
on the basis of the invariant subspaces of S. Since we chose
the generalized eigenvectors as bases of the subspaces Sm,d,
the expansion coefﬁcients are given by
where V is the generalized eigenvector matrix (38). The vector
of expansion coefﬁcients is given by
bs = V−1 s.
The union of the bases of all spectral components Sm,d,
i.e., the basis of generalized eigenvectors, is called the graph
Fourier basis. We call the expansion (25) of a signal s into the
graph Fourier basis the graph Fourier transform and denote
the graph Fourier transform matrix as
Following the conventions of classical DSP, we call the
coefﬁcients ˆsn in (25) the spectrum of a signal s. The inverse
graph Fourier transform is given by (24); it reconstructs the
signal from its spectrum.
Frequency Response of Graph Filters
The frequency response of a ﬁlter characterizes its effect on
the frequency content of the input signal. Let us rewrite the
ﬁltering of s by h(A) using (21) and (24) as
es = h(A)s = F−1 h(J) F s = F−1 h(J)ˆs
Fes = h(J)ˆs.
Hence, the spectrum of the output signal is the spectrum of
the input signal modiﬁed by the block-diagonal matrix
h(Jr0,0(λ0))
h(JrM−1,DM −1(λM−1))
so that the part of the spectrum corresponding to the invariant
subspace Sm,d is multiplied by h(Jm). Hence, h(J) in (28)
represents the frequency response of the ﬁlter h(A).
Notice that (27) also generalizes the convolution theorem
from classical DSP to arbitrary graphs.
Theorem 7: Filtering a signal is equivalent, in the frequency
domain, to multiplying its spectrum by the frequency response
of the ﬁlter.
Discussion
The connection (25) between the graph Fourier transform
and the Jordan decomposition (39) highlights some desirable
properties of representation graphs. For graphs with diagonalizable adjacency matrices A, which have N linearly independent eigenvectors, the frequency response (28) of ﬁlters
h(A) reduces to a diagonal matrix with the main diagonal
containing values h(λm), where λm are the eigenvalues of
A. Moreover, for these graphs, Theorem 6 provides the
closed-form expression (15) for the inverse graph Fourier
transform F−1 = V. Graphs with symmetric (or Hermitian)
matrices, such as undirected graphs, are always diagonalizable
and, moreover, have orthogonal graph Fourier transforms:
F−1 = FH. This property has signiﬁcant practical importance,
since it yields a closed-form expression (15) for F and F−1.
Moreover, orthogonal transforms are well-suited for efﬁcient
signal representation, as we demonstrate in Section VI.
DSPG is consistent with the classical DSP theory. As
mentioned in Section II, ﬁnite discrete periodic time series
are represented by the directed graph in Fig. 1(a). The corresponding adjacency matrix is the N × N circulant matrix (2).
Its eigendecomposition (and hence, Jordan decomposition) is
e−j 2π·(N−1)
where DFTN is the discrete Fourier transform matrix. Thus,
as expected, the graph Fourier transform is F = DFTN.
Furthermore, for a general ﬁlter h(CN) = PN−1
coefﬁcients of the output bs = h(CN)s are calculated as
hns0 + . . . + h0sn + hN−1sn+1 + . . . + hn+1sN−1
This is the standard circular convolution. Theorem 5 holds as
well, with impulse response identical to ﬁlter taps: u = h.
Similarly, it has been shown in , that unweighted
line graphs similar to Fig. 1(a), but with undirected edges and
different, non-periodic boundary conditions, give rise to all
16 types of discrete cosine and sine transforms as their graph
Fourier transform matrices. Combined with , it can be
shown that graph Fourier transforms for images on the lattice
in Fig. 1(b) are different types of two-dimensional discrete
cosine and sine transforms, depending on boundary conditions.
This result serves as additional motivation for the use of these
transforms in image representation and coding .
In discrete-time DSP, the concepts of ﬁltering, spectrum,
and Fourier transform have natural, physical interpretations.
In DSPG, when instantiated for various datasets, the interpretation of these concepts may be drastically different and not
immediately obvious. For example, if a representation graph
reﬂects the proximity of sensors in some metric (such as
time, space, or geodesic distance), and the dataset contains
sensor measurements, then ﬁltering corresponds to linear recombination of related measurements and can be viewed as a
graph form of regression analysis with constant coefﬁcients.
The graph Fourier transform then decomposes signals over
equilibrium points of this regression. On the other hand, if a
graph represents a social network of individuals and their communication patterns, and the signal is a social characteristic,
such as an opinion or a preference, then ﬁltering can be viewed
as diffusion of information along established communication
channels, and the graph Fourier transform characterizes signals
in terms of stable, unchangeable opinions or preferences.
VI. APPLICATIONS
We consider several applications of DSPG to data processing. These examples illustrate the effectiveness of the
framework in standard DSP tasks, such as predictive ﬁltering
and efﬁcient data representation, as well as demonstrate that
the framework can tackle problems less common in DSP, such
as data classiﬁcation and customer behavior prediction.
Linear Prediction
Linear prediction (LP) is an efﬁcient technique for representation, transmission, and generation of time series .
It is used in many applications, including power spectral
estimation and direction of arrival analysis. Two main steps of
LP are the construction of a prediction ﬁlter and the generation
of an (approximated) signal, implemented, respectively, with
forward and backward ﬁlters, shown in Fig. 2. The forward
(prediction) ﬁlter converts the signal into a residual, which is
(a) Forward (prediction) ﬁlter
(I ¡ h(A))¡1
(b) Backward (synthesis) ﬁlter
Components of linear prediction.
then closely approximated, for example, by a white noise–ﬂat
power spectrum signal or efﬁcient quantization with few bits.
The backward (synthesis) ﬁlter constructs an approximation of
the original signal from the approximated residual.
Using the DSPG, we can extend LP to graph signals.
We illustrate it with the dataset of daily temperature
measurements from sensors located near 150 major US cities.
Data from each sensor is a separate time series, but encoding
it requires buffering measurements from multiple days before
they can be encoded for storage or transmission. Instead, we
build a LP ﬁlter on a graph to encode daily snapshots of all
150 sensor measurements.
We construct a representation graph G = (V, A) for the
sensor measurements using geographical distances between
sensors. Each sensor corresponds to a node vn, 0 ≤n < 150,
and is connected to K nearest sensors with undirected edges
weighted by the normalized inverse exponents of the squared
distances: if dnm denotes the distance between the nth and
mth sensors5 and m ∈Nn, then
For each snapshot s of N
= 150 measurements, we
construct a prediction ﬁlter h(A) with L taps by minimizing
the energy of the residual r = s −h(A)s = (IN −h(A)) s.
We set h0 = 0 to avoid the trivial solution h(A) = I, and
T = (BT B)−1BT s.
is a N × (L −1) matrix. The
residual energy ||r||2
2 is relatively small compared to the energy
of the signal s, since shifted signals are close approximations
of s, as illustrated in Fig. 3. This phenomenon provides the
intuition for the graph shift: if the graph represents a similarity
relation, as in this example, then the shift replaces each signal
sample with a sum of related samples with more similar
samples weighted heavier than less similar ones.
The residual r is then quantized using B bits, and the
quantized residual br is processed with the inverse ﬁlter to
synthesize an approximated signal bs = (IN −h(A))−1 br.
We considered graphs with 1 ≤K ≤15 nearest neighbors,
and for each K constructed optimal prediction ﬁlters with 2 ≤
L ≤10 taps. As shown in Fig. 4, the lowest and highest errors
||s −bs||2/||s||2 were obtained for K = 11 and L = 3, and for
K = 8 and L = 9. During the experiments, we observed that
graphs with few neighbors (approximately, 3 ≤K ≤7) lead to
lower errors when prediction ﬁlters have impulse responses of
5The construction of representation graphs for datasets is an important
research problem and deserves a separate discussion that is beyond the scope
of this paper. The procedure we use here is a popular choice for construction
of similarity graphs based on distances between nodes , , .
Temperature (degrees Celsius)
Sensor index
Shifted signal
Twice shifted signal
A signal representing a snapshot of temperature measurements from
N = 150 sensors. Shifting the signal produces signals similar to the original.
Bits used for quantization
Average approximation errors ||s−bs||2/||s||2 for LP coding of 365
signals s representing daily temperature snapshots. Graphs with 1 ≤K ≤15
nearest neighbors for each sensor were analyzed, and ﬁlters with 2 ≤L ≤10
taps were constructed. The residual was quantized using 1 ≤B ≤16 bits.
The lowest, second lowest, and highest errors were obtained, respectively for
K = 11 and L = 3, K = 10 and L = 3, and K = 8 and L = 9.
medium length (4 ≤L ≤6), while graphs with 7 ≤K ≤11
neighbors yield lower errors for 3 ≤L ≤5. Using larger
values of K and L leads to large errors. This tendency may
be due to overﬁtting ﬁlters to signals, and demonstrates that
there exists a trade-off between graph and ﬁlter parameters.
Signal Compression
Efﬁcient signal representation is required in multiple DSP
areas, such as storage, compression, and transmission. Some
widely-used techniques are based on expanding signals into orthonormal bases with the expectation that most information is
captured with few basis functions. The expansion coefﬁcients
are calculated using an orthogonal transform. If the transform
represents a Fourier transform in some model, it means that
signals are sparse in the frequency domain in this model, i.e.,
they contain only few frequencies. Some widely-used image
compression standards, such as JPEG and JPEG 2000, use
orthogonal expansions implemented, respectively, by discrete
cosine and wavelet transforms .
As discussed in the previous example, given a signal s on a
graph G = (V, A), where A reﬂects similarities between data
elements, the shifted signal As can be a close approximation
of s, up to a scalar factor: As ≈ρs. This is illustrated in
105 120 135 150
Number of used coefficients
Average reconstruction error ||s−˜s||2/||s||2 for the compression of
365 daily temperature snapshots based on the graph Fourier transform using
1 ≤C ≤N coefﬁcients.
The Fourier basis vector that captures most energy of temperature
measurements reﬂects the relative distribution of temperature across the
mainland United States. The coefﬁcients are normalized to the interval .
Fig. 3, where ρ ≈1. Hence, s can be effectively expressed as
a linear combination of a few [generalized] eigenvectors of A.
Consider the above dataset of temperature measurements.
The matrix A in (29) is symmetric by construction, hence
its eigenvectors form an orthonormal basis, and the graph
Fourier transform matrix F is orthogonal. In this case, we can
compress each daily update s of N = 150 measurements by
keeping only the C spectrum coefﬁcients (25) ˆsn with largest
magnitudes. Assuming that |ˆs0| ≥|ˆs1| ≥. . . ≥|ˆsN−1|, the
signal reconstructed after compression is
˜s = F−1 (ˆs0, . . . ,ˆsC−1, 0, . . . , 0)T .
Fig. 5 shows the average reconstruction errors obtained by
retaining 1 ≤C ≤N spectrum coefﬁcients.
This example also provides interesting insights into the
temperature distribution pattern in the United States. Consider
the Fourier basis vector that most frequently (for 217 days out
of 365) captures most energy of the snapshot s, i.e., yields
the spectrum coefﬁcient ˆs0 in (30). Fig. 6 shows the vector
coefﬁcients plotted on the representation graph according to
the sensors’ geographical coordinates, so the graph naturally
takes the shape of the mainland US. It can be observed that
this basis vector reﬂects the relative temperature distribution
across the US: the south-eastern region is the hottest one, and
the Great Lakes region is the coldest one .
Data Classiﬁcation
Classiﬁcation and labeling are important problems in data
analysis. These problems have traditionally been studied in
machine learning , . Here, we propose a novel data
classiﬁcation algorithm by demonstrating that a classiﬁer
system can be interpreted as a ﬁlter on a graph. Thus, the
construction of an optimal classiﬁer can be viewed and studied
as the design of an adaptive graph ﬁlter. Our algorithm scales
linearly with the data size N, which makes it an attractive
alternative to existing classiﬁcation methods based on neural
networks and support vector machines.
Our approach is based on the label propagation , ,
which is a simple, yet efﬁcient technique that advances known
labels from labeled graph nodes along edges to unlabeled
nodes. Usually this propagation is modeled as a stationary
discrete-time Markov process , and the graph adjacency
matrix is constructed as a probability transition matrix, i.e.,
An,m ≥0 for all n, m, and A 1N = 1N, where 1N is a
column vector of N ones. Initially known labels determine
the initial probability distribution s. For a binary classiﬁcation
problem with only two labels, the resulting labels are determined by the distribution ˜s = AP s. If ˜sn ≤1/2, node vn is
assigned one label, and otherwise the other. The number P of
propagations is determined heuristically.
Our DSPG approach has two major distinctions from the
original label propagation. First, we do not require A to
be a stochastic matrix. We only assume that edge weights
Ak,m ≥0 are non-negative and indicate similarity or dependency between nodes. In this case, nodes with positive labels
˜sn > 0 are assigned to one class, and with negative labels to
another. Second, instead of propagating labels as in a Markov
chain, we construct a ﬁlter h(A) that produces labels
˜s = h(A)s.
The following example illustrates our approach. Consider
a set of N = 1224 political blogs on the Web that we
wish to classify as “conservative” or “liberal” based on their
context . Reading and labeling each blog is very timeconsuming. Instead, we read and label only a few blogs, and
use these labels to adaptively build a ﬁlter h(A) in (31).
Let signal s contain initially known labels, where “conservative,” “liberal,” and unclassiﬁed blogs are represented by
values sn = +1, −1, and 0, respectively. Also, let signal t
contain training labels, a subset of known labels from s. Both
s and t are represented by a graph G = (V, A), where node
vn containing the label of the nth blog, and edge An,m = 1
if and only if there is a hyperlink reference from the nth to
the mth blog; hence the graph is directed. Observe that the
discovery of hyperlink references is a fast, easily automated
task, unlike reading the blogs. An example subgraph for 50
blogs is shown in Fig. 1(d).
Recall that the graph shift A replaces each signal coefﬁcient
with a weighted combination of its neighbors. In this case,
processing training labels t with the ﬁlter
produces new labels ˜t = t + h1At. Here, every node label
is adjusted by a scaled sum of its neighbors’ labels. The
parameter h1 can be interpreted as the “conﬁdence” in our
knowledge of current labels: the higher the conﬁdence h1, the
Blog selection method
Fraction of initially known labels
Most hyperlinks
ACCURACY OF BLOG CLASSIFICATION USING ADAPTIVE FILTERS.
more neighbors’ labels should affect the current labels. We
restrict the value of h1 to be positive.
Since the sign of each label indicates its class, label ˜tn is
incorrect if its sign differs from sn, or ˜tnsn ≤0 for sn ̸=
0. We determine the optimal value of h1 by minimizing the
total error, given by the number of incorrect and undecided
labels. This is done in linear time proportional to the number
of initially known labels sn ̸= 0, since each constraint
is a linear inequality constraint on h1.
To propagate labels to all nodes, we repeatedly feed them
through P ﬁlters (32) of the form h(p)(A) = IN +hpA, each
time optimizing the value of hp using the greedy approach
discussed above. The obtained adaptive classiﬁcation ﬁlter is
h(A) = (IN +hP A)(IN +hP −1A) . . . (IN +h1A).
In experiments, we set P
= 10, since we observed that
ﬁlter (34) converges quickly, and in many cases, hp = 0
for p > 10, which is similar to the actual graph’s diameter
of 8. After the ﬁlter (34) is constructed, we apply it to all
known labels s, and classify all N nodes based on the signs
of resulting labels ˜s = h(A)s.
In our experiments, we considered two methods for selecting nodes to be labeled initially: random selection, and
selection of blogs with most hyperlinks. As Table I shows,
our algorithm achieves high accuracy for both methods. In
particular, assigning initial labels s to only 2% of blogs with
most hyperlinks leads to the correct classiﬁcation of 93 % of
unlabeled blogs.
Customer Behavior Prediction
The adaptive ﬁlter design discussed in the previous example
can be applied to other problems as well. Moreover, the linear
computational cost of the ﬁlter design makes the approach
easily scalable for the analysis of large datasets. Consider
an example of a mobile service provider that is interested in
keeping its customers. The company wants to predict which
users will stop using their services in the near future, and offer
them incentives for staying with the provider (improved call
plan, discounted phones, etc.). In particular, based on their
past behavior, such as number and length of calls within the
network, the company wants to predict whether customers will
stop using the services in the next month.
This problem can be formulated similarly to the previous
example. In this case, the value at node vn of the representation
Accuracy (%)
The accuracy of behavior prediction for customers of a mobile
provider. Predictions for customers who stopped using the provider and those
who continued are evaluated separately, and then combined into the overall
graph G = (V, A) indicates the probability that the nth
customer will not use the provider services in the next 30
days. The weight of a directed edge from node vn to vm is
the fraction of time the nth customer called and talked to the
mth customer; i.e., if Tn,m indicates the total time the nth
customer called and talked to the mth customer in the past
until the present moment, then
The initial input signal s has sn = 1 if the customer has
already stopped using the provider, and sn = 0 otherwise.
As in the previous example, we design a classiﬁer ﬁlter (34);
we set P = 10. We then process the entire signal s with the
designed ﬁlter obtaining the output signal ˜s of the predicted
probabilities; we conclude that the nth customer will stop
using the provider if ˜sn ≥1/2, and will continue if ˜sn < 1/2.
In our preliminary experiments, we used a ten-month-long
call log for approximately 3.5 million customers of a European
mobile service provider, approximately 10% of whom stopped
using the provider during this period6. Fig. 7 shows the
accuracy of predicting customer behavior for months 3-10
using ﬁlters with at most L ≤10 taps. The accuracy reﬂects
the ratio of correct predictions for all customers, the ones
who stop using the service and the ones who continue; it is
important to correctly identify both classes, so the provider
can focus on the proper set of customers. As can be seen from
the results, the designed ﬁlters achieve high accuracy in the
prediction of customer behavior. Unsurprisingly, the prediction
accuracy increases as more information becomes available,
since we optimize the ﬁlter for month K using cumulative
information from preceding K −1 months.
VII. CONCLUSIONS
We have proposed DSPG, a novel DSP theory for datasets
whose underlying similarity or dependency relations are represented by arbitrary graphs. Our framework extends fundamental DSP structures and concepts, including shift, ﬁlters,
signal and ﬁlter spaces, spectral decomposition, spectrum,
Fourier transform, and frequency response, to such datasets
6We use a large dataset on Call Detailed Records (CDRs) from a large
mobile operator in one European country, which we call EURMO for short.
by viewing them as signals indexed by graph nodes. We
demonstrated that DSPG is a natural extension of the classical
time-series DSP theory, and traditional deﬁnitions of the above
DSP concepts and structures can be obtained using a graph
representing discrete time series. We also provided example
applications of DSPG to various social science applications,
and our experimental results demonstrated the effectiveness
of using the DSPG framework for datasets of different nature.
Acknowledgment
We thank EURMO, CMU Prof. Pedro Ferreira, and the iLab
at CMU Heinz College for granting us access to EURMO CDR
database and related discussions.
APPENDIX A: MATRIX DECOMPOSITION AND PROPERTIES
We review relevant properties of the Jordan normal form,
and the characteristic and minimal polynomial of a matrix
A ∈CN×N; for a thorough review, see , .
Jordan Normal Form
Let λ0, . . . , λM−1 denote M ≤N distinct eigenvalues of
A. Let each eigenvalue λm have Dm linearly independent
eigenvectors vm,0, . . . , vm,Dm−1. The Dm is the geometric
multiplicity of λm. Each eigenvector vm,d generates a Jordan
chain of Rm,d ≥1 linearly independent generalized eigenvectors vm,d,r, 0 ≤r < Rm,d, where vm,d,0 = vm,d, that
(A −λm I)vm,d,r = vm,d,r−1.
For each eigenvector vm,d and its Jordan chain of length
Rm,d, we deﬁne a Jordan block matrix of dimension Rm,d as
Jrm,d(λm) =
∈CRm,d×Rm,d. (36)
Thus, each eigenvalue λm is associated with Dm Jordan
blocks, each with dimension Rm,d, 0 ≤d < Dm. Next,
for each eigenvector vm,d, we collect its Jordan chain into
a N × Rm,d matrix
vm,d,Rm,d−1
We concatenate all blocks Vm,d, 0 ≤d < Dm and 0 ≤m <
M, into one block matrix
so that Vm,d is at position Pm−1
k=0 Dk+d in this matrix. Then,
matrix A has the Jordan decomposition
A = V J V−1,
where the block-diagonal matrix
JRM−1,DM−1 (λM−1)
is called the Jordan normal form of A.
Minimal and Characteristic Polynomials
The minimal polynomial of matrix A is the monic polynomial of smallest possible degree that satisﬁes mA(A) = 0N.
Let Rm = max{Rm,0, . . . , Rm,Dm−1} denote the maximum
length of Jordan chains corresponding to eigenvalue λm. Then
the minimal polynomial mA(x) is given by
mA(x) = (x −λ0)R1 . . . (x −λM−1)RM−1.
The index of λm is Rm, 1 ≤m < M. Any polynomial
p(x) that satisﬁes p(A) = 0N, is a polynomial multiple of
mA(x), i.e., p(x) = q(x)mA(x). The degree of the minimal
polynomial satisﬁes
deg mA(x) = NA =
The characteristic polynomial of the matrix A is deﬁned as
pA(x) = det(λ I −A) = (x −λ0)A0 . . . (x −λM−1)AM−1.
Here: Am = Rm,0 + . . . + Rm,Dm−1 for 0 ≤m < M, is
the algebraic multiplicity of λm; deg pA(x) = N; pA(x) is
a multiple of mA(x); and pA(x) = mA(x) if and only if
the geometric multiplicity of each λm, Dm = 1, i.e., each
eigenvalue λm has exactly one eigenvector.
APPENDIX B: PROOF OF THEOREM 2
We will use the following lemma to prove Theorem 2.
Lemma 1: For polynomials h(x), g(x), and p(x)
h(x)g(x), and a Jordan block Jr(λ) as in (36) of arbitrary
dimension r and eigenvalue λ, the following equality holds:
h(Jr(λ))g(Jr(λ)) = p(Jr(λ)).
Proof: The (i, j)th element of h(Jr(λ)) is
h(Jr(λ))i,j =
(j −i)!h(j−i)(λ)
for j ≥i and 0 otherwise, where h(j−i)(λ) is the (j −
i)th derivative of h(λ) . Hence, the (i, j)th element of
h(Jr(λ))g(Jr(λ)) for j < i is zero and for j ≥i is
h(Jr(λ))i,kg(Jr(λ))k,j
(k −i)!h(k−i)(λ)
(j −k)!g(j−k)(λ)
h(k−i)(λ)g(j−k)(λ)
h(m)(λ)g(j−i−m)(λ)
Matrix equality (44) follows by comparing (46) with (45).
As before, let λ0, . . . , λM−1 denote distinct eigenvalues of
A. Consider the Jordan decomposition (39) of A. For each
0 ≤m < M, select distinct numbers eλm,0, . . . , eλm,Dm−1, so
that all eλm,d for 0 ≤d < Dm and 0 ≤m < M are distinct.
Construct the block-diagonal matrix
JR0,0(eλ0,0)
JRM−1,DM−1 (eλM−1,DM−1−1)
The Jordan blocks on the diagonal of eJ match the sizes of the
Jordan blocks of J in (40), but their elements are different.
Consider a polynomial r(x) = r0 +r1x+. . .+rN−1xN−1,
and assume that r(eJ) = J. By Lemma 1, this is equivalent to
r(eλm,d) = λm,
r(1)(eλm,d) = 1
r(i)(eλm,d) = 0, for 2 ≤i < Dm
for all 0 ≤d < Dm and 0 ≤m < M. This is a system of N
linear equations with N unknowns r0, . . . , rN−1 that can be
uniquely solved using inverse polynomial interpolation .
Using (39), we obtain A = V J V−1 = V r(eJ) V−1 =
r(V eJ V−1) = r( eA). Furthermore, since all eλm,d are distinct
numbers, their geometric multiplicities are equal to 1. As discussed in Appendix A, this is equivalent to p ˜A(x) = m ˜A(x).
APPENDIX C: PROOF OF THEOREM 4
Lemma 1 leads to the construction procedure of the inverse
polynomial g(x) of h(x), when it exists, and whose matrix
representation satisﬁes g(A)h(A) = IN. Observe that this
condition, together with (44), is equivalent to
h(λm)g(λm) = 1, for 0 ≤m ≤M −1
 h(λm)g(λm)
(i) = 0, for 1 ≤i < Rm.
Here, Rm is the degree of the factor (x −λm)Rm in the
minimal polynomial mA(λ) in (41). Since values of h(x) and
its derivatives at λm are known, (47) amount to NA linear
equations with NA unknowns. They have a unique solution
if and only if h(λm) ̸= 0 for all λm, and the coefﬁcients
g0, . . . , gMA−1 are then uniquely determined using inverse
polynomial interpolation .