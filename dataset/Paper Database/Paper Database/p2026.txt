The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Compressing Recurrent Neural Networks with Tensor Ring for Action Recognition
Yu Pan,1 Jing Xu,1 Maolin Wang,1 Jinmian Ye,1 Fei Wang,2 Kun Bai,3 Zenglin Xu1∗
1University of Electronic Science and Technology of China, Sichuan, China
Emails:{ypyupan, xujing.may, morin.w98, jinmian.y, zenglin}@gmail.com
2Weill Cornell Medical College, Cornell University, New York, NY, USA
Email: 
3Mobile Internet Group, Tencent Inc., Shenzhen, Guangdong, China
Email: 
Recurrent Neural Networks (RNNs) and their variants, such
as Long-Short Term Memory (LSTM) networks, and Gated
Recurrent Unit (GRU) networks, have achieved promising
performance in sequential data modeling. The hidden layers
in RNNs can be regarded as the memory units, which are
helpful in storing information in sequential contexts. However, when dealing with high dimensional input data, such
as video and text, the input-to-hidden linear transformation
in RNNs brings high memory usage and huge computational
cost. This makes the training of RNNs very difﬁcult. To address this challenge, we propose a novel compact LSTM
model, named as TR-LSTM, by utilizing the low-rank tensor
ring decomposition (TRD) to reformulate the input-to-hidden
transformation. Compared with other tensor decomposition
methods, TR-LSTM is more stable. In addition, TR-LSTM
can complete an end-to-end training and also provide a fundamental building block for RNNs in handling large input
data. Experiments on real-world action recognition datasets
have demonstrated the promising performance of the proposed TR-LSTM compared with the tensor-train LSTM and
other state-of-the-art competitors.
Introduction
Recurrent Neural Networks (RNNs) have achieved great
success in analyzing sequential data in various applications,
such as computer vision , natural language processing, etc.. Thanks to the ability in capturing long-range dependencies from input sequences . To address
the gradient vanishing issue which often leads to the failure of long-term memory in vanilla RNNs, advanced variants such as Gate Recurrent Unit (GRU) and Long-Short
Term Memory (LSTM) have been proposed and applied in
many learning tasks .
Despite the success, LSTMs and GRUs suffer from the
huge number of parameters, which makes the training process notoriously difﬁcult and easily over-ﬁtting. In particular, in the task of action recognition from videos, a video
frame usually forms a high-dimensional input, which makes
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
the size of the input-to-hidden matrix extremely large. For
example, in the UCF11 , a video
action recognition dataset, an RGB video clip is a frame with
a size of 160 × 120 × 3, and the dimension of the input vector fed to the vanilla RNN can be over 57,000. Assume that
the length of hidden layer vector is 256. Then, the input-tohidden layer matrix has a load of parameters up to 14 millions. Although a pre-processing feature extraction step via
deep convolutional neural networks can be utilized to obtain
static feature maps as inputs to RNNs ,
the over-parametric problem is still not fully solved.
A promising direction to reduce the parameter size is to
explore the low-rank structures in the weight matrices. Inspired from the success of tensor decomposition methods in
CNNs , various tensor
decomposition methods have been explored in RNNs . In particular, in
 , the tensor train (TT) decomposition has been applied to RNNs in an end-to-end way
to replace the input-to-hidden matrix, and achieved stateof-the-art performance in ﬁnding the low-rank structure in
RNNs. However, the restricted setting on the ranks and the
restrained order of core tensors makes TT-RNN models sensitive to parameter selection. In detail, the optimal setting of
TT-ranks is that they are small in the border cores and large
in middle cores, e.g., like an olive .
To address this issue, we propose to use the tensor ring
decomposition (TRD) to extract the
low-rank structure of the input-to-hidden matrix in RNNs.
Speciﬁcally, the input-to-hidden matrices are reshaped into
a high-dimensional tensor and then factorized using TRD.
Since this corresponding tensor ring layer automatically
models the inter-parameter correlations, the number of parameters can be much smaller than the original size of the
linear projection layer in standard RNNs. In this way, we
present a new TR-RNN model with a similar representation
power but with several orders of fewer parameters. In addition, since TRD can alleviate the strict constraints in tensor
train decomposition via interconnecting the ﬁrst and the last
core tensors circularly , we expect TR-
RNNs to have more expressive power. It is important to note
that the tensor ring layer can be optimized in an end-to-end
training, and can also be utilized as a building block into
current LSTM variants. For illustration, we implement an
LSTM with the tensor ring layer, named as TR-LSTM.1
evaluations
real-world action recognition datasets, i.e., UCF11 and
HMDB51 . For a fair comparison with
standard LSTM and TT-LSTM and BT-LSTM , we conduct experiments in an end-to-end training. As shown in Figure 7,
the proposed TR-LSTM has obtained an accuracy value of
0.869, which outperforms the results of standard LSTM (i.e.,
0.681), TT-LSTM (i.e.,0.803), and BT-LSTM (i.e., 0.856).
Meanwhile, the compression ratio over the standard LSTM
is over 34,000, which is also much higher than the compression ratio given by TT-LSTM and BT-LSTM. Moreover,
with the output by pre-trained CNN as the input to LSTMs,
TR-LSTM has outperformed most previous competitors including LSTM, TT-LSTM, BT-LSTM, and others recently
proposed action recognition methods. Since the TR layer
can be used as a building block to other LSTM-based approaches, such as the two-stream LSTM , we believe the proposed TR decomposition can be a
promising approach for action recognition, by considering
the tricks used in other state-of-the-art methods.
To handle the high dimensional input of RNNs, we introduce the tensor ring decomposition to represent the input-tohidden layer in RNNs in a compact structure. In the following, we ﬁrst present preliminaries and background of tensor
decomposition, including graphical illustrations of the tensor train decomposition and the tensor ring decomposition,
followed by our proposed LSTM model, namely, TR-LSTM.
Preliminaries and Background
In this paper, a d-order tensor, e.g., D
RL1×L2···×Ld is denoted by a boldface Euler script letter.
With all subscripts ﬁxed, each element of a tensor is expressed as: D
Dl1,l2,...ld ∈R. Given a subset of subscripts,
we can get a sub-tensor. For example, given a subset {L1 =
l1, L2 = l2}, we can obtain a sub-tensor D
Dl1,l2 ∈RL3···×Ld.
Speciﬁcally, we denote a vector by a bold lowercase letter,
e.g., v ∈RL, and matrices by bold uppercase letters, e.g.,
M ∈RL1×L2. We regard vectors and matrices as 1-order
tensors and 2-order tensors, respectively. Figure 1 draws the
tensor diagrams presenting the graphical notations and the
essential operations.
Tensor Contraction
Tensor contraction can be performed
between two tensors if some of their dimensions are
matched. For example, given two 3-order tensors AAA ∈
RL1×L2×L3 and BBB ∈RJ1×J2×J3, when L3 = J1, the contraction between these two tensors result in a tensor with the
size of L1 × L2 × J2 × J3, where the matching dimension
is reduced, as shown in Equation (1):
(AAABBB)l1,l2,j2,j3 = AAAl1,l2BBBj2,j3 =
AAAl1,l2,pBBBp,j2,j3
1Note that the tensor ring layer can also be plugged in the
vanilla RNN and GRU.
(a) a L1 × L2 matrix
(b) matrix contraction
(c) a L1×L2×L3 tensor
Figure 1: Tensor diagrams. (a) shows the graphical representation of the matrix M ∈RL1×L2, where L1 and L2
denote the matrix size. A matrix is represented by a rectangular node. (b) demonstrates the contraction between two
matrices(tensors), which is represented by an axis connecting them together and the contraction between M and N
resulting in a new matrix with shape RL1×L3. (c) presents
the graphical notation of a tensor TTT ∈RL1×L2×L3.
Figure 2: Tensor Train Decomposition: Noted that in tensor
train the rank R0 and Rd are constrained to 1, so the ﬁrst and
the last core are matrices while the inner cores are 3-order
Tensor Train Decomposition
Through the tensor train
decomposition (TTD), a high-order tensor can be decomposed as the product of a sequence of low-order tensors. For
example, a d-order tensor D
D ∈RL1×L2···×Ld can be decomposed as follows:
Dl1,l2,...,ld
l2 . . .GGG(d)
r1,r2,...,rd−1
r0,l1,r1 . . .GGG(d)
rd−1,ld,rd
where each GGG(k)
RRk−1×Lk×Rk is called a core
[R0, R1, R2, . . . , Rd], for each k ∈{0, 1, . . . , d}, 0 < rk ≤
Rk, corresponds to the complexity of tensor train decomposition. Generally, in tensor train decomposition, the constraint R0 = Rd = 1 should be satisﬁed and other ranks are
chosen manually. Figure 2 illustrates the form of tensor train
decomposition.
Tensor Ring Decomposition
The main drawback in tensor train decomposition is the limit of the ranks’ setting,
which hinders the representation ability and the ﬂexibility of
the TT-based models. At the same time, a strict order must be
followed when multiplying TT cores, so that the alignment
of the tensor dimensions is extremely important in obtaining
(a) TRD in a ring form
(b) TRD as the sum of TTs (r0 = rd)
Figure 3: Two representations of tensor ring decomposition
(TRD). In Figure 3(a), TRD is expounded in the traditional
way: the core tensors are multiplied one by one, and form
a ring structure. In Figure 3(b), TRD is illustrated in an alternative way: the summation of a series of tensor trains. By
ﬁxing the subscript r0 of GGG(1) and rd of GGG(d): r0 = rd = k,
where k ∈{1, 2, . . . , R}, both GGG(1) and GGG(d) are divided into
R matrices.
the optimized TT cores, but it is still a challenging issue in
ﬁnding the best alignment .
In the tensor ring decomposition(TRD), an important
modiﬁcation is interconnecting the ﬁrst and the last core tensors circularly and constructing a ring-like structure to alleviate the aforementioned limitations of the tensor train. Formally, we set R0 = Rd = R and R ≥1, and conduct the
decomposition as:
Dl1,l2,...,ld
r0=rd,r2,...,rd−1
r0,l1,r1GGG(2)
r1,l2,r2 . . .GGG(d)
rd−1,ld,rd
For a d-order tensor, by ﬁxing the index k where k ∈
{1, 2, . . . , R}, the ﬁrst order of the beginning core tensor
r0=k and the last order of the ending core tensor GGG(d)
can be reduced to matrices. Thus, along each of the R
slices of GGG(1), we can separate the tensor ring structure as
a summation of R of tensor trains. For example, by ﬁxing
r0 = rd = k, the product of GGG(1)
k,l1GGG(2)
l2 . . .GGG(d)
ld,k has the form
tensor train decomposition. Therefore, the tensor ring model
. . . . . .
Figure 4: TRL: XXX represents the input tensor with shape
RI1×I2×,...,×In after reshaping the input vector x ∈RI×1.
By performing the multiplication operation shown in Equation (6) with the weights in TRD form, the output tensor
YYY with shape RO1×O2×,...,×Omcan be obtained. Then, after
transforming YYY into vector, we can get the ﬁnal output vector
is essentially the linear combination of R different tensor
train models. Figure 3 demonstrates the tensor ring structure,
and the alternative interpretation as a summation of multiple
tensor train structures.
TR-RNN model
The core conception of our model is elaborated in this section. By transforming the input-to-hidden weight matrices in
TR form, and applying them into RNN and its variants, we
get our TR-RNN models.
Tensorizing x, y and W
Without loss of generality, we
tensorize the input vector x ∈RI, output vector y ∈RO,
and weight matrix W ∈RI×O into tensors XXX, YYY, and W
shown in Equation (4):
XXX ∈RI1×I2×,...,×In,YYY ∈RO1×O2×,...,×Om
W ∈RI1×I2×,...,×In×O1×O2,...,×Om
Decomposing W
For an n-order input and m-order output, we decompose the weight tensor into the form of TRD
with n + m core tensors multiplied one by one, each of
which is corresponding to an input dimension or an output
dimension, referring to Equation (5). Without loss of generality, the core tensors corresponding to the input dimensions
and output dimensions are grouped respectively, as shown in
W)i1,...,in,o1,...,om =
r0,...,rn,rn+1,...,rn+m−1
r0,i1,r1 . . .GGG(n)
rn−1,in,rnGGG(n+1)
rn,o1,rn+1 . . .GGG(n+m)
rn+m−1,om,r0
The tensor contraction from input to hidden layer in TR
form is shown in Equation (6). We multiply the input tensor
with input core tensors and output core tensors sequentially.
The complexity analysis of forward and backward process
is elaborated in the appendix.
YYYo1,o2,...,om =
XXXi1,...,inTRD(W
W)i1,...,in,o1,...,om
Compared with the redundant input-to-hidden weight matrix, the compression ratio in TR form is shown in Equation (7).
i=1 Ri−1IiRi + Pm
j=1 Rn+j−1OjRn+j
Tensor Ring Layer (TRL)
After reshaping the input vector x and the weight matrix W into tensor, and decomposing
weight tensor into TR representation, we can get the output
tensor YYY by manipulating W
W and XXX. The ﬁnal output vector y can be obtained by reshaping the output tensor YYY into
vector. Because the weight matrix is factorized with TRD,
we denote the whole calculation from the input vector x to
output vector y as tensor ring layer (TRL):
y = TRL(W, x)
which is illustrated in Figure 4.
By replacing the multiplication between weight
matrix Whx and input vector x with TRL in vanilla RNN.
We get our TR-RNN model. The hidden state at time t can
be expressed as:
ht = σ(TRL(Whx, xt) + Uhhht−1 + b)
where σ(·) denotes the sigmoid function and the hidden state
is denoted by ht. The input-to-hidden layer weight matrix
is denoted by Whx, and Uhh denotes the hidden-to-hidden
layer matrix.
By applying TRL to the standard LSTM,
which is the state-of-the-art variant of RNN, we can get the
TR-LSTM model as follows.
kt = σ(TRL(Wkx, xt) + Ukhht−1 + bk)
ft = σ(TRL(Wfx, xt) + Ufhht−1 + bf)
ot = σ(TRL(Wox, xt) + Uohht−1 + bo)
gt = tanh(TRL(Wgx, xt) + Ughht−1 + bg)
ct = ft ⊙ct−1 + kt ⊙gt
ht = ot ⊙tanh(ct),
where ⊙, σ(·) and tanh(·) denote the element-wise product,
the sigmoid function and the hyperbolic function, respectively. The weight matrices W∗x (where ∗can be k, f, o, or
g) denote the mapping from the input to hidden matrix, for
the input gate kt, the forget gate ft, the output gate ot, and
the cell update vector ct, respectively. The weight matrice
U∗h are deﬁned similarly for the hidden state ht−1.
Remark. As shown in Equation (6) and demonstrated
in Figure 4, the multiplication between the input tensor
data X and the input core tensors G(i) (for i = 1, . . . , n)
will produce a hidden matrix in the size of R0 × Rn. It
is important to note that the size of the hidden matrix is
much smaller than the original data size. In some sense,
the “compressed” hidden matrix can be regarded as the
information bottleneck , which seeks to achieve the
balance between maximally compressing the input information and preserving the prediction information of the output. Thus the proposed TR-LSTM has high potentials to reduce the redundant information in the high-dimensional input while achieving good performance compared with the
standard LSTM.
Experiments
To evaluate the proposed TR-LSTM model, we ﬁrst design a synthetic experiment to validate the advantage of
tensor ring decomposition over the tensor train decomposition. Through two real-world action recognition datasets,
i.e., UCF11(YouTube action dataset) and HMDB51 , we evaluate our
model from two settings: (1) end-to-end training, where
video frames are directly fed into the TR-LSTM; and
(2) pre-training to obtain features prior to LSTMs, where
a pre-trained CNN was used to extract meaningful lowdimensional features and then forwarded these features to
the TR-LSTM. For a fair comparison, we ﬁrst compare our
proposed method with the standard LSTM and previous lowrank decomposition methods, and then with the state-of-theart action recognition methods.
Synthetic Experiment
To verify the effectiveness of tensor decomposition methods in recover the original weights, we design a synthetic
dataset. Given a low-rank weight matrix W ∈R81×81,
which is illustrated in Figure 5(a). We ﬁrst sample 3200 examples, and each dimension follows a normal distribution,
i.e., x ∼N(0, 0.5I) where I ∈R81 is the identity matrix.
We then calculate y according to y = Wx + ϵ for each x
where ϵ ∼N(0, σ2I) is a random Gaussian noise and σ2 is
the variance. Since the y is generated from x, the recovered
weight matrix should be similar to the ground truth. We use
the root mean square error (RMSE) to measure the performance. Should be noted that since the purpose of this experiment is to provide a qualitative and intuitive comparison,
we do not add any regularization to the models.
Based on the input data and responses, we estimate the
weight matrix W by running the linear regression, tensor
train decomposition, and tensor ring decomposition, respectively. For tensor train and tensor ring, we ﬁrst reshape input
data to a tensor of 3 × 3 × 3 × 3, and reshape the weight
matrix to a tensor of the same size. For illustration, Figure 5
shows one of the recovered W (reshaped as a matrix) for
the three models when the noise variance is set to 0.05 .
Clearly, the proposed tensor ring model performs the best
among the three models. As for the tensor train model, it
is even worse than the linear regression model. We further
illustrate the recovered error of W with different levels of
noises in Figure 6. It demonstrates that the weight recovered
by the tensor ring model has the best tolerance with respect
to various injected noises.
(a) Ground truth W
(b) Linear Regression
(c) Tensor Train
(d) Tensor Ring
Figure 5: The illustration on the ground truth W and the recovered weights from different models. The recovered RM-
SEs of the linear model, tensor train, and tensor ring, are
0.16, 0.18, and 0.09, respectively.
Figure 6: The illustration on how the RMSEs of the linear regression(LR), the tensor ring (TR) and the tensor train (TT)
change with added noises.
Experiments on the UCF11 Dataset
The UCF11 dataset contains 1600 video clips of a resolution
320 × 240 divided into 11 action categories (e.g., basketball
shooting, biking/cycling, diving, etc.). Each category consist
of 25 groups of video, within more than 4 clips in one group.
It is a challenging dataset due to large variations in camera
motion, object appearance and pose, object scale, cluttered
background, and so on.
In this part, we conduct two experiments described as
“End-to-End Training” and “Pre-train with CNN” on this
dataset. In the “End-to-End Training”, we compare the proposed TR-LSTM model with other decomposition models
 and BT-
LSTM ) to show the superior performance.
And in another experiment, we apply decomposition on a
more general model, achieving a better performance with
less parameters.
Compression Ratio
(a) Compression Ratio
Train Loss
(b) Train Loss
Test Accuracy
LSTM: TOP=0.681
BT-LSTM: TOP=0.856
TT-LSTM: TOP=0.803
TR-LSTM: TOP=0.869
(c) Test Accuracy
Figure 7: The results of “End-to-End Training” on UCF11
dataset. (a) shows the different compression ratio based on
the vanilla LSTM. (b) and (c) shows the training and testing
End-to-End Training
Recent years, some tensor decomposition models are proposed to classify videos like TT-
LSTM , BT-LSTM and others. For the reason that they use the
end-to-end model for training, we set this experiment to
compare with them. In this experiments, we scale down
the original resolution to 160 × 120, and sample 6 frames
from each video clip randomly as the input data. Since every frame is RGB, the input data vector at each step is
160 × 120 × 3 = 57600, and there are 6 steps in every sam-
ple. We set the hidden layer as 256. So there should be a
fully-connected layer of 4 × 57600 × 256 = 58982400 parameters to achieve the mapping for the standard LSTM.
Table 1: Results of “End-to-End Training” on UCF11 reported in literature. TT-LSTM was reported in while the BT-LSTM was reported in .
We compare our model with BT-LSTM and TT-LSTM,
while using a standard LSTM as a baseline. The hyperparameters in BT-LSTM and TT-LSTM are set as announced in their papers. Figure 7(c) shows all decomposition methods converging faster than the LSTM. The accuracy of BT-LSTM is 0.856 which is much higher than
TT-LSTM with 0.803 while the LSTM only gain an accuracy of 0.681. In our TR-LSTM, the shape of input tensor
is 4 × 2 × 5 × 8 × 6 × 5 × 3 × 2, the output tensor’s shape
is set as 4 × 4 × 2 × 4 × 2 and all the TR-ranks are set as
5 except R0 = Rd = 10. Results are compared in Table 1.
With 1725 parameters in our model, about half of TT-LSTM
and BT-LSTM with parameters 3360 and 3387 respectively.
We gain the top accuracy 0.869, showing the outstanding
performance of our model in this experiment.
Table 2: The state-of-the-art performance on UCF11.
 
 
 
 
 
 
 
 
 
CNN + LSTM
CNN + TR-LSTM
Pre-train with CNN
Recently, some methods based on
RNNs achieved higher accuracy by using the extracted feature as input vectors in computer vision . Compared with using frames as input data, extracted
features are more compact. But there is still some room for
improving the ability of the models. The over-parametric
problem is just partial solved. To get better performance, we
use extracted features via the CNN model Inception-V3 as
input data to LSTM.
We set the size of the hidden layer as 32 × 64 = 2048,
which is consistent with the size of the output via Inception-
V3. After using the extracted feature as the inputs of LSTM,
the accuracy of the vanilla LSTM attains 0.923. At the same
time, the accuracy of our TR-LSTM model whose ranks are
set as 40 × 60 × 48 × 48 achieves 93.8. By replacing the
standard LSTM with our model, a compression ratio of 25
can be obtained. We compare some state-of-the-art methods
in Table 2 on UCF11. The Two Stream LSTM with highest accuracy has more than 141M parameters. The TR-LSTM can be used to replace the vanilla
LSTMs in the Two Stream LSTM model to reduce the parameters.
Experiments on the HMDB51 Dataset
The HMDB51 dataset is a large collection of realistic videos
from various sources, such as movies and web videos. The
dataset is composed of 6766 video clips from 51 action categories.
Table 3: Comparison with state-of-the-art results on
HMDB51. The best accuracy is 0.664 from the I3D model
reported in , which bases on
3D ConvNets and is not RNN-based method.
 
 
 
 
 
 
 
 
 
CNN + LSTM
CNN + TR-LSTM
In this experiment, we still use extracted features as the
input vector via Inception-V3 and reshape it into 64 × 32.
We sample 12 frames from each video clip randomly and be
processed through the CNN as the input data. The shape of
hidden layer tensor is set as 32 × 64 = 2048. The ranks of
our TR-LSTM are 40 × 60 × 48 × 48. Some of the state-ofthe-art models like I3D are
presented in Table 3. The I3D model with highest accuracy
based on 3D ConvNets, which is not RNN-based method,
has 25M parameters, while the TR-LSTM model only has
0.7M parameters. The TR-LSTM gains a higher accuracy of
63.8% than the standard LSTM with a compressing ratio of
Related Work
In the past decades, a number of variants of recurrent neural networks (RNNs) were proposed to capture sequential
information more accurately . However,
when dealing with the large input data, in the ﬁeld of computer vision, the input-to-hidden weight matrix owns loads
of parameters. The limitation of computation resources and
the severe over-ﬁtting problem are emerging. Some methods
used CNNs as feature extractors to pre-processing the input
data into a more compact way . These
methods have improved the classiﬁcation accuracy, but the
over-parametric problem is just still partially solved. In this
work, we focus on designing low-rank structure to replace
the redundant input-to-hidden weight matrix in RNNs, while
compressing the whole model and maintaining the model
performance.
The most straight-forward way to apply low-rank constraint is implementing matrix decomposition on weight matrices. Singular Value Decomposition (SVD) has been applied in convolutional neural networks to reduce parameters but incurred a loss in model performance. Besides, the compression ratio is limited because
of the rank in matrix decomposition still relatively large.
Compared with matrix decomposition, tensor decomposition conducts data decomposition in a higher dimension, capturing higher-order correlations while maintaining several orders of fewer parameters . Among these
methods, utilized CP decomposition to
speed up convolution computation, which has the similar design philosophy with the widely-used depth-wise separable
convolutions . However, the instability
issue hinders the low-rank CP decomposition from solving many important computer vision
tasks. used Tucker decomposition to decompose both the convolution layer and the fully connected
layer, reducing run-time and energy signiﬁcantly in mobile
applications with minor accuracy drop. Block-Term tensor
decomposition combines the CP and Tucker by summing up
multiple Tucker models to overcome their drawbacks and
has obtained a better performance in RNNs .
However, the computation of the core tensor in the Tucker
model is highly inefﬁcient due to the complex tensor ﬂatten
and permutation operations. Recent years, the tensor train
decomposition also used to substitute the redundant fully
connected layer in both CNNs and RNNs , preserving the performance while reducing the number of parameters signiﬁcantly up to 40 times.
But tensor train decomposition has some limitations: 1)
certain constraints for TT-rank, i.e., the ranks of the ﬁrst
and last factors are restricted to be 1, limiting its representation power and ﬂexibility. 2) A strict order must be followed when multiplying TT cores, so that the alignment of
the tensor dimensions is extremely important in obtaining
the optimized TT cores, but it is still a challenging issue in
ﬁnding the best alignment. In this paper, we use the Tensor
Ring(TR) decomposition to overcome the
drawbacks in TTD, while achieving more computation efﬁciency than BT decomposition.
Conclusion
In this paper, we applied TRD to plain RNNs to replace the
over-parametric input-to-hidden weight matrix when dealing with high-dimensional input data. The low-rank structure of TRD can capture the correlation between feature dimensions with fewer orders of magnitude parameters. Our
TR-LSTM model achieved best compression ratio with the
highest classiﬁcation accuracy on UCF11 dataset among
other end-to-end training RNNs based on low-rank methods. At the same time, when processing the extracted feature through InceptionV3 as the input vector, our TR-LSTM
model can still compress the LSTM while improving the
accuracy. We believe that our models provide fundamental
modules for RNNs, and can be widely used to handle large
input data. In future work, since our models are easy to be
extended, we want to apply our models to more advanced
RNN structures to get better performance.
Acknowledgments
We thank the anonymous reviewers for valuable comments
to improve the quality of our paper. This work was partially supported by National Natural Science Foundation
of China (Nos.61572111 and 61876034), and a Fundamental Research Fund for the Central Universities of China
(No.ZYGX2016Z003).