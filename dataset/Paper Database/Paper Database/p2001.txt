Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 925–930
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Retrieval-Based Neural Code Generation
Shirley Anugrah Hayati
Raphael Olivier
Pravalika Avvaru
Pengcheng Yin
Anthony Tomasic
Graham Neubig
Language Technologies Institute, Carnegie Mellon University
{shayati,rolivier,pavvaru,pcyin,tomasic,gneubig}@cs.cmu.edu
In models to generate program source code
from natural language, representing this code
in a tree structure has been a common approach. However, existing methods often fail
to generate complex code correctly due to a
lack of ability to memorize large and complex structures.
We introduce RECODE, a
method based on subtree retrieval that makes
it possible to explicitly reference existing code
examples within a neural code generation
First, we retrieve sentences that are
similar to input sentences using a dynamicprogramming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree.
Finally, we increase the
probability of actions that cause the retrieved
n-gram action subtree to be in the predicted
code. We show that our approach improves the
performance on two code generation tasks by
up to +2.6 BLEU.1
Introduction
Natural language to code generation, a subtask
of semantic parsing, is the problem of converting
natural language (NL) descriptions to code . This task is challenging because it
has a well-deﬁned structured output and the input structure and output structure are in different
A number of neural network approaches have
been proposed to solve this task.
Sequential approaches convert the target code into
a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct.
 
sweetpeach/ReCode
Tree-based approaches represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the
well-formedness of the output code.
representing code as a tree is not a trivial task, as
the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of
generating correct code for phrases in the corresponding NL description that have low frequency
in the training data.
In machine translation (MT) problems , hybrid methods combining retrieval of salient examples and neural models
have proven successful in dealing with rare words.
Following the intuition of these models, we hypothesize that our model can beneﬁt from querying pairs of NL descriptions and AST structures
from training data.
In this paper, we propose RECODE, and adaptation of Zhang et al. ’s retrieval-based approach neural MT method to the code generation problem by expanding it to apply to generation of tree structures.
Our main contribution is to introduce the use of retrieval methods
in neural code generation models. We also propose a dynamic programming-based sentence-tosentence alignment method that can be applied to
similar sentences to perform word substitution and
enable retrieval of imperfect matches. These contributions allow us to improve on previous stateof-the-art results.
Syntactic Code Generation
Given an NL description q, our purpose is to generate code (e.g. Python) represented as an AST a.
In this work, we start with the syntactic code gen-
eration model by Yin and Neubig , which
uses sequences of actions to generate the AST before converting it to surface code. Formally, we
want to ﬁnd the best generated AST ˆa given by:
ˆa = arg max
p(yt|y<t, q)
where yt is the action taken at time step t and
y<t = y1...yt−1 and T is the number of total time
steps of the whole action sequence resulting in
We have two types of actions to build an AST:
APPLYRULE and GENTOKEN.
APPLYRULE(r)
expands the current node in the tree by applying
production rule r from the abstract syntax grammar2 to the current node.
GENTOKEN(v) populates terminal nodes with the variable v which
can be generated from vocabulary or by COPYing
variable names or values from the NL description.
The generation process follows a preorder traversal starting with the root node. Figure 1 shows
an action tree for the example code: the nodes correspond to actions per time step in the construction
of the AST.
Interested readers can reference Yin and Neubig
 for more detail of the neural model, which
consists of a bidirectional LSTM encoder-decoder with action
embeddings, context vectors, parent feeding, and
a copy mechanism using pointer networks.
RECODE: Retrieval-Based Neural
Code Generation
We propose RECODE, a method for retrievalbased neural syntactic code generation, using retrieved action subtrees.
Following Zhang et al.
 ’s method for neural machine translation,
these retrieved subtrees act as templates that bias
the generation of output code. Our pipeline at test
time is as follows:
• retrieve from the training set NL descriptions
that are most similar with our input sentence
• extract n-gram action subtrees from these
retrieved sentences’ corresponding target
ASTs (§3.2),
2 
• alter the copying actions in these subtrees, by
substituting words of the retrieved sentence
with corresponding words in the input sentence (§3.3), and
• at every decoding step, increase the probability of actions that would lead to having these
subtrees in the produced tree (§3.4).
Retrieval of Training Instances
For every retrieved NL description qm from training set (or retrieved sentence for short), we compute its similarity with input q, using a sentence
similarity formula = 1 −
max(|q| ,|qm|)
where d is the edit distance.
We retrieve only the
top M sentences according to this metric where
M is a hyperparameter. These scores will later be
used to increase action probabilities accordingly.
Extracting N-gram Action Subtrees
In Zhang et al. , they collect n-grams from
the output side of the retrieved sentences and
encourage the model to generate these n-grams.
Word n-grams are obvious candidates when generating a sequence of words as output, as in NMT.
However, in syntax-based code generation, the
generation target is ASTs with no obvious linear
structure. To resolve this problem, we instead use
retrieved pieces of n-gram subtrees from the target code corresponding to the retrieved NL descriptions.
Though we could select successive
nodes in the AST as retrieved pieces, such as
[assign; expr*(targets); expr] from Figure
1, we would miss important structural information
from the rules that are used. Thus, we choose to
exploit actions in the generation model rather than
AST nodes themselves to be candidates for our retrieved pieces.
In the action tree (Figure 1), we considered only successive actions,
such as subtrees where each node has one or no children, to avoid overly rigid structures or combinatorial explosion of the number of retrieved
pieces the model has to consider.
For example, such an action subtree would be given
by [assign →expr*(targets), expr(value)
; expr(value) →List; List →epsilon].
As the node in the action tree holds structural
information about its children, we set the subtrees
root ­> assign
assign ­> expr*(targets), expr(value)
expr*(targets) ­> expr
expr ­> Name
Name ­> str
GENTOKEN[params]
GENTOKEN[/n]
expr(value) ­> List
List ­> epsilon
Input       :  params is an empty list
Target Code      : params = [ ]
Action Flow
Parent Feeding
Apply Rule
Generate Token
Generate Token
Retrieved: List lst is an empty list
Retrieved Code: lst = [ ]
GENTOKEN[lst]
GENTOKEN[/n]
Figure 1: The action sequence used to generate AST for the
target code given the input example. Dashed nodes represent terminals. Each node is labeled with time steps. AP-
PLYRULE action is represented as rule in this ﬁgure. Blue
dotted boxes denote 3-gram action subtrees. Italic words are
unedited words. Red bold words are different object names.
to have a ﬁxed depth, linear in the size of the
These can be considered “n-grams of actions”, emphasizing the comparison with machine
translation which uses n-grams of words. n is a
hyperparameter to be tuned.
Word Substitution in Copy Actions
Using the retrieved subtree without modiﬁcation
is problematic if it contains at least one node corresponding to a COPY action because copied tokens from the retrieved sentence may be different
from those in the input. Figure 1 shows an example when the input and retrieved sentence have
four common words, but the object names are different. The extracted action n-gram would contain
the rule that copies the second word (“lst”) of the
retrieved sentence while we want to copy the ﬁrst
word (“params”) from the input.
By computing word-based edit distance between the input description and the retrieved sentence, we implement a one-to-one sentence alignment method that infers correspondences between
uncommon words. For unaligned words, we alter
all COPY rules in the extracted n-grams to copy tokens by their aligned counterpart, such as replace
“params” with “lst”, and delete the n-gram subtree, as it is not likely to be relevant in the predicted tree.
Thus, in the example in Figure 1,
the GENTOKEN(LST) action in t5 will not be executed.
Retrieval-Guided Code Generation
N-gram subtrees from all retrieved sentences are
assigned a score, based on the best similarity score
Avg. tokens in description
Avg. number of nodes of AST
Table 1: Dataset statistics as reported Yin and Neubig 
of all instances where they appeared. We normalize the scores for each input sentence by subtracting the average over the training dataset.
At decoding time, incorporate these retrievalderived scores into beam search: for a given time
step, all actions that would result in one of the
retrieved n-grams u to be in the prediction tree
has its log probability log(p(yt | yt−1
)) increased
by λ ∗score(u) where λ is a hyperparameter, and
score(u) is the maximal sim(q, qm) from which u
is extracted. The probability distribution is then
renormalized.
Datasets and Evaluation Metrics
We evaluate RECODE with the Hearthstone (HS)
 and Django 
datasets, as preprocessed by Yin and Neubig
 . HS consists of Python classes that implement Hearthstone card descriptions while Django
contains pairs of Python source code and English
pseudo-code from Django web framework. Table
1 summarizes dataset statistics.
For evaluation metrics, we use accuracy of exact match and the BLEU score following Yin and
Neubig .
Experiments
For the neural code generation model, we use the
settings explained in Yin and Neubig . For
the retrieval method, we tuned hyperparameters
and achieved best result when we set nmax = 4
and λ = 3 for both datasets3. For HS, we set
M = 3 and M = 10 for Django.
We compare our model with Yin and Neubig
 ’s model that we call YN17 for brevity,
and a sequence-to-sequence (SEQ2SEQ) model
that we implemented. SEQ2SEQ is an attentionenabled encoder-decoder model . The encoder is a bidirectional LSTM and
the decoder is an LSTM.
Table 2 shows that RECODE outperforms the baselines in both BLEU and accuracy, providing ev-
3n-gram subtrees are collected up to nmax-gram
idence for the effectiveness of incorporating retrieval methods into tree-based approaches.
ASN + SUPATT†
Table 2: Results compared to baselines. YN17 result is taken
from Yin and Neubig . ASN result is taken from Rabinovich et al. 
We ran statistical signiﬁcance tests for RECODE
and YN17, using bootstrap resampling with N =
10,000. For the BLEU scores of both datasets, p <
0.001. For the exact match accuracy, p < 0.001
for Django dataset, but for Hearthstone, p > 0.3,
showing that the retrieval-based model is on par
with YN17. It is worth noting, though, that HS
consists of long and complex code, and that generating exact matches is very difﬁcult, making exact
match accuracy a less reliable metric.
We also compare RECODE with Rabinovich
et al. ’s Abstract Syntax Networks with
supervision
(ASN+SUPATT)
state-of-the-art system for HS.
RECODE exceeds ASN without extra supervision though
ASN+SUPATT has a slightly better result. However, ASN+SUPATT is trained with supervised
attention extracted through heuristic exact word
matches while our attention is unsupervised.
Discussion and Analysis
From our observation and as mentioned in Rabinovich et al. , HS contains classes with similar structure, so the code generation task could
be simply matching the tree structure and ﬁlling
the terminal tokens with correct variables and values. However, when the code consists of complex
logic, partial implementation errors occur, leading to low exact match accuracy . Analyzing our result, we ﬁnd this intuition
to be true not only for HS but also for Django.
Examining the generated output for the Django
dataset in Table 3, we can see that in the ﬁrst example, our retrieval model can successfully generate the correct code when YN17 fails.
difference suggests that our retrieval model beneﬁts from the action subtrees from the retrieved
sentences. In the second example, although our
generated code does not perfectly match the reference code, it has a higher BLEU score compared
“if oﬀset is lesser than integer 0, sign is set to ’-’, otherwise sign is ’+’ ”
sign = offset < 0 or ’-’
sign = ’-’ if offset < 0 else ’+’
sign = ’-’ if offset < 0 else ’+’
“evaluate the function timesince with d, now and reversed set
to boolean true as arguments, return the result.”
return reversed(d, reversed=now)
return timesince(d, now, reversed=now)
return timesince(d, now, reversed=True)
“return an instance of SafeText ,
created with an argument s converted into a string .”
return SafeText(bool(s))
return SafeText(s)
return SafeString(str(s))
Table 3: Django examples on correct code and predicted
code with retrieval (RECODE) and without retrieval (YN17).
NAME_BEGIN Earth Elemental NAME_END ATK_BEGIN 7
ATK_END DEF_BEGIN 8 DEF_END COST_BEGIN 5
COST_END DUR_BEGIN -1 DUR_END TYPE_BEGIN Minion
TYPE_END PLAYER_CLS_BEGIN Shaman PLAYER_CLS_END
RACE_BEGIN NIL RACE_END RARITY_BEGIN Epic RARITY_END
DESC_BEGIN Taunt . Overload : ( 3 ) DESC_END.
class EarthElemental (MinionCard) :
def __init__ (self) :
super ( ).__init__ ("Earth Elemental", 5,
CHARACTER_CLASS.SHAMAN, CARD_RARITY.EPIC,
buffs=[Buff(ManaChange(Count
(MinionSelector(None, BothPlayer())), -1))])
def create_minion (self, player) :
return Minion(7, 8, taunt=True)
class EarthElemental (MinionCard) :
def __init__ (self) :
super ( ).__init__ ("Earth Elemental", 5,
CHARACTER_CLASS.SHAMAN, CARD_RARITY.EPIC,
overload=3)
def create_minion (self, player) :
return Minion(7, 8, taunt=True)
class EarthElemental (MinionCard) :
def __init__ (self) :
super ( ).__init__ ("Earth Elemental", 5,
CHARACTER_CLASS.SHAMAN, CARD_RARITY.EPIC,
overload=1)
def create_minion (self, player) :
return Minion(7, 8, taunt=True)
Table 4: HS examples on correct code and predicted code
with retrieval (RECODE) and without retrieval (YN17).
to the output of YN17 because our model can
predict part of the code (timesince(d, now,
reversed)) correctly. The third example shows
where our method fails to apply the correct action
as it cannot cast s to str type while YN17 can at
least cast s into a type (bool). Another common
type of error that we found RECODE’s generated
outputs is incorrect variable copying, similarly to
what is discussed in Yin and Neubig and
Rabinovich et al. .
Table 4 presents a result on the HS dataset4. We
can see that our retrieval model can handle complex code more effectively.
Related Work
Several works on code generation focus on domain speciﬁc languages .
For general purpose
code generation, some data-driven work has been
4More example of HS code is provided in the supplementary material.
done for predicting input parsers 
or a set of relevant methods . Some attempts using neural networks have
used sequence-to-sequence models or tree-based architectures .
Ling et al. ; Jia and Liang ; Locascio et al. treat semantic parsing as a sequence generation task by linearizing trees. The
closest work to ours are Yin and Neubig 
and Rabinovich et al. which represent code
as an AST. Another close work is Dong and Lapata
 , which uses a two-staged structure-aware
neural architecture. They initially generate a lowlevel sketch and then ﬁll in the missing information using the NL and the sketch.
Recent works on retrieval-guided neural machine translation have been presented by Gu et al.
 ; Amin Farajian et al. ; Li et al.
 ; Zhang et al. . Gu et al. use
the retrieved sentence pairs as extra inputs to the
NMT model. Zhang et al. employ a simpler and faster retrieval method to guide neural
MT where translation pieces are n-grams from retrieved target sentences. We modify Zhang et al.
 ’s method from textual n-grams to n-grams
over subtrees to exploit the code structural similarity, and propose methods to deal with complex
statements and rare words.
In addition, some previous works have used
subtrees in structured prediction tasks.
For example, Galley et al. used them in syntaxbased translation models. In Galley et al. ,
subtrees of the input sentence’s parse tree are associated with corresponding words in the output
Conclusion
We proposed an action subtree retrieval method at
test time on top of an AST-driven neural model for
generating general-purpose code. The predicted
surface code is syntactically correct, and the retrieval component improves the performance of a
previously state-of-the-art model. Our successful
result suggests that our idea of retrieval-based generation can be potentially applied to other treestructured prediction tasks.
Acknowledgements
We are grateful to Lucile Callebert for insightful discussions, Aldrian Obaja Muis for helpful
input on early version writing, and anonymous
reviewers for useful feedback.
This material is
based upon work supported by the National Science Foundation under Grant No. 1815287.