Vol.:(0123456789)
 
An improved evolutionary wrapper‑filter feature selection
approach with a new initialisation scheme
Emrah Hancer1
Received: 15 January 2020 / Revised: 17 April 2021 / Accepted: 3 May 2021
© The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature 2021
Treated as one of the popular measures in information theory, fuzzy mutual information
quantifies the amount of information that one random variable has about another one. Different from standard mutual information, fuzzy mutual information can deal with not only
discrete-valued but also real-valued variables. Therefore, fuzzy mutual information has
been recently used in evolutionary filter feature selection approaches to measure the correlation between the classes and the features, and the dependencies within a feature set.
Typically, this way can be considered as computationally efficient but sometimes it may
not contribute to the performance of a classification algorithm. To address this issue, an
improved evolutionary wrapper-filter approach which integrates an initialisation scheme
and a local search module based on fuzzy mutual information in differential evolution is
proposed. According to a number of experiments conducted on several real-world benchmark datasets, the proposed approach does not only significantly improve the computational efficiency of an evolutionary computation technique but also the performance of a
classification algorithm.
Keywords  Feature selection · Information theory · Mutual information · Differential
1  Introduction
Information theory fundamentally involves the quantification, transmission, and extraction of information. Initially proposed by Shannon to investigate fundamental limits
on data compression, information theory has become a widespread field that has a crucial
effect on a number of social and technological developments, such as the invention of compact disc, the development of internet, and the feasibility of mobile phones, to name a few.
Editors: Tim Verdonck, Bart Baesens, María Óskarsdóttir and Seppe vanden Broucke.
* Emrah Hancer
 ; 
Department of Software Engineering, Mehmet Akif Ersoy University, Burdur 15039, Turkey
Published online: 12 May 2021
Machine Learning 113:4977–5000
Information theory also provides a broad theoretical framework for a variety of machine
learning tasks. One of the popular tasks which employs information theory is feature subset selection. Feature subset selection is the process of selecting beneficial features from
all available features in the data, which may maximally improve the performance of a
(supervised or unsupervised) training model. In other words, feature subset selection aims
to eliminate features which may degrade the performance of a training model. Besides
improving the effectiveness of a training model, feature subset selection can also enhance
the computational efficiency, reduce the storage requirements and eliminate noisy information. In case of evaluation criteria, feature subset selection is broadly investigated in three
groups: wrappers, filters and embedded approaches. While wrappers use a training model
to evaluate the goodness of a possible feature subset, filters use data intrinsic information.
Accordingly, wrappers can achieve better training performance than filters, but with a more
intensive computational time. Moreover, the selected feature subsets may vary according
to the considered training model in wrappers . Embedded approaches
apply feature selection during the model construction, i.e., they perform feature selection
and classification at the same time 
In the terminology of feature subset selection, information theoretic approaches are
evaluated in the category of filters. One of the most relevant measures of information theory is to use mutual information in feature subset evaluation. Mutual information has two
main characteristics: 1) it can measure any kind of dependency between random variables,
and 2) it cannot be manipulated under transformations in the possible solution space. The
landmark event that established the field of information theoretic feature subset selection
and brought it to worldwide attention was the publication of Battiti’s paper in 1994 . Battiti introduced a feature subset selection approach, namely mutual information
feature selection (MIFS), which selects a single feature at each iteration rather than evaluating the combinatorial explosion of all possible feature subsets. In this work, the main
assumptions are as follows : 1) the features are considered as
relevant and redundant, 2) a heuristic function is used to measure the relevancy and redundancy of a feature, 3) a greedy selection mechanism is applied, and 4) the chosen feature
subset is assumed optimal.
Despite the successful performance of information theoretic feature subset selection
approaches versus conventional approaches, their performance generally tends to deteriorate on high-dimensional problems. Therefore, there is a need to adopt information theoretic measures in a wrapper framework to improve the performance of a training algorithm;
otherwise, it is not possible to obtain satisfactory learning performance on high-dimensional problems by only considering information theoretic measures. However, not many
related works are present in the literature, and the current ones cannot deal with highdimensional problems.
In this study, we aim to develop an improved evolutionary wrapper-filter feature subset
selection approach to maximally reduce the running time of the search process and maximally enhance the performance of a training algorithm, especially in high-dimensional
problems. To achieve this aim, we first design a new initialisation scheme which initialises
feature subsets based on the dependency of features to the targets through a fuzzy mutual
estimator. We then utilize the initialisation scheme in differential evolution (DE) wrapper framework with a local search module inspired by a recent work
 . Why do we choose DE among a variety of successful evolutionary computation (EC) techniques? First, DE is simple to implement, computationally cheap and
does not frequently encounter local-convergence problems. Second, DE has only few control parameters to be predefined by a user compared to other well-known EC techniques,
Machine Learning 113:4977–5000
such as genetic algorithm (GA) , particle swarm optimization (PSO) and bacterial foraging optimization (BFO) .
Moreover, DE generally achieves better results than GA and PSO algorithms, and performs
competitively with recently introduced EC techniques, such as artificial bee colony (ABC)
 and firefly algorithms (FA) . The last and most important
source of motivation in this study to design feature subset selection approaches using DE is
our recent works ,
which successfully applied DE variants for feature subset selection. Particularly, we will
investigate the following:
• the performance analysis of the improved DE-based wrapper-filter approach versus
DE-based wrapper and wrapper-filter approaches.
• the performance analysis of the improved DE-based wrapper-filter approach versus
conventional subset selection approaches.
• the parameter analysis of the proposed initialisation scheme in the improved DE-based
wrapper-filter approach.
The rest of the paper is presented as follows. Section  2 provides a general knowledge
for curse of dimensionality, differential evolution, information theoretic measures, and
recently introduced works on information theoretic feature subset selection. Section  3
briefly describes the proposed feature subset selection approach. In Sect. 4, the datasets
and parameter settings used in experiments are clarified. In Sect. 5, we present the results
conducted on a variety of datasets with discussions. Section 6 finally concludes the study
with a general discussion on the results and future trends.
2  Background
2.1  Curse of dimensionality
Why do we frequently require dimensionality reduction techniques in machine learning applications? To be aware of the importance of dimensionality reduction, let’s consider a dataset
that includes a set of images, each of which represents either a cat or a dog. In order to automatically distinguish cats from dogs, we would like to train a learning (classification) algorithm. To build a classifier, we first define a descriptor for each category that is expressed
using numbers. For instance, one can possibly assume that cats and dogs generally differ in
terms of color. A color in RGB format, as a descriptor used to separate these two categories, comprises three components: red, green, and blue. A simple classifier is then expected
to combine these three-color features by building a mathematical model to decide on the class
label. However, such features may not be sufficient to achieve a successful classification task.
Therefore, we can add some features (e.g. texture of the image, and gradient intensity) to the
feature subset. Using 5 features, a classifier may possibly separate dogs from cats. What if we
add more features (e.g. statistical moments, texture histogram, etc.) to achieve more successful
classification performance? As the dimensionality increases, after a certain point, the performance of our classifier will degrade. The more features we add, the lower accuracy we might
obtain while we would also encounter more storage requirements. This is because the sparsity
Machine Learning 113:4977–5000
between the data instances increases proportionally to the number of features. The common
term concerning this problematic issue is defined as the curse of dimensionality.
2.2  Differential evolution
DE is one of the earliest evolutionary algorithms in the field of EC techniques. Although a variety of EC techniques have been proposed since it was introduced in
1997, its popularity is still growing due to the following advantages: 1) simple to implement,
2) computationally efficient, 3) few control parameters, and 4) local-global search balance.
The general procedure of standard DE is described as follows.
1) Initialisation: In this stage, a predefined number of individuals are initialised within the
predefined lower and upper boundaries of the solution space:
where i = {1, 2, … , SN} and SN is the population size; xi,j represents the jth position of ith
individual Xi ; xlower
and xupper
represent the lower and upper boundaries for the jth position;
U(0, 1) denotes the random number between 0 and 1.
2) Mutation: In this stage, a mutant X′
i is evolved for each ith individual using three randomly selected individuals from the population:
where Xr1 , Xr2 and Xr3 are randomly selected individuals; and F denotes the scaling factor.
3) Recombination: In this stage, a trial individual Ui is evolved by exchanging information
between Xi and X′
where CR denotes the crossover rate.
4) Selection: In this stage, the fitness value of Ui is evaluated. If Ui is fitter than Xi , it is used
instead of Xi in the population.
The stages 2–4 are repeated until the stopping criterion or maximum number of cycles is
satisfied.
2.3  Information theory
Information theory was first applied to address tasks related to the context of communication
theory, such as transmission rate and data compression. Since then, the principles of information theory have been largely adopted into pattern recognition and machine learning tasks. The
basic measures of information theory are as follows.
1) Entropy quantifies uncertainty of a random variable. The uncertainty can be defined as
the probability of occurrence of an event. The mass entropy of a random variable x is defined
as follows.
xi,j = xlower
+ U(0, 1)(xupper
i,j = xr3,j + F(xr2,j −xr1,j)
i,j, Uj(0, 1) < CR
xij, otherwise
p(x(i)) log(p(x(i)))
Machine Learning 113:4977–5000
where p(x(i)) = Pr{x = x(i)}, x(i) ∈X.
The joint entropy of two variables x and y with joint mass probability p(x(i), y(i)) is defined
as follows.
The conditional entropy of x which is associated with y is defined as follows.
2) Mutual information quantifies the amount of information that one variable knows about
another. Typically, mutual information between two variables is defined as follows.
To quantify the amount of uncertainty, fuzzy measures can be used as well as probabilistic measures. Khushaba et al. , 2011) reformulated the standard entropy and mutual
information measures using fuzzy membership, which was inspired by the fuzzy c-means
algorithm . 휇ik denotes the fuzzy membership value of kth feature belonging
to ith class, defined as follows.
where m is the parameter of fuzzification, 𝜖> 0 is selected as a small value to avoid
singularity, 휎 is the standard deviation used in the computation of distances, and
r = max ‖ ̄xi −xk‖𝜎 represents the radius of the data, where ̄xi is the mean of the data
instances belonging to ith class.
In case of the proposed fuzzy membership function defined by Eq. (8), c fuzzy sets, each
of which reflects the membership degree of instances in each of the c classes, are constructed
over each feature X. The fuzzy membership values of all the instances over the features X and
Y in each of the C fuzzy sets are respectively denoted as A and B with c × NP , where NP is the
number of instances in the data. Then, the fuzzy marginal probability of all data instances over
each feature X is defined as follows.
The fuzzy marginal entropy of X is then defined in case of Eq. (9), as follows.
where the summations denote the sum along the columns and rows.
The fuzzy joint entropy of X and Y is defined as follows.
H(x, y) = −
p(x(i), y(j)) log(p(x(i), y(j)))
p(y(j))H(x|y = y(j))
p(x(i), y(j)) log( p(x(i), y(j))
p(x(i))p(y(j)))
‖ ̄xi −xk‖𝜎
P(X) log P(X)
FH(X, Y) = −
Machine Learning 113:4977–5000
In case of fuzzy marginal and joint entropies, the fuzzy mutual information between X and
Y is then defined as follows.
For the implementation of the fuzzy entropy and mutual information metrics, please see
Khushaba .
2.4  Recent works
Due to the motivation of our work, we focus on information theoretic approaches in this paper.
For more information concerning feature selection approaches, please see Xue et al. ,
Nguyen et al. , Hancer et al. .
2.4.1  Conventional information theoretic approaches
Conventional information theoretic approaches generally apply greedy search to find an optimal feature subset. First, features in the data are ordered according to the specified information
theoretic criterion in terms of relevancy and redundancy. Then, a single feature with the highest score for the feature subset is selected. This procedure is repeated until the stopping criterion is met. The main advantage of information theoretic approaches is the robustness to both
noise and data transformation. However, the computational speed of these approaches may be
very slow on high dimensional datasets. A variety of information theoretic criteria have been
introduced since the work of Battiti which was published in 1994. The most popular ones are
presented as follows.
1) Mutual information feature selection (MIFS) selects an optimal feature
subset using Eq. 13.
where xi is a feature in the feature set, which has not yet been selected for the subset S, xs is
a selected feature in the subset S, and 훽 is a control parameter keeping a balance between
the components.
2) Peng et al. introduced the max-relevance min-redundancy (MRMR) criterion due
to the difficulties in determining the control parameter in Eq. 13. The MRMR criterion is
defined in Eq. 14. It can be inferred that MIFS is the generalized version of MRMR, i.e., with
|S| , Eq. 13 is equivalent to Eq. 14.
where |S| is the size of S.
3) Kwak and Choi proposed an improved version of MIFS (called MIFS-U) which
is more suitable for the datasets where the information is uniformly distributed. The MIFS-U
criterion is defined as follows.
FI(X, Y) = FH(X) + FH(Y) −FH(X, Y)
Jmifs = I(xi, y) −훽
Jmrmr = I(xi, y) −1
Jmifs−u = I(xi, y) −훽
H(xs) I(xi, xs)
Machine Learning 113:4977–5000
4) Yang and Moody introduced a specified version of the MRMR criterion (called
JMI) by considering the joint relationships between the class labels and a pair of random
variables. The JMI criterion is defined as follows.
5) Brown improved the MIFS criterion (referred to as FOU) by adding a new component that measures joint mutual information between the class labels and a pair of random variables. The FOU criterion is defined as follows.
where 휆 is also a control parameter like 훽 . It can be seen that with 휆= 0 , the FOU criterion
becomes the MIFS criterion.
6) The improved version of information gain, symmetric uncertainty is
a ratio of information gain to the intrinsic information:
where H(xi|y) is conditional entropy calculated by Eq. (6).
In case of fuzzy mutual information defined in Eq.(12), the fuzzy version of information
theoretic criteria can be redesigned. For instance, MIFS and SU (called FMIFS and FSU)
are respectively formulated in Eqs. (19) and (20) for the case of a fuzzy mutual estimator.
2.4.2  EC‑based information theoretic approaches
Conventional information theoretic approaches are able to find optimal feature subsets
that yield higher classification accuracy with fewer features. However, they generally
apply sequential forward greedy search mechanism which may lead to non-optimal feature subsets, i.e., local convergence problems. To alleviate this problem, EC techniques
have been successfully adopted into information theoretic feature subset selection frameworks. We investigate EC-based information theoretic approaches according to the number
of objectives:
1) Single-objective approaches: According to Xue et al. , PSO is known to be
one of the most commonly used EC technique in feature subset selection tasks due to
its longer and superior performance among EC techniques. Inspired by the MIFS criterion, a binary PSO-based information theoretic approach was
introduced. However, the binary PSO (BPSO) variant may encounter local-convergence
problems, and the control parameter, which is a trade-off between relevancy and redundancy, needs to be specified. Butler-Yeuoman et al. adopted a wrapper mechanism into a PSO-based information theoretic approach. Although this approach could be
Jjmi = I(xi, y) −1
I(xi, xs) −I(xi, xs|y)
Jfou = I(xi, y) −훽
I(xi, xs) + 휆
I(xi, xs|y)
SU(xi, y) = H(xi) −H(xi|y)
H(xi) + H(y)
Jfmifs =FI(xi, y) −훽
FI(xi, xs)
FSU(xi, y) =FH(xi) −FH(xi|y)
FH(xi) + FH(y)
Machine Learning 113:4977–5000
treated as computationally efficient compared to pure wrapper approaches, the feature
subsets selected by these approaches may not enhance the performance of a classification algorithm since the number of wrapper evaluations in the framework accounted
for only a small proportion of the evaluations. Nguyen et al. integrated a filter
backward elimination strategy based on the MIFS criterion into the PSO-based wrapper
framework to improve the effectiveness of feature subset selection process. According to
the results, it performed better than recent PSO-based wrapper approaches. Teng et al.
 introduced a V-shaped BPSO-based information theoretic approach by adopting correlation information entropy which was a measurement of redundancy in multisensor systems. According to the results, the proposed approach performed better than
a variety of traditional feature subset selection approaches, such as ReliefF and MRMR. Nguyen et al. utilized a kernel mutual
estimator in PSO framework to deal with not only discrete-valued but also real-valued
datasets. However, the computational cost of this approach may be high.
Hancer et  al. developed a DE-based information theoretic approach which
uses the ranking scores of features obtained by ReliefF to eliminate irrelevant features,
while maximizing mutual relevancy between the class labels and the features. According to the results, the introduced filter criterion outperformed the MIFS criterion. However, the control parameter is not easily predefined. In another study, Hancer 
adopted a fuzzy information theoretic local search module in pure wrapper DE feature
subset selection approach by adding and removing features to the best solution in a
probabilistic manner. Al-Ani introduced an information theoretic feature subset selection approach, named ANT, based on ant colony optimization (ACO) and mutual information evaluation function (MIEF) . In contrast to previous approaches, ANT considered feature subset selection as
a combinatorial problem. Khushaba et al. proposed an improved variant of ANT
by hybridizing ACO and DE for EEG and MEC signal classification.
2) Multi-objective approaches: As the control parameter between relevancy and
redundancy is difficult to be predefined, researchers have considered the related information theoretic criteria as a multi-objective problem. Xue et al. introduced a
multi-objective PSO-based information theoretic approach, which used the crowding
distance metric of NSGA-II algorithm to determine non-dominated
solutions. According to the results, the multi-objective PSO approach was superior to
the single-objective PSO approach in terms of classification performance and dimensionality reduction. Hancer et al. introduced a multi-objective ABC-based fuzzy
information theoretic approach, which searches for non-dominated solutions using
genetic operators. According to the results, fuzzy information theoretic criterion performed better than the standard information theoretic criterion. Hancer et al. also
introduced a multi-objective DE-based information theoretic approach by hybridizing
information theoretic measures with ReliefF and Fisher Score. In contrast to existing
multi-objective feature subset selection approaches, this approach uses three objectives
namely mutual relevancy, ranking scores of ReliefF and Fisher Score. Although this
approach outperformed the multi-objective and single-objective DE-based approaches
based on standard information theoretic measures, its performance highly depended on
both ReliefF and Fisher Score. Hancer proposed a multi-objective DE-based
information theoretic approach by hybridizing standard and fuzzy information theoretic
measures in a simple way. According to the results, the improved criterion obtained
better performance than the standard criterion in multi-objective DE design. In another
Machine Learning 113:4977–5000
study, Hancer introduced a multi-objective DE-based fuzzy kernel information
theoretic approach to deal with not only discrete-valued but also real-valued problems.
In summary, EC-based information theoretic approaches have exhibited a growing research interest in the past ten years. Especially when hybridized with wrapper
approaches, they tend to improve the classification performance in a considerable rate;
however, this is not mostly sufficient for high dimensional problems. Therefore, the need
for developing hybridized information theoretic approaches with wrapper approaches has
not come to an end.
3  Proposed wrapper‑filter approach
3.1  Overall approach
The general flowchart of the proposed wrapper-filter approach (called IDE-FLS) is presented in Fig. 1. The dataset is divided into training and test sets. After the division process, the evolutionary process is carried out to select optimal features in the training set.
First, the population is initialised based on random and fuzzy initialisation, described in
Sect. 3.2. For each solution Xi , a trial solution ( Ui ) is generated using mutation and recombination operators presented in Eqs. (2) and (3) as described in Sect. 2.2. If Ui is a fitter
solution in terms of the fitness function described in Sect. 3.3, then Ui is selected for the
population instead of Xi . After the selection process, the best solution (gbest) in the population is determined and then the local search module (described in Sect. 3.4) is carried
out on gbest to remove unwanted features from the subset or add beneficial features from
the unselected feature set into the subset. Once the evolutionary process is completed, the
dimensionality of training and test sets is reduced based on the selected features. The classification model is built on the reduced training set, and then the classification performance
is verified using the reduced test set.
3.2  Initialisation scheme
For subset selection, the most commonly applied search strategies are forward and backward selection. Forward selection strategy starts with an empty feature subset and adds
one feature to the subset according to a predefined selection criterion. Another feature is
then selected for the subset if adding the corresponding feature to the subset maximally
improves the performance of the evaluation criterion. This procedure is repeated until the
further addition of any candidate feature does not improve the performance of the evaluation criterion. On the other hand, backward selection strategy starts with all available
Local Search
Fig. 1   Overall mechanism of IDE-FLS
Machine Learning 113:4977–5000
features in the data and then sequentially removes features from the subset until the further removal of any candidate feature does not improve the performance of the evaluation
criterion. It can therefore be revealed that forward selection tends to select smaller feature
subsets and thereby is computationally less intensive than backward selection, but when
the optimal feature subset involves a large number of features, backward selection performs better in terms of finding relatively optimal feature subsets. Motivated by the selection strategies, we propose a new initialisation scheme using the principles of fuzzy mutual
information.
Assume that Xk = {xk1, xk2, … , xkd, … , xkD} is a D-dimensional kth possible solution
(feature subset) such that xkd ∈  , where D is the number of features, and xkd represents
the activation value of the dth feature in solution k. If xkd > 0.5 , dth feature is selected for
the feature subset of solution k. In the proposed scheme, the population is initialised using
two initialisation strategies:
1) Random initialisation: Each solution, as a possible feature subset, is initialised by the
random combination of individual features. Thus, the activation values ( xid ) of solution i
are randomly generated within the range of 0 and 1.
2) Fuzzy initialisation: The fuzzy mutual relevancy scores of all available features with
the class labels are first calculated using Eq. (12). All available features are then sorted in
descending order. Finally, a predefined percentage (ph1) of highly relevant features, i.e.,
the same best ph1 features, are selected for each solution. Thus, the activation values ( xid )
of the selected features in solution i will be between 0.5 and 1, while the values of the
remaining features in solution i will be between 0 and 0.5.
Notice that only a small number of solutions are generated using fuzzy initialisation,
i.e., the ratio of the number of solutions initialised by fuzzy initialisation to the population size is very small. Otherwise, a certain degree of diversity in the population cannot be
maintained and the evolutionary process will be prone to local convergence problems.
3.3  Fitness function
To achieve satisfactory results, a feature selection approach should fulfill the following
objectives: 1) maximize the classification performance, and 2) minimize the number of
features. In order to take into consideration the conflicting objectives in a single-objective
problem, a weighted linear aggregation function is defined as the fitness function, shown in
where ErrorRate is the ratio of the number of wrongly classified instances to the total number of instances, M is the number of features in the selected feature subset, and N is the
number of all available features in the data.
To calculate ErrorRate in Eq. (21), we require a classification algorithm. Among a
variety of classification algorithms in the literature, we select K-nearest neighbors as
a classification algorithm due to its efficiency and popularity in feature selection problems. In K-nearest neighbors, the number of neighbors, defined as K, is set to 5 (called
5NN), as in a variety of related works in the literature . To evaluate ErrorRate of a candidate solution, the training set is reduced through
the selected feature subset and then divided into 10 folds using the 10-fold cross validation method. All folds except for one-fold are used to build a classification model using
Fitness = 훼× ErrorRate + (1 −훼) × M
Machine Learning 113:4977–5000
5NN, while the remaining fold is used to validate the classification model. This process
is repeated 10 times, with a different fold reserved for validation each time. At the end
of the process, ErrorRate is obtained by computing the average for all the 10 validation
3.4  Local search module
The overall goal of local search module involves investigating in the neighborhood of
the best solution (called gbest) from the population to find fitter solutions,i.e., feature
subsets. This is achieved by eliminating or adding features in a probabilistic manner
such that if a randomly generated number within the range of 0 and 1 is smaller than a
user specified value (Th), the elimination process is carried out; otherwise, the add process is carried out. The corresponding processes are defined as follows.
1) The elimination process: In this process, we aim to remove unwanted features
from the feature subset for a further learning process. The selected features in gbest are
ranked in case of the fuzzy version of MIFS, defined by Eq. (19). The first predefined
percentage (ph2) of features are then determined as candidate features, and a random
number is generated between 0 and 1 for each candidate feature. If any generated random number is smaller than a user specified threshold ( 훽 ), its corresponding candidate
feature is eliminated from the selected subset. In other words, the corresponding activation codes of the features selected for elimination in gbest are set to between 0 and 0.5.
2) The add process: In this process, we aim to detect beneficial features from the
unselected features for the further learning process. The unselected features are ranked
in descending order in terms of the fuzzy version of symmetric uncertainty (FSU),
defined by Eq. (20). The first predefined percentage (ph2) of unselected features are
then determined as candidate features, and the mean FSU value of candidate features
is calculated. If the FSU value of any candidate feature is greater than the mean FSU
value, it is added to the feature subset. In other words, in gbest, the corresponding activation codes of the candidate features selected for adding are set to between 0.5 and 1.
Once the elimination/add processes are carried out in a probabilistic manner, the
selection process between the current gbest and updated gbest′ solutions is performed. If
the fitness value of gbest′ is better than that of gbest, gbest′ is kept as the best solution in
the population. This procedure is repeated for a specified number of trials (TC).
4  Experimental design
4.1  Datasets
To carry out a comprehensive comparative study, we select a variety of discrete-valued
and real-valued datasets from UCI machine learning and scikitfeature repositories, presented in Table 1. The first nine are low-dimensional datasets including features within the range of 35 and 180, while the remaining
are high-dimensional datasets including the number of features between 256 and 7129.
Each dataset is randomly divided into 7/10 as training set and 3/10 as test set.
Machine Learning 113:4977–5000
4.2  Approaches used in experiments
To prove the effectiveness of the proposed IDE-FLS approach, we first compare it with pure
wrapper DE, a recently introduced DE-based wrapper-filter approach (DE-FLS) , and the classification performance obtained by using all available features in the dataset. All the compared feature subset selection approaches use the same settings for fitness
function and common parameters. We also compare the proposed IDE-FLS approach with
conventional feature subset selection approaches. Due to the popularity and ability to automatically detect the number of selected features, we choose linear forward selection (LFS),
correlation-based feature selection (CFS) , and fast correlation-based feature
selection (FCBF) . Inspired by the sequential forward selection (SFS), LFS
is a wrapper approach which gradually adds features until no further improvement is observed
in the classification performance. LFS restricts the number of features to be considered in each
iteration and can therefore find smaller feature subsets than SFS with higher classification performance. In contrast to LFS, CFS is a filter approach which tends to select a feature subset
with relevant features based on the correlation metric. Since half of the selected datasets in the
experiments are high-dimensional, we apply the greedy stepwise selection strategy in CFS.
FCBF is a two-stage subset selection approach which first appends features in descending
order in terms of their correlation matrix and then applies a heuristic search to eliminate irrelevant and redundant features. Weka is used to run the three feature subset
selection approaches with their default settings.
Table 1   Datasets
No. of features
No. of classes
No. of instances
Machine Learning 113:4977–5000
4.3  Parameter settings
The common parameters of the DE-based feature subset selection approaches are defined
as follows : the population size and the maximum number of cycles are
selected as 30 and 35, respectively; F and CR are respectively set to the mostly preferred
values, 0.8 and 0.7; the 훼 value in Eq. (21) is set to 0.9. In IDE-FLS, only 5 solutions
are generated by fuzzy initialisation, where the percentages of the highly relevant features
selected for the feature subset (ph1) is set to 12.5% for all datasets. In both DE-FLS and
IDE-FLS, the local search module is performed 5 times at the end of each iteration. As
the elimination process becomes a much more crucial issue than the add process in highdimensional datasets, the 훽 and Th values used as thresholds for the elimination process
are set to 0.6 as suggested in Hancer . Moreover, the percentage of candidate features (ph2) used to carry out the elimination and add processes is set to 20%. Finally, to
accomplish fair comparative analysis with conventional approaches, we used the suggested
default parameter values for conventional subset selection approaches.
5  Experimental results
In this section, we first verify the superiority of the proposed feature subset selection
approach by comparing it with recently introduced DE-based feature subset selection
approaches and the original feature set over 30 independent runs. We then analyze the CPU
time complexity of DE-based approaches. Thirdly, we show the superiority of the proposed
approach against conventional feature subset selection approaches. We finally conclude the
section by presenting a case study on the parameter analysis of the proposed approach.
5.1  Comparisons with DE‑based approaches
Table 2 shows the training and testing accuracies of 5NN in terms of mean (’Avg’), standard deviation (’Std’) and best values using the available feature set (‘Full’), and the feature
subsets obtained by the three DE-based subset selection approaches on each dataset. The
mean values of the feature subset sizes are presented in the third column. Columns denoted
via ’T’ display the results of the Wilcoxon Rank Sum test (with a significance level of
0.005) of the proposed approach versus Full, DE and DE-FLS. Here, ’+’ (’−’) means the
proposed subset selection approach achieves significantly better (worse) results than the
corresponding subset selection approach, while ’=’ means the proposed and corresponding
subset selection approaches obtain similar results.
1) IDE-FLS versus Full: As seen from Table 2, the size of feature subsets selected by
IDE-FLS on all datasets is generally smaller than the size of the original feature set with
the best ratio of 47/7129 on Allaml. Aside the dramatically minimized number of features, there is also a maximization of the accuracy as IDE-FLS performs well on almost all
datasets. In particular, the feature subsets obtained by IDE-FLS significantly improve the
training performance on all datasets except for Optic, and the testing performance on fourteen out of eighteen datasets. In terms of testing accuracy, the highest improvement can be
observed for Splice with an average of 21.94%. For Leukemia dataset, the proposed subset
selection approach selects less than 250 features to achieve a 100% training and testing
Machine Learning 113:4977–5000
Table 2   Results of DE-based approaches
Training accuracy
Testing accuracy
92.35 ± 0.61
87.45 ± 1.17
91.99 ± 0.63
87.98 ± 1.28
93.22 ± 0.57
87.72 ± 1.87
93.78 ± 1.09
83.66 ± 6.68
93.97 ± 1.39
86.22 ± 4.26
95.21 ± 1.13
86.56 ± 5.29
89.27 ± 1.02
74.46 ± 4.32
89.44 ± 1.12
76.77 ± 4.61
90.36 ± 1.33
78.28 ± 4.23
75.14 ± 0.77
70.80 ± 8.82
79.33 ± 2.51
78.94 ± 3.39
86.61 ± 4e−16
97.84 ± 0.35
97.48 ± 0.41
97.57 ± 0.36
97.31 ± 0.38
97.62 ± 0.36
97.27 ± 0.36
84.74 ± 1.45
71.07 ± 1.31
84.79 ± 1.36
72.51 ± 2.48
83.30 ± 1.90
73.44 ± 1.71
93.73 ± 0.10
93.77 ± 0.11
94 ± 1.3e−05
94.04 ± 7.6e−05
94.01 ± 2.8e−04
94.04 ± 5.9e−04
90.65 ± 0.63
83.33 ± 2.19
90.44 ± 0.98
83.71 ± 2.36
90.67 ± 0.85
84.71 ± 2.51
86.28 ± 0.55
84.01 ± 1.04
89.81 ± 0.13
88.90 ± 1.61
92.05 ± 0.45
91.01 ± 0.94
91.74 ± 0.38
88.45 ± 1.13
91.49 ± 0.57
88.12 ± 1.17
91.06 ± 0.44
87.88 ± 0.99
Machine Learning 113:4977–5000
accuracies in all 30 independent runs, which is more than 8% improvement on Full. It is
no doubt that such high improvement on Full is the result of eliminating features from the
original feature set that have detrimental effect on training the classification model.
2) IDE-FLS versus standard DE: Although DE eliminates the original features at least
by half, IDE-FLS still selects smaller feature subsets than DE on all datasets and achieves
significantly better testing performance on 15 datasets than DE with the highest improvement of 19.05% and the highest dimensionality reduction of 98.64% on Allaml. Furthermore, IDE-FLS never obtains significantly worse training and testing performance than DE
Table 2   (continued)
Training accuracy
Testing accuracy
76.11 ± 1.1e−16
79.76 ± 0.51
85.95 ± 2.84
85.58 ± 4.09
88.46 ± 0.21
85.48 ± 0.56
84.80 ± 1.40
85.27 ± 1.10
85.59 ± 1.80
86.02 ± 1.03
85.63 ± 1.48
86.30 ± 2.41
68.89 ± 4.52
88.03 ± 2.54
73.15 ± 5.28
89.08 ± 1.88
73.15 ± 6.54
80.81 ± 0.68
77.53 ± 1.53
81.95 ± 1.13
80.87 ± 1.61
87.22 ± 0.79
84.40 ± 0.83
83.92 ± 0.55
79.53 ± 1.45
83.48 ± 0.62
80.46 ± 1.50
84.61 ± 0.87
80.89 ± 1.62
87.33 ± 0.54
88.14 ± 1.60
88.32 ± 1.15
88.89 ± 2.30
92.38 ± 0.54
91.11 ± 0.52
91.93 ± 1.94
91.43 ± 2.91
99.97 ± 0.10
97.78 ± 2.99
85.99 ± 0.54
74.44 ± 3.18
98.41 ± 2.57
93.33 ± 4.94
99.15 ± 0.29
93.49 ± 3.64
Machine Learning 113:4977–5000
on almost all datasets and obtains similar training and testing performance with DE on
only five and three datasets, respectively.
3) IDE-FLS versus DE-FLS: As seen in Table  2, IDE-FLS eliminates much more
unwanted features than DE-FLS on almost all datasets to further improve the training and
testing performance. The highest improvement is reported for Splice with 7.28% higher
training and 9.77% higher testing accuracy, respectively. Another higher improvement, in
terms of training and testing performance, is observed for PCMAC with 5.27% and 3.53%,
respectively. On the datasets where IDE-FLS and DE-FLS obtains similar performance,
IDE-FLS generally can reduce the feature subset size at least half or more than half of the
subset size selected by DE-FLS. The highest feature reduction is seen for Basehock where
IDE-FLS selects 8 times fewer features than DE-FLS but still averagely improves by 4.06%
and 2.22% in training and testing performance respectively.
In summary, IDE-FLS gets 77 wins, 6 losts and 25 draws out of the 108 comparisons
in terms of training and testing performance while eliminating higher number of features
from the original feature set in almost all cases. Thus, it can be deduced that IDE-FLS carries out a much better search than the compared subset selection approaches. The effectiveness of IDE-FLS is accounted for by two mechanisms, the initialisation scheme and the
local search module, both of which are constructed using the principles of fuzzy mutual
estimator. The initialisation scheme enables the algorithm to direct search efforts into profitable areas of the search space without losing diversity in the population. The local search
mechanism enables the algorithm to eliminate much more irrelevant and redundant features from the fittest feature subset without additional computational cost.
5.2  Comparisons of computational time
The experiments are carried on a PC with i5-6400 and 8 GB RAM using
MATLAB platform. Table  3 shows the results of CPU computational time in terms of
mean (’Avg’), standard deviation (’Std’) and the Wilcoxon Rank Sum test (’T’) over 30
independent runs. The symbols in each column ’T’ are denoted and used as in Table 2.
As seen from Table 3, IDE-FLS generally completes the selection process faster than
the other DE-based subset selection approaches. Especially, for large volume datasets
including higher number of features and samples, the gap between IDE-FLS and the other
compared approaches is very large, i.e., IDE-FLS can save a large amount of the evaluation
time needed for the evolutionary search. For instance, for both PCMAC and Relathe, the
required running time for the selection process in DE and DE-FLS is at least two times or
more than two times longer than IDE-FLS. It is only for experiments with some small volume datasets such as Soybean and Sonar that IDE-FLS does not show better efficiency than
both DE and DE-FLS. In summary, without no doubts, IDE-FLS is the fastest subset selection approach among all of the compared approaches. This may be due to the initialisation
mechanism which rapidly directs the algorithm into profitable regions of the search space
without consuming extra time.
5.3  Comparisons with conventional approaches
To show whether the proposed IDE-FLS approach performs better than the conventional
subset selection approaches, we compare the results of DE-FLS with those of LFS, CFS
and FCBF, all of which do not require the number of features as a predefined parameter to
determine the feature subset. Table 4 presents the results of the feature subset size (’No’),
Machine Learning 113:4977–5000
Table 3   Results of CPU time (in minutes)
10.44±0.18
22.97±0.64
20.93±0.31
19.22±0.54
17.84±1.16
11.63±0.53
82.77±1.25
62.81±2.01
77.84±4.07
57.41±1.89
34.51±1.74
26.39±1.24
63.26±11.4
Machine Learning 113:4977–5000
Table 4   IDE-FLS versus conventional approaches
87.72±1.87
86.56±5.29
78.28±4.23
97.27±0.36
73.44±1.71
94.04±5.9e−04
84.71±2.51
91.01±0.94
87.88+0.99
85.63+1.48
Machine Learning 113:4977–5000
Table 4   (continued)
73.15+6.54
84.40+0.83
80.89+1.62
91.11+0.52
93.49+3.64
Machine Learning 113:4977–5000
mean (’Avg’) and best (’Best’) testing accuracy, and the Wilcoxon Rank Sum test (’T’).
The symbols in each column ’T’ are denoted and used as in Table 2.
1) IDE-FLS versus LFS: As seen from Table 4, LFS finds feature subsets with less than
35 features for all datasets and so obtains the smallest feature subsets of all the compared
approaches. However, these smallest subsets produce significantly lower testing accuracy
than IDE-FLS for all datasets with at least or more than a 10% difference for eight cases.
For example, on Audiology, IDE-FLS selects 5 more features to obtain an average 15%
higher testing accuracy than LFS and a 20% higher performance for the best case. Only
for Optic, Coil2000 and DNA datasets, does IDE-FLS perform similarly as LFS on average, but in the best case, IDE-FLS performs better than LFS. It can therefore be concluded
that LFS frequently tends to encounter local convergence problems, resulting in smaller but
lower performance feature sets.
2) IDE-FLS versus CFS: Although CFS generally selects fewer features than IDE-FLS
especially for high-dimensional datasets, CFS obtains significantly worse performance than
IDE-FLS for almost all datasets. For instance, selecting 20 more features on Allaml, IDE-
FLS obtains 3.01% higher testing accuracy than CFS on average and 9.52% higher in the
best case. The only worse performance of IDE-FLS occurs while selecting a larger number
of features for Colon, where IDE-FLS obtains significantly worse testing performance on
average than CFS, but achieves similar performance in the best case. Moreover, IDE-FLS
selects a smaller or nearly similar number of features than CFS for eight datasets to obtain
significantly better performance. We also notice that CFS completes the selection process
within a longer time than IDE-FLS on high-dimensional datasets, even though it is a filter
which employs a deterministic subset selection approach.
3) IDE-FLS versus FCBF: FCBF tends to select smaller feature subsets but generally
gets significantly worse accuracy than IDE-FLS. In other words, these smallest feature subsets are not able to achieve significantly better performance than IDE-FLS for most datasets. Selecting 76 more features for Semeion, IDE-FLS gets 8.09% higher performance
than FCBF on average and 10.84% higher in the best case. Although FCBF sometimes
obtains significantly similar or better performance on average than IDE-FLS, IDE-FLS
always gets better accuracy in the best case. We also notice that FCBF sometimes tends to
be stuck in local optima. For example, on Madelon, FCBF only selects six features but gets
57.69% accuracy, while all of the compared approaches report more than 70% accuracy.
In summary, IDE-FLS gets 39 wins, 8 draws and 7 losts among 54 comparisons with the
three subset selection approaches.
5.4  Performance analysis of fuzzy initialisation parameter
The goal of this section is to further analyze the effect of fuzzy initialisation parameter on
the performance of the proposed IDE-FLS algorithm. Table 5 shows the results of IDE-
FLS with ph1 values from 0.5 to 0.125 in descending order over 30 independent runs in
terms of the subset size (’#No’), the training performance (’Training Accuracy’) and the
testing performance (’Testing Accuracy’).
According to Table 5, IDE-FLS with different ph1 values generally obtains similar performance for all performance evaluation criteria for the first nine datasets, i.e., the ph1
parameter does not make significant difference on the performance of the IDE-FLS algorithm. For the remaining datasets, IDE-FLS also achieves similar training and testing
performance with different ph1 values, but the number of features tends to dramatically
inversely decrease in proportion to the ph1 values for almost all datasets. In particular,
Machine Learning 113:4977–5000
Table 5   Results of IDE-FLS
with different ph1 values
Training accuracy
Testing accuracy
92.58±0.36
87.63±1.18
92.20±0.57
87.68±1.36
92.16±0.57
87.72±1.87
86.11±4.30
94.84±1.11
86.11±4.64
95.21±1.13
86.56±5.29
89.82±1.13
78.55±4.94
89.77±0.85
78.82±4.52
90.36±1.33
78.28±4.23
79.79±2.43
78.45±3.64
85.82±1.69
87.41±2.70
86.61±4e-16
97.87±0.33
97.57±0.37
97.75±0.34
97.31±0.46
97.62±0.36
97.27±0.36
83.17±2.30
73.79±2.44
83.30±1.55
73.74±2.97
83.30±1.90
73.44±1.71
94.00±1.4e-04
94.04±2e-04
94.01±4.2e-04
94.04±2.7e-04
94.01±2.8e-04
94.04±5.9e-04
90.82±0.86
84.07±1.80
90.79±0.79
83.74±1.74
90.67±0.85
84.71±2.51
90.23±1.01
88.61±1.73
90.90±0.75
89.84±0.86
92.05±0.45
91.01±0.94
90.97±0.57
87.69±1.18
87.63±0.87
91.06±0.44
87.88±0.99
87.43±1.75
87.34±2.38
88.32±0.37
88.59±1.02
88.46±0.21
85.97±1.19
85.33±1.31
86.27±1.05
85.61±1.38
86.02±1.03
85.63±1.48
89.44±2.04
74.81±5.97
89.29±2.43
73.52±4.04
89.08±1.88
73.15±6.54
83.93±0.83
81.55±1.42
85.97±0.75
83.91±1.43
87.22±0.79
84.40±0.83
83.59 ± 0.94
81.29±1.79
84.10± 0.83
80.86±1.92
84.61±0.87
80.89±1.62
Machine Learning 113:4977–5000
there is a significant difference between ph1 = 0.5 and the other ph1 values such that the
feature subset reduction is at least half or more than half in some higher dimensional datasets such as PCMAC, Basehock and Allaml. Notice that we have not provided the results of
a statistical test in this section since there is generally no significant difference between the
results for different ph1 cases in terms of the training performance and the testing performance. It can therefore be concluded that the ph1 parameter has a confident effect for the
elimination process of irrelevant features especially for high-dimensional datasets.
6  Conclusions
In this paper, we developed a new DE-based wrapper-filter feature subset selection
approach by adopting initialisation and local search mechanisms based on fuzzy mutual
estimator. According to a variety of experiments on several datasets, the proposed wrapperfilter approach outperformed recently introduced DE-based and traditional approaches in
terms of the classification performance, the feature subset size and the computational cost.
This is because the proposed approach can detect the effective regions of a possible solution space without much effort and thereby does not frequently encounter local-convergence problems. Secondly, the proposed approach has shown a remarkable performance
at removing redundant and irrelevant features especially in high-dimensional datasets
thanks to the proposed fuzzy initialisation scheme. To be specific, the proposed initialisation scheme initialises few solutions with a predefined number of best features according to the fuzzy mutual estimator and so helps the proposed approach to easily find the
relevant features without losing global search ability. Furthermore, the proposed approach
can deal with real-valued datasets without the requirement of discretization by applying
fuzzy estimator in the initialisation and local search mechanisms. Due to the high ability
of removing irrelevant and redundant features, the proposed method can be also used for
gene selection and classification processes. In the proposed approach, we use the typical
representation scheme which selects features by applying a threshold on the positions of a
solution. Although the typical representation scheme can be treated as simple and efficient,
the strong relationships between features may not be determined by applying this scheme.
This issue will be the motivation of our future work.
Acknowledgements  The author would like to thank the PhD candidate Emrah Budur from the Department
of Computer Engineering at Bogazici University for his helpful comments.
Table 5   (continued)
Training accuracy
Testing accuracy
90.19±1.19
90.49±1.64
90.79±1.17
90.63±1.50
92.38±0.54
91.11±0.52
98.89±2.05
99.84±0.87
98.53±1.74
93.97±3.52
99.09±0.17
93.17±2.98
99.15±0.29
93.49±3.64
Machine Learning 113:4977–5000
Declarations
Compliance with Ethical Standards  The author declares that he has no conflict of interest.