Distilling Knowledge from Deep Networks with
Applications to Healthcare Domain
Zhengping Che*, Sanjay Purushotham*, Robinder Khemani**, Yan Liu*
*Department of Computer Science, University of Southern California
**Children’s Hospital Los Angeles
*{zche, spurusho, yanliu.cs}@usc.edu, ** 
Exponential growth in Electronic Healthcare Records (EHR) has resulted in new
opportunities and urgent needs for discovery of meaningful data-driven representations and patterns of diseases in Computational Phenotyping research. Deep
Learning models have shown superior performance for robust prediction in computational phenotyping tasks, but suffer from the issue of model interpretability
which is crucial for clinicians involved in decision-making. In this paper, we
introduce a novel knowledge-distillation approach called Interpretable Mimic
Learning, to learn interpretable phenotype features for making robust prediction
while mimicking the performance of deep learning models. Our framework uses
Gradient Boosting Trees to learn interpretable features from deep learning models
such as Stacked Denoising Autoencoder and Long Short-Term Memory. Exhaustive experiments on a real-world clinical time-series dataset show that our method
obtains similar or better performance than the deep learning models, and it provides interpretable phenotypes for clinical decision making.
Introduction
With the exponential surge in the amount of electronic health records (EHR) data, there come both
the opportunities and the urgent needs for discovering meaningful data-driven characteristics and
patterns of diseases, which is known as phenotype discovery. Clinicians are collaborating with
machine learning researchers to tackle many computational phenotyping problems to improve the
state of healthcare services and this is paving the way for Personalized Healthcare . Robust prediction is critical in healthcare research since it is directly related to saving patient lives. Recent
works have used deep learning models to achieve state-of-the-art performance on computational phenotype prediction problems. However, deep learning models are less interpretable while
clinicians mainly rely on interpretable models to make informed clinical decisions. Thus, the fundamental question is how we can develop new data-driven machine learning techniques which can
achieve state-of-the-art performance as deep learning models and also discover interpretable features
(phenotypes).
Deep learning models are revolutionizing many ﬁelds such as computer vision , and
speech and language processing , and have achieved the status as the go-to state-of-theart techniques for many machine learning tasks. With the ﬂexibility and power of all kinds of
neural networks, some deep network variants are also potentially suitable for healthcare tasks .
Autoencoder is used to capture structures in data and aims to reconstruct the input. It has been
successfully used for feature extraction or as a pre-training step for a neural network . Long
Short-Term Memory (LSTM) architecture has been widely used for sequential data and tasks
recently . It reads the input sequence one time step at a time and provides ﬁxed-length
or step-by-step representations, while keeping the long range temporal dependencies. While deep
models perform superior to many approaches when the data is abundant, their performance can drop
 
signiﬁcantly when the data is noisy and sparse or when the model is not properly initialized .
Also, the features learned using deep models are generally not interpretable.
There is limited work on interpreting the features learned by deep learning outside of computer
vision . Recent research has begun to provide a more rigorous understanding of the representations learned by deep architectures. It has been shown that the semantics encoded by hidden unit
activations in one layer are preserved when projected onto random bases, instead of the next layer’s
bases . This implies that the practice of interpreting individual units can be misleading and that
the behavior of deep models may be more complex than previously believed. In healthcare, model
interpretability is not only important but also necessary, since the primary care providers, physicians
and clinical experts alike depend on the new healthcare technologies to help them in monitoring and
decision-making for patient care. A good interpretable model is shown to result in faster adoptability among the clinical staff and results in better quality of patient care . Therefore we need
to identify novel solutions which can provide interpretable models and achieve similar prediction
performance as deep models in healthcare domain.
In order to capture the performance of deep learning models using other models, a knowledge distillation process such as mimic learning or dark knowledge is useful. In these works, it
was noted that once a deep learning model is trained on a large-scale dataset, we can use a smaller
model to distill the knowledge by training it on the “soft target”, i.e., the class probability produced
by the former model. This means that simple models can possibly match (mimic) the prediction
performance of deep models. Thus, choosing the right simple model will help to extract informative
physiologic patterns which in-turn helps to discover meaningful interpretable features.
Building upon the recent breakthrough in mimic learning, in this paper, we introduce our knowledgedistillation approach called Interpretable Mimic Learning, to learn interpretable features for making robust prediction while mimicking the performance of deep learning models. Unlike the standard mimic learning , our interpretable mimic learning framework uses Gradient Boosting Trees
(GBT) to learn interpretable features from deep learning models. We use GBT as our mimicking
model since they not only provide interpretable decision rules and tree structures, but also successfully maintain the performance of original complex models such as deep networks. Our main
contributions in this paper include:
• We propose a novel knowledge distillation methodology called Interpretable Mimic Learning where we mimic the performance of state-of-the-art deep learning models using wellknown Gradient Boosting Trees (GBT).
• We conduct extensive experiments on several deep learning architectures including Stacked
denoising autoencoders (SDA) and Long Short Term Memory (LSTM) to show that our
Interpretable Mimic Learning models can achieve state-of-the-art performance on multiple
deep learning models.
• We discuss the interpretable features and decision rules learned by our Interpretable Mimic
Learning models, which is validated by the expert clinicians. We also conduct experiments
to investigate, for different deep networks, whether using neural network extracted features
rather than soft labels improves mimicking performance.
The remainder of this paper is arranged as follows: In Section 2, we provide an overview of the
related work; In Section 3, we describe our proposed Interpretable Mimic Learning framework
and discuss the related deep learning models; An evaluation on empirical results and interpretable
features is presented in the Section 4; We conclude with discussion, summary and future work in the
Section 5.
Related Work
In this section, we ﬁrst provide an overview of the state-of-the-art deep learning approaches used
in the healthcare domain, and then we discuss the recent advances in Mimic learning approaches.
Recently, there is a growing interest in applying deep learning techniques to computational phenotyping due to the increasing availability of the Electronic Healthcare Records (EHR) and the
need for Personalized Healthcare . One of the ﬁrst applications of modern deep learning
to clinical time series was described in , where the authors use autoencoders to learn features
from longitudinal clinical measurement time series and show interpretable features which are useful
for classifying and clustering different types of patients. In our previous work , we proposed a
novel scalable deep learning framework which models the prior-knowledge from medical ontologies
to learn interpretable and clinically relevant features for patient diagnosis in Intensive Care Units
(ICU). A recent study showed that a neural network model can improve the prediction of the
likelihood of several psychological conditions such as anxiety, behavioral disorders, depression, and
post-traumatic stress disorder. Other recent works also leverage the power of deep learning
approaches to model diseases and clinical time series data. These previous works have successfully
showed the state-of-the-art performance of deep learning models for the healthcare domain but they
have made limited attempts at the interpretability of the features learned by deep learning models,
which prevents the clinician from understanding and applying these models.
As pointed out in the introduction, model interpretability is not only important but also necessary
in healthcare domain. Decision trees - due to their easy interpretability - have been quite successfully employed in the healthcare domain and clinicians have embraced it to make
informed decisions. However, decision trees can easily overﬁt and they do not achieve good performance on datasets with missing values which is common in today’s healthcare datasets. On the
otherhand, deep learning models have achieved remarkable performance in healthcare as discussed
in the previous paragraph, but their learned features are hardly interpretable. Here, we review some
recent works on interpretability of deep learning features conducted in computer vision ﬁeld. 
studied the visualizations of the hierarchical representations learned by deep networks. investigated not only the visualizations but also demonstrated the feature generalizablility in convolutional
neural networks. argued that interpreting individual units can be misleading and postulated that
the semantics encoded by hidden unit activations in one layer are preserved when projected onto
random bases, instead of the next layer’s bases. These works show that interpreting deep learning
features is possible but the behavior of deep models may be more complex than previously believed.
Therefore we believe there is a need to identify novel solutions which can provide interpretable
models and achieve similar prediction performance as deep models.
Mimicking the performance of deep learning models using shallow models is a recent breakthrough
in deep learning which has captured the attention of the machine learning community. showed
empirically that shallow neural networks are capable of learning the same function as deep neural
networks. They demonstrated this by ﬁrst training a state-of-the-art deep model, and then training
a shallow model to mimic the deep model. Motivated by the model compression idea from ,
 proposed an efﬁcient knowledge distillation approach to transfer (dark) knowledge from model
ensembles into a single model. takes a Bayesian approach for distilling knowledge from a
teacher neural network to train a student neural network. Excellent performance on real world tasks
using mimic learning has been recently demonstrated in . All these previous works, motivate us
to employ mimic learning strategy to learn an interpretable model from a well-trained deep neural
network, which will be clearly discussed in the following section.
In this section, we will ﬁrst describe the notations used in the paper, and then we describe the stateof-the-art deep learning models which we use as the original models and the Gradient Boosting
Trees which we use as interpretable models for mimicking original models. Finally, we present the
general pipeline of our interpretable mimic learning framework.
EHR data contains both static and temporal features. Without loss of generality, we assume each
data sample has static records with Q variables and temporal data of length T and P variables, as
well as a binary label y ∈{0, 1}, where y usually represents a patient’s health state. By ﬂattening
the time series and concatenating static variables, we get an input vector X ∈RD for each sample,
where D = TP + Q.
Feedforward Network and Stacked Denosing Autoencoder
A multilayer feedforward network is a neural network with multiple nonlinear layers and possibly one prediction layer on the top. The ﬁrst layer takes X as the input, and the output of each
layer is used as the input of the next layer. The transformation of each layer l can be written as
X(l+1) = f (l)(X(l)) = s(l) 
W (l)X(l) + b(l)
where W (l) and b(l) are respectively the weight matrix and bias vector of layer l, and s(l) is a
nonlinear activation function, which usually takes logistic sigmoid, tanh, or ReLU functions.
While we optimize the cross-entropy prediction loss and get the prediction output from topmost
prediction layer, the activation of the hidden layers are also very useful as learned features.
The Stacked Autoencoder has a very similar structure as feedforward network, but its main
objective is to minimize the squared reconstruction loss to the input instead of the cross-entropy
prediction loss, by using encoder and decoder networks with tied weights. Assume the encoder has
the same structure as feedforward network, then the l-th layer of the decoder takes the output Z(l+1)
from the next layer and transforms it by
Z(l) = s(l) 
W (l)TZ(l+1) + bd
where ZL+1 = X(L+1) is the output from the encoder, and ﬁnally Z(0) can be treated as the reconstruction of the original input. By adding noise to the input, i.e. hiding some input variables
randomly, but still trying to recover the uncorrupted input, we obtain the Stacked Denoising Autoencoder which is more robust to noise than the Stacked Autoencoder. After training a
stacked autoencoder, we add a logistic prediction layer on the encoder to solve the prediction task.
Long Short-Term Memory
If we want to apply temporal model and only focus on the P temporal variables, we can apply time
series models on input Xts = (x1, x2, · · · , xT )T ∈RT ×P , where xt ∈RP is the variables at time
t. Long Short-Term Memory (LSTM) is a popular recurrent neural networks for sequential data
and tasks. It is used to avoid the vanishing gradient problem which is prevalent in other recurrent
neural network architectures. Figure 1(a) shows the standard structure of an LSTM block with input,
forget, and output gates, which we use in our method. In step t, one LSTM block takes the time
series input xt at that time, cell state Ct−1 and output ht−1 from previous time step, and calculates
the cell state Ct and output ht at this time step. We use the following steps to compute the output
from each gate:
ft = σ (W fhht−1 + W fxxt + bf)
it = σ (W ihht−1 + W ixxt + bi)
˜Ct = tanh (W Chht−1 + W Cxxt + bC)
ot = σ (W ohht−1 + W oxxt + bo)
And the outputs from this LSTM block is computed as:
Ct = ft ∗Ct−1 + it ∗˜Ct
ht = ot ∗tanh(Ct)
where ∗refers to the element-wise multiplication of two vectors.
In our LSTM prediction model, we ﬂatten the output from each block, which is denoted as Xnn =
(xnn1, xnn2, · · · , xnnT ) = (h1, h2, · · · , hT ), and we add another prediction layer on top of them.
The model is shown in Figure 1(b).
Gradient Boosting Trees
Gradient boosting is a method which takes an ensemble of weak learners, usually decision trees, to optimize a differentiable loss function by stages. The basic idea is that the prediction
function F(x) can be approximated by a linear combination of several functions (under some assumptions), and these functions can be seeked using gradient descent approaches. At each stage m,
assume the current model is Fm(x), then the Gradient Boosting method tries to ﬁnd a weak model
hm(x) to ﬁt the gradient of the loss function with respect to F(x) at Fm(x). The coefﬁcient γm of
Cell State
(a) LSTM Block
Prediction Layer
(b) LSTM Prediction Model
Figure 1: A Sketch of Long Short-Term Memory Model
the stage function is computed by the line search strategy to minimize the loss. The ﬁnal model with
M stages can be written as
γihi(x) + const
In Gradient Boosting Trees, each weak learner is a simple classiﬁcation or regression tree. To keep
gradient boosting from overﬁtting, a regularization method called shrinkage is usually employed,
which multiplies a small learning rate ν to the stage function in each stage. In this situation, the
term γihi(x) in the update rule above is replaced by νγihi(x).
Interpretable Mimic Learning method
In this section, we describe a simple but effective knowledge distillation framework - the Interpretable Mimic Learning method also termed as the GBTmimic model, which trains Gradient Boosting Trees to mimic the performance of deep network models. Our mimic method aims to recognize
interpretable features while maintaining the state-of-the-art classiﬁcation performance of the deep
learning models. To investigate, for different deep networks, whether using neural network extracted
features rather than soft labels improve the mimicking performance we present two general pipelines
for our GBTmimic model. The two pipelines of GBTmimic model are shown in Figure 2.
In Pipeline 1, we utilize the learned features from deep networks and resort to another classiﬁer such
as Logistic Regression:
1. Given the input features X and target y, we train a deep neural network, either Stacked
Denoising Autoencoder or Long Short-Term Memory, with several hidden layers and one
prediction layer. We take the activations of the highest hidden layers as the extracted features Xnn from that deep network.
2. We then feed these new features into a standard classiﬁer, e.g., Logistic Regression, to train
on the same classiﬁcation task, i.e. the target is still y, and take the soft prediction scores
yc from the classiﬁer.
3. Finally we train a mimic model, i.e., Gradient Boosting Regression Trees, given the raw
input X and the soft targets yc to get the ﬁnal output ym with minimum mean squared
In Pipeline 2, we directly use the predicted soft-labels from deep networks.
1. The ﬁrst step is similar to that in Pipeline 1, where we train a deep neural network with
input features X and target y, but we take take the soft prediction scores ynn directly from
the prediction layer of the neural network.
2. Instead of training an extra classiﬁer with extracted features, we take the soft prediction
scores ynn use it as the target in training the mimic model. In other words, we train Gradient
Boosting Trees, which can output ym with minimum mean squared error to ynn, given the
raw input X.
Classifier
Figure 2: Training Pipeline for Mimic Method
After ﬁnishing the training procedure, we can directly apply the mimic model trained in the ﬁnal step
to the original classiﬁcation task. We compare these two different pipelines to investigate whether
utilizing the features extracted from the neural networks (Pipeline 1) will provide more beneﬁts
than only taking the soft-labels from the neural networks (Pipeline 2), which is what existing mimic
methods usually do. These two pipelines will be evaluated and discussed with detailed experimental
results in Section 4.
Our interpretable mimic learning model using GBT has several advantages over existing (mimic)
methods. First, gradient boosting trees is good at maintaining the performance of the original complex model such as deep networks by mimicing its predictions. Second, it provides better interpretability than original model, from its decision rules and tree structures. Furthermore, using soft
targets from deep learning models avoids overﬁtting to the original data and provides good generalizations, which can not be achieved by standard decision tree methods.
Experiments
We demonstrate the performance of our interpretable mimic learning framework on a real-world
healthcare dataset and compare it to the several methods introduced in the previous section. Our
experiments will help us to answer the following questions:
• How does our interpretable mimic learning perform when compared with state-of-the-art
deep learning and other machine learning methods?
• What are the interpretable features identiﬁed by our mimic learning framework?
• Do soft-labels from top layer of deep networks (Pipeline 2 in Section 3.5) obtain better
results than soft-labels of the same networks with Logistic Regression (Pipeline 1 in Section 3.5) for prediction tasks?
• Do static features help in performance improvement in our mimic learning framework?
In the remainder of this section, we will describe the datasets, experimental design and discuss our
empirical results and interpretations to answer the above questions.
Dataset Descriptions
We conducted a series of experiments on VENT dataset . This dataset consists of data from 398
patients with acute hypoxemic respiratory failure in the intensive care unit at Children’s Hospital
Los Angeles (CHLA). It contains a set of 27 static features, such as demographic information and
admission diagnoses, and another set of 21 temporal features (recorded daily), including monitoring
features and discretized scores made by experts, during the initial 4 days of mechanical ventilation.
Two of the time series features start from 0 in time step 0, so when we ﬂatten time series and
concatenate all features together, we omit these two 0-valued features and obtain the input feature
vector with length 27 + 21 × 4 −2 = 109. The missing value rate of this dataset is 13.43%, with
some patients/variables having a missing rate of > 30% . We perform simple imputation for ﬁlling
the missing values where we take the majority value for binary variables, and empirical mean for
other variables. Please see table 1 for a detailed summary of this dataset.
Experimental Design
We conduct two binary classiﬁcation tasks on VENT dataset:
Table 1: VENT Dataset Details
Feature type
Number of features
Feature examples
Static features
PIM scores, Demographics, Admission diagnosis, etc.
Temporal features
Injury markers, Ventilator settings, blood gas values, etc.
• Mortality (MOR) task – In this task we predict whether the patient dies within 60 days after
admission or not. In the dataset, there are 80 patients with positive mortality label (patients
• Ventilator Free Days (VFD) task – In this task, we are interested in evaluating a surrogate
outcome of morbidity and mortality (Ventilator free Days, of which lower value is bad), by
identifying patients who survive and are on a ventilator for longer than 14 days. Since here
lower VFD is bad, it is a bad outcome if the value ≤14, otherwise it is a good outcome. In
the dataset, there are 235 patients with positive VFD labels (patients who survive and stay
long enough on ventilators).
We report Area Under ROC (AUC) as the evaluation metric to compare proposed and related methods.
Methods and Implementation Details
We categorize the methods in our main experiments into three groups:
• Baseline machine learning algorithms which are popularly used in the healthcare domain:
Linear Support Vector Machine (SVM), Logistic Regression (LR), Decision Tree (DT), and
Gradient Boosting Trees (GBT).
• Neural network-based methods (NN-based): Deep Feed-forward Neural Network (DNN),
Stack Denoising Autoencoder (SDA), and Long Short-Term Memory (LSTM). Based on
the two pipelines of our Interpretable Mimic Learning methods, we have two kinds of NNbased methods:
– Using the neural network models to directly make classiﬁcation. We denote these
methods as DNN, SDA, and LSTM. (Pipline 2 in Section 3.5)
– Taking the activations of the highest hidden layers of the networks as the output features, and feeding them into Logistic Regression to obtain ﬁnal prediction. These
methods are denoted by LR-* (LR-DNN, LR-SDA, LR-LSTM) in this section. (Pipline 1 in Section 3.5)
• Our Interpretable Mimic Learning methods: For each of the NN-based methods described
above, we take its soft predictions and treat it as the training target of Gradient Boosting
Trees. These methods are denoted by GBTmimic-* (E.g., GBTmimic-LSTM, GBTmimic-
LR-SDA, etc). As a comparison, we also try Decision Tree (DTmimic-*) as the mimic
We train all the algorithms with 5 random trials of 5-fold cross validation. Our DNN and SDA implementations have two hidden layers and one prediction layer. We set the size of each hidden layer
twice as large as input size. We do 50 epochs of stochastic gradient descent (SGD) with learning
rate 0.001. For LSTM, we only take the time series features to ﬁt this model and do 50 epochs RM-
Sprop training with learning rate 0.001. We stack a prediction layer over the sequence output
of LSTM. When we take the output features from the LSTM model, we take the ﬂattened sequence
output. We implement all the deep networks in Theano and Keras platforms on a desktop
with 4-core CPU and 16GB RAM. For Decision Trees, we expand the nodes as deep as possible
until all leaves are pure. For Gradient Boosting Trees, we use stage shrinking rate 0.1 and maximum
number of boosting stages 100. We set the depth of each individual trees to be 3, i.e., the number of
terminal nodes is no more than 8, which is fairly enough for boosting. We implement all decision
tree based methods using the scikit-learn package.
Quantitative Results
Table 2 shows the prediction performance comparison of the models introduced in Section 4.3. We
observe that for both the classiﬁcation tasks (MOR and VFD tasks), the deep models obtain better
performance than standard machine learning baselines; and our interpretable mimic methods obtain
similar or better performance than the deep models. Our GBTmimic-LR-SDA and GBTmimic-
LR-DNN obtains the best performance in MOR and VFD tasks, respectively. We found that the
predictions of DNN and SDA with Logistic Regression is better than just using the deep models,
however this is not true for LSTM model. One possible reason is that LSTM captures the temporal
dependencies which help in prediction, while in other methods the time series are ﬂattened during
input and thus the temporal relations are not efﬁciently modeled. Similarly, the performance of our
interpretable mimic learning methods always improve upon DNN and SDA, and are comparable to
LSTM based methods.
Table 2: Classiﬁcation Results.
AUC: Mean of Area Under ROC;
AUC(std): Standard Deviation of Area Under ROC.
GBTmimic-DNN
GBTmimic-SDA
GBTmimic-LSTM
GBTmimic-LR-DNN
GBTmimic-LR-SDA
GBTmimic-LR-LSTM
Based on the observations from the above prediction results in Table 2, and by noticing that LSTM
only takes temporal features, we investigated whether time series features themselves are sufﬁcient
for our prediction tasks (i.e. we do not consider static features in input vectors). In other words, it is
useful to demostrate whether that the temporal models are more relevant than the just static models
based on the initial settings.
We conducted two new sets of experiments, 1) with only temporal features as input, and 2) with
only static features and the initial values of temporal features at day 0. We present the results of
these experiments in Table 3 and Table 4, respectively. From Table 3 we can notice that, for MOR
task, the prediction differences between temporal and all features are quite small (i.e. AUC(diff)),
while in VFD task, we ﬁnd that adding static features is relatively more critical to the prediction
performance. The different behaviours on these two tasks also explain why LSTM performs better
in MOR task than in VFD task. Note that we don’t show the LSTM results in Table 3 since we have
already used only temporal features for LSTM prediction task and the corresponding results is in
Table 2. Results from Table 4 further veriﬁed the superiority of the temporal models over just the
static model. For both MOR and VFD tasks, the performances of only static variables and initial
values of temporal variables degraded signiﬁcantly on all tested models, and are even worse than the
models with only temporal features in Table 3.
Table 3: Classiﬁcation Results of Input with Only Temporal Features.
AUC: Mean of Area Under ROC;
AUC(diff): AUC of all features (Table 2) - AUC of temporal features (Table 3);
AUC(std): Standard Deviation of Area Under ROC.
GBTmimic-DNN
GBTmimic-SDA
GBTmimic-LSTM
GBTmimic-LR-DNN
GBTmimic-LR-SDA
GBTmimic-LR-LSTM
Table 4: Classiﬁcation Results of Input with Static and Initial Temporal (at Day 0) Features.
AUC: Mean of Area Under ROC;
AUC(diff): AUC of all features (Table 2) - AUC of static and initial temporal features (Table 4);
AUC(diff2): AUC of temporal features (Table 3) - AUC of static and initial temporal features (Table 4);
AUC(diff2)
AUC(diff2)
GBTmimic-DNN
GBTmimic-SDA
GBTmimic-LR-DNN
GBTmimic-LR-SDA
Interpretations
One advantage of decision tree methods is their interpretable feature selection and decision rules. In
this section, we ﬁrst interpret the trees learned by our mimic framework, and then we compare and
contrast trees from our GBTmimic with trees obtained using original GBT.
Table 5 shows the top useful features, found by GBT and our GBTmimic models, in terms of the
importance scores among all cross validations. We ﬁnd that some important features are shared with
several methods in these two tasks, e.g., MAP (Mean Airway Pressure) at day 1, δPF (Change of
PaO2/FIO2 Ratio) at day 1, etc. Another interesting ﬁnding is that almost all the top features are
temporal features, while among all static features, the PIM2 (Pediatric Index of Mortality 2) and
PRISM (Pediatric Risk of Mortality) scores, which are developed and widely used by the doctors
and medical experts, are the most useful variables.
We can compare and interpret the trees obtained by our Interpretable Mimic learning with the original GBT trees. Figure 3 shows the examples of the most important tree built by the original GBT and
our interpretable mimic learning methods on the same cross validation fold for the MOR prediction
task. As we can see, they share some common features and similar rules. These selected features
and rules can be evaluated and explained by healthcare experts which will help them to understand
these models better and to make better decisions on patients.
Table 5: Top Features and Corresponding Importance Scores.
Bold lines refer to the methods with the best classiﬁcation results in that task.
(a) MOR Task
Features (Importance Scores)
MAP-D1(0.052)
PaO2-D2(0.052)
FiO2-D3(0.037)
PH-D3(0.027)
MAP-D1(0.031)
δPF-D1(0.031)
PH-D1(0.029)
PIM2S(0.027)
OI-D1(0.036)
MAP-D1(0.032)
OI-D0(0.028)
LIS-D0(0.028)
δPF-D1(0.058)
MAP-D1(0.053)
BE-D0(0.043)
PH-D1(0.042)
GBT-LR-DNN
δPF-D1(0.032)
PRISM12ROM(0.031)
PIM2S(0.031)
Unplanned(0.030)
GBT-LR-SDA
PF-D0(0.036)
δPF-D1(0.036)
BE-D0(0.032)
MAP-D1(0.031)
GBT-LR-LSTM
δPF-D1(0.066)
PH-D1(0.044)
MAP-D1(0.044)
PH-D3(0.041)
(b) VFD Task
Features (Importance Scores)
MAP-D1(0.035)
MAP-D3(0.033)
PRISM12ROM(0.030)
VT-D1(0.029)
MAP-D1(0.042)
PaO2-D0(0.033)
PRISM12ROM(0.032)
PIM2S(0.030)
LIS-D0(0.049)
LIS-D1(0.039)
OI-D1(0.036)
PF-D3(0.032)
δPF-D1(0.054)
MAP-D1(0.049)
PH-D1(0.046)
BE-D0(0.040)
GBT-LR-DNN
PaO2-D0(0.047)
PIM2S(0.038)
MAP-D1(0.038)
VE-D0(0.034)
GBT-LR-SDA
PaO2-D0(0.038)
VE-D0(0.034)
PH-D3(0.030)
MAP-D1(0.030)
GBT-LR-LSTM
PH-D3(0.062)
PaO2-D0(0.055)
δPF-D1(0.043)
MAP-D1(0.037)
Gradient Boosting Trees method has several internal tree estimators. Because of this, it can be more
complex and harder to interpret than a single decision tree since a decision tree can be used to ﬁnd
one decision path for a single data sample. So, we compare our GBTmimic-* with DTmimic-*
which is obtained by mimicking a single decision tree. From Table 6 we notice that DTmimic-*
methods perform poorly compared to GBTmimic-* methods, which is not satisfying even if it (a
single decision tree) can be better interpreted and visualized.
Table 6: Comparison of Mimic Methods with Decision Tree and Gradient Boosting Trees.
AUC: Mean of Area Under ROC;
AUC(diff): AUC of GBTmimic (Table 2) - AUC of DTmimic (Table 6);
AUC(std): Standard Deviation of Area Under ROC.
DTmimic-DNN
DTmimic-SDA
DTmimic-LSTM
DTmimic-LR-DNN
DTmimic-LR-SDA
DTmimic-LR-LSTM
Discussions
In this paper, we proposed a novel knowledge distillation approach from deep networks via Gradient
Boosting Trees, which can be used to learn interpretable features and prediction rules. Our preliminary experimental results show similar or even better performance from our mimic methods on a
real world dataset, and demonstrate a very promising direction for future machine learning research
in healthcare domain.
For future work, we aim to extract more useful information like decision rules or tree node features from our mimic methods for better diagnosis interpretability. We will also apply our proposed
approaches on a larger healthcare dataset, such as MIMIC-II which is derived from multiple
clinical sources, to further verify our methods. We also plan to extend our mimic methods to other
state-of-the-art machine learning models, such as structured deep network models, to explore their
MAP-D1 <= 18.5562
samples = 100.0%
BE-D1 <= -5.5422
samples = 84.0%
BE-D2 <= -1.5519
samples = 16.0%
LISPEEP-D3 <= 0.5
samples = 10.7%
MAP-D3 <= 16.2054
samples = 73.3%
samples = 6.0%
value = 2.6769
samples = 4.7%
value = -0.4225
samples = 67.3%
value = -0.8159
samples = 6.0%
value = 1.6947
CRS-D0 <= 0.3714
samples = 5.3%
VI <= 197.5
samples = 10.7%
samples = 3.1%
value = 4.9688
samples = 2.2%
value = 2.3027
samples = 6.0%
value = -0.2697
samples = 4.7%
value = 2.4805
LIS-D0 <= 2.8333
samples = 100.0%
MAP-D1 <= 17.3486
samples = 67.6%
PF-D0 <= 80.0
samples = 32.4%
PH-D1 <= 7.3376
samples = 63.8%
OI-D0 <= 8.4343
samples = 3.8%
samples = 17.6%
value = -0.0142
samples = 46.2%
value = -0.9477
samples = 0.9%
value = 0.0787
samples = 2.8%
value = 2.0221
PIP-D2 <= 24.875
samples = 17.3%
BE-D2 <= 4.3463
samples = 15.1%
samples = 0.9%
value = -0.1231
samples = 16.4%
value = 1.8539
samples = 10.1%
value = 0.9884
samples = 5.0%
value = -0.3743
(b) GBTmimic-LR-SDA
MAP-D1 <= 18.962
samples = 100.0%
LIS-D0 <= 2.8333
samples = 85.5%
PH-D2 <= 7.356
samples = 14.5%
PH-D1 <= 7.3236
samples = 64.8%
BE-D1 <= -3.5879
samples = 20.8%
samples = 11.3%
value = -0.0078
samples = 53.5%
value = -0.1059
samples = 7.2%
value = 0.2209
samples = 13.5%
value = 0.0195
FiO2-D3 <= 0.5969
samples = 5.0%
VI <= 138.5
samples = 9.4%
samples = 3.5%
value = 0.4076
samples = 1.6%
value = 0.5543
samples = 4.1%
value = 0.0755
samples = 5.3%
value = 0.2429
(c) GBTmimic-LSTM
Figure 3: Important Decision Trees on MOR Task
Value of a leaf node: The prediction score of a sample from the corresponding decision rules (path).
application abilities in difﬁcult practical applications and help domain experts have a better understanding of these models.