26/03/2025 11:24
Meshed-Memory Transformer for Image Captioning / Cornia, Marcella; Stefanini, Matteo; Baraldi, Lorenzo;
Cucchiara, Rita. - , pp. 10575-10584. [10.1109/CVPR42600.2020.01059].
Terms of use:
The terms and conditions for the reuse of this version of the manuscript are specified in the publishing
policy. For all terms of use and more information see the publisher's website.
(Article begins on next page)
IEEE Computer Society
This is the peer reviewd version of the followng article:
Meshed-Memory Transformer for Image Captioning
Marcella Cornia∗
Matteo Stefanini∗
Lorenzo Baraldi∗
Rita Cucchiara
University of Modena and Reggio Emilia
{name.surname}@unimore.it
Transformer-based architectures represent the state of
the art in sequence modeling tasks like machine translation and language understanding.
Their applicability to
multi-modal contexts like image captioning, however, is
still largely under-explored.
With the aim of ﬁlling this
gap, we present M2 – a Meshed Transformer with Memory for Image Captioning. The architecture improves both
the image encoding and the language generation steps: it
learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage
to exploit low- and high-level features. Experimentally, we
investigate the performance of the M2 Transformer and
different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves
a new state of the art in single-model and ensemble con-
ﬁgurations on the “Karpathy” test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly
available at:
 
meshed-memory-transformer.
1. Introduction
Image captioning is the task of describing the visual content of an image in natural language. As such, it requires
an algorithm to understand and model the relationships between visual and textual elements, and to generate a sequence of output words. This has usually been tackled via
Recurrent Neural Network models , in
which the sequential nature of language is modeled with
the recurrent relations of either RNNs or LSTMs. Additive
attention or graph-like structures are often added to the
recurrence in order to model the relationships between image regions, words, and eventually tags .
This schema has remained the dominant approach in
∗Equal contribution.
A baseball player is
throwing a ball to
another player.
Memory-Augmented Encoding
Meshed Decoding
Figure 1: Our image captioning approach encodes relationships between image regions exploiting learned a priori knowledge. Multi-level encodings of image regions are
connected to a language decoder through a meshed and
learnable connectivity.
the last few years, with the exception of the investigation
of Convolutional language models , which however did
not become a leading choice. The recent advent of fullyattentive models, in which the recurrent relation is abandoned in favour of the use of self-attention, offers unique
opportunities in terms of set and sequence modeling performances, as testiﬁed by the Transformer and BERT 
models and their applications to retrieval and video understanding . Also, this setting offers novel architectural modeling capabilities, as for the ﬁrst time the attention operator is used in a multi-layer and extensible fashion.
Nevertheless, the multi-modal nature of image captioning
demands for speciﬁc architectures, different from those employed for the understanding of a single modality.
Following this premise, we investigate the design of
a novel fully-attentive approach for image captioning.
Our architecture takes inspiration from the Transformer
model for machine translation and incorporates two
key novelties with respect to all previous image captioning algorithms: (i) image regions and their relationships are
encoded in a multi-level fashion, in which low-level and
high-level relations are taken into account. When modeling
these relationships, our model can learn and encode a priori knowledge by using persistent memory vectors. (ii) The
generation of the sentence, done with a multi-layer architecture, exploits both low- and high-level visual relationships
instead of having just a single input from the visual modality. This is achieved through a learned gating mechanism,
which weights multi-level contributions at each stage. As
this creates a mesh connectivity schema between encoder
and decoder layers, we name our model Meshed-Memory
Transformer – M2 Transformer for short. Figure 1 depicts
a schema of the architecture.
Experimentally,
we explore different fully-attentive
baselines and recent proposals, gaining insights on the performance of fully-attentive models in image captioning.
Our M2 Transformer, when tested on the COCO benchmark, achieves a new state of the art on the “Karpathy”
test set, on both single-model and ensemble conﬁgurations.
Most importantly, it surpasses existing proposals on the online test server, ranking ﬁrst among published algorithms.
Contributions. To sum up, our contributions are as follows:
• We propose a novel fully-attentive image captioning
algorithm. Our model encapsulates a multi-layer encoder for image regions and a multi-layer decoder
which generates the output sentence. To exploit both
low-level and high-level contributions, encoding and
decoding layers are connected in a mesh-like structure,
weighted through a learnable gating mechanism;
• In our visual encoder, relationships between image regions are encoded in a multi-level fashion exploiting
learned a priori knowledge, which is modeled via persistent memory vectors;
• We show that the M2 Transformer surpasses all previous proposals for image captioning, achieving a new
state of the art on the online COCO evaluation server;
• As a complementary contribution, we conduct experiments to compare different fully-attentive architectures
on image captioning and validate the performance of
our model on novel object captioning, using the recently proposed nocaps dataset. Finally, to improve
reproducibility and foster new research in the ﬁeld, we
will publicly release the source code and trained models of all experiments.
2. Related work
A broad collection of methods have been proposed in the
ﬁeld of image captioning in the last few years. Earlier captioning approaches were based on the generation of simple
templates, ﬁlled by the output of an object detector or attribute predictor . With the advent of Deep Neural Networks, most captioning techniques have employed
RNNs as language models and used the output of one or
more layers of a CNN to encode visual information and condition language generation . On the training
side, while initial methods were based on a time-wise crossentropy training, a notable achievement has been made with
the introduction of Reinforcement Learning, which enabled
the use of non-differentiable caption metrics as optimization
objectives . On the image encoding side, instead, single-layer attention mechanisms have been adopted
to incorporate spatial knowledge, initially from a grid of
CNN features , and then using image regions
extracted with an object detector . To further improve the encoding of objects and their relationships, Yao et
al. have proposed to use a graph convolution neural
network in the image encoding phase to integrate semantic
and spatial relationships between objects. On the same line,
Yang et al. used a multi-modal graph convolution network to modulate scene graphs into visual representations.
Despite their wide adoption, RNN-based models suffer
from their limited representation power and sequential nature. After the emergence of Convolutional language models, which have been explored for captioning as well ,
new fully-attentive paradigms have been proposed and achieved state-of-the-art results in machine translation and language understanding tasks. Likewise, some
recent approaches have investigated the application of the
Transformer model to the image captioning task.
In a nutshell, the Transformer comprises an encoder
made of a stack of self-attention and feed-forward layers,
and a decoder which uses self-attention on words and crossattention over the output of the last encoder layer. Herdade et al. used the Transformer architecture for image
captioning and incorporated geometric relations between
detected input objects. In particular, they computed an additional geometric weight between object pairs which is used
to scale attention weights. Li et al. used the Transformer in a model that exploits visual information and additional semantic knowledge given by an external tagger. On a
related line, Huang et al. introduced an extension of the
attention operator in which the ﬁnal attended information is
weighted by a gate guided by the context. In their approach,
a Transformer-like encoder was paired with an LSTM decoder. While the aforementioned approaches have exploited
the original Transformer architecture, in this paper we devise a novel fully-attentive model that improves the design
of both the image encoder and the language decoder, introducing two novel attention operators and a different design
of the connectivity between encoder and decoder.
3. Meshed-Memory Transformer
Our model can be conceptually divided into an encoder
and a decoder module, both made of stacks of attentive layers. While the encoder is in charge of processing regions
from the input image and devising relationships between
masked self-attention
feed-forward
Memory-Augmented Encoder
Meshed Decoder
cross-attention
cross-attention
feed-forward
Figure 2: Architecture of the M2 Transformer. Our model is composed of a stack of memory-augmented encoding layers,
which encodes multi-level visual relationships with a priori knowledge, and a stack of decoder layers, in charge of generating
textual tokens. For the sake of clarity, AddNorm operations are not shown. Best seen in color.
them, the decoder reads from the output of each encoding
layer to generate the output caption word by word. All intramodality and cross-modality interactions between word and
image-level features are modeled via scaled dot-product attention, without using recurrence. Attention operates on
three sets of vectors, namely a set of queries Q, keys K
and values V , and takes a weighted sum of value vectors
according to a similarity distribution between query and key
vectors. In the case of scaled dot-product attention, the operator can be formally deﬁned as
Attention(Q, K, V ) = softmax
where Q is a matrix of nq query vectors, K and V both
contain nk keys and values, all with the same dimensionality, and d is a scaling factor.
3.1. Memory-Augmented Encoder
Given a set of image regions X extracted from an input image, attention can be used to obtain a permutation invariant encoding of X through the self-attention operations
used in the Transformer . In this case, queries, keys, and
values are obtained by linearly projecting the input features,
and the operator can be deﬁned as
S(X) = Attention(WqX, WkX, WvX),
where Wq, Wk, Wv are matrices of learnable weights. The
output of the self-attention operator is a new set of elements
S(X), with the same cardinality as X, in which each element of X is replaced with a weighted sum of the values,
i.e. of linear projections of the input (Eq. 1).
Noticeably, attentive weights depend solely on the pairwise similarities between linear projections of the input set
itself. Therefore, the self-attention operator can be seen as
a way of encoding pairwise relationships inside the input
set. When using image regions (or features derived from
image regions) as the input set, S(·) can naturally encode
the pairwise relationships between regions that are needed
to understand the input image before describing it1.
This peculiarity in the deﬁnition of self-attention has,
however, a signiﬁcant limitation. Because everything depends solely on pairwise similarities, self-attention cannot
model a priori knowledge on relationships between image
regions. For example, given one region encoding a man and
a region encoding a basketball ball, it would be difﬁcult to
infer the concept of player or game without any a priori
knowledge. Again, given regions encoding eggs and toasts,
the knowledge that the picture depicts a breakfast could be
easily inferred using a priori knowledge on relationships.
Memory-Augmented Attention. To overcome this limitation of self-attention, we propose a memory-augmented attention operator. In our proposal, the set of keys and values
used for self-attention is extended with additional “slots”
which can encode a priori information. To stress that a priori information should not depend on the input set X, the
additional keys and values are implemented as plain learnable vectors which can be directly updated via SGD. Formally, the operator is deﬁned as:
Mmem(X) = Attention(WqX, K, V )
K = [WkX, Mk]
V = [WvX, Mv] ,
where Mk and Mv are learnable matrices with nm rows,
and [·, ·] indicates concatenation.
Intuitively, by adding
1Taking another perspective, self-attention is also conceptually equivalent to an attentive encoding of graph nodes .
learnable keys and values, through attention it will be possible to retrieve learned knowledge which is not already embedded in X. At the same time, our formulation leaves the
set of queries unaltered.
Just like the self-attention operator,
our memoryaugmented attention can be applied in a multi-head fashion. In this case, the memory-augmented attention operation is repeated h times, using different projection matrices
Wq, Wk, Wv and different learnable memory slots Mk, Mv
for each head. Then, we concatenate the results from different heads and apply a linear projection.
Encoding layer. We embed our memory-augmented operator into a Transformer-like layer: the output of the memoryaugmented attention is applied to a position-wise feedforward layer composed of two afﬁne transformations with
a single non-linearity, which are independently applied to
each element of the set. Formally,
F(X)i = Uσ(V Xi + b) + c,
where Xi indicates the i-th vector of the input set, and
F(X)i the i-th vector of the output. Also, σ(·) is the ReLU
activation function, V and U are learnable weight matrices,
b and c are bias terms.
Each of these sub-components (memory-augmented attention and position-wise feed-forward) is then encapsulated within a residual connection and a layer norm operation. The complete deﬁnition of an encoding layer can be
ﬁnally written as:
Z = AddNorm(Mmem(X))
X = AddNorm(F(Z)),
where AddNorm indicates the composition of a residual
connection and of a layer normalization.
Full encoder. Given the aforementioned structure, multiple encoding layers are stacked in sequence, so that the ith layer consumes the output set computed by layer i −1.
This amounts to creating multi-level encodings of the relationships between image regions, in which higher encoding
layers can exploit and reﬁne relationships already identiﬁed
by previous layers, eventually using a priori knowledge. A
stack of N encoding layers will therefore produce a multilevel output ˜
X1, ..., ˜
XN), obtained from the outputs
of each encoding layer.
3.2. Meshed Decoder
Our decoder is conditioned on both previously generated
words and region encodings, and is in charge of generating the next tokens of the output caption. Here, we exploit
the aforementioned multi-level representation of the input
image while still building a multi-layer structure. To this
aim, we devise a meshed attention operator which, unlike
the cross-attention operator of the Transformer, can take advantage of all encoding layers during the generation of the
Meshed Cross-Attention. Given an input sequence of vectors Y , and outputs from all encoding layers ˜
X, the Meshed
Attention operator connects Y to all elements in ˜
gated cross-attentions. Instead of attending only the last encoding layer, we perform a cross-attention with all encoding
layers. These multi-level contributions are then summed together after being modulated. Formally, our meshed attention operator is deﬁned as
where C(·, ·) stands for the encoder-decoder cross-attention,
computed using queries from the decoder and keys and values from the encoder:
Xi, Y ) = Attention(WqY , Wk ˜
and αi is a matrix of weights having the same size as the
cross-attention results. Weights in αi modulate both the
single contribution of each encoding layer, and the relative
importance between different layers. These are computed
by measuring the relevance between the result of the crossattention computed with each encoding layer and the input
query, as follows:
where [·, ·] indicates concatenation, σ is the sigmoid activation, Wi is a 2d×d weight matrix, and bi is a learnable bias
Architecture of decoding layers. As for encoding layers,
we apply our meshed attention in a multi-head fashion. As
the prediction of a word should only depend on previously
predicted words, the decoder layer comprises a masked selfattention operation which connects queries derived from the
t-th element of its input sequence Y with keys and values
obtained from the left-hand subsequence, i.e. Y≤t. Also, the
decoder layer contains a position-wise feed-forward layer
(as in Eq. 4), and all components are encapsulated within
AddNorm operations. The ﬁnal structure of the decoder
layer can be written as:
Z = AddNorm(Mmesh( ˜
X, AddNorm(Smask(Y )))
˜Y = AddNorm(F(Z)),
where Y is the input sequence of vectors and Smask indicates a masked self-attention over time. Finally, our decoder
stacks together multiple decoder layers, helping to reﬁne
both the understanding of the textual input and the generation of next tokens. Overall, the decoder takes as input word
vectors, and the t-th element of its output sequence encodes
the prediction of a word at time t + 1, conditioned on Y≤t.
After taking a linear projection and a softmax operation, this
encodes a probability over words in the dictionary.
3.3. Training details
Following a standard practice in image captioning , we pre-train our model with a word-level crossentropy loss (XE) and ﬁnetune the sequence generation using reinforcement learning. When training with XE, the
model is trained to predict the next token given previous
ground-truth words; in this case, the input sequence for the
decoder is immediately available and the computation of the
entire output sequence can be done in a single pass, parallelizing all operations over time.
When training with reinforcement learning, we employ
a variant of the self-critical sequence training approach 
on sequences sampled using beam search : to decode,
we sample the top-k words from the decoder probability
distribution at each timestep, and always maintain the top-k
sequences with highest probability. As sequence decoding
is iterative in this step, the aforementioned parallelism over
time cannot be exploited. However, intermediate keys and
values used to compute the output token at time t can be
reused in the next iterations.
Following previous works , we use the CIDEr-D score
as reward, as it well correlates with human judgment .
We baseline the reward using the mean of the rewards rather
than greedy decoding as done in previous methods ,
as we found it to slightly improve the ﬁnal performance.
The ﬁnal gradient expression for one sample is thus:
∇θL(θ) = −1
 (r(wi) −b)∇θ log p(wi)
where wi is the i-th sentence in the beam, r(·) is the reward
function, and b =
/k is the baseline, computed
as the mean of the rewards obtained by the sampled sequences. At prediction time, we decode again using beam
search, and keep the sequence with highest predicted probability among those in the last beam.
4. Experiments
4.1. Datasets
We ﬁrst evaluate our model on the COCO dataset ,
which is the most commonly used test-bed for image captioning. Then, we assess the captioning of novel objects by
testing on the recently proposed nocaps dataset .
COCO. The dataset contains more than 120 000 images,
each of them annotated with 5 different captions. We follow the splits provided by Karpathy et al. , where 5 000
images are used for validation, 5 000 for testing and the rest
for training. We also evaluate the model on the COCO online test server, composed of 40 775 images for which annotations are not made publicly available.
The dataset consists of 15 100 images taken
from the Open Images validation and test sets, each
annotated with 11 human-generated captions. Images are
divided into validation and test splits, respectively composed of 4 500 and 10 600 elements. Images can be further grouped into three subsets depending on the nearness
to COCO, namely in-domain, near-domain, and out-ofdomain images. Under this setting, we use COCO as training data and evaluate our results on the nocaps test server.
4.2. Experimental settings
Metrics. Following the standard evaluation protocol, we
employ the full set of captioning metrics: BLEU , ME-
TEOR , ROUGE , CIDEr , and SPICE .
Implementation details. To represent image regions, we
use Faster R-CNN with ResNet-101 ﬁnetuned on
the Visual Genome dataset , thus obtaining a 2048dimensional feature vector for each region. To represent
words, we use one-hot vectors and linearly project them to
the input dimensionality of the model d. We also employ
sinusoidal positional encodings to represent word positions inside the sequence and sum the two embeddings
before the ﬁrst decoding layer.
In our model, we set the dimensionality d of each layer to
512, the number of heads to 8, and the number of memory
vectors to 40. We employ dropout with keep probability 0.9
after each attention and feed-forward layer. In our meshed
attention operator (Eq. 6), we normalize the output with a
scaling factor of
N. Pre-training with XE is done following the learning rate scheduling strategy of with a
warmup equal to 10 000 iterations. Then, during CIDEr-D
optimization, we use a ﬁxed learning rate of 5 × 10−6. We
train all models using the Adam optimizer , a batch size
of 50, and a beam size equal to 5.
Novel object captioning. To train the model on the nocaps dataset, instead of using one-hot vectors, we represent words with GloVe word embeddings . Two fullyconnected layers are added to convert between the GloVe
dimensionality and d before the ﬁrst decoding layer and after the last decoding layer. Before the ﬁnal softmax, we
multiply with the transpose of the word embeddings. All
other implementation details are kept unchanged.
Additional details on model architecture and training can be
found in the supplementary material.
4.3. Ablation study
Performance of the Transformer. In previous works, the
Transformer model has been applied to captioning only in
its original conﬁguration with six layers, with the structure
of connections that has been successful for uni-modal sce-
Transformer (w/ 6 layers as in ) 79.1 36.2 27.7 56.9 121.8 20.9
Transformer (w/ 3 layers)
79.6 36.5 27.8 57.0 123.6 21.1
Transformer (w/ AoA )
80.3 38.8 29.0 58.4 129.1 22.7
M2 Transformer1-to-1 (w/o mem.)
80.5 38.2 28.9 58.2 128.4 22.2
M2 Transformer1-to-1
80.3 38.2 28.9 58.2 129.2 22.5
M2 Transformer (w/o mem.)
80.4 38.3 29.0 58.2 129.4 22.6
M2 Transformer (w/ softmax)
80.3 38.4 29.1 58.3 130.3 22.5
M2 Transformer
80.8 39.1 29.2 58.6 131.2 22.6
Table 1: Ablation study and comparison with Transformerbased alternatives. All results are reported after the REIN-
FORCE optimization stage.
narios like machine translation. As we speculate that captioning requires speciﬁc architectures, we compare variations of the original Transformer with our approach.
Firstly, we investigate the impact of the number of encoding and decoding layers on captioning performance. As
it can be seen in Table 1, the original Transformer (six
layers) achieves 121.8 CIDEr, slightly superior to the Up-
Down approach which uses a two-layer recurrent language model with additive attention and includes a global
feature vector (120.1 CIDEr). Varying the number of layers,
we observe a signiﬁcant increase in performance when using three encoding and three decoding layers, which leads to
123.6 CIDEr. We hypothesize that this is due to the reduced
training set size and to the lower semantic complexities of
sentences in captioning with respect to those of language
understanding tasks. Following this ﬁnding, all subsequent
experiments will use three layers.
Attention on Attention baseline. We also evaluate a recent proposal that can be straightforwardly applied to the
Transformer as an alternative to standard dot-product attention. Speciﬁcally, we evaluate the addition of the “Attention
on Attention” (AoA) approach to the attentive layers,
both in the encoder and in the decoder. Noticeably, in 
this has been done with a Recurrent language model with
attention, but the approach is sufﬁciently general to be applied to any attention stage. In this case, the result of dotproduct attention is concatenated with the initial query and
fed to two fully connected layers to obtain an information
vector and a sigmoidal attention gate, then the two vectors
are multiplied together. The ﬁnal result is used as an alternative to the standard dot-product attention. This addition
to a standard Transformer with three layers leads to 129.1
CIDEr (Table 1), thus underlying the usefulness of the approach also in Transformer-based models.
Meshed Connectivity. We then evaluate the role of the
meshed connections between encoder and decoder layers.
In Table 1, we ﬁrstly introduce a reduced version of our approach in which the i-th decoder layer is only connected to
the corresponding i-th encoder layer (1-to-1), instead of being connected to all encoders. Using this 1-to-1 connectiv-
Up-Down 
RFNet 
Up-Down+HIP 
GCN-LSTM 
AoANet 
M2 Transformer
Table 2: Comparison with the state of the art on the “Karpathy” test split, in single-model setting.
Ensemble/Fusion of 2 models
GCN-LSTM 
GCN-LSTM+HIP 
M2 Transformer
Ensemble/Fusion of 4 models
RFNet 
AoANet 
M2 Transformer
Table 3: Comparison with the state of the art on the “Karpathy” test split, using an ensemble of models.
ity schema already brings an improvement with respect to
using the output of the last encoder layer as in the standard
Transformer (123.6 CIDEr vs 129.2 CIDEr), thus conﬁrming that exploiting a multi-level encoding of image regions
is beneﬁcial. When we instead use our meshed connectivity schema, that exploits relationships encoded at all levels
and weights them with a sigmoid gating, we observe a further performance improvement, from 129.2 CIDEr to 131.2
CIDEr. This amounts to a total improvement of 7.6 points
with respect to the standard Transformer. Also, the result of
our full model is superior to that obtained using the AoA.
As an alternative to the sigmoid gating approach for
weighting the contributions from different encoder layers
(Eq. 6), we also test with a softmax gating schema. In this
case, the element-wise sigmoid applied to each encoder is
replaced with a softmax operation over the rows of αi. Using this alternative brings to a reduction of around 1 CIDEr
point, underlying that it is beneﬁcial to exploit the full potentiality of a weighted sum of the contributions from all
encoding layers, rather than forcing a peaky distribution in
which one layer is given more importance than the others.
Role of persistent memory. We evaluate the role of memory vectors in both the 1-to-1 conﬁguration and in the ﬁ-
Up-Down 
RFNet 
GCN-LSTM 
AoANet 
GCN-LSTM+HIP 
M2 Transformer
Table 4: Leaderboard of various methods on the online MS-COCO test server.
nal conﬁguration with meshed connections. As it can be
seen from Table 1, removing memory vectors brings to a
reduction in performance of around 1 CIDEr point in both
connectivity settings, thus conﬁrming the usefulness of exploiting a priori learned knowledge when encoding image
regions. Further experiments on the number of memory
vectors can be found in the supplementary material.
4.4. Comparison with state of the art
We compare the performances of our approach with
those of several recent proposals for image captioning.
The models we compare to include SCST and Up-
Down , which respectively use attention over the grid
of features and attention over regions.
Also, we compare to RFNet , which uses a recurrent fusion network to merge different CNN features; GCN-LSTM ,
which exploits pairwise relationships between image regions through a Graph CNN; SGAE , which instead
uses auto-encoding scene graphs. Further, we compare with
the original AoANet approach, which uses attention
on attention for encoding image regions and an LSTM language model. Finally, we compare with ORT , which
uses a plain Transformer and weights attention scores in the
region encoder with pairwise distances between detections.
We evaluate our approach on the COCO “Karpathy” test
split, using both single model and ensemble conﬁgurations,
and on the online COCO evaluation server.
Single model. In Table 2 we report the performance of our
method in comparison with the aforementioned competitors, using captions predicted from a single model and optimization on the CIDEr-D score. As it can be observed, our
method surpasses all other approaches in terms of BLEU-4,
METEOR and CIDEr, while being competitive on BLEU-
1 and SPICE with the best performer, and slightly worse
on ROUGE with respect to AoANet . In particular, it
advances the current state of the art on CIDEr by 1.4 points.
Ensemble model. Following the common practice 
of building an ensemble of models, we also report the performances of our approach when averaging the output prob-
GT: A cat looking at his reﬂection in the mirror.
Transformer: A cat sitting in a window sill looking out.
M2 Transformer: A cat looking at its reﬂection
in a mirror.
GT: A plate of food including eggs and toast on a
table next to a stone railing.
Transformer: A group of food on a plate.
M2 Transformer: A plate of breakfast food with
eggs and toast.
GT: A truck parked near a tall pile of hay.
Transformer: A truck is parked in the grass in a
M2 Transformer: A green truck parked next to a
pile of hay.
Figure 3: Examples of captions generated by our approach
and the original Transformer model, as well as the corresponding ground-truths.
ability distributions of multiple and independently trained
instances of our model. In Table 3, we use ensembles of
two and four models, trained from different random seeds.
Noticeably, when using four models our approach achieves
the best performance according to all metrics, with an increase of 2.5 CIDEr points with respect to the current state
of the art .
Online Evaluation. Finally, we also report the performance
of our method on the online COCO test server2. In this case,
we use the ensemble of four models previously described,
trained on the “Karpathy” training split. The evaluation is
done on the COCO test split, for which ground-truth annotations are not publicly available. Results are reported in Table 4, in comparison with the top-performing approaches of
the leaderboard. For fairness of comparison, they also used
an ensemble conﬁguration. As it can be seen, our method
surpasses the current state of the art on all metrics, achieving an advancement of 1.4 CIDEr points with respect to the
best performer.
2 
Figure 4: Visualization of attention states for three sample captions. For each generated word, we show the attended image
regions, outlining the region with the maximum output attribution in red.
Out-of-Domain
CIDEr SPICE
CIDEr SPICE
CIDEr SPICE
NBT + CBS 
Up-Down + CBS 
Transformer
M2 Transformer
Transformer + CBS
M2 Transformer + CBS
Table 5: Performances on nocaps validation set, for indomain and out-of-domain captioning.
4.5. Describing novel objects
We also assess the performance of our approach when
dealing with images containing object categories that are
not seen in the training set. We compare with Up-Down 
and Neural Baby Talk , when using GloVe word embeddings and Constrained Beam Search (CBS) to address
the generation of out-of-vocabulary words and constrain the
presence of categories detected by an object detector. To
compare with our model, we use a simpliﬁed implementation of the procedure described in to extract constraints,
without using word phrases. Results are shown in Table 5:
as it can be seen, the original Transformer is signiﬁcantly
less performing than Up-Down on both in-domain and outof-domain categories, while our approach can properly deal
with novel categories, surpassing the Up-Down baseline in
both in-domain and out-of-domain images. As expected,
the use of CBS signiﬁcantly enhances the performances, in
particular on out-of-domain captioning.
4.6. Qualitative results and visualization
Figure 3 proposes qualitative results generated by our
model and the original Transformer. On average, our model
is able to generate more accurate and descriptive captions,
integrating ﬁne-grained details and object relations.
Finally, to better understand the effectiveness of our M2
Transformer, we investigate the contribution of detected regions to the model output. Differently from recurrent-based
captioning models, in which attention weights over regions
can be easily extracted, in our model the contribution of one
region with respect to the output is given by more complex
non-linear dependencies. Therefore, we revert to attribution
methods: speciﬁcally, we employ the Integrated Gradients
approach , which approximates the integral of gradients with respect to the given input. Results are presented
in Figure 4, where we observe that our approach correctly
grounds image regions to words, also in presence of object details and small detections. More visualizations are
included in the supplementary material.
5. Conclusion
We presented M2 Transformer, a novel Transformerbased architecture for image captioning. Our model incorporates a region encoding approach that exploits a priori
knowledge through memory vectors and a meshed connectivity between encoding and decoding modules. Noticeably,
this connectivity pattern is unprecedented for other fullyattentive architectures. Experimental results demonstrated
that our approach achieves a new state of the art on COCO,
ranking ﬁrst in the on-line leaderboard. Finally, we validated the components of our model through ablation studies, and its performances when describing novel objects.
Acknowledgment
This work was partially supported by the “IDEHA - Innovation for Data Elaboration in Heritage Areas” project (PON
ARS01 00421), funded by the Italian Ministry of Education
(MIUR). We also acknowledge the NVIDIA AI Technology Center, EMEA, for its support and access to computing resources.