Multivariate Matching Methods That Are
Monotonic Imbalance Bounding
Stefano M. IACUS, Gary KING, and Giuseppe PORRO
We introduce a new “Monotonic Imbalance Bounding” (MIB) class of matching methods for causal inference with a surprisingly large
number of attractive statistical properties. MIB generalizes and extends in several new directions the only existing class, “Equal Percent
Bias Reducing” (EPBR), which is designed to satisfy weaker properties and only in expectation. We also offer strategies to obtain speciﬁc
members of the MIB class, and analyze in more detail a member of this class, called Coarsened Exact Matching, whose properties we
analyze from this new perspective. We offer a variety of analytical results and numerical simulations that demonstrate how members of the
MIB class can dramatically improve inferences relative to EPBR-based matching methods.
KEY WORDS:
Causal inference; EPBR; Matching.
1. INTRODUCTION
A deﬁning characteristic of observational data is that the investigator does not control the data generation process. The resulting impossibility of random treatment assignment thus reduces attempts to achieve valid causal inference to the process
of selecting treatment and control groups that are as balanced
as possible with respect to available pretreatment variables. One
venerable but increasingly popular method of achieving balance
is through matching, where each of the treated units is matched
to one or more control units as similar as possible with respect
to the given set of pretreatment variables.
Once a matched dataset is selected, the causal effect is estimated by a simple difference in means of the outcome variable for the treated and control groups, assuming ignorability
holds, or by modeling any remaining pretreatment differences.
The advantage of matching is that inferences from better balanced datasets will be less model dependent .
Consider a sample of n units, a subset of a population of
N units, where n ≤N. For unit i, denote Ti as the treatment
variable, where Ti = 1 if unit i receives treatment (and so is
a member of the “treated” group) and Ti = 0 if not (and is
therefore a member of the “control” group). The outcome variable is Y, where Yi(0) is the “potential outcome” for observation i if the unit does not receive treatment and Yi(1) is the
potential outcome if the (same) unit receives treatment. For
each observed unit, only one potential outcome is observed,
Yi = TiYi(1) + (1 −Ti)Yi(0), which means that Yi(0) is unobserved if i receives treatment and Yi(1) is unobserved if i does
not receive treatment. Without loss of generality, when we refer
to unit i, we assume it is treated so that Yi(1) is observed while
Stefano M. Iacus is Associate Professor, Department of Economics, Business and Statistics, University of Milan, Via Conservatorio 7, I-20124 Milan,
Italy (E-mail: ). Gary King is the Albert J. Weatherhead
University Professor, Institute for Quantitative Social Science, 1737 Cambridge
Street, Harvard University, Cambridge MA 02138 (E-mail: ).
Giuseppe Porro is Associate Professor, Department of Economics and Statistics, University of Trieste, P.le Europa 1, I-34127 Trieste, Italy (E-mail:
 ). Open source software for R and Stata to implement the methods described herein is available at http:// gking.harvard.edu/cem;
the cem algorithm is also available via the R package MatchIt. Thanks to Erich
Battistin, Nathaniel Beck, Matt Blackwell, Andy Eggers, Adam Glynn, Justin
Grimmer, Jens Hainmueller, Ben Hansen, Kosuke Imai, Guido Imbens, Fabrizia Mealli, Walter Mebane, Clayton Nall, Enrico Rettore, Jamie Robins, Don
Rubin, Jas Sekhon, Jeff Smith, Kevin Quinn, and Chris Winship for helpful
Yi(0) is unobserved and thus estimated by matching it with one
or more units from a given reservoir of the control units.
Denote X = (X1,X2,...,Xk) as a k-dimensional data set,
where each Xj is a column vector of the observed values of pretreatment variable j for the n observations. That is, X = [Xij,i =
1,...,n,j = 1,...,k]. We denote by T = {i:Ti = 1} the set of
indexes for the treated units and by nT = #T the number of
treated units; similarly C = {i:Ti = 0}, nC = #C for the control
units, with nT + nC = n. Given a treated unit i ∈T with its vector of covariates Xi, the aim of matching is to discover a control
unit l ∈C with covariates Xl such that, the dissimilarity between
Xi and Xl is very small in some metric, that is, d(Xi,Xl) ≃0.
A special case is the exact matching algorithm where, for each
treated unit i, a control unit l is selected such that d(Xi,Xl) = 0,
with d of full rank [i.e., if d(a,b) = 0 if and only if a = b].
The literature includes many methods of selecting matches,
but only a single rigorous class of methods has been characterized, the so-called Equal Percent Bias Reducing (EPBR) methods. In introducing EPBR, Rubin recognized the need
for more general classes: “Even though nonlinear functions of
X deserve study..., it seems reasonable to begin study of multivariate matching methods in the simpler linear case and then
extend that work to the more complex nonlinear case. In that
sense then, EPBR matching methods are the simplest multivariate starting point.” The introduction of the EPBR class has
led to highly productive and, in recent years, fast growing literatures on the theory and application of matching methods.
Yet, in the more than three decades since Rubin’s original call
for continuing from this “starting point” to develop more general classes of matching models, none have appeared in the literature. We take up this call here and introduce a new class,
which we denote Monotonic Imbalance Bounding (MIB) methods. This new class of methods generalize EPBR in a variety of
useful ways.
In this article, we review EPBR, introduce MIB, discuss several speciﬁc matching methods within the new class, and illustrate their advantages for empirical analysis. Throughout, we
distinguish between classes of methods and speciﬁc methods
(or algorithms) within a class that can be used in applications.
© 2011 American Statistical Association
Journal of the American Statistical Association
March 2011, Vol. 106, No. 493, Theory and Methods
DOI: 10.1198/jasa.2011.tm09599
Journal of the American Statistical Association, March 2011
Classes of methods deﬁne properties which all matching methods within the class must posses. Some methods may also belong to more than one class. For a review of many existing
methods and their advantages and disadvantages, see Ho et al.
2. CLASSES OF MATCHING METHODS
In this section, we summarize the existing EPBR class of
matching methods, introduce our new MIB class, discuss example methods within each class along with various comparisons,
and show how MIB is able to explicitly bound model dependence, a longstanding goal of matching methods.
2.1 The Equal Percent Bias Reducing Class
Let µt ≡E(X|T = t), t = 0,1, be a vector of expected values and denote by mT and mC the number of treated and control units matched by some matching method. Let MT ⊆T
and MC ⊆C be the sets of indexes of the matched units in the
two groups. Let ¯XnT = 1
i∈T Xi, and ¯XnC = 1
the vector of sample means of the observed data and ¯XmT =
i∈MT Xi, and ¯XmC =
i∈MC Xi be the vector of sample means for the matched data only.
EPBR requires all treated units to be matched, that is, mT =
nT (thus MT = T ), but allows for the possibility that only mC ≤
nC control units are matched, where mC is chosen ex ante.
Deﬁnition 1 [Equal Percent Bias Reducing (EPBR); Rubin
1976a]. An EPBR matching solution satisﬁes
" ¯XmT −¯XmC
= γ (µ1 −µ0),
where 0 < γ < 1 is a scalar.
A condition of EPBR is that the number of matched control
units be ﬁxed ex ante and the particular
value of γ be calculated ex post, which we emphasize by writing γ ≡γ (mC). (The term “bias” in EPBR violates standard
statistical usage and refers instead to the equality across variables in the reduction in covariate imbalance.) If the realized
value of X is a random sample, then (1) can be expressed as
" ¯XmT −¯XmC
" ¯XnT −¯XnC
The right-hand side of (2) is the average mean-imbalance
in the population that gives rise to the original data, and the
left-hand side is the average mean-imbalance in the population
subsample of matched units. The EPBR property implies that
improving balance in the difference in means on one variable
also improves it on all others (and their linear combinations)
by a proportional amount, which is why γ is assumed to be a
scalar. EPBR is a relevant property only if one assumes that the
function which links the covariates and the outcome is equally
sensitive to all components (for example a linear function), or
if the analyst scales the covariates so this is the case.
EPBR attempts to improve only mean imbalance (or main
effects in X) and says nothing about other moments, interactions, or nonlinear relationships (except inasmuch as one includes in X speciﬁcally chosen terms like X2
j , Xj × Xk, and so
forth). Rubin and Thomas give some specialized conditions which can generate the maximum level of imbalance reduction possible for any EPBR matching method. Although this
result does not indicate which method will achieve the maximum, it may provide useful guidance about how well the search
No method of matching satisﬁes EPBR without data restrictions. To address these issues, Rosenbaum and Rubin 
suggest considering special conditions where controlling the
means enables one to control all expected differences between
the multivariate treated and control population distributions,
which is the ultimate goal of matching. The most general version of these assumptions now require:
(a) X is drawn randomly from a speciﬁed population X,
(b) The population distribution for X is an ellipsoidally symmetric density or a discriminant mixture of proportional ellipsoidally symmetric
densities , and
(c) The matching algorithm applied is invariant to afﬁne
transformations of X.
With these conditions, there is no risk of decreasing any type of
expected imbalance in some variables while increasing it in others. Checking balance in this situation involves checking only
the difference in means between the treated and control groups
for only one (and indeed, any one) covariate.
Although the requirement (c) can be satisﬁed (e.g., by
propensity score matching, unweighted Mahalanobis matching, discriminate matching), assumptions (a) and (b) rarely
hold (and are almost never known to hold) in observational
data. Rubin and Thomas give some simulated examples where certain violations of these conditions still yield the
desired properties for propensity score and Mahalanobis matching, but the practical problem of improving balance on one variable leading to a reduction in balance on others is very common
in real applications in many ﬁelds. Of course, these matching
methods are only potentially EPBR, since to apply them to real
data requires the additional assumptions (a) and (b).
2.2 The Monotonic Imbalance Bounding Class
We build our new class of matching methods in six steps,
by generalizing and modifying the deﬁnition of EPBR. First,
we drop any assumptions about the data, such as conditions
(a) and (b). Second, we focus on the actual in-sample imbalance, as compared to EPBR’s goal of increasing expected balance. Of course, efﬁciency of the ultimate causal quantity of
interest is a function of in-sample, not expected, balance, and
so this can be important . In addition,
achieving in-sample balance is an excellent way to achieve expected balance as well. Let ¯XnT,j, ¯XnC,j and ¯XmT,j, ¯XmC,j denote the prematch and postmatch sample means, for variable
Xj, j = 1,...,k, for the subsamples of treated and control units.
Then, third, we replace the equality in (2) by an inequality, and
focus on the variable-by-variable relationship | ¯XmT,j −¯XmC,j| ≤
γj| ¯XnT,j −¯XnC,j| which we rewrite as
$$ ¯XmT,j −¯XmC,j
j = 1,...,k,
where δj = γj| ¯XnT,j −¯XnC,j|. Fourth, we require δj to be chosen
ex ante and let mT and mC to be determined by the matching
algorithm instead of the reverse as under EPBR.
Iacus, King, and Porro: Multivariate Matching Methods
Figure 1. An example of a covariate for which minimizing mean-imbalance may be harmful. The example also shows that increasing
mean-imbalance for this variable under MIB can be used to match more relevant features of the distributions (such as the shaded areas),
without hurting mean-imbalance on other variables. This would be impossible under EPBR.
Equation (3) states that the maximum imbalance between
treated and matched control units, as measured by the absolute
difference in means for variable Xj, is bounded from above by
the constant δj. Analogous to EPBR, one would usually prefer
the situation when the bound on imbalance is reduced due to
matching, γj = δj/| ¯XnT,j −¯XnC,j| < 1, although this is not (yet)
guaranteed by a method in this class.
To motivate the next change, consider data where the subsample of treated units has a unimodal distribution with a sample mean zero, and the control group has a bimodal distribution
with almost zero empirical mean (see Figure 1). Then, reducing
the difference in means in these data with a matching algorithm
will be difﬁcult. Instead, one would prefer locally good matches
taken from where distributions are most similar (see the two
shaded boxes). Using these regions containing good matches
may increase the mean imbalance by construction, but overall
balance between the groups will greatly improve.
Thus, ﬁfth, we generalize (3) from mean imbalance to a general measure of imbalance. Denote by XnT = [(Xi1,...,Xik),i ∈
T ] the subset of the rows of treated units, and similarly for XnC,
XmT , and XmC. We also replace the difference in means with a
generic distance D(·,·). Further, instead of the empirical means,
we make use of a generic function of the sample, say f(·). This
function may take as argument one variable Xj at time, or more,
for example if we want to consider covariances. This leads us
to the intermediate deﬁnition:
Deﬁnition 2 [Imbalance Bounding (IB)]. A matching method
is Imbalance Bounding on the function of the data f(·) with respect to a distance D(·,·), or simply IB(f,D), if
where δ > 0 is a scalar.
In a sense, EPBR is a version of IB if we take D(x,y) =
E(x −y), f(·) the sample mean, that is, f(XmT) = ¯XmT and
f(XmC) = ¯XmC, δ = γ D(f(XnT),f(XnC)), the inequality replaces the equality, and γ < 1. Although quite abstract, IB
becomes natural when f(·) and D(·,·) are speciﬁed. Assume
f(·) = fj(·) is a function solely of the marginal empirical distribution of Xj. Then consider the following special cases:
• Let D(x,y) = |x−y| and fj(X) denote the sample mean for
the variable Xj of the observations in the subset X . Then,
(4) becomes (3), that is, | ¯XmT,j −¯XmC,j| ≤δj. Similarly, if
fj(·) is the sample variance, the kth centered moment, the
qth quantile, and so forth.
• If fj(·) is the empirical distribution function of Xj, and
D(·,·), the sup-norm distance, then (4) is just the Kolmogorov distance, and if a nontrivial bound δj exists, then
an IB methods would control the distance between the full
distributions of the treated and control groups.
• Let D(x,y) = |x| and f(·) = fjk(·) is the covariance of Xj
and Xk and δ = δjk; then |Cov(Xj,Xk)| ≤δjk.
• In Section 3 we introduce a global measure of multivariate
imbalance denoted L1 in (6), which is also a version of
D(f(·),f(·)).
To introduce our ﬁnal step, we need some additional notation. As in Deﬁnition 2, let f be any function of the empirical
distribution of covariate Xj of the data (such as the mean, variance, quantile, histogram, and so forth). Let π,π′ ∈Rk
nonnegative k-dimensional vectors and let the notation π ⪯π′
require that the two vectors π and π′ be equal on all indexes
except for a subset J ⊆{1,...,k}, for which πj < π′
For a given function f(·) and a distance D(·,·) we denote by
γf,D(·) : Rk
+ →R+ a monotonically increasing function of its
argument, that is, if π ⪯π′ then γf,D(π) ≤γf,D(π′). Then our
last step gives the deﬁnition of the new class:
Deﬁnition 3 [Monotonic Imbalance Bounding (MIB)].
A matching method is Monotonic Imbalance Bounding on the
function of the data f(·) with respect to a distance D(·,·), or
simply MIB(f,D), if for some monotonically increasing function γf,D(·) and any π ∈Rk
+ we have that
MIB is then a class of matching methods which produces
subsets XmT and XmC, where mT = mT(π) and mC = mC(π) on
the basis of a given vector π = (π1,π2,...,πk) of tuning parameters (such as a caliper), one for each covariate. As a result,
the number of matched units is a function of the tuning parameter and is not ﬁxed ex ante. In contrast, the function γf,D, once
f and D are speciﬁed, depends only on the tuning parameter
π, but not on the sample size mT or mC; indeed, it represents
a bound, or the worst situation for a given value of the tuning
parameter.
A crucial implication of the MIB property for practical data
analysis is the following. Suppose that for a matching method
Journal of the American Statistical Association, March 2011
in the MIB class (such as the one we introduce in the Section 3), such that for each variable j = 1,...,k, we have
f(x1,...,xj) = fj(xj) (e.g., the empirical mean of Xj) and a function γfj,D(π1,...,πk) = γj(πj), j = 1,...,k. Then, we can write
the system of inequalities
Now suppose a researcher changes only a single tuning parameter, for example for the ﬁrst variable: that is, we take a
new vector π′ = (π1 −ϵ,π2,...,πk), with ϵ > 0. The above
system of inequalities still holds, that is, all inequalities from
2 to k remain unchanged and only the ﬁrst one changes to
D(f1(XmT(π)),f1(XmC(π))) ≤γ1(π1 −ϵ) ≤γ1(π1).
This means that relaxation of one tuning parameter for one
variable controls monotonically the imbalance measures by (D,
fj), without altering the maximal imbalance on the remaining
variables. This property is especially useful if we conceptualize
the maximum imbalance in a variable as the maximal measurement error one can tolerate. For example, for many applications,
we can probably tolerate an imbalance of 2 pounds in weighting people (since individuals can vary this much over the course
of a day), ﬁve years of difference in age (for middle ages), or
a year or two of education not near the threshold of graduation from high school, college, etc. Once these thresholds are
set, an MIB method guarantees that no matter how much other
variables imbalance is adjusted, these maxima will not change.
2.3 Examples and Comparisons
Well known matching methods within the (potentially)
EPBR class include nearest neighbor matching based on a
propensity score or Mahalanobis distance. These methods are
not MIB, because they ﬁx the number of matched observations (mT,mC) ex ante (usually to twice the number of treated
units) rather than, as in MIB methods, letting the number of
matched units be the result of the user setting tuning parameters. These and other nearest neighbor matching methods applied with a scalar caliper, even when (mT,mC) is an outcome
of the method, are not MIB because the dimension of the tuning
parameter π in the deﬁnition has to be k in order to have separability as in (5). Caliper matching as deﬁned in Cochran and
Rubin is not MIB because of the orthogonalization and
overlapping regions; without orthogonalization, it is MIB when
the distance between the treated and control groups includes a
tuning parameter for each variable. 
More generally, let Xi = (Xi1,Xi2,...,Xik) and Xh = (Xh1,
Xh2,...,Xhk) be any two vectors of covariates for two sample
units i and h. Let dj(Xi,Xh) = dj(Xij,Xhj) deﬁne the distance
for covariate j (j = 1,...,k). Then, the caliper distance between Xi and Xh is d(Xi,Xh) = maxj=1,...,k 1{dj(Xi,Xh)≥ϵj}, where
1A is an indicator function for set A and ϵ1,...,ϵk are tuning
parameters. So when d(Xi,Xh) = 0, Xi and Xh are close, and
if d(Xi,Xh) = 1 units are far apart (i.e., unmatchable, which
could also be expressed by redeﬁning the latter as ∞). The
simplest choice to complete the deﬁnition is the caliper distance, dj(Xi,Xh) = dj(Xij,Xhj) = |Xij −Xhj| but any other one-dimensional distance will also
be MIB provided the tuning parameter ϵj is on the scale of covariate Xj and is deﬁned for all j. In this case, nearest neighbor
matching with caliper or full optimal matching is MIB.
Another member of the MIB class is coarsened exact matching, which applies exact matching after each variable is separately coarsened (CEM is detailed in Section 3). Numerous
other diverse MIB methods can be constructed by applying
non-MIB methods within CEM’s coarsened strata or within
variable-by-variable calipers. For one example, we can coarsen
with very wide bins, apply CEM, and then use the propensity
score, or Mahalanobis distance within each bin, to further prune
observations. The resulting methods are all MIB.
Both EPBR and MIB classes are designed to avoid, in different ways, the problem of making balance worse on some
variables while trying to improve it for others, a serious practical problem in real applications. With additional assumptions
about the data generation process, EPBR means that the degree
of imbalance changes for all variables at the same time by proportionally the same amount; MIB, without extra assumptions
on the data, means that changing one variable’s tuning parameter does not affect the maximum imbalance for the others.
Neither class can guarantee both a bound on the level of imbalance and, at the same time, a prescribed number of matched
observations. In EPBR methods, the user chooses the matched
sample size ex ante and computes balance ex post, whereas in
MIB methods the user chooses the maximal imbalance ex ante
and produces a matched sample size ex post. The latter would
generally be preferred in observational analyses, where data is
typically plentiful but is not under control of the investigator,
and so reducing bias rather than inefﬁciency is the main focus.
In real datasets that do not necessarily meet EPBR’s assumptions, no results are guaranteed from potentially EPBR methods
and so balance may be reduced for some or all variables. Thus,
methods that are potentially EPBR require verifying ex post that
balance has improved. For example, in propensity score matching, the functional form of the regression of T on X must be correct, but the only way to verify this is to check balance ex post.
Since the objective function used for estimating the propensity
score differs from the analytical goal of ﬁnding balance, applied
researchers commonly ﬁnd that substantial tweaking is required
to avoid degrading mean balance on at least some variables, and
other types of balance are rarely checked or reported.
Under MIB, by restricting only one tuning parameter per
variable, imbalance in the means, other moments, comoments,
interactions, nonlinearities, and the full multivariate distribution of the treated and control groups may be improved, without
hurting maximum imbalance on other variables and regardless
of the data type. The actual level of balance achieved by MIB
methods can of course be better than the maximum level set ex
ante. Moreover, as in CEM with the level of coarsening, setting
this single tuning parameter per variable is typically a choice
users are able to make based on their knowledge of the substantive problem and data.
In practice, MIB methods may sometimes generate too few
matched observations, which indicates that either the maximum
imbalance levels chosen are too restrictive (e.g., too stringent a
caliper), or that the data set cannot be used to make inferences
without high levels of model dependence. In observational data,
Iacus, King, and Porro: Multivariate Matching Methods
analyzing counterfactuals too far from the data to make reliable inferences is a constant concern and so MIB’s property of
sometimes producing no matched observations can also be considered an important advantage.
By attempting to reduce expected imbalance, potentially
EPBR methods attempt to approximate with observational data
the classic complete randomization experimental design, with
each unit randomly assigned a value of the treatment variable.
In contrast, MIB methods can be thought of as approximating
the randomized block experimental design, where values of the
treatment variable are assigned within strata deﬁned by the covariates. (Of course, methods from either class can be modiﬁed
to create randomized block designs.) Although both designs
are unbiased, randomized block designs have exact multivariate balance in each dataset on all observed covariates, whereas
complete randomization designs are balanced only on average
across random treatment assignments in different experiments,
with no guarantees for the one experiment being run. Randomized block designs, as a result, are considerably more efﬁcient,
powerful, and robust, regardless whether one is estimating insample or population quantities ; in an application
by Imai, King, and Nall , complete randomization gives
standard errors as much as six times larger than the corresponding randomized block design.
Finally, a consensus recommendation of the matching literature is that units from the control group too far outside the range
of the data of the treated group should be discarded as they lead
to unacceptable levels of model dependence. This means that
the application of potentially EPBR methods must be proceeded
by a separate method for eliminating these risky observations.
One way to eliminate extreme counterfactuals is to discard control units that fall outside the convex-hull 
or the hyper-rectangle delimited by the
empirical distribution of the treated units. Unfortunately, these
and other two-step matching approaches are not even potentially EPBR. In contrast, MIB methods which eliminate a distant risky extrapolations, often even without a separate step.
2.4 MIB Properties: Bounding Model Dependence
and Estimation Error
A key motivating factor in matching is to reduce model dependence . However, the relationship has never
been proven directly for EPBR methods, or any other aside
from exact matching. Thus, we contribute here a proof that the
maximum degree of model dependence can be controlled by the
tuning parameters for MIB methods. We also go a step farther
and show how the same parameters also bound causal effect
estimation error.
Model Dependence.
At the unit level, exact matching estimates the counterfactual Yi(0) ≡g0(Xi) for treated unit i with
the value of Y of the control unit j such that Xj = Xi. If exact matching is not feasible, then we use a model mℓto span
the remaining imbalance to Yi(0) with control units close to the
treated units, that is, using matched data such as ˆY(0) ≡mℓ( ˜Xj),
where ˜Xj is the vector of covariates for the control units close
to treated i. Model dependence is how much mℓ( ˜Xj) varies as a
function of the model mℓfor a given vector of covariates ˜Xj. We
restrict the attention to the set of competing Lipschitz models.
Deﬁnition 4 (Competing models). Let mℓ(ℓ= 1,2,...) be
statistical models for Y. For example, mℓ(x) may be a model
for E(Y|X = x). Then we consider the following class:
mℓ: |mℓ(x) −mℓ(y)| ≤Kℓd(x,y) and
|mi(x) −mk(x)| ≤h,i ̸= k,x ∈&
with exogenous choices of a small prescribed nonnegative value
for h and 0 < Kℓ< ∞and & = &1 × ··· × &k, where &j is the
empirical support of variable Xj. Here d(x,y) is some distance
on the space of covariates &.
In Mh, the Lipschitz constants Kℓare proper constants of
the models mℓand, given the speciﬁcation of mℓ, need not be
estimated. The class Mh represents competing models which
ﬁt the observed data about as well, or in other words do not
yield very different predictions for the same observed values ˜X;
if this were not the case, we could rule out a model based on the
data alone.
In this framework, for any two models m1,m2 ∈Mh, we de-
ﬁne model dependence as |m1( ˜Xi) −m2( ˜Xi)| . This leads to the key result for MIB methods with respect to D(x,y) = d(x,y) and f(x) = x:
|m1( ˜Xj) −m2( ˜Xj)|
= |m1( ˜Xj) ± m1(Xi) ± m2(Xi) −m2( ˜Xj)|
≤|m1(Xi) −m1( ˜Xj)| + |m2(Xi) −m2( ˜Xj)|
+ |m1(Xi) −m2(Xi)|
≤(K1 + K2)d(Xi, ˜Xj) + h ≤(K1 + K2)γ (π) + h.
Thus, the degree of model dependence is directly bounded by
the choice of π [via γ (π)] of the MIB method.
Estimation Error.
To show how MIB bounds estimation error, we need to recognize that under certain speciﬁc conditions
matching changes the estimand (Section 6.1 discusses when
this may be desirable and how to avoid it when not). For example, consider an MIB method which produces one-to-one
matching of treatment to control. We denote by Xi the values of
the covariates for treated unit i and by ˜Xj the values for control
unit j matched with this treated unit. If mT is the total number
of treated units matched, then estimand can be deﬁned as
(Yi(1) −Yi(0)).
Then, an estimator of τmT is given by
(Yi(1) −ˆYi(0)) = 1
(Yi(1) −g( ˜Xj,0)),
where ˆYi(0) = g( ˜Xj,0) and ˜Xj is the vector of covariate values
for control unit j matched with treated unit i.
Proposition 1. Let d(x,y) be a distance from & × & to R+
and let g(x,0) be differentiable with bounded partial derivates,
that is, | ∂
∂xi g(x,0)| ≤Ki, for some 0 < Ki < ∞, i = 1,...,k.
Then, for an MIB method with respect to D(x,y) = d(x,y) and
f(x) = x we have that |τmT −ˆτmT| ≤γ (π)K + o(γ (π)) with
Journal of the American Statistical Association, March 2011
Proof. Taylor expansion of g( ˜Xj,0) around Xi, gives
g( ˜Xj,0) −g(Xi,0)
= d( ˜Xj,Xi) ·
+ o(d( ˜Xj,Xi)).
We can decompose ˆτmT as follows:
(Yi(1) −ˆYi(0) ± Yi(0))
(Yi(1) −g( ˜Xj,0) ± g(Xi,0))
d( ˜Xj,Xi) ·
+ o(d( ˜Xj,Xi))
Therefore, we have the statement of the proposition.
In some cases, the requirement of partial boundedness of
derivates may be too stringent or the constants Ki can be too
large. In this situation, a direct proof can still be obtained by
imposing a weaker local Lipschitz condition directly on g(x,0),
that is, |g(x,0) −g(y,0)| ≤Kxd(x,y) for any y in a neighborhood of x. As a result, differentiability of g(·,0) is not required.
In all cases, the constants Ki [or Kx for locally Lipschitz g(·,0)]
interact with γ (π) and thus the bound may be sufﬁciently tight
because, for a MIB method, γ (π) can be controlled monotonically, while for a given g(·,0) all the constants are given. The
linear g(·,0) usually adopted in the literature, or the example
functions in Figure 5 below, are all locally Lipschitz functions.
The above results shows that, an MIB method bounds the
error |τmT −ˆτmT| as an explicit function of the vector of tuning
parameters π.
3. COARSENED EXACT MATCHING
AS AN MIB METHOD
In order to clarify how the MIB class of matching methods
works in practical applications, we now introduce one member of the MIB class of matching methods that comes from
the diverse set of approaches based on subclassiﬁcation (also
known as “stratiﬁcation” or “intersection” methods). We call
this particular method CEM for “Coarsened Exact Matching”
 .
CEM requires three steps: (1) Coarsen each of the original
variables in X as much as the analyst is willing into, say, C(X)
(e.g., years of education might be coarsened into grade school,
high school, college, graduate school, and so forth). (2) Apply
exact matching to C(X), which involves sorting the observations into strata, say s ∈S, each with unique values of C(X).
(3) Strata containing only control units are discarded; strata
with treated and control units are retained; and strata with only
treated units are used with extrapolated values of the control
units or discarded if the analyst is willing to narrow the quantity of interest to the remaining set of treated units for which a
counterfactual has been properly identiﬁed and estimated.
Denote by T s the treated units in stratum s, with count
T = #T s, and similarly for the control units, that is, Cs and
C = #Cs. The number of matched units are, respectively
for treated and controls, mT = !
T and mC = !
Then for subsequent analysis, assign each matched unit i in
stratum s, the following CEM-weights wi = 1, if i ∈T s and
wi = mC/mT · ms
C, if i ∈Cs, with unmatched units receiving weight wi = 0.
Coarsening Choices
Because coarsening is so closely related to the substance
of the problem being analyzed and works variable-by-variable,
data analysts understand how to decide how much each variable
can be coarsened without losing crucial information. Indeed,
even before the analyst obtains the data, the quantities being
measured are typically coarsened to some degree. Variables like
gender or the presence of war coarsen away enormous heterogeneity within the given categories. Data analysts also recognize that many measures include some degree of noise and, in
their ongoing efforts to ﬁnd a signal, often voluntarily coarsen
the data themselves. For example, seven-point partisan identi-
ﬁcation scales are recoded as Democrat, Independent, and Republican; Likert issue questions as agree, neutral, and disagree;
and multiparty vote returns as winners and losers. Many use a
small number of categories to represent religion, occupation,
U.S. Security and Exchange Commission industry codes, international classiﬁcation of disease codes, and many others. Indeed, epidemiologists routinely dichotomize all their covariates
on the theory that grouping bias is much less of a problem than
getting the functional form right. Although coarsening in CEM
is safer than at the analysis stage, the two procedures are similar in spirit since the discarded information in both is thought
to be relatively unimportant—small enough with CEM to trust
to statistical modeling.
For continuous variables, coarsening can cut the range of the
variable Xj into equal intervals of length ϵj. If the substance of
the problem suggests different interval lengths, we use ϵj to denote the maximum length. For categorical variables, coarsening
may correspond to grouping different levels of the variable.
Although we ﬁnd that data analysts have little trouble making
coarsening choices on the basis of their substantive information,
we have also developed a series of automated coarsening methods, such as those which automatically choose bin widths for
histograms and other more sophisticated approaches. These are
available in easy-to-use software that accompanies this article
for R and Stata at 
CEM as an MIB Method
We show here that CEM is a member of the MIB class with
respect to the mean, the centered absolute kth moment, and the
empirical and weighted quantiles (all proofs are in the Appendix). Other similar properties can be proved along these lines as
well. Beginning with Deﬁnition 3, let D(x,y) = |x −y|, πj = ϵj,
γj = γj(ϵj) be a function of ϵj, and the function f(·) vary for the
different propositions. Changing ϵj for one variable then does
not affect the imbalance on the other variables.
Iacus, King, and Porro: Multivariate Matching Methods
Denote the weighted mean for the treated and control
units respectively as ¯Xw
i∈T Xijwi and ¯Xw
i∈C Xijwi.
Proposition 2. For j = 1,...,k, | ¯Xw
mC,j| ≤ϵj.
Let Rj be the range of variable Xj and let θj = maxϵj≥ϵ∗
ϵj⌉), where ⌈x⌉is the ﬁrst integer greater or equal to x. In the
deﬁnition of θj, ϵ∗
j is any reasonable strictly positive value, for
example, the lowest value of ϵj which generates at most nT nonempty intervals in CEM.
Proposition 3. Let k ≥1 and consider the centered absolute kth moment for variable Xj for the treated and control
units as ¯µk
i∈T s |Xij −¯Xw
mT,j|kwi and ¯µk
i∈Cs |Xij −¯Xw
mC,j|kwi. Then, | ¯µk
1)k, j = 1,...,k, and ϵj ≥ϵ∗
Proposition 4. Assume one-to-one matching. Denote by
mT,j the qth empirical quantile of the distribution of the treated
units for covariate Xj, and similarly Xq
mC,j. Then, |Xq
mC,j| ≤ϵj for j = 1,...,k.
Deﬁne the weighted empirical distribution functions for
treated group as Fw
mT,j(x) = !
mT and for the control
group as Fw
mC,j(x) = !
mC . Deﬁne the qth quantile of
the weighted distribution Xq,w
mT,j as the ﬁrst observation in the
sample such that Fw
mT,j(x) ≥q and similarly for Xq,w
Proposition 5. Assume that the support of variable Xj is cut
on subintervals of exact length ϵj. Then |Xq,w
mT,j −Xq,w
mC,j| ≤ϵj for
j = 1,...,k.
On Filling CEM Strata
A problem may occur with MIB methods if too many treated
units are discarded. This can be ﬁxed of course by adjusting the
choice of maximum imbalance, but it is reasonable to ask how
often this problem occurs for a “reasonable” choice in real data.
One worry is the curse of dimensionality, which in this context
means that the number of hyper-rectangles, and thus the number
of possible strata #C(X1)×···×#C(Xk), is typically very large.
For example, suppose X is composed of 10,000 observations on
20 variables drawn from independent normal densities. Since
20-dimensional space is enormous, odds are that no treated unit
will be anywhere near any control unit. In this situation, even
very coarse bins under CEM will likely produce no matches.
For example, with only two bins for each variable, the 10,000
observations would need to be sorted into 220 possible strata, in
which case the probability would be extremely small of many
stratum winding up with both a treated and control unit.
Although EPBR methods ﬁx the number of matches ex ante
(on the hope that imbalance would be reduced on average across
experiments), no EPBR matching method would provide much
help in making inferences from these data either. The fact that
in these data CEM would likely produce very few matches may
be regarded as a disadvantage, since some estimate may still be
desired no matter how model dependent, it is better regarded as
an advantage in real applications, since no method of matching
will help produce high levels of local balance in this situation.
Fortunately, for two reasons, the sparseness that occurs in
multidimensional space turns out not to be much of an issue in
practice. First, real datasets have far more highly correlated data
structures than the independent draws in the example above,
and so CEM in practice tends to produce a reasonable number
of matches. This has been our overwhelming experience in the
numerous data sets we have analyzed. We studied this further
by attempting to sample from the set of social science causal
analyses in progress in many ﬁelds by broadcasting an offer
(through blogs and listservs) help in making causal analyses in
return for a look at their data (and a promise not to scoop anyone). We will report on this analysis in more detail in a later
article but note here that, for almost every dataset we studied,
CEM produced an inferentially useful number of matched observations and also generated substantially better balance for a
given number of matches than methods in the EPBR class.
Finally, if the reservoir of control units is sufﬁciently large,
it is possible to derive, following the proof of proposition 1 in
Abadie and Imbens , an exponential bound on the probability that the number of CEM strata with unmatched treated
units remains positive. In particular, the number of cells that
contain only (unmatched) treated units goes to zero exponentially fast with the number of treated units nT in the sample, if
the number of control units nC grows at rate nC = O(n1/r
r ≥k and k the number of continuous pretreatment covariates.
On CEM’s Computational Efﬁciency
Although the number of empty CEM strata in real data tends
to be small, the total number of cells to be explored in order
to determine in which strata we have a match is exponentially
large, for example, mk where k is the number of covariates and
m is the average number of intervals on which the support of
each covariate has been cut. So, for example, with k = 20 and
m = 6, the number of strata is huge: 620 = 3.656158·1015. Fortunately, CEM produces at most n strata if all observations fall
into different CEM strata. The implementation can then ignore
this space and focus on at most n strata. Thus, the coarsening
represents the original covariate values Xj with integers 1 to θj,
leading an observation to be a feature vector of integers such as
(Xi1,Xi2,Xi3,...,Xi98,Xi99,Xi100) = (1,3,5,...,3,2,7). The
algorithm we implement then represents all the information
in the feature vector for each observation with a string like
“1 ∗3 ∗5 ∗··· ∗3 ∗2 ∗7.” The result is that CEM matching has
the same complexity of a simple frequency tabulation, which is
of order n. This algorithm can even be easily implemented on
very large databases using SQL-like queries.
CEM and Propensity Scores
One way to convey how MIB generalizes the EPBR class of
methods, and thus includes its beneﬁts, is to note that a high
quality nonparametric estimate of the propensity score is available from CEM results by merely calculating the proportion
of units within each stratum that are treated. Indeed, this estimator would typically balance better than that from the usual
logit model (which optimizes an objective function based on
ﬁt rather than balance) or various unwieldly ad hoc alternatives that iterate between balance checking and tweaking and
rerunning the logit model. Of course, with CEM, estimating
the propensity score is not needed, since CEM determines the
matches ﬁrst, but this point helps convey how using CEM can
include the essential advantages of the propensity score.
Journal of the American Statistical Association, March 2011
4. REDUCING IMBALANCE
We now deﬁne and introduce a measure of imbalance between the treated and control groups. We then demonstrate how
CEM outperforms (potentially) EPBR methods even in data
generated to meet EPBR assumptions.
4.1 Deﬁnition and Measurement
Most matching methods were designed to reduce imbalance
in the mean of each pretreatment variable between the treated
and control groups, for which the balance metric is simply the
difference in means for each covariate. Of course, mean imbalance is only
part of the goal as it does not necessarily represent the desired
multidimensional imbalance between the treated and control
We thus now introduce a new and more encompassing imbalance measure aimed at representing the distance between the
multivariate empirical distributions of the treated and control
units of the pretreatment covariates. Our measure is intended to
be widely applicable across datasets and (postmatching) analysis methods, even though a better measure may well be available in situations where a researcher is restricted to a speciﬁc
analysis method.
The proposed measure uses the L1 norm to measure the distance between the multivariate histograms. To build this measure, we obtain the two multidimensional histograms by direct cross-tabulation of the covariates in the treated and control
groups, given a choice of bins for each variable. Let H(X1) denote the set of distinct values generated by the bins chosen for
variable X1, that is, the set of intervals into which the support of
variable X1 has been cut. Then, the multidimensional histogram
is constructed from the set of cells generated by the Cartesian
product H(X1) × ··· × H(Xk) = H(X) = H.
Let f and g be the relative empirical frequency distributions for the treated and control units, respectively. Observation
weights may exists as output of a matching method. For the
raw data, the weights are equal to one for all observations in the
sample. Let fℓ1···ℓk be the relative frequency for observations belonging to the cell with coordinates ℓ1 ···ℓk of the multivariate
cross-tabulation, and similarly for gℓ1···ℓk.
Deﬁnition 5. The multivariate imbalance measure is
L1(f,g;H) = 1
ℓ1···ℓk∈H(X)
$$fℓ1···ℓk −gℓ1···ℓk
We denote the measure by L1(f,g;H) = L1(H) to stress
its dependence on the choice of multidimensional bins H,
the deﬁnition of which we take up below. [Sometimes, when
we compare two methods MET1 and MET2 we will write
L1(MET1;H) and L1(MET2;H).] An important property of
this measure is that the typically numerous empty cells do not
affect L1(H), and so the summation in (6) has at most only
n nonzero terms. The relative frequencies also control for what
may be different sample sizes for the treated and control groups.
The L1 measure offers an intuitive interpretation, for any
given set of bins: If the two empirical distributions are completely separated (up to H), then L1 = 1; if the distributions
exactly coincide, then L1 = 0. In all other cases, L1 ∈(0,1). If
say L1 = 0.6, then 40% of the area under the two histograms
Let f m and gm denote the distributions of the matched treated
and control units corresponding to the distributions f , g of the
original unmatched data. Then a good matching method will result in matched sets such that L1(f m,gm) ≤L1(f,g). Of course,
to make coherent matching comparisons, the bins H must remain ﬁxed. See also Racine and Li .
Choosing a Bin Deﬁnition H for L1(H).
Although the definition of L1(H) is intuitive, it depends on the apparently arbitrary choice of the bins H: Like the bandwidth in nonparametric density estimation, bins too small provide exact separation
in multidimensional space [L1(H) = 1] and bins too large cannot discriminate [L1(H) = 0]. Thus, analogous to the purpose
of ROC curves in avoiding the choice of the differential costs
of misclassiﬁcation, we now develop a single deﬁnition for H
to represent all possible bin choices.
To begin, we study the L1-proﬁle, which is our name for
the distribution of L1(H) in the set of all possible bin deﬁnitions H ∈H. We study this distribution by drawing 250 random
samples from the L1-proﬁle for data from Lalonde , a
commonly used benchmark dataset in the matching literature
(the 10 variables included are not central to our illustration,
and so we refer the interested reader to the original article or
the replication ﬁle that accompanies our article). For each randomly drawn value of H, we calculate L1(H) based on the raw
data and on each of three matching methods: (1) nearest neighbors based on the propensity score calculated via logit model
(PSC); (2) a optimal matching (MAHO) solution based on the
Mahalanobis distance ; and (3) a CEM
solution with ﬁxed coarsening (10 intervals for each continuous
variable and no coarsening for the categorical variables).
The left panel of Figure 2 plots L1(H) vertically by the randomly chosen bin H horizontally, with bins sorted by the L1(H)
value based on the raw data. For this reason the raw data (the red
line) is monotonically increasing. For any given H (indicated
by a point on the horizontal axis), the method with the lowest
imbalance is preferred. The problem we address here is that different bin deﬁnitions can lead to different rankings among the
methods. Fortunately, however, in these data the rank order imbalance reduction of the methods remains stable across almost
the entire range of H values, aside from small random perturbations. For almost any value of H on the horizontal axis, the
largest imbalance reduction is generated by CEM, then propensity score matching and Mahalanobis distance matching. CEM
thus essentially dominates the other methods, in that regardless
of the deﬁnition of the bins used in deﬁning L1(H) its matched
data sets have the lowest imbalance. In these data, propensity
score matching and Mahalanobis matching are slightly better
than the raw data. The approximate invariance portrayed in this
ﬁgure is worth checking for in speciﬁc applications, but we ﬁnd
it to be an extremely common empirical regularity across almost all the data sets we have analyzed. For another view of
the same result, the right panel of Figure 2 represents the cumulative empirical distribution functions (ecdf) of the values
Iacus, King, and Porro: Multivariate Matching Methods
Figure 2. The L1-proﬁle of the raw (“RAW”) data (red line) compared to propensity score (“PSC”), Mahalanobis (“MAHO”), and Coarsened
Exact Matching (“CEM”) matched data. MAHO and PSC overlaps in the left panel. The left panel plots the L1 proﬁle by different bin choices
sorted by L1 for the raw data; the right panel plots the empirical cumulative distribution functions of the same set of L1 values.
of L1(H) over the set H: The method with the rightmost ecdf
produces the highest levels of imbalance.
Thus, we have shown that the bin deﬁnition H in our imbalance measure (6) is often unimportant. We thus propose ﬁxing
it to a speciﬁc value to produce a ﬁnal imbalance measure recommended for general use. The speciﬁc value we recommend
is the set of bins ¯H which corresponds to the median value of
L1 on the proﬁle of the raw data, ¯H = ¯HRAW. We denote this
value and new measure by ¯L1 ≡L1( ¯H). In Figure 2, ¯L1 is 0.43
for Mahalanobis matching, 0.41 for propensity score matching,
and 0.26 for CEM.
Is Balancing on the Means Enough?.
Although the point
is simple mathematically, a large empirical literature suggests that it may be worth clarifying why controlling for
one dimensional distributions is not enough to control the
global imbalance of the joint distribution (outside the special
cases such as multivariate Gaussians). Indeed, let pi = P(T =
1|Xi1,Xi2,...,Xik) = 1/[1+exp{−β0 −!k
j=1 βjXij}] be the logistic model for the propensity score. And let ˆpi be the propensity score estimated by maximum likelihood. Set wi = 1 −ˆpi,
for i ∈T and wi = ˆpi for i ∈C.
Matching in some way based on this propensity score in arbitrary data has no known theoretical properties (and does not
perform well in these data), and so for clariﬁcation we switch
to propensity score weighting, which is simpler in this situation. Denote the weighted means for treated and control units as
i∈T Xijwi/!
i∈T wi and ¯Xw
i∈C Xijwi/!
Then, it is well known that (without matching) ¯Xw
Although this weighting guarantees the elimination of all
mean imbalance, the multidimensional distribution of the data
may be still highly imbalanced. We consider again the same
data as before. The value of the median on the L1-proﬁle for
the data is equal to ¯L1 = 0.54. The univariate (I1) and global
( ¯L1) imbalance measures are given in Table 1 for the raw data,
propensity score weighting, and CEM. After applying propensity score weighting (see middle column) we get, as expected,
an almost perfect (weighted) match on the difference in means
for all variables, but the overall global imbalance is equal to
¯L1 = 0.53, which is almost the same as the original data. However, after matching the raw data with CEM (which we do by
coarsening the continuous variables into eight intervals), the
data are more balanced because CEM pruned observations that
would have led to large extrapolations. This can be seen in the
last line of the table which gives the global imbalance, which
has now been substantially reduced to ¯L1 = 0.34.
This example thus shows that simple weighting can reduce
or eliminate mean imbalance without improving global multivariate imbalance. The same of course holds for any matching
algorithm designed to improve imbalance computed one vari-
Table 1. Differences in means for each variable and global imbalance
measure ( ¯L1) on raw data from Lalonde , after propensity
score weighting, and following CEM matching. Variable names
are as in Lalonde’s original dataset. The propensity score is
estimated by a logit model; CEM coarsens the continuous
variables into eight categories
Pscore weighting
Journal of the American Statistical Association, March 2011
able at a time. CEM, as an MIB method, and ¯L1 as a measure
of imbalance, provide a simple way around these problems.
4.2 CEM versus EPBR Methods Under
EPBR-Compliant Data
We now simulate data best suited for EPBR methods and
compare CEM, an MIB matching method, to the propensity
score (PSC) and Mahalanobis distance matching from the
EPBR class of methods. We show that the MIB properties of
CEM (in particular, the in-sample multivariate imbalance reduction) enables CEM to outperform EPBR methods even in
data generated to optimize EPBR performance.
The propensity score model is estimated as usual including all the main effects. For PSC we use a 1-nearest neighbor method without replacement while we denote by MAH
1-nearest neighbor matching, also without replacement, on
the Mahalanobis distance and MAHO optimal matching on the Mahalanobis distance. Due to the fact
that CEM drops treated units, we also compute a second version of PSC where it is forced to further match on the subsample of treated and control units selected by the CEM algorithm
We begin by replicating an experiment proposed Gu and
Rosenbaum . This involves drawing two independent
multivariate normal datasets: XT ∼N5(µT,+) and XC ∼
N5(µC,+), with common variances (6,2,1,2,1) and covariances (2, 1, 0.4, −1, −0.2, 1, −0.4, 0.2, 0.4, 1), and means
vectors µT = (0,0,0,0,0) and µC = (1,1,1,1,1). We randomly sample nT = 1000 treated units from XT and nC = 3000
control units from XC. For CEM, we coarsen each covariate into 8 intervals of equal length. MAH, MAHO, and PSC
match mT = 1000 treated units against mC = 1000 control
units, whereas CEM selects both treated and control units and
in turn PSC2 select mT = mC depending on mT, which is output
from CEM in each simulation.
The properties of EPBR imply that MAH and PSC matching
will optimally minimize expected mean imbalance in these data when all treated units are
matched. In contrast, CEM is designed to reduce local multivariate imbalance, that is, the maximum distance between each
treated unit and the corresponding matched control units. In addition to the global imbalance, ¯L1, we compute for each variable the global difference in means between the treated and control groups (I1) and the average absolute difference in units stratum by stratum for CEM, and unit by unit for the other methods
See Table 2. Overall, we ﬁnd that CEM is substantially better than the other methods in terms of the difference in means,
as well as local and global imbalance. Since one may argue
that this effect is due to the fact that only CEM drops treated
units, we also consider the performance of PSC2, but the conclusion here remains unchanged. PSC2 seems to beneﬁt more
from being combined with CEM. Thus, CEM is indeed greatly
reducing the distance between the two k-dimensional distributions of treated and control units. Since the two EPBR methods
in these data are known to be optimal only in expectation, the
additional advantage of CEM is coming from MIB’s in-sample
multivariate imbalance reduction property.
Table 2. Imbalance in means I1 (top panel) and local imbalance I2
(bottom panel) remaining after matching, for each variable listed,
X1,...,X5. Also reported are the number of treated mT and control
mC units remaining after the match (top) and the multivariate L1
measure of imbalance (bottom, rightmost column). Results are
averaged over 1000 replications, with nT = 1000, nC = 3000
Difference in means I1
Initial imb.
Local imbalance I2
5. AN EMPIRICAL ANALYSIS: THE EFFECT OF
HAVING A DAUGHTER ON CONGRESSIONAL
REPRESENTATION
To illustrate how CEM works in practice, we replicate a recent article published in the leading journal in economics that
seeks to explain U.S. Congressional decision making, a central concern to political scientists, using the effect of children
on their parents, a longstanding subject of study among sociologists . In this article, Washington showed
that members of the U.S. House of Representatives who have
a daughter (rather than a son) vote more liberally, especially
on reproductive rights issues. Via a linear regression, Washington estimates this effect while controling for several discrete
variables (race, gender, the number of children, political party,
and religion) and continuous variables (seniority and its square,
age and its square, and Democratic vote share). The outcome
variable is a score given to each of 430 members of the 105th
Congress measuring agreement with the National
Organization for Women (NOW). These scores range from 0
(no support) to 100 (complete support). (Washington reports
that the NOW data were not available for other years.)
Conditional on Washington’s regression speciﬁcation, she
ﬁnds that each additional girl causes a legislator to vote 2.23
points more liberally (with a standard error of 1.02). Of course,
this is only one of many plausible speciﬁcations and so we study
the degree of model dependence in these results. We do this by
running all 656 possible linear regressions from the set of all
main effects from her original set of covariates, all interactions
up to order three, and all squared terms for the continuous variables. For each linear regression speciﬁcation, we estimate the
treatment effect, and our summary of model dependence is the
variation across these estimates. We portray this variation by
Iacus, King, and Porro: Multivariate Matching Methods
Figure 3. Quantifying Model Dependence: Density plots illustrating the variation in treatment effect estimates across different speciﬁcations of analysis models with (red, dashed) and without (black, solid) CEM matching. Dotted vertical lines give the point estimates using the
speciﬁcation chosen by Washington . The online version of this ﬁgure is in color.
plotting a histogram (density estimate) of these results. See the
solid line in Figure 3, where the estimated causal effects have
a huge and substantively unacceptable range of [−3.03,6.11].
By choosing only one speciﬁcation (as most articles do), the regression approach can produce what seems like “evidence” for
anything from a large negative result that rejects the hypothesis
to a large positive result that supports it.
We then preprocess the data with CEM (coarsening each of
the three continuous variables in 10 equal sized intervals). In
this dataset, imbalance is modestly reduced from L1 = 0.818
with n = 430 in the raw data to L1 = 0.762 with n = 175.
Yet, when we rerun all the same regression speciﬁcations on
the CEM matched data, the degree of model dependence is substantially reduced, with a range of treatment effects now of only
[3.04,4.80]. Unlike the model dependence from Washington’s
analyses, every point within this range supports her scientiﬁc
hypothesis. We can see this more clearly in Figure 3 by comparing the distribution of the treatment effects across different
regression speciﬁcations run on the raw data (in black) to the
distribution based on the CEM matched data (in dashed red).
Even ignoring the outliers for the raw data, the variation in estimated effects after matching is much smaller.
As a result of matching, the point estimate of the causal effect
has increased by a substantial 45%, from 2.2 to 3.2, and the new,
larger estimate has much less model dependence and so should
be regarded as considerably more reliable. We conclude that
the effect Washington hypothesized, even if not demonstrated
in her article, is even larger than her regression model indicated.
However, this more reliable causal effect is an average over
the causal effects for only a subset of observations and so is
making an inference about a different estimand. Most social science analyses with either observational or experimental data are
based on convenience samples, with statistical inference conditional on the data analyzed, but we should nevertheless clarify precisely what quantity is being estimated, especially with
matching analyses that preprocess the data and select a quantity
of interest in the process. This includes CEM, as well as propensity score or Mahalanobis distance matching with calipers.
In other words, all matching analyses should routinely identify for readers the subset of observations in the matched set
and summarize their values on the covariates. This step is rarely
done in the literature, but as we show here it is easy to do and
advisable. We do this in Figure 4 with a version of a “parallel
plot.” Each line a parallel plot corresponds to an observation in
the dataset, where the vertical axis indicates the range of the
values of each of the variables, and the horizontal axis is a variable list.
In the present example of a parallel plot in Figure 4, we have
colored the lines representing legislators (in the treated group,
i.e., with daughters) for whom matches were found in blue and
those who were not unmatched in red. The results indicate that
the matched treated units—which deﬁne the estimand—are a
sample of legislators with a higher concentration of young,
white, male Republicans than the full sample of representatives with daughters in the 105th Congress. In particular, the
representatives with daughters in the matched set are 10 percentage points more male, an average of 4 years younger in age
and 2.5 in seniority, 12 percentage points more white, and 24
percentage points more Republican. (Other differences are relatively minor.) Thus, because of who happened to be elected to
congress in the 1996 election, we are able to make a relatively
solid inference about this particular subset of legislators. For
this group, it happens that the causal effect turns out to be reasonably sized and consistent with the hypothesis. However, due
to the lack of an appropriate control groups, reliable inferences
without model dependence happens not to be available for other
groups of legislators with daughters.
6. REDUCING CAUSAL EFFECT ESTIMATION ERROR
In order to avoid inducing selection bias, statisticians suggest ignoring the outcome variable while choosing a matching
procedure and focusing primarily on reducing imbalance in the
covariates (as we did in Section 4). In this section, we go a
step further and switch focus to reducing estimation error in the
causal quantity of interest.
Journal of the American Statistical Association, March 2011
Figure 4. Parallel plot for the matched (blue) and unmatched (red) legislators. Each line describes a member of congress, with ranges for each
variable: Gender (male, female), Race (white, other), Seniority (1–47 years), Age (26–87 years), DemVote (0.26–0.94), PartyID (Republican,
Democrat), and Religion (protestant, none, catholic, other christian, other religion).
6.1 Deﬁnitions and Estimation
A crucial issue in causal inference is identifying the precise
quantity to be estimated. This is an issue in observational data,
which is typically based on convenience samples and may include whatever relevant data happen to be available. The same
issue applies to most randomized medical experiments, for example, since they are also based on convenience samples (such
as patients who happen to show up at a research hospital). In
these situations, the target causal effect is typically deﬁned for
the observed units only, and no attempt is made to infer formally to a broader population.
We therefore deﬁne two quantities of interest: The sample average treatment effect on the treated SATT = 1
i∈T [Yi(1) −
Yi(0)] and the population average treatment effect on the treated
PATT = Ei∈T ∗[Yi(1) −Yi(0)], where T and T ∗are the sets of
treated units in the sample and population, respectively.
SATT and PATT are especially convenient deﬁnitions for
matching methods which prune (only) control units from a
dataset and so do not change the estimand. In especially dif-
ﬁcult datasets, however, some treated units may have no reasonable match among the available pool of control units. These
treated units are easy to identify in MIB methods such as CEM,
since matches are only made when they meet the ex ante speciﬁed level of permissible imbalance; under EPBR methods, all
treated units are matched, no matter how deﬁcient the set of
available controls and so a separate analytical method must be
applied to identify these units.
When reasonable control units do not exist for one or more
treated units, high levels of model dependence can result. In this
situation, the analyst can choose to (a) create virtual controls
for the unmatched treated units via extrapolation and modeling
assumptions, (b) conclude that the data include insufﬁcient information to estimate the target causal effect and give up, or
(c) change the quantity of interest to the SATT or PATT deﬁned
for the subset of treated units that have good matches among the
pool of controls (for later use we denote as the “local” SATT
or PATT). Since the data are deﬁcient to the research question
posed, all three options are likely to be unsatisfying, (a) because
of model dependence, (b) because we learn nothing, and (c) because this is not the quantity we originally sought; although
each of these options can be reasonable in some circumstances.
We offer here a way to think about this problem more broadly
by combining all these options together. This process requires
four steps. First, preprocess the data to remove the worst potential matches (and thus the most strained counterfactuals) from
the set of available control units. This can be done easily using the convex hull or the hyper-rectangle approaches (see Section 2.3). Second, run CEM on these preprocessed data without
the extreme counterfactuals and obtain mT ≤nT treated units
matched with mC ≤nC control units. Third, use these results
to split the entire set of treated units in the two groups of mT
matched and nT −mT unmatched individuals.
Fourth, compute the SATT (or similarly for PATT) separately
in the two groups as follows. For the mT treated units, suppose
there exist mC acceptable counterfactuals (as deﬁned by the
coarsening in CEM say), and so we can reliably estimate this
“local SATT,” ˆτmT , using only this subset of treated units. Then,
for the rest of the treated units, either extrapolate the model estimated on the matched units to obtain virtual counterfactuals
for the unmatched treated units or consider all the unmatched
units as a single CEM stratum and estimate the SATT locally.
In either case, denote this estimate by ˆτnT−mT .
Finally, calculate the overall SATT estimate ˆτnT as the
weighted mean of the two estimates:
ˆτnT = ˆτmT · mT + ˆτnT−mT · (nT −mT)
This procedure keeps the overall quantity of interest, SATT
(or analogously PATT), ﬁxed and isolates the model dependent
piece of the estimator so it can be studied separately and its effects on the overall estimate isolated. In practice, analysts might
wish to present ˆτnT , which is necessarily model dependent, as
well as ˆτmT , which is well estimated (and not model dependent)
but is based on only a subset of treated units.
Iacus, King, and Porro: Multivariate Matching Methods
6.2 CEM versus Propensity Score Matching
We now compare CEM with the standard use of propensity
score matching. We focus on PATT rather than SATT to give the
advantage to PSM as an EPBR method. (The results strongly favor CEM, but would even more in estimating SATT.) For simplicity, we use for our estimator the simple difference in means
between matched treated and control groups, with weights for
the matched units
where MT and MC are, respectively, the sets of treated and control units matched and Yi is the observed outcome on the units
and wi are the weights of the different matching methods [for
clarity of our simulation setup, we do not use the method in
Equation (7)]. We run three separate experiments and evaluate results in terms of root mean square error (RMSE) for both
PATT and the local PATT.
One-Dimensional Case.
We begin with a population of
NT = 5000 treated units, with covariate X drawn from N(1,1)
and NC = 5000 control units with X drawn from N(5,1), which
ﬁts the EPBR data requirements. Denote the outcome variable
as Y, and write the potential outcomes as Y(t) = gk(x,t) for
t = 0,1, where gk is deﬁned in the six ways indicated in Figure 5. The solid lines represent different choices for gk(x,1) and
the dotted lines represent each gk(x,0); the vertical distance for
a given value of x in each graph is a treatment effect. These
functions represent constant, linear, and diverse nonlinear treatment effects. We construct the true outcome by applying the
gk(x,t) to a sample drawn from the given population. We create the observed Y as the truth plus Gaussian noise, N(0,0.3).
We randomly sample from the original population of the treated
units nT = 200 units and from the population of the control
units nC = 400 units.
We generate 1000 random datasets; for each sample, we attempt to estimate PATT τk and the local PATT τ m
k , and evaluate
the RMSE for different matching methods. The ﬁrst method
(which we denote PS0) includes a propensity score estimated
with a (main effects) logit model and with matching without replacement via nearest neighbors applied to the estimated score.
We also estimate a set of CEM methods, with coarsening generated by progressively cutting the support of X from 2–11
equally sized intervals. We denote these matching solutions C2,
C3,..., C11. For PS0 mT = mC = nT and for CEM mT and mC
are functions of the CEM solution. The weights wi for PS0 are
wi = 1/nT and for CEM are the usual CEM weights.
Figure 6 reports the ratio of the RSME (based on PATT) between PS0 and the different CEM methods (on the horizontal
axis) and for each of the six datasets (separate lines, numbered
according to gi). When the plotted points are below the dotted
horizontal line drawn at a ratio of 1, the RMSE is better for the
corresponding CEM method than for PS0. Thus, in this simulation, CEM has lower RMSE for 59 of the 60 experiments, and
is approximately tied in the last (at the top left of the graph).
We also offer in Figure 7 the results when the target quantity
of interest is changed from τk into the local PATT, τ m
k , using the
treated units matched by each CEM method. In this case, we report the absolute value of the RMSE because there is no propensity score or other base line to compare the different methods.
This graph shows that CEM does not suffer as it coarsens more
and drops more units; in fact, RMSE drops even as the number
of observations (in parentheses beneath each label on the horizontal axis) declines. This makes sense, of course, because the
variance is a function not only of n but also of the heterogeneity, which is reduced by matching. Thus, in these experiments,
CEM has lower RMSE than PS0, even though the data were
drawn to follow EPBR’s requirements.
Multidimensional Gaussian Case, With Propensity Score
Model Selection.
We now consider Gaussian data with ﬁve
covariates. We compare CEM with the standard propensity
score estimated by the usual logit model (“PS0”) and a more
Figure 5. Graphs of x horizontally by gk(x,t) vertically, for lines k = 1,...,6. In each, the functions generating the treated t = 1 (solid lines)
and control t = 0 (dotted lines) groups appear.
Journal of the American Statistical Association, March 2011
Figure 6. Ratio of RMSE of PS0 and 10 CEM methods for PATT. One dimensional case. Average values over 1000 Monte Carlo replications.
In the graphs: gi = i. A value less than 1 means the CEM method is preferable to PS0 in terms of RMSE. On the x axis Ck corresponds to a
CEM solution with support of x cut into k equal sized intervals (and below in parentheses the number of treated matched units). Total number of
treated units in the original sample is 200.
appropriate propensity score model optimized according to balance rather than ﬁt Imbens and Rubin (“PS1”).
For CEM, we consider a group of 50 different random coarsenings. For each variable Xi, i = 1,...,5, we cut the support of
Xi by a random number of equispaced cutpoints selected from
the uniform discrete distribution U( ). (Unlike our previous one dimensional simulation, the coarsenings here cannot
be ordered, and so we order results for CEM according to the
number of matched treated units.)
We draw ﬁve covariates with NT = NC = 5000 from N(0,I)
for the treated units and N(2,I) for the control units, where
I is the 5 × 5 identity matrix, 0 = (0,0,0,0,0)′ and 2 =
(2,2,2,2,2)′. This again ﬁts EPBR’s data requirements. We
use the following diverse multivariate gk(x,t) functions in the
same way our previous simulation: g1(x,t) = 100 · t + t · x1 ·
e|x2−2| +log(10+x3)+100·(1−t)·x2 ·e|x4+2| +x2
5 +x3 ·x4 ·x5,
g2(x,t) = 100 · t + !5
i=1 xi, g3(x,t) = 100 · t + !5
i , g4(x,t) = 100 + 100 · t · !5
i=1 xi + 250 · (t −1) ·
i , and g5(x,t) = 100 + 100 · t · !5
i=1 xi + 250 · (t −
i=1 xi. For each simulation, we randomly draw nT = 500
treated units and nC = 1000 control units.
Figure 8 reports the ratio of the RSME for PS0 to each of the
different CEM methods, for PATT as the target quantity of interest. In this plot, CEMk corresponds to the kth CEM solution
based on the 50 different coarsenings and the numerical labels
on the lines correspond to function gi, i = 1,...,6. The results
indicate that CEM dominates the propensity score methods, as
all experiments have lower RMSE than all the propensity score
solutions.
Figure 9 reports the absolute value of the RMSE for the different methods when the target quantity of interest is changed
to the local PATT deﬁned by the treated units matched by
CEM and PS1. In the top left corner of the plot, PS(i) is
PS1 for function gi, i = 1,...,6. Again we can see that the
RMSE does not increase as the number of matched observations drops.
Figure 7. Absolute RMSE for different CEM solutions, for the local PATT. One-dimensional case. Average values over 1000 Monte Carlo
replications. In the graphs: gi = i. On the x axis Ck corresponds to a CEM solution with support of x cut into k + 2 equal sized intervals (and
below in parentheses the number of treated matched units). Lower values are better. Number of treated units in the original sample is 200.
Iacus, King, and Porro: Multivariate Matching Methods
Figure 8. Ratio of RMSE of PS0 and CEM for PATT. Five-dimensional case. Average values over 1000 Monte Carlo replications for gi = i.
The x axis CEMk corresponds to the kth CEM solution (ordered by the number of treated matched units, given in parentheses). PS1(i) denotes
the ratio of RMSE for PS0 to PS1 for gi. A value less than 1 means that the given matching solution is preferable to PS0 in terms of RMSE.
Number of treated units in the sample prior to matching: 500.
Lalonde Data.
In this ﬁnal analysis, we use the data from
Lalonde as our population (with NT = 297 and NC =
425). From these data, we randomly sample nT = 150 and
nC = 300 observations from the treated and control groups. We
generate the outcome variable via gk(x,t) functions: g1(x,t) =
1000 + 2000 · t · age + re74 + log(1 + re75) + black ·
re752, g2(x,t) = 1000 + 2000 · t + age + re74 + re752,
g3(x,t) = 1000 + 2000 · t + age + re74 + re75 + black +
education, and g4(x,t) = 1000 + 2000 · t + age + re74 +
re75+black+education+hispanic+nodegree+
Figure 10 reports the ratio of the RSME between PS0 and
the other matching methods, for PATT as the target quantity of
interest, while Figure 11 reports the absolute value of RMSE
for target quantity of interest the local PATT. The notation in
the ﬁgures is analogous that in the previous section. The results
are also similar, in that CEM again clearly outperform the other
methods, with lower RMSE and error that does not increase as
matching becomes more stringent, leading to smaller matched
7. CONCLUDING REMARKS
We offer a new class of matching methods that generalizes
the only existing class proposed. This new monotonic imbalance bounding class enables the creation of methods that are
easy to apply and which we show possess a variety of desirable properties that should be of considerable use to applied
researchers. We offer Coarsened Exact Matching as one such
example, and demonstrate how it generates matching solutions
that are better balanced and estimates of the causal quantity
of interest that have lower root mean square error than methods under the older existing class, such as based on propensity
scores, Mahalanobis distance, nearest neighbors, and optimal
Figure 9. Absolute RMSE for different matching solutions, for PATT. Five-dimensional case. Average values over 1000 Monte Carlo replications for lines labeled as gi = i. On the x axis CEMk corresponds to the kth CEM solution (see text) (with the number of matched treated units
in parentheses). PS1(i) denotes the RMSE for PS1 for gi. Lower values are better. Total number of treated units in the sample: 500.
Journal of the American Statistical Association, March 2011
Figure 10. Ratio of RMSE of PS0 to CEM for PATT from the Lalonde data. Average values over 1000 Monte Carlo replications for lines
labeled as gi = i. On the x axis, CEMk corresponds to the kth CEM solution (see text) (with the number of matched treated units in parentheses).
PS1(i) is the ratio of RMSE for PS0 to PS1 for function gi. A value less than 1 means that the matching solution is preferable to PS0 in terms of
RMSE. Total number of treated units in the sample: 150.
APPENDIX: PROOFS OF PROPOSITIONS
IN SECTION 3
Proof of Proposition 2
Let us introduce the means by strata: ¯Xms
i∈T s Xij,
i∈Cs Xij. Then
i∈T Xijwi =
i∈Ts Xij =
T,j and ¯Xw
i∈Cs Xij mC
C,j. Hence, given
that the mean is internal, in each stratum observations are at most far as
ϵj; thus, | ¯Xw
Proof of Proposition 3
We ﬁrst rewrite ¯µk
$$Xij −¯Xw
"$$Xij −¯Xw
and then apply the binomial expansion to the inner term of the summation
"$$Xij −¯Xw
/$$Xij −¯Xw
by Proposition 2 we can write
"$$Xij −¯Xw
/$$Xij −¯Xw
θjh1k−h = ϵk
j (θj + 1)k.
Therefore,
j (θj + 1)k 1
i∈Cs wi = ϵk
j (θj + 1)k
C = 1. Since
i∈T s wi = 1. The same bound
Figure 11. Absolute RMSE for different matching solutions, for the local PATT from the Lalonde data. Values averaged over 1000 Monte
Carlo replications, with lines labeled as gi = i. On the x axis CEMk corresponds to the kth CEM solution (see text) (with the number of matched
treated units in parentheses). PS1(i) denotes the RMSE for PS1 for gi. Lower values are better. Total number of treated units in the sample: 150.
Iacus, King, and Porro: Multivariate Matching Methods
exists for ¯µk
T,j, so their absolute difference is | ¯µk
Proof of Proposition 4
Consider the qth empirical quantiles of the distribution of the treated
and control units, Xq
mT,j and Xq
mC,j. That is, Xq
mT,j is the qth ordered
observation of the subsample of mT matched treated units, and similarly for Xq
mC,j. In one-to-one matching, the ﬁrst treated observation is
matched against the ﬁrst control observation in the ﬁrst stratum and, in
general, the corresponding quantiles belong to the same strata. Therefore, |Xq
mC,j| < ϵj.
Proof of Proposition 5
Consider the generic stratum [as,bs], s ∈S, where as is the leftmost cut-point of the discretization and bs = as + ϵj. For simplicity,
take s = 1, so that Fw
mT,j(a1) = Fw
mC,j(a1) = 0. Then Fw
mT,j(b1) =
/mT because there are at most ms=1
treated units less than or
equal to b1. Similarly, for the weighted distribution of the control units
Thus, for each stratum, Fw
mT,j(bs) = ms
mC,j(bs), and hence
the difference between weighted empirical distribution functions at
the end points of each stratum [as,bs] is always zero. Therefore, the
weighted quantiles of the same order for treated and control units always belong to the same stratum and hence the difference between
them is at most ϵj.
[Received October 2009. Revised December 2010.]