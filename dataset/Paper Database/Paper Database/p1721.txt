Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 199–231,
Berlin, Germany, August 11-12, 2016. c⃝2016 Association for Computational Linguistics
Results of the WMT16 Metrics Shared Task
Ondˇrej Bojar
Charles Univ. in Prague
 
Yvette Graham
Dublin City Univ.
 
Amir Kamran and Miloˇs Stanojevi´c
Univ. of Amsterdam
{a.kamran,m.stanojevic}@uva.nl
This paper presents the results of the
WMT16 Metrics Shared Task. We asked
participants of this task to score the outputs of the MT systems involved in the
WMT16 Shared Translation Task.
collected scores of 16 metrics from 9 research groups. In addition to that, we computed scores of 9 standard metrics (BLEU,
SentBLEU, NIST, WER, PER, TER and
CDER) as baselines. The collected scores
were evaluated in terms of system-level
correlation (how well each metric’s scores
correlate with WMT16 ofﬁcial manual
ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two
translations of a particular sentence).
This year there are several additions to
the setup: large number of language pairs
(18 in total), datasets from different domains (news, IT and medical), and different kinds of judgments: relative ranking
(RR), direct assessment (DA) and HUME
manual semantic judgments. Finally, generation of large number of hybrid systems
was trialed for provision of more conclusive system-level metric rankings.
Introduction
Automatic evaluation of machine translation quality is essential in the development and selection of
machine translation systems. Many different automatic MT quality metrics are available and the
Metrics Shared Task1 is held annually at WMT
to assess their quality, starting with Koehn and
Monz and following up to Stanojevi´c et al.
1 
metrics-task/
Metrics participating in the metrics task rely on
the existence of reference translations with which
MT outputs are compared, and the metrics task itself then needs manual judgments of translation
quality in order to check the extent to which the
automatic metrics can approximate the judgment.
A related WMT task on quality estimation assesses the performance of methods where no reference translations are needed, requiring only the
manual quality judgments .
This year, we keep the two main types of metric evaluation: system-level, where a metric is expected to provide a quality score for the whole
translated document, and segment-level, where the
score is needed for every individual sentence.
We experiment with several novelties. Speciﬁcally, test sets this year come from three domains:
news, IT and medical/health-related texts.
The added domains bring in an extended set
of languages.
In sum, the metrics task this
year includes 18 language pairs, English paired
with Basque, Bulgarian, Czech, Dutch, Finnish,
German, Polish, Portuguese, Romanian, Russian,
Spanish, and Turkish, in one or both directions.
On the evaluation side, we rely on three golden
truths of manual judgment:
• Relative Ranking (RR) of up to 5 different
translation candidates at a time, as collected
in WMT in the past,
• Direct Assessment (DA) evaluating the adequacy of a translation candidate on an absolute scale in isolation from other translations,
• HUME, a composite segment-level score aggregated over manual judgments of translation quality of semantic units of the source
Additional changes to the task evaluation include a change in the way we compute conﬁdence
Tuning Task
HimL Year 1
cs de ro ﬁru tr
English into
into-English
cs de ro ﬁru tr bg es eu nl pl pt
newstest2016
it-test2016
newstest2016
newstest2016
newstest2016
Table 1: Overview of “tracks” of the WMT16 metrics task. “•” indicates language pairs covered in
the evaluation, “·” are language pairs planned but abandoned due to difﬁculties in obtaining human
judgments.
intervals for metric correlations with human assessment, resulting in more reliable conclusions as
to which metrics outperform others.
The ofﬁcial method of evaluation remains unchanged, relying on RR in both the system-level
(TrueSkill) and segment-level (Kendall’s τ) metrics, see below for details and references.
Our datasets are described in Section 2. This includes the test sets, system outputs, human judgments of translation quality as well as participating
metrics across the tasks. Results of system-level
metric evaluation are provided in Section 3.1 and
Section 3.2, the results of the segment-level evaluation are provided in Section 3.3.
Table 1 provides the complete picture of the
golden truths, test sets, translation systems and
language pairs involved in the metrics task this
year. For simplicity, we called each of these setups
a “track”, indicating the underlying type of golden
truth (RR/DA/HUME), system- or segment-level
evaluation (sys/seg) and the particular test set.
While the set of setups is much larger this year,
the participants of the task were affected rather
minimally. Participants were only required to run
metrics on the additional test sets and with an additional large set of hybrid systems in the systemlevel evaluation. As in the previous years, participants were allowed take part in any subset of language pairs and setups.
We use the following test sets:
newstest2016 is the main test set. It is the test set
used in WMT16 News Translation Task , with approximately 3,000
sentences for each translation direction (with
the exception of Romanian which only has
1,999 sentences).
The set includes a single reference translation for each direction,
except English→Finnish with two reference
translations.
it-test2016 is the set of 1,000 sentences translated from English into seven other European
languages.
The IT test sentences typically
contain instructions for operating commonly
used software like web browsers, mail clients
or image editors, e.g.: “In message box click
on More > Archived.”
himl2015 is part of the ofﬁcial test set created
by the EU project HimL.2 These are healthrelated texts from Cochrane summaries and
NHS 24 online content. The texts originated
in English and the target languages consist
of Czech, German, Polish and Romanian versions created by post-edition of phrase-based
MT output. From the full set of about 3,000
sentences, 800 were given as input to the
participants of the metrics task and in the
end about 340 sentences per language pair
were used for evaluation, as those sentences
have manual score suitable to employ as the
golden truth for metric evaluation.
The sentences of NHS 24 tend to be shorter
and simpler translations, e.g. “Choose lower
fat options such as semi-skimmed milk and
low fat yogurt.”, while Cochrane summaries
are longer and often contain speciﬁc terminology, e.g. “The purpose of this research
was to determine how good the TEG and
ROTEM assessments are at diagnosing TIC
in adult trauma patients who are bleeding.”
Translation Systems
Characteristics of the particular underlying translation task MT systems is likely an important fac-
2 
tor affecting the difﬁculty of the metrics task. For
instance, if all of the systems perform similarly, it
will be more difﬁcult, even for the humans, to distinguish between the quality of translations. If the
task includes a wide range of systems of varying
quality, however, or systems quite different in nature, this could in some way could make the task
easier for metrics, with metrics that are more sensitive to certain aspects of MT output performing
The MT systems included in evaluation of metrics are as follows:
News Task Systems are all MT systems participating in the WMT16 News Translation Task
 .
These systems differ widely in nature (standard phrase-based,
syntax-based, transfer-based or even rulebased systems, also with a large number of
neural MT systems), with the precise set of
systems and system types also depending on
speciﬁc language pair.
Tuning Task Systems are
phrasebased systems run by the organizers of the
WMT16 Tuning Task .
All of these systems share the same phrase
tables and language models, they are trained
on relatively large volumes of data, and differ
only in the model weights as provided by the
participants of the tuning task. Tuning task
was limited to Czech↔English language
IT Task Systems are participants of the WMT16
IT-domain Translation Task , translating only from English to
seven other European languages. This is generally a smaller set of systems and the number of covered system architectures here is
also smaller. As far as we know, no neural
system was involved in the task.
HimL Year 1 Systems are MT systems released
in the ﬁrst year of the EU project HimL3.
They are all Moses-based and trained on
available data in the medical or health-related
Hybrid Systems were created by combining the
output of two newstest2016 translation task
systems, with the aim of providing a larger
3 
set of systems against which to evaluate metrics, as described further in Section 3.1. In
short, we create 10K hybrid MT systems for
each language pair.
Excluding the hybrid systems, we ended up
with 171 system outputs across 18 language pairs
and 3 test sets.
Manual MT Quality Judgments
There are three distinct “golden truths” employed
to evaluate metrics this year: Relative Ranking
(RR, as in previous year), Direct Assessment (DA)
and HUME, a semantic-based manual metric.
The details of the methods are provided in
this section, separately for system-level evaluation
(Section 2.3.1, using RR and DA) and segmentlevel evaluation . DA judgments are
more robust in this respect and while the original plan was to collect DA from both researchers
and crowd-sourced non-experts, only the latter ultimately took place due to time constraints.
System-level Manual Quality
In system-level evaluation, the goal is to assess
the quality of translation of an MT system for the
whole document. Both our manual scoring methods RR and DA nevertheless proceed sentence by
sentence, aggregating the ﬁnal score in some way.
Relative Ranking (RR)
As in previous WMT
shared tasks, human assessors of MT output (only
researchers this year) were presented with the
source language input, target language reference
translation and the output of ﬁve distinct MT output translations. Human assessors were required
to rank the ﬁve translations from best to worse,
with ties allowed. As introduced in WMT15, identical translations from distinct systems were collapsed into a single translation before running the
human evaluation to increase the overall efﬁciency
of RR human assessment.
Each ﬁve-tuple relative ranking was employed
to produce 10 pairwise assessments, later combined into a score for each MT system that re-
ﬂects the frequency by which the output of that
system was preferred to the output of other systems. Several methods have been tested in the past
for the exact score calculation and WMT16 has
again adopted TrueSkill as the ofﬁcial ranking approach. Please see the WMT16 overview paper
for details on how this score is computed.
To increase annotator efﬁciency, a maximum
sentence length of 30 words was applied to RR
human assessment.
Direct Assessment (DA)
In addition to the standard relative ranking (RR) manual evaluation
employed to yield ofﬁcial system rankings in
WMT16 translation task, this year the translation
task also trialed a new method of human evaluation, monolingual direct assessment (DA) of translation ﬂuency and adequacy
 . For
investigatory purposes, therefore, we also include
evaluation of metrics with reference to the newly
trialed human assessment method.
Since sufﬁcient levels of agreement in human
assessment of translation quality are difﬁcult to
achieve, the DA setup simpliﬁes the task of translation assessment (conventionally a bilingual task)
into a simpler monolingual assessment for both
ﬂuency and adequacy. Furthermore, DA avoids
bias that has been problematic in previous evaluations introduced by simultaneous assessment
of several alternate translations of a given single
source language input, where scores of systems for
which translations were often compared to high or
low quality translations resulted in an unfair advantage or disadvantage . DA
achieves this by assessment of individual translations in isolation from other outputs of the same
source input.
Translation adequacy is structured as a monolingual assessment of similarity of meaning where
the target language reference translation and the
MT output are displayed to the human assessor.
Human assessors rate a given translation by how
adequately it expresses the meaning of the reference translation on an analogue scale corresponding to an underlying 0-100 rating scale.4 Fluency
assessment is similar to adequacy except that no
reference is displayed and assessors are asked to
rate how much they agree that a given translation
4The only numbering displayed on the rating scale are extreme points 0 and 100%, and three ticks indicate the levels
of 25, 50 and 75 %.
is ﬂuent target language text.
Large numbers of DA human assessments of
translations for seven language pairs (targeting
English and Russian) were collected on Amazon’s
Mechanical Turk,5 via sets of 100-translation hits
to ensure sufﬁcient repeat items per worker, before application of strict quality control measures
to ﬁlter out assessments from poorly performing
In order to iron out differences in scoring strategies attributed to distinct workers, human assessment scores for translations were standardized according to an individual worker’s overall mean
and standard deviation score. Mean standardized
scores for translation task participating systems
were computed by ﬁrstly taking the average of
scores for individual translations in the test set
(since some were assessed more than once), before
combining all scores for translations attributed to
a given MT system into its overall adequacy or ﬂuency score.
Although the WMT16 Translation Task included both ﬂuency and adequacy DA human assessment, the metrics task this year employed only
DA adequacy scores. We hope to incorporate DA
ﬂuency into future metric evaluations, however.
Finally, although it is common to apply a sentence length restriction in WMT human evaluation, the simpliﬁed DA setup does not require
restriction of the evaluation in this respect and
no sentence length restriction was applied in DA
Segment-level Manual Quality
Segment-level metrics have been evaluated against
the pairwise judgments implied by the 5-way relative ranking annotation. This year, we add two
new variants of human assessment: segment-level
DA and HUME.
Segment-level DA
Adequacy assessments were
collected for translations sampled from the output of systems participating in WMT16 translation task for seven language pairs 
RWTH Aachen University 
CHRF1,2,3, WORDF1,2,3
Humboldt University of Berlin 
Charles University, no corresponding paper
DPMFCOMB-WITHOUT-RED
Chinese Academy of Sciences and Dublin City University 
Jiangxi Normal University 
University of Wolverhampton 
UPF-COBALT, COBALTF, METRICSF
Universitat Pompeu Fabra 
University of St Andrews, 
Table 2: Participants of WMT16 Metrics Shared Task
Segment-level DA adequacy scores were collected as in system-level DA, described in Section 2.3.1, again with strict quality control and
score standardization applied. To achieve accurate
segment-level scores for translations, a human assessment of each translation was collected from 15
distinct human assessors before combination into
a mean adequacy score for each individual translation. Although in general agreement in human
assessment of MT has been difﬁcult to achieve,
segment-level DA scores employing a minimum
of 15 repeat assessments have been shown to be
almost perfectly replicable. In repeat experiments,
for all tested language pairs, a correlation of above
0.9 between (a) segment-level DA scores for translations collected in an initial experiment run and
(b) the same collected in a repeat evaluation of the
same translations, by combining assessments of a
minimum of 15 human assessors 
is a novel human evaluation measure that decomposes over the UCCA semantic units.
 is an appealing
candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and
adjectival argument structures and their interrelations. HUME operates by aggregating human
assessments of the translation quality of individual semantic units in the source sentence.
thus avoid the semantic annotation of machinegenerated text, which is often garbled or semantically unclear.
This also allows the re-use of
the source semantic annotation for measuring the
quality of different translations of the same source
sentence, and avoids reliance on possibly suboptimal reference translations. HUME shows good
inter-annotator agreement, and reasonable correlation with Direct Assessment .
Participants of the Metrics Shared Task
Table 2 lists the participants of the WMT16
Shared Metrics Task, along with their metrics. We
have collected 16 metrics from a total of 9 research
The following subsections provide a brief summary of all the metrics that participated.
list is concluded by our baseline metrics in Section 2.4.10.
BEER is a trained
evaluation metric with a linear model that combines features capturing character n-grams and
permutation trees. BEER has participated in previous years of the evaluation task. This year the
learning algorithm is improved (linear SVM instead of logistic regression) and some features that
are relatively slow to compute are removed (paraphrasing, syntax and permutation trees) which resulted in a very large speed-up. BEER is usually
trained for ranking but in this case there was a
compromise: the initial model is trained for ranking (RR) with ranking SVM and then the output from SVM is scaled using trained regression
model to approximate absolute judgment (DA).
CHARACTER is a novel
character-level metric inspired by the commonly
applied translation edit rate (TER). It is deﬁned as
the minimum number of character edits required
to adjust a hypothesis, until it completely matches
the reference, normalized by the length of the hypothesis sentence.
CHARACTER calculates the
character-level edit distance while performing the
shift edit on word level. Unlike the strict matching
criterion in TER, a hypothesis word is considered
to match a reference word and could be shifted, if
the edit distance between them is below a threshold value. The Levenshtein distance between the
reference and the shifted hypothesis sequence is
computed on the character level. In addition, the
lengths of hypothesis sequences instead of reference sequences are used for normalizing the edit
distance, which effectively counters the issue that
shorter translations normally achieve lower TER.
CHRF and WORDF
WORDF1,2,3 calculate a simple
F-score combination of the precision and recall of
word n-grams of maximal length 4 with different
setting for the β parameter (β = 1, 2, or 3). Precision and recall that are used in computation of
the F-score are arithmetic averages of precisions
and recalls, respectively, for the different n-gram
orders. CHRF1,2,3 calculate the F-score of character n-grams of maximal length 6. β parameter
gives β times weight to recall: β = 1 implies
equal weights for precision and recall.
DEPCHECK is based on the automatic post-editing
tool Depﬁx . For each sentence, DE-
PCHECK computes the percentage of nodes postedited by Depﬁx, obtaining a “relative depcheck
error rate” (RDER). The value of the DEPCHECK
metric is then deﬁned as 1 −RDER. DEPCHECK
does not distinguish the error types or whether
there was more than one Depﬁx rule applied to a
node. It is suggested for a future version of DE-
PCHECK to assign a weight (either by hand, or
training from some golden data) to each rule that
was applied to the MT output.
DPMFCOMB-WITHOUT-RED
The authors of DPMFCOMB-WITHOUT-RED follow the work on last year’s metric DPMFCOMB
 , but modify it with two main
differences.
Firstly, they use the ‘case insensitive’ instead of ‘case sensitive’ option when
using Asiya.
Secondly, REDP are not used.
Thus, DPMFCOMB-WITHOUT-RED is a combined metric including 57 single metrics. Weights
of the individual metrics are trained with SVMrank, using training data from the English-targeted
language pairs from WMT12 to WMT14.
the results DPMFCOMB-WITHOUT-RED is represented as DPMFCOMB for brevity.
DTED is based
on Tree Edit Distance. The scoring is done over
the dependency parse tree of the output where the
number of edit operations (insert, delete or substitute) needed to convert it to the correct (reference) dependency tree is used as an indicator of
the translation quality. Unlike the majority of metrics which evaluate many aspects of translation,
DTED evaluates only the word order.
MPEDA is developed on the
basis of the METEOR metric.
In order to accurately match words or phrases with the same
or similar meaning, it extracts a domain-speciﬁc
paraphrase table from the monolingual corpus and
applies that paraphrase table to the METEOR
metric to replace the general one.
Unlike traditional paraphrase extraction approaches, it ﬁrst
ﬁlters out a domain-speciﬁc sub-corpus from a
large general monolingual corpus and then extracts domain-speciﬁc paraphrase table from the
sub-corpus by Markov Network model.
the proposed paraphrase extraction approach can
be used in all languages, MPEDA is languageindependent.
dependency-tree
(LSTM) network to represent both the hypothesis
and the reference with a dense vector. Training
is performed using the judgements from WMT13
 converted to similarity scores.
The ﬁnal score at the system level is obtained
by averaging the segment level scores obtained
from a neural network which takes into account
both distance and Hadamard product of the two
representations.
UOW.REVAL is the same as UOW LSTM
 that participated in the
WMT15 task except that LSTM vector dimension
is 150 for UoW.ReVal instead of 300.
English into
into-English
T5,F4,T6 T5,F5
T5,F6 T5,F6
T8 T8,F7 T8 T8,F7
T4,F3,T7 T4,F1,T7 T4,F1,T7 T4,F1,T7 T4,F2,T7 T4,F2,T7
T11,F10 T11,F10 T11,F10
Table 3: Overview of tables (T) and ﬁgures (F) reporting results of the individual “tracks” and language
UPF-COBALT, COBALTF and
UPF-COBALT is an
alignment-based metric that examines the syntactic contexts of lexically similar candidate and
reference words in order to distinguish meaningpreserving variations from the differences indicative of MT errors. This year the metric was improved by explicitly addressing MT ﬂuency. The
new version of the metric, COBALTF, combines
various components of UPF-COBALT with a number of ﬁne-grained features intended to capture the
number and scale of disﬂuent fragments contained
in MT sentences.
METRICSF is a combination
of three evaluation systems, BLEU, METEOR and
UPF-COBALT, with the ﬂuency-oriented features.
Baseline Metrics
As mentioned by Bojar et al. , metrics
task occasionally suffers from “loss of knowledge” when successful metrics participate only in
We attempt to avoid this by regularly evaluating
also a range of “baseline metrics”:
MTEVALBLEU
NIST were computed
mteval-v13a.pl7
which is used in the OpenMT Evaluation
Campaign and includes its own tokenization.
We run mteval with the ﬂag
--international-tokenization
since it performs slightly better .
BLEU, MOSESTER ,
MOSESWER, MOSESPER and MOSECDER
 were produced by the
Moses scorer which is used in Moses model
7 
optimization. To tokenize the sentences, we
used the standard tokenizer script as available
in Moses toolkit. Since Moses scorer is versioned on Github, we strongly encourage authors of high-performing metrics to add them
to Moses scorer, as this will ensure that their
metric can be included in future tasks.
As for segment-level baselines, we employ the
following modiﬁed version of BLEU:
• SentBLEU. The metric SENTBLEU is computed using the script sentence-bleu, part of
the Moses toolkit. It is a smoothed version
of BLEU that correlates better with human
judgments for segment-level.
For computing system-level scores, the same
script was employed as in last year’s metric task.
New scripts have been added for system-level hybrids and segment-level evaluation.
Table 3 provides an overview of all the tables and
ﬁgures in the rest of the paper. We discuss systemlevel results for news task systems (including tuning task systems) in Section 3.1. The system-level
results for the IT domain are discussed in Section 3.2.
The segment-level results are in Section 3.3. We end with discussion in Section 3.4.
System-Level Results for News Task
As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for
system-level metrics, as follows:
i=1(Hi −H)(Mi −M)
i=1(Hi −H)2
i=1(Mi −M)2
where H are human assessment scores of all
systems in a given translation direction, M are
corresponding scores as predicted by a given metric. H and M are their means respectively.
Since some metrics, such as BLEU, for example, aim to achieve a strong positive correlation
with human assessment, while error metrics, such
as TER aim for a strong negative correlation, after
computation of r for metrics, we compare metrics
via the absolute value of a given metric’s correlation with human assessment.
Table 4 includes results for system-level into-
English metrics for evaluation of systems participating in the main translation task (newstest2016),
evaluated against RR and DA human assessment
variants, while Table 5 includes the same for the
newstest2016 out-of-English language pairs (only
Russian has the DA judgments).
Tuning systems were excluded from Tables 4 and 5 and they
are covered by Table 6 that shows correlations
achieved by metrics with RR when the set of systems additionally includes tuning task systems.
In previous years, we reported empirical con-
ﬁdence intervals of system-level correlations obtained by bootstrap resampling human assessments data and computing conﬁdence intervals for
individual correlations with human assessment.
Such conﬁdence intervals reﬂect the variance due
to particular sentences and assessors involved in
the evaluation but lead to over-estimation of signiﬁcant differences if employed to conclude which
metrics outperform others. This year, as recommended by Graham and Baldwin , instead
we employ Williams signiﬁcance test . Williams test is a test of signiﬁcance of a
difference in dependent correlations and therefore
suitable for evaluation of metrics. Correlations not
signiﬁcantly outperformed by any other are highlighted in bold in Tables 4 and 5. Since RR is the
ofﬁcial method of evaluation for this year’s metrics task, bolded correlations under RR comprise
ofﬁcial winners of the news domain portion of the
system-level metrics task. DA results are included
for comparison and are investigatory only.
With regard to which individual metric may or
may not outperform other metrics, such as the
important comparison as to which metrics signiﬁcantly outperform the most widely employed
metric BLEU (in its mteval or Moses scorer implementation), Figures 1, 2, 3, 4, 5, and 6 include signiﬁcance test results for every competing
pair of metrics including our baseline metrics. In
heatmaps in Figures 1, 2, 3, 4, 5, and 6, the column
labelled “MTEVALBLEU” or “MOSESBLEU” can
be used to quickly observe which metrics achieve
MTEVALBLEU
MTEVALNIST
newstest2016
Table 6: Absolute Pearson correlation of cs-en and
en-cs system-level metric scores with human assessment variant RR + TT, i.e. standard WMT
relative ranking including tuning task systems.
a signiﬁcant increase in correlation with human assessment over that of BLEU, where a green cell in
the column denotes outperformance of BLEU by
the metric in that row.
For investigatory purposes only, we also include
hybrid-supersample results for system-level metrics. 10K hybrid systems
were created per language pair, with corresponding DA human assessment scores, by sampling
pairs of systems from WMT16 translation task
and creating a hybrid system by combining translations from each system to create new hybrid output test set documents, each with a corresponding
DA human assessment score. Not all metrics participating in the system-level metrics shared task
submitted metric scores for the large set of hybrid systems, possibly due to the increased time
required to run metrics on the large set of 10K
systems. In this respect, DA hybrid may provide
some indication of which metrics are likely to be
more feasible to employ for tuning purposes in MT
systems out-of-the-box. Due to time constraints,
this year it was only possible to include hybridsupersampling results for language pairs evaluated
by the DA human assessment variant.
Correlations of metric scores with human assessment of the large set of hybrid systems are
MTEVALNIST
MTEVALBLEU
newstest2016
Table 4: Absolute Pearson correlation of to-English system-level metric scores with human assessment
variants: RR = standard WMT relative ranking; DA = direct assessment of translation adequacy.
MTEVALBLEU
MTEVALNIST
CHRF3.2REF
CHRF2.2REF
CHRF1.2REF
WORDF3.2REF
WORDF2.2REF
WORDF1.2REF
newstest2016
Table 5: Absolute Pearson correlation of out-of-English system-level metric scores with human assessment variants: RR = standard WMT relative ranking; DA = direct assessment of translation adequacy.
DA Hybrids
10 Systems
10 Systems
10k Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
newstest2016
DA Hybrids
10k Systems
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
newstest2016
DA Hybrids
10k Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
newstest2016
German-to-English (de-en), Finnish-to-English (ﬁ-en) and Romanian-to-English (ro-en)
system-level metric signiﬁcance test results for human assessment variants; green cells denote a signiﬁcant increase in correlation with human assessment for the metric in a given row over the metric in
a given column according to Williams test; RR = standard WMT relative ranking for translation task
systems only; DA = direct assessment of translation adequacy; DA Hybrids = direct assessment with
hybrid super-sampling.
DA Hybrids
10 Systems
10 Systems
10k Systems
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
newstest2016
DA Hybrids
10k Systems
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
newstest2016
DA Hybrids
12 Systems
12 Systems
10k Systems
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
newstest2016
Figure 2: Russian-to-English (ru-en), Turkish-to-English (tr-en) and English-to-Russian (en-ru) systemlevel metric signiﬁcance test results for human assessment variants; green cells denote a signiﬁcant
increase in correlation for the metric in a given row over the metric in a given column according to
Williams test; RR = standard WMT relative ranking for translation task systems only; DA = direct
assessment of translation adequacy; DA Hybrids = direct assessment with hybrid super-sampling.
12 Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
DA Hybrids
10k Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
newstest2016
Figure 3: Czech-to-English (cs-en) system-level metric signiﬁcance test results for human assessment
variants; a green cell corresponds to a signiﬁcant increase in correlation for the metric in a given row
over the metric in a given column according to Williams test; RR = standard WMT relative ranking for
translation task systems only; RR + TT = standard WMT relative ranking for all cs-en newstest2016
systems; DA = direct assessment of translation adequacy; DA Hybrids = direct assessment with hybrid
super-sampling.
20 Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
newstest2016
ittest2016
Figure 4: English-to-Czech (en-cs) system-level metric signiﬁcance test results; a green cell corresponds
to a signiﬁcant increase in correlation for the metric in a given row over the metric in a given column
according to Williams test; RR = standard WMT relative ranking; RR + TT = standard WMT relative
ranking for translation and tuning task systems.
15 Systems
10 Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
newstest2016
ittest2016
Figure 5: English-to-German (en-de) system-level metric signiﬁcance test results; a green cell corresponds to a signiﬁcant increase in correlation for the metric in a given row over the metric in a given
column according to Williams test; RR = standard WMT relative ranking.
13 Systems
chrF3.2ref
chrF2.2ref
chrF1.2ref
wordF3.2ref
wordF2.2ref
mtevalNIST
wordF1.2ref
mtevalBLEU
mtevalBLEU
wordF1.2ref
mtevalNIST
wordF2.2ref
wordF3.2ref
chrF1.2ref
chrF2.2ref
chrF3.2ref
12 Systems
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
newstest2016
Figure 6: English-to-Finnish (en-ﬁ), English-to-Romanian (en-ro) and English-to-Turkish (en-tr) systemlevel metric signiﬁcance test results; a green cell corresponds to a signiﬁcant increase in correlation for
the metric in a given row over the metric in a given column according to Williams test; RR = standard
WMT relative ranking.
shown in Table 7, where again metrics not significantly outperformed by any other are highlighted
Results are for investigatory purposes
only and do not indicate ofﬁcial winners, however.
Figures 1, 2 and 3 also include signiﬁcance test results for hybrid super-sampled correlations for all
pairs of competing metrics for a given language
In Appendix A, correlation plots for each language pair are also provided.
The left-hand
plot visualizes the correlation of MTEVALBLEU
and manual judgements, while the right-hand plot
shows the correlation for the best performing metrics for that pair according to both standard RR
and DA, as per Tables 4, 5 and 7.
System-Level Results for IT Task
Since systems participating in the IT domain translation task were manually evaluated with RR, we
include evaluation of metrics for translation of
this speciﬁc domain. Results of all metrics evaluated on the IT domain MT systems are shown
in Table 8, where ofﬁcial winning metrics for this
domain are identiﬁed as those not signiﬁcantly
outperformed by any other metric according to
Williams test, correlations for which are highlighted in bold.8
Full pairwise signiﬁcance test results for every
pair of competing metrics evaluated on IT domain
systems for Spanish, Dutch and Portuguese are
shown in Figure 7, German in Figure 5 and Czech
in Figure 4. No signiﬁcance tests are provided for
IT domain Bulgarian and Basque, as all metrics
achieved equal correlations.
We see from Table 8 and also Figure 7 that
MOSESBLEU does not belong to the winners for
several target languages (Czech, German, Dutch),
but across the board, metrics are hard to distinguish on this speciﬁc test set.
Segment-Level Results
In WMT16, the ofﬁcial method for segment-level
metric evaluation remains unchanged: a Kendall’s
Tau-like formulation of a given metric’s agreement
with pairwise human assessment of translations,
collected through 5-way relative ranking (RR).
However, we also trial evaluation of segmentlevel metrics with reference to segment-level DA
human assessment (for the main translation task
data set) and a semantic-based manual judgments
HUME (for himl2015 data set).
8Bulgarian and Basque IT translation tasks included only
two participating systems and all metrics were able to order
them correctly, all resulting in a correlation of 1.0.
MTEVALNIST
MTEVALBLEU
newstest2016
Table 7: Absolute Pearson correlation of system-level metric scores with 10K hybrid systems: DA Hybrid = direct assessment of translation adequacy of 10K hybrid MT systems.
MTEVALNIST
MTEVALBLEU
ittest2016
Table 8: System-level metric results (ittest2016): Pearson correlation of system-level metric scores with
human assessment computed over standard WMT relative ranking (RR) human assessments; absolute
values of correlation coefﬁcients reported for all metrics.
ittest2016
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
ittest2016
mtevalNIST
mtevalBLEU
mtevalBLEU
mtevalNIST
ittest2016
mtevalBLEU
mtevalNIST
mtevalNIST
mtevalBLEU
Figure 7: System-level metric ittest2016 signiﬁcance test results for differences in metric correlation
with human assessment for remaining out-of-English language pairs evaluated with relative ranking (RR)
human assessment.
Segment-level DA Evaluation
Segment-level
DA adequacy scores, as described in Section
2.3.2, are employed as gold standard human
scores for translations. Since DA segment-level
scores are absolute judgments, in their raw (nonstandardized) form corresponding simply to a percentage of the absolute adequacy of a given translation, evaluation of metrics simply takes the form
of the computation of a Pearson correlation coef-
ﬁcient between metric and DA scores for translations. Signiﬁcance of differences in metric performance, as in system-level DA metric evaluation, takes the form of Williams test for the signiﬁcance of a difference in dependent correlations
 .
Segment-level HUME evaluation
The evaluation of segment-level metrics with reference to
HUME scores operates in a similar way to DA,
by computing the Pearson correlation of HUME
evaluation scores for individual translations with
metric scores. Williams test is also applied to test
for signiﬁcant differences in metric performance.
Kendall’s Tau-like Formulation
We measure
the quality of metrics’ segment-level scores using a Kendall’s Tau-like formulation, which is
an adaptation of the conventional Kendall’s Tau
coefﬁcient.
Since we do not have a total order ranking of all translations we use to evaluate
metrics, it is not possible to apply conventional
Kendall’s Tau given the current RR human evaluation setup . Vazquez-Alvarez
and Huckvale also note that a genuine pairwise comparison is likely to lead to more stable
results for segment-level metric evaluation.
Our Kendall’s Tau-like formulation, τ, for
segment-level evaluation is as follows:
τ = |Concordant| −|Discordant|
|Concordant| + |Discordant|
where Concordant is the set of all human comparisons for which a given metric suggests the
same order and Discordant is the set of all human
comparisons for which a given metric disagrees.
The formula is not speciﬁc with respect to ties, i.e.
cases where the annotation says that the two outputs are equally good.
The way in which ties (both in human and
metric judgment) were incorporated in computing
Kendall τ has changed across the years of WMT
metrics tasks.
Here we adopt the version from
WMT14 and WMT15. For a detailed discussion
on other options, see Mach´aˇcek and Bojar .
The method is formally described using the following matrix:
Given such a matrix Ch,m where h, m ∈{<, =
, >}9 and a metric, we compute the Kendall’s τ for
the metric the following way:
9Here the relation < always means ”is better than“ even
for metrics where the better system receives a higher score.
h,m∈{<,=,>}
Ch,m|Sh,m|
h,m∈{<,=,>}
We insert each extracted human pairwise comparison into exactly one of the nine sets Sh,m according to human and metric ranks. For example
the set S<,> contains all comparisons where the
left-hand system was ranked better than right-hand
system by humans and it was ranked the other way
round by the metric in question.
To compute the numerator of our Kendall’s τ
formulation, we take the coefﬁcients from the matrix Ch,m, use them to multiply the sizes of the corresponding sets Sh,m and then sum them up. We
do not include sets for which the value of Ch,m is
X. To compute the denominator, we simply sum
the sizes of all the sets Sh,m except those where
To summarize, the WMT16 matrix speciﬁes to:
• exclude all human ties,
• count metric’s ties only for the denominator
(thus giving no credit for giving a tie),
disagreement
human and metric judgments are counted as
Discordant,
• all cases of agreement between human
Concordant.
In previous years, we reported conﬁdence intervals for the Kendall’s Tau formulation, see Bojar et
al. for details. However, since the formulation of Kendall’s Tau is not computed in the standard way (we do not have a single overall ranking of translations, but rather rankings of sets of
5 translations), the accuracy of conﬁdence intervals computed in this way is difﬁcult to verify. To
avoid the risk of drawing incorrect conclusions of
signiﬁcant differences in metric performance, we
Figure 9: Direct Assessment (DA) segment-level
metric signiﬁcance test results for English to Russian (newstest2016): Green cells denote a significant win for the metric in a given row over the
metric in a given column according to Williams
test for difference in dependent correlation.
do not include conﬁdence intervals with this year’s
Kendall’s Tau formulation results.
Results of the segment-level human evaluation
for translations sampled from the main translation
task are shown in Tables 9 and 10, where metric correlations (for DA human assessment variant
only) not signiﬁcantly outperformed by any other
metric are highlighted in bold. Since Kendall’s
Tau are traditionally employed to conclude task
winners, while at the same time we currently lack
a known reliable method of identifying significant differences between metrics, we postpone
announcement of ofﬁcial winning segment-level
metrics until further research has been carried out
to establish a reliable method in this respect.
DA human assessment pairwise signiﬁcance
test results for differences in metric performance
are included for investigatory purposes only in
Figures 8 and 9.
Results of segment-level metrics task evaluated
with HUME on the himl2015 data set are shown
in Table 11, where metrics not signiﬁcantly outperformed by any other in a given language pair
are highlighted in bold, and these metrics are ofﬁcial winners of the himl2015 segment-level metric
evaluation. Full pairwise signiﬁcance test results
for all metrics are shown in Figure 10.
Human Gold
# Assessments
# Translations
Correlation
newstest-2016
Table 9: Segment-level metric results for to-English language pairs (newstest2016): Correlation of
segment-level metric scores with human assessment variants, where τ are ofﬁcial results computed similar to Kendall’s τ and over standard WMT relative ranking (RR) human assessments; r are Pearson
correlation coefﬁcients of metric scores with direct assessment (DA) of absolute translation adequacy;
absolute value of correlation coefﬁcients reported for all metrics.
Human Gold
# Assessments
# Translations
Correlation
newstest-2016
Table 10: Segment-level metric results for out-of-English language pairs (newstest2016): Absolute correlation of segment-level metric scores with human assessment variants, where τ are ofﬁcial results
computed similar to Kendall’s τ and over standard WMT relative ranking (RR) human assessments; r
are Pearson correlation coefﬁcients of metric scores with direct assessment (DA) of absolute translation
adequacy; absolute value of correlation coefﬁcients reported for all metrics.
cobalt.f.comp
upf.cobalt
upf.cobalt
cobalt.f.comp
cobalt.f.comp
upf.cobalt
upf.cobalt
cobalt.f.comp
cobalt.f.comp
upf.cobalt
upf.cobalt
cobalt.f.comp
cobalt.f.comp
upf.cobalt
upf.cobalt
cobalt.f.comp
cobalt.f.comp
upf.cobalt
upf.cobalt
cobalt.f.comp
cobalt.f.comp
upf.cobalt
upf.cobalt
cobalt.f.comp
Figure 8: Direct Assessment (DA) segment-level metric signiﬁcance test results for to-English language
pairs (newstest2016): Green cells denote a signiﬁcant win for the metric in a given row over the metric
in a given column according to Williams test for difference in dependent correlation.
Figure 10: HUME segment-level metric signiﬁcance test results (himl2015): Green cells denote a signiﬁcant win for the metric in a given row over the metric in a given column according to Williams test
for difference in dependent correlation; Winning metrics are those not signiﬁcantly outperformed by
any other (en-cs: CHRF3; en-de: BEER, CHRF3, CHRF2, MPEDA, CHRF1; en-pl: BEER, CHRF1,
MPEDA, CHRF2; en-ro: CHRF3).
Human Gold
Correlation
Table 11: Pearson correlation of segment-level metric scores with HUME human assessment variant.
Discussion
During the task, the DA evaluation, other than being more principled and discerning, has proved
more reliable for crowd-sourcing human evaluation of MT.
It should be noted that DA requires distinct
DA human evaluation variants for system and
segment-level evaluation, but we may not see this
as a negative but rather that DA provides a new
method of human evaluation devised speciﬁcally
for accurate evaluation of segment-level metrics.
Although this year DA was carried out through
crowd-sourcing, while RR was completed by researchers, DA is not restricted to crowd-sourcing
and could be carried out as-is by researchers or by
slight modiﬁcation by removal of the overhead of
translation assessments included in DA for quality
control. With any method of human evaluation, if
we aim at crowd-sourcing, we must keep in mind
that some languages are difﬁcult to obtain workers for, observed in the fact that this year’s WMT
only collected crowd-sourced assessment for English and Russian as a target language. Although
we employed a minimum of 15 human assessors
for segment-level evaluation of metrics per segment, it might be worth noting that preliminary
empirical evaluation has shown that the 15 human
assessments we acquire do not need to be from
distinct workers and when repeat assessments are
allowed from the same worker, this also yields a
correlation of above 0.9 with assessments of translations collected from strictly distinct workers. In
other words, DA should be technically viable for
all language pairs, if we employ researchers as opposed to crowd-sourced assessors (who may not
be available for the language) and if we allow repeated assessments of the same segment by the
same person.
Hybrid supersampling is a novel way of doing
meta-evaluation of metric performance and it provided more conclusive results. Although we carried out hybrid supersampling for DA human evaluation only, the method is not DA speciﬁc, and it
would be interesting to trial it with RR the future.
Character-level metrics again gave very good
results on both system and segment level.
trend that started on WMT14 with BEER, then
continued on WMT15 with BEER and CHRF,
now happens with BEER, CHRF and CHARAC-
TER. This growing number of character-level metrics suggests that community (at least the one that
develops metrics) had started to adopt characterlevel matching as an important component of evaluation.
Just like in previous years, metrics that train
their parameters get very high correlation with
human judgment as exempliﬁed with BEER and
UOW.REVAL. This year’s edition of the metrics
task introduced different types of golden truths
that opens the question towards which golden truth
should metrics be trained. Should it be for RR by
using some learning-to-rank algorithms, or for DA
by using regression algorithms or some combination of the two.
The results this year again include surprises.
For instance, evaluation of English-to-Czech this
year suggests that WORDF, BLEU and NIST outperform CHRF under evaluation against RR both
with and without tuning systems (Figure 4) on the
news domain, whereas we have seen the exact op-
posite last year. The IT domain for English-to-
Czech stays in line with last year’s observations.
BLEU (and especially its Moses implementation) has been clearly outperformed by many metrics. That again highlights the question in MT as to
why almost all systems remain to be optimized for
BLEU. Optimization towards BLEU has driven
system development and certainly achieved results
in the past, but the relatively low correlation with
human judgment is a sign that some alternative
metrics should be considered. For this reason, we
encourage metrics developers to add their metric
to Moses scorer so that the MT community can
more easily experiment with employing them as
optimization objective functions.
An additional
motivation should also be so that valuable development work on metrics is not lost in the future. If
added to Moses scorer, future metrics tasks could
run easily these metrics as baselines, even if their
authors are not participating in the task that year.
That way, good performing metrics will live on
and the results of the metrics task will be more
comparable across years.
Conclusion
In this paper, we summarized the results of the
WMT16 Metrics Shared Task, which assesses the
quality of various automatic machine translation
metrics. As in previous years, human judgments
collected in WMT16 serve as the golden truth and
we check how well the metrics predict the judgments at the level of individual sentences as well
as at the level of the whole test set (system-level).
The more extensive meta-evaluation in this
years task that involved large number of language
pairs, different types of judgments and better measurements of the signiﬁcance would hopefully
shed some more light on the qualities of different
The patterns that can be observed in the results
are that character-level metrics perform really well
and that the number of them is growing over the
years. Also, the trained metrics on average are performing better than non-trained metrics, especially
for into-English language pairs.
Acknowledgments
We wouldn’t be able to put this experiment together without tight collaboration with Matt Post
and Christian Federmann who were running the
core of WMT Shared Translation Task.
project has received funding from the European
Union’s Horizon 2020 research and innovation
programme under grant agreements no 645452
(QT21) and no 644402 (HimL). The work on this
project was also supported by the Dutch organization for scientiﬁc research STW grant nr. 12271.