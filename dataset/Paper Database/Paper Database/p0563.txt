GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning
Govardana Sachithanandam Ramachandran1, Ivan Brugere2 *, Lav R. Varshney3* , and Caiming
1 Salesforce Research
2 University of Illinois at Chicago
3 University of Illinois at Urbana-Champaign
 , , , 
Disparate access to resources by different subpopulations is a
prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial
groups to more easily access resources such as high-quality
schools, grocery stores, and polling places. Similarly, social
networks within universities and organizations may enable
certain groups to more easily access people with valuable information or inﬂuence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to
enhance equity in networked systems by editing graph edges
under budget constraints. We prove such problems are NPhard, and cannot be approximated within a factor of (1 −
We develop a principled, sample- and time- efﬁcient Markov
Reward Process (MRP)-based mechanism design framework
for GAEA. Our algorithm outperforms baselines on a diverse
set of synthetic graphs. We further demonstrate the method on
real-world networks, by merging public census, school, and
transportation datasets for the city of Chicago and applying our
algorithm to ﬁnd human-interpretable edits to the bus network
that enhance equitable access to high-quality schools across
racial groups. Further experiments on Facebook networks of
universities yield sets of new social connections that would
increase equitable access to certain attributed nodes across
gender groups.
Introduction
Designing systems and infrastructures to enable equitable
access to resources has been a longstanding problem in economics, planning, and public policy. A classical setting where
equity is a strong design criterion is facility location for public goods such as schools, grocery stores, and voting booths
 . Since individual-level fairness is often impossible for these spatially-structured problems, the focus has
been on group-level fairness; several metrics have been proposed and optimized , including
simultaneous optimization of several metrics . In this paper, we take up the same challenge of equitable access to resources, but for network-structured rather
than spatially-structured problems. We speciﬁcally consider
editing the edges of graphs under budget constraints to improve equity in group-level access, under a diffusion model
*I. Brugere and L. R. Varshney completed this work while they
were with Salesforce Research.
of mobility dynamics. We use the demographic parity metric
 . In economics, a common measure
of (in)equity called the Gini index is often used to characterize the statistical dispersion in income or wealth, but also
access to resources at individual or group levels , which we
also measure.
Facility location problems (not considering equity) have
been well-studied in network settings for applications in epidemiology , surveillance , and inﬂuence maximization . These involve changing node properties of a
graph; contrarily we edit the edges of graphs. Formally we
prove our graph editing problem—Graph Augmentation for
Equitable Access (GAEA)—is a generalization of facility
location and that it is NP-hard.
In urban transportation networks, different racial, ethnic,
or socioeconomic groups may have varying access to highquality schools, libraries, grocery stores, and voting booths,
which in turn lead to disparate health and educational outcomes, and political power. Interventions to enhance equity
in transportation networks may include increasing public
transit for the most effective paths to resources such as highquality schools. From an implementation perspective, changing bus routes may be easier than relocating schools. We do
not consider strategic aspects of congestion in transportation
networks .
In social networks within organizations like schools, people in different racial or gender groups may have varying access to speciﬁc people that hold valuable information or have
signiﬁcant inﬂuence, whom we cast as network resources.
This in turn may lead to disparate outcomes in social life. Interventions to enhance equity include encouraging friendship
between speciﬁc individuals in the social network. This can
be done, e.g. in university settings by offering free meals for
two speciﬁc people to meet: what we call buddy lunches.
Since AI models are embedded in numerous consequential sociotechnical systems, ensuring equity in their operation has emerged as a fundamental challenge . Of more relevance to
us, however, AI techniques are being used to design equitable public policies e.g. for taxation .
Here we use AI methods for system/infrastructure redesign
 
(a) Chicago demographics by
race/ethnicity
(b) Chicago transit network
and school locations
Figure 1: Chicago demographics and infrastructure, (a) shows
demographics, demonstrating highly segregated areas of the
city by race and ethnicity; (b) shows a transit network (red)
we collected for this work, induced from Chicago Transit
Authority bus routes. We also show (yellow) the location of
schools within our dataset from the Chicago Public Schools.
to increase equitable access to resources for settings such as
transportation networks and social networks . In particular, we develop a Markov
Reward Process (MRP) framework and a principled, sampleand time-efﬁcient reinforcement learning technique for the
GAEA optimization.1 Our approach produces interpretable,
localized graph edits and outperforms deterministic baselines
on both synthetic and real-world networks.
We demonstrate the effectiveness of our approach on realworld transportation network data from the City of Chicago.
Chicago is the most spatially segregated city in the United
States via 2010 census data . At the same time,
prior work shows this segregation yields signiﬁcant disparity in education and
health outcomes
by race and ethnicity, particularly among White, Black, and
Hispanic communities. In the Chicago Public School District 299—constituting the entire city of Chicago—White
students are the equivalent of 3 academic years ahead of
Black students, and 2.3 years ahead of Hispanic students.
Our technique yields an edited graph which reduces disparity
in physical access to high-quality schools.
We also present a similar analysis for gender in university social networks using the node-attributed Facebook100
network data .
Contributions
Our main contributions are as follows.
• The novel Graph Augmentation for Equitable Access
(GAEA) problem, which generalizes facility location, and
which we prove is NP-hard.
1Code, and data: 
• Markov Reward Process framework to address this difﬁcult
optimization problem, yielding principled, sample- and
time-efﬁcient mechanism design based on reinforcement
• Demonstration of efﬁcacy in both synthetic and real-world
networks (transportation and social), showing performance
improvement over deterministic baselines.
Related Work
Equity in AI
Recent work on fairness in machine learning
has expanded very rapidly. Mehrabi et al. outline 23
deﬁnitions of bias introduced from underlying AI models, 6
deﬁnitions of discrimination (i.e. the prejudicial and disparate
impacts of bias), and 10 deﬁnitions of fairness which mitigate
these impacts. There is a similar zoo of deﬁnitions for equity
in facility location . We focus
here on demographic parity.
This is the ﬁrst work measuring and mitigating inequity
within an arbitrarily-structured graph environment.
Graph Diffusion, Augmentation, and Reinforcement
Our formulation of equitable access lies within
a larger body of work on diffusion in graphs. Prior work has
examined event detection time in sensor networks , or inﬂuence maximization in social networks
 . The
problem can also be viewed as maximization of graph connectivity, maximization of spectral radius, or maximization of
closeness centrality of reward nodes .
Graph augmentation is a class of combinatorial graph problems to identify the minimum-sized set of edges to add to
a graph to satisfy some connectivity property, such as kconnectivity or strong connectedness : our objective is not such a straightforward combinatorial property.
Prior work in graph reinforcement learning primarily focuses on coordinating agents with respect to local structure
for cooperative and/or competitive tasks .
Our work differs in that we learn a system design policy
rather than coordination among active agents.
Problem Deﬁnition
Toy Example
Figure 2 shows a toy example of our problem. Assume we
have a ﬁxed topology where new edges can form, shown in
the ﬁgure as gray arrows. We have two distinct population
of purple and red individuals distributed differently over the
nodes of the graph. Based on the graph edges, each group
has different levels of access to the same resources, in this
case access to schools. The individuals of purple population
can access the school at node 3 in two hops 1-2-3, whereas
individuals of red population have to traverse one extra hop
i.e. 4-1-2-3 to access the same resource.
Suppose we have a budget to edit one edge, forming the
edge 4-2 will match the red and purple population’s access to
school. If we have one more edge in the budget, augmenting
the additional edge 2-3 will improve access for both populations equitably. This sequence of edge augmentations that
improve resource access for the overall population and at the
Figure 2: A before and after equitable graph augmentation of
a toy schematic showing school access between two disparate
populations distributed spatially different from each other.
Individuals from the group traverse along the edges of their
respective color.
same time maintains equity among subpopulations is what
this work strives to achieve. We restrict edge formation to a
predeﬁned topology, since arbitrary edge formations are not
feasible in many real sociotechnical graphs, e.g. in spatial
graphs, an edge between two nodes that are arbitrarily far
apart cannot be formed.
Preliminaries
Let graph G = (V, E) have vertex set V = {v1, . . . , vn}
of size n and edge set E = {ei,j} of size m. Edges are
unweighted and directed. Let G be a set of groups such as
racial or gender groups. Let reward nodes be a subset of
nodes R ⊆V .
We consider a particle pg ∈V of group g ∈G as an
instance of starting node positions sampled from a groupspeciﬁc distribution µ0(g). Letting d(pg, r) be the shortest
path for particle pg along edges E of G to reach a reward
node in R, we deﬁne a utility function for each group as:
ug(pg; E) = Epg∼µ(g)
The utility function is parameterized by the edge set E. For
simplicity, we refer to ug(pg; E) as just ug and deﬁne the
utility of the entire group g as:
Ug = Epg∼µ(g)[ug].
We deﬁne U G = P
c∈G Ug/|G| as mean utility of all groups
and inequity as deviation of group utilities from the mean. i.e
g∈G |Ug −U G|. To minimize inequity is to minimize this
difference. Finally, let a graph augmentation function for a
budget B be deﬁned as:
e : G, B →G′
where G′ = (V, E ∪Eu), Eu is the edge augmentation to the
graph G constrained by budget B, under Hamming distance
D(G −G′) < B.
Now let us formalize our problem deﬁnition.
Problem 1: Graph Augmentation for Equitable Access
Find: G′ = e(G, B)
Where: G′ =
g∈G |Ug−U G|=0
Greedy Baseline
For prior baseline, to the best of our knowledge we are not
aware of earlier works that does budgeted discrete equitable
graph augmentation that maximize utility of disparate groups.
That said, we observe that without equitable access across
groups, optimizing for maximum utility, Ug alone reduces
the problem to maximizing centrality of the reward nodes
r ∈R. Hence we extend the greedy improvement for any
monotone submodular Ug proposed by 
to the equitable group access setting, which we call Greedy
Equitable Centrality Improvement (GECI) baseline. For a
given edge set E, we deﬁne neighborhood Ng(E) as all
nodes u ∈V and u ̸∈E that are in the shortest path to
reward node r less than T steps for candidate from group g.
With budget B, the GECI method is deﬁned in Method 2.
Method 2: Greedy equitable centrality improvement
Input: A directed graph G = (V, E), neighborhood
function Ng, and budget B
for b = 1, 2, . . . , B do
Ee := E ∪Eu
gmin := argmin {Ug(Ee)|g ∈G)}
for u ∈V |u ∈Ngmin(Ee) do
for v ∈V |v ∈Ee do
Compute Ugmin(Ee ∪{(u, v)})
umax, vmax := argmax {Ugmin(Ee ∪{(u, v)})}
Eu := Eu ∪{(umax, vmax)}
For every augmentation of edge Eu, we pick the group
gmin that is most disadvantageous or in other words the
group with least group utility Ug. We then pick a pair of
nodes (umax, vmax) to form an edge augmentation, such
that nodes vmax and umax are in E and in the neighborhood Ngmin respectively and result in maximum change
in most disadvantageous group’s utility Ugmin, for the candidate group gmin, We update the edge augmentation set
Eu := Eu ∪{(umax, vmax)}. The graph augmentation step
is repeated until the budget B is exhausted.
Optimization Formulation
Let us consider the GAEA problem from an optimization
perspective. Let Ug be the expected utility of a group. Then
Pareto optimality of utilities of all groups can be framed as:
g∈G |Ug−U G|=0
g∈G ∥Eu∥0<B
The ﬁrst constraint in (4) is the equity constraint while the
second is the budget constraint. The constraints are nondifferentiable, especially the number of discrete edges to edit,
and cannot be solved directly using typical algorithms. Hence
we develop our learning approach.
Equitable Mechanism Design in MRP
We frame the graph, G = (V, E), and dynamic process of
reaching reward nodes by particles of different group, g ∈G,
as mechanism design of a ﬁnite-horizon Markov Reward
Process (MRP). The MRP consists of a ﬁnite set of states,
S; dynamics of the system deﬁned by a set of Markov state
transition probability P; a reward function, R ∈R|S|; and a
horizon deﬁned by the maximum time step T reachable in
a random walk. Here states S corresponds to vertices V of
the graph G, and the transition dynamics are parameterized
by P = D−1E, with diagonal matrix D(i, i) = P
Unlike Markov Decision Processes (MDP), MRP does not
optimize for a policy, instead optimizing for dynamics that
maximize the state value function.
The state value function of the MRP for a particle, pg of
group g, spawned at state s0 is given by:
where γ ∈ is the discount factor. This γ encourages the
learning system to choose shorter paths reachable under the
horizon T. The expected value function for the group, g, is:
V g = Es0∼µ(g)[vg(s0)].
We parameterize the transition probability as P = D−1Ee,
where Ee = E + A ⊙Eu and diagonal matrix D satisﬁes
D(i, i) = P
j Ee(i, j).
Here E ∈{0, 1}|S|×|S| is the edge-adjacency matrix of
the unedited original graph G. Further, A ∈{0, 1}|S|×|S| is
a mask adjacency matrix corresponding to given topology,
used to restrict the candidate edges for edit. This restriction
is useful for spatial graphs where very distant nodes cannot
realistically form an edge. The set Eu ∈{0, 1}|S|×|S| is the
learned discrete choice of edges that are augmented. To make
discrete edge augmentation Eu differentiable, we perform
continuous relaxation, with reparameterization trick using
Gumbel sigmoid , deﬁned by
Eu(i, j) =
(1 + exp(−(φ(⃗0) + gi)/τ)
, ∀i, j ∈S
where, gi = −log(−log(U)) and U are the Gumbel noise
and uniform random noise respectively. Over the period of
training, the temperature τ is attenuated. As τ →0, Eu
becomes discrete. Hence we gradually attenuate τ ←τ ∗ν
at every epoch with a decay factor ν . Note that the function
φ(·) in (7) takes the zero vector as input, which effectively
forces the function to learn only the bias term and makes the
choice of edits independent of the input state. The problem
objective under MRP framing is:
g∈G |Vg−V G|=0
g∈G∥Eu∥0<B
Here V G = P
g∈G Vg/|G|. We recast the constrained optimization as unconstrained optimization using augmented
Lagrangian , as:
2 −µ2(min(0,
∥Eu∥0 −B))2
|Vg −VG|) −λ2(min(0,
∥Eu∥0 −B)).
Here µ1 and µ2 are problem-speciﬁc hyperparameters. The
Lagrangians of (9), λ1 and λ2, are updated at every epoch:
|Vg −V G|),
+ µ2(min(0,
∥Eu∥0 −B)).
This objective effectively learns the dynamics P, of the
MRP. Since (9) optimizes for Pareto optimality over multiple
objectives, the resulting gradient of the stochastic minibatch
will tend to be noisy. To prevent such noisy gradient, we train
only on either the main objective or one of the constraints
at any minibatch. We devise a training schedule where the
objective J is optimized without constraint and as it saturates
we introduce the equity constraint followed by the edit budget
constraint and ﬁnally as losses saturate, we force discretizing
the edge selection by gradually annealing the temperature
τ of the Gumbel sigmoid. These scheduling schemes are
problem-speciﬁc and are hyperparameter-tuned for best results. We detail this scheduling strategy in Method 3.
Facility Location as Special Case
An alternative to augmenting edges Eu in a graph G is to make resources equitably accessible to particles pg µ(g) of different groups
g ∈G by selecting optimal placement of reward nodes without changing the edges: a facility location problem . With minor changes to our
MRP framework, equitable facility placement can be solved.
Speciﬁcally, the dynamics of the MRP, P are ﬁxed and the objective is parameterized by the reward vector R ∈{0, 1}|S|.
The optimization in (8) can be adapted to:
g∈G |Vg−V G|=0
(1+exp(−(φ(⃗0)+gs)/τ), ∀s ∈S
Besides a small change to the objective, the original MRP
framework works for equitable facility placement as well.
Theoretical Analyses
Computational Complexity of GAEA
Theorem 1. The GAEA problem is in class NP-hard that
cannot be approximated within a factor of (1 −1
Method 3: Equitable mechanism design in MRP
Input: The original weight matrix W 0
Initialize τ = 1, λ1 = 0, λ2 = 0, µ1, µ2, α
update constraint = True
for until convergence do
for Each ADAM optimized minibatch do
Sample s0 ∼µ(g)
if update constraint then
θ ←θ −α∇θ(−µ1(P
g∈G |Vg −VG|)
g∈G |Vg −VG|))
α∇θ(−µ2(min(0, P
g∈G Eg −B))2 −
λ2(min(0, P
g∈G Eg −B)))
θ ←θ −α∇(P
update constraint ←not update constraint
if equity schedule condition is met then
Update λ1 (Equation 10)
if edit schedule condition is met then
Update λ2 (Equation 11)
if temperature schedule condition is met then
Proof. Consider a subproblem of GAEA: maximization of
expected utility of a single group and hence no constraints on
equity. Let us assume there is only one reward node r ∈V
and drop the constraint of the node being reachable in T
steps. Now the problem reduces to augmenting a set of edges,
Eu = {Eu(i, j) ̸∈E}, to improve closeness cr = 1/dvr of
reward nodes r. Here dvr is distance to vertex v from reward
node r; the optimization problem reduces to
s.t.∥Eu∥0<B
The GAEA problem now reduces to the Maximum Closeness
Improvement Problem which through
Maximum Set Cover, has been proven to be NP-hard and
cannot be approximated within a factor of (1 −1
3e), unless P
= NP .
Computational Complexity of Facility Placement
Facility placement is proven to be submodular . For the unit-cost case there exists a greedy solution that is (1 −1
e)-approximate. There is a tighter problemdependent 1
e) bound .
Complexity Analysis
Here we analyze the time complexity of each
minibatch while training in our MRP. The complexity of
the forward pass is mainly in estimating the expected value
function of each group vg(s0) = PT −1
t=0 γtRP ts0, which
is dominated by the computation of P ts. This can be done
by recursively computing Ps for T time steps, resulting in
minibatch complexity O(B · |G| · T · |V |2). Alternatively
P t can be computed once every minibatch, with complexity
O(|V |2.37) , resulting in overall complexity of
O(T ·|V |2.37+B·|G|·|V |2). For large networks |V | ≫BGT,
and the complexity reduces to O(|V |2).
Introducing group equity into the work of Crescenzi
et al. , the time complexity of greedy improvement
strategy becomes O(B ·|E|·|V |·|G|·O(Ug)), where O(Ug)
is the complexity of computing the utility of a group.
On Mixing Time
Here we analyze the mixing time of EDM-MRP formulation.
Let us add a virtual absorption node ra to the graph G, such
that all reward nodes r ∈R almost surely transition to ra.
The state distribution at time step t is given by st = P ts0. At
optimality, in a connected graph, the objective is to have all
nodes reach a reward node within timestep T and therefore
reach absorption node ra within T + 1 timesteps, which
results in a steady-state distribution limt→∞st = ra. The
convergence speed of s0 to ra is given by the asymptotic
convergence factor :
ρ(P) = sup
 ||st −ra||2
||s0 −ra||2
, s0 ∼µ(g)∀g ∈G
and associated convergence time
While discount factor γ relates to expected episode length T
 by
Let ρgadv and ρgdis−adv be the convergence factors corresponding to the most advantageous and disadvantageous
group that require least Tgadv and most Tgdis−adv timesteps to
ra. The constraints of the optimization is to make Tgdis−adv
match Tgadv with discrete edits to graph G less than budget
B. For such matching to happen, from (17) the choice of γ
should be.
Hence the optimal dynamics P ∗is bounded by choice of
the discount factor γ, in (6) of the MRP which in turn is
inﬂuenced by the given graph’s G inequity of convergence
factors ρ(Pdis−adv) - ρ(Padv) and budget B.
Evaluation: Synthetic Graphs
Many real-world sociotechnical networks can be approximated by synthetic graphs; hence we evaluate our method on
several synthetic graph models that yield instances of graphs
with a desired set of properties. We evaluate our proposed
graph editing method with respect to the parameters of the
graph model.
Erd¨os-R´enyi Random Graph (ER)
The Erd¨os-R´enyi random graph is parameterized by p, the uniform probability of an edge between two nodes. The expected
node degree is therefore p|N|, where |N| is the number of
nodes in G. We use this model to measure the effectiveness
of our method with varying graph densities. As the density
increases, it will be more difﬁcult to affect the reward of
nodes through uncoordinated edge changes.
Preferential Attachment Cluster Graph (PA)
The Preferential Attachment Cluster Graph graph model
 is an extension of the Barab´asi–Albert
graph model. This model is parameterized by m added edges
per new node, and the probability p of adding an edge to
close a triangle between three nodes.
Chung-Lu Power Law Graph (CL)
The Chung and Lu model yields a graph with expected degree distribution of an input degree sequence d. We
sample a power-law degree distribution, yielding a model
parameterized by γ for P(k) ∼k−γ. This is the likelihood
of sampling a node of degree k. In this model, γ = 0 yields
a random-degree graph and increasing γ yields more skewed
distribution (fewer high-degree nodes and more low-degree
nodes). We use this graph model to measure performance
with respect to node centrality. As γ increases, routing is
more likely through high-degree nodes (their centrality increases). We place rewards at high-degree nodes.
Stochastic Block Model (SBM)
The Stochastic Block Model samples edges within and between M clusters. The
model is parameterized by an [M × M] edge probability
matrix. Typically, intra-block edges have a higher probability: mi,i > mi,j, where j ̸= i.We use this model to measure performance at routing between clusters. In this setting,
we instantiate two equal-sized clusters with respective intraand inter-cluster probability: [0.1, 0.01]. We sample particles
starting within each cluster. This experiment measures ability
to direct particles into a sparsely connected area of the graph.
This may be relevant in social or information graphs where
rewards are only available in certain communities.
Edge and Particle Deﬁnitions
For each graph ensemble, we create a graph edge set, which
we then sample two or more edge-weight sets and sets of
diffusion particles. For simplicity we will cover sampling
two, for red and a black diffusion particles. For all synthetic
experiments, for black diffusion particles we deﬁne edge
weights proportional to node degree:
wi,j = deg(i) · deg(j).
For red particles, we deﬁne edge weights inversely proportional to degree nodes analogous to the above. For each diffusion step, a particle at node i transitions to a neighboring node
by sampling from the normalized distribution weight of edge
incident to i. This weighting means black particles probabilistically favor diffusion through high-degree nodes, whereas
red particles favor diffusion through low-degree nodes. We
use random initial placement of particles within the graph.
The difference in edge diffusion dynamics thus constitute
bias within the environment.
Problem Instances: Reward Placement
For each synthetic graph ensemble, we specify two different
problems by varying the deﬁnition of reward nodes on the
graph. For the high-degree problem, we sample k = 3 nodes
proportional to their degree:
j∈V deg(j).
For the low-degree problem, we sample k = 3 nodes inversely proportional to their degree, analogous to the above.
This means that especially in power-law graphs such as PA
and CL, black particles which favor high-degree nodes are advantaged and should have a higher expected reward. However,
we also hypothesize that black particles could be advantaged
in the low-degree placement, because routing necessarily
occurs through high-degree nodes for graphs with highly
skewed degree distributions. Overall, we hypothesize the
low-degree problem instance is relatively harder for graph
augmentation methods.
Evaluation
We evaluate equity and utility for the graph produced by
our method, comparing against the input graph and the baseline editing method. To deﬁne utility, we estimate the expected reward per group by repeated Monte Carlo sampling
of weighted walks through the graph. First, we sample the
starting node of an individual with respect to their initial distribution, then estimate their expected reward over weighted
walks from the starting node. Repeatedly sampling individuals yields an expected utility for the graph with respect to
each group. We measure the total expected reward per population (utility), and the difference in expected reward between
classes (equity). Further, while our model only optimizes the
expectation, it performs surprisingly well at minimizing the
Gini Index.
Evaluation Metrics
Average Reward
We measure three graphs in experiments:
the initial graph before editing, and outputs of the GECI baseline and our proposed method. We simulate 5000 weighted
walks by the initial distributions of each particle type. Average reward is aggregated across these particle types.
Gini Index
The Gini Index is a measure of inequality. It
measures the cumulative proportion of a population vs. the
cumulative share of value (e.g. reward) for the population. At
equality, the cumulative fraction of the population is equal to
the cumulative reward. The measure is the deviation from this
x = y line, with 1 being total inequality, and 0 total equality.
Synthetic Results
Figure 3 overviews our synthetic results. We see that on all
graphs and over almost all budgets, the proposed model outperforms the baseline. Further, we especially see that in the
low-budget scenario, our model outperforms on Gini Index.
Our model outperforms the baseline as much as 0.5 in utility under the same budget. In particular, PA and ER graphs
improve the most. Figure 4 gives the main empirical result
of our synthetic experiments. On the 8 experiments of 4 different synthetic graphs, we plot the utility vs. the Gini Index
across Monte Carlo simulations of the original, baseline, and
proposed method graphs. The bottom-right reﬂects the best
performance. Notice the proposed model outperforms the
baseline on all synthetic graphs. Particularly notable, the
(a) GECI baseline
(b) Proposed method
(c) GECI baseline
(d) Proposed model
Figure 3: (a-b) Budget vs Gini Index. This shows the Gini Index for varying budgets for (a) the GECI baseline and (b) our
proposed method. Our proposed model performs better, particularly at a smaller budget. (c-d) Budget vs Utility. This shows
the utility for varying budgets for (c) the GECI baseline and (d) our proposed method. Our proposed method outperforms the
deterministic baseline on all graphs at all budgets.
Chung-Lu Power Law graph is the worst performing original
graph in terms of both utility and Gini Index. However, on
the low-degree problem instance, our method nearly doubles
the utility performance of the baseline.
Facility Placement
As discussed, our proposed model also
solves the facility placement problem. The problem selects
a number of nodes which maximizes the reward for particles sampled onto the graph from initial distributions. Figure
5 shows a simple experiment adapting our model for this
problem. For brevity, we only show a small example experiment. In black, we see the curve of the Gini Index decreasing
on increased budget of 15 for a synthetic PA graph of size
|N| = 200. At the same time, the average utility increases
to approximately the same budget. Note the initial location
of PA high-degree under budget 3 using the greedy PA highdegree heuristic (Figure 4a). This is approximately 4 for both
Gini Index and Reward. Our method maintains far lower Gini
Index. Therefore this node set largely covers the transition
Chicago Schools
Proposed Model
Avg. Utility
Gini Index
Table 1: Chicago Public School with budget 400
dynamics of the initial distributions.
Real-World Applications
Equitable School Access in Chicago
In this section we study school inequity in the city of Chicago.
We infer a coarse transportation network using the trajectory
data of public bus lines from the Chicago Transit Authority
(CTA).2 Nodes are given by route intersections, and edges are
2 
(a) Unedited Graph
(b) GECI Baseline
(c) Proposed method
Figure 4: A comparison of Utility vs. Gini Index across 8
synthetic experiments. Bottom-right is best. The proposed
method performs best on all experiments.
Figure 5: Facility placement results showing varying budget
(facilities) vs. total Gini index and utility.
inferred from neighboring preceding and following intersections. This yields a graph with 2011 nodes and 7984 edges.
We collect school location and quality evaluation data from
the Chicago Public School (CPS) data portal.3 We use the
2018-2019 School Quality Rating Policy assessment and select elementary or high schools with an assessment of ”Level
1+,” corresponding to “exceptional performance” of schools
over the 90th percentile. We select only non-charter, “network” schools which represent typical public schools. We
use geolocation provided by CPS to create nodes within the
graph. We attach these nodes to the graph using 2-nearest
neighbor search to the transportation nodes. Finally, we collect tract-level demographic data from the 2010 census.4 We
sample three classes of particle onto the network, representing White, Black, and Hispanic individuals by their respective
empirical distribution over census tracts. We then randomly
sample nodes within that tract to assign the particle’s initial
position. We equally set initial edge weights for all groups,
with weights inversely proportional to edge distance.
Table 1 shows the result for a budget of 400 edges in the
Chicago transportation network. We see that the baseline is
surprisingly ineffective at increasing reward. Our method
successfully optimizes for both utility and equity, achieving
a very high performance on both metrics. Note that both the
baseline and our proposed model make the same number of
edits on the graph. We hypothesize the baseline performs
poorly on graphs with a high diameter such as infrastructure
graphs. Recall we similarly saw the baseline performs poorly
on ER (Figure 4b), which has relatively dense routing. In
contrast, our model learns the full reward function over the
3 
4 
(a) Caltech.
(b) Reed College.
(c) Michigan State.
Figure 6: Mean shortest path of gender groups from the
inﬂuence nodes
topology and can discover edits at the edge of its horizon.
Equitable Access in University Social Networks
To demonstrate the versatility of the proposed methods, we
also apply them to reducing inequity in social networks. Social networks within universities and organizations may enable certain groups to more easily access people with valuable information or inﬂuence. We report experiments for university social networks using the Facebook100 data . The Facebook100 dataset contains friendship networks at 100 US universities at some
time in 2005. The node attributes of this network include:
dorm, gender, graduation year, and academic major. Analyzing Facebook networks of universities yield sets of new social
connections that would increase equitable access to certain
attributed nodes across gender groups. We deﬁne popular
seniors as the reward nodes and the objective is for freshmen
of both genders to have equitable access to these inﬂuential
nodes. In this experiment we mask the speciﬁc gender information by the term white and black particles. We demonstrate
our method on three universities.
The results are in Figures 6a, 6b, and 6c which show the
mean shortest path of gender groups from the inﬂuence nodes
at Caltech, Reed College, and Michigan State University, respectively. Table 2 shows the intra-group Gini index. With
sufﬁcient hyperparameter tuning, our RL method within our
novel MDP framework consistently outperforms the greedy
GECI baseline on intra-group Gini index and minimizing
overall shortest path of the freshman from the inﬂuence node
across groups. On the other hand the difference between average shortest distance between groups, GECI maintains tighter
margin when compared to our EMD-MRP approach. This is
explained in the slackness in the constrained optimization of
the EMD-MRP approach.
Conclusion
In this work, we proposed the Graph Augmentation for Equitable Access problem, which entails editing of graph edges,
to achieve equitable diffusion dynamics-based utility across
Mich. State
num. nodes, |V |
num. edges, |E|
num. editable edges |A|
Table 2: Graph properties of university social networks
Michigan State
Table 3: Intra-group Gini Index
disparate groups. We motivated this problem through the
application of equitable access in graphs, and in particular
applications, equitable access in infrastructure and social
networks. We evaluated our method on extensive synthetic
experiments on 4 different synthetic graph models and 8 total experimental settings. We also evaluated on real-world
There are many avenues for future work. First, our reward
function is somewhat limiting. Ideally, individuals could collect rewards on the graph in a number of ways. Second, we
measured a particular equal opportunity fairness which has a
practical mapping to our application setting. However, other
deﬁnitions of group or subgroup-level fairness may not be
easily translatable to a graph routing/exploration problem.
In this appendix, we provide some further details on the
experiments in the main text.
Training Trajectories
Fig. 7 are the train trajectories on synthetic graphs. They
indicate the trends of mean utility across, difference in utility across group and the budget constraint. The kinks in
these learning curve are due to different scheduling schemes
kicking in. Refer to Table 4 for these scheduling details for
synthetic graph
Reproduciblity
Table 4 lists hyperparameters that were used for the different
networks we experimented with.